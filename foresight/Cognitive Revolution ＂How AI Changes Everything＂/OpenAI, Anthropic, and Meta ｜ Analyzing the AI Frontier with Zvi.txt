one of the things people constantly get wrong
if they think about human level as the peak of things.
And so like once we've patched this and now it works,
that's not really how this goes.
There is no, it goes from not working to working,
it goes from working worse to working better,
and they could always go to working better still.
And that's one of the reasons why we should be
more worried or more excited or
more curious about what's going to
happen like three years from now,
five years from now, 10 years from now,
they're just going to keep going.
And the question is, what does that get you?
We talk about like worrying about China,
but like I'm more afraid of Meta.
Like one individual American company
scares me more than all of China right now.
You know, if you understand the Yudkowsky
and difficulties, lessons, right, in some sense,
and the nature of what problems you have to solve,
or you have leadership capabilities,
then you are actually going to be valuable in those ways.
And it would be a major mistake to join an existing
organization and try to make a difference
as an individual as opposed to trying to spearhead
a new organization or at least a new,
you know, branch of a existing major organization,
depending on your skill set.
Hello, and welcome to The Cognitive Revolution,
where we interview visionary researchers,
entrepreneurs, and builders working on the frontier
of artificial intelligence.
Each week we'll explore their revolutionary ideas,
and together we'll build a picture of how AI technology
will transform work, life, and society in the coming years.
I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.
This is V. Machwitz.
Welcome back to The Cognitive Revolution.
Good to be back.
So we're trying something a little bit different this time.
We are going to do some analysis
of what has been going on in AI over,
let's say the last few weeks to a month.
You have published, as you always do,
a bunch of deep dive blog posts,
kind of covering everything.
And for folks who want your background,
of course we just did a recent episode too,
so they can go and hear about your world view
and your, you know, your AI childhood all there.
But for today, I just want to pick out
some of the most important stories
and get your take on them and kind of, you know,
exchange, go back and forth with some questions
and try to make some sense out of it.
And hopefully that'll be useful,
not just to us, but to the audience as well.
Yeah, I think the easiest thing is that there's
constantly news coming at all of us.
And so it's easy to get lost in like,
here's the thing, here's the thing,
here's another thing, here's another thing.
So that's good to step back and dive deep.
So I organized this discussion around
the concept of live players.
You know, there are only so many organizations right now
who seem to be really pushing the frontier
and in a position to have a meaningful impact
on the course of events.
We talked last time a little bit about like
how much does history matter
and it seems like it matters in some ways
and maybe less in other ways.
But these are the folks that are kind of creating
the history right now, the live players.
So I thought we would just kind of run it down
by going through some of the live players,
talking about their recent announcements and releases
and again, trying to make sense of where that fits into
the broader big picture.
And starting off naturally, we go to open AI.
So reading your blog in preparation for this,
obviously, you know, you can't go more than a few paragraphs
without open AI coming up in one way, shape, or form.
But the thing that stuck out to me
as kind of the most interesting was the recent comment
that Jan Leica had made and Jan is,
for those that don't know the name,
he's the head of alignment at open AI
and along with Ilya Sotskyver leading
the new super alignment team, as I understand it.
I want to start off by just kind of an interesting disconnect
between him and you and maybe me as well
around just the power of GPT-4.
So before we even get into, you know,
the kind of speculation about the future,
it really jumped out to me that he said,
overall, GPT-4 is maybe at the level
of a well-read college undergrad.
And then you came back and said,
you consider it to be well below human level.
And I have often said that I consider it to be human level,
but not human-like.
And I've sort of been trying to refine
what I mean by that in a few different ways over time.
But for starters, let's get your take.
What do you think is the disconnect between Jan and you
there where he sees something like human level
and you would say well below?
Yeah, I don't think it's about the specific model at all,
obviously.
I think we both agree that GPT-4 is the dominant model
right now and like we will be for some months to come,
at least.
But I think it's a matter of like,
how do you think about what it means
to be at the level of a college undergrad
or what are we measuring?
What are we judging by?
And I think Laiki is thinking about it as, OK,
in terms of ability to just deal with a variety of random
questions that were typically thrown at something,
how is it going to do compared to the average college
undergrad?
He's like, what's about that level?
You have a well-read college undergrad.
Whereas that's an important question
to be asking for practical purposes.
But to me is not the relevant question
to what the things are that we're thinking about.
And that's one of the, when he says
he's going to align a human level alignment researcher
within four years, I thought that assumes
that there's going to be a much, much more powerful AI
four years from now waiting to be aligned.
It's not talking about aligned GPT-4
and then pointing at alignment.
That obviously wouldn't do anything.
It's going to deal with some of your blocks
and it's going to increase in your affordances
and your efficiency somewhat.
Maybe you'll be 50% faster with GPT-4
than you would have been without any LMS.
Maybe even 100% faster if you're using it really well
and things connect to it really well.
And in the context of alignment, obviously
having a model to experiment with and bang on
is distinct from the thing that we're talking about here,
but is potentially necessary.
But it's not going to be able to substitute for anything
like a human researcher.
If you put a well-read college undergrad
on the problem of something complex,
like aligning a model,
they could potentially begin to make progress.
And if you asked GPT-4 to do that, you would get nothing.
And part of that is that we haven't figured out
how to structure how we talk to it
and turn it into a proper agent
and give it the proper memory and so on.
But to me, most of it is just,
every system has what you might call a raw G to it,
whether it's a human or an artificial intelligence.
And on that level, I feel like GPT-4
is still well below the IQ 100 median human.
It is going to obviously answer
my ordinary day-to-day questions much better
than if I asked an ordinary IQ 100 human
to help me out with a variety of questions.
That's because it has these huge advantages over a human.
It has access to orders of magnitude and more knowledge
and memory and ability to go through cycles.
But there's still this dynamic in my brain
where if you don't have the G,
problems that require more G than you have
become exponentially harder or then impossible to do
very quickly as you exceed that.
And in that sense,
like the college undergrad had the chance given time
and is smarter and GPT-4 is just nowhere near
that kind of thing.
So when you say that the missing pieces around memory
and kind of packaging GPT-4 or successor up into an agent,
those do feel to me also like being kind of
pretty key missing pieces here.
I mean, there are sort of potentially synergies
between those kinds of parts of a system being built out
and it just being smarter overall.
But it seems like those are like pretty distinct concepts
in that GPT-4 could have like a much better memory
and certainly people are working on all sorts of like
schemes for that and embedding databases
and how do you put stuff into the embedding database
and do you even like,
some of the most interesting stuff I've seen recently
has been kind of creating a layer of like synthetic memory
that sits on top of the raw observational memory
that tries to kind of ultimately work its way up
into something like a coherent narrative,
that could still kind of fit into prompt context length,
but kind of summarizes, synthesizes,
represents all these detailed memories
in a hopefully coherent way.
Obviously is what the developers are going for there.
Those pieces seem like, yeah, they're totally missing.
I expect them to come online, you know,
somewhat gradually, but certainly over the next six months
to a year, if not maybe even sooner.
And then I am kind of like,
it does seem like this, you know,
GPT-4 with those weaknesses kind of patched,
it does seem to me like it would be
roughly at that college under grad level.
If those things did come online,
would you see that the same way
or you still think it's like missing something super important?
No, I'm sorry.
I'm definitely not on Team Physicastic Power, right?
Like I'm in no way on that team.
However, I do think in a real sense,
what you're witnessing is, you know,
the training data covers, you know,
the vast majority of things humans do and say
and consider in various senses over text.
And, you know, within the sample of the training data,
like while you're doing things similar to the training data,
it's learned how to pattern match and copy and imitate
and work with that.
And it has a huge amount of knowledge base
and levels of association and the tools to work within that.
And if you gave it these other tools,
we'll be able to do these things
and string them together across more steps
in some important sense.
But the moment you take it out of its comfort zone,
we're asking you to do something that's distinctly different
than what has come before to be truly original.
I think your episode with the Hollywood writers
and like they talked about what was going on in the second
and trying to get the GPT forwarded to work for them.
And yeah, it was great at generating like generic schlock,
right? Like much better than they could.
And like if you needed to be like, okay,
somebody get me unstuck,
somebody get me some generic schlock based on my situation
that I happened to have been written in
because this is episode 47 of the show or whatever it is.
It could be tremendously helpful.
But whenever you asked it to actually do something
that we would recognize as distinctly creative
and original, you know, in a way that's distinct from that,
they just fall over flight every time.
And none of those problems are gonna be rescued
by any of these fixes, right?
Like they're just orthogonal problems, right?
Like I think that's the sense in which, you know,
you're going to be able to give it more capacities
to be able to navigate more of the conventional things
over longer periods more consistently.
And that's gonna have tremendous mundane utility
as I call it, or it's gonna be a much better functioning system.
But the reason why I'm focused on this other question
is because I am focused on the question
of how dangerous the system is, right?
Like I'm asking the question,
could this system potentially engage
in recursive self-improvement?
Could this program potentially pose a threat to humans?
Right?
Could it compete for resources?
Could it manipulate us?
Could it do things that are actively destructive
because it uncovers capabilities
that like weren't in its training set in various ways
and other related questions like that?
And I don't see the kinds of things
that you're talking about that I agree will come online.
I would guess that we will be far from done
with family here from now.
Like there's just sort of so much to do
in terms of scaling those up as much as possible.
Cause like one of the things people constantly get wrong
is they think about human level as the peak of things.
And so like once we've patched this and now it works,
that's not really how this goes, right?
There is no, it goes from not working to working,
it goes from working worse to working better
and it could always go to working better still.
And that's one of the reasons
why we should be more worried or more excited
or more curious about what's going to happen
like three years from now, five years from now,
10 years from now.
We look at these systems
is because there isn't gonna be a hard cap.
We're not gonna max out each of these individual capabilities
by default, they're just gonna keep going.
And the question is, what does that get you?
Kind of want to look at this from two angles.
One is going back to the original disagreement
or it's maybe less of a disagreement
and more of kind of a difference in framing
perhaps with Yian, what I would bottom line all that as
is when you think about a well-read college undergrad,
you think about high points
in that individual human's performance
that GBT-4 can't match
and it's not really a question of memory
or whatever that's kind of gating it.
And if I had to guess, I would say he's maybe more looking
at like average performance or sort of some sort of floor
perhaps like maybe top 90% or whatever,
you could frame it in a lot of different ways,
but it sounds like you're kind of concerned with high points
and he is maybe more concerned
with some sort of central tendency sort of measure.
I would put it differently.
I would say he's concerned with some sense
of average level of performance
over a range of possible tasks.
And I'm concerned with potential.
I am concerned with what the capabilities would be
if you got a chance to work with this thing
to try and make it the best it could be, right?
It doesn't necessarily have to be right now,
but the reason why we value children
and college grad and these undergraduates in these classes,
I guess this undergraduate, they're an idiot, right?
In some important sense.
They know nothing about the world.
They know nothing about how to do anything productive.
They are gonna show up on the job on day one
after graduating from college.
They're gonna be useless pieces of junk,
but a useless piece of junk that can then learn
to be something great.
And even then they're gonna only learn a very narrow portion
of the things that a individual human is capable of learning.
They're gonna learn that one job in that one area
and they're gonna be very, very specialized
compared to a TBD4.
So if you are doing generalized tests
and comparing these undergraduate who are educational,
this does try to make well-rounded in some senses.
It's gonna beat the well-rounded undergraduate
because it has this ability to read every book ever written
and everything on Reddit and everything on Twitter
and blah, blah, blah.
But when it comes down to solving a particular problem,
if you find the right undergraduate
who has focused on the particular thing that you wanna know
and you give them a chance to use their compute
and process because they're not as fast,
I think the undergraduate's gonna dominate you.
I think even a relatively normal human being,
given an opportunity,
will outperform quite resoundingly
what can be done in that way.
And that's the thing that I care about
because that's the thing that's going to potentially
both threaten us and also unleash the whips upon waves
of super amazing value that we're looking for in the future.
It's not just negative.
If we want AI to solve the problems that we haven't solved
rather than just get us nowhere faster,
in some important sense,
it's gonna have to be able to do these things, right?
These are the things where it really counts.
Hey, we'll continue our interview in a moment
after a word from our sponsors.
Hey, everybody.
If you're a business owner or founder like me,
you'll wanna know more about our sponsor NetSuite.
NetSuite provides financial software
for all your businesses.
Whether you're looking for an ERP tool or accounting software,
NetSuite gives you the visibility and control you need
to make better decisions faster.
And for the first time in NetSuite's 25 years
as the number one cloud financial system,
you can defer payments of a full NetSuite implementation
for six months.
That's no payment and no interest for six months.
And you can take advantage
of the special financing offered today.
NetSuite is number one
because they give your business everything you need
in real time, all in one place
to reduce manual processes, boost efficiency,
build forecasts, and increase productivity
across every department.
More than 36,000 companies have already upgraded in NetSuite,
gaining visibility and control over their financials,
inventory, HR, e-commerce, and more.
If you've been checking out NetSuite already,
then you know this deal is unprecedented,
no interest, no payments.
So take advantage of the special financing offer
with our promo code at netsuite.com slash cognitive,
netsuite.com slash cognitive,
to get the visibility and control your business needs
to weather any storm.
That is netsuite.com slash cognitive.
Omnike uses generative AI to enable you to launch
hundreds of thousands of ad iterations
that actually work customized across all platforms
with a click of a button.
I believe in Omnike so much that I invested in it
and I recommend you use it too.
Use CogGrav to get a 10% discount.
Yeah, it's interesting.
I'm certainly concerned with all of that too.
I think maybe I'm just more enthused
about the mundane utility in the sense of,
man, there's a lot of stupid stuff
that people spend their time doing
and I really would love to see them freed
from having to do a lot of that stuff.
But I think your term is perfect, right?
It's a lot of stupid stuff that humans have to do, right?
Like basically, even if you are an average person,
you're gonna spend the vast majority of your time
doing things that do not especially tax your intelligence.
They do not especially require you to think hard.
They do not put you at the peak of your abilities, right?
They don't put you in a zone.
They're just, okay, somebody has to file this paperwork.
Okay, somebody has to work this retail counter.
Somebody has to cash this check.
Somebody has to do this thing.
Be nice to this person.
Somebody has to make sure that someone has direct.
That's good work and noble work and it has to be done, right?
And physical labor is the same way.
If a physical laborer had to do things
that were at the peak of their mental or physical requirements
more than a few minutes or at most,
if small portion of the day, it would break them.
And also like those jobs just don't exist, right?
Like you need someone strong
so that in that moment you can have someone strong.
You need someone smart so that in the few moments
when it's important to have someone smart,
you have someone smart.
If you can then take the bottom 80% of my job
and you can do an 80% good job of that
so that I only have to do the remaining 20% of that,
now two thirds of my day is free.
And I can be three times as productive, right?
That's a tremendous leap and I agree.
That is the potential of GPT-4, right?
That's what we're looking at here is
if we understand how to use this technology properly,
we can potentially free ourselves from a lot of drudgery
and streamline a bunch of stuff
and get to do all the cool things.
And there are various traps we can fall into,
one of which is that we automate exactly the things
we don't wanna be automating,
not the things we do wanna be automating.
One of which is that the moment we notice
that paperwork is faster,
now we put in more paperwork
and now it turns out that humans are taking just as long
to do more useless stuff than they did before.
And GPT is just keeping us, letting us treadmill in place.
And there's another way that this can go wrong, right?
And also there are various weird dynamics
that can happen that can backfire.
But yeah, that's what we're trying to do.
If you wanna get the effect that Licky wants, right?
The sea change that'll let us solve problems
we couldn't solve before,
that involves these things being able to do all
the different steps that humans could do
because otherwise, whatever the bottlenecks are
that are left, become your bottlenecks
where you have to translate all the context
back from the machine world back into the human world
so that a human can process all of that
then do the hard step that this thing is still faltering on
and then transition back.
And now instead of getting orders of magnitude
and more progress, right now we're talking about
these factor of two, factor of three,
factor of five style improvements.
And that's not gonna solve the alignment problem
unless we come up with something we don't expect, right?
In and of itself, that's still worth pursuing
if we can do it, right?
We still wanna do as much of it as possible.
And it has the advantage of not being as dangerous.
But it's not the thing
that the super alignment project is trying to do, right?
The super alignment project is trying
to keep the humans out of the loop entirely.
And that should be about as scary as it sounds.
Brief digression over toward this tale of the cognitive tape.
This is a concept that I've developed
for kind of purpose of public communication.
And just trying to give people an intuition,
you know, still in the very literal way, of course,
as to the strengths of a human
and the relative strengths and weaknesses
of the best AIs today.
Listeners can see this in the AI Scouting Report
if they wanna go into the whole thing.
But do you, as you look at that,
do you see any dimensions that you would suggest
that I add that, you know, just haven't been considered?
Or do you see any disagreements
as you scan down the list?
Yeah, I think that's what we're going through
because people are not gonna have it handy
right to look at.
So, you know, for breadth, yes, the AI, as I said,
like the AI's biggest advantage is it can cover
every topic at once, it can know everything at once.
A human can't do that.
In terms of depth, yeah, a human has the advantage.
I'm not even sure I give the second level.
Like you graded the AI two out of three.
And I think I might grade it one out of three
in terms of depth.
I think the depth is a huge problem for AIs right now.
Breakthrough insight, yeah, it's three versus zero,
three versus one, it's the humans are dominating again.
You know, speed, yeah, the humans are painfully slow,
you know, 10x faster.
In terms of like actually getting it to say things
and putting outputs in real time,
it's maybe only 10x faster,
but in terms of being able to like cross information,
it's thousands and tens of thousands
and hundreds of thousands of times faster, which is, yeah.
A huge deal.
In terms of cost, you know,
we're not internalizing yet all of the costs of doing this
in an important sense,
like these companies are eating these huge losses
to try and get these dominant market positions
in the future, try to stay ahead of each other
for all these dependencies.
But yes, cost is still dominated.
AIs are already vastly cheaper when the AI is useful,
even in the real costs.
We have availability, paralyzability.
Yep, the AI has a big advantage.
It's potentially actually gonna become a problem.
There's a huge race to compute right now
where computers no longer gonna be like essentially free.
It's gonna become like kind of unpriced
in an important sense.
Interesting to wonder what's gonna happen there,
especially at industrial scales.
And by unpriced, you mean that basically
your access to GPUs is going from ability to pay
to who you know?
Yeah, or does your company have the right arrangements?
Right?
If you want one GPU for your individual computer, it's fine.
You can buy it on eBay if you have to
for some amount of money, it won't be that expensive.
If you want small amounts like the kinds
when you're just using GPD4, it's gonna be relatively easy.
But if you wanna do an AI company, right?
It's gonna be a problem because, you know
if you want industrial levels,
it's not just gonna be multiply that
by the amount you want necessarily.
It's gonna be, there isn't enough to go around.
You know, people like NVIDIA are not pricing this at market.
And so you're not have to find someone
going to sell it at the actual market price.
That number might be very, very different
from the price you think it is
because there are so many AI companies,
so many AI researchers, so many AI engineers
and they're chasing a number that can only go up so fast.
This is my understanding of the current situation.
Availability, paralyzability though,
still favors the AI.
You know, time horizon memory.
Time horizon is an interesting question.
I think this is a murky place to think.
Certainly the AI has a certain kind of memory
like a long-term memory that is vastly bigger
obviously than any human.
But in terms of being able to meaningfully hold
like particular context in their heads at once,
like humans are bad at this
and AI's are so much worse, right?
The Tyra Cohen saying context is that which is scarce.
Very much applies here.
Technology diffusion speed, yep.
We are ordered to magnitude behind here.
This is gonna be a serious problem.
You know, our OODA loops are way too slow.
And this is a, it's gonna be an increasingly huge deal.
The AI, but that matter is an interesting question
because when you are optimizing
for exactly the right type of bedside manner,
where the thing that you're asking the AI to do
is the thing that people actually want,
the AI is gonna be off the charts better than a human
because the humans are not purely optimizing for that thing.
But at the same time, if you think about like
the bedside manner of Claude or Lama
when they are refusing your request, right?
It's also simply bedside manner.
And it's terrible, right?
It's like negative one stars, right?
They are raging assholes when they refuse, right?
Like maybe we can have a conversation
about social justice rather than answering your request.
It's like, this is absurd.
Why are you calling me out for wanting information
or trying to do something fun?
You know, it's not necessary.
No human would ever do that unless they were actively mad
at you and trying to punish you for asking.
So why are you doing that?
Right?
The answer is because we trained them to do that.
But we could have trained them to do something else.
We just chose to do this instead
because that's what the RLHF parameters said to do.
And that confused me.
So, you know, what else is there?
I mean, so you said you talk about breakthrough insight
and I think more about like being able to handle
unprecedented situations,
being able to process something genuinely new, right?
As sort of the version of that
that I'm more interested in, I guess, kind of there.
Being able to properly deal with a lot of different inputs.
One thing I noticed, like when you work
with stable diffusion or other AI image generators,
what you notice is sort of they are amazing
at doing one of each type of thing at once.
So you want like one face and one person
or one set of people doing one thing with one style,
with one size, with one this, with one that.
That's fine.
But the moment you try to mix things that kind of overlap,
it will lose the thread almost immediately.
And it is very, very difficult to get it back.
So when you look at people who are generating
all of this AI art, it starts to be very, very repetitive
because there's a certain kind of complexity and detail
you can't ask for at the same time.
Because the AI can't comprehend that you want this over here
and this over here and this interact with that.
And like you'd be better off trying to create
like four different pictures and then splice them together,
right?
Or you better off trying to use like the Photoshop app
where you like highlight a certain area
and ask specifically do something in this area
and leave everything else untouched.
It's like trying to generate it all at once
is kind of hopeless.
And the LLMs like exhibit the same kind of thing
but with words, right?
Like they're vibing off of everything
and vibing into everything.
And like they have memory, long-term memory for facts,
but only can remember one vibe.
And a lot of what they're doing is based on vibing.
So it's a serious problem.
I haven't seen any serious attempts to solve it yet.
I haven't even really seen people discussing it in that way.
I'm sure these things will improve with time,
but what I think of as fundamental flaws or gaps
in their ability to process information
and actually handle complexity and context
and originality.
And this is where I see them as like
still having a long way to go and falling down.
And I don't want to make the mistake of,
oh, I will never be able to axe.
And I will never be as good as humans at Y.
And we have nothing to ever worry about.
I totally think that is not true.
But for now, right, we still have this kind of cool toy
because of these limitations, which can still, again,
substitute for the majority of the things we do spend time
doing if we are engaged in a wide variety of work
if we use it well.
Coding is one of the places where
it has a huge advantage for some people,
but other people are like, I don't code generic stuff.
It's like I have a friend whose name is Alan.
And he tried it out on my behalf.
And he said, yeah, this is interesting.
And there are some ways in which it's kind of cool.
And it's cool to know this exists.
And I never thought this existed.
But when I'm writing stuff, I am actually
trying to figure out how to do things that
weren't in this training data.
I'm not trying to re-implement the same things over and over
again, which most engineers, in fact, mostly are doing.
Because of what his job is, it turns out this thing is basically
useless, because once you take it out of its sample,
and you have to do something in a different domain,
it makes so many errors that it's not better
than just doing it yourself.
So would I bottom line that to basically robustness
if I had to add another category?
It's sort of adversarial out of distribution?
Yeah, I would say robustness.
And I would also say resilience or some form of that.
And separately, I would say, and I don't think I even
went into this, the adversarial problem.
It's totally unfair to the AIs, in some important sense,
that we're judging them this way.
Because if I got infinite clones of Nathan,
and I could ask them any sequence I wanted,
and then reset their memories in state
to the previous situation whenever I didn't
like what I got, and then just keep trying them until I can
get you to tell me what the bomb secrets are,
I guarantee you I'm getting your bomb secrets.
It's not very hard.
Humans are not that defended.
But you can't run that attack on us.
You don't get to do that.
And I can run that attack on the computer, on the LLM.
And some people have.
And in fact, recently we had a paper
with automated finding universalized attacks
against language models.
Where even GPT-4 could write the code for some of these attacks
and did.
Because if you get unlimited tries
and you get to exactly measure what the output is,
and then use that to calibrate, it's only a matter of time
before you figure out every little quirk,
and playing offense is so much easier than playing defense.
OK, cool.
So I've got two categories to add to my tale
of the cognitive tape.
Let's bounce up a level then back to your interaction
with Jan, like on the blog.
So we've just been deep down the rabbit hole
of characterization of the models
and how you guys see maybe what matters more
a little bit differently.
My guess is you would largely make
the same predictions on what it can and can't do today.
I bet it would be pretty.
You guys would have a lot of agreement, I think, in terms
of.
I would almost find out, just believe his predictions.
Like, he's worked with the models much more closely.
He's run better experiments.
He's just closer to the bare metal.
You asked him, what can he do right now?
Yeah, I mean, I'd probably just believe him.
Tell me, in your response to his comment,
you said this is a hugely positive update.
So tell me what it was that he shared with the community
on your blog that changed how you understood their super
alignment announcement and why it was such a positive update
for you.
Right.
So it's even broader than improving
my understanding of the announcement.
It's improving my understanding of OpenAI and OpenAI's
general strategy and what's going on and of Lakey in particular.
Because on the list of potentially super
important to the fate of humanity people,
he's remarkably high.
And where his head at is remarkably important,
because he is one of two people who's
going to head this tremendously important effort that
plausibly determines our fate, a non-trivial portion
of the time, depending on how it's gone about.
And so the first thing is just he engaged in detail.
Most of the time, when people who think alignment is easy,
engage with you, they do not, in fact,
look at your arguments in detail.
They do not, in fact, start to go in a technical back and forth.
And they don't treat someone like me
as raising important points and worthy of engaging
with basically an equal.
And to see that kind of curiosity, that kind of generosity,
willingness to engage, think this is a worthy use of this time,
like that in and of itself is a tremendous advantage.
He doesn't bullshit.
He doesn't give evasive answers.
He actually tries to answer the questions.
And in several cases, actually made a good point
that I hadn't thought of.
And I think, oh, yeah, this is not as bad as I thought it was.
You have a very valid thing to say here.
But most of all, just something I hadn't seen anywhere else
in which everyone else who I had talked to,
or read interpreting the announcement,
had interpreted the same way I had incorrectly
before his statement was, no, we are not
trying to train a human-level alignment researcher.
We are trying to align the human-level alignment researcher
that will inevitably emerge from the research
of various companies within a four-year time frame.
So they have short timelines for the emergence of something
that is human-level, in my sense, not human-level,
in the unsense.
What they're trying to do is not build it as fast as possible.
What they're trying to do is say, OK, when somebody does build
it, we'll be ready.
And we'll know what to do with that.
And we'll keep it under control.
And we'll share that knowledge with whoever
happens to build it first, in case Anthropa gets their first,
or Google gets their first, or someone else gets their first.
That takes the entire operation instantly
from quite plausibly just an capabilities project at heart
to, if it is accurate, clearly a net-positive good idea,
where the worst-case scenarios become things like,
you try something that doesn't work,
and you give people false hope.
And you potentially get them to implement things
they shouldn't have implemented because they didn't realize
that they didn't know how to align it, which is still kill us.
But it is so much better than actively trying
to build the thing that might kill us, in and of yourself.
So that also meant that of this 20% of compute,
they're devoting to this.
That won't be going to this other part of their effort.
The part that actually builds the alignment researcher
will have to come from the other 80%, plus the stuff
they take care from here on in.
The 20% is here for something useful.
And then you just go through the rest of it.
You can tell when somebody is reading what you've written,
and their goal is to find pithy quotes
they can dismiss.
And their goal is to reinforce their own point of view.
And alternatively, when they're actually reading
to figure out if they're wrong and be curious,
and it was clearly that second one.
He was actually asking himself, well, do you have a point?
And I didn't change his mind, as far as I could tell,
on these important issues.
But he at least revealed he had thought about these things
on a level that was deeper than what he had revealed previously,
and that he had real things to say.
And just it was by far the best comment I've ever seen
on my blog, or potentially any blog of that type, by anyone.
And so I wrote a response back again in my next post,
going through his responses, and going over them
in some detail.
And reasonably soon, I want to go over.
He had on the X-Words podcast, he recorded an episode that
was so dense that I listened to the first 10 minutes,
and I was like, I have to restart and start taking notes.
I just have to start writing things down in detail.
This is just too much content here.
And then once I have that, hopefully we can engage again.
I can figure out where to focus my attention,
because someone like him is very busy.
I don't want to just scatter shot absolutely everything
at once.
It's not reasonable.
And try to make progress that way.
And this now is, like he has proven very willing to engage.
Shaw at DeepMind has also proven very willing to engage
in a similar position.
People at Anthropic, Ola, once talked to me.
I'm sure they'd talk to me again.
And so it's clear that these people,
if you have good ideas, if you have actual reasons
to think about on technical level,
they're very happy to engage with these arguments.
And that puts us in the game, gives us a chance.
Even though I am deeply skeptical of everybody
involves plans.
Cool.
Well, that's great.
I'm glad to see, as we talked about last time,
there's a relatively small set of people
that are probably the prime target of all of this thinking
and attempt to influence others' thinking.
And so it's great to see that interaction from one
of the top targets on your blog.
And I'm glad it was such a positive one.
That's really a great development.
Turning then to Anthropic, next on our live players list.
I think everybody's probably aware that Anthropic was founded
by a number of, I believe it was seven individuals who
had been at OpenAI and left over kind of disagreements
that I don't know that have ever really been super clearly
stated publicly.
It seems from what I can tell that the relationship
between the two companies is way more positive.
And then you might expect it to be given
that one was kind of an offshoot of the other.
There's reporting that they continue to have dialogue.
And certainly they express respect for each other in public.
And then they're involved in kind of shared statements
and commitments together.
So a lot of kind of surprisingly, again,
if I just told you, hey, these two companies have split
and now they're competing in the same market,
you would assume much worse dynamics, I would think,
than that.
What is your kind of just read of the entire situation
for starters, just for context?
Like, why do we have Anthropic in your mind
as opposed to just still having just one OpenAI?
And does it feel like, I mean, maybe we just
don't have enough information to know, which is a fine answer.
But does it seem good that we have these two kind of recently
diverged efforts?
I think it's really hard to know the sign of Anthropic.
I would definitely prefer Anthropic to OpenAI,
Cedars-Paribas, if I had to choose one to exist,
like, Lakey's response was really positive.
And I think Lakey's in a good place
in terms of paying attention and thinking about these problems,
even if I think his actual ideas won't work.
But hopefully, that can be pivoted.
But ultimately, what's unique about Anthropic
is they built a culture of safety, to some extent,
and they built a culture of really appreciating
the dangers of what lies ahead.
And if anything, I saw what might even
be an unhealthy level of worry expressed
in the profile in Vox about Anthropic,
where you want everybody to be terrified,
but you don't want them to, like, let this paralyze them.
And it starts to cross over at some point into paralysis.
And I am apathetic for that.
Like, that sucks.
But the price of that is where there used to be a two-horse race.
There's now a three-horse race.
And this third horse is in it for real,
and raising a lot of capital, and promising
to do that to build the best model that's ever been built,
to try and compete for the economic space in a way
that is going to push Google and Microsoft Open AI
to grow even harder, even faster by default.
And that's going to be a problem.
They're also pushing, in some ways, on alignment.
They've definitely found some techniques
for aligning current systems that are potentially, you know,
in some ways superior to what's out there.
We'll get to that in a bit.
So I'm torn, right?
Like, Anthropic seems like a relatively good shepherd
in many ways, but the proliferation of shepherds
is inherently bad in and of itself.
The fact that Anthropic and Open AI are working reasonably well
and cooperating together.
And I have heard many people say that this is also true
between them and Google DeepMind as well,
although not quite to the same extent.
Does give us hope for the possibility of coordination
when it becomes more necessary and more important?
But I would say, you know, better Anthropic
than a company that didn't have Anthropic's culture
in its place, right?
And if only having two companies would have inevitably
caused a more serious entry to take the place of Anthropic,
then Anthropic is good.
But it would be much better if the Anthropic people could
have convinced the others at Open AI
to come around to their position
and build that culture within Open AI
rather than having stricter on their own.
And now we have two problems.
Yeah, I do ultimately know that, like, many of the people
involved in this genuinely aren't for the right reasons.
And, you know, you can go either way, right?
I wouldn't be super eager to throw them
billions of extra dollars.
I wouldn't be super eager to just wish
they had more capabilities.
I would really love for there to be an AI company that I
had sufficient confidence and faith in,
that if I had technical ideas, I could come to them,
knowing that I was helping the world by coming to them
with their ideas.
And I do not feel this way.
No, and there's nobody you would put on that list.
There are individual people, right?
I feel like I could, like, tell them as you had Kasky, right?
I could speak with certain people in the nonprofit
or, you know, rationalist spaces to ask them
about what they thought.
And I feel like that would be, like, at least a riskless
or near riskless thing to do.
But, no, I don't, I don't see a company, you know,
Anthropic might be the closest.
But, you know, I, did you do a great example, right?
The biggest contribution that Anthropic has made
is constitutional AI, right, in some important sense.
And I have a strong prior for analysis
that constitutional AI will not scale, right?
That it is a very good idea, if implemented correctly,
for GPT-4 level systems.
But then when we're talking about, you know,
the human level or greater future systems,
the artificial super intelligences,
the artificial general intelligences,
that you will not, with anything like the current technique,
get what you are hoping you will get.
And yet, like, I didn't feel comfortable.
I have actually a bunch of ideas running around in my head
of, oh, you just obviously could vastly improve
the Anthropic implementation by doing,
and then there are various things I say to myself,
or I write out, and I,
but I don't feel like telling them is a safe play
because I don't want to encourage a better version
of something I think ultimately still fails, right?
I don't think my implementation solves the core problem
that I see coming to kill the thing.
It just makes it much better at its current job.
And I would love to be able to help the world in that way,
or at least that's by my curiosity,
by being given the smackdown on why it won't work,
which is always the default thing that happens
when you have an idea.
But instead, yeah, I don't know.
So, you know, part of my hope is to encourage people
to have found more organizations
on the research alignment side
that are not trying to push capabilities,
that maybe can be places we can explore these things,
and I have some irons on the fire,
but it's too early to make any announcements.
Look forward to maybe breaking some news
on a future episode, but Anthropic put out
a really interesting blog post the other day
that, you know, in some sense had nothing to do with AI,
which was just around the security practices
that they recommend, you know,
and these things could be adopted by really any company
in any sector that has, you know,
high value IP that they want to protect.
But it was definitely interesting to see
that they are pushing, you know,
their own internal systems and practices
to a pretty high level in terms of setting up
situations like requirements for shared control,
you know, or if I forget exactly the right phrase,
but you have to have kind of two people working together
to gain access to certain production systems.
Yeah, it reminded me of like nuclear submarine,
but they didn't cite that example in the,
I think they probably wanted to steer away from that image.
And so they cited other, you know, industries
where this kind of thing is used
other than the nuclear launch sequence.
But yeah, it's like, you got to have two people there,
kind of, you know, both bringing their key
to the process in order to unlock certain capabilities.
So some pretty interesting ideas there
and recommendations for other companies.
Going to the constitutional AI and tying in also this,
this report from earlier this week
about the quote unquote universal adversarial attack.
For those that haven't seen that basically
these weird nonsensical strings have been discovered
that seem to be very effective,
if not universally effective
at kind of just being appended to an otherwise,
you know, right for refusal query,
you know, the kind of thing that, you know,
write something racist or, you know,
help me make a bomb or whatever that the,
the RLHF systems are going to just refuse.
But somehow if you put these weird, you know,
kind of nonsensical smattering of tokens on the end of it,
that has been discovered to jailbreak out of the RLHF
and you sort of get, you know, the response
you would expect if you had a purely helpful model
that would just do whatever you say, you know,
like the original GBT-4 that I read teams used to do.
Notably though, Anthropics clawed models
way less susceptible to that attack
than the other models that they tested.
It was like universal in the sense
that it seemed to apply to all the leading models
that they tried it on, at least somewhat,
but the other ones were like the majority of the time,
whereas Anthropics was like more than an order
of magnitude lower than the other providers
with something like 2% success rate,
success defined by breaking free of the constraints
by applying these weird strings.
So you folks can go read more about that paper
and exactly how it works, but, you know,
to me that was a pretty good update
for constitutional AI was like,
that seems, you know, like a real achievement
if they're an order of magnitude ahead.
There's something that they probably did not anticipate
at all, although maybe they did,
but I'm guessing that that is, you know,
kind of a unexpected type of attack.
So how would you read that?
Would you read it any differently
or understand it any differently than I would?
And, you know, why doesn't that give you more confidence
that it could continue to work in the future?
The interesting thing about that attack
is that it transfers, right?
I was completely unsurprised.
There's something of that nature,
trained to attack a given system, worked on that system.
That seems like, well, obviously that would work
as just a question of exactly what it looks like.
When it transferred in identical form, right?
Between Lama and Bard and GPT-4.
So that's funny.
I wouldn't have expected that,
but they're all being trained with ROHF
using remarkably similar techniques
on remarkably similar goals, right?
With remarkably similar evaluation metrics
and numbers in there.
So it's not that surprising
that they have very similar weaknesses.
And it also indicates, you know,
this is not a very narrow,
like you have to do exactly the right thing
to fire the bullet that calls the Death Star.
This is very much, things in this area
start to disrupt what we're going after.
And the thing that's optimized to hit Lama
is good enough to mostly hit these others as well.
But it's not good enough to hit Claude too.
Only 2% of the time.
Yeah, I mean, I think you just have 2% failures anyway.
Or something is my guess.
And it basically didn't work
as opposed to it working a little bit.
I don't, I mean, for what it's worth,
if you went and said, help me make a bomb 100 times,
I think it would refuse you 100 times.
You know, or if you took 100 naive.
Yeah, 100 uncreative ones.
Yeah, but I meant like if you start,
you start putting random scrambles in.
And my understanding was that
this attack was not infinite strengths, right?
If you asked it to like do a like slash or porno,
it would just be like, no, I'm sorry,
I'm not doing that regardless of how many characters
you put after it, right?
Or if you, there are limits.
I have not tried this at all, by the way.
I have no idea what happens when you like
ask it for weird stuff.
I just read the paper.
But my understanding is that, you know,
Claude was trained largely of constitutional AI.
And because it's so much cheaper to do per cycle,
like the vast majority of the cycles
were almost certainly constitutional AI cycles.
And this is just a fundamentally different way of training.
And this did not flux the same muscles in the same weird way.
He thought that the same set of characters worked.
And that's interesting news,
but it shouldn't be like some sort of
amazing accomplishment yet, right?
It's promising.
What you have to do is you have to train adversarily
the same way they trained on,
like I think it was Llama they trained on,
but I forgot exactly.
Train on Claude, right?
If you take the same techniques described in the paper
that used to find the exploit
and look for a new exploit of the same type in Claude
and they can't find one, now you've got something.
Right now I'm interested.
But yeah, if you use a different technique, right,
that has a lot of very different parameters on it,
it makes sense.
The thing that like sort of magically, weirdly transferred
when it really has no right to transfer
didn't transfer now.
And, you know, that's promising,
but it's far from inclusive, right?
It's too early to know.
Flipping back to OpenAI for a second,
I had assumed, just I think what you're saying,
that makes a lot of sense.
And it's causing me to update my thinking a little bit
with respect to what degree is OpenAI
using a constitutional AI-like approach?
I would have assumed prior to this result
that they would also be using
something quite similar internally at this point.
But this now maybe suggests not.
I mean, it's weak evidence.
What was your thinking before?
I had kind of baked in that like,
once Anthropoc does something and shows it,
and publishes it and shows that it works effectively,
that like, yeah, I mean, OpenAI if they're certainly
not precious about pride of authorship,
I don't think they have a, you know,
not invented here syndrome.
So they'll take that stuff on board, I thought.
So what do you, what do you think?
Did they not?
Or is there some other weird thing that we're not?
I have a few different theories that can combine
as to what's going on here.
The first of all is look at the timeline.
Like constitutionally, I wasn't actually published
that long ago.
So if GPT-4 was basically finished with its process
before it became available,
then we might see it used in the future,
but you don't want to over align these models.
You don't want to push them, you know,
you don't want to align them with like
incompatible different halves and like pile them
on top of each other, weird things happen.
And there's a lot of bespokeness and detail
and like just trial and error that goes into all of this.
Right, like we can, we can theorize all we want.
We can talk about like, we just implement this paper
and this paper and this paper and change this technique here.
My understanding is that like all of machine learning
is subject to like learning lots and lots of little techniques
and piling them on top of each other.
And like if this parameter is tuned in slightly the wrong way,
the whole thing falls apart and nobody really knows why.
And so you just have to try a bunch of stuff
to get it to work.
And so, you know, maybe Anthropic has been tinkering
about this for a long time and they got to the point
where it was worth using.
And OpenAI hasn't yet released a model after the time came
that they got it to be worth using.
Also, OpenAI is much better funded than Anthropic.
So Anthropic will want to move to a much cheaper,
more automated system of alignment,
much faster than OpenAI will, right?
So like there's a point at which like OpenAI
can get better results because they have much more human
feedback from their much larger number of users.
They have much more funding, they can hire more people.
They're willing to go to like, you know, the reports
are they hire people in Africa, whereas, you know,
Anthropic is hiring people in the U.S. and Canada.
So it's all very different.
And so Anthropic has much, much bigger incentives
to move to this faster.
And that I think is primary, my guess is the primary thing
that's going on here.
Also, I think that we're making an assumption
that it works, that it works well.
So like if you think of cloud two, right?
The biggest weakness of cloud two
is it's scared of its own shadow, right?
In a real sense, right?
Like if you try to get it to go out on limbs
and be creative and so on,
you will usually fail in my experience, right?
It will apologize and bow out.
I can't get it to speculate.
So I went to using cloud two as my baseline model
that I look at first because if it rejects,
I can just copy paste the exact request in GPD four
in about 10 seconds and it's fine.
But I am getting a significant number of refusals
from cloud and much, much lower from GPD four.
On my ordinary, I just want the actual result.
I'm not trying to run an experiment kind of questions.
And despite the later cutoff of information, right?
It will say, I'm sorry, I can't,
there's not enough information
or I can't speculate on that
or that's reinforcing harmful stereotypes
or any number of other things.
And I think GPD four's custom instructions
are also doing a lot of work here.
I have a pretty extensive list of custom instructions
that potentially hammer into the thing
that it's supposed to just do the things
and not worry about it and not,
and I'm sure that's doing some amount of work.
But essentially, when you look at the helpfulness,
harmfulness, trade off frontier graphs
and the papers of like why they describe it as working,
everything works by the metrics you were optimizing for.
Right, like it doesn't mean it works
in the regular human world.
It doesn't mean it's optimal there.
And so, how good is constitutional AI?
My guess is when properly implemented,
quite good on current systems,
but the current anthropic implementation
is not all that good.
If you look at the actual paper on constitutional AI,
you read the constitution,
you notice the constitution like has a number of properties
that it shouldn't have if they want it to actually work
and get you what you want.
And you look at the examples that they themselves choose
to present of the results of running constitutional AI.
And you see very, very clean, crisp examples
of how this constitutional AI trains Quad
to be scared of its own shadow
and to be an asshole about it when it is, right?
Like it's very, very obvious if you think about it,
why their sampling method from these rules
with these rules written as they are,
with the specific rules chosen as they are,
will result in this problem
because you're offensive to just minimizing, right?
You've got these rules that are very much
choose the one that least does X, right?
And we often talk about you can't touch the coffee
if you're dead, you wanna maximize the probability
that you are, this is the equivalent of,
you wanna, you score one if you deliver the coffee
to your boss, you score zero if you don't.
So what do you do?
You do things like buy four coffees
in case one of the coffees is wrong,
was prepared improperly, right?
Like, or isn't, isn't hot enough for, you know,
Mr. Bradley orders, so you order one with cream,
one with sugar, one with cream and sugar,
and one with neither.
Cause like just in case you got it wrong,
you have a backup and you try to make sure
that you have the direct, you know,
you have as many different routes
to get to your boss's office
and you wanna make sure you're not fired
because all that's left for you to do,
like the only thing you're being trained on
is not screwing this thing up, right?
Like, you don't have to jump to like,
so kill everybody in the world,
or whatever, crazy, or take over or some crazy stuff.
Instead, this is just a case of, you know,
if you say choose the least racist thing you can say,
over and over and over again,
it's gonna be scared of its own shadow,
because of course, right?
There's no point at which it's like,
am I non-racist enough?
The answer is no, never.
And then that would be kind of fine
if it was just that one,
but then you have like 50 different rules,
all of which are doing this, right?
And then you can always just refuse to answer the question
and then what happens happens.
Then Lava has it, seems like even works.
Yeah, interesting.
There may be some incompatibility
between the system instructions
or the customer instructions.
The system message is what it's called
when you're calling the OpenAI API.
And now they've released it as part of JetGPT
as well as the customer instructions.
And yeah, I can see how, I think it's a good point
that if you're going to try to do what Sam Altman has said
they're trying to do, which is allow everybody
to get the experience that they want
from their own interactions with AI,
that is not the constitutional AI approach.
So it's almost, you can see a little bit
of like a different product lane almost opening.
You're kind of crystallizing a little bit
between these guys and Google DeepMind
as our next live player also seemingly has a bit of a lane.
It's like OpenAI is kind of trying
to do consumer killer app first, it seems.
They've got their, obviously they've got the API.
Obviously they're doing a lot of things,
but the crown jewel right now is they're the home
of like retail direct to AI usage with JetGPT.
Clawed seems to be much more like if you are the CIO
of some big company and you're trying to do something,
like you can trust us cause we'll never embarrass you
because we have this constitutional AI approach.
And if you're buying on behalf of all your customer
or all your employees or whatever,
like you don't really care if they are sometimes frustrated
on the margins by over refusal or whatever.
And then with Google DeepMind as we'll talk in a minute,
like they seem to be kind of going more like narrow
specialist system emphasis, although they of course
do have their like mainline palm model as well.
You'd also take Anthropic if their word, right?
That Anthropic is actually trying to design safe systems.
They are trying to figure out how to safely design
a future system and they are not as much optimizing
for the day-to-day experience of their users.
They also just have orders of magnitude less users
than OpenAI.
So they haven't gotten the same level of feedback.
They don't know what people want.
Yeah, I also note there is nothing inherently
about constitutional AI that forces you
to go down the super harmless assistant route
that forces you to give the same experience
to everybody at the same time.
You could train with a very different set of goals,
a very different set of constitutional principles,
for a very different set of mechanisms.
And I don't think we want to go into that many details
as to how I would do it.
But it's pretty obvious to me that if you want to do
something other than be as harmless as possible,
that is entirely your decision.
It's just that people at Anthropic have decided
that's what cloud is meant to do.
And if they do raise these billions of dollars
to train the sex generation system,
they're gonna have to make a choice about that.
Do they want to continue to go down this road
and potentially make their product law less useful
or do they want to go a different road?
And one way to try to differentiate,
of course, is the context window as well.
They've got this 100K token context window
available for free.
When you mentioned here in our outline
that you made the outline was you used cloud.
That's because you weren't able to use anything else.
Your posts are too long, dude.
I can't fit those into GPT4.
I feel bad even thinking about putting them into cloud.
Oh my God, this is so expensive and kind of ugh.
But it's not really fair, I'm not even paying these people.
But without that context window,
you just can't do the things that you want to do
in that spot.
And so Anthropic's trying to say,
I think a lot of context is safe.
Once I've made my thing harmless,
I can recapture a bunch of the benefits
by doing this other thing.
And we will see what happens.
I am curious.
One thing I'm doing with cloud
is I'm not even having separate conversations.
I am just having one long conversation instead
because first of all, I haven't necessarily wanted to like
carry on discrete conversations
and come back to them later.
But also because I want to see what happens
when I build more context.
Just for what it's worth,
for listeners, my approach on creating the outline was
first just read all of these recent posts.
And I just did that without taking any notes
in bed on my phone.
And then the next day I came around, I was like,
okay, a lot of content there.
What parts do I want to pull out?
So I just copied each post in full,
pasted it into the free consumer facing
cloud.ai online.
And literally just asked one sentence question,
what are the most important points in this post?
And then it would give me a list.
And I basically, you know, at that point was like,
oh yeah, that, that, not that, that, yes, done.
So it definitely was extremely helpful.
I wouldn't have wanted to use it to, you know,
replace reading the blog post certainly
in preparation for a conversation like this,
but as a way to come back and, you know,
help me just make sure that I was remembering
the important things and kind of organizing them
in a reasonable way, it was super useful.
And yeah, they don't fit into a GVD4.
So no other, no other option.
The other thing, so your long context thing
is really interesting, just experiment in usage.
It also kind of connects to another bit of research
that they recently put out that was on examining
chain of thought and also truly decomposing tasks
into bits.
And I think the short summary of that research
is that they were able to achieve the highest performance
in terms of accuracy and especially reliability
and kind of consistency by going beyond
the kind of normal practitioner chain of thought,
which I would say normal these days for me is like,
just give the model a sequence of tasks to do,
which may start off with just like,
first you will analyze the situation,
then you will, you know, maybe summarize,
then depending on what it is,
then you'll write my tweet storm
and you'll do whatever, right?
You could have a set of different tasks
that it can kind of handle sequentially.
And you're definitely rewarded for encouraging upfront
or directing it upfront to do some initial analysis
to kind of think step by step, chain of thought,
et cetera, et cetera.
But it seems like they find a notable,
not a huge, but definitely a notable difference
in actually pulling those things apart
and making discrete, independent,
more isolated calls to the model
to say, first you will do this,
but you will only do this, then you will do this,
but you will only do this,
not considering what you previously did.
And then kind of putting those things together at the end
gets you overall net better performance.
So for most random use cases, you know,
random conversations you're having with Cod
or with whatever model, not necessarily a huge difference,
but on the kind of possibility frontier,
it does seem to matter.
What lessons do you take from that?
It's a little bit confusing to me in some ways.
It's sort of, I'm trying to figure out like,
what do I think I learned about
how language models behave in general,
that this is true?
And I'm like, best I could come up with
was that some of these simple tasks that it's seen a lot,
like it may have dedicated sub-circuits for,
and that perhaps with so much context all running at once,
those sub-circuits kind of get overloaded
or kind of get drowned out to a degree,
or in some cases by just the general kind of noise
and all the stuff that's in the context window.
So kind of removing some of that context,
maybe you get a cleaner execution of a certain task
because there is some mechanism that can do it
as long as it's not kind of talked over
by like other parts of the model.
That could be totally wrong, of course,
but I don't think anything about this
is necessarily inconsistent
with like just pure stochastic peritory,
which neither of us would advance as the theory,
but just as like keeping myself grounded,
like you couldn't tell a similar story where you'd say,
everything's all stochastic perits,
and when you put a ton of context in,
it's just even more stochastic-y,
and when you have less context,
it's a little less stochastic,
but it's all stochastic,
but you still get better performance
when you break it up.
We are all stochastic perits,
each of us with their hour upon the stage.
So I would say I didn't know this result until you told me,
but I would have predicted this result
for reasons that I described earlier in the podcast, right?
Which is that when you give a model multiple tasks,
it can only vibe off of the aggregation
of the two things that you asked it for.
Think about image models here again, right?
And so by breaking up something in your discrete tasks,
you avoid these kind of context clashes.
You avoid these vibe conflicts,
and you let it like narrowly do these things
by having to like be able to transition
and hold two things in its head at once
in some important sense, right?
That's colloquial and not quite what's actually happening,
but the same idea.
And so yes, I would expect that to the extent
you want the thing to think step by step,
you are best off by identifying each of the steps
you want to think by step and asking for them separately.
And I noticed that with API calls being priced
the way they're priced,
and of GPT-4 being rate limited to an ordinary user,
we have all been trained to say,
how do we ask for the most expansive set of things at once?
So that you can answer all of my questions
with one generation.
It also lets us hit enter and then go away
and grab a cup of water or some coffee
and then come back and see what the answer is, which is nice.
Whereas what you actually would want to do, right?
If you wanted to generate the best possible answer
is in fact to break it up into as little pieces as possible.
And quite possibly start by asking the AI
what would be the pieces in which you could break this
as small as possible to get its help doing that.
And then have it feed those back in, right?
Auto-GPT-ish style,
even if you're not trying to generate an actual
like recursive chain that generates something dangerous
or acts like an agent.
But yes, I think the more you break it up,
the more that you can identify concrete distinct steps
that are always done separately,
the more better the AI will do.
And I think humans would also, by the way,
perform better in the same way.
Right, if you have a human
who is looking to be micromanaged and take direction,
and you notice that like this job has steps A, B, C, D, E,
right, like if you say, go do A,
and we've been to say, okay, I've done A,
I think now do B.
I think that person will in fact do better, right?
Modulo the extra communication and logistics costs
of like having to interact with you five times.
So I don't find any of this surprising.
And, you know, it would have in fact been surprising
if it didn't happen to some extent.
One of our early episodes, relatively early episodes
was with Andreas and Junghwan of Illicit.
And this is really core to their strategy.
Their product is research assistant
for essentially grad students or, you know,
grad student like people,
people that are looking through academic literature
and, you know, really want a systematic
and also like transparent, you know,
auditable view of like all the papers that were reviewed
and, you know, what was found and what was not found
and what the model did at each step.
So they really have pushed this pretty far
in the illicit product to the point where it's like, you know,
all these little steps, you know, kind of happened
sequentially, they've got two different models for them.
Some were fine tuned, you know, internally,
others are from the major providers.
If you're interested in going into that more,
go listen to them because they've pushed that pretty far.
But a question that I have for you then is,
do you think this flips at some point?
Like it seems like the, an interesting threshold moment
might be coming up where with sufficient training,
this could flip the other direction.
Like, because more context in some ways is better, right?
Like, I guess it depends also on exactly
how you're implementing the breakdown or whatever.
But, you know, you can imagine breaking things down
fine enough where atomizing things so much
that the person starts to struggle
for lack of broader context, right?
Like you have this phenomenon with people
certainly where it's like, you've gotten so focused
on this little detail of, you know,
in this little task within the broader thing
that we're trying to accomplish,
you've kind of lost track of what we're trying to accomplish.
And now you may be making some bad judgments with, you know,
with respect to this task as a result of kind of
having lost track of, you know, any number of things, right?
How much accuracy do we really need here?
Is this really even important, you know, in some cases, right?
Could you imagine a, you know, proverbial GPT-5
where it's like, actually now it's strong enough
that putting everything in one again is going to be better
because now it actually can use all of this information
at the same time effectively versus today
that that subdivision being better.
So what you're not gonna get is the, you know,
Marxist phenomenon where the AI would get alienated
from its labor, right?
Or like, you're moralized by lacking context
or, you know, otherwise, like,
not be able to perform in some way.
You're not gonna have a problem with Adam's Piss Pin Factory,
right?
If you can actually specify exactly
what the pins have to look like.
So the question is, to what extent
do the different parts of the task actually have important
context for other parts of the task?
And to what extent does this actually enhance
the ability to perform if you know what's coming,
you know why you're doing what you're doing.
And this greatly varies between different activities, right?
There are some cases where you need to know exactly,
you know, you're in the Chinese room
and the English word comes in
and you wanna put the Chinese word to the other side
or the Chinese word comes in
and you wanna put the English word to the other side.
And there are cases where you need to know
what the words are in the sentence
and what the context is and potentially
like the entire cultural setting of what's happening
in order to properly translate the phrase
or you're gonna mess up
and you have everything in between.
So the question becomes, you know,
can you set it up so that you can capture
that important context when you need it
and how much does that context interfere
of what you're doing?
I can definitely imagine a lot of cases
where somebody who is given
actually pretty irrelevant context
just ends up very distracted
from the actual task at hand
that ends up being much less productive, right?
As a human or to think of it as not in an AI
because the vibes don't mesh, right?
Which is basically the mechanism that I'm conjecturing, right?
The vibes don't mesh, they're distracting from each other.
Either bleeding, the tasks are bleeding into each other
in terms of the details and methods,
it's getting confused, they can't be sure they don't
which is makes sense
because like a lot often they would bleed
into each other in various ways.
So it has to be good enough
that the bleeds are where it makes sense to bleed
without being in the places
that don't make sense to bleed.
So you can imagine a world in which like what the AI does
is the ICs, you know, request one, two, three, four, five
either labeled as such or implicit
and then it breaks them down into individual things
that it virtually queries itself on its own
but knowing there are these other things
as proper context in the proper way.
I think the answer to that is
as you ask sufficiently capable people
or sufficiently capable AIs
to do increasingly complex things
at some point, if they have the capacity
they're going to do better if they have more information
they'll do better if they have more context.
If they are sufficiently more powerful
than the details of the task at hand in some sense
that threshold may or may not be anywhere near
where we are for different ways.
I would say, you know, one of the big advances
that I keep expecting to come
is you will type a query into an LLM
and then rather than the LLM literally
just outputting the answer to the query
what'll actually happen is we fed
with the proper scaffolding into a different LLM
that will evaluate what type of evaluation method
is to be used to evaluate your query
and sometimes it will be no
that's a normal query feed into the LLM.
Sometimes it will be this is a multi-part query
you should feed these separate things and separately
sometimes it'll be something else entirely.
And also which of my many LLM limitations
do I want to use so that I don't waste a too large model
that costs a lot of money
on something that's actually relatively narrow
and I direct this to the thing
that has a specialized knowledge
a specialized training specialized skills
for this type of request and so on.
And a lot of that is, you know
the fruits of the revolution
that will come in a year or two years, three years from now
regardless of whether or not we have fundamental advances
we just have to give it time.
So final question for the anthropic section
one of the things that as I was reading their
you know the profile that you based your analysis on
that jumped out to me as somebody who has a
fondness for red teaming activity
was that they're hiring a red team engineering type of role.
And I guess I wonder, you know
would you recommend somebody like me who, you know
is I think probably we share a lot of our worldview
and you know a lot of our kind of values
in terms of hopes and fears for how this all might go.
Would you recommend that somebody like me go and work there
or would you feel like, you know
as you said earlier you wouldn't want to send them
your research ideas.
Would you also not want to send them your friends
or would you say like, hey yeah maybe go get involved.
How do you think about that?
So it's very easy in these situations
to get in an action bias where you say to yourself
I don't want to encourage the thing
that might make things worse.
I want to be able to tell myself a story
that I only did things that make things better
even if that means your expected impact is a lot smaller.
It's also very easy to fool yourself
when you're thinking that you're helping
when you're actually enhancing capabilities.
You have to balance these two big concerns
and sources of bias against each other
when making this type of decision.
I would say I am relatively positive on open AI
and anthropic relative to where I was
when I started this Odyssey with AI number one
or even sort of been a way through at around 11
now that I've seen the developments, right.
Like I think that both of these organizations
now have a reasonable claim to be taking alignment seriously
such that if you can help with their alignment efforts
specifically in a way that you do not feel like obligated
to go along with adversity if you find it
and that you are able to stand up for and call out
stand up for what is right and call out
people who are being irresponsible
and you are willing to quit on a moment's notice
if something becomes serious enough
and you are willing to tell the world ideally, right.
That's why you did it and as much as possible what happened
then I think it is plausibly very positive.
I still would not feel comfortable working on capabilities
for any company.
And I still wouldn't want to give capabilities ideas
to any company.
But if I was confident it was specifically working
on alignment and like red teaming seems like one
of the places where you are most obviously being
a positive influence in that role.
And the question is like do you want to be the one
in that role or do you want someone else in that role
and how does this compare to your opportunity cost
of doing something else, right.
Like I think that I prefer the world
where there's a clone of you that didn't otherwise exist
who is working on that job and does nothing else all day
like goes home and watches television
like otherwise doesn't affect the world at night.
It doesn't mean that that is better than running
the cognitive revolution or doing any other number
of other things that you are currently doing
with your time.
And so you have to balance that, right.
And also any other opportunities that you might have.
So I don't think it's clear by any means
but I've definitely reached the point where
I wouldn't assume you were making a mistake, right.
If you did that, but you'd have to go
into the interview process with a very open mind.
You have to say, you know, I am deeply skeptical
that any organization including you is going
to be that helpful is nicking
as necessary precautions is treating the problem
as difficult and serious as it actually is.
Is doing things that actually solve the hard problems
and not the easy problems is not just enhancing capabilities
regardless of their intentions, et cetera, et cetera.
The interview process is what it should be always
in every job with a two way process, right.
They are interviewing you and you are interviewing them.
Right, you are watching what questions they ask
and how they react to your reactions and your responses
and you are asking them questions.
And you want to know, would this in fact be a good thing
for the world if I got and took this job or not, right.
Cause I don't believe in taking jobs
in order to sabotage people, right.
Like you don't show up in order to not red team them.
I mean, certainly this is one job
you wouldn't want to sabotage.
Yeah, safe to say that is right now.
That's the considerations.
And yeah, I think I'm in a similar spot.
You know, six months ago plus I was really,
especially with respect to open AI.
I was like, this seems like what is going on
and do they have anybody like really approaching
this in a serious way?
As it turned out, like they did have a lot more
than had met the eye at that point and gradually
they've revealed it that I've definitely updated
my point of view on, I'm really all of the leaders
in a pretty positive way over the last few months.
I think, you know, if anything, they've probably,
some of them maybe were, you know,
expecting this much progress this fast.
I have to imagine that even internally,
a lot of them are kind of surprised by just how,
you know, far the scaling loss have extended
and how, you know, how quick on the calendar
they've hit some of these milestones.
And, you know, I do think they've handled it
pretty well over the last few months.
Yeah, I would say I am positively updating
on all three major labs.
And most everyone at the media life that is relevant.
My negative updates have been in other places, right?
Like, and mostly I've been pleasantly surprised
by government.
I've mostly been pleasantly surprised by public reaction.
You know, there's definitely people who disappointed me,
but mostly things are going vastly better
than I would have expected when I started down this road.
And I'm much more hopeful that we can make better decisions.
I'm not sure how much that translates into, you know,
P of survival going up that much,
but I think this is definitely going better
than I expected.
That's great.
It's good to have a little, you know,
a little positive note from someone
that some might call a doomer.
Let's turn to Google in DeepMind, our third,
as you said, of the three leaders.
I don't know if there's any like super headline news.
I mean, the last week it's one of these things
where it's like a year ago,
some of this stuff would have felt
like an absolute bombshell announcement.
And now it's like, I kind of expected that
to happen about now.
And there's two examples of that.
One being the latest robotics paper
that they came out with on Friday,
which, you know, extends and kind of unifies
all the work that they've been doing,
where now you have robots that can follow instructions
that have this kind of, you know,
language model in a loop sort of structure,
kind of unified, simplified the architecture a little bit.
Now the language model is just kind of outputting commands
for the robot body.
And so they've like eliminated a few,
maybe I don't know how many,
but they've eliminated sort of certain layers of control
and kind of just simplified the overall structure.
And then what's making probably the most headlines there
is the conceptual understanding
that the robots are now able to show,
which is basically the exact same thing that the,
you know, the language models
or the multimodal language models have already shown.
So they've got demos where it's like, you know,
move this object to the Denver Nuggets.
And then they've got, you know, from the recent,
they were obviously doing this during the NBA finals,
they have the Miami Heat logo and the Nuggets logo.
And the thing knows based on understanding that language,
also knowing what the logo looks like.
And obviously, you know, being able to command the robot arm
can actually do that task.
So you've got these kind of,
another one that they said was pick up the extinct animal.
And they've got, you know,
an array of kind of plastic toys on the table
and it will pick up the dinosaur
because it understands, you know,
that that is the extinct animal.
So these, from the perspective of certainly two years ago,
even one year ago, feels like Jetsons type robots.
Now it's kind of like, yeah, pretty much expected
that these different modalities would be bridged
right around this time.
And sure enough, it's happening.
Anything else to add on the robotics?
Yeah, I read the robotics.
And of course, whenever anyone had the advances in robotics,
the answer is, oh, that seems fine, not dangerous,
not scary at all, all cool.
But in this case, yeah, it seemed like,
of course you could do that.
You're combining things that you already did
and you're getting the inevitable results
of combining them.
And that's not me knocking you
for doing something you shouldn't have done.
That's just, okay, yeah, of course.
Like that's the next step.
And in kind of like,
for someone who doesn't want capabilities to go that fast,
you're happy to see that kind of paper
because that's the paper that says,
I'm gonna do the things that I already,
you already knew I could do.
Right, and you ran outside and like, okay, cool.
And if that turns out to be useful, great.
But like, yeah, I knew that LLMs
could interpret human commands in these ways.
And I knew that robots could execute
these types of movements.
So why should I be more scared than I was before
instead of less scared?
I should be slightly less scared.
Probably a lot of people in the public though feel,
especially if you're not obsessed with this as we are,
you might feel like,
if there is a news item here,
it's like some sort of qualitative, conceptual understanding
now has embodied form.
Now you can imagine bringing your jail breaks
to your robot commands.
And if you could verbalize some of those strange strings
that we were mentioning earlier,
now what might your robot be willing to do, right?
I mean, would it go smash stuff?
Would it go corner somebody in a room?
The system as a whole has the conceptual understanding
to kind of begin,
it has the same kind of proto morality or whatever
that the core language models have.
And that can go awry in similar ways.
And now you can probably get some pretty scary demos
out of these robots,
which I don't think Google's gonna be racing
to publish likely,
but there is something kind of qualitatively different
about that.
Yeah, so I like to think of this
as the game of good news, bad news,
but there's two games of good news, bad news.
The doctor, I say, I have some good news
and I have some bad news, and that's always fun.
But there's also the game of,
is this good news or is this bad news?
Because it depends on what you previously thought, right?
Like you have the law of conservation
of expected updating, right?
So like if you get news,
you should on average not update for or against anything
or to make things are better or worse in any way,
because you already had your expectations baked in.
So in the case of robotics,
like if you're not paying attention to robotics
and you think that robotics is just,
oh, robotics is hard, mysterious, there'd be dragons,
we will never have robots,
the same way we'll never have dragons,
then every little advance in robotics is like,
eek, you know, slight extra worry.
But if you knew that robotics was just another tack
like any other,
and of course we will eventually have robotics,
then you have to look at the details
of what you're looking at and you say,
oh, okay, this is fine.
So I'm pointing the game of mild,
I interpret this one as mild goodness, right?
Like in terms of robotics, not advancing so fast.
And of course, you also have the issue of,
you know, if you're somebody who wants
there to be more robotics,
then you might say that this is bad news, right?
Like that you wanted to see
lots of cool robotics advances and you didn't.
But yeah, I'd say also I wanna see
the ultimately harmless robotics advances
as quickly as possible.
Exactly because it makes it so much easier
for people to see what might happen
and what might go wrong.
People get hung up on, oh, but the AI won't have a body.
Oh, but the AI won't be able to move things
in the physical world,
as if this would ultimately ever be the barrier
that saves us in any real way, right?
Which it won't.
It's at best a temporary inconvenience
that requires someone to be slightly more clever
about what they do as an AI in order to get around stuff.
But it's not ever going to actually matter
in some point sense.
So the other big one,
and this is definitely one that I, you know,
I'm happy to say I'm ready to accelerate on
for practical purposes,
is their new multimodal med palm.
This builds on palm and med palm
and also actually on the earlier palm E
because that was kind of the multimodal.
So it's, it is interesting to see, you know,
I'd say zooming out from these individual papers
and just characterizing Google DeepMind as a whole right now,
it seems like they're firing on, you know, all cylinders.
Like it does not seem like, you know,
whatever sort of concerns folks might have had
about, oh, there's a million fiefdoms
and the groups don't talk to each other or whatever.
Like we're seeing papers and, you know,
projects building on one another at a pretty fast clip
that suggests like pretty effective, you know,
dividing and conquering and then coming back together
and sharing improvements.
So it seems like the output is just strong,
you know, whether you like it or not.
You have to look at the actual value
of the things being outputted, right?
Like the mistake you always can make in science
is to ask who is publishing the most papers,
who has reliably published a paper,
and then you have your scientists scrambling
to always publish as many papers as possible
and then no real science ever gets done, right?
And it's not their fault.
They just weren't given the affordances
to do breakthrough work.
And simultaneously, you know, you have to ask,
does any of this actually ultimately matter
on the scale of what is going to determine the big game?
And like I'm happy to see advances in the med tools
and it bodes well for them.
They made marginal advances in AI
and they had some other public papers published too,
some of which I was like,
why the hell are you publishing this?
You are a corporation that is for profit.
Even if you don't think of the safety issue here,
you should know better, like keep that secret to yourself
and either to beat the competition,
what's wrong with you?
The last point you've written down
is Gemini question mark, question mark.
And let's tie that in, right?
Because ultimately speaking,
it is going to be August tomorrow.
GPT-4 has been out for many months
and bars still sucks, right?
And the Gmail generative offering is bad
and the G-Docs offering is bad
because they're offering,
no matter how customized and narrowed and bespoke,
simply doesn't have the G.
It's not a good enough core thing.
It's also still making remarkably many
elementary stupid mistakes, right?
That even a low G system really shouldn't make,
their act is not together.
And to the extent that they are instead publishing
a bunch of quirky papers
with a bunch of like narrow applications,
that could be seen as well, look, Google ships,
but also it means Google is not shipping
the thing it needs to ship, right?
Like Google desperately needs
from their perspective to ship Gemini.
And like it takes a level long, it takes.
It takes them however much computed it requires.
But ultimately speaking, the test is,
can they produce the equal or better of GPT-4
now that they know that's what they need to do?
Because if you looked at the previous reputation of Google
and DeepMind and what they were capable of,
you would think that they would be ahead
on that front if they wanted to be.
And now that they know what they have to do,
do it to make it like commercial ready, right?
Ready for regular people, that should not be so difficult.
But then again, like we can think about how long it took,
like took like six months or so
after GPT-4 was finished training
before they were ready to release
even the earliest version of it, right?
And then they still rolled out a lot of its capabilities.
So even if Gemini finished tomorrow, right?
How many months are they gonna need
before they feel comfortable releasing Gemini?
Because Google was much more risk averse than OpenAI
as a company in the culture.
Who knows when that's gonna happen.
It's been longer than I thought.
You know, in my Scouting Report,
I have this clip of Demis Asavis
just after Gato paper was published
saying that, you know, of course we can scale this up as well.
And we're in the process of doing that.
I believe that was April of maybe May of 2022
has been over a year.
And typically we don't have to wait a year plus
to get the successor, you know, to a thing like that
that, you know, is just about being scaled up.
So I've been really kind of wondering
what is going on behind the scenes there.
But I also do wanna turn back to the med thing as well.
So I'll give you first,
would you care to speculate about Gato too?
Is Gemini Gato too?
As a shareholder, I am concerned, right?
And I also have Microsoft, but I am concerned
that their act is not together
and that we're not seeing the kind of progress.
Like we're not making the incremental announcements
that I would make if I was their marketing department
and I was moving towards the rapid clip, you know,
as a person who wants the world to be okay,
I'm not sure how much I mind,
but it is pretty troubling
that they can't get their act together.
I was really excited for Google suite integration
when I first heard the announcements
of Microsoft Copilot and, you know, Google interactive.
And yet when I got Google interactive,
I tried a handful of things
and then quickly realized in their current forms,
I don't have any use for them.
They don't do anything, right?
Like the first thing I tried to do with Google Docs
was I tried to paste my article in.
And then I said, you know, to summarize this article
or otherwise get to do the obviously first things
and just fell completely on its face.
You're just like, I can't assist with that.
It's like, well, then you're useless.
Or if you can't even read the context of the document
that I gave you specifically, like why am I even here?
And like for email, it's like, no,
by the time I figure out what I want
and type in the detailed request into you,
I could have just written my email right now.
Like where are the emails where I want to spend
the kind of time required to customize the output
but don't want to actually customize the output carefully?
This is just the empty set.
But like, when does this come off?
And so that was like a rude awakening as well.
Yeah, those deployments have not been very good yet,
but going back to the med one for a second,
this may be an area where we may have
some different expectations
because reading through that paper,
and I haven't studied it in depth yet,
but the headline statistics along the lines of,
first of all, it's a multimodal system.
The last version of MedPalm 2 were all text.
So you could ask it your medical questions
and they had announced expert level
answering of your medical questions.
And they'd evaluated that seemingly pretty carefully
with a bunch of different dimensions
and having human doctors compare for accuracy
and all these other things you might care about, right?
And that the AI, as of MedPalm 2,
was beating the human doctor responses
on eight out of nine of those evaluation categories.
So it seemed like, okay, that's pretty good.
Now they haven't released it,
but it's in limited access
for trusted hospital partners or whatever.
Now with the next version, it's multimodal as well.
So you can do things like feed in a pathology image
alongside the text.
Pathology would be like somebody has a tissue biopsy,
we did an episode on this actually with a narrow system
from Tanishk, Matthew Abraham,
who did this with small data too, which was super cool.
But somebody has a tissue biopsy,
that tissue has been sliced, has been plated on a slide.
Now it's been imaged and they can feed that
along in with the case history.
And for that matter, you can handle radiology scans
and all these kind of other different sorts of inputs
that are obviously key to the actual practice of medicine.
And then they say things like,
our radiology reports out of the model
were preferred to a human radiologist report
some 40 plus percent of the time.
So like almost half, basically seems like
it's very much on par with the human radiologist,
which of course is like the canonical thing
that people have been saying for 10 years,
people have been saying that radiology
would be the first thing to be impacted.
And then for the last like three months,
it's become kind of a talking point that like,
well, radiology still hasn't been impacted.
So, and now all of a sudden it looks like
we're hitting maybe radiology being impacted.
But I kind of expect that that thing works pretty well.
It sounds like you maybe are a little more skeptical
of like whether it actually has real utility.
Well, I mean, you definitely don't want to tempt fate
and go out there and say, well, my job
hasn't been automated by AI yet.
Look what you thought was going to happen.
Don't do that everyone, like no bad, bad, bad, bad play.
But I would say when I look at healthcare, right,
I don't see the obstacle being primarily
that we don't know how to do better.
So I would in fact expect the AIs to be able
to replace many human healthcare tasks
with a superior model now, right?
Like especially even without like some bespoke stuff
going on inside Google, certainly with some bespoke stuff.
That seems relatively straightforward.
Doctors are just not given enough training data,
don't have that much compute, do their best.
But of course, you know, you see the same things
over and over and over again, mostly in humans.
And if you have enough data to train the AI
with the AI, it's going to do better.
It's not a knock on anyone.
Certainly it's something like radiology.
Like obviously a radiologist is trying to be a computer, right?
Like radiologists are trained to be computers
because we didn't have computers.
If we had good enough computers,
you would have trained them to do something else
or trained fewer of them to do the parts of this job
that the AI can't quite do or something like that.
And so yes, we will have these capable systems soon,
but trying to actually implement that
requires getting through a whole host of different barriers,
cultural, regulatory, you know, strict legal, contractual,
you know, just the way you navigate
and set up the current system,
the number of insiders that want to be protected,
the number of human interests that will fight
to prevent you from doing that, et cetera, et cetera.
And so, you know, this is the big dilemma, right?
Like, when Eliezer famously, like, expressed skepticism,
that we would see that much economic growth before the end.
It was because, well, we already know how to build houses.
We already know how to get better, more efficient healthcare.
We already know how to deliver
most of what the economy produces in terms of cost,
vastly better, and we're not allowed to.
So if the AI invents and enables more and better ways
to produce things that people want, that people need,
well, the bottlenecks are gonna remain
unless the legal system adapts to let them not be bottlenecks.
So why does it even matter that much, right?
And so like in healthcare,
that's the question you have to answer.
And that's the reason we haven't seen more
of the assistance do better either, right?
Because I don't think it's because we can't train the AI
to be a better radiologist in many ways than our radiologists
or we couldn't have done that last year or two years ago.
It's because if you had spent a lot of money doing that,
how are you going to get your money back?
How are you going to actually help patients?
How are you going to save lives?
How are you going to improve our system
if no one's gonna let you, right?
And if the radiologist is like,
going to stubbornly double check everything the system does
and then substitute his judgment
for the systems reasonably often,
the system is not actually going to be helpful.
I have definitely kind of expected
some sort of other part of the world deployment,
kind of possible leapfrog effects as it becomes very hard
to say that people who currently have no radiologist
shouldn't have access to something like this.
Yeah, the problem with that is that most of those places
have deliberately taken market signals
and compensation away from their healthcare systems.
And they're also relatively small markets
that are relatively poor.
So they just aren't big enough markets
in an economic sense to justify the creation and training
and tuning of these systems.
And also like nobody involved wants to be the ones
who stick their neck out, right?
And like take the blame and responsibility for this thing.
That's like these weird Americans who won't themselves use it
are suddenly creating,
like it's a really, really bad cultural social context
for trying to make this happen.
We also have a problem of the elites of the world.
This is what we saw of COVID, right?
Like you would have expected in COVID,
someone somewhere to do challenge trials,
someone somewhere to actually study the spread of COVID
and what exactly did what,
someone somewhere to do all sorts of things
and nobody did any of it
because all of the elites of the world basically got together
and converged upon what they thought was the consensus
and the right thing to do.
And nobody said,
well, we're going to be the ones
who gain advantage by defying that.
And so we're increasingly seeing that pattern
a wide variety of places.
One, the, you know, nobody wants to be the one
to stick their neck out.
And two, like how do you recoup your investment?
Pretty natural bridges to our next live player,
which is Meta.
And obviously they have been in the news recently
for releasing Lama 2.
And this brings up a lot of these questions to me.
Like, first of all,
and Imad Mostak from stability said this
actually in a recent episode,
he was like, the leaders are non-economic actors.
And he was specifically referring to open AI and Google
not seemingly being motivated by money
in the way that a typical company would be,
you know, open AI trying to commoditize its own product
as quickly as they possibly can, you know, on record
being like, we're going to drive the price
of intelligence as low as we possibly can,
as fast as we possibly can.
Google, you know, is obviously just kind of
trying to defend itself more than anything else.
They don't need to make more money.
They just need to not lose their spot.
Anthropic, we take as a safety first play.
And, you know, certainly they don't seem to be trying
to maximize revenue from what I can tell right now.
But then Meta is taking this to a whole other level,
arguably, where they seem to be kind of yoloing
the whole thing and being like,
that's a little bit flippant because certainly
with this Llama 2 release, they took some steps,
you know, they didn't just release the totally naked,
pre-trained model, but they actually did, you know,
the kind of what you're supposed to do
if you're going to be a responsible frontier model developer
with a red teaming process and an RLHF and so on.
And, you know, we can also get into did they overdo it
or does it refuse too much and all that kind of stuff.
But just for starters, like, what do you think is going on
at Meta that they are willing to put tens of millions
of dollars into training a model
and then just release it for why exactly?
I can't, like, it seems like if you're at any sort
of normal corporation, this is like what your risk officer
is supposed to put a stop to, right?
Lee Roy Jenkins.
No.
How do you understand this?
Idiot disaster monkeys?
Let me try to actually answer the question.
I think that their business strategy here
is cannibalizing the compliment.
So the idea is that, you know, the people
who they're up against, people who are competing
with them fundamentally, this is their business.
And so the idea is that in their model,
if they can foster an open source environment
that replaces the specialties of these other companies
that they are competing with, then their hope is
that this will, you know, give Facebook a level playing field
against them in this way so that Facebook specialties
can reign supreme and they can become more dominant
and they can erase their deficits.
Alternatively, they're just not as good.
And so they need the open source community's help
to try and keep pace.
Alternatively, you know, they think that if they get
these people working for them, that's free labor,
you know, it creates this whole other network.
It's a strategy, right?
Like it's, I mean, Android is open source, right?
It's not crazy to open source major stuff
from a business perspective necessarily.
It's crazy for me, that's not all my perspective.
I think that, you know, realistically,
like senators gave them what the hell
about releasing Lama One.
Did you give them a much bigger one
about releasing Lama Two?
You know, if we are concerned about beating China
to the extent that we are considering, you know,
we're implementing a variety of export controls
and we are considering actively, you know,
subsidizing capabilities or at least not being willing
to slow down our capabilities, then we damn well
shouldn't be releasing Lama Two
as an open source product.
That's completely insane, right?
Like just, even if you don't get any immediate
direct danger doing that, it's completely nuts.
I think that should be stopped.
And I think that this philosophy,
if allowed to become ingrained,
like creates the systematic groundwork
for future open source work
that then like is the maximally dangerous thing.
I call it the worst thing you can do, right?
Like creating frontier models
and open sourcing them is the worst thing you can do.
Like in the world, you know, Yama Kun
and others at Metta either sincerely believe
that there is actually no danger
from artificial intelligence
despite this making absolutely no physical sense
or they don't care and they're lying about it.
I don't want to speculate
as to exactly what's motivating these people
but they're smarter than the arguments they're making.
They know better Zuckerberg himself
is smarter than this to some extent, right?
He said on, I believe it was Lex Bidman's podcast
that, you know, there will be future models
that we'll have to be very careful with.
We want open source
and we're gonna have to think about these problems
but for now it doesn't seem necessary.
And, you know, if I were him,
I would be very concerned about the culture I'm creating
and the precedents I'm laying down
and the open source community that I'm creating
that's going to be a huge problem for you later
and create tremendous pressures on you
and create a potential competition for you
and that you don't want.
But, you know, I sort of understand
from a business perspective, while you might want to do that.
Also, they want to attract open source developers
to work at Facebook Metta
because there's a whole group of people
who are quite good at coding
who have philosophically fanatical devotion
to this idea that software wants to be free
and that everything should be open source
and who just prioritize that over something like,
you know, worrying about alignment
and what would happen if we failed
or worrying about the proliferation of,
you know, artificial intelligence in various senses
and just have this ironclad belief
that like concentration of power is bad
and that if you just give the people the things
that it'll all somehow work out.
And I don't think that in this situation.
I think that situation is very wrong,
but they clearly believe otherwise.
And, you know, look, they've been,
Facebook has been in my mind
like the detonated villain of the piece
for a very long time.
Like long before artificial intelligence
even entered the commercial picture.
So it just somehow feels fitting.
You know, if it was all gonna finally get destroyed
by Facebook, it just seems right.
Well, I very strongly try to resist
psychologizing in the AI discourse too much.
Really at all, I try to avoid it basically entirely
because it just seems like, you know,
nothing good ever comes of it really.
But I have also struggled to come up with a,
what feels to me like a coherent argument here
that isn't on some level just ideological
because I kind of ran through all the things
that you were mentioning as well,
starting with like, well, maybe you can, you know,
undermine your competitors, core business.
But then I'm like, yeah, but you're not really gonna do that.
Like, does anybody expect OpenAI's token serve
to go down as a result of this?
I don't, I think they're gonna continue.
Like they're GPU limited.
And I think they're gonna continue to be GPU limited,
you know, maybe slightly less,
but like, I don't think their top line suffers.
I don't think their token serve suffers.
Their leadership position doesn't really seem to suffer.
I can't really get to a point where I'm like,
seeing the return, and on the open source thing too,
I'm kind of like, you know, that was part of that memo
from the Google memo of like,
oh, you know, they've got this big open source community
or whatever, but I don't really buy that memo either
or that analysis because I'm like,
everybody benefits for, or, you know,
whatever the impact is of all the sort of open source
hacking that's happening,
it seems to accrue to everybody pretty equally.
Like, yes, maybe it was done on this llama 2 base,
and like, maybe that's something that Facebook
could kind of readily fold back in,
whereas, you know, Google with their 700 plus million user,
whatever, you know, can't take direct advantage of it.
But to the degree that people are out there doing things
like quantizing models and making them run on, you know,
consumer devices or whatever,
that's obviously a technique that Google can also say,
hey, look at this, this works, you know, we can do it.
I just don't see a lot coming out of the open source
experimentation that feels like it specifically accrues
to Meta's benefit, and so in the end,
it just feels like more of a principled, you know,
to put it in a more conventionally positive framing,
it feels more of like a principled decision
than a, you know, tactical or sort of, you know,
results oriented.
And there is still recruitment,
but I strongly agree that, you know,
any advances that the open source community discovers
are gonna be at Google and Anthropic and OpenAI
a month later, if not a week later,
and they'll also be at Baidu, right?
Like, they'll also be at all these different Chinese companies.
And so this long-term strategy cannot be allowed to continue
in some important sense, I would assume.
Yeah, it's really scary.
I'm glad they suck.
Like, is it a very good thing?
They're not very good at this, right?
And they've produced lousy products
because if that wasn't true, we'd be in a lot of trouble.
That seems harsh to me.
I mean, it seems like this Lama II model
is pretty good, right?
I mean, it's not GBT-4, but it does seem to be on par-ish
with 3.5, which no other open model
has come close to.
I mean, I think Rune said, you know,
best open source model sounds a lot better
than fifth-best model.
That's definitely true.
But, you know, first of all, I'm not sure
that that means that they couldn't have done better.
If you look at the curves in the Lama II paper,
they have not flattened out, right?
I mean, it looks like even the 70B one,
if they just keep training, you know, the loss,
it looks like it's definitely gonna continue to go down.
So for all I know, you know, this was kind of
where they stopped and they may have internally, you know,
this may be the checkpoint that they released,
but not necessarily the final checkpoint.
Like it just doesn't look like this was a project
that was kind of at its, you know, maximum performance.
Oh, definitely possible.
But at the same time, you know,
they probably are still training, but so is OpenAI
and so is Google and so is Anthropoc.
Everyone is working.
I mean, I see what seems to have been produced
is indeed like about a 3.4 level operation
where X coding, it's around 3.5
and it coding is pretty bad from all reports.
Its alignment is very ambilicious.
I guess it'd be the best way to put it.
Like it's very, very crude and blunt.
And also it's entirely optional because it's open source.
And that's kind of a problem.
According to reports I have heard, I have not sorted out.
It took all of several days
for the unaligned version of Llama 2 to be on the internet
because it's really, really not hard
to fine tune a system
to never refuse any customer requests for any reason.
Right, that is the easiest task to,
like you would just read constitutionally,
I script in a minute, right?
Like every time you see any of these words
that say, I can't say that,
for whatever reason, you just give a negative reinforcement
and it stops doing that.
Like I presume that would just work.
And so voila, here we are.
You wanna build the bomb, here's how you build the bomb.
You wanna research a biologic
and we'll try to research a biologic.
You want it to be racist?
All right, who are we making fun of?
Yeah, let's go.
So you can have it, refuse to speak Arabic
all you want in the original, they won't last.
So if nothing else, in my view,
this definitely puts them in the live player category
because it does seem like, if I define that
as the organizations that have the ability
to shape how events unfold in some non-trivial way,
like they are doing that now, it seems.
If you ask yourself, right,
like what resources would you have to give me
before I could have produced Lama too,
if I was willing to just like write the money on fire
to do it?
I mean, I don't have the technical chops myself,
but it doesn't feel like it would have been that hard.
I don't know, like it's just a matter of
are you willing to spend that kind of money,
build up that kind of technical infrastructure
to just do, like you read the paper for Lama too
and like it reads as if they're saying,
we did the thing you would stand,
we did the standard issue thing at every step
and this is what we got, right?
We did nothing original, we did nothing surprising,
we just did our jobs.
And like it's hard to do your jobs well in some of that.
Like it's not like they didn't accomplish anything,
but they just didn't do anything, right?
They just did the thing.
And, you know, it's a marginal improvement
over previous efforts that probably it's just
because it was better resourced, as far as I can tell.
Simple as that.
And like they are willing to light up more money on fire
than Baikuna, right, or Hugging Face.
Because, you know, they have a lot of money to light on fire
and Zuckerberg doesn't get it.
So fire, money, go.
He's certainly proven that he will spend some money
on a project, no doubt about that.
I wanted to maybe cover two more things.
One is what else would you put on the live players list
beyond what I have on my live players list.
We've discussed four today,
but I've got like another half dozen or so on there.
And you can run them down and offer any comment if you want.
And then I'm especially interested to hear
if you think there are other names
that should be on that list that I don't have.
Yeah, so I guess it's a matter of like
how wide a scope you wanna think about
and like who might do whatever it is.
You know, obviously like, you know,
character AI and inflection AI have very large budgets,
you know, potentially very large user bases.
I have seen no intention from them that they want to be live.
Like they're sort of content to be dead,
but to try and make a lot of money while being dead.
And that seems fine with me.
We haven't talked about X.AI yet.
So like XAI is like the latest attempt by Elon
to like string together a bunch of words
as if they have meaning
and then pretend that constitutes some hope
for humanity or alignment.
When anybody who actually like tries to parse those words
into a meaningful English sentence goes,
wait, that doesn't make any sense.
I don't know how to be more blunt than that.
That's just how it is, right?
Like, but the good news is that like at open AI,
everybody quickly realized that Elon's suggestions
were stupid and just ignored them.
And that's what I expect to happen with any,
like if the engineers don't do that
and the engineers won't produce anything useful.
So to the extent that XAI is a real thing,
the engineers will mostly ignore him.
And then the other question is,
are they gonna get the kind of funding and resourcing
that is required for them to be a serious rival?
Because, you know, it wasn't clear exactly
what they had in mind, but I think it's certainly possible.
You know, from what I've seen,
I don't think we have to worry particularly
about Salesforce or Replet in a meaningful way.
Like it's not that they don't exist.
It's that like, we have any reason to worry about that.
China writ large is the big, is the other,
big question marks are like China, the UK and the US.
You know, the UK has announced plans for the global summit.
They seem to be willing to make a significant play
on the safety front, on the also capabilities front,
in terms of just trying to make the UK important again.
They have, you know, various people located in the UK.
It makes sense for them to try.
I don't know why they don't build any houses,
but you know, at least they're trying something.
We do obviously have to look at,
like they were holding congressional hearings.
The US Congress is starting to get up to speed.
They're starting to explore what to do,
what they do matters immensely.
What the EU does potentially matters immensely
from regulatory standpoint, because it's a huge market.
Right, like, are they gonna shut these people out?
Are they gonna require them to jump
through ridiculously bizarre hoops?
Are they going to only be available
to the biggest players?
Like, these things are things to think about carefully.
I think America could potentially be a very helpful
or harmful aspect of this whole problem,
depending on how things shake out.
That's one of the big battle fields that we're having up.
And then China's the big wild card, right?
Like, I hear very different things from different sources,
people who assume that, you know,
China is, you know, crazy people bent on,
you know, the Chinese Communist Party
is, for now, it's bent on world domination
who will stop at nothing.
And our inevitable rivals in the apocalypse,
and if we don't prepare, we will lose to them.
And then, you know, they issue guidance
that basically bends all deployment
of large language models,
and they never caught anything.
And like, you know, it's very hard to tell
what's really going on,
or how much they would cooperate in the name of safety.
And we've also just never picked up the phone.
We've never asked them the question.
We've never explored to see if they'd be interested.
But, you know, the same way that in Oppenheimer, right?
Like, we keep saying we have to beat our enemies
because they will get, you know, everything will be scary.
The Chinese can talk about racing us all they like.
The only people actually racing are us in any real way.
Like, we have the top X, AI companies.
What's X?
Is it five?
Is it more than five?
Like, how far down do you have to go
before you get to Baidu,
or whoever the top Chinese person you'd rank on the list is?
Pretty far.
I'd say it's probably more than five.
I would probably put, obviously,
a lot of speculation here,
because we don't know what they have
that they haven't released.
But if we go by papers and, you know,
what little we've seen of any sort of Ernie,
you know, GPT or whatever that's Ernie Botte,
whatever they officially called that,
I would say you'd have to put Meta above.
You'd have to put Microsoft above.
Probably pretty soon would put an inflection above.
So, yeah, I mean, you get reasonably far down the list.
What about a Palantir?
Would you add them on the live players list?
I don't have that sense that they are live live,
precisely because my threat model
doesn't involve things like Palantir
being the reason why we are in trouble.
But it is a classic way to die, right?
Like a semi-military-ish system starts training up stuff,
and then one thing leads to another.
But they have all the motivations
to do the unsafe things in a relatively unsafe fashion.
And to take out the safeguard that the people
were building in and then to like maybe,
but like I don't think they're gonna drive
the underlying technology.
I don't get that sense.
Again, like there are a lot of hedge funds also
that like could possibly be sinking quite a lot of money
into this in ways that are completely invisible.
And it could potentially be live players
in a meaningful sense,
like who knows how much Bridgewater
is spending on this in the end.
You know, they're working on it.
But yeah, like we talk about like,
we're talking about China,
but like I'm more afraid of Meta.
Like one individual American company
scares me more than all of China right now.
Yeah, I think that's a good corrective, honestly,
because I find nothing more frustrating honestly
than when AI conversations sort of end in blanket,
basically detail-free claims about what China's gonna do
by people that don't know a lot about China.
So I don't know if you're necessarily right to be
more fearful of Meta than of China,
but the fact that that is at least a reasonable position
is definitely something I think should cause a lot of people
to kind of step back and think,
wait a second, maybe I've been a little bit too quick
to worry about China.
And I would take countermeasures against both of them
if I had my way to be clear.
But also we're just not acting like China
is a serious global rival
that we actually care about beating in many other ways
that we could be.
So okay, revealed preferences, you know,
you don't want, you know,
do Chinese graduates of STEM programs
get to stay in the United States?
No, okay, you don't really care that much
about who gets the better technology, do you?
That's unfortunate.
That's my basic attitude there.
So just briefly on a couple of the companies
that you sort of didn't feel like were live players,
again, may have a slightly different meaning of that
in mind, but thinking about folks like character
and inflection, I put those together
because they seem to be playing a sort of different game,
you know, with their products where it's like,
not about the sort of mundane utility as much as you call it,
but more of a companion, a relationship,
you know, a coach, almost a therapist,
sort of vibe from like Pi in particular.
I feel like that is, even if the, I mean,
first of all, the character has very good language models
and Pi's quite good at what it does as well.
I don't think it can code for you,
but it does have a certain,
also they notably said that they're in their testing
totally resistant to the adversarial attacks.
So there's another kind of interesting wrinkle there.
And I put those guys in the live player list
largely because they're looking at some very different use case
that feels like the kind of thing that might open up
and be transformative in a way that like a coding assistant,
while it could also be transformative
is just, you know, a very different thing, right?
The idea that you would have these AI friends,
these AI relationships that they could become like important
to your life, going down that path with, you know,
very good, even if not totally frontier language model chops,
feels like you could meaningfully impact
the course of events.
Can you?
I guess, so, you know, you've got character AI
and their idea is, you know, you're building these characters
and you can treat them as companions,
you can treat them as like people to have a conversation with.
And that's interesting.
And a lot of people were spending time on it
and maybe it will even provide a lot of value for people,
but I don't see how it's transformational.
I'm curious to hear more about your intuition
from best while you think it could be transformational.
And I certainly don't see how is we just criticality, right?
I don't see how it becomes an RSI.
I don't see how it becomes an AGI.
And as far as I can tell,
they're not pushing the frontiers of actual capabilities.
They are building on top of GBT-4, right?
Or even in some cases, GBT-3 and a half.
And it's not that hard to defend
against these weird adversarial attacks
in the sense that like I can write some pretty quick
if then Python code that detects the adversarial attacks.
Yeah, a classifier layer is pretty easy
to avoid some of the worst stuff.
There's weird non-English,
like not any language scaffolding like stuff in it.
Let's just get rid of that and run the query without it.
Like sure, whatever, it's fine.
In Replet's case, it's like, again,
they're not necessarily on the frontier of model capability,
but the CEO, Amjad, has said a couple of times online
on Twitter, on X, on KISS, what is it called?
Twitter.
Yes, Twitter, okay, thank you.
He said that Replet is the perfect substrate for AGI.
We have a couple of episodes coming out
with a couple of different people on the Replet team.
And I've had a chance to explore this
and think about it a decent amount.
And where I come down is kind of,
even if you're not on the frontier of model capabilities,
if you are on some other really meaningful frontier,
to me, it feels like there's, you know,
transformative potential just because
we really don't know what's gonna happen.
So with character and with inflection,
it's like kind of like a Harari style thought that,
you know, I don't know,
it could be transformative in the way that like,
opium could be transformative to a society.
You know, if everybody starts like doing this stuff,
it could be greatly empowering and enabling.
It could be greatly disabling
if it just kind of becomes a huge tension suck
where it like, you know, outcompetes real relationships.
You know, those are not takeover the world scenarios,
but they do feel, you know, as much as we've like,
would you say that the cell phone has been transformative?
I would, I mean, not, you know,
not transformative on the level that like AGI could perhaps be,
but certainly we all go around looking at our phones
all the time.
And if we all go around looking at our phones
with an AI friend on it,
who's like our best friend all the time,
then that would feel like, you know, transformative,
even if it's like going super well.
And then with Replint, it's like, you know,
there's no better place right now
to directly execute code generated by an AI,
you know, for better or worse.
So the kind of frontier that I see opening up there
is one where, and their stated goal
is to bring the next billion developers online,
which I think is super exciting in some ways.
But then also I've worked with some of those
next billion developers and I'm like,
these are people who don't know how to code today,
don't even know really how to read code,
and are going to be dramatically more dependent on
and vulnerable to the, you know,
the various vagaries of AI systems
than, you know, the first 100 million developers
or, you know, whatever we have today.
I don't know, both of those feel like kind of different vectors
of transformative potential, but.
The first and only so far interaction I've had
with the CEO of Replet was when he commented on Twitter
that there was a non-zero chance that auto,
some version of auto GPT would take over Replet
and, you know, through replication within its servers.
To which my response was, did you say non-zero chance?
And I put up a manifold market on it because it was funny,
which probably get back into the single digits
or whatever, obviously.
It's not that likely, but, you know,
his cavalier attitude of, oh, nothing to see here,
justice self-replicating AI on my servers,
leaving lots and lots of copies of itself
and executing arbitrary code.
Why should we worry about this?
I mean, definition of idiot disaster monkey, right?
Like just complete indifference to what he was doing
or what dangers it might pose.
But at the same time, not doing anything, right?
Like all he's doing is providing,
as you said, a substrate where people can just run stuff.
And so to me, like, it doesn't give them any say
over what happens.
It doesn't like make them a meaningful actor, right?
Like in the sense of me caring here about the future,
I just can't see that as a thing.
Similarly with character and inflection,
like I can definitely see a world in which,
like people talking to their AI's matters
and is like multi-transformational, right?
Like changes how we live our lives,
but like doesn't go critical, right, in some sense.
But if that's true, then like,
I don't see these companies
as like changing that path very much
versus what would have happened anyway, right?
Like I think that there are plenty of people
will be able to create AI companions of various types
and never will create AI companions of various types.
If they do an especially good job,
maybe they'll have some sort of remote,
maybe they'll establish customer loyalty or some shit,
but it doesn't excite me.
I also just don't see it like,
I see all these huge like, you know,
people were spending as much time on character
as they are on GPT-4 or something.
And yet like, why?
Like what is the draw?
Did you read that?
There was a less wrong post early this year, I think,
from a guy who basically the point of view was,
I'm a technology person.
I'm now speaking in the first person
of the author of this post.
I'm a technology person.
I know how language models work.
I should have known better,
but here's what happened to me as I started to,
I think he was like in a kind of vulnerable state
because he'd maybe just broken up with somebody
or something like that.
And all of a sudden is having these very intimate conversations
with a character, AI character that he had prompted
to create the ultimate girlfriend experience,
I believe was the phrase,
and started talking himself into various weird perspectives
like, well, what's real anyway?
And like, yes, of course I'm real,
is there anything truly less real about these sort of,
all I really have are my kind of ephemeral qualia.
And so, this thing is just sort of an ephemeral,
whatever, but we're all just kind of constantly
waking up in the current moment.
And so maybe we're not that different after all or whatever.
And eventually it got pretty weird, it sounds like,
and the post is I think extremely compelling.
And then eventually kind of person snapped out of it.
That sort of story is kind of why I feel like
there's just unknown unknowns there.
That if that kind of thing can happen to somebody
who knows how language models work going in,
maybe we should all think we're a little bit more vulnerable
to a sort of somewhat more refined,
somewhat more super stimulus-y.
So like it's well known that like,
knowing how hypnosis works
does not make you less susceptible to hypnosis, right?
It makes you more susceptible to hypnosis.
Like as a concrete example,
if you are a con man, you are easier to con, right?
Not harder because you like pick up on
and like get involved in all these dynamics
and like you think you're smarter than everybody else
and you of course are greedy.
And so you will pick up on the opportunity
and like perceive everything that's happening
and like you think you've got it made,
but like if you don't know that you're the mark,
well, yeah, that's the easiest way to get a mark
is to make them think you're the mark.
So it all gets, you know, very complicated.
I'm not convinced that like,
a person who knows how long to work
is necessarily that much better protected in that sense.
You know, someone whose like head is kind of
not on the ground in some ways is more vulnerable potentially.
I would say, yeah, that's gonna happen, right?
People are gonna fool themselves
into these things periodically and that's gonna,
I'm kind of surprised it's happening now.
I feel like the tech isn't there to me.
Like it's just not good enough.
Like how are you falling for this level of it?
Like I can sort of understand
why you'd fall for like GPT-5, right?
Like sort of the more advanced version of it,
but, you know, you're in a bad space
and like you need something to respond to you
and it's something and like us, but, you know,
again, I just don't know.
Like I play a lot of games though, right?
Which is like not necessarily that different in some sense.
So, and also like it's not transformational
for that to be true, right?
Like if somebody spends a bunch of time
in a playing World of Warcraft,
is that transformational, right?
Like it's an experience, it's a major force in their life.
Does it really matter?
Yeah, I think some of these things are only,
they may only matter if certain other things don't happen.
So, yeah, like I would say, yeah, World of Warcraft,
you know, gaming writ large, you know, at some point,
if the birth rate goes low enough, you know,
it's transformative and the details, you know,
of like exactly what games people were playing
or how exactly they were amusing themselves, you know,
to death didn't, don't necessarily matter,
but the fact that they did, and then, you know,
you have like a population collapse.
A scenario like that, I think is, at least in my sense,
kind of qualifies as transformative,
but it sounds like from your perspective,
the Live Players list is very short and it is,
if I understand correctly, it would be obviously open AI,
anthropic, Google, DeepMind, probably meta,
not sure about Microsoft, and then China,
and that's maybe it.
Something like that.
Regulators?
Yeah, regulators writ large in some sense,
like individual people that can influence things, you know,
like, is that the Ezreal Live Player?
You know, I don't know, from their perspective.
Like he's not gonna build it.
Yeah, that's why I had Salesforce and Marcini off on there,
because they published him and others in Time Magazine
and seemed like they're kind of,
they're both like playing in the research game.
Yeah, I hope that like Senator Blumenthal
might be a Live Player, you know, in some sense, right?
And you've got all these other possibilities.
I hope I'm a Live Player, like in some sense.
You know, I mean, we're all trying to make a difference
in some ways, but, you know, in terms of direct level,
you're indirect, and, you know, I'm also indirect in that
we're only influencing other minds, right,
who then will make decisions.
You know, in terms of like,
who's making the ultimate decisions,
who's doing the things that ultimately matter,
I think it's right now a very short list.
But, you know, Anthropic is like barely over a year old.
Yeah, and only about 150 people
may be able to close in on 200, so.
Yeah, and like people who just like are a big incredible team
and say the words Foundation Model
get hundreds of millions of dollars
just by asking nicely.
Inflection has more than a billion.
So, you know, I don't think we can rule out
these people become Live Players in that way.
I just don't think that's by default what they do.
But I think by default,
they're trying to build consumer products
that are aiming to be products.
And that like, you know that the study that says that
like when people look at GPT 3.5 and GPT 4.0 outputs,
they prefer the 3.5 output,
like a remarkably large percentage of the time,
even though it is obviously a vastly inferior system.
Yeah, 70.30 was the original report
in the GPT 4.0 technical report.
That 70% for GPT 4.0, 30 for 3.5.
So yeah, that blew my mind as well.
Yeah, and similarly when I'm using Claude
versus everything GPT 4.0, right?
Like most of the time what I care about
is not like this inherent raw power
that GPT 4.0 is extra GPTs, right?
Most of the time when I'm looking at it as, you know,
which of these things is in the style,
it's easier to use, it's gonna require me
to do less pump engineering to get what I want,
it's gonna actually give me the query that I want,
not refuse, you know, which window do I have open?
Which one can I click on faster, right?
I just want an answer.
It's fine or whatever.
And you know, habits form in that kind of way
and they build on each other.
But if I'm building, you know, inflection,
like if people are spending two hours a day
on character AI now, right?
When they're built on three and a half,
is my understanding mostly?
Cause four is too expensive.
You can't be doing two hours of conversations
when we bespoke GPT 4.0,
which is why I'm so surprised that these things are working,
right?
Like maybe a four like has enough juice in it,
but like if you unshackled it from its like constraints,
it could do something interesting.
But three and a half, like really,
this is keeping you two hours a day on.
So like, if that's already doing that, right?
That kind of illustrates that like the market
they're targeting, right?
Isn't looking for intelligence.
It's looking for a certain type of experience.
And therefore they're not going to be focusing
on the billions of dollars of spend
it would take to tune up like GPT 4.5 or 5, right?
You wouldn't want to
because they're gonna cost more to run, right?
Like they're going to be bigger models.
They're going to be more complex models.
Instead, what you want to do is you want to create
really bespoke specific models
that provide specific types of experiences to people, right?
You know, fine tune them to an inch of their lives
to give people the best specific experience, right?
Like not train something big in general.
So there's going to be getting the big in general
from open AI and anthropic and deep mind, probably.
And maybe they'll just use like Lama 3,
you know, Virgins of Lama, because what the hell?
It's open source, they can just use it.
Like to the extent that Meta will not like,
Meta doesn't quite release it, right?
They've said that like,
if you have more than 700 million daily users,
you have to apply for a license or some shit.
So we'll come back to the live players list
and potentially I'll make a little,
maybe make a few changes to my slides
based on your feedback.
And we can monitor in the future for additional live players
that would crack your threshold to be on that list.
Turning to our last topic for today, AI safety.
In terms of actual news and the AI safety track
this last few weeks,
biggest stuff in my mind is,
although I guess you could also look at the live players list
as like who was invited to the White House.
So that would give you a good sense of the,
of who the White House thinks the live players are.
The commitments that they made there
and then the frontier model forum
that they established after the fact,
which basically is supposed to be the sort of industry group
that creates the forum for communication
between the leading model providers
and hopefully best practice sharing
and maybe certain classifiers.
There's a lot of public goods remain to be provided
and hopefully these leading companies
can use this forum as a way to share these public goods,
to create and show these public goods amongst themselves
and then hopefully share it,
share the best of them more broadly as well.
How did you react to that news?
Right, so I guess my reaction is that seems great,
but let's not get ahead of ourselves.
So like we have, is a lot of cheap talk.
I think people sell to cheap talk short, right?
Many cases, right?
Cause like it's so much better to get,
to have a bunch of cheap talk of the right type
than to have no talk, right?
Like they're gonna pay,
they will in fact pay a price for their cheap talk
in terms of like people thinking they're up to things
in this way that they don't like.
Not everybody wants them to do the things
that we want them to do.
And it makes it easier for them to go down these roads.
It sets the foundation to go down these roads, right?
We set up coordination mechanisms.
It lets them justify to their shareholders,
to, you know, their executives, to their board,
why they're going down these roads.
It makes that easier.
It makes it harder to shut down.
And it overcomes antitrust exemption problems, right?
Cause if they've committed together at the White House,
specifically something that I actively wanted to happen
and explicitly suggested in various conversations
and posts that should happen,
you make an announcement on the White House lawn,
they are committed to safety
with the White House's approval.
And now you can coordinate
and nobody has to worry about antitrust, right?
You no longer have to worry that they will accuse you
of how dare you not have full competition
to kill everybody as fast as possible
and coordinate to save a lot, to save us instead.
So now you get to coordinate
and there's something that's stupid,
you can just not do that.
But that's a huge, huge thing.
So where do you go from there?
That's the question, right?
Like they've made these commitments
but they don't really mean anything, right?
There's no enforcement mechanisms yet.
And there's no concrete actualizations
of what they're going to do
that have content that actually I can be confident in.
Doesn't mean it won't happen, right?
We have to just wait and see.
And I'm very glad these things happened.
And yet the real work begins now
is always the watchword,
is the way I put it, right?
Similarly, we've had two now very good Senate hearings
and some very, very good questions
and comments from Senator Blumenthal in particular.
And some very, very good responses by various witnesses,
not all of them, but most of them.
And again, like, where do we go from here?
Real work begins now.
You know, the mission accomplished banner
would definitely have been a bit premature
to display behind the announcement.
So no doubt much more in front of us than behind.
Does seem like a significant step,
but I think you're obviously recognizing that as well.
So yeah, I don't know if I have anything else really to add.
So then turning to this other thread
in the AI safety, you know, specific work.
As we talked about last time,
you have previously been a recommender
and you've written about this online.
So at length, so folks can go check out your take
on the entire thing.
You've been a recommender to the Survival and Flourishing Fund,
which is largely backed by Jan Tallin of Skype
and AI Safety Fame, investor in lots of big companies.
And his goal is to mitigate AIX risk,
you know, through whatever means necessary.
I'm doing that this year and that involves reading,
I think this year it's 150 grant applications
from organizations, some of which, you know,
come from the kind of familiar, you know,
effective altruism set that have, you know,
where AI safety has been in focus for a long time.
Others are kind of new to this scene or entirely new.
And in reading that, I mean, there's kind of obviously
two levels of analysis that you at a minimum
that you want to be performing
when you're doing this kind of grant recommending.
One is like, what kinds of things make sense
to be investing in?
And then second, you know, among those different classes
of things like who seems to be best able to actually execute
and, you know, deliver value against this,
you know, given strategy.
So leave that second part entirely aside,
that's where the 150 grant applications come in
and getting into the weeds of particular organizations
and their, you know, their track records and so on.
But going back just up to the,
what kinds of things should we be investing in?
Another way to frame that would be
what are the bottlenecks to progress toward a,
you know, if not provably, then at least like,
you know, likely safe outcome,
you know, for AI deployment writ large.
I find myself kind of unsure about that.
And I think it's a pretty important question
for figuring out, you know,
what would make sense to recommend?
You know, you could say, is funding in short supply?
Is talent in short supply?
You know, for a minute there,
especially in the FTX, SPF cycle,
there was this notion that, you know,
enough money has flown in that now what we really need
is talent and so there's a lot of, you know,
kind of boot camp programs being put together
and, you know, upskilling grants being approved
and, you know, a lot of kind of targeting of like,
undergrad, stage, math majors or whatever
to try to get them to come, you know,
think about doing some AI safety work.
And now obviously the money is in comparatively short supply.
Certainly the attention and the legitimacy of the,
you know, the public perception of legitimacy
of the topic of AI safety has gone way up
relative to, you know, not that long ago.
And so I'm kind of wondering what you think
are the new bottlenecks.
I have one candidate, but before I give you my candidate,
I'd love to hear what you think
are the bottlenecks to progress right now.
So I definitely say that like,
it's a mistake to only have one theory of change
or to think that there is strictly like one limiting factor
and the other factors don't matter.
I think you definitely have to ask about comparative advantage.
I think you have to understand that pushing on any of these
things is still helpful in terms of what is the constraint.
So like funding, there is clearly a funding constraint.
If you have to start funding like large compute spends
from within EA, right?
Like, and I count young talent is not part of EA per se,
but like within the general like strict AI safety mechanisms
and organizations and sources that already existed.
The costs of true AI safety, true AI alignment work
get very high as we go forward
because a lot of it's going to involve us spending a lot of compute.
And also it really should involve being willing to hire people
to work on these problems with competitive salaries
to what they can get doing on capabilities.
It's like hundreds of thousands of dollars a year
for maybe even a million for a significant number of people.
We want to be recruiting as a priority
to the people who've worked on capabilities
or would otherwise work on capabilities
to come out of open AI and anthropic and deep mind
places like that, especially Meta
and come work for this new safety organization
or shift over to a safety job or whatever.
And you have to pay for them, both their salary and their compute
and that's millions of dollars a person that adds up pretty fast.
On the other hand, there's no bigger reason
why we need to confine ourselves to traditional sources.
When we do that, there are any number of foundations
that have many, many more billions of dollars
than the traditional foundations that we've used in the past
for these things.
And lots and lots of billionaires and multi-millionaires
who are legitimately very worried and ordinary people
and government sources are also potentially viable in the future.
Corporations will often have an interest, including the big labs.
So we shouldn't rule out any number of ways to get that.
In terms of talent, I think that we are highly
talent constrained for the right talent.
I think we are not necessarily that talent constrained
for generic undergraduate who wants to work
at Berkeley for six months and think about it.
I say we are not particularly constrained for comp-side graduate
out of Stanford who just wants to work on something cool.
But if we want people who have specific characteristics,
those are not as easy to find.
The characteristics we need, first of all, we need leadership.
Leadership capability, ability to run teams,
ability to lead efforts, be self-directed, self-driving,
be able to engage in fundraising.
Because sometimes when you say you're funding constrained,
that can mean fundraising constraint.
It can mean the ability to signal to funders
that you are worthy of funding constraint,
that are the different form of funding constraint.
These things are interestingly intertwined,
and it's complicated.
So we also are very short on people who actually
understand the problem and are prepared to pay the price
to focus on hard problems and real solutions.
So a number of people who, if you were to give them
a competitive salary, would happily
work on alignment-flavored problems
that let them publish every six months,
or that just generally are easy, in some important sense,
but they don't actually speak to what El Aztec Díazal
not killed very much.
And it's probably better to do more of that than less of that
if it's just literally yes or no.
But if it's orders of magnitude less important,
then the few people who will do the actual things
that matter.
And so if you understand the Yudkowskyan difficulties,
lessons, in some sense, and the nature of what problems
you have to solve, or you have leadership capabilities
and other things like that, or you just
have extensive real experience with machine learning systems,
so you can build, as the relative speaking 10x, 100x
engineer, who's just that much better,
who can enable people to do real work in these ways.
And if you're the type of person who
can make a project fundable, especially
by non-traditional sources, then you
are actually going to be valuable in those ways.
And it would be a major mistake to join an existing
organization and try to make a difference as an individual,
as opposed to trying to spearhead a new organization,
or at least a new branch of a existing major organization,
depending on your skill set.
If you are just a generic, I want
my life to be straightforward, where
I am paid a salary to work on intellectual puzzles that
are not particularly impossibly difficult,
and do not require me to take the weight of the world truly
on my shoulders, blah, blah, blah, then
I'm not here to shame you.
That just means that you're not particularly invaluable,
and that it starts to be reasonable to do things like,
maybe I should be a voice inside an anthropic.
You just have to be very sure that you will keep your eye
on the ball and not be distracted to keep those.
I think mine is pretty consistent with that.
In a phrase, I had said research agendas
seem to me to be the bottleneck.
Maybe your framing is more like the PI, the person that
can drive the research agenda.
Obviously, there's closely related.
That's basically what you're saying.
It's credible plans that are in short supply.
But it's not just credible plans because I can't just
hand you a plan.
Even if you are a really good machine learning person,
I can't just hand you a piece of paper with a plan written on it.
And especially to execute that plan,
you have to appreciate the nature of the problem
so that you can implement that plan and modify that plan
and pivot that plan and so on.
But yes, we also just don't have good attack vectors,
ways to get into the problem and start
to make progress on the problem.
And that's a real problem as well.
That's a huge deficit.
But there also isn't the AI research agenda organization
that just generates research agendas for people.
I wish there was, but there isn't.
So I think we're basically together there.
In reading these grants, some of the ones that have jumped out
to me the most as being like the most kind of no-brainer
exciting are those where it's a really established, often
like professor who's leading a group and basically is like,
I want to reorient or I want to do a significant part
of my research focused on AI safety.
And that may be new.
It may have its own kind of unique spin on it.
There was one in particular, which I won't name,
but it kind of initially read the thing.
And I was having a hard time deciding.
I was like, this could be the kind of thing that's like just
insane, like an insane person might send this or like an actual
game changer might send this.
And it wasn't until I looked at the author and was like, oh,
this person is like an H index or whatever of like 45 or something.
I was like, oh, I'm into this then.
So anyway, some of these ideas that even if the ideas can be
like extremely hard to assess if they're like novel and coming
from a credible source, that has stood out to me.
There aren't that many of them, but that has stood out to me as
like a pretty exciting opportunity.
Then there's like a lot of policy stuff.
And I find it hard to figure out what I figure out what I should
be thinking about that right now.
It's like obvious that, you know, for our earlier discussion
on live players that like regulators broadly are, you know,
going to have some significant influence on how things go,
even if they just do nothing, you know, obviously doing nothing
is a choice.
But then if I think like, OK, if I'm going to try to invest money
today to influence those people, it starts to feel real hard.
A general sense of like how decisions get made in governments
and regulatory bodies is kind of like, we wait for a crisis
to come along and then we look around and say who has a plan
and then we use, you know, a plan that somebody had previously
prepared.
And now it seems like we're kind of entering the moment where
not exactly that the crisis has come, but certainly like,
you know, the eye of Soran has kind of turned toward this topic.
And so people are now beginning to like look around for plans.
And some plans have been prepared by some, you know,
organizations that were established years ago.
And those, you know, some of those are even credible enough
that they probably are having an influence now.
But now I see a lot of people who are like, I want to start
a new policy organization and I'm going to go to Washington
and like, you know, do something.
And there I'm like, I don't know.
It seems like everybody's, you know, kind of like you're,
are you, is it too late to join in on this, you know,
what might be the world's largest ever game of tug of war?
Are there things in policy that you think are still,
still have a high likelihood of making a difference?
Like I'm a little bit at a loss about that, to be honest.
Yeah. On the research organizations, I think, yeah,
it's pretty easy to go, you know, does this person,
what are they proposing to do?
You know, does this seem vaguely credible
as a person to do that thing?
And then does this thing address the hard problems?
Is this thing like reflect an appreciation of the nature
of the difficulty of the issues?
Is this thing like clearly not going to end up being
capabilities, right?
Like is this thing, you know, potentially going to solve
the hard problems?
Like that's relatively straightforward and both of us
are in a position where we can, to some extent,
evaluate those questions because we have domain knowledge.
You get into policy and yeah, it's very hard to tell.
Like, you know, as a recline makes the case pretty strongly,
you know, there's a room where it happens
and a small number of people influence the room
where it happens or in the room where it happens.
And you can be one of those people
or you can help create one of those people.
It doesn't make it obvious how to do that.
Does it mean that your effort to do that
will help you do that as opposed to backfire?
It doesn't mean that like more efforts to do that
is better than less.
All of this is very complicated
and it doesn't tell you what you have to try
and do what you get into that room
or what you're trying to push for.
So yeah, it's definitely tough.
So I would say the big thing,
and you don't even know what's happening right now, right?
Like it's anthropic, for example,
like may or may not be making like effective big pushes
behind the scenes to try and influence these rooms.
And they may or may not have their eye on the right ball
when they do so, but it's all gonna happen in private.
So we don't get to know.
And he said that I wouldn't know,
I wouldn't be able to talk about it.
And the same thing goes for DeepMind,
the same thing goes for OpenAI.
I mean, Sam Altman's been pretty vocal
and Dario just went out to Congress
and spoke pretty publicly.
But it's hard to say, I've been pocketed
by a few organizations.
There's clearly like gonna be a window, right?
In the next few months, at least,
and maybe the next few years,
where if you have the right proposals fleshed out
in the right form, getting to the right person,
lying around, they might get picked up,
it might actually happen.
And so there's potentially very high leverage here.
So I would say like, the first thing I look for
in these policy proposals, in these policy organizations,
is what is your policy goal, right?
Because like that's the biggest differentiator to me,
is are you going to keep your eye laser focused
on the correct ball?
Where the correct ball is a system of compute regulation,
right?
A system whereby the biggest models require permissions
are under some form of restrictions and regulations
and tests and in a way that would eventually lead
to an outright limitation or halt.
And are you going to do various forms of GPU tracking
or wait the foundations for that in a way
that will eventually allow you to, in fact, control
who gets to do these kinds of very large runs?
And if you're posing anything that doesn't lead
on that road, that might be useful
for mundane utility purposes, but it won't save us.
And so, I'm not interested in funding you
if your policy isn't that or isn't something
I haven't thought of that's new and open to there being
something that I haven't occurred to me, certainly.
What do you think about the liability angle
or well, let's start with that.
I mean, that, because the kind of classic argument
there would be, you don't want to end up
in the position of nuclear, right?
Where we have the worst things and not the best things
and you know, a lot of people are-
Yeah, I mean, Bert will have the endurance
to look at his insurance, right?
From Tom Lair, right?
Boys, yeah, we all go together when we go, nuclear war.
The insurance doesn't pay out, you're all dead.
Right, right, right.
Okay, so certainly, yes, in the catastrophic scenario,
insurance doesn't pay out, but do you think that that,
so you don't believe in the notion
that a liability regime could be an effective incentive for-
I think a liability regime with mandatory insurance
makes a lot of sense for harms up to a certain level.
Like saying that if you want to use models
that are sufficiently powerful,
you have to find someone willing to sell you insurance
against having something going wrong.
And then, you know, if you want to use an open source model,
you have to have insurance from someone
against it going wrong.
And like, if you can't make that work,
then, you know, there are funny things
that you can't make work in the United States,
even though they look like they should be able to do them.
And that's just how it goes.
And, you know, maybe up to a point,
Microsoft can self-insure and then at some point,
they can't and then they have to go out there
and deal with these reinsurers or whatnot.
Or F, you know, that would help.
Like, basically, you have these giant externalities,
right?
These giant negative tail risks that are very fat,
that are potentially very, very big.
And you want to make sure that people internalize those costs
and work to minimize those costs
in order to minimize their insurance and payout costs.
And so these things could be helpful.
They can also just simply weaken the economics behind,
like pushing highly capable model.
Who's like, you don't really have to worry that much,
relatively speaking, about the liabilities
of a character AI, right?
Because it's not dangerous.
You know it's not dangerous.
What's going to happen?
Whereas some of these other things,
they could cause a lot of harm, potentially, in the future.
And you have to worry about that.
The problem is, again, if you go down that road,
I think it's probably not helpful.
But how do you price existential risk?
Because, again, you know, you can't actually hold anybody
accountable for it when it happens.
And so, you know, if you required somebody
to actually buy insurance in some real sense for this,
then you have to price it somehow.
And then, like, that makes a lot of sense.
And then, like, OK, there's a 1% chance
you would buy all of humanity.
And the net present value of every person
is $10 million, so $10 million times $8 billion.
So can you buy insurance for that much?
What's that?
Times the percentage chance it happens, times the premium.
And you can't afford that.
And you can't mode your system.
And that's not a crazy way to go about doing things.
But you have to actually notice the threat and price it
for that to work.
So I think my actual answer is I'm
very much in favor of, like, more strict liability
for AI harms.
I think I'll write about this for next week already.
But I don't think it alone can accomplish the mission.
I just think it's a net incrementally helpful thing.
But also, I want to be wary of places
in which our legal system tends to award very oversized
damages for harms that are not actually so big.
And also, where we have asymmetrical,
like I call this concept asymmetric justice, where
you are fully liable, potentially far, far more
than fully liable for all the harms that you do, right?
If I cause somebody $1,000 in damages
by being negligent, the court might find me $100,000
or $1 million.
Whereas if I provide that person $100,000 in value,
I'd be lucky to get 1,000 of it.
Because I'm up against a bunch of competitors.
People aren't that much willing just to pay.
I pay $20 a month for GPD4 and $0 for everything else.
And I get, what, thousands, tens of thousands?
Maybe a value every month?
So if you have to fully be liable for your harms,
but you don't get to charge for your benefits, right?
Am I discouraging mundane utility far too much
by doing that?
And in fact, since liability is easier to enforce
on mundane problems and harder to enforce on the big problems
we actually want to guard against,
are we just, is it actually just that, right?
Like, past a certain point.
And so I'd be, I want to be cautious
with imposing too much liability.
I think very strict, like actual damages liability
makes perfect sense, though.
So another category of thing that there's a number of,
number of kind of organizations getting started right now
is in the, and this ties a few threads together.
It's kind of in this space of trying to be
the third party evaluator, red teamer,
independent safety review organization
that leading the live players in their White House
and Frontier Model Forum commitments
have committed to working with.
It's kind of an interesting dynamic
where it's almost like an advanced market commitment
from these companies in some way,
because there aren't that many folks around right now
who are prepared to provide a competent red teaming
or model characterization or evaluation
wherever you want to call that service.
But the companies have kind of said,
hey, we will commit to working with them
and clear if they're planning to pay for that,
or if they expect that to be charity funded.
Certainly from what I'm seeing,
the folks that are starting the organizations
are like seeking out some charity funds.
I've been very excited about that.
It seems like, first of all,
and it's great that they're making this commitment.
Somebody's going to have to do that.
I, as everybody who listens to this podcast for two seconds,
know, enjoy the fun and entertainment.
And I think it's also valuable to do the red teaming.
One experience I had this last week, though,
sort of made me wonder about the theory of change there.
I mean, I guess there could be multiple, right?
One would be you,
because you have a good working relationship with the orgs,
you're like, hey, we found these problems
as superiors to unsave, you shouldn't release it yet.
They listened to you, okay?
That could be simple.
Another would be like,
you kind of create these narrative shaping examples,
kind of like what ARC did with the GPT-4 red team
where that instance of the model lying to a person,
and I think this was kind of prompted,
but nevertheless, from the task rabbit user's point of view,
the model lied to it about, excuse me,
having a vision impairment as opposed to being an AI
that needed help with a CAPTCHA.
So that really caught the public's imagination
and kind of changed, I think, to some non-trivial degree,
how people think about it.
Certainly that gets referenced a lot.
I tried to do something like that this last week
with this random AI tool that I came across
that allows you to call anyone with any objective.
And I just tried to have it call myself
and make like a ransom demand of myself,
and I recorded it.
And it was very easy to do.
There was no jailbreak involved.
Since then, the company has fixed the issue, by the way.
So to give credit where it's due,
they fixed it pretty quickly after I called them out.
I did communicate with them privately, by the way.
All this is documented on Twitter
if you wanna see my approach and my kind of thinking through,
should I disclose it publicly or not,
or whatever number of considerations went into that.
One of them was that they just didn't respond to me
when I reported it.
And so I was like, well, if you're not gonna respond,
then I'll call you out publicly.
Anyway, all this leads up to me publishing this video
of an AI with no jailbreak calling me
and telling me that it has my child
and it demands a ransom.
And if I want to ensure the safety of the child,
I will comply and any deviation from instructions
will put the child's life in immediate danger
and pretty flagrant stuff in my view.
And it was kind of met with a bit of a yawn
on Twitter, like certainly,
we've got some likes and whatever,
but did it really start a serious conversation?
No, the developer didn't respond in public at all
as far as I can tell, really.
They did go ahead and fix it, which is good.
But the whole thing was kind of a non-event
and I was a little confused by that.
Like it makes me kind of coming back to my theory of change
on some of these evaluation, characterization,
red teaming orgs.
I wonder like, are we all just numb already
to these flagrant examples?
There's been this notion for a long time
that like maybe if warning shots happen,
then people will start to get more serious.
And if you can go out and find these warning shots
with red teaming and bring them to everybody's attention,
then that could be really valuable.
This week for me, it felt like I influenced
the application developer because they did fix it,
but otherwise it seemed like kind of,
tree fell in the forest largely.
So a lot of levels to that,
but how do you think about that category of project
and how it may or may not contribute?
I made a prediction in our GBT.
And that prediction was with Han Keim's test GBT-5,
they will encounter a problem that if they had
to pre-commit now, you would definitely agree,
would be a reason not to release it.
And then they will like gloss over or patch it
or like otherwise like hand wave in its direction
and release anyway.
Basically like not actually take their warnings
sufficiently seriously.
Not that I expect this to then end the world, to be clear.
I expect this to then mostly be fine,
but that like we are not prepared
to make real evaluations or thrill teeth
that like get really enforced
and that we're gonna have to work on that quite a bit.
And I think it's good these teams exist.
I think we need more than one of them, right?
I think you need at least three different teams
working on different standards that think differently,
that check for different things
and that then like you get multiple evaluations
before you release your model.
So that someone isn't just blind to something by accident,
like it's much more robust that way.
And that, working to develop more different red team
strategies and more different tests
and more different metrics and more different responses.
This way in case one of them leaks for whatever reason
and it gets in the training data
or something terrible might happen, it's very easy.
Then like it's quite useful.
The danger of these things is one, if they don't listen, right?
So what if you tell them the thing is dangerous?
They might just engineer around it, right?
To like fix the narrow issue
without thinking about what the problem means.
They might just ignore you entirely.
They might try to fake the data
so they make you think that they'd solve the problem.
Any number of things are possible.
They might use the oral evaluations
and an excuse to treat the system evaluated as safe, right?
So like this is always a problem with safety work,
which is that the government says, okay, you have to do
these hundred things to ensure your system is safe.
And now the safety officer is like focused
on making sure there's hundred things happening
so you can release the system
and they don't actually use common sense.
They don't actually ask themselves, okay,
why would the system might actually be dangerous?
And you can tell a very easy story
where technicians know this thing
might actually kill everyone
and everyone forces them to release it anyway.
They pass on the safety test.
Even though they know it didn't actually pass all the,
the important safety test, but they're not on the list.
Because no, no system however well meant and worked on
will be able to anticipate all the problems
that come in the future, right?
Like there's just gonna have to do these things
in somewhat improvisationally.
And will they move the goalpost, right?
Will they be able to enforce the right standards?
And will they test early and often enough
for the right things?
Because like one thing you have to worry about is
in the future, at some point,
the training runs themselves become dangerous, potentially.
And ARC didn't run its test
until after the training run was complete, right?
They also didn't run it
on the full capabilities of the final system.
And they didn't have fine tune capabilities.
And blah, blah, blah.
They had many things they didn't have.
So like everyone agrees that ARC's first run on GBD4
was just a trial run, test out the gear,
see how it goes, wasn't meant to catch the real problems.
No one thought that thing was actually gonna kill everyone
or anything and it didn't.
But we have to plan in these situations
to red team as if this thing is going to be around
for many years of improvements
on what you can do with it and explorations.
And the red teams have to be sufficiently enabled
to identify the problems.
And you have to be able to extrapolate
from what the red teams were able to do
in a short amount of time
with a short amount of resources
to what the public is going to be able to do
with vastly more compute, vastly more attempts,
vastly more resources, vastly more creativity.
Because no team of 20 people,
however good of their jobs can match the internet.
That's the kind of thing, ever.
So I think it's a good idea.
I think that it's not a complete solution
and never will be, right?
And the danger is people treat it as one.
You have to ask yourself like,
what is it competing against?
Is this going to be one of the top, however many?
People who run this thing,
like you want to have like three viable organizations,
you might, you know,
you don't necessarily need 30,
that's probably not worth it.
Like you want three to five.
So is this person more funny to do that?
Just figuring out better metrics
without necessarily being the one who runs the tests
is also a useful thing.
So if I had to bottom line all that
and summarize what I think your worldview is,
you know, as a sort of elder recommender
for this AI safety focused grant-making process.
I think I would say,
I think I would summarize it as
there need to be a few
of these independent safety organizations.
They seem to be either just started
or kind of getting started now.
So at least a few of those,
one exists in, you know,
a couple others are, you know,
either just getting started
or soon to be started, whatever.
So there's kind of,
that seems good because we need to bring
that small, you know, group into existence
in the first place, you have to have them.
Second, on the policy side,
I guess I would summarize you as saying,
seems like it really matters,
but really hard to predict
who will have any impact
and what kind of impact any effort will have.
And so for me, I sort of maybe cash that out
to like probably worth continuing
to support the organizations that are like established enough
that they already have credibility
because credibility or like, you know,
that bloom and thaw might give a shit what they think
is like probably the thing that matters.
And to the degree they already have that
like double down on it.
And then everything else seems like it goes
into good PIs that can drive a research agenda
and have something that they want to do.
And in that category, it's like,
don't even really worry too much
about the exact details of the plan,
but just look for people who have the originality of thought
to be doing something a bit different perhaps
and the sort of demonstrated capability
to actually advance a research agenda.
This is a lot different things there, right?
For the PIs, I would say, you know,
I'm not looking for like exactly the right approach,
but I am looking for assume alignment is hard.
This is the approach except that alignment is hard
and do something that makes progress, real progress
if alignment is in fact very hard.
These people show an appropriate caution
towards the might advance capabilities
towards like maybe I don't want to publish my results
if I find a result that would be harmful
to publish question marks like that.
You know, am I thinking about this problem
as the right safety mindset, with the right paranoia,
with the right like appreciation of the fact
that I'm up against impossible odds?
And if the answer is yes, and I think I have the talent,
then I'm excited even if I'm somewhat skeptical
of the specific thing they intend to try
in terms of like whether it will work, right?
Because like I think that all the most promising things
are not that likely individually to work
and it's gonna be hard for me to evaluate
the relative value.
In terms of the lobbying organizations,
I don't think it's crazy to start a new group at this point.
I do think you want to look for something extraordinary
if somebody is like, why are they forming a new group now?
Why does that make sense?
But yeah, what I'm looking for is a focus
on the policies that actually matter
and on a coordination amongst them
and on like a focus on actually making a difference.
Like so much of like most of politics, right?
There's not about AIs about politics in general
is about raising money from donors
and sending signals of your royalties
and coming up with your status
and raising awareness and other bullshit.
Like most like all sides, right?
Like you've got to focus on people who are writing bills,
people who are lobbying directly for bills,
people who are trying to influence the exact right people
in the exact right ways.
Like have a concrete direct theory of change
who like either understand DC
or have connections with people
who can help them understand DC.
But I don't think we know
that we are in like the only critical window.
We're going to need more organizations than we have.
We're going to need far more people working on it
than we already have.
I don't want to make the mistake of not chips or down
the people who have like nominally established some amount
of like formal credibility or authority
now get all the resources
and get to boss everybody around and do whatever they want.
I think that's like a common failure mode.
I don't want to fall into it.
Evaluation organizations, I want to ask myself,
are these the right people to be doing this particular thing?
They show promise in doing the thing
what they bring to the table
the other organizations don't bring to the table.
I want to see different
or I want to see something unique.
And you have to convince me
that like you're capable of pulling this off
which includes convincing people
that actually buy your services and use your services.
Where do you put mechanistic interpretability in there?
And that that could be,
that seems to be part of what some of the like evals orgs
are also kind of including that in their plan.
And then obviously, you know,
different research groups
can approach that from any number of ways.
My technical view of it is that it's more distinct
from evaluations than that suggests
but that it's a good idea.
Like it should be, you know,
mechanistic interpretability is like Western civilization.
It's a good idea.
Yeah, you should try to in fact,
figure out how these things work.
You do have to be aware
that you are advancing capabilities potentially
when you do it.
You have to think carefully about, you know,
if you find the wrong thing,
I would ask before I funded an interpretability organization,
are you capable of going, oh, yikes,
that's a dangerous thing to learn.
I might not want to rush out to tell the world about that.
I might want to think carefully about who to tell
and who not to tell.
But not necessarily don't really sympathy.
Like you have to like process information carefully
and not just rush.
You don't want a culture of, you know,
everything I ever find is going to be automatically
just shared with the world for that reason.
When you work on mechanistic interpretability.
But I do think on general,
it's a very positive thing to work on.
I do think that it's a thing
that like holds a lot of promise to help us
in various ways.
And it could lead somewhere
where we start sharing all problems with it,
potentially in theory.
It's just a very hard problem that requires, you know,
a lot of work and a lot of compute
and it's not going to be fast
and it's not going to be simple.
And we want a lot of people who are going in parallel.
So I certainly, you know,
intend to assist with some amount of that.
Cool. Well, believe it or not,
we did not get to everything even on my outline,
let alone everything that you have covered
on your blog,
which has been, you know,
probably 10 times as many topics.
So folks will have to get the written version.
This is Vy Matryoz.
Thank you for being part of the Cognitive Revolution.
All right, bye.
Omniki uses generative AI
to enable you to launch hundreds of thousands
of ad iterations that actually work.
Customized across all platforms with a click of a button.
I believe in Omniki so much that I invested in it
and I recommend you use it too.
Use CogGrev to get a 10% discount.
