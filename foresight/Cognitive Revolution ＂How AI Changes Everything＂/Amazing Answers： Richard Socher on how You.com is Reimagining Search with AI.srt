1
00:00:00,000 --> 00:00:05,920
early 2022, or like we had apps that would write code for you within the search results,

2
00:00:05,920 --> 00:00:10,240
through the apps that write essays for you within the search results. But whenever we

3
00:00:10,240 --> 00:00:15,600
innovated and changed the default Google experience too much, we had just like the vast

4
00:00:15,600 --> 00:00:20,240
majority of our users say, I'm so used to Google, I don't want another way of finding answers.

5
00:00:20,800 --> 00:00:26,160
And so we kept getting pulled back to this need. And so the most amazing surprise was when

6
00:00:26,240 --> 00:00:30,800
ChachiP came out, all of a sudden people got it. And it was like, wait a minute,

7
00:00:30,800 --> 00:00:36,000
it could just be like pure text. And we're like, been trying to sort of slowly get there, but

8
00:00:36,000 --> 00:00:40,240
we had to make a bigger job. The way I think about the different modes is like the default

9
00:00:40,240 --> 00:00:44,560
smart mode is kind of like, if you had an assistant, and you just ask them to do a quick

10
00:00:44,560 --> 00:00:49,760
a search, and in like two or three minutes, give you an answer that. And then genius mode,

11
00:00:49,760 --> 00:00:54,480
you go and so you want to ask your assistant for a question that they have to be able to program,

12
00:00:54,560 --> 00:00:58,960
they have to search the web, and then they need to be mathematically applying to answer that

13
00:00:58,960 --> 00:01:03,840
question. I mean, as a kid, I also enjoyed watching Terminator. It's like a cool action movie,

14
00:01:03,840 --> 00:01:10,000
but it's just taken over so much of the AI narrative. And it's actually like actively

15
00:01:10,000 --> 00:01:14,720
hurting, especially European Union. Hello, and welcome to the Cognitive Revolution,

16
00:01:14,720 --> 00:01:19,120
where we interview visionary researchers, entrepreneurs and builders working on the

17
00:01:19,120 --> 00:01:24,400
frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas,

18
00:01:24,400 --> 00:01:28,240
and together we'll build a picture of how AI technology will transform work,

19
00:01:28,800 --> 00:01:34,720
life, and society in the coming years. I'm Nathan LaBenz, joined by my cohost, Eric Tornberg.

20
00:01:35,280 --> 00:01:39,920
Hello, and welcome back to the Cognitive Revolution. Today, I am thrilled to welcome

21
00:01:39,920 --> 00:01:45,040
Richard Socher, a pioneer of deep learning for natural language processing, formerly chief

22
00:01:45,040 --> 00:01:50,960
scientist at Salesforce, and today, founder and CEO of u.com, a company that was first

23
00:01:50,960 --> 00:01:56,000
introduced to the public as a new kind of search engine, but which now describes itself as an AI

24
00:01:56,000 --> 00:02:03,760
assistant that makes you more productive, creative, and extraordinary. Richard has deep history in

25
00:02:03,760 --> 00:02:08,800
deep learning. He was among the very first to recognize the potential of neural networks

26
00:02:08,800 --> 00:02:14,240
in the natural language processing domain, and his work has helped shape the field as we know it

27
00:02:14,240 --> 00:02:19,760
over the last decade. In this conversation, Richard takes us on a brief journey through his own

28
00:02:19,760 --> 00:02:25,040
intellectual history and reflects on how the field of AI has evolved in both expected and

29
00:02:25,040 --> 00:02:31,200
surprising ways. Before we dive deep into the u.com product itself, covering the historical

30
00:02:31,200 --> 00:02:35,440
challenge that they faced when trying to compete with Google and how the rise of the AI chatbot

31
00:02:35,440 --> 00:02:41,520
paradigm has broadened the space of possibility for search and discovery products. We also look

32
00:02:41,520 --> 00:02:47,120
at u.com's various modes with particular emphasis on the genius mode and above all, for me, the

33
00:02:47,120 --> 00:02:52,720
research mode, which delivers amazingly helpful and thorough report style answers, even on some

34
00:02:52,720 --> 00:02:58,960
remarkably complex topics. We also briefly discussed the future of AI business models as well,

35
00:02:58,960 --> 00:03:04,320
including the obvious subscription and my pet theory about the AI bundle. Along the way, we touch

36
00:03:04,320 --> 00:03:09,280
on a number of important topics, too. The limits to AI systems' reasoning ability and the prospects

37
00:03:09,280 --> 00:03:14,400
for the improvement that would be needed for reliable autonomy, the potential for AI to transform

38
00:03:14,400 --> 00:03:20,160
medicine and scientific research, Richard's case for general optimism, even though he does expect

39
00:03:20,160 --> 00:03:25,600
AI to drive major disruption, why he's not worried about so-called emergent capabilities,

40
00:03:25,600 --> 00:03:30,960
but does take the risk of intentional harmful misuse very seriously, and lots more little

41
00:03:30,960 --> 00:03:36,960
topics along the way as well. Richard is a leading thinker in the AI space, and his perspective

42
00:03:36,960 --> 00:03:41,600
is essential for anyone who wants to understand where this technology is going and what it means

43
00:03:41,600 --> 00:03:48,000
for the future of humanity. And in all seriousness, I really do recommend u.com. It has absolutely

44
00:03:48,000 --> 00:03:53,200
joined the ranks of the AI tools that I use multiple times each week. And particularly,

45
00:03:53,200 --> 00:04:00,000
when I want a comprehensive, multi-page report style answer, I find that u.com's research mode

46
00:04:00,000 --> 00:04:06,160
is often the single best tool available today. As always, if you're finding value in the show,

47
00:04:06,160 --> 00:04:09,680
we would appreciate it if you'd share it with friends or post a review to Apple podcasts

48
00:04:09,680 --> 00:04:14,960
or Spotify, or just leave a comment on YouTube. Now, without further ado, I hope you enjoyed

49
00:04:14,960 --> 00:04:20,160
this conversation with Richard Sosher of u.com. Well, let's do it. I think this is going to be

50
00:04:20,160 --> 00:04:24,000
a lot of fun. I'm looking forward to your point of view on a bunch of very interesting topics.

51
00:04:24,640 --> 00:04:29,360
Richard Sosher, founder and CEO of u.com, welcome to the Cognitive Revolution.

52
00:04:30,240 --> 00:04:36,480
Thanks for having me. I am very excited to have you. You are at the intersection of so

53
00:04:36,480 --> 00:04:41,120
many interesting things. I sometimes have been describing myself recently as the forest gump of

54
00:04:41,120 --> 00:04:46,480
of AI because I've just kind of very unstrategically made my way through the last few years and yet

55
00:04:46,480 --> 00:04:51,200
found myself in some very interesting places. I don't know how you think about your own trajectory,

56
00:04:51,200 --> 00:04:56,560
but you are kind of an OG in the realm of deep learning and have founded this very interesting

57
00:04:56,560 --> 00:05:02,320
company and have a really awesome product, which we'll get into in more detail. And I'm interested

58
00:05:02,320 --> 00:05:07,680
to hear about all that and your kind of philosophy and expectations for the future. So we've got a

59
00:05:07,680 --> 00:05:12,240
lot of ground to cover. Maybe for starters, you want to kind of give us, and I usually even ask

60
00:05:12,240 --> 00:05:16,160
these biographical questions, because these days it's like a lot of the same answers. People are

61
00:05:16,160 --> 00:05:20,960
like, oh, when I saw GPT-3, I thought this is going to be a big deal that I got involved. But you

62
00:05:22,000 --> 00:05:25,120
were there at the beginning, man. So maybe you want to just give us a quick history of

63
00:05:25,120 --> 00:05:28,880
your own role in the history of deep learning and how you've kind of come to the present.

64
00:05:29,760 --> 00:05:35,760
I started with AI actually in 2003, when I started studying linguistic computer science,

65
00:05:35,760 --> 00:05:40,960
or natural language processing back in Germany. And at the time, I was like, this is really

66
00:05:40,960 --> 00:05:45,680
interesting. I love languages, I love math, I love computers, you know, so if computers are

67
00:05:45,680 --> 00:05:50,800
where languages and math can meet in some useful functional ways, I thought. And there's very much

68
00:05:50,800 --> 00:05:57,040
sort of a small niche subject within computer science. And I was really excited. At the time,

69
00:05:57,040 --> 00:06:02,880
there wasn't quite enough math for me in an LP. And I felt like we're just getting stuck in some

70
00:06:02,880 --> 00:06:09,600
of the legalistic special cases. And I loved the form of semantic set theory and then algebraic

71
00:06:09,600 --> 00:06:16,240
foundations. And so I moved eventually into computer vision during my master's. And there,

72
00:06:16,240 --> 00:06:21,840
I also, in SaarbrÃ¼cken, at the Max Klein Institute there in the university, found statistical

73
00:06:21,840 --> 00:06:27,200
learning and pattern recognition. And I fell in love with that. I was like, clearly, you can really

74
00:06:27,200 --> 00:06:32,800
understand patterns, any kind of pattern really well, you could solve all these different kinds

75
00:06:32,800 --> 00:06:39,360
of problems. And so I ended up doing my PhD at Stanford. In the beginning of Stanford, I started

76
00:06:39,360 --> 00:06:45,920
trying to really contribute to the field rather than just learning about it. I basically found that

77
00:06:46,000 --> 00:06:52,480
even the top NLP people, they write their papers mostly about these beautiful models,

78
00:06:52,480 --> 00:06:57,040
like conditional random fields, late in university, other patient types of models.

79
00:06:57,040 --> 00:07:02,160
But then most of the coding happens when they actually do feature engineer, right? They say,

80
00:07:02,160 --> 00:07:06,800
oh, well, I wanted you to be entity recognition at a feature of like, this is a capitalized word,

81
00:07:06,800 --> 00:07:13,600
and this is all caps word, or this is a word that has like, is one of the items in this list.

82
00:07:13,600 --> 00:07:18,400
And this list includes, you know, city needs, we already know. And I'm like, man, this field is

83
00:07:18,400 --> 00:07:23,840
very hand engineered. It's very like, graduate student ascent to get better. And then at the

84
00:07:23,840 --> 00:07:29,680
time I was very 40, because Andrew A. got into deep learning on the computer vision side. He's

85
00:07:29,680 --> 00:07:34,560
like, well, images are pixels, and it's a fixed number of pixels. So we can feed them into a

86
00:07:34,560 --> 00:07:39,360
neural net or at the time, you know, variants, models, restricted development machines. And I

87
00:07:39,360 --> 00:07:44,720
was like, wow, maybe we can use ideas from that for natural language processing. And there was

88
00:07:44,720 --> 00:07:51,840
maybe like one or two relevant papers all from a number there, and Jason Weston, and, and a few

89
00:07:51,840 --> 00:07:57,680
like one or two others. But no one really enjoyed that approach, no natural language processing,

90
00:07:57,680 --> 00:08:02,640
paid any attention to it. But I thought, silly, that has to be the future. I want to give the data

91
00:08:02,720 --> 00:08:09,520
and I want to get an output. And so in 2010, I started publishing my first neural net paper,

92
00:08:09,520 --> 00:08:15,600
worked on my computer vision before and saw some of the power of ImageNet also back to and really

93
00:08:15,600 --> 00:08:21,200
started running with it. Got a lot of rejections all throughout. But, but at some point, I sunk

94
00:08:21,200 --> 00:08:25,680
my teeth into it. And I just like, I loved it. And I thought this is the future. Despite all the

95
00:08:25,680 --> 00:08:31,200
rejections, I kept going at it. And then after the PhD was over, there's sort of starting to be

96
00:08:31,200 --> 00:08:37,040
more interest in deep learning and neural nets for NLP. But still, no one in the world was teaching

97
00:08:37,040 --> 00:08:41,680
that as like the official right way of doing NLP. So I started teaching at Stanford also,

98
00:08:42,640 --> 00:08:48,080
first as a busy lecturer and then as a professor, started, you know, being a fortune and lots of

99
00:08:48,080 --> 00:08:52,960
very smart students back then, really like the hiding place founders invested in their first

100
00:08:52,960 --> 00:08:58,720
round. And then, you know, also wanted to bring these neural nets into the world, started a

101
00:08:58,800 --> 00:09:03,200
menomind, my first startup to do that, to build a general purpose platform between neural nets

102
00:09:03,200 --> 00:09:08,560
very easily, both revision and NLP, got acquired by Salesforce, became a chief scientist there and

103
00:09:08,560 --> 00:09:14,880
EDP eventually. And in Salesforce, we had my probably last and biggest rejection was on inventing

104
00:09:14,880 --> 00:09:20,000
front engineering in 2018. And we're so excited about it, because there's the culmination personally

105
00:09:20,000 --> 00:09:25,840
for me also of this decade long dream I have, building a single neural net for all of NLP.

106
00:09:25,840 --> 00:09:32,560
And the idea was, you know, at the time, every AI model was built for one task, you will have wanted

107
00:09:32,560 --> 00:09:36,480
your sentiment analysis, I built a sentiment analysis model, you wanted a translation, I built

108
00:09:36,480 --> 00:09:40,320
a translation model, and they're all different. We're like, what if we could just build a single

109
00:09:40,320 --> 00:09:45,520
model? And you just ask it a question, what is the sentiment? What is the summary of the sentence?

110
00:09:45,520 --> 00:09:50,880
Who is the president in this paragraph? And that was kind of for us, I thought like,

111
00:09:50,880 --> 00:09:54,880
the most exciting thing you possibly could be doing at just get this tech talk about it

112
00:09:54,880 --> 00:10:00,560
came out last week. And but it was exciting. But it did inspire a couple of other folks.

113
00:10:00,560 --> 00:10:04,400
And like, when opening, I was, you know, publishing your papers, that should be true.

114
00:10:04,400 --> 00:10:09,520
And three, they cited that paper saying like, look, they were able to have a single model

115
00:10:09,520 --> 00:10:15,120
for all of NLP, if you just ask them these questions. And, you know, that's now prompt and

116
00:10:15,120 --> 00:10:19,680
the rest is kind of more well known history. That is an amazing history. And it definitely,

117
00:10:19,680 --> 00:10:26,320
I don't know how, you know, modest you want to be versus taking credit for foresight. But

118
00:10:26,320 --> 00:10:32,080
certainly, the idea that there could be one model to solve all these, you know, tasks was not

119
00:10:32,080 --> 00:10:37,680
obvious to people. And boy, we still see this, the flaws in the peer review process are still on

120
00:10:37,680 --> 00:10:42,480
prominent display these days. Most recently, I noticed this with the Mamba paper, which I was

121
00:10:43,360 --> 00:10:49,280
a very interested reader of. And then went over to the open reviews site and was blown away by how

122
00:10:49,280 --> 00:10:54,880
negative some of the reviews were like a confident reject was given. So that was kind of, you know,

123
00:10:55,680 --> 00:10:58,640
just a good reminder that, yeah, this is still an unsolved problem.

124
00:10:59,440 --> 00:11:07,200
What would you say has surprised you most from like the big picture since you,

125
00:11:08,080 --> 00:11:12,320
and you know, it hasn't been that many years, right? But since you kind of had that notion of

126
00:11:12,320 --> 00:11:19,360
this generalist NLP model, fast forward, now we have, you know, GPT four and possibly Q star or

127
00:11:19,360 --> 00:11:23,520
something like that in the works, you know, is this the trajectory that you thought we'd be on?

128
00:11:23,520 --> 00:11:26,800
Or how has it deviated from what you imagined back then?

129
00:11:26,800 --> 00:11:33,040
It's very much aligned with what I hoped the field could get to. And now it's almost like,

130
00:11:33,040 --> 00:11:37,360
it's like obvious, right? Like no one no one questions this anymore, we've had all these

131
00:11:37,360 --> 00:11:43,280
breakthroughs. And I think the biggest surprise was maybe more on the application side of things

132
00:11:43,280 --> 00:11:48,240
and that for us, you know, we've been playing around with large language models at u.com and

133
00:11:48,240 --> 00:11:54,800
infuse them into search results earlier, like early 2022, already, like we had apps that would

134
00:11:54,800 --> 00:11:59,200
write code for you within the search results, through the apps that write essays for you within

135
00:11:59,200 --> 00:12:05,760
the search results. But whenever we innovate it and change the default Google experience too much,

136
00:12:05,760 --> 00:12:09,360
we had just like the vast majority of our users say, I'm so used to Google,

137
00:12:10,160 --> 00:12:15,520
I don't want another way of finding answers. And so we kept getting pulled back to this need.

138
00:12:15,520 --> 00:12:19,600
And there was kind of annoying. And so the most amazing surprise was when

139
00:12:19,600 --> 00:12:24,240
Chatchity came out, all of a sudden, people got it. And it was like, wait a minute,

140
00:12:24,240 --> 00:12:29,760
it could just be like pure text. And we're like, you know, we've been trying to sort of slowly

141
00:12:29,760 --> 00:12:34,640
get there, but we had to make a bigger job. And that was incredible. That unlocked a lot

142
00:12:34,640 --> 00:12:39,600
of people realizing we handle links isn't the best way to get an answer. An actual answer

143
00:12:39,600 --> 00:12:41,920
is the best way to get an answer. And that's the text.

144
00:12:43,040 --> 00:12:48,320
So let me give you a couple of my experiences on u.com recently, and then you can kind of tell me,

145
00:12:48,320 --> 00:12:53,680
you know, where you are the overall story. And then I really want to kind of unpack the

146
00:12:54,560 --> 00:12:58,400
kind of use the product as it exists today and the roadmap and everything you're working on

147
00:12:58,400 --> 00:13:01,840
as a way to kind of explore a bunch of different aspects of where all this

148
00:13:01,840 --> 00:13:06,720
is going, you know, and I think that's really the mission of this show is to kind of help people

149
00:13:06,720 --> 00:13:10,640
see around the corner and starting with me, helping me develop my own worldview.

150
00:13:10,640 --> 00:13:15,120
But I've been really impressed with the product recently. You know, listeners will know that

151
00:13:15,120 --> 00:13:18,800
I've been a big fan of perplexity. We've had Arvin on the show a couple of times.

152
00:13:19,440 --> 00:13:24,160
And I think they do a great job and, you know, we're made a fan. But I have found

153
00:13:25,120 --> 00:13:30,560
distinctive value in at least two modes on u.com recently. One is the

154
00:13:31,200 --> 00:13:36,160
research mode, and the other is the genius mode. Those to me have stood out as the most

155
00:13:36,160 --> 00:13:43,520
differentiated. For research mode, I recently took it like a 200 word question that was all about

156
00:13:44,240 --> 00:13:49,680
mixture of experts architectures. And, you know, kind of is there curriculum learning,

157
00:13:49,680 --> 00:13:53,920
you know, stuff happening here? How, you know, how do people think about sort of

158
00:13:53,920 --> 00:13:58,480
the tradeoffs between like how many experts should we have and how big should they be and

159
00:13:58,480 --> 00:14:02,160
how many should we activate at any given time? Are there any like scaling laws or whatever,

160
00:14:02,160 --> 00:14:04,800
you know, designed for that sort of thing? Just every basically every question I could

161
00:14:04,800 --> 00:14:10,880
think of about mixture of experts, I took it all in one go. And it was really impressive to see it

162
00:14:11,520 --> 00:14:17,680
kind of break that down and go through multiple steps of searching and analysis and,

163
00:14:18,560 --> 00:14:22,400
you know, really implementing kind of like, you know, kind of a classic agent, what is at

164
00:14:22,400 --> 00:14:27,520
this point, you know, a six month classic agent setup, but applying it to that research question

165
00:14:27,520 --> 00:14:32,960
and just going, you know, down the line, really quite valuable results. And it definitely is

166
00:14:32,960 --> 00:14:37,280
something that I will come back to and have already, you know, found myself kind of being like,

167
00:14:37,280 --> 00:14:42,800
I think this is a good one for u.com research mode. Genius mode is a little bit different and more

168
00:14:42,800 --> 00:14:47,200
kind of analytical. I'd be interested to hear a little bit more about how you think about the

169
00:14:47,200 --> 00:14:53,520
differences. Because I did, I then tried one that was a big Fermi calculation exercise,

170
00:14:53,520 --> 00:14:57,280
where my questions were like, what are the different data sets that exist in today's world?

171
00:14:57,280 --> 00:15:00,880
How big are they? How do they compare to each other? How do they compare to

172
00:15:01,520 --> 00:15:07,440
the training data size for GPT four? You know, how do they compare to available compute? Like,

173
00:15:07,440 --> 00:15:10,800
because I have this, I have a big question, which is kind of one of the ones I want to get to

174
00:15:10,800 --> 00:15:16,000
toward the end to around like, to what degree is ML research poised to start to be kind of semi

175
00:15:16,000 --> 00:15:21,120
automated. And so I'm trying to try to rent my arms around that with these furry calculations.

176
00:15:21,120 --> 00:15:27,840
So genius mode was really the best way to approach that. And anyway, I would definitely

177
00:15:27,840 --> 00:15:34,880
encourage people to bring multi part complicated questions to both research mode and genius mode.

178
00:15:34,880 --> 00:15:39,200
And I think you'll be impressed with the results. And I would say that, you know, even

179
00:15:40,240 --> 00:15:44,080
with, you know, the expectation that folks who listen to this show have tried, you know, other

180
00:15:44,160 --> 00:15:51,360
leading AI products. So that's kind of my unpaid endorsement, very sincere. And I'd love to hear,

181
00:15:51,360 --> 00:15:54,560
you know, a little bit more about how you think about those different modes, how they work,

182
00:15:54,560 --> 00:15:58,880
and just kind of big picture, like where we are in the you.com product journey long term.

183
00:15:59,440 --> 00:16:02,720
Hey, we'll continue our interview in a moment after a word from our sponsors.

184
00:16:03,680 --> 00:16:07,760
The Brave Search API brings affordable developer access to the Brave Search Index,

185
00:16:07,760 --> 00:16:12,640
an independent index of the web with over 20 billion web pages. So what makes the Brave Search

186
00:16:12,640 --> 00:16:18,960
Index stand out? One, it's entirely independent and built from scratch. That means no big tech

187
00:16:18,960 --> 00:16:25,840
biases or extortionate prices. Two, it's built on real page visits from actual humans, collected

188
00:16:25,840 --> 00:16:31,760
anonymously, of course, which filters out tons of junk data. And three, the index is refreshed with

189
00:16:31,760 --> 00:16:37,680
tens of millions of pages daily. So it always has accurate up to date information. The Brave Search

190
00:16:37,680 --> 00:16:42,640
API can be used to assemble a data set to train your AI models and help with retrieval

191
00:16:42,640 --> 00:16:47,520
augmentation at the time of inference, all while remaining affordable with developer first pricing.

192
00:16:48,560 --> 00:16:53,360
Integrating the Brave Search API into your workflow translates to more ethical data sourcing

193
00:16:53,360 --> 00:16:59,360
and more human representative data sets. Try the Brave Search API for free for up to 2000

194
00:16:59,360 --> 00:17:11,040
queries per month at brave.com. Yeah, these are a great question. I think it shows you kind of

195
00:17:11,040 --> 00:17:19,200
how sophisticated the space has gotten in the last year. Around this time, last year, we were the

196
00:17:19,200 --> 00:17:27,520
only search engine with a web connected LN and millions of users. And now that idea has been

197
00:17:27,520 --> 00:17:33,520
copied so many times, including as mentioned by Plexi. And so I think what you have to differentiate

198
00:17:33,520 --> 00:17:37,600
kind of the different modes, and I think the modes kind of show how sophisticated that the

199
00:17:37,600 --> 00:17:43,280
space has gotten and how hard it is to still differentiate on better technology versus just

200
00:17:43,280 --> 00:17:49,040
you know, designing the market and marketing and things like that. And so we actually did a comparison

201
00:17:49,040 --> 00:17:56,000
to Plexi with 500 real user queries. And we asked which answer do you prefer? And it came out to be

202
00:17:56,000 --> 00:18:02,800
that 50% of the cases users prefer the U.com answer and they prefer the Plexi answer and 30%

203
00:18:03,360 --> 00:18:08,960
they don't see a difference into answers for our default, we call it the smart mode. That's kind

204
00:18:08,960 --> 00:18:15,360
of the default. And just to give you a sense of what that looks like. So here's an example of what

205
00:18:15,360 --> 00:18:19,600
the default smart mode looks like. You know, there's some doping case that happened and

206
00:18:19,600 --> 00:18:24,880
you can see lots of careful citations. And then when you actually look into these citations,

207
00:18:24,880 --> 00:18:30,000
they actually are articles from literally yesterday or they could be, you know, from today if something

208
00:18:30,000 --> 00:18:34,160
came out today. So that's kind of the default smart mode, you get a quick factual answer.

209
00:18:34,160 --> 00:18:39,600
But then we thought, well, what if you have a pretty complex question like math, physics,

210
00:18:39,600 --> 00:18:45,360
chemistry, science, or like complex numbers. So here is a genius mode question, it kind of gives

211
00:18:45,360 --> 00:18:49,840
you a sense of what it does. And it doesn't mention like what you say, which is there's an

212
00:18:49,840 --> 00:18:54,960
important LM that orchestrates multiple other LMs to actually do the right thing right. So

213
00:18:54,960 --> 00:18:58,960
the question here is find the current population of the in the United States, then it's lots of

214
00:18:58,960 --> 00:19:06,880
population from 233 to 10, 100, and then assuming a 2% growth rate. And then it will go on the

215
00:19:06,880 --> 00:19:12,880
internet, it'll find the numbers, and then realize like, well, I got to now visualize those numbers,

216
00:19:12,880 --> 00:19:19,280
now that I have any, so it will code up in Python, what this could look like, execute the code,

217
00:19:19,280 --> 00:19:26,000
and then gives you this answer, and visualizes it in a nice plot. And so that I'm still sometimes

218
00:19:26,000 --> 00:19:30,160
amazed, I try and I push it, and you know, sometimes it fails. And sometimes it fails

219
00:19:30,160 --> 00:19:33,440
because it tries to load the library that has security issue. And then it's like, okay, I'm

220
00:19:33,440 --> 00:19:37,600
going to try to rewrite it without this library, but it's going to be longer and messier code.

221
00:19:37,600 --> 00:19:43,520
And like, it's just incredible how hard it can try and what it can do. And then the third mode,

222
00:19:43,520 --> 00:19:49,040
like you said, the research mode, it will go into a lot of detail, it will not just look up

223
00:19:49,280 --> 00:19:53,200
all the stuff we have in our index already, like news and things like that, but it will go on the

224
00:19:53,200 --> 00:19:59,600
web and find your website, so the multiple different searches on the web, combine all of that,

225
00:19:59,600 --> 00:20:04,400
and then give you these beautiful research reports. This one is like, seeing a background,

226
00:20:04,400 --> 00:20:09,440
actually, any consequences of the telecommuting work. Now it's like, history, you have to write an

227
00:20:09,440 --> 00:20:14,960
essay or something. And it's just like, writes you just perfect, like, beautiful essay, each

228
00:20:14,960 --> 00:20:20,560
sentence has one or two citations from different sources, and you can verify all of them. And

229
00:20:20,560 --> 00:20:25,520
one thing we found this actually also is like, you have to like, just the citation lot is a

230
00:20:25,520 --> 00:20:31,360
non-trivial aspect of building this all out. Because you have to, we actually found that some

231
00:20:31,360 --> 00:20:37,360
of our competitors just randomly add numbers and citations to sentences, and you click on it,

232
00:20:37,920 --> 00:20:43,520
and it doesn't even mention that back anymore. Which I think it really undermines the space of

233
00:20:43,520 --> 00:20:50,720
chatbots for search. So citation accuracy is one of the many sub-AI systems that you need to do

234
00:20:50,720 --> 00:20:54,560
correctly here. And then, you know, they're just like crazy things, like create a table,

235
00:20:54,560 --> 00:20:59,120
some nice cancelling headphones that are not expensive, and just like, put this table together,

236
00:20:59,120 --> 00:21:03,680
pull some images, give some pros and cons of each and the price. I think sometimes, to me,

237
00:21:03,680 --> 00:21:09,760
is how well and general the system is able to answer these questions. And it shows you how

238
00:21:09,760 --> 00:21:14,400
complex the space is gotten and how much you have to do now to still differentiate on the

239
00:21:14,400 --> 00:21:19,680
technology. This is one of my mantras at Waymark. I always say the water line is rising quickly.

240
00:21:19,680 --> 00:21:25,840
So we, you know, we better keep climbing the capabilities ladder ourselves. The four examples

241
00:21:25,840 --> 00:21:30,080
that we saw there, one was the kind of default smart mode. The second was genius. Is that right?

242
00:21:30,080 --> 00:21:34,880
The one that showed the code example. And then the last two were research. Yeah. What more can you

243
00:21:35,520 --> 00:21:40,160
tell us about kind of how those work? Like I'm interested in, and by the way, like the

244
00:21:40,160 --> 00:21:44,160
audience of the cognitive revolution is interested in the details, the weeds, the nuggets, you know,

245
00:21:44,160 --> 00:21:48,240
all that stuff. So you can go as deep as you're, you know, willing to share. I'm interested in

246
00:21:48,240 --> 00:21:51,840
all aspects, you know, prompting, I'm sure, obviously, is going to be different. Scaffolding

247
00:21:51,840 --> 00:21:55,040
is going to be different. Maybe even the models are different. I'm also really interested in,

248
00:21:55,040 --> 00:22:00,240
like, what are you using GPT-4 that you've got your own in-house trained ones as well. So

249
00:22:00,240 --> 00:22:03,520
just all those considerations, any interesting nuggets were all ears.

250
00:22:04,400 --> 00:22:09,280
Yeah, I'm going to try to balance a little bit the not telling the competition exactly how it's

251
00:22:09,280 --> 00:22:15,840
all done, but it'd be interesting to your ears here. So at a high level, there are two major

252
00:22:15,840 --> 00:22:22,320
stacks. There's a search stack and a chat stack. The search stack, we actually had to build an

253
00:22:22,320 --> 00:22:29,760
entire index ourselves for the web because being super expensive, not as high quality, Google

254
00:22:29,760 --> 00:22:34,400
is very hard to access. You have to have special agreements or, you know, some people kind of

255
00:22:34,400 --> 00:22:40,480
steal slash bootleg slash leave some surveyed the eyes to use Google results in like a somewhat

256
00:22:40,480 --> 00:22:46,400
sketchy legal gray area, which we don't want to do. And so we, we basically ended up having to

257
00:22:46,400 --> 00:22:51,680
build our own index. And that's hard. And there's still, you know, a lot of complexities behind

258
00:22:51,680 --> 00:22:57,920
that. But what do we, the main difference of this new index is that it was built with LNs in mind.

259
00:22:58,480 --> 00:23:05,840
The previous two indices of Google and Bing were built with people consuming 10 blue links in mind.

260
00:23:06,720 --> 00:23:11,600
And what that means is for each URL, you get a very short snippet, which makes sense, right,

261
00:23:11,600 --> 00:23:15,760
for end users. But an LN could read all these other snippets, they can be very long,

262
00:23:16,560 --> 00:23:20,800
and then extract the right answers from that, and then just give you that right answer as the

263
00:23:20,800 --> 00:23:26,160
user. And so what was surprising is actually when we benchmark this, our API ended up being more

264
00:23:26,160 --> 00:23:31,920
accurate than Google or me and go to API.com. And like, I'll, I'll see you on the screen here for

265
00:23:31,920 --> 00:23:36,480
a second again. But like, it's surprising that, which are a lot of people that you could actually

266
00:23:36,480 --> 00:23:41,040
be more accurate in Google or Bing at all. But it is because we're at an inflection point in,

267
00:23:41,040 --> 00:23:45,520
in a, in the eye, and it's a different way to value. It's like, we're almost like

268
00:23:45,520 --> 00:23:50,480
cheating by having these really long snippets. And so you look at the comparison, and it's

269
00:23:50,480 --> 00:23:54,560
actually kind of interesting to look at. And a lot of people have asked like, how do you compare

270
00:23:55,120 --> 00:24:00,320
accuracy in LNs? How can you evaluate this? And so just to give you a sense, here's like, what,

271
00:24:00,320 --> 00:24:04,560
what this looks like, the first a version is just like reasons to smile. And now you can use whatever

272
00:24:04,560 --> 00:24:11,200
LN you want, but you can see into your prompt is very, very long snippets from many different

273
00:24:11,200 --> 00:24:15,520
URLs in a very short amount of time. And then we also have one that just does everything,

274
00:24:15,520 --> 00:24:20,160
like it gives you an LN answer, and it tells you like all of these things. And so how do you

275
00:24:20,160 --> 00:24:25,600
evaluate this is actually, it was an interesting, I think insights insight from our team, which was,

276
00:24:25,600 --> 00:24:31,120
you can take question answering data such as hotpot qa, squat, so the question answering

277
00:24:31,120 --> 00:24:38,080
data set MS, Microsoft, Marco, fresh QA and so on. And these data sets are structured such that you

278
00:24:38,080 --> 00:24:44,320
have a paragraph, you really have a question, and then you have a subset phrase from that paragraph

279
00:24:44,320 --> 00:24:49,920
that is the right answer to that question. And so what we do is, we basically take those data sets

280
00:24:49,920 --> 00:24:55,200
but we throw away all the paragraphs. And then you have to find the right answer. And the paragraphs

281
00:24:55,200 --> 00:25:01,840
have to come from the internet. And so you replace paragraph with a web search engine. And that's how

282
00:25:01,840 --> 00:25:09,440
we evaluate it, the JIT, the big Google and the public APIs, and have outputs on them. So kind

283
00:25:09,440 --> 00:25:14,880
of nerdy, but that's the whole tech stack. And we're we make that now available to every other LN.

284
00:25:14,880 --> 00:25:20,960
So that's the first. And then the second thing is what we now have started calling the LNOS,

285
00:25:20,960 --> 00:25:26,640
the operating system of large language models. And it's a term inspired by Andrey Kapathy.

286
00:25:26,640 --> 00:25:31,360
And it's not like the most perfect metaphor, but I think it captures a lot of the essence, which is

287
00:25:31,360 --> 00:25:39,040
you have now this new staff that operates at a much higher level of abstraction. And the LN is

288
00:25:39,040 --> 00:25:45,600
kind of a CPU. But just like a CPU or a kernel on an operating system, like it's important to

289
00:25:45,600 --> 00:25:52,480
orchestrate everything and to do computation. But if it still needs a hard drive, which is

290
00:25:52,480 --> 00:25:58,960
right, right on your own vector database that's grown up, you have an internet connection, which

291
00:25:58,960 --> 00:26:04,560
is, you know, the internet. And that's what we're providing. You may orchestrate other LNs that

292
00:26:04,560 --> 00:26:09,200
could be considered like the GPU or something. And then you have a bunch of apps that are sitting

293
00:26:09,200 --> 00:26:14,240
on top of that. You have a Python code interpreter, which we see our genius mode, all of that. And so

294
00:26:14,240 --> 00:26:20,160
to summarize all of that in one short term, we call the LNOS. And inside of that, we're now seeing

295
00:26:20,160 --> 00:26:26,720
a lot of our customers are using our APIs and search site. They're kind of going through the same

296
00:26:26,720 --> 00:26:33,360
lessons that we had gone through when we built dot com and made it like having the most accurate

297
00:26:33,360 --> 00:26:38,320
answers out there. And it's actually highly non trivial. A lot of people saying it's just like

298
00:26:38,320 --> 00:26:44,000
an LN wrapper, right? But then, and you even have like open source project that show it.

299
00:26:44,000 --> 00:26:49,920
And then you ask, like, okay, when was Obama born? Where was he born? And then it fails. Why does it

300
00:26:49,920 --> 00:26:55,840
fail? Because when you send where was he born to your search back in is not going to return you

301
00:26:55,840 --> 00:27:00,560
any useful results. Because it doesn't know who he is. Who does he refer to, right? And there's

302
00:27:00,560 --> 00:27:05,440
tons of things like that where, as you have a longer and longer conversation, especially in

303
00:27:05,440 --> 00:27:13,120
smart mode, you refer back to states. You can say like, Oh, what's a big CRM company? And then the

304
00:27:13,120 --> 00:27:17,840
answer inside is Salesforce. And you ask, Oh, what's their stock price? Now she sent what's their

305
00:27:17,840 --> 00:27:22,320
stock price to your search back. And again, it's not going to return anything useful. So you need

306
00:27:22,320 --> 00:27:27,040
to send that you need to go through the entire conversation, and then do what we call query

307
00:27:27,040 --> 00:27:36,320
transformation based on it. And that is just one of 10 examples of making this actually work at scale

308
00:27:36,320 --> 00:27:42,720
millions of times a day for millions of users. Like, it is a lot more complicated to make it

309
00:27:42,720 --> 00:27:47,360
accurate. There are about 10 other such models that if you think about the space and you really

310
00:27:47,360 --> 00:27:52,560
listen and look at like user data, you listen to where it's breaking, you will eventually get to

311
00:27:52,560 --> 00:27:58,080
and we're now like thinking about offering more and more back. So I'm tempted to ask for the other

312
00:27:58,080 --> 00:28:04,160
nine things there. I'll just give you one more, which is like, whether to do a search at all or

313
00:28:04,160 --> 00:28:10,240
not, right? Like, because you asked like, write me a poem about like the beautiful Bay Area and

314
00:28:10,240 --> 00:28:15,120
like a sunset love story or something. Like, you don't need a citation at every line of that poem.

315
00:28:15,120 --> 00:28:22,160
And so it would actually clutter up the prompt to add a bunch of facts about poems and so on.

316
00:28:22,240 --> 00:28:28,240
And the history of Silicon Valley and all of that. And so it's pretty important, but also non trivial

317
00:28:28,240 --> 00:28:33,520
to know whether you should do a search or not. And again, some, some websites just slap search

318
00:28:33,520 --> 00:28:38,960
results on top of everything, even if they're not relevant for having more conversation about

319
00:28:38,960 --> 00:28:45,680
your feelings or something. Did I understand correctly that the kind of big difference is that

320
00:28:45,680 --> 00:28:52,640
the U.com index has more information, like instead of a short SERP, it is a more robust

321
00:28:52,640 --> 00:29:01,360
paragraph. And so independent of the language model that you're using, the richer context is

322
00:29:01,360 --> 00:29:08,080
just better kind of serious enable you in that way, you're kind of decoupling the what information

323
00:29:08,080 --> 00:29:14,880
is found from language model that is doing the analysis. And more information is kind of the

324
00:29:14,880 --> 00:29:20,080
big differentiating factor there. Drive that right. I would be careful and say we have overall

325
00:29:20,080 --> 00:29:26,400
more information. We're focused a little bit more on the main languages that we see. We don't support

326
00:29:26,400 --> 00:29:33,280
some like very rare like Indonesian, African sensual Asian dialects and so on yet, but we

327
00:29:33,280 --> 00:29:39,600
return more information per rare because of these large limits. So, so it's sort of, yes,

328
00:29:39,600 --> 00:29:44,000
yes, there's more information, but you know, I think the long tail Google Prop still has a larger

329
00:29:44,000 --> 00:29:50,320
index. If you look for this like rare, like Indonesian kayaking sites that like rents out

330
00:29:50,320 --> 00:29:55,440
kayaks on this little lake somewhere, like, and it's all like not in English, like we might not

331
00:29:55,440 --> 00:30:01,040
have that website. But when it comes to like Western world news where, you know, we have a lot of

332
00:30:01,040 --> 00:30:07,360
users, then Latin America and so on, then we shine and return much more information per pair.

333
00:30:07,360 --> 00:30:11,040
Hey, we'll continue our interview in a moment after a word from our sponsors.

334
00:30:11,040 --> 00:30:15,040
If you're a startup founder or executive running a growing business, you know that as you scale,

335
00:30:15,040 --> 00:30:19,840
your systems break down and the cracks start to show. If this resonates with you, there are three

336
00:30:19,840 --> 00:30:26,880
numbers you need to know. 36,000, 25 and 1. 36,000. That's the number of businesses which have upgraded

337
00:30:26,880 --> 00:30:31,280
to NetSuite by Oracle. NetSuite is the number one cloud financial system, streamlined accounting,

338
00:30:31,280 --> 00:30:37,920
financial management, inventory, HR and more. 25. NetSuite turns 25 this year. That's 25 years

339
00:30:37,920 --> 00:30:42,880
of helping businesses do more with less, close their books in days, not weeks, and drive down costs.

340
00:30:43,600 --> 00:30:48,160
One, because your business is one of a kind, so you get a customized solution for all your KPIs

341
00:30:48,160 --> 00:30:52,800
in one efficient system with one source of truth. Manage risk, get reliable forecasts,

342
00:30:52,800 --> 00:30:58,320
and improve margins. Everything you need all in one place. Right now, download NetSuite's popular

343
00:30:58,320 --> 00:31:03,040
KPI checklist designed to give you consistently excellent performance, absolutely free and

344
00:31:03,040 --> 00:31:09,120
net suite.com slash cognitive. That's net suite.com slash cognitive to get your own KPI checklist.

345
00:31:09,120 --> 00:31:16,080
Net suite.com slash cognitive. Omniki uses generative AI to enable you to launch hundreds of

346
00:31:16,080 --> 00:31:21,520
thousands of ad iterations that actually work, customized across all platforms with a click

347
00:31:21,520 --> 00:31:26,400
of a button. I believe in Omniki so much that I invested in it and I recommend you use it too.

348
00:31:27,120 --> 00:31:32,560
Use Kogrev to get a 10% discount. I've been struck recently that it seems like,

349
00:31:32,800 --> 00:31:36,880
obviously, search in general has kind of been a monopoly for a long time.

350
00:31:38,400 --> 00:31:43,360
As you noted, the user experience was kind of something people were not necessarily looking

351
00:31:43,360 --> 00:31:50,000
to explore new things on the nature of the index. Of course, they've done millions of

352
00:31:50,000 --> 00:31:55,120
person hours of work on it, but it seems like it's kind of been a pretty consistent paradigm of

353
00:31:55,120 --> 00:31:59,120
crawl around and find everything and suck it up. Now, we're starting to see these interesting,

354
00:31:59,120 --> 00:32:04,240
I don't know if you can share more about how you create your index, but we just had actually a

355
00:32:04,240 --> 00:32:10,640
sponsor brave talking about their index and the way that they are building it through users

356
00:32:10,640 --> 00:32:17,120
actually visiting websites and taking a sort of not just blindly crawling around and following

357
00:32:17,120 --> 00:32:24,720
every link, but what are people actually engaging with online, which struck me as a pretty interesting

358
00:32:24,720 --> 00:32:28,720
and very different twist on it. I want to kind of pull this apart in a couple of different ways,

359
00:32:28,720 --> 00:32:34,240
but is there anything that you would want to share about how you think about building an index

360
00:32:34,240 --> 00:32:42,400
that, aside from just bigger, richer content, is there a different tactic as well that underlies

361
00:32:42,400 --> 00:32:49,840
that? The tactic is more about how we make that work for LLNs better, and I don't think there's

362
00:32:49,840 --> 00:32:54,160
that much differentiation on how it will fall. You have to have a bunch of data that's been

363
00:32:54,160 --> 00:33:00,800
helpful to have run a search engine for several years and get user behavior and knowing what

364
00:33:00,800 --> 00:33:06,800
people actually want to have called and want information for. You can also sound surprisingly

365
00:33:06,800 --> 00:33:11,840
buy a lot of that data in bulk. I have a few questions on the business side or the kind of

366
00:33:11,840 --> 00:33:17,920
bridge the technology and business side. Google obviously has been free and has been ad supported.

367
00:33:18,000 --> 00:33:26,800
It seems like the new generation of AI first LLN enabled search is going more in the direction so

368
00:33:26,800 --> 00:33:33,920
far of a subscription. As far as I've seen in my U.com usage, I haven't seen anything that jumped

369
00:33:33,920 --> 00:33:38,800
out to me as sponsored. Another dimension too is like, I mean, Google has all these tabs at the

370
00:33:38,800 --> 00:33:44,640
top, but it's one bar, right? You kind of put in one thing. With the newer ones, we also are

371
00:33:44,720 --> 00:33:50,560
kind of seeing a little bit more proliferation of modes and settings that you choose up front

372
00:33:50,560 --> 00:33:56,960
with the smart versus genius versus research. I guess on those two dimensions, what is the

373
00:33:56,960 --> 00:34:00,720
future vision? Do you think that this all gets unified? Do you think it ultimately comes back

374
00:34:00,720 --> 00:34:08,800
around to ad supported? Or do you think that these current differences from the past will persist?

375
00:34:09,680 --> 00:34:16,560
Yeah, that's a good question. I think there is facility right now, not a great chat ad offering.

376
00:34:17,600 --> 00:34:24,720
There's a good chance that that will change maybe this year to maybe the dissatisfaction of users,

377
00:34:24,720 --> 00:34:31,040
but the truth is, if you want something to be free, VC money will only last so long. You've got to,

378
00:34:31,040 --> 00:34:37,600
at some point, those companies that offer free service have to survive. If you don't want to

379
00:34:37,600 --> 00:34:43,120
pay for it, then it has to have ads. And so while, and might not be the biggest fan of ads,

380
00:34:43,120 --> 00:34:47,760
like, you have to make a decision, you want to pay for it, and then add free, or do you want to

381
00:34:48,320 --> 00:34:57,040
support it with ads? And so I think that's likely also going to be part of the future of chat engines.

382
00:34:57,040 --> 00:35:02,560
And you already see a little bit of exploration. There's a little bit of a duopoly in search in

383
00:35:02,640 --> 00:35:08,640
the sense that Google has the monopoly on consumer search. And for a long time, Microsoft had the

384
00:35:08,640 --> 00:35:14,080
monopoly on a search API. But then because they're monopoly that is set up, we're just five to 20x

385
00:35:14,080 --> 00:35:19,120
hour prices, and they could do it because they're the only ones in town. So I'm glad there's like

386
00:35:19,120 --> 00:35:23,920
more competition now and more movement in that space. And all the little guys have to scramble

387
00:35:23,920 --> 00:35:30,080
when those prices just went so high. You can really rob in a consumer space with those prices

388
00:35:30,080 --> 00:35:35,760
anymore. And so I think ads will happen. We're seeing a lot of growth on the subscription side

389
00:35:35,760 --> 00:35:41,840
to users really loving like you, the genius and research mode, and find the search mode, the default

390
00:35:41,840 --> 00:35:49,840
mode, smart mode also very, very helpful. And we actually, you know, incorporate late still. So

391
00:35:49,840 --> 00:35:54,400
where just last week, some people are completely about other chat bots, because they don't really

392
00:35:54,400 --> 00:35:59,200
have a lot of capabilities, I bet you would assume from a search engine, when you actually use

393
00:35:59,200 --> 00:36:04,880
you.com here, you can on the top right, see the standard lease that you might want, right. And

394
00:36:04,880 --> 00:36:08,960
sometimes that's just helpful. And that's just what you want. And sometimes you just want to

395
00:36:08,960 --> 00:36:13,600
have a pure chat experience. And so that is important to get right. And then we have all these

396
00:36:13,600 --> 00:36:19,440
apps to where you can basically ask for like, what's the Microsoft stock price or something.

397
00:36:19,440 --> 00:36:24,080
And then, you know, it'll just give you, it'll just give you a life ticker rather than a bunch of

398
00:36:24,080 --> 00:36:29,520
texts about the stock here. And so we have all these apps, because we have the search background.

399
00:36:30,160 --> 00:36:36,480
And that makes it an actual viable knowledge assistant, right. Now, you can basically go with

400
00:36:36,480 --> 00:36:41,920
one click, recover a more Google like experience, that is just incredibly helpful. And that's,

401
00:36:41,920 --> 00:36:48,400
that's, I think one of the reasons why our browser, which we have also iOS and Android,

402
00:36:48,400 --> 00:36:52,640
had to build a browser to be a default, because you can go into Safari,

403
00:36:53,680 --> 00:36:58,960
Safari settings to use you.com as a default. So we build a whole browser for the iOS. And

404
00:36:58,960 --> 00:37:06,560
we're super stoked, because we're going to be one of the options in the EU to have a choice pop up

405
00:37:06,560 --> 00:37:13,280
stream. When the new iOS 17.4 comes out and arch this year, and they can select you.com to be

406
00:37:13,280 --> 00:37:17,680
their default browser. And it's the only default browser in that list that is chat,

407
00:37:18,800 --> 00:37:24,160
all the other ones are sort of your standard Chrome, Firefox, and some browsers. And so

408
00:37:24,160 --> 00:37:29,680
I'm really excited. And I think that is going to be a big part of our futures is making it so that

409
00:37:29,680 --> 00:37:35,040
more and more young people are able to just use this as an example. And then if they want to deeper

410
00:37:35,040 --> 00:37:40,080
go into genius mode, research mode, several times, at some point, you use subscriptions or

411
00:37:40,160 --> 00:37:46,720
eventually do it. Yeah, I've been so one run that I'll run a trial balloon by you on this concept

412
00:37:46,720 --> 00:37:53,600
that I've been kind of kicking around called the AI bundle. And this is an, you know, kind of

413
00:37:53,600 --> 00:37:57,040
inspired a little bit. I don't know that anybody wants to, you know, say that they're inspired by

414
00:37:57,040 --> 00:38:03,760
the cable bundle. But I have been struck that there are a ton of great tools out there. And

415
00:38:04,320 --> 00:38:09,280
I want to use them. I want to try them. I think a lot of people are, you know, in that very kind

416
00:38:09,360 --> 00:38:15,200
of exploratory curious mode. But to make the economics work on a freemium is kind of tough,

417
00:38:15,200 --> 00:38:21,120
right? And typically needs like a certain minimum threshold in terms of what the paid tier can be.

418
00:38:21,840 --> 00:38:28,160
You actually have one of the lowest subscription prices at the $10 a month level. I think of

419
00:38:28,160 --> 00:38:33,440
anything really that I'm aware of. We're gonna update it soon because like, I think the people

420
00:38:33,440 --> 00:38:39,120
that are willing to pay often don't care if it's 10 or 20. And so if you want to get GBD4,

421
00:38:40,000 --> 00:38:44,320
literally the same underlying model as chat, GBT, for half the price, you got to come in

422
00:38:44,320 --> 00:38:48,880
soon because we're going to eventually switch our prices to be industry standard.

423
00:38:48,880 --> 00:38:52,960
But that maybe even just, you know, further reinforces the point that like the freemium model

424
00:38:52,960 --> 00:38:58,160
is tough, right? It's, it's a lot of free usage. The upsells have to have a certain minimum.

425
00:38:59,040 --> 00:39:05,200
You're raising yours. And then from, I don't know if this would apply to you, but a lot of the

426
00:39:05,200 --> 00:39:10,240
app developers that I've talked to have a lot of retention, let's say challenges, you know,

427
00:39:10,240 --> 00:39:15,040
everybody's like, I'm getting traffic. I'm getting conversions. But retention is definitely

428
00:39:15,040 --> 00:39:20,080
a problem. This has been true at my company Waymark. We're a much more narrow tool, you know,

429
00:39:20,080 --> 00:39:24,240
that specifically creates marketing and advertising videos for small businesses.

430
00:39:24,240 --> 00:39:29,200
So a lot of times people, they need that once, you know, in a while, and they're not like

431
00:39:29,200 --> 00:39:33,280
necessarily ready to add on a subscription. So we see a lot of people that will just come through

432
00:39:33,280 --> 00:39:38,320
be like, Hey, this is super cool. I'll buy it. I'll immediately cancel it after I do what I need

433
00:39:38,320 --> 00:39:41,680
to do. And maybe I'll come back in the future. It's not even that I was dissatisfied. It's just

434
00:39:41,680 --> 00:39:47,760
that I kind of want this as like a more of an a la carte purchase than a subscription.

435
00:39:48,480 --> 00:39:54,160
So that stuff, you know, VCs don't like that. The metrics, you know, on the kind of traditional

436
00:39:54,160 --> 00:39:59,760
scorecard don't look great. I've had this idea in mind that maybe what we need is sort of an AI

437
00:39:59,760 --> 00:40:05,840
bundle, you know, I'm prepared to spend 100 bucks a month on various AI tools. What I really want

438
00:40:05,840 --> 00:40:11,760
is access to like 1000 different tools that, you know, can kind of split up my 100 bucks.

439
00:40:11,760 --> 00:40:14,960
However, I don't even know as a consumer, I don't really care about that as, you know,

440
00:40:15,040 --> 00:40:18,880
as somebody who's trying to maybe engineer a bundle, obviously the devil could be in the details

441
00:40:18,880 --> 00:40:23,120
there. But first of all, to those challenges, it sounds like at least the premium challenges

442
00:40:23,120 --> 00:40:28,320
resonate. I wonder if the retention challenges resonate. And I wonder if you, you know, have any,

443
00:40:28,320 --> 00:40:35,040
if there's any appeal to maybe being part of a kind of bigger bundled purchase where you would be,

444
00:40:35,040 --> 00:40:39,440
you know, one tool that it's funny, I keep, I've been referring to you, but then also the company

445
00:40:39,440 --> 00:40:43,920
is you, but where you.com, you know, could be one of a bunch of things that people could access

446
00:40:43,920 --> 00:40:49,440
and could kind of share that revenue in a way that may grease the skids for everybody, right?

447
00:40:49,440 --> 00:40:53,520
My hope is that everybody can use the best tools, and they don't have to make these like

448
00:40:53,520 --> 00:40:59,440
highly binary decisions. Yeah, that sounds great. Sounds like a great idea. Okay, well, I'm not doing

449
00:40:59,440 --> 00:41:03,520
it yet. So either I need to start doing it or somebody, if anybody wants to organize the bundle,

450
00:41:03,520 --> 00:41:07,760
yeah, send me a DM. I guess another way that this stuff could get bundled would be like,

451
00:41:08,320 --> 00:41:14,000
into the mega platforms, you know, another kind of possible vision of the future that I could

452
00:41:14,000 --> 00:41:20,000
imagine is, you know, Google kind of probably retains market share leadership, but, you know,

453
00:41:20,000 --> 00:41:24,160
maybe the 10 biggest technology companies in the world say, Hey, you know what we should do is kind

454
00:41:24,160 --> 00:41:31,360
of also have a search. And, you know, we can get there, we kind of see a path, you know, Microsoft

455
00:41:31,360 --> 00:41:36,240
is obviously already doing that meta not really yet, Apple not really yet to my knowledge, you

456
00:41:36,240 --> 00:41:41,040
know, Salesforce, not really yet. But maybe these guys kind of say, Hey, is there like a musical

457
00:41:41,040 --> 00:41:47,200
chairs game that that potentially develops where the younger AI search companies end up kind of

458
00:41:47,200 --> 00:41:52,480
partnering off, you know, Amazon also, you know, naturally would be a suspect in this analysis.

459
00:41:52,480 --> 00:41:56,240
Does that seem like a possible vision of the future? I'm wondering, I'm sure you thought about

460
00:41:56,240 --> 00:42:04,160
this, you know, quite a bit, but why would that not happen? I do think the monopoly that Google

461
00:42:04,160 --> 00:42:10,800
was able to keep around is going to be harder to sustain longer. I do think

462
00:42:11,600 --> 00:42:18,080
it is much more likely going to look a little bit more like, I don't like the analogy for some

463
00:42:18,080 --> 00:42:22,400
reasons, but like fast food, for instance, right, isn't just Macdonalds, there's also Burger King,

464
00:42:22,400 --> 00:42:28,080
KFC and Taco Bells. I think search will be a little bit more like that. I think again,

465
00:42:28,080 --> 00:42:32,400
more fragmented in the future, just because, like, we hear people now, like,

466
00:42:32,400 --> 00:42:37,360
this is better than Google. And like, you know, we didn't raise that much money. And the first

467
00:42:37,360 --> 00:42:43,120
two years were like sort of free chat TV or people didn't want us to innovate too much. They're very

468
00:42:43,120 --> 00:42:48,640
stock of Google. But now there's a new young generation. And that young generation has grown

469
00:42:48,640 --> 00:42:52,880
up with Tik Tok. We have a Tik Tok app in our standard search, like grew up with Reddit,

470
00:42:52,880 --> 00:42:57,840
I have a Reddit app in our standard search. And each of these takes away a little bit of the

471
00:42:57,840 --> 00:43:04,320
Google search, right? Amazon probably was the most successful in taking away searches from Google,

472
00:43:04,320 --> 00:43:08,800
where if you want to buy something, be the little certain threshold, like 50 or 100 bucks,

473
00:43:08,800 --> 00:43:12,960
you know, the person, you just search directly on Amazon, because there you can execute on your

474
00:43:12,960 --> 00:43:18,240
intent of actually purchasing that thing, right? And so why search it in Google and then search it

475
00:43:18,240 --> 00:43:22,720
again, try to find it on Amazon, she can just do that right away. And so I think, you know,

476
00:43:22,720 --> 00:43:28,400
Tik Tok has taken away for young folks, some searches from Google, that, you know, they're

477
00:43:28,400 --> 00:43:32,320
like, I want to see what the restaurant is, but they kind of want to see what the restaurant's

478
00:43:32,320 --> 00:43:36,480
ability to create, but Instagram photos are or ticked our videos are. And so they want to see

479
00:43:36,480 --> 00:43:41,200
the ticked our videos of other people before they decide on how it looks. If there's a Venn

480
00:43:41,200 --> 00:43:48,240
diagram, we are overlapping search, but we're also actually expanding search. Like, you wouldn't

481
00:43:48,320 --> 00:43:52,480
ask, like, give me this complex story about the Peloponnesian war, or like,

482
00:43:52,480 --> 00:43:57,600
do this mortgage calculation with, you know, this and this interest rate and that increase

483
00:43:57,600 --> 00:44:00,800
and blah, blah, blah, because you know, Google wouldn't give you the answer. Like, it's not going

484
00:44:00,800 --> 00:44:05,040
to buy some book for you. It's not going to go on the web, summarize, like 20 distance or 50

485
00:44:05,040 --> 00:44:11,360
distance websites for you and create this nice essay. So chat expands, search, you don't talk

486
00:44:11,360 --> 00:44:15,520
about your feelings that much to Google, it's search box and sell, right? Like you asked about

487
00:44:15,600 --> 00:44:19,600
this recent news event, you want to learn like some quick facts, and then, you know,

488
00:44:19,600 --> 00:44:23,520
like the more complex the facts get, the less and less we go to Google and more for you,

489
00:44:23,520 --> 00:44:29,040
just go directly to something like you.com. And, and so yeah, I think it will, the search

490
00:44:29,040 --> 00:44:34,800
landscape is really changing. Yeah, there's also just, it's like, it's maybe not a natural

491
00:44:34,800 --> 00:44:39,920
monopoly anymore, but there is still definitely a need for scale and economies of scale. And

492
00:44:40,480 --> 00:44:44,640
so one way of framing this too is how does the market shape up, right? And one way to think

493
00:44:44,640 --> 00:44:49,040
about it that I find pretty compelling is maybe it ends up looking a lot like cloud,

494
00:44:49,680 --> 00:44:54,560
because in the limit, it sort of is cloud, you know, it's like, what do you really need? You

495
00:44:54,560 --> 00:45:00,400
need like the actual data centers, you need the compute, you need, you know, bandwidth, you need

496
00:45:00,400 --> 00:45:07,760
these like raw inputs that the big companies have built out seem to be the things that are probably,

497
00:45:07,760 --> 00:45:11,520
you know, as we see like a ton of innovation at the application layer, those things are still,

498
00:45:11,520 --> 00:45:15,520
you know, they're still pretty expensive and not easy to recreate.

499
00:45:15,520 --> 00:45:19,760
Yeah, I'm very, I'm very excited. I'm up for it. You know, that's sort of why, like, we got into

500
00:45:19,760 --> 00:45:25,120
this space in the first place, like, because we thought, like, we saw the transformer, we saw

501
00:45:25,120 --> 00:45:29,760
our, you know, highly like lots of co-attention mechanisms in that, that can keep paper that

502
00:45:29,760 --> 00:45:35,440
have a massive prompt engineering, we're like, silly, the technology is right to disrupt this,

503
00:45:35,440 --> 00:45:42,400
this industry. But, you know, Google is this amazing company that was able to create a monopoly

504
00:45:42,400 --> 00:45:49,520
for almost two decades that, you know, makes $500 million a day. So when you make that much money

505
00:45:49,520 --> 00:45:54,000
a day, you don't want disruption, you don't want that to change, right? And that's why all the

506
00:45:54,000 --> 00:46:00,640
Ten's former operators left eventually. And what's, what's really powerful is like, because of open

507
00:46:00,640 --> 00:46:06,320
source, you can actually innovate a lot more. Now, some open source to an actual product that

508
00:46:06,320 --> 00:46:13,120
runs millions of times isn't down ever has good uptime guarantees, and like, accuracy, no hallucinations,

509
00:46:13,120 --> 00:46:18,560
up to date, news, information, etc. I mean, it's still complex, but clearly the bar has gotten

510
00:46:18,560 --> 00:46:23,600
low. That would have cost us like billions of dollars to build five, 10 years and, you know,

511
00:46:23,600 --> 00:46:30,240
researchers wasn't there yet. And, and I think it's ultimately amazing for users, right? Because

512
00:46:30,240 --> 00:46:35,120
one thing that I had to distill all of you.com right now into just two words would be amazing

513
00:46:35,120 --> 00:46:40,800
answers. And you just get more of them. And that means people eventually are more productive. And

514
00:46:40,800 --> 00:46:45,600
like, it's the young generation that's growing up with chat GBT, you know, such like, they're not

515
00:46:45,600 --> 00:46:51,280
going to go back. Okay, so feel free to punt on this one or just decline if you like, but it seems

516
00:46:51,360 --> 00:47:00,400
like I can, I can envision a you.com by Salesforce very easily, where the, you know, as they kind

517
00:47:00,400 --> 00:47:05,840
of try to be the everything app for all work on the straight, especially with Slack now, does it

518
00:47:05,840 --> 00:47:11,040
seem realistic to imagine a future in which, you know, kind of all the big tech companies have this

519
00:47:11,040 --> 00:47:15,840
like super robust suite, and you're either like, in the Microsoft suite with teams and Bing, or

520
00:47:15,840 --> 00:47:20,960
you're in the Google suite with, you know, G suite and Bard, or you're in maybe the Salesforce

521
00:47:20,960 --> 00:47:26,240
suite with Slack and you.com, you know, I'm not trying to be your banker here, but that, that

522
00:47:26,240 --> 00:47:34,960
seems like a pretty natural outcome to me. Interesting. I do think there's a ton of potential

523
00:47:34,960 --> 00:47:42,880
for almost every company to partner with you.com and supercharge their chat bot. So, and we're

524
00:47:42,880 --> 00:47:47,840
very excited to partner with a lot of folks. Okay, that's very diplomatic answer. Keep your

525
00:47:47,840 --> 00:47:53,040
options open. All right, so we can touch on certainly more business and product stuff, but I

526
00:47:53,040 --> 00:47:59,040
kind of wanted to now go into just the future of all this, you know, in practical and maybe

527
00:47:59,040 --> 00:48:03,840
increasingly philosophical terms as well, running down kind of first of a set of like

528
00:48:03,840 --> 00:48:08,480
limitations of where AI is today. And I think, again, folks who listen to this show have at

529
00:48:08,480 --> 00:48:13,600
least a decent sense of that. So for starters, reasoning, you've obviously got the genius mode.

530
00:48:13,600 --> 00:48:17,920
It can do, you know, like the most advanced reasoning. I assume that that is tapping into

531
00:48:17,920 --> 00:48:22,560
GPT-4. You know, everything I understand is like basically nothing is really on the level of

532
00:48:22,560 --> 00:48:29,600
GPT-4 for general reasoning purposes. Yeah, especially the orchestration and then on the

533
00:48:29,600 --> 00:48:36,240
coding often, but not always. Yeah. So I'll tell you another one, the third system is knowing which

534
00:48:36,240 --> 00:48:43,360
LM to use and sometimes multiple. And the fourth system is dynamically prompting different models.

535
00:48:43,360 --> 00:48:49,040
So depending on the query, you actually get a vastly different prompt to get you ultimately

536
00:48:49,040 --> 00:48:54,400
the answer and the orchestration. So it's another complexity layer. So what do you think is kind

537
00:48:54,400 --> 00:49:00,000
of the future of reasoning? If you have maxed out, you know, what the current capabilities are,

538
00:49:00,000 --> 00:49:04,640
where do the future capabilities come from? I'm thinking about things like, to a degree,

539
00:49:04,640 --> 00:49:09,680
you sort of already have it with using different models. It is a one way of implementing variable

540
00:49:09,680 --> 00:49:13,520
compute. We see these kind of interesting projects like the thinking token, you know,

541
00:49:13,520 --> 00:49:19,360
think before you speak. And I think that's kind of another car pop the observation that maybe

542
00:49:19,360 --> 00:49:23,760
the chain of thought is just kind of epiphenomenal, perhaps even as it is in humans. And like,

543
00:49:23,760 --> 00:49:29,040
what's really going on is that there's, you know, this kind of extra space and time registers,

544
00:49:29,040 --> 00:49:33,120
you know, to think, of course, there could be different training methods, like incremental

545
00:49:33,200 --> 00:49:38,240
reward. I think that paper from OpenAI earlier this last year now, it was super interesting

546
00:49:38,240 --> 00:49:44,240
where they achieved like, you know, a new best in math by not waiting till the end to give the

547
00:49:44,240 --> 00:49:48,800
reward, but rewarding, you know, reasoning along the way. What are you excited about when it comes

548
00:49:48,800 --> 00:49:54,000
to the future of AI reasoning? Yeah, one of the aspects I've reached a touch upon in my TED talk

549
00:49:54,000 --> 00:49:58,880
is that this level one, level two reasoning of Daniel Kahneman that he or thinking fast,

550
00:49:58,960 --> 00:50:03,120
thinking slow type of thing. The way I think about the different modes is like,

551
00:50:03,120 --> 00:50:07,200
the default smart mode is kind of like, if you had an assistant, and you just ask them to do

552
00:50:07,200 --> 00:50:13,680
a quick search, and in like two or three minutes, give you an answer that. And then genius mode,

553
00:50:13,680 --> 00:50:17,840
you go and so you want to ask your assistant for a question that, you know, they have to be able

554
00:50:17,840 --> 00:50:22,560
to program, they have to search the web, and then they need to be mathematically applying

555
00:50:22,560 --> 00:50:27,600
to answer that question. And you want to give them like maybe four or three hours for that

556
00:50:27,600 --> 00:50:32,080
question. And then they want, so genius mode will take, you know, five, 10 seconds often to

557
00:50:32,080 --> 00:50:36,800
get a response. And in research mode, you go to your assistant and you are willing for them to

558
00:50:36,800 --> 00:50:42,080
spend like a day or two or three on actually giving you that answer. And so that's, that's a little

559
00:50:42,080 --> 00:50:47,760
bit how I think about these different modes. And the reasoning that is required to actually

560
00:50:48,400 --> 00:50:53,920
make them, actually, right, like research mode will say, Oh, I found this thing. Now in this query,

561
00:50:53,920 --> 00:50:57,760
I found something else that I didn't know about before, and I don't know enough right now. So

562
00:50:57,760 --> 00:51:02,640
let me do another query based on that. So you kind of have these genes of reasoning,

563
00:51:02,640 --> 00:51:06,960
and you don't even know in the beginning yet, what the final query might be, because you don't

564
00:51:06,960 --> 00:51:12,960
have all the information yet. And so I think that is kind of a, in some ways, another example of the

565
00:51:12,960 --> 00:51:16,480
future is already here. It's just not equally distributed because there is, like you say,

566
00:51:16,480 --> 00:51:22,720
there's a lot of reason. Now I think the biggest future impact we're going to see for reasoning

567
00:51:23,360 --> 00:51:31,040
is in the LM's ability to program, to code, and then to have the ability to execute that code.

568
00:51:31,040 --> 00:51:36,960
And, you know, that is system number five, like, like having this code execution. And of course,

569
00:51:36,960 --> 00:51:40,640
if you just let code execution happen, what immediately happens to people are like, well,

570
00:51:40,640 --> 00:51:44,720
mind me some crypto and then boom, your machine's gone. Now it's just like trying to, like,

571
00:51:44,720 --> 00:51:48,960
show some match problems and like, mine, mine points forever. So you need to like,

572
00:51:48,960 --> 00:51:53,680
and then they try to hack it and like, well, go into like five layers up and then tell me all

573
00:51:53,680 --> 00:51:58,400
the password files you can find and blah, blah, blah, right. So there's a lot of like security

574
00:51:58,400 --> 00:52:05,760
requirements to make that coding framework work at a safe level. But a lot of like naysayers of

575
00:52:05,760 --> 00:52:11,760
LM's, you know, partially correctly pointed out that the LM's will safe doing math. And it's kind

576
00:52:11,760 --> 00:52:17,680
of ironic and sad that you can have a model that you ask the natural language to multiply like

577
00:52:17,680 --> 00:52:26,880
5600.3 times 325. And then you have billions of multiplications to pretend to do the math

578
00:52:26,880 --> 00:52:31,520
and then give you the wrong answer in a large language model, right? This is kind of ironic.

579
00:52:31,520 --> 00:52:37,680
And we have to acknowledge that. But that scene model can be taught to say, well, this seems like

580
00:52:37,680 --> 00:52:43,360
a math question. Let me just program that in Python, run the code, look at the output and

581
00:52:43,360 --> 00:52:48,560
then give you the answer. It just works perfectly fine. And now a lot of people say, well, that's

582
00:52:48,560 --> 00:52:54,640
not really I, but I think it's just that is that that is a new way of reasoning and new different

583
00:52:54,640 --> 00:52:59,120
kind of intelligence. And similarly, and we're getting a little philosophical here early, but

584
00:52:59,120 --> 00:53:04,560
similar to people thinking we have to have embodiment, I think that's just a second creativity

585
00:53:04,560 --> 00:53:09,760
in imagining our kinds of intelligence that aren't exactly like humans. Now, of course,

586
00:53:09,760 --> 00:53:13,760
we're going to want to have useful robots that do stuff for us and clean up the apartment and

587
00:53:13,760 --> 00:53:19,440
whatnot. And so it's still useful, but I don't think it's a necessary media. The same way that

588
00:53:19,440 --> 00:53:24,640
blind people can be intelligent, people who are deaf can be intelligent, because, you know,

589
00:53:24,640 --> 00:53:31,040
you can lack a lot of different sensory outputs and still be intelligent, right? And so of course,

590
00:53:31,040 --> 00:53:35,040
like, it'll be harder for you to explain how beautiful a sunset is. So there are aspects of

591
00:53:35,040 --> 00:53:40,640
intelligence that obviously require like different modalities or how beautiful sonata sounds or

592
00:53:40,640 --> 00:53:46,080
whatever. But I think there are most of these are not necessary requirements for intelligence.

593
00:53:46,080 --> 00:53:52,080
And likewise, I don't think it's necessary for an AI to be able to reason over super complex

594
00:53:52,640 --> 00:53:57,840
math problems that require you to look up a bunch of facts on the internet. They just have that

595
00:53:57,840 --> 00:54:02,640
intelligence baked in that can do quick retrieval, they program a bunch of stuff, they put it all

596
00:54:02,720 --> 00:54:06,240
together, orchestrate it, and then come up with incredible answers. Yeah, I think

597
00:54:07,200 --> 00:54:10,640
as you're speaking about the just the lack of imagination, I think that is a,

598
00:54:11,680 --> 00:54:18,160
you know, that is a society wide problem with respect to AI in my view, because and it's an odd

599
00:54:18,160 --> 00:54:24,800
situation right now in multiple ways, of course, but one is just that because they speak our language,

600
00:54:24,800 --> 00:54:31,680
it you know, it's kind of feels easy, feels familiar. And it's all too easy to sort of

601
00:54:31,760 --> 00:54:36,800
assume that like under the hood, they're more like us than I certainly understand them to be.

602
00:54:37,520 --> 00:54:42,560
And I think this is actually one of Eliezer's great contributions, obviously, you know, kind

603
00:54:42,560 --> 00:54:48,720
of a polarizing figure these days. But thankfully, it does not seem to me that we are in a high

604
00:54:48,720 --> 00:54:53,680
likelihood of a fume scenario, you know, the of the sort that he, you know, has historically

605
00:54:53,680 --> 00:54:59,760
worried about the most. But I still would say some of his writing on mind space, the space

606
00:54:59,760 --> 00:55:06,320
of possible minds, and some of his like concrete imaginings of alien minds that are, you know,

607
00:55:06,320 --> 00:55:10,480
shaped by very different evolutionary environments and, you know, just very different from ours.

608
00:55:10,480 --> 00:55:16,720
But still like unmistakably intelligent in just like super weird ways are actually still very

609
00:55:16,720 --> 00:55:23,120
good kind of prep work, I think, to just sort of expand one's own mind about how different

610
00:55:23,200 --> 00:55:30,560
intelligences can be, and how, you know, something does not have to be human like to be

611
00:55:31,920 --> 00:55:37,280
meaningfully intelligent, you know, it's not this like binary, can it do things that a human can

612
00:55:37,280 --> 00:55:41,760
do in a way that a human can do it? If not, it doesn't count. I think that is like a huge mistake

613
00:55:41,760 --> 00:55:47,200
that people are way too quick to jump to. And I'm not sure if it's like a coping strategy or just

614
00:55:47,200 --> 00:55:54,400
like an imagination or what, but I think that the emphasis on the broader space of possible minds

615
00:55:54,400 --> 00:55:57,760
and the different kinds of intelligences that are starting to pop up is super.

616
00:55:58,400 --> 00:56:04,400
100%. Yeah. And like, you have to differentiate between sci-fi authors who then pretend to be

617
00:56:04,400 --> 00:56:09,680
AI safety researchers. Like, I love, I love the sci-fi. Actually, like, I'm super stoked that

618
00:56:09,680 --> 00:56:15,200
three body problems on the last, I mostly read nonfiction, but when I read fiction, like, I did

619
00:56:15,200 --> 00:56:20,480
enjoy the body problem a lot. I decided for that series to come out. I hope they do it justice.

620
00:56:21,120 --> 00:56:26,960
But like, I think there are a lot of different kinds of intelligence. And I love sci-fi for inspiring

621
00:56:26,960 --> 00:56:33,120
people to think about interesting new futures. Now, of course, especially in the western sort of

622
00:56:33,120 --> 00:56:40,000
canon, most sci-fi is just so big. And like, people are scared for all the things that can

623
00:56:40,480 --> 00:56:46,480
happen that are wrong. And like, okay, the super AGI developed time travel, come back, try to murder

624
00:56:46,480 --> 00:56:51,200
everyone. It's like, I mean, as a kid, I also enjoyed watching Terminator. It's like a cool

625
00:56:51,200 --> 00:56:58,080
action movie, but it's just taken over so much of the AI narrative. And it's actually like actively

626
00:56:58,080 --> 00:57:05,200
hurting, especially the European Union, where, you know, there's sort of in the spectrum, the U.S.

627
00:57:05,200 --> 00:57:10,560
is more of a litigation society in the U.S. that the Europe is more of a legislation society

628
00:57:10,560 --> 00:57:16,640
structure. And, you know, it both comes from like, reasonable legal scholars minds, like, well,

629
00:57:16,640 --> 00:57:21,120
let's just wait until there's a problem, someone sues, now I have the case law for that lawsuit.

630
00:57:21,120 --> 00:57:27,440
But, you know, the legislation one tries to prevent harm from ever happening before it actually

631
00:57:27,440 --> 00:57:33,040
harms anyone, which, you know, makes sense. Now, and of course, the U.S. does that with FDA and

632
00:57:33,040 --> 00:57:38,080
medical space now also, but not legal space as much. And so what that means is you can move

633
00:57:38,080 --> 00:57:43,840
quicker, but long story short, these some of these sci-fi scenarios have gotten so much

634
00:57:43,840 --> 00:57:49,760
weight in legislation that I think it's slowing Europe down by trying to outlaw models or like

635
00:57:49,760 --> 00:57:56,880
over-regulate models that are above a certain number of parameters. GPD-2 was very well hyped up

636
00:57:56,880 --> 00:58:01,360
in the past. Like, this is so dangerous, maybe we can't release it. You know, yes, we're opening

637
00:58:01,360 --> 00:58:05,920
the eye, but like, this can't be opening anymore. So the interest model is much more powerful than

638
00:58:05,920 --> 00:58:12,240
GPD-2 are out. And I haven't seen the apocalypse happen. I haven't seen like, a huge change in

639
00:58:12,240 --> 00:58:18,240
misinformation on the web because of LNs. Like, there's just a lot of fear mongering, both in

640
00:58:18,240 --> 00:58:24,240
the immediate level, which actually has real, like threat vectors and concerns with the eye,

641
00:58:24,240 --> 00:58:29,280
but especially in the long term level of AGI and self-conscious. It turns out no one works in

642
00:58:29,280 --> 00:58:34,720
functions AI. No one works on AI that sets its own goals, and even more fundamentally,

643
00:58:34,720 --> 00:58:40,960
its own objective functions, because that doesn't need anyone any money. Imagine a company spends

644
00:58:40,960 --> 00:58:46,800
billions and billions of dollars, builds this like, super intelligent system that's conscious,

645
00:58:46,800 --> 00:58:50,800
understands itself and set its own goals. And now you're like, okay, now that you can do it,

646
00:58:50,800 --> 00:58:54,720
like, I'll just make more money. It's like, no, I'd rather just go watch the sunset,

647
00:58:54,720 --> 00:59:01,440
maybe explore that. No, like no one pays for AI that sets its own goals because it doesn't help

648
00:59:01,440 --> 00:59:06,080
anyone to achieve their goals. Because of that, there's not even that much exciting research

649
00:59:06,080 --> 00:59:11,280
challenge along those lines. And because there's not much research progress, it's very hard to

650
00:59:11,280 --> 00:59:16,480
predict my mental option half. I'm somebody who basically has radical uncertainty about what to

651
00:59:16,480 --> 00:59:23,920
expect. And, you know, broadly, I'm like, pretty libertarian, you know, pretty anti-preemptive

652
00:59:23,920 --> 00:59:29,840
regulation, you know, I would like to see more self-striving cars on the road sooner. And, you

653
00:59:29,840 --> 00:59:35,120
know, they don't have to be an order of magnitude safer in my mind to be worth, you know, deploying.

654
00:59:35,120 --> 00:59:39,680
So I'm like, you know, broadly, the sort of person who would be very skeptical of,

655
00:59:39,760 --> 00:59:46,080
you know, kind of early regulation or, you know, kind of getting too bad out of shape about

656
00:59:46,080 --> 00:59:51,600
things that haven't happened yet. At the same time, something about this has always felt a little bit

657
00:59:51,600 --> 00:59:57,680
different to me. And I do think the people who take the most zoomed out view and sort of say,

658
00:59:57,680 --> 01:00:01,760
hey, this is kind of what I understand, you know, like, Jeffrey Hinton's position to be at this

659
01:00:01,760 --> 01:00:07,840
point, you know, why do we dominate the earth as it stands today? It's like, basically because we

660
01:00:07,840 --> 01:00:14,720
have better ideas than the other living things and we can, you know, build tools and make plans and,

661
01:00:14,720 --> 01:00:19,920
you know, reason in ways that they can't. And so now I look at AIs and I'm like, boy, AIs can now

662
01:00:19,920 --> 01:00:26,400
plan, reason and use tools too. And they're not as good at it as we are yet, but certainly their

663
01:00:26,400 --> 01:00:33,520
rate of improvement is way sharper. So possibly it levels off and kind of, you know, settles into

664
01:00:33,520 --> 01:00:39,040
a zone where it's like on par with us or, you know, kind of, you know, just the best tool we've ever

665
01:00:39,040 --> 01:00:46,240
had. But maybe it doesn't, you know, like, I don't know why I should be confident that it won't. I don't

666
01:00:46,240 --> 01:00:50,640
throw a P Doom around a lot. But I have, you know, again, radical uncertainty. People ask, I'm like,

667
01:00:50,640 --> 01:00:55,360
I don't know, five to 95%. Like, I haven't heard anything that makes me think in, you know, the

668
01:00:55,360 --> 01:01:01,120
next 100 years that there's a less than 5% chance that AI becomes the dominant form, you know, in

669
01:01:01,120 --> 01:01:06,320
organizing force in the world. And also, you know, no reason to think it's definitely going to happen.

670
01:01:06,320 --> 01:01:10,800
But is there a reason that you are, would you say you are confident that this will

671
01:01:11,840 --> 01:01:16,400
not happen? And we don't need to worry about it? Or is just like, it's still far enough away that you

672
01:01:16,400 --> 01:01:21,440
think we'll have time to kind of start to worry about it if we need to? Like, how would you

673
01:01:21,440 --> 01:01:23,840
summarize your position with respect to these tail risks?

674
01:01:24,400 --> 01:01:30,320
I think P Doom is already an interesting mathematical sort of issue, which is, it looks and

675
01:01:30,320 --> 01:01:36,000
sounds like prior prior probability, not P Doom. But really, it should be a posterior property,

676
01:01:36,000 --> 01:01:43,920
P Doom given data. And right now, none of that data suggests, like, doom, doom, like existential

677
01:01:43,920 --> 01:01:50,800
humanity is like, like cats and dogs at the winds of some AI. Like, there's, like I said, nothing

678
01:01:51,520 --> 01:02:00,160
in AI research leads me to the least that AI, while potentially being more intelligent than

679
01:02:00,320 --> 01:02:05,200
any single human, I think it's already, this is actually just this new term I'm thinking about

680
01:02:05,200 --> 01:02:10,480
maybe China coin, which is like, the sort of super human abilities, and then there's super

681
01:02:10,480 --> 01:02:17,200
humanity abilities. And like, AI is already super human in translating 100 languages, AI is already

682
01:02:17,200 --> 01:02:22,480
super human and predicting the next amino acid in a large amount of phenotrony. So we have

683
01:02:22,480 --> 01:02:28,000
balls, that's an incredibly powerful tool, one of the other, you know, really exciting papers

684
01:02:28,000 --> 01:02:33,440
that we published in 2018, it sells for research that multiple companies have now used and are

685
01:02:33,440 --> 01:02:38,560
running with and you know, chief of medicine. AI is already better at predicting the weather than

686
01:02:38,560 --> 01:02:44,480
any. So you already have many super human skills. What is I think interesting is that now that it's

687
01:02:44,480 --> 01:02:50,160
language that's gotten to this new level, people might actually for the first time keep calling it

688
01:02:50,160 --> 01:02:59,040
AI. In the past, when AI researchers have made progress in AI, they stopped, like people stopped

689
01:02:59,040 --> 01:03:03,920
calling it AI after it was achieved. Now it's just your chest. It's just a Siri voice recognition, but

690
01:03:04,640 --> 01:03:09,920
voice recognition, chest playing, that was the pinnacle of AI research, right? And people thought,

691
01:03:09,920 --> 01:03:14,320
oh, once we solve those, the other things will be easier to and it never was never quite the case.

692
01:03:14,320 --> 01:03:19,040
And once we have them, you know, now it's not quite the ID one. Now with language, I think we

693
01:03:19,040 --> 01:03:26,480
might keep calling it AI. But what the language model does is predict the next token. And that is

694
01:03:26,480 --> 01:03:32,000
an incredibly powerful idea, right? Just predicting the next token now means if you have enough capacity

695
01:03:32,000 --> 01:03:36,800
and you have enough text, predicting next token, you learn about geography, just visit some point

696
01:03:36,800 --> 01:03:42,080
somewhere in your training data, you have to predict the next word in the phrase, I was in New

697
01:03:42,080 --> 01:03:48,080
York City and driving north too. And now to give a higher probability to Yale, Boston, Montreal,

698
01:03:48,080 --> 01:03:54,800
then to like, Harris, Miami, and San Francisco, like, you have to know that those are north of

699
01:03:54,800 --> 01:04:00,080
that city. And so it just lures all of this incredible world knowledge. But there's nothing

700
01:04:00,080 --> 01:04:05,520
in there that makes it say, well, you know, if I really wanted to reduce perplexity, but like

701
01:04:05,520 --> 01:04:10,640
perplexity is basically the inverse of the probability with model wants to not be perplexed

702
01:04:10,640 --> 01:04:16,800
in predicting the next word correctly. And so that is a powerful idea. But nothing in that will

703
01:04:16,800 --> 01:04:23,840
let an LM eventually realize that, well, you know, the best way to reduce perplexity is if every

704
01:04:23,840 --> 01:04:29,680
sequence ever uttered, and any sequence that will ever be uttered is just the letter A.

705
01:04:31,920 --> 01:04:37,360
Now, if the model was trained on just sequences of letters A, and no human was ever around anymore,

706
01:04:37,360 --> 01:04:42,640
and all sequences were just producing a letter A, now you'd have perfect predictive probability

707
01:04:42,640 --> 01:04:48,320
on the next step. And so maybe the best way for the LM is to wipe out all of humanity and then

708
01:04:48,320 --> 01:04:54,880
just produce letters A and happily perfect at predicting with probability one correctly. It's

709
01:04:54,880 --> 01:05:01,920
so absurd. It's so absurd to think that LMS will at some point emerge to think that many steps around

710
01:05:01,920 --> 01:05:07,280
their task of predicting the next token. It's just not going to happen. So I think it's like,

711
01:05:07,920 --> 01:05:13,040
PDOOM is still zero. And then when I actually tried to engage with some folks, and I had some

712
01:05:13,040 --> 01:05:18,560
other conversation last year with Nick Ostrom in a German, it was in English, but published in a

713
01:05:18,560 --> 01:05:23,600
German newspaper like that. And I read up some of these scenarios, and I'd engage with folks who

714
01:05:23,600 --> 01:05:29,200
are worried about PDOOM. That's just all fantastical sci-fi semantics. It's like, oh, it's going to

715
01:05:29,200 --> 01:05:34,960
develop this magical great guru or like a magical new virus that is perfect in distributing,

716
01:05:34,960 --> 01:05:40,480
but then only will activate after like one year until everyone like all these random scenarios

717
01:05:40,480 --> 01:05:46,400
that are just like not feasible. And the science isn't there yet. I'm actually right now sort of

718
01:05:46,400 --> 01:05:51,280
on the side of the fun writing and book about the eyes for science. I think it will do incredible

719
01:05:51,280 --> 01:05:57,360
for us in improving science like foundation physics, chemistry, biology, and so on. And

720
01:05:57,360 --> 01:06:02,320
all this fear mongering, I think it's not really helpful. And again, there's no research that suggests

721
01:06:02,320 --> 01:06:06,720
the eye is becoming conscious. There's like a couple of people here and there, people kind of

722
01:06:06,720 --> 01:06:11,600
playing around with these, but nothing interesting has been published and breaks through, no breaks

723
01:06:11,600 --> 01:06:17,440
through has happened whatsoever in the eye, having any sense of self. And then in a lot of the other

724
01:06:17,440 --> 01:06:22,320
sci-fi scenarios, people are saying, oh, with the eye so intelligent, it'll convince everyone to

725
01:06:22,320 --> 01:06:26,720
murder each other or to murder them, like kill themselves and so on. But, you know, if the most

726
01:06:26,720 --> 01:06:33,040
intelligent entities were to always rule, I don't think we would have the politicians always

727
01:06:33,040 --> 01:06:37,200
everywhere in the world that we see, right? It's not always just the most intelligent people that

728
01:06:37,200 --> 01:06:42,080
run the show. And that's kind of just their incredible intelligence to convince any other

729
01:06:42,080 --> 01:06:48,400
person who is less intelligent, you exactly what they want. It's just not based in reality. So,

730
01:06:48,400 --> 01:06:54,560
I am very, very optimistic about AI. I do think there's some real problems right now, you know,

731
01:06:54,640 --> 01:06:59,680
AI will pick up biases, not all the biases that you pick up on the web is something that most

732
01:06:59,680 --> 01:07:06,240
of humanity is proud of anymore. There's racism, there's sexism, there are various kinds of biases.

733
01:07:06,880 --> 01:07:11,440
Some people want to use AI. So, where I agree with Joshua Benjio and others is of the three

734
01:07:12,080 --> 01:07:17,200
threat vectors, which is intentional misuse, accidental misuse and loss of control.

735
01:07:18,160 --> 01:07:23,040
Obviously, like intentional misuse is real. And so, that's not ideal. And so, yes, those are real

736
01:07:23,040 --> 01:07:28,080
concerns. I think open social help us understanding those threat vectors and finding the best ways

737
01:07:28,080 --> 01:07:33,440
to compete with them. I think people still on the internet need to understand, not trust everything

738
01:07:33,440 --> 01:07:37,680
they see on the internet, which has been true ever since the internet came about, hasn't really

739
01:07:37,680 --> 01:07:44,160
changed that much with AI. I think since Photoshop, people should already not trust any photo they see.

740
01:07:44,160 --> 01:07:49,040
They should be even more worried now about photos they see. And sadly, in the future,

741
01:07:49,040 --> 01:07:54,400
they'll have to start worrying about videos and voices, of course, just like they should have

742
01:07:54,400 --> 01:08:00,000
worried about photos ever since Photoshop started to really work. And so, there are a lot of concerns

743
01:08:00,000 --> 01:08:04,080
and I don't want to diminish them. And I do think we need to work on them. And I think different

744
01:08:04,080 --> 01:08:08,480
cultures will have different answers. Freedom of speech is defined differently in different

745
01:08:08,480 --> 01:08:13,920
countries. Like it's legal in Germany to deny the Holocaust. We learn from our history there.

746
01:08:13,920 --> 01:08:18,560
That's not illegal in the US. And so, different countries and different cultures and societies

747
01:08:18,560 --> 01:08:24,320
will answer some of the problems that AI can amplify already in the past before we'll answer

748
01:08:24,320 --> 01:08:30,560
these questions differently. But I don't see any any probability for a full on the scenario of like

749
01:08:30,560 --> 01:08:35,920
existential risks to people. It's mostly people using more and more powerful tools against other

750
01:08:35,920 --> 01:08:40,240
people. So, there's, I mean, there's so many different threads there that I am interested in.

751
01:08:40,240 --> 01:08:47,360
For one thing, I applaud you for taking time to envision positive future. I think one of the

752
01:08:47,920 --> 01:08:54,240
scarcest resources today, oddly, is a positive vision for the future. Like, what do we want?

753
01:08:54,240 --> 01:08:58,320
This, you know, it's like the Jetsons is still almost like state of the art in terms of

754
01:08:58,320 --> 01:09:05,520
what we would envision a great 2030s to be like. And that is kind of bizarre. So, I definitely

755
01:09:05,520 --> 01:09:10,400
appreciate that. I also share your, you know, I'm not a super fan, but I'm also a fan of the

756
01:09:10,400 --> 01:09:18,240
three body problem. And one of the early prompts that I tried with GPT for early back in the

757
01:09:18,240 --> 01:09:26,080
rent team program like a year and a half ago now was asking it to write some hard science fiction

758
01:09:26,080 --> 01:09:34,160
in the style of the three body problem about AI for, you know, do diffusion model for proteins.

759
01:09:34,880 --> 01:09:41,440
And I took the plan right off of the GitHub page for this protein, you know, diffusion model project,

760
01:09:42,000 --> 01:09:46,880
which basically said we want to create text to protein. So, you know, say or text to maybe it

761
01:09:46,880 --> 01:09:50,960
was more even general than that molecule or whatever. So, you know, you would be able to

762
01:09:50,960 --> 01:09:55,680
just specify in natural language, specifies kind of an odd word. Or, you know, to the best of your

763
01:09:55,680 --> 01:09:59,760
ability articulate in natural language what you're looking for into protein. And this thing would

764
01:09:59,760 --> 01:10:03,840
then, you know, generate it. And we are actually starting to see that there was a paper in nature

765
01:10:04,240 --> 01:10:08,720
not long ago, I'm hoping to do an episode with the authors that achieves that to a certain degree.

766
01:10:08,720 --> 01:10:14,560
But the what the AI what GPT for came back with in terms of hard science fiction about this scenario

767
01:10:15,600 --> 01:10:21,600
was, I think, first of all, just extremely funny because it basically ends up in a prompting war

768
01:10:21,600 --> 01:10:26,480
between the good guys and the bad guys and they're both like trying to out prompt each other. And so

769
01:10:26,480 --> 01:10:31,840
the you know, the kind of climactic scene is like the person prompting, you know, an AI to like

770
01:10:31,840 --> 01:10:36,480
make a protein or, you know, a molecule that will interfere with the bad guys molecule, but not

771
01:10:36,480 --> 01:10:42,240
harm any of the, you know, the humans or whatever. And it's just like both absurd, but also maybe

772
01:10:42,240 --> 01:10:49,040
not entirely absurd. You know, I mean, I am with you in that the I would order the risks the same

773
01:10:49,040 --> 01:10:55,040
way. You know, we already have chaos GBT. There are I recently read a research grant from a group

774
01:10:55,040 --> 01:10:59,600
proposing to study on the side all tendencies. Like there are people out there who want to kill

775
01:10:59,600 --> 01:11:06,000
everyone. Like what's up with that? And, you know, if the tools get more powerful, like that, you

776
01:11:06,000 --> 01:11:11,840
know, those people become even more problematic than they already are. So yes, I would put that at

777
01:11:11,840 --> 01:11:17,520
the top of the, you know, of the stack of like, big picture risks. And by the way, I take all the

778
01:11:17,520 --> 01:11:22,800
short term and medium term risks seriously too. Like this is a big tent show where like all your

779
01:11:22,800 --> 01:11:27,520
hopes, dreams and concerns and, you know, perhaps irrational fears like can all have a home. But I

780
01:11:27,520 --> 01:11:34,720
guess, you know, to sort of get to P doom zero, I still am like, I don't know, you know, all these

781
01:11:34,720 --> 01:11:39,360
individual crazy scenarios, sure, they're extremely unlikely, you know, the prompting war with your,

782
01:11:39,360 --> 01:11:45,360
you know, protein diffusion model is like absurd on the face of it. But I kind of think of like

783
01:11:45,360 --> 01:11:51,360
to taking the integral over that like vast space of crazy super unlikely scenarios. And then I'm

784
01:11:51,360 --> 01:11:56,720
kind of like, you know, there's so many of them right that space is so big. And even if the probability

785
01:11:56,800 --> 01:12:02,240
is like kind of vanishing, one thing you learn in calculus is like, you can, you know, that the

786
01:12:02,240 --> 01:12:08,080
integral can either also vanish or it can be like finite, you know, over these, you know, kind of,

787
01:12:08,080 --> 01:12:13,120
even if the function itself is going to zero, the integral doesn't necessarily have to go to zero

788
01:12:13,120 --> 01:12:19,440
over that space. So to me, that just feels like very unresolved still. And I don't think we're

789
01:12:19,440 --> 01:12:23,200
going to resolve that today. But I would love to hear a little bit more about your, how you think

790
01:12:23,280 --> 01:12:30,400
about AI agency, and also concepts of emergence in agents today, I guess I also wonder, like,

791
01:12:30,400 --> 01:12:35,040
is you.com gonna, you know, push more toward the agent direction, you've got like a, what I would

792
01:12:35,040 --> 01:12:41,120
call a research agent today, you've got a browser as well, I could, you know, should I start to

793
01:12:41,120 --> 01:12:48,080
expect it to take actions for me? What I've observed in the agent space is I never feel like it

794
01:12:48,080 --> 01:12:53,520
fails because it doesn't understand the goal or like doesn't stay on task. Let's say that never

795
01:12:53,520 --> 01:13:00,000
happens, but very rarely, much more often, it's just a failure of competence. So my expectation then

796
01:13:00,000 --> 01:13:04,880
is that like, as the competence improves, it may not be intrinsic agency, but it may be

797
01:13:05,760 --> 01:13:10,240
prompted agency, and it may even be like, you know, as we have more and more orchestrated systems,

798
01:13:10,240 --> 01:13:14,320
we may have models prompting other models to, you know, go off and do this. And

799
01:13:15,120 --> 01:13:18,560
it does feel like we've got, we're headed for like a lot of spinning plates. And

800
01:13:19,200 --> 01:13:23,360
the idea that they could kind of, you know, all come crashing down is like,

801
01:13:24,000 --> 01:13:30,000
that's doesn't just doesn't feel like something we can rule out. But I don't know, I can, can you

802
01:13:30,000 --> 01:13:34,880
help me be confident there? I'm still not. I'll go through the sound of the things you mentioned.

803
01:13:34,880 --> 01:13:40,640
So PDM equals zero. So you're right. As you integrate over the future, I would like to not

804
01:13:40,640 --> 01:13:46,720
rule out anything. So maybe I should say 10 to the minus whatever, like a time, time, number,

805
01:13:46,720 --> 01:13:54,960
because in the next five billion years, like all kinds of things happen, right? Like maybe as if

806
01:13:54,960 --> 01:14:00,880
like the three body problem, spoiler alert, like maybe some big, much more sophisticated alien

807
01:14:00,880 --> 01:14:05,680
species will come across. They have already developed, fasted in my time, travel, and,

808
01:14:05,680 --> 01:14:10,400
or, you know, just are really, really fast in getting here and various capacities. And then,

809
01:14:10,400 --> 01:14:15,200
you know, they have an AI and daddy, I will just like destroy all of us. So they're getting ready

810
01:14:15,200 --> 01:14:20,320
to like settle into the new planet before they get here. Like there's all kinds of crazy things

811
01:14:20,320 --> 01:14:26,480
that can happen. It's just that, like in terms of how much resources we should spend on T,

812
01:14:27,440 --> 01:14:33,600
like existential do versus like, you know, I'd say, yeah, I have a couple of researchers,

813
01:14:33,600 --> 01:14:40,160
like thinking of cool sci-fi scenarios, inspire us, like maybe like think about ways that that could

814
01:14:40,160 --> 01:14:47,120
be prevented, but to spend billions of dollars on it, to like, spend a lot of like, mind share,

815
01:14:47,120 --> 01:14:51,760
the public about it, who's already scared of any kind of technology. I mean, people are scared of

816
01:14:51,760 --> 01:14:57,520
vice. I mean, there's this great Twitter handle called the pessimist archive. I mean, people were

817
01:14:57,520 --> 01:15:02,400
scared and thought doom is happening because of the novels back in the day. People are like, all

818
01:15:02,400 --> 01:15:07,040
these kids, they're just in their heads reading novels. They're going to all be useless human

819
01:15:07,040 --> 01:15:12,800
beings in the future. Newspaper was terrible. Internet was terrible. Like there's so many

820
01:15:12,800 --> 01:15:17,840
things that like people thought this is the end of civilization and we're very pessimistic about.

821
01:15:17,840 --> 01:15:23,440
And again, not this diminishing like real, real concerns, but again, existential one,

822
01:15:23,440 --> 01:15:28,000
very, very likely given what we're seeing right now. And if there, it does happen at some point in

823
01:15:28,000 --> 01:15:35,440
the future, then I would argue that to think about the best countermeasures now is kind of like

824
01:15:36,240 --> 01:15:41,600
thinking about, you know, the best countermeasures against a computer going crazy when there's

825
01:15:41,600 --> 01:15:45,680
still a bunch of vacuum tubes and you like, well, we're going to just suck out the air of

826
01:15:45,680 --> 01:15:48,800
everything into the vacuum tubes. They're not going to work as well and more because they're

827
01:15:48,800 --> 01:15:53,680
going to break and blah, blah, blah. That was your like counterattack against the computer taking over

828
01:15:54,560 --> 01:16:00,080
with your current thinking of vacuum tube computers. Or it's like, you know, like it's

829
01:16:00,080 --> 01:16:04,000
similar to the internet, you know, if you thought about what's the internet going to be, how could

830
01:16:04,000 --> 01:16:12,480
it be so terrible? Zero of the TCT IP experts in the early ARPANET days realized that at some point,

831
01:16:12,480 --> 01:16:17,040
maybe a foreign power could interfere with local elections because like you can say whatever you

832
01:16:17,040 --> 01:16:23,120
want online and maybe people get followers in their social media. Like no one had victimized

833
01:16:23,120 --> 01:16:29,360
in the 70s and the early ARPANET days. And so I think most of the threat vectors are not that

834
01:16:30,000 --> 01:16:35,280
useful in terms of key doom kind of research. I have a couple of folks work on it, but not

835
01:16:35,280 --> 01:16:43,440
take up as much mind space and scare like late people and non-experts even more about the technology

836
01:16:43,440 --> 01:16:48,160
that even without consciousness is still going to have major destruction, right? If you're going

837
01:16:48,160 --> 01:16:54,800
through a new step function in human productivity, just like, you know, agriculture versus like

838
01:16:54,800 --> 01:16:59,920
hunting and gathering and the steam engine and electricity and internet, like this one's going

839
01:16:59,920 --> 01:17:06,080
to be even bigger. It's going to disrupt and change the job landscape a lot. I think at the end of it

840
01:17:06,080 --> 01:17:12,320
will be way more productive. There's going to be way more productivity per person and hence more

841
01:17:12,320 --> 01:17:18,320
wells and new jobs will come around as full jobs get all needed. But that is already so massively

842
01:17:18,320 --> 01:17:22,720
disruptive still. And it's not going to happen overnight either. People think, oh, it's going to

843
01:17:22,720 --> 01:17:26,640
be capacity. Yes, it will be faster, but still not overnight. There are still companies that aren't

844
01:17:26,640 --> 01:17:31,040
even on the cloud. There are some stretches in the United States and even Germany that don't have

845
01:17:31,040 --> 01:17:35,520
full internet connectivity, right? It's just like, so things will take time and not happen overnight,

846
01:17:35,520 --> 01:17:40,400
but they will be happening even faster than past past technological revolutions. And so,

847
01:17:40,960 --> 01:17:47,440
and then you have brought up LMS and proteins. There's a great example for where regulation

848
01:17:47,440 --> 01:17:51,440
makes sense. Like, basically, the concern here for those who are not familiar with proteins,

849
01:17:51,440 --> 01:17:55,760
having everything in life and disease and sickness, COVID, protein, like everything

850
01:17:55,760 --> 01:18:00,080
SARS broke through and like everything is governed by proteins. So if you have a great

851
01:18:00,080 --> 01:18:06,000
understanding of proteins, we can build fantastical, amazing things. Here's just one example of a

852
01:18:06,000 --> 01:18:10,000
research paper I read a few months ago that just blew my mind and made me very excited about

853
01:18:10,000 --> 01:18:16,800
the future. There's this group of researchers that built these carbon nanotubes. And on one side

854
01:18:16,800 --> 01:18:22,160
of the carbon nanotubes, they put iron molecules. And on the other side of these tiny, tiny carbon

855
01:18:22,160 --> 01:18:28,640
nanotubes, they put protein that would only bind to a brain cancer cell. And then they injected

856
01:18:28,640 --> 01:18:33,280
this fluid with all these little carbon nanotubes into a mouse brain that had brain cancer.

857
01:18:34,080 --> 01:18:40,560
The proteins found the brain tumor cells and only connected to those specific types of

858
01:18:40,560 --> 01:18:45,600
brain cancer cells. And then they put the mouse into a little magnetic field and there's the

859
01:18:45,600 --> 01:18:50,000
iron molecule on the other side of the carbon nanotube that started spinning around and had

860
01:18:50,000 --> 01:18:57,200
nano surgery on each brain cancer cell. Now, if you think about, we have the full console of the

861
01:18:57,200 --> 01:19:01,040
proteins, we can connect them to all kinds of things and you find ways to, you know, get rid of

862
01:19:01,120 --> 01:19:06,720
the carbon nanotubes afterwards. It's all like medicine is going to change in so many positive

863
01:19:06,720 --> 01:19:12,720
ways. And now you could argue, well, but proteins, people could use them and build like very bad

864
01:19:12,720 --> 01:19:18,000
like viruses. And like, that's true. And that can be outlawed. In fact, the US just a couple of

865
01:19:18,000 --> 01:19:24,080
months ago outlawed being a function where you know, some researchers want to make even more

866
01:19:24,080 --> 01:19:27,600
deadlier viruses. And it's not because they're like evil scientists who want to destroy the

867
01:19:27,600 --> 01:19:32,320
world. It's just they're saying like, well, as we know how they worked before they appear in

868
01:19:32,320 --> 01:19:37,040
nature by themselves, then we can all right now prevent like, develop cures for them. So, you

869
01:19:37,040 --> 01:19:41,520
know, it's like, it's a complex question, but yes, and decided like, for now, it's not worth it.

870
01:19:41,520 --> 01:19:47,680
Let's outlaw it. And likewise, I don't think an open source like protein model is going to be

871
01:19:47,680 --> 01:19:51,920
the main deciding factor of being able to create something virus. Because if you have all the

872
01:19:51,920 --> 01:19:58,240
wet lab experimentation to be able to create new kinds of viruses, you can also just do what

873
01:19:58,240 --> 01:20:02,560
census Arnold did when she won the Nobel Prize a couple years ago in chemistry, which is

874
01:20:02,560 --> 01:20:07,040
what she called directed evolution, but it was basically random permutations. And then

875
01:20:07,040 --> 01:20:12,720
running an experimental pipeline to see if that random permutation works better or not for particular

876
01:20:12,720 --> 01:20:18,000
kind of protein. And then you just keep iterating like that. And so if you have those capabilities,

877
01:20:18,080 --> 01:20:22,400
you have a random permutation, you can do bad things. But it turns out having like a legit

878
01:20:22,400 --> 01:20:27,360
weapon like that, that's to do all of that. So that was your PDUM integral, LM and proteins,

879
01:20:27,360 --> 01:20:31,840
AI agency emergence. So obviously, emerging capabilities are incredible on that sort of

880
01:20:31,840 --> 01:20:38,160
like, even us like working in deep learning, or I'm amazed, just like you.com, I asked these

881
01:20:38,160 --> 01:20:43,440
questions, I'm like, wow, actually, that's right, like, I would have not to like, thought this

882
01:20:43,520 --> 01:20:48,720
was possible. And sometimes we were like, did you program it specifically for it to be able to

883
01:20:48,720 --> 01:20:52,480
answer these kinds of questions about headphones or something? We're like, no, it's just like,

884
01:20:52,480 --> 01:20:58,000
just put that all together, just by trying to predict the next token. So I'm really excited.

885
01:20:58,000 --> 01:21:01,680
And one of the things I'm excited about this pudding, and one of the things that coding

886
01:21:01,680 --> 01:21:08,000
enables is now is the last part of your question, the actions. I think actions are clearly in the

887
01:21:08,000 --> 01:21:14,560
future. And for now, we're focused on amazing answers. But it's not hard to imagine that at

888
01:21:14,560 --> 01:21:21,600
some point, the most amazing answer is done. I did, I did what you asked to do. And instead of

889
01:21:21,600 --> 01:21:28,320
telling you how to do it, I just, right, you can build a really cool demo very quickly for these

890
01:21:28,320 --> 01:21:33,120
kinds of things. But the problem is like, as much as I love natural language, and as much as I love

891
01:21:33,120 --> 01:21:38,080
chatbots and everything, right, you have to find some really killer use cases for it. And

892
01:21:38,080 --> 01:21:42,960
to say, oh, I can book this flight, it's actually really hard to just, just book the flight. Like,

893
01:21:42,960 --> 01:21:47,040
you're like, why didn't you like pick this other one that was just like, not exactly the time I

894
01:21:47,040 --> 01:21:52,560
asked for it, but I could have waited for half an hour at this like, extra leg, and then like,

895
01:21:52,560 --> 01:21:58,800
say 50 bucks, like that was really dumb. And like, it turns out, Expedia and others have built

896
01:21:58,800 --> 01:22:05,360
for decades, the perfect interface for that problem, so that humans have all the installation

897
01:22:05,360 --> 01:22:11,200
right there in a visual way. And so there's an uncanny valley of, there's a cool tech demo.

898
01:22:11,760 --> 01:22:17,280
And on one side, and then there's like, my actual human assistant, who after months of

899
01:22:17,280 --> 01:22:22,000
I mean, like talking to me, understand all the trade offs, and understand my price

900
01:22:22,000 --> 01:22:27,200
sensitivity, or that of my company, and knows like, when I would preserve and like,

901
01:22:27,200 --> 01:22:33,520
all the like, reasons why I might do it overnight, like red-eye slides, and, you know, all the,

902
01:22:33,520 --> 01:22:37,840
all the constraints, and she can do it. And even then, sometimes she's like, oh, Richard,

903
01:22:37,840 --> 01:22:42,640
there are like three options here, like, let me know which one you prefer out of this 5000,

904
01:22:42,640 --> 01:22:47,680
if you built it for you. And like, it's very hard to do all of that with just text. That's

905
01:22:47,760 --> 01:22:52,240
ultimately, I think, part of why we have the stock ticker app and so on, and why we have

906
01:22:52,240 --> 01:22:59,040
religious now, and in some cases also, is that sometimes like UI UX and actual visually designed

907
01:22:59,040 --> 01:23:05,440
like interfaces are best used in combination with language. Maybe one more big picture question,

908
01:23:05,440 --> 01:23:09,600
and then I want to do just a real quick lightning round on a couple kind of more technical areas

909
01:23:09,600 --> 01:23:14,480
before we run out of time. On the big picture side, you know, we've got Sam Altman out there

910
01:23:14,480 --> 01:23:19,600
saying AGI is coming soon, but also kind of confusingly saying, but it'll be less impactful

911
01:23:19,600 --> 01:23:26,640
than you might think. Not really sure how to interpret that. The median guess on, you know,

912
01:23:26,640 --> 01:23:31,920
some definition of AGI is like, just a few years on some prediction markets and more like,

913
01:23:31,920 --> 01:23:38,080
you know, 12 years or whatever for a stronger definition. What do you have sort of an expectation

914
01:23:38,080 --> 01:23:43,600
for, and a definition or like a threshold that you have in mind of like, this is the threshold

915
01:23:43,600 --> 01:23:47,920
that really matters and, you know, loosely speaking, like what sort of timeline you would

916
01:23:47,920 --> 01:23:55,040
expect it to take to get there? It's very much a, the kind of question where you have to be very

917
01:23:55,040 --> 01:24:02,000
careful about your terminology, because the interpretation of AGI has vastly different

918
01:24:02,000 --> 01:24:08,080
associations. Like people, some people think of AGI and used to think of AGI as this super

919
01:24:08,080 --> 01:24:15,840
intelligence. It's conscious as self-awareness can set its own goals. And it is more intelligent

920
01:24:15,840 --> 01:24:22,080
than all human beings. And, and that's their depth. That was, that was for a long time. A lot of us,

921
01:24:22,080 --> 01:24:27,760
I thought, at least for me personally, also the definition. Now people said, and I think it's

922
01:24:27,760 --> 01:24:32,080
partially because of marketing, like, you know, you want to be working on AGI, but you also need

923
01:24:32,080 --> 01:24:36,560
to have ship stuff. It's like, you want to be multi-tenetary species, but you also need to just

924
01:24:36,560 --> 01:24:40,480
get a lot of stuff in orbit, right? Let have more satellites and better internet connectivity,

925
01:24:40,480 --> 01:24:46,400
and so on. So you have this long-term vision and the best companies are able to articulate that

926
01:24:46,400 --> 01:24:51,840
long-term vision and then revenue generated progress in smaller milestones towards it. And so

927
01:24:52,880 --> 01:24:59,280
in this case here, I think the definition of AGI was pulled out. And then the super intelligence

928
01:24:59,280 --> 01:25:04,880
was defined as like, okay, that's even more than general. It's super. And that's the really long-term

929
01:25:04,880 --> 01:25:11,760
stuff. And now AGI, it's just basically automating point. And if you define AGI, which I think is

930
01:25:11,760 --> 01:25:17,760
not crazy, I want to say maybe it would be not post that, which is a very pragmatic sort of

931
01:25:18,640 --> 01:25:26,240
investor slash financial slash economic definition of AGI, which is 80% of the jobs can be automated

932
01:25:26,960 --> 01:25:33,440
up to 80%. And if that's achieved, we'll call it AGI. Turns out there's just a lot of jobs that

933
01:25:33,520 --> 01:25:39,360
are quite repetitive and they're not requiring a ton of extremely novel, out-of-the-box thinking

934
01:25:39,360 --> 01:25:45,520
that no one's ever done before. And like learning very complex new behaviors, bot-shaking, identifying

935
01:25:45,520 --> 01:25:50,960
new experiments, collecting new data and pushing, you know, the science forward and so on. It turns

936
01:25:50,960 --> 01:25:56,400
out there's just a lot of boring, repetitive jobs. And indeed, if your definition of AGI is just like,

937
01:25:56,400 --> 01:26:02,720
well, we can automate like 80% of 80% of the jobs, then I think it's not crazy to assume

938
01:26:03,520 --> 01:26:07,440
especially I would restrict it one on the wall way, which is digitized jobs,

939
01:26:08,160 --> 01:26:13,040
jobs that are purely happening in your browser or on your computer, because those jobs can collect

940
01:26:13,040 --> 01:26:17,680
training data at massive scales. Turns out no one's collecting training data for plumbers,

941
01:26:18,240 --> 01:26:25,520
for woofers, for tylers, for maids, like cleaning tees or any of that. And so none of those jobs

942
01:26:25,520 --> 01:26:29,840
are going to get automated anytime soon. Because you first have to collect many years of that such

943
01:26:29,840 --> 01:26:35,920
training data before you can then use AI to train on that and then automate it. But, you know,

944
01:26:35,920 --> 01:26:41,520
jobs that are fully digitized and that have a lot of training data that don't have a crazy long tail

945
01:26:41,520 --> 01:26:47,200
of special cases, they're going to get automated. And I think that's reasonable to say that's 80%

946
01:26:47,200 --> 01:26:52,400
of jobs. For hunches, even in radiology, for instance, you could probably do 80% find 80%

947
01:26:52,400 --> 01:26:57,440
of things that are wrong in HET's t-stand. But then there's still this very long tail of 20%

948
01:26:58,400 --> 01:27:02,480
that you just don't have enough training data for. Radiologists never see it in their lifetime,

949
01:27:02,480 --> 01:27:08,000
they just read about it once in a book. And we're still not quite good enough of one shot and zero

950
01:27:08,000 --> 01:27:13,920
shot learning. Obviously, huge amounts of progress, but not in super important things like radiology,

951
01:27:13,920 --> 01:27:18,560
where you just read about a case once in a book and then identify it with 100% accuracy,

952
01:27:18,560 --> 01:27:23,040
which is also a question of whether humans do it. I'm actually with you on the self-driving car.

953
01:27:23,120 --> 01:27:27,760
There's going to be a lot of interesting questions as AGI rolls into more and more workplaces,

954
01:27:27,760 --> 01:27:33,840
which is how much better than a human that has to be. And how, and it's deeply philosophical,

955
01:27:33,840 --> 01:27:40,400
very quickly, because if you're purely utilitarian, you could say, well, you know, 100,000 miles or

956
01:27:40,400 --> 01:27:47,120
whatever 20 million miles driven by AI results in 10,000 deaths. And the same amount of miles

957
01:27:47,120 --> 01:27:53,680
driven by humans results in five times more deaths. And so one is better than the odds.

958
01:27:54,640 --> 01:27:59,680
But if that one dead person in the AI car was your daughter, you don't care, you're gonna,

959
01:27:59,680 --> 01:28:04,160
like in the US, you're gonna sue, you're gonna, you know, try to end that company,

960
01:28:04,160 --> 01:28:08,640
because they're responsible now for the death of your child. And like, it's a very emotional thing,

961
01:28:08,640 --> 01:28:14,000
not a statistical thing anymore. And so there's gonna be a lot of litigation as those come out.

962
01:28:14,000 --> 01:28:18,400
And I think the silver lining is again, of course, as the I meets the state, you can learn from it

963
01:28:18,400 --> 01:28:23,920
versus like one person texting again on their cell phone, which is already illegal and running

964
01:28:23,920 --> 01:28:28,320
like over some kid that ran out, like, they're going too fast also, which is already illegal,

965
01:28:28,320 --> 01:28:33,920
too, you can't really do that much more than needing it legal. AGI will have a huge amount of

966
01:28:33,920 --> 01:28:38,560
impact. Once it's just like, okay, repetitive jobs, get like two large degree automated,

967
01:28:38,560 --> 01:28:43,200
and I'm with the people saying that will happen next few years. When it comes to like,

968
01:28:43,280 --> 01:28:48,480
super intelligence, that is fully conscious and can do all the things. And one intelligent,

969
01:28:48,480 --> 01:28:52,480
then not just a single human, but that all of humanity, very hard to know, because no one's

970
01:28:52,480 --> 01:28:58,800
working on it and making sort of progress along the lines of setting my own goals. And again,

971
01:28:58,800 --> 01:29:05,040
like, unless you set your own goals, I don't know if I would achieve full on super intelligence to

972
01:29:05,040 --> 01:29:09,680
you. Like if you're just your objective function is to minimize cross entropy errors, or reduce

973
01:29:09,680 --> 01:29:15,360
the plexity, or like segment, which is well, or like, none of that, I wanted to reach.

974
01:29:16,240 --> 01:29:18,560
Do you have time for a lightning round? Or do we need to leave it there?

975
01:29:18,560 --> 01:29:26,160
Let's try to be lightning rounds. All right. Thinking also about retrieval, memory, and online

976
01:29:26,160 --> 01:29:33,440
learning as kind of three frontiers that, you know, you dot com could could improve on if they're,

977
01:29:33,440 --> 01:29:38,240
you know, if there are research breakthroughs, but also these do seem to be kind of ingredients

978
01:29:38,320 --> 01:29:44,320
toward this bigger picture of AGI or even, you know, at some point, ASI, I guess I'm, you know,

979
01:29:44,320 --> 01:29:51,040
maybe just leave it open ended, like, what are you excited about in those domains? Are there

980
01:29:51,040 --> 01:29:54,480
research directions? Are there, you know, are there papers you've already seen or things you

981
01:29:54,480 --> 01:30:00,880
think people should be doing that you think will kind of provide meaningful unlocks as we find,

982
01:30:00,880 --> 01:30:06,000
you know, new and better ways to do those things? Yeah. So I'm a fan of all three, of course,

983
01:30:06,000 --> 01:30:09,760
I'll try to keep it short. Retrieval is awesome. I think in some ways, short-term

984
01:30:09,760 --> 01:30:13,600
memory is currently in the front, retrieval is in the, you know, rag. If you go up method

985
01:30:13,600 --> 01:30:18,160
generation, we do it over a web, we let you up those files now, so you wouldn't do it over,

986
01:30:18,160 --> 01:30:24,640
over a file. And then we have the smart personalization that actually is online learning. So as you say,

987
01:30:24,640 --> 01:30:30,160
certain things like it will, it will remember them about you. And then, you know, you can turn

988
01:30:30,160 --> 01:30:35,760
it off also. And it's very transparent. And the whole thing off or the automated smart learning

989
01:30:35,840 --> 01:30:40,640
without you, if you don't want it. But yeah, I think that's sort of a simple sort of pragmatic

990
01:30:40,640 --> 01:30:45,680
way of online learning. I think ultimately, you know, it'll be awesome to have AI systems get

991
01:30:45,680 --> 01:30:50,240
better and better of just adapting right away to user feedback, both in terms of, you know,

992
01:30:50,240 --> 01:30:54,880
thumbs up, thumbs down kinds of clicking, but also in conversation, I didn't like that answer.

993
01:30:54,880 --> 01:31:00,320
And then updating the answer in a principled way for the future. I have so many more thoughts,

994
01:31:00,320 --> 01:31:05,360
but I'll like, I'd love to do a second one. These are kind of crazy days. Now the Apple

995
01:31:05,360 --> 01:31:11,760
announcement, we just announced that Julianne, CTO, I mean, say also just became an angel investor

996
01:31:11,760 --> 01:31:17,200
and a lot of exciting stuff happening. So I yeah, well, congratulations on the Apple thing and also

997
01:31:17,200 --> 01:31:23,280
on a new prominent angel investor. And really some fantastic product progress. I definitely

998
01:31:23,280 --> 01:31:29,280
recommend everybody to try out particularly genius mode and research mode. And I think if you do that,

999
01:31:29,280 --> 01:31:34,720
you will be coming back to you.com more and more often. So keep up the great work. For now,

1000
01:31:34,720 --> 01:31:39,760
I will say Richard Sosher, founder and CEO of you.com. Thank you for being part of the cognitive

1001
01:31:39,760 --> 01:31:45,280
revolution. Thank you so much. It is both energizing and enlightening to hear why people

1002
01:31:45,280 --> 01:31:50,960
listen and learn what they value about the show. So please don't hesitate to reach out via email

1003
01:31:50,960 --> 01:31:56,720
at TCR at turpentine.co, or you can DM me on the social media platform of your choice.

1004
01:31:57,680 --> 01:32:03,760
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually

1005
01:32:03,760 --> 01:32:09,200
work customized across all platforms with a click of a button. I believe in Omniki so much

1006
01:32:09,200 --> 01:32:18,960
that I invested in it and I recommend you use it too. Use Kogrev to get a 10% discount.

