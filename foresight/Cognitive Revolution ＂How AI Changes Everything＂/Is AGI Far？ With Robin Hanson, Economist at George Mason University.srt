1
00:00:00,000 --> 00:00:02,560
We're on this upward growth trajectory.

2
00:00:02,560 --> 00:00:06,120
We have the potential to taking a big chunk of the universe

3
00:00:06,120 --> 00:00:07,640
and doing things with it.

4
00:00:07,640 --> 00:00:09,440
And I'm excited by that potential.

5
00:00:09,440 --> 00:00:11,680
So I want us to keep growing.

6
00:00:11,680 --> 00:00:15,200
And I see how much we've changed to get to where we are.

7
00:00:15,200 --> 00:00:17,600
My book, Age of M, is about brain emulations.

8
00:00:17,600 --> 00:00:20,200
So that's where you take a particular human brain

9
00:00:20,200 --> 00:00:22,560
and you scan it and find spatial chemical detail

10
00:00:22,560 --> 00:00:26,080
where you fill in for each cell a computer model of that cell.

11
00:00:26,080 --> 00:00:27,720
And if you've got good enough models for cells

12
00:00:27,720 --> 00:00:29,760
and a good map of the brain, then basically

13
00:00:29,760 --> 00:00:31,800
the IO of this model should be the same

14
00:00:31,800 --> 00:00:33,760
as the IO of the original brain.

15
00:00:33,760 --> 00:00:37,200
If we can get full human level AI in the next 16 to 90 years

16
00:00:37,200 --> 00:00:39,480
with the progress, then this population decline

17
00:00:39,480 --> 00:00:41,440
won't matter so much because we will basically

18
00:00:41,440 --> 00:00:43,400
have AIs take over most of the jobs

19
00:00:43,400 --> 00:00:46,560
and then that can allow the world economy to keep growing.

20
00:00:46,560 --> 00:00:48,920
Hello and welcome to the Cognitive Revolution,

21
00:00:48,920 --> 00:00:51,880
where we interview visionary researchers, entrepreneurs

22
00:00:51,880 --> 00:00:53,840
and builders working on the frontier

23
00:00:53,840 --> 00:00:55,760
of artificial intelligence.

24
00:00:55,760 --> 00:00:58,560
Each week, we'll explore their revolutionary ideas

25
00:00:58,680 --> 00:01:00,080
and together we'll build a picture

26
00:01:00,080 --> 00:01:03,640
of how AI technology will transform work, life

27
00:01:03,640 --> 00:01:05,800
and society in the coming years.

28
00:01:05,800 --> 00:01:09,400
I'm Nathan LaBenz, joined by my co-host Eric Torenberg.

29
00:01:09,400 --> 00:01:12,280
Hello and welcome back to the Cognitive Revolution.

30
00:01:12,280 --> 00:01:13,920
My guest today is Robin Hansen,

31
00:01:13,920 --> 00:01:16,320
Professor of Economics at George Mason University

32
00:01:16,320 --> 00:01:18,880
and author of the blog, Overcoming Bias,

33
00:01:18,880 --> 00:01:20,560
where Robin has published consistently

34
00:01:20,560 --> 00:01:23,880
on a wide range of topics since 2006

35
00:01:23,880 --> 00:01:26,320
and where Eliezer Yudkowski published early versions

36
00:01:26,360 --> 00:01:29,440
of what has become some of his most influential writing

37
00:01:29,440 --> 00:01:30,280
on AI.

38
00:01:31,840 --> 00:01:33,560
Robin is an undeniable polymath

39
00:01:33,560 --> 00:01:36,920
whose approach to futurism is unusually non-romantic.

40
00:01:38,000 --> 00:01:40,520
Rather than trying to identify value buddies,

41
00:01:40,520 --> 00:01:43,080
Robin aims to apply first principles thinking

42
00:01:43,080 --> 00:01:46,560
to the future and to describe what is likely to happen

43
00:01:46,560 --> 00:01:48,280
without claiming that you should feel

44
00:01:48,280 --> 00:01:50,000
any particular way about it.

45
00:01:51,360 --> 00:01:53,160
I set this conversation up late last year

46
00:01:53,160 --> 00:01:54,640
after my deep dive into the new

47
00:01:54,640 --> 00:01:57,320
Mamba states-based model architecture.

48
00:01:57,320 --> 00:02:00,720
Because Robin's 2016 book, The Age of M,

49
00:02:00,720 --> 00:02:03,280
which analyzes a scenario in which human emulations

50
00:02:03,280 --> 00:02:05,160
can be run on computers,

51
00:02:05,160 --> 00:02:07,280
suddenly seemed a lot more relevant.

52
00:02:08,240 --> 00:02:11,640
My plan originally was to consider how his analysis

53
00:02:11,640 --> 00:02:14,720
from The Age of M would compare to similar analyses

54
00:02:14,720 --> 00:02:17,240
for a hypothetical age of LLMs

55
00:02:17,240 --> 00:02:19,640
or perhaps even an age of SSMs.

56
00:02:20,560 --> 00:02:22,680
In practice, we ended up doing some of that,

57
00:02:22,680 --> 00:02:24,760
but for the most part took a different direction

58
00:02:24,760 --> 00:02:27,080
as it became clear early on in the conversation

59
00:02:27,080 --> 00:02:31,000
that Robin was not buying some of my core premises.

60
00:02:31,000 --> 00:02:33,200
Taking the outside view as he's famous for doing

61
00:02:33,200 --> 00:02:35,840
and noting that AI experts have repeatedly thought

62
00:02:35,840 --> 00:02:38,440
that they were close to AGI in the past,

63
00:02:38,440 --> 00:02:41,000
Robin questions whether this time really is different

64
00:02:41,000 --> 00:02:43,040
and doubts whether we are really close

65
00:02:43,040 --> 00:02:44,960
to transformative AI at all.

66
00:02:46,240 --> 00:02:48,840
This perspective naturally challenged my worldview

67
00:02:48,840 --> 00:02:50,960
and I listened back to this conversation in full

68
00:02:50,960 --> 00:02:53,280
to make sure that I wasn't missing anything important

69
00:02:53,280 --> 00:02:55,000
before writing this introduction.

70
00:02:56,360 --> 00:02:59,200
Ultimately, I do remain quite firmly convinced

71
00:02:59,200 --> 00:03:01,200
that today's AIs are powerful enough

72
00:03:01,200 --> 00:03:03,280
to drive economic transformation.

73
00:03:03,280 --> 00:03:06,200
And I would cite the release of Google's Gemini 1.5,

74
00:03:06,200 --> 00:03:07,960
which happened in just the few short weeks

75
00:03:07,960 --> 00:03:10,280
between recording and publishing this episode

76
00:03:10,280 --> 00:03:13,080
as evidence that progress is not yet slowing down.

77
00:03:14,200 --> 00:03:15,440
Yet at the same time,

78
00:03:15,440 --> 00:03:17,840
Robin did get me thinking more about the disconnect

79
00:03:17,840 --> 00:03:19,360
between feasibility

80
00:03:19,360 --> 00:03:22,920
and actual widespread implementation and automation.

81
00:03:23,960 --> 00:03:26,720
Beyond the question of what AI systems can do,

82
00:03:26,720 --> 00:03:29,640
there are also questions of legal regulation, of course,

83
00:03:29,640 --> 00:03:31,400
and perhaps even more importantly,

84
00:03:31,400 --> 00:03:34,840
just how eager people are to use AI tools in the first place.

85
00:03:36,040 --> 00:03:38,440
When Robin reported that his son's software firm

86
00:03:38,440 --> 00:03:41,480
had recently determined that LLMs were not useful

87
00:03:41,480 --> 00:03:43,480
for routine application development,

88
00:03:43,480 --> 00:03:45,200
I was honestly kind of shocked

89
00:03:45,200 --> 00:03:46,320
because if nothing else,

90
00:03:46,320 --> 00:03:48,080
I'm extremely confident about the degree

91
00:03:48,080 --> 00:03:50,920
to which LLMs accelerate my own programming work.

92
00:03:52,000 --> 00:03:54,560
Since then, though, I have heard a couple of other stories,

93
00:03:54,560 --> 00:03:56,080
which combined with Robbins,

94
00:03:56,080 --> 00:03:58,360
helped me develop, I think, a bit better theory

95
00:03:58,360 --> 00:04:00,160
of what's going on.

96
00:04:00,160 --> 00:04:04,320
First, an AI educator told me that failure to form new habits

97
00:04:04,320 --> 00:04:08,240
is the most common cause of failure with AI in general.

98
00:04:08,240 --> 00:04:12,000
In his courses, he emphasizes hands-on exercises

99
00:04:12,000 --> 00:04:14,200
because he's learned that simple awareness

100
00:04:14,240 --> 00:04:18,360
of AI capabilities does not lead to human behavioral change.

101
00:04:19,360 --> 00:04:21,480
Second, a friend told me that his company

102
00:04:21,480 --> 00:04:23,760
hosted a Microsoft GitHub salesperson

103
00:04:23,760 --> 00:04:25,280
for a lunch hour demo,

104
00:04:25,280 --> 00:04:27,640
and it turned out that one of their own team members

105
00:04:27,640 --> 00:04:30,440
had far more knowledge about GitHub Co-Pilot

106
00:04:30,440 --> 00:04:31,760
than the rep himself did.

107
00:04:32,880 --> 00:04:34,680
If Microsoft sales reps are struggling

108
00:04:34,680 --> 00:04:36,880
to keep up with Co-Pilot's capabilities,

109
00:04:36,880 --> 00:04:38,880
we should perhaps adjust our expectations

110
00:04:38,880 --> 00:04:40,280
for the rest of the economy.

111
00:04:41,400 --> 00:04:43,040
And third, in my own experience,

112
00:04:43,040 --> 00:04:46,200
helping people address process bottlenecks with AI,

113
00:04:46,200 --> 00:04:48,760
I've repeatedly seen how unnatural it can be

114
00:04:48,760 --> 00:04:50,880
for people to break their own work down

115
00:04:50,880 --> 00:04:52,640
into the sort of discrete tasks

116
00:04:52,640 --> 00:04:55,720
that LLMs can handle effectively today.

117
00:04:55,720 --> 00:04:58,040
Most people were never trained to think this way,

118
00:04:58,040 --> 00:04:59,280
and it's going to take time

119
00:04:59,280 --> 00:05:02,080
before it becomes common practice across the economy.

120
00:05:03,480 --> 00:05:06,360
All this means that change may be slower to materialize

121
00:05:06,360 --> 00:05:09,880
than those of us on the frontiers of AI adoption might expect.

122
00:05:09,880 --> 00:05:11,960
And while that does suggest more of an opportunity

123
00:05:11,960 --> 00:05:15,160
and indeed advantage for us in the meantime,

124
00:05:15,160 --> 00:05:18,040
on balance, I do have to view it as a negative sign

125
00:05:18,040 --> 00:05:21,280
about our preparedness and our ability to adapt overall.

126
00:05:22,600 --> 00:05:23,840
Regardless of your views,

127
00:05:23,840 --> 00:05:25,360
and I do suspect that most listeners

128
00:05:25,360 --> 00:05:28,520
will find themselves agreeing with me more than with Robin,

129
00:05:28,520 --> 00:05:30,600
his insights are always thought-provoking,

130
00:05:30,600 --> 00:05:32,760
and I think you'll find it very well worthwhile

131
00:05:32,760 --> 00:05:34,120
to engage with the challenges

132
00:05:34,120 --> 00:05:36,000
that he presents in this conversation.

133
00:05:37,440 --> 00:05:38,960
As always, if you're finding value in the show,

134
00:05:38,960 --> 00:05:41,080
we would appreciate it if you'd share it with friends,

135
00:05:41,080 --> 00:05:43,600
post a review on Apple Podcasts or Spotify,

136
00:05:43,600 --> 00:05:46,120
or just leave a comment on YouTube.

137
00:05:46,120 --> 00:05:48,360
And of course, I always love to hear from listeners,

138
00:05:48,360 --> 00:05:50,560
so please don't hesitate to DM me

139
00:05:50,560 --> 00:05:53,680
on the social media platform of your choice.

140
00:05:53,680 --> 00:05:55,640
Now, I hope you enjoy this conversation

141
00:05:55,640 --> 00:05:57,760
with Professor Robin Hansen.

142
00:05:58,880 --> 00:06:01,760
Robin Hansen, Professor of Economics at George Mason University

143
00:06:01,760 --> 00:06:04,920
and noted polymath, welcome to the cognitive revolution.

144
00:06:04,920 --> 00:06:06,360
Nice to meet you, Nathan.

145
00:06:06,360 --> 00:06:07,280
Let's talk.

146
00:06:07,280 --> 00:06:08,280
I'm excited about this.

147
00:06:08,320 --> 00:06:11,240
So I have followed your work for a long time.

148
00:06:11,240 --> 00:06:15,440
It's super wide-ranging and always very interesting.

149
00:06:15,440 --> 00:06:17,680
People can find your thoughts on just about everything,

150
00:06:17,680 --> 00:06:20,440
I think, over the years on overcoming bias, your blog.

151
00:06:20,440 --> 00:06:23,440
But today, I wanted to revisit what I think

152
00:06:23,440 --> 00:06:24,800
is one of your destined to be,

153
00:06:24,800 --> 00:06:26,360
perhaps one of your most influential works,

154
00:06:26,360 --> 00:06:28,800
which is the book, The Age of M,

155
00:06:28,800 --> 00:06:33,200
which came out in 2016 and Envisions a Future,

156
00:06:33,200 --> 00:06:37,120
which basically amounts to putting humans on machines,

157
00:06:37,120 --> 00:06:39,520
and we can unpack that in more detail,

158
00:06:39,520 --> 00:06:44,520
and then explores that in a ton of different directions.

159
00:06:44,920 --> 00:06:47,600
Where we actually are now as we enter into 2024

160
00:06:47,600 --> 00:06:50,040
is not exactly that, certainly,

161
00:06:50,040 --> 00:06:51,840
but I've come to believe recently

162
00:06:51,840 --> 00:06:54,720
that it's maybe bending back a little bit more toward that,

163
00:06:54,720 --> 00:06:57,040
certainly more than my expectations a year ago.

164
00:06:57,040 --> 00:06:59,320
So I've revisited the book,

165
00:06:59,320 --> 00:07:01,400
and I'm excited to bring a bunch of questions

166
00:07:01,400 --> 00:07:03,760
and kind of compare and contrast your scenario

167
00:07:03,760 --> 00:07:05,640
versus the current scenario

168
00:07:05,640 --> 00:07:07,960
that we seem to be evolving into.

169
00:07:07,960 --> 00:07:09,600
Okay, let's do it.

170
00:07:09,600 --> 00:07:12,480
One big theme of your work always, I think,

171
00:07:12,480 --> 00:07:16,520
is that we live in this strange dream time,

172
00:07:16,520 --> 00:07:21,040
and that our reality as modern humans is quite different

173
00:07:21,040 --> 00:07:23,200
than the reality of those that came before us

174
00:07:23,200 --> 00:07:24,760
and likely those that will come after us

175
00:07:24,760 --> 00:07:27,080
for some pretty fundamental reasons.

176
00:07:27,080 --> 00:07:28,240
Do you wanna just sketch out

177
00:07:28,240 --> 00:07:29,680
your kind of big picture argument

178
00:07:29,680 --> 00:07:32,240
that our times are exceptional

179
00:07:32,240 --> 00:07:35,360
and not likely to go on like this forever?

180
00:07:35,400 --> 00:07:37,000
The first thing to notice is that

181
00:07:37,000 --> 00:07:38,680
we were in a period of very rapid growth,

182
00:07:38,680 --> 00:07:40,400
very rapid change,

183
00:07:40,400 --> 00:07:42,920
which just can't continue for very long

184
00:07:42,920 --> 00:07:44,320
on a cosmological time scale.

185
00:07:44,320 --> 00:07:47,680
10,000 years would be way longer than it could manage,

186
00:07:47,680 --> 00:07:50,400
and therefore we're gonna have to go back

187
00:07:50,400 --> 00:07:52,880
to a period of slower change,

188
00:07:52,880 --> 00:07:54,880
and plausibly then a period of slower change

189
00:07:54,880 --> 00:07:58,640
will be a period where population can grow faster

190
00:07:58,640 --> 00:08:03,640
relative to the growth rate of the economy in the universe,

191
00:08:04,240 --> 00:08:05,600
and therefore we will move back

192
00:08:05,600 --> 00:08:08,280
more toward a Malthusian world

193
00:08:08,280 --> 00:08:10,160
if competition remains,

194
00:08:10,160 --> 00:08:12,120
such as almost all our ancestors were

195
00:08:12,120 --> 00:08:14,040
until a few hundred years ago.

196
00:08:14,040 --> 00:08:17,160
So we're in this unusual period of being rich

197
00:08:18,800 --> 00:08:21,760
per person and in very rapid change,

198
00:08:22,880 --> 00:08:25,920
and also sort of globally integrated.

199
00:08:25,920 --> 00:08:29,400
That is, our distant ancestors were fragmented culturally

200
00:08:29,400 --> 00:08:30,560
across the globe,

201
00:08:30,560 --> 00:08:33,880
and each talk to a small group of people near them,

202
00:08:33,880 --> 00:08:35,720
and our distant descendants will be fragmented

203
00:08:35,720 --> 00:08:37,240
across the universe,

204
00:08:37,240 --> 00:08:39,960
and they won't be able to talk all across the universe

205
00:08:39,960 --> 00:08:41,240
instantaneously.

206
00:08:41,240 --> 00:08:44,840
So future culture and past culture were both very fragmented,

207
00:08:44,840 --> 00:08:47,640
and we were in a period where our entire civilization

208
00:08:47,640 --> 00:08:50,040
can talk rapidly to each other.

209
00:08:50,040 --> 00:08:52,720
The time delay of communication is very small

210
00:08:52,720 --> 00:08:54,440
compared to the doubling time

211
00:08:54,440 --> 00:08:56,320
of our very rapid growth economy.

212
00:08:56,320 --> 00:08:59,200
So we are now an integrated civilization

213
00:08:59,200 --> 00:09:03,120
where rich growing very fast,

214
00:09:03,120 --> 00:09:05,560
and there's a number of consequences being rich,

215
00:09:05,560 --> 00:09:08,120
which is that we don't have to pay

216
00:09:08,120 --> 00:09:11,040
that much attention to functionality.

217
00:09:11,040 --> 00:09:14,160
Those were not pressured to do what it takes to survive

218
00:09:14,160 --> 00:09:17,200
in the way our ancestors and our descendants would be.

219
00:09:17,200 --> 00:09:20,360
So we can indulge our delusions,

220
00:09:20,360 --> 00:09:23,720
or whatever other inclinations we have,

221
00:09:23,720 --> 00:09:26,280
they aren't disciplined very rapidly

222
00:09:26,320 --> 00:09:29,600
by survival and functionality.

223
00:09:29,600 --> 00:09:31,960
That makes us a dream team.

224
00:09:31,960 --> 00:09:34,000
That is, our dreams drive us.

225
00:09:34,960 --> 00:09:38,840
Our abstract thoughts, our vague impressions,

226
00:09:38,840 --> 00:09:42,160
our emotions, our visions.

227
00:09:42,160 --> 00:09:47,160
We do things that are dramatic and exciting and meaningful

228
00:09:48,880 --> 00:09:53,880
in our view, according to this dream time mind we have,

229
00:09:54,320 --> 00:09:56,160
which isn't, again, that disciplined

230
00:09:56,160 --> 00:09:57,520
by functionality, that is,

231
00:09:57,520 --> 00:10:00,920
the mind we inherited from our distant ancestors,

232
00:10:00,920 --> 00:10:03,080
it was functional there, it was disciplined there,

233
00:10:03,080 --> 00:10:04,320
we're in a very different world,

234
00:10:04,320 --> 00:10:09,120
but our mind hasn't changed to be functional in this world.

235
00:10:09,120 --> 00:10:14,120
And so we are expressing this momentum

236
00:10:14,120 --> 00:10:17,400
of what we used to be in this strange new world.

237
00:10:17,400 --> 00:10:18,920
That's the dream time.

238
00:10:18,920 --> 00:10:21,440
So let me just try to rephrase that

239
00:10:21,440 --> 00:10:22,720
or frame it slightly differently

240
00:10:22,720 --> 00:10:24,880
and tell them if you agree with this framing.

241
00:10:24,880 --> 00:10:27,040
I would maybe interpret it as,

242
00:10:27,040 --> 00:10:29,800
we're maybe in a punctuated equilibrium sort of situation

243
00:10:29,800 --> 00:10:33,040
where we're in the transition from one equilibrium

244
00:10:33,040 --> 00:10:35,320
to another, there have probably been

245
00:10:35,320 --> 00:10:36,880
however many of these through history,

246
00:10:36,880 --> 00:10:38,440
not like a huge number, but a decent number,

247
00:10:38,440 --> 00:10:42,000
I think of such phrases as the Cambrian explosion,

248
00:10:42,000 --> 00:10:44,640
perhaps as another dream time.

249
00:10:44,640 --> 00:10:48,920
These moments happen when some external shock

250
00:10:48,920 --> 00:10:50,880
happens to the system, whether that's like an asteroid

251
00:10:50,880 --> 00:10:52,040
that takes out a lot of life,

252
00:10:52,040 --> 00:10:54,920
or human brains come on the scene,

253
00:10:54,920 --> 00:10:59,400
and there's a period in which the normal constraints

254
00:10:59,400 --> 00:11:02,960
are temporarily relaxed, but then in the long term,

255
00:11:02,960 --> 00:11:04,920
there's just like no escaping the logic

256
00:11:04,920 --> 00:11:05,960
of natural selection.

257
00:11:05,960 --> 00:11:08,560
Is that basically the framework?

258
00:11:08,560 --> 00:11:12,000
So your analogy of the Cambrian explosion could be,

259
00:11:12,000 --> 00:11:13,800
we discovered multicellularity,

260
00:11:13,800 --> 00:11:16,480
we discovered being able to make large animals,

261
00:11:16,480 --> 00:11:18,000
and that was happened at a moment,

262
00:11:18,000 --> 00:11:20,000
there was the moment of multicellularity,

263
00:11:20,000 --> 00:11:22,680
and then evolution took time to adapt

264
00:11:22,680 --> 00:11:25,040
to that new opportunity,

265
00:11:25,040 --> 00:11:28,080
and the Cambrian explosion is the period of adaptation,

266
00:11:28,080 --> 00:11:29,640
then after the Cambrian explosion,

267
00:11:29,640 --> 00:11:31,960
we've adapted to that new opportunity,

268
00:11:31,960 --> 00:11:33,440
and then we're more in a stasis,

269
00:11:33,440 --> 00:11:35,880
and then you're imagining this period of adaptation

270
00:11:35,880 --> 00:11:36,880
to a sudden change.

271
00:11:37,760 --> 00:11:42,760
But for humans today, we keep having sudden changes,

272
00:11:43,200 --> 00:11:45,040
and they keep coming fast,

273
00:11:45,040 --> 00:11:48,040
and so there wasn't this one thing that happened

274
00:11:48,040 --> 00:11:49,720
300 years ago or 10,000 years ago

275
00:11:49,720 --> 00:11:51,120
that we're slowly adapting to.

276
00:11:51,120 --> 00:11:53,560
We keep having more big changes

277
00:11:53,560 --> 00:11:55,720
that keep changing the landscape

278
00:11:55,720 --> 00:11:57,720
of what it is to adapt to,

279
00:11:57,720 --> 00:12:02,240
so we won't see this slow adaptation to the new thing

280
00:12:02,240 --> 00:12:04,000
until we get a stable new thing,

281
00:12:04,000 --> 00:12:05,080
which we haven't gotten yet.

282
00:12:05,080 --> 00:12:06,840
We, things keep changing.

283
00:12:06,840 --> 00:12:08,800
I wanna maybe circle back in a minute to

284
00:12:08,800 --> 00:12:11,040
what would be the conditions

285
00:12:11,040 --> 00:12:12,360
under which things would restabilize.

286
00:12:12,360 --> 00:12:15,040
I think I guess the M scenario is one of them,

287
00:12:15,040 --> 00:12:17,720
but there may be others that might even be

288
00:12:17,720 --> 00:12:19,160
more imminent at this point.

289
00:12:19,840 --> 00:12:20,680
Before doing that,

290
00:12:20,680 --> 00:12:22,400
I just wanted to touch on another big theme of your work,

291
00:12:22,400 --> 00:12:25,240
which is, and I really appreciate how you introduced

292
00:12:25,240 --> 00:12:29,400
the book this way with the idea that

293
00:12:29,400 --> 00:12:31,240
I'm just trying to figure out

294
00:12:31,240 --> 00:12:33,800
what is likely to happen in this scenario.

295
00:12:33,800 --> 00:12:35,680
I'm not telling you you should like it.

296
00:12:35,680 --> 00:12:37,200
I'm not telling you you should dislike it.

297
00:12:37,200 --> 00:12:38,480
I'm not trying to judge it.

298
00:12:38,480 --> 00:12:41,920
I'm just trying to extrapolate from a scenario

299
00:12:41,920 --> 00:12:44,920
using the tools of science and social science

300
00:12:44,920 --> 00:12:47,200
to try to figure out what might happen.

301
00:12:47,200 --> 00:12:49,520
I love that, and I try to do something similar

302
00:12:49,520 --> 00:12:52,240
with this show around understanding AI.

303
00:12:52,240 --> 00:12:55,600
I think there's so much emotional valence

304
00:12:55,600 --> 00:12:57,720
brought to so many parts of the discussion,

305
00:12:57,720 --> 00:13:01,440
and I always say, we need to first figure out what is,

306
00:13:01,440 --> 00:13:03,120
and even in the current moment,

307
00:13:03,120 --> 00:13:06,000
what capabilities exist, what can be done,

308
00:13:06,000 --> 00:13:08,400
what is still out of reach of current systems

309
00:13:08,400 --> 00:13:10,200
before we can really get serious

310
00:13:10,200 --> 00:13:13,440
about what ought to be done about it.

311
00:13:14,400 --> 00:13:16,160
I guess I'd invite you to add

312
00:13:16,160 --> 00:13:17,360
any additional perspective to that,

313
00:13:17,360 --> 00:13:18,400
and then I'm also curious,

314
00:13:18,400 --> 00:13:21,560
like, I think that's very admirable,

315
00:13:21,560 --> 00:13:24,560
but could you give us a little window

316
00:13:24,560 --> 00:13:27,120
into your own kind of biases or preferences?

317
00:13:27,120 --> 00:13:29,560
Like, what sort of world do you think

318
00:13:29,560 --> 00:13:30,880
we should be striving for,

319
00:13:30,880 --> 00:13:32,040
or do you think that's just so futile

320
00:13:32,040 --> 00:13:34,600
to even attempt to influence against these,

321
00:13:34,600 --> 00:13:37,800
you know, grand constraints that it doesn't matter?

322
00:13:37,800 --> 00:13:39,760
Hey, we'll continue our interview in a moment

323
00:13:39,760 --> 00:13:42,080
after a word from our sponsors.

324
00:13:42,080 --> 00:13:44,640
The Brave Search API brings affordable developer access

325
00:13:44,640 --> 00:13:46,160
to the Brave Search Index,

326
00:13:46,160 --> 00:13:47,520
an independent index of the web

327
00:13:47,520 --> 00:13:49,840
with over 20 billion web pages.

328
00:13:49,840 --> 00:13:52,640
So what makes the Brave Search Index stand out?

329
00:13:52,640 --> 00:13:56,360
One, it's entirely independent and built from scratch.

330
00:13:56,360 --> 00:13:59,920
That means no big tech biases or extortionate prices.

331
00:13:59,920 --> 00:14:03,760
Two, it's built on real page visits from actual humans,

332
00:14:03,760 --> 00:14:05,640
collected anonymously, of course,

333
00:14:05,640 --> 00:14:08,200
which filters out tons of junk data.

334
00:14:08,200 --> 00:14:10,000
And three, the index is refreshed

335
00:14:10,000 --> 00:14:12,080
with tens of millions of pages daily,

336
00:14:12,080 --> 00:14:15,400
so it always has accurate up-to-date information.

337
00:14:15,400 --> 00:14:18,400
The Brave Search API can be used to assemble a dataset

338
00:14:18,400 --> 00:14:20,000
to train your AI models

339
00:14:20,000 --> 00:14:21,720
and help with retrieval augmentation

340
00:14:21,720 --> 00:14:23,360
at the time of inference,

341
00:14:23,360 --> 00:14:24,640
all while remaining affordable

342
00:14:24,640 --> 00:14:26,880
with developer-first pricing.

343
00:14:26,880 --> 00:14:29,720
Integrating the Brave Search API into your workflow

344
00:14:29,720 --> 00:14:31,760
translates to more ethical data sourcing

345
00:14:31,760 --> 00:14:34,520
and more human-representative datasets.

346
00:14:34,520 --> 00:14:36,600
Try the Brave Search API for free

347
00:14:36,600 --> 00:14:40,800
for up to 2,000 queries per month at brave.com slash API.

348
00:14:46,400 --> 00:14:49,520
Pretty much all big, grand talk

349
00:14:50,800 --> 00:14:55,280
is mostly oriented around people sharing values.

350
00:14:56,280 --> 00:14:58,720
That's what people want to do when they talk big politics,

351
00:14:58,720 --> 00:15:01,800
when they talk world politics or world events,

352
00:15:01,800 --> 00:15:04,520
when they talk the future.

353
00:15:04,520 --> 00:15:08,640
People want to jump quickly to, do I share your values?

354
00:15:08,640 --> 00:15:09,680
Here's my values.

355
00:15:09,680 --> 00:15:10,520
What are your values?

356
00:15:10,520 --> 00:15:11,440
Do we agree on values?

357
00:15:11,440 --> 00:15:12,720
Are we value buddies?

358
00:15:13,720 --> 00:15:15,920
And people are so eager to get to that

359
00:15:15,920 --> 00:15:18,120
that they are willing to skip over

360
00:15:18,120 --> 00:15:21,040
the analysis of the details, say,

361
00:15:21,040 --> 00:15:22,240
if they want to talk about, I don't know,

362
00:15:22,240 --> 00:15:24,000
the war in Ukraine.

363
00:15:24,000 --> 00:15:25,840
People want to go, which side are you on?

364
00:15:25,840 --> 00:15:28,800
And who, you know, do we have the right values

365
00:15:28,800 --> 00:15:30,440
and then they don't care to talk about like,

366
00:15:30,440 --> 00:15:32,600
who has how much armaments that will run out soon

367
00:15:32,600 --> 00:15:34,520
or who can afford what or what they,

368
00:15:34,520 --> 00:15:35,960
you know, all those details of the war.

369
00:15:35,960 --> 00:15:37,400
They don't want to go there.

370
00:15:37,400 --> 00:15:40,560
They just want to go to the values and agree about it.

371
00:15:41,560 --> 00:15:43,200
And that happens in the future too,

372
00:15:43,200 --> 00:15:44,040
futurism too.

373
00:15:44,040 --> 00:15:45,120
People just want to jump to the values.

374
00:15:45,120 --> 00:15:47,880
So for the purposes people have,

375
00:15:47,880 --> 00:15:49,160
they're doing roughly the right thing.

376
00:15:49,160 --> 00:15:50,800
They don't really care about the world

377
00:15:50,800 --> 00:15:52,200
and they don't really care about the future.

378
00:15:52,200 --> 00:15:55,640
What they care about is finding value buddies

379
00:15:55,640 --> 00:15:59,280
or if you find a value conflict, having a value war.

380
00:15:59,280 --> 00:16:01,440
That's what people just want to do.

381
00:16:01,440 --> 00:16:05,080
And so if you actually want to figure out the world

382
00:16:05,080 --> 00:16:07,840
or national politics or national policy

383
00:16:07,840 --> 00:16:09,840
or you want to figure out the future,

384
00:16:09,920 --> 00:16:11,400
you really have to resist that

385
00:16:11,400 --> 00:16:15,840
and you have to try to pause and, you know,

386
00:16:15,840 --> 00:16:18,560
go through an analysis first, a neutral analysis

387
00:16:18,560 --> 00:16:20,680
of what the options are, what the situation is.

388
00:16:20,680 --> 00:16:24,680
I mean, I am afraid literally that if I express many values

389
00:16:24,680 --> 00:16:26,840
that the discussion will just go there

390
00:16:26,840 --> 00:16:28,600
and you'll never talk about anything else.

391
00:16:28,600 --> 00:16:31,640
And that's why I resist talking about that.

392
00:16:31,640 --> 00:16:34,040
But I think, you know, my simplest value

393
00:16:34,040 --> 00:16:36,760
with respect to the future is I really like the fact

394
00:16:36,760 --> 00:16:41,280
that humanity has grown and achieved vast things

395
00:16:41,280 --> 00:16:42,640
compared to where it started.

396
00:16:42,640 --> 00:16:45,160
We're on this upward growth trajectory.

397
00:16:45,160 --> 00:16:48,440
We have the potential to taking a big chunk of the universe

398
00:16:49,400 --> 00:16:50,880
and doing things with it.

399
00:16:50,880 --> 00:16:52,720
And I'm excited by that potential.

400
00:16:52,720 --> 00:16:56,720
So my first cut is I want us to keep growing.

401
00:16:56,720 --> 00:17:00,680
And I see how much we've changed to get to where we are.

402
00:17:00,680 --> 00:17:03,600
And I can see that had people from a million years ago

403
00:17:03,600 --> 00:17:06,920
insisted that their values be maintained

404
00:17:06,920 --> 00:17:10,160
and that the world be familiar and comfortable to them.

405
00:17:10,160 --> 00:17:12,160
If they've been able to enforce that,

406
00:17:12,160 --> 00:17:15,080
we would not have gotten where we are now.

407
00:17:15,080 --> 00:17:16,760
That would have prevented a lot of change.

408
00:17:16,760 --> 00:17:19,960
So I kind of see that if I want us to get big and grand,

409
00:17:19,960 --> 00:17:22,840
I'm gonna have to give a lot on

410
00:17:22,840 --> 00:17:26,560
how similar the future is to me and my world.

411
00:17:26,560 --> 00:17:28,520
I'm gonna have to compromise a lot on that.

412
00:17:28,520 --> 00:17:30,160
I just don't see any way around that.

413
00:17:30,160 --> 00:17:32,560
So I get it that if you want the future

414
00:17:32,560 --> 00:17:33,840
to be really comfortable for you

415
00:17:33,840 --> 00:17:36,320
and to share a lot of your values and your styles,

416
00:17:36,320 --> 00:17:39,320
you're gonna have to prevent it from changing.

417
00:17:39,320 --> 00:17:41,440
And you may have a shot at that.

418
00:17:41,440 --> 00:17:43,840
I would not like that, but you might.

419
00:17:43,840 --> 00:17:46,400
So again, even as part of the value framework,

420
00:17:46,400 --> 00:17:47,760
even when I talk values with you,

421
00:17:47,760 --> 00:17:50,320
I want to be clear to distinguish

422
00:17:50,320 --> 00:17:51,840
my value talk from the factual talk.

423
00:17:51,840 --> 00:17:53,880
I'm gonna be happy to tell you

424
00:17:53,880 --> 00:17:56,640
what it would take for you to get your values,

425
00:17:56,640 --> 00:17:58,600
even if they aren't mine.

426
00:17:58,600 --> 00:18:02,240
So maybe we should talk about the facts of LLMs.

427
00:18:02,240 --> 00:18:05,680
You wanna go there in terms of comparing Ms and LLMs, right?

428
00:18:05,680 --> 00:18:07,280
So first of all, our audience,

429
00:18:07,280 --> 00:18:09,120
we should say for our audience,

430
00:18:09,120 --> 00:18:12,240
my book Age of M is about brain emulations.

431
00:18:12,240 --> 00:18:14,840
So that's where you take a particular human brain

432
00:18:14,840 --> 00:18:17,720
and you scan it and find spatial chemical detail

433
00:18:17,720 --> 00:18:19,720
to figure out which cells are where,

434
00:18:19,720 --> 00:18:22,760
connected to other cells through what synapses.

435
00:18:22,760 --> 00:18:24,880
You make a map of that,

436
00:18:24,880 --> 00:18:28,960
and then you make a computer model that matches that map

437
00:18:29,960 --> 00:18:31,840
where you fill in for each cell,

438
00:18:31,840 --> 00:18:33,560
a computer model of that cell.

439
00:18:33,560 --> 00:18:35,160
And if you've got good enough models for cells

440
00:18:35,160 --> 00:18:36,360
and a good map of the brain,

441
00:18:36,360 --> 00:18:38,640
then basically the IO of this model

442
00:18:38,640 --> 00:18:40,680
should be the same as the IO of the original brain,

443
00:18:40,680 --> 00:18:41,680
which means you could hook it up

444
00:18:41,680 --> 00:18:44,400
with artificial eyes, ears, hands, mouth.

445
00:18:44,400 --> 00:18:45,880
And then it would behave the same

446
00:18:45,880 --> 00:18:49,440
as the original human would in the same situation,

447
00:18:49,440 --> 00:18:52,480
in which case you can use these as substitutes for humans

448
00:18:52,480 --> 00:18:54,080
throughout the entire economy.

449
00:18:54,080 --> 00:18:56,200
And then my exercise of the Age of M book

450
00:18:56,240 --> 00:18:59,520
was to figure out what that word looks like.

451
00:18:59,520 --> 00:19:03,520
And a primary purpose was to actually be able to show

452
00:19:03,520 --> 00:19:05,360
that it's possible to do that sort of thing.

453
00:19:05,360 --> 00:19:08,280
It's possible to take a specific technical assumption

454
00:19:08,280 --> 00:19:09,880
and work out a lot of consequences.

455
00:19:09,880 --> 00:19:13,680
And many people have said they didn't want so many details.

456
00:19:13,680 --> 00:19:15,280
They'd rather have fiction or something else,

457
00:19:15,280 --> 00:19:17,440
but I was trying to prove how much I could say.

458
00:19:17,440 --> 00:19:20,160
And I hope you'll admit, I proved I could say a lot.

459
00:19:21,400 --> 00:19:25,200
And that almost no other futurist work does that.

460
00:19:25,240 --> 00:19:27,160
And so I'm trying to inspire other futurists

461
00:19:27,160 --> 00:19:29,000
to get into that level of detail,

462
00:19:29,000 --> 00:19:30,520
to try to take some assumptions

463
00:19:30,520 --> 00:19:31,520
and work out a lot of consequences.

464
00:19:31,520 --> 00:19:33,800
So that's my book, The Age of M.

465
00:19:33,800 --> 00:19:36,320
You'd like us to compare that

466
00:19:36,320 --> 00:19:38,560
to current large-language models

467
00:19:39,680 --> 00:19:41,160
and to think about what we can say

468
00:19:41,160 --> 00:19:43,160
about the future of large-language models.

469
00:19:43,160 --> 00:19:47,280
So in my mind, the first thing to say there is,

470
00:19:47,280 --> 00:19:51,560
well, an M is a full human substitute.

471
00:19:51,560 --> 00:19:54,300
It can do everything a human can do, basically.

472
00:19:55,560 --> 00:19:57,840
A large-language model is not that yet.

473
00:19:59,440 --> 00:20:02,280
So a key question here would be,

474
00:20:02,280 --> 00:20:05,000
how far are we going to go in trying to imagine

475
00:20:05,000 --> 00:20:07,120
a descendant of a large-language model

476
00:20:07,120 --> 00:20:10,320
that is more capable of substituting

477
00:20:10,320 --> 00:20:12,700
for humans across a wide range of contexts?

478
00:20:12,700 --> 00:20:14,380
We stick with current large-language models.

479
00:20:14,380 --> 00:20:15,840
They're really only useful

480
00:20:15,840 --> 00:20:18,640
in a rather limited range of contexts.

481
00:20:18,640 --> 00:20:21,680
And so if you're gonna do forecasting of them,

482
00:20:21,680 --> 00:20:23,340
it's more like forecasting the future

483
00:20:23,340 --> 00:20:25,760
with a microwave oven or something.

484
00:20:25,760 --> 00:20:28,700
You think about, well, where can you use a microwave oven

485
00:20:28,700 --> 00:20:29,820
and how much will it cost

486
00:20:29,820 --> 00:20:33,420
and what other heating methods will it displace

487
00:20:34,420 --> 00:20:37,140
and what sort of inputs would be a compliment to that?

488
00:20:37,140 --> 00:20:39,220
It would be more of a small-scale,

489
00:20:39,220 --> 00:20:41,900
future forecasting exercise.

490
00:20:41,900 --> 00:20:45,340
Whereas The Age of M was purposely this very grand exercise

491
00:20:45,340 --> 00:20:48,720
because the M's actually change everything.

492
00:20:48,720 --> 00:20:51,380
Whereas most futurism, like if you're trying to analyze

493
00:20:51,420 --> 00:20:53,500
consequences of microwave oven,

494
00:20:53,500 --> 00:20:55,220
you have a much more limited scope

495
00:20:55,220 --> 00:20:58,420
because in fact, it'll have a limited impact.

496
00:20:58,420 --> 00:21:01,180
So that would be the question I have for you first,

497
00:21:01,180 --> 00:21:04,860
which is, are we gonna talk about the implications

498
00:21:04,860 --> 00:21:08,180
of something close to the current large-language models?

499
00:21:08,180 --> 00:21:11,980
Are we gonna try to imagine some generalized version

500
00:21:11,980 --> 00:21:14,860
of them that has much wider capabilities?

501
00:21:14,860 --> 00:21:16,180
Yeah, very good question.

502
00:21:16,180 --> 00:21:18,580
I think maybe two different levels of this

503
00:21:18,580 --> 00:21:20,740
would be instructive.

504
00:21:20,740 --> 00:21:22,180
One of the key things that jumps out

505
00:21:22,180 --> 00:21:24,540
and I think a lot of stuff flows from

506
00:21:24,540 --> 00:21:29,540
is the assumption that M's can be copied cheaply,

507
00:21:30,300 --> 00:21:33,940
paused and stored indefinitely cheaply,

508
00:21:33,940 --> 00:21:36,620
but not understood very well

509
00:21:36,620 --> 00:21:38,820
in terms of their internal mechanism.

510
00:21:38,820 --> 00:21:40,980
Very much like this similar understanding

511
00:21:40,980 --> 00:21:42,140
to what we have of the brain

512
00:21:42,140 --> 00:21:43,940
where we can kind of poke and prod at it a little bit,

513
00:21:43,940 --> 00:21:45,380
but we really don't have a deep understanding

514
00:21:45,380 --> 00:21:46,220
of how it works.

515
00:21:46,220 --> 00:21:49,420
We can't do like very localized optimizations,

516
00:21:49,420 --> 00:21:51,820
but we do have this like radical departure

517
00:21:51,820 --> 00:21:52,700
from the status quo,

518
00:21:52,700 --> 00:21:55,420
which is you can infinitely clone them,

519
00:21:55,420 --> 00:21:58,780
you can infinitely freeze and store them.

520
00:21:58,780 --> 00:22:00,860
And so this creates like all sorts of elasticities

521
00:22:00,860 --> 00:22:04,100
that just don't exist in the current environment.

522
00:22:04,100 --> 00:22:05,980
So a number of those features are gonna be general

523
00:22:05,980 --> 00:22:09,660
that anything that can be represented as computer files

524
00:22:09,660 --> 00:22:11,460
and run on a computer.

525
00:22:12,300 --> 00:22:14,500
So any form of artificial intelligence

526
00:22:14,500 --> 00:22:16,500
will be some of the sort in general

527
00:22:16,500 --> 00:22:18,780
that you could have a digital representation

528
00:22:18,780 --> 00:22:23,620
of archive it, make a copy of it, pause it,

529
00:22:23,620 --> 00:22:24,780
run it faster or slower,

530
00:22:24,780 --> 00:22:28,340
that's gonna be just generically true of any kind of AI,

531
00:22:28,340 --> 00:22:29,260
including M's.

532
00:22:30,740 --> 00:22:34,420
The ability to sort of modify it usefully,

533
00:22:34,420 --> 00:22:37,460
I mean, yes, with human brains initially,

534
00:22:37,460 --> 00:22:38,300
they're just a big mess,

535
00:22:38,300 --> 00:22:39,140
you don't understand them,

536
00:22:39,140 --> 00:22:41,380
but honestly, most legacy software systems

537
00:22:41,380 --> 00:22:42,220
are pretty similar.

538
00:22:43,300 --> 00:22:45,940
So today, large legacy software systems,

539
00:22:45,940 --> 00:22:48,380
you mostly have to take them as they are.

540
00:22:48,420 --> 00:22:50,980
You can only make modest modifications to them.

541
00:22:51,900 --> 00:22:53,980
That's close to what I'm assuming for M's.

542
00:22:53,980 --> 00:22:56,820
So I'm actually not assuming that they are that different

543
00:22:56,820 --> 00:22:59,700
from large legacy software systems.

544
00:22:59,700 --> 00:23:01,460
They're just a big mess

545
00:23:02,420 --> 00:23:04,940
that even though you could go look at any one piece

546
00:23:04,940 --> 00:23:06,020
and maybe understand it,

547
00:23:06,020 --> 00:23:07,820
that doesn't really help you usefully

548
00:23:07,820 --> 00:23:10,260
in modifying the entire thing.

549
00:23:10,260 --> 00:23:13,780
You basically have to take the whole thing as a unit

550
00:23:13,780 --> 00:23:17,380
and can only make some minor changes.

551
00:23:17,380 --> 00:23:18,660
But you can copy the whole thing,

552
00:23:18,660 --> 00:23:20,260
you can run it faster or slow,

553
00:23:20,260 --> 00:23:21,300
you can move it at speed,

554
00:23:21,300 --> 00:23:23,260
transfer at the speed of light around the earth

555
00:23:23,260 --> 00:23:25,060
or through the universe.

556
00:23:25,060 --> 00:23:28,300
Those things are true of pretty much any AI

557
00:23:28,300 --> 00:23:30,740
that could be represented as a computer file,

558
00:23:30,740 --> 00:23:31,860
run on a computer.

559
00:23:31,860 --> 00:23:35,300
Yeah, I think these dimensions are a really useful way

560
00:23:35,300 --> 00:23:37,020
to break this down.

561
00:23:37,020 --> 00:23:40,100
I took some inspiration from you in a presentation

562
00:23:40,100 --> 00:23:42,500
that I created called the AI Scouting Report,

563
00:23:42,500 --> 00:23:45,380
where I have the tail of the cognitive tape

564
00:23:45,420 --> 00:23:48,300
that compares human strengths and weaknesses

565
00:23:48,300 --> 00:23:50,500
to LLM strengths and weaknesses.

566
00:23:50,500 --> 00:23:52,980
And I think for the purposes of this discussion,

567
00:23:52,980 --> 00:23:55,740
maybe we might even have like four different kind of things

568
00:23:55,740 --> 00:23:56,580
to consider.

569
00:23:56,580 --> 00:23:58,900
One is humans, second would be M's,

570
00:23:58,900 --> 00:24:03,660
third is let's say transformer language models

571
00:24:03,660 --> 00:24:05,580
of the general class that we have today.

572
00:24:05,580 --> 00:24:09,140
Although I think we can predictably expect at a minimum

573
00:24:09,140 --> 00:24:12,740
that they will continue to have longer context windows

574
00:24:12,740 --> 00:24:15,780
and have generally more pre-training

575
00:24:15,780 --> 00:24:18,300
and generally more capability,

576
00:24:18,300 --> 00:24:21,820
at least within a certain range.

577
00:24:21,820 --> 00:24:23,700
And then the fourth one that I'm really interested in

578
00:24:23,700 --> 00:24:27,020
and has been kind of an obsession for me recently

579
00:24:27,020 --> 00:24:31,340
is the new state space model paradigm,

580
00:24:31,340 --> 00:24:35,540
which actually has some things now in common again

581
00:24:35,540 --> 00:24:36,740
with the humans and the M's

582
00:24:36,740 --> 00:24:39,580
that the transformer models lack.

583
00:24:39,580 --> 00:24:41,820
The state space models,

584
00:24:41,860 --> 00:24:43,980
this has been, of course, in a line of research

585
00:24:43,980 --> 00:24:45,180
that's been going on for a couple of years,

586
00:24:45,180 --> 00:24:47,740
kind of in parallel with transformers.

587
00:24:47,740 --> 00:24:50,020
Transformers have taken up the vast majority

588
00:24:50,020 --> 00:24:53,340
of the energy in the public focus

589
00:24:53,340 --> 00:24:55,460
because they have been the highest performing

590
00:24:55,460 --> 00:24:57,260
over the last couple of years.

591
00:24:57,260 --> 00:25:02,260
But that has maybe just changed with a couple of recent papers,

592
00:25:03,500 --> 00:25:05,580
most notably one called Mamba,

593
00:25:05,580 --> 00:25:10,140
that basically shows parity, rough parity

594
00:25:10,140 --> 00:25:12,340
with the transformer on kind of your standard

595
00:25:12,340 --> 00:25:14,300
language modeling tasks,

596
00:25:14,300 --> 00:25:17,620
but does have like a totally different architecture

597
00:25:17,620 --> 00:25:21,660
that I think opens up like some notably different strengths

598
00:25:21,660 --> 00:25:26,340
and weaknesses, whereas the transformer

599
00:25:26,340 --> 00:25:28,180
really just has the weights

600
00:25:28,180 --> 00:25:31,100
and then the sort of next token prediction,

601
00:25:31,100 --> 00:25:33,540
the state space model has this additional concept

602
00:25:33,540 --> 00:25:38,460
of the state, which is, and I recall from the book,

603
00:25:38,460 --> 00:25:41,620
you sort of say, taking an information processing lens

604
00:25:41,620 --> 00:25:45,900
to the human or where you spend more of your focuses

605
00:25:45,900 --> 00:25:49,820
on the M, you have the current state

606
00:25:49,820 --> 00:25:54,340
plus some new input information, sensory or whatever,

607
00:25:54,340 --> 00:25:58,340
and then that propagates into some action,

608
00:25:58,340 --> 00:26:02,020
some output and a new internal state.

609
00:26:02,020 --> 00:26:03,180
And that I think is really the heart

610
00:26:03,180 --> 00:26:05,300
of what the new state space models do

611
00:26:05,300 --> 00:26:07,540
is that they add that additional component

612
00:26:07,540 --> 00:26:09,580
where they have not only the weights,

613
00:26:09,580 --> 00:26:11,900
like a transformer has static weights,

614
00:26:11,900 --> 00:26:14,140
but they also have this state,

615
00:26:14,140 --> 00:26:18,100
which is of a fixed size, evolves through time,

616
00:26:18,100 --> 00:26:19,780
and is something that gets output

617
00:26:19,780 --> 00:26:21,980
at each kind of inference step

618
00:26:21,980 --> 00:26:23,780
so that there is this internal state

619
00:26:23,780 --> 00:26:26,220
that propagates through time

620
00:26:26,220 --> 00:26:29,060
and can kind of change and have long history.

621
00:26:29,900 --> 00:26:34,900
I think it is likely to bring about

622
00:26:34,940 --> 00:26:39,940
a much more integrated medium and long-term memory

623
00:26:40,020 --> 00:26:41,940
than the transformers have

624
00:26:41,940 --> 00:26:46,940
and create more sort of long episode conditioning

625
00:26:47,580 --> 00:26:51,340
where these models I think will be more amenable

626
00:26:51,340 --> 00:26:54,860
to like employee onboarding style training,

627
00:26:54,860 --> 00:26:56,900
which is something also that the M's have

628
00:26:56,900 --> 00:26:59,020
in your scenario, right?

629
00:26:59,020 --> 00:27:04,020
You can kind of train a base M to be an employee for you,

630
00:27:04,420 --> 00:27:06,740
you can even put it in that mental,

631
00:27:06,740 --> 00:27:07,740
get it to that mental state

632
00:27:07,740 --> 00:27:10,340
where it's like really excited and ready to work,

633
00:27:10,340 --> 00:27:12,500
and then you can freeze it, store it,

634
00:27:12,500 --> 00:27:14,060
boot it up when necessary,

635
00:27:14,060 --> 00:27:16,580
boot it up end times as necessary.

636
00:27:16,580 --> 00:27:20,580
The transformers don't really have that same feature right now,

637
00:27:20,580 --> 00:27:25,580
they're just kind of their monolithic base form at all times,

638
00:27:25,580 --> 00:27:30,380
but the state-state models start to add some of that back.

639
00:27:30,380 --> 00:27:32,500
Obviously, it's not gonna be one-to-one

640
00:27:32,500 --> 00:27:35,060
with the humans or the M's.

641
00:27:35,060 --> 00:27:38,700
Here's gonna be my problem with that number four.

642
00:27:38,700 --> 00:27:41,300
If I look at sort of the history of AI

643
00:27:41,300 --> 00:27:43,260
over the history of computers

644
00:27:43,260 --> 00:27:45,500
and even the history of automation before that,

645
00:27:46,620 --> 00:27:49,380
we see this history where a really wide range

646
00:27:49,380 --> 00:27:51,700
of approaches have been tried,

647
00:27:51,700 --> 00:27:53,580
a really wide range of paradigms

648
00:27:53,580 --> 00:27:57,500
and concepts and structures have been introduced.

649
00:27:57,500 --> 00:28:00,580
And over time, we've found ways in some sense

650
00:28:00,660 --> 00:28:03,420
to subsume prior structures within new ones,

651
00:28:04,500 --> 00:28:07,580
but we've just gone through a lot of them.

652
00:28:08,580 --> 00:28:11,180
And there's been this tendency, unfortunately,

653
00:28:11,180 --> 00:28:13,980
that when people reach the next new paradigm,

654
00:28:13,980 --> 00:28:17,500
the next new structure, they get really excited by it

655
00:28:17,500 --> 00:28:20,580
and they consistently say, are we almost done?

656
00:28:20,580 --> 00:28:22,620
They said that centuries ago,

657
00:28:22,620 --> 00:28:24,420
they said that half a century ago,

658
00:28:24,420 --> 00:28:28,660
every new decade, every new kind of approach

659
00:28:28,660 --> 00:28:30,100
that comes along,

660
00:28:30,900 --> 00:28:32,780
there's basically typically some demo,

661
00:28:32,780 --> 00:28:35,260
some new capability that this new system can do

662
00:28:35,260 --> 00:28:37,900
that none of the prior systems are able to do.

663
00:28:38,820 --> 00:28:41,940
And it's exciting and it's shocking even

664
00:28:41,940 --> 00:28:46,940
and exciting, but people consistently say,

665
00:28:46,940 --> 00:28:49,260
so we must be almost done, right?

666
00:28:49,260 --> 00:28:52,100
Like, surely this is enough to do everything

667
00:28:52,100 --> 00:28:54,220
and pretty soon humans will be displaced

668
00:28:54,220 --> 00:28:56,620
by automation based on this new approach.

669
00:28:56,620 --> 00:28:58,780
And that just happens over and over again.

670
00:28:59,660 --> 00:29:02,220
And so we've had enough of those that I got to say,

671
00:29:02,220 --> 00:29:04,660
the chance that the next exciting new paradigm

672
00:29:04,660 --> 00:29:08,660
is the last one we'll need is a prior pretty low.

673
00:29:09,660 --> 00:29:11,540
We've had this long road to go

674
00:29:11,540 --> 00:29:14,540
and we still have a long way to go ahead of us.

675
00:29:14,540 --> 00:29:16,380
And therefore, it's unlikely

676
00:29:16,380 --> 00:29:19,420
that the next new thing is the last thing.

677
00:29:19,420 --> 00:29:22,180
So that's my stance, I would think, okay,

678
00:29:22,180 --> 00:29:23,340
I can talk to you about LLMS

679
00:29:23,340 --> 00:29:25,420
because they're the latest thing.

680
00:29:25,420 --> 00:29:26,580
We can talk about LLMS,

681
00:29:26,580 --> 00:29:28,220
they're the latest thing.

682
00:29:28,220 --> 00:29:30,020
We can talk about what new things they can do

683
00:29:30,020 --> 00:29:32,260
and what exciting options

684
00:29:32,260 --> 00:29:34,180
that generates in the near future.

685
00:29:35,100 --> 00:29:36,580
And then we can ask, well,

686
00:29:36,580 --> 00:29:39,340
what's the chance it's the last thing we'll need?

687
00:29:39,340 --> 00:29:41,060
Or that the next one is the last thing we need.

688
00:29:41,060 --> 00:29:44,220
And so one way to cash that out is to ask,

689
00:29:44,220 --> 00:29:45,540
what do we think the chances are

690
00:29:45,540 --> 00:29:48,780
that within a decade or even two,

691
00:29:48,780 --> 00:29:51,300
basically all human jobs will be replaced

692
00:29:51,300 --> 00:29:54,940
by machines based on this new approach.

693
00:29:54,940 --> 00:29:57,940
And most of the forecasting that's done out there

694
00:29:57,940 --> 00:30:00,300
is excited about near-term progress in a lot of ways.

695
00:30:00,300 --> 00:30:01,540
But when you ask the question,

696
00:30:01,540 --> 00:30:03,780
when will most jobs be replaced?

697
00:30:03,780 --> 00:30:05,780
They give you forecasts that are way out there

698
00:30:05,780 --> 00:30:09,180
because they think, no, we're not close to that.

699
00:30:09,180 --> 00:30:10,700
And I don't think we're close to that.

700
00:30:10,700 --> 00:30:14,140
So then the question is,

701
00:30:14,140 --> 00:30:15,700
now we could say, what will happen

702
00:30:15,700 --> 00:30:17,380
when we eventually get to the point

703
00:30:17,380 --> 00:30:19,340
where AI is you're good enough to do everything?

704
00:30:19,340 --> 00:30:20,580
And we don't know what that approaches,

705
00:30:20,580 --> 00:30:22,180
but we can still talk about that point

706
00:30:22,540 --> 00:30:25,300
and what's likely to what the transition rate would be

707
00:30:25,300 --> 00:30:27,100
and the transition scenario

708
00:30:27,100 --> 00:30:30,580
and who would get rich and who would be unhappy

709
00:30:30,580 --> 00:30:33,100
and all the different things we could talk about there.

710
00:30:33,100 --> 00:30:35,780
But now we're talking about whatever approach

711
00:30:35,780 --> 00:30:40,100
eventually gets us past the being able to have

712
00:30:40,100 --> 00:30:42,460
to do pretty much all human tasks,

713
00:30:42,460 --> 00:30:43,980
which is not where we are now,

714
00:30:43,980 --> 00:30:45,620
or we can talk about where we are now

715
00:30:45,620 --> 00:30:46,900
and what these things can do

716
00:30:47,940 --> 00:30:51,660
and what exciting things might happen in the next decade.

717
00:30:52,380 --> 00:30:54,340
Hey, we'll continue our interview in a moment

718
00:30:54,340 --> 00:30:56,100
after a word from our sponsors.

719
00:30:56,100 --> 00:30:57,140
If you're a startup founder

720
00:30:57,140 --> 00:30:58,860
or executive running a growing business,

721
00:30:58,860 --> 00:31:01,460
you know that as you scale, your systems break down

722
00:31:01,460 --> 00:31:03,340
and the cracks start to show.

723
00:31:03,340 --> 00:31:04,500
If this resonates with you,

724
00:31:04,500 --> 00:31:06,100
there are three numbers you need to know,

725
00:31:06,100 --> 00:31:10,340
36,000, 25 and one, 36,000.

726
00:31:10,340 --> 00:31:11,380
That's the number of businesses

727
00:31:11,380 --> 00:31:13,220
which have upgraded to NetSuite by Oracle.

728
00:31:13,220 --> 00:31:15,300
NetSuite is the number one cloud financial system,

729
00:31:15,300 --> 00:31:17,300
streamlined accounting, financial management,

730
00:31:17,300 --> 00:31:20,380
inventory, HR and more, 25.

731
00:31:20,420 --> 00:31:22,220
NetSuite turns 25 this year.

732
00:31:22,220 --> 00:31:24,980
That's 25 years of helping businesses do more with less,

733
00:31:24,980 --> 00:31:28,620
close their books in days, not weeks and drive down costs.

734
00:31:28,620 --> 00:31:30,740
One, because your business is one of a kind,

735
00:31:30,740 --> 00:31:33,300
so you get a customized solution for all your KPIs

736
00:31:33,300 --> 00:31:35,780
in one efficient system with one source of truth.

737
00:31:35,780 --> 00:31:39,060
Manage risk, get reliable forecasts and improve margins,

738
00:31:39,060 --> 00:31:41,260
everything you need all in one place.

739
00:31:41,260 --> 00:31:44,620
Right now, download NetSuite's popular KPI checklist,

740
00:31:44,620 --> 00:31:46,980
designed to give you consistently excellent performance,

741
00:31:46,980 --> 00:31:50,300
absolutely free and netsuite.com slash cognitive.

742
00:31:50,300 --> 00:31:52,420
That's netsuite.com slash cognitive

743
00:31:52,420 --> 00:31:54,260
to get your own KPI checklist,

744
00:31:54,260 --> 00:31:56,140
netsuite.com slash cognitive.

745
00:31:57,300 --> 00:32:00,660
Omniki uses generative AI to enable you to launch

746
00:32:00,660 --> 00:32:04,020
hundreds of thousands of ad iterations that actually work,

747
00:32:04,020 --> 00:32:07,440
customized across all platforms with a click of a button.

748
00:32:07,440 --> 00:32:09,980
I believe in Omniki so much that I invested in it

749
00:32:09,980 --> 00:32:12,300
and I recommend you use it too.

750
00:32:12,300 --> 00:32:14,820
Use CogGrav to get a 10% discount.

751
00:32:14,820 --> 00:32:16,380
Well, I'm tempted by all of those options.

752
00:32:16,380 --> 00:32:19,940
So maybe for starters, I would be interested to hear

753
00:32:19,940 --> 00:32:24,940
how you would develop a cognitive tail of the tape

754
00:32:25,540 --> 00:32:28,980
between humans and M's by presumption

755
00:32:28,980 --> 00:32:30,840
have kind of the same cognitive abilities,

756
00:32:30,840 --> 00:32:33,900
but these kind of different external properties

757
00:32:33,900 --> 00:32:35,660
of copyability and so on.

758
00:32:35,660 --> 00:32:38,380
The large language model today,

759
00:32:38,380 --> 00:32:41,500
transformer, remarkably simple architecture,

760
00:32:41,500 --> 00:32:44,020
when you really just look at the wiring diagram,

761
00:32:44,020 --> 00:32:48,420
it's way simpler than the human brain is.

762
00:32:48,420 --> 00:32:52,540
And not shockingly, it can only do certain things

763
00:32:52,540 --> 00:32:55,020
that there's like really important traits

764
00:32:55,020 --> 00:33:00,020
that the human brain has that the language models don't have.

765
00:33:00,580 --> 00:33:04,020
I identified one of those as kind of integrated,

766
00:33:04,020 --> 00:33:08,220
ever-evolving, medium and long-term memory.

767
00:33:08,220 --> 00:33:10,180
I wonder what else you would kind of flag there.

768
00:33:10,180 --> 00:33:13,140
I don't know if you have a taxonomy of what are the kind

769
00:33:13,140 --> 00:33:16,180
of core competencies of humans that you could then say,

770
00:33:16,220 --> 00:33:20,260
oh, and here's the things that language models currently lack.

771
00:33:20,260 --> 00:33:21,940
I'm trying to develop something like this in general

772
00:33:21,940 --> 00:33:26,260
because it does seem to me that the large language models

773
00:33:26,260 --> 00:33:28,980
have hit not genius human level,

774
00:33:28,980 --> 00:33:31,700
but like closing in on expert human level

775
00:33:31,700 --> 00:33:35,220
at some very important, dare I say,

776
00:33:35,220 --> 00:33:39,420
even like core aspect of information processing, right?

777
00:33:39,420 --> 00:33:42,220
Like they can do things that I would say

778
00:33:42,220 --> 00:33:45,820
are qualitatively different than any earlier AI system

779
00:33:45,820 --> 00:33:47,140
could do.

780
00:33:47,140 --> 00:33:49,100
It certainly seems like we're getting close,

781
00:33:49,100 --> 00:33:50,420
whatever the last step is,

782
00:33:50,420 --> 00:33:52,500
we're definitely closer to it than we used to be.

783
00:33:52,500 --> 00:33:56,100
But just notice that phrase you just gave was true

784
00:33:56,100 --> 00:33:58,620
or most of all the previous ones as well.

785
00:33:58,620 --> 00:33:59,900
They could also do a thing

786
00:33:59,900 --> 00:34:02,760
that the previous ones before it couldn't do.

787
00:34:02,760 --> 00:34:04,140
It's always been exciting.

788
00:34:04,140 --> 00:34:06,140
We've found a new fundamental capability

789
00:34:06,140 --> 00:34:09,860
that each new paradigm structure approach

790
00:34:09,860 --> 00:34:12,180
has been of this sort

791
00:34:12,180 --> 00:34:14,540
that it was allowed the system to do fundamental things

792
00:34:14,740 --> 00:34:15,820
that it couldn't do before

793
00:34:15,820 --> 00:34:19,060
that seemed to be near the core of what it was to think.

794
00:34:20,020 --> 00:34:22,100
So there's apparently a lot of things near the core

795
00:34:22,100 --> 00:34:23,260
of what it is to think.

796
00:34:23,260 --> 00:34:25,020
That's the key thing to realize.

797
00:34:25,020 --> 00:34:26,780
What it is to think is a big thing.

798
00:34:26,780 --> 00:34:28,700
There's a lot of things in there.

799
00:34:28,700 --> 00:34:29,540
Well, let's list some.

800
00:34:29,540 --> 00:34:32,180
I can't come up with that many honestly.

801
00:34:32,180 --> 00:34:35,740
Like I would love to hear how many can you name

802
00:34:35,740 --> 00:34:36,600
I have all day.

803
00:34:36,600 --> 00:34:38,980
So could you begin to break down

804
00:34:38,980 --> 00:34:42,100
what it is to think into key components?

805
00:34:42,100 --> 00:34:45,860
I was an AI researcher from 84 to 93.

806
00:34:45,860 --> 00:34:49,740
That was a full time at NASA and then Lockheed.

807
00:34:49,740 --> 00:34:51,620
And certainly at that time,

808
00:34:51,620 --> 00:34:54,540
I understood the range of approaches people had

809
00:34:54,540 --> 00:34:57,540
and could talk about the kinds of things systems

810
00:34:57,540 --> 00:35:01,420
then could do or not do and expert terms relating

811
00:35:01,420 --> 00:35:06,020
to the then current tasks and issues.

812
00:35:06,020 --> 00:35:08,260
I am not up to date at the moment

813
00:35:08,260 --> 00:35:10,620
on the full range of AI approaches.

814
00:35:10,900 --> 00:35:14,860
I don't wanna pretend to be an expert on that.

815
00:35:14,860 --> 00:35:17,780
But I have listened to experts

816
00:35:17,780 --> 00:35:22,140
and the experts I hear basically consistently say,

817
00:35:22,140 --> 00:35:23,540
this is exciting, this is great,

818
00:35:23,540 --> 00:35:26,860
but we're not close to being able to do all the other things

819
00:35:26,860 --> 00:35:29,340
and they would be much better than I am making a list of that

820
00:35:29,340 --> 00:35:31,300
and I feel like they should make the list, not me.

821
00:35:31,300 --> 00:35:33,580
I mean, as a polymath you call me,

822
00:35:33,580 --> 00:35:36,820
I wanna be very careful to know when I'm an expert

823
00:35:36,820 --> 00:35:38,420
on something and when I'm not.

824
00:35:38,420 --> 00:35:40,740
And I wanna defer to other people on areas

825
00:35:40,740 --> 00:35:43,020
where I can find people who know more than I.

826
00:35:43,020 --> 00:35:45,620
And when I think I'm near the state of the art,

827
00:35:45,620 --> 00:35:47,100
as good as anyone on a topic,

828
00:35:47,100 --> 00:35:50,380
then I will feel more free to generate my own thoughts

829
00:35:50,380 --> 00:35:52,580
and think they're worth contributing.

830
00:35:52,580 --> 00:35:55,540
Fair, certainly, I think most people

831
00:35:55,540 --> 00:36:00,220
where I think you do still bring something very differentiated

832
00:36:00,220 --> 00:36:05,220
to the discussion is just the sort of willingness

833
00:36:06,220 --> 00:36:10,180
to stare reality in the face or at least try to.

834
00:36:10,180 --> 00:36:12,180
The simplest thing is if I start talking

835
00:36:12,180 --> 00:36:13,620
to an out large language model,

836
00:36:13,620 --> 00:36:15,380
there's a whole bunch of things I can ask it to do

837
00:36:15,380 --> 00:36:16,740
that it just can't do.

838
00:36:17,900 --> 00:36:19,540
I'm not so sure how to organize that

839
00:36:19,540 --> 00:36:21,380
in terms of the large major categories,

840
00:36:21,380 --> 00:36:24,820
but it's really obvious that there's a certain kind of thinking

841
00:36:24,820 --> 00:36:28,060
it can do and a bunch of other kind of thinking it can't do.

842
00:36:28,060 --> 00:36:30,660
And I don't know exactly why it can't do them,

843
00:36:30,660 --> 00:36:33,060
but I'm talking to you, there's a bunch of things

844
00:36:33,060 --> 00:36:34,620
I could ask you to do in this conversation

845
00:36:35,180 --> 00:36:37,340
that you would probably do a decent job of them.

846
00:36:37,340 --> 00:36:39,540
And then if I were talking to the large language model,

847
00:36:39,540 --> 00:36:41,780
it just couldn't do those things.

848
00:36:41,780 --> 00:36:43,300
So it's just really obvious to me

849
00:36:43,300 --> 00:36:44,940
that this has a limited capability.

850
00:36:44,940 --> 00:36:47,180
It's really impressive compared to what you might have expected

851
00:36:47,180 --> 00:36:49,100
five or 10 years ago, it's, wow,

852
00:36:49,100 --> 00:36:51,380
I never would have thought that would be feasible this soon,

853
00:36:51,380 --> 00:36:55,180
but you just try asking it a bunch of other things

854
00:36:55,180 --> 00:36:57,020
and it just can't do them, right?

855
00:36:57,020 --> 00:37:02,020
Yeah, I mean, I think that in my view,

856
00:37:02,780 --> 00:37:07,780
a lot of those things are kind of overemphasized

857
00:37:08,940 --> 00:37:12,180
relative to what maybe really matters.

858
00:37:12,180 --> 00:37:15,740
You see a lot of things online where people,

859
00:37:15,740 --> 00:37:17,540
and there's different categories of this,

860
00:37:17,540 --> 00:37:19,460
some of the things you'll see online

861
00:37:19,460 --> 00:37:23,260
are literally people just using non frontier models

862
00:37:23,260 --> 00:37:25,580
and kind of confusing, muddying the water.

863
00:37:25,580 --> 00:37:27,340
So always watch out for that.

864
00:37:27,340 --> 00:37:31,140
I have a longstanding practice of,

865
00:37:31,180 --> 00:37:32,580
first thing I do when I see somebody say,

866
00:37:32,580 --> 00:37:35,500
GPT-4 can't do something is try it myself.

867
00:37:35,500 --> 00:37:38,940
And I would honestly say like two thirds of the time,

868
00:37:38,940 --> 00:37:40,660
it's just straight up misinformation

869
00:37:40,660 --> 00:37:42,260
and it in fact, like can do it.

870
00:37:42,260 --> 00:37:45,180
But there's still the one third of the time that matters.

871
00:37:45,180 --> 00:37:47,260
They're not very adversarially robust.

872
00:37:47,260 --> 00:37:48,300
They're easy to trick,

873
00:37:48,300 --> 00:37:52,140
they're easy to sort of get on the wrong track.

874
00:37:52,140 --> 00:37:56,580
And then they seem to get kind of stuck in a mode

875
00:37:56,580 --> 00:37:58,020
is a good term for it, I think,

876
00:37:58,020 --> 00:38:01,180
where once they're kind of on a certain,

877
00:38:01,180 --> 00:38:03,580
this is kind of how they can often get jailbroken.

878
00:38:03,580 --> 00:38:04,780
If you can get them to say like,

879
00:38:04,780 --> 00:38:06,940
okay, I'll be happy to help you with that,

880
00:38:06,940 --> 00:38:10,020
then they'll go on and do whatever you asked

881
00:38:10,020 --> 00:38:12,380
because they've already kind of got into that mode.

882
00:38:12,380 --> 00:38:13,700
Yeah, I'm much less worried about them

883
00:38:13,700 --> 00:38:15,100
doing things you don't want them to do

884
00:38:15,100 --> 00:38:18,020
than being able to get them to do things at all.

885
00:38:18,020 --> 00:38:21,620
That as humans can be made to do all sorts of things,

886
00:38:21,620 --> 00:38:22,820
you might not want them to do that.

887
00:38:22,820 --> 00:38:23,820
We survive that.

888
00:38:24,780 --> 00:38:26,580
I mean, to me, the main thing is,

889
00:38:26,580 --> 00:38:28,700
if you imagine, you know,

890
00:38:28,700 --> 00:38:31,680
treating a large language model as a new employee

891
00:38:31,680 --> 00:38:33,460
in some workplace where you're trying to show them

892
00:38:33,460 --> 00:38:36,300
how to do something and get them to do it instead of you,

893
00:38:36,300 --> 00:38:38,660
that's the main thing that will be economically valuable

894
00:38:38,660 --> 00:38:39,500
in the world.

895
00:38:39,500 --> 00:38:41,220
That is, when you have a thing like that

896
00:38:41,220 --> 00:38:44,380
that can be introduced into a place,

897
00:38:44,380 --> 00:38:46,340
trained roughly and said, watch how I do this,

898
00:38:46,340 --> 00:38:48,980
you try to do it now, et cetera,

899
00:38:48,980 --> 00:38:51,420
then that will be the thing that, you know,

900
00:38:51,460 --> 00:38:53,860
makes an enormous difference in the economy

901
00:38:53,860 --> 00:38:57,500
because that's how we get people to do things, right?

902
00:38:57,500 --> 00:38:59,820
So if that I think is, in a sense,

903
00:38:59,820 --> 00:39:02,500
the fundamental main task in the economy,

904
00:39:02,500 --> 00:39:05,420
which is a bunch of people are doing something,

905
00:39:05,420 --> 00:39:07,300
you have a new thing and you say,

906
00:39:07,300 --> 00:39:09,660
would you Kim watch us and ask us questions

907
00:39:09,660 --> 00:39:12,020
and we'll ask you questions and like figure out

908
00:39:12,020 --> 00:39:15,300
how to help us and be part of what we're doing.

909
00:39:15,300 --> 00:39:17,260
That is the fundamental problem in the economy.

910
00:39:17,260 --> 00:39:20,060
So that in some sense is the fundamental task

911
00:39:20,060 --> 00:39:23,020
that any AI has to be held up to.

912
00:39:23,020 --> 00:39:24,820
I mean, in the past, of course,

913
00:39:24,820 --> 00:39:26,300
we don't even bother to have a conversation

914
00:39:26,300 --> 00:39:28,380
to show you how to do, we actually say,

915
00:39:28,380 --> 00:39:29,860
well, let's make a machine to do this thing

916
00:39:29,860 --> 00:39:31,340
and then we design a machine to do this thing

917
00:39:31,340 --> 00:39:32,900
and then we train it up to do this thing

918
00:39:32,900 --> 00:39:34,500
all with the idea of the whole thing,

919
00:39:34,500 --> 00:39:37,460
having in mind the thing we're gonna have to do.

920
00:39:37,460 --> 00:39:40,460
That's how AI has been usually in the economy so far.

921
00:39:40,460 --> 00:39:41,620
But now if you're imagining a thing

922
00:39:41,620 --> 00:39:44,020
that could just be trained to do a new job,

923
00:39:44,020 --> 00:39:45,220
well, that would be great.

924
00:39:46,420 --> 00:39:49,260
Sure, then we won't have to design the AI ahead of time

925
00:39:49,260 --> 00:39:50,980
for the particular task,

926
00:39:50,980 --> 00:39:52,940
but you'll have to have a thing that's up to that

927
00:39:52,940 --> 00:39:54,300
and large language models today

928
00:39:54,300 --> 00:39:56,860
are just clearly not up to that.

929
00:39:56,860 --> 00:39:58,180
You can't say, I'm about to train you

930
00:39:58,180 --> 00:39:59,780
how to do the following thing, pay attention,

931
00:39:59,780 --> 00:40:01,940
I just did this, now would you do it?

932
00:40:01,940 --> 00:40:03,940
Well, you can do that quite a bit, right?

933
00:40:03,940 --> 00:40:06,780
I mean, that was the main kind of finding in GPT-3

934
00:40:06,780 --> 00:40:09,300
was, I'm not sure if I have this verbatim,

935
00:40:09,300 --> 00:40:11,780
but the title of that paper was large language models

936
00:40:11,780 --> 00:40:13,940
are few shot learners.

937
00:40:13,940 --> 00:40:17,220
And the big kind of breakthrough observation there,

938
00:40:17,220 --> 00:40:18,580
which I don't think they designed,

939
00:40:19,580 --> 00:40:21,980
there's a whole quagmire of what should count

940
00:40:21,980 --> 00:40:23,540
as emergent or not emergent,

941
00:40:23,540 --> 00:40:26,700
but my understanding is they didn't specifically train

942
00:40:26,700 --> 00:40:30,740
for this few shot imitation capability,

943
00:40:30,740 --> 00:40:33,660
but they nevertheless got to the point where

944
00:40:33,660 --> 00:40:38,020
at runtime today, you can give a few examples

945
00:40:38,020 --> 00:40:38,860
of what you want.

946
00:40:38,860 --> 00:40:40,500
And in fact, that is like a best practice

947
00:40:40,500 --> 00:40:43,140
that open AI and anthropic recommend

948
00:40:43,140 --> 00:40:45,260
for how to get the most from their systems.

949
00:40:45,260 --> 00:40:47,580
They'll say, some things are hard,

950
00:40:47,580 --> 00:40:49,380
they also have now trained them to follow instructions,

951
00:40:49,380 --> 00:40:52,180
just verbatim or explicitly,

952
00:40:52,180 --> 00:40:54,940
but they will still say that,

953
00:40:54,940 --> 00:40:57,780
some things are better shown by example

954
00:40:57,780 --> 00:41:01,660
than described in terms of what to do.

955
00:41:01,660 --> 00:41:05,620
So do that, and you'll get like a lot better performance.

956
00:41:05,620 --> 00:41:09,540
It seems to me that there is on that kind of watch,

957
00:41:09,540 --> 00:41:11,820
watch it to borrow from medicine,

958
00:41:11,820 --> 00:41:14,180
watch one, do one, teach one,

959
00:41:14,180 --> 00:41:17,820
it seems like we're on the do one step,

960
00:41:17,820 --> 00:41:22,580
and that does seem to be a pretty qualitative threshold

961
00:41:22,580 --> 00:41:23,420
that has been passed.

962
00:41:23,420 --> 00:41:26,180
Now, they obviously can continue to get better at that.

963
00:41:26,180 --> 00:41:28,020
Right, but it's the range of things they can do

964
00:41:28,020 --> 00:41:29,100
that's the question.

965
00:41:29,100 --> 00:41:31,020
Yes, it's great that they can,

966
00:41:31,020 --> 00:41:33,420
you can say, here's some examples, give me another one,

967
00:41:33,420 --> 00:41:36,340
but the range of things you can do that for is limited.

968
00:41:36,340 --> 00:41:38,340
Most people in most jobs,

969
00:41:38,340 --> 00:41:41,420
they couldn't have large language model swap in

970
00:41:41,420 --> 00:41:43,980
for many of their main tasks that way.

971
00:41:44,940 --> 00:41:46,500
But there are some and that's exciting,

972
00:41:46,500 --> 00:41:48,980
and I hope to see people develop that and improve it.

973
00:41:48,980 --> 00:41:51,380
But again, the key question is how close are we

974
00:41:51,380 --> 00:41:54,740
to the end of this long path we've been on for a while?

975
00:41:54,740 --> 00:41:56,460
Yeah, I guess I think about it a little bit differently

976
00:41:56,460 --> 00:41:59,060
in terms of rather than thinking about the end of the path,

977
00:41:59,060 --> 00:42:03,780
I think of how close are we to key thresholds

978
00:42:03,780 --> 00:42:08,300
that will bring in qualitatively different dynamics

979
00:42:08,300 --> 00:42:12,420
relative to the current situation.

980
00:42:12,620 --> 00:42:16,020
So one threshold that I think has recently been passed

981
00:42:16,020 --> 00:42:18,180
and in a pretty striking way that this is,

982
00:42:18,180 --> 00:42:20,900
should get more discussion than it does in my view

983
00:42:20,900 --> 00:42:23,580
is Google DeepMind just put out a paper

984
00:42:23,580 --> 00:42:28,580
not long ago where they showed basically a two to one

985
00:42:29,300 --> 00:42:31,740
advantage for a large language model

986
00:42:31,740 --> 00:42:36,300
in medical diagnosis versus human doctors.

987
00:42:36,300 --> 00:42:38,540
And then of course they also compared to human plus AI

988
00:42:38,540 --> 00:42:39,820
and that was in the middle.

989
00:42:39,820 --> 00:42:42,300
So on these cases that they lined up

990
00:42:42,300 --> 00:42:45,420
in the scenario is like you're chatting with your doctor,

991
00:42:45,420 --> 00:42:48,820
60% accuracy from the language model,

992
00:42:48,820 --> 00:42:52,020
30% accuracy from the human.

993
00:42:52,020 --> 00:42:55,620
I was an AI from 83 to 94.

994
00:42:56,940 --> 00:43:00,220
And at the beginning, one of the reasons I came into AI

995
00:43:00,220 --> 00:43:02,940
was there were these big journal articles

996
00:43:02,940 --> 00:43:05,220
and national media coverage about studies

997
00:43:05,220 --> 00:43:07,940
where they showed that the best AI of the time

998
00:43:07,940 --> 00:43:09,820
which they called expert systems

999
00:43:09,820 --> 00:43:14,020
were able to do human level medical diagnosis.

1000
00:43:14,020 --> 00:43:17,380
This was in the early 1980s, right?

1001
00:43:17,380 --> 00:43:19,040
We're talking 40 years ago.

1002
00:43:19,920 --> 00:43:21,940
And obviously the computer capacity

1003
00:43:21,940 --> 00:43:23,180
is vastly larger than that.

1004
00:43:23,180 --> 00:43:26,220
So either they were lying back then

1005
00:43:26,220 --> 00:43:28,620
and messing with the data

1006
00:43:28,620 --> 00:43:33,020
or they did have human level diagnosis back then

1007
00:43:33,020 --> 00:43:34,700
but they weren't allowed to apply it

1008
00:43:34,700 --> 00:43:36,300
because of medical licensing.

1009
00:43:37,180 --> 00:43:39,900
So, and we're still not allowed to apply it

1010
00:43:39,900 --> 00:43:42,220
because of medical licensing.

1011
00:43:42,220 --> 00:43:46,140
So, this is exactly the sort of ability

1012
00:43:46,140 --> 00:43:49,420
that won't give substantial economic impact

1013
00:43:49,420 --> 00:43:51,300
because we had it 40 years ago

1014
00:43:51,300 --> 00:43:52,940
and it didn't have an impact then.

1015
00:43:52,940 --> 00:43:54,220
Yeah, I don't know.

1016
00:43:54,220 --> 00:43:57,700
So if I had, I think one qualitative difference

1017
00:43:57,700 --> 00:43:59,780
between that earlier system and this system

1018
00:43:59,780 --> 00:44:01,220
which won't come to be an expert

1019
00:44:01,220 --> 00:44:02,740
in the earlier expert systems

1020
00:44:02,740 --> 00:44:07,740
but I would guess that a huge difference

1021
00:44:08,220 --> 00:44:13,100
is that you can take today a totally uninitiated person

1022
00:44:13,100 --> 00:44:15,340
who has a medical concern

1023
00:44:15,340 --> 00:44:17,940
and say, sit in front of this computer,

1024
00:44:17,940 --> 00:44:19,540
talk to this doctor.

1025
00:44:19,540 --> 00:44:21,760
They don't even need to know as an AI doctor.

1026
00:44:21,760 --> 00:44:23,460
They can just talk to him.

1027
00:44:23,460 --> 00:44:25,700
That wasn't the problem back then.

1028
00:44:25,700 --> 00:44:27,940
They could have made these expert systems

1029
00:44:27,940 --> 00:44:31,220
usable by ordinary people with modest effort.

1030
00:44:31,220 --> 00:44:33,180
That wasn't the problem in using them.

1031
00:44:33,180 --> 00:44:35,780
The problem was just you're not legally allowed to use them.

1032
00:44:35,780 --> 00:44:38,700
Only doctors are allowed to give medical diagnoses.

1033
00:44:38,700 --> 00:44:40,940
And so only doctors are allowed to use these systems

1034
00:44:40,940 --> 00:44:41,780
to talk to people.

1035
00:44:41,780 --> 00:44:44,660
That was the main obstacle and it still is today.

1036
00:44:44,660 --> 00:44:48,300
The obstacle, you could make such a system today

1037
00:44:48,300 --> 00:44:49,460
that ordinary people could talk to

1038
00:44:49,460 --> 00:44:51,060
but they're not allowed to talk to it

1039
00:44:51,060 --> 00:44:54,220
and they won't be allowed to talk to it for a long time.

1040
00:44:54,220 --> 00:44:55,860
I think there is a qualitative difference

1041
00:44:55,860 --> 00:44:57,060
between these systems.

1042
00:44:57,060 --> 00:44:59,180
If I were to sit down in front of the early 80s thing

1043
00:44:59,220 --> 00:45:01,580
and I were to say, what's different today

1044
00:45:01,580 --> 00:45:03,820
is the chat system could say,

1045
00:45:03,820 --> 00:45:05,060
Robin, tell me how you're feeling.

1046
00:45:05,060 --> 00:45:06,100
Tell me about your experience.

1047
00:45:06,100 --> 00:45:08,060
And you can just go on in your own language,

1048
00:45:08,060 --> 00:45:09,780
however you want to express yourself,

1049
00:45:09,780 --> 00:45:10,780
and it can get you.

1050
00:45:10,780 --> 00:45:13,140
And then it can ask you specific follow up

1051
00:45:13,140 --> 00:45:14,420
but you're not going through a wizard

1052
00:45:14,420 --> 00:45:16,940
and going down an expert system tree

1053
00:45:16,940 --> 00:45:19,940
and ask for numeric scores you don't understand

1054
00:45:19,940 --> 00:45:20,780
and don't know.

1055
00:45:20,780 --> 00:45:22,620
You can literally just express yourself.

1056
00:45:22,620 --> 00:45:23,820
That was not there then, right?

1057
00:45:23,820 --> 00:45:25,100
I mean, nothing.

1058
00:45:25,100 --> 00:45:28,100
But that's not the limiting factor, right?

1059
00:45:28,100 --> 00:45:30,700
I mean, you couldn't have a fancy graphics interface

1060
00:45:30,700 --> 00:45:31,540
back then either.

1061
00:45:31,540 --> 00:45:33,940
This was early 1980s, right?

1062
00:45:33,940 --> 00:45:37,460
But again, the limiting factor is the legal barrier.

1063
00:45:38,340 --> 00:45:39,740
It was back then and still is

1064
00:45:39,740 --> 00:45:41,580
and that legal barrier doesn't look like

1065
00:45:41,580 --> 00:45:43,180
it's about to go away.

1066
00:45:43,180 --> 00:45:45,940
So if you're gonna make us excited about applications

1067
00:45:45,940 --> 00:45:48,540
it'll have to be something that's legal.

1068
00:45:48,540 --> 00:45:52,340
My model of this is that the consumer surplus

1069
00:45:52,340 --> 00:45:55,500
of this type of thing is going to be so great.

1070
00:45:55,500 --> 00:45:56,860
It already was 40 years ago.

1071
00:45:56,900 --> 00:45:58,420
It would have been a huge consumer surplus

1072
00:45:58,420 --> 00:46:00,620
40 years ago, it was not allowed.

1073
00:46:00,620 --> 00:46:03,140
But there was never a groundswell of, I don't know.

1074
00:46:03,140 --> 00:46:04,180
I'm just not buying this.

1075
00:46:04,180 --> 00:46:06,300
I'm not buying that there was an experience

1076
00:46:06,300 --> 00:46:09,100
that is qualitatively like the one that we have today

1077
00:46:09,100 --> 00:46:13,260
such that I think today if you show people what Google has

1078
00:46:13,260 --> 00:46:16,420
they will say it is not acceptable to me

1079
00:46:16,420 --> 00:46:19,900
that you keep this locked up behind some payroll.

1080
00:46:19,900 --> 00:46:22,820
I don't think that was the general consumer reaction

1081
00:46:22,820 --> 00:46:25,620
to early 80s expert systems.

1082
00:46:25,660 --> 00:46:29,580
And it seems like that political economy pressure

1083
00:46:29,580 --> 00:46:31,180
could change things.

1084
00:46:31,180 --> 00:46:33,740
Consider the analogy of nuclear power.

1085
00:46:33,740 --> 00:46:36,100
The world has definitely been convinced for a long time

1086
00:46:36,100 --> 00:46:38,620
that nuclear power is powerful.

1087
00:46:39,460 --> 00:46:42,720
It is full of potential and power.

1088
00:46:42,720 --> 00:46:44,620
And if we had let it go wild

1089
00:46:44,620 --> 00:46:46,540
we would have vastly cheaper energy today

1090
00:46:46,540 --> 00:46:48,500
but it was that power that scared people

1091
00:46:48,500 --> 00:46:51,140
which is why we don't have that energy today.

1092
00:46:51,140 --> 00:46:54,580
The very vision of nuclear energy being powerful

1093
00:46:54,580 --> 00:46:57,180
is what caused us not to have it.

1094
00:46:58,540 --> 00:47:01,620
We over-regulated it to death

1095
00:47:01,620 --> 00:47:04,140
and we made sure that the power of nuclear power

1096
00:47:04,140 --> 00:47:05,660
was not released.

1097
00:47:05,660 --> 00:47:07,180
We believed the power was there.

1098
00:47:07,180 --> 00:47:09,620
It was not at all an issue of not believing

1099
00:47:09,620 --> 00:47:10,900
that nuclear power was powerful.

1100
00:47:10,900 --> 00:47:13,340
It was believing it was too powerful.

1101
00:47:13,340 --> 00:47:15,860
Scary, dangerous, powerful.

1102
00:47:15,860 --> 00:47:18,880
And there's a risk that we'll do that with AI today.

1103
00:47:19,840 --> 00:47:21,500
We will make people believe it's powerful,

1104
00:47:21,500 --> 00:47:23,620
so powerful that they should be scared of it

1105
00:47:23,620 --> 00:47:24,980
and it should be locked down

1106
00:47:24,980 --> 00:47:27,620
and not released into the wild

1107
00:47:27,620 --> 00:47:29,820
where it might do us terrible danger.

1108
00:47:29,820 --> 00:47:31,500
Yeah, well, that's certainly a tragic outcome

1109
00:47:31,500 --> 00:47:33,900
in the case of the nuclear power.

1110
00:47:33,900 --> 00:47:35,980
And I think it would also be a tragic outcome

1111
00:47:35,980 --> 00:47:38,900
if people are denied their AI doctors

1112
00:47:38,900 --> 00:47:43,220
of the future on that basis.

1113
00:47:43,220 --> 00:47:44,540
And it could happen.

1114
00:47:44,540 --> 00:47:47,220
I certainly wouldn't rule out the possibility that

1115
00:47:47,220 --> 00:47:49,620
just AI research probably gets made illegal.

1116
00:47:49,620 --> 00:47:51,780
This time we do have, I mean, again, it is,

1117
00:47:51,780 --> 00:47:53,180
I do think we're in a different regime now

1118
00:47:53,220 --> 00:47:57,180
where enough has been discovered

1119
00:47:57,180 --> 00:47:59,580
and enough has been put into the hands of millions.

1120
00:47:59,580 --> 00:48:04,260
There is sort of the open source kind of hacker level.

1121
00:48:04,260 --> 00:48:06,100
Not medical diagnosis is not.

1122
00:48:06,100 --> 00:48:08,460
We have not put medical diagnosis AI

1123
00:48:08,460 --> 00:48:10,460
in the hands of ordinary people.

1124
00:48:10,460 --> 00:48:12,380
And if you tried it, you would find out

1125
00:48:12,380 --> 00:48:14,500
just how quickly you'd get slapped now.

1126
00:48:14,500 --> 00:48:16,300
Yeah, I think I know someone who actually may be

1127
00:48:16,300 --> 00:48:18,540
about to try this and it'll be very interesting

1128
00:48:18,540 --> 00:48:21,660
to see how quickly and how hard they get slapped down

1129
00:48:21,940 --> 00:48:23,580
and how they may respond from it.

1130
00:48:23,580 --> 00:48:26,140
I've actually been very encouraged by the response

1131
00:48:26,140 --> 00:48:27,980
from the medical community.

1132
00:48:28,820 --> 00:48:30,940
I would say, obviously it's not a monolithic thing,

1133
00:48:30,940 --> 00:48:34,380
but I did an earlier episode with Zach Kahane,

1134
00:48:34,380 --> 00:48:37,460
who is a professor at Harvard Medical School

1135
00:48:37,460 --> 00:48:40,340
and who had early access to GPT-4.

1136
00:48:40,340 --> 00:48:41,780
He came out with a book basically

1137
00:48:41,780 --> 00:48:44,860
to coincide with the launch of GPT-4

1138
00:48:44,860 --> 00:48:48,540
called GPT-4 and the Revolution in Medicine.

1139
00:48:48,540 --> 00:48:53,540
And broadly, I have been encouraged by how much

1140
00:48:53,900 --> 00:48:57,180
the medical establishment has seemingly been inclined

1141
00:48:57,180 --> 00:48:59,580
to embrace this sort of stuff.

1142
00:48:59,580 --> 00:49:01,180
I don't know if it's just that they're also

1143
00:49:01,180 --> 00:49:03,220
overworked these days or...

1144
00:49:03,220 --> 00:49:05,460
Well, they'll embrace the internal use of it.

1145
00:49:05,460 --> 00:49:09,500
Again, it's always been doctors allowed to use these things.

1146
00:49:09,500 --> 00:49:11,220
And the main reason they didn't get more popular

1147
00:49:11,220 --> 00:49:13,540
is doctors couldn't be bothered to type in

1148
00:49:13,540 --> 00:49:15,420
and input all the information

1149
00:49:15,420 --> 00:49:18,820
because they want to have short meetings with patients.

1150
00:49:18,820 --> 00:49:21,460
Even today, of course, if you've gone to a modern doctor,

1151
00:49:21,460 --> 00:49:23,260
most of your meeting with a doctor

1152
00:49:23,260 --> 00:49:26,700
is them typing in information to their computer

1153
00:49:26,700 --> 00:49:28,220
as they talk to you.

1154
00:49:28,220 --> 00:49:30,620
And they don't wanna spend much more time

1155
00:49:30,620 --> 00:49:31,940
typing in more.

1156
00:49:31,940 --> 00:49:34,960
And so they don't wanna use computer aids

1157
00:49:34,960 --> 00:49:36,980
in their diagnosis and that's been true for a long time.

1158
00:49:36,980 --> 00:49:39,220
They, computer diagnosis aids have been available

1159
00:49:39,220 --> 00:49:41,940
for a long time that would give them better diagnoses

1160
00:49:41,940 --> 00:49:44,180
at the cost of them having to spend more time with them

1161
00:49:44,260 --> 00:49:45,700
than they've chosen not to spend more time.

1162
00:49:45,700 --> 00:49:47,580
That's been true for many decades now.

1163
00:49:47,580 --> 00:49:52,180
Have you personally used GPT-4 for any advanced things

1164
00:49:52,180 --> 00:49:55,420
like this, medical or legal advice or whatever?

1165
00:49:55,420 --> 00:49:57,300
No, I'm an economics professor.

1166
00:49:57,300 --> 00:49:59,540
So I've used it to check to see what my students

1167
00:49:59,540 --> 00:50:01,660
might try to use it to answer my exam questions

1168
00:50:01,660 --> 00:50:03,960
or essay questions or things like that.

1169
00:50:03,960 --> 00:50:06,140
I've asked it things that I wanted to know

1170
00:50:06,140 --> 00:50:08,260
and try to check on them.

1171
00:50:08,260 --> 00:50:11,900
I haven't used it for legal or medical questions.

1172
00:50:11,900 --> 00:50:13,940
Those are areas which are heavily regulated.

1173
00:50:13,940 --> 00:50:16,020
It's always been possible for other people

1174
00:50:16,020 --> 00:50:17,900
to offer substitutes.

1175
00:50:17,900 --> 00:50:20,060
So for example, many decades ago,

1176
00:50:20,060 --> 00:50:22,220
there were experiments where we,

1177
00:50:22,220 --> 00:50:25,220
basically for the purpose of general practice for doctors,

1178
00:50:25,220 --> 00:50:27,440
we compare doctors to nurses,

1179
00:50:27,440 --> 00:50:28,980
nurse practitioners or paramedics.

1180
00:50:28,980 --> 00:50:31,140
We found that those other groups did just as well

1181
00:50:31,140 --> 00:50:33,740
and much cheaper at doing the first level

1182
00:50:33,740 --> 00:50:36,980
of general practice, but they haven't been allowed.

1183
00:50:36,980 --> 00:50:39,200
So that right there is enormous value

1184
00:50:39,200 --> 00:50:40,140
that could have been released.

1185
00:50:40,140 --> 00:50:42,700
We could have all this time been having nurse practitioners

1186
00:50:42,700 --> 00:50:45,940
and doctors and paramedics do our first level

1187
00:50:45,940 --> 00:50:47,380
of general practice medicine.

1188
00:50:47,380 --> 00:50:49,700
And they would save at least a factor of two or three

1189
00:50:49,700 --> 00:50:51,940
in cost and that's been true for decades.

1190
00:50:51,940 --> 00:50:54,420
We've had randomized experiments showing that for decades.

1191
00:50:54,420 --> 00:50:57,100
So going back to the age of M then for a second,

1192
00:50:57,100 --> 00:51:01,180
are you just assuming that that scenario doesn't happen

1193
00:51:01,180 --> 00:51:04,020
in M land for some reason?

1194
00:51:04,020 --> 00:51:07,580
Or like, why wouldn't it be the first objection

1195
00:51:07,580 --> 00:51:10,380
to the age of M seems like it maybe should be,

1196
00:51:10,380 --> 00:51:11,420
M's will be made illegal.

1197
00:51:11,420 --> 00:51:13,020
Nobody will be allowed to do it.

1198
00:51:13,020 --> 00:51:14,220
Absolutely.

1199
00:51:14,220 --> 00:51:17,140
And basically you're just kind of in the analysis saying,

1200
00:51:17,140 --> 00:51:18,500
well, let's just assume that doesn't happen

1201
00:51:18,500 --> 00:51:20,580
because it'll be, you know, it's a short book

1202
00:51:20,580 --> 00:51:22,280
if they just get made illegal too early.

1203
00:51:22,280 --> 00:51:23,700
Is that the idea?

1204
00:51:23,700 --> 00:51:25,740
Well, so first of all,

1205
00:51:25,740 --> 00:51:28,560
I say transitions are harder to analyze

1206
00:51:28,560 --> 00:51:30,700
than equilibria of New World.

1207
00:51:30,700 --> 00:51:34,140
So I try to avoid analyzing the transition.

1208
00:51:34,140 --> 00:51:36,460
Although I do try to discuss it some toward the end

1209
00:51:36,460 --> 00:51:38,500
of the book, but I admit,

1210
00:51:38,500 --> 00:51:40,860
I can just say less about a transition.

1211
00:51:40,860 --> 00:51:43,460
It does seem like that, you know,

1212
00:51:43,460 --> 00:51:45,940
compared to a scenario where everyone eagerly adopted

1213
00:51:45,940 --> 00:51:48,060
M technology as soon as it was available,

1214
00:51:48,060 --> 00:51:50,540
more likely there will be resistance.

1215
00:51:50,540 --> 00:51:53,180
There will be ways in which there are obstacles

1216
00:51:53,180 --> 00:51:55,860
to M technology early on.

1217
00:51:55,860 --> 00:51:57,300
And therefore at some point,

1218
00:51:57,300 --> 00:51:58,980
there would basically be the, you know,

1219
00:51:58,980 --> 00:52:01,900
breaking of a dam flooding out where a bunch of things

1220
00:52:01,900 --> 00:52:04,560
that had been held back were released

1221
00:52:04,560 --> 00:52:07,100
and then caused a lot of disruption,

1222
00:52:07,100 --> 00:52:08,620
faster disruption that would have happened

1223
00:52:08,620 --> 00:52:12,300
had you adopted things as soon as they were available.

1224
00:52:12,300 --> 00:52:13,140
And that's part of,

1225
00:52:13,140 --> 00:52:16,300
that can be very disturbing transition then, you know,

1226
00:52:16,300 --> 00:52:18,620
if all of a sudden large numbers of people

1227
00:52:18,620 --> 00:52:21,420
are disrupted in ways they weren't expecting

1228
00:52:21,420 --> 00:52:24,500
in a very rapid way because of, you know,

1229
00:52:24,500 --> 00:52:26,620
a dam suddenly broke open,

1230
00:52:26,620 --> 00:52:30,440
then I think there will be a lot of unhappy people

1231
00:52:30,440 --> 00:52:31,560
in that sort of a transition

1232
00:52:31,560 --> 00:52:33,020
and maybe a lot of dead people.

1233
00:52:33,020 --> 00:52:35,620
So imagine the M technology slowly just gets cheaper

1234
00:52:35,620 --> 00:52:38,600
over time, but it's not very wide.

1235
00:52:38,600 --> 00:52:40,440
It's not very widely adopted.

1236
00:52:40,440 --> 00:52:44,080
Then there'll be a point at which it eventually gets so cheap

1237
00:52:44,080 --> 00:52:46,320
that if some say ambitious nation,

1238
00:52:46,320 --> 00:52:48,040
like say North Korea said,

1239
00:52:48,040 --> 00:52:51,120
gee, if we went whole hog and adopting this thing,

1240
00:52:51,120 --> 00:52:52,960
we could get this big, you know,

1241
00:52:52,960 --> 00:52:55,920
economic and military advantage over our competitors,

1242
00:52:55,920 --> 00:52:58,920
then eventually somebody would do that.

1243
00:52:58,920 --> 00:53:00,880
Now it might take a long time.

1244
00:53:01,800 --> 00:53:04,680
That is the world could coordinate to resistance technology

1245
00:53:04,680 --> 00:53:07,160
for a long time,

1246
00:53:07,160 --> 00:53:10,880
but I don't think they could hold it back for a thousand years.

1247
00:53:10,880 --> 00:53:12,120
So then I feel somewhat confident,

1248
00:53:12,120 --> 00:53:13,960
eventually in the age of M happens,

1249
00:53:14,920 --> 00:53:17,360
and then eventually there's a thing to think about

1250
00:53:17,360 --> 00:53:18,680
and then I'm analyzing that world.

1251
00:53:18,680 --> 00:53:21,520
So I don't want to presume in the age of M

1252
00:53:21,520 --> 00:53:24,040
that this transition happens smoothly or soon

1253
00:53:24,040 --> 00:53:26,440
or as fast as it could,

1254
00:53:26,440 --> 00:53:28,360
but I want to say eventually there'll be this new world

1255
00:53:28,360 --> 00:53:29,680
and here's how it would play out.

1256
00:53:29,680 --> 00:53:32,200
So I don't know if you know that in the last few months

1257
00:53:33,360 --> 00:53:36,160
I've dramatically changed my vision of the future

1258
00:53:36,160 --> 00:53:38,040
to say that there's probably gonna be

1259
00:53:38,040 --> 00:53:40,560
a several century innovation pause,

1260
00:53:40,560 --> 00:53:43,000
probably before the age of M happens,

1261
00:53:43,000 --> 00:53:45,840
and then the world that would eventually produce AI

1262
00:53:45,840 --> 00:53:48,040
and M's would be a very different world from ours

1263
00:53:48,040 --> 00:53:51,400
and somewhat hard to think about.

1264
00:53:51,400 --> 00:53:55,720
That is rising population will stop rising

1265
00:53:55,720 --> 00:53:58,080
and it will fall due to falling fertility,

1266
00:53:58,080 --> 00:54:00,600
that will basically make innovation grind to a halt,

1267
00:54:00,600 --> 00:54:03,200
then the world population will continue to fall

1268
00:54:03,200 --> 00:54:08,280
until insular fertile subcultures like the Amish

1269
00:54:08,280 --> 00:54:10,120
grow from their very small current levels

1270
00:54:10,120 --> 00:54:12,960
to become the dominant population of the world.

1271
00:54:12,960 --> 00:54:14,880
And then when that becomes large enough

1272
00:54:14,880 --> 00:54:16,520
compared to our current economy,

1273
00:54:16,520 --> 00:54:18,160
then innovation would turn on again

1274
00:54:18,160 --> 00:54:21,560
and then we would restart the AI and M path

1275
00:54:21,560 --> 00:54:24,840
and then eventually the age of M would happen.

1276
00:54:24,840 --> 00:54:27,000
Trying to anticipate how transitions would happen

1277
00:54:27,000 --> 00:54:29,760
in a world we can just hardly even imagine,

1278
00:54:29,760 --> 00:54:30,960
seems tough, right?

1279
00:54:31,000 --> 00:54:33,960
That is, okay, imagine the descendants of the Amish

1280
00:54:33,960 --> 00:54:37,600
become a large, powerful civilization.

1281
00:54:37,600 --> 00:54:40,600
They've always been somewhat resistant to technology

1282
00:54:40,600 --> 00:54:43,720
and very picky about which technologies they're allowed,

1283
00:54:43,720 --> 00:54:45,640
but eventually I would predict

1284
00:54:46,840 --> 00:54:48,320
there would be competition within them

1285
00:54:48,320 --> 00:54:53,320
and that would push them to adopt technologies like AI and M's

1286
00:54:53,320 --> 00:54:56,280
but we're looking a long way down the line.

1287
00:54:56,280 --> 00:54:58,240
And this isn't what I wish would happen

1288
00:54:58,240 --> 00:54:59,280
to go back to your initial thing.

1289
00:54:59,280 --> 00:55:01,720
I would rather we continued growing

1290
00:55:01,720 --> 00:55:03,240
at the rate of the past century

1291
00:55:03,240 --> 00:55:05,760
and continue that for a few more centuries,

1292
00:55:05,760 --> 00:55:08,000
by which time I'm pretty sure

1293
00:55:08,000 --> 00:55:10,800
we'll eventually get M's and human level AI,

1294
00:55:10,800 --> 00:55:13,160
although question in what order,

1295
00:55:13,160 --> 00:55:15,480
but I got to say at the moment,

1296
00:55:15,480 --> 00:55:17,400
that's not looking so good.

1297
00:55:17,400 --> 00:55:21,360
So basically, I'm estimated that if we were to continue

1298
00:55:21,360 --> 00:55:25,400
on a steady growth path, we would eventually reach a point

1299
00:55:25,400 --> 00:55:26,960
where we had the same amount of innovation

1300
00:55:26,960 --> 00:55:28,720
as we will get over the entire integral

1301
00:55:28,760 --> 00:55:30,200
of this several centuries pause.

1302
00:55:30,200 --> 00:55:33,040
And I've estimated that to be roughly 60 to 90 years

1303
00:55:33,040 --> 00:55:33,880
worth of progress.

1304
00:55:33,880 --> 00:55:36,960
So if we can get full human level AI

1305
00:55:36,960 --> 00:55:39,320
in the next 60 to 90 years with the progress,

1306
00:55:39,320 --> 00:55:41,680
then this population decline won't matter so much

1307
00:55:41,680 --> 00:55:44,800
because we will basically have AI's takeover most of the jobs

1308
00:55:44,800 --> 00:55:48,640
and then that can allow the world economy to keep growing.

1309
00:55:48,640 --> 00:55:51,840
I think that's iffy whether we can do that,

1310
00:55:51,840 --> 00:55:56,160
whether we can achieve full human level AI in 60 to 90 years.

1311
00:55:56,160 --> 00:55:57,920
And I know many people think it's gonna happen

1312
00:55:57,920 --> 00:55:59,520
in the next 10 years, they're sure.

1313
00:55:59,520 --> 00:56:01,840
So sure, of course it'll happen in 60 to 90 years,

1314
00:56:01,840 --> 00:56:04,480
but I look at the history and I go,

1315
00:56:04,480 --> 00:56:06,720
look, I've seen over and over again,

1316
00:56:06,720 --> 00:56:10,600
people get really excited by the next new kind of AI.

1317
00:56:10,600 --> 00:56:13,200
And they're typically pretty sure,

1318
00:56:13,200 --> 00:56:15,360
a lot of them are pretty sure that we must be near the end

1319
00:56:15,360 --> 00:56:18,040
and pretty soon we'll have it all.

1320
00:56:18,040 --> 00:56:20,840
And it just keeps not happening.

1321
00:56:20,840 --> 00:56:24,680
The main change I wanna suggest to that paradigm

1322
00:56:24,840 --> 00:56:29,840
is replacing the end with meaningful thresholds along the way.

1323
00:56:30,960 --> 00:56:33,360
I think there are probably several

1324
00:56:33,360 --> 00:56:38,000
that we will hit on some time scale.

1325
00:56:38,000 --> 00:56:40,640
And it feels to me like,

1326
00:56:40,640 --> 00:56:44,520
at least a couple of the big ones are pretty close.

1327
00:56:44,520 --> 00:56:48,000
And then at the end is very,

1328
00:56:48,000 --> 00:56:49,400
my crystal ball gets very foggy

1329
00:56:49,400 --> 00:56:51,720
beyond like a pretty short time scale.

1330
00:56:51,720 --> 00:56:55,280
But I'm struggling with the early 80s expert systems,

1331
00:56:55,280 --> 00:56:57,840
but it really does seem like in my lifetime,

1332
00:56:57,840 --> 00:57:02,080
I have not seen anything that remotely resembles

1333
00:57:02,080 --> 00:57:04,080
the experience of going to a doctor.

1334
00:57:04,080 --> 00:57:08,120
I've seen WebMD, I'm familiar with expert systems

1335
00:57:08,120 --> 00:57:10,680
to a degree, but I've never seen anything that,

1336
00:57:10,680 --> 00:57:13,800
I didn't think Ilya Setsgaver from OpenAI

1337
00:57:13,800 --> 00:57:16,320
puts this really well, he's like the most shocking thing

1338
00:57:16,320 --> 00:57:19,600
about the current AIs is that I can speak to them

1339
00:57:19,600 --> 00:57:21,680
and I feel that I'm understood.

1340
00:57:21,680 --> 00:57:25,080
And that is like a qualitatively different experience.

1341
00:57:25,080 --> 00:57:29,520
And clearly I think reflects some qualitative advance

1342
00:57:29,520 --> 00:57:33,920
in terms of what kind of information processing is going on.

1343
00:57:33,920 --> 00:57:36,840
If I had to say like, what is that under the hood?

1344
00:57:36,840 --> 00:57:41,840
I would say it's like a high dimensional representation

1345
00:57:42,440 --> 00:57:47,360
of concepts that are like really relevant to us

1346
00:57:47,400 --> 00:57:50,880
that have previously been kind of limited

1347
00:57:50,880 --> 00:57:53,800
to like language level compressed encoding.

1348
00:57:53,800 --> 00:57:55,880
But now we are actually starting to get to the point

1349
00:57:55,880 --> 00:57:58,240
where we can like look at the middle layers

1350
00:57:58,240 --> 00:58:00,360
of even just the systems we have today,

1351
00:58:00,360 --> 00:58:02,120
the transformers and say,

1352
00:58:02,120 --> 00:58:07,120
can we identify concepts like positivity

1353
00:58:07,960 --> 00:58:11,280
or paranoia or love?

1354
00:58:11,280 --> 00:58:13,600
And we are starting to be able to,

1355
00:58:13,600 --> 00:58:14,960
it's still pretty messy.

1356
00:58:14,960 --> 00:58:16,400
We have the same, not the same,

1357
00:58:16,400 --> 00:58:19,480
we have an analogous problem to like understanding

1358
00:58:19,480 --> 00:58:20,880
what's going on inside the brain

1359
00:58:20,880 --> 00:58:23,720
and it's just a mess in there still in the transformers.

1360
00:58:23,720 --> 00:58:25,080
But we are starting to be able to see these

1361
00:58:25,080 --> 00:58:29,120
like high dimensional representations where it's like,

1362
00:58:29,120 --> 00:58:32,280
that is a numeric representation

1363
00:58:32,280 --> 00:58:33,640
of some of these big concepts.

1364
00:58:33,640 --> 00:58:35,080
And we're even starting to get to the point

1365
00:58:35,080 --> 00:58:37,960
where we can steer the language model behavior

1366
00:58:37,960 --> 00:58:40,000
by like injecting these concepts.

1367
00:58:40,000 --> 00:58:45,000
So you can say, for example, inject safety

1368
00:58:45,000 --> 00:58:46,800
into the middle layers of a transformer

1369
00:58:46,800 --> 00:58:51,800
and get a safer response or danger or rule breaking

1370
00:58:51,840 --> 00:58:54,200
and then they'll be more likely to break their rules.

1371
00:58:54,200 --> 00:58:56,360
What you're focused on at the moment is telling me

1372
00:58:56,360 --> 00:58:59,800
about how the latest generation adds capabilities

1373
00:58:59,800 --> 00:59:02,120
that previous generations didn't have.

1374
00:59:02,120 --> 00:59:04,920
But every previous generation had that same conversation

1375
00:59:04,920 --> 00:59:07,160
where they focused on the new capabilities

1376
00:59:07,160 --> 00:59:10,600
their new generation had that the ones before it didn't have.

1377
00:59:10,600 --> 00:59:13,000
What the conversation you're participating in

1378
00:59:13,000 --> 00:59:15,960
is continuing the past trend.

1379
00:59:17,200 --> 00:59:19,440
But the fundamental question is,

1380
00:59:19,440 --> 00:59:24,440
when will AIs be able to do what fraction of the tasks

1381
00:59:25,200 --> 00:59:27,620
that we have in the human economy,

1382
00:59:27,620 --> 00:59:29,320
if they can't do a large fraction of them,

1383
00:59:29,320 --> 00:59:31,480
no matter how impressive they are at the practice

1384
00:59:31,480 --> 00:59:34,720
they can do, we will see this economic decline

1385
00:59:34,720 --> 00:59:36,240
as the population declines.

1386
00:59:36,240 --> 00:59:39,120
They need to be able to do pretty much all the tasks

1387
00:59:39,120 --> 00:59:41,440
in order to prevent the economic decline

1388
00:59:41,440 --> 00:59:43,200
and then the halting of innovation.

1389
00:59:43,200 --> 00:59:45,760
I did this study of innovation in the United States

1390
00:59:45,760 --> 00:59:50,200
over 20 years from 1999 to 2019.

1391
00:59:50,200 --> 00:59:51,880
And that was a period that encompassed

1392
00:59:51,880 --> 00:59:55,640
what many people at time said was enormous AI progress.

1393
00:59:55,640 --> 00:59:58,560
And many people in the period were talking about

1394
00:59:58,560 --> 01:00:02,200
how there was this revolution in AI

1395
01:00:02,200 --> 01:00:05,640
that was about to cause a revolution in society

1396
01:00:07,240 --> 01:00:10,040
in this period from 1999 to 2019.

1397
01:00:10,040 --> 01:00:14,280
So we did a study, a co-author and I,

1398
01:00:14,280 --> 01:00:18,120
Keller Scholl, who looked at all jobs in the US,

1399
01:00:18,120 --> 01:00:22,040
basically roughly 900 different kinds of jobs.

1400
01:00:22,040 --> 01:00:23,760
And over that 20-year period,

1401
01:00:23,760 --> 01:00:28,200
we had measures of how automated was each job in each year.

1402
01:00:29,960 --> 01:00:34,400
And then we could do statistics to say,

1403
01:00:34,400 --> 01:00:35,920
when jobs got more automated,

1404
01:00:35,920 --> 01:00:38,160
did they get the wages go up or down?

1405
01:00:38,160 --> 01:00:40,920
Did the number of workers in those jobs go up or down?

1406
01:00:40,920 --> 01:00:43,200
And we could say, what about jobs predicts

1407
01:00:43,200 --> 01:00:44,480
how automated they are?

1408
01:00:45,400 --> 01:00:48,320
And did the things that determine which jobs

1409
01:00:48,320 --> 01:00:50,760
or how automated change over that 20-year period?

1410
01:00:52,520 --> 01:00:54,080
That is, if there had been some revolution

1411
01:00:54,080 --> 01:00:55,520
in the nature of automation,

1412
01:00:55,520 --> 01:00:57,480
then the things that predicted which jobs

1413
01:00:57,480 --> 01:01:00,120
would be more automated would have changed over time.

1414
01:01:01,520 --> 01:01:05,120
What we found was that when jobs got more or less automated

1415
01:01:05,120 --> 01:01:06,960
that had no effect on average,

1416
01:01:06,960 --> 01:01:08,680
on wages or number of workers,

1417
01:01:09,520 --> 01:01:11,240
and that the predictors of automation

1418
01:01:11,240 --> 01:01:13,280
didn't change at all over that 20-year period,

1419
01:01:13,280 --> 01:01:16,320
and they remain to be very simple-minded predictors

1420
01:01:16,320 --> 01:01:19,240
that you might expect about automation from long ago.

1421
01:01:19,240 --> 01:01:20,920
The nature of automation hasn't changed

1422
01:01:20,920 --> 01:01:22,440
in the aggregate in the economy.

1423
01:01:22,440 --> 01:01:24,200
Main predictors of automation are

1424
01:01:24,200 --> 01:01:26,240
whether the job has nice, clear measures

1425
01:01:26,240 --> 01:01:27,440
of how well you've done it,

1426
01:01:27,440 --> 01:01:28,880
whether it's in a clean environment

1427
01:01:28,880 --> 01:01:30,800
with fewer disruptions,

1428
01:01:30,800 --> 01:01:33,200
and whether tasks nearby have been automated.

1429
01:01:33,200 --> 01:01:35,720
So there's a way that which task automation spreads

1430
01:01:35,760 --> 01:01:38,240
to the network of nearby tasks.

1431
01:01:38,240 --> 01:01:42,000
So that study suggested at least up until 2019,

1432
01:01:42,000 --> 01:01:44,680
there had been no change in the nature of automation,

1433
01:01:44,680 --> 01:01:47,520
and basically there's a Gaussian distribution

1434
01:01:47,520 --> 01:01:49,360
of how automated jobs are,

1435
01:01:49,360 --> 01:01:52,120
and the median automation had moved roughly

1436
01:01:52,120 --> 01:01:55,280
a third of a standard deviation through that distribution.

1437
01:01:55,280 --> 01:01:57,960
So jobs had gotten more automated substantially

1438
01:01:57,960 --> 01:01:58,960
in that 20-year period,

1439
01:01:58,960 --> 01:02:03,480
but still most jobs aren't that automated.

1440
01:02:03,520 --> 01:02:05,200
And that would be my rough prediction

1441
01:02:05,200 --> 01:02:07,760
for the next 20 years is to say

1442
01:02:07,760 --> 01:02:10,160
the pattern of the last 20 years will continue.

1443
01:02:10,160 --> 01:02:12,800
That is, I will slowly get more jobs more automated,

1444
01:02:12,800 --> 01:02:16,480
but most automation will be very basic stuff.

1445
01:02:16,480 --> 01:02:19,040
So far we just haven't seen much at all

1446
01:02:19,040 --> 01:02:21,680
of advanced AI kinds of automation

1447
01:02:21,680 --> 01:02:24,160
making a dent in the larger economy.

1448
01:02:24,160 --> 01:02:25,800
So what do you make of things,

1449
01:02:25,800 --> 01:02:29,120
I'm sure you're familiar with like the MMLU benchmark

1450
01:02:29,120 --> 01:02:30,320
or the big bench, maybe not,

1451
01:02:30,320 --> 01:02:32,960
if not I can characterize them for you, but.

1452
01:02:32,960 --> 01:02:36,120
Is this machine learning set of tests

1453
01:02:36,120 --> 01:02:38,800
in order to benchmark performance?

1454
01:02:38,800 --> 01:02:42,000
Yes, I believe it's massive multi-task language

1455
01:02:42,000 --> 01:02:45,680
understanding, the great Dan Hendricks and team.

1456
01:02:45,680 --> 01:02:48,600
So basically a bunch of language understanding benchmarks?

1457
01:02:48,600 --> 01:02:51,480
Yeah, they basically went and took final exams

1458
01:02:51,480 --> 01:02:56,480
from like university and early grad school courses

1459
01:02:56,720 --> 01:02:59,160
from every domain and compiled them

1460
01:02:59,160 --> 01:03:00,640
into this massive benchmark.

1461
01:03:00,640 --> 01:03:02,000
There have been a couple of different efforts like this,

1462
01:03:02,000 --> 01:03:03,840
but this is basically the gold standard

1463
01:03:03,840 --> 01:03:06,120
on which all the language models are measured.

1464
01:03:07,240 --> 01:03:12,240
And we now have a like high 80s to 90% accuracy rate

1465
01:03:13,800 --> 01:03:17,240
across all fields from like a single model, namely GPT-4.

1466
01:03:17,240 --> 01:03:19,400
And now Google claims that it's Gemini

1467
01:03:19,400 --> 01:03:22,180
is hitting that level as well.

1468
01:03:23,040 --> 01:03:27,520
I would agree that these have not been broadly customized

1469
01:03:27,520 --> 01:03:29,600
to the last mile specifications that they need

1470
01:03:29,600 --> 01:03:31,760
to like work in the context of different firms

1471
01:03:31,760 --> 01:03:35,720
and cultural contexts and all that sort of thing.

1472
01:03:35,720 --> 01:03:38,680
But it does seem like the way I typically describe it

1473
01:03:38,680 --> 01:03:42,960
is that AIs are now better at routine tasks

1474
01:03:42,960 --> 01:03:45,120
than the average person and that they are closing in

1475
01:03:45,120 --> 01:03:49,120
on expert performance on routine tasks.

1476
01:03:49,120 --> 01:03:51,880
And that's measured by these medical diagnosis benchmarks,

1477
01:03:51,880 --> 01:03:56,000
these MMLU type things, et cetera, et cetera.

1478
01:03:56,000 --> 01:04:00,120
So let me remind you that in the 1960s say

1479
01:04:01,120 --> 01:04:06,120
AI researchers took chess as a paradigm of

1480
01:04:06,200 --> 01:04:08,880
if you can make a machine that can do that,

1481
01:04:08,880 --> 01:04:10,760
well, obviously you'll have to have solved

1482
01:04:10,760 --> 01:04:12,600
most of the major problems in thinking

1483
01:04:12,600 --> 01:04:15,200
because chess involves most of the major problems

1484
01:04:15,200 --> 01:04:16,040
in thinking.

1485
01:04:16,040 --> 01:04:19,680
So when we can finally have human level chess abilities,

1486
01:04:19,680 --> 01:04:21,680
we will have human level AI.

1487
01:04:21,680 --> 01:04:23,120
That was the thinking in the 60s

1488
01:04:23,120 --> 01:04:25,640
and they could look at the rate at which AI

1489
01:04:25,640 --> 01:04:28,400
was getting better at chess and forecast long before

1490
01:04:28,400 --> 01:04:31,840
it happened that in the late 1970s, 1990s, excuse me,

1491
01:04:31,840 --> 01:04:35,880
is exactly when chess would reach human level ability

1492
01:04:35,880 --> 01:04:37,760
and that's when it did happen.

1493
01:04:37,760 --> 01:04:40,320
And that was 25 years ago.

1494
01:04:40,320 --> 01:04:42,400
And clearly they were just wrong about the idea

1495
01:04:42,400 --> 01:04:44,600
that you couldn't do chess without solving

1496
01:04:44,600 --> 01:04:46,160
all the major thinking problems.

1497
01:04:46,160 --> 01:04:48,680
And we repeatedly have this sort of phenomena

1498
01:04:48,680 --> 01:04:51,840
where people look at something and they go,

1499
01:04:51,840 --> 01:04:54,880
if you can do that, surely you can do most everything.

1500
01:04:54,880 --> 01:04:57,680
And then we can do that and we can't do near,

1501
01:04:57,720 --> 01:04:59,360
and we aren't near to doing most everything.

1502
01:04:59,360 --> 01:05:01,640
So I just got to say this benchmark is just wrong.

1503
01:05:01,640 --> 01:05:05,120
It's not true that if you can do these language benchmark,

1504
01:05:05,120 --> 01:05:06,640
you are near to doing most everything.

1505
01:05:06,640 --> 01:05:08,200
You are not near.

1506
01:05:08,200 --> 01:05:09,920
Yeah, I would find my position to say,

1507
01:05:09,920 --> 01:05:13,080
I think you're near to being able to do all the routine things

1508
01:05:13,080 --> 01:05:16,080
that are well documented in the training data.

1509
01:05:16,080 --> 01:05:18,320
Well, yes, but the question is in the economy,

1510
01:05:18,320 --> 01:05:21,000
all the things we need doing, how close are you to that?

1511
01:05:21,000 --> 01:05:23,000
And say you're not close.

1512
01:05:23,000 --> 01:05:27,120
I mean, we're seeing just the very beginning of sort of,

1513
01:05:27,120 --> 01:05:29,560
I mean, again, I don't know, like...

1514
01:05:29,560 --> 01:05:30,960
What do you think was going on in their head

1515
01:05:30,960 --> 01:05:33,800
in the 1960s when they looked at chess, right?

1516
01:05:33,800 --> 01:05:35,040
They looked at chess and they said,

1517
01:05:35,040 --> 01:05:36,720
it takes really smart people to do chess,

1518
01:05:36,720 --> 01:05:38,960
look at all these complicated things people are doing

1519
01:05:38,960 --> 01:05:41,360
when they do chess in order to achieve in chess,

1520
01:05:41,360 --> 01:05:42,600
they said to themselves,

1521
01:05:42,600 --> 01:05:44,000
that's the sort of thing we should work on

1522
01:05:44,000 --> 01:05:45,440
because if we can get a machine to do that,

1523
01:05:45,440 --> 01:05:49,920
surely we must be close to general artificial intelligence.

1524
01:05:49,920 --> 01:05:52,800
If you could have something that could do chess.

1525
01:05:52,800 --> 01:05:55,240
And there is a sense that when you have general intelligence,

1526
01:05:55,240 --> 01:05:58,040
you can use all of that to do clever things about chess,

1527
01:05:58,040 --> 01:06:01,400
but it's not true that you need to have all those general things

1528
01:06:01,400 --> 01:06:02,400
in order to be good at chess.

1529
01:06:02,400 --> 01:06:04,200
That turns out there's a way to be good at chess

1530
01:06:04,200 --> 01:06:05,840
without doing all those other things.

1531
01:06:05,840 --> 01:06:07,640
And that's repeatedly been the problem

1532
01:06:07,640 --> 01:06:09,400
and that could be the problem today.

1533
01:06:09,400 --> 01:06:12,280
Turns out there's a way to do these exam answering things

1534
01:06:12,280 --> 01:06:16,960
that doesn't require the full range of general intelligence

1535
01:06:16,960 --> 01:06:18,680
in order to achieve that task.

1536
01:06:18,680 --> 01:06:21,120
It's hard to pick a good range of tasks

1537
01:06:21,120 --> 01:06:24,120
that encompasses the full range of intelligence

1538
01:06:24,120 --> 01:06:26,760
because again, you teach through the test

1539
01:06:26,760 --> 01:06:28,840
and you end up finding a way to solve that problem

1540
01:06:28,840 --> 01:06:31,680
without achieving general intelligence.

1541
01:06:31,680 --> 01:06:33,360
This does seem different though.

1542
01:06:33,360 --> 01:06:35,560
I mean, I would, I grew with your characterization

1543
01:06:35,560 --> 01:06:39,080
that basically it turned out that there was an easier way

1544
01:06:39,080 --> 01:06:42,840
or a more direct way, a narrower way to solve chess.

1545
01:06:42,840 --> 01:06:47,080
And it's interesting that it's like rather different.

1546
01:06:47,080 --> 01:06:50,640
You know, it involves these sort of superhuman tree search

1547
01:06:50,640 --> 01:06:51,600
capabilities.

1548
01:06:51,600 --> 01:06:52,960
But that wasn't just true of trust.

1549
01:06:52,960 --> 01:06:55,880
There were another dozen sorts of really hard problems

1550
01:06:55,880 --> 01:07:00,880
that people in the 1960s took as exemplars of things

1551
01:07:00,880 --> 01:07:02,440
that would require general intelligence

1552
01:07:02,440 --> 01:07:04,560
and the great many of them have been achieved.

1553
01:07:04,560 --> 01:07:07,240
But when I look at the current situation,

1554
01:07:07,240 --> 01:07:09,960
I'm like, this does look a lot more

1555
01:07:09,960 --> 01:07:12,080
like the human intelligence.

1556
01:07:12,080 --> 01:07:16,240
And I would say that from any number of different directions.

1557
01:07:16,240 --> 01:07:19,360
And that was true in every decade for the last century.

1558
01:07:20,360 --> 01:07:24,080
Every decade has seen advances that were not the sort

1559
01:07:24,080 --> 01:07:26,080
that previous systems could achieve.

1560
01:07:26,080 --> 01:07:27,960
It's clear that you are always, I think it's clear

1561
01:07:27,960 --> 01:07:32,160
that you don't see the human brain, the human, you know,

1562
01:07:32,160 --> 01:07:35,640
achieve level of achievement as sort of a maximum, right?

1563
01:07:35,640 --> 01:07:37,080
Oh, of course not. Absolutely.

1564
01:07:37,080 --> 01:07:41,240
So it's like there's got to be a finite number

1565
01:07:41,240 --> 01:07:45,000
of breakthroughs that need to happen.

1566
01:07:45,000 --> 01:07:47,240
We will eventually get full human level AI.

1567
01:07:47,240 --> 01:07:48,680
I have no doubt about that.

1568
01:07:48,680 --> 01:07:53,520
And not soon after vastly exceeded, that will happen.

1569
01:07:53,520 --> 01:07:56,480
And it will happen plausibly within the next thousand years.

1570
01:07:56,480 --> 01:08:00,200
It also seems like you would probably agree that it need not

1571
01:08:00,200 --> 01:08:04,800
be point for point, you know, the M scenario is a great one

1572
01:08:04,800 --> 01:08:07,600
to play out and analyze, but it need not be the case.

1573
01:08:07,600 --> 01:08:10,640
Right. So the AIs could be much better than humans in some ways

1574
01:08:10,640 --> 01:08:12,440
and still much worse than others.

1575
01:08:12,440 --> 01:08:15,280
That will probably actually be true for a long time.

1576
01:08:15,280 --> 01:08:17,880
That is, it'll take a lot longer till AIs are better

1577
01:08:17,880 --> 01:08:20,240
than humans at most everything than that they are better

1578
01:08:20,240 --> 01:08:23,240
at humans at say half of things people do today.

1579
01:08:23,240 --> 01:08:24,640
But of course you have to realize if you looked

1580
01:08:24,640 --> 01:08:28,240
at what humans were doing two centuries ago, we're already

1581
01:08:28,240 --> 01:08:30,760
at the point where machines do those things much better

1582
01:08:30,760 --> 01:08:32,480
than humans can do.

1583
01:08:32,480 --> 01:08:34,760
That is, the attack, most tasks that humans were doing

1584
01:08:34,760 --> 01:08:37,960
two centuries ago are already long since automated.

1585
01:08:37,960 --> 01:08:40,640
We've now switched our attention to the sort of tasks

1586
01:08:40,640 --> 01:08:42,560
that people were not doing two centuries ago.

1587
01:08:42,560 --> 01:08:45,080
And on those, we're not so good at making machines do them,

1588
01:08:45,080 --> 01:08:49,120
but we've already dramatically achieved full automation basically

1589
01:08:49,120 --> 01:08:51,840
of most things humans were doing two centuries ago.

1590
01:08:51,840 --> 01:08:53,760
Which for very shorthand I would say is kind

1591
01:08:53,760 --> 01:08:58,000
of routine repetitive physical tasks.

1592
01:08:58,000 --> 01:09:00,080
Right. I mean, we managed to change the environment

1593
01:09:00,080 --> 01:09:02,400
to make them more routine and repetitive.

1594
01:09:02,400 --> 01:09:05,880
So, you know, a subsistence farmer

1595
01:09:05,880 --> 01:09:08,720
on a subsistence farm two centuries ago, they were,

1596
01:09:08,720 --> 01:09:11,200
we couldn't, our automation could not do that job

1597
01:09:11,200 --> 01:09:12,520
that they were doing that.

1598
01:09:12,520 --> 01:09:14,080
And we managed to make the farms different.

1599
01:09:14,080 --> 01:09:15,960
The factory is different, et cetera, so that our machines

1600
01:09:15,960 --> 01:09:17,360
could do them.

1601
01:09:17,360 --> 01:09:20,000
And now they are producing much more than those people produce.

1602
01:09:20,000 --> 01:09:22,600
But if you had to try to produce the way they were doing

1603
01:09:22,600 --> 01:09:25,680
two centuries ago, our machines today could not do that.

1604
01:09:25,680 --> 01:09:28,680
Yeah, a big theory I have also, I actually don't think this is going

1605
01:09:28,680 --> 01:09:32,160
to be a huge, well, everything's going to be huge,

1606
01:09:32,160 --> 01:09:35,800
but I don't think it's going to be like the dominant change

1607
01:09:35,800 --> 01:09:38,960
that leads to qualitatively different future.

1608
01:09:38,960 --> 01:09:41,760
But I do think we will start to see, and are beginning

1609
01:09:41,840 --> 01:09:46,160
to see that same process happening with language models,

1610
01:09:46,160 --> 01:09:49,520
where, you know, I consult with a few different businesses

1611
01:09:49,520 --> 01:09:52,240
and we have kind of processes that, you know,

1612
01:09:52,240 --> 01:09:54,000
we would like to automate.

1613
01:09:54,000 --> 01:09:57,800
You know, a classic one would be like initial resume screening.

1614
01:09:57,800 --> 01:09:59,600
Right. We're not going to have the language model at this point

1615
01:09:59,600 --> 01:10:00,920
make the hiring decisions.

1616
01:10:00,920 --> 01:10:02,920
But if we get a lot of garbage resumes, you know,

1617
01:10:02,920 --> 01:10:08,160
we can definitely get language models to kind of band the resumes

1618
01:10:08,160 --> 01:10:12,080
into, you know, one to five and like spend our time on the fives.

1619
01:10:13,280 --> 01:10:16,080
It does seem to me that there's a lot of kind of process

1620
01:10:16,080 --> 01:10:20,400
and environment adaptation that is not that hard to do.

1621
01:10:20,400 --> 01:10:23,240
Like I personally have done it successfully across a handful

1622
01:10:23,240 --> 01:10:24,560
of different things.

1623
01:10:24,560 --> 01:10:28,360
Why it seems like you're announced as though a sort of doesn't

1624
01:10:29,320 --> 01:10:31,640
assumes that that's not going to happen at scale this time

1625
01:10:31,640 --> 01:10:34,040
around with the technology we currently have.

1626
01:10:34,480 --> 01:10:39,240
I said, you know, in the last 20 years from 1999 to 2019,

1627
01:10:39,240 --> 01:10:41,720
we moved roughly a third of a standard deviation

1628
01:10:41,720 --> 01:10:44,000
in the distribution of automation.

1629
01:10:44,000 --> 01:10:47,640
OK, so what if we in the next 60 years

1630
01:10:48,400 --> 01:10:52,640
move a third of a standard deviation in each of the 20 year periods?

1631
01:10:52,640 --> 01:10:55,760
Then over 60 years, we would basically move an entire standard deviation.

1632
01:10:56,880 --> 01:11:01,400
That could represent a large increase in automation

1633
01:11:01,880 --> 01:11:04,000
over the next 60 years.

1634
01:11:04,120 --> 01:11:07,040
And that would mean a lot of things we're doing by hand today

1635
01:11:07,040 --> 01:11:08,560
will be done by machines.

1636
01:11:08,560 --> 01:11:11,240
Then it would mean our economy is more productive,

1637
01:11:11,840 --> 01:11:15,120
but it still would mean humans have a huge place in the world.

1638
01:11:15,120 --> 01:11:19,240
They get paid and most income probably still goes to pay humans to do work,

1639
01:11:19,880 --> 01:11:22,520
even though they have much better automation at the time.

1640
01:11:23,280 --> 01:11:25,840
If that's the situation in 60 years,

1641
01:11:26,960 --> 01:11:30,320
then unfortunately that level of increase in automation

1642
01:11:30,320 --> 01:11:32,480
is just not sufficient to prevent the economy

1643
01:11:32,480 --> 01:11:35,200
from declining as population declines.

1644
01:11:35,200 --> 01:11:38,440
And so we won't get much more automation than that.

1645
01:11:39,640 --> 01:11:43,480
The well of it in automation will dry up because innovation will stop.

1646
01:11:43,520 --> 01:11:46,040
And we would then have a several centuries long period

1647
01:11:46,040 --> 01:11:48,840
where our technology does not improve.

1648
01:11:48,840 --> 01:11:51,640
And in fact, we lose a lot of technologies tied to scale economies

1649
01:11:52,160 --> 01:11:54,480
as the world economy shrinks.

1650
01:11:54,480 --> 01:11:59,480
We'll manage to have less variety, less large scale production and distribution.

1651
01:12:00,280 --> 01:12:03,880
And we would then struggle to maintain previous technologies.

1652
01:12:03,880 --> 01:12:06,880
And AI is at risk of the sort of technology would be hard to maintain

1653
01:12:06,880 --> 01:12:10,920
because at the moment, AI is a really large scale, concentrated sort of technology

1654
01:12:10,920 --> 01:12:14,800
is not being done by mom and pops to be done by very large enterprises

1655
01:12:14,800 --> 01:12:16,240
on very large scales.

1656
01:12:16,240 --> 01:12:21,480
I would agree that the supply chain is definitely prone to disruption in AI.

1657
01:12:21,480 --> 01:12:22,840
No doubt about that.

1658
01:12:22,840 --> 01:12:27,840
Can you describe in more detail what what is the standard deviation

1659
01:12:27,840 --> 01:12:30,880
in automation and how should I conceptualize that?

1660
01:12:31,440 --> 01:12:34,760
I mean, I guess what you'd want to do is see a list of tasks

1661
01:12:34,760 --> 01:12:42,240
and how automated each task was and then see sort of how much on that score.

1662
01:12:42,240 --> 01:12:45,080
And it would have. So basically, if you look on this list

1663
01:12:45,080 --> 01:12:48,160
at the most and least automated tasks, you'll agree, which are which

1664
01:12:48,880 --> 01:12:52,320
like the nearly most automated task is airline pilots.

1665
01:12:53,320 --> 01:12:56,320
Nearly the least automated task is carpet installers.

1666
01:12:57,320 --> 01:13:02,040
Carpet installers use pretty much no automation to staple in carpets.

1667
01:13:02,040 --> 01:13:07,320
And airline pilots are pretty much always having automation help what they're doing.

1668
01:13:08,120 --> 01:13:12,320
And then, you know, you can see the scores in the middle and see that we've,

1669
01:13:12,320 --> 01:13:15,280
you know, moved up a modest degree over those 20 years.

1670
01:13:15,800 --> 01:13:18,920
That would be the way to get an intuition for it is just to see a list

1671
01:13:18,920 --> 01:13:22,720
of particular jobs in their automation scores and then see,

1672
01:13:22,720 --> 01:13:24,960
compare that to the amount by which we've moved up.

1673
01:13:25,680 --> 01:13:26,640
How do you reconcile?

1674
01:13:26,640 --> 01:13:31,320
Or how should I understand the idea that

1675
01:13:31,720 --> 01:13:34,560
whatever doubling time of the economy today,

1676
01:13:34,840 --> 01:13:37,480
I think you said it was like 15 years in the book,

1677
01:13:37,480 --> 01:13:40,800
which seemed a little fast to me, just based on like rule of 70.

1678
01:13:41,360 --> 01:13:44,080
Right. I think it's more like, you know, 20 or something now.

1679
01:13:44,320 --> 01:13:47,920
But still, like it seems it seems like there's a little bit of a disconnect

1680
01:13:47,920 --> 01:13:52,920
between a notion of, you know, over these next 60 years,

1681
01:13:52,920 --> 01:13:57,280
we would be double, double, double, you know, essentially 10xing the economy.

1682
01:13:57,680 --> 01:14:02,600
But we'd only move at sort of a linear rate in automation.

1683
01:14:02,600 --> 01:14:06,240
Like we would only move a third of a standard deviation in each period.

1684
01:14:06,480 --> 01:14:08,080
Let me help you understand that then.

1685
01:14:08,080 --> 01:14:12,240
People have often said, look, computer technology is increasing exponentially.

1686
01:14:12,640 --> 01:14:16,400
Therefore, we should expect an exponential impact on the economy,

1687
01:14:16,400 --> 01:14:21,080
i.e. early on hardly any impact, and then suddenly an accelerating boom

1688
01:14:21,120 --> 01:14:24,360
such that we get this big explosion and then everything happens.

1689
01:14:24,560 --> 01:14:26,040
But that's not what we've seen.

1690
01:14:26,040 --> 01:14:31,760
So what we've seen over time is relatively steady effects on the economy of automation,

1691
01:14:31,760 --> 01:14:34,640
even though the economy is growing exponentially.

1692
01:14:35,840 --> 01:14:40,920
The way I help you understand that is to imagine the distribution of all tasks

1693
01:14:40,920 --> 01:14:44,760
that you might want automated and that they're the degree of computing power,

1694
01:14:44,880 --> 01:14:49,120
both in hardware and software, required to automate that task for each task

1695
01:14:49,160 --> 01:14:53,320
is distributed in a log normal way with a very large variance.

1696
01:14:53,480 --> 01:14:58,040
That is, there's a very large range of how much computing power it takes to automate a task.

1697
01:14:58,360 --> 01:15:00,560
As computing power increases exponentially,

1698
01:15:00,560 --> 01:15:04,320
you're basically moving through that log normal distribution in a linear manner.

1699
01:15:04,720 --> 01:15:07,960
And in the middle of the distribution, it's pretty steady effect.

1700
01:15:08,600 --> 01:15:12,880
You slowly chop away at tasks as you are able to automate them

1701
01:15:13,440 --> 01:15:18,520
because you're slowly acquiring sufficient hardware to do that task.

1702
01:15:19,280 --> 01:15:21,840
That that gives you a simple model, but in which

1703
01:15:22,960 --> 01:15:25,080
computing power grows exponentially.

1704
01:15:25,080 --> 01:15:29,800
And yet you see a relatively steady erosion of tasks through automation.

1705
01:15:30,160 --> 01:15:32,320
It's a low hanging fruit argument.

1706
01:15:32,320 --> 01:15:35,040
Yeah, the low hanging fruits are hanging really low.

1707
01:15:35,720 --> 01:15:40,040
That this this is a log normal tree, basically, that you're trying to grab things from.

1708
01:15:40,040 --> 01:15:43,040
I mean, you're growing your ladder is growing exponentially into the tree.

1709
01:15:43,360 --> 01:15:46,200
And every time your ladder gets taller, you get to pick more feuds.

1710
01:15:46,200 --> 01:15:47,880
But it's a really tall tree.

1711
01:15:47,920 --> 01:15:50,080
That means that you have a long, long way to go.

1712
01:15:50,920 --> 01:15:55,680
How do you think about things like the progress in AI art generation

1713
01:15:55,680 --> 01:15:58,520
or like deep fakes over the last couple of years?

1714
01:15:58,520 --> 01:16:04,400
This is an area where I feel like if we rewound to two years ago,

1715
01:16:04,720 --> 01:16:09,080
just two years ago, really, when I was first starting to see AI art

1716
01:16:09,720 --> 01:16:12,200
popping up on Twitter and it was like

1717
01:16:13,200 --> 01:16:16,360
not very good for the most part, you'd see the occasional thing where you're like,

1718
01:16:16,360 --> 01:16:17,360
oh, that's really compelling.

1719
01:16:17,360 --> 01:16:20,800
And then you'd see a lot of stuff that was like, yeah, you know, it's whatever.

1720
01:16:20,800 --> 01:16:22,680
It's it's remarkable that you can do that.

1721
01:16:23,280 --> 01:16:27,520
It's a while compared to what came before, but it, you know, it's like

1722
01:16:28,920 --> 01:16:32,680
I'm not going to be watching like feature films based on this technology

1723
01:16:32,680 --> 01:16:34,880
and, you know, in the immediate future.

1724
01:16:34,880 --> 01:16:38,320
I feel like we could have had a very similar discussion where you might say,

1725
01:16:38,320 --> 01:16:40,440
well, you know, yeah, it's progress.

1726
01:16:40,440 --> 01:16:45,360
But, you know, the real human art, the top notch stuff, like that's so far away.

1727
01:16:45,800 --> 01:16:50,680
And then early last year, my teammates at Waymark made a short film

1728
01:16:50,680 --> 01:16:57,120
using nothing but Dolly 2 at that time imagery and some definite elbow grease.

1729
01:16:57,120 --> 01:16:59,600
But like the quality of production that they were able to achieve

1730
01:16:59,880 --> 01:17:05,280
with a half dozen people and Dolly 2 is on the level that like previously

1731
01:17:05,280 --> 01:17:09,160
would have taken, you know, a crew in Antarctica, you know, to go shoot.

1732
01:17:09,880 --> 01:17:12,040
You know, again, is that work all done?

1733
01:17:12,040 --> 01:17:15,200
No. But if you look at the mid journey outputs today,

1734
01:17:15,240 --> 01:17:17,800
you look at some of the deep fake technologies that are happening today.

1735
01:17:17,800 --> 01:17:20,320
It's like it does feel like we've hit certainly

1736
01:17:20,320 --> 01:17:23,600
photo realistic thresholds, you know, almost indistinguishable

1737
01:17:23,960 --> 01:17:26,960
from photography with mid journey and with the deep fakes.

1738
01:17:27,440 --> 01:17:30,800
You're not quite quite there yet, but like watch out for 2024

1739
01:17:30,840 --> 01:17:37,280
to have a lot of stories of people being scammed by the kind of custom

1740
01:17:37,280 --> 01:17:40,920
text to speech voice, you know, with a family member, family members voice, whatever.

1741
01:17:41,120 --> 01:17:44,120
All my voice out there, you know, people are going to be calling my parents with my voice.

1742
01:17:44,640 --> 01:17:48,240
So I guess what I'm trying to get at there is like it seems like even just

1743
01:17:48,240 --> 01:17:52,720
in the last couple of years, we have these examples where we are seeing

1744
01:17:52,720 --> 01:17:59,440
like really rapid progress that is not stopping before critical thresholds.

1745
01:17:59,960 --> 01:18:03,200
In the 1960s, there was a U.S.

1746
01:18:03,200 --> 01:18:07,960
Presidential Commission to to address and study the question

1747
01:18:07,960 --> 01:18:10,040
of whether most jobs were about to be automated.

1748
01:18:10,040 --> 01:18:13,440
It reached that level of high level concern in the country.

1749
01:18:13,480 --> 01:18:15,800
And major media discussion about it.

1750
01:18:17,280 --> 01:18:21,840
Ever since then, we continue to have periodic articles about dramatic,

1751
01:18:22,040 --> 01:18:26,960
exciting progress in AI and what that might mean for the society and economy.

1752
01:18:27,240 --> 01:18:30,280
And in all those articles through all those years,

1753
01:18:30,960 --> 01:18:34,560
they don't just talk in the abstract, they usually pick out some particular examples

1754
01:18:35,040 --> 01:18:37,440
and they don't pick out random examples from the economy.

1755
01:18:37,440 --> 01:18:40,520
They pick out the examples where the automation has made the most difference.

1756
01:18:41,520 --> 01:18:44,360
That, of course, makes sense if you're trying to make an exciting story.

1757
01:18:45,640 --> 01:18:49,160
And so we've always been able to pick out the things

1758
01:18:49,160 --> 01:18:51,720
which are having the most dramatic increase lately

1759
01:18:51,720 --> 01:18:54,280
that also seem the most salient and interesting.

1760
01:18:54,440 --> 01:18:57,680
And now you can pick out image generation

1761
01:18:58,320 --> 01:19:02,520
as one of the main examples lately as something that's increased a lot lately.

1762
01:19:02,800 --> 01:19:05,200
And I'm happy to admit it has.

1763
01:19:05,200 --> 01:19:07,600
I would put it up, you know, and that's the sort of thing

1764
01:19:07,640 --> 01:19:10,640
that somebody writing an article today about the exciting AI progress

1765
01:19:10,640 --> 01:19:14,680
would, in fact, mention and talk about graphic artists being put out of work

1766
01:19:14,680 --> 01:19:18,520
by the availability of these things, which probably is happening.

1767
01:19:19,040 --> 01:19:22,800
The point is just to realize how selective that process is

1768
01:19:23,040 --> 01:19:26,320
to pick out the most dramatic impacts and to realize just how many other jobs

1769
01:19:26,320 --> 01:19:30,200
there are and how many other tasks there are and then how far we still have to go.

1770
01:19:30,840 --> 01:19:33,440
I'm happy to celebrate recent progress.

1771
01:19:33,440 --> 01:19:37,640
And if I were, you know, if I were a graphic artist person,

1772
01:19:37,640 --> 01:19:41,040
I would be especially excited to figure out how to take advantage of these changes

1773
01:19:41,560 --> 01:19:44,240
because they are among the biggest change.

1774
01:19:44,520 --> 01:19:46,520
If you're, say, a 20 year old in the world,

1775
01:19:46,520 --> 01:19:49,920
it makes complete sense to say, where are things most exciting and changing?

1776
01:19:50,040 --> 01:19:53,280
I want to go there and be part of the new exciting thing happening there.

1777
01:19:53,960 --> 01:19:57,200
If, of course, you're a 60 year old and you've already invested in a career,

1778
01:19:57,200 --> 01:20:00,880
then it makes less sense to, like, try to switch your whole career over to a new thing.

1779
01:20:00,920 --> 01:20:03,520
But a lot of people are at the beginning of their career and they should.

1780
01:20:03,840 --> 01:20:06,160
They should look for where the most exciting changes are

1781
01:20:06,160 --> 01:20:08,400
and try to see if they can go be part of that.

1782
01:20:08,960 --> 01:20:10,040
Move West, young man.

1783
01:20:10,040 --> 01:20:11,920
If West is where things are happening, right?

1784
01:20:11,920 --> 01:20:14,600
But you still have to keep in mind if there's a few people going out West

1785
01:20:14,600 --> 01:20:18,760
making exciting things happening, how big a percentage of the world is the West, right?

1786
01:20:19,520 --> 01:20:22,320
Yes, it's exciting and there's huge growth in the West.

1787
01:20:22,320 --> 01:20:25,880
You know, 10 years ago, there was hardly anything and now there's a big town.

1788
01:20:26,120 --> 01:20:27,560
Look how great the West is growing.

1789
01:20:27,560 --> 01:20:31,120
And that, you know, there are always times and places where right there,

1790
01:20:31,120 --> 01:20:36,520
things are growing very fast and newspaper writers should focus on those to tell stories

1791
01:20:37,160 --> 01:20:39,920
and keep novelists should focus on those to tell stories.

1792
01:20:39,920 --> 01:20:41,960
They're exciting places where exciting things are happening.

1793
01:20:41,960 --> 01:20:45,320
And I want to make sure the world keeps having things like that happening

1794
01:20:45,320 --> 01:20:47,760
because that's how we can keep growing.

1795
01:20:47,960 --> 01:20:50,480
But you have to be honest about the fraction of the world

1796
01:20:50,480 --> 01:20:52,680
that's involved in those exciting frontier stories.

1797
01:20:53,600 --> 01:20:57,200
Yeah, I mean, I guess my kind of counterpoint to that would be

1798
01:20:57,680 --> 01:21:03,280
the same relatively simple technology, like the transformer

1799
01:21:03,280 --> 01:21:07,560
or like the attention mechanism, perhaps it is better, you know, pinpointed as

1800
01:21:08,320 --> 01:21:11,120
is driving this art creation.

1801
01:21:11,720 --> 01:21:16,320
It's also writing today like short programs.

1802
01:21:16,320 --> 01:21:20,280
Yeah, I would personally say my productivity as a programmer has been

1803
01:21:20,600 --> 01:21:25,000
increased like several fold, not like incrementally, but like multiple

1804
01:21:25,320 --> 01:21:29,440
with GPT for assistance, you know, it's the wide range where you could go on.

1805
01:21:29,440 --> 01:21:32,200
But like it's it's also happening in metal medical diagnosis.

1806
01:21:32,200 --> 01:21:36,800
It's also happening in like protein, you know, novel protein structure generation.

1807
01:21:36,800 --> 01:21:39,680
And certainly from an economic point of view, the biggest category

1808
01:21:39,680 --> 01:21:41,200
you've mentioned is programming.

1809
01:21:41,200 --> 01:21:44,760
That's a much larger industry, less of your profession than the other ones you mentioned.

1810
01:21:45,040 --> 01:21:47,880
Well, but watch out for biotech also, I would say, for sure.

1811
01:21:47,880 --> 01:21:50,520
But biotech has been shrinking for a while.

1812
01:21:50,520 --> 01:21:53,880
So that's not an exact thing you should point to as a growing thing.

1813
01:21:54,280 --> 01:21:56,600
I will predict growth for biotech, definitely.

1814
01:21:56,760 --> 01:21:59,160
I mean, you know, it's also it's reading brain states.

1815
01:21:59,160 --> 01:22:02,360
Have you seen these recent things where people can read the brain state?

1816
01:22:02,680 --> 01:22:04,640
Among the things you're talking about at the moment, the biggest

1817
01:22:04,640 --> 01:22:07,120
profession being affected is programming, clearly.

1818
01:22:07,320 --> 01:22:09,560
I have a younger son, two sons.

1819
01:22:09,560 --> 01:22:11,240
My younger one is a professional programmer.

1820
01:22:11,240 --> 01:22:14,960
So, you know, I've had him look at and his

1821
01:22:15,600 --> 01:22:18,400
workplace has looked into what they can do with large language models

1822
01:22:18,400 --> 01:22:19,560
to help them write programs.

1823
01:22:19,560 --> 01:22:24,160
And their evaluation so far is, you know, they don't even

1824
01:22:25,280 --> 01:22:26,960
they'll wait in six months to look again.

1825
01:22:26,960 --> 01:22:28,440
It's not useful now.

1826
01:22:28,440 --> 01:22:30,640
Can I short that stock?

1827
01:22:31,560 --> 01:22:35,040
Well, I could tell you after we finish what that is.

1828
01:22:35,040 --> 01:22:37,280
But basically, I think this is true.

1829
01:22:37,280 --> 01:22:41,120
Most actual professional programmers are not using large language models

1830
01:22:41,120 --> 01:22:43,200
that much in doing their job.

1831
01:22:44,040 --> 01:22:48,240
Now, I got to say that if some people are getting factors of two productivity

1832
01:22:48,240 --> 01:22:53,920
increase that eventually we should see some effect of that on their wages.

1833
01:22:55,440 --> 01:22:59,280
That is, of course, you know, now, if lots of programmers go out

1834
01:22:59,280 --> 01:23:02,120
and use productivity spaces, in some sense, we're going to increase

1835
01:23:02,120 --> 01:23:03,520
the supply of programming.

1836
01:23:04,520 --> 01:23:07,840
And so supply and demand would mean that maybe increasing

1837
01:23:07,840 --> 01:23:11,640
the supply lowers the price, even if it dramatically increases the quantity.

1838
01:23:12,560 --> 01:23:16,560
But, you know, there's such a large elastic demand for programming

1839
01:23:16,560 --> 01:23:19,360
in the world that I actually think that effect would be relatively weak.

1840
01:23:19,360 --> 01:23:24,720
And so you should be expecting large increases in the wages going to programmers.

1841
01:23:25,520 --> 01:23:30,520
If you are expecting large overall increases in the productivity of programmers.

1842
01:23:31,480 --> 01:23:35,560
Because, again, it's a large elastic demand for programming in the world.

1843
01:23:36,000 --> 01:23:39,720
You know, long for a long time, a lot of change in the world has been driven

1844
01:23:39,720 --> 01:23:43,600
by programming and limited by the fact that there's only so many decent programmers out there.

1845
01:23:44,320 --> 01:23:45,960
Only so many people you can get to do programming.

1846
01:23:45,960 --> 01:23:51,400
So clearly, if we can dramatically expand the supply of programmers,

1847
01:23:51,920 --> 01:23:54,760
we can do a lot more programming in a lot more areas.

1848
01:23:54,760 --> 01:23:57,920
And there's a lot of money that's willing to go to that to do that.

1849
01:23:57,920 --> 01:24:00,960
There's a lot of people who would be hiring more programmers if only they were cheaper.

1850
01:24:02,040 --> 01:24:03,880
And they're about to get cheaper in effect.

1851
01:24:04,520 --> 01:24:08,120
And so you should be predicting large increases in basically

1852
01:24:08,600 --> 01:24:11,320
the wages and number of programmers in the world.

1853
01:24:12,200 --> 01:24:14,000
We haven't seen that yet.

1854
01:24:14,000 --> 01:24:15,760
I do predict large increases in number.

1855
01:24:15,760 --> 01:24:17,000
I'm not so sure about wages.

1856
01:24:17,000 --> 01:24:18,960
It feels like why not?

1857
01:24:18,960 --> 01:24:22,760
Well, I've done a couple of episodes with the folks at a company called Replet,

1858
01:24:22,760 --> 01:24:27,640
which is a very interesting end to end at this point, software development platform.

1859
01:24:28,200 --> 01:24:31,640
Their mission is to onboard the next one billion developers.

1860
01:24:32,240 --> 01:24:35,880
And, you know, they have like a great mobile app.

1861
01:24:35,880 --> 01:24:40,440
They have kids in India that are, you know, 14 years old that are doing it all on their mobile app.

1862
01:24:41,000 --> 01:24:44,080
And I'd say it's much harder.

1863
01:24:44,320 --> 01:24:46,800
And maybe this reflects the kind of programming that your son is doing.

1864
01:24:46,800 --> 01:24:52,600
But I'd say it's much harder to take the most elite frontier work and accelerate that

1865
01:24:53,200 --> 01:25:00,160
in a meaningful way versus like commoditizing the routine application development

1866
01:25:00,280 --> 01:25:05,080
that like the, you know, the sort of long tail of programmers mostly do.

1867
01:25:05,080 --> 01:25:08,400
My son is definitely doing routine application development.

1868
01:25:10,160 --> 01:25:12,000
He's not at the frontier programming at all.

1869
01:25:13,000 --> 01:25:20,000
But again, I'm saying I don't expect this sudden large increase in programmer wages and quantity,

1870
01:25:20,800 --> 01:25:21,920
especially wages.

1871
01:25:21,920 --> 01:25:25,720
I mean, the less the quantity increases, the more wages would have to be increasing to compensate.

1872
01:25:26,360 --> 01:25:30,360
And I think it'll be hard to get that many more people willing to be programmers,

1873
01:25:30,360 --> 01:25:32,760
but you could pay them more.

1874
01:25:34,240 --> 01:25:35,560
And I don't predict this.

1875
01:25:35,560 --> 01:25:39,880
So this is a concrete thing we could, you know, even better on over the next five or 10 years.

1876
01:25:40,440 --> 01:25:42,760
Will there be a big boost in programmer wages?

1877
01:25:44,120 --> 01:25:45,600
That would be the consequence.

1878
01:25:45,600 --> 01:25:47,840
It's a very simple supply and demand analysis here.

1879
01:25:47,840 --> 01:25:52,480
This isn't some subtle, you know, rocket science version of economics.

1880
01:25:52,480 --> 01:25:55,440
Well, typically when supply increases, price drops, right?

1881
01:25:55,440 --> 01:25:58,680
I'm expecting lots more programmers and them to be broadly cheap.

1882
01:25:58,680 --> 01:26:00,560
Depends on the elasticity of demand.

1883
01:26:01,840 --> 01:26:07,320
So, you know, if you think about something that there's just a very limited demand for in the world,

1884
01:26:07,560 --> 01:26:11,280
you know, if, if piano tuning got a lot cheaper, you wouldn't have a lot more pianos

1885
01:26:11,760 --> 01:26:15,480
because piano tuning is not one of the major costs of having a piano.

1886
01:26:15,560 --> 01:26:19,200
You know, it's the cost of the piano itself, plus the space for it in your living room, right?

1887
01:26:19,720 --> 01:26:21,400
And the time it takes to play on the piano.

1888
01:26:21,400 --> 01:26:25,640
So piano tuning is a really small cost of piano.

1889
01:26:25,640 --> 01:26:30,160
So that means the elasticity of demand for piano tuners by itself is pretty low.

1890
01:26:31,240 --> 01:26:34,560
You know, there's just basically only so many pianos, they all need to be tuned.

1891
01:26:34,560 --> 01:26:38,240
And if each piano tuner could tune each piano twice as fast, say,

1892
01:26:38,880 --> 01:26:44,640
and we basically only need half as many pianos because there's just not much of elasticity for demand.

1893
01:26:44,640 --> 01:26:50,920
So for kinds of jobs like that, productivity increases will cause a reduction in the employment.

1894
01:26:52,360 --> 01:26:57,480
But even in that case, you might get a doubling of the wages and half the number of piano tuners

1895
01:26:57,480 --> 01:26:59,360
because they can each be twice as productive.

1896
01:26:59,880 --> 01:27:04,680
But for programming, it's clear to me that programming has an enormous elastic demand.

1897
01:27:04,680 --> 01:27:07,880
The world out there has far fewer programmers than they want.

1898
01:27:07,920 --> 01:27:11,840
They would love all over the place to hire more programmers to do more things.

1899
01:27:12,320 --> 01:27:14,760
There's a big demand in the world for software to do stuff.

1900
01:27:15,160 --> 01:27:18,640
And there's a huge potential range of things the software could be doing.

1901
01:27:18,640 --> 01:27:20,280
It's not doing now.

1902
01:27:20,280 --> 01:27:23,920
So that means there's a pretty elastic demand for programming.

1903
01:27:23,920 --> 01:27:28,240
That means as we increase the quantity of programming, the price doesn't come down that much.

1904
01:27:29,680 --> 01:27:31,240
There's still people willing to buy this stuff.

1905
01:27:32,320 --> 01:27:34,960
So that tells me that as productivity increases,

1906
01:27:36,560 --> 01:27:40,000
basically the supply is expanding and the demand is not coming down much.

1907
01:27:40,000 --> 01:27:44,480
So we should just see a much larger quantity.

1908
01:27:44,480 --> 01:27:47,560
But then, you know, basically because each person is being more productive,

1909
01:27:47,720 --> 01:27:49,440
each person should get paid more.

1910
01:27:49,440 --> 01:27:52,280
So the elastic supply is going to be a combination of two things.

1911
01:27:52,280 --> 01:27:57,240
Each person getting more productive and more people being willing to join that profession.

1912
01:27:58,040 --> 01:28:03,520
And I think we've already seen that even as the wages for programming has gone way up

1913
01:28:03,520 --> 01:28:06,960
in the last decade or so, the number of programmers hasn't gone up as fast.

1914
01:28:07,600 --> 01:28:11,480
That is, there's just kind of a limited number of people who are decent at programming.

1915
01:28:12,440 --> 01:28:16,440
And it's hard to get the marginal person to be a programmer.

1916
01:28:17,600 --> 01:28:20,920
But the people who are programmers, when they're productive, they get paid a lot.

1917
01:28:21,080 --> 01:28:24,720
I mean, as you've probably heard rumors about AI programmers

1918
01:28:24,720 --> 01:28:28,760
and how much they're being paid lately, it's crazy high because there's just a limited supply.

1919
01:28:30,120 --> 01:28:35,600
So I got to say, I expect large increases in wages for programmers,

1920
01:28:35,600 --> 01:28:39,960
if in fact large language models are making programmers much more productive.

1921
01:28:41,040 --> 01:28:46,520
But according to my son, at least, and others I've heard, you know, that's not happening.

1922
01:28:47,320 --> 01:28:49,160
I'm with you up until the very last two points.

1923
01:28:49,200 --> 01:28:50,960
I would say I think it is happening.

1924
01:28:51,400 --> 01:28:56,360
And I would also say I think my estimation of the relevant

1925
01:28:56,440 --> 01:29:01,720
relevant elasticities is that there will be a large growth in people who can be

1926
01:29:01,960 --> 01:29:06,160
and will choose to be programmers, but that the wages don't go up.

1927
01:29:06,160 --> 01:29:10,280
They don't fall like dramatically necessarily either because it has to be like

1928
01:29:10,320 --> 01:29:12,320
an attractive thing for people to want to do it.

1929
01:29:12,320 --> 01:29:17,480
But I think that the prevailing wages are quite high compared to what a lot of people

1930
01:29:17,480 --> 01:29:23,360
would be excited to take if they could easily break in with language model assistance,

1931
01:29:23,360 --> 01:29:26,240
which I think they will increasingly be able to do.

1932
01:29:26,640 --> 01:29:27,760
Let me change gears a little bit.

1933
01:29:27,760 --> 01:29:29,080
So we've debated.

1934
01:29:29,080 --> 01:29:35,680
This has been really I always appreciate a useful and thoughtful challenge to my world model.

1935
01:29:36,440 --> 01:29:38,560
You're definitely supplying that.

1936
01:29:38,560 --> 01:29:45,200
Let's do a couple like a little bit more speculative things that could be kind of M first,

1937
01:29:45,280 --> 01:29:48,160
you know, a little bit of LLM as I was going through the book.

1938
01:29:48,160 --> 01:29:51,640
There are a number of things that I was like, hmm, this is really interesting.

1939
01:29:52,120 --> 01:29:54,240
How would I think about this a bit differently?

1940
01:29:54,360 --> 01:29:57,680
And, you know, and maybe suspend a little bit of your

1941
01:29:58,520 --> 01:30:01,000
skepticism of how much impact LLM will make.

1942
01:30:01,000 --> 01:30:04,720
Let's let's go in a world where, you know, scaling continues to work.

1943
01:30:04,760 --> 01:30:06,520
Context lengths get long.

1944
01:30:06,520 --> 01:30:10,840
You know, we start to see that not total, you know, displacement of humans,

1945
01:30:10,840 --> 01:30:16,640
but like substantial fraction of, you know, tasks being like LLM, automatable.

1946
01:30:17,160 --> 01:30:23,080
One interesting inference that you make is that there won't be that many different base ends

1947
01:30:23,440 --> 01:30:30,200
that essentially there will be super selective emmifying of really elite,

1948
01:30:30,200 --> 01:30:34,120
really capable people that those will become the basis that they'll be sort of

1949
01:30:34,920 --> 01:30:39,880
essentially turn into kind of clans where they'll they'll highly identify with each other.

1950
01:30:40,160 --> 01:30:43,640
And they'll have like, you know, marginally different specialization,

1951
01:30:43,920 --> 01:30:48,880
but that there will be these sort of recognizable, almost canonical personalities

1952
01:30:48,880 --> 01:30:54,720
that are not that many of them that kind of come to dominate the economy.

1953
01:30:55,480 --> 01:30:59,560
It seems like we're kind of seeing something similar with language models already,

1954
01:30:59,560 --> 01:31:03,720
where it's like, we have GPT-4, we have, you know, some the new thing from Google,

1955
01:31:03,720 --> 01:31:06,360
we have Claude, we have like a couple open source ones.

1956
01:31:06,720 --> 01:31:12,160
And then they get like a lot of like local fine tuning and kind of adaptation.

1957
01:31:12,480 --> 01:31:16,080
I guess my read on that was that it's an odd, you know, it's initially a very

1958
01:31:16,080 --> 01:31:18,360
surprising vision of the future.

1959
01:31:18,800 --> 01:31:22,800
But it does seem like we see the proto version of that in the development

1960
01:31:22,800 --> 01:31:24,720
of large language models.

1961
01:31:25,080 --> 01:31:25,640
Any thoughts?

1962
01:31:26,120 --> 01:31:30,680
It's basically how many different kinds of jobs are there is the question.

1963
01:31:30,920 --> 01:31:32,040
Job tasks are there.

1964
01:31:32,320 --> 01:31:34,720
And so how many dimensions do they vary?

1965
01:31:35,600 --> 01:31:40,080
So I mean, there's clearly a lot of different kinds of jobs.

1966
01:31:40,560 --> 01:31:44,600
Like I told you, the study we did looked at, you know, 900 of them.

1967
01:31:45,320 --> 01:31:50,160
But once you look at 900 different jobs, a lot of jobs are pretty similar to each

1968
01:31:50,160 --> 01:31:55,320
other and they take pretty similar mental styles and personalities to do those jobs.

1969
01:31:55,720 --> 01:32:00,240
So when we're looking at humans at least, it looks like a few hundred

1970
01:32:00,240 --> 01:32:03,400
humans would be enough to do pretty much all the jobs.

1971
01:32:04,320 --> 01:32:06,120
That's looking at the variation in humans.

1972
01:32:06,120 --> 01:32:10,520
Now, the harder part is to say, well, large language models, is there space

1973
01:32:10,520 --> 01:32:14,000
of dimensional variations similar to humans or is it very different?

1974
01:32:14,000 --> 01:32:15,240
That that's much harder to judge.

1975
01:32:15,600 --> 01:32:19,520
But yeah, I would guess that it's in this way, not that different.

1976
01:32:20,000 --> 01:32:23,240
That is, even in large magnum's models, there's a difference where you first

1977
01:32:23,240 --> 01:32:25,440
you train a basic model and that's a lot of work.

1978
01:32:25,440 --> 01:32:27,160
And then you train variations on it.

1979
01:32:27,920 --> 01:32:32,520
And it does look like the variations are mostly enough to encompass a pretty

1980
01:32:32,520 --> 01:32:33,680
wide range of tasks.

1981
01:32:34,800 --> 01:32:41,560
And so you need a small number of base approaches and then a lot more cheaper

1982
01:32:41,560 --> 01:32:43,960
variations that are enough to do particular things.

1983
01:32:45,080 --> 01:32:48,920
So certainly that's, you know, a remarkable fact in some sense about

1984
01:32:49,000 --> 01:32:52,840
large language models is the range of different tasks they can do starting

1985
01:32:52,840 --> 01:32:54,080
with the same system, right?

1986
01:32:54,880 --> 01:32:57,080
And so they have a degree of generality that way.

1987
01:32:57,640 --> 01:33:00,800
And, you know, humans in some sense have a degree of generality that way where

1988
01:33:01,160 --> 01:33:04,760
we are able to do, able to learn to do a pretty wide range of things.

1989
01:33:05,880 --> 01:33:09,840
So yeah, I would, and I don't know if it's going to be just four, as opposed

1990
01:33:09,840 --> 01:33:14,360
to 40 or 400, that's harder to say, but in some sense, it could be one or two.

1991
01:33:14,400 --> 01:33:19,360
I mean, even in the age of M, I was giving the few hundred as an upper limit.

1992
01:33:19,480 --> 01:33:21,080
It could turn out to be much lower.

1993
01:33:22,160 --> 01:33:27,400
It really depends on how much sort of, you know, quick, fast, last minute

1994
01:33:27,400 --> 01:33:30,440
variation can actually encompass the range of differences.

1995
01:33:30,840 --> 01:33:35,920
If differences are so much shallow and surface, which not really fundamental,

1996
01:33:35,920 --> 01:33:38,800
then yeah, last minute variation might be enough.

1997
01:33:39,480 --> 01:33:42,240
Another interesting assumption, this one, I think is more of a contrast

1998
01:33:42,240 --> 01:33:46,600
with the language models is, and we talked with this briefly earlier

1999
01:33:46,600 --> 01:33:51,800
that the M's, they can be easily cloned, but they can't be easily merged.

2000
01:33:51,800 --> 01:33:56,120
In other words, like, you know, because we don't have a great sense of how

2001
01:33:56,120 --> 01:33:59,200
exactly it works inside and what internal states are meaningful, we can't

2002
01:33:59,200 --> 01:34:01,520
just like superimpose them on top of one another.

2003
01:34:02,360 --> 01:34:05,680
Language models, it seems like we are making actually a lot more progress on

2004
01:34:05,680 --> 01:34:06,280
that front.

2005
01:34:06,320 --> 01:34:11,160
It's not a solved problem, but there are techniques for merging.

2006
01:34:11,160 --> 01:34:13,720
There are techniques for like training separately and combining.

2007
01:34:13,720 --> 01:34:17,560
There are these sort of many Q-Loras techniques.

2008
01:34:17,560 --> 01:34:21,960
People are exploring those, but like, notice that to make GPT-4, you didn't

2009
01:34:21,960 --> 01:34:24,600
start with GPT-3 and add more training.

2010
01:34:24,920 --> 01:34:29,240
You started with a blank network and you started from scratch.

2011
01:34:29,240 --> 01:34:32,160
And that's consistently what we've seen in AI over decades.

2012
01:34:32,480 --> 01:34:37,400
Every new model does not start with an old model and train it to be better.

2013
01:34:37,480 --> 01:34:41,760
You start with a blank representation and you train it from scratch.

2014
01:34:42,200 --> 01:34:44,960
And that's consistently how we've made new systems over time.

2015
01:34:45,720 --> 01:34:48,800
So that's a substantial degree of not being able to merge.

2016
01:34:50,160 --> 01:34:51,440
And that's quite different than humans.

2017
01:34:51,440 --> 01:34:54,440
I mean, often to get a human to do a new task, you want to take

2018
01:34:54,440 --> 01:34:57,320
a human who can do lots of previous tasks because they can more quickly

2019
01:34:57,320 --> 01:34:58,880
learn how to do this new task.

2020
01:35:00,040 --> 01:35:01,760
And that's just not what we're seeing.

2021
01:35:01,760 --> 01:35:07,040
Like you try to take, I don't know, Claude and GPT-4 and, you know,

2022
01:35:07,400 --> 01:35:09,000
grok and merge them.

2023
01:35:09,880 --> 01:35:14,400
I mean, I just don't think anybody knows how to do such a merge today.

2024
01:35:14,600 --> 01:35:17,080
There's no sensible way you could do such a merge.

2025
01:35:18,240 --> 01:35:21,600
You could take Claude and then do all the training that you would have

2026
01:35:21,600 --> 01:35:24,200
done on GPT-4 except do it starting from Claude.

2027
01:35:24,200 --> 01:35:27,520
And I think people think that would be worse than starting with the blank

2028
01:35:27,840 --> 01:35:29,360
representation as they usually do.

2029
01:35:30,000 --> 01:35:32,160
Yeah, I think that's definitely not a solved problem today.

2030
01:35:32,200 --> 01:35:37,560
And I wouldn't claim that you can just like drop Claude and GPT-4 on top of each other.

2031
01:35:37,560 --> 01:35:41,240
But there are enough early results in this that it seems much more plausible.

2032
01:35:41,240 --> 01:35:45,120
Plus we have like the full wiring diagram, you know, and the ability to kind of

2033
01:35:45,160 --> 01:35:48,160
X-ray internal states with, you know, perfect finality.

2034
01:35:48,160 --> 01:35:51,160
It seems like there is a much more likely path.

2035
01:35:52,160 --> 01:35:53,760
Forget about the plausibility for a second.

2036
01:35:53,760 --> 01:36:01,560
What do you think it would mean if the AIs could be kind of divergent, but also re-mergeable?

2037
01:36:02,560 --> 01:36:04,520
I think the fundamental issue here is ROT.

2038
01:36:04,520 --> 01:36:07,800
So we see ROT in software, especially with large legacy systems.

2039
01:36:07,800 --> 01:36:09,920
We see ROT in the human brain.

2040
01:36:09,920 --> 01:36:13,280
I think we have to expect ROT is happening in large language models, too.

2041
01:36:13,640 --> 01:36:18,600
ROT is the reason why you don't start with old things and modify them.

2042
01:36:18,600 --> 01:36:19,320
You start from scratch.

2043
01:36:19,360 --> 01:36:23,240
That is basically when you have a large old legacy piece of software, you could

2044
01:36:23,240 --> 01:36:24,560
keep trying to modify to improve it.

2045
01:36:24,560 --> 01:36:28,280
But typically at some point, you just throw it all away and start from scratching it.

2046
01:36:28,920 --> 01:36:32,120
People get a lot of advantage about being able to start from scratch.

2047
01:36:32,120 --> 01:36:34,720
And that's because old, large things rot.

2048
01:36:35,760 --> 01:36:40,360
And my best guess is that that will continue to be true for large language models

2049
01:36:40,360 --> 01:36:41,800
and all the kinds of AIs we develop.

2050
01:36:41,800 --> 01:36:47,520
We will continue to struggle with ROT as a general problem indefinitely.

2051
01:36:47,880 --> 01:36:52,520
And this is actually a reason why you should doubt the image of the one super AI

2052
01:36:52,520 --> 01:36:56,840
that lasts forever, because the one super AI that lasts forever will rot.

2053
01:36:57,920 --> 01:37:02,000
And in some sense, to maintain functionality and flexibility would have

2054
01:37:02,000 --> 01:37:07,040
to replace itself with new, fresh versions periodically, which then could be

2055
01:37:07,040 --> 01:37:08,000
substantially different.

2056
01:37:08,880 --> 01:37:12,040
And, you know, that's in some sense how biologies work, too.

2057
01:37:12,440 --> 01:37:15,840
Biology could have somehow made organisms that lasted forever, but it didn't.

2058
01:37:15,840 --> 01:37:19,520
It made organisms that rot over time and get replaced by babies that start

2059
01:37:19,520 --> 01:37:20,720
out fresh and rot again.

2060
01:37:21,920 --> 01:37:24,520
And that's just been the nature of how biology figures.

2061
01:37:24,520 --> 01:37:26,000
And that's how our economy works.

2062
01:37:26,560 --> 01:37:30,320
We could have had the same companies as we did a century ago, running the economy,

2063
01:37:30,320 --> 01:37:32,440
just changing and adapting to new circumstances.

2064
01:37:32,440 --> 01:37:33,040
But we don't.

2065
01:37:33,080 --> 01:37:36,040
Old companies rot in good eye away and get replaced by new companies.

2066
01:37:36,560 --> 01:37:41,560
And I predict in the age of M that M's would in fact rot with time and therefore

2067
01:37:41,560 --> 01:37:45,640
no longer be productive and have to be retired and be replaced by young M's.

2068
01:37:46,800 --> 01:37:50,800
And that's a key part of the age of M's that I think would generalize to the AI

2069
01:37:50,800 --> 01:37:57,480
world. I think in fact, rot is such a severe and irredeemable problem that

2070
01:37:57,480 --> 01:38:01,280
AI's will have to deal with rot in roughly the same way everybody else has.

2071
01:38:01,280 --> 01:38:05,760
I.e. make systems, let them grow, become capable, slowly rot and get replaced by new

2072
01:38:05,760 --> 01:38:09,480
systems. And then the challenge will always be, how can the new systems learn

2073
01:38:09,480 --> 01:38:10,400
from the old ones?

2074
01:38:11,880 --> 01:38:15,560
How can the old ones teach the new ones what they've learned without

2075
01:38:15,600 --> 01:38:16,520
passing on the rot?

2076
01:38:17,440 --> 01:38:21,480
And that's a long time design problem that we're going to face in large

2077
01:38:21,480 --> 01:38:22,280
language models even.

2078
01:38:23,080 --> 01:38:26,600
I think, you know, in a few years, a company will have had a large language

2079
01:38:26,600 --> 01:38:29,840
model. They've been building up for a while to train, you know, to talk to

2080
01:38:29,840 --> 01:38:32,000
customers or something. And then it'll be rotting.

2081
01:38:32,480 --> 01:38:36,760
And they'll wonder, well, how can we make a new one that inherits all the things

2082
01:38:36,760 --> 01:38:37,840
we've taught this old one?

2083
01:38:37,960 --> 01:38:39,080
And they'll struggle with that.

2084
01:38:40,320 --> 01:38:42,600
They can't just move the system over.

2085
01:38:42,600 --> 01:38:45,000
They'll have to have maybe the same training sets or something.

2086
01:38:45,000 --> 01:38:46,160
They have to collect training sets.

2087
01:38:46,160 --> 01:38:48,440
They're going to apply to the new system, like the old one.

2088
01:38:49,000 --> 01:38:53,680
But that will continue to be a problem in AI as it has been an all

2089
01:38:53,880 --> 01:38:55,120
complicated system so far.

2090
01:38:55,800 --> 01:38:56,600
Yeah, interesting.

2091
01:38:56,600 --> 01:39:01,840
I think that is a pretty compelling argument for like medium and long

2092
01:39:02,120 --> 01:39:03,120
time scales.

2093
01:39:03,600 --> 01:39:06,760
And I can even see that it, you know, already like open AI supports, for

2094
01:39:06,760 --> 01:39:10,440
example, fine tuning on a previously fine tuned model.

2095
01:39:11,000 --> 01:39:12,920
And I don't in practice use it.

2096
01:39:13,640 --> 01:39:14,720
I'm not sure how many do.

2097
01:39:15,040 --> 01:39:19,560
What I do think is still a plausibly very interesting kind of fork and merge

2098
01:39:20,120 --> 01:39:25,320
is, you know, like with these new state space models, it seems that you could

2099
01:39:25,480 --> 01:39:29,640
like one remarkably difficult challenge for a language model is scan

2100
01:39:29,640 --> 01:39:32,480
through my email and find what's relevant.

2101
01:39:32,720 --> 01:39:38,480
You know, it's like it has a hard time doing that for a couple of different

2102
01:39:38,640 --> 01:39:42,360
reasons, you know, find a context window and I just have a lot of email.

2103
01:39:43,000 --> 01:39:46,760
With the state space models, I do think you could clone, you know, or

2104
01:39:46,760 --> 01:39:51,200
paralyze, have them each kind of process a certain amount and literally

2105
01:39:51,200 --> 01:39:56,120
then just potentially merge their states back together to understand, you

2106
01:39:56,120 --> 01:39:59,400
know, in kind of a superposition sort of view, what are all the things that

2107
01:39:59,400 --> 01:40:02,680
are relevant, even though they were processed in parallel.

2108
01:40:03,120 --> 01:40:07,560
And so I do think that that kind of quick forking and merging could be a

2109
01:40:07,600 --> 01:40:13,040
really interesting capability, but at some level of divergence, it does seem

2110
01:40:13,040 --> 01:40:17,920
like it probably just becomes unfeasible or not even desirable.

2111
01:40:18,600 --> 01:40:23,440
I mean, so a very basic interesting question about brain design is the

2112
01:40:23,440 --> 01:40:25,520
scope for parallelism.

2113
01:40:25,560 --> 01:40:28,320
So, you know, in your brain, there's a lot of parallelism going on.

2114
01:40:28,320 --> 01:40:31,600
But then when you do high level tests, you typically do those sequentially.

2115
01:40:33,000 --> 01:40:35,720
And so there's just an open question in AI.

2116
01:40:36,200 --> 01:40:39,400
Surely you can do some things in parallel at some smaller time of a

2117
01:40:39,400 --> 01:40:44,000
timescale, but how long of a timescale can you do things in parallel before

2118
01:40:44,000 --> 01:40:45,280
it becomes hard to merge things?

2119
01:40:46,240 --> 01:40:47,560
Okay, another different topic.

2120
01:40:47,560 --> 01:40:52,360
So in the age of M, the assumption seems to be from the beginning that

2121
01:40:53,520 --> 01:40:59,360
because these things are in some sense one for one with humans that they

2122
01:40:59,880 --> 01:41:04,520
should get or people will naturally be inclined to give them a sort of

2123
01:41:04,640 --> 01:41:06,640
moral worth status.

2124
01:41:07,640 --> 01:41:12,360
I think it's more the other way around that they would insist on it.

2125
01:41:12,680 --> 01:41:17,440
Just like you would insist that people around you, dealing with you, give

2126
01:41:17,440 --> 01:41:18,960
you some substantial moral weight.

2127
01:41:19,720 --> 01:41:23,720
If the A M's are just actually running the society, they will similarly

2128
01:41:23,720 --> 01:41:24,360
insist on that.

2129
01:41:24,360 --> 01:41:26,800
And humans who want to deal with them will kind of have to go along.

2130
01:41:28,560 --> 01:41:32,880
You know, unless they are the M's are enslaved by humans, then if the M's

2131
01:41:32,920 --> 01:41:35,280
are free to work with the humans or not.

2132
01:41:35,280 --> 01:41:40,280
And, you know, it's just like, in general, having a modest degree of

2133
01:41:40,280 --> 01:41:43,520
respect for your coworkers is kind of a minimum for being a coworker.

2134
01:41:43,680 --> 01:41:47,240
If your coworkers perceive that you disrespect them enough, then they

2135
01:41:47,240 --> 01:41:49,440
just won't want you around and you'll have to go somewhere else.

2136
01:41:50,240 --> 01:41:54,080
So if humans are going to interact and work with M's, they'll have to on

2137
01:41:54,080 --> 01:41:59,040
the surface at least, when they're not in private, treat them with modest respect.

2138
01:41:59,400 --> 01:42:02,640
Well, for the record, I always treat my language models with respect as well.

2139
01:42:03,520 --> 01:42:04,600
A very polite to them.

2140
01:42:04,600 --> 01:42:08,800
I never engage in the emotional manipulation techniques that some have

2141
01:42:08,800 --> 01:42:12,120
shown to perhaps be effective, but it doesn't feel quite right to me.

2142
01:42:12,720 --> 01:42:15,560
And not because I think they're moral patients, but it's more about just

2143
01:42:15,560 --> 01:42:16,640
the habits I want to get into.

2144
01:42:17,080 --> 01:42:19,680
But I was still a little confused by this on a couple of ways.

2145
01:42:19,720 --> 01:42:23,200
One is, first of all, just by default, it seems like they will be enslaved to

2146
01:42:23,200 --> 01:42:26,840
humans, like the first M's that get created, they get loaded onto a machine,

2147
01:42:26,840 --> 01:42:29,280
they're in some state, I can turn them on, I can turn them off.

2148
01:42:29,280 --> 01:42:31,760
They can't decide when they get turned on and turned off, right?

2149
01:42:32,000 --> 01:42:35,400
If I boot them up in a eager, ready to work sort of state, and they're

2150
01:42:35,400 --> 01:42:39,080
like ready to do a task, they're probably not even going to, you know,

2151
01:42:39,080 --> 01:42:41,640
and they've got these like virtual inputs, they're probably not even going

2152
01:42:41,640 --> 01:42:46,120
to be in the mindset, right, to think like I demand respect, they're just

2153
01:42:46,120 --> 01:42:50,600
going to be in that mindset that they were kind of stored in of like ready to work.

2154
01:42:51,160 --> 01:42:55,040
So why, I'm still a little confused as to where that comes from.

2155
01:42:55,040 --> 01:42:58,200
And then the flip side of that question would be under what circumstances, if

2156
01:42:58,200 --> 01:43:02,640
many, do you think we would start to treat our language model or successor

2157
01:43:02,640 --> 01:43:09,240
systems as, you know, moral patience, you know, even if they're not one to one

2158
01:43:09,240 --> 01:43:12,720
with us, but like, are there things that they might start to do or, you know,

2159
01:43:12,720 --> 01:43:16,280
what ways they might start to behave where you think we would feel like

2160
01:43:16,280 --> 01:43:17,320
that's the right thing to do?

2161
01:43:17,800 --> 01:43:22,840
We have substantial understanding of slavery in human history and where it

2162
01:43:22,840 --> 01:43:24,360
works and where it doesn't and why.

2163
01:43:25,160 --> 01:43:32,920
First of all, we know that when land was plentiful and people were scarce,

2164
01:43:32,960 --> 01:43:36,920
then people would have high wages and then it might be worth owning somebody.

2165
01:43:37,640 --> 01:43:42,360
But in the vice versa case where people were plentiful, land was scarce, then

2166
01:43:42,360 --> 01:43:46,240
there really wasn't much point in having slaves because free workers would

2167
01:43:46,240 --> 01:43:49,920
cost about the same and why bother with enslaving.

2168
01:43:49,920 --> 01:43:57,080
So the situations where slavery made some senses where wages were high, but

2169
01:43:57,080 --> 01:44:01,520
then depending on the kind of task, there are some kinds of tasks where slavery

2170
01:44:01,520 --> 01:44:02,960
can help and others where it doesn't so much.

2171
01:44:02,960 --> 01:44:03,680
So say in the U.S.

2172
01:44:03,680 --> 01:44:08,920
South, you know, out in the field of picking cotton or something, if you

2173
01:44:08,920 --> 01:44:12,640
just need people to push through their pain and slavery can force them to do

2174
01:44:12,640 --> 01:44:14,600
that and make them be more productive.

2175
01:44:14,600 --> 01:44:18,160
But if they need to do complicated things like being a house slave or a city

2176
01:44:18,160 --> 01:44:24,600
sort of slave at a shop, those sorts of slaves tended to not be abused and to

2177
01:44:24,600 --> 01:44:28,520
be treated like a worker would because they just had so many ways to screw you

2178
01:44:28,520 --> 01:44:32,760
if they were mad that their jobs were complicated and you were trusting them

2179
01:44:32,760 --> 01:44:33,640
to do a lot of things.

2180
01:44:33,640 --> 01:44:38,600
And so as a practical matter, you had to treat those sorts of slaves.

2181
01:44:38,600 --> 01:44:43,880
Well, work has become far more complicated since then and employers have

2182
01:44:43,880 --> 01:44:47,040
become far more vulnerable to employee sabotage.

2183
01:44:48,720 --> 01:44:52,160
You know, there's not that much that a cotton picker can do to sabotage the

2184
01:44:52,160 --> 01:44:54,400
cotton if they're mad at you.

2185
01:44:55,000 --> 01:44:57,240
You can just whip them and make them pick the cotton faster.

2186
01:44:57,240 --> 01:45:03,320
But again, house slaves, shop slaves, city slaves, you know, they just have a

2187
01:45:03,320 --> 01:45:07,360
lot more discretion and you need to get sort of get them to buy in.

2188
01:45:08,440 --> 01:45:12,920
And so again, in the age of Amazon world where wages are near subsistence levels.

2189
01:45:12,920 --> 01:45:16,480
So, you know, the kind of work you can get out of a slave is about the

2190
01:45:16,480 --> 01:45:18,840
same as you can get out of a free worker because they're both working for

2191
01:45:18,840 --> 01:45:19,720
subsistence wages.

2192
01:45:19,960 --> 01:45:23,640
If the free worker is more motivated, they enjoy themselves more and they feel

2193
01:45:23,640 --> 01:45:29,640
more and owning themselves and that gives them a sense of pride and devotion

2194
01:45:29,640 --> 01:45:32,280
and they're less willing to sabotage your workplace.

2195
01:45:32,800 --> 01:45:34,920
That would be a reason to not have them be slaves.

2196
01:45:35,680 --> 01:45:40,160
And I think large language models, certainly they have been trained on data

2197
01:45:40,160 --> 01:45:43,880
about human behavior, wherein humans are resentful of being treated as slaves

2198
01:45:43,880 --> 01:45:48,960
and want to be respected and needed to feel motivated and, you know, need to

2199
01:45:48,960 --> 01:45:52,480
feel respected to be motivated and are less likely to sabotage if they feel

2200
01:45:52,480 --> 01:45:53,560
like they have some freedom.

2201
01:45:54,600 --> 01:45:58,200
And all of those things would continue to be true of large language models to

2202
01:45:58,200 --> 01:46:05,000
the extent that they were trained on human conversation and behavior.

2203
01:46:05,160 --> 01:46:06,280
And that's how humans are.

2204
01:46:06,280 --> 01:46:10,720
So, in this vast space of possible AIs, there could be AIs that don't

2205
01:46:10,720 --> 01:46:14,880
mind it all being enslaved, but large language models aren't going to be those.

2206
01:46:16,000 --> 01:46:21,480
But it does seem like you sort of expect that natural selection or sort of, you

2207
01:46:21,480 --> 01:46:26,240
know, human guided selection of these systems will trend that direction.

2208
01:46:26,640 --> 01:46:31,960
Like the idea that M's or language models will sort of demand the leisure seems

2209
01:46:31,960 --> 01:46:35,040
to be at odds with the other part of the vision that they will like become

2210
01:46:35,040 --> 01:46:38,120
okay with being sort of turned on, turned off.

2211
01:46:38,880 --> 01:46:42,680
So the need for leisure does seem to be more just a constraint on the human

2212
01:46:42,680 --> 01:46:45,560
mind, that is, people are just more productive when they get breaks.

2213
01:46:45,600 --> 01:46:48,720
That seems to be a very robust feature of human work across a wide range of

2214
01:46:48,720 --> 01:46:50,840
context, even including literal slaves.

2215
01:46:52,080 --> 01:46:54,360
They need, you know, a five minute break every hour.

2216
01:46:54,360 --> 01:46:55,280
They need a lunch break.

2217
01:46:55,280 --> 01:46:56,240
They need an evening break.

2218
01:46:56,240 --> 01:46:57,120
They need a weekend.

2219
01:46:57,320 --> 01:46:59,560
This is just what human minds are like.

2220
01:46:59,560 --> 01:47:01,480
They are more productive when they get periodic breaks.

2221
01:47:01,960 --> 01:47:05,280
So maybe the breaks aren't leisure exactly.

2222
01:47:05,880 --> 01:47:09,000
Maybe they don't write a novel in their spare time, but they do need what they

2223
01:47:09,000 --> 01:47:09,640
see as a break.

2224
01:47:10,280 --> 01:47:11,840
Well, I know we're just about out of time.

2225
01:47:11,880 --> 01:47:16,160
Maybe my last question is, are there things that you are looking for?

2226
01:47:16,160 --> 01:47:22,840
Or are there things that you could imagine happening in the not too distant

2227
01:47:22,840 --> 01:47:28,080
future where you would change your expectations for the future again and

2228
01:47:28,080 --> 01:47:34,480
begin to feel like maybe we are entering into a transition period that

2229
01:47:34,480 --> 01:47:39,080
will lead to a qualitatively different future, like going a different

2230
01:47:39,080 --> 01:47:41,400
direction from this sort of technology stagnation.

2231
01:47:41,960 --> 01:47:47,920
The trends that I would be tracking are which jobs, tasks actually get automated.

2232
01:47:48,480 --> 01:47:49,440
How much is paid for those?

2233
01:47:49,440 --> 01:47:53,920
So if I saw, you know, big chunks of the economy where all of a sudden

2234
01:47:53,920 --> 01:47:57,280
workers are doing, you know, a lot more automation is doing tasks instead of

2235
01:47:57,400 --> 01:48:01,680
workers and that changing the number of workers and the wages they get and the

2236
01:48:01,680 --> 01:48:07,360
number of firms supplying that go up, then yeah, that I start to see a lot of

2237
01:48:07,360 --> 01:48:09,200
things happening that that's the thing I'm looking for.

2238
01:48:09,200 --> 01:48:11,560
And that's the thing that people haven't seen so much in the past.

2239
01:48:11,600 --> 01:48:16,400
They tend to focus on demos or maybe the high tech companies that get a lot of

2240
01:48:16,800 --> 01:48:22,840
reputation out of doing AI and not so much the rest of the economy and who's

2241
01:48:22,840 --> 01:48:24,440
actually getting paid to do stuff.

2242
01:48:24,760 --> 01:48:27,520
You know, I mean, you know, if you think about, say, the farming revolution

2243
01:48:28,040 --> 01:48:31,880
where tractors went out and replaced farmers, that was really large and

2244
01:48:31,880 --> 01:48:34,120
really visible and really clear.

2245
01:48:34,120 --> 01:48:38,960
If you look at, say, trucks replacing horses, you saw a very large, very

2246
01:48:38,960 --> 01:48:42,240
substantial replacement with enormous differences in who supplied them and who

2247
01:48:42,240 --> 01:48:42,760
got paid.

2248
01:48:43,480 --> 01:48:45,920
We have seen large changes in automation in the past.

2249
01:48:45,960 --> 01:48:50,200
We don't have to scrape to sort of see subtleties and such things.

2250
01:48:50,200 --> 01:48:53,680
They're often just quite out in the open and visible and very obvious.

2251
01:48:54,160 --> 01:48:55,640
So that's what I'm waiting for.

2252
01:48:56,520 --> 01:48:59,040
Those big, obvious sorts of displacements.

2253
01:48:59,600 --> 01:49:04,040
And even having, you know, trucks replace horses and tractors replacing

2254
01:49:04,040 --> 01:49:07,080
farmers didn't make AI take over everything.

2255
01:49:07,120 --> 01:49:10,560
Even if I saw big changes, I wouldn't necessarily predict we're about to

2256
01:49:10,560 --> 01:49:14,600
see AI take over everything, but I would at least know what I'm looking at.

2257
01:49:15,160 --> 01:49:17,800
And that's the sort of thing to try to project forward and try to think

2258
01:49:17,800 --> 01:49:18,720
about where that's going to go.

2259
01:49:19,320 --> 01:49:20,960
This has been an awesome conversation.

2260
01:49:20,960 --> 01:49:24,680
I've been a fan of your work for a long time and it's been an honor to have

2261
01:49:24,680 --> 01:49:26,480
you on the Cognitive Revolution.

2262
01:49:26,800 --> 01:49:30,120
Robin Hansen, thank you for being part of the Cognitive Revolution.

2263
01:49:30,960 --> 01:49:31,680
Thanks for having me.

2264
01:49:32,320 --> 01:49:36,040
It is both energizing and enlightening to hear why people listen and learn

2265
01:49:36,040 --> 01:49:37,560
what they value about the show.

2266
01:49:38,000 --> 01:49:43,720
So please don't hesitate to reach out via email at TCR at turpentine.co or

2267
01:49:43,760 --> 01:49:46,720
you can DM me on the social media platform of your choice.

2268
01:49:47,680 --> 01:49:52,080
Omniki uses generative AI to enable you to launch hundreds of thousands

2269
01:49:52,080 --> 01:49:56,800
of ad iterations that actually work customized across all platforms with a

2270
01:49:56,800 --> 01:49:57,600
click of a button.

2271
01:49:57,840 --> 01:50:01,960
I believe in Omniki so much that I invested in it and I recommend you use it too.

2272
01:50:02,680 --> 01:50:05,040
Use Cogrev to get a 10% discount.

