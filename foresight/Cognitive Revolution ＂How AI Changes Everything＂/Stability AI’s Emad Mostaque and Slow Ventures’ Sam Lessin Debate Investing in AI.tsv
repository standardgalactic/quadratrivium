start	end	text
0	3200	I think a key question to this is like, you know, people say hallucinations.
3840	8000	I was like, what does that mean? Well, I mean, it doesn't get every single fact completely right.
8640	12800	Challenge E.P.T. is probably like 100 gigabytes down from like 10 trillion words.
13440	17280	The fact you can get anything right is an absolute technical marvel that
17280	22240	no one's really sure exactly how that happens. What if you had an AI tutor for every child?
22880	25760	What does that look like? What if you had 100 AI tutors for every child?
25760	31680	For the first time, every single person can have hundreds of characters that like and
31680	35920	support them all the time. Basically, you log into social media or whatever, and you're like,
35920	39840	hey, I'm Sam, and he's like, cool, what type of people, instead of who do you want to follow?
39840	43440	It's like, who do you want to follow you? For example, there aren't enough therapists in the
43440	48800	world, you know, and it is a regulated industry. But at the same time, there is a gap for therapists,
48800	52720	just like you have the meditation apps kind of step in, and they created calm and they
52720	57120	created these other things that were huge. Hello, and welcome to the Cognitive Revolution,
57120	61600	where we interview visionary researchers, entrepreneurs, and builders working on the
61600	66800	frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas,
66800	71200	and together we'll build a picture of how AI technology will transform work,
71200	77120	life, and society in the coming years. I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.
77760	81760	Hello, and welcome back to the Cognitive Revolution. Today's episode is a super
81760	85920	interesting one on a number of levels, as we're hosting a discussion between two
85920	91520	super influential technology thinkers. Sam Lesson, former VP of product at Facebook,
91520	98080	now early stage technology investor and writer, Andy Modmostock, founder and CEO of Stability AI,
98080	103360	whose work at Stability, highlighted of course by Stable Diffusion, has already been incredibly
103360	108800	influential, but has also come under intense scrutiny in the months since he raised $100
108880	115440	million at a $1 billion valuation. This conversation started on Twitter with a short essay that Sam
115440	122240	wrote, arguing that AI is mostly a bad investment for VC. Imad responded and suggested a podcast on
122240	129280	the topic, and Eric and I were naturally happy to volunteer to host. Both Sam and Imad talk fast.
129280	135360	This is a 1.5x speed episode for me, down from the usual 2x, and both had a lot to say,
135440	139440	so we mostly let them speak directly to each other before I jumped in at the end,
139440	143840	to ask some concrete question. I think regular listeners to the show will know that I definitely
143840	150240	share Sam's point of view around investing in AI. AI may well disrupt society at large,
150240	154880	but it doesn't seem likely to disrupt many existing SaaS markets between now and then.
155840	160720	There will, as Sam says, always be exception, but for someone whose focus is on looking for
160720	166000	those early-stage companies with 100x potential investment returns, I think he's quite right
166000	170640	that there'll be few and far between, at least at the model and application layers.
172080	176080	For what it's worth, though, I do think Sam is quite wrong to limit his thinking about LLM
176080	182960	function to the no-real-intelligence-just-association-between-words paradigm. There is now ample
182960	188240	mechanistic interpretability work that shows quite conclusively that AI models are indeed
188240	194000	groping much more than statistical correlation, but that's a topic for another episode.
195120	200400	For today, the subtext of the conversation seemed to be this question. Will stability AI
200400	206480	prove to be one of those exceptional, highly successful startups deserving of its
206480	212800	unicorn status and valuation? From my standpoint, the answer may depend on your definition of
212800	218880	success. Stability is as much a movement as a company and has already left an indelible mark
218880	224560	on AI open-source culture. Their impact goes beyond the groundbreaking stable diffusion,
224560	230960	including major dataset releases such as the Lyon 5 billion image dataset, various language models
230960	235600	and accompanying open-source RLHF libraries, which enable further downstream training and
235600	240560	customization, and many, many other projects across a wide range of modalities.
240560	246720	They've also established themselves as tremendous identifiers of and supporters of talent,
247360	253680	including another upcoming guest, 19-year-old PhD Tanishk Matthew Abraham, who just published a
253680	261200	literal mind-reading paper that converts fMRI data into reconstructed images of what the person saw,
261200	267200	truly mind-blowing work. But perhaps more important than any of that has been
267200	272880	a mod's unique ability to articulate an inspiring vision for the future of AI.
272880	277520	While the positive vision of AI that we tend to hear when we hear one at all often centers
277520	282880	around the possibility of large and powerful AGI's, of which there might only be a few,
282880	288560	presumably built and owned by leading technology firms. A mod has not only signed the AGI pause
288560	294320	letter and the extinction risk statement, but has articulated a very different positive vision
294320	299920	for a panoply of smaller AI models, mostly presumably derived from the open-source standards
299920	305840	that he and the team at Stability are creating, but all highly specialized for specific purposes
305840	311680	and localized to specific contexts and culture. This is an extremely appealing notion to billions
311680	316320	of people around the world who don't want to be beholden to American or, for that matter,
316320	322080	Chinese corporations or their access to AI. A mod has been criticized recently for allegedly
322080	326560	exaggerating certain claims and affiliations, and for some operational problems at stability
326560	332400	that resulted in people sometimes being paid late. And while some of that may well have happened,
332400	336000	I will say that I've followed a mod quite closely now for at least a year,
336000	341840	and have generally found him to be very reasonable. He has, for example, always recognized the reality
341840	347120	of open AI in Google's modes, and has projected that open-source models will continue to lag
347120	352320	leading closed-source models by a year or more. All of this seems quite right and reasonable
352320	358000	to me. Given a mod's comments about the centrality of stories, I think it's safe to say he understands
358000	363120	the task of developing a positive vision for AI, a vision that others can really buy into
363120	369920	as a core part of his role and strategy. This is quite different from other AI CEOs who often
369920	375200	seem to be sharing their plans more for your information than for your input, and it really
375200	380480	does seem to be working. I've joined the discords of many stability-affiliated projects,
380480	385040	and have been very impressed with the quality of people and conversations that they contain.
385680	390480	So, whether stability will ultimately deliver a great return for the investors who bought in at
390480	396080	that $1 billion dollar valuation is, for me, not the most interesting question about the company.
397040	400880	I'd be very surprised if they failed outright given the quality of talent that they have,
401520	405760	and so the question that matters more to me is simply, what impact will they have?
406400	412720	Will their push toward decentralization prove democratizing, destabilizing, or both? If you
412720	418240	fear centralization of power and you want to see a rich ecology of AI's develop around the world,
418240	424880	you might expect their contribution to be extremely positive. If, on the other hand, you fear chaos
424880	430480	and see AI's as invasive species colonizing niche after niche and ultimately perhaps competing with
430480	435120	humans, you might feel quite the opposite indeed. For my part, as you can probably guess,
435120	439920	I expect the outcome will ultimately be a bit of both. Throughout this conversation,
439920	444720	you'll hear just how much change both Sam and Imad take for granted as they think about the future.
445520	450000	Culture, entertainment, and relationships, they agree, are in for a shock.
451440	455200	The global south may well have leapfrog moments in education and even medicine.
456160	462560	Online communities may come to contain AI characters that we can't even identify as non-human.
462560	466960	Given the magnitude of all these changes and the resources and talent that Imad has amassed,
466960	472720	the inspiration he's provided, and the tremendous global need that AI seems so well suited to fill,
473680	477200	I think stability has a real chance not only to become a great company,
477200	484320	but to help shape a global universal basic intelligence standard, a potentially historic
484400	488960	development. How humans ultimately wield the new power that Imad and others unlock,
488960	493520	and whether we can control AI long-term at all, is much harder to predict,
494720	502160	but can ultimately only go one way or the other. Now, I hope you enjoy this fast-paced
502160	508560	conversation with Imad Mostak and Sam Lesson. I think that large language models and a lot of
508560	513200	the AI stuff that we're seeing kind of start to get consumerized right now and become real,
513200	518560	it's super cool. There's no question about that. There are absolutely going to be
519120	525520	great product experiences improved by it and opportunities to create more efficiency,
525520	531440	create better interfaces. I am not negative on how some of the stuff will find its way
531440	536720	into consumer product experiences and make things better. My wife's company,
536720	541200	the publication information, we've already deployed a bunch of AI stuff that makes
541200	546000	search for the information go from absolutely terrible to pretty good, and there's a bunch
546000	552080	more stuff coming that will get better. I'm not against that. I do think the things that I keep
552080	559360	in mind, one as an investor, is I think the case about why a bunch of this technology is going to
559360	566720	make meta, and Amazon, and Google, and a bunch of big players, an assload of money are clear.
567520	573920	I think the idea that it is a wedge or an angle that's going to allow a bunch of companies from
573920	578800	zero to come out of nowhere and then become wildly profitable or compete with those guys,
578800	584000	types of big players, I think is much more sus, as they would say. It's because I mean,
584000	587760	to really take advantage of the stuff, you need a ton of distribution, you need a ton of data,
587760	594880	and I really see a lot of what I've seen is opportunities to extend innovation that already
594880	600560	exists versus completely reshuffled the deck. That's my big thing. I am very bullish on crypto
600560	606160	long-term. Crypto is undeniably whatever you think of it, a deck reshuffled. AI, and what we're
606160	610640	seeing is not a deck reshuffled from my perspective as an extender. People come pitch me, we're going
610640	618640	to be the Adobe of AI. Adobe is going to be the Adobe of AI, from my deployment. I think
618640	621760	it's a very tough one to see. Will there be exceptions? Of course there will be exceptions.
621760	626480	There will be exceptions, but I think it's a seen thing. I think it's hard. I'd also say as an investor,
626480	635040	a seed investor, which is how I earn my daily bread. I'd say that the opportunities to deploy
635040	640800	a few million dollars, turn over a card and have an experience like, oh my god, there's something
640800	644960	here, now let's have a series A investor put a ton more money in and see it scale up, I think are
644960	650080	few and far between. And because everyone's so excited, everything's way mispriced. And so for
650080	655040	me as an investor, I think it's an extremely hard market to get excited about. What else can I say?
655040	659920	I mean, look, I do think that the elephant in the room, which I'm sure we can discuss or not, is
661120	667760	for the companies that have gone out so far, talk about chat, GPT, I think there's huge regulatory
667760	672160	problems which are becoming clearer. And it's not about the machine is going to eat us all.
672160	676400	I think that's a load of crap. And it's been in the record for quite some time,
676400	680800	being very, very negative and cynical about kind of a lot of those narratives. I mean,
680800	685440	at the end of the day, token guessing, guess the next token is not a fundamentally dangerous
685440	689840	piece of technology. I do think that the copyright issues are deeply real
690480	694000	and complicated. And there's a bunch of other challenges that these guys are going to face
694000	697680	that, you know, again, because the world has a general viewpoint of like,
697680	702880	fool me once, shame on you, fool me twice, shame on me is, you know, the era of from social media
702880	707200	to Uber to whatever, like, I think people are going to be way more quick reactive to like what's
707200	712800	going on from regulatory environment here, I hope, then that historically. But I don't know,
712800	715680	that's a ton of ground. And I don't know, where do you want to go?
716800	720400	Yeah, no, there's a ton of ground. I think, you know, there's this question of, is this a
720400	724480	disruptive or sustaining innovation? And the question of what this is, you know, you have
724480	729120	the classical big data and then you extrapolate it to sell you ads. And that was good old internet.
729120	734160	And it created these kind of behemoths in matter and Google in particular. But then you have the
734160	739040	application of computer vision and these other things largely to the incumbents. So value was
739040	743920	captured there. I was at your mobile and mobile's a great example of like, just double down, right?
743920	749200	Yeah. And that's why kind of Facebook's first shift, my while was good. Next shift to matter.
749200	754160	And maybe they'll rename themselves spatial or something. But, you know, this becomes very
754160	758480	interesting. Because like, these models are something a bit different. So I would stable
758480	763040	the fusion to 100,000 gigs of images and the app was a two gig file. And it was four of the top
763040	768320	10 absolutely app store in December. We're having that as the entire backend. You put words in and
768320	772720	images pop out and it makes pretty pictures in your face, right? But then they all dropped off and
772720	777760	they disappeared. Because there are more features than apps, they're cool features. But they weren't
777760	782160	kind of product experiences. That is exactly what happened when the apps were launched, right? You
782160	787600	had like fart apps as number one for $599. There's a brief moment where it's cool and you're experimenting
787600	792160	with it and you have these kind of poops, right? But they're not. Yeah, I think like poops. Yeah,
792160	796880	you're right. Exactly. Literally. It's not real because you have to have the user experience
796880	801280	and build products like normal. But where I feel right now is that we're at the primitive stage
801280	807280	and very boring in terms of one to one interaction is very boring. I think it is again, very surface
807280	812960	level without any memory. And it's ephemeral and fleeting. My thing is that probably iPhone 2G,
812960	817120	iPhone 3G bit, we're just getting copy paste. Because what's happened is you've got technology
817120	822240	that's gone from research and it's now starting to go into engineering. What are the design patterns
822240	826640	for this? How is it implemented? Was it good for? I think a key question to this is like, you know,
826640	831440	people say hallucinations. I was like, what does that mean? Well, I mean, it doesn't get every
831440	837200	single fact completely right. Charge GPT is probably like 100 gigabytes down from like 10 trillion
837200	842560	words. The fact you can get anything right is an absolute technical marvel that no one's really
842640	847840	sure exactly how that happens. You know, it's like in a pipe pipe from Silicon Valley. Like,
847840	851120	that Weisman stool would be even more intensive, even repress all that knowledge.
851840	854720	Because what these really are, they're reasoning machines. They're not facts,
854720	857760	because we've got two parts of our brain. Are they reasoning machines? Aren't they
857760	863040	guess the next token machines? Like, that's the I think I think that's a really fundamental thing.
863040	867840	Like, I think that my model and the easiest way for most consumers to think about this,
868160	873120	basically accurate, right, is like, there's no actual intelligence to be system. So right,
873120	876720	all they're doing is saying, okay, based on all the words I've seen in the graph of language that
876720	881200	I've been able to observe, here's the most likely next token. And that's really cool to be clear.
881200	885600	That's like super useful. But calling that intelligence is a real stretch in my mind.
885600	889040	Well, I think it depends on your definition of intelligence, like you're applying the free
889040	893200	energy principles of Carl Friston, and where everything just intelligence from energy kind
893280	898160	of dropping to its last date or different definition of intelligence. I think what I
898160	902560	look at it is like this. One to one is getting the next token for language models for image
902560	907760	models that diffusion based and now generate all sorts of other architectures. But it's about output
907760	913440	and what can it do? So one on one, it's a bit dumb, it doesn't have memory. You have the meta
913440	918240	paper by Cicero, whereby they had eight language models interacting with each other, and it out
918240	922800	performed humans in the game of diplomacy. You know, you just like all that good old Alpha
922800	927360	Go type stuff, which he's reinforcement learning. Is that intelligence? Probably still not,
927360	930880	but it can augment intelligence. That's something that we've been focusing on a lot because
931440	936480	you can use it for actual intelligence augmenting things, you can use it for reasoning things.
936480	941200	Give it a PDF and say, well, on Earth, this is PDF talking about, you can do that right now.
941200	946960	And that's a useful thing that reduces frustration. I used to invest in video games. I used to look
947120	951040	at time to fund flow and frustration. I look at things like, you know, this podcast we're doing
951040	956000	any year, it'll be automatically transcribed and edited and added to our knowledge base through
956000	960880	next token prediction. Does that require AGI? No. Yeah. Although interesting, let's talk about
960880	964640	this podcast. It's a really interesting case. You know, in the early days of Clubhouse, when
964640	970400	Clubhouse was ripping, I used to like go after Paul all the time. And I wrote about this being like,
970400	974880	you are so stupid for not recording this stuff. I was like, look, here's the reality. These
974880	979680	conversations in Clubhouse are dribble, right? Like 99% of them is crap. And I don't want to
979680	985440	listen to it. However, if you've created a magical pump that says the internet is full of SEO,
985440	990560	shit, and Wikipedia, we have a magic pump of people wanting to talk to each other live.
990560	995760	Here's the thing, people want to talk, no one wants to listen. But if you transcribe and record it all,
995760	999520	and you can create an index out of it, and then all of a sudden you have this meta, this next
999520	1003680	generation search engine, but that's fucking interesting, right? Here's the problem. We're
1003680	1006960	Paul's side of the time, which I think turns out to be totally wrong, given where AI is coming.
1006960	1011360	He's like, yes, Sam, but like, there's no way to index it and blah, blah, blah. There will be,
1011360	1015520	like, there's clearly going to be, right? And it turns out, I'd like to, you know,
1015520	1019520	because I like seeing, I told you so, like, I told you so, like, they're definitely a way to do that
1019520	1023600	now, right? And like, that would have been super sweet. I think we're great. Here's the problem,
1023600	1030080	though, is with a lot of these visions of like, oh, we'll just like take all the recorded podcasts,
1030080	1034960	right? And then kind of put a front end and tie to them and like, compress them down and be done,
1034960	1039280	is there's no economic model to that. And maybe we can get into business models for a second.
1039280	1044000	That's going to make sense for anyone to publicly share anything, right? Like the way, the reason
1044000	1048560	that like, people put things on the web was because they were getting paid for it in one form or
1048560	1053200	another, because the whole ecosystem of Google, where it created was a trade, it was, okay,
1053200	1057360	like you get to index this shit, but you're going to send me traffic and I can monetize. And like,
1057360	1062240	you know, the publishers got snowed by that for a while, right? And like almost lost, almost went
1062240	1067760	away until they figured out paywalls, right? We're doing this now because it's kind of fun and
1067760	1073120	bullshit and we'll learn, right? But we're also kind of doing it, at least I'll do it. I'll post it,
1073120	1076640	maybe someone will follow me out of it, it's a fun hour to spend with interesting people, right?
1076640	1081920	But there's an economics to it in some form, social or financial capital. This model, I actually
1081920	1085920	think that the interesting thing about AI, if you take that view, whatever you would think is
1085920	1091040	interesting, is like, it's already going to crush the information economy of the web, right?
1091040	1096320	I think that if you roll it forward, like this conversation will not be in the public domain,
1096320	1099840	right? Going forward, because there'll be no, there'll be no social economics to it,
1099840	1104080	just be a compression on top of it. And if anything AI, again, if you take the model of,
1104080	1109120	oh, it'll take a bunch of podcasts and compress them down into tweets, right, will end up kind of
1109120	1114480	collapsing on itself, if you need people, what you do, right, to ultimately be the source of truth
1114480	1118400	and information about the world. And so that's that point of view, but I'm not sure I entirely
1118400	1123360	agree, because you know, it's fun to shoot the shit. And Tony, you do have a podcast thing,
1123360	1126960	they've got their ads, which is about that. But I think the attention economy is a very
1126960	1130560	interesting element to this, particularly because these models are based on attention.
1130560	1133600	So the differential of these models versus previous is that you have the attention of all
1133600	1137520	you need, Peko, where it's like, from an information theory perspective, information is
1137520	1141360	valuable in as much it changes the state. So you take this whole podcast and compare this down
1141440	1144720	to a few tweets, that's all you need to see. But sometimes people want to see the full kind of
1144720	1150080	thing. No one really wants the whole thing. Oh, no, they do. They do. Sometimes it's quite fun to
1150080	1155680	kind of do it because I mean, let's say the Christianson thing of a job to be done, right?
1155680	1158400	You have a functional component, a social component and an emotional component.
1158960	1162080	You know, why does everyone want to go to a concert? You know, why do people want to have
1162080	1169120	collectors items things? Products have different aspects and different elements to it. People
1169120	1172800	still read full books. They don't kind of read the summaries of the book. They don't read the
1172800	1176880	simulacrums of it. I mean, like, look, to me, there's there's there's two different, again,
1176880	1181680	this gets into some old Facebook stuff. But like, I think we can talk about let's take
1181680	1185200	financial economy out of it and just talk about like informational and social economy.
1185760	1191760	There's the entertainment economy, right? For sure, AI is going to crush the entertainment
1191760	1196960	economy, right? Like, there's no question about that, right? Like, you start with porn and go on
1196960	1201360	through. And the reality is, is that, you know, we went from, you know, people magazine to your
1201360	1204560	friends and your friends are more interesting than people magazine. And that's what's more
1204560	1208640	impressing your friends is professional friends who are like hotter and funnier. And guess who's
1208640	1213360	more interesting than hot, funny, professional friends, it's going to end up being actually,
1213360	1217840	I said, there's a more tick tock was it turns out algorithmically, find the best person from the
1217840	1222400	universe, you'll find some niche that's better. What's better than that? Synthetic, right? We will
1222400	1227280	get to the point where we say, Hey, like, there will be like a hotter, funnier, more interesting,
1227280	1233040	more personalized AI thing, which is derived, like, I totally buy that, right? And I think that's
1233040	1238560	why actually some it's been funny to watch some pretty interesting influencers who are smart,
1238560	1243280	be like, Oh, my God, this is the end of the world for us, right? I agree with that. Information is a
1243280	1248320	very, very different beast, right? And entertainment, though, right, because the value is not like
1248400	1253600	engagement. That is, that is actually the, in the broad sense, the attention is everything,
1253600	1259040	where it's totally raw, right? Which is like, that is for sure true, if you're trying to optimize
1259040	1263040	for entertainment. And it's not true, right? If you actually know what needs to know what's going
1263040	1267920	on in the world, right? Or you need to like, you're dealing with a real world. And that interface
1267920	1272320	between the real world, the digital world, where the systems have no knowledge of what actually is
1272320	1277040	truth is to your, the point where I think is probably that already even falls down the most.
1277760	1282320	Well, I mean, maybe this is why, you know, if you say that kind of hallucinations are kind of core
1282320	1286560	and it's the creativity machine, media is where it's more impactful, where the truth is in the
1286560	1290480	element there, right? What happened a little bit to date is a few of the AI companies wanted to
1290480	1295360	talk about themselves as information machines. And they realized, right? And so they'll be like,
1295360	1299200	you're like, we're not instead of we're creative, don't trust us for facts is like,
1300080	1305040	fine. And I agree, they'll be useful entertainment machines. But I do, I think that goes into the
1305040	1309200	whole like, what are we actually talking about here? What are the actual value is? And like,
1309200	1314000	how scoped it is, which is not it's not zero. It's just not like every social societies are
1314000	1319920	based on stories. You know, like, all of my view on finance, pretty much all of finance is
1319920	1324000	securitization and leverage telling stories. And then how did you tell them? Like, and we can
1324000	1328960	see the power of stories as they move around. So Silicon Valley Bank was a story that was true.
1329520	1333200	And led to an $18 billion outflow like that. All of us are kind of familiar with that.
1333200	1337280	Probably listen to this podcast. I think it's pretty cynical to say it's all in the stories.
1337280	1341520	I mean, it's like, I think there are there's reality in the world, like the economy is not
1341520	1346320	based just on storytelling. No, I mean, the dollar is a story. The economy is based on the dollar.
1347600	1352080	And so you have the Fed confidence, you have confidence in the stock markets,
1352080	1356240	it's kind of layers of these things. And then you have this technology, you need trust, that's
1356240	1360320	for sure true. And trust, I mean, ultimately goes all the way down to like, is there a military
1360320	1363760	behind it, which is somewhat of a story. And that I agree with. But I think that's like,
1363760	1368800	a pretty abstract view, right? Like companies earn cash flows, they're real or not real,
1368800	1372800	they release products, they do work, it's real or not real. It's not just storytelling.
1372800	1376240	What is the multiple? Maybe it's because I'm a former hedge fund manager. So I always looked at
1376240	1380400	what was the incremental story for a stock that adjusted the multiples and other things.
1380400	1385440	Sure, I agree that if you look at the world of multiples, you say, why do you multiple expansion
1385440	1390800	or compression, right? And that's based on people's feelings about the world in future cash flows,
1390800	1394880	right? And in theory that that is a lot of storytelling. I don't think that's actually the
1394880	1400160	vast majority of the economy, right? That's the stock market. So I think that separating out what
1400160	1404720	is the stock market from what's the economy is pretty important. Hey, we'll continue our interview
1404720	1410320	in a moment after a word from our sponsors. Omnike uses generative AI to enable you to launch
1410320	1415760	hundreds of thousands of ad iterations that actually work, customized across all platforms
1415760	1420480	with a click of a button. I believe in Omnike so much that I invested in it. And I recommend
1420480	1426240	you use it too. Use CogGrav to get a 10% discount. And I think this is the important thing. We
1426240	1429600	separate it out and we see where does this technology affect? And when does it go to the
1429600	1435520	incumbents versus startups? All of these things kind of fundable, right? And so we have one area
1435520	1439680	of media. We can discuss that very concretely. I think it will have a massive impact on media
1439680	1444000	at stability. We have leading media team, right? And so we haven't agreed with that, but we can
1444000	1448400	dig into that. The other area is a lot of these things are language models right now that chat
1448400	1455440	bots and it's like, it's nice. But Bing is not the top search engine. It's not even top 20
1455440	1459280	on the app store, right? Because it's still a terrible experience, rather to be speaking.
1459920	1464160	Yeah. So even though some people like why use it for all the things, you don't really, you know,
1464160	1469360	chat GPT rows really fast. And it's useful for things like doing your own work. But do you
1469360	1475440	really use it that much? So where I find it interesting is really looking at where companies
1475440	1480640	are trying to go beyond the basic search patterns and have the classical kind of feedback loops
1480640	1486160	with engaging content and see how that grows. So I think mid journey is a good example of that,
1486160	1491440	whereby David delivery built a community, took it to like 14 million people and is making money
1491440	1496800	hand over fist because he built even though discord is fricking weird, a good experience on
1496800	1501040	existing infrastructure, Facebook, App Store. But how many of those have you seen looking across
1501040	1506480	the entire AI space? Most of this stuff right now is terrible. But again, the other question is who
1506480	1510160	gets the value, right? And I think like, let's talk about the internet because we actually agree
1510160	1515040	on the entertainment thing. Like, you know, world of closed loop, it's all about what's the most
1515040	1519840	engaging thing and attention is everything, right? Yes, like, these systems are like quite
1519840	1525680	assuming that you don't end up getting into hell, which I do think is a really big problem around
1525680	1529920	human creativity and copyright and a bunch of other points of legal leverage on these things.
1529920	1535680	I agree that you can make really compelling cases and it's going to hurt a lot of the human
1535680	1540080	entertainment industry, right? That that I agree with. But the question is, who's going to win it?
1540080	1544320	Is it going to be the Hollywood studios? Is it going to be, you know, is it going to be the
1544320	1548960	existing publishers who just start adding incrementally more of this stuff in, etc. Or is it going to
1548960	1554720	be new startups or new people? You know, look, there's always exceptions to the rule. But I think
1554720	1558720	almost the entire pie is going to be the people who have the distribution, they have the IP,
1558720	1561680	they have all the pieces they need to just plug this shit in.
1561680	1566000	Well, but I mean, maybe we can look at it in terms of the consumption of content went to zero
1566000	1569600	with streaming and kind of all these things that led to some winners coming
1570640	1575440	because you have Netflix, you have Spotify, etc. The creation of content basically goes to zero
1575440	1580720	with this technology as well. And basically, I believe in a few years who will feature films
1580720	1585120	using this. Yeah, but I guess I own and distribute those, right? And the reality is,
1585120	1589200	I think I'll be the Hollywood studios, because they have the distribution, right? Like,
1589200	1592720	if you believe that's kind of the distribution mechanism, but there's a whole ecosystem that
1592720	1597360	can build around that. Things like D-Nag, things like industrial light and magnitude,
1597360	1600000	do you need that when you have rendering, you know, at scale?
1600000	1605200	To be clear, I think the thing I think you could totally see changing or evolving is going to be
1605200	1611920	the factory, right? So like, you know, meaning like, yes, are there capital investments that
1611920	1617680	people have made that will become less relevant because of AI? Absolutely. There's no question.
1617680	1621840	Will you almost certainly still have human writer rooms for the foreseeable future?
1621840	1625440	For sure, right? Like, is whatever it is. So there's going to be hybrids. I just,
1625440	1630640	I think saying that, but my basic point is that IP matters, distribution matters,
1630640	1634800	like there are things that matter. I agree with you that the factory plumbing in some of these
1634800	1640240	places gets a lot less valuable if you have better AI tooling. I just don't mind matters.
1640240	1643120	Well, I think it's a bit of a disruptive innovation for that side of things,
1643120	1648880	increasing the pace of output. So Pixar can do six movies a year rather than two.
1648880	1652480	And so the question around the industry, so a few weeks ago I was at Cannondale gave a talk,
1652480	1656240	so I used to be a video game investor and player. And I was like, the video game industry over the
1656240	1661920	last 10 years has gone from 70 billion to 170 billion. The average score has gone from 69% to 74%.
1662720	1667680	Movies are 40 billion to 50 billion. The score is 6.4 on IMDB. Are you going to be able to make
1667680	1671520	better movies and have a bigger market then, in which case there's more rooms for people to make
1671520	1676480	money? Or is it going to be a case of, it cannibalizes itself? There's some key questions
1676480	1681040	around kind of media, right? And media consumption. In the end of the day, the media consumption
1681040	1684800	thing, though, again, depending on like how you want to factor it and look at it, it really just
1684800	1690000	comes back. There's 24 human hours in a day, right? Like, and the reality is, is like, where
1690080	1695760	time spends, it shifts, right? As a result of this stuff, like for sure, time spent dramatically
1695760	1702800	into social, right? Off of other things, right? When that thing. Will social get more compelling,
1702800	1708320	right? With AI? Absolutely, right? And so will more attention shift into Instagram because of it?
1708320	1712480	Absolutely. Do I believe there's going to be another platform that comes out of nowhere
1712480	1718400	and swipes Instagram because the cost of production goes down? Nah, right? Like, do I believe that,
1718400	1723200	like, some new studio is going to come out and take out Pixar? Nah, Pixel will just make a few
1723200	1727680	more films, right? And like, that's cool. Like, I'm not against that happening. I think that's
1727680	1733920	completely fine. And like, people will make money on that in some places. The cost of production
1733920	1738480	and therefore the war of content gets more intense, for sure. You'll get to a point where like,
1738480	1742400	if you don't use this stuff, you're going to get fucked. But like, just because the competition
1742400	1748080	level rises, doesn't necessarily change the scorecard very much through that, how these things
1748160	1753120	go. So believe this question of do you use legacy systems or do you use systems such as runway
1753120	1758080	MLs, such as one of the dynamics, and some of these other ones that are engineered differently?
1758080	1761680	I think there's a lot of kind of legacy stuff where you used to Photoshop and you used to continue
1761680	1766800	to use Photoshop. And now they're introducing features like infill. But is there room for a
1766800	1772480	ground up kind of interface? And we see that sometimes kind of a character. And my assertion is
1772480	1777760	broadly no, but there will be exceptions. And the broadly no is going to be it's just it's
1777760	1781280	not to your point about innovate about is it a sustaining or is it disruptive? It's like,
1781920	1786960	Photoshop will get 95% right. They already have everyone's payment on file. They already have
1786960	1790960	the infrastructure. This is not like the internet. People like in the internet, there was a bunch
1790960	1796240	of companies that were fundamentally unprepared for this, right? I do not think that most of the
1796240	1800320	incumbents are fundamentally unprepared for this. Yeah. And you know, there's a question of do you
1800320	1804960	create brand new markets? So I was an early investment via the Chinese kind of Twitch. And
1804960	1810320	there was two hours a day on average per user. Now on character AI, I think it's still number
1810320	1814800	two on the app store. We're seeing two hours a day on average of usage, where she has some insane
1814800	1819040	kind of engagement metrics. It's quite nice to have a chat with it. But there's a question,
1819040	1823760	can that become then a product or a network? I think that we may be looking at some of the wrong
1823760	1828560	areas here because what you have is you have the consumer experience, the media experience and
1828560	1832560	enterprise experience. I think one of the things that's most interesting for me in terms of where
1832560	1837440	money could potentially be made is actually the regulated experience. So at stability, we make
1837440	1842000	open models, open source, but actually what we do is open auditable models for enterprise,
1842000	1845920	private data, governments, et cetera. So we've got a whole bunch of stuff that doesn't have any
1845920	1851200	web crawls, et cetera, employed via Bedrock and others. Then that's valuable data. So one of the
1851200	1856000	things we do is kind of education. And that's where I look at some of these areas and they've
1856000	1862240	been the main contributors to US inflation and CPI education and healthcare. And unlike you
1862240	1866880	could do something different there. And maybe that's where a significant amount of value will be.
1867680	1872080	I mean, I think it's sad from the Silicon Valley story if the answer is like, well, the money's
1872080	1876640	all going to be made for regulation. I don't disagree with you for what it's worth. Disrupting
1876640	1880640	regulated industries, which is different. I do believe that someone's going to make a lot of
1880640	1886640	money on AI regulatory points, right? There's no question. AI insurance. There we go.
1887600	1894800	Like, you know, there's a bunch of things that are like really sad things that you have to do and
1894800	1898240	like, you know, people will make money on. There's no question that people will find niche markets.
1898240	1903040	They're super boring. And not the type of thing I want to be involved in. But like, yes, like some
1903040	1909200	enterprise investors will have will make bank on like, you know, the whatever Europe comes up with
1909200	1915760	certifying your models are compliant and GDPR 8.0 to like deal with fucking data request removals.
1915840	1921200	Like that will happen as kind of the stuff happens. I'm like pretty uninspired by that,
1921200	1926240	right? Like, I think that's like, pretty sad that that's if that's if the net income of like,
1926240	1931280	new opportunities in AI is just going to be like, opportunities to like interface with government
1931280	1936240	and reign it in will be sad. Yeah, but like said, regulated industries. So the example that I have
1936240	1940560	there is education and healthcare. So like, one of the things you work with a range of charities
1940560	1945440	and multinational is deploying tablets into entire countries in Africa with AI that teaches
1945440	1950400	and learns. You give every kid a tablet, the young ladies illustrated primer, what does that do to
1950400	1955120	an entire nation? You know, the only thing that's been provably to work in education is the bloom
1955120	1959680	effect, the two segment effect. Right now, our kind of sister charity Imagine Worldwide has
1959680	1964640	been deploying the global XPRIZE for learning adaptive learning. And we're teaching 76% of kids
1964640	1968080	literacy and numeracy in 13 months and one hour a day with older kids teaching younger kids.
1968800	1973200	I look at this technology and I'm like, there are certain areas where there's a gap that nothing
1973200	1978720	could fill before. What if you had an AI tutor for every child? What does that look like? What if
1978720	1983440	you had 100 AI tutors for every child? I get it. And like, I do think that we can always go back to
1983440	1987600	the industries that tech has been trying to disrupt for a million years and like for lots of structural
1987600	1993520	reasons has not and say, ah, but now with this new tech will disrupt it, you know, I look forward to
1993520	1998880	the, to the years of debate in the, we'll talk about the US between the teachers unions and people
1998880	2003200	trying to deploy tablets for AI. We can say, Oh, no, no, no, no, we're going to do it in Africa,
2003200	2007520	skip the regulator, like the teachers, but I'm just saying it's like, yes, there's always hope
2007520	2013040	that the next wave of technology will somehow unstick a bunch of problems technologists hate
2013040	2018160	because of the regulatory or the structural issues with them. But I have no confidence that this
2018160	2023280	one is meaningfully different. But I mean, this is the question structural issues, right? Regulation
2023280	2029360	is one thing. You look at kind of BG use some of the other Indian kind of education companies,
2029360	2034080	you look at the Chinese ones across emerging markets, maybe it'll be the case here. I mean,
2034080	2040160	this is what I believe that much of the productivity enhancements, aside from maybe coding and things
2040160	2045440	like that, which we can get on to. And the biggest leaps will happen in the global south, because
2045440	2050000	they left a mobile and there's a whole mobile economy and massive companies created from that.
2050000	2054960	What if they make a leap to intelligence augmentation with this technology? Because right now they
2054960	2060640	can't service that. Now they could potentially service it, given the decreased cost of creativity
2060640	2064720	of engagement and other things from education to healthcare to other things.
2064720	2070560	I think if your argument is that there's a bunch of countries outside of the US that have lagged
2070560	2076160	in a bunch of infrastructure effectively or ability to like execute certain things in education,
2076160	2081040	et cetera, it will be able to allow the cell phone have like a leap frog moment and move forward.
2081040	2086080	Yeah, I don't object to that. I think that's like basically true. Again, I goes back to the thing
2086080	2091760	where like, I'm excited about kind of like the US, I think lives in the future relatively speaking
2091760	2095520	to most other people in countries. And like, I think the thing most people are excited about
2095520	2100880	is how like we can how AI changes like the top of the top. I agree with that. So I think if your
2100880	2105600	argument is it doesn't change the top of the top, but it does kind of catch up a bunch of the third
2105680	2108240	world, like I do think that there are places that will be true.
2108240	2114400	Well, so let's look at the top of the top then. So I think Microsoft put out that 50% of all code
2114400	2118640	is AI generated on GitHub now from code products, et cetera, and there's 40% improvement in
2119200	2124960	efficiency. I mean, my top coders really enjoy it because they train them in models. We have code
2124960	2130240	models too, and they are showing more and better code. What do you think about it with respect
2130240	2134000	to that industry? Because that's obviously a large industry, which is technology disrupted.
2134000	2140720	The only thing that I actually think is fucking awesome for chat GPT effectively is, I'll call it
2140720	2146080	Stack Overflow 2.0. It's fucking great for that. And like, if you think about it, why is it great
2146080	2152160	for that? Like, why? I think it is the perfect problem for the existing technology we have.
2152160	2159840	You have a shitload of open source code that these models can look at. Plus, you scrape all of Stack
2159840	2164640	Overflow, which Cyanar is Stack Overflow, and that goes back to the whole copyright issue,
2164640	2168960	as well as the issue of where some of the inputs come from, but most of the copyright issue.
2168960	2175440	Plus, the nice part about computer code is that it's test driven in a lot of cases. You either
2175440	2182160	pass it to the fucking test or it doesn't pass the test. So you have the perfect dataset of digital
2182160	2189680	only self-contained reality, which I totally agree chat GPT is great at. And frankly, I'm
2189680	2194400	the type of person who, like, I told, but I would never consider myself an engineer. It makes coding
2194400	2198240	for me so much more fun because all this shit I don't want to deal with, like, what the fuck is
2198240	2202960	this random error? What package do I have to install that manages this? It's all great. Now,
2202960	2209360	it does lie, and it does make up wrong answers, and it's not perfect. But I fully agree that the
2209360	2215920	co-pilot as thing is very powerful and like a really great specific use case. And I do agree that
2216080	2221200	talking about business models or what happens is like, like Stack Overflow is the poster. Stack
2221200	2227200	Overflow is the Yelp of this generation, right? You know how Yelp had this huge lawsuit with
2227200	2232480	Google that's gone on forever because Google basically just sold their results, right? Stack
2232480	2238720	Overflow is going to be that of this because they are screwed, right? And like, it is a great example
2238720	2242320	of a place where the tech is better because it was basically lifted. Yeah. And you know, it becomes
2242320	2246560	very interesting as well because now what you have is regulatory arbitrage, like the good old
2246560	2252480	double Irish with the Dutch sandwich on taxation, whereby Israel and Japan have said you can scrape
2252480	2258240	anything for any reason, which is kind of crazy, commercial or otherwise. So you maybe scrape in
2258240	2263120	one area, trade in another, and you serve it up in a different country. So I think this technology
2263120	2268160	is kind of inevitable. But then what is the implication of that? Like, my take is that as we
2268160	2272240	move through the next kind of five years or something like that, the nature of coding will
2272240	2277200	change. Like I started coding what 22 years ago, we had like assembler and subversion and stuff
2277200	2281840	like that. And kids these days have it so easy with get up, you know, and all these libraries.
2281840	2284880	What does it look like in a few years when you've got these technologies that you can describe
2284880	2289280	something and start building apps? You know, what does the whole ecosystem look like again when
2289280	2294640	the creation of these cells? It will just make them like much less valuable, right? Like is what
2294720	2298960	I basically come down to. And what ends up remaining valuable is distribution and data,
2298960	2302880	right? Because like right now you can be a great engineer or solve a problem, whatever, and there's
2302880	2306480	like a value. You can create a product that's actually worth something. If everyone can make
2306480	2311680	products, theoretically, that are like cost nothing, right, or really easily, then like there's
2311680	2315680	just no leverage in that anymore. And again, this goes back to who wins, who wins with people with
2315680	2320160	distribution data, right? That's the answer, like it from existing now to your point of
2320160	2324480	a regulatory arbitrage and data, I think this is really, I think the sad part about a lot of
2324480	2329760	this AI stuff, everything is going right, right? Like that's what the net of this is going to be
2329760	2334240	is like any, anything that has historically been an open data set, or people are able to say like,
2334240	2338240	okay, well, like I'll share this, but in return, I get traffic or notoriety, and that's like a
2338240	2343600	fair economics rate over, right? And so what's going to end up happening is walls are going to go
2343600	2348160	up everywhere, everything's going to go private. And that's going to be the interesting question
2348160	2352960	about where you end up from all this stuff from an economics perspective in the next few years.
2353040	2357040	But what is where this has happened many times before, right? Like this is not the first time in
2357040	2361440	human history, this happens that you know, people, you know, if you look at news industry,
2362080	2365360	you know, people are like, Oh, like the news industry used to be so great, and then whatever,
2365360	2369520	it's like bullshit. It's like the number of times in the history of news, basically, you had
2369520	2375520	growth and distribution, right? Things get super scammy. The elites retreat to private newsletters,
2375520	2379360	like in its cycles, it's happened like six or seven times. And like, I think this is going to be a
2379360	2385360	hard pin. In some ways, I think the biggest thing is that I'm very confident of is that AI will be
2385360	2389520	the death of the public web, and will be the death of a lot of open information, specifically
2389520	2394000	because of what you said, right? Which is that just will not that it's me too valuable and too,
2394000	2398400	and too, too important. But the reality is AI doesn't need any more information,
2398400	2403040	because of your short letter. But it does, it doesn't for entertainment. And that's why I think
2403040	2408960	entertainment is screwed. I think it absolutely the the oracle problem in crypto, where how you
2408960	2414640	keep a system, a digital system in sync with reality and be meaningful is exactly the same
2414640	2420080	problem that AI has, which is it can go in any direction it wants, as long as the data is self
2420080	2424800	contained. The second it's not, and it's trying to be synced to reality or a real world, it does
2424800	2429040	need more data. It does need to be continuously updated or addressed in whatever direction,
2429040	2434240	you know, cars attention. But then, you know, you have public broadcasting data, you have some
2434240	2438480	of these other things as well, whereby the oracle problem comes a lot easier to do when you can
2438480	2443680	do retrieval augmented models and other things like that. I mean, there are sources of verifiable
2443680	2448080	data for leasing. Maybe it comes down to the use case. My main point is that they're going to be
2448080	2453040	increasingly cut off, right, if there's no economic model for supporting them, and they're all getting
2453040	2457200	abstract and scraped by model. I would disagree with this. So you know, like, I made it deliberately
2457280	2461040	open so that we could highlight how that shapes that I think they're unsafe as well. And we're
2461040	2466400	the only company to offer octab, but we work with multiple governments on national data sets and
2466400	2471840	national models, using broadcasts of data and other things like that that are continuously updated
2471840	2476400	as national infrastructure. Because I think these models are a form of infrastructure, they're a
2476400	2480960	weird type of primitive, they're like a mega codec type thing, where stuff goes in stuff comes out,
2480960	2486000	but people do want to have relevance and updates. So I think you will have an open version that is
2486080	2491600	updated continuously. But then maybe again, that's where value is. Which parts of information go
2491600	2496080	private and are served up through models and who is providing them? Is this financial data? Is it
2496080	2501760	this? Is it that? And what is the quality of these fine-tuned models? Because what you just described
2501760	2507840	as well is a bit of a Armageddon for consumer apps in a way, right? Because it goes down to zero.
2507840	2512160	So then what becomes useful is that then the Apple takes a massively forward because they've got
2512240	2516160	this identity structure, and they have all the data there, and they can do apps quicker than
2516160	2520800	anything else. Yeah, except for the fact that Apple's entire shtick about encryption and privacy
2520800	2524240	is going to make it literally impossible for them to play in this. I actually think Apple's role
2524240	2527920	in the future of this stuff is going to be one of the most interesting big tech questions,
2527920	2532720	because they have positioned themselves so hardcore against all the things you would need to get
2532720	2537440	leveraged right from AI, that it's going to be very interesting to see how they navigate. Google,
2537520	2547040	fine, meta, fine. But despite the fact, I am very skeptical of what Apple's AI approach is going
2547040	2552160	to be, or I will say on the flip side, they're incredible at government relations and PR. So
2552160	2556720	if they see you have to figure out a way to totally recant on all their encryption and their
2556720	2561760	approaches to this type of stuff and have a new model where they somehow are the privacy heroes,
2561760	2565040	but also doing AI, I'm very curious how that's going to work.
2566000	2569600	They can keep a perfection and they can keep a customized rule. Because again,
2569600	2574080	you don't need to take everyone's days to the trainer. You have a generalized model. I think
2574080	2580320	local model, mini models on your local device, like a general model behind it. Exactly. I think
2580320	2583920	in practice, we'll see how it plays out. I'm skeptical. It works with an embedding layer
2583920	2591200	potentially, but it is kind of very interesting because, again, the technology doesn't matter.
2591200	2596880	It's the use that matters. What use can you get out of it? So yesterday, they had the thing
2596880	2601920	whereby they said, oh, it learns automatically with a little ML model in there. It learned
2601920	2606800	through a small embedding layer. They don't talk about the technology that much because Apple
2606800	2611360	always just talks about what the use actually is. I think the question is, what is disruptive?
2611360	2616560	What can engage more? What could attract more? And so I think that you've got apps coming down
2616560	2621760	there, which is why the bar generally rises. I think we see this with technology as it goes. The
2621760	2626320	bar generally rises, and so attention becomes even more difficult, where it does come down to
2626320	2631920	distribution. I think about that. What's your take on the nature of virality in this type of age?
2631920	2637680	Because these things are good at optimizing for virality, potentially, right? Like, again,
2637680	2641760	you can build better content. You can build better engagement once you get the puddles down.
2641760	2647200	And that is the start of many of these apps. Yeah, I just think virality is a war in a lot of
2647200	2653120	ways. So look, I think in the end of the day, will newsfeeds get more compelling for people?
2653120	2659280	Absolutely. Will ads get more compelling for people individually? Absolutely. There's no
2659280	2664080	question if these things are true and the existing players will get the vast majority of the pie of
2664080	2669840	that type of stuff. I do think you'll tend towards more and more niche interests. So let's talk about
2670000	2677440	porn for a second. Porn is always fascinating. You can go on Reddit and find the weirdest
2677440	2681600	fucking porn in the world of all these sub communities that have filtered into these weird
2681600	2689280	things that they're interested in. AI will make this 10 times weirder. Or if 100 times weirder.
2689280	2692800	And people are just going to keep filtering. Now, why does this weird filtering happen?
2693920	2697520	There's a bunch of reasons and different things. I think part of it, moving away from
2697520	2701680	porn for a second in the broader ecosystem is people are desperate for a sense of purpose and
2701680	2707040	place. And the reality is the internet makes you feel very small. There's millions of people just
2707040	2712720	like you. And that encourages people to seek out right sized communities that are smaller and smaller.
2712720	2718480	With AI, I think the interesting thing will be when it comes to attention and things like that is,
2718480	2725200	look, for the first time, every single person can have hundreds of characters that like and
2725200	2731680	support them all the time. The math of it all used to be, okay, you're trying to find a community
2731680	2737600	that's the right size and knows you have a price and you're valued in. But it's hard to, you're
2737600	2741600	not necessarily the hero. So you go find out a smaller niche or a different niche where you're
2741600	2747280	more of a hero or you create a spit of it and try to lead that. I think a future where basically
2747280	2751120	you log into social media or whatever and you're like, hey, I'm Sam. And it's like, cool. What
2751200	2755200	type of people, instead of who do you want to follow? It's like, who do you want to follow you?
2756000	2760320	And like you end up with like hundreds of AI characters or frankly, I think what's more likely
2760320	2764800	is it's a mix of humans, AI's and you're not really sure which is which. But they're caught,
2764800	2769040	they're the ones commenting on your posting, like, you're fucking great. Or like, here's a cool question
2769040	2772960	or whatever. Like, I think that's the world we're gonna end up on is like more and more segmented
2772960	2778160	niches, right? Where the ultimate end would be the her model where it's like, you just have one AI
2778160	2781600	girlfriend. I'm not sure we'll go there. I think that's really hard to pull off. And I think like
2781600	2787440	that's a tough thing. But if you told me that like, in the future, you know, on Twitter, good
2787440	2793280	example, you know, everyone has 100,000 followers, right? You're not exactly sure who's a person and
2793280	2798160	who's a robot, right? And they all fucking love you and it makes it super compelling and you feel
2798160	2803440	great. Like that's a very plausible future. Come on, birth rates are gonna do that. Have you seen
2803520	2807920	that child of young male virginity under 13 the US for the Washington Post? They went from 8% in
2807920	2815760	2008 to 27% in 2018. That do you see what happened with replica on Valentine's Day this year? So
2815760	2819920	replica was originally about that was designed to be your mental health buddy, right? Until they
2819920	2826720	did realize you could charge $300 a year for a lot of roleplay until the 13th of February 2023,
2826720	2830960	when they get a message from Apple saying shut this off. So on Valentine's Day, they shut that
2830960	2835920	off. And then 68,000 people joined the Reddit the day after and said, why'd you do the boss of mine
2835920	2841760	with my girlfriend? You know, like, it was quite a massacre. That's where we're going. And look,
2841760	2845840	there's a whole history. I mean, again, like, we'll go back to porn for a second. Like, the whole
2845840	2850240	it's always fascinating. It's such an interesting base human thing. But it's like, look, it's like
2850240	2856560	the whole dynamic of like, you know, you know, how Tinder has affected sexuality and equality,
2856560	2859680	right? It's like, fascinating. Like, there's all these really interesting studies on this,
2859680	2864080	like technology has a deep impact on this type of stuff, right? But if people ultimately want
2864080	2868800	care about validation, titillation, whatever it's going to be, there's no question that
2868800	2875120	plays one place you, you and I will agree is that AI does dramatically shift the power on these
2875120	2879200	things, they will end up with weird or sub communities. And he says, here's my question to
2879200	2883840	you, though, we talk about power dynamics, I still think, and in this, I might be wrong about, I
2883840	2888480	will admit, because it's a little bit of a niche, weird industry. But my bet is that porn hub is
2888480	2893600	still the winner. I actually, I assume they're the biggest porn company, like I, or whatever Reddit
2893600	2898800	is, it was like, the place porn is doesn't shift, the platforms don't shift. It's just going to be
2898800	2904960	like, weirder, weirder stuff, and more and more AI generated. I don't know. I mean, this porn
2904960	2908960	hub isn't that big. So Mind Geek is the company behind it. They were just bought by ethical
2908960	2915120	capital partners. Because, you know, life is weird. Reddit could be a big winner of this. But I think,
2915840	2920080	you know, I've been already, like Reddit is already just full of porn, right? So it's like,
2920080	2923280	I just assume you are more full of porn. I'm sure they're going to be very smart about this,
2923280	2928160	you know, and engaging porn. But really, what you're saying is go along AI voices, you know,
2928160	2933040	like this kind of loneliness that they fill in, that could be a good investment team. Because
2933040	2937280	again, you have the whole whole life stuff that then emerges to these engaging people.
2937280	2940800	I think it's going to happen, but I don't think it's a good investment. And like, let me just go
2940800	2944320	back to like, just because it's going to happen doesn't make it a good thing to invest in. And
2944320	2950160	like, to me, it's really unclear where the leverage is in that, right? Like, it's like,
2950880	2954800	you're, you'd have to believe that somehow you're going to have dramatically more compelling
2954800	2960720	characters than like the next company also provide, right? Or you'd have to use it like,
2960720	2965520	I just don't use any lock in, I think, and I don't think there's any like other and so it's
2965520	2968960	really unclear just because it's going to happen doesn't make it an investment.
2968960	2974000	Well, I think there is kind of, if you kind of look at hook dynamics, there's kind of that trigger
2974080	2978960	reward kind of dopamine rush and lots of stuff that you invest into each character. So there's
2978960	2982560	probably going to be a lot of first mood and advantage here. On the other side, you have the
2982560	2986800	licenses, you have the IPs that can be brought to this, like not on the form side, but as a whole
2986800	2991680	gamut from board to your mental health buddy, right? I mean, I think ultimately, if you're
2991680	2997520	basically saying, is there a solution to loneliness and solution to making you feel good,
2997520	3000880	there's a whole gamut of different things that can happen here, where you've got IP,
3000880	3003920	wait for these other things. Again, the example I think that comes from that is
3004640	3007760	the hollow life influences that going up like that.
3007760	3011760	Not to push you with it. I mean, it sounds like you're agreeing with me, which is like the leverages
3011760	3016720	in IP, right? Or the leverages in distribution, right? For this type of stuff, because the pure
3016720	3023520	tech stuff to it, it's like, yes, there'll be good jillions of, you know, virtual girlfriendy,
3023520	3027840	whatever things, but it's not, those are not platforms you can invest in. And they're like,
3027840	3031680	they're not really valuable, even if there's a lot. I think bringing it all together is something
3031680	3035840	that will take time. So I think there will be a lot of first-degree advantage. So like with stability,
3035840	3040240	again, data distribution are key, right? So my thing is, take the best of open, which we stimulate
3040240	3045120	and we fund lots of, build the stable series of models already without any data and distribution
3045120	3050560	to it. So open data, commonsense data, national data, and then we take it through cloud system
3050560	3055200	integrators on-prem and I take a share of all that revenue. So I agree, that's kind of cool to a good
3055200	3060560	business. But what I'm saying is, I don't believe in this particular area, going from port at one
3060560	3065440	end to mental health buddies at the other end, there are established distribution networks.
3066560	3071280	I think there'll be a lot of opportunity there for first mover advantage.
3071280	3078080	In the history of investing, first mover advantage has generally turned out to be a pretty bad
3078080	3084720	investment. Okay, maybe not first mover advantage, just say first proper entity advantage that takes
3084720	3090560	advantage of classical good company dynamics. There aren't big companies there yet.
3091120	3096560	Yeah, maybe. Again, I think it's a little hard to know exactly. There's a huge spectrum here,
3096560	3101120	it's hard to like, exactly react. But I would say, like, look, I think we're agreeing that like,
3101120	3105280	entertainment's going to get more entertaining, right, and cheaper to produce, right?
3106080	3110320	I think we're agreeing that IP is very valuable and maybe it's more valuable. Like, so maybe the
3110320	3116400	answer is buy Disney stock. Because Elsa is going to be a way cooler character when like,
3116400	3121040	that's kind of obvious, kind of obvious, right? And like, I think we can all agree on that. I think
3121040	3124720	what is not clear to me is outside of the IP plays, outside of the distribution,
3124720	3131040	existing distribution plays, like what IP, what AI really unlocks is a new disruptive
3132160	3137120	vector for this type of stuff. Because I don't, I do think that there are some pure AI type things
3137120	3142800	you can do. Again, we'll talk about the AI girlfriend thing is just unclear what the payoff
3142800	3146880	is there, right? Because they don't have any modes. Well, I think if you look kind of, you can scale
3146880	3152160	a certain type of human endeavours, shall we say, for example, the origin of therapists in the world,
3152800	3157280	you know, and it is a regulated industry. But at the same time, there is a gap for therapists,
3157280	3161200	just like you have the meditation apps kind of step in, and they created calm, and they
3161200	3166160	created these other things that were huge. Now this is more engaging. So I think one of the areas
3166160	3170880	to look at is where can you not find enough people that can fill in some of these things and then
3170880	3174720	build good experiences around that if you're looking at companies that can come to the fore,
3174720	3179200	because there isn't an existing solution. This is why, like I said, for me, I look at the global
3179200	3184240	south, I'm like, there's lots of gaps. I look at kind of here, and there's again gaps, where are
3184240	3188640	the gaps that you want to go because you can basically create a market need to fulfill a key
3188640	3193920	customer need. And so again, I looked at mental health in particular, and that goes again from
3193920	3199840	the porn AI waifens all the way through to proper mental health, kind of therapists. There's a huge
3199840	3205040	gap in that particular market, and there's a huge chasm of loneliness, and a lot of products that
3205040	3209920	could be built that are generally useful. And that can go quite fast enabled by this technology,
3209920	3213920	where they were not enabled before. I think this has been fascinating. I have kind of a handful
3213920	3219680	of concrete prediction questions that I kind of want to get you guys on record with if you're
3219680	3225520	up for it, and see if you have similar concrete predictions are different. And then we can obviously
3225520	3232240	check back in on in the future. How does the market for inference shape up? And for a jumping off point,
3232880	3237760	how do you think it might look different from the current cloud infrastructure market?
3238640	3243040	I think inference will be the vast majority, but I think it's like GPUs to assets with Bitcoin
3243040	3248160	mining. Because these are big research artifacts that are pie torches, but the output is a little
3248240	3251920	tiny part of binaries. And that's not a complicated thing to run inference on.
3251920	3258160	You see in Forensia 2 on Amazon Cloud, you see kind of the TPU v5s and others. I think there'll
3258160	3262800	be more and more customized solutions as you move from that research to engineering bit. And then
3262800	3267600	the cost competition goes massive in a few years time. Over the next few years, I think there'll
3267600	3271200	be a shortage because everyone will try to use this technology. There won't be enough. And then
3271200	3275840	eventually it'll move towards the edge because I think there's just all this magnitude optimization
3275840	3279200	that we can do from here. Yeah, I mean, this is a little bit beyond my
3279200	3283680	direct wheelhouse. But I think in the end of the day, what I'd say is like, I highly suspect
3283680	3287680	because the distribution is indifferent, right? And the patterns aren't different in any of this
3287680	3292720	stuff that we're going to see is everything from chipsets all the way through to cloud providers.
3292720	3296800	Things look basically the same as they do today. Everyone's just making more money.
3297360	3301200	Yeah, I think inference is also interesting because in the cloud, you just move to wherever
3301200	3305360	the cheapest inference is for these models. And so it's quite a mobile thing. So you've
3305360	3311280	got NVIDIA coming forward for that reason. Question two, what happens to the price
3311280	3315200	of primary care medicine in the United States over the next 10 years?
3317440	3323760	Unfortunately, given the issues, I think it's correct. It should go down. The regulatory capture
3323760	3330400	is far too strong. Unless something major, major happens. Question three, you guys both
3330480	3335840	have kind of said there's a ton of junk out there. It seems like broadly, we're not expecting
3336960	3342960	that many major incumbents to be disrupted. What would you guess would be the most likely
3342960	3349760	incumbents to be disrupted if you had to pick some? Stack overflow. I think it's 4.1.6 billion
3349760	3356720	by process, right? Whoever. I mean, I think the process is probably fine. You've seen disruption
3356800	3362080	in CHEG and other things. We didn't really get into this, but I do think that some SaaS companies
3362080	3367200	with low switching costs will be at risk from some of these higher context window companies
3367200	3372160	where you can put 10,000 words of instructions in. Because some of them are relatively basic
3373040	3377280	in that way. Actually, for words, I think we once again mostly agree. The only thing I think is
3377280	3383120	at risk for things like Zapier, right? Or some of these like kind of like, and it's kind of a 50-50
3383120	3387680	because they also get way more powerful. But I think there's a bunch of SaaS tools that
3389120	3393120	probably end up looking more like features where they used to look maybe like companies
3393120	3399440	because of the AI. But real incumbents, like public big multi-billion-dollar companies,
3400000	3403120	I mean, I don't think any of them are really at risk in disruption. I think they're all just going
3403120	3408480	to get stronger. I think a bunch of startups or series A companies are going to get swiped out
3408480	3412160	or all of a sudden not going to be able to grow, right? Because I think the big guys will just get
3412160	3418640	better faster. Will the big tech companies that are currently open sourcing,
3419440	3426320	for example, Meta, Salesforce, will they continue to do so? Or will they stop?
3427040	3430800	Well, I think Meta has moved to non-commercial open source for all their open source.
3431360	3435840	Now, I think Salesforce has kind of continued to do full open source. I think it's just very
3435840	3440560	difficult because the regulatory environment become tougher and tougher. And it's not called to their
3440560	3448800	business to open source. I think that it would be 100% driven by business models, right? So like
3448800	3455840	Meta, if you think about it, is incredibly well positioned should generally the level of AI continue
3455840	3460000	to grow in the world, right? If you think about it, it's like the way they're going to monetize that
3460000	3466000	is having dramatically better ads, right? And like dramatically better content in a bunch of ways.
3466000	3470000	And so I think they have a heavy incentive to think about it, to like keep up and sourcing
3470000	3474240	it, they want the talent, they want, you know, the reason companies also open source is like,
3474240	3479040	there's like a real internal external interplay, right? In terms of how you build an ecosystem,
3479040	3483600	they attract great talent. So I think they'll still keep happening. But I think you'll, I think
3483600	3487280	the list of people who are supporting open source stuff will shrink, right? If that makes
3487280	3491520	sense, as people get super competitive about this stuff, and the battle lines are drawn.
3492080	3499200	If you had a billion dollar company, you know, of any, of any kind, could you come up with a story?
3499200	3504000	Could you identify a type of company that should not, you know, where it wouldn't make sense, or
3504000	3510880	let's even frame it more decisively, where it would be defensible to not be investing, say,
3510880	3516720	at least a million dollars in figuring generative AI out today? In other words, is there anywhere
3516720	3521760	where this is not relevant? I mean, I'm sure there is, but nowhere I can think of offhand.
3521760	3526000	I think it's relevant just about everywhere, just because you always get a level of productivity
3526000	3531360	increase. But, you know, as Sam said, for a lot of industries, is this sustaining innovation? It's
3531360	3537200	just the next stage, as opposed to massively well changing, shall we say? What happens to
3537200	3546080	the marriage rate and the birth rate in, say, the United States as AI companions of all sorts
3546720	3551840	become available? It clearly goes down everywhere. I mean, like, look at South Korea, they're at
3551840	3556320	0.8 now on their fertility rate thanks to video games and a few other factors.
3556320	3560640	There are negative and positive ways to spin this, actually. Like, I think I personally, I
3560640	3563920	have the negative take on this, like, I think that's the future and a bunch of other things,
3563920	3568000	but here's the reality. It's just a simple economics thing, which is like, if the world was
3568000	3574000	more entertaining, then like, that makes doing un-entertaining, hard, long things like having
3574000	3579440	kids and raising them like less appealing, right? It's like, Tinder, Tinder is going to hurt the
3579440	3584080	birth rate. Like, AI is going to hurt, again, it's just sustaining innovation, which is technology
3584080	3587920	generally is going to hurt the birth rate. Yeah, and then you see places like Japan where you've
3587920	3591680	got declining birth rates, really embracing this because they want the productivity increase,
3591680	3595440	which is the other flip side of this. See, if you're more productive, you less people.
3595440	3599520	Yeah, I mean, that's the irony that you talk about the long-lived and the diamond age you
3599520	3603040	referenced earlier, like a really long-term sci-fi story is pretty simple, which is like,
3603840	3609760	a highest, highest, highest level, like technology will drive there to be fewer people. And then
3609760	3614480	because there are fewer people, we need more technology, right? And like, it becomes a symbiotic
3614480	3617520	thing. That's the really sad part. I mean, like, it's all sort of people thinking about like, oh,
3617520	3621920	shit, like the entire human population is going to fall off a cliff, right? It's because we're
3621920	3628720	like entertaining ourselves to death. Do you think any AI leader, you know, open AI right now or
3629680	3634640	somebody who takes, you know, the leading position from them in terms of having the best model,
3635680	3643360	can sustain super high gross margins for a few years into the future?
3643360	3647120	Based purely on the AI, no. It needs to be distribution data.
3647120	3654560	I think the proprietary side, it's just unless it's super data unique, you're going to zero.
3654640	3659840	I think that you have Google and open AI as uneconomic actors. And that's incredibly difficult.
3659840	3663840	You know, so just to unpack that, you mean that basically they won't allow,
3664400	3667840	they don't intend to make a ton of money on this and they won't allow any models to either because
3667840	3672320	they're going to provide it at cost. They don't care about, yeah, they're probably under cost to
3672320	3676400	get the data. You know, again, they have different business models, Google cost shifts all the time,
3676400	3681600	right? This is why I went to the other side for open models to private data and standardizing that.
3681600	3686320	No one's making money on open models alone. Well, I mean, there is a way, there is a way.
3687440	3691120	So what basically what I do with my business model is standardizing it.
3692320	3696720	And then providing all the services around it as a blueprint for my partners to take forward.
3696720	3703520	Yeah, I mean, they're the consulting nexus version of this, like that you can probably pull off.
3703520	3707680	Again, consulting models, I think, again, obviously saying you're pursuing, but very difficult.
3707680	3710720	I build the models, I give it to my consulting partners and they take it forward.
3710720	3717200	That's why this is my theory of stability has been a partial theory. It's obviously a lot of
3717200	3725440	facets to the organization. But I kind of view stability as the the provider for like the non
3725440	3730000	aligned countries, if you will, like those that are like, we definitely don't want to buy from
3730720	3738160	corporate America. We want to own our own. We want control. Those folks seem like they have
3738160	3743920	nowhere close to the resources domestically to build their own systems. But they do have kind of
3743920	3749680	a point of pride and also just practicality, right? Like if you're an African government
3749680	3753440	and you want to get your own legal system into a language model, you know, who's going to do that
3753440	3758160	for you? That feels like a real sweet spot for stability. How much of the future do you think
3758160	3763360	is kind of serving that kind of third set of countries? No, I mean, look, we're carrying
3763360	3768000	subsidiaries and dozens of countries bringing all the top family offices with data and distribution.
3768400	3772960	And national models and national data sets based on broadcast data, we take a subset of that make
3772960	3777600	that open. And we've got the rest of that for our commercial side. So I think the global south
3777600	3782800	is the focus for us. Plus, there were these big multinational companies building dedicated teams
3782800	3786240	with them. Because we're the only company in the world that can build you a model of any
3786240	3791280	single majority or type. Is that sustaining? Who knows, but it's a decent business. And so my
3791280	3795360	thing was build a decent business doing decent stuff, doing something different to other people.
3795360	3798080	I'm sure there'll be more competitors. But again, let's see how it goes.
3798960	3804960	Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually
3804960	3810400	work customized across all platforms with a click of a button. I believe in Omniki so much
3810400	3820160	that I invested in it. And I recommend you use it too. Use Kogrev to get a 10% discount.
