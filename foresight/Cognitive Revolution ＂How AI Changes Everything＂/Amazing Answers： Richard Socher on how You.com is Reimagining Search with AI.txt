early 2022, or like we had apps that would write code for you within the search results,
through the apps that write essays for you within the search results. But whenever we
innovated and changed the default Google experience too much, we had just like the vast
majority of our users say, I'm so used to Google, I don't want another way of finding answers.
And so we kept getting pulled back to this need. And so the most amazing surprise was when
ChachiP came out, all of a sudden people got it. And it was like, wait a minute,
it could just be like pure text. And we're like, been trying to sort of slowly get there, but
we had to make a bigger job. The way I think about the different modes is like the default
smart mode is kind of like, if you had an assistant, and you just ask them to do a quick
a search, and in like two or three minutes, give you an answer that. And then genius mode,
you go and so you want to ask your assistant for a question that they have to be able to program,
they have to search the web, and then they need to be mathematically applying to answer that
question. I mean, as a kid, I also enjoyed watching Terminator. It's like a cool action movie,
but it's just taken over so much of the AI narrative. And it's actually like actively
hurting, especially European Union. Hello, and welcome to the Cognitive Revolution,
where we interview visionary researchers, entrepreneurs and builders working on the
frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas,
and together we'll build a picture of how AI technology will transform work,
life, and society in the coming years. I'm Nathan LaBenz, joined by my cohost, Eric Tornberg.
Hello, and welcome back to the Cognitive Revolution. Today, I am thrilled to welcome
Richard Socher, a pioneer of deep learning for natural language processing, formerly chief
scientist at Salesforce, and today, founder and CEO of u.com, a company that was first
introduced to the public as a new kind of search engine, but which now describes itself as an AI
assistant that makes you more productive, creative, and extraordinary. Richard has deep history in
deep learning. He was among the very first to recognize the potential of neural networks
in the natural language processing domain, and his work has helped shape the field as we know it
over the last decade. In this conversation, Richard takes us on a brief journey through his own
intellectual history and reflects on how the field of AI has evolved in both expected and
surprising ways. Before we dive deep into the u.com product itself, covering the historical
challenge that they faced when trying to compete with Google and how the rise of the AI chatbot
paradigm has broadened the space of possibility for search and discovery products. We also look
at u.com's various modes with particular emphasis on the genius mode and above all, for me, the
research mode, which delivers amazingly helpful and thorough report style answers, even on some
remarkably complex topics. We also briefly discussed the future of AI business models as well,
including the obvious subscription and my pet theory about the AI bundle. Along the way, we touch
on a number of important topics, too. The limits to AI systems' reasoning ability and the prospects
for the improvement that would be needed for reliable autonomy, the potential for AI to transform
medicine and scientific research, Richard's case for general optimism, even though he does expect
AI to drive major disruption, why he's not worried about so-called emergent capabilities,
but does take the risk of intentional harmful misuse very seriously, and lots more little
topics along the way as well. Richard is a leading thinker in the AI space, and his perspective
is essential for anyone who wants to understand where this technology is going and what it means
for the future of humanity. And in all seriousness, I really do recommend u.com. It has absolutely
joined the ranks of the AI tools that I use multiple times each week. And particularly,
when I want a comprehensive, multi-page report style answer, I find that u.com's research mode
is often the single best tool available today. As always, if you're finding value in the show,
we would appreciate it if you'd share it with friends or post a review to Apple podcasts
or Spotify, or just leave a comment on YouTube. Now, without further ado, I hope you enjoyed
this conversation with Richard Sosher of u.com. Well, let's do it. I think this is going to be
a lot of fun. I'm looking forward to your point of view on a bunch of very interesting topics.
Richard Sosher, founder and CEO of u.com, welcome to the Cognitive Revolution.
Thanks for having me. I am very excited to have you. You are at the intersection of so
many interesting things. I sometimes have been describing myself recently as the forest gump of
of AI because I've just kind of very unstrategically made my way through the last few years and yet
found myself in some very interesting places. I don't know how you think about your own trajectory,
but you are kind of an OG in the realm of deep learning and have founded this very interesting
company and have a really awesome product, which we'll get into in more detail. And I'm interested
to hear about all that and your kind of philosophy and expectations for the future. So we've got a
lot of ground to cover. Maybe for starters, you want to kind of give us, and I usually even ask
these biographical questions, because these days it's like a lot of the same answers. People are
like, oh, when I saw GPT-3, I thought this is going to be a big deal that I got involved. But you
were there at the beginning, man. So maybe you want to just give us a quick history of
your own role in the history of deep learning and how you've kind of come to the present.
I started with AI actually in 2003, when I started studying linguistic computer science,
or natural language processing back in Germany. And at the time, I was like, this is really
interesting. I love languages, I love math, I love computers, you know, so if computers are
where languages and math can meet in some useful functional ways, I thought. And there's very much
sort of a small niche subject within computer science. And I was really excited. At the time,
there wasn't quite enough math for me in an LP. And I felt like we're just getting stuck in some
of the legalistic special cases. And I loved the form of semantic set theory and then algebraic
foundations. And so I moved eventually into computer vision during my master's. And there,
I also, in Saarbr√ºcken, at the Max Klein Institute there in the university, found statistical
learning and pattern recognition. And I fell in love with that. I was like, clearly, you can really
understand patterns, any kind of pattern really well, you could solve all these different kinds
of problems. And so I ended up doing my PhD at Stanford. In the beginning of Stanford, I started
trying to really contribute to the field rather than just learning about it. I basically found that
even the top NLP people, they write their papers mostly about these beautiful models,
like conditional random fields, late in university, other patient types of models.
But then most of the coding happens when they actually do feature engineer, right? They say,
oh, well, I wanted you to be entity recognition at a feature of like, this is a capitalized word,
and this is all caps word, or this is a word that has like, is one of the items in this list.
And this list includes, you know, city needs, we already know. And I'm like, man, this field is
very hand engineered. It's very like, graduate student ascent to get better. And then at the
time I was very 40, because Andrew A. got into deep learning on the computer vision side. He's
like, well, images are pixels, and it's a fixed number of pixels. So we can feed them into a
neural net or at the time, you know, variants, models, restricted development machines. And I
was like, wow, maybe we can use ideas from that for natural language processing. And there was
maybe like one or two relevant papers all from a number there, and Jason Weston, and, and a few
like one or two others. But no one really enjoyed that approach, no natural language processing,
paid any attention to it. But I thought, silly, that has to be the future. I want to give the data
and I want to get an output. And so in 2010, I started publishing my first neural net paper,
worked on my computer vision before and saw some of the power of ImageNet also back to and really
started running with it. Got a lot of rejections all throughout. But, but at some point, I sunk
my teeth into it. And I just like, I loved it. And I thought this is the future. Despite all the
rejections, I kept going at it. And then after the PhD was over, there's sort of starting to be
more interest in deep learning and neural nets for NLP. But still, no one in the world was teaching
that as like the official right way of doing NLP. So I started teaching at Stanford also,
first as a busy lecturer and then as a professor, started, you know, being a fortune and lots of
very smart students back then, really like the hiding place founders invested in their first
round. And then, you know, also wanted to bring these neural nets into the world, started a
menomind, my first startup to do that, to build a general purpose platform between neural nets
very easily, both revision and NLP, got acquired by Salesforce, became a chief scientist there and
EDP eventually. And in Salesforce, we had my probably last and biggest rejection was on inventing
front engineering in 2018. And we're so excited about it, because there's the culmination personally
for me also of this decade long dream I have, building a single neural net for all of NLP.
And the idea was, you know, at the time, every AI model was built for one task, you will have wanted
your sentiment analysis, I built a sentiment analysis model, you wanted a translation, I built
a translation model, and they're all different. We're like, what if we could just build a single
model? And you just ask it a question, what is the sentiment? What is the summary of the sentence?
Who is the president in this paragraph? And that was kind of for us, I thought like,
the most exciting thing you possibly could be doing at just get this tech talk about it
came out last week. And but it was exciting. But it did inspire a couple of other folks.
And like, when opening, I was, you know, publishing your papers, that should be true.
And three, they cited that paper saying like, look, they were able to have a single model
for all of NLP, if you just ask them these questions. And, you know, that's now prompt and
the rest is kind of more well known history. That is an amazing history. And it definitely,
I don't know how, you know, modest you want to be versus taking credit for foresight. But
certainly, the idea that there could be one model to solve all these, you know, tasks was not
obvious to people. And boy, we still see this, the flaws in the peer review process are still on
prominent display these days. Most recently, I noticed this with the Mamba paper, which I was
a very interested reader of. And then went over to the open reviews site and was blown away by how
negative some of the reviews were like a confident reject was given. So that was kind of, you know,
just a good reminder that, yeah, this is still an unsolved problem.
What would you say has surprised you most from like the big picture since you,
and you know, it hasn't been that many years, right? But since you kind of had that notion of
this generalist NLP model, fast forward, now we have, you know, GPT four and possibly Q star or
something like that in the works, you know, is this the trajectory that you thought we'd be on?
Or how has it deviated from what you imagined back then?
It's very much aligned with what I hoped the field could get to. And now it's almost like,
it's like obvious, right? Like no one no one questions this anymore, we've had all these
breakthroughs. And I think the biggest surprise was maybe more on the application side of things
and that for us, you know, we've been playing around with large language models at u.com and
infuse them into search results earlier, like early 2022, already, like we had apps that would
write code for you within the search results, through the apps that write essays for you within
the search results. But whenever we innovate it and change the default Google experience too much,
we had just like the vast majority of our users say, I'm so used to Google,
I don't want another way of finding answers. And so we kept getting pulled back to this need.
And there was kind of annoying. And so the most amazing surprise was when
Chatchity came out, all of a sudden, people got it. And it was like, wait a minute,
it could just be like pure text. And we're like, you know, we've been trying to sort of slowly
get there, but we had to make a bigger job. And that was incredible. That unlocked a lot
of people realizing we handle links isn't the best way to get an answer. An actual answer
is the best way to get an answer. And that's the text.
So let me give you a couple of my experiences on u.com recently, and then you can kind of tell me,
you know, where you are the overall story. And then I really want to kind of unpack the
kind of use the product as it exists today and the roadmap and everything you're working on
as a way to kind of explore a bunch of different aspects of where all this
is going, you know, and I think that's really the mission of this show is to kind of help people
see around the corner and starting with me, helping me develop my own worldview.
But I've been really impressed with the product recently. You know, listeners will know that
I've been a big fan of perplexity. We've had Arvin on the show a couple of times.
And I think they do a great job and, you know, we're made a fan. But I have found
distinctive value in at least two modes on u.com recently. One is the
research mode, and the other is the genius mode. Those to me have stood out as the most
differentiated. For research mode, I recently took it like a 200 word question that was all about
mixture of experts architectures. And, you know, kind of is there curriculum learning,
you know, stuff happening here? How, you know, how do people think about sort of
the tradeoffs between like how many experts should we have and how big should they be and
how many should we activate at any given time? Are there any like scaling laws or whatever,
you know, designed for that sort of thing? Just every basically every question I could
think of about mixture of experts, I took it all in one go. And it was really impressive to see it
kind of break that down and go through multiple steps of searching and analysis and,
you know, really implementing kind of like, you know, kind of a classic agent, what is at
this point, you know, a six month classic agent setup, but applying it to that research question
and just going, you know, down the line, really quite valuable results. And it definitely is
something that I will come back to and have already, you know, found myself kind of being like,
I think this is a good one for u.com research mode. Genius mode is a little bit different and more
kind of analytical. I'd be interested to hear a little bit more about how you think about the
differences. Because I did, I then tried one that was a big Fermi calculation exercise,
where my questions were like, what are the different data sets that exist in today's world?
How big are they? How do they compare to each other? How do they compare to
the training data size for GPT four? You know, how do they compare to available compute? Like,
because I have this, I have a big question, which is kind of one of the ones I want to get to
toward the end to around like, to what degree is ML research poised to start to be kind of semi
automated. And so I'm trying to try to rent my arms around that with these furry calculations.
So genius mode was really the best way to approach that. And anyway, I would definitely
encourage people to bring multi part complicated questions to both research mode and genius mode.
And I think you'll be impressed with the results. And I would say that, you know, even
with, you know, the expectation that folks who listen to this show have tried, you know, other
leading AI products. So that's kind of my unpaid endorsement, very sincere. And I'd love to hear,
you know, a little bit more about how you think about those different modes, how they work,
and just kind of big picture, like where we are in the you.com product journey long term.
Hey, we'll continue our interview in a moment after a word from our sponsors.
The Brave Search API brings affordable developer access to the Brave Search Index,
an independent index of the web with over 20 billion web pages. So what makes the Brave Search
Index stand out? One, it's entirely independent and built from scratch. That means no big tech
biases or extortionate prices. Two, it's built on real page visits from actual humans, collected
anonymously, of course, which filters out tons of junk data. And three, the index is refreshed with
tens of millions of pages daily. So it always has accurate up to date information. The Brave Search
API can be used to assemble a data set to train your AI models and help with retrieval
augmentation at the time of inference, all while remaining affordable with developer first pricing.
Integrating the Brave Search API into your workflow translates to more ethical data sourcing
and more human representative data sets. Try the Brave Search API for free for up to 2000
queries per month at brave.com. Yeah, these are a great question. I think it shows you kind of
how sophisticated the space has gotten in the last year. Around this time, last year, we were the
only search engine with a web connected LN and millions of users. And now that idea has been
copied so many times, including as mentioned by Plexi. And so I think what you have to differentiate
kind of the different modes, and I think the modes kind of show how sophisticated that the
space has gotten and how hard it is to still differentiate on better technology versus just
you know, designing the market and marketing and things like that. And so we actually did a comparison
to Plexi with 500 real user queries. And we asked which answer do you prefer? And it came out to be
that 50% of the cases users prefer the U.com answer and they prefer the Plexi answer and 30%
they don't see a difference into answers for our default, we call it the smart mode. That's kind
of the default. And just to give you a sense of what that looks like. So here's an example of what
the default smart mode looks like. You know, there's some doping case that happened and
you can see lots of careful citations. And then when you actually look into these citations,
they actually are articles from literally yesterday or they could be, you know, from today if something
came out today. So that's kind of the default smart mode, you get a quick factual answer.
But then we thought, well, what if you have a pretty complex question like math, physics,
chemistry, science, or like complex numbers. So here is a genius mode question, it kind of gives
you a sense of what it does. And it doesn't mention like what you say, which is there's an
important LM that orchestrates multiple other LMs to actually do the right thing right. So
the question here is find the current population of the in the United States, then it's lots of
population from 233 to 10, 100, and then assuming a 2% growth rate. And then it will go on the
internet, it'll find the numbers, and then realize like, well, I got to now visualize those numbers,
now that I have any, so it will code up in Python, what this could look like, execute the code,
and then gives you this answer, and visualizes it in a nice plot. And so that I'm still sometimes
amazed, I try and I push it, and you know, sometimes it fails. And sometimes it fails
because it tries to load the library that has security issue. And then it's like, okay, I'm
going to try to rewrite it without this library, but it's going to be longer and messier code.
And like, it's just incredible how hard it can try and what it can do. And then the third mode,
like you said, the research mode, it will go into a lot of detail, it will not just look up
all the stuff we have in our index already, like news and things like that, but it will go on the
web and find your website, so the multiple different searches on the web, combine all of that,
and then give you these beautiful research reports. This one is like, seeing a background,
actually, any consequences of the telecommuting work. Now it's like, history, you have to write an
essay or something. And it's just like, writes you just perfect, like, beautiful essay, each
sentence has one or two citations from different sources, and you can verify all of them. And
one thing we found this actually also is like, you have to like, just the citation lot is a
non-trivial aspect of building this all out. Because you have to, we actually found that some
of our competitors just randomly add numbers and citations to sentences, and you click on it,
and it doesn't even mention that back anymore. Which I think it really undermines the space of
chatbots for search. So citation accuracy is one of the many sub-AI systems that you need to do
correctly here. And then, you know, they're just like crazy things, like create a table,
some nice cancelling headphones that are not expensive, and just like, put this table together,
pull some images, give some pros and cons of each and the price. I think sometimes, to me,
is how well and general the system is able to answer these questions. And it shows you how
complex the space is gotten and how much you have to do now to still differentiate on the
technology. This is one of my mantras at Waymark. I always say the water line is rising quickly.
So we, you know, we better keep climbing the capabilities ladder ourselves. The four examples
that we saw there, one was the kind of default smart mode. The second was genius. Is that right?
The one that showed the code example. And then the last two were research. Yeah. What more can you
tell us about kind of how those work? Like I'm interested in, and by the way, like the
audience of the cognitive revolution is interested in the details, the weeds, the nuggets, you know,
all that stuff. So you can go as deep as you're, you know, willing to share. I'm interested in
all aspects, you know, prompting, I'm sure, obviously, is going to be different. Scaffolding
is going to be different. Maybe even the models are different. I'm also really interested in,
like, what are you using GPT-4 that you've got your own in-house trained ones as well. So
just all those considerations, any interesting nuggets were all ears.
Yeah, I'm going to try to balance a little bit the not telling the competition exactly how it's
all done, but it'd be interesting to your ears here. So at a high level, there are two major
stacks. There's a search stack and a chat stack. The search stack, we actually had to build an
entire index ourselves for the web because being super expensive, not as high quality, Google
is very hard to access. You have to have special agreements or, you know, some people kind of
steal slash bootleg slash leave some surveyed the eyes to use Google results in like a somewhat
sketchy legal gray area, which we don't want to do. And so we, we basically ended up having to
build our own index. And that's hard. And there's still, you know, a lot of complexities behind
that. But what do we, the main difference of this new index is that it was built with LNs in mind.
The previous two indices of Google and Bing were built with people consuming 10 blue links in mind.
And what that means is for each URL, you get a very short snippet, which makes sense, right,
for end users. But an LN could read all these other snippets, they can be very long,
and then extract the right answers from that, and then just give you that right answer as the
user. And so what was surprising is actually when we benchmark this, our API ended up being more
accurate than Google or me and go to API.com. And like, I'll, I'll see you on the screen here for
a second again. But like, it's surprising that, which are a lot of people that you could actually
be more accurate in Google or Bing at all. But it is because we're at an inflection point in,
in a, in the eye, and it's a different way to value. It's like, we're almost like
cheating by having these really long snippets. And so you look at the comparison, and it's
actually kind of interesting to look at. And a lot of people have asked like, how do you compare
accuracy in LNs? How can you evaluate this? And so just to give you a sense, here's like, what,
what this looks like, the first a version is just like reasons to smile. And now you can use whatever
LN you want, but you can see into your prompt is very, very long snippets from many different
URLs in a very short amount of time. And then we also have one that just does everything,
like it gives you an LN answer, and it tells you like all of these things. And so how do you
evaluate this is actually, it was an interesting, I think insights insight from our team, which was,
you can take question answering data such as hotpot qa, squat, so the question answering
data set MS, Microsoft, Marco, fresh QA and so on. And these data sets are structured such that you
have a paragraph, you really have a question, and then you have a subset phrase from that paragraph
that is the right answer to that question. And so what we do is, we basically take those data sets
but we throw away all the paragraphs. And then you have to find the right answer. And the paragraphs
have to come from the internet. And so you replace paragraph with a web search engine. And that's how
we evaluate it, the JIT, the big Google and the public APIs, and have outputs on them. So kind
of nerdy, but that's the whole tech stack. And we're we make that now available to every other LN.
So that's the first. And then the second thing is what we now have started calling the LNOS,
the operating system of large language models. And it's a term inspired by Andrey Kapathy.
And it's not like the most perfect metaphor, but I think it captures a lot of the essence, which is
you have now this new staff that operates at a much higher level of abstraction. And the LN is
kind of a CPU. But just like a CPU or a kernel on an operating system, like it's important to
orchestrate everything and to do computation. But if it still needs a hard drive, which is
right, right on your own vector database that's grown up, you have an internet connection, which
is, you know, the internet. And that's what we're providing. You may orchestrate other LNs that
could be considered like the GPU or something. And then you have a bunch of apps that are sitting
on top of that. You have a Python code interpreter, which we see our genius mode, all of that. And so
to summarize all of that in one short term, we call the LNOS. And inside of that, we're now seeing
a lot of our customers are using our APIs and search site. They're kind of going through the same
lessons that we had gone through when we built dot com and made it like having the most accurate
answers out there. And it's actually highly non trivial. A lot of people saying it's just like
an LN wrapper, right? But then, and you even have like open source project that show it.
And then you ask, like, okay, when was Obama born? Where was he born? And then it fails. Why does it
fail? Because when you send where was he born to your search back in is not going to return you
any useful results. Because it doesn't know who he is. Who does he refer to, right? And there's
tons of things like that where, as you have a longer and longer conversation, especially in
smart mode, you refer back to states. You can say like, Oh, what's a big CRM company? And then the
answer inside is Salesforce. And you ask, Oh, what's their stock price? Now she sent what's their
stock price to your search back. And again, it's not going to return anything useful. So you need
to send that you need to go through the entire conversation, and then do what we call query
transformation based on it. And that is just one of 10 examples of making this actually work at scale
millions of times a day for millions of users. Like, it is a lot more complicated to make it
accurate. There are about 10 other such models that if you think about the space and you really
listen and look at like user data, you listen to where it's breaking, you will eventually get to
and we're now like thinking about offering more and more back. So I'm tempted to ask for the other
nine things there. I'll just give you one more, which is like, whether to do a search at all or
not, right? Like, because you asked like, write me a poem about like the beautiful Bay Area and
like a sunset love story or something. Like, you don't need a citation at every line of that poem.
And so it would actually clutter up the prompt to add a bunch of facts about poems and so on.
And the history of Silicon Valley and all of that. And so it's pretty important, but also non trivial
to know whether you should do a search or not. And again, some, some websites just slap search
results on top of everything, even if they're not relevant for having more conversation about
your feelings or something. Did I understand correctly that the kind of big difference is that
the U.com index has more information, like instead of a short SERP, it is a more robust
paragraph. And so independent of the language model that you're using, the richer context is
just better kind of serious enable you in that way, you're kind of decoupling the what information
is found from language model that is doing the analysis. And more information is kind of the
big differentiating factor there. Drive that right. I would be careful and say we have overall
more information. We're focused a little bit more on the main languages that we see. We don't support
some like very rare like Indonesian, African sensual Asian dialects and so on yet, but we
return more information per rare because of these large limits. So, so it's sort of, yes,
yes, there's more information, but you know, I think the long tail Google Prop still has a larger
index. If you look for this like rare, like Indonesian kayaking sites that like rents out
kayaks on this little lake somewhere, like, and it's all like not in English, like we might not
have that website. But when it comes to like Western world news where, you know, we have a lot of
users, then Latin America and so on, then we shine and return much more information per pair.
Hey, we'll continue our interview in a moment after a word from our sponsors.
If you're a startup founder or executive running a growing business, you know that as you scale,
your systems break down and the cracks start to show. If this resonates with you, there are three
numbers you need to know. 36,000, 25 and 1. 36,000. That's the number of businesses which have upgraded
to NetSuite by Oracle. NetSuite is the number one cloud financial system, streamlined accounting,
financial management, inventory, HR and more. 25. NetSuite turns 25 this year. That's 25 years
of helping businesses do more with less, close their books in days, not weeks, and drive down costs.
One, because your business is one of a kind, so you get a customized solution for all your KPIs
in one efficient system with one source of truth. Manage risk, get reliable forecasts,
and improve margins. Everything you need all in one place. Right now, download NetSuite's popular
KPI checklist designed to give you consistently excellent performance, absolutely free and
net suite.com slash cognitive. That's net suite.com slash cognitive to get your own KPI checklist.
Net suite.com slash cognitive. Omniki uses generative AI to enable you to launch hundreds of
thousands of ad iterations that actually work, customized across all platforms with a click
of a button. I believe in Omniki so much that I invested in it and I recommend you use it too.
Use Kogrev to get a 10% discount. I've been struck recently that it seems like,
obviously, search in general has kind of been a monopoly for a long time.
As you noted, the user experience was kind of something people were not necessarily looking
to explore new things on the nature of the index. Of course, they've done millions of
person hours of work on it, but it seems like it's kind of been a pretty consistent paradigm of
crawl around and find everything and suck it up. Now, we're starting to see these interesting,
I don't know if you can share more about how you create your index, but we just had actually a
sponsor brave talking about their index and the way that they are building it through users
actually visiting websites and taking a sort of not just blindly crawling around and following
every link, but what are people actually engaging with online, which struck me as a pretty interesting
and very different twist on it. I want to kind of pull this apart in a couple of different ways,
but is there anything that you would want to share about how you think about building an index
that, aside from just bigger, richer content, is there a different tactic as well that underlies
that? The tactic is more about how we make that work for LLNs better, and I don't think there's
that much differentiation on how it will fall. You have to have a bunch of data that's been
helpful to have run a search engine for several years and get user behavior and knowing what
people actually want to have called and want information for. You can also sound surprisingly
buy a lot of that data in bulk. I have a few questions on the business side or the kind of
bridge the technology and business side. Google obviously has been free and has been ad supported.
It seems like the new generation of AI first LLN enabled search is going more in the direction so
far of a subscription. As far as I've seen in my U.com usage, I haven't seen anything that jumped
out to me as sponsored. Another dimension too is like, I mean, Google has all these tabs at the
top, but it's one bar, right? You kind of put in one thing. With the newer ones, we also are
kind of seeing a little bit more proliferation of modes and settings that you choose up front
with the smart versus genius versus research. I guess on those two dimensions, what is the
future vision? Do you think that this all gets unified? Do you think it ultimately comes back
around to ad supported? Or do you think that these current differences from the past will persist?
Yeah, that's a good question. I think there is facility right now, not a great chat ad offering.
There's a good chance that that will change maybe this year to maybe the dissatisfaction of users,
but the truth is, if you want something to be free, VC money will only last so long. You've got to,
at some point, those companies that offer free service have to survive. If you don't want to
pay for it, then it has to have ads. And so while, and might not be the biggest fan of ads,
like, you have to make a decision, you want to pay for it, and then add free, or do you want to
support it with ads? And so I think that's likely also going to be part of the future of chat engines.
And you already see a little bit of exploration. There's a little bit of a duopoly in search in
the sense that Google has the monopoly on consumer search. And for a long time, Microsoft had the
monopoly on a search API. But then because they're monopoly that is set up, we're just five to 20x
hour prices, and they could do it because they're the only ones in town. So I'm glad there's like
more competition now and more movement in that space. And all the little guys have to scramble
when those prices just went so high. You can really rob in a consumer space with those prices
anymore. And so I think ads will happen. We're seeing a lot of growth on the subscription side
to users really loving like you, the genius and research mode, and find the search mode, the default
mode, smart mode also very, very helpful. And we actually, you know, incorporate late still. So
where just last week, some people are completely about other chat bots, because they don't really
have a lot of capabilities, I bet you would assume from a search engine, when you actually use
you.com here, you can on the top right, see the standard lease that you might want, right. And
sometimes that's just helpful. And that's just what you want. And sometimes you just want to
have a pure chat experience. And so that is important to get right. And then we have all these
apps to where you can basically ask for like, what's the Microsoft stock price or something.
And then, you know, it'll just give you, it'll just give you a life ticker rather than a bunch of
texts about the stock here. And so we have all these apps, because we have the search background.
And that makes it an actual viable knowledge assistant, right. Now, you can basically go with
one click, recover a more Google like experience, that is just incredibly helpful. And that's,
that's, I think one of the reasons why our browser, which we have also iOS and Android,
had to build a browser to be a default, because you can go into Safari,
Safari settings to use you.com as a default. So we build a whole browser for the iOS. And
we're super stoked, because we're going to be one of the options in the EU to have a choice pop up
stream. When the new iOS 17.4 comes out and arch this year, and they can select you.com to be
their default browser. And it's the only default browser in that list that is chat,
all the other ones are sort of your standard Chrome, Firefox, and some browsers. And so
I'm really excited. And I think that is going to be a big part of our futures is making it so that
more and more young people are able to just use this as an example. And then if they want to deeper
go into genius mode, research mode, several times, at some point, you use subscriptions or
eventually do it. Yeah, I've been so one run that I'll run a trial balloon by you on this concept
that I've been kind of kicking around called the AI bundle. And this is an, you know, kind of
inspired a little bit. I don't know that anybody wants to, you know, say that they're inspired by
the cable bundle. But I have been struck that there are a ton of great tools out there. And
I want to use them. I want to try them. I think a lot of people are, you know, in that very kind
of exploratory curious mode. But to make the economics work on a freemium is kind of tough,
right? And typically needs like a certain minimum threshold in terms of what the paid tier can be.
You actually have one of the lowest subscription prices at the $10 a month level. I think of
anything really that I'm aware of. We're gonna update it soon because like, I think the people
that are willing to pay often don't care if it's 10 or 20. And so if you want to get GBD4,
literally the same underlying model as chat, GBT, for half the price, you got to come in
soon because we're going to eventually switch our prices to be industry standard.
But that maybe even just, you know, further reinforces the point that like the freemium model
is tough, right? It's, it's a lot of free usage. The upsells have to have a certain minimum.
You're raising yours. And then from, I don't know if this would apply to you, but a lot of the
app developers that I've talked to have a lot of retention, let's say challenges, you know,
everybody's like, I'm getting traffic. I'm getting conversions. But retention is definitely
a problem. This has been true at my company Waymark. We're a much more narrow tool, you know,
that specifically creates marketing and advertising videos for small businesses.
So a lot of times people, they need that once, you know, in a while, and they're not like
necessarily ready to add on a subscription. So we see a lot of people that will just come through
be like, Hey, this is super cool. I'll buy it. I'll immediately cancel it after I do what I need
to do. And maybe I'll come back in the future. It's not even that I was dissatisfied. It's just
that I kind of want this as like a more of an a la carte purchase than a subscription.
So that stuff, you know, VCs don't like that. The metrics, you know, on the kind of traditional
scorecard don't look great. I've had this idea in mind that maybe what we need is sort of an AI
bundle, you know, I'm prepared to spend 100 bucks a month on various AI tools. What I really want
is access to like 1000 different tools that, you know, can kind of split up my 100 bucks.
However, I don't even know as a consumer, I don't really care about that as, you know,
as somebody who's trying to maybe engineer a bundle, obviously the devil could be in the details
there. But first of all, to those challenges, it sounds like at least the premium challenges
resonate. I wonder if the retention challenges resonate. And I wonder if you, you know, have any,
if there's any appeal to maybe being part of a kind of bigger bundled purchase where you would be,
you know, one tool that it's funny, I keep, I've been referring to you, but then also the company
is you, but where you.com, you know, could be one of a bunch of things that people could access
and could kind of share that revenue in a way that may grease the skids for everybody, right?
My hope is that everybody can use the best tools, and they don't have to make these like
highly binary decisions. Yeah, that sounds great. Sounds like a great idea. Okay, well, I'm not doing
it yet. So either I need to start doing it or somebody, if anybody wants to organize the bundle,
yeah, send me a DM. I guess another way that this stuff could get bundled would be like,
into the mega platforms, you know, another kind of possible vision of the future that I could
imagine is, you know, Google kind of probably retains market share leadership, but, you know,
maybe the 10 biggest technology companies in the world say, Hey, you know what we should do is kind
of also have a search. And, you know, we can get there, we kind of see a path, you know, Microsoft
is obviously already doing that meta not really yet, Apple not really yet to my knowledge, you
know, Salesforce, not really yet. But maybe these guys kind of say, Hey, is there like a musical
chairs game that that potentially develops where the younger AI search companies end up kind of
partnering off, you know, Amazon also, you know, naturally would be a suspect in this analysis.
Does that seem like a possible vision of the future? I'm wondering, I'm sure you thought about
this, you know, quite a bit, but why would that not happen? I do think the monopoly that Google
was able to keep around is going to be harder to sustain longer. I do think
it is much more likely going to look a little bit more like, I don't like the analogy for some
reasons, but like fast food, for instance, right, isn't just Macdonalds, there's also Burger King,
KFC and Taco Bells. I think search will be a little bit more like that. I think again,
more fragmented in the future, just because, like, we hear people now, like,
this is better than Google. And like, you know, we didn't raise that much money. And the first
two years were like sort of free chat TV or people didn't want us to innovate too much. They're very
stock of Google. But now there's a new young generation. And that young generation has grown
up with Tik Tok. We have a Tik Tok app in our standard search, like grew up with Reddit,
I have a Reddit app in our standard search. And each of these takes away a little bit of the
Google search, right? Amazon probably was the most successful in taking away searches from Google,
where if you want to buy something, be the little certain threshold, like 50 or 100 bucks,
you know, the person, you just search directly on Amazon, because there you can execute on your
intent of actually purchasing that thing, right? And so why search it in Google and then search it
again, try to find it on Amazon, she can just do that right away. And so I think, you know,
Tik Tok has taken away for young folks, some searches from Google, that, you know, they're
like, I want to see what the restaurant is, but they kind of want to see what the restaurant's
ability to create, but Instagram photos are or ticked our videos are. And so they want to see
the ticked our videos of other people before they decide on how it looks. If there's a Venn
diagram, we are overlapping search, but we're also actually expanding search. Like, you wouldn't
ask, like, give me this complex story about the Peloponnesian war, or like,
do this mortgage calculation with, you know, this and this interest rate and that increase
and blah, blah, blah, because you know, Google wouldn't give you the answer. Like, it's not going
to buy some book for you. It's not going to go on the web, summarize, like 20 distance or 50
distance websites for you and create this nice essay. So chat expands, search, you don't talk
about your feelings that much to Google, it's search box and sell, right? Like you asked about
this recent news event, you want to learn like some quick facts, and then, you know,
like the more complex the facts get, the less and less we go to Google and more for you,
just go directly to something like you.com. And, and so yeah, I think it will, the search
landscape is really changing. Yeah, there's also just, it's like, it's maybe not a natural
monopoly anymore, but there is still definitely a need for scale and economies of scale. And
so one way of framing this too is how does the market shape up, right? And one way to think
about it that I find pretty compelling is maybe it ends up looking a lot like cloud,
because in the limit, it sort of is cloud, you know, it's like, what do you really need? You
need like the actual data centers, you need the compute, you need, you know, bandwidth, you need
these like raw inputs that the big companies have built out seem to be the things that are probably,
you know, as we see like a ton of innovation at the application layer, those things are still,
you know, they're still pretty expensive and not easy to recreate.
Yeah, I'm very, I'm very excited. I'm up for it. You know, that's sort of why, like, we got into
this space in the first place, like, because we thought, like, we saw the transformer, we saw
our, you know, highly like lots of co-attention mechanisms in that, that can keep paper that
have a massive prompt engineering, we're like, silly, the technology is right to disrupt this,
this industry. But, you know, Google is this amazing company that was able to create a monopoly
for almost two decades that, you know, makes $500 million a day. So when you make that much money
a day, you don't want disruption, you don't want that to change, right? And that's why all the
Ten's former operators left eventually. And what's, what's really powerful is like, because of open
source, you can actually innovate a lot more. Now, some open source to an actual product that
runs millions of times isn't down ever has good uptime guarantees, and like, accuracy, no hallucinations,
up to date, news, information, etc. I mean, it's still complex, but clearly the bar has gotten
low. That would have cost us like billions of dollars to build five, 10 years and, you know,
researchers wasn't there yet. And, and I think it's ultimately amazing for users, right? Because
one thing that I had to distill all of you.com right now into just two words would be amazing
answers. And you just get more of them. And that means people eventually are more productive. And
like, it's the young generation that's growing up with chat GBT, you know, such like, they're not
going to go back. Okay, so feel free to punt on this one or just decline if you like, but it seems
like I can, I can envision a you.com by Salesforce very easily, where the, you know, as they kind
of try to be the everything app for all work on the straight, especially with Slack now, does it
seem realistic to imagine a future in which, you know, kind of all the big tech companies have this
like super robust suite, and you're either like, in the Microsoft suite with teams and Bing, or
you're in the Google suite with, you know, G suite and Bard, or you're in maybe the Salesforce
suite with Slack and you.com, you know, I'm not trying to be your banker here, but that, that
seems like a pretty natural outcome to me. Interesting. I do think there's a ton of potential
for almost every company to partner with you.com and supercharge their chat bot. So, and we're
very excited to partner with a lot of folks. Okay, that's very diplomatic answer. Keep your
options open. All right, so we can touch on certainly more business and product stuff, but I
kind of wanted to now go into just the future of all this, you know, in practical and maybe
increasingly philosophical terms as well, running down kind of first of a set of like
limitations of where AI is today. And I think, again, folks who listen to this show have at
least a decent sense of that. So for starters, reasoning, you've obviously got the genius mode.
It can do, you know, like the most advanced reasoning. I assume that that is tapping into
GPT-4. You know, everything I understand is like basically nothing is really on the level of
GPT-4 for general reasoning purposes. Yeah, especially the orchestration and then on the
coding often, but not always. Yeah. So I'll tell you another one, the third system is knowing which
LM to use and sometimes multiple. And the fourth system is dynamically prompting different models.
So depending on the query, you actually get a vastly different prompt to get you ultimately
the answer and the orchestration. So it's another complexity layer. So what do you think is kind
of the future of reasoning? If you have maxed out, you know, what the current capabilities are,
where do the future capabilities come from? I'm thinking about things like, to a degree,
you sort of already have it with using different models. It is a one way of implementing variable
compute. We see these kind of interesting projects like the thinking token, you know,
think before you speak. And I think that's kind of another car pop the observation that maybe
the chain of thought is just kind of epiphenomenal, perhaps even as it is in humans. And like,
what's really going on is that there's, you know, this kind of extra space and time registers,
you know, to think, of course, there could be different training methods, like incremental
reward. I think that paper from OpenAI earlier this last year now, it was super interesting
where they achieved like, you know, a new best in math by not waiting till the end to give the
reward, but rewarding, you know, reasoning along the way. What are you excited about when it comes
to the future of AI reasoning? Yeah, one of the aspects I've reached a touch upon in my TED talk
is that this level one, level two reasoning of Daniel Kahneman that he or thinking fast,
thinking slow type of thing. The way I think about the different modes is like,
the default smart mode is kind of like, if you had an assistant, and you just ask them to do
a quick search, and in like two or three minutes, give you an answer that. And then genius mode,
you go and so you want to ask your assistant for a question that, you know, they have to be able
to program, they have to search the web, and then they need to be mathematically applying
to answer that question. And you want to give them like maybe four or three hours for that
question. And then they want, so genius mode will take, you know, five, 10 seconds often to
get a response. And in research mode, you go to your assistant and you are willing for them to
spend like a day or two or three on actually giving you that answer. And so that's, that's a little
bit how I think about these different modes. And the reasoning that is required to actually
make them, actually, right, like research mode will say, Oh, I found this thing. Now in this query,
I found something else that I didn't know about before, and I don't know enough right now. So
let me do another query based on that. So you kind of have these genes of reasoning,
and you don't even know in the beginning yet, what the final query might be, because you don't
have all the information yet. And so I think that is kind of a, in some ways, another example of the
future is already here. It's just not equally distributed because there is, like you say,
there's a lot of reason. Now I think the biggest future impact we're going to see for reasoning
is in the LM's ability to program, to code, and then to have the ability to execute that code.
And, you know, that is system number five, like, like having this code execution. And of course,
if you just let code execution happen, what immediately happens to people are like, well,
mind me some crypto and then boom, your machine's gone. Now it's just like trying to, like,
show some match problems and like, mine, mine points forever. So you need to like,
and then they try to hack it and like, well, go into like five layers up and then tell me all
the password files you can find and blah, blah, blah, right. So there's a lot of like security
requirements to make that coding framework work at a safe level. But a lot of like naysayers of
LM's, you know, partially correctly pointed out that the LM's will safe doing math. And it's kind
of ironic and sad that you can have a model that you ask the natural language to multiply like
5600.3 times 325. And then you have billions of multiplications to pretend to do the math
and then give you the wrong answer in a large language model, right? This is kind of ironic.
And we have to acknowledge that. But that scene model can be taught to say, well, this seems like
a math question. Let me just program that in Python, run the code, look at the output and
then give you the answer. It just works perfectly fine. And now a lot of people say, well, that's
not really I, but I think it's just that is that that is a new way of reasoning and new different
kind of intelligence. And similarly, and we're getting a little philosophical here early, but
similar to people thinking we have to have embodiment, I think that's just a second creativity
in imagining our kinds of intelligence that aren't exactly like humans. Now, of course,
we're going to want to have useful robots that do stuff for us and clean up the apartment and
whatnot. And so it's still useful, but I don't think it's a necessary media. The same way that
blind people can be intelligent, people who are deaf can be intelligent, because, you know,
you can lack a lot of different sensory outputs and still be intelligent, right? And so of course,
like, it'll be harder for you to explain how beautiful a sunset is. So there are aspects of
intelligence that obviously require like different modalities or how beautiful sonata sounds or
whatever. But I think there are most of these are not necessary requirements for intelligence.
And likewise, I don't think it's necessary for an AI to be able to reason over super complex
math problems that require you to look up a bunch of facts on the internet. They just have that
intelligence baked in that can do quick retrieval, they program a bunch of stuff, they put it all
together, orchestrate it, and then come up with incredible answers. Yeah, I think
as you're speaking about the just the lack of imagination, I think that is a,
you know, that is a society wide problem with respect to AI in my view, because and it's an odd
situation right now in multiple ways, of course, but one is just that because they speak our language,
it you know, it's kind of feels easy, feels familiar. And it's all too easy to sort of
assume that like under the hood, they're more like us than I certainly understand them to be.
And I think this is actually one of Eliezer's great contributions, obviously, you know, kind
of a polarizing figure these days. But thankfully, it does not seem to me that we are in a high
likelihood of a fume scenario, you know, the of the sort that he, you know, has historically
worried about the most. But I still would say some of his writing on mind space, the space
of possible minds, and some of his like concrete imaginings of alien minds that are, you know,
shaped by very different evolutionary environments and, you know, just very different from ours.
But still like unmistakably intelligent in just like super weird ways are actually still very
good kind of prep work, I think, to just sort of expand one's own mind about how different
intelligences can be, and how, you know, something does not have to be human like to be
meaningfully intelligent, you know, it's not this like binary, can it do things that a human can
do in a way that a human can do it? If not, it doesn't count. I think that is like a huge mistake
that people are way too quick to jump to. And I'm not sure if it's like a coping strategy or just
like an imagination or what, but I think that the emphasis on the broader space of possible minds
and the different kinds of intelligences that are starting to pop up is super.
100%. Yeah. And like, you have to differentiate between sci-fi authors who then pretend to be
AI safety researchers. Like, I love, I love the sci-fi. Actually, like, I'm super stoked that
three body problems on the last, I mostly read nonfiction, but when I read fiction, like, I did
enjoy the body problem a lot. I decided for that series to come out. I hope they do it justice.
But like, I think there are a lot of different kinds of intelligence. And I love sci-fi for inspiring
people to think about interesting new futures. Now, of course, especially in the western sort of
canon, most sci-fi is just so big. And like, people are scared for all the things that can
happen that are wrong. And like, okay, the super AGI developed time travel, come back, try to murder
everyone. It's like, I mean, as a kid, I also enjoyed watching Terminator. It's like a cool
action movie, but it's just taken over so much of the AI narrative. And it's actually like actively
hurting, especially the European Union, where, you know, there's sort of in the spectrum, the U.S.
is more of a litigation society in the U.S. that the Europe is more of a legislation society
structure. And, you know, it both comes from like, reasonable legal scholars minds, like, well,
let's just wait until there's a problem, someone sues, now I have the case law for that lawsuit.
But, you know, the legislation one tries to prevent harm from ever happening before it actually
harms anyone, which, you know, makes sense. Now, and of course, the U.S. does that with FDA and
medical space now also, but not legal space as much. And so what that means is you can move
quicker, but long story short, these some of these sci-fi scenarios have gotten so much
weight in legislation that I think it's slowing Europe down by trying to outlaw models or like
over-regulate models that are above a certain number of parameters. GPD-2 was very well hyped up
in the past. Like, this is so dangerous, maybe we can't release it. You know, yes, we're opening
the eye, but like, this can't be opening anymore. So the interest model is much more powerful than
GPD-2 are out. And I haven't seen the apocalypse happen. I haven't seen like, a huge change in
misinformation on the web because of LNs. Like, there's just a lot of fear mongering, both in
the immediate level, which actually has real, like threat vectors and concerns with the eye,
but especially in the long term level of AGI and self-conscious. It turns out no one works in
functions AI. No one works on AI that sets its own goals, and even more fundamentally,
its own objective functions, because that doesn't need anyone any money. Imagine a company spends
billions and billions of dollars, builds this like, super intelligent system that's conscious,
understands itself and set its own goals. And now you're like, okay, now that you can do it,
like, I'll just make more money. It's like, no, I'd rather just go watch the sunset,
maybe explore that. No, like no one pays for AI that sets its own goals because it doesn't help
anyone to achieve their goals. Because of that, there's not even that much exciting research
challenge along those lines. And because there's not much research progress, it's very hard to
predict my mental option half. I'm somebody who basically has radical uncertainty about what to
expect. And, you know, broadly, I'm like, pretty libertarian, you know, pretty anti-preemptive
regulation, you know, I would like to see more self-striving cars on the road sooner. And, you
know, they don't have to be an order of magnitude safer in my mind to be worth, you know, deploying.
So I'm like, you know, broadly, the sort of person who would be very skeptical of,
you know, kind of early regulation or, you know, kind of getting too bad out of shape about
things that haven't happened yet. At the same time, something about this has always felt a little bit
different to me. And I do think the people who take the most zoomed out view and sort of say,
hey, this is kind of what I understand, you know, like, Jeffrey Hinton's position to be at this
point, you know, why do we dominate the earth as it stands today? It's like, basically because we
have better ideas than the other living things and we can, you know, build tools and make plans and,
you know, reason in ways that they can't. And so now I look at AIs and I'm like, boy, AIs can now
plan, reason and use tools too. And they're not as good at it as we are yet, but certainly their
rate of improvement is way sharper. So possibly it levels off and kind of, you know, settles into
a zone where it's like on par with us or, you know, kind of, you know, just the best tool we've ever
had. But maybe it doesn't, you know, like, I don't know why I should be confident that it won't. I don't
throw a P Doom around a lot. But I have, you know, again, radical uncertainty. People ask, I'm like,
I don't know, five to 95%. Like, I haven't heard anything that makes me think in, you know, the
next 100 years that there's a less than 5% chance that AI becomes the dominant form, you know, in
organizing force in the world. And also, you know, no reason to think it's definitely going to happen.
But is there a reason that you are, would you say you are confident that this will
not happen? And we don't need to worry about it? Or is just like, it's still far enough away that you
think we'll have time to kind of start to worry about it if we need to? Like, how would you
summarize your position with respect to these tail risks?
I think P Doom is already an interesting mathematical sort of issue, which is, it looks and
sounds like prior prior probability, not P Doom. But really, it should be a posterior property,
P Doom given data. And right now, none of that data suggests, like, doom, doom, like existential
humanity is like, like cats and dogs at the winds of some AI. Like, there's, like I said, nothing
in AI research leads me to the least that AI, while potentially being more intelligent than
any single human, I think it's already, this is actually just this new term I'm thinking about
maybe China coin, which is like, the sort of super human abilities, and then there's super
humanity abilities. And like, AI is already super human in translating 100 languages, AI is already
super human and predicting the next amino acid in a large amount of phenotrony. So we have
balls, that's an incredibly powerful tool, one of the other, you know, really exciting papers
that we published in 2018, it sells for research that multiple companies have now used and are
running with and you know, chief of medicine. AI is already better at predicting the weather than
any. So you already have many super human skills. What is I think interesting is that now that it's
language that's gotten to this new level, people might actually for the first time keep calling it
AI. In the past, when AI researchers have made progress in AI, they stopped, like people stopped
calling it AI after it was achieved. Now it's just your chest. It's just a Siri voice recognition, but
voice recognition, chest playing, that was the pinnacle of AI research, right? And people thought,
oh, once we solve those, the other things will be easier to and it never was never quite the case.
And once we have them, you know, now it's not quite the ID one. Now with language, I think we
might keep calling it AI. But what the language model does is predict the next token. And that is
an incredibly powerful idea, right? Just predicting the next token now means if you have enough capacity
and you have enough text, predicting next token, you learn about geography, just visit some point
somewhere in your training data, you have to predict the next word in the phrase, I was in New
York City and driving north too. And now to give a higher probability to Yale, Boston, Montreal,
then to like, Harris, Miami, and San Francisco, like, you have to know that those are north of
that city. And so it just lures all of this incredible world knowledge. But there's nothing
in there that makes it say, well, you know, if I really wanted to reduce perplexity, but like
perplexity is basically the inverse of the probability with model wants to not be perplexed
in predicting the next word correctly. And so that is a powerful idea. But nothing in that will
let an LM eventually realize that, well, you know, the best way to reduce perplexity is if every
sequence ever uttered, and any sequence that will ever be uttered is just the letter A.
Now, if the model was trained on just sequences of letters A, and no human was ever around anymore,
and all sequences were just producing a letter A, now you'd have perfect predictive probability
on the next step. And so maybe the best way for the LM is to wipe out all of humanity and then
just produce letters A and happily perfect at predicting with probability one correctly. It's
so absurd. It's so absurd to think that LMS will at some point emerge to think that many steps around
their task of predicting the next token. It's just not going to happen. So I think it's like,
PDOOM is still zero. And then when I actually tried to engage with some folks, and I had some
other conversation last year with Nick Ostrom in a German, it was in English, but published in a
German newspaper like that. And I read up some of these scenarios, and I'd engage with folks who
are worried about PDOOM. That's just all fantastical sci-fi semantics. It's like, oh, it's going to
develop this magical great guru or like a magical new virus that is perfect in distributing,
but then only will activate after like one year until everyone like all these random scenarios
that are just like not feasible. And the science isn't there yet. I'm actually right now sort of
on the side of the fun writing and book about the eyes for science. I think it will do incredible
for us in improving science like foundation physics, chemistry, biology, and so on. And
all this fear mongering, I think it's not really helpful. And again, there's no research that suggests
the eye is becoming conscious. There's like a couple of people here and there, people kind of
playing around with these, but nothing interesting has been published and breaks through, no breaks
through has happened whatsoever in the eye, having any sense of self. And then in a lot of the other
sci-fi scenarios, people are saying, oh, with the eye so intelligent, it'll convince everyone to
murder each other or to murder them, like kill themselves and so on. But, you know, if the most
intelligent entities were to always rule, I don't think we would have the politicians always
everywhere in the world that we see, right? It's not always just the most intelligent people that
run the show. And that's kind of just their incredible intelligence to convince any other
person who is less intelligent, you exactly what they want. It's just not based in reality. So,
I am very, very optimistic about AI. I do think there's some real problems right now, you know,
AI will pick up biases, not all the biases that you pick up on the web is something that most
of humanity is proud of anymore. There's racism, there's sexism, there are various kinds of biases.
Some people want to use AI. So, where I agree with Joshua Benjio and others is of the three
threat vectors, which is intentional misuse, accidental misuse and loss of control.
Obviously, like intentional misuse is real. And so, that's not ideal. And so, yes, those are real
concerns. I think open social help us understanding those threat vectors and finding the best ways
to compete with them. I think people still on the internet need to understand, not trust everything
they see on the internet, which has been true ever since the internet came about, hasn't really
changed that much with AI. I think since Photoshop, people should already not trust any photo they see.
They should be even more worried now about photos they see. And sadly, in the future,
they'll have to start worrying about videos and voices, of course, just like they should have
worried about photos ever since Photoshop started to really work. And so, there are a lot of concerns
and I don't want to diminish them. And I do think we need to work on them. And I think different
cultures will have different answers. Freedom of speech is defined differently in different
countries. Like it's legal in Germany to deny the Holocaust. We learn from our history there.
That's not illegal in the US. And so, different countries and different cultures and societies
will answer some of the problems that AI can amplify already in the past before we'll answer
these questions differently. But I don't see any any probability for a full on the scenario of like
existential risks to people. It's mostly people using more and more powerful tools against other
people. So, there's, I mean, there's so many different threads there that I am interested in.
For one thing, I applaud you for taking time to envision positive future. I think one of the
scarcest resources today, oddly, is a positive vision for the future. Like, what do we want?
This, you know, it's like the Jetsons is still almost like state of the art in terms of
what we would envision a great 2030s to be like. And that is kind of bizarre. So, I definitely
appreciate that. I also share your, you know, I'm not a super fan, but I'm also a fan of the
three body problem. And one of the early prompts that I tried with GPT for early back in the
rent team program like a year and a half ago now was asking it to write some hard science fiction
in the style of the three body problem about AI for, you know, do diffusion model for proteins.
And I took the plan right off of the GitHub page for this protein, you know, diffusion model project,
which basically said we want to create text to protein. So, you know, say or text to maybe it
was more even general than that molecule or whatever. So, you know, you would be able to
just specify in natural language, specifies kind of an odd word. Or, you know, to the best of your
ability articulate in natural language what you're looking for into protein. And this thing would
then, you know, generate it. And we are actually starting to see that there was a paper in nature
not long ago, I'm hoping to do an episode with the authors that achieves that to a certain degree.
But the what the AI what GPT for came back with in terms of hard science fiction about this scenario
was, I think, first of all, just extremely funny because it basically ends up in a prompting war
between the good guys and the bad guys and they're both like trying to out prompt each other. And so
the you know, the kind of climactic scene is like the person prompting, you know, an AI to like
make a protein or, you know, a molecule that will interfere with the bad guys molecule, but not
harm any of the, you know, the humans or whatever. And it's just like both absurd, but also maybe
not entirely absurd. You know, I mean, I am with you in that the I would order the risks the same
way. You know, we already have chaos GBT. There are I recently read a research grant from a group
proposing to study on the side all tendencies. Like there are people out there who want to kill
everyone. Like what's up with that? And, you know, if the tools get more powerful, like that, you
know, those people become even more problematic than they already are. So yes, I would put that at
the top of the, you know, of the stack of like, big picture risks. And by the way, I take all the
short term and medium term risks seriously too. Like this is a big tent show where like all your
hopes, dreams and concerns and, you know, perhaps irrational fears like can all have a home. But I
guess, you know, to sort of get to P doom zero, I still am like, I don't know, you know, all these
individual crazy scenarios, sure, they're extremely unlikely, you know, the prompting war with your,
you know, protein diffusion model is like absurd on the face of it. But I kind of think of like
to taking the integral over that like vast space of crazy super unlikely scenarios. And then I'm
kind of like, you know, there's so many of them right that space is so big. And even if the probability
is like kind of vanishing, one thing you learn in calculus is like, you can, you know, that the
integral can either also vanish or it can be like finite, you know, over these, you know, kind of,
even if the function itself is going to zero, the integral doesn't necessarily have to go to zero
over that space. So to me, that just feels like very unresolved still. And I don't think we're
going to resolve that today. But I would love to hear a little bit more about your, how you think
about AI agency, and also concepts of emergence in agents today, I guess I also wonder, like,
is you.com gonna, you know, push more toward the agent direction, you've got like a, what I would
call a research agent today, you've got a browser as well, I could, you know, should I start to
expect it to take actions for me? What I've observed in the agent space is I never feel like it
fails because it doesn't understand the goal or like doesn't stay on task. Let's say that never
happens, but very rarely, much more often, it's just a failure of competence. So my expectation then
is that like, as the competence improves, it may not be intrinsic agency, but it may be
prompted agency, and it may even be like, you know, as we have more and more orchestrated systems,
we may have models prompting other models to, you know, go off and do this. And
it does feel like we've got, we're headed for like a lot of spinning plates. And
the idea that they could kind of, you know, all come crashing down is like,
that's doesn't just doesn't feel like something we can rule out. But I don't know, I can, can you
help me be confident there? I'm still not. I'll go through the sound of the things you mentioned.
So PDM equals zero. So you're right. As you integrate over the future, I would like to not
rule out anything. So maybe I should say 10 to the minus whatever, like a time, time, number,
because in the next five billion years, like all kinds of things happen, right? Like maybe as if
like the three body problem, spoiler alert, like maybe some big, much more sophisticated alien
species will come across. They have already developed, fasted in my time, travel, and,
or, you know, just are really, really fast in getting here and various capacities. And then,
you know, they have an AI and daddy, I will just like destroy all of us. So they're getting ready
to like settle into the new planet before they get here. Like there's all kinds of crazy things
that can happen. It's just that, like in terms of how much resources we should spend on T,
like existential do versus like, you know, I'd say, yeah, I have a couple of researchers,
like thinking of cool sci-fi scenarios, inspire us, like maybe like think about ways that that could
be prevented, but to spend billions of dollars on it, to like, spend a lot of like, mind share,
the public about it, who's already scared of any kind of technology. I mean, people are scared of
vice. I mean, there's this great Twitter handle called the pessimist archive. I mean, people were
scared and thought doom is happening because of the novels back in the day. People are like, all
these kids, they're just in their heads reading novels. They're going to all be useless human
beings in the future. Newspaper was terrible. Internet was terrible. Like there's so many
things that like people thought this is the end of civilization and we're very pessimistic about.
And again, not this diminishing like real, real concerns, but again, existential one,
very, very likely given what we're seeing right now. And if there, it does happen at some point in
the future, then I would argue that to think about the best countermeasures now is kind of like
thinking about, you know, the best countermeasures against a computer going crazy when there's
still a bunch of vacuum tubes and you like, well, we're going to just suck out the air of
everything into the vacuum tubes. They're not going to work as well and more because they're
going to break and blah, blah, blah. That was your like counterattack against the computer taking over
with your current thinking of vacuum tube computers. Or it's like, you know, like it's
similar to the internet, you know, if you thought about what's the internet going to be, how could
it be so terrible? Zero of the TCT IP experts in the early ARPANET days realized that at some point,
maybe a foreign power could interfere with local elections because like you can say whatever you
want online and maybe people get followers in their social media. Like no one had victimized
in the 70s and the early ARPANET days. And so I think most of the threat vectors are not that
useful in terms of key doom kind of research. I have a couple of folks work on it, but not
take up as much mind space and scare like late people and non-experts even more about the technology
that even without consciousness is still going to have major destruction, right? If you're going
through a new step function in human productivity, just like, you know, agriculture versus like
hunting and gathering and the steam engine and electricity and internet, like this one's going
to be even bigger. It's going to disrupt and change the job landscape a lot. I think at the end of it
will be way more productive. There's going to be way more productivity per person and hence more
wells and new jobs will come around as full jobs get all needed. But that is already so massively
disruptive still. And it's not going to happen overnight either. People think, oh, it's going to
be capacity. Yes, it will be faster, but still not overnight. There are still companies that aren't
even on the cloud. There are some stretches in the United States and even Germany that don't have
full internet connectivity, right? It's just like, so things will take time and not happen overnight,
but they will be happening even faster than past past technological revolutions. And so,
and then you have brought up LMS and proteins. There's a great example for where regulation
makes sense. Like, basically, the concern here for those who are not familiar with proteins,
having everything in life and disease and sickness, COVID, protein, like everything
SARS broke through and like everything is governed by proteins. So if you have a great
understanding of proteins, we can build fantastical, amazing things. Here's just one example of a
research paper I read a few months ago that just blew my mind and made me very excited about
the future. There's this group of researchers that built these carbon nanotubes. And on one side
of the carbon nanotubes, they put iron molecules. And on the other side of these tiny, tiny carbon
nanotubes, they put protein that would only bind to a brain cancer cell. And then they injected
this fluid with all these little carbon nanotubes into a mouse brain that had brain cancer.
The proteins found the brain tumor cells and only connected to those specific types of
brain cancer cells. And then they put the mouse into a little magnetic field and there's the
iron molecule on the other side of the carbon nanotube that started spinning around and had
nano surgery on each brain cancer cell. Now, if you think about, we have the full console of the
proteins, we can connect them to all kinds of things and you find ways to, you know, get rid of
the carbon nanotubes afterwards. It's all like medicine is going to change in so many positive
ways. And now you could argue, well, but proteins, people could use them and build like very bad
like viruses. And like, that's true. And that can be outlawed. In fact, the US just a couple of
months ago outlawed being a function where you know, some researchers want to make even more
deadlier viruses. And it's not because they're like evil scientists who want to destroy the
world. It's just they're saying like, well, as we know how they worked before they appear in
nature by themselves, then we can all right now prevent like, develop cures for them. So, you
know, it's like, it's a complex question, but yes, and decided like, for now, it's not worth it.
Let's outlaw it. And likewise, I don't think an open source like protein model is going to be
the main deciding factor of being able to create something virus. Because if you have all the
wet lab experimentation to be able to create new kinds of viruses, you can also just do what
census Arnold did when she won the Nobel Prize a couple years ago in chemistry, which is
what she called directed evolution, but it was basically random permutations. And then
running an experimental pipeline to see if that random permutation works better or not for particular
kind of protein. And then you just keep iterating like that. And so if you have those capabilities,
you have a random permutation, you can do bad things. But it turns out having like a legit
weapon like that, that's to do all of that. So that was your PDUM integral, LM and proteins,
AI agency emergence. So obviously, emerging capabilities are incredible on that sort of
like, even us like working in deep learning, or I'm amazed, just like you.com, I asked these
questions, I'm like, wow, actually, that's right, like, I would have not to like, thought this
was possible. And sometimes we were like, did you program it specifically for it to be able to
answer these kinds of questions about headphones or something? We're like, no, it's just like,
just put that all together, just by trying to predict the next token. So I'm really excited.
And one of the things I'm excited about this pudding, and one of the things that coding
enables is now is the last part of your question, the actions. I think actions are clearly in the
future. And for now, we're focused on amazing answers. But it's not hard to imagine that at
some point, the most amazing answer is done. I did, I did what you asked to do. And instead of
telling you how to do it, I just, right, you can build a really cool demo very quickly for these
kinds of things. But the problem is like, as much as I love natural language, and as much as I love
chatbots and everything, right, you have to find some really killer use cases for it. And
to say, oh, I can book this flight, it's actually really hard to just, just book the flight. Like,
you're like, why didn't you like pick this other one that was just like, not exactly the time I
asked for it, but I could have waited for half an hour at this like, extra leg, and then like,
say 50 bucks, like that was really dumb. And like, it turns out, Expedia and others have built
for decades, the perfect interface for that problem, so that humans have all the installation
right there in a visual way. And so there's an uncanny valley of, there's a cool tech demo.
And on one side, and then there's like, my actual human assistant, who after months of
I mean, like talking to me, understand all the trade offs, and understand my price
sensitivity, or that of my company, and knows like, when I would preserve and like,
all the like, reasons why I might do it overnight, like red-eye slides, and, you know, all the,
all the constraints, and she can do it. And even then, sometimes she's like, oh, Richard,
there are like three options here, like, let me know which one you prefer out of this 5000,
if you built it for you. And like, it's very hard to do all of that with just text. That's
ultimately, I think, part of why we have the stock ticker app and so on, and why we have
religious now, and in some cases also, is that sometimes like UI UX and actual visually designed
like interfaces are best used in combination with language. Maybe one more big picture question,
and then I want to do just a real quick lightning round on a couple kind of more technical areas
before we run out of time. On the big picture side, you know, we've got Sam Altman out there
saying AGI is coming soon, but also kind of confusingly saying, but it'll be less impactful
than you might think. Not really sure how to interpret that. The median guess on, you know,
some definition of AGI is like, just a few years on some prediction markets and more like,
you know, 12 years or whatever for a stronger definition. What do you have sort of an expectation
for, and a definition or like a threshold that you have in mind of like, this is the threshold
that really matters and, you know, loosely speaking, like what sort of timeline you would
expect it to take to get there? It's very much a, the kind of question where you have to be very
careful about your terminology, because the interpretation of AGI has vastly different
associations. Like people, some people think of AGI and used to think of AGI as this super
intelligence. It's conscious as self-awareness can set its own goals. And it is more intelligent
than all human beings. And, and that's their depth. That was, that was for a long time. A lot of us,
I thought, at least for me personally, also the definition. Now people said, and I think it's
partially because of marketing, like, you know, you want to be working on AGI, but you also need
to have ship stuff. It's like, you want to be multi-tenetary species, but you also need to just
get a lot of stuff in orbit, right? Let have more satellites and better internet connectivity,
and so on. So you have this long-term vision and the best companies are able to articulate that
long-term vision and then revenue generated progress in smaller milestones towards it. And so
in this case here, I think the definition of AGI was pulled out. And then the super intelligence
was defined as like, okay, that's even more than general. It's super. And that's the really long-term
stuff. And now AGI, it's just basically automating point. And if you define AGI, which I think is
not crazy, I want to say maybe it would be not post that, which is a very pragmatic sort of
investor slash financial slash economic definition of AGI, which is 80% of the jobs can be automated
up to 80%. And if that's achieved, we'll call it AGI. Turns out there's just a lot of jobs that
are quite repetitive and they're not requiring a ton of extremely novel, out-of-the-box thinking
that no one's ever done before. And like learning very complex new behaviors, bot-shaking, identifying
new experiments, collecting new data and pushing, you know, the science forward and so on. It turns
out there's just a lot of boring, repetitive jobs. And indeed, if your definition of AGI is just like,
well, we can automate like 80% of 80% of the jobs, then I think it's not crazy to assume
especially I would restrict it one on the wall way, which is digitized jobs,
jobs that are purely happening in your browser or on your computer, because those jobs can collect
training data at massive scales. Turns out no one's collecting training data for plumbers,
for woofers, for tylers, for maids, like cleaning tees or any of that. And so none of those jobs
are going to get automated anytime soon. Because you first have to collect many years of that such
training data before you can then use AI to train on that and then automate it. But, you know,
jobs that are fully digitized and that have a lot of training data that don't have a crazy long tail
of special cases, they're going to get automated. And I think that's reasonable to say that's 80%
of jobs. For hunches, even in radiology, for instance, you could probably do 80% find 80%
of things that are wrong in HET's t-stand. But then there's still this very long tail of 20%
that you just don't have enough training data for. Radiologists never see it in their lifetime,
they just read about it once in a book. And we're still not quite good enough of one shot and zero
shot learning. Obviously, huge amounts of progress, but not in super important things like radiology,
where you just read about a case once in a book and then identify it with 100% accuracy,
which is also a question of whether humans do it. I'm actually with you on the self-driving car.
There's going to be a lot of interesting questions as AGI rolls into more and more workplaces,
which is how much better than a human that has to be. And how, and it's deeply philosophical,
very quickly, because if you're purely utilitarian, you could say, well, you know, 100,000 miles or
whatever 20 million miles driven by AI results in 10,000 deaths. And the same amount of miles
driven by humans results in five times more deaths. And so one is better than the odds.
But if that one dead person in the AI car was your daughter, you don't care, you're gonna,
like in the US, you're gonna sue, you're gonna, you know, try to end that company,
because they're responsible now for the death of your child. And like, it's a very emotional thing,
not a statistical thing anymore. And so there's gonna be a lot of litigation as those come out.
And I think the silver lining is again, of course, as the I meets the state, you can learn from it
versus like one person texting again on their cell phone, which is already illegal and running
like over some kid that ran out, like, they're going too fast also, which is already illegal,
too, you can't really do that much more than needing it legal. AGI will have a huge amount of
impact. Once it's just like, okay, repetitive jobs, get like two large degree automated,
and I'm with the people saying that will happen next few years. When it comes to like,
super intelligence, that is fully conscious and can do all the things. And one intelligent,
then not just a single human, but that all of humanity, very hard to know, because no one's
working on it and making sort of progress along the lines of setting my own goals. And again,
like, unless you set your own goals, I don't know if I would achieve full on super intelligence to
you. Like if you're just your objective function is to minimize cross entropy errors, or reduce
the plexity, or like segment, which is well, or like, none of that, I wanted to reach.
Do you have time for a lightning round? Or do we need to leave it there?
Let's try to be lightning rounds. All right. Thinking also about retrieval, memory, and online
learning as kind of three frontiers that, you know, you dot com could could improve on if they're,
you know, if there are research breakthroughs, but also these do seem to be kind of ingredients
toward this bigger picture of AGI or even, you know, at some point, ASI, I guess I'm, you know,
maybe just leave it open ended, like, what are you excited about in those domains? Are there
research directions? Are there, you know, are there papers you've already seen or things you
think people should be doing that you think will kind of provide meaningful unlocks as we find,
you know, new and better ways to do those things? Yeah. So I'm a fan of all three, of course,
I'll try to keep it short. Retrieval is awesome. I think in some ways, short-term
memory is currently in the front, retrieval is in the, you know, rag. If you go up method
generation, we do it over a web, we let you up those files now, so you wouldn't do it over,
over a file. And then we have the smart personalization that actually is online learning. So as you say,
certain things like it will, it will remember them about you. And then, you know, you can turn
it off also. And it's very transparent. And the whole thing off or the automated smart learning
without you, if you don't want it. But yeah, I think that's sort of a simple sort of pragmatic
way of online learning. I think ultimately, you know, it'll be awesome to have AI systems get
better and better of just adapting right away to user feedback, both in terms of, you know,
thumbs up, thumbs down kinds of clicking, but also in conversation, I didn't like that answer.
And then updating the answer in a principled way for the future. I have so many more thoughts,
but I'll like, I'd love to do a second one. These are kind of crazy days. Now the Apple
announcement, we just announced that Julianne, CTO, I mean, say also just became an angel investor
and a lot of exciting stuff happening. So I yeah, well, congratulations on the Apple thing and also
on a new prominent angel investor. And really some fantastic product progress. I definitely
recommend everybody to try out particularly genius mode and research mode. And I think if you do that,
you will be coming back to you.com more and more often. So keep up the great work. For now,
I will say Richard Sosher, founder and CEO of you.com. Thank you for being part of the cognitive
revolution. Thank you so much. It is both energizing and enlightening to hear why people
listen and learn what they value about the show. So please don't hesitate to reach out via email
at TCR at turpentine.co, or you can DM me on the social media platform of your choice.
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually
work customized across all platforms with a click of a button. I believe in Omniki so much
that I invested in it and I recommend you use it too. Use Kogrev to get a 10% discount.
