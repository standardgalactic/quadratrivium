{"text": " When I have an article idea, I'll often start with just this like really messy document full of quotes and sentences and little like things that might go into it. And then I'll be like, I don't even know where to start with this. This is crazy. And then I will just be like, can you put this into an outline? And I'll just paste the entire document into chat GT and it'll often find an outline. And like the the outlines it comes up with are like really basic. But sometimes I think what is one of the things is really good at is like pointing out the obvious solution that you missed because you're too like close to the problem. Hello, and welcome to the cognitive revolution, where we interview visionary researchers, entrepreneurs and builders working on the frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas. And together, we'll build a picture of how AI technology will transform work, life and society in the coming years. I'm Nathan LaBenz, joined by my co host, Eric Torenberg. Hello, and welcome back to the cognitive revolution. Today, we're sharing an episode of the new podcast. How do you use chat GPT? How do you use chat GPT is hosted by Dan shipper, founder and CEO of every a daily newsletter that promises the best business writing on the Internet. In just his first few episodes, he's had guests on the show, including saw Hill, LaVingia, Matt Eliason, Linus Lee, and today yours truly. This conversation is both extremely practical and a real exchange of ideas. Coming into it, I had used chat GPT mostly for unfamiliar tasks where I really needed help orienting myself and getting started. And of course, I've got great value from a wide range of different use cases. But to be honest, I hadn't found chat GPT super helpful for my own writing process. So I was really interested to learn more about the methods that Dan has developed to use chat GPT as a thought partner and a writing assistant. Learning from him inspired me to do more of this for myself. Toward the end of the episode, Dan asks me what I am most excited about next. And I mentioned the new Mamba architecture and state space models more generally, which I honestly can't stop thinking and talking about. We'll have a big episode on this coming very soon. And I'm glad to report that I did use some of Dan's recommendations to help develop the strategy, the devices and the overall structure for that episode in a way that I did find legitimately very helpful. One note before we get started, there are a few points in this episode where we each shared our screens to show off content visually. And while I think you will be fine with just the audio version, if you want to see the visuals, you can check out the YouTube version of this episode. Of course, there's always lots more to learn. So if you like this sort of content, I encourage you to check out how do you use chat GPT with Dan Shipper. Welcome to the show. Thank you, Dan. Great to be here. I'm excited for this. I'm excited too. For people who don't know, you are the founder of Waymark. You are the host of the excellent podcast, Cognitive Revolution, and you were a GPT for a red teamer. So you were responsible or one of the people on a team of people who were trying to figure out how to make GPT for do bad stuff before it was released, which you had a really interesting tweet thread about. I don't know. I think a couple of weeks ago or two weeks ago, something like that. So we're very excited to have you. I think you'll have a lot of insights that I'm excited to share with everyone. I think one of the things in thinking about your work that stands out and thinking about Cognitive Revolution, in particular, the podcast that you run, is I think you have this idea that one of the values of AI is in helping us to offload Cognitive Work. So just like in the way that machines in the Industrial Revolution, we offloaded like manual physical labor, AI will augment or offload a lot of cognitive labor from humans. And I wanted you to just talk about that. Tell me more about what that means. And then tell me, is that a good thing and where is it a good thing? Well, that's a big question. I would say I talk about AI doing work and helping us in a couple of different modes for starters. We will probably spend most of our time today in what I call co-pilot mode, which is the chat GPT experience of you are as a human going through your life and going through your work and encountering situations where, especially as you get used to it, you realize, oh, AI can help me here. So you make a conscious decision in real time to switch over to interacting with AI for a second or a minute or whatever to get the help that you need. And then you proceed. But you are the agent in that situation going around and pursuing your goals. In contrast, the other mode that I think is also really interesting is delegation mode. And that is where you are truly offloading a task. And I always say the goal of delegation mode is to get the output to the point where it is consistent enough that you don't have to review every single output. And if you can get there, then you can start to really shift work to AI in a way that you no longer have to do it. And that can that can be useful in different kinds of ways, right? The co-pilot mode is about helping you be better. That's your classic symbiosis or intelligence augmentation. And then the delegation mode is more like we can save a ton of time and money on things that used to be a pain in our butts, or we can scale things that are not currently scalable. And there's a lot of that in the world, right? I think almost everybody has things where they would say, you know, if you just ask the question, is there stuff that you could be doing that would be really valuable to have done, but you just don't have time to do it? There's a lot of that that can be quite transformative. In the middle, and what's kind of missing right now still is between co-pilot mode, where you're getting this kind of real time help and deciding how to work it into whatever you're doing. And delegation mode on the other end in between is ad hoc delegation, where it's I'm going along, but I want to, ideally, I would like to delegate more and bigger sub tasks to AI on the fly. And that's where we're not quite there yet. The agents probably can't do much in the way of a significant task. So it's, you're still shoehorned into one of two scenarios where you're engaging with it in real time and getting help, or you're going through the process of doing a setup and doing a validation, setting up a workflow to where you can truly delegate. And it's that in between that I think is probably that gap gets closed over the next year as agents, quote unquote, begin to work. And then we can start to delegate bigger chunks of work on the fly. The next question was, is it good? I don't know if I have a great answer to that. I think it's largely good. It's I think it says it's good as long as it's it's good as long as humans stay in control of the overall dynamic. And I'm definitely one who considers everything to be in play for the future, both on the positive side, I don't think it's crazy to think of a post scarcity world. And on the negative side, to quote Sam Altman, I wouldn't rule out lights out for all of us. I think we are definitely playing with a force here that has the potential to be totally transformative in good and bad and probably a combination of ways. I'm thrilled by how much more productive I can be. And that's some of the stuff that we'll get into in more detail. I am thrilled by the prospect of having infinite access to expertise, and especially for people who have far less means than I do to have that kind of access to expertise. I am a pretty privileged person who can go to the doctor without really thinking twice about taking the time off from work or what that's going to cost me or whatever. Obviously, a lot of people don't have that luxury. I think there is a real way in which AI can cover a lot of those gaps, not fully yet, but already significantly and obviously more and more over time. I think that kind of stuff is going to be potentially disruptive and maybe the source of a lot of political debates and challenges. But anyway, yeah, there's there's so much upside, but I think there is very real risk. And it's very easy to hold those two perspectives at the same time, to be just thrilled by the capability, but also to be always, always keeping in mind a sort of healthy fear. I love that. I think that's such a rare perspective. And as humans, we just tend to collapse on one. Either it's horrible or it's great. And then we have these camps. And I think, like, obviously, the wise perspective is there's going to be some really amazing stuff about this. And there are dangers, like when technology changes society and change, it'll change our brains, like we will adapt to this. And in the same way that it is adapting to us, that will change things and we'll need to, like, deal with the dangers that it presents. I think that's a very wise perspective. And I ask that question, is it a good thing that cognitive work will be offloaded? Because I think that there's good and bad. But one of the things that I feel is the fear scenario is like, is quite dominant for a lot of people. And, and I think the people who are like anti fear or presenting a hopeful view are a little but they're a little bit too like rose colored glasses. And I think finding real ways and real use cases for how offloading some of this cognitive work actually helps people is just like a really important part of creating a world where AI is a force for good or force for creativity, rather than a world where it just replaces people or it creates dangers or there's all the bad scenarios. And one of the things that I've felt going back to your kind of co pilot mode versus delegation mode point, one of the things that I felt is that AI reveals to me how much drudgery there is, even in highly valuable, highly creative knowledge work. And that we sort of like lie to ourselves about the amount of drudgery because that work is so romantic compared to, I don't know, I don't know, working in a factory maybe or just any other kind of job. And it's, it's easy to look at a lawyer and be like a lawyer's job is full of drudgery or whatever. But I write, I run a business. I have a YouTube show. Now I have a podcast. There's a lot of stuff that's like just pure drudgery of that. And I find it really interesting because using chat GBT using AI tools more broadly, it has made me aware of how many repetitive or like just overall kind of brain dead things I have to do just to write something smart on the internet on every. And once it's visible, I use AI for it. And then I don't have to think about it as much anymore. And I think that's a really cool thing. Totally. For me, coding comes to mind most there when you talk about the drudgery of high value and again, pretty privileged work to be doing. But I'm not a full time coder have been for a couple short stretches in life, but more often I've been somebody who's dipped in and out of it. And it is a real pain in the butt to have to Google everything. Obviously, different people have different strengths and weaknesses. I do not remember syntax super well. Sometimes if it's been a while, I'm like, wait a second, am I, is this am I remembering JavaScript or am I remembering Python? Yeah, what exactly is going on here? Yeah. And so to be able to just have the thing type out even relatively simple stuff for me is a multiple x speed up often in terms of productivity improvements, often an improvement in just strict quality to compared to what I would have done on my own. Yeah. And makes it so much easier to get into the mode in the first place. There's this kind of, I wouldn't even call this drudgery, but it's gearing up just like somehow getting my people talk about in birdwatching getting your eyes on really focusing on what are you seeing and trying to like get that detector right. There's like a similar thing, at least for me in terms of getting into code mode. And it also just streamlines that tremendously, because next thing you know, it's writing the code and I'm reading the code reading the codes a lot easier than writing the code. So I do find, yeah, just tremendous satisfaction, you know, pleasure in just seeing this stuff like outputted for me at superhuman pace better, better than me quality, maybe not superhuman quality, but super Nathan quality. It's awesome. What you're making me think about is, because I think in large part, not all of it, but in large part what the current class, especially of text models are doing is different forms of summarizing and how like how much summarizing is involved in creative work and programming in writing in decision making a lot of it is just summarizing like in programming, you're summarizing what you find on Google. You have to decide what to summarize and you have to summarize it in the right exact way for like your specific use case. But that's a lot of times what you're doing. Same thing for writing, like a lot of the stuff in my pieces are summaries of books that I've read or conversations I've had or ideas that I found somewhere else that I'm like stringing together in a sort of unique way. And obviously, I still have to do the management overall management task of deciding which summaries to put in which order and like how they work or whatever but like a lot of it is summary. And I think that's a way that using these tools, you start to see the world a little bit differently and you're like, Oh yeah, there's a whole, there's a whole class of things I'm doing that are summaries that I don't have to do anymore. And I really think that's cool. Yeah, I should be one of the areas where I have not adopted AI as much as I probably should have is in repurposing content, making more of what I do with the podcast, because I've put out a lot of episodes, there's a lot of stuff there. And we do use AI in our workflows to, for example, create the time stamp outline right of the different discussion topics at different times throughout the show. That's the most classic. Summarization where I'm not looking for a lot of color commentary. It's literally just what was the topic at each time get it right. So we've got some stuff like that we go to pretty regularly. But I have not done as much as I probably could or should maybe this will be a New Year's resolution to bring that to all the different platforms. And it is I think it's an actually an interesting, it's partly a personal quirk. And it is also I think a limitation of the current language. And I think that's one of the other models that I never quite feel like I want them to write as me. I'm very eager to hear your thoughts on how you relate to it in the writing process. Yeah, when I put something out in my own name, I basically don't use chat GBT at all for it. I can use it I find for like voice of the show if I want to do like that time stamp outline or just create a quick summary that's in kind of a neutral voice where it's not signed Nathan and isn't supposed to be like representing my perspective. I haven't really had a great synthesis yet to help create stuff that I want to say in my own voice in my own name. So if you have tips on that, that would be something I would love to come away with a better plan of attack on because I'm not quite there. Hey, we'll continue our interview in a moment after a word from our sponsors. Real quick, what's the easiest choice you can make taking the window instead of the middle seat outsourcing business tasks that you absolutely hate? What about selling with Shopify? Shopify is the global commerce platform that helps you sell at every stage of your business. Shopify powers 10% of all e-commerce in the US and Shopify is the global force behind Allbirds, Rothes and Brooklyn and millions of other entrepreneurs of every size across 175 countries. Whether you're selling security systems or marketing memory modules, Shopify helps you sell everywhere from their all in one e-commerce platform to their in person POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the past at the companies I founded and when we launch Merch here at Turpentine, Shopify will be our go-to. Shopify helps turn browsers into buyers with the internet's best converting checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell more with less effort thanks to Shopify Magic, your AI-powered All-Star. With Shopify Magic, whip up captivating content that converts from blog posts to product descriptions. Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify Magic is free for every Shopify seller. Businesses that grow, grow with Shopify. Sign up for a $1 per month trial period at Shopify.com slash Cognitive. Go to Shopify.com slash Cognitive now to grow your business no matter what stage you're in. Shopify.com slash Cognitive. I do. I definitely do. I love it. I think it goes back again to when you talk about being a co-pilot. I think that the failure mode is usually trying to use it when it's a little bit more in delegation mode. Just go do this whole thing. That's when it doesn't really work. But as a co-pilot, it really works incredibly well for specific micro tasks in writing. First example, as I just brought up, everything is a summary. I often have to explain an idea. I was writing a piece a couple months ago where I had to explain an idea. I knew the idea. I was talking about SPF and FTX's collapse and how utilitarianism and effective altruism, whether or not that philosophy contributed to the collapse. In order to write that article, I had to summarize the main tenets of utilitarianism. I studied philosophy in college and I've read a lot of Peter Singer's work. I just generally know it, but I haven't written about that in a while. Ordinarily, I would have had to spend three hours going back through all the different stuff to formulate my three or four-sentence summary. I just asked Chatcha BT and it gave me the summary in the context that I needed it in three or four sentences. I didn't use that wholesale, but it gave me basically the thing I needed to tweak it and put it into my voice. That's a really simple example, but I think you can use it in all different parts in the writing process. At the very beginning, I'll often just record myself on a walk, just spewing ideas and random thoughts reassociating. Then I'll have it transcribe it and summarize it and pull out the main things and then it'll help me find little article ideas. When I have an article idea, I'll often start with just this really messy document full of quotes and sentences and little things that might go into it. Then I'll be like, I don't even know where to start with this. This is crazy. Then I will just be like, can you put this into an outline? I'll just paste the entire document into Chatcha BT and it'll often find an outline. The outlines it comes up with are really basic. I think one of the things that's really good at is pointing out the obvious solution that you missed because you're too close to the problem. Of course, the outline for this article is set up the problem and then talk about the solution to the problem that you came up with or whatever. That's such a common format for an article, but if you're in your head about it and you're being really precious, it can be hard to be like, for this special article, it's going to be this basic thing that you've written a thousand times before, the same basic structure. Then I think one of the other really great things is it's just incredibly good for helping you figure out what you're trying to express, put into words what you're going for, and then also going through the different options of how to express what you want to express until you find something that exactly says the thing you want. For example, trying to find exactly the right metaphor. What kind of metaphor are you trying to find? What's the idea you're trying to express? Here's 50 different options of ways to express that with a metaphor. 49 of them will be trash and one of them will be amazing or one of them will push you in the direction of the one that you come up with is. I have zillions of examples of that. I find that ChatGBT, it's all over my writing, but none of the stuff that makes it into the writing I publish is wholesale from ChatGBT. It's like doing some of those micro tasks for me all the time. Yeah, that's interesting. Some of the stuff that you mentioned there I have had some luck with. The talking to it on a walk is quite helpful in some cases. I've done a couple things where I tried to draft a letter and do, as you said, talk my way through it. Here's what I want to say. I'm writing here to this person. Here's a little context. Here's the key points I want to get across. Can you do a draft? And then iterating verbally on that draft. A lot of times I'll follow up and be like, okay, that's pretty good, but you can give it pre-detailed feedback too. The transcription in the app is so good that it is, again, point of privilege. It understands me extremely well. So I can literally just have to scroll through its first generation and say, in the first paragraph, I don't really want to say that. It's more like this in the second paragraph. More emphasis on this. Add this detail. Give it like eight things. You could wish it would do a little bit better on the revision. I've had a few moments there where at the end of that process, I have something like, all right, when I get back to the desk, it's not that far of a leap from that to the actual version that I'll use. It's probably still underutilized for me. I should go on more walks, honestly. Get more time away from the screen. Get the blood flowing a little bit and use a different modality. The micro tasks, I should do more, though. I think that's the tip that I'm taking here is, and there's a separation between, like, sometimes where I feel like it's hurting me is if I haven't... And this doesn't even now start to happen in Gmail or anywhere where there's this auto-complete that's popping up. Sometimes I'm on the verge of a thought that is really the thought that I'm trying to articulate. And then this auto-complete comes up and it's like, that's not right. But it can derail you at times where you're like, don't guess for me right now. Let me get the core ideas down first. If you don't have those core ideas, then for me, it's been a real struggle to get anything good. But I think I've probably not done enough experimentation in the writing process of, okay, I do have some core ideas. Can you help me order them, structure them, iterate on them? Interestingly, I also do use it at the other end often. Critique this. Here's an email. Here's a whatever. Here's an intro to a podcast. Critique it. That could be really useful if critiques are usually worthy of consideration, at least, I would say. Yeah, it truly is good at that. We have multiple editors who are highly skilled and I still use it to be like, what do you think of this intro? Because it's up at 2 a.m. when I'm a night before a deadline. Yeah, it's real hard to beat the availability. The responsiveness is clearly superhuman on that. I think the writing stuff is really fun. I would love to, if you're ready for it, I would love to start just diving into how you actually, how you use chatGBT. Sure. You sent me a doc with a bunch of historical chats and this is the first one. Give us the setup. What were you doing? At what point were you like, oh, I need to go into chatGBT and then take us from there? I am working as the AI advisor at a company called Athena, which was founded by a friend of mine named Jonathan. Is this the virtual assistant company, the Thumbtack? Yes. He used one of the founders of Thumbtack and this is a different company, but founded on some of the lessons that he learned in the Thumbtack experience. He legendarily built up like a really amazing operation powered by contractors in the Philippines. And that included hiring an assistant for himself and his role at Thumbtack, who became like a almost another key partner in his life over a long time. And then Athena was built to essentially try to scale that magic for startup founders, executives in general. They hire executive assistants in the Philippines. They pay premium wage. They're really focused on getting super high quality people. And the idea is to empower the most ambitious and most high impact people by equipping them with this ability to delegate to their assistant in a transformative way. Okay. Now we're working on what does AI mean for us, right? How do we bring that into the assistance work? So one of the things I've done is train the assistance on the use of AI. And that's been a fascinating experience, putting content together, examples, et cetera. Another thing that I've done is just worked on building a number of kind of prototype demos for what the technology of the future might start to look like. And this chat, which we call Athena chat, is basically our own custom in-house chat GPT is built on an open source project. So I didn't have to code every line of it. But it is amazing how quickly you can build things like this today with a bit of know-how. So it's been me and one other person who have built a number of these prototypes. In this case, what we wanted to do is say, can we create a long lived profile that represents the client that can assist the EA in all sorts of ways? So it's essentially a plugin. But with plugins, you have some limitations, whatever we're experimenting with this on our own. One of the big things we wanted to enable is adding information to the client profile, updating information that's already in there. So the hope is that this could be a hub where over time client preferences and history and even background context documents all can gradually find their way in there. And you have this holistic view where the assistant can go query anything they need. But again, also update add to it's in theory supposed to evolve over time. Right. So we have this chat GPT like interface. And one of the things that we've noticed is that we still see, despite our attempts at education, like it's not perfect. We still see that assistants sometimes need coaching on how to effectively prompt a language model. So that was my motivation coming into this little thing. I already had this react app, which is again, just the chat GPT like little app. And I wanted to add a module to it. The module I wanted to add was a prompt coach. So I wanted to take a put in another little layer or would look at what the assistant, the human assistant put into the chat app and send that through its own prompt to say, are you applying all the best practices? Are you telling the AI like what role you wanted to play? What job you wanted to do? Are you specifying a format that you want your response back in? Are you are you often these days will do it by default, but are you setting it up in such a way where it will do some sort of chain of thought, think out loud, think step by step reasoning before giving a final answer? That's actually one of the the most common things I see people do to shoot themselves in the foot with AI performance is prompt in such a way where it prevents the what is now the kind of trained in default behavior. Explain, analyze, think about it a little bit before getting to a final answer. So you just have a number of best practices. Let me stop you there real quick. What are people doing that would prevent the model from doing the kind of chain of thought best practice that that makes it reason the best. Anything that just sets it up in such a way where it's got to answer immediately with no ability to scratch its way through the problem is bad. And I see that very often it's common. It happens even in like academic publications, not infrequently. Yeah. Often that's a hangover from the earlier era of multi shot prompting. And obviously this is all changing super quick, right? But if you go back, the first instruction model that hit the public was open AI's text of inchy 002 in January of 2022. So we're almost on two years, but still not even two years since you could first just tell the AI write me a haiku and it would attempt to write you a haiku. At that point, it was not necessarily going to get the syllables right. The earlier generations were, you would have to say a haiku by author name colon and then hope that it would continue the pattern. That's the classic prompting. And with instructions, now you can tell it what you want to do. And obviously that's gotten better and better. But in the benchmarking in an academic context that was developed before this instruction change, typically you would have like question, answer, question, answer, question, answer, question. And the AI's job would be to give you the answer. And so they would be measured off it on five shot prompts or what have you. But a lot of that stuff was all that scaffolding was built before people had even figured out chain of thought. And so now if you take that exact structure and you bring it to a GPT-4, you're often much better off just giving it the single question with no structure, letting it spell out its reasoning because again, now it will do that by default and then give you an answer. Versus if you set up question, answer, question, answer, question, answer, it will respect to the implicit structure that you are establishing and it will jump straight to an answer. Often these are like multiple choice or they could be a number or what have you. It will jump to an answer, but the quality of the answer is much reduced compared to default behavior. If you just let it think it think itself through it. And I've even seen this in Bard. I think this is hopefully now fixed, but not too long ago, Bard would give you an answer before explanation by default. And again, that's just like you're going to have a problem so that sometimes people do that by mistake. They'll say give an answer and then explain your reasoning. You're just hurting yourself, right? Because it will explain its reasoning for a wrong answer once the wrong answer is established. So AAA is my in the EA education. It's AAA for AAA results analysis before answer always. I never heard that before. I like that. It's hopefully hopefully they'll remember it coming out. No, I like it. Hey, we'll continue our interview in a moment after a word from our sponsors. If you're a startup founder or executive running a growing business, you know that as you scale your systems break down and the cracks start to show. If this resonates with you, there are three numbers you need to know. 36,000, 25, and 1. 36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system, streamlined accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more with less, close their books in days, not weeks, and drive down costs. One, because your business is one of a kind, so you get a customized solution for all your KPIs in one efficient system with one source of truth. Manage risk, get reliable forecasts, and improve margins. Everything you need, all in one place. Right now, download NetSuite's popular KPI checklist, designed to give you consistently excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive. Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work, customized across all platforms with a click of a button. I believe in Omnike so much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount. And just to summarize, I think basically what you're saying is a previous generation of prompting really encouraged in your prompt to give multiple examples of the kind of question and answer kind of thing that you wanted the model to do and then set up the last example such that the next thing the model would do is give you a direct response. But what we found over time is one other really effective thing to do is rather than have the model give a direct response or direct answer to a question or a problem posed to it is letting the model quote unquote think out loud first by reasoning through the problem just like a human would do a word problem. And then at the end of its response given answer improves the quality of the answer that you're giving and improves the quality of the result that you get from the model. And what has happened is OpenAI and other model providers have made that more of the default behavior so that you'll pretty much always do that. But using previous prompting techniques, a few shot prompting or multi shot prompting where you're giving examples might lead it to just answer directly and you should look out for that and try to avoid that. Great summary. Yes. If it is jumping directly to an answer, you are for sure leaving performance on the table for all but maybe the most trivial tasks. Yeah. And just an aside, see how much creative work is just summarizing. Important because I tend to give you the long version by default. That's my default behavior. This is one of those micro tasks that I'll just be handing off to an AI avatar version of me at some point. Okay, so let's get back to this. You're working on an app and you want to add a module to it that explains some prompting techniques. And it looks like the app itself is something that you didn't build from scratch and you're trying to get the lay of the land so you know what to do. Exactly. Yeah. And the problem is, I know how to code generally, and I've even coded in JavaScript quite a bit, but React is a JavaScript framework that has a sort of hierarchy of best practices that if you know them and you can easily apply them, then you can work quickly with the framework. That's the value of all these frameworks. But if you don't know them and you're coming in cold like I was, then where do I even go to where there's all these different folders and file structure and where exactly am I supposed to look for the kind of thing that I want to do and where do I put my new module. And so that's where this chat really starts. I have a working app. I have the code for the app. But I've never worked with a React app personally hands on before. So I literally just set up the scenario. And I don't really use too much in the way of like custom instructions or like super elaborate prompts. In my co-pilot mode work, certainly in delegation mode, then you get into a lot more detailed prompts with if this, then that, cases, structured formats, etc, etc. But I often just find a pretty naive approach is effective for things like this. And so I just start off by telling it, I'm working on this React app project and I am a bit lost. Can you explain the structure of the app? I give it a little bit more information and it starts giving me a tutorial of what it is that I'm looking at. And then you've got React and you've got Redux and then you've got these kind of additional slice JS toolkits and sagas and these frameworks and in some cases take on a life of their own where there's whole conferences, right, and companies and it's like you can be very deep down this rabbit hole and whoever like built this open source project that I'm trying to modify. They're using a bunch of different things that are not even necessarily standard but are common or whatever. So just like five different things here that I have no idea about. And without this kind of tutorial, I'd be like going off to search for, okay, what is this saga JS? What does that even do? It's able to give me that entire rundown extremely quickly. And then this I thought was a really interesting moment because I get a lot of value from things like this where I feel like it's prompting me and it wasn't exactly that here. But it gives me this general structure. Yeah. And then I was like, oh, I find it as a general pattern. If you can give it something in a format that it natively showed you, that's probably going to work pretty well. So sometimes even in kind of the delegation mode, sometimes I'll be like, I don't exactly know what structure this should have. But maybe if I have it suggest the structure, then we'll get a structure that it can naturally work well with. In this case, the structure is like dictated by the world, but it's pretty well known that, okay, this is going to be your structure of a project in this react framework. Cool. But this got me thinking, I should give it my actual structure. Like I want to print this thing out for this project that I'm working on because I didn't make it. I don't know what it is. And I want to have it help me interpret that full thing. But then again, I'm like, how do I print something like this? I don't even know how to do that. So my next question for it is, can you write me the command to print out the file structure? And this is where you're like, okay, this is magic, right? Because now again, I don't know how to do this, this tree command. I don't know if it was installed for me or not. But okay, it shows me how to do it. And next thing, oh, there's another step here of installing some package that needed to be installed. Okay, it was helping with that. So I'm just encountering all these. This is the classic developer experience. Conceptually, I have a clear idea of what I want to do. But now I'm like three levels, three nested problems down here where I'm like, oh, okay, I need to understand this framework. Right. Oh, okay. I need to print out the structure to better understand the version I'm working with in this framework. Oh, now I need to install something so I can do that print. And this is where people just time goes to die, right? It's like, yeah, you talk to programmers and you're like, yeah, you didn't get anything done today on there. But what happened was I was on the way to the market to get my app together. And then I had to install this thing and then I couldn't install. But each of these things, like it's helping me get over. And now finally I'm able to say, okay, here is my app. This is the app that I actually am working with. And now we're really getting into something good because it can now break that down. And the names of the things are like pretty semantic. I noticed I haven't even given it any code here. I've just given it the file names, but the file names have a kind of an indication of what is what. And it gets a sense just from that of what the app actually is. So let's go over to, I think I just got a link to a working version of the app. It's pretty simple. It's a chat GPT like environment. We can create these client profiles. We have our chats. We have our history, a couple of different models. And there's function calling in the background that connects the chat experience to the client profile. And what I'm trying to add is a module in the lower right hand corner, which I'm actually not sure if this version has. But the point of it is to take my prompts, run them through this meta prompt as we discussed, and then show feedback warnings. Okay, you may or may not be doing this quite right. So back to the thing. I've given the file structure. It's now able to understand the file structure. And now I'm saying, okay, here's what I'm trying to do. I'm trying to create this prompt coach. I forget exactly how I had approached this. Yeah, this is a different file. We see exactly what I'm doing here. Seems like maybe you had some sample code or something you've written or. Yeah, I did. I guess I took one stab at it myself and it didn't work. I see where I'm looking at the human version. I was looking at the same file structure and I'm like, okay, I see that there's this module. There's like a sidebar here. And as you see these names, right? So you've got sidebar and search and there's going to be like chat history here somewhere chat. I'm looking at this and I'm like, okay, I see all these different elements and I see all these things. Let me just try to copy one and mess with it a little bit and hopefully get somewhere. Right. And then I'm not getting anywhere. It's not showing up where I want to show up. I'm not seeing it. And so that's where I come to say, okay, now here's what I tried. Why isn't it working? Yeah. And I explain my problem here at the end. The problem I have is that it's being shown in the wrong place. Right. So then it explains the answer. Yeah. Next thing we're motto, it's giving me instructions with code, modify this, put it over here. This is pretty cool too. Unfortunately, we can't share the old screenshots. I don't know exactly what I used, but this is right as vision was being introduced to chat GPT as well. So I was able to then say, here's my screenshot. Here's where it is showing up and here's where I want it to show up. Right. And can you help me with that as well? So from the screenshots, from the HTML structure, basically we just work through this entire thing. I continue to run into issues where only 25% of the way through this whole thing. Oh, wow. This probably took me, I don't know, two to three hours total to get these suggestions, implement them, see what's going wrong. Yada, yada, yada. It writes all the code basically. Right. Because again, I've never written a line of React code in my life. So I don't know any of this syntax, there's a million ways to get it not quite right when you have no idea what you're doing anyway. And so it's writing all the code and just bit by bit where we're finding the experience, we're finding the interface, here we're creating some CSS. We're using, we have a particular style pack that's already built into this. So again, that's just another thing I'm not at all familiar with. You know, this is the syntax for figuring out how to use that style pack. Good luck making that up on your own. And then we go basically after a couple of hours, I got to a working module where the prompt coach, you know, would intercept your call, do the meta prompt, parse the response, identify, I had it giving the suggestions and the urgency of the suggestions. So we're color coding those suggestions as they come up. If it's serious, then you get it in red. And if it's not, you can get it in yellow or just a notice. And I would have, I would guess that this would have taken me easily order of magnitude longer in a pre chat GPT here. If this was two to three hours, it's probably two to three days of work to figure out all this stuff. And I know a lot more frustration that is because I'm not a super patient person. The feeling of a million people have done something almost exactly like this. There's nothing differentiated or special about what I'm doing. I'm just like in this phase of kind of not knowing what I'm doing and just getting constantly stuck, constantly stumbling, constantly running into friction. I really don't enjoy that. I think most people don't. This is none of that or almost none of it, right? Even just that going back to the install, right? Or the command to print out the structure. Man, this is so stupid. I know exactly what I want. I know that it is doable. I know that it's been done a million times, a million places. And yet I don't know how to do it. And then liberating me from that frustration is, it turns out, it would go to your drudgery point, right? That was probably 80 to 90% of the time in a world where I was doing this on my own. And now we're down to the two to three hours where it was really about defining what I want. This could have been one hour if I really knew React, but it taught me the ropes and did the task in probably again, 80 to 90% time savings compared to the unassisted version. I love this. I think this is such a cool example. I really appreciate you bringing this. One, yeah, it's obvious that this kind of thing, which if you're not a programmer, like as a programmer looking at this, I'm like, yeah, this is so much of what you do as a programmer, especially if you're a programmer like working on startup stuff is like this kind of thing. It's like, this is doable. It's been achieved before. I just need to do it in this, in my specific context. And it's obvious that this would have taken you days or taken really anyone days to do from scratch, but with chat GBT, it makes it like the way quicker and takes away a lot of the drudgery. But I think what's really cool and really beautiful, which is like weird to say about this stuff. It's striking me right now is there's this dance happening in this chat where at the beginning, obviously you're asking it for to help you. But you are giving it what it needs and filling in the gaps that it needs in order to like help you and it is filling in the gaps for you as well. So it is explaining react to you, but you are explaining. Here is the project that I have and here are the specific details that I want done. And then there's this like dance back and forth where you're mutually filling in gaps that both of you can't on your own fill in. And I think that is like really cool to just watch that evolve where at the start you don't know react and you don't know like where to where to put your code and you don't know why it's not working. And at the start, it doesn't know who you are or what you're trying to accomplish or what the specifics of your project is. But as you build up this chat, you yourself are starting to understand things more. Like you didn't ask it, just go do this for me. You asked how does a react project work and like what is the structure. And so you learned more about react and it learned more about you. And as your mutual understanding increased, you were both able to accomplish the thing together. And I think that's really cool. Yeah, it's awesome. The next generation, it's episodic. We were still only halfway through this scroll for all the scrolling I've done. I just highlighted this. Okay, cool. This is working because at this point I'm starting to get into like refinements. Okay, now I want to dial in the styling and basically at this point, the core problems have been solved. And now again, it's just going to do the drudgery of making sure that there's padding and things are centered and so on and so forth. I try to be polite and encouraging to my AIs wherever I possibly can. But you can envision a future. And I think that future is already starting to become visible through the through the mist a little bit as more and more stuff gets published on the research side. Where this sort of episodic relationship where I started a new chat, it now knows nothing about this, right? I can continue this chat up to a limit. And obviously, superhuman expansive background knowledge, but zero contextual knowledge. And we can't retain that from one episode to the next. But I do think that is also coming soon too. And there's a couple different ways it could shape up. But I think we will, in a year, certainly not that much longer than that, I can't imagine. Start to see things where all this history is accumulated or maybe divided into different threads or whatever. But where this kind of can follow you forward into different tasks as well in a history aware way, I think will be another level of unlock. I think you're totally right. That's what custom instructions is. It's a step in that direction. Unfortunately, custom instructions is very hard to set up. But if you do set it up, it's really great. It's really nice for it to have context on you. But I do think you're right. ChatGPT will definitely have a memory that it can reference this stuff and reference the context of what you need and who you are. And that will make it, even with the same level of intelligence as the model, will make it like 10x more useful and 10x faster to get to the right answer. How much do you put into custom instructions? Because for something like this, it might be my profile, my writing sample, maybe whatever. But I probably wouldn't have, by the way, Nathan's a React novice and doesn't know how to install anything. Do you have a vision or a sort of recommendation for a custom instruction that would help me with things like this? You're asking the right person. I have a very extensive custom instruction and a lot of opinions about it. If you want, I can share them. I can share it with you right now and we can talk about it. Sure. Yeah, let's check it out. Okay. The first part of custom instructions is, what do you want ChatGPT to know about you? And I actually like really having it know a little bit about who I am because there's enough about me on the internet that it knows my name and that actually helps. Same thing with every, there's enough about every internet that it knows my name. And every once in a while, not having to explain who I am or what the company is that I run is like really useful. For example, I was thinking a couple of weeks ago about starting a course and I was working with ChatGPT to decide how to do the course and whatever. And the first problem was I want to do a course. Can you help me think about it? And with custom instructions on it knows that I'm a writer and entrepreneur and so cool. I'll help you build a course. Here's how to think about it because it knows that I'm probably going to build one. But if I turn custom instructions off, it will be like, cool, what course do you want to take? And it's those like little things that like really make a difference for me. But basically like who serious relationships in my life are I have in here, like my sister, her husband, her son, I have my girlfriend up there. Who people are at every because referencing their names is just much easier for me to be like, okay, when I'm talking about something and not have to explain who she is every single time is really helpful. I think another really interesting thing is adding into custom instructions. What are the things about you that you know that you're trying to like work on? For example, I feel like I'm I have a fear of rejecting people which causes me to be too agreeable. I'm a little bit too opportunistic and I would like to be more strategic like stuff like that is really helpful to put in custom instructions because it's these little realizations that you have every day. You're like, wow, yeah, I am a little too opportunistic. I think Chatchity is great for being the thing that can help you as you're in the moment day to day. Remember to pull back and incorporate some of these insights that you have that everyone has about themselves. And same thing for goals like having know what your goals are and bring you back to those things all the time as you're using is really helpful. Cool. Well, thanks for sharing. I think I use it a lot more for just like very unfamiliar topics. I'm very just looking at these examples right that we had queued up. Okay, there's an app in a framework that I've never touched and know nothing about working on a patent application and creating diagrams for a patent application. I don't really know how to do that at all. Is there again, I'm starting with these very basic questions. What's a good syntax that I might use to create a diagram for a patent application? I just come in so cold. But it does suggest that you are doing a lot more kind of thought partner like brainstorming about your kind of core stuff, which is interesting. I'm much more on these kind of episodic things that we're like my history and this doesn't overlap almost at all in a lot of cases. But it just goes to show how many different ways of using these tools there are too. And yeah, this could be another new year's resolution to try to bring it a little bit closer to the core of what I do. It's not to say that it's not at the core of what I do, but not in this co-pilot way. With things like Waymark, I'm working very closely with language models to make an app work well. And I feel like I have intimate knowledge of the details of how it works in that respect. It's a big project for me, but again, a different mode than the interactive dance kind of mode that you describe. Fascinating. Yeah, that makes a lot of sense. I definitely use it for some of this knowledge exploration stuff too, but yeah, it's totally a sort of thought partner for me. But I'd love to keep looking through some of the other chats you brought. Cool. Next one on working on diagrams. We're going to have a combination of a provisional patent application and the supporting diagrams for the patent application. This is something that I was doing for Waymark. And we have this ensemble method of creating advertising video for small business. Basically, folks come into the site. They get to enter a website URL. Typically, people will give like the homepage of their small business website. We have some code that goes and like grabs content off of that website. And then we build a profile on a synthetically if you're custom instructions, so to speak, within the context of our app. Who are you as a user? What's your business? What are you all about? What kind of business? What images? And then to actually create the video, you give a very specific, although it's like a super short instruction. I want to make a video for my sale this Saturday where I'm opening a new location and here's the address or whatever. This is my purpose in this moment prompt. And then we've got a pretty complicated machinery that takes all those inputs and it works with a language model to write a script. And then it has computer vision components that decide which of the images from your library should be used to complement the script and all these different points along the way. And that is, it's a pretty cool experience. Now, really compared to, again, you think about pre AI and now what we had before was an easy to use template library. And what we have now is really the AI makes you content. Like it's a phase change in terms of how easy it is to use, how quick the experience is, how much you can just rifle through ideas. If you don't like the first thing, you just ask it to do another and it's qualitatively just way more fun. People used to have to sit there and type stuff in and they were like, okay, what do I say? And I'm not sure what to say. And a lot of people are not content creators, but everybody always referred to the Mr. Burns episode from the Simpsons. This is a long time ago, but he goes to an art museum for a reveal of some piece of art and they reveal it. And he says, I'm no art critic, but I know what I hate. And that's, I feel like, exactly how our users operate. They ask for something. They wait 30 seconds. They now get to watch a video featuring their business. And if they like it, they can proceed. And if they don't like it, it's very obvious to them and they can very quickly be like, no, not that. Give me another one. And here's an alternate instruction. So anyway, this is the app that we built. And now we're like, okay, maybe we should think about filing a provisional patent on that. Like most software companies, we're never going to prosecute our patents, but we just want to make sure nobody can come in and give us a hard time. So how do I write a patent? And how do I create the diagrams? And I want to be able to update it. I want to have something that's not like just a total mess. So this was a series of different interactions that ultimately led me to these diagrams. But I provided initially basically what I just said to you, which is a rambling sort of instruction on here is my app. And here's what it does. Here's how it works. Here's some of the parts behind it. The language model writes the script and the code that's scripted from the website. And then the other part with the computer vision that figures out what I just literally tell the whole thing and say, now can you use some syntax? To make me a diagram that shows the structure of that app that I just word vomited to you. And so there's like a bunch of different structures out there. So that's the first part of that conversation as well. You could use the mermaid syntax or you could use graph viz or you could use a couple other things. But what are the pros and cons of those and can they represent certain different kinds of structures? We dialed it in on either mermaid or graph is it started to make me a thing. And then you can see here too. This is interesting because I did find in this one that at some point it got confused. I'd given it this thing and had generated this syntax, asked for refinements on the syntax because I'm taking the syntax by the way, going over to another app. What's cool about the syntax is you drop in this pure text syntax and it will render the app for you. So you've got things like this graph is diagram. What's a digraph? I don't even really know. It's called this digraph is G and it has these elements and they have these properties and they're connected in this sort of graph structure, blah, blah, blah. You load it in half a second, you know, it renders it. You're like, oh, no, that's not quite right. This point should be connected to this point. And it's it's skipping one that it's so whatever. So you give it these kind of iterations. It would make progress, but then it would also get confused. It seemed after a number of rounds because there's just maybe like too much syntax. So at some point, I did say, OK, using the episodic memory to my advantage or working around its working memory weaknesses by just wiping and starting over. I'm like, OK, here's the best one from that chat. It was closest to what I wanted it to represent. We just go have another chat. And this time we're going to skip all the part about which format do we use and skip all the word salad. And I can just be like, here's a diagram. I want to make some changes to it. And now have it do more localized edits for me. And again, a lot of little details, a lot of nuance here, but it's happy to do that. We work through a number of rounds of it. And I believe I attached the thing for you what I ended up with after or a couple chats. You even get to the point where you're like color coding and really starting to make sense of it. It's like, right. The green in this diagram now is the things that the user does. So the user tells us what their business website is. Then there's code to go scrape. Then there's this fork where we have to grab all the images and process them in various ways. One of the big challenges is which parts of this can happen in parallel and which parts depend on which parts. This is actually something that we didn't have until I did this, even for the technology team. And I'm not sure how well all the members of the technology team could have even drawn this. So now we actually have a better reference internally also to be like, hey, if we like, what depends on the image aesthetic step? Now we can go look at it and be like, oh, okay. Yeah, you can't select best images until you have the aesthetic scores. Yeah. Good. Completed. Just having that clarity is also I think just operationally useful. But this is the sort of thing that you can attach to a provisional patent application and at least begin to protect yourself from future patent rolls coming your way. Again, how long would this take? If I had drawn it freehand, I maybe could have drawn it in a somewhat comparable time to the amount of time that I spent exchange, but having the syntax and now having it in that kind of structured language way also makes this like much more maintainable, can fit in other things, even can be like more readily used in language models. The vision understanding is getting very good. But I would say it's probably still better at understanding the syntax of the graph more than this visual rendering of the graph. I think that's great. Obviously, ChatGPT has the Dolly integration. So I'm familiar with that, but I've been thinking a lot about sometimes I want to create something that looks like this, like in a graph with text and boxes and all that kind of stuff. And I didn't even think to do to have it just write graph is markup or something like that and paste it somewhere else. So I think that's a really cool thing to know it can do. And it's also pretty clear, I don't know, in a year, it'll probably just render the graph is stuff for you. And you'll be able to like move it around and do all that kind of stuff without even necessarily having to chat back and forth after the first round or something like that. I think that would be a very cool next step for ChatGPT is jump into an edit mode for something like this. Closest thing I've seen to that so far is diagram GPT. This is a slightly different notation. But basically you can prompt in natural language. Yeah, it will then generate, in this case, mermaid syntax for you in response. And then it'll immediately render your image and you can then edit the syntax. You can't quite like drag and drop within the interface itself. But I think this is a really interesting question around highlights and really interesting question around like what things should be in chat GPT versus what things should have their own distinct experience, even if there's still like a very AI assistant component to it. This is one actually that I would expect lives outside of chat GPT. And who knows, right? In the fullness of time, maybe you have like dynamic UI is getting generated on the fly. We're starting to see that a little bit already. But I don't think open AI is going to say what we need to do is create like a UI where people can edit these graph things. If possible, you could do that. GPTs don't really give you the ability to create like custom editor experiences yet anyway. So for now, if you want to have something like that, you have to bring it to a different app. But increasingly, these are out there as well, right? They just use chat GPT and just a renderer. So I had the AI doing all the syntax and then the renderer showing me what it actually is and then going back and continuing the dialogue with chat GPT. I think you're right. Like I could see a world where they let developers build their own renderers inside of chat GPT, not for like really serious stuff. I think like dabble in half one time or make a little video or whatever that like having something in the interface so that you can do it in there. Like a rough thing is really helpful. But then yeah, I think you're right. There will have to be other pro tools for people that all they do all day is make graphs that are not inside of chat GPT. So here's another one. This is recent episode in my life where I had to admit defeat after 10 years of swearing that I would not replace my car until the replacement was self driving. Wow. And we're not quite there. So I finally and I've had three kids in the meantime. So I finally had to break down and get a mini man like many parents of young kids. I'm like, what my kids do is they really depreciate stuff pretty quickly. So I was like, I think I'll get a used right mini man because if I get a new one, it's going to be pretty used pretty quick anyway. So let me just look at what is out there. Now, anybody who's ever shop for a used car knows that it's a total jungle, right? And the car dealer websites are terrible. What features they have is a huge question. And what you end up encountering very quickly is these trim levels, which if you're not like a car head, you may not even know what that is. But that is the sort of you've got your make, which is the brand of the car, your Chevrolet or your Toyota or whatever. You've got your model, which is the kind of car. And then the Dodge caravan is the make and model. And then you've got this trim, which is often just like a couple of letters or whatever. It's like the XRT or the SRT or the L limited or whatever. They just have all these like little and these are package levels, right? What features, what upsells have been included? Does it have a sunroof? Does it have a screen in the back that drops down out of the ceiling for the kids or whatever? Right. And it's just a jungle to even try to figure out what those things have, what levels there are and what those things have. So this is perplexity, which is a great compliment to chat GPT. It is more specifically focused on answering questions. So in this way, it's a more direct rival to a Google search. It's not so much meant to be like a brainstorming partner. They really aim for accurate answers to concrete questions, do a phenomenal job on it. So here I had a number of runs of this as well, different kinds of questions or whatever. But okay, these minivans that are like not super old, but old, pretty cheap. What do they have? What do they not have? And this would have taken, I don't even know if I had really tried, I wouldn't have done it. This is one of those things that you just, I wouldn't do. But if you had set out to go collate, okay, here's all the makes and the models and the trims and what they have, you're going to be in like user manuals or something. I don't even know really where that information is stored around truth. But just in asking that question, I was able to get the trim levels for all of the different brands for this window of time. And just easily get a handle now that I could reference back to, okay, this one on this dealer site, it doesn't have any pictures. It doesn't say anything, but it does say, for example, oh, it's an, it's a SXT. Okay, cool. Now I can at least know that is the second of however many trim levels or whatever. So the SE, that's your top one, your SXT, that's your, you can imagine, right? Trying to sort this out on your own. And then you get the AVP slash SE. Well, who comes up with this stuff? It's ridiculous. But it's super useful if you're like, I don't want to drive across Metro Detroit to go look at this minivan. If it doesn't have something that I really cared about. And the things that I were zeroed in on were like fairly basic safety features. I wanted the blind spot detection and the backup camera. So there were other questions too, like when did USB charging get introduced into cars in general? I didn't know the answer to that. I'm old enough to remember when you had to plug the thing into the lighter and I didn't want that. So I don't want a car that's that old where I have to use the lighter outlet anymore. I want a car that's at least into the like USB charger era, but when did the USB charger era begin for cars? That was another one that perplexity was able to answer. And it is so good. I think this is about to be a huge trend. If I had to guess, because I've been a big fan of this app for a while. I had the CEO, Arvind on the Cognitive Edition twice and they just they ship super fast. They win head to head comparisons for answer accuracy. The product itself is super fast. It's got great UI with these sources and others starting to become more multimodal with images as well, which is relatively new. We're just a great experience all the way around. And I see it as like setting a new standard for answers that are like I started. I'm starting to use the term per perplexity to say I'm not sure this is necessarily rock solid ground truth. Like perplexity is not always right, but it's the most accurate AI tool. It's usually right in my experience. You might be able to find something here that is wrong, but everything I ended up fact checking turned out to be true. And so there's this kind of very interesting, good enough for practical purposes standard where I don't necessarily need it to be 100, 100 percent accurate for it to be very useful. And I would make my decisions. Did I trust it enough, for example, to be confident that there was in fact going to be a USB charger in the car that I went to go look at? Yes. And in fact, it was correct about that. And so I have this kind of per perplexity standard of verification in my mind now where I'm like, yeah, it's in many situations. It's like good enough to act on. I wouldn't make life and death decisions without more fact checking, but I don't even need to follow these links in most cases now for something like this. I'll trust it. And it's an emerging like standard in the family as well. My wife asks, do we really have to get a car that's that old? Do they have this? Do they have that? And I was able to ask perplexity and send her like, yep, it should have a backup camera per perplexity. It should have a USB charger. It should have the blind spot detection. And it's an incredible time saver. I think I'm just a worthy alternative to even something like a wire cutter, which has been the standard that my wife has used for a long time. But obviously that's an editorial approach where you can't just ask any question you want to ask here. You can ask any question you want to ask. And I think you do get something oftentimes that is like a worthy rival even to a much more editorial product. No, that makes perfect sense. It reminds me of wire cutter reminds me of there are all those sites that are like Cora, but it's like for this new generation where no one had to think previously to ask this particular question. I can just gather and answer the question for you immediately. And I think that's so it's so powerful. Like it's really starting to click for me when and how I might, I might use it. There are so many questions I have this where I'm like, I basically want to get to the best answer for a fact based question, more or less. And I'm so lazy. I really don't want to do all the research and chat to BT will kind of like, it'll do one search and then sort of crib the first article. And this feels a lot better than that. Yeah, it's really good. It's faster than chat to BT on the browsing side. So you're getting to answer notably faster and marginally more accurate, just more the sort of answer that I want. A lot of times like I've had a couple of instances where I tried the same thing with chat to BT and I was able to get there, but it was like slower on the browse. Didn't give me the full answer the first time. I was like, no, but I need a little more. And then I was able to get over the hump and get there. But this was definitely just a faster, cleaner experience that I do believe is a bit more accurate as well. It goes to show that there are different roles that you want AI to play. And I think there is, it's interesting. There's forces pushing both ways, right? What makes the AIs so compelling is that they're extremely general purpose. And it seems like there is something, there is like a fundamental reality that they get really powerful at scale and to scale. They have to be the general purpose. And that kind of comes as a package. But here the scope has been narrowed. And there are a lot of things that chat to BT does for people that this is not trying to do for people. And in its specialization, it does seem to be achieving higher heights in the domain that it really attempts to be best. So I definitely recommend perplexity a lot. And I'm just old enough to remember when people were first saying that they were Googling it. And this has a similar vibe to me where it's a standard that I think people can comfortably socially transact on and feel like they're pretty solid. I love this. Like you're using it to build stuff or also really using it to fuel your curiosity. And I'm curious, like, you know, before we wrap up, what are you excited about now? What are you thinking about right now? Like what's on your radar that you think people should be paying attention to in chat to BT, maybe specifically, but like broadly in AI over the next couple years. Boy, broadly in AI over the next couple years, I think almost anything's possible. I take the leaders of the field pretty much at their word in terms of being honest reflections of their expectations. And you listen to what Sam Altman thinks might happen over the next couple years. You listen to what Dario Amade from Anthropic thinks might happen over the next couple years. And we are potentially looking at something that is superhuman in very substantial and meaningful ways. I think there's a lot of kind of conflation and talking past one another when people try to analyze this. And I do think it's important to say you can be superhuman in very consequential ways without being like omnipotent or infallible. And I think there's actually quite a lot of space right between like human performance and omnipotence or infallibility. And I kind of expect that AI is going to land there for a lot of different things over the next couple of years. So I think the value of the kinds of things that we will be engaging with it for is only headed up. Just a recent result from Google DeepMind on using their best language models for differential diagnosis was an extremely striking result. This team has been on an absolute terror. It was only maybe like a year ago that they first got a language model to like hit passing level on medical licensing tests, which hey, that's crazy. But you can just kind of say, well, it's a test. It's more structured. The real world is messy and they're only passing. You would want a doctor. It's just merely passing. OK, guess what? We didn't stop there. Next thing you know, it was hitting expert level performance on the test. Next thing you know, it's they've added multi modality and it can now do a pretty good job of reading your x-rays and other tissue slides. And again, is it perfect? No, it would be probably on the lower end of what the actual human radiologist could do. Right. Oh, they're even there. It was like 6040, I think. I think it was like 60% to 40% that the human radiologist was beating the AI radiologist. So it's OK. That's a pretty narrow margin. Obviously, we're not done. The current thing is taking case studies out of medical journals, case studies being like extreme hard to figure out cases, right? When a case gets reported in a medical journal, that's because this case, you know, is thought to be highly instructive, right? It was a confusing situation. It's an unfamiliar combination of symptoms or what have you. So they don't publish just the routine cold, right? In the journals. So they take these case studies out of journals and they had a study of comparing AI's effectiveness at doing the differential diagnosis versus human with access to AI. And AI was the best by like a significant margin. The human alone was last. And so they, in their kind of presentation of this, they're very modest and they take almost like a, in my view, almost like a two-grounded willfully bearing the lead. Almost at times, it seems. And what one of the main conclusions of the paper was we need better interfaces so that doctors can take better advantage of this. But it was like, to me, that's yes, that's one lesson I would take away from this paper. But the other lesson is that the AI is getting it right like twice as often as the human clinician, like 60% to 30%. That's another big lesson, too, that I take away from a lot of these things. We don't often measure human performance. We think because we've lived in a world for a long time, we're like, a human doctor is human. We know that like some are better than others, but we look at that as a standard, that there's a human doctor and their license and they're supposed to be good. But like, how often do they get the right diagnosis on this? It turned out in this particular data set, it was in the ballpark of 30%. So there's a lot of room for improvement. And you could perhaps say, what's the best doctor in the world do that? Best doctor in the world, I'm sure is a lot better. Maybe even better than the 60% that their language model was able to do. But you probably can't access that person. We are apparently headed for a world where you should be able to access that AI doctor. And if it's a 2x better performance on such a challenging task as differential diagnosis that I think we're headed for a world of radical access to expertise, which I think is going to be at unbelievably low prices, which I think is going to be a transformative force in society, right? It's going to be one of the greatest blows ever struck for equality of opportunity, equality of access in many ways. It's also going to change a lot of market dynamics and change what wages can be commanded for different kinds of services. I'm excited about that. I think it probably is going to be fairly disruptive and it probably is going to become more and more political. I think that the upside of that, I think is pretty clear and really extremely compelling. So I hope we do get to actually enjoy the fruits of that future. And then one other thing I'll say is just, I don't think we are it. The transformer is not the end of history. That's a chat GPT is not the end of history. It's kind of no memory AI. Just this last week or two, we've seen a flurry of activity in the state space model architecture. And again, it's been reported the headlines if you're on, if you're on Twitter and seeing this stuff, it's hey, there's a new thing that might even be better than the transformer. It might be a transformer successor. It might be a transformer alternative. It might be a transformer replacement. It has some nice properties that the transformers don't have better long term memory, better scaling, better speed, better throughput. Maybe we just all flip over from one to the other. And if the transformer was the old thing, this is the new thing. But I strongly suspect that what we are going to see is a mixture of these architectures where just like in the brain, we obviously don't have just one single unit of the brain that gets repeated over and over again. We have a lot of different modules, including some that do get repeated. It seems like we're almost for sure headed for AIs that are like composites of different kinds of architectures that bring their own strengths and weaknesses in information processing to the table. Such that as much as this has been a shocking amount of progress to get to GPT-4 from GPT-2 just four years ago, I have to say I think the next few years are going to bring at least as much more change. And it's going to be a wild ride. It's exciting. It's inspiring. I'm excited for the future and I really appreciate you taking the time to share your thoughts and show us how you use chat GPT. And I'd love to have you back and see where we are, see what new stuff comes up on the horizon. Yeah, thank you. I appreciate the opportunity Dan. This has been a lot of fun. And I definitely learned some things and was inspired to go chase down a few more use cases as well. So hopefully next time I'll have some better custom instructions and a little bit better track record in the brainstorming department. Let me submit a great exchange. That sounds great. Yeah, thanks a lot. It is both energizing and enlightening to hear why people listen and learn what they value about the show. So please don't hesitate to reach out via email at TCR at turpentine.co or you can DM me on the social media platform of your choice. Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work customized across all platforms with a click of a button. I believe in Omnike so much that I invested in it and I recommend you use it too. Use CogGrav to get a 10% discount.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " When I have an article idea, I'll often start with just this like really messy document full of quotes and sentences and little like things that might go into it.", "tokens": [50364, 1133, 286, 362, 364, 7222, 1558, 11, 286, 603, 2049, 722, 365, 445, 341, 411, 534, 16191, 4166, 1577, 295, 19963, 293, 16579, 293, 707, 411, 721, 300, 1062, 352, 666, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17640251772744314, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.06644728034734726}, {"id": 1, "seek": 0, "start": 9.0, "end": 15.68, "text": " And then I'll be like, I don't even know where to start with this. This is crazy. And then I will just be like, can you put this into an outline?", "tokens": [50814, 400, 550, 286, 603, 312, 411, 11, 286, 500, 380, 754, 458, 689, 281, 722, 365, 341, 13, 639, 307, 3219, 13, 400, 550, 286, 486, 445, 312, 411, 11, 393, 291, 829, 341, 666, 364, 16387, 30, 51148], "temperature": 0.0, "avg_logprob": -0.17640251772744314, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.06644728034734726}, {"id": 2, "seek": 0, "start": 15.68, "end": 23.400000000000002, "text": " And I'll just paste the entire document into chat GT and it'll often find an outline. And like the the outlines it comes up with are like really basic.", "tokens": [51148, 400, 286, 603, 445, 9163, 264, 2302, 4166, 666, 5081, 17530, 293, 309, 603, 2049, 915, 364, 16387, 13, 400, 411, 264, 264, 40125, 309, 1487, 493, 365, 366, 411, 534, 3875, 13, 51534], "temperature": 0.0, "avg_logprob": -0.17640251772744314, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.06644728034734726}, {"id": 3, "seek": 2340, "start": 23.52, "end": 34.839999999999996, "text": " But sometimes I think what is one of the things is really good at is like pointing out the obvious solution that you missed because you're too like close to the problem.", "tokens": [50370, 583, 2171, 286, 519, 437, 307, 472, 295, 264, 721, 307, 534, 665, 412, 307, 411, 12166, 484, 264, 6322, 3827, 300, 291, 6721, 570, 291, 434, 886, 411, 1998, 281, 264, 1154, 13, 50936], "temperature": 0.0, "avg_logprob": -0.14788809689608487, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.1293400079011917}, {"id": 4, "seek": 2340, "start": 34.96, "end": 43.56, "text": " Hello, and welcome to the cognitive revolution, where we interview visionary researchers, entrepreneurs and builders working on the frontier of artificial intelligence.", "tokens": [50942, 2425, 11, 293, 2928, 281, 264, 15605, 8894, 11, 689, 321, 4049, 49442, 10309, 11, 12639, 293, 36281, 1364, 322, 264, 35853, 295, 11677, 7599, 13, 51372], "temperature": 0.0, "avg_logprob": -0.14788809689608487, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.1293400079011917}, {"id": 5, "seek": 4356, "start": 44.160000000000004, "end": 53.64, "text": " Each week, we'll explore their revolutionary ideas. And together, we'll build a picture of how AI technology will transform work, life and society in the coming years.", "tokens": [50394, 6947, 1243, 11, 321, 603, 6839, 641, 22687, 3487, 13, 400, 1214, 11, 321, 603, 1322, 257, 3036, 295, 577, 7318, 2899, 486, 4088, 589, 11, 993, 293, 4086, 294, 264, 1348, 924, 13, 50868], "temperature": 0.0, "avg_logprob": -0.20515463087293836, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.02368147484958172}, {"id": 6, "seek": 4356, "start": 54.120000000000005, "end": 57.32, "text": " I'm Nathan LaBenz, joined by my co host, Eric Torenberg.", "tokens": [50892, 286, 478, 20634, 2369, 33, 11368, 11, 6869, 538, 452, 598, 3975, 11, 9336, 314, 10948, 6873, 13, 51052], "temperature": 0.0, "avg_logprob": -0.20515463087293836, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.02368147484958172}, {"id": 7, "seek": 4356, "start": 57.92, "end": 65.44, "text": " Hello, and welcome back to the cognitive revolution. Today, we're sharing an episode of the new podcast. How do you use chat GPT?", "tokens": [51082, 2425, 11, 293, 2928, 646, 281, 264, 15605, 8894, 13, 2692, 11, 321, 434, 5414, 364, 3500, 295, 264, 777, 7367, 13, 1012, 360, 291, 764, 5081, 26039, 51, 30, 51458], "temperature": 0.0, "avg_logprob": -0.20515463087293836, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.02368147484958172}, {"id": 8, "seek": 6544, "start": 66.12, "end": 74.28, "text": " How do you use chat GPT is hosted by Dan shipper, founder and CEO of every a daily newsletter that promises the best business writing on the Internet.", "tokens": [50398, 1012, 360, 291, 764, 5081, 26039, 51, 307, 19204, 538, 3394, 402, 15124, 11, 14917, 293, 9282, 295, 633, 257, 5212, 26469, 300, 16403, 264, 1151, 1606, 3579, 322, 264, 7703, 13, 50806], "temperature": 0.0, "avg_logprob": -0.2148646460639106, "compression_ratio": 1.465863453815261, "no_speech_prob": 0.17317694425582886}, {"id": 9, "seek": 6544, "start": 75.08, "end": 83.36, "text": " In just his first few episodes, he's had guests on the show, including saw Hill, LaVingia, Matt Eliason, Linus Lee, and today yours truly.", "tokens": [50846, 682, 445, 702, 700, 1326, 9313, 11, 415, 311, 632, 9804, 322, 264, 855, 11, 3009, 1866, 9109, 11, 2369, 53, 278, 654, 11, 7397, 16943, 1258, 11, 9355, 301, 6957, 11, 293, 965, 6342, 4908, 13, 51260], "temperature": 0.0, "avg_logprob": -0.2148646460639106, "compression_ratio": 1.465863453815261, "no_speech_prob": 0.17317694425582886}, {"id": 10, "seek": 6544, "start": 84.64, "end": 89.28, "text": " This conversation is both extremely practical and a real exchange of ideas.", "tokens": [51324, 639, 3761, 307, 1293, 4664, 8496, 293, 257, 957, 7742, 295, 3487, 13, 51556], "temperature": 0.0, "avg_logprob": -0.2148646460639106, "compression_ratio": 1.465863453815261, "no_speech_prob": 0.17317694425582886}, {"id": 11, "seek": 8928, "start": 89.96000000000001, "end": 96.8, "text": " Coming into it, I had used chat GPT mostly for unfamiliar tasks where I really needed help orienting myself and getting started.", "tokens": [50398, 12473, 666, 309, 11, 286, 632, 1143, 5081, 26039, 51, 5240, 337, 29415, 9608, 689, 286, 534, 2978, 854, 8579, 278, 2059, 293, 1242, 1409, 13, 50740], "temperature": 0.0, "avg_logprob": -0.08526762249400315, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.019110754132270813}, {"id": 12, "seek": 8928, "start": 97.32000000000001, "end": 101.32000000000001, "text": " And of course, I've got great value from a wide range of different use cases.", "tokens": [50766, 400, 295, 1164, 11, 286, 600, 658, 869, 2158, 490, 257, 4874, 3613, 295, 819, 764, 3331, 13, 50966], "temperature": 0.0, "avg_logprob": -0.08526762249400315, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.019110754132270813}, {"id": 13, "seek": 8928, "start": 102.24000000000001, "end": 107.2, "text": " But to be honest, I hadn't found chat GPT super helpful for my own writing process.", "tokens": [51012, 583, 281, 312, 3245, 11, 286, 8782, 380, 1352, 5081, 26039, 51, 1687, 4961, 337, 452, 1065, 3579, 1399, 13, 51260], "temperature": 0.0, "avg_logprob": -0.08526762249400315, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.019110754132270813}, {"id": 14, "seek": 8928, "start": 107.64, "end": 113.92, "text": " So I was really interested to learn more about the methods that Dan has developed to use chat GPT as a thought partner and a writing assistant.", "tokens": [51282, 407, 286, 390, 534, 3102, 281, 1466, 544, 466, 264, 7150, 300, 3394, 575, 4743, 281, 764, 5081, 26039, 51, 382, 257, 1194, 4975, 293, 257, 3579, 10994, 13, 51596], "temperature": 0.0, "avg_logprob": -0.08526762249400315, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.019110754132270813}, {"id": 15, "seek": 11392, "start": 114.88, "end": 117.84, "text": " Learning from him inspired me to do more of this for myself.", "tokens": [50412, 15205, 490, 796, 7547, 385, 281, 360, 544, 295, 341, 337, 2059, 13, 50560], "temperature": 0.0, "avg_logprob": -0.08110126901845463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.010647803544998169}, {"id": 16, "seek": 11392, "start": 118.24000000000001, "end": 122.0, "text": " Toward the end of the episode, Dan asks me what I am most excited about next.", "tokens": [50580, 33814, 515, 264, 917, 295, 264, 3500, 11, 3394, 8962, 385, 437, 286, 669, 881, 2919, 466, 958, 13, 50768], "temperature": 0.0, "avg_logprob": -0.08110126901845463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.010647803544998169}, {"id": 17, "seek": 11392, "start": 122.16, "end": 129.12, "text": " And I mentioned the new Mamba architecture and state space models more generally, which I honestly can't stop thinking and talking about.", "tokens": [50776, 400, 286, 2835, 264, 777, 376, 23337, 9482, 293, 1785, 1901, 5245, 544, 5101, 11, 597, 286, 6095, 393, 380, 1590, 1953, 293, 1417, 466, 13, 51124], "temperature": 0.0, "avg_logprob": -0.08110126901845463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.010647803544998169}, {"id": 18, "seek": 11392, "start": 129.8, "end": 132.28, "text": " We'll have a big episode on this coming very soon.", "tokens": [51158, 492, 603, 362, 257, 955, 3500, 322, 341, 1348, 588, 2321, 13, 51282], "temperature": 0.0, "avg_logprob": -0.08110126901845463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.010647803544998169}, {"id": 19, "seek": 11392, "start": 132.64, "end": 143.84, "text": " And I'm glad to report that I did use some of Dan's recommendations to help develop the strategy, the devices and the overall structure for that episode in a way that I did find legitimately very helpful.", "tokens": [51300, 400, 286, 478, 5404, 281, 2275, 300, 286, 630, 764, 512, 295, 3394, 311, 10434, 281, 854, 1499, 264, 5206, 11, 264, 5759, 293, 264, 4787, 3877, 337, 300, 3500, 294, 257, 636, 300, 286, 630, 915, 44431, 588, 4961, 13, 51860], "temperature": 0.0, "avg_logprob": -0.08110126901845463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.010647803544998169}, {"id": 20, "seek": 14392, "start": 144.92, "end": 151.39999999999998, "text": " One note before we get started, there are a few points in this episode where we each shared our screens to show off content visually.", "tokens": [50414, 1485, 3637, 949, 321, 483, 1409, 11, 456, 366, 257, 1326, 2793, 294, 341, 3500, 689, 321, 1184, 5507, 527, 11171, 281, 855, 766, 2701, 19622, 13, 50738], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 21, "seek": 14392, "start": 151.72, "end": 158.51999999999998, "text": " And while I think you will be fine with just the audio version, if you want to see the visuals, you can check out the YouTube version of this episode.", "tokens": [50754, 400, 1339, 286, 519, 291, 486, 312, 2489, 365, 445, 264, 6278, 3037, 11, 498, 291, 528, 281, 536, 264, 26035, 11, 291, 393, 1520, 484, 264, 3088, 3037, 295, 341, 3500, 13, 51094], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 22, "seek": 14392, "start": 159.6, "end": 161.56, "text": " Of course, there's always lots more to learn.", "tokens": [51148, 2720, 1164, 11, 456, 311, 1009, 3195, 544, 281, 1466, 13, 51246], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 23, "seek": 14392, "start": 161.56, "end": 167.51999999999998, "text": " So if you like this sort of content, I encourage you to check out how do you use chat GPT with Dan Shipper.", "tokens": [51246, 407, 498, 291, 411, 341, 1333, 295, 2701, 11, 286, 5373, 291, 281, 1520, 484, 577, 360, 291, 764, 5081, 26039, 51, 365, 3394, 1160, 15124, 13, 51544], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 24, "seek": 14392, "start": 167.92, "end": 168.67999999999998, "text": " Welcome to the show.", "tokens": [51564, 4027, 281, 264, 855, 13, 51602], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 25, "seek": 14392, "start": 169.51999999999998, "end": 170.0, "text": " Thank you, Dan.", "tokens": [51644, 1044, 291, 11, 3394, 13, 51668], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 26, "seek": 14392, "start": 170.0, "end": 170.6, "text": " Great to be here.", "tokens": [51668, 3769, 281, 312, 510, 13, 51698], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 27, "seek": 14392, "start": 170.6, "end": 171.35999999999999, "text": " I'm excited for this.", "tokens": [51698, 286, 478, 2919, 337, 341, 13, 51736], "temperature": 0.0, "avg_logprob": -0.10827486358419822, "compression_ratio": 1.6775244299674268, "no_speech_prob": 0.0013665788574144244}, {"id": 28, "seek": 17136, "start": 171.84, "end": 172.64000000000001, "text": " I'm excited too.", "tokens": [50388, 286, 478, 2919, 886, 13, 50428], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 29, "seek": 17136, "start": 172.92000000000002, "end": 175.36, "text": " For people who don't know, you are the founder of Waymark.", "tokens": [50442, 1171, 561, 567, 500, 380, 458, 11, 291, 366, 264, 14917, 295, 9558, 5638, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 30, "seek": 17136, "start": 175.4, "end": 181.44000000000003, "text": " You are the host of the excellent podcast, Cognitive Revolution, and you were a GPT for a red teamer.", "tokens": [50566, 509, 366, 264, 3975, 295, 264, 7103, 7367, 11, 383, 2912, 2187, 16617, 11, 293, 291, 645, 257, 26039, 51, 337, 257, 2182, 1469, 260, 13, 50868], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 31, "seek": 17136, "start": 181.44000000000003, "end": 190.72000000000003, "text": " So you were responsible or one of the people on a team of people who were trying to figure out how to make GPT for do bad stuff before it was released, which you had a really interesting tweet thread about.", "tokens": [50868, 407, 291, 645, 6250, 420, 472, 295, 264, 561, 322, 257, 1469, 295, 561, 567, 645, 1382, 281, 2573, 484, 577, 281, 652, 26039, 51, 337, 360, 1578, 1507, 949, 309, 390, 4736, 11, 597, 291, 632, 257, 534, 1880, 15258, 7207, 466, 13, 51332], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 32, "seek": 17136, "start": 191.04000000000002, "end": 191.36, "text": " I don't know.", "tokens": [51348, 286, 500, 380, 458, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 33, "seek": 17136, "start": 191.36, "end": 193.72000000000003, "text": " I think a couple of weeks ago or two weeks ago, something like that.", "tokens": [51364, 286, 519, 257, 1916, 295, 3259, 2057, 420, 732, 3259, 2057, 11, 746, 411, 300, 13, 51482], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 34, "seek": 17136, "start": 194.20000000000002, "end": 195.64000000000001, "text": " So we're very excited to have you.", "tokens": [51506, 407, 321, 434, 588, 2919, 281, 362, 291, 13, 51578], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 35, "seek": 17136, "start": 195.64000000000001, "end": 199.04000000000002, "text": " I think you'll have a lot of insights that I'm excited to share with everyone.", "tokens": [51578, 286, 519, 291, 603, 362, 257, 688, 295, 14310, 300, 286, 478, 2919, 281, 2073, 365, 1518, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12989958738669372, "compression_ratio": 1.771341463414634, "no_speech_prob": 0.00955172162503004}, {"id": 36, "seek": 19904, "start": 199.67999999999998, "end": 215.64, "text": " I think one of the things in thinking about your work that stands out and thinking about Cognitive Revolution, in particular, the podcast that you run, is I think you have this idea that one of the values of AI is in helping us to offload Cognitive Work.", "tokens": [50396, 286, 519, 472, 295, 264, 721, 294, 1953, 466, 428, 589, 300, 7382, 484, 293, 1953, 466, 383, 2912, 2187, 16617, 11, 294, 1729, 11, 264, 7367, 300, 291, 1190, 11, 307, 286, 519, 291, 362, 341, 1558, 300, 472, 295, 264, 4190, 295, 7318, 307, 294, 4315, 505, 281, 766, 2907, 383, 2912, 2187, 6603, 13, 51194], "temperature": 0.0, "avg_logprob": -0.1332512865162859, "compression_ratio": 1.790794979079498, "no_speech_prob": 0.009700508788228035}, {"id": 37, "seek": 19904, "start": 215.68, "end": 227.64, "text": " So just like in the way that machines in the Industrial Revolution, we offloaded like manual physical labor, AI will augment or offload a lot of cognitive labor from humans.", "tokens": [51196, 407, 445, 411, 294, 264, 636, 300, 8379, 294, 264, 32059, 16617, 11, 321, 766, 2907, 292, 411, 9688, 4001, 5938, 11, 7318, 486, 29919, 420, 766, 2907, 257, 688, 295, 15605, 5938, 490, 6255, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1332512865162859, "compression_ratio": 1.790794979079498, "no_speech_prob": 0.009700508788228035}, {"id": 38, "seek": 22764, "start": 227.64, "end": 229.76, "text": " And I wanted you to just talk about that.", "tokens": [50364, 400, 286, 1415, 291, 281, 445, 751, 466, 300, 13, 50470], "temperature": 0.0, "avg_logprob": -0.15278017366087282, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.015895100310444832}, {"id": 39, "seek": 22764, "start": 230.07999999999998, "end": 231.6, "text": " Tell me more about what that means.", "tokens": [50486, 5115, 385, 544, 466, 437, 300, 1355, 13, 50562], "temperature": 0.0, "avg_logprob": -0.15278017366087282, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.015895100310444832}, {"id": 40, "seek": 22764, "start": 231.6, "end": 233.95999999999998, "text": " And then tell me, is that a good thing and where is it a good thing?", "tokens": [50562, 400, 550, 980, 385, 11, 307, 300, 257, 665, 551, 293, 689, 307, 309, 257, 665, 551, 30, 50680], "temperature": 0.0, "avg_logprob": -0.15278017366087282, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.015895100310444832}, {"id": 41, "seek": 22764, "start": 234.64, "end": 235.6, "text": " Well, that's a big question.", "tokens": [50714, 1042, 11, 300, 311, 257, 955, 1168, 13, 50762], "temperature": 0.0, "avg_logprob": -0.15278017366087282, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.015895100310444832}, {"id": 42, "seek": 22764, "start": 235.64, "end": 243.88, "text": " I would say I talk about AI doing work and helping us in a couple of different modes for starters.", "tokens": [50764, 286, 576, 584, 286, 751, 466, 7318, 884, 589, 293, 4315, 505, 294, 257, 1916, 295, 819, 14068, 337, 35131, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15278017366087282, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.015895100310444832}, {"id": 43, "seek": 24388, "start": 244.04, "end": 262.64, "text": " We will probably spend most of our time today in what I call co-pilot mode, which is the chat GPT experience of you are as a human going through your life and going through your work and encountering situations where, especially as you get used to it, you realize, oh, AI can help me here.", "tokens": [50372, 492, 486, 1391, 3496, 881, 295, 527, 565, 965, 294, 437, 286, 818, 598, 12, 79, 31516, 4391, 11, 597, 307, 264, 5081, 26039, 51, 1752, 295, 291, 366, 382, 257, 1952, 516, 807, 428, 993, 293, 516, 807, 428, 589, 293, 8593, 278, 6851, 689, 11, 2318, 382, 291, 483, 1143, 281, 309, 11, 291, 4325, 11, 1954, 11, 7318, 393, 854, 385, 510, 13, 51302], "temperature": 0.0, "avg_logprob": -0.14119625091552734, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.7179492712020874}, {"id": 44, "seek": 24388, "start": 263.0, "end": 271.15999999999997, "text": " So you make a conscious decision in real time to switch over to interacting with AI for a second or a minute or whatever to get the help that you need.", "tokens": [51320, 407, 291, 652, 257, 6648, 3537, 294, 957, 565, 281, 3679, 670, 281, 18017, 365, 7318, 337, 257, 1150, 420, 257, 3456, 420, 2035, 281, 483, 264, 854, 300, 291, 643, 13, 51728], "temperature": 0.0, "avg_logprob": -0.14119625091552734, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.7179492712020874}, {"id": 45, "seek": 24388, "start": 271.6, "end": 272.8, "text": " And then you proceed.", "tokens": [51750, 400, 550, 291, 8991, 13, 51810], "temperature": 0.0, "avg_logprob": -0.14119625091552734, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.7179492712020874}, {"id": 46, "seek": 27280, "start": 273.44, "end": 278.72, "text": " But you are the agent in that situation going around and pursuing your goals.", "tokens": [50396, 583, 291, 366, 264, 9461, 294, 300, 2590, 516, 926, 293, 20222, 428, 5493, 13, 50660], "temperature": 0.0, "avg_logprob": -0.11038048700852827, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.007811573799699545}, {"id": 47, "seek": 27280, "start": 279.16, "end": 282.64, "text": " In contrast, the other mode that I think is also really interesting is delegation mode.", "tokens": [50682, 682, 8712, 11, 264, 661, 4391, 300, 286, 519, 307, 611, 534, 1880, 307, 36602, 4391, 13, 50856], "temperature": 0.0, "avg_logprob": -0.11038048700852827, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.007811573799699545}, {"id": 48, "seek": 27280, "start": 283.2, "end": 287.76, "text": " And that is where you are truly offloading a task.", "tokens": [50884, 400, 300, 307, 689, 291, 366, 4908, 766, 2907, 278, 257, 5633, 13, 51112], "temperature": 0.0, "avg_logprob": -0.11038048700852827, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.007811573799699545}, {"id": 49, "seek": 27280, "start": 287.8, "end": 298.08000000000004, "text": " And I always say the goal of delegation mode is to get the output to the point where it is consistent enough that you don't have to review every single output.", "tokens": [51114, 400, 286, 1009, 584, 264, 3387, 295, 36602, 4391, 307, 281, 483, 264, 5598, 281, 264, 935, 689, 309, 307, 8398, 1547, 300, 291, 500, 380, 362, 281, 3131, 633, 2167, 5598, 13, 51628], "temperature": 0.0, "avg_logprob": -0.11038048700852827, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.007811573799699545}, {"id": 50, "seek": 29808, "start": 298.91999999999996, "end": 306.96, "text": " And if you can get there, then you can start to really shift work to AI in a way that you no longer have to do it.", "tokens": [50406, 400, 498, 291, 393, 483, 456, 11, 550, 291, 393, 722, 281, 534, 5513, 589, 281, 7318, 294, 257, 636, 300, 291, 572, 2854, 362, 281, 360, 309, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10881128957716085, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.05662340298295021}, {"id": 51, "seek": 29808, "start": 307.2, "end": 309.76, "text": " And that can that can be useful in different kinds of ways, right?", "tokens": [50820, 400, 300, 393, 300, 393, 312, 4420, 294, 819, 3685, 295, 2098, 11, 558, 30, 50948], "temperature": 0.0, "avg_logprob": -0.10881128957716085, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.05662340298295021}, {"id": 52, "seek": 29808, "start": 309.76, "end": 312.68, "text": " The co-pilot mode is about helping you be better.", "tokens": [50948, 440, 598, 12, 79, 31516, 4391, 307, 466, 4315, 291, 312, 1101, 13, 51094], "temperature": 0.0, "avg_logprob": -0.10881128957716085, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.05662340298295021}, {"id": 53, "seek": 29808, "start": 312.68, "end": 317.47999999999996, "text": " That's your classic symbiosis or intelligence augmentation.", "tokens": [51094, 663, 311, 428, 7230, 43700, 48783, 420, 7599, 14501, 19631, 13, 51334], "temperature": 0.0, "avg_logprob": -0.10881128957716085, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.05662340298295021}, {"id": 54, "seek": 29808, "start": 318.12, "end": 327.84, "text": " And then the delegation mode is more like we can save a ton of time and money on things that used to be a pain in our butts, or we can scale things that are not currently", "tokens": [51366, 400, 550, 264, 36602, 4391, 307, 544, 411, 321, 393, 3155, 257, 2952, 295, 565, 293, 1460, 322, 721, 300, 1143, 281, 312, 257, 1822, 294, 527, 46789, 11, 420, 321, 393, 4373, 721, 300, 366, 406, 4362, 51852], "temperature": 0.0, "avg_logprob": -0.10881128957716085, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.05662340298295021}, {"id": 55, "seek": 32784, "start": 327.84, "end": 331.08, "text": " scalable. And there's a lot of that in the world, right?", "tokens": [50364, 38481, 13, 400, 456, 311, 257, 688, 295, 300, 294, 264, 1002, 11, 558, 30, 50526], "temperature": 0.0, "avg_logprob": -0.13520655177888416, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.00271421461366117}, {"id": 56, "seek": 32784, "start": 331.35999999999996, "end": 344.52, "text": " I think almost everybody has things where they would say, you know, if you just ask the question, is there stuff that you could be doing that would be really valuable to have done, but you just don't have time to do it?", "tokens": [50540, 286, 519, 1920, 2201, 575, 721, 689, 436, 576, 584, 11, 291, 458, 11, 498, 291, 445, 1029, 264, 1168, 11, 307, 456, 1507, 300, 291, 727, 312, 884, 300, 576, 312, 534, 8263, 281, 362, 1096, 11, 457, 291, 445, 500, 380, 362, 565, 281, 360, 309, 30, 51198], "temperature": 0.0, "avg_logprob": -0.13520655177888416, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.00271421461366117}, {"id": 57, "seek": 32784, "start": 344.64, "end": 347.47999999999996, "text": " There's a lot of that that can be quite transformative.", "tokens": [51204, 821, 311, 257, 688, 295, 300, 300, 393, 312, 1596, 36070, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13520655177888416, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.00271421461366117}, {"id": 58, "seek": 34748, "start": 348.0, "end": 360.72, "text": " In the middle, and what's kind of missing right now still is between co-pilot mode, where you're getting this kind of real time help and deciding how to work it into whatever you're doing.", "tokens": [50390, 682, 264, 2808, 11, 293, 437, 311, 733, 295, 5361, 558, 586, 920, 307, 1296, 598, 12, 79, 31516, 4391, 11, 689, 291, 434, 1242, 341, 733, 295, 957, 565, 854, 293, 17990, 577, 281, 589, 309, 666, 2035, 291, 434, 884, 13, 51026], "temperature": 0.0, "avg_logprob": -0.16103042440211518, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.721156895160675}, {"id": 59, "seek": 34748, "start": 361.04, "end": 373.44, "text": " And delegation mode on the other end in between is ad hoc delegation, where it's I'm going along, but I want to, ideally, I would like to delegate more and bigger sub tasks to AI on the fly.", "tokens": [51042, 400, 36602, 4391, 322, 264, 661, 917, 294, 1296, 307, 614, 16708, 36602, 11, 689, 309, 311, 286, 478, 516, 2051, 11, 457, 286, 528, 281, 11, 22915, 11, 286, 576, 411, 281, 40999, 544, 293, 3801, 1422, 9608, 281, 7318, 322, 264, 3603, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16103042440211518, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.721156895160675}, {"id": 60, "seek": 37344, "start": 373.92, "end": 380.04, "text": " And that's where we're not quite there yet. The agents probably can't do much in the way of a significant task.", "tokens": [50388, 400, 300, 311, 689, 321, 434, 406, 1596, 456, 1939, 13, 440, 12554, 1391, 393, 380, 360, 709, 294, 264, 636, 295, 257, 4776, 5633, 13, 50694], "temperature": 0.0, "avg_logprob": -0.13779688689668299, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.01691076159477234}, {"id": 61, "seek": 37344, "start": 380.04, "end": 394.28, "text": " So it's, you're still shoehorned into one of two scenarios where you're engaging with it in real time and getting help, or you're going through the process of doing a setup and doing a validation, setting up a workflow to where you can truly delegate.", "tokens": [50694, 407, 309, 311, 11, 291, 434, 920, 12796, 31990, 292, 666, 472, 295, 732, 15077, 689, 291, 434, 11268, 365, 309, 294, 957, 565, 293, 1242, 854, 11, 420, 291, 434, 516, 807, 264, 1399, 295, 884, 257, 8657, 293, 884, 257, 24071, 11, 3287, 493, 257, 20993, 281, 689, 291, 393, 4908, 40999, 13, 51406], "temperature": 0.0, "avg_logprob": -0.13779688689668299, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.01691076159477234}, {"id": 62, "seek": 37344, "start": 394.4, "end": 401.32, "text": " And it's that in between that I think is probably that gap gets closed over the next year as agents, quote unquote, begin to work.", "tokens": [51412, 400, 309, 311, 300, 294, 1296, 300, 286, 519, 307, 1391, 300, 7417, 2170, 5395, 670, 264, 958, 1064, 382, 12554, 11, 6513, 37557, 11, 1841, 281, 589, 13, 51758], "temperature": 0.0, "avg_logprob": -0.13779688689668299, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.01691076159477234}, {"id": 63, "seek": 40132, "start": 401.56, "end": 405.56, "text": " And then we can start to delegate bigger chunks of work on the fly.", "tokens": [50376, 400, 550, 321, 393, 722, 281, 40999, 3801, 24004, 295, 589, 322, 264, 3603, 13, 50576], "temperature": 0.0, "avg_logprob": -0.13583054542541503, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.012050916440784931}, {"id": 64, "seek": 40132, "start": 406.4, "end": 410.36, "text": " The next question was, is it good? I don't know if I have a great answer to that. I think it's largely good.", "tokens": [50618, 440, 958, 1168, 390, 11, 307, 309, 665, 30, 286, 500, 380, 458, 498, 286, 362, 257, 869, 1867, 281, 300, 13, 286, 519, 309, 311, 11611, 665, 13, 50816], "temperature": 0.0, "avg_logprob": -0.13583054542541503, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.012050916440784931}, {"id": 65, "seek": 40132, "start": 410.52, "end": 418.48, "text": " It's I think it says it's good as long as it's it's good as long as humans stay in control of the overall dynamic.", "tokens": [50824, 467, 311, 286, 519, 309, 1619, 309, 311, 665, 382, 938, 382, 309, 311, 309, 311, 665, 382, 938, 382, 6255, 1754, 294, 1969, 295, 264, 4787, 8546, 13, 51222], "temperature": 0.0, "avg_logprob": -0.13583054542541503, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.012050916440784931}, {"id": 66, "seek": 40132, "start": 418.8, "end": 427.6, "text": " And I'm definitely one who considers everything to be in play for the future, both on the positive side, I don't think it's crazy to think of a post scarcity world.", "tokens": [51238, 400, 286, 478, 2138, 472, 567, 33095, 1203, 281, 312, 294, 862, 337, 264, 2027, 11, 1293, 322, 264, 3353, 1252, 11, 286, 500, 380, 519, 309, 311, 3219, 281, 519, 295, 257, 2183, 44181, 1002, 13, 51678], "temperature": 0.0, "avg_logprob": -0.13583054542541503, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.012050916440784931}, {"id": 67, "seek": 42760, "start": 428.20000000000005, "end": 442.48, "text": " And on the negative side, to quote Sam Altman, I wouldn't rule out lights out for all of us. I think we are definitely playing with a force here that has the potential to be totally transformative in good and bad and probably a combination of ways.", "tokens": [50394, 400, 322, 264, 3671, 1252, 11, 281, 6513, 4832, 15992, 1601, 11, 286, 2759, 380, 4978, 484, 5811, 484, 337, 439, 295, 505, 13, 286, 519, 321, 366, 2138, 2433, 365, 257, 3464, 510, 300, 575, 264, 3995, 281, 312, 3879, 36070, 294, 665, 293, 1578, 293, 1391, 257, 6562, 295, 2098, 13, 51108], "temperature": 0.0, "avg_logprob": -0.10210920750409708, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.016910070553421974}, {"id": 68, "seek": 42760, "start": 443.12, "end": 448.40000000000003, "text": " I'm thrilled by how much more productive I can be. And that's some of the stuff that we'll get into in more detail.", "tokens": [51140, 286, 478, 18744, 538, 577, 709, 544, 13304, 286, 393, 312, 13, 400, 300, 311, 512, 295, 264, 1507, 300, 321, 603, 483, 666, 294, 544, 2607, 13, 51404], "temperature": 0.0, "avg_logprob": -0.10210920750409708, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.016910070553421974}, {"id": 69, "seek": 44840, "start": 448.91999999999996, "end": 462.0, "text": " I am thrilled by the prospect of having infinite access to expertise, and especially for people who have far less means than I do to have that kind of access to expertise.", "tokens": [50390, 286, 669, 18744, 538, 264, 15005, 295, 1419, 13785, 2105, 281, 11769, 11, 293, 2318, 337, 561, 567, 362, 1400, 1570, 1355, 813, 286, 360, 281, 362, 300, 733, 295, 2105, 281, 11769, 13, 51044], "temperature": 0.0, "avg_logprob": -0.08604374447384396, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.5923996567726135}, {"id": 70, "seek": 44840, "start": 462.03999999999996, "end": 470.44, "text": " I am a pretty privileged person who can go to the doctor without really thinking twice about taking the time off from work or what that's going to cost me or whatever.", "tokens": [51046, 286, 669, 257, 1238, 25293, 954, 567, 393, 352, 281, 264, 4631, 1553, 534, 1953, 6091, 466, 1940, 264, 565, 766, 490, 589, 420, 437, 300, 311, 516, 281, 2063, 385, 420, 2035, 13, 51466], "temperature": 0.0, "avg_logprob": -0.08604374447384396, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.5923996567726135}, {"id": 71, "seek": 47044, "start": 470.68, "end": 482.48, "text": " Obviously, a lot of people don't have that luxury. I think there is a real way in which AI can cover a lot of those gaps, not fully yet, but already significantly and obviously more and more over time.", "tokens": [50376, 7580, 11, 257, 688, 295, 561, 500, 380, 362, 300, 15558, 13, 286, 519, 456, 307, 257, 957, 636, 294, 597, 7318, 393, 2060, 257, 688, 295, 729, 15031, 11, 406, 4498, 1939, 11, 457, 1217, 10591, 293, 2745, 544, 293, 544, 670, 565, 13, 50966], "temperature": 0.0, "avg_logprob": -0.12710729598999024, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.13647057116031647}, {"id": 72, "seek": 47044, "start": 482.76, "end": 488.76, "text": " I think that kind of stuff is going to be potentially disruptive and maybe the source of a lot of political debates and challenges.", "tokens": [50980, 286, 519, 300, 733, 295, 1507, 307, 516, 281, 312, 7263, 37865, 293, 1310, 264, 4009, 295, 257, 688, 295, 3905, 24203, 293, 4759, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12710729598999024, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.13647057116031647}, {"id": 73, "seek": 47044, "start": 489.24, "end": 493.88, "text": " But anyway, yeah, there's there's so much upside, but I think there is very real risk.", "tokens": [51304, 583, 4033, 11, 1338, 11, 456, 311, 456, 311, 370, 709, 14119, 11, 457, 286, 519, 456, 307, 588, 957, 3148, 13, 51536], "temperature": 0.0, "avg_logprob": -0.12710729598999024, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.13647057116031647}, {"id": 74, "seek": 49388, "start": 493.96, "end": 506.64, "text": " And it's very easy to hold those two perspectives at the same time, to be just thrilled by the capability, but also to be always, always keeping in mind a sort of healthy fear.", "tokens": [50368, 400, 309, 311, 588, 1858, 281, 1797, 729, 732, 16766, 412, 264, 912, 565, 11, 281, 312, 445, 18744, 538, 264, 13759, 11, 457, 611, 281, 312, 1009, 11, 1009, 5145, 294, 1575, 257, 1333, 295, 4627, 4240, 13, 51002], "temperature": 0.0, "avg_logprob": -0.13213592239573033, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17305070161819458}, {"id": 75, "seek": 49388, "start": 507.15999999999997, "end": 518.56, "text": " I love that. I think that's such a rare perspective. And as humans, we just tend to collapse on one. Either it's horrible or it's great.", "tokens": [51028, 286, 959, 300, 13, 286, 519, 300, 311, 1270, 257, 5892, 4585, 13, 400, 382, 6255, 11, 321, 445, 3928, 281, 15584, 322, 472, 13, 13746, 309, 311, 9263, 420, 309, 311, 869, 13, 51598], "temperature": 0.0, "avg_logprob": -0.13213592239573033, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17305070161819458}, {"id": 76, "seek": 51856, "start": 519.0, "end": 528.3199999999999, "text": " And then we have these camps. And I think, like, obviously, the wise perspective is there's going to be some really amazing stuff about this.", "tokens": [50386, 400, 550, 321, 362, 613, 16573, 13, 400, 286, 519, 11, 411, 11, 2745, 11, 264, 10829, 4585, 307, 456, 311, 516, 281, 312, 512, 534, 2243, 1507, 466, 341, 13, 50852], "temperature": 0.0, "avg_logprob": -0.16486891520392036, "compression_ratio": 1.755458515283843, "no_speech_prob": 0.11583293229341507}, {"id": 77, "seek": 51856, "start": 528.3599999999999, "end": 534.68, "text": " And there are dangers, like when technology changes society and change, it'll change our brains, like we will adapt to this.", "tokens": [50854, 400, 456, 366, 27701, 11, 411, 562, 2899, 2962, 4086, 293, 1319, 11, 309, 603, 1319, 527, 15442, 11, 411, 321, 486, 6231, 281, 341, 13, 51170], "temperature": 0.0, "avg_logprob": -0.16486891520392036, "compression_ratio": 1.755458515283843, "no_speech_prob": 0.11583293229341507}, {"id": 78, "seek": 51856, "start": 534.7199999999999, "end": 541.8, "text": " And in the same way that it is adapting to us, that will change things and we'll need to, like, deal with the dangers that it presents.", "tokens": [51172, 400, 294, 264, 912, 636, 300, 309, 307, 34942, 281, 505, 11, 300, 486, 1319, 721, 293, 321, 603, 643, 281, 11, 411, 11, 2028, 365, 264, 27701, 300, 309, 13533, 13, 51526], "temperature": 0.0, "avg_logprob": -0.16486891520392036, "compression_ratio": 1.755458515283843, "no_speech_prob": 0.11583293229341507}, {"id": 79, "seek": 54180, "start": 541.8, "end": 548.3199999999999, "text": " I think that's a very wise perspective. And I ask that question, is it a good thing that cognitive work will be offloaded?", "tokens": [50364, 286, 519, 300, 311, 257, 588, 10829, 4585, 13, 400, 286, 1029, 300, 1168, 11, 307, 309, 257, 665, 551, 300, 15605, 589, 486, 312, 766, 2907, 292, 30, 50690], "temperature": 0.0, "avg_logprob": -0.13571590127296818, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.28750795125961304}, {"id": 80, "seek": 54180, "start": 548.3199999999999, "end": 557.3199999999999, "text": " Because I think that there's good and bad. But one of the things that I feel is the fear scenario is like, is quite dominant for a lot of people.", "tokens": [50690, 1436, 286, 519, 300, 456, 311, 665, 293, 1578, 13, 583, 472, 295, 264, 721, 300, 286, 841, 307, 264, 4240, 9005, 307, 411, 11, 307, 1596, 15657, 337, 257, 688, 295, 561, 13, 51140], "temperature": 0.0, "avg_logprob": -0.13571590127296818, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.28750795125961304}, {"id": 81, "seek": 54180, "start": 557.8, "end": 567.4, "text": " And, and I think the people who are like anti fear or presenting a hopeful view are a little but they're a little bit too like rose colored glasses.", "tokens": [51164, 400, 11, 293, 286, 519, 264, 561, 567, 366, 411, 6061, 4240, 420, 15578, 257, 20531, 1910, 366, 257, 707, 457, 436, 434, 257, 707, 857, 886, 411, 10895, 14332, 10812, 13, 51644], "temperature": 0.0, "avg_logprob": -0.13571590127296818, "compression_ratio": 1.7020408163265306, "no_speech_prob": 0.28750795125961304}, {"id": 82, "seek": 56740, "start": 567.8, "end": 585.92, "text": " And I think finding real ways and real use cases for how offloading some of this cognitive work actually helps people is just like a really important part of creating a world where AI is a force for good or force for creativity,", "tokens": [50384, 400, 286, 519, 5006, 957, 2098, 293, 957, 764, 3331, 337, 577, 766, 2907, 278, 512, 295, 341, 15605, 589, 767, 3665, 561, 307, 445, 411, 257, 534, 1021, 644, 295, 4084, 257, 1002, 689, 7318, 307, 257, 3464, 337, 665, 420, 3464, 337, 12915, 11, 51290], "temperature": 0.0, "avg_logprob": -0.12863813687677253, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.03620067983865738}, {"id": 83, "seek": 56740, "start": 585.92, "end": 592.0, "text": " rather than a world where it just replaces people or it creates dangers or there's all the bad scenarios.", "tokens": [51290, 2831, 813, 257, 1002, 689, 309, 445, 46734, 561, 420, 309, 7829, 27701, 420, 456, 311, 439, 264, 1578, 15077, 13, 51594], "temperature": 0.0, "avg_logprob": -0.12863813687677253, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.03620067983865738}, {"id": 84, "seek": 59200, "start": 592.0, "end": 608.8, "text": " And one of the things that I've felt going back to your kind of co pilot mode versus delegation mode point, one of the things that I felt is that AI reveals to me how much drudgery there is, even in highly valuable, highly creative knowledge work.", "tokens": [50364, 400, 472, 295, 264, 721, 300, 286, 600, 2762, 516, 646, 281, 428, 733, 295, 598, 9691, 4391, 5717, 36602, 4391, 935, 11, 472, 295, 264, 721, 300, 286, 2762, 307, 300, 7318, 20893, 281, 385, 577, 709, 1224, 532, 7337, 456, 307, 11, 754, 294, 5405, 8263, 11, 5405, 5880, 3601, 589, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13760644418221932, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.1687496453523636}, {"id": 85, "seek": 59200, "start": 609.6, "end": 619.48, "text": " And that we sort of like lie to ourselves about the amount of drudgery because that work is so romantic compared to, I don't know, I don't know, working in a factory maybe or just any other kind of job.", "tokens": [51244, 400, 300, 321, 1333, 295, 411, 4544, 281, 4175, 466, 264, 2372, 295, 1224, 532, 7337, 570, 300, 589, 307, 370, 13590, 5347, 281, 11, 286, 500, 380, 458, 11, 286, 500, 380, 458, 11, 1364, 294, 257, 9265, 1310, 420, 445, 604, 661, 733, 295, 1691, 13, 51738], "temperature": 0.0, "avg_logprob": -0.13760644418221932, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.1687496453523636}, {"id": 86, "seek": 61948, "start": 619.88, "end": 631.48, "text": " And it's, it's easy to look at a lawyer and be like a lawyer's job is full of drudgery or whatever. But I write, I run a business. I have a YouTube show. Now I have a podcast. There's a lot of stuff that's like just pure drudgery of that.", "tokens": [50384, 400, 309, 311, 11, 309, 311, 1858, 281, 574, 412, 257, 11613, 293, 312, 411, 257, 11613, 311, 1691, 307, 1577, 295, 1224, 532, 7337, 420, 2035, 13, 583, 286, 2464, 11, 286, 1190, 257, 1606, 13, 286, 362, 257, 3088, 855, 13, 823, 286, 362, 257, 7367, 13, 821, 311, 257, 688, 295, 1507, 300, 311, 411, 445, 6075, 1224, 532, 7337, 295, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15089780776227107, "compression_ratio": 1.6655172413793105, "no_speech_prob": 0.04335121065378189}, {"id": 87, "seek": 61948, "start": 631.52, "end": 646.72, "text": " And I find it really interesting because using chat GBT using AI tools more broadly, it has made me aware of how many repetitive or like just overall kind of brain dead things I have to do just to write something smart on the internet on every.", "tokens": [50966, 400, 286, 915, 309, 534, 1880, 570, 1228, 5081, 26809, 51, 1228, 7318, 3873, 544, 19511, 11, 309, 575, 1027, 385, 3650, 295, 577, 867, 29404, 420, 411, 445, 4787, 733, 295, 3567, 3116, 721, 286, 362, 281, 360, 445, 281, 2464, 746, 4069, 322, 264, 4705, 322, 633, 13, 51726], "temperature": 0.0, "avg_logprob": -0.15089780776227107, "compression_ratio": 1.6655172413793105, "no_speech_prob": 0.04335121065378189}, {"id": 88, "seek": 64672, "start": 647.1600000000001, "end": 655.5600000000001, "text": " And once it's visible, I use AI for it. And then I don't have to think about it as much anymore. And I think that's a really cool thing.", "tokens": [50386, 400, 1564, 309, 311, 8974, 11, 286, 764, 7318, 337, 309, 13, 400, 550, 286, 500, 380, 362, 281, 519, 466, 309, 382, 709, 3602, 13, 400, 286, 519, 300, 311, 257, 534, 1627, 551, 13, 50806], "temperature": 0.0, "avg_logprob": -0.11088916358597782, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.02756941132247448}, {"id": 89, "seek": 64672, "start": 656.52, "end": 676.08, "text": " Totally. For me, coding comes to mind most there when you talk about the drudgery of high value and again, pretty privileged work to be doing. But I'm not a full time coder have been for a couple short stretches in life, but more often I've been somebody who's dipped in and out of it.", "tokens": [50854, 22837, 13, 1171, 385, 11, 17720, 1487, 281, 1575, 881, 456, 562, 291, 751, 466, 264, 1224, 532, 7337, 295, 1090, 2158, 293, 797, 11, 1238, 25293, 589, 281, 312, 884, 13, 583, 286, 478, 406, 257, 1577, 565, 17656, 260, 362, 668, 337, 257, 1916, 2099, 29058, 294, 993, 11, 457, 544, 2049, 286, 600, 668, 2618, 567, 311, 45162, 294, 293, 484, 295, 309, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11088916358597782, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.02756941132247448}, {"id": 90, "seek": 67608, "start": 676.32, "end": 695.34, "text": " And it is a real pain in the butt to have to Google everything. Obviously, different people have different strengths and weaknesses. I do not remember syntax super well. Sometimes if it's been a while, I'm like, wait a second, am I, is this am I remembering JavaScript or am I remembering Python?", "tokens": [50376, 400, 309, 307, 257, 957, 1822, 294, 264, 6660, 281, 362, 281, 3329, 1203, 13, 7580, 11, 819, 561, 362, 819, 16986, 293, 24381, 13, 286, 360, 406, 1604, 28431, 1687, 731, 13, 4803, 498, 309, 311, 668, 257, 1339, 11, 286, 478, 411, 11, 1699, 257, 1150, 11, 669, 286, 11, 307, 341, 669, 286, 20719, 15778, 420, 669, 286, 20719, 15329, 30, 51327], "temperature": 0.0, "avg_logprob": -0.1914852226481718, "compression_ratio": 1.48, "no_speech_prob": 0.013629770837724209}, {"id": 91, "seek": 69534, "start": 695.34, "end": 716.26, "text": " Yeah, what exactly is going on here? Yeah. And so to be able to just have the thing type out even relatively simple stuff for me is a multiple x speed up often in terms of productivity improvements, often an improvement in just strict quality to compared to what I would have done on my own.", "tokens": [50364, 865, 11, 437, 2293, 307, 516, 322, 510, 30, 865, 13, 400, 370, 281, 312, 1075, 281, 445, 362, 264, 551, 2010, 484, 754, 7226, 2199, 1507, 337, 385, 307, 257, 3866, 2031, 3073, 493, 2049, 294, 2115, 295, 15604, 13797, 11, 2049, 364, 10444, 294, 445, 10910, 3125, 281, 5347, 281, 437, 286, 576, 362, 1096, 322, 452, 1065, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1845710901113657, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.46432942152023315}, {"id": 92, "seek": 71626, "start": 716.46, "end": 734.9399999999999, "text": " Yeah. And makes it so much easier to get into the mode in the first place. There's this kind of, I wouldn't even call this drudgery, but it's gearing up just like somehow getting my people talk about in birdwatching getting your eyes on really focusing on what are you seeing and trying to like get that detector right.", "tokens": [50374, 865, 13, 400, 1669, 309, 370, 709, 3571, 281, 483, 666, 264, 4391, 294, 264, 700, 1081, 13, 821, 311, 341, 733, 295, 11, 286, 2759, 380, 754, 818, 341, 1224, 532, 7337, 11, 457, 309, 311, 1519, 1921, 493, 445, 411, 6063, 1242, 452, 561, 751, 466, 294, 5255, 15219, 278, 1242, 428, 2575, 322, 534, 8416, 322, 437, 366, 291, 2577, 293, 1382, 281, 411, 483, 300, 25712, 558, 13, 51298], "temperature": 0.0, "avg_logprob": -0.11618668154666298, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.46845561265945435}, {"id": 93, "seek": 73494, "start": 734.94, "end": 746.34, "text": " There's like a similar thing, at least for me in terms of getting into code mode. And it also just streamlines that tremendously, because next thing you know, it's writing the code and I'm reading the code reading the codes a lot easier than writing the code.", "tokens": [50364, 821, 311, 411, 257, 2531, 551, 11, 412, 1935, 337, 385, 294, 2115, 295, 1242, 666, 3089, 4391, 13, 400, 309, 611, 445, 4309, 11045, 300, 27985, 11, 570, 958, 551, 291, 458, 11, 309, 311, 3579, 264, 3089, 293, 286, 478, 3760, 264, 3089, 3760, 264, 14211, 257, 688, 3571, 813, 3579, 264, 3089, 13, 50934], "temperature": 0.0, "avg_logprob": -0.14592962349410607, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.10965920984745026}, {"id": 94, "seek": 73494, "start": 746.7, "end": 763.1800000000001, "text": " So I do find, yeah, just tremendous satisfaction, you know, pleasure in just seeing this stuff like outputted for me at superhuman pace better, better than me quality, maybe not superhuman quality, but super Nathan quality. It's awesome.", "tokens": [50952, 407, 286, 360, 915, 11, 1338, 11, 445, 10048, 18715, 11, 291, 458, 11, 6834, 294, 445, 2577, 341, 1507, 411, 5598, 14727, 337, 385, 412, 1687, 18796, 11638, 1101, 11, 1101, 813, 385, 3125, 11, 1310, 406, 1687, 18796, 3125, 11, 457, 1687, 20634, 3125, 13, 467, 311, 3476, 13, 51776], "temperature": 0.0, "avg_logprob": -0.14592962349410607, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.10965920984745026}, {"id": 95, "seek": 76318, "start": 763.42, "end": 788.26, "text": " What you're making me think about is, because I think in large part, not all of it, but in large part what the current class, especially of text models are doing is different forms of summarizing and how like how much summarizing is involved in creative work and programming in writing in decision making a lot of it is just summarizing", "tokens": [50376, 708, 291, 434, 1455, 385, 519, 466, 307, 11, 570, 286, 519, 294, 2416, 644, 11, 406, 439, 295, 309, 11, 457, 294, 2416, 644, 437, 264, 2190, 1508, 11, 2318, 295, 2487, 5245, 366, 884, 307, 819, 6422, 295, 14611, 3319, 293, 577, 411, 577, 709, 14611, 3319, 307, 3288, 294, 5880, 589, 293, 9410, 294, 3579, 294, 3537, 1455, 257, 688, 295, 309, 307, 445, 14611, 3319, 51618], "temperature": 0.0, "avg_logprob": -0.15632172153420645, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.11268371343612671}, {"id": 96, "seek": 78826, "start": 788.26, "end": 812.38, "text": " like in programming, you're summarizing what you find on Google. You have to decide what to summarize and you have to summarize it in the right exact way for like your specific use case. But that's a lot of times what you're doing. Same thing for writing, like a lot of the stuff in my pieces are summaries of books that I've read or conversations I've had or ideas that I found somewhere else that I'm like stringing together in a sort of unique way.", "tokens": [50364, 411, 294, 9410, 11, 291, 434, 14611, 3319, 437, 291, 915, 322, 3329, 13, 509, 362, 281, 4536, 437, 281, 20858, 293, 291, 362, 281, 20858, 309, 294, 264, 558, 1900, 636, 337, 411, 428, 2685, 764, 1389, 13, 583, 300, 311, 257, 688, 295, 1413, 437, 291, 434, 884, 13, 10635, 551, 337, 3579, 11, 411, 257, 688, 295, 264, 1507, 294, 452, 3755, 366, 8367, 4889, 295, 3642, 300, 286, 600, 1401, 420, 7315, 286, 600, 632, 420, 3487, 300, 286, 1352, 4079, 1646, 300, 286, 478, 411, 6798, 278, 1214, 294, 257, 1333, 295, 3845, 636, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1196904182434082, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.06369607150554657}, {"id": 97, "seek": 81238, "start": 812.66, "end": 833.58, "text": " And obviously, I still have to do the management overall management task of deciding which summaries to put in which order and like how they work or whatever but like a lot of it is summary. And I think that's a way that using these tools, you start to see the world a little bit differently and you're like, Oh yeah, there's a whole, there's a whole class of things I'm doing that are summaries that I don't have to do anymore. And I really think that's cool.", "tokens": [50378, 400, 2745, 11, 286, 920, 362, 281, 360, 264, 4592, 4787, 4592, 5633, 295, 17990, 597, 8367, 4889, 281, 829, 294, 597, 1668, 293, 411, 577, 436, 589, 420, 2035, 457, 411, 257, 688, 295, 309, 307, 12691, 13, 400, 286, 519, 300, 311, 257, 636, 300, 1228, 613, 3873, 11, 291, 722, 281, 536, 264, 1002, 257, 707, 857, 7614, 293, 291, 434, 411, 11, 876, 1338, 11, 456, 311, 257, 1379, 11, 456, 311, 257, 1379, 1508, 295, 721, 286, 478, 884, 300, 366, 8367, 4889, 300, 286, 500, 380, 362, 281, 360, 3602, 13, 400, 286, 534, 519, 300, 311, 1627, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1085243662562939, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.4529830515384674}, {"id": 98, "seek": 83358, "start": 833.58, "end": 862.58, "text": " Yeah, I should be one of the areas where I have not adopted AI as much as I probably should have is in repurposing content, making more of what I do with the podcast, because I've put out a lot of episodes, there's a lot of stuff there. And we do use AI in our workflows to, for example, create the time stamp outline right of the different discussion topics at different times throughout the show. That's the most classic.", "tokens": [50414, 865, 11, 286, 820, 312, 472, 295, 264, 3179, 689, 286, 362, 406, 12175, 7318, 382, 709, 382, 286, 1391, 820, 362, 307, 294, 1085, 20130, 6110, 2701, 11, 1455, 544, 295, 437, 286, 360, 365, 264, 7367, 11, 570, 286, 600, 829, 484, 257, 688, 295, 9313, 11, 456, 311, 257, 688, 295, 1507, 456, 13, 400, 321, 360, 764, 7318, 294, 527, 43461, 281, 11, 337, 1365, 11, 1884, 264, 565, 9921, 16387, 558, 295, 264, 819, 5017, 8378, 412, 819, 1413, 3710, 264, 855, 13, 663, 311, 264, 881, 7230, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14671682574085354, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.20162342488765717}, {"id": 99, "seek": 86358, "start": 863.58, "end": 892.58, "text": " Summarization where I'm not looking for a lot of color commentary. It's literally just what was the topic at each time get it right. So we've got some stuff like that we go to pretty regularly. But I have not done as much as I probably could or should maybe this will be a New Year's resolution to bring that to all the different platforms. And it is I think it's an actually an interesting, it's partly a personal quirk. And it is also I think a limitation of the current language.", "tokens": [50364, 8626, 6209, 2144, 689, 286, 478, 406, 1237, 337, 257, 688, 295, 2017, 23527, 13, 467, 311, 3736, 445, 437, 390, 264, 4829, 412, 1184, 565, 483, 309, 558, 13, 407, 321, 600, 658, 512, 1507, 411, 300, 321, 352, 281, 1238, 11672, 13, 583, 286, 362, 406, 1096, 382, 709, 382, 286, 1391, 727, 420, 820, 1310, 341, 486, 312, 257, 1873, 10289, 311, 8669, 281, 1565, 300, 281, 439, 264, 819, 9473, 13, 400, 309, 307, 286, 519, 309, 311, 364, 767, 364, 1880, 11, 309, 311, 17031, 257, 2973, 421, 18610, 13, 400, 309, 307, 611, 286, 519, 257, 27432, 295, 264, 2190, 2856, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15299118316925323, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.1257523149251938}, {"id": 100, "seek": 89358, "start": 893.58, "end": 922.58, "text": " And I think that's one of the other models that I never quite feel like I want them to write as me. I'm very eager to hear your thoughts on how you relate to it in the writing process. Yeah, when I put something out in my own name, I basically don't use chat GBT at all for it. I can use it I find for like voice of the show if I want to do like that time stamp outline or just create a quick summary that's in kind of a neutral voice where it's not signed Nathan and isn't supposed to be like representing my perspective.", "tokens": [50364, 400, 286, 519, 300, 311, 472, 295, 264, 661, 5245, 300, 286, 1128, 1596, 841, 411, 286, 528, 552, 281, 2464, 382, 385, 13, 286, 478, 588, 18259, 281, 1568, 428, 4598, 322, 577, 291, 10961, 281, 309, 294, 264, 3579, 1399, 13, 865, 11, 562, 286, 829, 746, 484, 294, 452, 1065, 1315, 11, 286, 1936, 500, 380, 764, 5081, 26809, 51, 412, 439, 337, 309, 13, 286, 393, 764, 309, 286, 915, 337, 411, 3177, 295, 264, 855, 498, 286, 528, 281, 360, 411, 300, 565, 9921, 16387, 420, 445, 1884, 257, 1702, 12691, 300, 311, 294, 733, 295, 257, 10598, 3177, 689, 309, 311, 406, 8175, 20634, 293, 1943, 380, 3442, 281, 312, 411, 13460, 452, 4585, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2402742462158203, "compression_ratio": 1.6677316293929711, "no_speech_prob": 0.4750949740409851}, {"id": 101, "seek": 92258, "start": 922.58, "end": 937.58, "text": " I haven't really had a great synthesis yet to help create stuff that I want to say in my own voice in my own name. So if you have tips on that, that would be something I would love to come away with a better plan of attack on because I'm not quite there.", "tokens": [50364, 286, 2378, 380, 534, 632, 257, 869, 30252, 1939, 281, 854, 1884, 1507, 300, 286, 528, 281, 584, 294, 452, 1065, 3177, 294, 452, 1065, 1315, 13, 407, 498, 291, 362, 6082, 322, 300, 11, 300, 576, 312, 746, 286, 576, 959, 281, 808, 1314, 365, 257, 1101, 1393, 295, 2690, 322, 570, 286, 478, 406, 1596, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1101056680840961, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.17308567464351654}, {"id": 102, "seek": 92258, "start": 937.58, "end": 941.58, "text": " Hey, we'll continue our interview in a moment after a word from our sponsors.", "tokens": [51114, 1911, 11, 321, 603, 2354, 527, 4049, 294, 257, 1623, 934, 257, 1349, 490, 527, 22593, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1101056680840961, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.17308567464351654}, {"id": 103, "seek": 92258, "start": 941.58, "end": 950.58, "text": " Real quick, what's the easiest choice you can make taking the window instead of the middle seat outsourcing business tasks that you absolutely hate? What about selling with Shopify?", "tokens": [51314, 8467, 1702, 11, 437, 311, 264, 12889, 3922, 291, 393, 652, 1940, 264, 4910, 2602, 295, 264, 2808, 6121, 14758, 41849, 1606, 9608, 300, 291, 3122, 4700, 30, 708, 466, 6511, 365, 43991, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1101056680840961, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.17308567464351654}, {"id": 104, "seek": 95058, "start": 951.58, "end": 956.58, "text": " Shopify is the global commerce platform that helps you sell at every stage of your business.", "tokens": [50414, 43991, 307, 264, 4338, 26320, 3663, 300, 3665, 291, 3607, 412, 633, 3233, 295, 428, 1606, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11200693249702454, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.41065123677253723}, {"id": 105, "seek": 95058, "start": 956.58, "end": 969.58, "text": " Shopify powers 10% of all e-commerce in the US and Shopify is the global force behind Allbirds, Rothes and Brooklyn and millions of other entrepreneurs of every size across 175 countries.", "tokens": [50664, 43991, 8674, 1266, 4, 295, 439, 308, 12, 26926, 294, 264, 2546, 293, 43991, 307, 264, 4338, 3464, 2261, 1057, 31473, 11, 28089, 279, 293, 21872, 293, 6803, 295, 661, 12639, 295, 633, 2744, 2108, 41165, 3517, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11200693249702454, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.41065123677253723}, {"id": 106, "seek": 95058, "start": 969.58, "end": 978.58, "text": " Whether you're selling security systems or marketing memory modules, Shopify helps you sell everywhere from their all in one e-commerce platform to their in person POS system.", "tokens": [51314, 8503, 291, 434, 6511, 3825, 3652, 420, 6370, 4675, 16679, 11, 43991, 3665, 291, 3607, 5315, 490, 641, 439, 294, 472, 308, 12, 26926, 3663, 281, 641, 294, 954, 430, 4367, 1185, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11200693249702454, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.41065123677253723}, {"id": 107, "seek": 97858, "start": 978.58, "end": 981.58, "text": " Wherever and whatever you're selling, Shopify's got you covered.", "tokens": [50364, 30903, 293, 2035, 291, 434, 6511, 11, 43991, 311, 658, 291, 5343, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11124080770155963, "compression_ratio": 1.5912408759124088, "no_speech_prob": 0.0791563168168068}, {"id": 108, "seek": 97858, "start": 981.58, "end": 988.58, "text": " I've used it in the past at the companies I founded and when we launch Merch here at Turpentine, Shopify will be our go-to.", "tokens": [50514, 286, 600, 1143, 309, 294, 264, 1791, 412, 264, 3431, 286, 13234, 293, 562, 321, 4025, 6124, 339, 510, 412, 5712, 22786, 533, 11, 43991, 486, 312, 527, 352, 12, 1353, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11124080770155963, "compression_ratio": 1.5912408759124088, "no_speech_prob": 0.0791563168168068}, {"id": 109, "seek": 97858, "start": 988.58, "end": 996.58, "text": " Shopify helps turn browsers into buyers with the internet's best converting checkout up to 36% better compared to other leading commerce platforms.", "tokens": [50864, 43991, 3665, 1261, 36069, 666, 23465, 365, 264, 4705, 311, 1151, 29942, 37153, 493, 281, 8652, 4, 1101, 5347, 281, 661, 5775, 26320, 9473, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11124080770155963, "compression_ratio": 1.5912408759124088, "no_speech_prob": 0.0791563168168068}, {"id": 110, "seek": 97858, "start": 996.58, "end": 1001.58, "text": " And Shopify helps you sell more with less effort thanks to Shopify Magic, your AI-powered All-Star.", "tokens": [51264, 400, 43991, 3665, 291, 3607, 544, 365, 1570, 4630, 3231, 281, 43991, 16154, 11, 428, 7318, 12, 27178, 1057, 12, 24659, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11124080770155963, "compression_ratio": 1.5912408759124088, "no_speech_prob": 0.0791563168168068}, {"id": 111, "seek": 100158, "start": 1001.58, "end": 1009.58, "text": " With Shopify Magic, whip up captivating content that converts from blog posts to product descriptions. Generate instant FAQ answers.", "tokens": [50364, 2022, 43991, 16154, 11, 22377, 493, 40769, 990, 2701, 300, 38874, 490, 6968, 12300, 281, 1674, 24406, 13, 15409, 473, 9836, 19894, 48, 6338, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 112, "seek": 100158, "start": 1009.58, "end": 1015.58, "text": " Pick the perfect email send time. Plus, Shopify Magic is free for every Shopify seller.", "tokens": [50764, 14129, 264, 2176, 3796, 2845, 565, 13, 7721, 11, 43991, 16154, 307, 1737, 337, 633, 43991, 23600, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 113, "seek": 100158, "start": 1015.58, "end": 1017.58, "text": " Businesses that grow, grow with Shopify.", "tokens": [51064, 10715, 279, 300, 1852, 11, 1852, 365, 43991, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 114, "seek": 100158, "start": 1017.58, "end": 1022.58, "text": " Sign up for a $1 per month trial period at Shopify.com slash Cognitive.", "tokens": [51164, 13515, 493, 337, 257, 1848, 16, 680, 1618, 7308, 2896, 412, 43991, 13, 1112, 17330, 383, 2912, 2187, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 115, "seek": 100158, "start": 1022.58, "end": 1027.58, "text": " Go to Shopify.com slash Cognitive now to grow your business no matter what stage you're in.", "tokens": [51414, 1037, 281, 43991, 13, 1112, 17330, 383, 2912, 2187, 586, 281, 1852, 428, 1606, 572, 1871, 437, 3233, 291, 434, 294, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 116, "seek": 100158, "start": 1027.58, "end": 1030.58, "text": " Shopify.com slash Cognitive.", "tokens": [51664, 43991, 13, 1112, 17330, 383, 2912, 2187, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09206795274165638, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.7895684838294983}, {"id": 117, "seek": 103158, "start": 1032.58, "end": 1040.58, "text": " I do. I definitely do. I love it. I think it goes back again to when you talk about being a co-pilot.", "tokens": [50414, 286, 360, 13, 286, 2138, 360, 13, 286, 959, 309, 13, 286, 519, 309, 1709, 646, 797, 281, 562, 291, 751, 466, 885, 257, 598, 12, 79, 31516, 13, 50814], "temperature": 0.0, "avg_logprob": -0.04951092661643515, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.040809012949466705}, {"id": 118, "seek": 103158, "start": 1040.58, "end": 1047.58, "text": " I think that the failure mode is usually trying to use it when it's a little bit more in delegation mode.", "tokens": [50814, 286, 519, 300, 264, 7763, 4391, 307, 2673, 1382, 281, 764, 309, 562, 309, 311, 257, 707, 857, 544, 294, 36602, 4391, 13, 51164], "temperature": 0.0, "avg_logprob": -0.04951092661643515, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.040809012949466705}, {"id": 119, "seek": 103158, "start": 1047.58, "end": 1050.58, "text": " Just go do this whole thing. That's when it doesn't really work.", "tokens": [51164, 1449, 352, 360, 341, 1379, 551, 13, 663, 311, 562, 309, 1177, 380, 534, 589, 13, 51314], "temperature": 0.0, "avg_logprob": -0.04951092661643515, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.040809012949466705}, {"id": 120, "seek": 103158, "start": 1050.58, "end": 1057.58, "text": " But as a co-pilot, it really works incredibly well for specific micro tasks in writing.", "tokens": [51314, 583, 382, 257, 598, 12, 79, 31516, 11, 309, 534, 1985, 6252, 731, 337, 2685, 4532, 9608, 294, 3579, 13, 51664], "temperature": 0.0, "avg_logprob": -0.04951092661643515, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.040809012949466705}, {"id": 121, "seek": 105758, "start": 1058.58, "end": 1063.58, "text": " First example, as I just brought up, everything is a summary.", "tokens": [50414, 2386, 1365, 11, 382, 286, 445, 3038, 493, 11, 1203, 307, 257, 12691, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10472831060720045, "compression_ratio": 1.5727699530516432, "no_speech_prob": 0.12919361889362335}, {"id": 122, "seek": 105758, "start": 1063.58, "end": 1068.58, "text": " I often have to explain an idea. I was writing a piece a couple months ago where I had to explain an idea.", "tokens": [50664, 286, 2049, 362, 281, 2903, 364, 1558, 13, 286, 390, 3579, 257, 2522, 257, 1916, 2493, 2057, 689, 286, 632, 281, 2903, 364, 1558, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10472831060720045, "compression_ratio": 1.5727699530516432, "no_speech_prob": 0.12919361889362335}, {"id": 123, "seek": 105758, "start": 1068.58, "end": 1077.58, "text": " I knew the idea. I was talking about SPF and FTX's collapse and how utilitarianism and effective altruism,", "tokens": [50914, 286, 2586, 264, 1558, 13, 286, 390, 1417, 466, 8420, 37, 293, 46675, 55, 311, 15584, 293, 577, 4976, 13707, 1434, 293, 4942, 4955, 894, 1434, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10472831060720045, "compression_ratio": 1.5727699530516432, "no_speech_prob": 0.12919361889362335}, {"id": 124, "seek": 105758, "start": 1077.58, "end": 1080.58, "text": " whether or not that philosophy contributed to the collapse.", "tokens": [51364, 1968, 420, 406, 300, 10675, 18434, 281, 264, 15584, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10472831060720045, "compression_ratio": 1.5727699530516432, "no_speech_prob": 0.12919361889362335}, {"id": 125, "seek": 108058, "start": 1080.58, "end": 1086.58, "text": " In order to write that article, I had to summarize the main tenets of utilitarianism.", "tokens": [50364, 682, 1668, 281, 2464, 300, 7222, 11, 286, 632, 281, 20858, 264, 2135, 2064, 1385, 295, 4976, 13707, 1434, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11994865292408427, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.4336663484573364}, {"id": 126, "seek": 108058, "start": 1086.58, "end": 1090.58, "text": " I studied philosophy in college and I've read a lot of Peter Singer's work.", "tokens": [50664, 286, 9454, 10675, 294, 3859, 293, 286, 600, 1401, 257, 688, 295, 6508, 44184, 311, 589, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11994865292408427, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.4336663484573364}, {"id": 127, "seek": 108058, "start": 1090.58, "end": 1093.58, "text": " I just generally know it, but I haven't written about that in a while.", "tokens": [50864, 286, 445, 5101, 458, 309, 11, 457, 286, 2378, 380, 3720, 466, 300, 294, 257, 1339, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11994865292408427, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.4336663484573364}, {"id": 128, "seek": 108058, "start": 1093.58, "end": 1100.58, "text": " Ordinarily, I would have had to spend three hours going back through all the different stuff to formulate my three or four-sentence summary.", "tokens": [51014, 1610, 17557, 3289, 11, 286, 576, 362, 632, 281, 3496, 1045, 2496, 516, 646, 807, 439, 264, 819, 1507, 281, 47881, 452, 1045, 420, 1451, 12, 49315, 655, 12691, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11994865292408427, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.4336663484573364}, {"id": 129, "seek": 108058, "start": 1100.58, "end": 1106.58, "text": " I just asked Chatcha BT and it gave me the summary in the context that I needed it in three or four sentences.", "tokens": [51364, 286, 445, 2351, 761, 852, 64, 31144, 293, 309, 2729, 385, 264, 12691, 294, 264, 4319, 300, 286, 2978, 309, 294, 1045, 420, 1451, 16579, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11994865292408427, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.4336663484573364}, {"id": 130, "seek": 110658, "start": 1107.58, "end": 1113.58, "text": " I didn't use that wholesale, but it gave me basically the thing I needed to tweak it and put it into my voice.", "tokens": [50414, 286, 994, 380, 764, 300, 43982, 11, 457, 309, 2729, 385, 1936, 264, 551, 286, 2978, 281, 29879, 309, 293, 829, 309, 666, 452, 3177, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10763120229265331, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.01321743056178093}, {"id": 131, "seek": 110658, "start": 1113.58, "end": 1120.58, "text": " That's a really simple example, but I think you can use it in all different parts in the writing process.", "tokens": [50714, 663, 311, 257, 534, 2199, 1365, 11, 457, 286, 519, 291, 393, 764, 309, 294, 439, 819, 3166, 294, 264, 3579, 1399, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10763120229265331, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.01321743056178093}, {"id": 132, "seek": 110658, "start": 1120.58, "end": 1127.58, "text": " At the very beginning, I'll often just record myself on a walk, just spewing ideas and random thoughts reassociating.", "tokens": [51064, 1711, 264, 588, 2863, 11, 286, 603, 2049, 445, 2136, 2059, 322, 257, 1792, 11, 445, 768, 7904, 3487, 293, 4974, 4598, 319, 49146, 990, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10763120229265331, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.01321743056178093}, {"id": 133, "seek": 110658, "start": 1127.58, "end": 1133.58, "text": " Then I'll have it transcribe it and summarize it and pull out the main things and then it'll help me find little article ideas.", "tokens": [51414, 1396, 286, 603, 362, 309, 1145, 8056, 309, 293, 20858, 309, 293, 2235, 484, 264, 2135, 721, 293, 550, 309, 603, 854, 385, 915, 707, 7222, 3487, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10763120229265331, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.01321743056178093}, {"id": 134, "seek": 113358, "start": 1134.58, "end": 1143.58, "text": " When I have an article idea, I'll often start with just this really messy document full of quotes and sentences and little things that might go into it.", "tokens": [50414, 1133, 286, 362, 364, 7222, 1558, 11, 286, 603, 2049, 722, 365, 445, 341, 534, 16191, 4166, 1577, 295, 19963, 293, 16579, 293, 707, 721, 300, 1062, 352, 666, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09399719413267363, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.053342554718256}, {"id": 135, "seek": 113358, "start": 1143.58, "end": 1146.58, "text": " Then I'll be like, I don't even know where to start with this. This is crazy.", "tokens": [50864, 1396, 286, 603, 312, 411, 11, 286, 500, 380, 754, 458, 689, 281, 722, 365, 341, 13, 639, 307, 3219, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09399719413267363, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.053342554718256}, {"id": 136, "seek": 113358, "start": 1146.58, "end": 1150.58, "text": " Then I will just be like, can you put this into an outline?", "tokens": [51014, 1396, 286, 486, 445, 312, 411, 11, 393, 291, 829, 341, 666, 364, 16387, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09399719413267363, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.053342554718256}, {"id": 137, "seek": 113358, "start": 1150.58, "end": 1154.58, "text": " I'll just paste the entire document into Chatcha BT and it'll often find an outline.", "tokens": [51214, 286, 603, 445, 9163, 264, 2302, 4166, 666, 761, 852, 64, 31144, 293, 309, 603, 2049, 915, 364, 16387, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09399719413267363, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.053342554718256}, {"id": 138, "seek": 113358, "start": 1154.58, "end": 1158.58, "text": " The outlines it comes up with are really basic.", "tokens": [51414, 440, 40125, 309, 1487, 493, 365, 366, 534, 3875, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09399719413267363, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.053342554718256}, {"id": 139, "seek": 115858, "start": 1158.58, "end": 1169.58, "text": " I think one of the things that's really good at is pointing out the obvious solution that you missed because you're too close to the problem.", "tokens": [50364, 286, 519, 472, 295, 264, 721, 300, 311, 534, 665, 412, 307, 12166, 484, 264, 6322, 3827, 300, 291, 6721, 570, 291, 434, 886, 1998, 281, 264, 1154, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09121115489672589, "compression_ratio": 1.7621145374449338, "no_speech_prob": 0.07362647354602814}, {"id": 140, "seek": 115858, "start": 1169.58, "end": 1177.58, "text": " Of course, the outline for this article is set up the problem and then talk about the solution to the problem that you came up with or whatever.", "tokens": [50914, 2720, 1164, 11, 264, 16387, 337, 341, 7222, 307, 992, 493, 264, 1154, 293, 550, 751, 466, 264, 3827, 281, 264, 1154, 300, 291, 1361, 493, 365, 420, 2035, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09121115489672589, "compression_ratio": 1.7621145374449338, "no_speech_prob": 0.07362647354602814}, {"id": 141, "seek": 115858, "start": 1177.58, "end": 1184.58, "text": " That's such a common format for an article, but if you're in your head about it and you're being really precious,", "tokens": [51314, 663, 311, 1270, 257, 2689, 7877, 337, 364, 7222, 11, 457, 498, 291, 434, 294, 428, 1378, 466, 309, 293, 291, 434, 885, 534, 12406, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09121115489672589, "compression_ratio": 1.7621145374449338, "no_speech_prob": 0.07362647354602814}, {"id": 142, "seek": 118458, "start": 1184.58, "end": 1191.58, "text": " it can be hard to be like, for this special article, it's going to be this basic thing that you've written a thousand times before, the same basic structure.", "tokens": [50364, 309, 393, 312, 1152, 281, 312, 411, 11, 337, 341, 2121, 7222, 11, 309, 311, 516, 281, 312, 341, 3875, 551, 300, 291, 600, 3720, 257, 4714, 1413, 949, 11, 264, 912, 3875, 3877, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08531958903741399, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.14393198490142822}, {"id": 143, "seek": 118458, "start": 1191.58, "end": 1201.58, "text": " Then I think one of the other really great things is it's just incredibly good for helping you figure out what you're trying to express,", "tokens": [50714, 1396, 286, 519, 472, 295, 264, 661, 534, 869, 721, 307, 309, 311, 445, 6252, 665, 337, 4315, 291, 2573, 484, 437, 291, 434, 1382, 281, 5109, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08531958903741399, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.14393198490142822}, {"id": 144, "seek": 118458, "start": 1201.58, "end": 1210.58, "text": " put into words what you're going for, and then also going through the different options of how to express what you want to express until you find something that exactly says the thing you want.", "tokens": [51214, 829, 666, 2283, 437, 291, 434, 516, 337, 11, 293, 550, 611, 516, 807, 264, 819, 3956, 295, 577, 281, 5109, 437, 291, 528, 281, 5109, 1826, 291, 915, 746, 300, 2293, 1619, 264, 551, 291, 528, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08531958903741399, "compression_ratio": 1.8415094339622642, "no_speech_prob": 0.14393198490142822}, {"id": 145, "seek": 121058, "start": 1210.58, "end": 1213.58, "text": " For example, trying to find exactly the right metaphor.", "tokens": [50364, 1171, 1365, 11, 1382, 281, 915, 2293, 264, 558, 19157, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 146, "seek": 121058, "start": 1213.58, "end": 1216.58, "text": " What kind of metaphor are you trying to find?", "tokens": [50514, 708, 733, 295, 19157, 366, 291, 1382, 281, 915, 30, 50664], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 147, "seek": 121058, "start": 1216.58, "end": 1218.58, "text": " What's the idea you're trying to express?", "tokens": [50664, 708, 311, 264, 1558, 291, 434, 1382, 281, 5109, 30, 50764], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 148, "seek": 121058, "start": 1218.58, "end": 1223.58, "text": " Here's 50 different options of ways to express that with a metaphor.", "tokens": [50764, 1692, 311, 2625, 819, 3956, 295, 2098, 281, 5109, 300, 365, 257, 19157, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 149, "seek": 121058, "start": 1223.58, "end": 1233.58, "text": " 49 of them will be trash and one of them will be amazing or one of them will push you in the direction of the one that you come up with is.", "tokens": [51014, 16513, 295, 552, 486, 312, 11321, 293, 472, 295, 552, 486, 312, 2243, 420, 472, 295, 552, 486, 2944, 291, 294, 264, 3513, 295, 264, 472, 300, 291, 808, 493, 365, 307, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 150, "seek": 121058, "start": 1233.58, "end": 1235.58, "text": " I have zillions of examples of that.", "tokens": [51514, 286, 362, 710, 46279, 295, 5110, 295, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10681820860003481, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.5768498182296753}, {"id": 151, "seek": 123558, "start": 1235.58, "end": 1243.58, "text": " I find that ChatGBT, it's all over my writing, but none of the stuff that makes it into the writing I publish is wholesale from ChatGBT.", "tokens": [50364, 286, 915, 300, 27503, 8769, 51, 11, 309, 311, 439, 670, 452, 3579, 11, 457, 6022, 295, 264, 1507, 300, 1669, 309, 666, 264, 3579, 286, 11374, 307, 43982, 490, 27503, 8769, 51, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 152, "seek": 123558, "start": 1243.58, "end": 1245.58, "text": " It's like doing some of those micro tasks for me all the time.", "tokens": [50764, 467, 311, 411, 884, 512, 295, 729, 4532, 9608, 337, 385, 439, 264, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 153, "seek": 123558, "start": 1245.58, "end": 1247.58, "text": " Yeah, that's interesting.", "tokens": [50864, 865, 11, 300, 311, 1880, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 154, "seek": 123558, "start": 1247.58, "end": 1250.58, "text": " Some of the stuff that you mentioned there I have had some luck with.", "tokens": [50964, 2188, 295, 264, 1507, 300, 291, 2835, 456, 286, 362, 632, 512, 3668, 365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 155, "seek": 123558, "start": 1250.58, "end": 1256.58, "text": " The talking to it on a walk is quite helpful in some cases.", "tokens": [51114, 440, 1417, 281, 309, 322, 257, 1792, 307, 1596, 4961, 294, 512, 3331, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 156, "seek": 123558, "start": 1256.58, "end": 1262.58, "text": " I've done a couple things where I tried to draft a letter and do, as you said, talk my way through it.", "tokens": [51414, 286, 600, 1096, 257, 1916, 721, 689, 286, 3031, 281, 11206, 257, 5063, 293, 360, 11, 382, 291, 848, 11, 751, 452, 636, 807, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10844621350688319, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.06004560738801956}, {"id": 157, "seek": 126258, "start": 1262.58, "end": 1263.58, "text": " Here's what I want to say.", "tokens": [50364, 1692, 311, 437, 286, 528, 281, 584, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 158, "seek": 126258, "start": 1263.58, "end": 1265.58, "text": " I'm writing here to this person.", "tokens": [50414, 286, 478, 3579, 510, 281, 341, 954, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 159, "seek": 126258, "start": 1265.58, "end": 1266.58, "text": " Here's a little context.", "tokens": [50514, 1692, 311, 257, 707, 4319, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 160, "seek": 126258, "start": 1266.58, "end": 1268.58, "text": " Here's the key points I want to get across.", "tokens": [50564, 1692, 311, 264, 2141, 2793, 286, 528, 281, 483, 2108, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 161, "seek": 126258, "start": 1268.58, "end": 1269.58, "text": " Can you do a draft?", "tokens": [50664, 1664, 291, 360, 257, 11206, 30, 50714], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 162, "seek": 126258, "start": 1269.58, "end": 1273.58, "text": " And then iterating verbally on that draft.", "tokens": [50714, 400, 550, 17138, 990, 48162, 322, 300, 11206, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 163, "seek": 126258, "start": 1273.58, "end": 1280.58, "text": " A lot of times I'll follow up and be like, okay, that's pretty good, but you can give it pre-detailed feedback too.", "tokens": [50914, 316, 688, 295, 1413, 286, 603, 1524, 493, 293, 312, 411, 11, 1392, 11, 300, 311, 1238, 665, 11, 457, 291, 393, 976, 309, 659, 12, 17863, 24731, 5824, 886, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 164, "seek": 126258, "start": 1280.58, "end": 1286.58, "text": " The transcription in the app is so good that it is, again, point of privilege.", "tokens": [51264, 440, 35288, 294, 264, 724, 307, 370, 665, 300, 309, 307, 11, 797, 11, 935, 295, 12122, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 165, "seek": 126258, "start": 1286.58, "end": 1289.58, "text": " It understands me extremely well.", "tokens": [51564, 467, 15146, 385, 4664, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09342431634422241, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08501852303743362}, {"id": 166, "seek": 128958, "start": 1289.58, "end": 1295.58, "text": " So I can literally just have to scroll through its first generation and say, in the first paragraph, I don't really want to say that.", "tokens": [50364, 407, 286, 393, 3736, 445, 362, 281, 11369, 807, 1080, 700, 5125, 293, 584, 11, 294, 264, 700, 18865, 11, 286, 500, 380, 534, 528, 281, 584, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 167, "seek": 128958, "start": 1295.58, "end": 1297.58, "text": " It's more like this in the second paragraph.", "tokens": [50664, 467, 311, 544, 411, 341, 294, 264, 1150, 18865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 168, "seek": 128958, "start": 1297.58, "end": 1298.58, "text": " More emphasis on this.", "tokens": [50764, 5048, 16271, 322, 341, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 169, "seek": 128958, "start": 1298.58, "end": 1299.58, "text": " Add this detail.", "tokens": [50814, 5349, 341, 2607, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 170, "seek": 128958, "start": 1299.58, "end": 1300.58, "text": " Give it like eight things.", "tokens": [50864, 5303, 309, 411, 3180, 721, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 171, "seek": 128958, "start": 1300.58, "end": 1303.58, "text": " You could wish it would do a little bit better on the revision.", "tokens": [50914, 509, 727, 3172, 309, 576, 360, 257, 707, 857, 1101, 322, 264, 34218, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 172, "seek": 128958, "start": 1303.58, "end": 1313.58, "text": " I've had a few moments there where at the end of that process, I have something like, all right, when I get back to the desk, it's not that far of a leap from that to the actual version that I'll use.", "tokens": [51064, 286, 600, 632, 257, 1326, 6065, 456, 689, 412, 264, 917, 295, 300, 1399, 11, 286, 362, 746, 411, 11, 439, 558, 11, 562, 286, 483, 646, 281, 264, 10026, 11, 309, 311, 406, 300, 1400, 295, 257, 19438, 490, 300, 281, 264, 3539, 3037, 300, 286, 603, 764, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 173, "seek": 128958, "start": 1313.58, "end": 1315.58, "text": " It's probably still underutilized for me.", "tokens": [51564, 467, 311, 1391, 920, 833, 20835, 1602, 337, 385, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 174, "seek": 128958, "start": 1315.58, "end": 1316.58, "text": " I should go on more walks, honestly.", "tokens": [51664, 286, 820, 352, 322, 544, 12896, 11, 6095, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11086900067177548, "compression_ratio": 1.7122093023255813, "no_speech_prob": 0.022272499278187752}, {"id": 175, "seek": 131658, "start": 1316.58, "end": 1319.58, "text": " Get more time away from the screen.", "tokens": [50364, 3240, 544, 565, 1314, 490, 264, 2568, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13468846568354853, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.031136436387896538}, {"id": 176, "seek": 131658, "start": 1319.58, "end": 1323.58, "text": " Get the blood flowing a little bit and use a different modality.", "tokens": [50514, 3240, 264, 3390, 13974, 257, 707, 857, 293, 764, 257, 819, 1072, 1860, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13468846568354853, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.031136436387896538}, {"id": 177, "seek": 131658, "start": 1323.58, "end": 1325.58, "text": " The micro tasks, I should do more, though.", "tokens": [50714, 440, 4532, 9608, 11, 286, 820, 360, 544, 11, 1673, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13468846568354853, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.031136436387896538}, {"id": 178, "seek": 131658, "start": 1325.58, "end": 1336.58, "text": " I think that's the tip that I'm taking here is, and there's a separation between, like, sometimes where I feel like it's hurting me is if I haven't...", "tokens": [50814, 286, 519, 300, 311, 264, 4125, 300, 286, 478, 1940, 510, 307, 11, 293, 456, 311, 257, 14634, 1296, 11, 411, 11, 2171, 689, 286, 841, 411, 309, 311, 17744, 385, 307, 498, 286, 2378, 380, 485, 51364], "temperature": 0.0, "avg_logprob": -0.13468846568354853, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.031136436387896538}, {"id": 179, "seek": 131658, "start": 1336.58, "end": 1342.58, "text": " And this doesn't even now start to happen in Gmail or anywhere where there's this auto-complete that's popping up.", "tokens": [51364, 400, 341, 1177, 380, 754, 586, 722, 281, 1051, 294, 36732, 420, 4992, 689, 456, 311, 341, 8399, 12, 1112, 17220, 300, 311, 18374, 493, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13468846568354853, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.031136436387896538}, {"id": 180, "seek": 134258, "start": 1342.58, "end": 1348.58, "text": " Sometimes I'm on the verge of a thought that is really the thought that I'm trying to articulate.", "tokens": [50364, 4803, 286, 478, 322, 264, 37164, 295, 257, 1194, 300, 307, 534, 264, 1194, 300, 286, 478, 1382, 281, 30305, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06588725773793347, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.5693674683570862}, {"id": 181, "seek": 134258, "start": 1348.58, "end": 1351.58, "text": " And then this auto-complete comes up and it's like, that's not right.", "tokens": [50664, 400, 550, 341, 8399, 12, 1112, 17220, 1487, 493, 293, 309, 311, 411, 11, 300, 311, 406, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06588725773793347, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.5693674683570862}, {"id": 182, "seek": 134258, "start": 1351.58, "end": 1356.58, "text": " But it can derail you at times where you're like, don't guess for me right now.", "tokens": [50814, 583, 309, 393, 1163, 864, 291, 412, 1413, 689, 291, 434, 411, 11, 500, 380, 2041, 337, 385, 558, 586, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06588725773793347, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.5693674683570862}, {"id": 183, "seek": 134258, "start": 1356.58, "end": 1358.58, "text": " Let me get the core ideas down first.", "tokens": [51064, 961, 385, 483, 264, 4965, 3487, 760, 700, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06588725773793347, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.5693674683570862}, {"id": 184, "seek": 134258, "start": 1358.58, "end": 1364.58, "text": " If you don't have those core ideas, then for me, it's been a real struggle to get anything good.", "tokens": [51164, 759, 291, 500, 380, 362, 729, 4965, 3487, 11, 550, 337, 385, 11, 309, 311, 668, 257, 957, 7799, 281, 483, 1340, 665, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06588725773793347, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.5693674683570862}, {"id": 185, "seek": 136458, "start": 1364.58, "end": 1373.58, "text": " But I think I've probably not done enough experimentation in the writing process of, okay, I do have some core ideas.", "tokens": [50364, 583, 286, 519, 286, 600, 1391, 406, 1096, 1547, 37142, 294, 264, 3579, 1399, 295, 11, 1392, 11, 286, 360, 362, 512, 4965, 3487, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 186, "seek": 136458, "start": 1373.58, "end": 1378.58, "text": " Can you help me order them, structure them, iterate on them?", "tokens": [50814, 1664, 291, 854, 385, 1668, 552, 11, 3877, 552, 11, 44497, 322, 552, 30, 51064], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 187, "seek": 136458, "start": 1378.58, "end": 1381.58, "text": " Interestingly, I also do use it at the other end often.", "tokens": [51064, 30564, 11, 286, 611, 360, 764, 309, 412, 264, 661, 917, 2049, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 188, "seek": 136458, "start": 1381.58, "end": 1382.58, "text": " Critique this.", "tokens": [51214, 23202, 1925, 341, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 189, "seek": 136458, "start": 1382.58, "end": 1383.58, "text": " Here's an email.", "tokens": [51264, 1692, 311, 364, 3796, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 190, "seek": 136458, "start": 1383.58, "end": 1384.58, "text": " Here's a whatever.", "tokens": [51314, 1692, 311, 257, 2035, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 191, "seek": 136458, "start": 1384.58, "end": 1385.58, "text": " Here's an intro to a podcast.", "tokens": [51364, 1692, 311, 364, 12897, 281, 257, 7367, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 192, "seek": 136458, "start": 1385.58, "end": 1386.58, "text": " Critique it.", "tokens": [51414, 23202, 1925, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 193, "seek": 136458, "start": 1386.58, "end": 1392.58, "text": " That could be really useful if critiques are usually worthy of consideration, at least, I would say.", "tokens": [51464, 663, 727, 312, 534, 4420, 498, 3113, 4911, 366, 2673, 14829, 295, 12381, 11, 412, 1935, 11, 286, 576, 584, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08810920074206441, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.3343847393989563}, {"id": 194, "seek": 139258, "start": 1392.58, "end": 1394.58, "text": " Yeah, it truly is good at that.", "tokens": [50364, 865, 11, 309, 4908, 307, 665, 412, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 195, "seek": 139258, "start": 1394.58, "end": 1401.58, "text": " We have multiple editors who are highly skilled and I still use it to be like, what do you think of this intro?", "tokens": [50464, 492, 362, 3866, 31446, 567, 366, 5405, 19690, 293, 286, 920, 764, 309, 281, 312, 411, 11, 437, 360, 291, 519, 295, 341, 12897, 30, 50814], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 196, "seek": 139258, "start": 1401.58, "end": 1404.58, "text": " Because it's up at 2 a.m. when I'm a night before a deadline.", "tokens": [50814, 1436, 309, 311, 493, 412, 568, 257, 13, 76, 13, 562, 286, 478, 257, 1818, 949, 257, 20615, 13, 50964], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 197, "seek": 139258, "start": 1404.58, "end": 1406.58, "text": " Yeah, it's real hard to beat the availability.", "tokens": [50964, 865, 11, 309, 311, 957, 1152, 281, 4224, 264, 17945, 13, 51064], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 198, "seek": 139258, "start": 1406.58, "end": 1409.58, "text": " The responsiveness is clearly superhuman on that.", "tokens": [51064, 440, 2914, 8477, 307, 4448, 1687, 18796, 322, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 199, "seek": 139258, "start": 1409.58, "end": 1410.58, "text": " I think the writing stuff is really fun.", "tokens": [51214, 286, 519, 264, 3579, 1507, 307, 534, 1019, 13, 51264], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 200, "seek": 139258, "start": 1410.58, "end": 1416.58, "text": " I would love to, if you're ready for it, I would love to start just diving into how you actually, how you use chatGBT.", "tokens": [51264, 286, 576, 959, 281, 11, 498, 291, 434, 1919, 337, 309, 11, 286, 576, 959, 281, 722, 445, 20241, 666, 577, 291, 767, 11, 577, 291, 764, 5081, 8769, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 201, "seek": 139258, "start": 1416.58, "end": 1417.58, "text": " Sure.", "tokens": [51564, 4894, 13, 51614], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 202, "seek": 139258, "start": 1417.58, "end": 1421.58, "text": " You sent me a doc with a bunch of historical chats and this is the first one.", "tokens": [51614, 509, 2279, 385, 257, 3211, 365, 257, 3840, 295, 8584, 38057, 293, 341, 307, 264, 700, 472, 13, 51814], "temperature": 0.0, "avg_logprob": -0.122361827206302, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.007815133780241013}, {"id": 203, "seek": 142158, "start": 1421.58, "end": 1422.58, "text": " Give us the setup.", "tokens": [50364, 5303, 505, 264, 8657, 13, 50414], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 204, "seek": 142158, "start": 1422.58, "end": 1423.58, "text": " What were you doing?", "tokens": [50414, 708, 645, 291, 884, 30, 50464], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 205, "seek": 142158, "start": 1423.58, "end": 1428.58, "text": " At what point were you like, oh, I need to go into chatGBT and then take us from there?", "tokens": [50464, 1711, 437, 935, 645, 291, 411, 11, 1954, 11, 286, 643, 281, 352, 666, 5081, 8769, 51, 293, 550, 747, 505, 490, 456, 30, 50714], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 206, "seek": 142158, "start": 1428.58, "end": 1441.58, "text": " I am working as the AI advisor at a company called Athena, which was founded by a friend of mine named Jonathan.", "tokens": [50714, 286, 669, 1364, 382, 264, 7318, 19161, 412, 257, 2237, 1219, 36827, 11, 597, 390, 13234, 538, 257, 1277, 295, 3892, 4926, 15471, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 207, "seek": 142158, "start": 1441.58, "end": 1444.58, "text": " Is this the virtual assistant company, the Thumbtack?", "tokens": [51364, 1119, 341, 264, 6374, 10994, 2237, 11, 264, 334, 2860, 83, 501, 30, 51514], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 208, "seek": 142158, "start": 1444.58, "end": 1445.58, "text": " Yes.", "tokens": [51514, 1079, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 209, "seek": 142158, "start": 1445.58, "end": 1450.58, "text": " He used one of the founders of Thumbtack and this is a different company, but founded on", "tokens": [51564, 634, 1143, 472, 295, 264, 25608, 295, 334, 2860, 83, 501, 293, 341, 307, 257, 819, 2237, 11, 457, 13234, 322, 51814], "temperature": 0.0, "avg_logprob": -0.14996591047807173, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.036193445324897766}, {"id": 210, "seek": 145058, "start": 1450.58, "end": 1453.58, "text": " some of the lessons that he learned in the Thumbtack experience.", "tokens": [50364, 512, 295, 264, 8820, 300, 415, 3264, 294, 264, 334, 2860, 83, 501, 1752, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09246690649735301, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.010321482084691525}, {"id": 211, "seek": 145058, "start": 1453.58, "end": 1461.58, "text": " He legendarily built up like a really amazing operation powered by contractors in the Philippines.", "tokens": [50514, 634, 9451, 3289, 3094, 493, 411, 257, 534, 2243, 6916, 17786, 538, 28377, 294, 264, 20153, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09246690649735301, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.010321482084691525}, {"id": 212, "seek": 145058, "start": 1461.58, "end": 1471.58, "text": " And that included hiring an assistant for himself and his role at Thumbtack, who became like a almost another key partner in his life over a long time.", "tokens": [50914, 400, 300, 5556, 15335, 364, 10994, 337, 3647, 293, 702, 3090, 412, 334, 2860, 83, 501, 11, 567, 3062, 411, 257, 1920, 1071, 2141, 4975, 294, 702, 993, 670, 257, 938, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09246690649735301, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.010321482084691525}, {"id": 213, "seek": 145058, "start": 1471.58, "end": 1478.58, "text": " And then Athena was built to essentially try to scale that magic for startup founders, executives in general.", "tokens": [51414, 400, 550, 36827, 390, 3094, 281, 4476, 853, 281, 4373, 300, 5585, 337, 18578, 25608, 11, 28485, 294, 2674, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09246690649735301, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.010321482084691525}, {"id": 214, "seek": 147858, "start": 1478.58, "end": 1482.58, "text": " They hire executive assistants in the Philippines.", "tokens": [50364, 814, 11158, 10140, 34949, 294, 264, 20153, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 215, "seek": 147858, "start": 1482.58, "end": 1484.58, "text": " They pay premium wage.", "tokens": [50564, 814, 1689, 12049, 15444, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 216, "seek": 147858, "start": 1484.58, "end": 1487.58, "text": " They're really focused on getting super high quality people.", "tokens": [50664, 814, 434, 534, 5178, 322, 1242, 1687, 1090, 3125, 561, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 217, "seek": 147858, "start": 1487.58, "end": 1498.58, "text": " And the idea is to empower the most ambitious and most high impact people by equipping them with this ability to delegate to their assistant in a transformative way.", "tokens": [50814, 400, 264, 1558, 307, 281, 11071, 264, 881, 20239, 293, 881, 1090, 2712, 561, 538, 1267, 6297, 552, 365, 341, 3485, 281, 40999, 281, 641, 10994, 294, 257, 36070, 636, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 218, "seek": 147858, "start": 1498.58, "end": 1499.58, "text": " Okay.", "tokens": [51364, 1033, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 219, "seek": 147858, "start": 1499.58, "end": 1501.58, "text": " Now we're working on what does AI mean for us, right?", "tokens": [51414, 823, 321, 434, 1364, 322, 437, 775, 7318, 914, 337, 505, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 220, "seek": 147858, "start": 1501.58, "end": 1504.58, "text": " How do we bring that into the assistance work?", "tokens": [51514, 1012, 360, 321, 1565, 300, 666, 264, 9683, 589, 30, 51664], "temperature": 0.0, "avg_logprob": -0.08262031594502557, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.0031722423154860735}, {"id": 221, "seek": 150458, "start": 1504.58, "end": 1511.58, "text": " So one of the things I've done is train the assistance on the use of AI.", "tokens": [50364, 407, 472, 295, 264, 721, 286, 600, 1096, 307, 3847, 264, 9683, 322, 264, 764, 295, 7318, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06163495295756572, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.19169925153255463}, {"id": 222, "seek": 150458, "start": 1511.58, "end": 1517.58, "text": " And that's been a fascinating experience, putting content together, examples, et cetera.", "tokens": [50714, 400, 300, 311, 668, 257, 10343, 1752, 11, 3372, 2701, 1214, 11, 5110, 11, 1030, 11458, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06163495295756572, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.19169925153255463}, {"id": 223, "seek": 150458, "start": 1517.58, "end": 1526.58, "text": " Another thing that I've done is just worked on building a number of kind of prototype demos for what the technology of the future might start to look like.", "tokens": [51014, 3996, 551, 300, 286, 600, 1096, 307, 445, 2732, 322, 2390, 257, 1230, 295, 733, 295, 19475, 33788, 337, 437, 264, 2899, 295, 264, 2027, 1062, 722, 281, 574, 411, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06163495295756572, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.19169925153255463}, {"id": 224, "seek": 152658, "start": 1526.58, "end": 1535.58, "text": " And this chat, which we call Athena chat, is basically our own custom in-house chat GPT is built on an open source project.", "tokens": [50364, 400, 341, 5081, 11, 597, 321, 818, 36827, 5081, 11, 307, 1936, 527, 1065, 2375, 294, 12, 6410, 5081, 26039, 51, 307, 3094, 322, 364, 1269, 4009, 1716, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10570729573567708, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.581010639667511}, {"id": 225, "seek": 152658, "start": 1535.58, "end": 1537.58, "text": " So I didn't have to code every line of it.", "tokens": [50814, 407, 286, 994, 380, 362, 281, 3089, 633, 1622, 295, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10570729573567708, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.581010639667511}, {"id": 226, "seek": 152658, "start": 1537.58, "end": 1544.58, "text": " But it is amazing how quickly you can build things like this today with a bit of know-how.", "tokens": [50914, 583, 309, 307, 2243, 577, 2661, 291, 393, 1322, 721, 411, 341, 965, 365, 257, 857, 295, 458, 12, 4286, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10570729573567708, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.581010639667511}, {"id": 227, "seek": 152658, "start": 1544.58, "end": 1548.58, "text": " So it's been me and one other person who have built a number of these prototypes.", "tokens": [51264, 407, 309, 311, 668, 385, 293, 472, 661, 954, 567, 362, 3094, 257, 1230, 295, 613, 42197, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10570729573567708, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.581010639667511}, {"id": 228, "seek": 154858, "start": 1548.58, "end": 1561.58, "text": " In this case, what we wanted to do is say, can we create a long lived profile that represents the client that can assist the EA in all sorts of ways?", "tokens": [50364, 682, 341, 1389, 11, 437, 321, 1415, 281, 360, 307, 584, 11, 393, 321, 1884, 257, 938, 5152, 7964, 300, 8855, 264, 6423, 300, 393, 4255, 264, 35747, 294, 439, 7527, 295, 2098, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07733576423243473, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3846316933631897}, {"id": 229, "seek": 154858, "start": 1561.58, "end": 1563.58, "text": " So it's essentially a plugin.", "tokens": [51014, 407, 309, 311, 4476, 257, 23407, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07733576423243473, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3846316933631897}, {"id": 230, "seek": 154858, "start": 1563.58, "end": 1568.58, "text": " But with plugins, you have some limitations, whatever we're experimenting with this on our own.", "tokens": [51114, 583, 365, 33759, 11, 291, 362, 512, 15705, 11, 2035, 321, 434, 29070, 365, 341, 322, 527, 1065, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07733576423243473, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3846316933631897}, {"id": 231, "seek": 154858, "start": 1568.58, "end": 1575.58, "text": " One of the big things we wanted to enable is adding information to the client profile, updating information that's already in there.", "tokens": [51364, 1485, 295, 264, 955, 721, 321, 1415, 281, 9528, 307, 5127, 1589, 281, 264, 6423, 7964, 11, 25113, 1589, 300, 311, 1217, 294, 456, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07733576423243473, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3846316933631897}, {"id": 232, "seek": 157558, "start": 1575.58, "end": 1587.58, "text": " So the hope is that this could be a hub where over time client preferences and history and even background context documents all can gradually find their way in there.", "tokens": [50364, 407, 264, 1454, 307, 300, 341, 727, 312, 257, 11838, 689, 670, 565, 6423, 21910, 293, 2503, 293, 754, 3678, 4319, 8512, 439, 393, 13145, 915, 641, 636, 294, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11566032062877309, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.11271029710769653}, {"id": 233, "seek": 157558, "start": 1587.58, "end": 1592.58, "text": " And you have this holistic view where the assistant can go query anything they need.", "tokens": [50964, 400, 291, 362, 341, 30334, 1910, 689, 264, 10994, 393, 352, 14581, 1340, 436, 643, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11566032062877309, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.11271029710769653}, {"id": 234, "seek": 157558, "start": 1592.58, "end": 1596.58, "text": " But again, also update add to it's in theory supposed to evolve over time.", "tokens": [51214, 583, 797, 11, 611, 5623, 909, 281, 309, 311, 294, 5261, 3442, 281, 16693, 670, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11566032062877309, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.11271029710769653}, {"id": 235, "seek": 157558, "start": 1596.58, "end": 1597.58, "text": " Right.", "tokens": [51414, 1779, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11566032062877309, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.11271029710769653}, {"id": 236, "seek": 157558, "start": 1597.58, "end": 1598.58, "text": " So we have this chat GPT like interface.", "tokens": [51464, 407, 321, 362, 341, 5081, 26039, 51, 411, 9226, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11566032062877309, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.11271029710769653}, {"id": 237, "seek": 159858, "start": 1598.58, "end": 1606.58, "text": " And one of the things that we've noticed is that we still see, despite our attempts at education, like it's not perfect.", "tokens": [50364, 400, 472, 295, 264, 721, 300, 321, 600, 5694, 307, 300, 321, 920, 536, 11, 7228, 527, 15257, 412, 3309, 11, 411, 309, 311, 406, 2176, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11788857403923483, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.41771215200424194}, {"id": 238, "seek": 159858, "start": 1606.58, "end": 1614.58, "text": " We still see that assistants sometimes need coaching on how to effectively prompt a language model.", "tokens": [50764, 492, 920, 536, 300, 34949, 2171, 643, 15818, 322, 577, 281, 8659, 12391, 257, 2856, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11788857403923483, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.41771215200424194}, {"id": 239, "seek": 159858, "start": 1614.58, "end": 1617.58, "text": " So that was my motivation coming into this little thing.", "tokens": [51164, 407, 300, 390, 452, 12335, 1348, 666, 341, 707, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11788857403923483, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.41771215200424194}, {"id": 240, "seek": 159858, "start": 1617.58, "end": 1623.58, "text": " I already had this react app, which is again, just the chat GPT like little app.", "tokens": [51314, 286, 1217, 632, 341, 4515, 724, 11, 597, 307, 797, 11, 445, 264, 5081, 26039, 51, 411, 707, 724, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11788857403923483, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.41771215200424194}, {"id": 241, "seek": 162358, "start": 1623.58, "end": 1628.58, "text": " And I wanted to add a module to it.", "tokens": [50364, 400, 286, 1415, 281, 909, 257, 10088, 281, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09062203582452268, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.3657556474208832}, {"id": 242, "seek": 162358, "start": 1628.58, "end": 1631.58, "text": " The module I wanted to add was a prompt coach.", "tokens": [50614, 440, 10088, 286, 1415, 281, 909, 390, 257, 12391, 6560, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09062203582452268, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.3657556474208832}, {"id": 243, "seek": 162358, "start": 1631.58, "end": 1646.58, "text": " So I wanted to take a put in another little layer or would look at what the assistant, the human assistant put into the chat app and send that through its own prompt to say, are you applying all the best practices?", "tokens": [50764, 407, 286, 1415, 281, 747, 257, 829, 294, 1071, 707, 4583, 420, 576, 574, 412, 437, 264, 10994, 11, 264, 1952, 10994, 829, 666, 264, 5081, 724, 293, 2845, 300, 807, 1080, 1065, 12391, 281, 584, 11, 366, 291, 9275, 439, 264, 1151, 7525, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09062203582452268, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.3657556474208832}, {"id": 244, "seek": 162358, "start": 1646.58, "end": 1649.58, "text": " Are you telling the AI like what role you wanted to play?", "tokens": [51514, 2014, 291, 3585, 264, 7318, 411, 437, 3090, 291, 1415, 281, 862, 30, 51664], "temperature": 0.0, "avg_logprob": -0.09062203582452268, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.3657556474208832}, {"id": 245, "seek": 162358, "start": 1649.58, "end": 1651.58, "text": " What job you wanted to do?", "tokens": [51664, 708, 1691, 291, 1415, 281, 360, 30, 51764], "temperature": 0.0, "avg_logprob": -0.09062203582452268, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.3657556474208832}, {"id": 246, "seek": 165158, "start": 1651.58, "end": 1656.58, "text": " Are you specifying a format that you want your response back in?", "tokens": [50364, 2014, 291, 1608, 5489, 257, 7877, 300, 291, 528, 428, 4134, 646, 294, 30, 50614], "temperature": 0.0, "avg_logprob": -0.0871099969615107, "compression_ratio": 1.7482517482517483, "no_speech_prob": 0.0757867619395256}, {"id": 247, "seek": 165158, "start": 1656.58, "end": 1668.58, "text": " Are you are you often these days will do it by default, but are you setting it up in such a way where it will do some sort of chain of thought, think out loud, think step by step reasoning before giving a final answer?", "tokens": [50614, 2014, 291, 366, 291, 2049, 613, 1708, 486, 360, 309, 538, 7576, 11, 457, 366, 291, 3287, 309, 493, 294, 1270, 257, 636, 689, 309, 486, 360, 512, 1333, 295, 5021, 295, 1194, 11, 519, 484, 6588, 11, 519, 1823, 538, 1823, 21577, 949, 2902, 257, 2572, 1867, 30, 51214], "temperature": 0.0, "avg_logprob": -0.0871099969615107, "compression_ratio": 1.7482517482517483, "no_speech_prob": 0.0757867619395256}, {"id": 248, "seek": 165158, "start": 1668.58, "end": 1680.58, "text": " That's actually one of the the most common things I see people do to shoot themselves in the foot with AI performance is prompt in such a way where it prevents the what is now the kind of trained in default behavior.", "tokens": [51214, 663, 311, 767, 472, 295, 264, 264, 881, 2689, 721, 286, 536, 561, 360, 281, 3076, 2969, 294, 264, 2671, 365, 7318, 3389, 307, 12391, 294, 1270, 257, 636, 689, 309, 22367, 264, 437, 307, 586, 264, 733, 295, 8895, 294, 7576, 5223, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0871099969615107, "compression_ratio": 1.7482517482517483, "no_speech_prob": 0.0757867619395256}, {"id": 249, "seek": 168058, "start": 1680.58, "end": 1685.58, "text": " Explain, analyze, think about it a little bit before getting to a final answer.", "tokens": [50364, 39574, 11, 12477, 11, 519, 466, 309, 257, 707, 857, 949, 1242, 281, 257, 2572, 1867, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11130939080164982, "compression_ratio": 1.6286764705882353, "no_speech_prob": 0.11266057193279266}, {"id": 250, "seek": 168058, "start": 1685.58, "end": 1687.58, "text": " So you just have a number of best practices.", "tokens": [50614, 407, 291, 445, 362, 257, 1230, 295, 1151, 7525, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11130939080164982, "compression_ratio": 1.6286764705882353, "no_speech_prob": 0.11266057193279266}, {"id": 251, "seek": 168058, "start": 1687.58, "end": 1689.58, "text": " Let me stop you there real quick.", "tokens": [50714, 961, 385, 1590, 291, 456, 957, 1702, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11130939080164982, "compression_ratio": 1.6286764705882353, "no_speech_prob": 0.11266057193279266}, {"id": 252, "seek": 168058, "start": 1689.58, "end": 1695.58, "text": " What are people doing that would prevent the model from doing the kind of chain of thought best practice that that makes it reason the best.", "tokens": [50814, 708, 366, 561, 884, 300, 576, 4871, 264, 2316, 490, 884, 264, 733, 295, 5021, 295, 1194, 1151, 3124, 300, 300, 1669, 309, 1778, 264, 1151, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11130939080164982, "compression_ratio": 1.6286764705882353, "no_speech_prob": 0.11266057193279266}, {"id": 253, "seek": 168058, "start": 1695.58, "end": 1706.58, "text": " Anything that just sets it up in such a way where it's got to answer immediately with no ability to scratch its way through the problem is bad.", "tokens": [51114, 11998, 300, 445, 6352, 309, 493, 294, 1270, 257, 636, 689, 309, 311, 658, 281, 1867, 4258, 365, 572, 3485, 281, 8459, 1080, 636, 807, 264, 1154, 307, 1578, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11130939080164982, "compression_ratio": 1.6286764705882353, "no_speech_prob": 0.11266057193279266}, {"id": 254, "seek": 170658, "start": 1706.58, "end": 1710.58, "text": " And I see that very often it's common.", "tokens": [50364, 400, 286, 536, 300, 588, 2049, 309, 311, 2689, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 255, "seek": 170658, "start": 1710.58, "end": 1714.58, "text": " It happens even in like academic publications, not infrequently.", "tokens": [50564, 467, 2314, 754, 294, 411, 7778, 25618, 11, 406, 1536, 265, 47519, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 256, "seek": 170658, "start": 1714.58, "end": 1715.58, "text": " Yeah.", "tokens": [50764, 865, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 257, "seek": 170658, "start": 1715.58, "end": 1719.58, "text": " Often that's a hangover from the earlier era of multi shot prompting.", "tokens": [50814, 20043, 300, 311, 257, 3967, 3570, 490, 264, 3071, 4249, 295, 4825, 3347, 12391, 278, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 258, "seek": 170658, "start": 1719.58, "end": 1721.58, "text": " And obviously this is all changing super quick, right?", "tokens": [51014, 400, 2745, 341, 307, 439, 4473, 1687, 1702, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 259, "seek": 170658, "start": 1721.58, "end": 1730.58, "text": " But if you go back, the first instruction model that hit the public was open AI's text of inchy 002 in January of 2022.", "tokens": [51114, 583, 498, 291, 352, 646, 11, 264, 700, 10951, 2316, 300, 2045, 264, 1908, 390, 1269, 7318, 311, 2487, 295, 7227, 88, 7143, 17, 294, 7061, 295, 20229, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15560346904553865, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.2278822958469391}, {"id": 260, "seek": 173058, "start": 1730.58, "end": 1740.58, "text": " So we're almost on two years, but still not even two years since you could first just tell the AI write me a haiku and it would attempt to write you a haiku.", "tokens": [50364, 407, 321, 434, 1920, 322, 732, 924, 11, 457, 920, 406, 754, 732, 924, 1670, 291, 727, 700, 445, 980, 264, 7318, 2464, 385, 257, 324, 24320, 293, 309, 576, 5217, 281, 2464, 291, 257, 324, 24320, 13, 50864], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 261, "seek": 173058, "start": 1740.58, "end": 1743.58, "text": " At that point, it was not necessarily going to get the syllables right.", "tokens": [50864, 1711, 300, 935, 11, 309, 390, 406, 4725, 516, 281, 483, 264, 45364, 558, 13, 51014], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 262, "seek": 173058, "start": 1743.58, "end": 1751.58, "text": " The earlier generations were, you would have to say a haiku by author name colon and then hope that it would continue the pattern.", "tokens": [51014, 440, 3071, 10593, 645, 11, 291, 576, 362, 281, 584, 257, 324, 24320, 538, 3793, 1315, 8255, 293, 550, 1454, 300, 309, 576, 2354, 264, 5102, 13, 51414], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 263, "seek": 173058, "start": 1751.58, "end": 1753.58, "text": " That's the classic prompting.", "tokens": [51414, 663, 311, 264, 7230, 12391, 278, 13, 51514], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 264, "seek": 173058, "start": 1753.58, "end": 1756.58, "text": " And with instructions, now you can tell it what you want to do.", "tokens": [51514, 400, 365, 9415, 11, 586, 291, 393, 980, 309, 437, 291, 528, 281, 360, 13, 51664], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 265, "seek": 173058, "start": 1756.58, "end": 1759.58, "text": " And obviously that's gotten better and better.", "tokens": [51664, 400, 2745, 300, 311, 5768, 1101, 293, 1101, 13, 51814], "temperature": 0.0, "avg_logprob": -0.145461181640625, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.3846875727176666}, {"id": 266, "seek": 175958, "start": 1759.58, "end": 1772.58, "text": " But in the benchmarking in an academic context that was developed before this instruction change, typically you would have like question, answer, question, answer, question, answer, question.", "tokens": [50364, 583, 294, 264, 18927, 278, 294, 364, 7778, 4319, 300, 390, 4743, 949, 341, 10951, 1319, 11, 5850, 291, 576, 362, 411, 1168, 11, 1867, 11, 1168, 11, 1867, 11, 1168, 11, 1867, 11, 1168, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10026341255265053, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.015895195305347443}, {"id": 267, "seek": 175958, "start": 1772.58, "end": 1774.58, "text": " And the AI's job would be to give you the answer.", "tokens": [51014, 400, 264, 7318, 311, 1691, 576, 312, 281, 976, 291, 264, 1867, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10026341255265053, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.015895195305347443}, {"id": 268, "seek": 175958, "start": 1774.58, "end": 1778.58, "text": " And so they would be measured off it on five shot prompts or what have you.", "tokens": [51114, 400, 370, 436, 576, 312, 12690, 766, 309, 322, 1732, 3347, 41095, 420, 437, 362, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10026341255265053, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.015895195305347443}, {"id": 269, "seek": 175958, "start": 1778.58, "end": 1784.58, "text": " But a lot of that stuff was all that scaffolding was built before people had even figured out chain of thought.", "tokens": [51314, 583, 257, 688, 295, 300, 1507, 390, 439, 300, 44094, 278, 390, 3094, 949, 561, 632, 754, 8932, 484, 5021, 295, 1194, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10026341255265053, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.015895195305347443}, {"id": 270, "seek": 178458, "start": 1784.58, "end": 1800.58, "text": " And so now if you take that exact structure and you bring it to a GPT-4, you're often much better off just giving it the single question with no structure, letting it spell out its reasoning because again, now it will do that by default and then give you an answer.", "tokens": [50364, 400, 370, 586, 498, 291, 747, 300, 1900, 3877, 293, 291, 1565, 309, 281, 257, 26039, 51, 12, 19, 11, 291, 434, 2049, 709, 1101, 766, 445, 2902, 309, 264, 2167, 1168, 365, 572, 3877, 11, 8295, 309, 9827, 484, 1080, 21577, 570, 797, 11, 586, 309, 486, 360, 300, 538, 7576, 293, 550, 976, 291, 364, 1867, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09798339155853772, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5536914467811584}, {"id": 271, "seek": 178458, "start": 1800.58, "end": 1810.58, "text": " Versus if you set up question, answer, question, answer, question, answer, it will respect to the implicit structure that you are establishing and it will jump straight to an answer.", "tokens": [51164, 12226, 301, 498, 291, 992, 493, 1168, 11, 1867, 11, 1168, 11, 1867, 11, 1168, 11, 1867, 11, 309, 486, 3104, 281, 264, 26947, 3877, 300, 291, 366, 22494, 293, 309, 486, 3012, 2997, 281, 364, 1867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09798339155853772, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5536914467811584}, {"id": 272, "seek": 178458, "start": 1810.58, "end": 1813.58, "text": " Often these are like multiple choice or they could be a number or what have you.", "tokens": [51664, 20043, 613, 366, 411, 3866, 3922, 420, 436, 727, 312, 257, 1230, 420, 437, 362, 291, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09798339155853772, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5536914467811584}, {"id": 273, "seek": 181358, "start": 1813.58, "end": 1819.58, "text": " It will jump to an answer, but the quality of the answer is much reduced compared to default behavior.", "tokens": [50364, 467, 486, 3012, 281, 364, 1867, 11, 457, 264, 3125, 295, 264, 1867, 307, 709, 9212, 5347, 281, 7576, 5223, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09571362476722867, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.05498393625020981}, {"id": 274, "seek": 181358, "start": 1819.58, "end": 1822.58, "text": " If you just let it think it think itself through it.", "tokens": [50664, 759, 291, 445, 718, 309, 519, 309, 519, 2564, 807, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09571362476722867, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.05498393625020981}, {"id": 275, "seek": 181358, "start": 1822.58, "end": 1824.58, "text": " And I've even seen this in Bard.", "tokens": [50814, 400, 286, 600, 754, 1612, 341, 294, 26841, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09571362476722867, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.05498393625020981}, {"id": 276, "seek": 181358, "start": 1824.58, "end": 1833.58, "text": " I think this is hopefully now fixed, but not too long ago, Bard would give you an answer before explanation by default.", "tokens": [50914, 286, 519, 341, 307, 4696, 586, 6806, 11, 457, 406, 886, 938, 2057, 11, 26841, 576, 976, 291, 364, 1867, 949, 10835, 538, 7576, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09571362476722867, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.05498393625020981}, {"id": 277, "seek": 181358, "start": 1833.58, "end": 1838.58, "text": " And again, that's just like you're going to have a problem so that sometimes people do that by mistake.", "tokens": [51364, 400, 797, 11, 300, 311, 445, 411, 291, 434, 516, 281, 362, 257, 1154, 370, 300, 2171, 561, 360, 300, 538, 6146, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09571362476722867, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.05498393625020981}, {"id": 278, "seek": 183858, "start": 1838.58, "end": 1841.58, "text": " They'll say give an answer and then explain your reasoning.", "tokens": [50364, 814, 603, 584, 976, 364, 1867, 293, 550, 2903, 428, 21577, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 279, "seek": 183858, "start": 1841.58, "end": 1843.58, "text": " You're just hurting yourself, right?", "tokens": [50514, 509, 434, 445, 17744, 1803, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 280, "seek": 183858, "start": 1843.58, "end": 1848.58, "text": " Because it will explain its reasoning for a wrong answer once the wrong answer is established.", "tokens": [50614, 1436, 309, 486, 2903, 1080, 21577, 337, 257, 2085, 1867, 1564, 264, 2085, 1867, 307, 7545, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 281, "seek": 183858, "start": 1848.58, "end": 1852.58, "text": " So AAA is my in the EA education.", "tokens": [50864, 407, 34347, 307, 452, 294, 264, 35747, 3309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 282, "seek": 183858, "start": 1852.58, "end": 1857.58, "text": " It's AAA for AAA results analysis before answer always.", "tokens": [51064, 467, 311, 34347, 337, 34347, 3542, 5215, 949, 1867, 1009, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 283, "seek": 183858, "start": 1857.58, "end": 1859.58, "text": " I never heard that before. I like that.", "tokens": [51314, 286, 1128, 2198, 300, 949, 13, 286, 411, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 284, "seek": 183858, "start": 1859.58, "end": 1862.58, "text": " It's hopefully hopefully they'll remember it coming out.", "tokens": [51414, 467, 311, 4696, 4696, 436, 603, 1604, 309, 1348, 484, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 285, "seek": 183858, "start": 1862.58, "end": 1863.58, "text": " No, I like it.", "tokens": [51564, 883, 11, 286, 411, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 286, "seek": 183858, "start": 1863.58, "end": 1866.58, "text": " Hey, we'll continue our interview in a moment after a word from our sponsors.", "tokens": [51614, 1911, 11, 321, 603, 2354, 527, 4049, 294, 257, 1623, 934, 257, 1349, 490, 527, 22593, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1390318437056108, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.550266683101654}, {"id": 287, "seek": 186658, "start": 1866.58, "end": 1873.58, "text": " If you're a startup founder or executive running a growing business, you know that as you scale your systems break down and the cracks start to show.", "tokens": [50364, 759, 291, 434, 257, 18578, 14917, 420, 10140, 2614, 257, 4194, 1606, 11, 291, 458, 300, 382, 291, 4373, 428, 3652, 1821, 760, 293, 264, 21770, 722, 281, 855, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 288, "seek": 186658, "start": 1873.58, "end": 1876.58, "text": " If this resonates with you, there are three numbers you need to know.", "tokens": [50714, 759, 341, 41051, 365, 291, 11, 456, 366, 1045, 3547, 291, 643, 281, 458, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 289, "seek": 186658, "start": 1876.58, "end": 1879.58, "text": " 36,000, 25, and 1.", "tokens": [50864, 8652, 11, 1360, 11, 3552, 11, 293, 502, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 290, "seek": 186658, "start": 1879.58, "end": 1880.58, "text": " 36,000.", "tokens": [51014, 8652, 11, 1360, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 291, "seek": 186658, "start": 1880.58, "end": 1883.58, "text": " That's the number of businesses which have upgraded to NetSuite by Oracle.", "tokens": [51064, 663, 311, 264, 1230, 295, 6011, 597, 362, 24133, 281, 6188, 50, 21681, 538, 25654, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 292, "seek": 186658, "start": 1883.58, "end": 1889.58, "text": " NetSuite is the number one cloud financial system, streamlined accounting, financial management, inventory, HR, and more.", "tokens": [51214, 6188, 50, 21681, 307, 264, 1230, 472, 4588, 4669, 1185, 11, 48155, 19163, 11, 4669, 4592, 11, 14228, 11, 19460, 11, 293, 544, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 293, "seek": 186658, "start": 1889.58, "end": 1890.58, "text": " 25.", "tokens": [51514, 3552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 294, "seek": 186658, "start": 1890.58, "end": 1892.58, "text": " NetSuite turns 25 this year.", "tokens": [51564, 6188, 50, 21681, 4523, 3552, 341, 1064, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07668427058628627, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.4293458163738251}, {"id": 295, "seek": 189258, "start": 1892.58, "end": 1898.58, "text": " That's 25 years of helping businesses do more with less, close their books in days, not weeks, and drive down costs.", "tokens": [50364, 663, 311, 3552, 924, 295, 4315, 6011, 360, 544, 365, 1570, 11, 1998, 641, 3642, 294, 1708, 11, 406, 3259, 11, 293, 3332, 760, 5497, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08579580570624126, "compression_ratio": 1.5835866261398177, "no_speech_prob": 0.11914056539535522}, {"id": 296, "seek": 189258, "start": 1898.58, "end": 1905.58, "text": " One, because your business is one of a kind, so you get a customized solution for all your KPIs in one efficient system with one source of truth.", "tokens": [50664, 1485, 11, 570, 428, 1606, 307, 472, 295, 257, 733, 11, 370, 291, 483, 257, 30581, 3827, 337, 439, 428, 41371, 6802, 294, 472, 7148, 1185, 365, 472, 4009, 295, 3494, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08579580570624126, "compression_ratio": 1.5835866261398177, "no_speech_prob": 0.11914056539535522}, {"id": 297, "seek": 189258, "start": 1905.58, "end": 1909.58, "text": " Manage risk, get reliable forecasts, and improve margins.", "tokens": [51014, 2458, 609, 3148, 11, 483, 12924, 49421, 11, 293, 3470, 30317, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08579580570624126, "compression_ratio": 1.5835866261398177, "no_speech_prob": 0.11914056539535522}, {"id": 298, "seek": 189258, "start": 1909.58, "end": 1911.58, "text": " Everything you need, all in one place.", "tokens": [51214, 5471, 291, 643, 11, 439, 294, 472, 1081, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08579580570624126, "compression_ratio": 1.5835866261398177, "no_speech_prob": 0.11914056539535522}, {"id": 299, "seek": 189258, "start": 1911.58, "end": 1920.58, "text": " Right now, download NetSuite's popular KPI checklist, designed to give you consistently excellent performance, absolutely free, and netsuite.com slash cognitive.", "tokens": [51314, 1779, 586, 11, 5484, 6188, 50, 21681, 311, 3743, 591, 31701, 30357, 11, 4761, 281, 976, 291, 14961, 7103, 3389, 11, 3122, 1737, 11, 293, 2533, 33136, 13, 1112, 17330, 15605, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08579580570624126, "compression_ratio": 1.5835866261398177, "no_speech_prob": 0.11914056539535522}, {"id": 300, "seek": 192058, "start": 1920.58, "end": 1924.58, "text": " That's netsuite.com slash cognitive to get your own KPI checklist.", "tokens": [50364, 663, 311, 2533, 33136, 13, 1112, 17330, 15605, 281, 483, 428, 1065, 591, 31701, 30357, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12007675918878294, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.03113524243235588}, {"id": 301, "seek": 192058, "start": 1924.58, "end": 1927.58, "text": " NetSuite.com slash cognitive.", "tokens": [50564, 6188, 50, 21681, 13, 1112, 17330, 15605, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12007675918878294, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.03113524243235588}, {"id": 302, "seek": 192058, "start": 1927.58, "end": 1937.58, "text": " Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work, customized across all platforms with a click of a button.", "tokens": [50714, 9757, 77, 1123, 4960, 1337, 1166, 7318, 281, 9528, 291, 281, 4025, 6779, 295, 5383, 295, 614, 36540, 300, 767, 589, 11, 30581, 2108, 439, 9473, 365, 257, 2052, 295, 257, 2960, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12007675918878294, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.03113524243235588}, {"id": 303, "seek": 192058, "start": 1937.58, "end": 1942.58, "text": " I believe in Omnike so much that I invested in it, and I recommend you use it too.", "tokens": [51214, 286, 1697, 294, 9757, 77, 1123, 370, 709, 300, 286, 13104, 294, 309, 11, 293, 286, 2748, 291, 764, 309, 886, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12007675918878294, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.03113524243235588}, {"id": 304, "seek": 192058, "start": 1942.58, "end": 1945.58, "text": " Use Kogrev to get a 10% discount.", "tokens": [51464, 8278, 591, 664, 40382, 281, 483, 257, 1266, 4, 11635, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12007675918878294, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.03113524243235588}, {"id": 305, "seek": 194558, "start": 1945.58, "end": 1965.58, "text": " And just to summarize, I think basically what you're saying is a previous generation of prompting really encouraged in your prompt to give multiple examples of the kind of question and answer kind of thing that you wanted the model to do and then set up the last example such that the next thing the model would do is give you a direct response.", "tokens": [50364, 400, 445, 281, 20858, 11, 286, 519, 1936, 437, 291, 434, 1566, 307, 257, 3894, 5125, 295, 12391, 278, 534, 14658, 294, 428, 12391, 281, 976, 3866, 5110, 295, 264, 733, 295, 1168, 293, 1867, 733, 295, 551, 300, 291, 1415, 264, 2316, 281, 360, 293, 550, 992, 493, 264, 1036, 1365, 1270, 300, 264, 958, 551, 264, 2316, 576, 360, 307, 976, 291, 257, 2047, 4134, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12966448730892605, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.4293324947357178}, {"id": 306, "seek": 196558, "start": 1965.58, "end": 1982.58, "text": " But what we found over time is one other really effective thing to do is rather than have the model give a direct response or direct answer to a question or a problem posed to it is letting the model quote unquote think out loud first by reasoning through the problem just like a human would do a word problem.", "tokens": [50364, 583, 437, 321, 1352, 670, 565, 307, 472, 661, 534, 4942, 551, 281, 360, 307, 2831, 813, 362, 264, 2316, 976, 257, 2047, 4134, 420, 2047, 1867, 281, 257, 1168, 420, 257, 1154, 31399, 281, 309, 307, 8295, 264, 2316, 6513, 37557, 519, 484, 6588, 700, 538, 21577, 807, 264, 1154, 445, 411, 257, 1952, 576, 360, 257, 1349, 1154, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09575572816452177, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.82547926902771}, {"id": 307, "seek": 196558, "start": 1982.58, "end": 1993.58, "text": " And then at the end of its response given answer improves the quality of the answer that you're giving and improves the quality of the result that you get from the model.", "tokens": [51214, 400, 550, 412, 264, 917, 295, 1080, 4134, 2212, 1867, 24771, 264, 3125, 295, 264, 1867, 300, 291, 434, 2902, 293, 24771, 264, 3125, 295, 264, 1874, 300, 291, 483, 490, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09575572816452177, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.82547926902771}, {"id": 308, "seek": 199358, "start": 1993.58, "end": 2001.58, "text": " And what has happened is OpenAI and other model providers have made that more of the default behavior so that you'll pretty much always do that.", "tokens": [50364, 400, 437, 575, 2011, 307, 7238, 48698, 293, 661, 2316, 11330, 362, 1027, 300, 544, 295, 264, 7576, 5223, 370, 300, 291, 603, 1238, 709, 1009, 360, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07588140066567954, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14391261339187622}, {"id": 309, "seek": 199358, "start": 2001.58, "end": 2011.58, "text": " But using previous prompting techniques, a few shot prompting or multi shot prompting where you're giving examples might lead it to just answer directly and you should look out for that and try to avoid that.", "tokens": [50764, 583, 1228, 3894, 12391, 278, 7512, 11, 257, 1326, 3347, 12391, 278, 420, 4825, 3347, 12391, 278, 689, 291, 434, 2902, 5110, 1062, 1477, 309, 281, 445, 1867, 3838, 293, 291, 820, 574, 484, 337, 300, 293, 853, 281, 5042, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07588140066567954, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.14391261339187622}, {"id": 310, "seek": 201158, "start": 2011.58, "end": 2023.58, "text": " Great summary. Yes. If it is jumping directly to an answer, you are for sure leaving performance on the table for all but maybe the most trivial tasks.", "tokens": [50364, 3769, 12691, 13, 1079, 13, 759, 309, 307, 11233, 3838, 281, 364, 1867, 11, 291, 366, 337, 988, 5012, 3389, 322, 264, 3199, 337, 439, 457, 1310, 264, 881, 26703, 9608, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1446286358879608, "compression_ratio": 1.58364312267658, "no_speech_prob": 0.8572538495063782}, {"id": 311, "seek": 201158, "start": 2023.58, "end": 2027.58, "text": " Yeah. And just an aside, see how much creative work is just summarizing.", "tokens": [50964, 865, 13, 400, 445, 364, 7359, 11, 536, 577, 709, 5880, 589, 307, 445, 14611, 3319, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1446286358879608, "compression_ratio": 1.58364312267658, "no_speech_prob": 0.8572538495063782}, {"id": 312, "seek": 201158, "start": 2027.58, "end": 2032.58, "text": " Important because I tend to give you the long version by default. That's my default behavior.", "tokens": [51164, 42908, 570, 286, 3928, 281, 976, 291, 264, 938, 3037, 538, 7576, 13, 663, 311, 452, 7576, 5223, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1446286358879608, "compression_ratio": 1.58364312267658, "no_speech_prob": 0.8572538495063782}, {"id": 313, "seek": 201158, "start": 2032.58, "end": 2037.58, "text": " This is one of those micro tasks that I'll just be handing off to an AI avatar version of me at some point.", "tokens": [51414, 639, 307, 472, 295, 729, 4532, 9608, 300, 286, 603, 445, 312, 34774, 766, 281, 364, 7318, 36205, 3037, 295, 385, 412, 512, 935, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1446286358879608, "compression_ratio": 1.58364312267658, "no_speech_prob": 0.8572538495063782}, {"id": 314, "seek": 203758, "start": 2037.58, "end": 2043.58, "text": " Okay, so let's get back to this. You're working on an app and you want to add a module to it that explains some prompting techniques.", "tokens": [50364, 1033, 11, 370, 718, 311, 483, 646, 281, 341, 13, 509, 434, 1364, 322, 364, 724, 293, 291, 528, 281, 909, 257, 10088, 281, 309, 300, 13948, 512, 12391, 278, 7512, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11051589867164349, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.2810695171356201}, {"id": 315, "seek": 203758, "start": 2043.58, "end": 2051.58, "text": " And it looks like the app itself is something that you didn't build from scratch and you're trying to get the lay of the land so you know what to do.", "tokens": [50664, 400, 309, 1542, 411, 264, 724, 2564, 307, 746, 300, 291, 994, 380, 1322, 490, 8459, 293, 291, 434, 1382, 281, 483, 264, 2360, 295, 264, 2117, 370, 291, 458, 437, 281, 360, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11051589867164349, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.2810695171356201}, {"id": 316, "seek": 203758, "start": 2051.58, "end": 2065.58, "text": " Exactly. Yeah. And the problem is, I know how to code generally, and I've even coded in JavaScript quite a bit, but React is a JavaScript framework that has a sort of hierarchy of best practices", "tokens": [51064, 7587, 13, 865, 13, 400, 264, 1154, 307, 11, 286, 458, 577, 281, 3089, 5101, 11, 293, 286, 600, 754, 34874, 294, 15778, 1596, 257, 857, 11, 457, 30644, 307, 257, 15778, 8388, 300, 575, 257, 1333, 295, 22333, 295, 1151, 7525, 51764], "temperature": 0.0, "avg_logprob": -0.11051589867164349, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.2810695171356201}, {"id": 317, "seek": 206558, "start": 2065.58, "end": 2070.58, "text": " that if you know them and you can easily apply them, then you can work quickly with the framework. That's the value of all these frameworks.", "tokens": [50364, 300, 498, 291, 458, 552, 293, 291, 393, 3612, 3079, 552, 11, 550, 291, 393, 589, 2661, 365, 264, 8388, 13, 663, 311, 264, 2158, 295, 439, 613, 29834, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09195230102539062, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.17309977114200592}, {"id": 318, "seek": 206558, "start": 2070.58, "end": 2078.58, "text": " But if you don't know them and you're coming in cold like I was, then where do I even go to where there's all these different folders and file structure", "tokens": [50614, 583, 498, 291, 500, 380, 458, 552, 293, 291, 434, 1348, 294, 3554, 411, 286, 390, 11, 550, 689, 360, 286, 754, 352, 281, 689, 456, 311, 439, 613, 819, 31082, 293, 3991, 3877, 51014], "temperature": 0.0, "avg_logprob": -0.09195230102539062, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.17309977114200592}, {"id": 319, "seek": 206558, "start": 2078.58, "end": 2084.58, "text": " and where exactly am I supposed to look for the kind of thing that I want to do and where do I put my new module.", "tokens": [51014, 293, 689, 2293, 669, 286, 3442, 281, 574, 337, 264, 733, 295, 551, 300, 286, 528, 281, 360, 293, 689, 360, 286, 829, 452, 777, 10088, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09195230102539062, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.17309977114200592}, {"id": 320, "seek": 206558, "start": 2084.58, "end": 2089.58, "text": " And so that's where this chat really starts. I have a working app. I have the code for the app.", "tokens": [51314, 400, 370, 300, 311, 689, 341, 5081, 534, 3719, 13, 286, 362, 257, 1364, 724, 13, 286, 362, 264, 3089, 337, 264, 724, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09195230102539062, "compression_ratio": 1.8224637681159421, "no_speech_prob": 0.17309977114200592}, {"id": 321, "seek": 208958, "start": 2089.58, "end": 2095.58, "text": " But I've never worked with a React app personally hands on before.", "tokens": [50364, 583, 286, 600, 1128, 2732, 365, 257, 30644, 724, 5665, 2377, 322, 949, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12671881455641526, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.32733798027038574}, {"id": 322, "seek": 208958, "start": 2095.58, "end": 2107.58, "text": " So I literally just set up the scenario. And I don't really use too much in the way of like custom instructions or like super elaborate prompts.", "tokens": [50664, 407, 286, 3736, 445, 992, 493, 264, 9005, 13, 400, 286, 500, 380, 534, 764, 886, 709, 294, 264, 636, 295, 411, 2375, 9415, 420, 411, 1687, 20945, 41095, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12671881455641526, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.32733798027038574}, {"id": 323, "seek": 208958, "start": 2107.58, "end": 2118.58, "text": " In my co-pilot mode work, certainly in delegation mode, then you get into a lot more detailed prompts with if this, then that, cases, structured formats, etc, etc.", "tokens": [51264, 682, 452, 598, 12, 79, 31516, 4391, 589, 11, 3297, 294, 36602, 4391, 11, 550, 291, 483, 666, 257, 688, 544, 9942, 41095, 365, 498, 341, 11, 550, 300, 11, 3331, 11, 18519, 25879, 11, 5183, 11, 5183, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12671881455641526, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.32733798027038574}, {"id": 324, "seek": 211858, "start": 2118.58, "end": 2123.58, "text": " But I often just find a pretty naive approach is effective for things like this.", "tokens": [50364, 583, 286, 2049, 445, 915, 257, 1238, 29052, 3109, 307, 4942, 337, 721, 411, 341, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08876039227868757, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.007814585231244564}, {"id": 325, "seek": 211858, "start": 2123.58, "end": 2128.58, "text": " And so I just start off by telling it, I'm working on this React app project and I am a bit lost.", "tokens": [50614, 400, 370, 286, 445, 722, 766, 538, 3585, 309, 11, 286, 478, 1364, 322, 341, 30644, 724, 1716, 293, 286, 669, 257, 857, 2731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08876039227868757, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.007814585231244564}, {"id": 326, "seek": 211858, "start": 2128.58, "end": 2137.58, "text": " Can you explain the structure of the app? I give it a little bit more information and it starts giving me a tutorial of what it is that I'm looking at.", "tokens": [50864, 1664, 291, 2903, 264, 3877, 295, 264, 724, 30, 286, 976, 309, 257, 707, 857, 544, 1589, 293, 309, 3719, 2902, 385, 257, 7073, 295, 437, 309, 307, 300, 286, 478, 1237, 412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08876039227868757, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.007814585231244564}, {"id": 327, "seek": 211858, "start": 2137.58, "end": 2145.58, "text": " And then you've got React and you've got Redux and then you've got these kind of additional slice JS toolkits and sagas and these frameworks", "tokens": [51314, 400, 550, 291, 600, 658, 30644, 293, 291, 600, 658, 4477, 2449, 293, 550, 291, 600, 658, 613, 733, 295, 4497, 13153, 33063, 2290, 74, 1208, 293, 15274, 296, 293, 613, 29834, 51714], "temperature": 0.0, "avg_logprob": -0.08876039227868757, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.007814585231244564}, {"id": 328, "seek": 214558, "start": 2145.58, "end": 2156.58, "text": " and in some cases take on a life of their own where there's whole conferences, right, and companies and it's like you can be very deep down this rabbit hole and whoever like built this open source project that I'm trying to modify.", "tokens": [50364, 293, 294, 512, 3331, 747, 322, 257, 993, 295, 641, 1065, 689, 456, 311, 1379, 22032, 11, 558, 11, 293, 3431, 293, 309, 311, 411, 291, 393, 312, 588, 2452, 760, 341, 19509, 5458, 293, 11387, 411, 3094, 341, 1269, 4009, 1716, 300, 286, 478, 1382, 281, 16927, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12278751404054704, "compression_ratio": 1.7129032258064516, "no_speech_prob": 0.3204624354839325}, {"id": 329, "seek": 214558, "start": 2156.58, "end": 2162.58, "text": " They're using a bunch of different things that are not even necessarily standard but are common or whatever.", "tokens": [50914, 814, 434, 1228, 257, 3840, 295, 819, 721, 300, 366, 406, 754, 4725, 3832, 457, 366, 2689, 420, 2035, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12278751404054704, "compression_ratio": 1.7129032258064516, "no_speech_prob": 0.3204624354839325}, {"id": 330, "seek": 214558, "start": 2162.58, "end": 2165.58, "text": " So just like five different things here that I have no idea about.", "tokens": [51214, 407, 445, 411, 1732, 819, 721, 510, 300, 286, 362, 572, 1558, 466, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12278751404054704, "compression_ratio": 1.7129032258064516, "no_speech_prob": 0.3204624354839325}, {"id": 331, "seek": 214558, "start": 2165.58, "end": 2174.58, "text": " And without this kind of tutorial, I'd be like going off to search for, okay, what is this saga JS? What does that even do?", "tokens": [51364, 400, 1553, 341, 733, 295, 7073, 11, 286, 1116, 312, 411, 516, 766, 281, 3164, 337, 11, 1392, 11, 437, 307, 341, 34250, 33063, 30, 708, 775, 300, 754, 360, 30, 51814], "temperature": 0.0, "avg_logprob": -0.12278751404054704, "compression_ratio": 1.7129032258064516, "no_speech_prob": 0.3204624354839325}, {"id": 332, "seek": 217458, "start": 2174.58, "end": 2178.58, "text": " It's able to give me that entire rundown extremely quickly.", "tokens": [50364, 467, 311, 1075, 281, 976, 385, 300, 2302, 23096, 648, 4664, 2661, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 333, "seek": 217458, "start": 2178.58, "end": 2187.58, "text": " And then this I thought was a really interesting moment because I get a lot of value from things like this where I feel like it's prompting me and it wasn't exactly that here.", "tokens": [50564, 400, 550, 341, 286, 1194, 390, 257, 534, 1880, 1623, 570, 286, 483, 257, 688, 295, 2158, 490, 721, 411, 341, 689, 286, 841, 411, 309, 311, 12391, 278, 385, 293, 309, 2067, 380, 2293, 300, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 334, "seek": 217458, "start": 2187.58, "end": 2190.58, "text": " But it gives me this general structure.", "tokens": [51014, 583, 309, 2709, 385, 341, 2674, 3877, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 335, "seek": 217458, "start": 2190.58, "end": 2191.58, "text": " Yeah.", "tokens": [51164, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 336, "seek": 217458, "start": 2191.58, "end": 2194.58, "text": " And then I was like, oh, I find it as a general pattern.", "tokens": [51214, 400, 550, 286, 390, 411, 11, 1954, 11, 286, 915, 309, 382, 257, 2674, 5102, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 337, "seek": 217458, "start": 2194.58, "end": 2202.58, "text": " If you can give it something in a format that it natively showed you, that's probably going to work pretty well.", "tokens": [51364, 759, 291, 393, 976, 309, 746, 294, 257, 7877, 300, 309, 8470, 356, 4712, 291, 11, 300, 311, 1391, 516, 281, 589, 1238, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08205206640835466, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.07581347227096558}, {"id": 338, "seek": 220258, "start": 2202.58, "end": 2208.58, "text": " So sometimes even in kind of the delegation mode, sometimes I'll be like, I don't exactly know what structure this should have.", "tokens": [50364, 407, 2171, 754, 294, 733, 295, 264, 36602, 4391, 11, 2171, 286, 603, 312, 411, 11, 286, 500, 380, 2293, 458, 437, 3877, 341, 820, 362, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08527924650806491, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02675190567970276}, {"id": 339, "seek": 220258, "start": 2208.58, "end": 2213.58, "text": " But maybe if I have it suggest the structure, then we'll get a structure that it can naturally work well with.", "tokens": [50664, 583, 1310, 498, 286, 362, 309, 3402, 264, 3877, 11, 550, 321, 603, 483, 257, 3877, 300, 309, 393, 8195, 589, 731, 365, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08527924650806491, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02675190567970276}, {"id": 340, "seek": 220258, "start": 2213.58, "end": 2222.58, "text": " In this case, the structure is like dictated by the world, but it's pretty well known that, okay, this is going to be your structure of a project in this react framework.", "tokens": [50914, 682, 341, 1389, 11, 264, 3877, 307, 411, 12569, 770, 538, 264, 1002, 11, 457, 309, 311, 1238, 731, 2570, 300, 11, 1392, 11, 341, 307, 516, 281, 312, 428, 3877, 295, 257, 1716, 294, 341, 4515, 8388, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08527924650806491, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02675190567970276}, {"id": 341, "seek": 220258, "start": 2222.58, "end": 2223.58, "text": " Cool.", "tokens": [51364, 8561, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08527924650806491, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02675190567970276}, {"id": 342, "seek": 220258, "start": 2223.58, "end": 2227.58, "text": " But this got me thinking, I should give it my actual structure.", "tokens": [51414, 583, 341, 658, 385, 1953, 11, 286, 820, 976, 309, 452, 3539, 3877, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08527924650806491, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.02675190567970276}, {"id": 343, "seek": 222758, "start": 2227.58, "end": 2232.58, "text": " Like I want to print this thing out for this project that I'm working on because I didn't make it.", "tokens": [50364, 1743, 286, 528, 281, 4482, 341, 551, 484, 337, 341, 1716, 300, 286, 478, 1364, 322, 570, 286, 994, 380, 652, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 344, "seek": 222758, "start": 2232.58, "end": 2233.58, "text": " I don't know what it is.", "tokens": [50614, 286, 500, 380, 458, 437, 309, 307, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 345, "seek": 222758, "start": 2233.58, "end": 2236.58, "text": " And I want to have it help me interpret that full thing.", "tokens": [50664, 400, 286, 528, 281, 362, 309, 854, 385, 7302, 300, 1577, 551, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 346, "seek": 222758, "start": 2236.58, "end": 2240.58, "text": " But then again, I'm like, how do I print something like this?", "tokens": [50814, 583, 550, 797, 11, 286, 478, 411, 11, 577, 360, 286, 4482, 746, 411, 341, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 347, "seek": 222758, "start": 2240.58, "end": 2241.58, "text": " I don't even know how to do that.", "tokens": [51014, 286, 500, 380, 754, 458, 577, 281, 360, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 348, "seek": 222758, "start": 2241.58, "end": 2248.58, "text": " So my next question for it is, can you write me the command to print out the file structure?", "tokens": [51064, 407, 452, 958, 1168, 337, 309, 307, 11, 393, 291, 2464, 385, 264, 5622, 281, 4482, 484, 264, 3991, 3877, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 349, "seek": 222758, "start": 2248.58, "end": 2251.58, "text": " And this is where you're like, okay, this is magic, right?", "tokens": [51414, 400, 341, 307, 689, 291, 434, 411, 11, 1392, 11, 341, 307, 5585, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 350, "seek": 222758, "start": 2251.58, "end": 2254.58, "text": " Because now again, I don't know how to do this, this tree command.", "tokens": [51564, 1436, 586, 797, 11, 286, 500, 380, 458, 577, 281, 360, 341, 11, 341, 4230, 5622, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07657223016443387, "compression_ratio": 1.8, "no_speech_prob": 0.43734341859817505}, {"id": 351, "seek": 225458, "start": 2254.58, "end": 2256.58, "text": " I don't know if it was installed for me or not.", "tokens": [50364, 286, 500, 380, 458, 498, 309, 390, 8899, 337, 385, 420, 406, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 352, "seek": 225458, "start": 2256.58, "end": 2258.58, "text": " But okay, it shows me how to do it.", "tokens": [50464, 583, 1392, 11, 309, 3110, 385, 577, 281, 360, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 353, "seek": 225458, "start": 2258.58, "end": 2264.58, "text": " And next thing, oh, there's another step here of installing some package that needed to be installed.", "tokens": [50564, 400, 958, 551, 11, 1954, 11, 456, 311, 1071, 1823, 510, 295, 20762, 512, 7372, 300, 2978, 281, 312, 8899, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 354, "seek": 225458, "start": 2264.58, "end": 2265.58, "text": " Okay, it was helping with that.", "tokens": [50864, 1033, 11, 309, 390, 4315, 365, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 355, "seek": 225458, "start": 2265.58, "end": 2267.58, "text": " So I'm just encountering all these.", "tokens": [50914, 407, 286, 478, 445, 8593, 278, 439, 613, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 356, "seek": 225458, "start": 2267.58, "end": 2269.58, "text": " This is the classic developer experience.", "tokens": [51014, 639, 307, 264, 7230, 10754, 1752, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 357, "seek": 225458, "start": 2269.58, "end": 2272.58, "text": " Conceptually, I have a clear idea of what I want to do.", "tokens": [51114, 47482, 671, 11, 286, 362, 257, 1850, 1558, 295, 437, 286, 528, 281, 360, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 358, "seek": 225458, "start": 2272.58, "end": 2280.58, "text": " But now I'm like three levels, three nested problems down here where I'm like, oh, okay, I need to understand this framework.", "tokens": [51264, 583, 586, 286, 478, 411, 1045, 4358, 11, 1045, 15646, 292, 2740, 760, 510, 689, 286, 478, 411, 11, 1954, 11, 1392, 11, 286, 643, 281, 1223, 341, 8388, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 359, "seek": 225458, "start": 2280.58, "end": 2281.58, "text": " Right.", "tokens": [51664, 1779, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10870504379272461, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.038453347980976105}, {"id": 360, "seek": 228158, "start": 2281.58, "end": 2282.58, "text": " Oh, okay.", "tokens": [50364, 876, 11, 1392, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 361, "seek": 228158, "start": 2282.58, "end": 2287.58, "text": " I need to print out the structure to better understand the version I'm working with in this framework.", "tokens": [50414, 286, 643, 281, 4482, 484, 264, 3877, 281, 1101, 1223, 264, 3037, 286, 478, 1364, 365, 294, 341, 8388, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 362, "seek": 228158, "start": 2287.58, "end": 2290.58, "text": " Oh, now I need to install something so I can do that print.", "tokens": [50664, 876, 11, 586, 286, 643, 281, 3625, 746, 370, 286, 393, 360, 300, 4482, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 363, "seek": 228158, "start": 2290.58, "end": 2292.58, "text": " And this is where people just time goes to die, right?", "tokens": [50814, 400, 341, 307, 689, 561, 445, 565, 1709, 281, 978, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 364, "seek": 228158, "start": 2292.58, "end": 2296.58, "text": " It's like, yeah, you talk to programmers and you're like, yeah, you didn't get anything done today on there.", "tokens": [50914, 467, 311, 411, 11, 1338, 11, 291, 751, 281, 41504, 293, 291, 434, 411, 11, 1338, 11, 291, 994, 380, 483, 1340, 1096, 965, 322, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 365, "seek": 228158, "start": 2296.58, "end": 2301.58, "text": " But what happened was I was on the way to the market to get my app together.", "tokens": [51114, 583, 437, 2011, 390, 286, 390, 322, 264, 636, 281, 264, 2142, 281, 483, 452, 724, 1214, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 366, "seek": 228158, "start": 2301.58, "end": 2304.58, "text": " And then I had to install this thing and then I couldn't install.", "tokens": [51364, 400, 550, 286, 632, 281, 3625, 341, 551, 293, 550, 286, 2809, 380, 3625, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 367, "seek": 228158, "start": 2304.58, "end": 2307.58, "text": " But each of these things, like it's helping me get over.", "tokens": [51514, 583, 1184, 295, 613, 721, 11, 411, 309, 311, 4315, 385, 483, 670, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 368, "seek": 228158, "start": 2307.58, "end": 2310.58, "text": " And now finally I'm able to say, okay, here is my app.", "tokens": [51664, 400, 586, 2721, 286, 478, 1075, 281, 584, 11, 1392, 11, 510, 307, 452, 724, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09386620109463915, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.0012446922482922673}, {"id": 369, "seek": 231058, "start": 2310.58, "end": 2313.58, "text": " This is the app that I actually am working with.", "tokens": [50364, 639, 307, 264, 724, 300, 286, 767, 669, 1364, 365, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 370, "seek": 231058, "start": 2313.58, "end": 2319.58, "text": " And now we're really getting into something good because it can now break that down.", "tokens": [50514, 400, 586, 321, 434, 534, 1242, 666, 746, 665, 570, 309, 393, 586, 1821, 300, 760, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 371, "seek": 231058, "start": 2319.58, "end": 2323.58, "text": " And the names of the things are like pretty semantic.", "tokens": [50814, 400, 264, 5288, 295, 264, 721, 366, 411, 1238, 47982, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 372, "seek": 231058, "start": 2323.58, "end": 2325.58, "text": " I noticed I haven't even given it any code here.", "tokens": [51014, 286, 5694, 286, 2378, 380, 754, 2212, 309, 604, 3089, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 373, "seek": 231058, "start": 2325.58, "end": 2331.58, "text": " I've just given it the file names, but the file names have a kind of an indication of what is what.", "tokens": [51114, 286, 600, 445, 2212, 309, 264, 3991, 5288, 11, 457, 264, 3991, 5288, 362, 257, 733, 295, 364, 18877, 295, 437, 307, 437, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 374, "seek": 231058, "start": 2331.58, "end": 2335.58, "text": " And it gets a sense just from that of what the app actually is.", "tokens": [51414, 400, 309, 2170, 257, 2020, 445, 490, 300, 295, 437, 264, 724, 767, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07495285914494441, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0236734077334404}, {"id": 375, "seek": 233558, "start": 2335.58, "end": 2340.58, "text": " So let's go over to, I think I just got a link to a working version of the app.", "tokens": [50364, 407, 718, 311, 352, 670, 281, 11, 286, 519, 286, 445, 658, 257, 2113, 281, 257, 1364, 3037, 295, 264, 724, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 376, "seek": 233558, "start": 2340.58, "end": 2341.58, "text": " It's pretty simple.", "tokens": [50614, 467, 311, 1238, 2199, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 377, "seek": 233558, "start": 2341.58, "end": 2343.58, "text": " It's a chat GPT like environment.", "tokens": [50664, 467, 311, 257, 5081, 26039, 51, 411, 2823, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 378, "seek": 233558, "start": 2343.58, "end": 2345.58, "text": " We can create these client profiles.", "tokens": [50764, 492, 393, 1884, 613, 6423, 23693, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 379, "seek": 233558, "start": 2345.58, "end": 2346.58, "text": " We have our chats.", "tokens": [50864, 492, 362, 527, 38057, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 380, "seek": 233558, "start": 2346.58, "end": 2348.58, "text": " We have our history, a couple of different models.", "tokens": [50914, 492, 362, 527, 2503, 11, 257, 1916, 295, 819, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 381, "seek": 233558, "start": 2348.58, "end": 2354.58, "text": " And there's function calling in the background that connects the chat experience to the client profile.", "tokens": [51014, 400, 456, 311, 2445, 5141, 294, 264, 3678, 300, 16967, 264, 5081, 1752, 281, 264, 6423, 7964, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 382, "seek": 233558, "start": 2354.58, "end": 2360.58, "text": " And what I'm trying to add is a module in the lower right hand corner, which I'm actually not sure if this version has.", "tokens": [51314, 400, 437, 286, 478, 1382, 281, 909, 307, 257, 10088, 294, 264, 3126, 558, 1011, 4538, 11, 597, 286, 478, 767, 406, 988, 498, 341, 3037, 575, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07498427910533378, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.28760626912117004}, {"id": 383, "seek": 236058, "start": 2360.58, "end": 2367.58, "text": " But the point of it is to take my prompts, run them through this meta prompt as we discussed, and then show feedback warnings.", "tokens": [50364, 583, 264, 935, 295, 309, 307, 281, 747, 452, 41095, 11, 1190, 552, 807, 341, 19616, 12391, 382, 321, 7152, 11, 293, 550, 855, 5824, 30009, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 384, "seek": 236058, "start": 2367.58, "end": 2370.58, "text": " Okay, you may or may not be doing this quite right.", "tokens": [50714, 1033, 11, 291, 815, 420, 815, 406, 312, 884, 341, 1596, 558, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 385, "seek": 236058, "start": 2370.58, "end": 2371.58, "text": " So back to the thing.", "tokens": [50864, 407, 646, 281, 264, 551, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 386, "seek": 236058, "start": 2371.58, "end": 2372.58, "text": " I've given the file structure.", "tokens": [50914, 286, 600, 2212, 264, 3991, 3877, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 387, "seek": 236058, "start": 2372.58, "end": 2375.58, "text": " It's now able to understand the file structure.", "tokens": [50964, 467, 311, 586, 1075, 281, 1223, 264, 3991, 3877, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 388, "seek": 236058, "start": 2375.58, "end": 2378.58, "text": " And now I'm saying, okay, here's what I'm trying to do.", "tokens": [51114, 400, 586, 286, 478, 1566, 11, 1392, 11, 510, 311, 437, 286, 478, 1382, 281, 360, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 389, "seek": 236058, "start": 2378.58, "end": 2380.58, "text": " I'm trying to create this prompt coach.", "tokens": [51264, 286, 478, 1382, 281, 1884, 341, 12391, 6560, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 390, "seek": 236058, "start": 2380.58, "end": 2382.58, "text": " I forget exactly how I had approached this.", "tokens": [51364, 286, 2870, 2293, 577, 286, 632, 17247, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 391, "seek": 236058, "start": 2382.58, "end": 2386.58, "text": " Yeah, this is a different file.", "tokens": [51464, 865, 11, 341, 307, 257, 819, 3991, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 392, "seek": 236058, "start": 2386.58, "end": 2387.58, "text": " We see exactly what I'm doing here.", "tokens": [51664, 492, 536, 2293, 437, 286, 478, 884, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09242188669469235, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.16878698766231537}, {"id": 393, "seek": 238758, "start": 2387.58, "end": 2391.58, "text": " Seems like maybe you had some sample code or something you've written or.", "tokens": [50364, 22524, 411, 1310, 291, 632, 512, 6889, 3089, 420, 746, 291, 600, 3720, 420, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 394, "seek": 238758, "start": 2391.58, "end": 2392.58, "text": " Yeah, I did.", "tokens": [50564, 865, 11, 286, 630, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 395, "seek": 238758, "start": 2392.58, "end": 2395.58, "text": " I guess I took one stab at it myself and it didn't work.", "tokens": [50614, 286, 2041, 286, 1890, 472, 16343, 412, 309, 2059, 293, 309, 994, 380, 589, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 396, "seek": 238758, "start": 2395.58, "end": 2398.58, "text": " I see where I'm looking at the human version.", "tokens": [50764, 286, 536, 689, 286, 478, 1237, 412, 264, 1952, 3037, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 397, "seek": 238758, "start": 2398.58, "end": 2401.58, "text": " I was looking at the same file structure and I'm like, okay, I see that there's this module.", "tokens": [50914, 286, 390, 1237, 412, 264, 912, 3991, 3877, 293, 286, 478, 411, 11, 1392, 11, 286, 536, 300, 456, 311, 341, 10088, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 398, "seek": 238758, "start": 2401.58, "end": 2403.58, "text": " There's like a sidebar here.", "tokens": [51064, 821, 311, 411, 257, 1252, 5356, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 399, "seek": 238758, "start": 2403.58, "end": 2405.58, "text": " And as you see these names, right?", "tokens": [51164, 400, 382, 291, 536, 613, 5288, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 400, "seek": 238758, "start": 2405.58, "end": 2412.58, "text": " So you've got sidebar and search and there's going to be like chat history here somewhere chat.", "tokens": [51264, 407, 291, 600, 658, 1252, 5356, 293, 3164, 293, 456, 311, 516, 281, 312, 411, 5081, 2503, 510, 4079, 5081, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 401, "seek": 238758, "start": 2412.58, "end": 2416.58, "text": " I'm looking at this and I'm like, okay, I see all these different elements and I see all these things.", "tokens": [51614, 286, 478, 1237, 412, 341, 293, 286, 478, 411, 11, 1392, 11, 286, 536, 439, 613, 819, 4959, 293, 286, 536, 439, 613, 721, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1141657829284668, "compression_ratio": 1.8989547038327526, "no_speech_prob": 0.008845411241054535}, {"id": 402, "seek": 241658, "start": 2416.58, "end": 2420.58, "text": " Let me just try to copy one and mess with it a little bit and hopefully get somewhere.", "tokens": [50364, 961, 385, 445, 853, 281, 5055, 472, 293, 2082, 365, 309, 257, 707, 857, 293, 4696, 483, 4079, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 403, "seek": 241658, "start": 2420.58, "end": 2421.58, "text": " Right.", "tokens": [50564, 1779, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 404, "seek": 241658, "start": 2421.58, "end": 2422.58, "text": " And then I'm not getting anywhere.", "tokens": [50614, 400, 550, 286, 478, 406, 1242, 4992, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 405, "seek": 241658, "start": 2422.58, "end": 2424.58, "text": " It's not showing up where I want to show up.", "tokens": [50664, 467, 311, 406, 4099, 493, 689, 286, 528, 281, 855, 493, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 406, "seek": 241658, "start": 2424.58, "end": 2425.58, "text": " I'm not seeing it.", "tokens": [50764, 286, 478, 406, 2577, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 407, "seek": 241658, "start": 2425.58, "end": 2428.58, "text": " And so that's where I come to say, okay, now here's what I tried.", "tokens": [50814, 400, 370, 300, 311, 689, 286, 808, 281, 584, 11, 1392, 11, 586, 510, 311, 437, 286, 3031, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 408, "seek": 241658, "start": 2428.58, "end": 2429.58, "text": " Why isn't it working?", "tokens": [50964, 1545, 1943, 380, 309, 1364, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 409, "seek": 241658, "start": 2429.58, "end": 2430.58, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 410, "seek": 241658, "start": 2430.58, "end": 2431.58, "text": " And I explain my problem here at the end.", "tokens": [51064, 400, 286, 2903, 452, 1154, 510, 412, 264, 917, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 411, "seek": 241658, "start": 2431.58, "end": 2435.58, "text": " The problem I have is that it's being shown in the wrong place.", "tokens": [51114, 440, 1154, 286, 362, 307, 300, 309, 311, 885, 4898, 294, 264, 2085, 1081, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 412, "seek": 241658, "start": 2435.58, "end": 2436.58, "text": " Right.", "tokens": [51314, 1779, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 413, "seek": 241658, "start": 2436.58, "end": 2437.58, "text": " So then it explains the answer.", "tokens": [51364, 407, 550, 309, 13948, 264, 1867, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 414, "seek": 241658, "start": 2437.58, "end": 2438.58, "text": " Yeah.", "tokens": [51414, 865, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 415, "seek": 241658, "start": 2438.58, "end": 2444.58, "text": " Next thing we're motto, it's giving me instructions with code, modify this, put it over here.", "tokens": [51464, 3087, 551, 321, 434, 32680, 11, 309, 311, 2902, 385, 9415, 365, 3089, 11, 16927, 341, 11, 829, 309, 670, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 416, "seek": 241658, "start": 2444.58, "end": 2445.58, "text": " This is pretty cool too.", "tokens": [51764, 639, 307, 1238, 1627, 886, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09898488661822151, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.014954591169953346}, {"id": 417, "seek": 244558, "start": 2445.58, "end": 2447.58, "text": " Unfortunately, we can't share the old screenshots.", "tokens": [50364, 8590, 11, 321, 393, 380, 2073, 264, 1331, 40661, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 418, "seek": 244558, "start": 2447.58, "end": 2454.58, "text": " I don't know exactly what I used, but this is right as vision was being introduced to chat GPT as well.", "tokens": [50464, 286, 500, 380, 458, 2293, 437, 286, 1143, 11, 457, 341, 307, 558, 382, 5201, 390, 885, 7268, 281, 5081, 26039, 51, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 419, "seek": 244558, "start": 2454.58, "end": 2458.58, "text": " So I was able to then say, here's my screenshot.", "tokens": [50814, 407, 286, 390, 1075, 281, 550, 584, 11, 510, 311, 452, 27712, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 420, "seek": 244558, "start": 2458.58, "end": 2461.58, "text": " Here's where it is showing up and here's where I want it to show up.", "tokens": [51014, 1692, 311, 689, 309, 307, 4099, 493, 293, 510, 311, 689, 286, 528, 309, 281, 855, 493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 421, "seek": 244558, "start": 2461.58, "end": 2462.58, "text": " Right.", "tokens": [51164, 1779, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 422, "seek": 244558, "start": 2462.58, "end": 2464.58, "text": " And can you help me with that as well?", "tokens": [51214, 400, 393, 291, 854, 385, 365, 300, 382, 731, 30, 51314], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 423, "seek": 244558, "start": 2464.58, "end": 2470.58, "text": " So from the screenshots, from the HTML structure, basically we just work through this entire thing.", "tokens": [51314, 407, 490, 264, 40661, 11, 490, 264, 17995, 3877, 11, 1936, 321, 445, 589, 807, 341, 2302, 551, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09236021801433732, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010322943329811096}, {"id": 424, "seek": 247058, "start": 2470.58, "end": 2477.58, "text": " I continue to run into issues where only 25% of the way through this whole thing.", "tokens": [50364, 286, 2354, 281, 1190, 666, 2663, 689, 787, 3552, 4, 295, 264, 636, 807, 341, 1379, 551, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 425, "seek": 247058, "start": 2477.58, "end": 2478.58, "text": " Oh, wow.", "tokens": [50714, 876, 11, 6076, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 426, "seek": 247058, "start": 2478.58, "end": 2483.58, "text": " This probably took me, I don't know, two to three hours total to get these suggestions,", "tokens": [50764, 639, 1391, 1890, 385, 11, 286, 500, 380, 458, 11, 732, 281, 1045, 2496, 3217, 281, 483, 613, 13396, 11, 51014], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 427, "seek": 247058, "start": 2483.58, "end": 2485.58, "text": " implement them, see what's going wrong.", "tokens": [51014, 4445, 552, 11, 536, 437, 311, 516, 2085, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 428, "seek": 247058, "start": 2485.58, "end": 2486.58, "text": " Yada, yada, yada.", "tokens": [51114, 398, 1538, 11, 288, 1538, 11, 288, 1538, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 429, "seek": 247058, "start": 2486.58, "end": 2488.58, "text": " It writes all the code basically.", "tokens": [51164, 467, 13657, 439, 264, 3089, 1936, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 430, "seek": 247058, "start": 2488.58, "end": 2489.58, "text": " Right.", "tokens": [51264, 1779, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 431, "seek": 247058, "start": 2489.58, "end": 2492.58, "text": " Because again, I've never written a line of React code in my life.", "tokens": [51314, 1436, 797, 11, 286, 600, 1128, 3720, 257, 1622, 295, 30644, 3089, 294, 452, 993, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 432, "seek": 247058, "start": 2492.58, "end": 2497.58, "text": " So I don't know any of this syntax, there's a million ways to get it not quite right when", "tokens": [51464, 407, 286, 500, 380, 458, 604, 295, 341, 28431, 11, 456, 311, 257, 2459, 2098, 281, 483, 309, 406, 1596, 558, 562, 51714], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 433, "seek": 247058, "start": 2497.58, "end": 2499.58, "text": " you have no idea what you're doing anyway.", "tokens": [51714, 291, 362, 572, 1558, 437, 291, 434, 884, 4033, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08677690492259513, "compression_ratio": 1.59, "no_speech_prob": 0.24490587413311005}, {"id": 434, "seek": 249958, "start": 2499.58, "end": 2504.58, "text": " And so it's writing all the code and just bit by bit where we're finding the experience,", "tokens": [50364, 400, 370, 309, 311, 3579, 439, 264, 3089, 293, 445, 857, 538, 857, 689, 321, 434, 5006, 264, 1752, 11, 50614], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 435, "seek": 249958, "start": 2504.58, "end": 2507.58, "text": " we're finding the interface, here we're creating some CSS.", "tokens": [50614, 321, 434, 5006, 264, 9226, 11, 510, 321, 434, 4084, 512, 24387, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 436, "seek": 249958, "start": 2507.58, "end": 2511.58, "text": " We're using, we have a particular style pack that's already built into this.", "tokens": [50764, 492, 434, 1228, 11, 321, 362, 257, 1729, 3758, 2844, 300, 311, 1217, 3094, 666, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 437, "seek": 249958, "start": 2511.58, "end": 2514.58, "text": " So again, that's just another thing I'm not at all familiar with.", "tokens": [50964, 407, 797, 11, 300, 311, 445, 1071, 551, 286, 478, 406, 412, 439, 4963, 365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 438, "seek": 249958, "start": 2514.58, "end": 2519.58, "text": " You know, this is the syntax for figuring out how to use that style pack.", "tokens": [51114, 509, 458, 11, 341, 307, 264, 28431, 337, 15213, 484, 577, 281, 764, 300, 3758, 2844, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 439, "seek": 249958, "start": 2519.58, "end": 2521.58, "text": " Good luck making that up on your own.", "tokens": [51364, 2205, 3668, 1455, 300, 493, 322, 428, 1065, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 440, "seek": 249958, "start": 2521.58, "end": 2526.58, "text": " And then we go basically after a couple of hours, I got to a working module where the", "tokens": [51464, 400, 550, 321, 352, 1936, 934, 257, 1916, 295, 2496, 11, 286, 658, 281, 257, 1364, 10088, 689, 264, 51714], "temperature": 0.0, "avg_logprob": -0.07903292047695851, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.003272121772170067}, {"id": 441, "seek": 252658, "start": 2526.58, "end": 2533.58, "text": " prompt coach, you know, would intercept your call, do the meta prompt, parse the response,", "tokens": [50364, 12391, 6560, 11, 291, 458, 11, 576, 24700, 428, 818, 11, 360, 264, 19616, 12391, 11, 48377, 264, 4134, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 442, "seek": 252658, "start": 2533.58, "end": 2538.58, "text": " identify, I had it giving the suggestions and the urgency of the suggestions.", "tokens": [50714, 5876, 11, 286, 632, 309, 2902, 264, 13396, 293, 264, 29734, 295, 264, 13396, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 443, "seek": 252658, "start": 2538.58, "end": 2541.58, "text": " So we're color coding those suggestions as they come up.", "tokens": [50964, 407, 321, 434, 2017, 17720, 729, 13396, 382, 436, 808, 493, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 444, "seek": 252658, "start": 2541.58, "end": 2543.58, "text": " If it's serious, then you get it in red.", "tokens": [51114, 759, 309, 311, 3156, 11, 550, 291, 483, 309, 294, 2182, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 445, "seek": 252658, "start": 2543.58, "end": 2546.58, "text": " And if it's not, you can get it in yellow or just a notice.", "tokens": [51214, 400, 498, 309, 311, 406, 11, 291, 393, 483, 309, 294, 5566, 420, 445, 257, 3449, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 446, "seek": 252658, "start": 2546.58, "end": 2550.58, "text": " And I would have, I would guess that this would have taken me easily order of magnitude", "tokens": [51364, 400, 286, 576, 362, 11, 286, 576, 2041, 300, 341, 576, 362, 2726, 385, 3612, 1668, 295, 15668, 51564], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 447, "seek": 252658, "start": 2550.58, "end": 2554.58, "text": " longer in a pre chat GPT here.", "tokens": [51564, 2854, 294, 257, 659, 5081, 26039, 51, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11647217213606634, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.01691027358174324}, {"id": 448, "seek": 255458, "start": 2554.58, "end": 2560.58, "text": " If this was two to three hours, it's probably two to three days of work to figure out all this stuff.", "tokens": [50364, 759, 341, 390, 732, 281, 1045, 2496, 11, 309, 311, 1391, 732, 281, 1045, 1708, 295, 589, 281, 2573, 484, 439, 341, 1507, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 449, "seek": 255458, "start": 2560.58, "end": 2566.58, "text": " And I know a lot more frustration that is because I'm not a super patient person.", "tokens": [50664, 400, 286, 458, 257, 688, 544, 20491, 300, 307, 570, 286, 478, 406, 257, 1687, 4537, 954, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 450, "seek": 255458, "start": 2566.58, "end": 2571.58, "text": " The feeling of a million people have done something almost exactly like this.", "tokens": [50964, 440, 2633, 295, 257, 2459, 561, 362, 1096, 746, 1920, 2293, 411, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 451, "seek": 255458, "start": 2571.58, "end": 2574.58, "text": " There's nothing differentiated or special about what I'm doing.", "tokens": [51214, 821, 311, 1825, 27372, 770, 420, 2121, 466, 437, 286, 478, 884, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 452, "seek": 255458, "start": 2574.58, "end": 2580.58, "text": " I'm just like in this phase of kind of not knowing what I'm doing and just getting constantly stuck,", "tokens": [51364, 286, 478, 445, 411, 294, 341, 5574, 295, 733, 295, 406, 5276, 437, 286, 478, 884, 293, 445, 1242, 6460, 5541, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 453, "seek": 255458, "start": 2580.58, "end": 2583.58, "text": " constantly stumbling, constantly running into friction.", "tokens": [51664, 6460, 342, 14188, 11, 6460, 2614, 666, 17710, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06366961044177674, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.012818166986107826}, {"id": 454, "seek": 258358, "start": 2583.58, "end": 2584.58, "text": " I really don't enjoy that.", "tokens": [50364, 286, 534, 500, 380, 2103, 300, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 455, "seek": 258358, "start": 2584.58, "end": 2586.58, "text": " I think most people don't.", "tokens": [50414, 286, 519, 881, 561, 500, 380, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 456, "seek": 258358, "start": 2586.58, "end": 2589.58, "text": " This is none of that or almost none of it, right?", "tokens": [50514, 639, 307, 6022, 295, 300, 420, 1920, 6022, 295, 309, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 457, "seek": 258358, "start": 2589.58, "end": 2591.58, "text": " Even just that going back to the install, right?", "tokens": [50664, 2754, 445, 300, 516, 646, 281, 264, 3625, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 458, "seek": 258358, "start": 2591.58, "end": 2593.58, "text": " Or the command to print out the structure.", "tokens": [50764, 1610, 264, 5622, 281, 4482, 484, 264, 3877, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 459, "seek": 258358, "start": 2593.58, "end": 2594.58, "text": " Man, this is so stupid.", "tokens": [50864, 2458, 11, 341, 307, 370, 6631, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 460, "seek": 258358, "start": 2594.58, "end": 2596.58, "text": " I know exactly what I want.", "tokens": [50914, 286, 458, 2293, 437, 286, 528, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 461, "seek": 258358, "start": 2596.58, "end": 2598.58, "text": " I know that it is doable.", "tokens": [51014, 286, 458, 300, 309, 307, 41183, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 462, "seek": 258358, "start": 2598.58, "end": 2601.58, "text": " I know that it's been done a million times, a million places.", "tokens": [51114, 286, 458, 300, 309, 311, 668, 1096, 257, 2459, 1413, 11, 257, 2459, 3190, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 463, "seek": 258358, "start": 2601.58, "end": 2602.58, "text": " And yet I don't know how to do it.", "tokens": [51264, 400, 1939, 286, 500, 380, 458, 577, 281, 360, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 464, "seek": 258358, "start": 2602.58, "end": 2609.58, "text": " And then liberating me from that frustration is, it turns out, it would go to your drudgery point, right?", "tokens": [51314, 400, 550, 6774, 990, 385, 490, 300, 20491, 307, 11, 309, 4523, 484, 11, 309, 576, 352, 281, 428, 1224, 532, 7337, 935, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.12573268678453234, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0029803160578012466}, {"id": 465, "seek": 260958, "start": 2609.58, "end": 2615.58, "text": " That was probably 80 to 90% of the time in a world where I was doing this on my own.", "tokens": [50364, 663, 390, 1391, 4688, 281, 4289, 4, 295, 264, 565, 294, 257, 1002, 689, 286, 390, 884, 341, 322, 452, 1065, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 466, "seek": 260958, "start": 2615.58, "end": 2620.58, "text": " And now we're down to the two to three hours where it was really about defining what I want.", "tokens": [50664, 400, 586, 321, 434, 760, 281, 264, 732, 281, 1045, 2496, 689, 309, 390, 534, 466, 17827, 437, 286, 528, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 467, "seek": 260958, "start": 2620.58, "end": 2623.58, "text": " This could have been one hour if I really knew React,", "tokens": [50914, 639, 727, 362, 668, 472, 1773, 498, 286, 534, 2586, 30644, 11, 51064], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 468, "seek": 260958, "start": 2623.58, "end": 2628.58, "text": " but it taught me the ropes and did the task in probably again,", "tokens": [51064, 457, 309, 5928, 385, 264, 32964, 293, 630, 264, 5633, 294, 1391, 797, 11, 51314], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 469, "seek": 260958, "start": 2628.58, "end": 2633.58, "text": " 80 to 90% time savings compared to the unassisted version.", "tokens": [51314, 4688, 281, 4289, 4, 565, 13454, 5347, 281, 264, 517, 640, 33250, 3037, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 470, "seek": 260958, "start": 2633.58, "end": 2634.58, "text": " I love this.", "tokens": [51564, 286, 959, 341, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 471, "seek": 260958, "start": 2634.58, "end": 2636.58, "text": " I think this is such a cool example.", "tokens": [51614, 286, 519, 341, 307, 1270, 257, 1627, 1365, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 472, "seek": 260958, "start": 2636.58, "end": 2638.58, "text": " I really appreciate you bringing this.", "tokens": [51714, 286, 534, 4449, 291, 5062, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06357060582184594, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.034086864441633224}, {"id": 473, "seek": 263858, "start": 2638.58, "end": 2643.58, "text": " One, yeah, it's obvious that this kind of thing, which if you're not a programmer,", "tokens": [50364, 1485, 11, 1338, 11, 309, 311, 6322, 300, 341, 733, 295, 551, 11, 597, 498, 291, 434, 406, 257, 32116, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 474, "seek": 263858, "start": 2643.58, "end": 2647.58, "text": " like as a programmer looking at this, I'm like, yeah, this is so much of what you do as a programmer,", "tokens": [50614, 411, 382, 257, 32116, 1237, 412, 341, 11, 286, 478, 411, 11, 1338, 11, 341, 307, 370, 709, 295, 437, 291, 360, 382, 257, 32116, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 475, "seek": 263858, "start": 2647.58, "end": 2651.58, "text": " especially if you're a programmer like working on startup stuff is like this kind of thing.", "tokens": [50814, 2318, 498, 291, 434, 257, 32116, 411, 1364, 322, 18578, 1507, 307, 411, 341, 733, 295, 551, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 476, "seek": 263858, "start": 2651.58, "end": 2652.58, "text": " It's like, this is doable.", "tokens": [51014, 467, 311, 411, 11, 341, 307, 41183, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 477, "seek": 263858, "start": 2652.58, "end": 2653.58, "text": " It's been achieved before.", "tokens": [51064, 467, 311, 668, 11042, 949, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 478, "seek": 263858, "start": 2653.58, "end": 2656.58, "text": " I just need to do it in this, in my specific context.", "tokens": [51114, 286, 445, 643, 281, 360, 309, 294, 341, 11, 294, 452, 2685, 4319, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 479, "seek": 263858, "start": 2656.58, "end": 2662.58, "text": " And it's obvious that this would have taken you days or taken really anyone days to do from scratch,", "tokens": [51264, 400, 309, 311, 6322, 300, 341, 576, 362, 2726, 291, 1708, 420, 2726, 534, 2878, 1708, 281, 360, 490, 8459, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 480, "seek": 263858, "start": 2662.58, "end": 2667.58, "text": " but with chat GBT, it makes it like the way quicker and takes away a lot of the drudgery.", "tokens": [51564, 457, 365, 5081, 26809, 51, 11, 309, 1669, 309, 411, 264, 636, 16255, 293, 2516, 1314, 257, 688, 295, 264, 1224, 532, 7337, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09279726101801945, "compression_ratio": 1.8790849673202614, "no_speech_prob": 0.03112035244703293}, {"id": 481, "seek": 266758, "start": 2667.58, "end": 2671.58, "text": " But I think what's really cool and really beautiful, which is like weird to say about this stuff.", "tokens": [50364, 583, 286, 519, 437, 311, 534, 1627, 293, 534, 2238, 11, 597, 307, 411, 3657, 281, 584, 466, 341, 1507, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 482, "seek": 266758, "start": 2671.58, "end": 2680.58, "text": " It's striking me right now is there's this dance happening in this chat where at the beginning,", "tokens": [50564, 467, 311, 18559, 385, 558, 586, 307, 456, 311, 341, 4489, 2737, 294, 341, 5081, 689, 412, 264, 2863, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 483, "seek": 266758, "start": 2680.58, "end": 2683.58, "text": " obviously you're asking it for to help you.", "tokens": [51014, 2745, 291, 434, 3365, 309, 337, 281, 854, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 484, "seek": 266758, "start": 2683.58, "end": 2690.58, "text": " But you are giving it what it needs and filling in the gaps that it needs in order to like help you", "tokens": [51164, 583, 291, 366, 2902, 309, 437, 309, 2203, 293, 10623, 294, 264, 15031, 300, 309, 2203, 294, 1668, 281, 411, 854, 291, 51514], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 485, "seek": 266758, "start": 2690.58, "end": 2692.58, "text": " and it is filling in the gaps for you as well.", "tokens": [51514, 293, 309, 307, 10623, 294, 264, 15031, 337, 291, 382, 731, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 486, "seek": 266758, "start": 2692.58, "end": 2696.58, "text": " So it is explaining react to you, but you are explaining.", "tokens": [51614, 407, 309, 307, 13468, 4515, 281, 291, 11, 457, 291, 366, 13468, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10415461233683995, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0022514844313263893}, {"id": 487, "seek": 269658, "start": 2696.58, "end": 2700.58, "text": " Here is the project that I have and here are the specific details that I want done.", "tokens": [50364, 1692, 307, 264, 1716, 300, 286, 362, 293, 510, 366, 264, 2685, 4365, 300, 286, 528, 1096, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07945729019050311, "compression_ratio": 1.967032967032967, "no_speech_prob": 0.0169055312871933}, {"id": 488, "seek": 269658, "start": 2700.58, "end": 2707.58, "text": " And then there's this like dance back and forth where you're mutually filling in gaps that both of you can't on your own fill in.", "tokens": [50564, 400, 550, 456, 311, 341, 411, 4489, 646, 293, 5220, 689, 291, 434, 39144, 10623, 294, 15031, 300, 1293, 295, 291, 393, 380, 322, 428, 1065, 2836, 294, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07945729019050311, "compression_ratio": 1.967032967032967, "no_speech_prob": 0.0169055312871933}, {"id": 489, "seek": 269658, "start": 2707.58, "end": 2715.58, "text": " And I think that is like really cool to just watch that evolve where at the start you don't know react", "tokens": [50914, 400, 286, 519, 300, 307, 411, 534, 1627, 281, 445, 1159, 300, 16693, 689, 412, 264, 722, 291, 500, 380, 458, 4515, 51314], "temperature": 0.0, "avg_logprob": -0.07945729019050311, "compression_ratio": 1.967032967032967, "no_speech_prob": 0.0169055312871933}, {"id": 490, "seek": 269658, "start": 2715.58, "end": 2719.58, "text": " and you don't know like where to where to put your code and you don't know why it's not working.", "tokens": [51314, 293, 291, 500, 380, 458, 411, 689, 281, 689, 281, 829, 428, 3089, 293, 291, 500, 380, 458, 983, 309, 311, 406, 1364, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07945729019050311, "compression_ratio": 1.967032967032967, "no_speech_prob": 0.0169055312871933}, {"id": 491, "seek": 269658, "start": 2719.58, "end": 2725.58, "text": " And at the start, it doesn't know who you are or what you're trying to accomplish or what the specifics of your project is.", "tokens": [51514, 400, 412, 264, 722, 11, 309, 1177, 380, 458, 567, 291, 366, 420, 437, 291, 434, 1382, 281, 9021, 420, 437, 264, 28454, 295, 428, 1716, 307, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07945729019050311, "compression_ratio": 1.967032967032967, "no_speech_prob": 0.0169055312871933}, {"id": 492, "seek": 272558, "start": 2725.58, "end": 2730.58, "text": " But as you build up this chat, you yourself are starting to understand things more.", "tokens": [50364, 583, 382, 291, 1322, 493, 341, 5081, 11, 291, 1803, 366, 2891, 281, 1223, 721, 544, 13, 50614], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 493, "seek": 272558, "start": 2730.58, "end": 2732.58, "text": " Like you didn't ask it, just go do this for me.", "tokens": [50614, 1743, 291, 994, 380, 1029, 309, 11, 445, 352, 360, 341, 337, 385, 13, 50714], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 494, "seek": 272558, "start": 2732.58, "end": 2735.58, "text": " You asked how does a react project work and like what is the structure.", "tokens": [50714, 509, 2351, 577, 775, 257, 4515, 1716, 589, 293, 411, 437, 307, 264, 3877, 13, 50864], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 495, "seek": 272558, "start": 2735.58, "end": 2739.58, "text": " And so you learned more about react and it learned more about you.", "tokens": [50864, 400, 370, 291, 3264, 544, 466, 4515, 293, 309, 3264, 544, 466, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 496, "seek": 272558, "start": 2739.58, "end": 2744.58, "text": " And as your mutual understanding increased, you were both able to accomplish the thing together.", "tokens": [51064, 400, 382, 428, 16917, 3701, 6505, 11, 291, 645, 1293, 1075, 281, 9021, 264, 551, 1214, 13, 51314], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 497, "seek": 272558, "start": 2744.58, "end": 2746.58, "text": " And I think that's really cool.", "tokens": [51314, 400, 286, 519, 300, 311, 534, 1627, 13, 51414], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 498, "seek": 272558, "start": 2746.58, "end": 2748.58, "text": " Yeah, it's awesome.", "tokens": [51414, 865, 11, 309, 311, 3476, 13, 51514], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 499, "seek": 272558, "start": 2748.58, "end": 2751.58, "text": " The next generation, it's episodic.", "tokens": [51514, 440, 958, 5125, 11, 309, 311, 39200, 299, 13, 51664], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 500, "seek": 272558, "start": 2751.58, "end": 2754.58, "text": " We were still only halfway through this scroll for all the scrolling I've done.", "tokens": [51664, 492, 645, 920, 787, 15461, 807, 341, 11369, 337, 439, 264, 29053, 286, 600, 1096, 13, 51814], "temperature": 0.0, "avg_logprob": -0.087859546436983, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.0037063208874315023}, {"id": 501, "seek": 275458, "start": 2754.58, "end": 2756.58, "text": " I just highlighted this. Okay, cool.", "tokens": [50364, 286, 445, 17173, 341, 13, 1033, 11, 1627, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10407044471950706, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.01541577186435461}, {"id": 502, "seek": 275458, "start": 2756.58, "end": 2760.58, "text": " This is working because at this point I'm starting to get into like refinements.", "tokens": [50464, 639, 307, 1364, 570, 412, 341, 935, 286, 478, 2891, 281, 483, 666, 411, 44395, 6400, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10407044471950706, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.01541577186435461}, {"id": 503, "seek": 275458, "start": 2760.58, "end": 2766.58, "text": " Okay, now I want to dial in the styling and basically at this point, the core problems have been solved.", "tokens": [50664, 1033, 11, 586, 286, 528, 281, 5502, 294, 264, 27944, 293, 1936, 412, 341, 935, 11, 264, 4965, 2740, 362, 668, 13041, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10407044471950706, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.01541577186435461}, {"id": 504, "seek": 275458, "start": 2766.58, "end": 2772.58, "text": " And now again, it's just going to do the drudgery of making sure that there's padding and things are centered and so on and so forth.", "tokens": [50964, 400, 586, 797, 11, 309, 311, 445, 516, 281, 360, 264, 1224, 532, 7337, 295, 1455, 988, 300, 456, 311, 39562, 293, 721, 366, 18988, 293, 370, 322, 293, 370, 5220, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10407044471950706, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.01541577186435461}, {"id": 505, "seek": 275458, "start": 2772.58, "end": 2777.58, "text": " I try to be polite and encouraging to my AIs wherever I possibly can.", "tokens": [51264, 286, 853, 281, 312, 25171, 293, 14580, 281, 452, 316, 6802, 8660, 286, 6264, 393, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10407044471950706, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.01541577186435461}, {"id": 506, "seek": 277758, "start": 2777.58, "end": 2779.58, "text": " But you can envision a future.", "tokens": [50364, 583, 291, 393, 24739, 257, 2027, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1194406588052966, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.45302850008010864}, {"id": 507, "seek": 277758, "start": 2779.58, "end": 2788.58, "text": " And I think that future is already starting to become visible through the through the mist a little bit as more and more stuff gets published on the research side.", "tokens": [50464, 400, 286, 519, 300, 2027, 307, 1217, 2891, 281, 1813, 8974, 807, 264, 807, 264, 3544, 257, 707, 857, 382, 544, 293, 544, 1507, 2170, 6572, 322, 264, 2132, 1252, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1194406588052966, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.45302850008010864}, {"id": 508, "seek": 277758, "start": 2788.58, "end": 2795.58, "text": " Where this sort of episodic relationship where I started a new chat, it now knows nothing about this, right?", "tokens": [50914, 2305, 341, 1333, 295, 39200, 299, 2480, 689, 286, 1409, 257, 777, 5081, 11, 309, 586, 3255, 1825, 466, 341, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1194406588052966, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.45302850008010864}, {"id": 509, "seek": 277758, "start": 2795.58, "end": 2797.58, "text": " I can continue this chat up to a limit.", "tokens": [51264, 286, 393, 2354, 341, 5081, 493, 281, 257, 4948, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1194406588052966, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.45302850008010864}, {"id": 510, "seek": 277758, "start": 2797.58, "end": 2804.58, "text": " And obviously, superhuman expansive background knowledge, but zero contextual knowledge.", "tokens": [51364, 400, 2745, 11, 1687, 18796, 46949, 3678, 3601, 11, 457, 4018, 35526, 3601, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1194406588052966, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.45302850008010864}, {"id": 511, "seek": 280458, "start": 2804.58, "end": 2808.58, "text": " And we can't retain that from one episode to the next.", "tokens": [50364, 400, 321, 393, 380, 18340, 300, 490, 472, 3500, 281, 264, 958, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1138721572028266, "compression_ratio": 1.5822222222222222, "no_speech_prob": 0.03307081386446953}, {"id": 512, "seek": 280458, "start": 2808.58, "end": 2811.58, "text": " But I do think that is also coming soon too.", "tokens": [50564, 583, 286, 360, 519, 300, 307, 611, 1348, 2321, 886, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1138721572028266, "compression_ratio": 1.5822222222222222, "no_speech_prob": 0.03307081386446953}, {"id": 513, "seek": 280458, "start": 2811.58, "end": 2814.58, "text": " And there's a couple different ways it could shape up.", "tokens": [50714, 400, 456, 311, 257, 1916, 819, 2098, 309, 727, 3909, 493, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1138721572028266, "compression_ratio": 1.5822222222222222, "no_speech_prob": 0.03307081386446953}, {"id": 514, "seek": 280458, "start": 2814.58, "end": 2821.58, "text": " But I think we will, in a year, certainly not that much longer than that, I can't imagine.", "tokens": [50864, 583, 286, 519, 321, 486, 11, 294, 257, 1064, 11, 3297, 406, 300, 709, 2854, 813, 300, 11, 286, 393, 380, 3811, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1138721572028266, "compression_ratio": 1.5822222222222222, "no_speech_prob": 0.03307081386446953}, {"id": 515, "seek": 280458, "start": 2821.58, "end": 2829.58, "text": " Start to see things where all this history is accumulated or maybe divided into different threads or whatever.", "tokens": [51214, 6481, 281, 536, 721, 689, 439, 341, 2503, 307, 31346, 420, 1310, 6666, 666, 819, 19314, 420, 2035, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1138721572028266, "compression_ratio": 1.5822222222222222, "no_speech_prob": 0.03307081386446953}, {"id": 516, "seek": 282958, "start": 2829.58, "end": 2837.58, "text": " But where this kind of can follow you forward into different tasks as well in a history aware way, I think will be another level of unlock.", "tokens": [50364, 583, 689, 341, 733, 295, 393, 1524, 291, 2128, 666, 819, 9608, 382, 731, 294, 257, 2503, 3650, 636, 11, 286, 519, 486, 312, 1071, 1496, 295, 11634, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 517, "seek": 282958, "start": 2837.58, "end": 2838.58, "text": " I think you're totally right.", "tokens": [50764, 286, 519, 291, 434, 3879, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 518, "seek": 282958, "start": 2838.58, "end": 2840.58, "text": " That's what custom instructions is.", "tokens": [50814, 663, 311, 437, 2375, 9415, 307, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 519, "seek": 282958, "start": 2840.58, "end": 2841.58, "text": " It's a step in that direction.", "tokens": [50914, 467, 311, 257, 1823, 294, 300, 3513, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 520, "seek": 282958, "start": 2841.58, "end": 2844.58, "text": " Unfortunately, custom instructions is very hard to set up.", "tokens": [50964, 8590, 11, 2375, 9415, 307, 588, 1152, 281, 992, 493, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 521, "seek": 282958, "start": 2844.58, "end": 2846.58, "text": " But if you do set it up, it's really great.", "tokens": [51114, 583, 498, 291, 360, 992, 309, 493, 11, 309, 311, 534, 869, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 522, "seek": 282958, "start": 2846.58, "end": 2848.58, "text": " It's really nice for it to have context on you.", "tokens": [51214, 467, 311, 534, 1481, 337, 309, 281, 362, 4319, 322, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 523, "seek": 282958, "start": 2848.58, "end": 2849.58, "text": " But I do think you're right.", "tokens": [51314, 583, 286, 360, 519, 291, 434, 558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 524, "seek": 282958, "start": 2849.58, "end": 2855.58, "text": " ChatGPT will definitely have a memory that it can reference this stuff and reference the context of what you need and who you are.", "tokens": [51364, 27503, 38, 47, 51, 486, 2138, 362, 257, 4675, 300, 309, 393, 6408, 341, 1507, 293, 6408, 264, 4319, 295, 437, 291, 643, 293, 567, 291, 366, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08390135198206335, "compression_ratio": 1.8172757475083057, "no_speech_prob": 0.22244256734848022}, {"id": 525, "seek": 285558, "start": 2855.58, "end": 2864.58, "text": " And that will make it, even with the same level of intelligence as the model, will make it like 10x more useful and 10x faster to get to the right answer.", "tokens": [50364, 400, 300, 486, 652, 309, 11, 754, 365, 264, 912, 1496, 295, 7599, 382, 264, 2316, 11, 486, 652, 309, 411, 1266, 87, 544, 4420, 293, 1266, 87, 4663, 281, 483, 281, 264, 558, 1867, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10360272407531738, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.08257913589477539}, {"id": 526, "seek": 285558, "start": 2864.58, "end": 2868.58, "text": " How much do you put into custom instructions?", "tokens": [50814, 1012, 709, 360, 291, 829, 666, 2375, 9415, 30, 51014], "temperature": 0.0, "avg_logprob": -0.10360272407531738, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.08257913589477539}, {"id": 527, "seek": 285558, "start": 2868.58, "end": 2874.58, "text": " Because for something like this, it might be my profile, my writing sample, maybe whatever.", "tokens": [51014, 1436, 337, 746, 411, 341, 11, 309, 1062, 312, 452, 7964, 11, 452, 3579, 6889, 11, 1310, 2035, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10360272407531738, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.08257913589477539}, {"id": 528, "seek": 285558, "start": 2874.58, "end": 2880.58, "text": " But I probably wouldn't have, by the way, Nathan's a React novice and doesn't know how to install anything.", "tokens": [51314, 583, 286, 1391, 2759, 380, 362, 11, 538, 264, 636, 11, 20634, 311, 257, 30644, 23883, 573, 293, 1177, 380, 458, 577, 281, 3625, 1340, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10360272407531738, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.08257913589477539}, {"id": 529, "seek": 288058, "start": 2880.58, "end": 2887.58, "text": " Do you have a vision or a sort of recommendation for a custom instruction that would help me with things like this?", "tokens": [50364, 1144, 291, 362, 257, 5201, 420, 257, 1333, 295, 11879, 337, 257, 2375, 10951, 300, 576, 854, 385, 365, 721, 411, 341, 30, 50714], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 530, "seek": 288058, "start": 2887.58, "end": 2888.58, "text": " You're asking the right person.", "tokens": [50714, 509, 434, 3365, 264, 558, 954, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 531, "seek": 288058, "start": 2888.58, "end": 2892.58, "text": " I have a very extensive custom instruction and a lot of opinions about it.", "tokens": [50764, 286, 362, 257, 588, 13246, 2375, 10951, 293, 257, 688, 295, 11819, 466, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 532, "seek": 288058, "start": 2892.58, "end": 2893.58, "text": " If you want, I can share them.", "tokens": [50964, 759, 291, 528, 11, 286, 393, 2073, 552, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 533, "seek": 288058, "start": 2893.58, "end": 2895.58, "text": " I can share it with you right now and we can talk about it.", "tokens": [51014, 286, 393, 2073, 309, 365, 291, 558, 586, 293, 321, 393, 751, 466, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 534, "seek": 288058, "start": 2895.58, "end": 2896.58, "text": " Sure.", "tokens": [51114, 4894, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 535, "seek": 288058, "start": 2896.58, "end": 2897.58, "text": " Yeah, let's check it out.", "tokens": [51164, 865, 11, 718, 311, 1520, 309, 484, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 536, "seek": 288058, "start": 2897.58, "end": 2898.58, "text": " Okay.", "tokens": [51214, 1033, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 537, "seek": 288058, "start": 2898.58, "end": 2902.58, "text": " The first part of custom instructions is, what do you want ChatGPT to know about you?", "tokens": [51264, 440, 700, 644, 295, 2375, 9415, 307, 11, 437, 360, 291, 528, 27503, 38, 47, 51, 281, 458, 466, 291, 30, 51464], "temperature": 0.0, "avg_logprob": -0.07102879539864962, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.3660007119178772}, {"id": 538, "seek": 290258, "start": 2902.58, "end": 2914.58, "text": " And I actually like really having it know a little bit about who I am because there's enough about me on the internet that it knows my name and that actually helps.", "tokens": [50364, 400, 286, 767, 411, 534, 1419, 309, 458, 257, 707, 857, 466, 567, 286, 669, 570, 456, 311, 1547, 466, 385, 322, 264, 4705, 300, 309, 3255, 452, 1315, 293, 300, 767, 3665, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08371770577352555, "compression_ratio": 1.9067164179104477, "no_speech_prob": 0.6852078437805176}, {"id": 539, "seek": 290258, "start": 2914.58, "end": 2917.58, "text": " Same thing with every, there's enough about every internet that it knows my name.", "tokens": [50964, 10635, 551, 365, 633, 11, 456, 311, 1547, 466, 633, 4705, 300, 309, 3255, 452, 1315, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08371770577352555, "compression_ratio": 1.9067164179104477, "no_speech_prob": 0.6852078437805176}, {"id": 540, "seek": 290258, "start": 2917.58, "end": 2923.58, "text": " And every once in a while, not having to explain who I am or what the company is that I run is like really useful.", "tokens": [51114, 400, 633, 1564, 294, 257, 1339, 11, 406, 1419, 281, 2903, 567, 286, 669, 420, 437, 264, 2237, 307, 300, 286, 1190, 307, 411, 534, 4420, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08371770577352555, "compression_ratio": 1.9067164179104477, "no_speech_prob": 0.6852078437805176}, {"id": 541, "seek": 290258, "start": 2923.58, "end": 2930.58, "text": " For example, I was thinking a couple of weeks ago about starting a course and I was working with ChatGPT to decide how to do the course and whatever.", "tokens": [51414, 1171, 1365, 11, 286, 390, 1953, 257, 1916, 295, 3259, 2057, 466, 2891, 257, 1164, 293, 286, 390, 1364, 365, 27503, 38, 47, 51, 281, 4536, 577, 281, 360, 264, 1164, 293, 2035, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08371770577352555, "compression_ratio": 1.9067164179104477, "no_speech_prob": 0.6852078437805176}, {"id": 542, "seek": 293058, "start": 2930.58, "end": 2932.58, "text": " And the first problem was I want to do a course.", "tokens": [50364, 400, 264, 700, 1154, 390, 286, 528, 281, 360, 257, 1164, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 543, "seek": 293058, "start": 2932.58, "end": 2933.58, "text": " Can you help me think about it?", "tokens": [50464, 1664, 291, 854, 385, 519, 466, 309, 30, 50514], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 544, "seek": 293058, "start": 2933.58, "end": 2937.58, "text": " And with custom instructions on it knows that I'm a writer and entrepreneur and so cool.", "tokens": [50514, 400, 365, 2375, 9415, 322, 309, 3255, 300, 286, 478, 257, 9936, 293, 14307, 293, 370, 1627, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 545, "seek": 293058, "start": 2937.58, "end": 2938.58, "text": " I'll help you build a course.", "tokens": [50714, 286, 603, 854, 291, 1322, 257, 1164, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 546, "seek": 293058, "start": 2938.58, "end": 2941.58, "text": " Here's how to think about it because it knows that I'm probably going to build one.", "tokens": [50764, 1692, 311, 577, 281, 519, 466, 309, 570, 309, 3255, 300, 286, 478, 1391, 516, 281, 1322, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 547, "seek": 293058, "start": 2941.58, "end": 2945.58, "text": " But if I turn custom instructions off, it will be like, cool, what course do you want to take?", "tokens": [50914, 583, 498, 286, 1261, 2375, 9415, 766, 11, 309, 486, 312, 411, 11, 1627, 11, 437, 1164, 360, 291, 528, 281, 747, 30, 51114], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 548, "seek": 293058, "start": 2945.58, "end": 2949.58, "text": " And it's those like little things that like really make a difference for me.", "tokens": [51114, 400, 309, 311, 729, 411, 707, 721, 300, 411, 534, 652, 257, 2649, 337, 385, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 549, "seek": 293058, "start": 2949.58, "end": 2958.58, "text": " But basically like who serious relationships in my life are I have in here, like my sister, her husband, her son, I have my girlfriend up there.", "tokens": [51314, 583, 1936, 411, 567, 3156, 6159, 294, 452, 993, 366, 286, 362, 294, 510, 11, 411, 452, 4892, 11, 720, 5213, 11, 720, 1872, 11, 286, 362, 452, 10369, 493, 456, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0979551463932186, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.09264574199914932}, {"id": 550, "seek": 295858, "start": 2958.58, "end": 2969.58, "text": " Who people are at every because referencing their names is just much easier for me to be like, okay, when I'm talking about something and not have to explain who she is every single time is really helpful.", "tokens": [50364, 2102, 561, 366, 412, 633, 570, 40582, 641, 5288, 307, 445, 709, 3571, 337, 385, 281, 312, 411, 11, 1392, 11, 562, 286, 478, 1417, 466, 746, 293, 406, 362, 281, 2903, 567, 750, 307, 633, 2167, 565, 307, 534, 4961, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0809047357091364, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.07803770154714584}, {"id": 551, "seek": 295858, "start": 2969.58, "end": 2975.58, "text": " I think another really interesting thing is adding into custom instructions.", "tokens": [50914, 286, 519, 1071, 534, 1880, 551, 307, 5127, 666, 2375, 9415, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0809047357091364, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.07803770154714584}, {"id": 552, "seek": 295858, "start": 2975.58, "end": 2979.58, "text": " What are the things about you that you know that you're trying to like work on?", "tokens": [51214, 708, 366, 264, 721, 466, 291, 300, 291, 458, 300, 291, 434, 1382, 281, 411, 589, 322, 30, 51414], "temperature": 0.0, "avg_logprob": -0.0809047357091364, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.07803770154714584}, {"id": 553, "seek": 295858, "start": 2979.58, "end": 2984.58, "text": " For example, I feel like I'm I have a fear of rejecting people which causes me to be too agreeable.", "tokens": [51414, 1171, 1365, 11, 286, 841, 411, 286, 478, 286, 362, 257, 4240, 295, 45401, 561, 597, 7700, 385, 281, 312, 886, 3986, 712, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0809047357091364, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.07803770154714584}, {"id": 554, "seek": 298458, "start": 2984.58, "end": 2994.58, "text": " I'm a little bit too opportunistic and I would like to be more strategic like stuff like that is really helpful to put in custom instructions because it's these little realizations that you have every day.", "tokens": [50364, 286, 478, 257, 707, 857, 886, 2070, 3142, 293, 286, 576, 411, 281, 312, 544, 10924, 411, 1507, 411, 300, 307, 534, 4961, 281, 829, 294, 2375, 9415, 570, 309, 311, 613, 707, 957, 14455, 300, 291, 362, 633, 786, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14434706198202596, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.21721090376377106}, {"id": 555, "seek": 298458, "start": 2994.58, "end": 2998.58, "text": " You're like, wow, yeah, I am a little too opportunistic.", "tokens": [50864, 509, 434, 411, 11, 6076, 11, 1338, 11, 286, 669, 257, 707, 886, 2070, 3142, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14434706198202596, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.21721090376377106}, {"id": 556, "seek": 298458, "start": 2998.58, "end": 3004.58, "text": " I think Chatchity is great for being the thing that can help you as you're in the moment day to day.", "tokens": [51064, 286, 519, 761, 852, 507, 307, 869, 337, 885, 264, 551, 300, 393, 854, 291, 382, 291, 434, 294, 264, 1623, 786, 281, 786, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14434706198202596, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.21721090376377106}, {"id": 557, "seek": 298458, "start": 3004.58, "end": 3009.58, "text": " Remember to pull back and incorporate some of these insights that you have that everyone has about themselves.", "tokens": [51364, 5459, 281, 2235, 646, 293, 16091, 512, 295, 613, 14310, 300, 291, 362, 300, 1518, 575, 466, 2969, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14434706198202596, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.21721090376377106}, {"id": 558, "seek": 300958, "start": 3009.58, "end": 3019.58, "text": " And same thing for goals like having know what your goals are and bring you back to those things all the time as you're using is really helpful.", "tokens": [50364, 400, 912, 551, 337, 5493, 411, 1419, 458, 437, 428, 5493, 366, 293, 1565, 291, 646, 281, 729, 721, 439, 264, 565, 382, 291, 434, 1228, 307, 534, 4961, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12032108542359905, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.21193774044513702}, {"id": 559, "seek": 300958, "start": 3019.58, "end": 3020.58, "text": " Cool.", "tokens": [50864, 8561, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12032108542359905, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.21193774044513702}, {"id": 560, "seek": 300958, "start": 3020.58, "end": 3021.58, "text": " Well, thanks for sharing.", "tokens": [50914, 1042, 11, 3231, 337, 5414, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12032108542359905, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.21193774044513702}, {"id": 561, "seek": 300958, "start": 3021.58, "end": 3028.58, "text": " I think I use it a lot more for just like very unfamiliar topics.", "tokens": [50964, 286, 519, 286, 764, 309, 257, 688, 544, 337, 445, 411, 588, 29415, 8378, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12032108542359905, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.21193774044513702}, {"id": 562, "seek": 300958, "start": 3028.58, "end": 3033.58, "text": " I'm very just looking at these examples right that we had queued up.", "tokens": [51314, 286, 478, 588, 445, 1237, 412, 613, 5110, 558, 300, 321, 632, 631, 5827, 493, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12032108542359905, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.21193774044513702}, {"id": 563, "seek": 303358, "start": 3033.58, "end": 3042.58, "text": " Okay, there's an app in a framework that I've never touched and know nothing about working on a patent application and creating diagrams for a patent application.", "tokens": [50364, 1033, 11, 456, 311, 364, 724, 294, 257, 8388, 300, 286, 600, 1128, 9828, 293, 458, 1825, 466, 1364, 322, 257, 20495, 3861, 293, 4084, 36709, 337, 257, 20495, 3861, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13115963884579238, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.6580292582511902}, {"id": 564, "seek": 303358, "start": 3042.58, "end": 3044.58, "text": " I don't really know how to do that at all.", "tokens": [50814, 286, 500, 380, 534, 458, 577, 281, 360, 300, 412, 439, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13115963884579238, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.6580292582511902}, {"id": 565, "seek": 303358, "start": 3044.58, "end": 3046.58, "text": " Is there again, I'm starting with these very basic questions.", "tokens": [50914, 1119, 456, 797, 11, 286, 478, 2891, 365, 613, 588, 3875, 1651, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13115963884579238, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.6580292582511902}, {"id": 566, "seek": 303358, "start": 3046.58, "end": 3050.58, "text": " What's a good syntax that I might use to create a diagram for a patent application?", "tokens": [51014, 708, 311, 257, 665, 28431, 300, 286, 1062, 764, 281, 1884, 257, 10686, 337, 257, 20495, 3861, 30, 51214], "temperature": 0.0, "avg_logprob": -0.13115963884579238, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.6580292582511902}, {"id": 567, "seek": 303358, "start": 3050.58, "end": 3052.58, "text": " I just come in so cold.", "tokens": [51214, 286, 445, 808, 294, 370, 3554, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13115963884579238, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.6580292582511902}, {"id": 568, "seek": 305258, "start": 3052.58, "end": 3065.58, "text": " But it does suggest that you are doing a lot more kind of thought partner like brainstorming about your kind of core stuff, which is interesting.", "tokens": [50364, 583, 309, 775, 3402, 300, 291, 366, 884, 257, 688, 544, 733, 295, 1194, 4975, 411, 35245, 278, 466, 428, 733, 295, 4965, 1507, 11, 597, 307, 1880, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0937531500151663, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.13279254734516144}, {"id": 569, "seek": 305258, "start": 3065.58, "end": 3075.58, "text": " I'm much more on these kind of episodic things that we're like my history and this doesn't overlap almost at all in a lot of cases.", "tokens": [51014, 286, 478, 709, 544, 322, 613, 733, 295, 39200, 299, 721, 300, 321, 434, 411, 452, 2503, 293, 341, 1177, 380, 19959, 1920, 412, 439, 294, 257, 688, 295, 3331, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0937531500151663, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.13279254734516144}, {"id": 570, "seek": 307558, "start": 3075.58, "end": 3082.58, "text": " But it just goes to show how many different ways of using these tools there are too.", "tokens": [50364, 583, 309, 445, 1709, 281, 855, 577, 867, 819, 2098, 295, 1228, 613, 3873, 456, 366, 886, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09156123352050781, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.5920601487159729}, {"id": 571, "seek": 307558, "start": 3082.58, "end": 3088.58, "text": " And yeah, this could be another new year's resolution to try to bring it a little bit closer to the core of what I do.", "tokens": [50714, 400, 1338, 11, 341, 727, 312, 1071, 777, 1064, 311, 8669, 281, 853, 281, 1565, 309, 257, 707, 857, 4966, 281, 264, 4965, 295, 437, 286, 360, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09156123352050781, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.5920601487159729}, {"id": 572, "seek": 307558, "start": 3088.58, "end": 3091.58, "text": " It's not to say that it's not at the core of what I do, but not in this co-pilot way.", "tokens": [51014, 467, 311, 406, 281, 584, 300, 309, 311, 406, 412, 264, 4965, 295, 437, 286, 360, 11, 457, 406, 294, 341, 598, 12, 79, 31516, 636, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09156123352050781, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.5920601487159729}, {"id": 573, "seek": 307558, "start": 3091.58, "end": 3096.58, "text": " With things like Waymark, I'm working very closely with language models to make an app work well.", "tokens": [51164, 2022, 721, 411, 9558, 5638, 11, 286, 478, 1364, 588, 8185, 365, 2856, 5245, 281, 652, 364, 724, 589, 731, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09156123352050781, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.5920601487159729}, {"id": 574, "seek": 307558, "start": 3096.58, "end": 3102.58, "text": " And I feel like I have intimate knowledge of the details of how it works in that respect.", "tokens": [51414, 400, 286, 841, 411, 286, 362, 20215, 3601, 295, 264, 4365, 295, 577, 309, 1985, 294, 300, 3104, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09156123352050781, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.5920601487159729}, {"id": 575, "seek": 310258, "start": 3102.58, "end": 3111.58, "text": " It's a big project for me, but again, a different mode than the interactive dance kind of mode that you describe.", "tokens": [50364, 467, 311, 257, 955, 1716, 337, 385, 11, 457, 797, 11, 257, 819, 4391, 813, 264, 15141, 4489, 733, 295, 4391, 300, 291, 6786, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 576, "seek": 310258, "start": 3111.58, "end": 3112.58, "text": " Fascinating.", "tokens": [50814, 49098, 8205, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 577, "seek": 310258, "start": 3112.58, "end": 3114.58, "text": " Yeah, that makes a lot of sense.", "tokens": [50864, 865, 11, 300, 1669, 257, 688, 295, 2020, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 578, "seek": 310258, "start": 3114.58, "end": 3120.58, "text": " I definitely use it for some of this knowledge exploration stuff too, but yeah, it's totally a sort of thought partner for me.", "tokens": [50964, 286, 2138, 764, 309, 337, 512, 295, 341, 3601, 16197, 1507, 886, 11, 457, 1338, 11, 309, 311, 3879, 257, 1333, 295, 1194, 4975, 337, 385, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 579, "seek": 310258, "start": 3120.58, "end": 3124.58, "text": " But I'd love to keep looking through some of the other chats you brought.", "tokens": [51264, 583, 286, 1116, 959, 281, 1066, 1237, 807, 512, 295, 264, 661, 38057, 291, 3038, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 580, "seek": 310258, "start": 3124.58, "end": 3125.58, "text": " Cool.", "tokens": [51464, 8561, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11692215998967488, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.1643245369195938}, {"id": 581, "seek": 312558, "start": 3125.58, "end": 3128.58, "text": " Next one on working on diagrams.", "tokens": [50364, 3087, 472, 322, 1364, 322, 36709, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 582, "seek": 312558, "start": 3128.58, "end": 3136.58, "text": " We're going to have a combination of a provisional patent application and the supporting diagrams for the patent application.", "tokens": [50514, 492, 434, 516, 281, 362, 257, 6562, 295, 257, 1439, 271, 1966, 20495, 3861, 293, 264, 7231, 36709, 337, 264, 20495, 3861, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 583, "seek": 312558, "start": 3136.58, "end": 3139.58, "text": " This is something that I was doing for Waymark.", "tokens": [50914, 639, 307, 746, 300, 286, 390, 884, 337, 9558, 5638, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 584, "seek": 312558, "start": 3139.58, "end": 3146.58, "text": " And we have this ensemble method of creating advertising video for small business.", "tokens": [51064, 400, 321, 362, 341, 19492, 3170, 295, 4084, 13097, 960, 337, 1359, 1606, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 585, "seek": 312558, "start": 3146.58, "end": 3150.58, "text": " Basically, folks come into the site.", "tokens": [51414, 8537, 11, 4024, 808, 666, 264, 3621, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 586, "seek": 312558, "start": 3150.58, "end": 3153.58, "text": " They get to enter a website URL.", "tokens": [51614, 814, 483, 281, 3242, 257, 3144, 12905, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1298198475557215, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.7301636934280396}, {"id": 587, "seek": 315358, "start": 3153.58, "end": 3156.58, "text": " Typically, people will give like the homepage of their small business website.", "tokens": [50364, 23129, 11, 561, 486, 976, 411, 264, 31301, 295, 641, 1359, 1606, 3144, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 588, "seek": 315358, "start": 3156.58, "end": 3160.58, "text": " We have some code that goes and like grabs content off of that website.", "tokens": [50514, 492, 362, 512, 3089, 300, 1709, 293, 411, 30028, 2701, 766, 295, 300, 3144, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 589, "seek": 315358, "start": 3160.58, "end": 3166.58, "text": " And then we build a profile on a synthetically if you're custom instructions, so to speak, within the context of our app.", "tokens": [50714, 400, 550, 321, 1322, 257, 7964, 322, 257, 10657, 22652, 498, 291, 434, 2375, 9415, 11, 370, 281, 1710, 11, 1951, 264, 4319, 295, 527, 724, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 590, "seek": 315358, "start": 3166.58, "end": 3167.58, "text": " Who are you as a user?", "tokens": [51014, 2102, 366, 291, 382, 257, 4195, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 591, "seek": 315358, "start": 3167.58, "end": 3168.58, "text": " What's your business?", "tokens": [51064, 708, 311, 428, 1606, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 592, "seek": 315358, "start": 3168.58, "end": 3169.58, "text": " What are you all about?", "tokens": [51114, 708, 366, 291, 439, 466, 30, 51164], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 593, "seek": 315358, "start": 3169.58, "end": 3170.58, "text": " What kind of business?", "tokens": [51164, 708, 733, 295, 1606, 30, 51214], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 594, "seek": 315358, "start": 3170.58, "end": 3171.58, "text": " What images?", "tokens": [51214, 708, 5267, 30, 51264], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 595, "seek": 315358, "start": 3171.58, "end": 3175.58, "text": " And then to actually create the video, you give a very specific, although it's like a super short instruction.", "tokens": [51264, 400, 550, 281, 767, 1884, 264, 960, 11, 291, 976, 257, 588, 2685, 11, 4878, 309, 311, 411, 257, 1687, 2099, 10951, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 596, "seek": 315358, "start": 3175.58, "end": 3182.58, "text": " I want to make a video for my sale this Saturday where I'm opening a new location and here's the address or whatever.", "tokens": [51464, 286, 528, 281, 652, 257, 960, 337, 452, 8680, 341, 8803, 689, 286, 478, 5193, 257, 777, 4914, 293, 510, 311, 264, 2985, 420, 2035, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10606035220077614, "compression_ratio": 1.7565217391304349, "no_speech_prob": 0.04332675412297249}, {"id": 597, "seek": 318258, "start": 3182.58, "end": 3185.58, "text": " This is my purpose in this moment prompt.", "tokens": [50364, 639, 307, 452, 4334, 294, 341, 1623, 12391, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10786623017400758, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.029287928715348244}, {"id": 598, "seek": 318258, "start": 3185.58, "end": 3193.58, "text": " And then we've got a pretty complicated machinery that takes all those inputs and it works with a language model to write a script.", "tokens": [50514, 400, 550, 321, 600, 658, 257, 1238, 6179, 27302, 300, 2516, 439, 729, 15743, 293, 309, 1985, 365, 257, 2856, 2316, 281, 2464, 257, 5755, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10786623017400758, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.029287928715348244}, {"id": 599, "seek": 318258, "start": 3193.58, "end": 3201.58, "text": " And then it has computer vision components that decide which of the images from your library should be used to complement the script and all these different points along the way.", "tokens": [50914, 400, 550, 309, 575, 3820, 5201, 6677, 300, 4536, 597, 295, 264, 5267, 490, 428, 6405, 820, 312, 1143, 281, 17103, 264, 5755, 293, 439, 613, 819, 2793, 2051, 264, 636, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10786623017400758, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.029287928715348244}, {"id": 600, "seek": 318258, "start": 3201.58, "end": 3204.58, "text": " And that is, it's a pretty cool experience.", "tokens": [51314, 400, 300, 307, 11, 309, 311, 257, 1238, 1627, 1752, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10786623017400758, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.029287928715348244}, {"id": 601, "seek": 318258, "start": 3204.58, "end": 3211.58, "text": " Now, really compared to, again, you think about pre AI and now what we had before was an easy to use template library.", "tokens": [51464, 823, 11, 534, 5347, 281, 11, 797, 11, 291, 519, 466, 659, 7318, 293, 586, 437, 321, 632, 949, 390, 364, 1858, 281, 764, 12379, 6405, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10786623017400758, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.029287928715348244}, {"id": 602, "seek": 321158, "start": 3211.58, "end": 3214.58, "text": " And what we have now is really the AI makes you content.", "tokens": [50364, 400, 437, 321, 362, 586, 307, 534, 264, 7318, 1669, 291, 2701, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 603, "seek": 321158, "start": 3214.58, "end": 3222.58, "text": " Like it's a phase change in terms of how easy it is to use, how quick the experience is, how much you can just rifle through ideas.", "tokens": [50514, 1743, 309, 311, 257, 5574, 1319, 294, 2115, 295, 577, 1858, 309, 307, 281, 764, 11, 577, 1702, 264, 1752, 307, 11, 577, 709, 291, 393, 445, 18008, 807, 3487, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 604, "seek": 321158, "start": 3222.58, "end": 3227.58, "text": " If you don't like the first thing, you just ask it to do another and it's qualitatively just way more fun.", "tokens": [50914, 759, 291, 500, 380, 411, 264, 700, 551, 11, 291, 445, 1029, 309, 281, 360, 1071, 293, 309, 311, 31312, 356, 445, 636, 544, 1019, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 605, "seek": 321158, "start": 3227.58, "end": 3231.58, "text": " People used to have to sit there and type stuff in and they were like, okay, what do I say?", "tokens": [51164, 3432, 1143, 281, 362, 281, 1394, 456, 293, 2010, 1507, 294, 293, 436, 645, 411, 11, 1392, 11, 437, 360, 286, 584, 30, 51364], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 606, "seek": 321158, "start": 3231.58, "end": 3232.58, "text": " And I'm not sure what to say.", "tokens": [51364, 400, 286, 478, 406, 988, 437, 281, 584, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 607, "seek": 321158, "start": 3232.58, "end": 3239.58, "text": " And a lot of people are not content creators, but everybody always referred to the Mr. Burns episode from the Simpsons.", "tokens": [51414, 400, 257, 688, 295, 561, 366, 406, 2701, 16039, 11, 457, 2201, 1009, 10839, 281, 264, 2221, 13, 41195, 3500, 490, 264, 3998, 1878, 892, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08437196301742339, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.04598524793982506}, {"id": 608, "seek": 323958, "start": 3239.58, "end": 3246.58, "text": " This is a long time ago, but he goes to an art museum for a reveal of some piece of art and they reveal it.", "tokens": [50364, 639, 307, 257, 938, 565, 2057, 11, 457, 415, 1709, 281, 364, 1523, 8441, 337, 257, 10658, 295, 512, 2522, 295, 1523, 293, 436, 10658, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 609, "seek": 323958, "start": 3246.58, "end": 3249.58, "text": " And he says, I'm no art critic, but I know what I hate.", "tokens": [50714, 400, 415, 1619, 11, 286, 478, 572, 1523, 7850, 11, 457, 286, 458, 437, 286, 4700, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 610, "seek": 323958, "start": 3249.58, "end": 3253.58, "text": " And that's, I feel like, exactly how our users operate.", "tokens": [50864, 400, 300, 311, 11, 286, 841, 411, 11, 2293, 577, 527, 5022, 9651, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 611, "seek": 323958, "start": 3253.58, "end": 3255.58, "text": " They ask for something.", "tokens": [51064, 814, 1029, 337, 746, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 612, "seek": 323958, "start": 3255.58, "end": 3256.58, "text": " They wait 30 seconds.", "tokens": [51164, 814, 1699, 2217, 3949, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 613, "seek": 323958, "start": 3256.58, "end": 3259.58, "text": " They now get to watch a video featuring their business.", "tokens": [51214, 814, 586, 483, 281, 1159, 257, 960, 19742, 641, 1606, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 614, "seek": 323958, "start": 3259.58, "end": 3261.58, "text": " And if they like it, they can proceed.", "tokens": [51364, 400, 498, 436, 411, 309, 11, 436, 393, 8991, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 615, "seek": 323958, "start": 3261.58, "end": 3266.58, "text": " And if they don't like it, it's very obvious to them and they can very quickly be like, no, not that.", "tokens": [51464, 400, 498, 436, 500, 380, 411, 309, 11, 309, 311, 588, 6322, 281, 552, 293, 436, 393, 588, 2661, 312, 411, 11, 572, 11, 406, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08130536150576463, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.18226855993270874}, {"id": 616, "seek": 326658, "start": 3266.58, "end": 3267.58, "text": " Give me another one.", "tokens": [50364, 5303, 385, 1071, 472, 13, 50414], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 617, "seek": 326658, "start": 3267.58, "end": 3269.58, "text": " And here's an alternate instruction.", "tokens": [50414, 400, 510, 311, 364, 18873, 10951, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 618, "seek": 326658, "start": 3269.58, "end": 3271.58, "text": " So anyway, this is the app that we built.", "tokens": [50514, 407, 4033, 11, 341, 307, 264, 724, 300, 321, 3094, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 619, "seek": 326658, "start": 3271.58, "end": 3275.58, "text": " And now we're like, okay, maybe we should think about filing a provisional patent on that.", "tokens": [50614, 400, 586, 321, 434, 411, 11, 1392, 11, 1310, 321, 820, 519, 466, 26854, 257, 1439, 271, 1966, 20495, 322, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 620, "seek": 326658, "start": 3275.58, "end": 3281.58, "text": " Like most software companies, we're never going to prosecute our patents, but we just want to make sure nobody can come in and give us a hard time.", "tokens": [50814, 1743, 881, 4722, 3431, 11, 321, 434, 1128, 516, 281, 22382, 1169, 527, 38142, 11, 457, 321, 445, 528, 281, 652, 988, 5079, 393, 808, 294, 293, 976, 505, 257, 1152, 565, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 621, "seek": 326658, "start": 3281.58, "end": 3283.58, "text": " So how do I write a patent?", "tokens": [51114, 407, 577, 360, 286, 2464, 257, 20495, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 622, "seek": 326658, "start": 3283.58, "end": 3286.58, "text": " And how do I create the diagrams?", "tokens": [51214, 400, 577, 360, 286, 1884, 264, 36709, 30, 51364], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 623, "seek": 326658, "start": 3286.58, "end": 3288.58, "text": " And I want to be able to update it.", "tokens": [51364, 400, 286, 528, 281, 312, 1075, 281, 5623, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 624, "seek": 326658, "start": 3288.58, "end": 3291.58, "text": " I want to have something that's not like just a total mess.", "tokens": [51464, 286, 528, 281, 362, 746, 300, 311, 406, 411, 445, 257, 3217, 2082, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07484552134638248, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.07803035527467728}, {"id": 625, "seek": 329158, "start": 3291.58, "end": 3297.58, "text": " So this was a series of different interactions that ultimately led me to these diagrams.", "tokens": [50364, 407, 341, 390, 257, 2638, 295, 819, 13280, 300, 6284, 4684, 385, 281, 613, 36709, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 626, "seek": 329158, "start": 3297.58, "end": 3305.58, "text": " But I provided initially basically what I just said to you, which is a rambling sort of instruction on here is my app.", "tokens": [50664, 583, 286, 5649, 9105, 1936, 437, 286, 445, 848, 281, 291, 11, 597, 307, 257, 367, 19391, 1333, 295, 10951, 322, 510, 307, 452, 724, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 627, "seek": 329158, "start": 3305.58, "end": 3306.58, "text": " And here's what it does.", "tokens": [51064, 400, 510, 311, 437, 309, 775, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 628, "seek": 329158, "start": 3306.58, "end": 3307.58, "text": " Here's how it works.", "tokens": [51114, 1692, 311, 577, 309, 1985, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 629, "seek": 329158, "start": 3307.58, "end": 3308.58, "text": " Here's some of the parts behind it.", "tokens": [51164, 1692, 311, 512, 295, 264, 3166, 2261, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 630, "seek": 329158, "start": 3308.58, "end": 3312.58, "text": " The language model writes the script and the code that's scripted from the website.", "tokens": [51214, 440, 2856, 2316, 13657, 264, 5755, 293, 264, 3089, 300, 311, 5755, 292, 490, 264, 3144, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 631, "seek": 329158, "start": 3312.58, "end": 3320.58, "text": " And then the other part with the computer vision that figures out what I just literally tell the whole thing and say, now can you use some syntax?", "tokens": [51414, 400, 550, 264, 661, 644, 365, 264, 3820, 5201, 300, 9624, 484, 437, 286, 445, 3736, 980, 264, 1379, 551, 293, 584, 11, 586, 393, 291, 764, 512, 28431, 30, 51814], "temperature": 0.0, "avg_logprob": -0.07750840074434055, "compression_ratio": 1.6938110749185669, "no_speech_prob": 0.027574889361858368}, {"id": 632, "seek": 332058, "start": 3320.58, "end": 3327.58, "text": " To make me a diagram that shows the structure of that app that I just word vomited to you.", "tokens": [50364, 1407, 652, 385, 257, 10686, 300, 3110, 264, 3877, 295, 300, 724, 300, 286, 445, 1349, 10135, 1226, 281, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 633, "seek": 332058, "start": 3327.58, "end": 3329.58, "text": " And so there's like a bunch of different structures out there.", "tokens": [50714, 400, 370, 456, 311, 411, 257, 3840, 295, 819, 9227, 484, 456, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 634, "seek": 332058, "start": 3329.58, "end": 3331.58, "text": " So that's the first part of that conversation as well.", "tokens": [50814, 407, 300, 311, 264, 700, 644, 295, 300, 3761, 382, 731, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 635, "seek": 332058, "start": 3331.58, "end": 3337.58, "text": " You could use the mermaid syntax or you could use graph viz or you could use a couple other things.", "tokens": [50914, 509, 727, 764, 264, 43146, 28431, 420, 291, 727, 764, 4295, 371, 590, 420, 291, 727, 764, 257, 1916, 661, 721, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 636, "seek": 332058, "start": 3337.58, "end": 3341.58, "text": " But what are the pros and cons of those and can they represent certain different kinds of structures?", "tokens": [51214, 583, 437, 366, 264, 6267, 293, 1014, 295, 729, 293, 393, 436, 2906, 1629, 819, 3685, 295, 9227, 30, 51414], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 637, "seek": 332058, "start": 3341.58, "end": 3346.58, "text": " We dialed it in on either mermaid or graph is it started to make me a thing.", "tokens": [51414, 492, 5502, 292, 309, 294, 322, 2139, 43146, 420, 4295, 307, 309, 1409, 281, 652, 385, 257, 551, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08564451535542807, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.006902401801198721}, {"id": 638, "seek": 334658, "start": 3346.58, "end": 3347.58, "text": " And then you can see here too.", "tokens": [50364, 400, 550, 291, 393, 536, 510, 886, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 639, "seek": 334658, "start": 3347.58, "end": 3353.58, "text": " This is interesting because I did find in this one that at some point it got confused.", "tokens": [50414, 639, 307, 1880, 570, 286, 630, 915, 294, 341, 472, 300, 412, 512, 935, 309, 658, 9019, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 640, "seek": 334658, "start": 3353.58, "end": 3360.58, "text": " I'd given it this thing and had generated this syntax, asked for refinements on the syntax because I'm taking the syntax by the way, going over to another app.", "tokens": [50714, 286, 1116, 2212, 309, 341, 551, 293, 632, 10833, 341, 28431, 11, 2351, 337, 44395, 6400, 322, 264, 28431, 570, 286, 478, 1940, 264, 28431, 538, 264, 636, 11, 516, 670, 281, 1071, 724, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 641, "seek": 334658, "start": 3360.58, "end": 3365.58, "text": " What's cool about the syntax is you drop in this pure text syntax and it will render the app for you.", "tokens": [51064, 708, 311, 1627, 466, 264, 28431, 307, 291, 3270, 294, 341, 6075, 2487, 28431, 293, 309, 486, 15529, 264, 724, 337, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 642, "seek": 334658, "start": 3365.58, "end": 3370.58, "text": " So you've got things like this graph is diagram.", "tokens": [51314, 407, 291, 600, 658, 721, 411, 341, 4295, 307, 10686, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 643, "seek": 334658, "start": 3370.58, "end": 3371.58, "text": " What's a digraph?", "tokens": [51564, 708, 311, 257, 2528, 2662, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12636661529541016, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.09265776723623276}, {"id": 644, "seek": 337158, "start": 3371.58, "end": 3372.58, "text": " I don't even really know.", "tokens": [50364, 286, 500, 380, 754, 534, 458, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 645, "seek": 337158, "start": 3372.58, "end": 3380.58, "text": " It's called this digraph is G and it has these elements and they have these properties and they're connected in this sort of graph structure, blah, blah, blah.", "tokens": [50414, 467, 311, 1219, 341, 2528, 2662, 307, 460, 293, 309, 575, 613, 4959, 293, 436, 362, 613, 7221, 293, 436, 434, 4582, 294, 341, 1333, 295, 4295, 3877, 11, 12288, 11, 12288, 11, 12288, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 646, "seek": 337158, "start": 3380.58, "end": 3383.58, "text": " You load it in half a second, you know, it renders it.", "tokens": [50814, 509, 3677, 309, 294, 1922, 257, 1150, 11, 291, 458, 11, 309, 6125, 433, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 647, "seek": 337158, "start": 3383.58, "end": 3385.58, "text": " You're like, oh, no, that's not quite right.", "tokens": [50964, 509, 434, 411, 11, 1954, 11, 572, 11, 300, 311, 406, 1596, 558, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 648, "seek": 337158, "start": 3385.58, "end": 3387.58, "text": " This point should be connected to this point.", "tokens": [51064, 639, 935, 820, 312, 4582, 281, 341, 935, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 649, "seek": 337158, "start": 3387.58, "end": 3390.58, "text": " And it's it's skipping one that it's so whatever.", "tokens": [51164, 400, 309, 311, 309, 311, 31533, 472, 300, 309, 311, 370, 2035, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 650, "seek": 337158, "start": 3390.58, "end": 3392.58, "text": " So you give it these kind of iterations.", "tokens": [51314, 407, 291, 976, 309, 613, 733, 295, 36540, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 651, "seek": 337158, "start": 3392.58, "end": 3396.58, "text": " It would make progress, but then it would also get confused.", "tokens": [51414, 467, 576, 652, 4205, 11, 457, 550, 309, 576, 611, 483, 9019, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 652, "seek": 337158, "start": 3396.58, "end": 3400.58, "text": " It seemed after a number of rounds because there's just maybe like too much syntax.", "tokens": [51614, 467, 6576, 934, 257, 1230, 295, 13757, 570, 456, 311, 445, 1310, 411, 886, 709, 28431, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12671221315471176, "compression_ratio": 1.7392638036809815, "no_speech_prob": 0.010012203827500343}, {"id": 653, "seek": 340058, "start": 3400.58, "end": 3411.58, "text": " So at some point, I did say, OK, using the episodic memory to my advantage or working around its working memory weaknesses by just wiping and starting over.", "tokens": [50364, 407, 412, 512, 935, 11, 286, 630, 584, 11, 2264, 11, 1228, 264, 39200, 299, 4675, 281, 452, 5002, 420, 1364, 926, 1080, 1364, 4675, 24381, 538, 445, 40611, 293, 2891, 670, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 654, "seek": 340058, "start": 3411.58, "end": 3413.58, "text": " I'm like, OK, here's the best one from that chat.", "tokens": [50914, 286, 478, 411, 11, 2264, 11, 510, 311, 264, 1151, 472, 490, 300, 5081, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 655, "seek": 340058, "start": 3413.58, "end": 3416.58, "text": " It was closest to what I wanted it to represent.", "tokens": [51014, 467, 390, 13699, 281, 437, 286, 1415, 309, 281, 2906, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 656, "seek": 340058, "start": 3416.58, "end": 3417.58, "text": " We just go have another chat.", "tokens": [51164, 492, 445, 352, 362, 1071, 5081, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 657, "seek": 340058, "start": 3417.58, "end": 3422.58, "text": " And this time we're going to skip all the part about which format do we use and skip all the word salad.", "tokens": [51214, 400, 341, 565, 321, 434, 516, 281, 10023, 439, 264, 644, 466, 597, 7877, 360, 321, 764, 293, 10023, 439, 264, 1349, 12604, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 658, "seek": 340058, "start": 3422.58, "end": 3424.58, "text": " And I can just be like, here's a diagram.", "tokens": [51464, 400, 286, 393, 445, 312, 411, 11, 510, 311, 257, 10686, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 659, "seek": 340058, "start": 3424.58, "end": 3426.58, "text": " I want to make some changes to it.", "tokens": [51564, 286, 528, 281, 652, 512, 2962, 281, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10484302700973872, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0025502063799649477}, {"id": 660, "seek": 342658, "start": 3426.58, "end": 3430.58, "text": " And now have it do more localized edits for me.", "tokens": [50364, 400, 586, 362, 309, 360, 544, 44574, 41752, 337, 385, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 661, "seek": 342658, "start": 3430.58, "end": 3434.58, "text": " And again, a lot of little details, a lot of nuance here, but it's happy to do that.", "tokens": [50564, 400, 797, 11, 257, 688, 295, 707, 4365, 11, 257, 688, 295, 42625, 510, 11, 457, 309, 311, 2055, 281, 360, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 662, "seek": 342658, "start": 3434.58, "end": 3437.58, "text": " We work through a number of rounds of it.", "tokens": [50764, 492, 589, 807, 257, 1230, 295, 13757, 295, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 663, "seek": 342658, "start": 3437.58, "end": 3444.58, "text": " And I believe I attached the thing for you what I ended up with after or a couple chats.", "tokens": [50914, 400, 286, 1697, 286, 8570, 264, 551, 337, 291, 437, 286, 4590, 493, 365, 934, 420, 257, 1916, 38057, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 664, "seek": 342658, "start": 3444.58, "end": 3448.58, "text": " You even get to the point where you're like color coding and really starting to make sense of it.", "tokens": [51264, 509, 754, 483, 281, 264, 935, 689, 291, 434, 411, 2017, 17720, 293, 534, 2891, 281, 652, 2020, 295, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 665, "seek": 342658, "start": 3448.58, "end": 3449.58, "text": " It's like, right.", "tokens": [51464, 467, 311, 411, 11, 558, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 666, "seek": 342658, "start": 3449.58, "end": 3453.58, "text": " The green in this diagram now is the things that the user does.", "tokens": [51514, 440, 3092, 294, 341, 10686, 586, 307, 264, 721, 300, 264, 4195, 775, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07834921592523243, "compression_ratio": 1.6109090909090908, "no_speech_prob": 0.05338412895798683}, {"id": 667, "seek": 345358, "start": 3453.58, "end": 3456.58, "text": " So the user tells us what their business website is.", "tokens": [50364, 407, 264, 4195, 5112, 505, 437, 641, 1606, 3144, 307, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 668, "seek": 345358, "start": 3456.58, "end": 3457.58, "text": " Then there's code to go scrape.", "tokens": [50514, 1396, 456, 311, 3089, 281, 352, 32827, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 669, "seek": 345358, "start": 3457.58, "end": 3462.58, "text": " Then there's this fork where we have to grab all the images and process them in various ways.", "tokens": [50564, 1396, 456, 311, 341, 17716, 689, 321, 362, 281, 4444, 439, 264, 5267, 293, 1399, 552, 294, 3683, 2098, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 670, "seek": 345358, "start": 3462.58, "end": 3468.58, "text": " One of the big challenges is which parts of this can happen in parallel and which parts depend on which parts.", "tokens": [50814, 1485, 295, 264, 955, 4759, 307, 597, 3166, 295, 341, 393, 1051, 294, 8952, 293, 597, 3166, 5672, 322, 597, 3166, 13, 51114], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 671, "seek": 345358, "start": 3468.58, "end": 3473.58, "text": " This is actually something that we didn't have until I did this, even for the technology team.", "tokens": [51114, 639, 307, 767, 746, 300, 321, 994, 380, 362, 1826, 286, 630, 341, 11, 754, 337, 264, 2899, 1469, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 672, "seek": 345358, "start": 3473.58, "end": 3479.58, "text": " And I'm not sure how well all the members of the technology team could have even drawn this.", "tokens": [51364, 400, 286, 478, 406, 988, 577, 731, 439, 264, 2679, 295, 264, 2899, 1469, 727, 362, 754, 10117, 341, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05237782105155613, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0064868382178246975}, {"id": 673, "seek": 347958, "start": 3479.58, "end": 3486.58, "text": " So now we actually have a better reference internally also to be like, hey, if we like, what depends on the image aesthetic step?", "tokens": [50364, 407, 586, 321, 767, 362, 257, 1101, 6408, 19501, 611, 281, 312, 411, 11, 4177, 11, 498, 321, 411, 11, 437, 5946, 322, 264, 3256, 20092, 1823, 30, 50714], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 674, "seek": 347958, "start": 3486.58, "end": 3488.58, "text": " Now we can go look at it and be like, oh, okay.", "tokens": [50714, 823, 321, 393, 352, 574, 412, 309, 293, 312, 411, 11, 1954, 11, 1392, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 675, "seek": 347958, "start": 3488.58, "end": 3492.58, "text": " Yeah, you can't select best images until you have the aesthetic scores.", "tokens": [50814, 865, 11, 291, 393, 380, 3048, 1151, 5267, 1826, 291, 362, 264, 20092, 13444, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 676, "seek": 347958, "start": 3492.58, "end": 3493.58, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 677, "seek": 347958, "start": 3493.58, "end": 3494.58, "text": " Good.", "tokens": [51064, 2205, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 678, "seek": 347958, "start": 3494.58, "end": 3495.58, "text": " Completed.", "tokens": [51114, 33736, 10993, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 679, "seek": 347958, "start": 3495.58, "end": 3497.58, "text": " Just having that clarity is also I think just operationally useful.", "tokens": [51164, 1449, 1419, 300, 16992, 307, 611, 286, 519, 445, 6916, 379, 4420, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 680, "seek": 347958, "start": 3497.58, "end": 3508.58, "text": " But this is the sort of thing that you can attach to a provisional patent application and at least begin to protect yourself from future patent rolls coming your way.", "tokens": [51264, 583, 341, 307, 264, 1333, 295, 551, 300, 291, 393, 5085, 281, 257, 1439, 271, 1966, 20495, 3861, 293, 412, 1935, 1841, 281, 2371, 1803, 490, 2027, 20495, 15767, 1348, 428, 636, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13527133852936502, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.2225365936756134}, {"id": 681, "seek": 350858, "start": 3508.58, "end": 3509.58, "text": " Again, how long would this take?", "tokens": [50364, 3764, 11, 577, 938, 576, 341, 747, 30, 50414], "temperature": 0.0, "avg_logprob": -0.10415802196580537, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.016392692923545837}, {"id": 682, "seek": 350858, "start": 3509.58, "end": 3518.58, "text": " If I had drawn it freehand, I maybe could have drawn it in a somewhat comparable time to the amount of time that I spent exchange,", "tokens": [50414, 759, 286, 632, 10117, 309, 1737, 5543, 11, 286, 1310, 727, 362, 10117, 309, 294, 257, 8344, 25323, 565, 281, 264, 2372, 295, 565, 300, 286, 4418, 7742, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10415802196580537, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.016392692923545837}, {"id": 683, "seek": 350858, "start": 3518.58, "end": 3527.58, "text": " but having the syntax and now having it in that kind of structured language way also makes this like much more maintainable,", "tokens": [50864, 457, 1419, 264, 28431, 293, 586, 1419, 309, 294, 300, 733, 295, 18519, 2856, 636, 611, 1669, 341, 411, 709, 544, 6909, 712, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10415802196580537, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.016392692923545837}, {"id": 684, "seek": 350858, "start": 3527.58, "end": 3531.58, "text": " can fit in other things, even can be like more readily used in language models.", "tokens": [51314, 393, 3318, 294, 661, 721, 11, 754, 393, 312, 411, 544, 26336, 1143, 294, 2856, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10415802196580537, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.016392692923545837}, {"id": 685, "seek": 350858, "start": 3531.58, "end": 3533.58, "text": " The vision understanding is getting very good.", "tokens": [51514, 440, 5201, 3701, 307, 1242, 588, 665, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10415802196580537, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.016392692923545837}, {"id": 686, "seek": 353358, "start": 3533.58, "end": 3541.58, "text": " But I would say it's probably still better at understanding the syntax of the graph more than this visual rendering of the graph.", "tokens": [50364, 583, 286, 576, 584, 309, 311, 1391, 920, 1101, 412, 3701, 264, 28431, 295, 264, 4295, 544, 813, 341, 5056, 22407, 295, 264, 4295, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 687, "seek": 353358, "start": 3541.58, "end": 3542.58, "text": " I think that's great.", "tokens": [50764, 286, 519, 300, 311, 869, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 688, "seek": 353358, "start": 3542.58, "end": 3544.58, "text": " Obviously, ChatGPT has the Dolly integration.", "tokens": [50814, 7580, 11, 27503, 38, 47, 51, 575, 264, 1144, 13020, 10980, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 689, "seek": 353358, "start": 3544.58, "end": 3550.58, "text": " So I'm familiar with that, but I've been thinking a lot about sometimes I want to create something that looks like this,", "tokens": [50914, 407, 286, 478, 4963, 365, 300, 11, 457, 286, 600, 668, 1953, 257, 688, 466, 2171, 286, 528, 281, 1884, 746, 300, 1542, 411, 341, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 690, "seek": 353358, "start": 3550.58, "end": 3552.58, "text": " like in a graph with text and boxes and all that kind of stuff.", "tokens": [51214, 411, 294, 257, 4295, 365, 2487, 293, 9002, 293, 439, 300, 733, 295, 1507, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 691, "seek": 353358, "start": 3552.58, "end": 3557.58, "text": " And I didn't even think to do to have it just write graph is markup or something like that and paste it somewhere else.", "tokens": [51314, 400, 286, 994, 380, 754, 519, 281, 360, 281, 362, 309, 445, 2464, 4295, 307, 1491, 1010, 420, 746, 411, 300, 293, 9163, 309, 4079, 1646, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 692, "seek": 353358, "start": 3557.58, "end": 3561.58, "text": " So I think that's a really cool thing to know it can do.", "tokens": [51564, 407, 286, 519, 300, 311, 257, 534, 1627, 551, 281, 458, 309, 393, 360, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10596765598780672, "compression_ratio": 1.7306501547987616, "no_speech_prob": 0.12583403289318085}, {"id": 693, "seek": 356158, "start": 3561.58, "end": 3566.58, "text": " And it's also pretty clear, I don't know, in a year, it'll probably just render the graph is stuff for you.", "tokens": [50364, 400, 309, 311, 611, 1238, 1850, 11, 286, 500, 380, 458, 11, 294, 257, 1064, 11, 309, 603, 1391, 445, 15529, 264, 4295, 307, 1507, 337, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08664078866281817, "compression_ratio": 1.625, "no_speech_prob": 0.008574514649808407}, {"id": 694, "seek": 356158, "start": 3566.58, "end": 3574.58, "text": " And you'll be able to like move it around and do all that kind of stuff without even necessarily having to chat back and forth after the first round or something like that.", "tokens": [50614, 400, 291, 603, 312, 1075, 281, 411, 1286, 309, 926, 293, 360, 439, 300, 733, 295, 1507, 1553, 754, 4725, 1419, 281, 5081, 646, 293, 5220, 934, 264, 700, 3098, 420, 746, 411, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08664078866281817, "compression_ratio": 1.625, "no_speech_prob": 0.008574514649808407}, {"id": 695, "seek": 356158, "start": 3574.58, "end": 3579.58, "text": " I think that would be a very cool next step for ChatGPT is jump into an edit mode for something like this.", "tokens": [51014, 286, 519, 300, 576, 312, 257, 588, 1627, 958, 1823, 337, 27503, 38, 47, 51, 307, 3012, 666, 364, 8129, 4391, 337, 746, 411, 341, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08664078866281817, "compression_ratio": 1.625, "no_speech_prob": 0.008574514649808407}, {"id": 696, "seek": 356158, "start": 3579.58, "end": 3585.58, "text": " Closest thing I've seen to that so far is diagram GPT.", "tokens": [51264, 2033, 329, 377, 551, 286, 600, 1612, 281, 300, 370, 1400, 307, 10686, 26039, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08664078866281817, "compression_ratio": 1.625, "no_speech_prob": 0.008574514649808407}, {"id": 697, "seek": 356158, "start": 3585.58, "end": 3587.58, "text": " This is a slightly different notation.", "tokens": [51564, 639, 307, 257, 4748, 819, 24657, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08664078866281817, "compression_ratio": 1.625, "no_speech_prob": 0.008574514649808407}, {"id": 698, "seek": 358758, "start": 3587.58, "end": 3591.58, "text": " But basically you can prompt in natural language.", "tokens": [50364, 583, 1936, 291, 393, 12391, 294, 3303, 2856, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11977183703079965, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12926562130451202}, {"id": 699, "seek": 358758, "start": 3591.58, "end": 3596.58, "text": " Yeah, it will then generate, in this case, mermaid syntax for you in response.", "tokens": [50564, 865, 11, 309, 486, 550, 8460, 11, 294, 341, 1389, 11, 43146, 28431, 337, 291, 294, 4134, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11977183703079965, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12926562130451202}, {"id": 700, "seek": 358758, "start": 3596.58, "end": 3602.58, "text": " And then it'll immediately render your image and you can then edit the syntax.", "tokens": [50814, 400, 550, 309, 603, 4258, 15529, 428, 3256, 293, 291, 393, 550, 8129, 264, 28431, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11977183703079965, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12926562130451202}, {"id": 701, "seek": 358758, "start": 3602.58, "end": 3606.58, "text": " You can't quite like drag and drop within the interface itself.", "tokens": [51114, 509, 393, 380, 1596, 411, 5286, 293, 3270, 1951, 264, 9226, 2564, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11977183703079965, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12926562130451202}, {"id": 702, "seek": 358758, "start": 3606.58, "end": 3616.58, "text": " But I think this is a really interesting question around highlights and really interesting question around like what things should be in chat GPT versus what things should have their own distinct experience,", "tokens": [51314, 583, 286, 519, 341, 307, 257, 534, 1880, 1168, 926, 14254, 293, 534, 1880, 1168, 926, 411, 437, 721, 820, 312, 294, 5081, 26039, 51, 5717, 437, 721, 820, 362, 641, 1065, 10644, 1752, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11977183703079965, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12926562130451202}, {"id": 703, "seek": 361658, "start": 3616.58, "end": 3620.58, "text": " even if there's still like a very AI assistant component to it.", "tokens": [50364, 754, 498, 456, 311, 920, 411, 257, 588, 7318, 10994, 6542, 281, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 704, "seek": 361658, "start": 3620.58, "end": 3624.58, "text": " This is one actually that I would expect lives outside of chat GPT.", "tokens": [50564, 639, 307, 472, 767, 300, 286, 576, 2066, 2909, 2380, 295, 5081, 26039, 51, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 705, "seek": 361658, "start": 3624.58, "end": 3625.58, "text": " And who knows, right?", "tokens": [50764, 400, 567, 3255, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 706, "seek": 361658, "start": 3625.58, "end": 3629.58, "text": " In the fullness of time, maybe you have like dynamic UI is getting generated on the fly.", "tokens": [50814, 682, 264, 45262, 295, 565, 11, 1310, 291, 362, 411, 8546, 15682, 307, 1242, 10833, 322, 264, 3603, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 707, "seek": 361658, "start": 3629.58, "end": 3631.58, "text": " We're starting to see that a little bit already.", "tokens": [51014, 492, 434, 2891, 281, 536, 300, 257, 707, 857, 1217, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 708, "seek": 361658, "start": 3631.58, "end": 3641.58, "text": " But I don't think open AI is going to say what we need to do is create like a UI where people can edit these graph things.", "tokens": [51114, 583, 286, 500, 380, 519, 1269, 7318, 307, 516, 281, 584, 437, 321, 643, 281, 360, 307, 1884, 411, 257, 15682, 689, 561, 393, 8129, 613, 4295, 721, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10352042869285301, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.024408703669905663}, {"id": 709, "seek": 364158, "start": 3641.58, "end": 3642.58, "text": " If possible, you could do that.", "tokens": [50364, 759, 1944, 11, 291, 727, 360, 300, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 710, "seek": 364158, "start": 3642.58, "end": 3648.58, "text": " GPTs don't really give you the ability to create like custom editor experiences yet anyway.", "tokens": [50414, 26039, 33424, 500, 380, 534, 976, 291, 264, 3485, 281, 1884, 411, 2375, 9839, 5235, 1939, 4033, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 711, "seek": 364158, "start": 3648.58, "end": 3652.58, "text": " So for now, if you want to have something like that, you have to bring it to a different app.", "tokens": [50714, 407, 337, 586, 11, 498, 291, 528, 281, 362, 746, 411, 300, 11, 291, 362, 281, 1565, 309, 281, 257, 819, 724, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 712, "seek": 364158, "start": 3652.58, "end": 3655.58, "text": " But increasingly, these are out there as well, right?", "tokens": [50914, 583, 12980, 11, 613, 366, 484, 456, 382, 731, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 713, "seek": 364158, "start": 3655.58, "end": 3657.58, "text": " They just use chat GPT and just a renderer.", "tokens": [51064, 814, 445, 764, 5081, 26039, 51, 293, 445, 257, 15529, 260, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 714, "seek": 364158, "start": 3657.58, "end": 3666.58, "text": " So I had the AI doing all the syntax and then the renderer showing me what it actually is and then going back and continuing the dialogue with chat GPT.", "tokens": [51164, 407, 286, 632, 264, 7318, 884, 439, 264, 28431, 293, 550, 264, 15529, 260, 4099, 385, 437, 309, 767, 307, 293, 550, 516, 646, 293, 9289, 264, 10221, 365, 5081, 26039, 51, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 715, "seek": 364158, "start": 3666.58, "end": 3667.58, "text": " I think you're right.", "tokens": [51614, 286, 519, 291, 434, 558, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09911853075027466, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2780247628688812}, {"id": 716, "seek": 366758, "start": 3667.58, "end": 3673.58, "text": " Like I could see a world where they let developers build their own renderers inside of chat GPT, not for like really serious stuff.", "tokens": [50364, 1743, 286, 727, 536, 257, 1002, 689, 436, 718, 8849, 1322, 641, 1065, 15529, 433, 1854, 295, 5081, 26039, 51, 11, 406, 337, 411, 534, 3156, 1507, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 717, "seek": 366758, "start": 3673.58, "end": 3681.58, "text": " I think like dabble in half one time or make a little video or whatever that like having something in the interface so that you can do it in there.", "tokens": [50664, 286, 519, 411, 28964, 638, 294, 1922, 472, 565, 420, 652, 257, 707, 960, 420, 2035, 300, 411, 1419, 746, 294, 264, 9226, 370, 300, 291, 393, 360, 309, 294, 456, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 718, "seek": 366758, "start": 3681.58, "end": 3683.58, "text": " Like a rough thing is really helpful.", "tokens": [51064, 1743, 257, 5903, 551, 307, 534, 4961, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 719, "seek": 366758, "start": 3683.58, "end": 3684.58, "text": " But then yeah, I think you're right.", "tokens": [51164, 583, 550, 1338, 11, 286, 519, 291, 434, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 720, "seek": 366758, "start": 3684.58, "end": 3691.58, "text": " There will have to be other pro tools for people that all they do all day is make graphs that are not inside of chat GPT.", "tokens": [51214, 821, 486, 362, 281, 312, 661, 447, 3873, 337, 561, 300, 439, 436, 360, 439, 786, 307, 652, 24877, 300, 366, 406, 1854, 295, 5081, 26039, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 721, "seek": 366758, "start": 3691.58, "end": 3692.58, "text": " So here's another one.", "tokens": [51564, 407, 510, 311, 1071, 472, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06452580103798518, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.06950036436319351}, {"id": 722, "seek": 369258, "start": 3692.58, "end": 3704.58, "text": " This is recent episode in my life where I had to admit defeat after 10 years of swearing that I would not replace my car until the replacement was self driving.", "tokens": [50364, 639, 307, 5162, 3500, 294, 452, 993, 689, 286, 632, 281, 9796, 11785, 934, 1266, 924, 295, 2484, 1921, 300, 286, 576, 406, 7406, 452, 1032, 1826, 264, 14419, 390, 2698, 4840, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 723, "seek": 369258, "start": 3704.58, "end": 3705.58, "text": " Wow.", "tokens": [50964, 3153, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 724, "seek": 369258, "start": 3705.58, "end": 3707.58, "text": " And we're not quite there.", "tokens": [51014, 400, 321, 434, 406, 1596, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 725, "seek": 369258, "start": 3707.58, "end": 3710.58, "text": " So I finally and I've had three kids in the meantime.", "tokens": [51114, 407, 286, 2721, 293, 286, 600, 632, 1045, 2301, 294, 264, 14991, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 726, "seek": 369258, "start": 3710.58, "end": 3715.58, "text": " So I finally had to break down and get a mini man like many parents of young kids.", "tokens": [51264, 407, 286, 2721, 632, 281, 1821, 760, 293, 483, 257, 8382, 587, 411, 867, 3152, 295, 2037, 2301, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 727, "seek": 369258, "start": 3715.58, "end": 3720.58, "text": " I'm like, what my kids do is they really depreciate stuff pretty quickly.", "tokens": [51514, 286, 478, 411, 11, 437, 452, 2301, 360, 307, 436, 534, 40609, 473, 1507, 1238, 2661, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08224563598632813, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.2874395549297333}, {"id": 728, "seek": 372058, "start": 3720.58, "end": 3728.58, "text": " So I was like, I think I'll get a used right mini man because if I get a new one, it's going to be pretty used pretty quick anyway.", "tokens": [50364, 407, 286, 390, 411, 11, 286, 519, 286, 603, 483, 257, 1143, 558, 8382, 587, 570, 498, 286, 483, 257, 777, 472, 11, 309, 311, 516, 281, 312, 1238, 1143, 1238, 1702, 4033, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 729, "seek": 372058, "start": 3728.58, "end": 3730.58, "text": " So let me just look at what is out there.", "tokens": [50764, 407, 718, 385, 445, 574, 412, 437, 307, 484, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 730, "seek": 372058, "start": 3730.58, "end": 3735.58, "text": " Now, anybody who's ever shop for a used car knows that it's a total jungle, right?", "tokens": [50864, 823, 11, 4472, 567, 311, 1562, 3945, 337, 257, 1143, 1032, 3255, 300, 309, 311, 257, 3217, 18228, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 731, "seek": 372058, "start": 3735.58, "end": 3737.58, "text": " And the car dealer websites are terrible.", "tokens": [51114, 400, 264, 1032, 24896, 12891, 366, 6237, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 732, "seek": 372058, "start": 3737.58, "end": 3741.58, "text": " What features they have is a huge question.", "tokens": [51214, 708, 4122, 436, 362, 307, 257, 2603, 1168, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 733, "seek": 372058, "start": 3741.58, "end": 3749.58, "text": " And what you end up encountering very quickly is these trim levels, which if you're not like a car head, you may not even know what that is.", "tokens": [51414, 400, 437, 291, 917, 493, 8593, 278, 588, 2661, 307, 613, 10445, 4358, 11, 597, 498, 291, 434, 406, 411, 257, 1032, 1378, 11, 291, 815, 406, 754, 458, 437, 300, 307, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08582227413470929, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.022968797013163567}, {"id": 734, "seek": 374958, "start": 3749.58, "end": 3758.58, "text": " But that is the sort of you've got your make, which is the brand of the car, your Chevrolet or your Toyota or whatever.", "tokens": [50364, 583, 300, 307, 264, 1333, 295, 291, 600, 658, 428, 652, 11, 597, 307, 264, 3360, 295, 264, 1032, 11, 428, 44236, 340, 2631, 420, 428, 22926, 420, 2035, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 735, "seek": 374958, "start": 3758.58, "end": 3762.58, "text": " You've got your model, which is the kind of car.", "tokens": [50814, 509, 600, 658, 428, 2316, 11, 597, 307, 264, 733, 295, 1032, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 736, "seek": 374958, "start": 3762.58, "end": 3766.58, "text": " And then the Dodge caravan is the make and model.", "tokens": [51014, 400, 550, 264, 41883, 1032, 21071, 307, 264, 652, 293, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 737, "seek": 374958, "start": 3766.58, "end": 3769.58, "text": " And then you've got this trim, which is often just like a couple of letters or whatever.", "tokens": [51214, 400, 550, 291, 600, 658, 341, 10445, 11, 597, 307, 2049, 445, 411, 257, 1916, 295, 7825, 420, 2035, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 738, "seek": 374958, "start": 3769.58, "end": 3774.58, "text": " It's like the XRT or the SRT or the L limited or whatever.", "tokens": [51364, 467, 311, 411, 264, 1783, 49, 51, 420, 264, 20840, 51, 420, 264, 441, 5567, 420, 2035, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 739, "seek": 374958, "start": 3774.58, "end": 3777.58, "text": " They just have all these like little and these are package levels, right?", "tokens": [51614, 814, 445, 362, 439, 613, 411, 707, 293, 613, 366, 7372, 4358, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.09798766745895636, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.004904787987470627}, {"id": 740, "seek": 377758, "start": 3777.58, "end": 3780.58, "text": " What features, what upsells have been included?", "tokens": [50364, 708, 4122, 11, 437, 493, 14555, 82, 362, 668, 5556, 30, 50514], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 741, "seek": 377758, "start": 3780.58, "end": 3782.58, "text": " Does it have a sunroof?", "tokens": [50514, 4402, 309, 362, 257, 3295, 340, 2670, 30, 50614], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 742, "seek": 377758, "start": 3782.58, "end": 3788.58, "text": " Does it have a screen in the back that drops down out of the ceiling for the kids or whatever?", "tokens": [50614, 4402, 309, 362, 257, 2568, 294, 264, 646, 300, 11438, 760, 484, 295, 264, 13655, 337, 264, 2301, 420, 2035, 30, 50914], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 743, "seek": 377758, "start": 3788.58, "end": 3789.58, "text": " Right.", "tokens": [50914, 1779, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 744, "seek": 377758, "start": 3789.58, "end": 3797.58, "text": " And it's just a jungle to even try to figure out what those things have, what levels there are and what those things have.", "tokens": [50964, 400, 309, 311, 445, 257, 18228, 281, 754, 853, 281, 2573, 484, 437, 729, 721, 362, 11, 437, 4358, 456, 366, 293, 437, 729, 721, 362, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 745, "seek": 377758, "start": 3797.58, "end": 3801.58, "text": " So this is perplexity, which is a great compliment to chat GPT.", "tokens": [51364, 407, 341, 307, 680, 18945, 507, 11, 597, 307, 257, 869, 16250, 281, 5081, 26039, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 746, "seek": 377758, "start": 3801.58, "end": 3806.58, "text": " It is more specifically focused on answering questions.", "tokens": [51564, 467, 307, 544, 4682, 5178, 322, 13430, 1651, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10587201676927172, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.033076900988817215}, {"id": 747, "seek": 380658, "start": 3806.58, "end": 3811.58, "text": " So in this way, it's a more direct rival to a Google search.", "tokens": [50364, 407, 294, 341, 636, 11, 309, 311, 257, 544, 2047, 16286, 281, 257, 3329, 3164, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 748, "seek": 380658, "start": 3811.58, "end": 3814.58, "text": " It's not so much meant to be like a brainstorming partner.", "tokens": [50614, 467, 311, 406, 370, 709, 4140, 281, 312, 411, 257, 35245, 278, 4975, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 749, "seek": 380658, "start": 3814.58, "end": 3821.58, "text": " They really aim for accurate answers to concrete questions, do a phenomenal job on it.", "tokens": [50764, 814, 534, 5939, 337, 8559, 6338, 281, 9859, 1651, 11, 360, 257, 17778, 1691, 322, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 750, "seek": 380658, "start": 3821.58, "end": 3825.58, "text": " So here I had a number of runs of this as well, different kinds of questions or whatever.", "tokens": [51114, 407, 510, 286, 632, 257, 1230, 295, 6676, 295, 341, 382, 731, 11, 819, 3685, 295, 1651, 420, 2035, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 751, "seek": 380658, "start": 3825.58, "end": 3831.58, "text": " But okay, these minivans that are like not super old, but old, pretty cheap.", "tokens": [51314, 583, 1392, 11, 613, 923, 592, 599, 300, 366, 411, 406, 1687, 1331, 11, 457, 1331, 11, 1238, 7084, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 752, "seek": 380658, "start": 3831.58, "end": 3832.58, "text": " What do they have?", "tokens": [51614, 708, 360, 436, 362, 30, 51664], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 753, "seek": 380658, "start": 3832.58, "end": 3833.58, "text": " What do they not have?", "tokens": [51664, 708, 360, 436, 406, 362, 30, 51714], "temperature": 0.0, "avg_logprob": -0.08960739771525066, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.011329278349876404}, {"id": 754, "seek": 383358, "start": 3833.58, "end": 3837.58, "text": " And this would have taken, I don't even know if I had really tried, I wouldn't have done it.", "tokens": [50364, 400, 341, 576, 362, 2726, 11, 286, 500, 380, 754, 458, 498, 286, 632, 534, 3031, 11, 286, 2759, 380, 362, 1096, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0932404949979962, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.022278226912021637}, {"id": 755, "seek": 383358, "start": 3837.58, "end": 3839.58, "text": " This is one of those things that you just, I wouldn't do.", "tokens": [50564, 639, 307, 472, 295, 729, 721, 300, 291, 445, 11, 286, 2759, 380, 360, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0932404949979962, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.022278226912021637}, {"id": 756, "seek": 383358, "start": 3839.58, "end": 3850.58, "text": " But if you had set out to go collate, okay, here's all the makes and the models and the trims and what they have, you're going to be in like user manuals or something.", "tokens": [50664, 583, 498, 291, 632, 992, 484, 281, 352, 1263, 473, 11, 1392, 11, 510, 311, 439, 264, 1669, 293, 264, 5245, 293, 264, 10445, 82, 293, 437, 436, 362, 11, 291, 434, 516, 281, 312, 294, 411, 4195, 9688, 82, 420, 746, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0932404949979962, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.022278226912021637}, {"id": 757, "seek": 383358, "start": 3850.58, "end": 3854.58, "text": " I don't even know really where that information is stored around truth.", "tokens": [51214, 286, 500, 380, 754, 458, 534, 689, 300, 1589, 307, 12187, 926, 3494, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0932404949979962, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.022278226912021637}, {"id": 758, "seek": 385458, "start": 3855.58, "end": 3864.58, "text": " But just in asking that question, I was able to get the trim levels for all of the different brands for this window of time.", "tokens": [50414, 583, 445, 294, 3365, 300, 1168, 11, 286, 390, 1075, 281, 483, 264, 10445, 4358, 337, 439, 295, 264, 819, 11324, 337, 341, 4910, 295, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10492515563964844, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.8436976075172424}, {"id": 759, "seek": 385458, "start": 3864.58, "end": 3872.58, "text": " And just easily get a handle now that I could reference back to, okay, this one on this dealer site, it doesn't have any pictures.", "tokens": [50864, 400, 445, 3612, 483, 257, 4813, 586, 300, 286, 727, 6408, 646, 281, 11, 1392, 11, 341, 472, 322, 341, 24896, 3621, 11, 309, 1177, 380, 362, 604, 5242, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10492515563964844, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.8436976075172424}, {"id": 760, "seek": 385458, "start": 3872.58, "end": 3877.58, "text": " It doesn't say anything, but it does say, for example, oh, it's an, it's a SXT.", "tokens": [51264, 467, 1177, 380, 584, 1340, 11, 457, 309, 775, 584, 11, 337, 1365, 11, 1954, 11, 309, 311, 364, 11, 309, 311, 257, 318, 20542, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10492515563964844, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.8436976075172424}, {"id": 761, "seek": 385458, "start": 3877.58, "end": 3878.58, "text": " Okay, cool.", "tokens": [51514, 1033, 11, 1627, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10492515563964844, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.8436976075172424}, {"id": 762, "seek": 385458, "start": 3878.58, "end": 3882.58, "text": " Now I can at least know that is the second of however many trim levels or whatever.", "tokens": [51564, 823, 286, 393, 412, 1935, 458, 300, 307, 264, 1150, 295, 4461, 867, 10445, 4358, 420, 2035, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10492515563964844, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.8436976075172424}, {"id": 763, "seek": 388258, "start": 3882.58, "end": 3886.58, "text": " So the SE, that's your top one, your SXT, that's your, you can imagine, right?", "tokens": [50364, 407, 264, 10269, 11, 300, 311, 428, 1192, 472, 11, 428, 318, 20542, 11, 300, 311, 428, 11, 291, 393, 3811, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 764, "seek": 388258, "start": 3886.58, "end": 3888.58, "text": " Trying to sort this out on your own.", "tokens": [50564, 20180, 281, 1333, 341, 484, 322, 428, 1065, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 765, "seek": 388258, "start": 3888.58, "end": 3890.58, "text": " And then you get the AVP slash SE.", "tokens": [50664, 400, 550, 291, 483, 264, 30198, 47, 17330, 10269, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 766, "seek": 388258, "start": 3890.58, "end": 3892.58, "text": " Well, who comes up with this stuff?", "tokens": [50764, 1042, 11, 567, 1487, 493, 365, 341, 1507, 30, 50864], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 767, "seek": 388258, "start": 3892.58, "end": 3893.58, "text": " It's ridiculous.", "tokens": [50864, 467, 311, 11083, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 768, "seek": 388258, "start": 3893.58, "end": 3898.58, "text": " But it's super useful if you're like, I don't want to drive across Metro Detroit to go look at this minivan.", "tokens": [50914, 583, 309, 311, 1687, 4420, 498, 291, 434, 411, 11, 286, 500, 380, 528, 281, 3332, 2108, 25598, 20887, 281, 352, 574, 412, 341, 923, 24193, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 769, "seek": 388258, "start": 3898.58, "end": 3901.58, "text": " If it doesn't have something that I really cared about.", "tokens": [51164, 759, 309, 1177, 380, 362, 746, 300, 286, 534, 19779, 466, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 770, "seek": 388258, "start": 3901.58, "end": 3904.58, "text": " And the things that I were zeroed in on were like fairly basic safety features.", "tokens": [51314, 400, 264, 721, 300, 286, 645, 4018, 292, 294, 322, 645, 411, 6457, 3875, 4514, 4122, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 771, "seek": 388258, "start": 3904.58, "end": 3907.58, "text": " I wanted the blind spot detection and the backup camera.", "tokens": [51464, 286, 1415, 264, 6865, 4008, 17784, 293, 264, 14807, 2799, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14212980470457276, "compression_ratio": 1.6134185303514377, "no_speech_prob": 0.037320688366889954}, {"id": 772, "seek": 390758, "start": 3907.58, "end": 3915.58, "text": " So there were other questions too, like when did USB charging get introduced into cars in general?", "tokens": [50364, 407, 456, 645, 661, 1651, 886, 11, 411, 562, 630, 10109, 11379, 483, 7268, 666, 5163, 294, 2674, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 773, "seek": 390758, "start": 3915.58, "end": 3917.58, "text": " I didn't know the answer to that.", "tokens": [50764, 286, 994, 380, 458, 264, 1867, 281, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 774, "seek": 390758, "start": 3917.58, "end": 3922.58, "text": " I'm old enough to remember when you had to plug the thing into the lighter and I didn't want that.", "tokens": [50864, 286, 478, 1331, 1547, 281, 1604, 562, 291, 632, 281, 5452, 264, 551, 666, 264, 11546, 293, 286, 994, 380, 528, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 775, "seek": 390758, "start": 3922.58, "end": 3925.58, "text": " So I don't want a car that's that old where I have to use the lighter outlet anymore.", "tokens": [51114, 407, 286, 500, 380, 528, 257, 1032, 300, 311, 300, 1331, 689, 286, 362, 281, 764, 264, 11546, 20656, 3602, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 776, "seek": 390758, "start": 3925.58, "end": 3931.58, "text": " I want a car that's at least into the like USB charger era, but when did the USB charger era begin for cars?", "tokens": [51264, 286, 528, 257, 1032, 300, 311, 412, 1935, 666, 264, 411, 10109, 22213, 4249, 11, 457, 562, 630, 264, 10109, 22213, 4249, 1841, 337, 5163, 30, 51564], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 777, "seek": 390758, "start": 3931.58, "end": 3933.58, "text": " That was another one that perplexity was able to answer.", "tokens": [51564, 663, 390, 1071, 472, 300, 680, 18945, 507, 390, 1075, 281, 1867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 778, "seek": 390758, "start": 3933.58, "end": 3935.58, "text": " And it is so good.", "tokens": [51664, 400, 309, 307, 370, 665, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07646267396166809, "compression_ratio": 1.8455882352941178, "no_speech_prob": 0.4763862192630768}, {"id": 779, "seek": 393558, "start": 3936.58, "end": 3939.58, "text": " I think this is about to be a huge trend.", "tokens": [50414, 286, 519, 341, 307, 466, 281, 312, 257, 2603, 6028, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 780, "seek": 393558, "start": 3939.58, "end": 3942.58, "text": " If I had to guess, because I've been a big fan of this app for a while.", "tokens": [50564, 759, 286, 632, 281, 2041, 11, 570, 286, 600, 668, 257, 955, 3429, 295, 341, 724, 337, 257, 1339, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 781, "seek": 393558, "start": 3942.58, "end": 3949.58, "text": " I had the CEO, Arvind on the Cognitive Edition twice and they just they ship super fast.", "tokens": [50714, 286, 632, 264, 9282, 11, 1587, 85, 471, 322, 264, 383, 2912, 2187, 25301, 6091, 293, 436, 445, 436, 5374, 1687, 2370, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 782, "seek": 393558, "start": 3949.58, "end": 3953.58, "text": " They win head to head comparisons for answer accuracy.", "tokens": [51064, 814, 1942, 1378, 281, 1378, 33157, 337, 1867, 14170, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 783, "seek": 393558, "start": 3953.58, "end": 3955.58, "text": " The product itself is super fast.", "tokens": [51264, 440, 1674, 2564, 307, 1687, 2370, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 784, "seek": 393558, "start": 3955.58, "end": 3962.58, "text": " It's got great UI with these sources and others starting to become more multimodal with images as well, which is relatively new.", "tokens": [51364, 467, 311, 658, 869, 15682, 365, 613, 7139, 293, 2357, 2891, 281, 1813, 544, 32972, 378, 304, 365, 5267, 382, 731, 11, 597, 307, 7226, 777, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13149907759257726, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.005383792333304882}, {"id": 785, "seek": 396258, "start": 3962.58, "end": 3965.58, "text": " We're just a great experience all the way around.", "tokens": [50364, 492, 434, 445, 257, 869, 1752, 439, 264, 636, 926, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10039473616558572, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.015184505842626095}, {"id": 786, "seek": 396258, "start": 3965.58, "end": 3972.58, "text": " And I see it as like setting a new standard for answers that are like I started.", "tokens": [50514, 400, 286, 536, 309, 382, 411, 3287, 257, 777, 3832, 337, 6338, 300, 366, 411, 286, 1409, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10039473616558572, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.015184505842626095}, {"id": 787, "seek": 396258, "start": 3972.58, "end": 3981.58, "text": " I'm starting to use the term per perplexity to say I'm not sure this is necessarily rock solid ground truth.", "tokens": [50864, 286, 478, 2891, 281, 764, 264, 1433, 680, 680, 18945, 507, 281, 584, 286, 478, 406, 988, 341, 307, 4725, 3727, 5100, 2727, 3494, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10039473616558572, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.015184505842626095}, {"id": 788, "seek": 396258, "start": 3981.58, "end": 3987.58, "text": " Like perplexity is not always right, but it's the most accurate AI tool.", "tokens": [51314, 1743, 680, 18945, 507, 307, 406, 1009, 558, 11, 457, 309, 311, 264, 881, 8559, 7318, 2290, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10039473616558572, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.015184505842626095}, {"id": 789, "seek": 396258, "start": 3987.58, "end": 3990.58, "text": " It's usually right in my experience.", "tokens": [51614, 467, 311, 2673, 558, 294, 452, 1752, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10039473616558572, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.015184505842626095}, {"id": 790, "seek": 399058, "start": 3990.58, "end": 3997.58, "text": " You might be able to find something here that is wrong, but everything I ended up fact checking turned out to be true.", "tokens": [50364, 509, 1062, 312, 1075, 281, 915, 746, 510, 300, 307, 2085, 11, 457, 1203, 286, 4590, 493, 1186, 8568, 3574, 484, 281, 312, 2074, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09681856948717506, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.010983219370245934}, {"id": 791, "seek": 399058, "start": 3997.58, "end": 4010.58, "text": " And so there's this kind of very interesting, good enough for practical purposes standard where I don't necessarily need it to be 100, 100 percent accurate for it to be very useful.", "tokens": [50714, 400, 370, 456, 311, 341, 733, 295, 588, 1880, 11, 665, 1547, 337, 8496, 9932, 3832, 689, 286, 500, 380, 4725, 643, 309, 281, 312, 2319, 11, 2319, 3043, 8559, 337, 309, 281, 312, 588, 4420, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09681856948717506, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.010983219370245934}, {"id": 792, "seek": 399058, "start": 4010.58, "end": 4012.58, "text": " And I would make my decisions.", "tokens": [51364, 400, 286, 576, 652, 452, 5327, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09681856948717506, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.010983219370245934}, {"id": 793, "seek": 399058, "start": 4012.58, "end": 4019.58, "text": " Did I trust it enough, for example, to be confident that there was in fact going to be a USB charger in the car that I went to go look at?", "tokens": [51464, 2589, 286, 3361, 309, 1547, 11, 337, 1365, 11, 281, 312, 6679, 300, 456, 390, 294, 1186, 516, 281, 312, 257, 10109, 22213, 294, 264, 1032, 300, 286, 1437, 281, 352, 574, 412, 30, 51814], "temperature": 0.0, "avg_logprob": -0.09681856948717506, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.010983219370245934}, {"id": 794, "seek": 401958, "start": 4019.58, "end": 4023.58, "text": " Yes. And in fact, it was correct about that.", "tokens": [50364, 1079, 13, 400, 294, 1186, 11, 309, 390, 3006, 466, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 795, "seek": 401958, "start": 4023.58, "end": 4032.58, "text": " And so I have this kind of per perplexity standard of verification in my mind now where I'm like, yeah, it's in many situations.", "tokens": [50564, 400, 370, 286, 362, 341, 733, 295, 680, 680, 18945, 507, 3832, 295, 30206, 294, 452, 1575, 586, 689, 286, 478, 411, 11, 1338, 11, 309, 311, 294, 867, 6851, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 796, "seek": 401958, "start": 4032.58, "end": 4034.58, "text": " It's like good enough to act on.", "tokens": [51014, 467, 311, 411, 665, 1547, 281, 605, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 797, "seek": 401958, "start": 4034.58, "end": 4042.58, "text": " I wouldn't make life and death decisions without more fact checking, but I don't even need to follow these links in most cases now for something like this.", "tokens": [51114, 286, 2759, 380, 652, 993, 293, 2966, 5327, 1553, 544, 1186, 8568, 11, 457, 286, 500, 380, 754, 643, 281, 1524, 613, 6123, 294, 881, 3331, 586, 337, 746, 411, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 798, "seek": 401958, "start": 4042.58, "end": 4043.58, "text": " I'll trust it.", "tokens": [51514, 286, 603, 3361, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 799, "seek": 401958, "start": 4043.58, "end": 4047.58, "text": " And it's an emerging like standard in the family as well.", "tokens": [51564, 400, 309, 311, 364, 14989, 411, 3832, 294, 264, 1605, 382, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10046502639507425, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.004467822145670652}, {"id": 800, "seek": 404758, "start": 4047.58, "end": 4051.58, "text": " My wife asks, do we really have to get a car that's that old? Do they have this? Do they have that?", "tokens": [50364, 1222, 3836, 8962, 11, 360, 321, 534, 362, 281, 483, 257, 1032, 300, 311, 300, 1331, 30, 1144, 436, 362, 341, 30, 1144, 436, 362, 300, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 801, "seek": 404758, "start": 4051.58, "end": 4057.58, "text": " And I was able to ask perplexity and send her like, yep, it should have a backup camera per perplexity.", "tokens": [50564, 400, 286, 390, 1075, 281, 1029, 680, 18945, 507, 293, 2845, 720, 411, 11, 18633, 11, 309, 820, 362, 257, 14807, 2799, 680, 680, 18945, 507, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 802, "seek": 404758, "start": 4057.58, "end": 4060.58, "text": " It should have a USB charger.", "tokens": [50864, 467, 820, 362, 257, 10109, 22213, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 803, "seek": 404758, "start": 4060.58, "end": 4063.58, "text": " It should have the blind spot detection.", "tokens": [51014, 467, 820, 362, 264, 6865, 4008, 17784, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 804, "seek": 404758, "start": 4063.58, "end": 4067.58, "text": " And it's an incredible time saver.", "tokens": [51164, 400, 309, 311, 364, 4651, 565, 601, 331, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 805, "seek": 404758, "start": 4067.58, "end": 4074.58, "text": " I think I'm just a worthy alternative to even something like a wire cutter, which has been the standard that my wife has used for a long time.", "tokens": [51364, 286, 519, 286, 478, 445, 257, 14829, 8535, 281, 754, 746, 411, 257, 6234, 25531, 11, 597, 575, 668, 264, 3832, 300, 452, 3836, 575, 1143, 337, 257, 938, 565, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08138960551440232, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.03208979219198227}, {"id": 806, "seek": 407458, "start": 4074.58, "end": 4079.58, "text": " But obviously that's an editorial approach where you can't just ask any question you want to ask here.", "tokens": [50364, 583, 2745, 300, 311, 364, 33412, 3109, 689, 291, 393, 380, 445, 1029, 604, 1168, 291, 528, 281, 1029, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08352964958258435, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.022281013429164886}, {"id": 807, "seek": 407458, "start": 4079.58, "end": 4081.58, "text": " You can ask any question you want to ask.", "tokens": [50614, 509, 393, 1029, 604, 1168, 291, 528, 281, 1029, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08352964958258435, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.022281013429164886}, {"id": 808, "seek": 407458, "start": 4081.58, "end": 4088.58, "text": " And I think you do get something oftentimes that is like a worthy rival even to a much more editorial product.", "tokens": [50714, 400, 286, 519, 291, 360, 483, 746, 18349, 300, 307, 411, 257, 14829, 16286, 754, 281, 257, 709, 544, 33412, 1674, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08352964958258435, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.022281013429164886}, {"id": 809, "seek": 407458, "start": 4088.58, "end": 4090.58, "text": " No, that makes perfect sense.", "tokens": [51064, 883, 11, 300, 1669, 2176, 2020, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08352964958258435, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.022281013429164886}, {"id": 810, "seek": 407458, "start": 4090.58, "end": 4103.58, "text": " It reminds me of wire cutter reminds me of there are all those sites that are like Cora, but it's like for this new generation where no one had to think previously to ask this particular question.", "tokens": [51164, 467, 12025, 385, 295, 6234, 25531, 12025, 385, 295, 456, 366, 439, 729, 7533, 300, 366, 411, 383, 3252, 11, 457, 309, 311, 411, 337, 341, 777, 5125, 689, 572, 472, 632, 281, 519, 8046, 281, 1029, 341, 1729, 1168, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08352964958258435, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.022281013429164886}, {"id": 811, "seek": 410358, "start": 4103.58, "end": 4106.58, "text": " I can just gather and answer the question for you immediately.", "tokens": [50364, 286, 393, 445, 5448, 293, 1867, 264, 1168, 337, 291, 4258, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 812, "seek": 410358, "start": 4106.58, "end": 4109.58, "text": " And I think that's so it's so powerful.", "tokens": [50514, 400, 286, 519, 300, 311, 370, 309, 311, 370, 4005, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 813, "seek": 410358, "start": 4109.58, "end": 4114.58, "text": " Like it's really starting to click for me when and how I might, I might use it.", "tokens": [50664, 1743, 309, 311, 534, 2891, 281, 2052, 337, 385, 562, 293, 577, 286, 1062, 11, 286, 1062, 764, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 814, "seek": 410358, "start": 4114.58, "end": 4121.58, "text": " There are so many questions I have this where I'm like, I basically want to get to the best answer for a fact based question, more or less.", "tokens": [50914, 821, 366, 370, 867, 1651, 286, 362, 341, 689, 286, 478, 411, 11, 286, 1936, 528, 281, 483, 281, 264, 1151, 1867, 337, 257, 1186, 2361, 1168, 11, 544, 420, 1570, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 815, "seek": 410358, "start": 4121.58, "end": 4122.58, "text": " And I'm so lazy.", "tokens": [51264, 400, 286, 478, 370, 14847, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 816, "seek": 410358, "start": 4122.58, "end": 4130.58, "text": " I really don't want to do all the research and chat to BT will kind of like, it'll do one search and then sort of crib the first article.", "tokens": [51314, 286, 534, 500, 380, 528, 281, 360, 439, 264, 2132, 293, 5081, 281, 31144, 486, 733, 295, 411, 11, 309, 603, 360, 472, 3164, 293, 550, 1333, 295, 47163, 264, 700, 7222, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 817, "seek": 410358, "start": 4130.58, "end": 4132.58, "text": " And this feels a lot better than that.", "tokens": [51714, 400, 341, 3417, 257, 688, 1101, 813, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11394659220743522, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3004460036754608}, {"id": 818, "seek": 413258, "start": 4132.58, "end": 4133.58, "text": " Yeah, it's really good.", "tokens": [50364, 865, 11, 309, 311, 534, 665, 13, 50414], "temperature": 0.0, "avg_logprob": -0.11891597566150483, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.00348276668228209}, {"id": 819, "seek": 413258, "start": 4133.58, "end": 4137.58, "text": " It's faster than chat to BT on the browsing side.", "tokens": [50414, 467, 311, 4663, 813, 5081, 281, 31144, 322, 264, 38602, 1252, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11891597566150483, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.00348276668228209}, {"id": 820, "seek": 413258, "start": 4137.58, "end": 4146.58, "text": " So you're getting to answer notably faster and marginally more accurate, just more the sort of answer that I want.", "tokens": [50614, 407, 291, 434, 1242, 281, 1867, 31357, 4663, 293, 10270, 379, 544, 8559, 11, 445, 544, 264, 1333, 295, 1867, 300, 286, 528, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11891597566150483, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.00348276668228209}, {"id": 821, "seek": 413258, "start": 4146.58, "end": 4155.58, "text": " A lot of times like I've had a couple of instances where I tried the same thing with chat to BT and I was able to get there, but it was like slower on the browse.", "tokens": [51064, 316, 688, 295, 1413, 411, 286, 600, 632, 257, 1916, 295, 14519, 689, 286, 3031, 264, 912, 551, 365, 5081, 281, 31144, 293, 286, 390, 1075, 281, 483, 456, 11, 457, 309, 390, 411, 14009, 322, 264, 31442, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11891597566150483, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.00348276668228209}, {"id": 822, "seek": 413258, "start": 4155.58, "end": 4158.58, "text": " Didn't give me the full answer the first time.", "tokens": [51514, 11151, 380, 976, 385, 264, 1577, 1867, 264, 700, 565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11891597566150483, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.00348276668228209}, {"id": 823, "seek": 415858, "start": 4158.58, "end": 4160.58, "text": " I was like, no, but I need a little more.", "tokens": [50364, 286, 390, 411, 11, 572, 11, 457, 286, 643, 257, 707, 544, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 824, "seek": 415858, "start": 4160.58, "end": 4162.58, "text": " And then I was able to get over the hump and get there.", "tokens": [50464, 400, 550, 286, 390, 1075, 281, 483, 670, 264, 47093, 293, 483, 456, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 825, "seek": 415858, "start": 4162.58, "end": 4171.58, "text": " But this was definitely just a faster, cleaner experience that I do believe is a bit more accurate as well.", "tokens": [50564, 583, 341, 390, 2138, 445, 257, 4663, 11, 16532, 1752, 300, 286, 360, 1697, 307, 257, 857, 544, 8559, 382, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 826, "seek": 415858, "start": 4171.58, "end": 4175.58, "text": " It goes to show that there are different roles that you want AI to play.", "tokens": [51014, 467, 1709, 281, 855, 300, 456, 366, 819, 9604, 300, 291, 528, 7318, 281, 862, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 827, "seek": 415858, "start": 4175.58, "end": 4178.58, "text": " And I think there is, it's interesting.", "tokens": [51214, 400, 286, 519, 456, 307, 11, 309, 311, 1880, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 828, "seek": 415858, "start": 4178.58, "end": 4180.58, "text": " There's forces pushing both ways, right?", "tokens": [51364, 821, 311, 5874, 7380, 1293, 2098, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 829, "seek": 415858, "start": 4180.58, "end": 4184.58, "text": " What makes the AIs so compelling is that they're extremely general purpose.", "tokens": [51464, 708, 1669, 264, 316, 6802, 370, 20050, 307, 300, 436, 434, 4664, 2674, 4334, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08776950006899627, "compression_ratio": 1.6051660516605166, "no_speech_prob": 0.04082663357257843}, {"id": 830, "seek": 418458, "start": 4184.58, "end": 4192.58, "text": " And it seems like there is something, there is like a fundamental reality that they get really powerful at scale and to scale.", "tokens": [50364, 400, 309, 2544, 411, 456, 307, 746, 11, 456, 307, 411, 257, 8088, 4103, 300, 436, 483, 534, 4005, 412, 4373, 293, 281, 4373, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 831, "seek": 418458, "start": 4192.58, "end": 4194.58, "text": " They have to be the general purpose.", "tokens": [50764, 814, 362, 281, 312, 264, 2674, 4334, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 832, "seek": 418458, "start": 4194.58, "end": 4196.58, "text": " And that kind of comes as a package.", "tokens": [50864, 400, 300, 733, 295, 1487, 382, 257, 7372, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 833, "seek": 418458, "start": 4196.58, "end": 4198.58, "text": " But here the scope has been narrowed.", "tokens": [50964, 583, 510, 264, 11923, 575, 668, 9432, 292, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 834, "seek": 418458, "start": 4198.58, "end": 4204.58, "text": " And there are a lot of things that chat to BT does for people that this is not trying to do for people.", "tokens": [51064, 400, 456, 366, 257, 688, 295, 721, 300, 5081, 281, 31144, 775, 337, 561, 300, 341, 307, 406, 1382, 281, 360, 337, 561, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 835, "seek": 418458, "start": 4204.58, "end": 4213.58, "text": " And in its specialization, it does seem to be achieving higher heights in the domain that it really attempts to be best.", "tokens": [51364, 400, 294, 1080, 2121, 2144, 11, 309, 775, 1643, 281, 312, 19626, 2946, 25930, 294, 264, 9274, 300, 309, 534, 15257, 281, 312, 1151, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09947939086378667, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.1438954919576645}, {"id": 836, "seek": 421358, "start": 4214.58, "end": 4218.58, "text": " So I definitely recommend perplexity a lot.", "tokens": [50414, 407, 286, 2138, 2748, 680, 18945, 507, 257, 688, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13615991714152884, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.043334558606147766}, {"id": 837, "seek": 421358, "start": 4218.58, "end": 4223.58, "text": " And I'm just old enough to remember when people were first saying that they were Googling it.", "tokens": [50614, 400, 286, 478, 445, 1331, 1547, 281, 1604, 562, 561, 645, 700, 1566, 300, 436, 645, 45005, 1688, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13615991714152884, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.043334558606147766}, {"id": 838, "seek": 421358, "start": 4223.58, "end": 4232.58, "text": " And this has a similar vibe to me where it's a standard that I think people can comfortably socially transact on and feel like they're pretty solid.", "tokens": [50864, 400, 341, 575, 257, 2531, 14606, 281, 385, 689, 309, 311, 257, 3832, 300, 286, 519, 561, 393, 25101, 21397, 46688, 322, 293, 841, 411, 436, 434, 1238, 5100, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13615991714152884, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.043334558606147766}, {"id": 839, "seek": 421358, "start": 4232.58, "end": 4233.58, "text": " I love this.", "tokens": [51314, 286, 959, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13615991714152884, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.043334558606147766}, {"id": 840, "seek": 421358, "start": 4233.58, "end": 4239.58, "text": " Like you're using it to build stuff or also really using it to fuel your curiosity.", "tokens": [51364, 1743, 291, 434, 1228, 309, 281, 1322, 1507, 420, 611, 534, 1228, 309, 281, 6616, 428, 18769, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13615991714152884, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.043334558606147766}, {"id": 841, "seek": 423958, "start": 4239.58, "end": 4244.58, "text": " And I'm curious, like, you know, before we wrap up, what are you excited about now?", "tokens": [50364, 400, 286, 478, 6369, 11, 411, 11, 291, 458, 11, 949, 321, 7019, 493, 11, 437, 366, 291, 2919, 466, 586, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1431153338888417, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.12929168343544006}, {"id": 842, "seek": 423958, "start": 4244.58, "end": 4246.58, "text": " What are you thinking about right now?", "tokens": [50614, 708, 366, 291, 1953, 466, 558, 586, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1431153338888417, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.12929168343544006}, {"id": 843, "seek": 423958, "start": 4246.58, "end": 4254.58, "text": " Like what's on your radar that you think people should be paying attention to in chat to BT, maybe specifically, but like broadly in AI over the next couple years.", "tokens": [50714, 1743, 437, 311, 322, 428, 16544, 300, 291, 519, 561, 820, 312, 6229, 3202, 281, 294, 5081, 281, 31144, 11, 1310, 4682, 11, 457, 411, 19511, 294, 7318, 670, 264, 958, 1916, 924, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1431153338888417, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.12929168343544006}, {"id": 844, "seek": 423958, "start": 4254.58, "end": 4262.58, "text": " Boy, broadly in AI over the next couple years, I think almost anything's possible.", "tokens": [51114, 9486, 11, 19511, 294, 7318, 670, 264, 958, 1916, 924, 11, 286, 519, 1920, 1340, 311, 1944, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1431153338888417, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.12929168343544006}, {"id": 845, "seek": 426258, "start": 4263.58, "end": 4272.58, "text": " I take the leaders of the field pretty much at their word in terms of being honest reflections of their expectations.", "tokens": [50414, 286, 747, 264, 3523, 295, 264, 2519, 1238, 709, 412, 641, 1349, 294, 2115, 295, 885, 3245, 30679, 295, 641, 9843, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0935226637741615, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.11911217868328094}, {"id": 846, "seek": 426258, "start": 4272.58, "end": 4276.58, "text": " And you listen to what Sam Altman thinks might happen over the next couple years.", "tokens": [50864, 400, 291, 2140, 281, 437, 4832, 15992, 1601, 7309, 1062, 1051, 670, 264, 958, 1916, 924, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0935226637741615, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.11911217868328094}, {"id": 847, "seek": 426258, "start": 4276.58, "end": 4281.58, "text": " You listen to what Dario Amade from Anthropic thinks might happen over the next couple years.", "tokens": [51064, 509, 2140, 281, 437, 413, 4912, 2012, 762, 490, 12727, 39173, 7309, 1062, 1051, 670, 264, 958, 1916, 924, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0935226637741615, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.11911217868328094}, {"id": 848, "seek": 426258, "start": 4281.58, "end": 4290.58, "text": " And we are potentially looking at something that is superhuman in very substantial and meaningful ways.", "tokens": [51314, 400, 321, 366, 7263, 1237, 412, 746, 300, 307, 1687, 18796, 294, 588, 16726, 293, 10995, 2098, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0935226637741615, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.11911217868328094}, {"id": 849, "seek": 429058, "start": 4290.58, "end": 4296.58, "text": " I think there's a lot of kind of conflation and talking past one another when people try to analyze this.", "tokens": [50364, 286, 519, 456, 311, 257, 688, 295, 733, 295, 1497, 24278, 293, 1417, 1791, 472, 1071, 562, 561, 853, 281, 12477, 341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.04395579047825025, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.019111141562461853}, {"id": 850, "seek": 429058, "start": 4296.58, "end": 4304.58, "text": " And I do think it's important to say you can be superhuman in very consequential ways without being like omnipotent or infallible.", "tokens": [50664, 400, 286, 360, 519, 309, 311, 1021, 281, 584, 291, 393, 312, 1687, 18796, 294, 588, 7242, 2549, 2098, 1553, 885, 411, 36874, 647, 310, 317, 420, 1536, 336, 964, 13, 51064], "temperature": 0.0, "avg_logprob": -0.04395579047825025, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.019111141562461853}, {"id": 851, "seek": 429058, "start": 4304.58, "end": 4311.58, "text": " And I think there's actually quite a lot of space right between like human performance and omnipotence or infallibility.", "tokens": [51064, 400, 286, 519, 456, 311, 767, 1596, 257, 688, 295, 1901, 558, 1296, 411, 1952, 3389, 293, 36874, 647, 310, 655, 420, 1536, 336, 2841, 13, 51414], "temperature": 0.0, "avg_logprob": -0.04395579047825025, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.019111141562461853}, {"id": 852, "seek": 429058, "start": 4311.58, "end": 4318.58, "text": " And I kind of expect that AI is going to land there for a lot of different things over the next couple of years.", "tokens": [51414, 400, 286, 733, 295, 2066, 300, 7318, 307, 516, 281, 2117, 456, 337, 257, 688, 295, 819, 721, 670, 264, 958, 1916, 295, 924, 13, 51764], "temperature": 0.0, "avg_logprob": -0.04395579047825025, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.019111141562461853}, {"id": 853, "seek": 431858, "start": 4318.58, "end": 4325.58, "text": " So I think the value of the kinds of things that we will be engaging with it for is only headed up.", "tokens": [50364, 407, 286, 519, 264, 2158, 295, 264, 3685, 295, 721, 300, 321, 486, 312, 11268, 365, 309, 337, 307, 787, 12798, 493, 13, 50714], "temperature": 0.0, "avg_logprob": -0.05542091153702646, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.007117855828255415}, {"id": 854, "seek": 431858, "start": 4325.58, "end": 4337.58, "text": " Just a recent result from Google DeepMind on using their best language models for differential diagnosis was an extremely striking result.", "tokens": [50714, 1449, 257, 5162, 1874, 490, 3329, 14895, 44, 471, 322, 1228, 641, 1151, 2856, 5245, 337, 15756, 15217, 390, 364, 4664, 18559, 1874, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05542091153702646, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.007117855828255415}, {"id": 855, "seek": 433758, "start": 4337.58, "end": 4340.58, "text": " This team has been on an absolute terror.", "tokens": [50364, 639, 1469, 575, 668, 322, 364, 8236, 8127, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 856, "seek": 433758, "start": 4340.58, "end": 4350.58, "text": " It was only maybe like a year ago that they first got a language model to like hit passing level on medical licensing tests, which hey, that's crazy.", "tokens": [50514, 467, 390, 787, 1310, 411, 257, 1064, 2057, 300, 436, 700, 658, 257, 2856, 2316, 281, 411, 2045, 8437, 1496, 322, 4625, 29759, 6921, 11, 597, 4177, 11, 300, 311, 3219, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 857, "seek": 433758, "start": 4350.58, "end": 4352.58, "text": " But you can just kind of say, well, it's a test.", "tokens": [51014, 583, 291, 393, 445, 733, 295, 584, 11, 731, 11, 309, 311, 257, 1500, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 858, "seek": 433758, "start": 4352.58, "end": 4353.58, "text": " It's more structured.", "tokens": [51114, 467, 311, 544, 18519, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 859, "seek": 433758, "start": 4353.58, "end": 4356.58, "text": " The real world is messy and they're only passing.", "tokens": [51164, 440, 957, 1002, 307, 16191, 293, 436, 434, 787, 8437, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 860, "seek": 433758, "start": 4356.58, "end": 4357.58, "text": " You would want a doctor.", "tokens": [51314, 509, 576, 528, 257, 4631, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 861, "seek": 433758, "start": 4357.58, "end": 4358.58, "text": " It's just merely passing.", "tokens": [51364, 467, 311, 445, 17003, 8437, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 862, "seek": 433758, "start": 4358.58, "end": 4359.58, "text": " OK, guess what?", "tokens": [51414, 2264, 11, 2041, 437, 30, 51464], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 863, "seek": 433758, "start": 4359.58, "end": 4360.58, "text": " We didn't stop there.", "tokens": [51464, 492, 994, 380, 1590, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 864, "seek": 433758, "start": 4360.58, "end": 4363.58, "text": " Next thing you know, it was hitting expert level performance on the test.", "tokens": [51514, 3087, 551, 291, 458, 11, 309, 390, 8850, 5844, 1496, 3389, 322, 264, 1500, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11025408542517459, "compression_ratio": 1.5993265993265993, "no_speech_prob": 0.5425812602043152}, {"id": 865, "seek": 436358, "start": 4363.58, "end": 4371.58, "text": " Next thing you know, it's they've added multi modality and it can now do a pretty good job of reading your x-rays and other tissue slides.", "tokens": [50364, 3087, 551, 291, 458, 11, 309, 311, 436, 600, 3869, 4825, 1072, 1860, 293, 309, 393, 586, 360, 257, 1238, 665, 1691, 295, 3760, 428, 2031, 12, 36212, 293, 661, 12404, 9788, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 866, "seek": 436358, "start": 4371.58, "end": 4373.58, "text": " And again, is it perfect?", "tokens": [50764, 400, 797, 11, 307, 309, 2176, 30, 50864], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 867, "seek": 436358, "start": 4373.58, "end": 4378.58, "text": " No, it would be probably on the lower end of what the actual human radiologist could do.", "tokens": [50864, 883, 11, 309, 576, 312, 1391, 322, 264, 3126, 917, 295, 437, 264, 3539, 1952, 16335, 9201, 727, 360, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 868, "seek": 436358, "start": 4378.58, "end": 4379.58, "text": " Right.", "tokens": [51114, 1779, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 869, "seek": 436358, "start": 4379.58, "end": 4380.58, "text": " Oh, they're even there.", "tokens": [51164, 876, 11, 436, 434, 754, 456, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 870, "seek": 436358, "start": 4380.58, "end": 4381.58, "text": " It was like 6040, I think.", "tokens": [51214, 467, 390, 411, 4060, 5254, 11, 286, 519, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 871, "seek": 436358, "start": 4381.58, "end": 4388.58, "text": " I think it was like 60% to 40% that the human radiologist was beating the AI radiologist.", "tokens": [51264, 286, 519, 309, 390, 411, 4060, 4, 281, 3356, 4, 300, 264, 1952, 16335, 9201, 390, 13497, 264, 7318, 16335, 9201, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 872, "seek": 436358, "start": 4388.58, "end": 4389.58, "text": " So it's OK.", "tokens": [51614, 407, 309, 311, 2264, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 873, "seek": 436358, "start": 4389.58, "end": 4390.58, "text": " That's a pretty narrow margin.", "tokens": [51664, 663, 311, 257, 1238, 9432, 10270, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 874, "seek": 436358, "start": 4390.58, "end": 4391.58, "text": " Obviously, we're not done.", "tokens": [51714, 7580, 11, 321, 434, 406, 1096, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10366451317536915, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.01542089506983757}, {"id": 875, "seek": 439158, "start": 4391.58, "end": 4401.58, "text": " The current thing is taking case studies out of medical journals, case studies being like extreme hard to figure out cases, right?", "tokens": [50364, 440, 2190, 551, 307, 1940, 1389, 5313, 484, 295, 4625, 29621, 11, 1389, 5313, 885, 411, 8084, 1152, 281, 2573, 484, 3331, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 876, "seek": 439158, "start": 4401.58, "end": 4407.58, "text": " When a case gets reported in a medical journal, that's because this case, you know, is thought to be highly instructive, right?", "tokens": [50864, 1133, 257, 1389, 2170, 7055, 294, 257, 4625, 6708, 11, 300, 311, 570, 341, 1389, 11, 291, 458, 11, 307, 1194, 281, 312, 5405, 7232, 488, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 877, "seek": 439158, "start": 4407.58, "end": 4408.58, "text": " It was a confusing situation.", "tokens": [51164, 467, 390, 257, 13181, 2590, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 878, "seek": 439158, "start": 4408.58, "end": 4412.58, "text": " It's an unfamiliar combination of symptoms or what have you.", "tokens": [51214, 467, 311, 364, 29415, 6562, 295, 8332, 420, 437, 362, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 879, "seek": 439158, "start": 4412.58, "end": 4415.58, "text": " So they don't publish just the routine cold, right?", "tokens": [51414, 407, 436, 500, 380, 11374, 445, 264, 9927, 3554, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 880, "seek": 439158, "start": 4415.58, "end": 4416.58, "text": " In the journals.", "tokens": [51564, 682, 264, 29621, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08349710352280561, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.010649173520505428}, {"id": 881, "seek": 441658, "start": 4416.58, "end": 4429.58, "text": " So they take these case studies out of journals and they had a study of comparing AI's effectiveness at doing the differential diagnosis versus human with access to AI.", "tokens": [50364, 407, 436, 747, 613, 1389, 5313, 484, 295, 29621, 293, 436, 632, 257, 2979, 295, 15763, 7318, 311, 21208, 412, 884, 264, 15756, 15217, 5717, 1952, 365, 2105, 281, 7318, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10589012807729292, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.003944259136915207}, {"id": 882, "seek": 441658, "start": 4429.58, "end": 4432.58, "text": " And AI was the best by like a significant margin.", "tokens": [51014, 400, 7318, 390, 264, 1151, 538, 411, 257, 4776, 10270, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10589012807729292, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.003944259136915207}, {"id": 883, "seek": 441658, "start": 4432.58, "end": 4435.58, "text": " The human alone was last.", "tokens": [51164, 440, 1952, 3312, 390, 1036, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10589012807729292, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.003944259136915207}, {"id": 884, "seek": 441658, "start": 4435.58, "end": 4445.58, "text": " And so they, in their kind of presentation of this, they're very modest and they take almost like a, in my view, almost like a two-grounded willfully bearing the lead.", "tokens": [51314, 400, 370, 436, 11, 294, 641, 733, 295, 5860, 295, 341, 11, 436, 434, 588, 25403, 293, 436, 747, 1920, 411, 257, 11, 294, 452, 1910, 11, 1920, 411, 257, 732, 12, 2921, 292, 486, 2277, 17350, 264, 1477, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10589012807729292, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.003944259136915207}, {"id": 885, "seek": 444558, "start": 4445.58, "end": 4446.58, "text": " Almost at times, it seems.", "tokens": [50364, 12627, 412, 1413, 11, 309, 2544, 13, 50414], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 886, "seek": 444558, "start": 4446.58, "end": 4454.58, "text": " And what one of the main conclusions of the paper was we need better interfaces so that doctors can take better advantage of this.", "tokens": [50414, 400, 437, 472, 295, 264, 2135, 22865, 295, 264, 3035, 390, 321, 643, 1101, 28416, 370, 300, 8778, 393, 747, 1101, 5002, 295, 341, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 887, "seek": 444558, "start": 4454.58, "end": 4458.58, "text": " But it was like, to me, that's yes, that's one lesson I would take away from this paper.", "tokens": [50814, 583, 309, 390, 411, 11, 281, 385, 11, 300, 311, 2086, 11, 300, 311, 472, 6898, 286, 576, 747, 1314, 490, 341, 3035, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 888, "seek": 444558, "start": 4458.58, "end": 4466.58, "text": " But the other lesson is that the AI is getting it right like twice as often as the human clinician, like 60% to 30%.", "tokens": [51014, 583, 264, 661, 6898, 307, 300, 264, 7318, 307, 1242, 309, 558, 411, 6091, 382, 2049, 382, 264, 1952, 45962, 11, 411, 4060, 4, 281, 2217, 6856, 51414], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 889, "seek": 444558, "start": 4466.58, "end": 4469.58, "text": " That's another big lesson, too, that I take away from a lot of these things.", "tokens": [51414, 663, 311, 1071, 955, 6898, 11, 886, 11, 300, 286, 747, 1314, 490, 257, 688, 295, 613, 721, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 890, "seek": 444558, "start": 4469.58, "end": 4472.58, "text": " We don't often measure human performance.", "tokens": [51564, 492, 500, 380, 2049, 3481, 1952, 3389, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0824534200852917, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.010650277137756348}, {"id": 891, "seek": 447258, "start": 4472.58, "end": 4478.58, "text": " We think because we've lived in a world for a long time, we're like, a human doctor is human.", "tokens": [50364, 492, 519, 570, 321, 600, 5152, 294, 257, 1002, 337, 257, 938, 565, 11, 321, 434, 411, 11, 257, 1952, 4631, 307, 1952, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 892, "seek": 447258, "start": 4478.58, "end": 4484.58, "text": " We know that like some are better than others, but we look at that as a standard, that there's a human doctor and their license and they're supposed to be good.", "tokens": [50664, 492, 458, 300, 411, 512, 366, 1101, 813, 2357, 11, 457, 321, 574, 412, 300, 382, 257, 3832, 11, 300, 456, 311, 257, 1952, 4631, 293, 641, 10476, 293, 436, 434, 3442, 281, 312, 665, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 893, "seek": 447258, "start": 4484.58, "end": 4486.58, "text": " But like, how often do they get the right diagnosis on this?", "tokens": [50964, 583, 411, 11, 577, 2049, 360, 436, 483, 264, 558, 15217, 322, 341, 30, 51064], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 894, "seek": 447258, "start": 4486.58, "end": 4490.58, "text": " It turned out in this particular data set, it was in the ballpark of 30%.", "tokens": [51064, 467, 3574, 484, 294, 341, 1729, 1412, 992, 11, 309, 390, 294, 264, 2594, 31239, 295, 2217, 6856, 51264], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 895, "seek": 447258, "start": 4490.58, "end": 4492.58, "text": " So there's a lot of room for improvement.", "tokens": [51264, 407, 456, 311, 257, 688, 295, 1808, 337, 10444, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 896, "seek": 447258, "start": 4492.58, "end": 4496.58, "text": " And you could perhaps say, what's the best doctor in the world do that?", "tokens": [51364, 400, 291, 727, 4317, 584, 11, 437, 311, 264, 1151, 4631, 294, 264, 1002, 360, 300, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 897, "seek": 447258, "start": 4496.58, "end": 4499.58, "text": " Best doctor in the world, I'm sure is a lot better.", "tokens": [51564, 9752, 4631, 294, 264, 1002, 11, 286, 478, 988, 307, 257, 688, 1101, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12466458826257079, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.011685383506119251}, {"id": 898, "seek": 449958, "start": 4499.58, "end": 4503.58, "text": " Maybe even better than the 60% that their language model was able to do.", "tokens": [50364, 2704, 754, 1101, 813, 264, 4060, 4, 300, 641, 2856, 2316, 390, 1075, 281, 360, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05231540343340706, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03208022192120552}, {"id": 899, "seek": 449958, "start": 4503.58, "end": 4505.58, "text": " But you probably can't access that person.", "tokens": [50564, 583, 291, 1391, 393, 380, 2105, 300, 954, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05231540343340706, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03208022192120552}, {"id": 900, "seek": 449958, "start": 4505.58, "end": 4510.58, "text": " We are apparently headed for a world where you should be able to access that AI doctor.", "tokens": [50664, 492, 366, 7970, 12798, 337, 257, 1002, 689, 291, 820, 312, 1075, 281, 2105, 300, 7318, 4631, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05231540343340706, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03208022192120552}, {"id": 901, "seek": 449958, "start": 4510.58, "end": 4523.58, "text": " And if it's a 2x better performance on such a challenging task as differential diagnosis that I think we're headed for a world of radical access to expertise,", "tokens": [50914, 400, 498, 309, 311, 257, 568, 87, 1101, 3389, 322, 1270, 257, 7595, 5633, 382, 15756, 15217, 300, 286, 519, 321, 434, 12798, 337, 257, 1002, 295, 12001, 2105, 281, 11769, 11, 51564], "temperature": 0.0, "avg_logprob": -0.05231540343340706, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03208022192120552}, {"id": 902, "seek": 452358, "start": 4523.58, "end": 4531.58, "text": " which I think is going to be at unbelievably low prices, which I think is going to be a transformative force in society, right?", "tokens": [50364, 597, 286, 519, 307, 516, 281, 312, 412, 43593, 2295, 7901, 11, 597, 286, 519, 307, 516, 281, 312, 257, 36070, 3464, 294, 4086, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.05269006582406851, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.7394989728927612}, {"id": 903, "seek": 452358, "start": 4531.58, "end": 4539.58, "text": " It's going to be one of the greatest blows ever struck for equality of opportunity, equality of access in many ways.", "tokens": [50764, 467, 311, 516, 281, 312, 472, 295, 264, 6636, 18458, 1562, 13159, 337, 14949, 295, 2650, 11, 14949, 295, 2105, 294, 867, 2098, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05269006582406851, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.7394989728927612}, {"id": 904, "seek": 452358, "start": 4539.58, "end": 4547.58, "text": " It's also going to change a lot of market dynamics and change what wages can be commanded for different kinds of services.", "tokens": [51164, 467, 311, 611, 516, 281, 1319, 257, 688, 295, 2142, 15679, 293, 1319, 437, 20097, 393, 312, 34359, 337, 819, 3685, 295, 3328, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05269006582406851, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.7394989728927612}, {"id": 905, "seek": 452358, "start": 4547.58, "end": 4548.58, "text": " I'm excited about that.", "tokens": [51564, 286, 478, 2919, 466, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.05269006582406851, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.7394989728927612}, {"id": 906, "seek": 454858, "start": 4548.58, "end": 4554.58, "text": " I think it probably is going to be fairly disruptive and it probably is going to become more and more political.", "tokens": [50364, 286, 519, 309, 1391, 307, 516, 281, 312, 6457, 37865, 293, 309, 1391, 307, 516, 281, 1813, 544, 293, 544, 3905, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 907, "seek": 454858, "start": 4554.58, "end": 4560.58, "text": " I think that the upside of that, I think is pretty clear and really extremely compelling.", "tokens": [50664, 286, 519, 300, 264, 14119, 295, 300, 11, 286, 519, 307, 1238, 1850, 293, 534, 4664, 20050, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 908, "seek": 454858, "start": 4560.58, "end": 4566.58, "text": " So I hope we do get to actually enjoy the fruits of that future.", "tokens": [50964, 407, 286, 1454, 321, 360, 483, 281, 767, 2103, 264, 12148, 295, 300, 2027, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 909, "seek": 454858, "start": 4566.58, "end": 4569.58, "text": " And then one other thing I'll say is just, I don't think we are it.", "tokens": [51264, 400, 550, 472, 661, 551, 286, 603, 584, 307, 445, 11, 286, 500, 380, 519, 321, 366, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 910, "seek": 454858, "start": 4569.58, "end": 4571.58, "text": " The transformer is not the end of history.", "tokens": [51414, 440, 31782, 307, 406, 264, 917, 295, 2503, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 911, "seek": 454858, "start": 4571.58, "end": 4573.58, "text": " That's a chat GPT is not the end of history.", "tokens": [51514, 663, 311, 257, 5081, 26039, 51, 307, 406, 264, 917, 295, 2503, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1366351301019842, "compression_ratio": 1.7773109243697478, "no_speech_prob": 0.239127978682518}, {"id": 912, "seek": 457358, "start": 4573.58, "end": 4576.58, "text": " It's kind of no memory AI.", "tokens": [50364, 467, 311, 733, 295, 572, 4675, 7318, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 913, "seek": 457358, "start": 4576.58, "end": 4583.58, "text": " Just this last week or two, we've seen a flurry of activity in the state space model architecture.", "tokens": [50514, 1449, 341, 1036, 1243, 420, 732, 11, 321, 600, 1612, 257, 932, 30614, 295, 5191, 294, 264, 1785, 1901, 2316, 9482, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 914, "seek": 457358, "start": 4583.58, "end": 4592.58, "text": " And again, it's been reported the headlines if you're on, if you're on Twitter and seeing this stuff, it's hey, there's a new thing that might even be better than the transformer.", "tokens": [50864, 400, 797, 11, 309, 311, 668, 7055, 264, 23867, 498, 291, 434, 322, 11, 498, 291, 434, 322, 5794, 293, 2577, 341, 1507, 11, 309, 311, 4177, 11, 456, 311, 257, 777, 551, 300, 1062, 754, 312, 1101, 813, 264, 31782, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 915, "seek": 457358, "start": 4592.58, "end": 4594.58, "text": " It might be a transformer successor.", "tokens": [51314, 467, 1062, 312, 257, 31782, 31864, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 916, "seek": 457358, "start": 4594.58, "end": 4595.58, "text": " It might be a transformer alternative.", "tokens": [51414, 467, 1062, 312, 257, 31782, 8535, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 917, "seek": 457358, "start": 4595.58, "end": 4597.58, "text": " It might be a transformer replacement.", "tokens": [51464, 467, 1062, 312, 257, 31782, 14419, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 918, "seek": 457358, "start": 4597.58, "end": 4602.58, "text": " It has some nice properties that the transformers don't have better long term memory, better scaling, better speed, better throughput.", "tokens": [51564, 467, 575, 512, 1481, 7221, 300, 264, 4088, 433, 500, 380, 362, 1101, 938, 1433, 4675, 11, 1101, 21589, 11, 1101, 3073, 11, 1101, 44629, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13448176913791232, "compression_ratio": 1.907216494845361, "no_speech_prob": 0.429840087890625}, {"id": 919, "seek": 460258, "start": 4602.58, "end": 4605.58, "text": " Maybe we just all flip over from one to the other.", "tokens": [50364, 2704, 321, 445, 439, 7929, 670, 490, 472, 281, 264, 661, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07440741037585072, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.01168214064091444}, {"id": 920, "seek": 460258, "start": 4605.58, "end": 4607.58, "text": " And if the transformer was the old thing, this is the new thing.", "tokens": [50514, 400, 498, 264, 31782, 390, 264, 1331, 551, 11, 341, 307, 264, 777, 551, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07440741037585072, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.01168214064091444}, {"id": 921, "seek": 460258, "start": 4607.58, "end": 4620.58, "text": " But I strongly suspect that what we are going to see is a mixture of these architectures where just like in the brain, we obviously don't have just one single unit of the brain that gets repeated over and over again.", "tokens": [50614, 583, 286, 10613, 9091, 300, 437, 321, 366, 516, 281, 536, 307, 257, 9925, 295, 613, 6331, 1303, 689, 445, 411, 294, 264, 3567, 11, 321, 2745, 500, 380, 362, 445, 472, 2167, 4985, 295, 264, 3567, 300, 2170, 10477, 670, 293, 670, 797, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07440741037585072, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.01168214064091444}, {"id": 922, "seek": 460258, "start": 4620.58, "end": 4624.58, "text": " We have a lot of different modules, including some that do get repeated.", "tokens": [51264, 492, 362, 257, 688, 295, 819, 16679, 11, 3009, 512, 300, 360, 483, 10477, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07440741037585072, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.01168214064091444}, {"id": 923, "seek": 462458, "start": 4624.58, "end": 4635.58, "text": " It seems like we're almost for sure headed for AIs that are like composites of different kinds of architectures that bring their own strengths and weaknesses in information processing to the table.", "tokens": [50364, 467, 2544, 411, 321, 434, 1920, 337, 988, 12798, 337, 316, 6802, 300, 366, 411, 10199, 3324, 295, 819, 3685, 295, 6331, 1303, 300, 1565, 641, 1065, 16986, 293, 24381, 294, 1589, 9007, 281, 264, 3199, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0934964305949661, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.5307180285453796}, {"id": 924, "seek": 462458, "start": 4635.58, "end": 4648.58, "text": " Such that as much as this has been a shocking amount of progress to get to GPT-4 from GPT-2 just four years ago, I have to say I think the next few years are going to bring at least as much more change.", "tokens": [50914, 9653, 300, 382, 709, 382, 341, 575, 668, 257, 18776, 2372, 295, 4205, 281, 483, 281, 26039, 51, 12, 19, 490, 26039, 51, 12, 17, 445, 1451, 924, 2057, 11, 286, 362, 281, 584, 286, 519, 264, 958, 1326, 924, 366, 516, 281, 1565, 412, 1935, 382, 709, 544, 1319, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0934964305949661, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.5307180285453796}, {"id": 925, "seek": 462458, "start": 4648.58, "end": 4651.58, "text": " And it's going to be a wild ride.", "tokens": [51564, 400, 309, 311, 516, 281, 312, 257, 4868, 5077, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0934964305949661, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.5307180285453796}, {"id": 926, "seek": 465158, "start": 4651.58, "end": 4659.58, "text": " It's exciting. It's inspiring. I'm excited for the future and I really appreciate you taking the time to share your thoughts and show us how you use chat GPT.", "tokens": [50364, 467, 311, 4670, 13, 467, 311, 15883, 13, 286, 478, 2919, 337, 264, 2027, 293, 286, 534, 4449, 291, 1940, 264, 565, 281, 2073, 428, 4598, 293, 855, 505, 577, 291, 764, 5081, 26039, 51, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 927, "seek": 465158, "start": 4659.58, "end": 4664.58, "text": " And I'd love to have you back and see where we are, see what new stuff comes up on the horizon.", "tokens": [50764, 400, 286, 1116, 959, 281, 362, 291, 646, 293, 536, 689, 321, 366, 11, 536, 437, 777, 1507, 1487, 493, 322, 264, 18046, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 928, "seek": 465158, "start": 4664.58, "end": 4665.58, "text": " Yeah, thank you.", "tokens": [51014, 865, 11, 1309, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 929, "seek": 465158, "start": 4665.58, "end": 4667.58, "text": " I appreciate the opportunity Dan. This has been a lot of fun.", "tokens": [51064, 286, 4449, 264, 2650, 3394, 13, 639, 575, 668, 257, 688, 295, 1019, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 930, "seek": 465158, "start": 4667.58, "end": 4673.58, "text": " And I definitely learned some things and was inspired to go chase down a few more use cases as well.", "tokens": [51164, 400, 286, 2138, 3264, 512, 721, 293, 390, 7547, 281, 352, 15359, 760, 257, 1326, 544, 764, 3331, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 931, "seek": 465158, "start": 4673.58, "end": 4679.58, "text": " So hopefully next time I'll have some better custom instructions and a little bit better track record in the brainstorming department.", "tokens": [51464, 407, 4696, 958, 565, 286, 603, 362, 512, 1101, 2375, 9415, 293, 257, 707, 857, 1101, 2837, 2136, 294, 264, 35245, 278, 5882, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09844871189283289, "compression_ratio": 1.693452380952381, "no_speech_prob": 0.09526582807302475}, {"id": 932, "seek": 467958, "start": 4679.58, "end": 4680.58, "text": " Let me submit a great exchange.", "tokens": [50364, 961, 385, 10315, 257, 869, 7742, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09669724826155038, "compression_ratio": 1.569078947368421, "no_speech_prob": 0.1621943563222885}, {"id": 933, "seek": 467958, "start": 4680.58, "end": 4682.58, "text": " That sounds great. Yeah, thanks a lot.", "tokens": [50414, 663, 3263, 869, 13, 865, 11, 3231, 257, 688, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09669724826155038, "compression_ratio": 1.569078947368421, "no_speech_prob": 0.1621943563222885}, {"id": 934, "seek": 467958, "start": 4682.58, "end": 4687.58, "text": " It is both energizing and enlightening to hear why people listen and learn what they value about the show.", "tokens": [50514, 467, 307, 1293, 10575, 3319, 293, 18690, 4559, 281, 1568, 983, 561, 2140, 293, 1466, 437, 436, 2158, 466, 264, 855, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09669724826155038, "compression_ratio": 1.569078947368421, "no_speech_prob": 0.1621943563222885}, {"id": 935, "seek": 467958, "start": 4687.58, "end": 4697.58, "text": " So please don't hesitate to reach out via email at TCR at turpentine.co or you can DM me on the social media platform of your choice.", "tokens": [50764, 407, 1767, 500, 380, 20842, 281, 2524, 484, 5766, 3796, 412, 314, 18547, 412, 3243, 22786, 533, 13, 1291, 420, 291, 393, 15322, 385, 322, 264, 2093, 3021, 3663, 295, 428, 3922, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09669724826155038, "compression_ratio": 1.569078947368421, "no_speech_prob": 0.1621943563222885}, {"id": 936, "seek": 467958, "start": 4697.58, "end": 4707.58, "text": " Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work customized across all platforms with a click of a button.", "tokens": [51264, 9757, 77, 1123, 4960, 1337, 1166, 7318, 281, 9528, 291, 281, 4025, 6779, 295, 5383, 295, 614, 36540, 300, 767, 589, 30581, 2108, 439, 9473, 365, 257, 2052, 295, 257, 2960, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09669724826155038, "compression_ratio": 1.569078947368421, "no_speech_prob": 0.1621943563222885}, {"id": 937, "seek": 470758, "start": 4707.58, "end": 4712.58, "text": " I believe in Omnike so much that I invested in it and I recommend you use it too.", "tokens": [50364, 286, 1697, 294, 9757, 77, 1123, 370, 709, 300, 286, 13104, 294, 309, 293, 286, 2748, 291, 764, 309, 886, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15620997077540347, "compression_ratio": 1.1262135922330097, "no_speech_prob": 0.6399479508399963}, {"id": 938, "seek": 470758, "start": 4712.58, "end": 4715.58, "text": " Use CogGrav to get a 10% discount.", "tokens": [50614, 8278, 383, 664, 38, 13404, 281, 483, 257, 1266, 4, 11635, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15620997077540347, "compression_ratio": 1.1262135922330097, "no_speech_prob": 0.6399479508399963}], "language": "en"}