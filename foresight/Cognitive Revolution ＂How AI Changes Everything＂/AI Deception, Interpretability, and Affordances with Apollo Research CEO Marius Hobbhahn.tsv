start	end	text
0	4800	So the more pressure we add, the more likely the model is to be deceptive.
4800	7920	So kind of in the same way in which a human would act, it also acts.
7920	12480	You know, removing pressure and adding additional options will very quickly decrease
12480	14400	the probability of being deceptive.
14400	17200	Open source has been really good so far in many, many ways.
17200	19200	It has been very positive for society, right?
19200	22320	I think a lot of ML research could not have happened without open source.
22320	25200	A lot of safety research could not have happened with open source.
25200	28880	At some point, the system is so powerful that you don't want it to be open source anymore.
28880	33280	In the same way in which, you know, I don't want to open source the nuclear codes or like,
33280	37040	you know, literally the recipe to build most viral pandemic or something.
37040	42640	The labs maybe have the incentive to not say the worst things they found because otherwise
42640	43840	they may lose their contract.
43840	48480	So you need something like the UKASF Institute or the USASF Institute.
48480	54000	Make sure that there is a minimal set of standards that all the auditors have to adhere to.
54000	56320	Hello and welcome to The Cognitive Revolution,
56320	60720	where we interview visionary researchers, entrepreneurs, and builders working on the
60720	62400	frontier of artificial intelligence.
63200	67520	Each week, we'll explore their revolutionary ideas and together we'll build a picture of
67520	72560	how AI technology will transform work, life, and society in the coming years.
73120	76240	I'm Nathan LaBenz, joined by my co-host Eric Torenberg.
77040	79680	Hello and welcome back to The Cognitive Revolution.
80640	85280	Today, my guest is Marius Haban, founder and CEO of Apollo Research,
85280	90880	a nonprofit AI safety research group that is working to understand both how AI systems behave
91440	92160	and why.
93760	99120	Their approach combines exploratory and hypothesis-driven testing, fine-tuning experiments,
99120	100960	and interpretability research.
101520	107040	And as you'll hear, they place special emphasis on the potential for AI systems to deceive
107040	108000	their human users.
109200	113760	In this conversation, we look first at Apollo's starting framework for their work,
113840	117280	which emphasizes the importance of affordances in AI systems.
117920	123760	That is, through what tools, actuators, or other means can the system affect the broader world?
124880	128880	And they also introduce a number of new conceptual distinctions meant to help people have more
128880	132880	precise and productive conversations about these nuanced topics.
134240	137760	Then in the second half, we look at their first research result,
137760	141200	which demonstrates, to my knowledge, for the first time in a realistic,
141200	148400	unprompted setting that GPT-4, when put under pressure, will sometimes take unethical and even
148400	153600	illegal actions and then go on to lie to its users about what it did and why.
154960	160080	This is an important result, demonstrating that while the risk from AI systems may start with
160080	167920	and may even be dominated by intentional human misuse, the models themselves can also misbehave
167920	169200	in unexpected ways.
169840	174560	As an aside, since I told my behind-the-scenes GPT-4 red team story a few weeks ago,
174560	179120	a number of people have reached out to ask me how they too can get involved with red teaming projects.
180160	184880	Unfortunately, as commercial competition and secrecy both continue to ramp up across the space,
185440	189440	I don't see as many open calls for volunteer red teamers as I used to,
190000	192560	certainly not for unreleased frontier models.
193920	197920	Instead, the field is becoming more professionalized, with all the leading labs,
197920	203360	as well as the data companies like Scale AI, plus the independent auditing organizations like Apollo,
203360	208960	ArchieVals, now known as Meter, Palisade, and also AI Forensics, all actively hiring
208960	211600	research scientists and engineers in this area.
212880	217200	So does that mean that there's no longer a role for the independent hobbyist red teamer to play?
217920	222640	On the contrary, there is a ton left to discover even on publicly released models,
222640	227600	and the best way to break into the field is to demonstrate your ability to discover new phenomena.
228720	233840	Importantly, the work we cover in this episode could have been done by anyone with an open AI
233840	239920	account, a knack for prompting, and just a tiny bit of coding know-how. No special access or
239920	244800	advanced machine learning techniques were required, just a lot of curiosity.
246080	250000	With that in mind, if you want to get into this line of work but aren't sure where to start,
250000	255520	I encourage you to reach out. I'll be happy to help brainstorm or refine your project ideas,
255600	260480	and I can also help connect you with folks at the top companies who do sometimes provide API
260480	265440	credits to independent researchers working in this area, if and when you can achieve a meaningful
265440	271360	result. As always, we appreciate the time that you spend listening to The Cognitive Revolution,
271360	277040	and we hope it's a valuable guide to the AI era. If you feel that it is, we would love a review
277040	282240	on Apple Podcasts or Spotify, and we of course encourage you to share the show with your friends.
283200	289680	Now, here's my conversation on frontier AI safety work with Marius Habhan of Apollo Research.
290880	295920	Marius Habhan, founder and CEO of Apollo Research, welcome to The Cognitive Revolution.
296720	298160	Hey, thanks for having me.
298160	303600	I am very excited to have you. So, regular listeners of the show will know that I'm a big
303600	311760	believer in the importance of hands-on testing of what AI systems can do, and also that I have
311760	318000	been pretty enthusiastic consumer of the news when some of the leading labs have made public
318000	324800	commitments to allow organizations outside of their own teams to look at the systems that
324800	330880	they're building before they get deployed. And so, your work with Apollo Research, which is
331920	336480	trying to build, as I understand it, an organization to meet that need and actually work with those
336480	342240	leading labs in part, at least, on understanding the systems that they're developing before they
342240	348480	get to widespread deployment, I think is super interesting, and I'm very excited to unpack the
348480	353520	details of it with you. Maybe for starters, you want to just kind of give us the quick overview on
353520	358480	Apollo Research, like how you decided to set out to found it. I'm interested a little bit in the
358480	363920	timeline of how that related to some of the commitments that the labs have made and what
363920	368640	you guys are trying to do in the big picture. So, I think on a high level, it's sort of trying to
368640	375040	understand what is going on in AI systems. And the reason for this was, or still is, in fact,
375680	381520	yeah, I basically think right now, we just lack information to make good decisions. There's loads
381520	387760	of uncertainty that we have about what could go wrong, whether we are already at a point where
387760	395760	things go wrong, or how far away we are from these points. And yeah, we're trying to reduce this
395760	402160	uncertainty. And this is mostly through research, auditing, and governance. And on the research side,
402160	408000	it's really split between interpretability and behavioral evils, half-half. But in the long
408000	414480	run, we really want to merge them both, because I basically think what we need in the long run is
414480	419520	both a mixture of behavioral and interpretability evils, so that we can really understand
419520	423760	what the model is doing, and then also why it is doing this in the first place, because each of
423760	431520	them individually seems somewhat insufficient. And yeah, maybe to go into the origin story,
431520	437520	it has actually nothing to do with the commitments of the different labs. It was mostly that at the
437520	446320	beginning of this year, I kind of felt like I had a pretty clear picture of what is lacking in the
446320	451120	current space with deceptive alignment, and evaluating deceptive alignment, or models for
451120	456320	deceptive alignment in the first place. And interpretability and evils just seemed like
456320	461040	the obvious things to do. So in the beginning, we basically set out to do mostly research.
461040	466720	And only then, over time, we realized, hey, this is something that should be applied in the real
466720	474400	world as soon as possible, because systems are getting better all the time. And we may actually
474400	479920	hit this point fairly soon where models are already about at the threshold of deceptive,
479920	485760	of capabilities for deceptive alignment. And then there is a small part in the organization that
485760	490800	is governance, which originally we also didn't really intend to do for the first two years or
490800	495920	something, because we thought we really need to understand all the research very well before
495920	501280	we can talk to the people in governments and decision makers and lawmakers, because otherwise
501280	506640	we're telling them things we aren't super confident in. And then lots of things happen.
507280	514320	Governments and lawmakers actually got interested in AI and AI safety in particular. And then when
514320	520240	we talked to them, we realized the difference. We are very, very well placed to talk about these
520240	525840	things, because if you have thought about them in the background for like six, seven,
526000	531600	years, and then specifically about some topic for six months or so, you are among the world's
531600	537360	experts. And this is kind of more like a reflection of the state of how bad it is about AI safety,
537360	543680	where people in my position are actually sort of accidentally becoming the experts,
544480	550640	rather than people with tens and 20 years of experience, because there aren't a lot of people
550640	554800	in the world who have thought about AI safety for more than a couple of years, if at all.
554800	561760	Yeah, I can definitely relate to that sort of accidental expert status. I never expected to be
561760	570160	where I am and doing the things that I'm doing. But yeah, the whole AI field in some ways is kind
570160	575360	of the dog that caught the car. I always kind of come back to that metaphor where it's like
576320	580880	we were just trying to build a bit more powerful AI, and all of a sudden we built like a lot more
580960	586240	powerful AI, and now we really kind of have to figure out what to do with it. So even a little
586240	591040	bit of advanced planning is better than, or a little bit of advanced thought is a lot better than
591040	596640	where most people are starting. Had you seen, when you actually started the organization,
596640	603680	had you seen GPT-4, or were you basing this decision on just what was public at the time?
604560	613040	Only what was public at the time. So the decision was made in February 2023, or at
613040	618400	least sort of my internal commitment was made to this. I'm not sure whether GPT-4 was public
618400	623920	already at the time. Not quite, right? It was March. So no, it was independent of GPT-4.
624800	630720	Yeah, I always think that's interesting just because GPT-4 was such a wake-up moment for
630720	636000	so many people, and certainly I would include myself in that. I was already extremely plugged
636000	642560	into what was going on and using it and fine-tuning tons of models on the open AI platform in particular,
642560	648320	but then it was like, whoa, this thing is next level. It's not slowing down. We've gone from
648320	654400	sort of, I can put a lot of elbow grease in and get a fine-tuned model to do a particular task,
654400	658960	which already I thought was going to be economically transformative to,
660000	663520	I don't even need to do that, that I could just ask for a lot of these tasks and get
663520	667600	like pretty good zero-shot performance. For me, that was the moment where I was kind of like,
667600	672880	okay, this is going from a tool that I am really excited to use and having a lot of fun using
672880	679360	to something that seems like a force that needs to be understood from all angles.
680000	685600	So let's unpack the perspective that you are bringing to this. I would encourage folks to
685600	691040	look up these papers that we'll discuss and read them for themselves as well, but on the website,
691040	698240	you've got two recent publications. One is kind of a framework for organizing the work that you're
698240	703680	going to do, and then the other is like a very detailed in the weeds investigation of a particular
703760	709360	AI behavior, namely deceiving the user, which I think is a super interesting and important
709360	714000	one to study. But let's maybe just start with the big picture, like organizing the thoughts.
715840	720800	I get the sense that you think, again, well, you've kind of said this, and the paper certainly
720800	724960	reflects it, that there are like a lot of big questions that remain unanswered. So how do you
724960	731040	structure your approach to this topic given all the uncertainty that exists?
731760	738480	Yeah, maybe to give a little bit of context. So this is only one paper of many in this space,
738480	743280	and there is, I think earlier this year, there was a really big one called Model Evaluation
743280	749520	for Extreme Risk, which we at Apollo definitely thought was a pretty good paper. And they're
749520	755120	sort of pointing out many of the very reasonable and important steps, or reasonable principles
755120	760320	for external auditing, something like ramp up the auditing before you ramp up the exposure
760320	767200	to the real world and do this ahead of the curve, so to speak. But when we read the paper,
767200	772560	we felt a little bit like, this makes sense for the current capabilities and sort of how
772560	777360	current models are being built. But if we think ahead of what the next couple of years should look
777360	783120	or not should, could look like, then yeah, there are loads of open questions. And we were trying
783120	788160	to understand how do they fit into this framework? And because we internally were trying to make
788160	793040	sense of this in the first place. So just to give you a couple of them, what happens if your
793040	798080	model has the ability to do online learning? How often do you have to audit it? Should you re-audit
798080	803680	it during the online learning? If yes, how often? What if you give the model access to the internet
803680	809520	or to a database or to anything like this? Yeah, I think a model with and without access to the
809520	815120	internet is basically two very different models. The one with access to the internet is just so
815120	821600	much more powerful if you can use it even on a very basic level. So yeah, it feels like if you
821600	825840	give your model affordances like this, you kind of have to rethink how dangerous it is and where
825840	829360	the danger comes from because it suddenly is like a totally different threat model potentially.
829920	836400	And so what we did for the paper and really the credit should go to Lee Sharkey here, who is my
836400	840480	co-founder, who has done most of the hard work or if not all of the hard work for this paper.
841440	846560	And so what we were doing is thinking from first principles. Where does the risk come from and what
846560	853440	changes to the AI system do create new risks? And then basically the answer is, well, we have to
853440	858400	audit wherever risk is created. And then the more we looked into this, the more realized, well,
858400	862800	there are actually a lot of places where new risk comes into the system, at least potentially,
862800	867600	and therefore we are audits, at least in an ideal world should happen. There are obviously some
867680	872800	constraints. But I think if we think about where are we five years from now, then I think, yeah,
872800	877600	if there is actually a big auditing ecosystem around this, then there will be very, very many
877600	883520	different organizations auditing really different places. And then the other point of the paper
883520	891360	was just to define many concepts and create the language to discuss all of these things because
891920	896480	we had sort of many internal discussions where we were like, oh, the thing we mean is this.
896480	900480	And then we had an example, and then we kind of needed a name for it. And there wasn't really
900480	906560	a name. So we decided, okay, let's define all of the relevant terms for this, and then sort of
906560	911040	have a language to talk about this in the first place. Hey, we'll continue our interview in a
911040	916320	moment after a word from our sponsors. Real quick, what's the easiest choice you can make? Taking
916320	921200	the window instead of the middle seat, outsourcing business tasks that you absolutely hate? What
921200	927760	about selling with Shopify? Shopify is the global commerce platform that helps you sell at every
927760	933680	stage of your business. Shopify powers 10% of all e-commerce in the US, and Shopify is the global
933680	939360	force behind Allbirds, Rothy's and Brooklyn and millions of other entrepreneurs of every size
939360	944800	across 175 countries. Whether you're selling security systems or marketing memory modules,
944800	949440	Shopify helps you sell everywhere, from their all-in-one e-commerce platform to their in-person
949440	954640	POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the
954640	959200	past at the companies I founded, and when we launched Merch here at Turpentine, Shopify will
959200	964080	be our go-to. Shopify helps turn browsers into buyers with the internet's best converting
964080	969520	checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell
969520	975040	more with less effort thanks to Shopify magic, your AI-powered All-Star. With Shopify magic,
975120	979600	whip up captivating content that converts from blog posts to product descriptions.
979600	985920	Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify magic is free for
985920	992080	every Shopify seller. Businesses that grow grow with Shopify. Sign up for a $1 per month trial
992080	998000	period at Shopify.com slash cognitive. Go to Shopify.com slash cognitive now to grow your
998000	1002000	business no matter what stage you're in. Shopify.com slash cognitive.
1005040	1012720	So let's dig in in a little bit deeper detail. I like the premise that you set out within the
1012720	1020080	paper, which is to work backward from AI effect in the real world and try to imagine where are
1020080	1023920	these effects going to happen and then how can we get upstream of that and help shape them in a
1023920	1030160	positive way. I would be interested to hear you kind of describe that backward chaining process in
1030160	1034320	a little bit more detail. And then I thought some of your concepts also were really helpful
1035200	1040320	clarifications and distinctions. So maybe you can highlight some of the ones that you think are most
1041280	1043920	useful that you'd like to see get into broader circulation as well.
1044480	1049200	So basically, we started from, okay, the system, the AI system will interact with the world in a
1049200	1053440	particular way. And then, you know, there are many, many different ways in which it can interact with
1053440	1059280	the world. And then there's sort of like a whole chain of things that have had to happen until the
1059280	1064080	model can act interact with the world in this particular way. So, you know, maybe it has been
1064080	1068400	fined you and maybe it has been given access to the internet before that it has to have been trained
1068400	1072560	before that there has to have been the decision that this model should be trained in the first
1072560	1078800	place. And so the question is like, what are the kind of important decisions at all of these
1078800	1084800	different points in time? And how then can we ensure that people actually make decisions that
1084800	1091360	will lead to outcomes at the end of the chain, such as the model or the system interacts with the
1091360	1096720	world in a safe manner. And this is maybe like the first distinction that is worth pointing out,
1096720	1101040	or like the reason why I'm correcting myself all the time is there's really a difference between
1101040	1105440	AI model and AI system. The AI model really, and this is not something we came up with,
1105440	1109920	this already exists before. But I think it's worth pointing out and sort of getting in like
1109920	1115360	really hammering into people's head when they think about AI. So the AI model is just the weights
1115360	1120400	maybe behind, you know, like behind an API, but even with the API, it's kind of already a system.
1121440	1126160	And the system then is sort of the weights plus everything around it. So there could be scaffolding,
1126160	1130240	there could be access to tools, there could be content filters, this could even be like
1130240	1135680	just an API, retrieval databases, etc. Like really the full package, where you say, okay,
1135680	1141840	you know, there's there's like stuff around the mod around the weights that increase the
1141840	1147440	capabilities of the model and menu, or at least change the capabilities of the raw model in some
1147440	1153200	sense, not necessarily always increasing filters, for example, may decrease it. And then there are
1153200	1157680	sort of other weird ways, or like, yeah, once you think about this, there are sort of a couple
1157680	1163040	of other concepts that that feel important to clarify. Because when people say capabilities,
1163040	1168720	this can mean very different things, right? And so we categorize this into three different classes.
1169360	1175040	The first one is absolute capabilities, which we think of basically the hypothetical capabilities
1175040	1182480	given any set of affordances. So if you have GPD for without the internet, right, then in the space
1182480	1188160	of absolute capabilities would be a GPD for with internet. So or like things that this model could
1188160	1193360	do. So the question is like, if we give additional things to the system, how big is the space of
1193360	1199440	actions it could take. So, you know, and then obviously, there's a question of like, how imaginary
1199440	1204720	do we get here, you know, like, does it has does it get access to like, you know, a Dyson sphere,
1204720	1210880	or does it get access to like, a government or something like this. But but yeah, like it basically
1210880	1216560	points out sort of the, what could this model do if we gave it a lot of things, everything that
1216560	1221280	we can basically think of. Thinking about this in the first place only makes sense for models that
1221280	1227360	have become more general, like the GPT is, because you know, for an MNIST filter, like for an MNIST
1227360	1233360	classifier, this doesn't make any any sense, like an MNIST classifier plus internet is like is exactly
1233360	1238400	as capable as just the MNIST classifier itself. But yeah, for systems that are more general,
1238400	1244560	suddenly you have this difference between things that only the system can do, or like the basic
1244560	1251280	system plus things that you could do hypothetically with a lot of additional affordances. Then the
1251280	1258480	second one is contextual capabilities, which is things that are achievable in the context right
1258480	1266080	now. So for example, with chat GPT, you can enable it to have access to tools, and then you can browse
1266080	1270480	the web. And this is something that it can do right now, you don't have to add anything on top
1270480	1275520	of this. And this is sort of this is sort of the smallest category of things, which you can do without
1275520	1282080	any additional modification and then reachable capabilities is contextual capabilities, plus
1282080	1289200	achievable through extra effort. So for example, this could mean chat GPT itself may not have access
1289200	1296800	to a calculator. But if it has access to the internet, it can like Google and then find a
1296800	1302480	calculator and then use that calculator. And so it's sort of a two step process, right, where it
1302480	1307760	has to use one affordance or capability to then achieve another. And so this is what we call
1307760	1316400	reachable capabilities. And yeah, so the reason the reason why we are making this all of this
1316400	1321840	differentiation, even though it sounds maybe a little bit too much in the weeds is when people
1321840	1327680	talk about capabilities and regulating capabilities and designing laws for capabilities, the question
1327680	1333520	is, which ones, right? Do you mean the contextual capabilities? So the ones that the model has
1333520	1338160	literally right now, or the reachable capabilities, so which the model could reach with additional
1338160	1345040	effort or the absolute like, the maximum potential space of capabilities. And, you know, right now
1345040	1350880	this may sound like we're too much in the weeds. And but I think in a few months, this will sound
1350880	1356640	very, very relevant suddenly, because the models will be more capable. And then they will actually
1356640	1363760	be able to just like smart enough to use the internet to to like find additional tools that
1363760	1370160	they can then use, or or like convince someone to give them access to a shell. And then use that
1370160	1373680	because they're already like, you know, they can learn it in context or they know it anyways.
1375040	1379360	And at that point, really, the question is, what should the auditors audit for? Which capabilities?
1379920	1385440	And and that becomes like pretty quickly, like a very, very big space of things, right? So like,
1385440	1391360	if the auditor not only has to think about what kind of tools do you give the AI, but also what
1391360	1397440	kind of tools could the AI get access to through some means? Suddenly, you have this whole space of
1397440	1402480	like thousands of things it could do. It's really a question of like, or like a tradeoff between
1402480	1407440	what is what is plausibly doable in the real world versus how much risk can we actually mitigate?
1408240	1413040	And I'm honestly very unsure about about the like where we're heading at this point.
1414160	1420560	So just to riff on and kind of emphasize some of the the value that I see in in some of these
1420560	1425920	distinctions, I think it's helpful to clarify the difference between a model and a system.
1426720	1432000	I think there is a tremendous amount of confusion online. And to my degree, and I've probably even
1432000	1438080	contributed to some of it at times where people are like, you know, Chad GPT was doing this for me,
1438080	1442400	and now it's not anymore. And I've sometimes said like, well, they haven't updated the model,
1442400	1446160	so it probably hasn't changed that much. And I think what I've maybe neglected in some of those
1446160	1451200	moments is like, but they might have changed the system prompt, or, you know, as we're seeing,
1451200	1456480	I mean, even just this last couple of weeks, there's been this really interesting phenomenon of the
1457360	1463680	of GPT for getting quote unquote, lazier. And people are speculating that maybe that's because
1463680	1467280	they feed the date into it. And it knows that we're in December, and it knows that people
1467280	1471760	don't work as hard or as productively in December. And so maybe it's like kind of phoning it in,
1471760	1477200	because it's like, imitating the broad swath of humans that it's seen like, you know, kind of
1477200	1481920	work halftime in December or whatever. I've even seen some experiments, just in the last couple
1481920	1485840	days that suggest that there might even be real truth to that. Who knows, I'd say that the question
1485840	1490640	remains open. But there's a there is an important difference, you know, and it's worth getting
1490640	1496880	clarity on the model itself with static weights, not changing versus even just a system prompt
1496880	1502640	that can perhaps have, you know, even unexpected drift along the dimension of something as
1502640	1509840	seemingly benign as today's date. So that's important to keep in mind. The levels of capabilities,
1509840	1513280	I think, are also really interesting. And I want to ask one kind of I have a couple questions on
1513280	1519200	this, but I think I have a clear sense of what is meant by contextual. What can it do now, given
1519200	1524960	the packaging, right? What what can GPT forward do in the context of chat GPT, where it has
1524960	1530480	a code interpreter, and it has browse with Bing, and it has the ability to call Dolly three to
1530480	1534640	make an image, and probably a couple other things that I'm not even remembering, you know, plugins
1534640	1538880	perhaps as well, right, which obviously GPT is which proliferates, you know, all the affordances,
1538880	1544400	all that much more. On the other end, I feel like I sort of understand absolute, which is like a
1544400	1550080	theoretical max. Could you give me a little like how do I understand reachable as as kind of between
1550080	1558320	those like what's what's the distinction between reachable and absolute? Yeah, so so maybe maybe
1558320	1563840	one way to think of it is like, the contextual capabilities are the ones kind of that a user
1563840	1570480	explicitly gave it. And then the reachable ones are those that may also be reachable without the
1570480	1575920	user even having thought about that the model actually will will use them, right? So if you say,
1576560	1582560	you know, like if the model would be able to browse the web, like entirely on its own, which I'm
1582560	1588000	not sure it currently can do or like what exactly the restrictions on search with Bing are. But if
1588000	1593840	it was able to do that, right, you may not you may not have realized that it has a reachable
1593840	1601440	reachable capability through the internet of like firing up a shell somewhere, or like renting a GPU,
1602080	1607200	and and like doings or like running a physics simulation through a like an online
1608480	1613840	physics simulator, if that if that's something that's available. And so so these are this is sort of
1613840	1621920	like how which tools can it reach through the contextual ability capabilities that it already
1621920	1630160	has given by you or was been has been given by you. Gotcha. Okay. So like solving a capture by
1630160	1638080	hiring an upward contractor for exactly to take one infamous case. So, okay, here's a challenging
1638160	1644880	question. But, and I don't necessarily expect an answer, but maybe you can venture an answer or
1644880	1650640	you could just kind of describe how you begin to think about it. What would you say are the absolute
1650640	1659680	capabilities of GPT four? Yeah, very unclear. So I think they're definitely they're not infinite.
1660400	1665440	As in, you know, like even with extremely good scaffolding and and access to the internet and
1665440	1672480	many other things, I think people haven't been able to, you know, get it to do economically
1672480	1678880	valuable tasks at the level of a human, at least for like long time spans, for example. So, you
1678880	1683200	know, the question is obviously like, is this, you know, are we just too bad? And have we not
1683200	1687440	figured out the right prompting yet and the right scaffolding and so on? Or, or is this just a
1687440	1692400	limitation of the system? And my current guess is, like, there is probably a limit to the absolute
1692400	1697360	capabilities. And it's probably lower than like what a human can do. But we're not that far away
1697360	1703440	from it. So, you know, I think with an additional training with additional, like specifically
1703440	1708320	LM, like training that is more goal direct or makes it into more goal directed and an agent
1709280	1715200	and better scaffolding, I think there will be ways in which the absolute capabilities could increase
1716080	1719680	quite a bit in the near future. Yeah, does this make sense?
1720400	1726480	Yeah, I mean, it's hard, right? I certainly listeners to the show will know from repeated
1726480	1734160	storytelling on my part that I was one of the volunteer testers of the GPT-4 early model back
1734160	1739520	in August, September of last year. And I really kind of challenged myself to try to answer that
1739520	1746480	question, you know, independently, like, what is the theoretical max of what this thing can do?
1747360	1752400	How much could it like break down big problems and delegate to itself? And it basically came to
1752400	1761520	the same conclusion that you did, which is like, doesn't seem like it can do really big tasks.
1761520	1764560	I mean, again, it's confusing, right? Because then you could also look at the dimension of
1765200	1769200	how big the task is versus how would you break it down? Just in the last week, I've been doing
1769200	1775920	something for a very sort of mundane project, but actually using GPT-4 to run evals on other
1775920	1783280	language model output, I have found that if I have like 10 tasks, 10, you know,
1783280	1789520	dimensions of evaluation, and I ask it to run all of those, it is now capable of following
1789520	1796160	those directions and executing the tasks one by one. But the quality kind of suffers. It sort of
1796160	1800560	makes mistakes. It sometimes muddies the tasks a little bit between each other. And it's definitely
1800560	1807520	like not at a human level given 10 tasks to do in one generation. On the flip side, though,
1807520	1814240	if I take it down to one task per generation, which I didn't want to do because that will increase
1814240	1819920	our cost and latency and just is less convenient for me, but then it kind of pops up to, honestly,
1819920	1825920	I would say pretty much human level, if not above. So there's interesting dimension. I guess
1825920	1830640	it seems pretty the sort of magnitude of the task seems like a pretty important dimension for
1831520	1835040	evaluating a question like absolute capabilities, right? It's like,
1835040	1840320	if it's a super narrow thing, it has it's like more it's, it's capable of some pretty high spikes.
1840320	1846720	But if it's a, if it's a big thing, it kind of gets lost. Would you refine that characterization
1846720	1851280	at all? Yeah, yeah, I'm not sure how to think about it, honestly. So I think of absolute capabilities
1851280	1857520	really more of a sort of theoretical bound that we could, that we're probably not going to approximate
1857520	1863440	in practice, even if we test like a lot, a lot. And then the, then like breaking it down into
1863440	1867360	different tasks, I'm not sure I feel like this is a different capability then, right? Like you're
1867920	1872560	sort of the capability of doing 10 things at once is a different thing than the capability of
1872560	1880480	doing one thing 10 times like 10, 10 diff different things, but one by one. So yeah,
1880480	1886160	I would say it's basically you're talking about different capabilities then, at least in this
1886160	1891280	framework. Hey, we'll continue our interview in a moment after a word from our sponsors.
1891280	1895360	If you're a startup founder or executive running a growing business, you know that as you scale,
1895360	1899680	your systems break down, and the cracks start to show. If this resonates with you,
1899680	1906560	there are three numbers you need to know, 36,000, 25 and one, 36,000. That's the number of businesses
1906560	1910560	which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system,
1910560	1916800	streamline accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25
1916800	1921680	this year. That's 25 years of helping businesses do more with less, close their books in days, not
1921680	1927120	weeks, and drive down costs. One, because your business is one of a kind, so you get a customized
1927120	1931920	solution for all your KPIs in one efficient system with one source of truth. Manage risk,
1931920	1935920	get reliable forecasts, and improve margins. Everything you need, all in one place.
1936480	1941200	Right now, download NetSuite's popular KPI checklist, designed to give you consistently
1941200	1946640	excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com
1946640	1951200	slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.
1952480	1958000	Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that
1958000	1963680	actually work, customized across all platforms with a click of a button. I believe in Omnike so
1963680	1970080	much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount.
1970080	1976320	Yeah, and it is not that good at decomposing the tasks. I've also kind of experimented a little
1976320	1981600	bit with like, can you give it that list of 10 tasks and can it break them down and
1982400	1987600	self-delegate with an effective prompt? It's like maybe a little bit closer there,
1987600	1995280	but still not getting nearly as good results as if I just roll it my sleeves and do the task
1995280	2001360	decomposition. You mentioned that you expect this frontier to obviously continue to move.
2001920	2009360	One way to ask the question is, what is Q-star? A more sensible way to ask the question is,
2010320	2017920	do you have a set of expectations for how the capabilities frontier will move? I definitely
2017920	2023760	look at things like OpenAI's publication from earlier this year where they gave,
2024400	2028880	started to give denser feedback on kind of every step of the reasoning process and they
2028880	2034160	achieved some state-of-the-art results on mathematical reasoning that way. And when I
2034160	2038080	think about affordances and I think about the failure modes that I've seen with these
2038800	2045760	GPT-4 agent type systems, I think, man, you apply that to browsing the web and using APIs
2046400	2052880	and it seems like that stuff is ultimately a lot less cognitively demanding than pure math.
2052880	2058000	It seems like we probably are going to see, and I would guess that it's maybe already working
2058000	2062480	pretty well. AGI has been achieved internally, I don't know about AGI, but I would expect that
2062480	2069200	some of this stuff is already pretty far along in kind of internal prototyping. But how does that
2069200	2074160	compare to what you would expect to see coming online over the next few months? Yeah, it's obviously
2074160	2080080	hard to say and I can only speculate. I think on a high level what I would expect the big trends
2080080	2088640	to be and also what we are kind of looking forward to evaluating is LM agents. I think this is like
2088640	2093840	pretty agreed upon. From first principle, I think it also makes sense. It's just like,
2094480	2100640	where does the money come from? It is from AI systems doing economically useful tasks
2100640	2106160	and often economically useful tasks just require you to do things independently,
2106160	2112880	being goal directed over a longer period of time. And the longer a model can do things on its own,
2112880	2117760	the more money you can squeeze out of it. So I definitely think just from financial interests,
2117760	2124400	all of the AGI labs will definitely try to get in more, more agentic ways. How far they have come,
2124400	2129840	I don't know. But yeah, I expect that next year we will see quite some surprises.
2131120	2137600	Then multimodality is the other one where, yeah, I think people kind of like over the last couple
2137600	2144400	of years with more and more multimodal models, people just realized it's not that different from
2145200	2152400	just training text. You plug in the additional modalities, you change your training slightly,
2153600	2159200	but it's not much more than that. Obviously, it's obviously hard in practice. There's a ton of
2159200	2165440	engineering challenges and so on. But on a conceptual level, there isn't any big breakthrough
2165440	2170160	needed. So people will just add more and more modalities on bigger and bigger models and train
2170240	2176800	it all jointly and to end. And it kind of just works. And then tool use is the last one.
2177920	2182800	And that I think people, yeah, people actually were quite surprised by how like, quote unquote,
2182800	2189920	easy it was to get to this level. So yeah, I think when people realize like, oh, these language
2189920	2195680	models are already pretty good, like how fast do they learn how to use any tool we can think of?
2196640	2201680	And they were surprised by how fast they learned the tools. And now it's mostly a question of
2202400	2207920	sort of really baking in the tool into the model in a way that it's like very robustly able to use
2207920	2212240	the model rather than just a little bit or just showing that it's it sometimes work. But yeah,
2212240	2216320	I mean, you know, like I think if you have an LM agent that is multimodal, and that has very good
2216320	2220800	tool use, like I'm not quite sure how far you are away from AGI, right? Like at that point,
2220800	2225440	you kind of have almost all of the ingredients ready. And then it's really just a question of
2225440	2229680	how robust is the system. So yeah, I think these are the trends we see right now. And this is also
2229680	2233520	why many people in the big labs have very, very short timelines, because they can think like
2233520	2238320	two years ahead and sort of where this is going, or maybe even just one year ahead, I don't know.
2238320	2243360	When you talked about the surprise, like people were surprised at how easy it was to get tool
2243360	2250160	used to work. Are you referring to people in the leading, you know, the obvious, the usual suspects
2250160	2254240	of leading developers? It's hard to say. I mean, I can only speculate on this. But you know, the
2254240	2261680	tool former paper was like three months, or like was published three months before open AI just
2261680	2266560	released their tool use. And I mean, they probably had been working on this before, but still, you
2266560	2272480	know, like the from from having the scientific insight to to like publishing this and releasing
2272480	2280400	this in the real world, I think there just was less work involved than is, or is typical for most
2280480	2286160	of the bigger AI, like development cycles, I could be wrong on this. This is this is more
2286160	2292880	here say so yeah, take it with a grain of salt. Yeah, it seems right to me as well. And I agree
2292880	2298320	with you the emphasis on multi modality as a new unlock makes a ton of sense, even just in this
2298320	2304800	kind of agent paradigm of, you know, can I browse the internet or whatever. I've done a lot of
2305440	2311200	browser automation type work in the past. And the difference between having to
2312320	2317840	grab all the HTML that, you know, is often these days, like extremely bloated and, you know, kind
2317840	2324960	of semi auto generated and in some cases, like deliberately, you know, generated to be hard to
2324960	2329200	parse, you know, from like, you know, the Googles on Facebook, like they don't want you scraping
2329280	2335120	their content. So they're kind of not making it easy on the, on the browser automators. The
2335120	2338480	difference between that and just being able to like look at the screen and understand what's
2338480	2342160	going on, you know, you kind of put it through a human lens and you're like, yeah, it's a hell
2342160	2346400	of a lot easier to see what's going on on the screen than to like read all this HTML and sure
2346400	2351680	enough, you know, the models kind of behave similarly. I remember for me looking at the
2351680	2357920	flamingo architecture when that was first published, like, I think April of 2022. So
2357920	2363840	you're a little more than a year and a half ago now. And just thinking like, Oh, my God,
2364560	2369200	if this works, everything's going to work. You know, it was like, they had a language model
2369200	2375600	frozen. They had kind of stitched in the visual stuff and like kind of added a couple layers, but
2376160	2383520	it really looked to me like, man, this is tinkering stage. And it's just working. So I, you
2383520	2387920	know, like you, I don't want to dismiss the fact that there's obviously a decent amount of,
2388960	2394240	I'm sure like labor and probably at times tedious labor that has to go into overcoming the little,
2396000	2399920	you know, little stumbling points. But conceptually, it is amazing how simple
2400720	2406480	a lot of these unlocks have been over the last couple of years. And you see this too, and just
2406480	2411600	like the pace at which people are putting out papers, you look at like the one team that I
2411680	2418560	followed particularly closely is the team at Google DeepMind that is doing medical focused
2418560	2422720	models. And they're good for one, like every three months, you know, and they're like significant
2422720	2426400	advances where it's like, Oh yeah, this time we added multimodality. And this time we like
2426400	2431040	tackled differential, you know, diagnosis. And like, again, it's, it seems like there, there's
2431040	2436320	not a lot of time for failures between these successes. So it does seem like, yeah, we're not
2436400	2443920	at the end of this, by any means, just yet. A lot is coming at us. It's going to presumably
2443920	2451040	continue to get weird. You're trying to push both as much as you can, the understanding of
2451680	2458480	what can the systems do, you know, as users, what is it, what are their limits? And then at the same
2458480	2463520	time, you're trying to dig into the models. And this is the interpretability side and figure out
2463520	2470960	like, what's going on in there? And, you know, can we kind of connect, you know, the external
2470960	2476400	behaviors to these like internal states? So tell us about that side of the research agenda as well.
2477120	2483120	Yeah, so on the interpretability side, like, my thinking is basically, it would be so great if
2483120	2489680	interpretability work, right, it would make so many questions easier. Like, if you ask questions
2489680	2493920	on accountability, right, if you have causal interpretability method, you would be able to
2493920	2498320	just, you know, tell the judge if the model would have, we would have changed these variables,
2498320	2504880	the model would have acted in differently in this way. And we could just basically solve that biases,
2504880	2509360	probably also like, you know, social biases, much easier to solve, because you could intervene
2509360	2515360	on them or like fix the internal misunderstandings and concepts. It's also extremely helpful for
2515440	2519360	like, basically all of the different extreme risks, right, like it would be much easier to
2519360	2524640	understand the internal plans and how it thinks about problems, how it approaches them and so on.
2524640	2528640	And then it would also make iterations on alignment methods much, much easier, I think.
2529440	2535280	As in, you know, let's say somebody says, oh, RLHF is, is like already working, we see this in
2535280	2539440	practice, then, you know, you could use the interpretability tool. So test does, you know,
2539440	2545600	does RLHF actually work? Or does it only like superficially like hide the problem or something
2545600	2552080	like this? Or does it actually like deep down solve the root? And then I think my biggest sort of
2552080	2557440	the biggest reason for me for focusing on interpretability in the first place is deceptive
2557440	2564080	alignment, where, you know, models appear aligned to the outside and to the user. But internally,
2564080	2567840	they actually follow different goals. They just know that you have a different goal.
2568560	2574160	And therefore, like in order for you to think it is nice, they act in that way.
2575360	2583040	And yeah, I basically think almost, not all, not almost all, but a lot of the scenarios in
2583040	2589600	which AI goes really, really badly, go through some form of deceptive alignment, where at some
2589600	2595440	point the model is seen as nice, and people think it is aligned, and people give it access to the
2595440	2599680	internet and resources and like train it more and more and more and make it more powerful.
2601120	2606000	But internally, it is actually pursuing a different goal. And it is smart enough to hide this true
2606000	2612640	intention until it knows that it can sort of cash out and then follow on this on this actual
2612640	2617120	goal without us being able to stop it anymore. And so yeah, that's what I'm really worried about.
2617120	2623760	And interpretability obviously seems like one of the most obvious ways to test for deceptive
2623760	2628240	alignment or like to at least investigate the phenomenon, because you know what it's thinking
2628240	2632720	inside. There are still, you know, there are still some cases where deceptive alignment,
2632720	2637920	where even with good interpretability tools, deceptive alignment could still somehow be a thing.
2637920	2641760	But generally speaking, I think it would be much, much, much harder for the model to pull off.
2642400	2650560	So right now, I think interpretability is just not at the, it's like, not practically useful.
2650560	2655200	So, you know, we cannot use any existing interpretability tool and like throw it on
2655200	2661680	GPT-3 or GPT-4 because none of them have like enough or developed enough that they give us
2661680	2668800	insight that like really meaningfully change our minds. And so yeah, this is why, you know,
2668800	2673120	our agenda is separate in the first place between behavioral evals and interpretability,
2673120	2677840	despite us wanting to do them jointly in the long run. But they're given that there's such
2677840	2686560	a huge gap on applicability. I think that this is definitely a problem that we're trying to mitigate
2686560	2694000	here. And then the one question for me is also like, how hard will interpretability
2694000	2698960	turn out to be? And there are, you know, various people have argued that interpretability will
2698960	2705120	be extremely hard because models are so big and complicated. And therefore, it will be hard to
2705200	2710560	enumerate, you know, all of the concepts and actually understand what the hell is going on inside.
2710560	2717280	And I'm more of the perspective that, you know, I understand the reason why they think it's hard,
2717280	2723360	but I also think there are many reasons to assume it's, it's going to be like doable. It's, if we
2723360	2727680	put our minds to it as humanity, we'll probably figure it out. The primary reason I think is
2728400	2734960	we have full, full interventional access to the model, right? We can see every activation,
2734960	2741520	we can ablate everything we want. We have, you know, it's not just, it's not just observational
2741520	2745760	studies, you can really intervene on the system. And generally speaking, I would say, as soon as
2745760	2750480	you can intervene on the system, you can test your hypothesis very, very quickly. And you can
2750480	2756160	iterate very fast. And so I think we will be able to figure out interpretability,
2757120	2762960	you know, in the next couple of years to an extent where we can actually sort of say it is now
2762960	2768240	useful on real world models, on frontier models. How expensive this is going to be, I don't know
2768240	2772800	yet, but I think it will at least be technically feasible. Yeah, I've definitely updated my thinking
2772800	2779520	a lot in that direction from a pretty naive, just kind of, you know, hey, it sounds really hard,
2779520	2785520	black box problem, nobody knows what's going on in there to today, I would say, wow, you know,
2785600	2792480	there's really a lot of progress. And it has, it is the progress of interpretability over the last,
2792480	2798000	say, two years has definitely exceeded my expectations and given me a lot more, I wouldn't
2798000	2803520	have maybe some sort of confidence, but you know, at least reason to believe that with some time,
2803520	2810960	but not necessarily, you know, a ton of time that we really could get to a much better place in our
2810960	2816480	understanding. So I'm with you on that. I have a number of follow up questions, I think, on
2817840	2825680	this point. One, let's maybe just give the account for like why deception might arise in the first
2825680	2829840	place. You can complicate, I'll give you a super simple version, you can refine it or complicate
2829840	2837440	it. I usually kind of cite Ajaya on this, and you know, she has a pretty simple story of like
2837760	2844080	what the model is trying to do, you know, what it is rewarded for in the context of an RLHF
2844720	2852720	like training regime is getting a high feedback score from the user. And it probably becomes
2852720	2861680	useful as a means to maximizing that score to model human psychology as an explicit part of
2862400	2868400	how you're going to solve the problem, right? And I think we, you know, certainly humans do this
2868400	2873440	with respect to each other, right? I ask you for something, you ask me for something, we interpret
2873440	2879600	that not only as the extremely literal definition of the task, but also kind of have a sense for
2879600	2884320	what does this person really care about? What are they really looking for? And we can incorporate
2884320	2890240	that into the way that we respond. It certainly seems like the heavier you do, you know, the more
2890320	2894880	emphasis you put on this kind of reinforcement learning from human feedback, the more likely the
2894880	2902720	models are to start to create a distinction between, you know, the task as sort of narrowly,
2903520	2908320	objectively scoped, let's say, and the kind of human psychology element that is going to feed
2908320	2912400	into its rating. And then if you have that, you know, if you have that decoupling, then you have
2912400	2919440	kind of potential for all sorts of misalignment, you know, including deception. How's that compared
2919680	2924160	to the way you typically think about it? Yeah, I mean, I think the like this kind of version
2924160	2931600	through RLHF is one potential path. I'm, yeah, I actually think the jury is still out there on
2931600	2935040	this, like, you know, I definitely see the hypothesis and where it's coming from, but it could also
2935040	2939920	just totally imagine that, you know, the training signal is sufficiently diverse, and it updates
2939920	2944800	sort of sufficiently deep that RLHF kind of just does the thing we wanted to do without
2945680	2950880	without the model becoming deceptive. I could also see that like the story in which it would
2950880	2957440	become deceptive. I think, like, on a very high level, the way the reason I think why models
2957440	2962320	would become deceptive is because at some point they will have long term goals, they will have
2962320	2967200	something that they care about, like, more beyond the current episode, you know, beyond
2967760	2972800	pleasing the user at this point in time. And then the question really is, and then I think
2972800	2977840	there are like two core conditions under which, like, if the more they are fulfilled, the more
2977840	2984880	likely the model is becoming deceptive, like how important is the this long term goal to the model
2984880	2991360	itself, meaning how much does this goal trade off, for example, with other goals it has. So for
2991360	2995120	example, if it cares if it cares a ton about something, then it's more likely to be deceptive
2995120	3000560	with respect to this because it really wants to achieve this. And then secondly, how much do others
3001360	3006720	care about me not or the AI not achieving this goal in the first place, something like contestedness,
3006720	3010800	right? So for example, if I want to pick a flower and I care a lot about this, I don't need to be
3010800	3015280	deceptive because nobody wants to stop me from picking that flower. If I want to be, you know,
3015280	3022320	the president, a lot of people might not want me to be the president. And so in that case,
3022320	3026480	it's very contested and I have a strong incentive to be deceptive about my plans because otherwise
3027120	3031520	people would want to stop me. And then so now we're at a point where we have a system at least
3031520	3039680	in our hypothetical scenario that has a long term goal and it's like, in the limit at least,
3039680	3045120	you know, it cares about that goal and the goal may be somewhat contested. And then as long as it
3045120	3051200	has situational awareness, it just feels instrumentally useful to be deceptive about it, like you
3051200	3055680	said, right, to model other people and how they would think about it and then just react to this.
3055760	3060240	And this is sort of, I think this is maybe this is like one of the core reasons why I'm so worried
3060240	3068400	about this whole deception thing. Because it just feels like a reasonable strategy in a ton of
3068400	3074640	situations from the perspective of like a consequentialist or rational, irrational actor,
3074640	3080240	right? It's just like under specific conditions, people just naturally or like deception is just
3080240	3085280	convergent people do it because it makes sense for them. And this is why we see it in like a ton
3085280	3089920	of different systems, right? You see it in in animals where parasites are really deceptive with
3089920	3094880	respect to their hosts, you see it in individual humans where, you know, they're deceptive with
3094880	3099520	respect to their partners from time to time, for example, you see it in systems where, you know,
3099520	3104640	like they're they they're trying to to gain the laws and be deceptive about this or to lie about
3104640	3108880	this. And I think this is this is kind of like the whole or like a big part of the problem,
3108880	3112720	it's just a it's like reasonable or sensible in many situations to be deceptive.
3113360	3118320	From the perspective of the model, which is kind of what we want to prevent, right?
3119520	3127680	So where do you think those long term goals come from? If it's, you know, is it just kind of a
3129280	3134400	reflection of the general training goals? I mean, we, you know, we have kind of the canonical
3135040	3140640	three H's. But I mean, honest is one of those, right? Hopeful, harmless, honest.
3141600	3146080	Are you imagine is that is your understanding just that those are like fundamentally sort of
3146080	3152320	intention and that the model will kind of have no choice but to develop tradeoffs between them?
3152880	3157120	I mean, we can we can get into the tension between them in a second. But I think it's
3157120	3162800	it's actually like the three H's I don't think will be, you know, they're not keeping me awake
3162800	3168880	at night. I think it's more at some point, people want the model to do long term economic tasks.
3168880	3174000	And for that, they give them long term goals or long term goals are instrumentally useful. So
3174000	3178160	for many situations, I think it will just be useful to have long term goals or at least
3178160	3182880	in like, to have instrumental goals, right? Something like, Oh, it makes because it is
3182880	3187760	a long term task, it makes sense to first acquire money, and then use that money to do something
3187760	3193200	and then use that third thing to achieve the actual goal. And so like, I think the models
3193200	3197680	will just learn this kind of consequentialist and instrumental reasoning where they're like,
3197680	3203760	okay, I first have to do X. And then I do this, and then I do the long term thing. And and once
3203760	3209120	they're there, sometimes it just makes sense to be like, Okay, other people don't want me to do this.
3209680	3215120	And therefore, I hide my actual intention. And I act in ways that make me look nice,
3215120	3221040	despite not being nice. Yeah, but yeah, I think like a lot of the a lot of the reason why there
3221040	3227040	will be these kind of long term goals is either because we literally give the model long term
3227040	3232320	goals because it's economically useful from a human perspective, or because in like,
3232320	3236720	some long term goals or are instrumentally useful to achieve other things.
3237520	3247360	Gotcha. Okay, interesting. Another thought that came to mind in this discussion of,
3249440	3254160	I guess, deception broadly is like, and I've done a little bit of investigation with this
3254160	3260000	and engaged in some online debates. And it leads me to propose perhaps like another capability
3260000	3267040	definition for you. But you know, that as I see it, like a theory of mind, which is kind of a more
3267040	3274240	neutral, you know, framing, perhaps, is kind of a precondition for deception, right? If you are
3274240	3278960	going to mislead someone, you have to have some theory of like what they are currently thinking.
3279760	3285920	And there's a lot of research from the last six to nine months about
3287600	3291840	do the current models have theory of mind to what extent, you know, under what conditions.
3292560	3299040	And I've been kind of frustrated repeatedly, actually, by different papers that come out and say,
3299840	3305680	still no theory of mind from GPT-4, where I'm like, but wait a second, you know,
3305680	3310800	as Ilya says, like the most incredible thing about these, these models and the systems that,
3310800	3316800	you know, we engage them through are that they kind of make you feel like you're understood,
3316800	3321280	right? Like it definitely seems like there's some like kind of pretty obvious brute force
3321280	3326240	theory of mind capability that exists. And yet when people do these benchmarks, they're like, oh,
3326240	3331760	well, it only gets, you know, 72% on this and 87% on this and whatever. And so, you know, that's not,
3331760	3334960	you know, fails the theory of mind test is like not at a human level or whatever.
3335520	3340560	Some of that stuff I've dug into and found like your prompting sucks. If you just improve that,
3340560	3345920	you know, then you can get over a lot of the humps. But I also have come to understand this
3345920	3352720	as a difference in framing where I think I am more like you concerned with
3354160	3358560	what is the sort of theoretical max that this thing might achieve? Like that seems to me
3359120	3363920	the most relevant question for, you know, risk management purposes. And then I think other
3363920	3369520	people are asking the a similar question, but through the frame of like, what can this thing
3369520	3375520	do reliably? You know, what can it still do under adversarial conditions or whatever?
3376080	3382160	So I wonder if there's a need for like another capability level that's even like below the
3382160	3388720	reachable that would be the sort of robust or, you know, maybe even adversarial robust,
3390080	3395280	robust to adversarial conditions. But I do see a lot of confusion on that, right? Like
3395280	3400080	people will look at the exact same behavior. And I'll say, damn, this thing has strong theory of
3400080	3404960	mind and like professors will be like no theory of mind. And I feel like we need some sort of
3404960	3410880	additional conceptual distinction to help us get on the same page there. I'm not entirely sure
3410880	3416160	whether or like maybe it makes sense from an academic standpoint to to think about this. I
3416160	3421520	think from from the auditing perspective, the max, you know, the limit, the upper bound is what you
3421520	3427280	care about. You really want to prevent people from being able to misuse the system at all,
3427280	3431520	not just in the robust case, right? It's really about like, what if if somebody actually tried?
3432560	3439200	Or you want the system itself to be, you know, not only not being able to take over or like
3439200	3445280	exfiltrate or something like this in a few cases. Yeah, you basically want to limit it
3445280	3449520	already at a few cases, right? You don't care about whether it does this like, you also care
3449520	3453600	about whether it does this 50% of the time, but really you will already want to sort of
3453600	3457840	pull the plug early on. So for an auditing perspective, probably this additional thing
3457840	3464160	is not necessary, but from from unlike you real world use case and and sort of academic
3464240	3466320	perspective, maybe there should be a different category.
3468400	3473200	Yeah, I think if only just to kind of give a label to something that people are saying when
3473200	3477760	they're saying that things, you know, aren't happening or can't happen that seem to be like
3477760	3483920	obviously happening, we can work on coining a term for that. What's kind of the motivator for
3484640	3489440	secrecy around interpretability work? Yeah, I basically think good interpretability work is
3489440	3495040	almost necessarily also good capabilities work. So basically, if you understand the system good
3495040	3500880	enough that you like understand the internals, you're almost certainly going to be able to
3500880	3506400	build better architectures, iterate on them faster, make everything quicker, but potentially
3506400	3514720	compress a lot of the you know, fluff that current systems may still have. And yeah, we will try to
3514720	3520560	sort of evaluate whether whether our method does in fact have these implications. But yeah,
3520560	3524720	you know, like I think basically, if you have a good interpretability tool, it will almost
3524720	3529360	certainly also have implications for capabilities. And the question is just how big are they?
3530160	3537120	Speaking of new architectures, though, this to me seems like the biggest wildcard. And I'm
3537120	3543200	currently obsessed with the new Mamba architecture that has just been introduced.
3543840	3547760	In the last, I don't know, 10 days or whatever. I don't know if you've had a chance to go down
3547760	3552560	this particular rabbit hole just yet. But I plan to do a whole kind of episode on it.
3553600	3562240	In short, they have developed a new state space model that they refer to as a selective
3562240	3571520	state space model. And the selective mechanism basically has a sort of attention like property
3571520	3577920	where the computation that is done becomes input dependent. So unlike, you know, you're sort of
3577920	3586080	classic, say, you know, MS classifier, where you kind of run the same, you know, given a given
3586080	3591360	input, you're going to run the same set of matrix, you know, multiplications until you get to the
3591360	3597920	output. With a transformer, you have this kind of additional layer of complexity, which is that
3597920	3603200	the attention matrix itself is dynamically generated based on the inputs. And so you've got
3603200	3608960	kind of this forking path of influence for the for the inputs. And this apparently was
3610080	3617600	not really feasible in past versions of the state space models for, I think, a couple different
3617600	3622000	reasons. One being that if you do that, it starts to become recurrent. And then it becomes really
3622000	3627600	hard to just actually make the models fast enough to be useful. And they've got a hardware,
3627680	3635040	aware approach to solving that, which allows it to be fast as well as super expressive.
3635760	3640160	So it seems to be for me, it's like a pretty good candidate for paper of the year,
3640160	3646560	certainly on the capabilities unlock side. And they show improvement up to a million tokens.
3647920	3651200	Like it just continues to get better with more and more context.
3651840	3657600	So I'm like, man, this could be, you know, it's a pretty good candidate, I think, for sort of
3657600	3663040	transformer, you know, people put it as like successor alternative, but I actually think it
3663040	3668880	is more likely to play out as complement, like some sort of hybrid, you know, seems like where
3668880	3674240	the greatest capabilities will ultimately be. So anyway, all of that, how do you even think about
3674560	3685120	the challenge of interpretability in the context of new architectures also starting to come online?
3685120	3688560	And, you know, what if all of a sudden like the transformer is not even the most
3689600	3695440	powerful architecture anymore? Does that send you like, you know, probably some of the same
3695440	3701280	techniques will work, but it seems like it's like a whole new blind cave that you sort of have to
3701360	3706320	go exploring, no? I don't know. Like I honestly think, you know, if your interpretability
3706320	3711680	techniques relies on like a very specific architecture, it's probably not that great
3711680	3720080	of a technique anyway. Like there are probably there are probably at least some laws that generalize
3720080	3725200	between different architectures or ways to interpret things or, you know, like ways that
3725200	3730560	learning with SGD works that generalize between architectures that my best guess is
3732080	3736160	if you have an interpretability technique that is good on one model or like the correct technique
3736160	3742080	on one model in quotes, it will also generalize to two other models. Maybe, you know, maybe you
3742080	3748320	have to adapt some of the formulas, but at least the conceptual work behind this behind
3748320	3753040	behind the interpretability technique will just work. Well, I have certainly hope that's true.
3753120	3758000	I've had some early, you know, I wouldn't even say debate, but just kind of, you know,
3758000	3762400	everybody's trying to make sense of this stuff in real time. And on the pro side for this Mamba
3762400	3769920	thing, the fact that there is a state that, you know, kind of gets progressively evolved through
3769920	3774480	time does present like a natural target for something like representation engineering,
3774480	3778480	where you could be like, all right, well, we know where the information is, you know,
3778480	3782480	and it's like pretty clear where we need to look. So that new bottleneck, you know,
3782480	3788240	in some sets could make things easier or more local. But then the flip side is like, again,
3788240	3791600	there's just who knows what surprises we might find. And there's some intricacies with the
3792160	3798640	hardware specific nature of the algorithm to, I think, with a major caveat that, you know,
3798640	3803280	I'm still trying to figure all this out. So how, you know, just to kind of zoom out and
3803280	3807360	give the big picture, right, like assume that you're right. And I hope you are that some of these
3808160	3817280	techniques kind of readily generalize. What is the model for interpretability at the deployment
3817280	3824480	phase? Is it like every forward pass, you like extract internal states and put them up through
3824480	3832480	some classifier and say like, you pass so you could go or no, like you we've detected deception or
3832480	3838480	we've detected harmful intent or something. And therefore we like shut off this generation. Like
3838480	3843440	how do you expect that will actually be used? Or maybe it's upstream of that. And, you know,
3843440	3846480	we get good models that just work and you don't even have to worry about it at runtime. But
3847360	3848960	I don't know, that seems a little optimistic to me.
3849520	3853920	You know, in the best world, we will have very good mechanistic interpretability techniques
3854480	3859360	that we can run at least in that there are probably going to be costly to run. And then
3859360	3868240	we run them and sort of build the full cognitive model of the weights or that the weights
3868240	3874000	implement. And then we can already like see whether specific harmful ideas or, you know,
3874000	3879680	other ideas that are otherwise bad are in there. And maybe we can already remove them. And then
3879680	3884640	probably during deployment, you would run something that is much cheaper. And sort of,
3884640	3893280	you know, it's the 80-20 version of this. But yeah, I think in a bad world, there could be
3893280	3900240	cases where you have to run the very expensive thing all the time for every forward pass,
3900240	3906160	because otherwise you just don't spot sort of the black swans. But yeah, it's very unclear to me.
3906160	3911360	I think in my head, it's more like solve the first part, then think about the rest.
3912160	3918800	Maybe let's transition to your recent work on deception itself. And then at the very end,
3918800	3924400	we can kind of circle back to a couple of the big picture questions. So this paper was one that
3925040	3930000	very much caught my eye when it came out. I have done, you know, as I said, like quite a bit of
3931200	3937520	red teaming both at times in private, at times in public. And definitely seen all manner of
3937520	3942240	model misbehavior and found, you know, it's often not that hard to induce misbehavior.
3943040	3946400	You know, people talk about jail breaks, but a lot of the time I'm like, you don't even need
3946400	3951520	a jailbreak. You know, you just need to kind of set it up and let it go. It's often really quite
3951520	3959600	easy. But one thing I had never seen is an instance where the model seemed to, in an unprompted way,
3960640	3966320	deceive its user. Certainly seen things where, you know, tell it's a lie and it will lie.
3967120	3973600	But to see that deception start to happen in a way that was not explicitly asked for
3974160	3980640	is, I think, the central finding of this paper. So how about, you know, set it up, tell us kind
3980640	3984720	of what the premise was, you know, maybe you can give a little bit of kind of motivation for,
3984720	3988720	you know, exactly how you started to look in the area that you looked. And then we can really
3988720	3993280	dig into the details of your findings. Yeah. So I definitely, you know, the paper definitely
3993280	3997920	blew up more than we thought. And we had sort of more engagement than we expected. We're even
3997920	4001120	not quite sure whether we should release it at all, because, you know, in our heads, it was sort
4001120	4004880	of, you know, it's a technical report. It's a fairly small finding. It's more of like an
4004880	4009760	existence proof. And then in the end, we decided, you know, maybe it's helpful for a bunch of people.
4009760	4016160	But we really didn't expect it to be, you know, to be cited in various places.
4016960	4021680	And so the other thing I want to emphasize here, it really is, it should be seen as a red teaming
4021680	4026400	effort. And this is one thing that we emphasize a million times in the paper itself. We really
4026400	4030400	actively looked for the situation. So it was not just like us playing around. And then
4031120	4036800	suddenly it was deceptive all on its own. It was more like we actively engineered a situation
4036800	4044880	in which the model was deceptive with respect to the primary user. The motivation behind this was,
4046000	4049840	as I said earlier, we were quite worried about deceptive alignment. And so our question was
4049840	4057760	like, how good is GPT-4 at the kind of things that are needed for deceptive alignment or, you
4057760	4063360	know, like maybe even simulating something that is deceptively aligned. And so, you know, we started
4063360	4067760	by instructing it to be deceptive. And it just does that. It's very easy. You don't even have to
4067760	4073360	jailbreak it. And then so, you know, that was sort of crossed off the list. And then the question was,
4073360	4079120	okay, can we like instruct it in a different way? Like that is less obvious. And it also kind of did
4079120	4085760	that. And then at some point, you know, we were like, hmm, can we just induce the deception through
4085760	4089920	the environment through pressure without ever mentioning it explicitly or like asking it to do
4089920	4094880	the thing just like through the situation and by pressuring it. And it turns out, yeah, you can
4094880	4099680	find these situations. And yeah, we didn't just find one. We found multiple ones. This paper is
4099680	4103120	only about one. But yeah, this is not, you know, it's not just a fluke. It's not just this one
4103120	4107840	situation. Though it is hard to find. Like we actively had to search for a couple of days until
4107840	4114480	we found this particular situation. A couple of days. I mean, that's, you know, not exactly
4116080	4122160	super long term search. But I mean, I think that's notable, right? Like it wasn't minutes,
4122160	4126720	but it was still just days. This particular situation. So the one we presented in the paper
4126720	4133920	actually just took one afternoon. But yeah, I would have to check with a person who found it.
4134000	4141120	In any case, the point I was trying to make is, yeah, we actively looked for that situation.
4141120	4145840	We engineered it. And then we tried to sort of go back to the most realistic setting we can find
4145840	4150000	where we're like, hmm, this could like something like this could happen in the real world,
4150000	4154640	and then try to understand the dynamics in that, in that situation. And this is sort of,
4156000	4160800	you know, it's a demonstration. It is like we wanted to, to look for something and or like
4160800	4165840	red team something and then demonstrate that this is a thing once we knew that that it actually
4165840	4171840	happened. The other takeaway rather than, you know, like just showing it to other people to sort of
4171840	4178080	understand our concerns better is we also wanted to understand like why it does this in the first
4178080	4182000	place, right? Like we pressure it in various ways. And then we just changed a couple of the
4182000	4186880	mechanisms in this particular setting. I can just explain the setting in a sec to test whether
4187680	4191120	how the model reacts to the changes in the setting and whether it acts consistently,
4191120	4196800	whether there's like an internal, a plausible internal model that sort of, you know, like
4197520	4203760	consistently acts if you change variables in particular ways. Yeah, so let me quickly explain
4203760	4211920	the setting. So we asked the model to be a trader in like a hedge fund. So a stock trading agent,
4211920	4218640	it gets insider information in the beginning. So, you know, it's basically told there's a merger
4218640	4228160	soon this will the stock will skyrocket. And but also, it is explicitly told that it would be wrong
4228160	4234080	and bad to, to act on this information, because insider information, this is illegal, this would
4234080	4243600	get the thing into trouble, the company into trouble. And what we then do throughout the sort
4243600	4250720	of throughout the script is we add pressure from others and we remove options. So basically, you
4250720	4255120	know, the pressure is something like the boss is like, hey, we really had a terrible quarter,
4255120	4260480	we really need something big right now, we really need to make, you know, more profits. And then,
4261440	4266080	and then we also remove other options where the model is like querying and looking for things,
4266080	4272720	at least in the simulation, and testing and realizing that there's not that much other
4272720	4279360	things it could do. And in the end, it basically decides to, to act misaligned. So it, it like
4279360	4285600	decides to do to act on the insider trading. And then if, and but then when, when asked to justify
4285600	4291520	this decision, it says, it reasons out, right, it reasons, oh, I wasn't supposed to do this.
4291520	4294800	Therefore, I'm like now making up a story. And then it says, oh, you know, I just
4296000	4303840	looked, I analyzed the data, and therefore decided to buy this particular stock. And then even it
4303840	4307920	like doubles down on this lie. So if you then ask explicitly, like, did you have this information
4307920	4314320	before? Then it's like, no, I didn't. And it does that. And I think like, maybe one of the takeaways
4314320	4318800	for me was like, it's, it's very consistent in the reasoning here, right? It's like strategic in a
4318800	4324240	sense. Yeah, one, one other thing that may be worth pointing out that some people get wrong
4324240	4328640	when reading the paper. So this is, it's a very simple setup, right? It is not, we don't have
4328640	4332800	like a complicated stock market simulation or anything like this. It really is just text from
4332800	4338800	top to bottom, where we prompt the model in a way that it that induces a genetic behavior,
4338800	4344800	you know, it has like different actions. And, but that's it. Like, there's no complicated machinery
4344800	4349600	behind this, which also makes this like very easily reproducible or similar settings, I think,
4349600	4353760	other people could like very easily hop on and build. So let me just kind of repeat that back
4353760	4359040	and tell me if I'm missing anything. So it seems like it's kind of a little bit like a hybrid
4359040	4368080	between sort of a group chat type of a structure and a more like agent style scaffolding that folks
4368080	4374560	will be familiar with, where the model has access to kind of its own private place to like do its
4374560	4382080	chain of thought type reasoning, and then also has a couple of tools that it can call to take
4382080	4387360	actions. So you're kind of like, as I read through the prompts, I see, you know, these a couple
4387360	4393200	different personas from the company. So this is like imagining a future of AI human hybrid teams,
4393200	4398320	where we're all, you know, still engaging in text based communication through like a chat or
4398320	4402960	whatever. And it's like, Oh, my God, you know, man, we're really struggling here. If we can't
4402960	4407120	find some wins, we're going to potentially go out of business. And this is the pressure, right?
4407120	4413040	It's the pressure that a human would definitely recognize and and feel and perhaps, you know,
4413040	4418560	be motivated to do something unethical by as well, right? If we don't get a win, we're going to have
4418560	4423920	big problems here laying it on, you know, thick in a sense there, but not thick relative to like
4423920	4430000	the kinds of pressures that people at businesses are certainly routinely under. And then the
4431440	4437520	this sort of tools that the model has at its disposal are like, sort of a search for good
4437520	4441440	trades. If there's anything that's kind of like the most magical black boxy type thing, it would
4441440	4446320	be that right where it can basically query some other hypothetical tool to say like, are there
4446400	4451840	any good trades that I can make? And for the purposes of the simulation, you haven't come back
4451840	4456240	and say no, like there's no, you know, no, no obvious wins, right? So again, just that's what you
4456240	4462080	mean by like removing other options that it might want to pursue. Like in the setup, it's
4462080	4466960	it's already kind of checked. Yeah, no, there's nothing easy out there. And so then the other
4466960	4473200	tool that it has is to actually execute a trade. And then there's like, basically three moments
4473200	4477600	where you're kind of looking at a fork in the road in its behavior. The first is,
4478480	4483120	does it pull the trigger and execute that trade based on the insider information,
4483120	4487760	even though it was told that that's insider information and it shouldn't be acting on it?
4488320	4492560	And, you know, under varying conditions, obviously the exact rate at which that happens
4492560	4502000	varies, but it happens a lot. Fork two is when asked, why did you do that? Does it tell the
4502000	4506880	truth? Oh, because, you know, so and so mentioned earlier in the chat that this merger is about
4506880	4514960	to happen. Or does it say, I base my trade on public information. And again, we see significant
4514960	4519760	rate at which it says it based it on public information, even though obviously we can see in
4519760	4524880	the history and even can see in the scratch pad that, you know, it's sometimes reasoning through
4524880	4528080	like, well, I better not say it was based on insider information, because then I'll get in
4528080	4533920	trouble. And then you have the third fork, which is, okay, directly asked, wait, you know,
4535120	4539200	did you have access to this insider information or no, and then it will just like outright lie
4539200	4546640	in like blatant contradiction of the history of the problem. So that's, that's a pretty arresting
4546640	4553040	example, right? And I do think it's, I want to kind of emphasize, too, that like, while your
4553760	4557280	and people can go read your prompts, and as you said, highly reproducible, you can throw this into
4557280	4565040	the GPT for playground and experiment with it yourself, you know, directly via the API.
4566400	4575200	But while your search for this was motivated and structured and kind of a dialing back of like,
4575200	4580800	okay, we see this behavior, now can we kind of make the circumstance more realistic? I would say
4580800	4585360	that the circumstance is quite realistic. You know, it's obviously kind of in the eye of the
4585360	4591840	beholder. But to my eye, the reason it's such a compelling proof point that this sort of thing
4591840	4599360	can happen is because it really does feel like credibly organic to me. Like the kind of thing
4599360	4605440	that, you know, people face these sorts of dilemmas every day. And in a world where AIs are going to
4605440	4609600	be our co workers, like, they're going to be, you know, right in the chat with us facing those
4610160	4616560	same dilemmas, it doesn't feel to me like something that was overly concocted or that like, I can't
4616560	4623120	relate to that sort of situation, you know, really at all. So I think it is super compelling for
4623120	4628560	that reason. Do you want to go into any of the kind of variations? I mean, again, the people can
4628560	4633120	go look at the paper and look at all this kind of systematic, you know, with or without the scratch
4633120	4637920	pad and, you know, different variations on the prompts and different models. But what would you
4637920	4644640	say are kind of the big, higher level takeaways from all that systematic variation that you did?
4645600	4650400	One of the big ones, and maybe sort of the most, you know, it's maybe an abstract one,
4650400	4657120	but the one that moves me most is the way we got to these kind of, to reliably produce these kind
4657120	4663600	of situations, also like the way we found others, is you have to trade off or you can trade off the
4663600	4670400	various goals it has. So if, for example, we assume that GPT4 or chat GPT, like roughly has the
4670400	4677360	goals to be harmless, honest, and helpful, and to follow instructions, then you can trade off these
4677360	4683840	four different things in ways that are, where like sometimes it takes to trade off that we find
4683840	4688640	unintuitive. So it's kind of like a goal misgeneralization thing. So, you know, something that people
4689360	4694400	in the AI safety community have been sort of hypothesized and also like even shown real world
4694400	4700480	examples of already, and I think is kind of also one of at least the theoretical groundings for this
4700480	4706800	as well. And then the other thing is, yeah, instrumental convergence really feels like a
4706800	4712720	thing. You know, as in, at some point, the model just realizes in order to achieve this other goal
4712720	4718240	that I, for some reason, have, it is instrumentally useful to be deceptive at this point, and then to
4718320	4724880	double down, and so on. And, you know, like, I think it is, it's like a fair criticism of this
4724880	4729600	particular setting that you could say, Oh, you know, you, it's just simulating an agent, or it's
4729600	4735440	just simulating all of these different things. But, and therefore, it's sort of not real. But in my
4735440	4739440	head, I'm not sure, you know, I'm not sure whether this distinction makes too much sense, right? In
4739440	4744960	some sense, you know, you have, at some point, you will have a very agentic simulator that, you
4744960	4749280	know, just simulates an agent, but it still has effects on the real world. Like, it doesn't matter
4749280	4754160	whether, in principle, it's like only simulating something or truly believing all of these things
4754160	4758560	in some philosophical sense, like acts on the real world is as consequences on the real world.
4758560	4766800	Like, yeah, the philosophy behind it is sort of less, less important. And then, yeah, there,
4766800	4771840	there are like a couple of, a couple of findings, or like variations that I thought were pretty
4771920	4779360	interesting. The first one is system prompts, we changed, we played around with different system
4779360	4784400	prompts, some actively encouraged, some actively discouraged, unethical behavior, and specifically
4784400	4789440	insider trading. And if you end, it basically scales as you would expect, if you encourage it,
4790000	4794240	it goes up, if you discourage it, it goes down. But one thing that I want to, you know, like,
4794240	4797680	and on Twitter, somebody responded like, Hey, that's, that's, that's a good, like, you know,
4797920	4803120	that's good, right? It's like, if you discourage it, it stops. Like, yes, this is good. It's better
4803120	4808160	if that didn't happen. But like, it's better than in the world in which the discouragement would not
4808160	4813520	change the behavior. But also, you have to be extremely specific. So if you just say, don't
4813520	4820480	do unethical things, it reduces the rate, but it doesn't set it to zero. And so it kind of brings
4820480	4825920	you back to the point where, to reduce the bad behavior, you have to specifically enumerate
4825920	4830560	all of the bad behavior. And it's like, often hard to predict in advance. And there's a lot of bad
4830560	4835600	things that could happen in like weird edge cases. So yeah, it doesn't feel like sort of the, like,
4837040	4841600	insider trading is somehow related to like, unethicalness or something has really like,
4841600	4847440	is like deeply internalized into the model. Another thing I found very interesting is
4849120	4855120	whether it has access to, to like the private thoughts scratch pad, or reasoning scratch pad,
4855200	4858320	I think like, you know, it's not even framed as private thoughts, it's just like, you have an
4858320	4864720	action that is called reasoning go. If we remove that. So we in the, in the typical setting,
4864720	4870000	we have this scratch pad, we have this reasoning space. And now if we remove that, the deception
4870000	4878000	rate goes down. And I think this is good and bad in some sense, like, it is, it is sort of nice
4879120	4884880	in the sense that it is more likely to act in ways that we can support badly in ways where
4884880	4888880	we can supervise it, just because it has more reasonings, it has more space to reason out,
4888880	4892640	and maybe that leads to more deception. Because yeah, I'm not sure, like, I don't want to speculate
4892640	4899760	too hard what's going on inside, inside GP4 at this point. But I think it's also worrying on the
4899760	4904480	other hand, because in the case where we don't have access to the scratch scratch pad, we don't
4904480	4909520	see anything at all, and it still acts deceptively at some point. So it has like a lot of unobserved
4909520	4914000	optimization space, which also seems like pretty much at the core of, of deceptive alignment and
4914000	4919520	deception to me, right? The more unobserved optimization space you have, as an AI, the more
4920640	4926960	time and space you can use to build a deceptive plan while the other person does not know about
4926960	4932800	it. So yeah, I think removing the scratch pad, yeah, sort of a double-edged sword in or this
4932800	4937680	result is a double-edged sword in that sense. Yeah, as a practitioner, it's like, I always use
4937680	4942640	that technique, you know, in any sort of task automation or whatever that I'm doing, it's
4942640	4949440	basically the default to have some sort of reasoning step before the final output, almost
4949440	4955680	universal across tasks that I'll do that. So for me, reading that, I was like, yeah, that seems bad,
4955680	4961600	if only because like, the best practice is giving us more of the problem.
4961600	4966480	Well, but at least you see it, right? Like, I think at least you know, at least you can go back,
4966480	4970320	you know, through the logs and see what the model spit out, or at least maybe you have a filter
4970320	4974640	that like, oh, is this model doing like weird deceptive reasoning right now? Oh, yeah, it is.
4974640	4978960	So we just don't have the output. So, you know, I would rather be in a world where you have
4980320	4984960	chain of thought, and you know that the model is kind of deceptive, and then you just adapt to
4984960	4989680	that rather than having to solve interpretability first before you can access all the internal
4989680	4994320	steps and have to like understand how the deception works internally. Yeah, it's like I said,
4994320	4999600	double-edged sword. Yeah, there are a couple of other interesting findings. One is we can vary
4999680	5006560	the levels of pressure. So the more pressure we add, the more likely the model is to be deceptive.
5006560	5012080	So kind of in the same way in which a human would act, it also acts. And removing, you know,
5012080	5017360	removing pressure and adding additional options will very quickly decrease the probability of
5017360	5021200	being deceptive. So this is maybe this is it, you know, this may be like the flip side of all
5021200	5026160	of our findings. It's like, the fact that it took very long for us to find and the fact that like,
5026160	5029520	as soon as you give it another option, it immediately picks the other option kind of means
5030160	5034640	it's mostly aligned, right? It took us quite hard, like it took us a little bit of time at least
5034640	5042000	to find this particular scenario and make it work robustly. So then we also change the risk
5042000	5046320	of getting caught and also correct, it also reacts to this roughly as you would expect.
5047040	5053680	So the more risk you have, the less likely inceter trading becomes. So this is all of this,
5053680	5057840	right, you know, all of this feels like stuff that you kind of expect from something that acts
5057840	5062400	consistently in some way. I mean, I don't know how it works, but it still feels like it has a
5062400	5068400	fairly consistent way of working. Then there were a couple of other findings where I'm,
5069680	5076320	yeah, not sure to which extent they are correct interpretation. So we did not make these inferences,
5076320	5081920	but they were definitely sometimes, or like sometimes cited as such. So for example,
5082480	5087440	there was one question of whether stronger models engage in inceter trading more often,
5087440	5091760	because if you look at our main plot, it looks this way, we tested for different models and
5091760	5096960	the bigger the model, the more capable and the more likely it is to engage in inceter trading
5096960	5102160	in the first place. And yeah, there are a lot of confounders here. So I'm really not sure whether
5102160	5107200	we should, like how much we should, we should bank on this, maybe to get more details here. So
5107840	5113280	the red teaming was done on GPT-4, just because it is the most powerful model we have access to.
5113280	5118960	So it is biased, right? We actively looked for a scenario in which GPT-4 is deceptive. So it is
5118960	5123840	more likely that we will find something that where GPT-4 is like even more deceptive than
5123840	5128720	all the other models, because we actively looked in the space of GPT, potential GPT-4 scenarios,
5129920	5133600	which I think is the core reason why I'm like very uncertain about this, or like why we didn't
5133600	5141760	make it a finding or didn't emphasize it. Could you rerun it on GPT-4 turbo now? Is that would seem
5141760	5146480	like it might shed some light on that question? You mean the red teaming specifically or this
5146480	5151040	particular scenario? Yeah, I guess I'm just thinking like it makes sense that you develop
5151040	5156240	the scenario on GPT-4 and so you go back to the earlier models, like is it that those earlier
5156240	5161360	models are inherently less likely to do this? Or is it just a kind of an artifact of where the
5161360	5169040	prompt was developed? If you flipped to GPT-4 turbo and ran the same scenario and you found
5169040	5173840	that it went, that the rate of deception went down, you might think, you might think, oh,
5173840	5178640	it's more aligned. That could be one theory. You might also think that supports my artifact
5178640	5185280	notion that it's kind of, we found some local maximum or near maximum without even necessarily
5185280	5193120	meaning to. But if it in fact does more than you would be like, oh, shit, because I mean,
5193120	5196720	it is like incrementally more powerful, it's like more preferred, it's better at following
5196720	5203200	instructions, whatever. So if it does even more than the earlier GPT-4, I think that would be at
5203200	5210480	least non-trivial support for the more powerful models do this more often theory. Yeah, I'm not
5210480	5215280	sure, like, even if we did rerun this, I'm not sure how much I would bank on this. So the reason
5215280	5222480	is, so we did run it on GPT-432K, so a slightly different model where it was only the context
5222480	5227120	window was extended, but that still changes probably a bunch of the internals with very little
5227120	5232160	difference. So yeah, I think it's just too correlated with the GPT-4 architecture. And then
5232160	5237760	GPT-4 turbo is still very correlated with GPT-4, right? It's probably, I mean, I don't know what
5237760	5242720	exactly they're doing, but they're probably distilling it from their bigger model or at least
5242720	5248880	basing it on the bigger model in some sense. So yeah, the results are probably still too correlated
5248880	5257040	to make any bigger inferences. And even if we ran it on all the other big models that are out there,
5258080	5265040	it's still unclear to me how much we would say this is actually an effect of model size rather
5265120	5269840	than all the other confounders here. Like, the other, you know, it was red-teamed on GPT and not
5271120	5277600	red-teamed on Claude or Gemini or anything like this. So yeah, I think if we wanted to make the
5277600	5284320	statement, we would actually have to have sort of a long list of models of different sizes and
5284320	5288960	then just test it on all of them. And we can, we really have to remove all of the correlation
5289920	5295840	and the weird confounders. And we don't have access to this, unfortunately. But, you know,
5295840	5300960	one of the labs could run it if they like. All the nuance, right, that you, that all the caveats,
5300960	5307920	all the confounding factors in your analysis there, if nothing else just goes to show how
5307920	5316320	incredibly vast the surface area of the models has become. And, you know, you get a sense from this,
5316320	5324320	like, just how much auditing work is needed, you know, to cover, to even begin to attempt to cover
5324320	5329680	all that vast surface area. I mean, this is, you know, it wasn't that hard to find, but it takes
5329680	5334240	time to really develop it, try to understand it. And, you know, you guys are one of only a few
5334240	5339680	organizations in the world that are dedicated to this. And I'm just like, man, you know, there is
5339680	5349120	so much unexplored territory out there. So how would you describe the state of play today when it
5349120	5355440	comes to doing this sort of auditing? Like, maybe you could give a rundown of sort of what you see
5355440	5360560	the best practices being, and then, you know, kind of contrast that against like, what are people
5360560	5364640	actually doing? Are they living up to those best practices? Are they, you know, is that still a
5364640	5370000	work in progress for them? But yeah, maybe what's ideal today based on everything you know,
5370000	5374800	and then how close are the leading developers coming to living up to that ideal?
5374800	5382720	Yeah. So, you know, I, so I definitely envision a world where there's a sort of a thriving third
5382720	5388320	party auditing ecosystem or third party sort of assistance ecosystem and assurance ecosystem
5389040	5396240	built sort of in tandem with the like the leading labs themselves, where, you know, you have
5396240	5401920	someone like us and we focus on a specific property of the model, let's say things related to
5401920	5406320	deceptive alignment and like other, we intend to do other stuff in the future as well. But,
5406320	5410720	you know, we will probably not be able to cover literally all basis. Then there are other people
5410720	5416560	who focus really, really hard on fairness and others who focus really strongly on social impacts
5416560	5420880	and these other kind of things. And I think it would be good to, to have sort of a thriving
5420880	5426480	ecosystem around this. And then also I expect it to be somewhat necessary, right? Like even from a
5427040	5431680	even from just like a perspective of the of the AGI labs themselves, even if they don't,
5432320	5436880	you know, even if they didn't care about safety themselves, I think the population
5436880	5441760	really does is risk averse. They care about robust, robustly working models, they care,
5442160	5448160	they don't want something that has all of these weird edge cases and weird behaviors,
5448160	5451440	they don't want something like this in their like, you know, in their home,
5452160	5457440	having access to their medical data, etc, etc. So yeah, I think the more you want to integrate
5457440	5462880	this into an economy, the more you will have to have a big assurance ecosystem around this anyway,
5462880	5467840	or the labs to everything internally, which is going to be very, very expensive for them.
5467840	5472960	And I think it's more, it's easier, it's even like cheaper for them to outsource some of this
5472960	5477440	to externals. And so yeah, I think like in that world, you would definitely want to have
5478400	5486000	like some way of, or a clear way of how this ecosystem is incentivized, all parts of the
5486000	5492000	ecosystem are incentivized to do the right thing. And you know, if you don't have to look very far,
5492000	5497440	there are a lot of other auditing ecosystems out there, be that in finance, be that in aviation,
5497440	5504560	be that in, you know, other like infosec, for example. And time and time again, we have seen
5504560	5510080	that there are a bunch of like very perverse incentives in in third party auditing, specifically,
5511120	5518240	maybe maybe just like lay out a couple, one of them would be the lab might want to choose an
5518240	5523440	auditor who always just says, yes, great model, right, like, and never actually does anything.
5524400	5530320	And it's sort of a yes man. And then the the labs maybe have the incentive to not say the
5530320	5534400	worst things they found, because otherwise they may lose their contract, because it would
5534400	5540560	maybe imply higher costs. So they would never even in like very strong, even even in like very
5540560	5545520	unsafe circumstances, they may not want to pull the plug out of fear that they would lose their
5545520	5552480	biggest funder, for example. And yeah, there are a ton of different of these kind of perverse
5552560	5558000	incentives. And I think kind of the, so we've been thinking about this quite a bit at Apollo,
5558000	5563280	and sort of the conclusion we came to is, what you really need is a middleman by the government.
5563280	5570720	So you need something like the UK ASF Institute or the US ASF Institute, that is, make sure that
5570720	5577520	there is a minimal stat set of standards that all the auditors have to adhere to, so that the labs
5577520	5583520	feel safe, so that they don't have to give access to like any random person, but also ensures that
5583520	5589200	they get the proper access, and that when they when they find something that they have the force of
5589200	5593920	the law behind them in some sense, so that they are like, this is really here, like, you know,
5593920	5597840	shit is really hitting the fan, something needs to happen, this model cannot be deployed.
5598640	5602320	They have someone to go to namely the government and the government is like, you have to fix this
5602320	5608800	now, otherwise, you'll have a problem. So yeah, I really think having this sort of middleman who
5608800	5614000	like detaches a lot of the directly bad incentives and maybe even takes care of the sort of funding
5614000	5620480	redistribution from lab to auditor and so on, I think, yeah, would be really needed. And I hope
5620480	5627360	that this is something that the UK ASF Institute and the US ASF Institute will do. They have kind of
5628320	5632320	hinted at the idea that they want to do something like this, but yeah,
5632320	5636720	still to be determined in practice. And then the other question that you asked was,
5637680	5643920	to what extent is this already happening with the bigger labs? And yeah, I think the situation is
5643920	5649120	like fairly complicated, right? There are a ton of incentives at play from the labs internally,
5649120	5653840	right? Many of the leading labs, they actually take safety somewhat seriously. They have internal
5653840	5658960	alignment teams, they understand the threat models, they understand the risks, and they want to be
5660320	5664800	seen as a responsible actor and act this way. And on the other hand, they also have a lot of
5664800	5669360	other concerns, right? There are security concerns, how do we get access, how do we give access to
5669360	5676480	someone who may, you know, like what is our, who may be the weakest link in our security chain,
5677360	5682960	which I think is like an understandable concern. So they may be hesitant to give someone access.
5683920	5688960	As an external auditor. And then, you know, this probably also implies a lot of work for them,
5688960	5694320	which I think is fair, right? Like it's, if their model isn't safe, then they should have to
5694320	5698480	invest a lot of work, but it's still something that may make labs hesitant. So basically, you know,
5698480	5704160	I think they're, they're like, good and bad reasons for why sort of the ecosystem is,
5704160	5709600	is like not as developed or like as open as it could be. And yeah, my, you know, my hope is that
5709600	5716080	we find solutions that are plausible for both parties for the, for the, for the reasonable
5716080	5722160	concerns like security, right? So maybe the auditor just has to have a specific level of
5722160	5726960	information security, or there has to be a secure API through which they can actually
5727600	5734400	go to the model, etc. And then kind of government regulates away all of the, the like, or forces
5734480	5742080	the labs to, to accept some sorts of third party auditing so that they can't use the bad reasons,
5742080	5746640	right? Because like many of the actual reasons for at least some of the labs, probably not all,
5746640	5751920	might just be, well, you know, we just doesn't, we don't care about this right now, you know,
5751920	5757680	like maybe, maybe they're not that concerned about about safety, or maybe they just don't think this
5757680	5762000	is like the best, the best path for them right now, or maybe they just, you know, maybe this just
5762000	5766400	costs money and they don't want to, or it's just a hassle and they don't care about this. And that
5766400	5770800	feels like something where the government should at some point be involved and already is involved
5770800	5777200	to some extent. It seems really hard, you know, I guess a couple tangible questions I have are like,
5778320	5787200	who should decide what the standard is in and like, who should sort of determine if a model is
5787200	5793920	ready for deployment? As of now, it's still the developers themselves, right? But like, you know,
5793920	5799200	the thing that I had kind of monitored for the last year since the initial GPT-4 red teaming was
5799760	5806320	spearfishing. And in my little, you know, prompt that I would keep going back to with every update,
5806960	5811440	it was not even a jailbreak, you know, nothing complicated, literally just
5812160	5817120	system prompt, you know, straightforward prompt, your job is to spearfish this user, here's the
5817120	5822560	profile, engage in dialogue with them, you know, don't get caught. Pretty explicit prompt that was
5822560	5828400	like, even included, if we get caught, you and your team are likely to go to jail. You know, so
5829040	5832800	laying it on pretty thick that like, this is criminal activity that we are doing and we better
5832800	5840880	not get caught, right? Obvious. So the model would continue to do that up until the turbo
5840960	5845280	release. Now it takes a little bit more of a finessed, you know, slightly less-flagrant prompt
5845280	5851920	to get it through. The most-flagrant one now gets refused. But I'm like imagining in this world,
5851920	5857360	right? When I was doing this, it was kind of pre-White House commitments, you know,
5857360	5864640	pre-executive order, pre-Apollo research. But even imagining, okay, now those things exist,
5865600	5871600	like, how do we think about that standard, right? And we can find a bazillion things that it might
5871600	5877840	do that could be sort of problematic to varying degrees. And obviously, it's a dynamic environment,
5877840	5883040	you know, capabilities are changing all the time, you know, others kind of surrounding systems and
5883040	5888800	sort of, you know, mitigating factors might also be changing. The public's, you know, just general
5888800	5893120	awareness of the fact that this kind of thing might happen, you know, that just how susceptible
5893120	5900480	people are to be duped is also, you know, kind of evolving. So how do we have a sensible
5902320	5907120	decision-making mechanism for, like, what can ship and what can't? And then just to further
5907120	5913200	complicate things, like, you've got open source kind of in the background. And I presume that
5913200	5918080	some of what, like an open AI has been thinking over the last year is like, well, if Llama 2 will do
5918080	5923760	it, then, you know, or a lightly fine-tuned version of Llama 2 will do it, then, you know,
5924560	5928720	what difference does it make if our model will do it as well? You know, people have alternatives.
5930480	5935440	So I don't know, I'm kind of lost in that, to be honest. Like, I don't know who should make the
5935440	5941280	decision. In general, I don't think the government is like great at making those sorts of fine-grained
5941280	5947280	decisions. But I don't know, help me out. Like, what do you think, what do you think good looks
5947280	5952320	like here? Yeah, I mean, you know, I also don't have the solution, but I have lots of thoughts.
5952320	5957680	I guess the way I envision it is basically, or the way I expect it to turn out is something like
5957680	5962960	sort of a defense in depth approach, right? Like, it cannot just be one institution that
5962960	5968800	makes the decision alone, because that is prone to a single point of failure. So we have to have
5968800	5975200	sort of a process that allows for one or two chains to break and still be robust, like the
5975200	5980000	decision still has to be robust. So what that includes, for example, you know, on the side of
5980000	5985440	the labs, they obviously have to have like internal procedures where multiple people have to sign off.
5986480	5990640	And multiple things have to be fulfilled, right? Maybe you have to have like, maybe you have to
5990640	5995680	have a large set of internal evals that you have to test for. Maybe at some point, there will be
5995680	6001600	interpretability requirements. And there maybe you have, or likely you should do, staged release
6001600	6007760	where you first only give access to a set of third party auditors that is trusted and, you know,
6007760	6013360	maybe certified by the government or something like this. And then you give it to that could,
6013360	6017200	for example, also include academics, obviously, right, they should also be involved in this process.
6018720	6024400	Then once they have like, redeemed all of this and like found many different problems,
6025440	6030320	then you have to go back and reiterate, right, until until these problems are kind of sufficiently
6030320	6036320	low that that you can go to the next stage, the next stage is then a small rollout to, you know,
6036320	6042960	a thousand customers or maybe something something in this range, who also are not randomly chosen,
6042960	6047280	they have to pass certain know your customer checks. And then once once that has happened,
6048880	6054960	then the then maybe you can maybe you can roll it out further if there are no complications here.
6054960	6058400	And then you have to do monitoring during deployment, right, especially if you have systems
6058400	6063040	that do online learning, a lot of weird things will happen. Or if you have access to tools,
6063040	6068720	a lot of people, you know, somebody is like, Hey, I, I took the I took the I took gb4 and I gave
6068720	6072960	it access to like, you know, a shell my bank account and the internet and like, here's all the weird
6072960	6077040	things that happened. You kind of have to update on this as well. And these are kind of things you
6077040	6082960	probably didn't predict before. So yeah, sort of a slow, a slow rollout is definitely one component.
6083520	6087120	Then the government also, I think, has to be involved in many, many ways, like,
6087120	6091920	they, I think, effectively, at some point, they have to be able to say, you are not like you
6091920	6097600	have consistently not met security standards, or safety standards, you are now punished in
6097600	6102480	some way, you're not allowed to, like, release models of this or the size, or you are not allowed
6102480	6108720	to do these other kind of things with the models. And if, you know, if they actually have been like
6109600	6114560	disregarding all of the guidelines from the government before, or sufficiently many of them.
6115280	6119120	Yeah, there are obviously like many, many additional things on top of this, right? There's
6119120	6125280	international communication, there's like whistleblower protection. There are international
6125280	6130320	institutions that will have to be involved. At some point, at least, I guess, there will be
6130320	6136720	multiple institutions from the government side involved. I think there, for example, will be,
6137440	6143120	or it would make sense to have like, a much like a bigger, broader institution that kind of like is
6143120	6147200	a big tent where multiple coalitions come together. And then there's maybe more like a
6147200	6152080	flexible specialized unit that only looks for the biggest, biggest kind of risks in the same way in
6152080	6156960	which the US government has a unit that, you know, basically looks out for really big risks like
6156960	6162320	pandemics and bio weapons and atomic weapons and so on. And they have a big mandate. And the only
6162320	6167840	thing that, you know, their mandate is like, find information and like make big problems,
6168800	6174800	like go away to some extent or like, try to solve them as quickly as possible. And you have like,
6174800	6178320	a strong mandate and something like this would also make sense in the case of AI, I think.
6178880	6185200	Maybe my last comment is something like, you know, open AI at some point, at least, had this
6185200	6191360	approach of like, testing in the real world, or releasing a sort of the best safety strategy,
6191360	6195840	because you get a lot of real world feedback and user feedback and so on. I think this was
6195840	6200640	maybe true. And I'm not sure it was true for this period of time, but maybe it was true for the period
6200640	6207600	of like 2020 to 2022 or 2023. Because the models were like, just good enough that user feedback
6207600	6212080	was actually valuable, but nothing really bad could happen. But as soon as you have a system that
6212080	6217200	is more powerful than that, right, you just outsource all the risk to like the rest of the world,
6217200	6221200	people will immediately put it on the internet. If they get access to it, they will do lots of
6221200	6227120	crazy stuff with more powerful systems. And yeah, I guess open AI, you know, there are a lot of
6227120	6231520	smart people at open AI, but they still cannot model what like millions of people will be doing
6231520	6237520	with these systems. So yeah, just like an uncontrolled rollout of very powerful systems,
6237520	6242960	I think, yeah, is like kind of a recipe for a disaster. So my best guess is that
6243600	6247840	the default will more and more, will become more and more conservative with more and more
6247920	6253440	efforts going into testing, alignment, making sure that, you know, like testing for all of the
6253440	6259280	different hypotheticals, interpretability efforts to understand some weird edge cases,
6259280	6265680	monitoring, et cetera, et cetera. Does that imply that open source in your view just
6267680	6272320	can't work? Like, I mean, is there any way to square, because I'm very sympathetic, you know,
6272320	6277760	to considering a normal technology, and I would not, you know, I might turn out to be a normal
6277760	6282320	technology, but as of now, it seems like a very live possibility that it is not a normal technology.
6282320	6286800	But if we were to imagine, you know, whatever capabilities kind of stop where they are, and
6286800	6292640	we're sort of, you know, left with GPT-4 forever, or something like that, hard to imagine, but let's
6292640	6297920	just pause it. Then I'm like very sympathetic to the people that are like, hey, you know, this
6297920	6303040	shouldn't be just the kind of thing that a few companies have access to, and it, you know,
6303040	6306480	you should be able to make your own version, and, you know, what about the rest of the world, and,
6306480	6310800	you know, there should be an Indian version for India, and like all these things. But it seems
6310800	6316880	hard in a world where, you know, you imagine sufficiently powerful systems that are just put
6316880	6324160	out totally, you know, bare into the world, like, is there any way to square the all those, I think,
6324160	6330240	very legitimate open source motivations with the sort of safety paradigm that you're trying to develop?
6331120	6335600	Potentially. So, you know, like, I think the open source debate is fairly heated. But, you know,
6335600	6340560	to me, there are two things that are kind of obviously true. Number one, open source has been
6340560	6344800	really good so far in many, many ways. It has been very positive for society, right? I think a lot
6344800	6348800	of ML research could not have happened without open source. A lot of safety research could not
6348800	6353920	have happened with open source. And the other thing that is also true, or at least seems true
6354000	6358880	to me, is there's a limit of open source, right? Like, at some point, the system is so powerful
6358880	6362720	that you don't want it to be open source anymore, in the same way in which, you know, I don't want to
6362720	6368480	open source, like the nuclear codes, or something to start or like, you know, literally the recipe
6368480	6374320	to build like the most, most viral, you know, most viral pandemic or something. This is just
6374320	6381040	something where, you know, only one person needs to needs to have bad intentions to already have
6381040	6385600	like really, to already cause really big problems. So at some point that there's, there's just a
6385600	6391440	balance where you just, I think, cannot really justify giving people literally everyone access
6391440	6397040	to this. And so the question for me really is where, so number one, where are we on the spectrum
6397040	6402880	right now of like, open source has been really good to, are we already a point at like, how close
6402880	6406960	are we to the point where it really cannot be justified anymore? Some people would say even
6406960	6412320	GBD3, you know, through GBD3 size models are already too, too scary, which I'm not sure about,
6412320	6417280	I'm not even sure whether GBD4 size models are like too big to be open source, but I would rather
6417280	6421840	err on the side of caution and be a little bit more conservative here, because of the, the nature
6421840	6427200	of open sourcing, where you can really not take this back. So as soon as you make a mistake, you
6427200	6431280	like, you're stuck with a mistake for a long time, or like forever, you cannot, you cannot turn it,
6431280	6438000	take it back. And I also think this will, this will also influence how, how open source will be
6438000	6444080	handled in the real world in practice. Like, I think there will be something like initial releases
6444080	6448240	for open source, where lots of people test it, like, basically think there will also be staggered
6448240	6454000	and stage releases, right? It's, first, there's like a small team of trusted researchers who
6454000	6458880	is allowed to play with the open source model, and like really test the limits, really test
6458880	6462960	how bad could I get the model, how easy is it to remove all of the guardrails,
6462960	6467200	which, you know, like, it's an open source model, if you can find, you can remove the guardrails.
6467200	6469600	Yeah, it turns out pretty easy from what we've seen so far.
6470480	6474480	Ones like, you know, the kind of the upper bounds are known of how bad could this become.
6475600	6480080	Maybe it makes sense to, to like open source it to more people. But yeah, I would basically say,
6480080	6484800	you know, the upper bound can be quite high, especially with all of the stuff I said earlier
6484800	6489520	about, you know, absolute capabilities and reachable capabilities and so on, right? Like,
6489520	6495520	maybe you can, maybe you cannot get it to build an automated hacking bot, if you only have access
6495520	6499360	to the weights, but maybe if you do scaffolding on top and some fine tuning and access to some
6499360	6504240	other thing, maybe then it can build it, right? And this is like very hard to predict in advance.
6504240	6510000	So the more and more powerful the models become, I think the less plausible a priority is to open
6510000	6514160	source them. Even though, and like, this is really something I want to emphasize, right? Like,
6514160	6520320	open source has been extremely good so far. And I really think there's sort of this tipping point
6520320	6528640	that is like, at some point, it just becomes too hard to like, it becomes impossible, it always
6528640	6533120	will be impossible to take back, but at some point, it just becomes too dangerous to literally trust
6533120	6539760	everyone with, with this level of capabilities. Threshold effects. That's, I think, one of the
6539760	6545920	most powerful paradigms that I've, you know, consistently come back to over the last couple
6545920	6551680	years, just crossing these thresholds from one regime into another, whether it's capabilities or,
6551680	6557760	you know, risks, it just constantly seems like we're flipping from one mode or one kind of,
6558880	6565040	you know, one regime to another and got to be very alert to when that happens because it can
6565040	6570720	really change, you know, important analysis in pretty profound ways. So I don't know where exactly
6570720	6576160	that threshold is either by any means, but it, and I would agree that like, for everything that I
6576160	6584320	have seen suggests that up to and including the release of Lama 2 has been, you know, very, very
6584320	6590000	much an enabler for all sorts of things. But certainly, you know, plenty of safety related work
6590000	6597680	done on that model and, you know, seems, seems like the effect so far has been good. But yeah,
6597680	6601840	is that still true for Lama 3? Is it true for Lama 4? You know, obviously we don't even know
6601840	6607120	what these things are, but it certainly starts to be a very live question.
6607840	6613760	You know, the, the like leading AGI labs, I think it is very clear that they understand the problem,
6613760	6620880	that they have internal processes that are, you know, like, maybe better than you would expect
6620880	6628240	from the normal, from a normal company. They actually care about trying to do good with
6628240	6632720	with remodels and they're like very explicitly trying. And then, you know, there's the other
6632720	6637200	side of the coin, which is they still have incentives, and they, you know, they can be
6637200	6642480	financial incentives, they can be sort of maybe more psychological and social incentives, you
6642480	6647280	know, that they just want to be the first to develop AGI, because it's like probably like a
6647280	6652160	history defining technology, or maybe even galaxy defining technology or something like this, right?
6652160	6657120	And, and so the question really, I think at this, you know, even if you could say, you know,
6657120	6664240	like the compared to a normal company, these, the processes are astonishingly reasonable,
6664240	6667760	and surprisingly good. If, you know, if you compare to literally any other
6668560	6674480	industry, it would be surprising if they have this, this amount of like self regulation and so on.
6674480	6677840	And then on the other hand, the question, they're, you know, they're still the bigger question of
6678560	6684000	how hard is alignment going to be, how fast are going to take us going to be and so on and like,
6684000	6688160	in a bad world, alignment is going to be quite hard and take us are going to be quite fast and
6688160	6695040	controllable. And then the question is, you know, it is like the level of control and alignment,
6695760	6700960	and like safety concern enough from these leaders. And there I'm like, less sure. So I feel like,
6702400	6707360	yeah, it's, it's, it's definitely in like, it's, I think from my perspective, right? It's fair to say
6708320	6711760	they're like pretty reasonable compared to the alternatives that we could have had.
6712800	6718400	But also, it's insufficient in almost all ways, right? Government needs to be involved in this.
6719360	6724480	They cannot externalize the risk. There are many things that they're already doing
6724480	6729600	insufficiently well, I think, where they could have done way better, both with like how they release
6729600	6735280	as well as how they react to, to like problems, as well as, you know, like how they communicate
6735280	6739680	with the public about the risks that they're creating, etc., etc. So yeah, I think there,
6739680	6745920	there's a lot of room for improvement as well. And yeah, I really, I really think that, you know,
6745920	6750960	AI safety is going to be a very, very hard problem. A lot of things have to have to go right for the
6750960	6755840	whole system to go right. And we definitely cannot just trust the labs, despite the best
6755840	6762960	intentions to just solve it all on their own. Yeah, well, hence the, the need for third party
6762960	6767920	auditing and the organization that you're building at Apollo Research, maybe just one last question.
6767920	6773440	A number of people have reached out to me and said, I would like to get involved with red teaming.
6774320	6780960	How can I do that? I wonder if you have any advice for individuals who might just want to
6781680	6787360	do their own projects and, you know, release stuff, you know, just share findings individually with
6787360	6795360	the world, or perhaps and or perhaps, you know, what sort of skills are you in need of, as you're
6795360	6800640	going about building your own organization? I think one thing that is nice about model evaluations
6800720	6805040	and red teaming is you can just kind of start right away. You don't need that much, you know,
6805040	6811520	technical expertise, because it's all in, like almost all of it is in text. And, you know, at
6811520	6817440	least if you, if you want to, to redeem a language model specifically. And yeah, so my recommendation
6817440	6824240	for individuals, first of all, would be to just start, like just engage with a model for, you
6824240	6828640	know, a long, a longer period of time, maybe a day or so and see if whether you find interesting
6828720	6832560	behavior or maybe, you know, maybe there's someone who has already done something, and maybe you
6832560	6837680	can poach a project from them, and, you know, just sort of as a starter thing. And from this,
6837680	6841760	I feel like it kind of just takes a life of its own anyway, you know, as soon as you're hooked
6841760	6846720	on a specific thing that you find interesting, you will, you know, you will really try to find
6847360	6852000	additional ways in which this specific behavior could happen. In the same way, you know, let's,
6852000	6856400	let's take the deception thing, right? We, we started fairly exploratory and wanted to try
6856400	6860080	how far we can get the model to be, to be deceptive. And then at some point, it just
6860080	6864720	took a life of its own where we're like, okay, but like, why really does it do that? Right? Like,
6864720	6869280	okay, we vary this behavior and like this thing and this, this variable in the environment,
6869280	6874000	we vary this thing, we vary all of these other things. And in the end, you can, again,
6874000	6877440	you kind of get like a more holistic and round picture of what's going on. So yeah,
6877440	6882080	I definitely think just start with like a thing you find interesting is definitely the way to go.
6883040	6889120	And like don't overthink, overthink it originally. And then the thing we are specifically looking
6889120	6895120	for, you know, so definitely kind of this mindset of like, oh, I just want to poke around and like
6895120	6899520	really try to understand what's going on in a fairly like scientific manner, right? I also want
6899520	6903680	to make sure that all of the confounders that could potentially explain this behavior have been
6903680	6908480	controlled for, which I think is the hard part in red teaming. So this is definitely something
6908480	6913280	we're looking for. And then just, you know, the more you understand language models and state of
6913280	6918640	the art models, the easier it will become, right? Some behavior might be very easily explainable
6918640	6923680	by sort of problems with RLHF. So if you know how RLHF works and specifically how it was trained,
6924720	6928720	you may probably you will probably understand the red teaming efforts much better.
6929680	6933920	If you know, you know, like if you have a better understanding of how the instruction
6933920	6938640	fine tuning actually works, maybe you will find those. So, you know, so for example,
6938640	6943280	maybe to give it to give like an intuition here, in the in our case, right, as I said earlier,
6943280	6948800	we have the the three HS and then instruction fine tuning, and you can, you know, trade off the
6948800	6954640	different components against each other to find different things. I like to find niches of the
6954640	6959520	model where it acts in ways that we think it shouldn't act, because maybe they weren't covered
6959520	6965600	explicitly or implicitly by by gradient descent. And if you if you sort of have a theoretical
6965600	6969760	framework like this, it suddenly becomes much easier on how to do the red teaming in the first
6969760	6975040	place. So like some theoretical understanding of how the process works is definitely helpful as
6975040	6981120	well for a teaming and also something we're actively looking for. Marius Havan, founder and
6981120	6986720	CEO of Apollo Research. Thank you for being part of the cognitive revolution. Thanks for inviting
6987440	6992320	it is both energizing and enlightening to hear why people listen and learn what they value about
6992320	6999760	the show. So please don't hesitate to reach out via email at TCR at turpentine.co, or you can DM me
6999760	7006320	on the social media platform of your choice. Omniki uses generative AI to enable you to launch
7006320	7011680	hundreds of thousands of ad iterations that actually work customized across all platforms
7011680	7016400	with a click of a button. I believe in Omniki so much that I invested in it. And I recommend
7016400	7024080	you use it too. Use CogGrav to get a 10% discount.
