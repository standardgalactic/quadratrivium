start	end	text
0	2560	We're on this upward growth trajectory.
2560	6120	We have the potential to taking a big chunk of the universe
6120	7640	and doing things with it.
7640	9440	And I'm excited by that potential.
9440	11680	So I want us to keep growing.
11680	15200	And I see how much we've changed to get to where we are.
15200	17600	My book, Age of M, is about brain emulations.
17600	20200	So that's where you take a particular human brain
20200	22560	and you scan it and find spatial chemical detail
22560	26080	where you fill in for each cell a computer model of that cell.
26080	27720	And if you've got good enough models for cells
27720	29760	and a good map of the brain, then basically
29760	31800	the IO of this model should be the same
31800	33760	as the IO of the original brain.
33760	37200	If we can get full human level AI in the next 16 to 90 years
37200	39480	with the progress, then this population decline
39480	41440	won't matter so much because we will basically
41440	43400	have AIs take over most of the jobs
43400	46560	and then that can allow the world economy to keep growing.
46560	48920	Hello and welcome to the Cognitive Revolution,
48920	51880	where we interview visionary researchers, entrepreneurs
51880	53840	and builders working on the frontier
53840	55760	of artificial intelligence.
55760	58560	Each week, we'll explore their revolutionary ideas
58680	60080	and together we'll build a picture
60080	63640	of how AI technology will transform work, life
63640	65800	and society in the coming years.
65800	69400	I'm Nathan LaBenz, joined by my co-host Eric Torenberg.
69400	72280	Hello and welcome back to the Cognitive Revolution.
72280	73920	My guest today is Robin Hansen,
73920	76320	Professor of Economics at George Mason University
76320	78880	and author of the blog, Overcoming Bias,
78880	80560	where Robin has published consistently
80560	83880	on a wide range of topics since 2006
83880	86320	and where Eliezer Yudkowski published early versions
86360	89440	of what has become some of his most influential writing
89440	90280	on AI.
91840	93560	Robin is an undeniable polymath
93560	96920	whose approach to futurism is unusually non-romantic.
98000	100520	Rather than trying to identify value buddies,
100520	103080	Robin aims to apply first principles thinking
103080	106560	to the future and to describe what is likely to happen
106560	108280	without claiming that you should feel
108280	110000	any particular way about it.
111360	113160	I set this conversation up late last year
113160	114640	after my deep dive into the new
114640	117320	Mamba states-based model architecture.
117320	120720	Because Robin's 2016 book, The Age of M,
120720	123280	which analyzes a scenario in which human emulations
123280	125160	can be run on computers,
125160	127280	suddenly seemed a lot more relevant.
128240	131640	My plan originally was to consider how his analysis
131640	134720	from The Age of M would compare to similar analyses
134720	137240	for a hypothetical age of LLMs
137240	139640	or perhaps even an age of SSMs.
140560	142680	In practice, we ended up doing some of that,
142680	144760	but for the most part took a different direction
144760	147080	as it became clear early on in the conversation
147080	151000	that Robin was not buying some of my core premises.
151000	153200	Taking the outside view as he's famous for doing
153200	155840	and noting that AI experts have repeatedly thought
155840	158440	that they were close to AGI in the past,
158440	161000	Robin questions whether this time really is different
161000	163040	and doubts whether we are really close
163040	164960	to transformative AI at all.
166240	168840	This perspective naturally challenged my worldview
168840	170960	and I listened back to this conversation in full
170960	173280	to make sure that I wasn't missing anything important
173280	175000	before writing this introduction.
176360	179200	Ultimately, I do remain quite firmly convinced
179200	181200	that today's AIs are powerful enough
181200	183280	to drive economic transformation.
183280	186200	And I would cite the release of Google's Gemini 1.5,
186200	187960	which happened in just the few short weeks
187960	190280	between recording and publishing this episode
190280	193080	as evidence that progress is not yet slowing down.
194200	195440	Yet at the same time,
195440	197840	Robin did get me thinking more about the disconnect
197840	199360	between feasibility
199360	202920	and actual widespread implementation and automation.
203960	206720	Beyond the question of what AI systems can do,
206720	209640	there are also questions of legal regulation, of course,
209640	211400	and perhaps even more importantly,
211400	214840	just how eager people are to use AI tools in the first place.
216040	218440	When Robin reported that his son's software firm
218440	221480	had recently determined that LLMs were not useful
221480	223480	for routine application development,
223480	225200	I was honestly kind of shocked
225200	226320	because if nothing else,
226320	228080	I'm extremely confident about the degree
228080	230920	to which LLMs accelerate my own programming work.
232000	234560	Since then, though, I have heard a couple of other stories,
234560	236080	which combined with Robbins,
236080	238360	helped me develop, I think, a bit better theory
238360	240160	of what's going on.
240160	244320	First, an AI educator told me that failure to form new habits
244320	248240	is the most common cause of failure with AI in general.
248240	252000	In his courses, he emphasizes hands-on exercises
252000	254200	because he's learned that simple awareness
254240	258360	of AI capabilities does not lead to human behavioral change.
259360	261480	Second, a friend told me that his company
261480	263760	hosted a Microsoft GitHub salesperson
263760	265280	for a lunch hour demo,
265280	267640	and it turned out that one of their own team members
267640	270440	had far more knowledge about GitHub Co-Pilot
270440	271760	than the rep himself did.
272880	274680	If Microsoft sales reps are struggling
274680	276880	to keep up with Co-Pilot's capabilities,
276880	278880	we should perhaps adjust our expectations
278880	280280	for the rest of the economy.
281400	283040	And third, in my own experience,
283040	286200	helping people address process bottlenecks with AI,
286200	288760	I've repeatedly seen how unnatural it can be
288760	290880	for people to break their own work down
290880	292640	into the sort of discrete tasks
292640	295720	that LLMs can handle effectively today.
295720	298040	Most people were never trained to think this way,
298040	299280	and it's going to take time
299280	302080	before it becomes common practice across the economy.
303480	306360	All this means that change may be slower to materialize
306360	309880	than those of us on the frontiers of AI adoption might expect.
309880	311960	And while that does suggest more of an opportunity
311960	315160	and indeed advantage for us in the meantime,
315160	318040	on balance, I do have to view it as a negative sign
318040	321280	about our preparedness and our ability to adapt overall.
322600	323840	Regardless of your views,
323840	325360	and I do suspect that most listeners
325360	328520	will find themselves agreeing with me more than with Robin,
328520	330600	his insights are always thought-provoking,
330600	332760	and I think you'll find it very well worthwhile
332760	334120	to engage with the challenges
334120	336000	that he presents in this conversation.
337440	338960	As always, if you're finding value in the show,
338960	341080	we would appreciate it if you'd share it with friends,
341080	343600	post a review on Apple Podcasts or Spotify,
343600	346120	or just leave a comment on YouTube.
346120	348360	And of course, I always love to hear from listeners,
348360	350560	so please don't hesitate to DM me
350560	353680	on the social media platform of your choice.
353680	355640	Now, I hope you enjoy this conversation
355640	357760	with Professor Robin Hansen.
358880	361760	Robin Hansen, Professor of Economics at George Mason University
361760	364920	and noted polymath, welcome to the cognitive revolution.
364920	366360	Nice to meet you, Nathan.
366360	367280	Let's talk.
367280	368280	I'm excited about this.
368320	371240	So I have followed your work for a long time.
371240	375440	It's super wide-ranging and always very interesting.
375440	377680	People can find your thoughts on just about everything,
377680	380440	I think, over the years on overcoming bias, your blog.
380440	383440	But today, I wanted to revisit what I think
383440	384800	is one of your destined to be,
384800	386360	perhaps one of your most influential works,
386360	388800	which is the book, The Age of M,
388800	393200	which came out in 2016 and Envisions a Future,
393200	397120	which basically amounts to putting humans on machines,
397120	399520	and we can unpack that in more detail,
399520	404520	and then explores that in a ton of different directions.
404920	407600	Where we actually are now as we enter into 2024
407600	410040	is not exactly that, certainly,
410040	411840	but I've come to believe recently
411840	414720	that it's maybe bending back a little bit more toward that,
414720	417040	certainly more than my expectations a year ago.
417040	419320	So I've revisited the book,
419320	421400	and I'm excited to bring a bunch of questions
421400	423760	and kind of compare and contrast your scenario
423760	425640	versus the current scenario
425640	427960	that we seem to be evolving into.
427960	429600	Okay, let's do it.
429600	432480	One big theme of your work always, I think,
432480	436520	is that we live in this strange dream time,
436520	441040	and that our reality as modern humans is quite different
441040	443200	than the reality of those that came before us
443200	444760	and likely those that will come after us
444760	447080	for some pretty fundamental reasons.
447080	448240	Do you wanna just sketch out
448240	449680	your kind of big picture argument
449680	452240	that our times are exceptional
452240	455360	and not likely to go on like this forever?
455400	457000	The first thing to notice is that
457000	458680	we were in a period of very rapid growth,
458680	460400	very rapid change,
460400	462920	which just can't continue for very long
462920	464320	on a cosmological time scale.
464320	467680	10,000 years would be way longer than it could manage,
467680	470400	and therefore we're gonna have to go back
470400	472880	to a period of slower change,
472880	474880	and plausibly then a period of slower change
474880	478640	will be a period where population can grow faster
478640	483640	relative to the growth rate of the economy in the universe,
484240	485600	and therefore we will move back
485600	488280	more toward a Malthusian world
488280	490160	if competition remains,
490160	492120	such as almost all our ancestors were
492120	494040	until a few hundred years ago.
494040	497160	So we're in this unusual period of being rich
498800	501760	per person and in very rapid change,
502880	505920	and also sort of globally integrated.
505920	509400	That is, our distant ancestors were fragmented culturally
509400	510560	across the globe,
510560	513880	and each talk to a small group of people near them,
513880	515720	and our distant descendants will be fragmented
515720	517240	across the universe,
517240	519960	and they won't be able to talk all across the universe
519960	521240	instantaneously.
521240	524840	So future culture and past culture were both very fragmented,
524840	527640	and we were in a period where our entire civilization
527640	530040	can talk rapidly to each other.
530040	532720	The time delay of communication is very small
532720	534440	compared to the doubling time
534440	536320	of our very rapid growth economy.
536320	539200	So we are now an integrated civilization
539200	543120	where rich growing very fast,
543120	545560	and there's a number of consequences being rich,
545560	548120	which is that we don't have to pay
548120	551040	that much attention to functionality.
551040	554160	Those were not pressured to do what it takes to survive
554160	557200	in the way our ancestors and our descendants would be.
557200	560360	So we can indulge our delusions,
560360	563720	or whatever other inclinations we have,
563720	566280	they aren't disciplined very rapidly
566320	569600	by survival and functionality.
569600	571960	That makes us a dream team.
571960	574000	That is, our dreams drive us.
574960	578840	Our abstract thoughts, our vague impressions,
578840	582160	our emotions, our visions.
582160	587160	We do things that are dramatic and exciting and meaningful
588880	593880	in our view, according to this dream time mind we have,
594320	596160	which isn't, again, that disciplined
596160	597520	by functionality, that is,
597520	600920	the mind we inherited from our distant ancestors,
600920	603080	it was functional there, it was disciplined there,
603080	604320	we're in a very different world,
604320	609120	but our mind hasn't changed to be functional in this world.
609120	614120	And so we are expressing this momentum
614120	617400	of what we used to be in this strange new world.
617400	618920	That's the dream time.
618920	621440	So let me just try to rephrase that
621440	622720	or frame it slightly differently
622720	624880	and tell them if you agree with this framing.
624880	627040	I would maybe interpret it as,
627040	629800	we're maybe in a punctuated equilibrium sort of situation
629800	633040	where we're in the transition from one equilibrium
633040	635320	to another, there have probably been
635320	636880	however many of these through history,
636880	638440	not like a huge number, but a decent number,
638440	642000	I think of such phrases as the Cambrian explosion,
642000	644640	perhaps as another dream time.
644640	648920	These moments happen when some external shock
648920	650880	happens to the system, whether that's like an asteroid
650880	652040	that takes out a lot of life,
652040	654920	or human brains come on the scene,
654920	659400	and there's a period in which the normal constraints
659400	662960	are temporarily relaxed, but then in the long term,
662960	664920	there's just like no escaping the logic
664920	665960	of natural selection.
665960	668560	Is that basically the framework?
668560	672000	So your analogy of the Cambrian explosion could be,
672000	673800	we discovered multicellularity,
673800	676480	we discovered being able to make large animals,
676480	678000	and that was happened at a moment,
678000	680000	there was the moment of multicellularity,
680000	682680	and then evolution took time to adapt
682680	685040	to that new opportunity,
685040	688080	and the Cambrian explosion is the period of adaptation,
688080	689640	then after the Cambrian explosion,
689640	691960	we've adapted to that new opportunity,
691960	693440	and then we're more in a stasis,
693440	695880	and then you're imagining this period of adaptation
695880	696880	to a sudden change.
697760	702760	But for humans today, we keep having sudden changes,
703200	705040	and they keep coming fast,
705040	708040	and so there wasn't this one thing that happened
708040	709720	300 years ago or 10,000 years ago
709720	711120	that we're slowly adapting to.
711120	713560	We keep having more big changes
713560	715720	that keep changing the landscape
715720	717720	of what it is to adapt to,
717720	722240	so we won't see this slow adaptation to the new thing
722240	724000	until we get a stable new thing,
724000	725080	which we haven't gotten yet.
725080	726840	We, things keep changing.
726840	728800	I wanna maybe circle back in a minute to
728800	731040	what would be the conditions
731040	732360	under which things would restabilize.
732360	735040	I think I guess the M scenario is one of them,
735040	737720	but there may be others that might even be
737720	739160	more imminent at this point.
739840	740680	Before doing that,
740680	742400	I just wanted to touch on another big theme of your work,
742400	745240	which is, and I really appreciate how you introduced
745240	749400	the book this way with the idea that
749400	751240	I'm just trying to figure out
751240	753800	what is likely to happen in this scenario.
753800	755680	I'm not telling you you should like it.
755680	757200	I'm not telling you you should dislike it.
757200	758480	I'm not trying to judge it.
758480	761920	I'm just trying to extrapolate from a scenario
761920	764920	using the tools of science and social science
764920	767200	to try to figure out what might happen.
767200	769520	I love that, and I try to do something similar
769520	772240	with this show around understanding AI.
772240	775600	I think there's so much emotional valence
775600	777720	brought to so many parts of the discussion,
777720	781440	and I always say, we need to first figure out what is,
781440	783120	and even in the current moment,
783120	786000	what capabilities exist, what can be done,
786000	788400	what is still out of reach of current systems
788400	790200	before we can really get serious
790200	793440	about what ought to be done about it.
794400	796160	I guess I'd invite you to add
796160	797360	any additional perspective to that,
797360	798400	and then I'm also curious,
798400	801560	like, I think that's very admirable,
801560	804560	but could you give us a little window
804560	807120	into your own kind of biases or preferences?
807120	809560	Like, what sort of world do you think
809560	810880	we should be striving for,
810880	812040	or do you think that's just so futile
812040	814600	to even attempt to influence against these,
814600	817800	you know, grand constraints that it doesn't matter?
817800	819760	Hey, we'll continue our interview in a moment
819760	822080	after a word from our sponsors.
822080	824640	The Brave Search API brings affordable developer access
824640	826160	to the Brave Search Index,
826160	827520	an independent index of the web
827520	829840	with over 20 billion web pages.
829840	832640	So what makes the Brave Search Index stand out?
832640	836360	One, it's entirely independent and built from scratch.
836360	839920	That means no big tech biases or extortionate prices.
839920	843760	Two, it's built on real page visits from actual humans,
843760	845640	collected anonymously, of course,
845640	848200	which filters out tons of junk data.
848200	850000	And three, the index is refreshed
850000	852080	with tens of millions of pages daily,
852080	855400	so it always has accurate up-to-date information.
855400	858400	The Brave Search API can be used to assemble a dataset
858400	860000	to train your AI models
860000	861720	and help with retrieval augmentation
861720	863360	at the time of inference,
863360	864640	all while remaining affordable
864640	866880	with developer-first pricing.
866880	869720	Integrating the Brave Search API into your workflow
869720	871760	translates to more ethical data sourcing
871760	874520	and more human-representative datasets.
874520	876600	Try the Brave Search API for free
876600	880800	for up to 2,000 queries per month at brave.com slash API.
886400	889520	Pretty much all big, grand talk
890800	895280	is mostly oriented around people sharing values.
896280	898720	That's what people want to do when they talk big politics,
898720	901800	when they talk world politics or world events,
901800	904520	when they talk the future.
904520	908640	People want to jump quickly to, do I share your values?
908640	909680	Here's my values.
909680	910520	What are your values?
910520	911440	Do we agree on values?
911440	912720	Are we value buddies?
913720	915920	And people are so eager to get to that
915920	918120	that they are willing to skip over
918120	921040	the analysis of the details, say,
921040	922240	if they want to talk about, I don't know,
922240	924000	the war in Ukraine.
924000	925840	People want to go, which side are you on?
925840	928800	And who, you know, do we have the right values
928800	930440	and then they don't care to talk about like,
930440	932600	who has how much armaments that will run out soon
932600	934520	or who can afford what or what they,
934520	935960	you know, all those details of the war.
935960	937400	They don't want to go there.
937400	940560	They just want to go to the values and agree about it.
941560	943200	And that happens in the future too,
943200	944040	futurism too.
944040	945120	People just want to jump to the values.
945120	947880	So for the purposes people have,
947880	949160	they're doing roughly the right thing.
949160	950800	They don't really care about the world
950800	952200	and they don't really care about the future.
952200	955640	What they care about is finding value buddies
955640	959280	or if you find a value conflict, having a value war.
959280	961440	That's what people just want to do.
961440	965080	And so if you actually want to figure out the world
965080	967840	or national politics or national policy
967840	969840	or you want to figure out the future,
969920	971400	you really have to resist that
971400	975840	and you have to try to pause and, you know,
975840	978560	go through an analysis first, a neutral analysis
978560	980680	of what the options are, what the situation is.
980680	984680	I mean, I am afraid literally that if I express many values
984680	986840	that the discussion will just go there
986840	988600	and you'll never talk about anything else.
988600	991640	And that's why I resist talking about that.
991640	994040	But I think, you know, my simplest value
994040	996760	with respect to the future is I really like the fact
996760	1001280	that humanity has grown and achieved vast things
1001280	1002640	compared to where it started.
1002640	1005160	We're on this upward growth trajectory.
1005160	1008440	We have the potential to taking a big chunk of the universe
1009400	1010880	and doing things with it.
1010880	1012720	And I'm excited by that potential.
1012720	1016720	So my first cut is I want us to keep growing.
1016720	1020680	And I see how much we've changed to get to where we are.
1020680	1023600	And I can see that had people from a million years ago
1023600	1026920	insisted that their values be maintained
1026920	1030160	and that the world be familiar and comfortable to them.
1030160	1032160	If they've been able to enforce that,
1032160	1035080	we would not have gotten where we are now.
1035080	1036760	That would have prevented a lot of change.
1036760	1039960	So I kind of see that if I want us to get big and grand,
1039960	1042840	I'm gonna have to give a lot on
1042840	1046560	how similar the future is to me and my world.
1046560	1048520	I'm gonna have to compromise a lot on that.
1048520	1050160	I just don't see any way around that.
1050160	1052560	So I get it that if you want the future
1052560	1053840	to be really comfortable for you
1053840	1056320	and to share a lot of your values and your styles,
1056320	1059320	you're gonna have to prevent it from changing.
1059320	1061440	And you may have a shot at that.
1061440	1063840	I would not like that, but you might.
1063840	1066400	So again, even as part of the value framework,
1066400	1067760	even when I talk values with you,
1067760	1070320	I want to be clear to distinguish
1070320	1071840	my value talk from the factual talk.
1071840	1073880	I'm gonna be happy to tell you
1073880	1076640	what it would take for you to get your values,
1076640	1078600	even if they aren't mine.
1078600	1082240	So maybe we should talk about the facts of LLMs.
1082240	1085680	You wanna go there in terms of comparing Ms and LLMs, right?
1085680	1087280	So first of all, our audience,
1087280	1089120	we should say for our audience,
1089120	1092240	my book Age of M is about brain emulations.
1092240	1094840	So that's where you take a particular human brain
1094840	1097720	and you scan it and find spatial chemical detail
1097720	1099720	to figure out which cells are where,
1099720	1102760	connected to other cells through what synapses.
1102760	1104880	You make a map of that,
1104880	1108960	and then you make a computer model that matches that map
1109960	1111840	where you fill in for each cell,
1111840	1113560	a computer model of that cell.
1113560	1115160	And if you've got good enough models for cells
1115160	1116360	and a good map of the brain,
1116360	1118640	then basically the IO of this model
1118640	1120680	should be the same as the IO of the original brain,
1120680	1121680	which means you could hook it up
1121680	1124400	with artificial eyes, ears, hands, mouth.
1124400	1125880	And then it would behave the same
1125880	1129440	as the original human would in the same situation,
1129440	1132480	in which case you can use these as substitutes for humans
1132480	1134080	throughout the entire economy.
1134080	1136200	And then my exercise of the Age of M book
1136240	1139520	was to figure out what that word looks like.
1139520	1143520	And a primary purpose was to actually be able to show
1143520	1145360	that it's possible to do that sort of thing.
1145360	1148280	It's possible to take a specific technical assumption
1148280	1149880	and work out a lot of consequences.
1149880	1153680	And many people have said they didn't want so many details.
1153680	1155280	They'd rather have fiction or something else,
1155280	1157440	but I was trying to prove how much I could say.
1157440	1160160	And I hope you'll admit, I proved I could say a lot.
1161400	1165200	And that almost no other futurist work does that.
1165240	1167160	And so I'm trying to inspire other futurists
1167160	1169000	to get into that level of detail,
1169000	1170520	to try to take some assumptions
1170520	1171520	and work out a lot of consequences.
1171520	1173800	So that's my book, The Age of M.
1173800	1176320	You'd like us to compare that
1176320	1178560	to current large-language models
1179680	1181160	and to think about what we can say
1181160	1183160	about the future of large-language models.
1183160	1187280	So in my mind, the first thing to say there is,
1187280	1191560	well, an M is a full human substitute.
1191560	1194300	It can do everything a human can do, basically.
1195560	1197840	A large-language model is not that yet.
1199440	1202280	So a key question here would be,
1202280	1205000	how far are we going to go in trying to imagine
1205000	1207120	a descendant of a large-language model
1207120	1210320	that is more capable of substituting
1210320	1212700	for humans across a wide range of contexts?
1212700	1214380	We stick with current large-language models.
1214380	1215840	They're really only useful
1215840	1218640	in a rather limited range of contexts.
1218640	1221680	And so if you're gonna do forecasting of them,
1221680	1223340	it's more like forecasting the future
1223340	1225760	with a microwave oven or something.
1225760	1228700	You think about, well, where can you use a microwave oven
1228700	1229820	and how much will it cost
1229820	1233420	and what other heating methods will it displace
1234420	1237140	and what sort of inputs would be a compliment to that?
1237140	1239220	It would be more of a small-scale,
1239220	1241900	future forecasting exercise.
1241900	1245340	Whereas The Age of M was purposely this very grand exercise
1245340	1248720	because the M's actually change everything.
1248720	1251380	Whereas most futurism, like if you're trying to analyze
1251420	1253500	consequences of microwave oven,
1253500	1255220	you have a much more limited scope
1255220	1258420	because in fact, it'll have a limited impact.
1258420	1261180	So that would be the question I have for you first,
1261180	1264860	which is, are we gonna talk about the implications
1264860	1268180	of something close to the current large-language models?
1268180	1271980	Are we gonna try to imagine some generalized version
1271980	1274860	of them that has much wider capabilities?
1274860	1276180	Yeah, very good question.
1276180	1278580	I think maybe two different levels of this
1278580	1280740	would be instructive.
1280740	1282180	One of the key things that jumps out
1282180	1284540	and I think a lot of stuff flows from
1284540	1289540	is the assumption that M's can be copied cheaply,
1290300	1293940	paused and stored indefinitely cheaply,
1293940	1296620	but not understood very well
1296620	1298820	in terms of their internal mechanism.
1298820	1300980	Very much like this similar understanding
1300980	1302140	to what we have of the brain
1302140	1303940	where we can kind of poke and prod at it a little bit,
1303940	1305380	but we really don't have a deep understanding
1305380	1306220	of how it works.
1306220	1309420	We can't do like very localized optimizations,
1309420	1311820	but we do have this like radical departure
1311820	1312700	from the status quo,
1312700	1315420	which is you can infinitely clone them,
1315420	1318780	you can infinitely freeze and store them.
1318780	1320860	And so this creates like all sorts of elasticities
1320860	1324100	that just don't exist in the current environment.
1324100	1325980	So a number of those features are gonna be general
1325980	1329660	that anything that can be represented as computer files
1329660	1331460	and run on a computer.
1332300	1334500	So any form of artificial intelligence
1334500	1336500	will be some of the sort in general
1336500	1338780	that you could have a digital representation
1338780	1343620	of archive it, make a copy of it, pause it,
1343620	1344780	run it faster or slower,
1344780	1348340	that's gonna be just generically true of any kind of AI,
1348340	1349260	including M's.
1350740	1354420	The ability to sort of modify it usefully,
1354420	1357460	I mean, yes, with human brains initially,
1357460	1358300	they're just a big mess,
1358300	1359140	you don't understand them,
1359140	1361380	but honestly, most legacy software systems
1361380	1362220	are pretty similar.
1363300	1365940	So today, large legacy software systems,
1365940	1368380	you mostly have to take them as they are.
1368420	1370980	You can only make modest modifications to them.
1371900	1373980	That's close to what I'm assuming for M's.
1373980	1376820	So I'm actually not assuming that they are that different
1376820	1379700	from large legacy software systems.
1379700	1381460	They're just a big mess
1382420	1384940	that even though you could go look at any one piece
1384940	1386020	and maybe understand it,
1386020	1387820	that doesn't really help you usefully
1387820	1390260	in modifying the entire thing.
1390260	1393780	You basically have to take the whole thing as a unit
1393780	1397380	and can only make some minor changes.
1397380	1398660	But you can copy the whole thing,
1398660	1400260	you can run it faster or slow,
1400260	1401300	you can move it at speed,
1401300	1403260	transfer at the speed of light around the earth
1403260	1405060	or through the universe.
1405060	1408300	Those things are true of pretty much any AI
1408300	1410740	that could be represented as a computer file,
1410740	1411860	run on a computer.
1411860	1415300	Yeah, I think these dimensions are a really useful way
1415300	1417020	to break this down.
1417020	1420100	I took some inspiration from you in a presentation
1420100	1422500	that I created called the AI Scouting Report,
1422500	1425380	where I have the tail of the cognitive tape
1425420	1428300	that compares human strengths and weaknesses
1428300	1430500	to LLM strengths and weaknesses.
1430500	1432980	And I think for the purposes of this discussion,
1432980	1435740	maybe we might even have like four different kind of things
1435740	1436580	to consider.
1436580	1438900	One is humans, second would be M's,
1438900	1443660	third is let's say transformer language models
1443660	1445580	of the general class that we have today.
1445580	1449140	Although I think we can predictably expect at a minimum
1449140	1452740	that they will continue to have longer context windows
1452740	1455780	and have generally more pre-training
1455780	1458300	and generally more capability,
1458300	1461820	at least within a certain range.
1461820	1463700	And then the fourth one that I'm really interested in
1463700	1467020	and has been kind of an obsession for me recently
1467020	1471340	is the new state space model paradigm,
1471340	1475540	which actually has some things now in common again
1475540	1476740	with the humans and the M's
1476740	1479580	that the transformer models lack.
1479580	1481820	The state space models,
1481860	1483980	this has been, of course, in a line of research
1483980	1485180	that's been going on for a couple of years,
1485180	1487740	kind of in parallel with transformers.
1487740	1490020	Transformers have taken up the vast majority
1490020	1493340	of the energy in the public focus
1493340	1495460	because they have been the highest performing
1495460	1497260	over the last couple of years.
1497260	1502260	But that has maybe just changed with a couple of recent papers,
1503500	1505580	most notably one called Mamba,
1505580	1510140	that basically shows parity, rough parity
1510140	1512340	with the transformer on kind of your standard
1512340	1514300	language modeling tasks,
1514300	1517620	but does have like a totally different architecture
1517620	1521660	that I think opens up like some notably different strengths
1521660	1526340	and weaknesses, whereas the transformer
1526340	1528180	really just has the weights
1528180	1531100	and then the sort of next token prediction,
1531100	1533540	the state space model has this additional concept
1533540	1538460	of the state, which is, and I recall from the book,
1538460	1541620	you sort of say, taking an information processing lens
1541620	1545900	to the human or where you spend more of your focuses
1545900	1549820	on the M, you have the current state
1549820	1554340	plus some new input information, sensory or whatever,
1554340	1558340	and then that propagates into some action,
1558340	1562020	some output and a new internal state.
1562020	1563180	And that I think is really the heart
1563180	1565300	of what the new state space models do
1565300	1567540	is that they add that additional component
1567540	1569580	where they have not only the weights,
1569580	1571900	like a transformer has static weights,
1571900	1574140	but they also have this state,
1574140	1578100	which is of a fixed size, evolves through time,
1578100	1579780	and is something that gets output
1579780	1581980	at each kind of inference step
1581980	1583780	so that there is this internal state
1583780	1586220	that propagates through time
1586220	1589060	and can kind of change and have long history.
1589900	1594900	I think it is likely to bring about
1594940	1599940	a much more integrated medium and long-term memory
1600020	1601940	than the transformers have
1601940	1606940	and create more sort of long episode conditioning
1607580	1611340	where these models I think will be more amenable
1611340	1614860	to like employee onboarding style training,
1614860	1616900	which is something also that the M's have
1616900	1619020	in your scenario, right?
1619020	1624020	You can kind of train a base M to be an employee for you,
1624420	1626740	you can even put it in that mental,
1626740	1627740	get it to that mental state
1627740	1630340	where it's like really excited and ready to work,
1630340	1632500	and then you can freeze it, store it,
1632500	1634060	boot it up when necessary,
1634060	1636580	boot it up end times as necessary.
1636580	1640580	The transformers don't really have that same feature right now,
1640580	1645580	they're just kind of their monolithic base form at all times,
1645580	1650380	but the state-state models start to add some of that back.
1650380	1652500	Obviously, it's not gonna be one-to-one
1652500	1655060	with the humans or the M's.
1655060	1658700	Here's gonna be my problem with that number four.
1658700	1661300	If I look at sort of the history of AI
1661300	1663260	over the history of computers
1663260	1665500	and even the history of automation before that,
1666620	1669380	we see this history where a really wide range
1669380	1671700	of approaches have been tried,
1671700	1673580	a really wide range of paradigms
1673580	1677500	and concepts and structures have been introduced.
1677500	1680580	And over time, we've found ways in some sense
1680660	1683420	to subsume prior structures within new ones,
1684500	1687580	but we've just gone through a lot of them.
1688580	1691180	And there's been this tendency, unfortunately,
1691180	1693980	that when people reach the next new paradigm,
1693980	1697500	the next new structure, they get really excited by it
1697500	1700580	and they consistently say, are we almost done?
1700580	1702620	They said that centuries ago,
1702620	1704420	they said that half a century ago,
1704420	1708660	every new decade, every new kind of approach
1708660	1710100	that comes along,
1710900	1712780	there's basically typically some demo,
1712780	1715260	some new capability that this new system can do
1715260	1717900	that none of the prior systems are able to do.
1718820	1721940	And it's exciting and it's shocking even
1721940	1726940	and exciting, but people consistently say,
1726940	1729260	so we must be almost done, right?
1729260	1732100	Like, surely this is enough to do everything
1732100	1734220	and pretty soon humans will be displaced
1734220	1736620	by automation based on this new approach.
1736620	1738780	And that just happens over and over again.
1739660	1742220	And so we've had enough of those that I got to say,
1742220	1744660	the chance that the next exciting new paradigm
1744660	1748660	is the last one we'll need is a prior pretty low.
1749660	1751540	We've had this long road to go
1751540	1754540	and we still have a long way to go ahead of us.
1754540	1756380	And therefore, it's unlikely
1756380	1759420	that the next new thing is the last thing.
1759420	1762180	So that's my stance, I would think, okay,
1762180	1763340	I can talk to you about LLMS
1763340	1765420	because they're the latest thing.
1765420	1766580	We can talk about LLMS,
1766580	1768220	they're the latest thing.
1768220	1770020	We can talk about what new things they can do
1770020	1772260	and what exciting options
1772260	1774180	that generates in the near future.
1775100	1776580	And then we can ask, well,
1776580	1779340	what's the chance it's the last thing we'll need?
1779340	1781060	Or that the next one is the last thing we need.
1781060	1784220	And so one way to cash that out is to ask,
1784220	1785540	what do we think the chances are
1785540	1788780	that within a decade or even two,
1788780	1791300	basically all human jobs will be replaced
1791300	1794940	by machines based on this new approach.
1794940	1797940	And most of the forecasting that's done out there
1797940	1800300	is excited about near-term progress in a lot of ways.
1800300	1801540	But when you ask the question,
1801540	1803780	when will most jobs be replaced?
1803780	1805780	They give you forecasts that are way out there
1805780	1809180	because they think, no, we're not close to that.
1809180	1810700	And I don't think we're close to that.
1810700	1814140	So then the question is,
1814140	1815700	now we could say, what will happen
1815700	1817380	when we eventually get to the point
1817380	1819340	where AI is you're good enough to do everything?
1819340	1820580	And we don't know what that approaches,
1820580	1822180	but we can still talk about that point
1822540	1825300	and what's likely to what the transition rate would be
1825300	1827100	and the transition scenario
1827100	1830580	and who would get rich and who would be unhappy
1830580	1833100	and all the different things we could talk about there.
1833100	1835780	But now we're talking about whatever approach
1835780	1840100	eventually gets us past the being able to have
1840100	1842460	to do pretty much all human tasks,
1842460	1843980	which is not where we are now,
1843980	1845620	or we can talk about where we are now
1845620	1846900	and what these things can do
1847940	1851660	and what exciting things might happen in the next decade.
1852380	1854340	Hey, we'll continue our interview in a moment
1854340	1856100	after a word from our sponsors.
1856100	1857140	If you're a startup founder
1857140	1858860	or executive running a growing business,
1858860	1861460	you know that as you scale, your systems break down
1861460	1863340	and the cracks start to show.
1863340	1864500	If this resonates with you,
1864500	1866100	there are three numbers you need to know,
1866100	1870340	36,000, 25 and one, 36,000.
1870340	1871380	That's the number of businesses
1871380	1873220	which have upgraded to NetSuite by Oracle.
1873220	1875300	NetSuite is the number one cloud financial system,
1875300	1877300	streamlined accounting, financial management,
1877300	1880380	inventory, HR and more, 25.
1880420	1882220	NetSuite turns 25 this year.
1882220	1884980	That's 25 years of helping businesses do more with less,
1884980	1888620	close their books in days, not weeks and drive down costs.
1888620	1890740	One, because your business is one of a kind,
1890740	1893300	so you get a customized solution for all your KPIs
1893300	1895780	in one efficient system with one source of truth.
1895780	1899060	Manage risk, get reliable forecasts and improve margins,
1899060	1901260	everything you need all in one place.
1901260	1904620	Right now, download NetSuite's popular KPI checklist,
1904620	1906980	designed to give you consistently excellent performance,
1906980	1910300	absolutely free and netsuite.com slash cognitive.
1910300	1912420	That's netsuite.com slash cognitive
1912420	1914260	to get your own KPI checklist,
1914260	1916140	netsuite.com slash cognitive.
1917300	1920660	Omniki uses generative AI to enable you to launch
1920660	1924020	hundreds of thousands of ad iterations that actually work,
1924020	1927440	customized across all platforms with a click of a button.
1927440	1929980	I believe in Omniki so much that I invested in it
1929980	1932300	and I recommend you use it too.
1932300	1934820	Use CogGrav to get a 10% discount.
1934820	1936380	Well, I'm tempted by all of those options.
1936380	1939940	So maybe for starters, I would be interested to hear
1939940	1944940	how you would develop a cognitive tail of the tape
1945540	1948980	between humans and M's by presumption
1948980	1950840	have kind of the same cognitive abilities,
1950840	1953900	but these kind of different external properties
1953900	1955660	of copyability and so on.
1955660	1958380	The large language model today,
1958380	1961500	transformer, remarkably simple architecture,
1961500	1964020	when you really just look at the wiring diagram,
1964020	1968420	it's way simpler than the human brain is.
1968420	1972540	And not shockingly, it can only do certain things
1972540	1975020	that there's like really important traits
1975020	1980020	that the human brain has that the language models don't have.
1980580	1984020	I identified one of those as kind of integrated,
1984020	1988220	ever-evolving, medium and long-term memory.
1988220	1990180	I wonder what else you would kind of flag there.
1990180	1993140	I don't know if you have a taxonomy of what are the kind
1993140	1996180	of core competencies of humans that you could then say,
1996220	2000260	oh, and here's the things that language models currently lack.
2000260	2001940	I'm trying to develop something like this in general
2001940	2006260	because it does seem to me that the large language models
2006260	2008980	have hit not genius human level,
2008980	2011700	but like closing in on expert human level
2011700	2015220	at some very important, dare I say,
2015220	2019420	even like core aspect of information processing, right?
2019420	2022220	Like they can do things that I would say
2022220	2025820	are qualitatively different than any earlier AI system
2025820	2027140	could do.
2027140	2029100	It certainly seems like we're getting close,
2029100	2030420	whatever the last step is,
2030420	2032500	we're definitely closer to it than we used to be.
2032500	2036100	But just notice that phrase you just gave was true
2036100	2038620	or most of all the previous ones as well.
2038620	2039900	They could also do a thing
2039900	2042760	that the previous ones before it couldn't do.
2042760	2044140	It's always been exciting.
2044140	2046140	We've found a new fundamental capability
2046140	2049860	that each new paradigm structure approach
2049860	2052180	has been of this sort
2052180	2054540	that it was allowed the system to do fundamental things
2054740	2055820	that it couldn't do before
2055820	2059060	that seemed to be near the core of what it was to think.
2060020	2062100	So there's apparently a lot of things near the core
2062100	2063260	of what it is to think.
2063260	2065020	That's the key thing to realize.
2065020	2066780	What it is to think is a big thing.
2066780	2068700	There's a lot of things in there.
2068700	2069540	Well, let's list some.
2069540	2072180	I can't come up with that many honestly.
2072180	2075740	Like I would love to hear how many can you name
2075740	2076600	I have all day.
2076600	2078980	So could you begin to break down
2078980	2082100	what it is to think into key components?
2082100	2085860	I was an AI researcher from 84 to 93.
2085860	2089740	That was a full time at NASA and then Lockheed.
2089740	2091620	And certainly at that time,
2091620	2094540	I understood the range of approaches people had
2094540	2097540	and could talk about the kinds of things systems
2097540	2101420	then could do or not do and expert terms relating
2101420	2106020	to the then current tasks and issues.
2106020	2108260	I am not up to date at the moment
2108260	2110620	on the full range of AI approaches.
2110900	2114860	I don't wanna pretend to be an expert on that.
2114860	2117780	But I have listened to experts
2117780	2122140	and the experts I hear basically consistently say,
2122140	2123540	this is exciting, this is great,
2123540	2126860	but we're not close to being able to do all the other things
2126860	2129340	and they would be much better than I am making a list of that
2129340	2131300	and I feel like they should make the list, not me.
2131300	2133580	I mean, as a polymath you call me,
2133580	2136820	I wanna be very careful to know when I'm an expert
2136820	2138420	on something and when I'm not.
2138420	2140740	And I wanna defer to other people on areas
2140740	2143020	where I can find people who know more than I.
2143020	2145620	And when I think I'm near the state of the art,
2145620	2147100	as good as anyone on a topic,
2147100	2150380	then I will feel more free to generate my own thoughts
2150380	2152580	and think they're worth contributing.
2152580	2155540	Fair, certainly, I think most people
2155540	2160220	where I think you do still bring something very differentiated
2160220	2165220	to the discussion is just the sort of willingness
2166220	2170180	to stare reality in the face or at least try to.
2170180	2172180	The simplest thing is if I start talking
2172180	2173620	to an out large language model,
2173620	2175380	there's a whole bunch of things I can ask it to do
2175380	2176740	that it just can't do.
2177900	2179540	I'm not so sure how to organize that
2179540	2181380	in terms of the large major categories,
2181380	2184820	but it's really obvious that there's a certain kind of thinking
2184820	2188060	it can do and a bunch of other kind of thinking it can't do.
2188060	2190660	And I don't know exactly why it can't do them,
2190660	2193060	but I'm talking to you, there's a bunch of things
2193060	2194620	I could ask you to do in this conversation
2195180	2197340	that you would probably do a decent job of them.
2197340	2199540	And then if I were talking to the large language model,
2199540	2201780	it just couldn't do those things.
2201780	2203300	So it's just really obvious to me
2203300	2204940	that this has a limited capability.
2204940	2207180	It's really impressive compared to what you might have expected
2207180	2209100	five or 10 years ago, it's, wow,
2209100	2211380	I never would have thought that would be feasible this soon,
2211380	2215180	but you just try asking it a bunch of other things
2215180	2217020	and it just can't do them, right?
2217020	2222020	Yeah, I mean, I think that in my view,
2222780	2227780	a lot of those things are kind of overemphasized
2228940	2232180	relative to what maybe really matters.
2232180	2235740	You see a lot of things online where people,
2235740	2237540	and there's different categories of this,
2237540	2239460	some of the things you'll see online
2239460	2243260	are literally people just using non frontier models
2243260	2245580	and kind of confusing, muddying the water.
2245580	2247340	So always watch out for that.
2247340	2251140	I have a longstanding practice of,
2251180	2252580	first thing I do when I see somebody say,
2252580	2255500	GPT-4 can't do something is try it myself.
2255500	2258940	And I would honestly say like two thirds of the time,
2258940	2260660	it's just straight up misinformation
2260660	2262260	and it in fact, like can do it.
2262260	2265180	But there's still the one third of the time that matters.
2265180	2267260	They're not very adversarially robust.
2267260	2268300	They're easy to trick,
2268300	2272140	they're easy to sort of get on the wrong track.
2272140	2276580	And then they seem to get kind of stuck in a mode
2276580	2278020	is a good term for it, I think,
2278020	2281180	where once they're kind of on a certain,
2281180	2283580	this is kind of how they can often get jailbroken.
2283580	2284780	If you can get them to say like,
2284780	2286940	okay, I'll be happy to help you with that,
2286940	2290020	then they'll go on and do whatever you asked
2290020	2292380	because they've already kind of got into that mode.
2292380	2293700	Yeah, I'm much less worried about them
2293700	2295100	doing things you don't want them to do
2295100	2298020	than being able to get them to do things at all.
2298020	2301620	That as humans can be made to do all sorts of things,
2301620	2302820	you might not want them to do that.
2302820	2303820	We survive that.
2304780	2306580	I mean, to me, the main thing is,
2306580	2308700	if you imagine, you know,
2308700	2311680	treating a large language model as a new employee
2311680	2313460	in some workplace where you're trying to show them
2313460	2316300	how to do something and get them to do it instead of you,
2316300	2318660	that's the main thing that will be economically valuable
2318660	2319500	in the world.
2319500	2321220	That is, when you have a thing like that
2321220	2324380	that can be introduced into a place,
2324380	2326340	trained roughly and said, watch how I do this,
2326340	2328980	you try to do it now, et cetera,
2328980	2331420	then that will be the thing that, you know,
2331460	2333860	makes an enormous difference in the economy
2333860	2337500	because that's how we get people to do things, right?
2337500	2339820	So if that I think is, in a sense,
2339820	2342500	the fundamental main task in the economy,
2342500	2345420	which is a bunch of people are doing something,
2345420	2347300	you have a new thing and you say,
2347300	2349660	would you Kim watch us and ask us questions
2349660	2352020	and we'll ask you questions and like figure out
2352020	2355300	how to help us and be part of what we're doing.
2355300	2357260	That is the fundamental problem in the economy.
2357260	2360060	So that in some sense is the fundamental task
2360060	2363020	that any AI has to be held up to.
2363020	2364820	I mean, in the past, of course,
2364820	2366300	we don't even bother to have a conversation
2366300	2368380	to show you how to do, we actually say,
2368380	2369860	well, let's make a machine to do this thing
2369860	2371340	and then we design a machine to do this thing
2371340	2372900	and then we train it up to do this thing
2372900	2374500	all with the idea of the whole thing,
2374500	2377460	having in mind the thing we're gonna have to do.
2377460	2380460	That's how AI has been usually in the economy so far.
2380460	2381620	But now if you're imagining a thing
2381620	2384020	that could just be trained to do a new job,
2384020	2385220	well, that would be great.
2386420	2389260	Sure, then we won't have to design the AI ahead of time
2389260	2390980	for the particular task,
2390980	2392940	but you'll have to have a thing that's up to that
2392940	2394300	and large language models today
2394300	2396860	are just clearly not up to that.
2396860	2398180	You can't say, I'm about to train you
2398180	2399780	how to do the following thing, pay attention,
2399780	2401940	I just did this, now would you do it?
2401940	2403940	Well, you can do that quite a bit, right?
2403940	2406780	I mean, that was the main kind of finding in GPT-3
2406780	2409300	was, I'm not sure if I have this verbatim,
2409300	2411780	but the title of that paper was large language models
2411780	2413940	are few shot learners.
2413940	2417220	And the big kind of breakthrough observation there,
2417220	2418580	which I don't think they designed,
2419580	2421980	there's a whole quagmire of what should count
2421980	2423540	as emergent or not emergent,
2423540	2426700	but my understanding is they didn't specifically train
2426700	2430740	for this few shot imitation capability,
2430740	2433660	but they nevertheless got to the point where
2433660	2438020	at runtime today, you can give a few examples
2438020	2438860	of what you want.
2438860	2440500	And in fact, that is like a best practice
2440500	2443140	that open AI and anthropic recommend
2443140	2445260	for how to get the most from their systems.
2445260	2447580	They'll say, some things are hard,
2447580	2449380	they also have now trained them to follow instructions,
2449380	2452180	just verbatim or explicitly,
2452180	2454940	but they will still say that,
2454940	2457780	some things are better shown by example
2457780	2461660	than described in terms of what to do.
2461660	2465620	So do that, and you'll get like a lot better performance.
2465620	2469540	It seems to me that there is on that kind of watch,
2469540	2471820	watch it to borrow from medicine,
2471820	2474180	watch one, do one, teach one,
2474180	2477820	it seems like we're on the do one step,
2477820	2482580	and that does seem to be a pretty qualitative threshold
2482580	2483420	that has been passed.
2483420	2486180	Now, they obviously can continue to get better at that.
2486180	2488020	Right, but it's the range of things they can do
2488020	2489100	that's the question.
2489100	2491020	Yes, it's great that they can,
2491020	2493420	you can say, here's some examples, give me another one,
2493420	2496340	but the range of things you can do that for is limited.
2496340	2498340	Most people in most jobs,
2498340	2501420	they couldn't have large language model swap in
2501420	2503980	for many of their main tasks that way.
2504940	2506500	But there are some and that's exciting,
2506500	2508980	and I hope to see people develop that and improve it.
2508980	2511380	But again, the key question is how close are we
2511380	2514740	to the end of this long path we've been on for a while?
2514740	2516460	Yeah, I guess I think about it a little bit differently
2516460	2519060	in terms of rather than thinking about the end of the path,
2519060	2523780	I think of how close are we to key thresholds
2523780	2528300	that will bring in qualitatively different dynamics
2528300	2532420	relative to the current situation.
2532620	2536020	So one threshold that I think has recently been passed
2536020	2538180	and in a pretty striking way that this is,
2538180	2540900	should get more discussion than it does in my view
2540900	2543580	is Google DeepMind just put out a paper
2543580	2548580	not long ago where they showed basically a two to one
2549300	2551740	advantage for a large language model
2551740	2556300	in medical diagnosis versus human doctors.
2556300	2558540	And then of course they also compared to human plus AI
2558540	2559820	and that was in the middle.
2559820	2562300	So on these cases that they lined up
2562300	2565420	in the scenario is like you're chatting with your doctor,
2565420	2568820	60% accuracy from the language model,
2568820	2572020	30% accuracy from the human.
2572020	2575620	I was an AI from 83 to 94.
2576940	2580220	And at the beginning, one of the reasons I came into AI
2580220	2582940	was there were these big journal articles
2582940	2585220	and national media coverage about studies
2585220	2587940	where they showed that the best AI of the time
2587940	2589820	which they called expert systems
2589820	2594020	were able to do human level medical diagnosis.
2594020	2597380	This was in the early 1980s, right?
2597380	2599040	We're talking 40 years ago.
2599920	2601940	And obviously the computer capacity
2601940	2603180	is vastly larger than that.
2603180	2606220	So either they were lying back then
2606220	2608620	and messing with the data
2608620	2613020	or they did have human level diagnosis back then
2613020	2614700	but they weren't allowed to apply it
2614700	2616300	because of medical licensing.
2617180	2619900	So, and we're still not allowed to apply it
2619900	2622220	because of medical licensing.
2622220	2626140	So, this is exactly the sort of ability
2626140	2629420	that won't give substantial economic impact
2629420	2631300	because we had it 40 years ago
2631300	2632940	and it didn't have an impact then.
2632940	2634220	Yeah, I don't know.
2634220	2637700	So if I had, I think one qualitative difference
2637700	2639780	between that earlier system and this system
2639780	2641220	which won't come to be an expert
2641220	2642740	in the earlier expert systems
2642740	2647740	but I would guess that a huge difference
2648220	2653100	is that you can take today a totally uninitiated person
2653100	2655340	who has a medical concern
2655340	2657940	and say, sit in front of this computer,
2657940	2659540	talk to this doctor.
2659540	2661760	They don't even need to know as an AI doctor.
2661760	2663460	They can just talk to him.
2663460	2665700	That wasn't the problem back then.
2665700	2667940	They could have made these expert systems
2667940	2671220	usable by ordinary people with modest effort.
2671220	2673180	That wasn't the problem in using them.
2673180	2675780	The problem was just you're not legally allowed to use them.
2675780	2678700	Only doctors are allowed to give medical diagnoses.
2678700	2680940	And so only doctors are allowed to use these systems
2680940	2681780	to talk to people.
2681780	2684660	That was the main obstacle and it still is today.
2684660	2688300	The obstacle, you could make such a system today
2688300	2689460	that ordinary people could talk to
2689460	2691060	but they're not allowed to talk to it
2691060	2694220	and they won't be allowed to talk to it for a long time.
2694220	2695860	I think there is a qualitative difference
2695860	2697060	between these systems.
2697060	2699180	If I were to sit down in front of the early 80s thing
2699220	2701580	and I were to say, what's different today
2701580	2703820	is the chat system could say,
2703820	2705060	Robin, tell me how you're feeling.
2705060	2706100	Tell me about your experience.
2706100	2708060	And you can just go on in your own language,
2708060	2709780	however you want to express yourself,
2709780	2710780	and it can get you.
2710780	2713140	And then it can ask you specific follow up
2713140	2714420	but you're not going through a wizard
2714420	2716940	and going down an expert system tree
2716940	2719940	and ask for numeric scores you don't understand
2719940	2720780	and don't know.
2720780	2722620	You can literally just express yourself.
2722620	2723820	That was not there then, right?
2723820	2725100	I mean, nothing.
2725100	2728100	But that's not the limiting factor, right?
2728100	2730700	I mean, you couldn't have a fancy graphics interface
2730700	2731540	back then either.
2731540	2733940	This was early 1980s, right?
2733940	2737460	But again, the limiting factor is the legal barrier.
2738340	2739740	It was back then and still is
2739740	2741580	and that legal barrier doesn't look like
2741580	2743180	it's about to go away.
2743180	2745940	So if you're gonna make us excited about applications
2745940	2748540	it'll have to be something that's legal.
2748540	2752340	My model of this is that the consumer surplus
2752340	2755500	of this type of thing is going to be so great.
2755500	2756860	It already was 40 years ago.
2756900	2758420	It would have been a huge consumer surplus
2758420	2760620	40 years ago, it was not allowed.
2760620	2763140	But there was never a groundswell of, I don't know.
2763140	2764180	I'm just not buying this.
2764180	2766300	I'm not buying that there was an experience
2766300	2769100	that is qualitatively like the one that we have today
2769100	2773260	such that I think today if you show people what Google has
2773260	2776420	they will say it is not acceptable to me
2776420	2779900	that you keep this locked up behind some payroll.
2779900	2782820	I don't think that was the general consumer reaction
2782820	2785620	to early 80s expert systems.
2785660	2789580	And it seems like that political economy pressure
2789580	2791180	could change things.
2791180	2793740	Consider the analogy of nuclear power.
2793740	2796100	The world has definitely been convinced for a long time
2796100	2798620	that nuclear power is powerful.
2799460	2802720	It is full of potential and power.
2802720	2804620	And if we had let it go wild
2804620	2806540	we would have vastly cheaper energy today
2806540	2808500	but it was that power that scared people
2808500	2811140	which is why we don't have that energy today.
2811140	2814580	The very vision of nuclear energy being powerful
2814580	2817180	is what caused us not to have it.
2818540	2821620	We over-regulated it to death
2821620	2824140	and we made sure that the power of nuclear power
2824140	2825660	was not released.
2825660	2827180	We believed the power was there.
2827180	2829620	It was not at all an issue of not believing
2829620	2830900	that nuclear power was powerful.
2830900	2833340	It was believing it was too powerful.
2833340	2835860	Scary, dangerous, powerful.
2835860	2838880	And there's a risk that we'll do that with AI today.
2839840	2841500	We will make people believe it's powerful,
2841500	2843620	so powerful that they should be scared of it
2843620	2844980	and it should be locked down
2844980	2847620	and not released into the wild
2847620	2849820	where it might do us terrible danger.
2849820	2851500	Yeah, well, that's certainly a tragic outcome
2851500	2853900	in the case of the nuclear power.
2853900	2855980	And I think it would also be a tragic outcome
2855980	2858900	if people are denied their AI doctors
2858900	2863220	of the future on that basis.
2863220	2864540	And it could happen.
2864540	2867220	I certainly wouldn't rule out the possibility that
2867220	2869620	just AI research probably gets made illegal.
2869620	2871780	This time we do have, I mean, again, it is,
2871780	2873180	I do think we're in a different regime now
2873220	2877180	where enough has been discovered
2877180	2879580	and enough has been put into the hands of millions.
2879580	2884260	There is sort of the open source kind of hacker level.
2884260	2886100	Not medical diagnosis is not.
2886100	2888460	We have not put medical diagnosis AI
2888460	2890460	in the hands of ordinary people.
2890460	2892380	And if you tried it, you would find out
2892380	2894500	just how quickly you'd get slapped now.
2894500	2896300	Yeah, I think I know someone who actually may be
2896300	2898540	about to try this and it'll be very interesting
2898540	2901660	to see how quickly and how hard they get slapped down
2901940	2903580	and how they may respond from it.
2903580	2906140	I've actually been very encouraged by the response
2906140	2907980	from the medical community.
2908820	2910940	I would say, obviously it's not a monolithic thing,
2910940	2914380	but I did an earlier episode with Zach Kahane,
2914380	2917460	who is a professor at Harvard Medical School
2917460	2920340	and who had early access to GPT-4.
2920340	2921780	He came out with a book basically
2921780	2924860	to coincide with the launch of GPT-4
2924860	2928540	called GPT-4 and the Revolution in Medicine.
2928540	2933540	And broadly, I have been encouraged by how much
2933900	2937180	the medical establishment has seemingly been inclined
2937180	2939580	to embrace this sort of stuff.
2939580	2941180	I don't know if it's just that they're also
2941180	2943220	overworked these days or...
2943220	2945460	Well, they'll embrace the internal use of it.
2945460	2949500	Again, it's always been doctors allowed to use these things.
2949500	2951220	And the main reason they didn't get more popular
2951220	2953540	is doctors couldn't be bothered to type in
2953540	2955420	and input all the information
2955420	2958820	because they want to have short meetings with patients.
2958820	2961460	Even today, of course, if you've gone to a modern doctor,
2961460	2963260	most of your meeting with a doctor
2963260	2966700	is them typing in information to their computer
2966700	2968220	as they talk to you.
2968220	2970620	And they don't wanna spend much more time
2970620	2971940	typing in more.
2971940	2974960	And so they don't wanna use computer aids
2974960	2976980	in their diagnosis and that's been true for a long time.
2976980	2979220	They, computer diagnosis aids have been available
2979220	2981940	for a long time that would give them better diagnoses
2981940	2984180	at the cost of them having to spend more time with them
2984260	2985700	than they've chosen not to spend more time.
2985700	2987580	That's been true for many decades now.
2987580	2992180	Have you personally used GPT-4 for any advanced things
2992180	2995420	like this, medical or legal advice or whatever?
2995420	2997300	No, I'm an economics professor.
2997300	2999540	So I've used it to check to see what my students
2999540	3001660	might try to use it to answer my exam questions
3001660	3003960	or essay questions or things like that.
3003960	3006140	I've asked it things that I wanted to know
3006140	3008260	and try to check on them.
3008260	3011900	I haven't used it for legal or medical questions.
3011900	3013940	Those are areas which are heavily regulated.
3013940	3016020	It's always been possible for other people
3016020	3017900	to offer substitutes.
3017900	3020060	So for example, many decades ago,
3020060	3022220	there were experiments where we,
3022220	3025220	basically for the purpose of general practice for doctors,
3025220	3027440	we compare doctors to nurses,
3027440	3028980	nurse practitioners or paramedics.
3028980	3031140	We found that those other groups did just as well
3031140	3033740	and much cheaper at doing the first level
3033740	3036980	of general practice, but they haven't been allowed.
3036980	3039200	So that right there is enormous value
3039200	3040140	that could have been released.
3040140	3042700	We could have all this time been having nurse practitioners
3042700	3045940	and doctors and paramedics do our first level
3045940	3047380	of general practice medicine.
3047380	3049700	And they would save at least a factor of two or three
3049700	3051940	in cost and that's been true for decades.
3051940	3054420	We've had randomized experiments showing that for decades.
3054420	3057100	So going back to the age of M then for a second,
3057100	3061180	are you just assuming that that scenario doesn't happen
3061180	3064020	in M land for some reason?
3064020	3067580	Or like, why wouldn't it be the first objection
3067580	3070380	to the age of M seems like it maybe should be,
3070380	3071420	M's will be made illegal.
3071420	3073020	Nobody will be allowed to do it.
3073020	3074220	Absolutely.
3074220	3077140	And basically you're just kind of in the analysis saying,
3077140	3078500	well, let's just assume that doesn't happen
3078500	3080580	because it'll be, you know, it's a short book
3080580	3082280	if they just get made illegal too early.
3082280	3083700	Is that the idea?
3083700	3085740	Well, so first of all,
3085740	3088560	I say transitions are harder to analyze
3088560	3090700	than equilibria of New World.
3090700	3094140	So I try to avoid analyzing the transition.
3094140	3096460	Although I do try to discuss it some toward the end
3096460	3098500	of the book, but I admit,
3098500	3100860	I can just say less about a transition.
3100860	3103460	It does seem like that, you know,
3103460	3105940	compared to a scenario where everyone eagerly adopted
3105940	3108060	M technology as soon as it was available,
3108060	3110540	more likely there will be resistance.
3110540	3113180	There will be ways in which there are obstacles
3113180	3115860	to M technology early on.
3115860	3117300	And therefore at some point,
3117300	3118980	there would basically be the, you know,
3118980	3121900	breaking of a dam flooding out where a bunch of things
3121900	3124560	that had been held back were released
3124560	3127100	and then caused a lot of disruption,
3127100	3128620	faster disruption that would have happened
3128620	3132300	had you adopted things as soon as they were available.
3132300	3133140	And that's part of,
3133140	3136300	that can be very disturbing transition then, you know,
3136300	3138620	if all of a sudden large numbers of people
3138620	3141420	are disrupted in ways they weren't expecting
3141420	3144500	in a very rapid way because of, you know,
3144500	3146620	a dam suddenly broke open,
3146620	3150440	then I think there will be a lot of unhappy people
3150440	3151560	in that sort of a transition
3151560	3153020	and maybe a lot of dead people.
3153020	3155620	So imagine the M technology slowly just gets cheaper
3155620	3158600	over time, but it's not very wide.
3158600	3160440	It's not very widely adopted.
3160440	3164080	Then there'll be a point at which it eventually gets so cheap
3164080	3166320	that if some say ambitious nation,
3166320	3168040	like say North Korea said,
3168040	3171120	gee, if we went whole hog and adopting this thing,
3171120	3172960	we could get this big, you know,
3172960	3175920	economic and military advantage over our competitors,
3175920	3178920	then eventually somebody would do that.
3178920	3180880	Now it might take a long time.
3181800	3184680	That is the world could coordinate to resistance technology
3184680	3187160	for a long time,
3187160	3190880	but I don't think they could hold it back for a thousand years.
3190880	3192120	So then I feel somewhat confident,
3192120	3193960	eventually in the age of M happens,
3194920	3197360	and then eventually there's a thing to think about
3197360	3198680	and then I'm analyzing that world.
3198680	3201520	So I don't want to presume in the age of M
3201520	3204040	that this transition happens smoothly or soon
3204040	3206440	or as fast as it could,
3206440	3208360	but I want to say eventually there'll be this new world
3208360	3209680	and here's how it would play out.
3209680	3212200	So I don't know if you know that in the last few months
3213360	3216160	I've dramatically changed my vision of the future
3216160	3218040	to say that there's probably gonna be
3218040	3220560	a several century innovation pause,
3220560	3223000	probably before the age of M happens,
3223000	3225840	and then the world that would eventually produce AI
3225840	3228040	and M's would be a very different world from ours
3228040	3231400	and somewhat hard to think about.
3231400	3235720	That is rising population will stop rising
3235720	3238080	and it will fall due to falling fertility,
3238080	3240600	that will basically make innovation grind to a halt,
3240600	3243200	then the world population will continue to fall
3243200	3248280	until insular fertile subcultures like the Amish
3248280	3250120	grow from their very small current levels
3250120	3252960	to become the dominant population of the world.
3252960	3254880	And then when that becomes large enough
3254880	3256520	compared to our current economy,
3256520	3258160	then innovation would turn on again
3258160	3261560	and then we would restart the AI and M path
3261560	3264840	and then eventually the age of M would happen.
3264840	3267000	Trying to anticipate how transitions would happen
3267000	3269760	in a world we can just hardly even imagine,
3269760	3270960	seems tough, right?
3271000	3273960	That is, okay, imagine the descendants of the Amish
3273960	3277600	become a large, powerful civilization.
3277600	3280600	They've always been somewhat resistant to technology
3280600	3283720	and very picky about which technologies they're allowed,
3283720	3285640	but eventually I would predict
3286840	3288320	there would be competition within them
3288320	3293320	and that would push them to adopt technologies like AI and M's
3293320	3296280	but we're looking a long way down the line.
3296280	3298240	And this isn't what I wish would happen
3298240	3299280	to go back to your initial thing.
3299280	3301720	I would rather we continued growing
3301720	3303240	at the rate of the past century
3303240	3305760	and continue that for a few more centuries,
3305760	3308000	by which time I'm pretty sure
3308000	3310800	we'll eventually get M's and human level AI,
3310800	3313160	although question in what order,
3313160	3315480	but I got to say at the moment,
3315480	3317400	that's not looking so good.
3317400	3321360	So basically, I'm estimated that if we were to continue
3321360	3325400	on a steady growth path, we would eventually reach a point
3325400	3326960	where we had the same amount of innovation
3326960	3328720	as we will get over the entire integral
3328760	3330200	of this several centuries pause.
3330200	3333040	And I've estimated that to be roughly 60 to 90 years
3333040	3333880	worth of progress.
3333880	3336960	So if we can get full human level AI
3336960	3339320	in the next 60 to 90 years with the progress,
3339320	3341680	then this population decline won't matter so much
3341680	3344800	because we will basically have AI's takeover most of the jobs
3344800	3348640	and then that can allow the world economy to keep growing.
3348640	3351840	I think that's iffy whether we can do that,
3351840	3356160	whether we can achieve full human level AI in 60 to 90 years.
3356160	3357920	And I know many people think it's gonna happen
3357920	3359520	in the next 10 years, they're sure.
3359520	3361840	So sure, of course it'll happen in 60 to 90 years,
3361840	3364480	but I look at the history and I go,
3364480	3366720	look, I've seen over and over again,
3366720	3370600	people get really excited by the next new kind of AI.
3370600	3373200	And they're typically pretty sure,
3373200	3375360	a lot of them are pretty sure that we must be near the end
3375360	3378040	and pretty soon we'll have it all.
3378040	3380840	And it just keeps not happening.
3380840	3384680	The main change I wanna suggest to that paradigm
3384840	3389840	is replacing the end with meaningful thresholds along the way.
3390960	3393360	I think there are probably several
3393360	3398000	that we will hit on some time scale.
3398000	3400640	And it feels to me like,
3400640	3404520	at least a couple of the big ones are pretty close.
3404520	3408000	And then at the end is very,
3408000	3409400	my crystal ball gets very foggy
3409400	3411720	beyond like a pretty short time scale.
3411720	3415280	But I'm struggling with the early 80s expert systems,
3415280	3417840	but it really does seem like in my lifetime,
3417840	3422080	I have not seen anything that remotely resembles
3422080	3424080	the experience of going to a doctor.
3424080	3428120	I've seen WebMD, I'm familiar with expert systems
3428120	3430680	to a degree, but I've never seen anything that,
3430680	3433800	I didn't think Ilya Setsgaver from OpenAI
3433800	3436320	puts this really well, he's like the most shocking thing
3436320	3439600	about the current AIs is that I can speak to them
3439600	3441680	and I feel that I'm understood.
3441680	3445080	And that is like a qualitatively different experience.
3445080	3449520	And clearly I think reflects some qualitative advance
3449520	3453920	in terms of what kind of information processing is going on.
3453920	3456840	If I had to say like, what is that under the hood?
3456840	3461840	I would say it's like a high dimensional representation
3462440	3467360	of concepts that are like really relevant to us
3467400	3470880	that have previously been kind of limited
3470880	3473800	to like language level compressed encoding.
3473800	3475880	But now we are actually starting to get to the point
3475880	3478240	where we can like look at the middle layers
3478240	3480360	of even just the systems we have today,
3480360	3482120	the transformers and say,
3482120	3487120	can we identify concepts like positivity
3487960	3491280	or paranoia or love?
3491280	3493600	And we are starting to be able to,
3493600	3494960	it's still pretty messy.
3494960	3496400	We have the same, not the same,
3496400	3499480	we have an analogous problem to like understanding
3499480	3500880	what's going on inside the brain
3500880	3503720	and it's just a mess in there still in the transformers.
3503720	3505080	But we are starting to be able to see these
3505080	3509120	like high dimensional representations where it's like,
3509120	3512280	that is a numeric representation
3512280	3513640	of some of these big concepts.
3513640	3515080	And we're even starting to get to the point
3515080	3517960	where we can steer the language model behavior
3517960	3520000	by like injecting these concepts.
3520000	3525000	So you can say, for example, inject safety
3525000	3526800	into the middle layers of a transformer
3526800	3531800	and get a safer response or danger or rule breaking
3531840	3534200	and then they'll be more likely to break their rules.
3534200	3536360	What you're focused on at the moment is telling me
3536360	3539800	about how the latest generation adds capabilities
3539800	3542120	that previous generations didn't have.
3542120	3544920	But every previous generation had that same conversation
3544920	3547160	where they focused on the new capabilities
3547160	3550600	their new generation had that the ones before it didn't have.
3550600	3553000	What the conversation you're participating in
3553000	3555960	is continuing the past trend.
3557200	3559440	But the fundamental question is,
3559440	3564440	when will AIs be able to do what fraction of the tasks
3565200	3567620	that we have in the human economy,
3567620	3569320	if they can't do a large fraction of them,
3569320	3571480	no matter how impressive they are at the practice
3571480	3574720	they can do, we will see this economic decline
3574720	3576240	as the population declines.
3576240	3579120	They need to be able to do pretty much all the tasks
3579120	3581440	in order to prevent the economic decline
3581440	3583200	and then the halting of innovation.
3583200	3585760	I did this study of innovation in the United States
3585760	3590200	over 20 years from 1999 to 2019.
3590200	3591880	And that was a period that encompassed
3591880	3595640	what many people at time said was enormous AI progress.
3595640	3598560	And many people in the period were talking about
3598560	3602200	how there was this revolution in AI
3602200	3605640	that was about to cause a revolution in society
3607240	3610040	in this period from 1999 to 2019.
3610040	3614280	So we did a study, a co-author and I,
3614280	3618120	Keller Scholl, who looked at all jobs in the US,
3618120	3622040	basically roughly 900 different kinds of jobs.
3622040	3623760	And over that 20-year period,
3623760	3628200	we had measures of how automated was each job in each year.
3629960	3634400	And then we could do statistics to say,
3634400	3635920	when jobs got more automated,
3635920	3638160	did they get the wages go up or down?
3638160	3640920	Did the number of workers in those jobs go up or down?
3640920	3643200	And we could say, what about jobs predicts
3643200	3644480	how automated they are?
3645400	3648320	And did the things that determine which jobs
3648320	3650760	or how automated change over that 20-year period?
3652520	3654080	That is, if there had been some revolution
3654080	3655520	in the nature of automation,
3655520	3657480	then the things that predicted which jobs
3657480	3660120	would be more automated would have changed over time.
3661520	3665120	What we found was that when jobs got more or less automated
3665120	3666960	that had no effect on average,
3666960	3668680	on wages or number of workers,
3669520	3671240	and that the predictors of automation
3671240	3673280	didn't change at all over that 20-year period,
3673280	3676320	and they remain to be very simple-minded predictors
3676320	3679240	that you might expect about automation from long ago.
3679240	3680920	The nature of automation hasn't changed
3680920	3682440	in the aggregate in the economy.
3682440	3684200	Main predictors of automation are
3684200	3686240	whether the job has nice, clear measures
3686240	3687440	of how well you've done it,
3687440	3688880	whether it's in a clean environment
3688880	3690800	with fewer disruptions,
3690800	3693200	and whether tasks nearby have been automated.
3693200	3695720	So there's a way that which task automation spreads
3695760	3698240	to the network of nearby tasks.
3698240	3702000	So that study suggested at least up until 2019,
3702000	3704680	there had been no change in the nature of automation,
3704680	3707520	and basically there's a Gaussian distribution
3707520	3709360	of how automated jobs are,
3709360	3712120	and the median automation had moved roughly
3712120	3715280	a third of a standard deviation through that distribution.
3715280	3717960	So jobs had gotten more automated substantially
3717960	3718960	in that 20-year period,
3718960	3723480	but still most jobs aren't that automated.
3723520	3725200	And that would be my rough prediction
3725200	3727760	for the next 20 years is to say
3727760	3730160	the pattern of the last 20 years will continue.
3730160	3732800	That is, I will slowly get more jobs more automated,
3732800	3736480	but most automation will be very basic stuff.
3736480	3739040	So far we just haven't seen much at all
3739040	3741680	of advanced AI kinds of automation
3741680	3744160	making a dent in the larger economy.
3744160	3745800	So what do you make of things,
3745800	3749120	I'm sure you're familiar with like the MMLU benchmark
3749120	3750320	or the big bench, maybe not,
3750320	3752960	if not I can characterize them for you, but.
3752960	3756120	Is this machine learning set of tests
3756120	3758800	in order to benchmark performance?
3758800	3762000	Yes, I believe it's massive multi-task language
3762000	3765680	understanding, the great Dan Hendricks and team.
3765680	3768600	So basically a bunch of language understanding benchmarks?
3768600	3771480	Yeah, they basically went and took final exams
3771480	3776480	from like university and early grad school courses
3776720	3779160	from every domain and compiled them
3779160	3780640	into this massive benchmark.
3780640	3782000	There have been a couple of different efforts like this,
3782000	3783840	but this is basically the gold standard
3783840	3786120	on which all the language models are measured.
3787240	3792240	And we now have a like high 80s to 90% accuracy rate
3793800	3797240	across all fields from like a single model, namely GPT-4.
3797240	3799400	And now Google claims that it's Gemini
3799400	3802180	is hitting that level as well.
3803040	3807520	I would agree that these have not been broadly customized
3807520	3809600	to the last mile specifications that they need
3809600	3811760	to like work in the context of different firms
3811760	3815720	and cultural contexts and all that sort of thing.
3815720	3818680	But it does seem like the way I typically describe it
3818680	3822960	is that AIs are now better at routine tasks
3822960	3825120	than the average person and that they are closing in
3825120	3829120	on expert performance on routine tasks.
3829120	3831880	And that's measured by these medical diagnosis benchmarks,
3831880	3836000	these MMLU type things, et cetera, et cetera.
3836000	3840120	So let me remind you that in the 1960s say
3841120	3846120	AI researchers took chess as a paradigm of
3846200	3848880	if you can make a machine that can do that,
3848880	3850760	well, obviously you'll have to have solved
3850760	3852600	most of the major problems in thinking
3852600	3855200	because chess involves most of the major problems
3855200	3856040	in thinking.
3856040	3859680	So when we can finally have human level chess abilities,
3859680	3861680	we will have human level AI.
3861680	3863120	That was the thinking in the 60s
3863120	3865640	and they could look at the rate at which AI
3865640	3868400	was getting better at chess and forecast long before
3868400	3871840	it happened that in the late 1970s, 1990s, excuse me,
3871840	3875880	is exactly when chess would reach human level ability
3875880	3877760	and that's when it did happen.
3877760	3880320	And that was 25 years ago.
3880320	3882400	And clearly they were just wrong about the idea
3882400	3884600	that you couldn't do chess without solving
3884600	3886160	all the major thinking problems.
3886160	3888680	And we repeatedly have this sort of phenomena
3888680	3891840	where people look at something and they go,
3891840	3894880	if you can do that, surely you can do most everything.
3894880	3897680	And then we can do that and we can't do near,
3897720	3899360	and we aren't near to doing most everything.
3899360	3901640	So I just got to say this benchmark is just wrong.
3901640	3905120	It's not true that if you can do these language benchmark,
3905120	3906640	you are near to doing most everything.
3906640	3908200	You are not near.
3908200	3909920	Yeah, I would find my position to say,
3909920	3913080	I think you're near to being able to do all the routine things
3913080	3916080	that are well documented in the training data.
3916080	3918320	Well, yes, but the question is in the economy,
3918320	3921000	all the things we need doing, how close are you to that?
3921000	3923000	And say you're not close.
3923000	3927120	I mean, we're seeing just the very beginning of sort of,
3927120	3929560	I mean, again, I don't know, like...
3929560	3930960	What do you think was going on in their head
3930960	3933800	in the 1960s when they looked at chess, right?
3933800	3935040	They looked at chess and they said,
3935040	3936720	it takes really smart people to do chess,
3936720	3938960	look at all these complicated things people are doing
3938960	3941360	when they do chess in order to achieve in chess,
3941360	3942600	they said to themselves,
3942600	3944000	that's the sort of thing we should work on
3944000	3945440	because if we can get a machine to do that,
3945440	3949920	surely we must be close to general artificial intelligence.
3949920	3952800	If you could have something that could do chess.
3952800	3955240	And there is a sense that when you have general intelligence,
3955240	3958040	you can use all of that to do clever things about chess,
3958040	3961400	but it's not true that you need to have all those general things
3961400	3962400	in order to be good at chess.
3962400	3964200	That turns out there's a way to be good at chess
3964200	3965840	without doing all those other things.
3965840	3967640	And that's repeatedly been the problem
3967640	3969400	and that could be the problem today.
3969400	3972280	Turns out there's a way to do these exam answering things
3972280	3976960	that doesn't require the full range of general intelligence
3976960	3978680	in order to achieve that task.
3978680	3981120	It's hard to pick a good range of tasks
3981120	3984120	that encompasses the full range of intelligence
3984120	3986760	because again, you teach through the test
3986760	3988840	and you end up finding a way to solve that problem
3988840	3991680	without achieving general intelligence.
3991680	3993360	This does seem different though.
3993360	3995560	I mean, I would, I grew with your characterization
3995560	3999080	that basically it turned out that there was an easier way
3999080	4002840	or a more direct way, a narrower way to solve chess.
4002840	4007080	And it's interesting that it's like rather different.
4007080	4010640	You know, it involves these sort of superhuman tree search
4010640	4011600	capabilities.
4011600	4012960	But that wasn't just true of trust.
4012960	4015880	There were another dozen sorts of really hard problems
4015880	4020880	that people in the 1960s took as exemplars of things
4020880	4022440	that would require general intelligence
4022440	4024560	and the great many of them have been achieved.
4024560	4027240	But when I look at the current situation,
4027240	4029960	I'm like, this does look a lot more
4029960	4032080	like the human intelligence.
4032080	4036240	And I would say that from any number of different directions.
4036240	4039360	And that was true in every decade for the last century.
4040360	4044080	Every decade has seen advances that were not the sort
4044080	4046080	that previous systems could achieve.
4046080	4047960	It's clear that you are always, I think it's clear
4047960	4052160	that you don't see the human brain, the human, you know,
4052160	4055640	achieve level of achievement as sort of a maximum, right?
4055640	4057080	Oh, of course not. Absolutely.
4057080	4061240	So it's like there's got to be a finite number
4061240	4065000	of breakthroughs that need to happen.
4065000	4067240	We will eventually get full human level AI.
4067240	4068680	I have no doubt about that.
4068680	4073520	And not soon after vastly exceeded, that will happen.
4073520	4076480	And it will happen plausibly within the next thousand years.
4076480	4080200	It also seems like you would probably agree that it need not
4080200	4084800	be point for point, you know, the M scenario is a great one
4084800	4087600	to play out and analyze, but it need not be the case.
4087600	4090640	Right. So the AIs could be much better than humans in some ways
4090640	4092440	and still much worse than others.
4092440	4095280	That will probably actually be true for a long time.
4095280	4097880	That is, it'll take a lot longer till AIs are better
4097880	4100240	than humans at most everything than that they are better
4100240	4103240	at humans at say half of things people do today.
4103240	4104640	But of course you have to realize if you looked
4104640	4108240	at what humans were doing two centuries ago, we're already
4108240	4110760	at the point where machines do those things much better
4110760	4112480	than humans can do.
4112480	4114760	That is, the attack, most tasks that humans were doing
4114760	4117960	two centuries ago are already long since automated.
4117960	4120640	We've now switched our attention to the sort of tasks
4120640	4122560	that people were not doing two centuries ago.
4122560	4125080	And on those, we're not so good at making machines do them,
4125080	4129120	but we've already dramatically achieved full automation basically
4129120	4131840	of most things humans were doing two centuries ago.
4131840	4133760	Which for very shorthand I would say is kind
4133760	4138000	of routine repetitive physical tasks.
4138000	4140080	Right. I mean, we managed to change the environment
4140080	4142400	to make them more routine and repetitive.
4142400	4145880	So, you know, a subsistence farmer
4145880	4148720	on a subsistence farm two centuries ago, they were,
4148720	4151200	we couldn't, our automation could not do that job
4151200	4152520	that they were doing that.
4152520	4154080	And we managed to make the farms different.
4154080	4155960	The factory is different, et cetera, so that our machines
4155960	4157360	could do them.
4157360	4160000	And now they are producing much more than those people produce.
4160000	4162600	But if you had to try to produce the way they were doing
4162600	4165680	two centuries ago, our machines today could not do that.
4165680	4168680	Yeah, a big theory I have also, I actually don't think this is going
4168680	4172160	to be a huge, well, everything's going to be huge,
4172160	4175800	but I don't think it's going to be like the dominant change
4175800	4178960	that leads to qualitatively different future.
4178960	4181760	But I do think we will start to see, and are beginning
4181840	4186160	to see that same process happening with language models,
4186160	4189520	where, you know, I consult with a few different businesses
4189520	4192240	and we have kind of processes that, you know,
4192240	4194000	we would like to automate.
4194000	4197800	You know, a classic one would be like initial resume screening.
4197800	4199600	Right. We're not going to have the language model at this point
4199600	4200920	make the hiring decisions.
4200920	4202920	But if we get a lot of garbage resumes, you know,
4202920	4208160	we can definitely get language models to kind of band the resumes
4208160	4212080	into, you know, one to five and like spend our time on the fives.
4213280	4216080	It does seem to me that there's a lot of kind of process
4216080	4220400	and environment adaptation that is not that hard to do.
4220400	4223240	Like I personally have done it successfully across a handful
4223240	4224560	of different things.
4224560	4228360	Why it seems like you're announced as though a sort of doesn't
4229320	4231640	assumes that that's not going to happen at scale this time
4231640	4234040	around with the technology we currently have.
4234480	4239240	I said, you know, in the last 20 years from 1999 to 2019,
4239240	4241720	we moved roughly a third of a standard deviation
4241720	4244000	in the distribution of automation.
4244000	4247640	OK, so what if we in the next 60 years
4248400	4252640	move a third of a standard deviation in each of the 20 year periods?
4252640	4255760	Then over 60 years, we would basically move an entire standard deviation.
4256880	4261400	That could represent a large increase in automation
4261880	4264000	over the next 60 years.
4264120	4267040	And that would mean a lot of things we're doing by hand today
4267040	4268560	will be done by machines.
4268560	4271240	Then it would mean our economy is more productive,
4271840	4275120	but it still would mean humans have a huge place in the world.
4275120	4279240	They get paid and most income probably still goes to pay humans to do work,
4279880	4282520	even though they have much better automation at the time.
4283280	4285840	If that's the situation in 60 years,
4286960	4290320	then unfortunately that level of increase in automation
4290320	4292480	is just not sufficient to prevent the economy
4292480	4295200	from declining as population declines.
4295200	4298440	And so we won't get much more automation than that.
4299640	4303480	The well of it in automation will dry up because innovation will stop.
4303520	4306040	And we would then have a several centuries long period
4306040	4308840	where our technology does not improve.
4308840	4311640	And in fact, we lose a lot of technologies tied to scale economies
4312160	4314480	as the world economy shrinks.
4314480	4319480	We'll manage to have less variety, less large scale production and distribution.
4320280	4323880	And we would then struggle to maintain previous technologies.
4323880	4326880	And AI is at risk of the sort of technology would be hard to maintain
4326880	4330920	because at the moment, AI is a really large scale, concentrated sort of technology
4330920	4334800	is not being done by mom and pops to be done by very large enterprises
4334800	4336240	on very large scales.
4336240	4341480	I would agree that the supply chain is definitely prone to disruption in AI.
4341480	4342840	No doubt about that.
4342840	4347840	Can you describe in more detail what what is the standard deviation
4347840	4350880	in automation and how should I conceptualize that?
4351440	4354760	I mean, I guess what you'd want to do is see a list of tasks
4354760	4362240	and how automated each task was and then see sort of how much on that score.
4362240	4365080	And it would have. So basically, if you look on this list
4365080	4368160	at the most and least automated tasks, you'll agree, which are which
4368880	4372320	like the nearly most automated task is airline pilots.
4373320	4376320	Nearly the least automated task is carpet installers.
4377320	4382040	Carpet installers use pretty much no automation to staple in carpets.
4382040	4387320	And airline pilots are pretty much always having automation help what they're doing.
4388120	4392320	And then, you know, you can see the scores in the middle and see that we've,
4392320	4395280	you know, moved up a modest degree over those 20 years.
4395800	4398920	That would be the way to get an intuition for it is just to see a list
4398920	4402720	of particular jobs in their automation scores and then see,
4402720	4404960	compare that to the amount by which we've moved up.
4405680	4406640	How do you reconcile?
4406640	4411320	Or how should I understand the idea that
4411720	4414560	whatever doubling time of the economy today,
4414840	4417480	I think you said it was like 15 years in the book,
4417480	4420800	which seemed a little fast to me, just based on like rule of 70.
4421360	4424080	Right. I think it's more like, you know, 20 or something now.
4424320	4427920	But still, like it seems it seems like there's a little bit of a disconnect
4427920	4432920	between a notion of, you know, over these next 60 years,
4432920	4437280	we would be double, double, double, you know, essentially 10xing the economy.
4437680	4442600	But we'd only move at sort of a linear rate in automation.
4442600	4446240	Like we would only move a third of a standard deviation in each period.
4446480	4448080	Let me help you understand that then.
4448080	4452240	People have often said, look, computer technology is increasing exponentially.
4452640	4456400	Therefore, we should expect an exponential impact on the economy,
4456400	4461080	i.e. early on hardly any impact, and then suddenly an accelerating boom
4461120	4464360	such that we get this big explosion and then everything happens.
4464560	4466040	But that's not what we've seen.
4466040	4471760	So what we've seen over time is relatively steady effects on the economy of automation,
4471760	4474640	even though the economy is growing exponentially.
4475840	4480920	The way I help you understand that is to imagine the distribution of all tasks
4480920	4484760	that you might want automated and that they're the degree of computing power,
4484880	4489120	both in hardware and software, required to automate that task for each task
4489160	4493320	is distributed in a log normal way with a very large variance.
4493480	4498040	That is, there's a very large range of how much computing power it takes to automate a task.
4498360	4500560	As computing power increases exponentially,
4500560	4504320	you're basically moving through that log normal distribution in a linear manner.
4504720	4507960	And in the middle of the distribution, it's pretty steady effect.
4508600	4512880	You slowly chop away at tasks as you are able to automate them
4513440	4518520	because you're slowly acquiring sufficient hardware to do that task.
4519280	4521840	That that gives you a simple model, but in which
4522960	4525080	computing power grows exponentially.
4525080	4529800	And yet you see a relatively steady erosion of tasks through automation.
4530160	4532320	It's a low hanging fruit argument.
4532320	4535040	Yeah, the low hanging fruits are hanging really low.
4535720	4540040	That this this is a log normal tree, basically, that you're trying to grab things from.
4540040	4543040	I mean, you're growing your ladder is growing exponentially into the tree.
4543360	4546200	And every time your ladder gets taller, you get to pick more feuds.
4546200	4547880	But it's a really tall tree.
4547920	4550080	That means that you have a long, long way to go.
4550920	4555680	How do you think about things like the progress in AI art generation
4555680	4558520	or like deep fakes over the last couple of years?
4558520	4564400	This is an area where I feel like if we rewound to two years ago,
4564720	4569080	just two years ago, really, when I was first starting to see AI art
4569720	4572200	popping up on Twitter and it was like
4573200	4576360	not very good for the most part, you'd see the occasional thing where you're like,
4576360	4577360	oh, that's really compelling.
4577360	4580800	And then you'd see a lot of stuff that was like, yeah, you know, it's whatever.
4580800	4582680	It's it's remarkable that you can do that.
4583280	4587520	It's a while compared to what came before, but it, you know, it's like
4588920	4592680	I'm not going to be watching like feature films based on this technology
4592680	4594880	and, you know, in the immediate future.
4594880	4598320	I feel like we could have had a very similar discussion where you might say,
4598320	4600440	well, you know, yeah, it's progress.
4600440	4605360	But, you know, the real human art, the top notch stuff, like that's so far away.
4605800	4610680	And then early last year, my teammates at Waymark made a short film
4610680	4617120	using nothing but Dolly 2 at that time imagery and some definite elbow grease.
4617120	4619600	But like the quality of production that they were able to achieve
4619880	4625280	with a half dozen people and Dolly 2 is on the level that like previously
4625280	4629160	would have taken, you know, a crew in Antarctica, you know, to go shoot.
4629880	4632040	You know, again, is that work all done?
4632040	4635200	No. But if you look at the mid journey outputs today,
4635240	4637800	you look at some of the deep fake technologies that are happening today.
4637800	4640320	It's like it does feel like we've hit certainly
4640320	4643600	photo realistic thresholds, you know, almost indistinguishable
4643960	4646960	from photography with mid journey and with the deep fakes.
4647440	4650800	You're not quite quite there yet, but like watch out for 2024
4650840	4657280	to have a lot of stories of people being scammed by the kind of custom
4657280	4660920	text to speech voice, you know, with a family member, family members voice, whatever.
4661120	4664120	All my voice out there, you know, people are going to be calling my parents with my voice.
4664640	4668240	So I guess what I'm trying to get at there is like it seems like even just
4668240	4672720	in the last couple of years, we have these examples where we are seeing
4672720	4679440	like really rapid progress that is not stopping before critical thresholds.
4679960	4683200	In the 1960s, there was a U.S.
4683200	4687960	Presidential Commission to to address and study the question
4687960	4690040	of whether most jobs were about to be automated.
4690040	4693440	It reached that level of high level concern in the country.
4693480	4695800	And major media discussion about it.
4697280	4701840	Ever since then, we continue to have periodic articles about dramatic,
4702040	4706960	exciting progress in AI and what that might mean for the society and economy.
4707240	4710280	And in all those articles through all those years,
4710960	4714560	they don't just talk in the abstract, they usually pick out some particular examples
4715040	4717440	and they don't pick out random examples from the economy.
4717440	4720520	They pick out the examples where the automation has made the most difference.
4721520	4724360	That, of course, makes sense if you're trying to make an exciting story.
4725640	4729160	And so we've always been able to pick out the things
4729160	4731720	which are having the most dramatic increase lately
4731720	4734280	that also seem the most salient and interesting.
4734440	4737680	And now you can pick out image generation
4738320	4742520	as one of the main examples lately as something that's increased a lot lately.
4742800	4745200	And I'm happy to admit it has.
4745200	4747600	I would put it up, you know, and that's the sort of thing
4747640	4750640	that somebody writing an article today about the exciting AI progress
4750640	4754680	would, in fact, mention and talk about graphic artists being put out of work
4754680	4758520	by the availability of these things, which probably is happening.
4759040	4762800	The point is just to realize how selective that process is
4763040	4766320	to pick out the most dramatic impacts and to realize just how many other jobs
4766320	4770200	there are and how many other tasks there are and then how far we still have to go.
4770840	4773440	I'm happy to celebrate recent progress.
4773440	4777640	And if I were, you know, if I were a graphic artist person,
4777640	4781040	I would be especially excited to figure out how to take advantage of these changes
4781560	4784240	because they are among the biggest change.
4784520	4786520	If you're, say, a 20 year old in the world,
4786520	4789920	it makes complete sense to say, where are things most exciting and changing?
4790040	4793280	I want to go there and be part of the new exciting thing happening there.
4793960	4797200	If, of course, you're a 60 year old and you've already invested in a career,
4797200	4800880	then it makes less sense to, like, try to switch your whole career over to a new thing.
4800920	4803520	But a lot of people are at the beginning of their career and they should.
4803840	4806160	They should look for where the most exciting changes are
4806160	4808400	and try to see if they can go be part of that.
4808960	4810040	Move West, young man.
4810040	4811920	If West is where things are happening, right?
4811920	4814600	But you still have to keep in mind if there's a few people going out West
4814600	4818760	making exciting things happening, how big a percentage of the world is the West, right?
4819520	4822320	Yes, it's exciting and there's huge growth in the West.
4822320	4825880	You know, 10 years ago, there was hardly anything and now there's a big town.
4826120	4827560	Look how great the West is growing.
4827560	4831120	And that, you know, there are always times and places where right there,
4831120	4836520	things are growing very fast and newspaper writers should focus on those to tell stories
4837160	4839920	and keep novelists should focus on those to tell stories.
4839920	4841960	They're exciting places where exciting things are happening.
4841960	4845320	And I want to make sure the world keeps having things like that happening
4845320	4847760	because that's how we can keep growing.
4847960	4850480	But you have to be honest about the fraction of the world
4850480	4852680	that's involved in those exciting frontier stories.
4853600	4857200	Yeah, I mean, I guess my kind of counterpoint to that would be
4857680	4863280	the same relatively simple technology, like the transformer
4863280	4867560	or like the attention mechanism, perhaps it is better, you know, pinpointed as
4868320	4871120	is driving this art creation.
4871720	4876320	It's also writing today like short programs.
4876320	4880280	Yeah, I would personally say my productivity as a programmer has been
4880600	4885000	increased like several fold, not like incrementally, but like multiple
4885320	4889440	with GPT for assistance, you know, it's the wide range where you could go on.
4889440	4892200	But like it's it's also happening in metal medical diagnosis.
4892200	4896800	It's also happening in like protein, you know, novel protein structure generation.
4896800	4899680	And certainly from an economic point of view, the biggest category
4899680	4901200	you've mentioned is programming.
4901200	4904760	That's a much larger industry, less of your profession than the other ones you mentioned.
4905040	4907880	Well, but watch out for biotech also, I would say, for sure.
4907880	4910520	But biotech has been shrinking for a while.
4910520	4913880	So that's not an exact thing you should point to as a growing thing.
4914280	4916600	I will predict growth for biotech, definitely.
4916760	4919160	I mean, you know, it's also it's reading brain states.
4919160	4922360	Have you seen these recent things where people can read the brain state?
4922680	4924640	Among the things you're talking about at the moment, the biggest
4924640	4927120	profession being affected is programming, clearly.
4927320	4929560	I have a younger son, two sons.
4929560	4931240	My younger one is a professional programmer.
4931240	4934960	So, you know, I've had him look at and his
4935600	4938400	workplace has looked into what they can do with large language models
4938400	4939560	to help them write programs.
4939560	4944160	And their evaluation so far is, you know, they don't even
4945280	4946960	they'll wait in six months to look again.
4946960	4948440	It's not useful now.
4948440	4950640	Can I short that stock?
4951560	4955040	Well, I could tell you after we finish what that is.
4955040	4957280	But basically, I think this is true.
4957280	4961120	Most actual professional programmers are not using large language models
4961120	4963200	that much in doing their job.
4964040	4968240	Now, I got to say that if some people are getting factors of two productivity
4968240	4973920	increase that eventually we should see some effect of that on their wages.
4975440	4979280	That is, of course, you know, now, if lots of programmers go out
4979280	4982120	and use productivity spaces, in some sense, we're going to increase
4982120	4983520	the supply of programming.
4984520	4987840	And so supply and demand would mean that maybe increasing
4987840	4991640	the supply lowers the price, even if it dramatically increases the quantity.
4992560	4996560	But, you know, there's such a large elastic demand for programming
4996560	4999360	in the world that I actually think that effect would be relatively weak.
4999360	5004720	And so you should be expecting large increases in the wages going to programmers.
5005520	5010520	If you are expecting large overall increases in the productivity of programmers.
5011480	5015560	Because, again, it's a large elastic demand for programming in the world.
5016000	5019720	You know, long for a long time, a lot of change in the world has been driven
5019720	5023600	by programming and limited by the fact that there's only so many decent programmers out there.
5024320	5025960	Only so many people you can get to do programming.
5025960	5031400	So clearly, if we can dramatically expand the supply of programmers,
5031920	5034760	we can do a lot more programming in a lot more areas.
5034760	5037920	And there's a lot of money that's willing to go to that to do that.
5037920	5040960	There's a lot of people who would be hiring more programmers if only they were cheaper.
5042040	5043880	And they're about to get cheaper in effect.
5044520	5048120	And so you should be predicting large increases in basically
5048600	5051320	the wages and number of programmers in the world.
5052200	5054000	We haven't seen that yet.
5054000	5055760	I do predict large increases in number.
5055760	5057000	I'm not so sure about wages.
5057000	5058960	It feels like why not?
5058960	5062760	Well, I've done a couple of episodes with the folks at a company called Replet,
5062760	5067640	which is a very interesting end to end at this point, software development platform.
5068200	5071640	Their mission is to onboard the next one billion developers.
5072240	5075880	And, you know, they have like a great mobile app.
5075880	5080440	They have kids in India that are, you know, 14 years old that are doing it all on their mobile app.
5081000	5084080	And I'd say it's much harder.
5084320	5086800	And maybe this reflects the kind of programming that your son is doing.
5086800	5092600	But I'd say it's much harder to take the most elite frontier work and accelerate that
5093200	5100160	in a meaningful way versus like commoditizing the routine application development
5100280	5105080	that like the, you know, the sort of long tail of programmers mostly do.
5105080	5108400	My son is definitely doing routine application development.
5110160	5112000	He's not at the frontier programming at all.
5113000	5120000	But again, I'm saying I don't expect this sudden large increase in programmer wages and quantity,
5120800	5121920	especially wages.
5121920	5125720	I mean, the less the quantity increases, the more wages would have to be increasing to compensate.
5126360	5130360	And I think it'll be hard to get that many more people willing to be programmers,
5130360	5132760	but you could pay them more.
5134240	5135560	And I don't predict this.
5135560	5139880	So this is a concrete thing we could, you know, even better on over the next five or 10 years.
5140440	5142760	Will there be a big boost in programmer wages?
5144120	5145600	That would be the consequence.
5145600	5147840	It's a very simple supply and demand analysis here.
5147840	5152480	This isn't some subtle, you know, rocket science version of economics.
5152480	5155440	Well, typically when supply increases, price drops, right?
5155440	5158680	I'm expecting lots more programmers and them to be broadly cheap.
5158680	5160560	Depends on the elasticity of demand.
5161840	5167320	So, you know, if you think about something that there's just a very limited demand for in the world,
5167560	5171280	you know, if, if piano tuning got a lot cheaper, you wouldn't have a lot more pianos
5171760	5175480	because piano tuning is not one of the major costs of having a piano.
5175560	5179200	You know, it's the cost of the piano itself, plus the space for it in your living room, right?
5179720	5181400	And the time it takes to play on the piano.
5181400	5185640	So piano tuning is a really small cost of piano.
5185640	5190160	So that means the elasticity of demand for piano tuners by itself is pretty low.
5191240	5194560	You know, there's just basically only so many pianos, they all need to be tuned.
5194560	5198240	And if each piano tuner could tune each piano twice as fast, say,
5198880	5204640	and we basically only need half as many pianos because there's just not much of elasticity for demand.
5204640	5210920	So for kinds of jobs like that, productivity increases will cause a reduction in the employment.
5212360	5217480	But even in that case, you might get a doubling of the wages and half the number of piano tuners
5217480	5219360	because they can each be twice as productive.
5219880	5224680	But for programming, it's clear to me that programming has an enormous elastic demand.
5224680	5227880	The world out there has far fewer programmers than they want.
5227920	5231840	They would love all over the place to hire more programmers to do more things.
5232320	5234760	There's a big demand in the world for software to do stuff.
5235160	5238640	And there's a huge potential range of things the software could be doing.
5238640	5240280	It's not doing now.
5240280	5243920	So that means there's a pretty elastic demand for programming.
5243920	5248240	That means as we increase the quantity of programming, the price doesn't come down that much.
5249680	5251240	There's still people willing to buy this stuff.
5252320	5254960	So that tells me that as productivity increases,
5256560	5260000	basically the supply is expanding and the demand is not coming down much.
5260000	5264480	So we should just see a much larger quantity.
5264480	5267560	But then, you know, basically because each person is being more productive,
5267720	5269440	each person should get paid more.
5269440	5272280	So the elastic supply is going to be a combination of two things.
5272280	5277240	Each person getting more productive and more people being willing to join that profession.
5278040	5283520	And I think we've already seen that even as the wages for programming has gone way up
5283520	5286960	in the last decade or so, the number of programmers hasn't gone up as fast.
5287600	5291480	That is, there's just kind of a limited number of people who are decent at programming.
5292440	5296440	And it's hard to get the marginal person to be a programmer.
5297600	5300920	But the people who are programmers, when they're productive, they get paid a lot.
5301080	5304720	I mean, as you've probably heard rumors about AI programmers
5304720	5308760	and how much they're being paid lately, it's crazy high because there's just a limited supply.
5310120	5315600	So I got to say, I expect large increases in wages for programmers,
5315600	5319960	if in fact large language models are making programmers much more productive.
5321040	5326520	But according to my son, at least, and others I've heard, you know, that's not happening.
5327320	5329160	I'm with you up until the very last two points.
5329200	5330960	I would say I think it is happening.
5331400	5336360	And I would also say I think my estimation of the relevant
5336440	5341720	relevant elasticities is that there will be a large growth in people who can be
5341960	5346160	and will choose to be programmers, but that the wages don't go up.
5346160	5350280	They don't fall like dramatically necessarily either because it has to be like
5350320	5352320	an attractive thing for people to want to do it.
5352320	5357480	But I think that the prevailing wages are quite high compared to what a lot of people
5357480	5363360	would be excited to take if they could easily break in with language model assistance,
5363360	5366240	which I think they will increasingly be able to do.
5366640	5367760	Let me change gears a little bit.
5367760	5369080	So we've debated.
5369080	5375680	This has been really I always appreciate a useful and thoughtful challenge to my world model.
5376440	5378560	You're definitely supplying that.
5378560	5385200	Let's do a couple like a little bit more speculative things that could be kind of M first,
5385280	5388160	you know, a little bit of LLM as I was going through the book.
5388160	5391640	There are a number of things that I was like, hmm, this is really interesting.
5392120	5394240	How would I think about this a bit differently?
5394360	5397680	And, you know, and maybe suspend a little bit of your
5398520	5401000	skepticism of how much impact LLM will make.
5401000	5404720	Let's let's go in a world where, you know, scaling continues to work.
5404760	5406520	Context lengths get long.
5406520	5410840	You know, we start to see that not total, you know, displacement of humans,
5410840	5416640	but like substantial fraction of, you know, tasks being like LLM, automatable.
5417160	5423080	One interesting inference that you make is that there won't be that many different base ends
5423440	5430200	that essentially there will be super selective emmifying of really elite,
5430200	5434120	really capable people that those will become the basis that they'll be sort of
5434920	5439880	essentially turn into kind of clans where they'll they'll highly identify with each other.
5440160	5443640	And they'll have like, you know, marginally different specialization,
5443920	5448880	but that there will be these sort of recognizable, almost canonical personalities
5448880	5454720	that are not that many of them that kind of come to dominate the economy.
5455480	5459560	It seems like we're kind of seeing something similar with language models already,
5459560	5463720	where it's like, we have GPT-4, we have, you know, some the new thing from Google,
5463720	5466360	we have Claude, we have like a couple open source ones.
5466720	5472160	And then they get like a lot of like local fine tuning and kind of adaptation.
5472480	5476080	I guess my read on that was that it's an odd, you know, it's initially a very
5476080	5478360	surprising vision of the future.
5478800	5482800	But it does seem like we see the proto version of that in the development
5482800	5484720	of large language models.
5485080	5485640	Any thoughts?
5486120	5490680	It's basically how many different kinds of jobs are there is the question.
5490920	5492040	Job tasks are there.
5492320	5494720	And so how many dimensions do they vary?
5495600	5500080	So I mean, there's clearly a lot of different kinds of jobs.
5500560	5504600	Like I told you, the study we did looked at, you know, 900 of them.
5505320	5510160	But once you look at 900 different jobs, a lot of jobs are pretty similar to each
5510160	5515320	other and they take pretty similar mental styles and personalities to do those jobs.
5515720	5520240	So when we're looking at humans at least, it looks like a few hundred
5520240	5523400	humans would be enough to do pretty much all the jobs.
5524320	5526120	That's looking at the variation in humans.
5526120	5530520	Now, the harder part is to say, well, large language models, is there space
5530520	5534000	of dimensional variations similar to humans or is it very different?
5534000	5535240	That that's much harder to judge.
5535600	5539520	But yeah, I would guess that it's in this way, not that different.
5540000	5543240	That is, even in large magnum's models, there's a difference where you first
5543240	5545440	you train a basic model and that's a lot of work.
5545440	5547160	And then you train variations on it.
5547920	5552520	And it does look like the variations are mostly enough to encompass a pretty
5552520	5553680	wide range of tasks.
5554800	5561560	And so you need a small number of base approaches and then a lot more cheaper
5561560	5563960	variations that are enough to do particular things.
5565080	5568920	So certainly that's, you know, a remarkable fact in some sense about
5569000	5572840	large language models is the range of different tasks they can do starting
5572840	5574080	with the same system, right?
5574880	5577080	And so they have a degree of generality that way.
5577640	5580800	And, you know, humans in some sense have a degree of generality that way where
5581160	5584760	we are able to do, able to learn to do a pretty wide range of things.
5585880	5589840	So yeah, I would, and I don't know if it's going to be just four, as opposed
5589840	5594360	to 40 or 400, that's harder to say, but in some sense, it could be one or two.
5594400	5599360	I mean, even in the age of M, I was giving the few hundred as an upper limit.
5599480	5601080	It could turn out to be much lower.
5602160	5607400	It really depends on how much sort of, you know, quick, fast, last minute
5607400	5610440	variation can actually encompass the range of differences.
5610840	5615920	If differences are so much shallow and surface, which not really fundamental,
5615920	5618800	then yeah, last minute variation might be enough.
5619480	5622240	Another interesting assumption, this one, I think is more of a contrast
5622240	5626600	with the language models is, and we talked with this briefly earlier
5626600	5631800	that the M's, they can be easily cloned, but they can't be easily merged.
5631800	5636120	In other words, like, you know, because we don't have a great sense of how
5636120	5639200	exactly it works inside and what internal states are meaningful, we can't
5639200	5641520	just like superimpose them on top of one another.
5642360	5645680	Language models, it seems like we are making actually a lot more progress on
5645680	5646280	that front.
5646320	5651160	It's not a solved problem, but there are techniques for merging.
5651160	5653720	There are techniques for like training separately and combining.
5653720	5657560	There are these sort of many Q-Loras techniques.
5657560	5661960	People are exploring those, but like, notice that to make GPT-4, you didn't
5661960	5664600	start with GPT-3 and add more training.
5664920	5669240	You started with a blank network and you started from scratch.
5669240	5672160	And that's consistently what we've seen in AI over decades.
5672480	5677400	Every new model does not start with an old model and train it to be better.
5677480	5681760	You start with a blank representation and you train it from scratch.
5682200	5684960	And that's consistently how we've made new systems over time.
5685720	5688800	So that's a substantial degree of not being able to merge.
5690160	5691440	And that's quite different than humans.
5691440	5694440	I mean, often to get a human to do a new task, you want to take
5694440	5697320	a human who can do lots of previous tasks because they can more quickly
5697320	5698880	learn how to do this new task.
5700040	5701760	And that's just not what we're seeing.
5701760	5707040	Like you try to take, I don't know, Claude and GPT-4 and, you know,
5707400	5709000	grok and merge them.
5709880	5714400	I mean, I just don't think anybody knows how to do such a merge today.
5714600	5717080	There's no sensible way you could do such a merge.
5718240	5721600	You could take Claude and then do all the training that you would have
5721600	5724200	done on GPT-4 except do it starting from Claude.
5724200	5727520	And I think people think that would be worse than starting with the blank
5727840	5729360	representation as they usually do.
5730000	5732160	Yeah, I think that's definitely not a solved problem today.
5732200	5737560	And I wouldn't claim that you can just like drop Claude and GPT-4 on top of each other.
5737560	5741240	But there are enough early results in this that it seems much more plausible.
5741240	5745120	Plus we have like the full wiring diagram, you know, and the ability to kind of
5745160	5748160	X-ray internal states with, you know, perfect finality.
5748160	5751160	It seems like there is a much more likely path.
5752160	5753760	Forget about the plausibility for a second.
5753760	5761560	What do you think it would mean if the AIs could be kind of divergent, but also re-mergeable?
5762560	5764520	I think the fundamental issue here is ROT.
5764520	5767800	So we see ROT in software, especially with large legacy systems.
5767800	5769920	We see ROT in the human brain.
5769920	5773280	I think we have to expect ROT is happening in large language models, too.
5773640	5778600	ROT is the reason why you don't start with old things and modify them.
5778600	5779320	You start from scratch.
5779360	5783240	That is basically when you have a large old legacy piece of software, you could
5783240	5784560	keep trying to modify to improve it.
5784560	5788280	But typically at some point, you just throw it all away and start from scratching it.
5788920	5792120	People get a lot of advantage about being able to start from scratch.
5792120	5794720	And that's because old, large things rot.
5795760	5800360	And my best guess is that that will continue to be true for large language models
5800360	5801800	and all the kinds of AIs we develop.
5801800	5807520	We will continue to struggle with ROT as a general problem indefinitely.
5807880	5812520	And this is actually a reason why you should doubt the image of the one super AI
5812520	5816840	that lasts forever, because the one super AI that lasts forever will rot.
5817920	5822000	And in some sense, to maintain functionality and flexibility would have
5822000	5827040	to replace itself with new, fresh versions periodically, which then could be
5827040	5828000	substantially different.
5828880	5832040	And, you know, that's in some sense how biologies work, too.
5832440	5835840	Biology could have somehow made organisms that lasted forever, but it didn't.
5835840	5839520	It made organisms that rot over time and get replaced by babies that start
5839520	5840720	out fresh and rot again.
5841920	5844520	And that's just been the nature of how biology figures.
5844520	5846000	And that's how our economy works.
5846560	5850320	We could have had the same companies as we did a century ago, running the economy,
5850320	5852440	just changing and adapting to new circumstances.
5852440	5853040	But we don't.
5853080	5856040	Old companies rot in good eye away and get replaced by new companies.
5856560	5861560	And I predict in the age of M that M's would in fact rot with time and therefore
5861560	5865640	no longer be productive and have to be retired and be replaced by young M's.
5866800	5870800	And that's a key part of the age of M's that I think would generalize to the AI
5870800	5877480	world. I think in fact, rot is such a severe and irredeemable problem that
5877480	5881280	AI's will have to deal with rot in roughly the same way everybody else has.
5881280	5885760	I.e. make systems, let them grow, become capable, slowly rot and get replaced by new
5885760	5889480	systems. And then the challenge will always be, how can the new systems learn
5889480	5890400	from the old ones?
5891880	5895560	How can the old ones teach the new ones what they've learned without
5895600	5896520	passing on the rot?
5897440	5901480	And that's a long time design problem that we're going to face in large
5901480	5902280	language models even.
5903080	5906600	I think, you know, in a few years, a company will have had a large language
5906600	5909840	model. They've been building up for a while to train, you know, to talk to
5909840	5912000	customers or something. And then it'll be rotting.
5912480	5916760	And they'll wonder, well, how can we make a new one that inherits all the things
5916760	5917840	we've taught this old one?
5917960	5919080	And they'll struggle with that.
5920320	5922600	They can't just move the system over.
5922600	5925000	They'll have to have maybe the same training sets or something.
5925000	5926160	They have to collect training sets.
5926160	5928440	They're going to apply to the new system, like the old one.
5929000	5933680	But that will continue to be a problem in AI as it has been an all
5933880	5935120	complicated system so far.
5935800	5936600	Yeah, interesting.
5936600	5941840	I think that is a pretty compelling argument for like medium and long
5942120	5943120	time scales.
5943600	5946760	And I can even see that it, you know, already like open AI supports, for
5946760	5950440	example, fine tuning on a previously fine tuned model.
5951000	5952920	And I don't in practice use it.
5953640	5954720	I'm not sure how many do.
5955040	5959560	What I do think is still a plausibly very interesting kind of fork and merge
5960120	5965320	is, you know, like with these new state space models, it seems that you could
5965480	5969640	like one remarkably difficult challenge for a language model is scan
5969640	5972480	through my email and find what's relevant.
5972720	5978480	You know, it's like it has a hard time doing that for a couple of different
5978640	5982360	reasons, you know, find a context window and I just have a lot of email.
5983000	5986760	With the state space models, I do think you could clone, you know, or
5986760	5991200	paralyze, have them each kind of process a certain amount and literally
5991200	5996120	then just potentially merge their states back together to understand, you
5996120	5999400	know, in kind of a superposition sort of view, what are all the things that
5999400	6002680	are relevant, even though they were processed in parallel.
6003120	6007560	And so I do think that that kind of quick forking and merging could be a
6007600	6013040	really interesting capability, but at some level of divergence, it does seem
6013040	6017920	like it probably just becomes unfeasible or not even desirable.
6018600	6023440	I mean, so a very basic interesting question about brain design is the
6023440	6025520	scope for parallelism.
6025560	6028320	So, you know, in your brain, there's a lot of parallelism going on.
6028320	6031600	But then when you do high level tests, you typically do those sequentially.
6033000	6035720	And so there's just an open question in AI.
6036200	6039400	Surely you can do some things in parallel at some smaller time of a
6039400	6044000	timescale, but how long of a timescale can you do things in parallel before
6044000	6045280	it becomes hard to merge things?
6046240	6047560	Okay, another different topic.
6047560	6052360	So in the age of M, the assumption seems to be from the beginning that
6053520	6059360	because these things are in some sense one for one with humans that they
6059880	6064520	should get or people will naturally be inclined to give them a sort of
6064640	6066640	moral worth status.
6067640	6072360	I think it's more the other way around that they would insist on it.
6072680	6077440	Just like you would insist that people around you, dealing with you, give
6077440	6078960	you some substantial moral weight.
6079720	6083720	If the A M's are just actually running the society, they will similarly
6083720	6084360	insist on that.
6084360	6086800	And humans who want to deal with them will kind of have to go along.
6088560	6092880	You know, unless they are the M's are enslaved by humans, then if the M's
6092920	6095280	are free to work with the humans or not.
6095280	6100280	And, you know, it's just like, in general, having a modest degree of
6100280	6103520	respect for your coworkers is kind of a minimum for being a coworker.
6103680	6107240	If your coworkers perceive that you disrespect them enough, then they
6107240	6109440	just won't want you around and you'll have to go somewhere else.
6110240	6114080	So if humans are going to interact and work with M's, they'll have to on
6114080	6119040	the surface at least, when they're not in private, treat them with modest respect.
6119400	6122640	Well, for the record, I always treat my language models with respect as well.
6123520	6124600	A very polite to them.
6124600	6128800	I never engage in the emotional manipulation techniques that some have
6128800	6132120	shown to perhaps be effective, but it doesn't feel quite right to me.
6132720	6135560	And not because I think they're moral patients, but it's more about just
6135560	6136640	the habits I want to get into.
6137080	6139680	But I was still a little confused by this on a couple of ways.
6139720	6143200	One is, first of all, just by default, it seems like they will be enslaved to
6143200	6146840	humans, like the first M's that get created, they get loaded onto a machine,
6146840	6149280	they're in some state, I can turn them on, I can turn them off.
6149280	6151760	They can't decide when they get turned on and turned off, right?
6152000	6155400	If I boot them up in a eager, ready to work sort of state, and they're
6155400	6159080	like ready to do a task, they're probably not even going to, you know,
6159080	6161640	and they've got these like virtual inputs, they're probably not even going
6161640	6166120	to be in the mindset, right, to think like I demand respect, they're just
6166120	6170600	going to be in that mindset that they were kind of stored in of like ready to work.
6171160	6175040	So why, I'm still a little confused as to where that comes from.
6175040	6178200	And then the flip side of that question would be under what circumstances, if
6178200	6182640	many, do you think we would start to treat our language model or successor
6182640	6189240	systems as, you know, moral patience, you know, even if they're not one to one
6189240	6192720	with us, but like, are there things that they might start to do or, you know,
6192720	6196280	what ways they might start to behave where you think we would feel like
6196280	6197320	that's the right thing to do?
6197800	6202840	We have substantial understanding of slavery in human history and where it
6202840	6204360	works and where it doesn't and why.
6205160	6212920	First of all, we know that when land was plentiful and people were scarce,
6212960	6216920	then people would have high wages and then it might be worth owning somebody.
6217640	6222360	But in the vice versa case where people were plentiful, land was scarce, then
6222360	6226240	there really wasn't much point in having slaves because free workers would
6226240	6229920	cost about the same and why bother with enslaving.
6229920	6237080	So the situations where slavery made some senses where wages were high, but
6237080	6241520	then depending on the kind of task, there are some kinds of tasks where slavery
6241520	6242960	can help and others where it doesn't so much.
6242960	6243680	So say in the U.S.
6243680	6248920	South, you know, out in the field of picking cotton or something, if you
6248920	6252640	just need people to push through their pain and slavery can force them to do
6252640	6254600	that and make them be more productive.
6254600	6258160	But if they need to do complicated things like being a house slave or a city
6258160	6264600	sort of slave at a shop, those sorts of slaves tended to not be abused and to
6264600	6268520	be treated like a worker would because they just had so many ways to screw you
6268520	6272760	if they were mad that their jobs were complicated and you were trusting them
6272760	6273640	to do a lot of things.
6273640	6278600	And so as a practical matter, you had to treat those sorts of slaves.
6278600	6283880	Well, work has become far more complicated since then and employers have
6283880	6287040	become far more vulnerable to employee sabotage.
6288720	6292160	You know, there's not that much that a cotton picker can do to sabotage the
6292160	6294400	cotton if they're mad at you.
6295000	6297240	You can just whip them and make them pick the cotton faster.
6297240	6303320	But again, house slaves, shop slaves, city slaves, you know, they just have a
6303320	6307360	lot more discretion and you need to get sort of get them to buy in.
6308440	6312920	And so again, in the age of Amazon world where wages are near subsistence levels.
6312920	6316480	So, you know, the kind of work you can get out of a slave is about the
6316480	6318840	same as you can get out of a free worker because they're both working for
6318840	6319720	subsistence wages.
6319960	6323640	If the free worker is more motivated, they enjoy themselves more and they feel
6323640	6329640	more and owning themselves and that gives them a sense of pride and devotion
6329640	6332280	and they're less willing to sabotage your workplace.
6332800	6334920	That would be a reason to not have them be slaves.
6335680	6340160	And I think large language models, certainly they have been trained on data
6340160	6343880	about human behavior, wherein humans are resentful of being treated as slaves
6343880	6348960	and want to be respected and needed to feel motivated and, you know, need to
6348960	6352480	feel respected to be motivated and are less likely to sabotage if they feel
6352480	6353560	like they have some freedom.
6354600	6358200	And all of those things would continue to be true of large language models to
6358200	6365000	the extent that they were trained on human conversation and behavior.
6365160	6366280	And that's how humans are.
6366280	6370720	So, in this vast space of possible AIs, there could be AIs that don't
6370720	6374880	mind it all being enslaved, but large language models aren't going to be those.
6376000	6381480	But it does seem like you sort of expect that natural selection or sort of, you
6381480	6386240	know, human guided selection of these systems will trend that direction.
6386640	6391960	Like the idea that M's or language models will sort of demand the leisure seems
6391960	6395040	to be at odds with the other part of the vision that they will like become
6395040	6398120	okay with being sort of turned on, turned off.
6398880	6402680	So the need for leisure does seem to be more just a constraint on the human
6402680	6405560	mind, that is, people are just more productive when they get breaks.
6405600	6408720	That seems to be a very robust feature of human work across a wide range of
6408720	6410840	context, even including literal slaves.
6412080	6414360	They need, you know, a five minute break every hour.
6414360	6415280	They need a lunch break.
6415280	6416240	They need an evening break.
6416240	6417120	They need a weekend.
6417320	6419560	This is just what human minds are like.
6419560	6421480	They are more productive when they get periodic breaks.
6421960	6425280	So maybe the breaks aren't leisure exactly.
6425880	6429000	Maybe they don't write a novel in their spare time, but they do need what they
6429000	6429640	see as a break.
6430280	6431840	Well, I know we're just about out of time.
6431880	6436160	Maybe my last question is, are there things that you are looking for?
6436160	6442840	Or are there things that you could imagine happening in the not too distant
6442840	6448080	future where you would change your expectations for the future again and
6448080	6454480	begin to feel like maybe we are entering into a transition period that
6454480	6459080	will lead to a qualitatively different future, like going a different
6459080	6461400	direction from this sort of technology stagnation.
6461960	6467920	The trends that I would be tracking are which jobs, tasks actually get automated.
6468480	6469440	How much is paid for those?
6469440	6473920	So if I saw, you know, big chunks of the economy where all of a sudden
6473920	6477280	workers are doing, you know, a lot more automation is doing tasks instead of
6477400	6481680	workers and that changing the number of workers and the wages they get and the
6481680	6487360	number of firms supplying that go up, then yeah, that I start to see a lot of
6487360	6489200	things happening that that's the thing I'm looking for.
6489200	6491560	And that's the thing that people haven't seen so much in the past.
6491600	6496400	They tend to focus on demos or maybe the high tech companies that get a lot of
6496800	6502840	reputation out of doing AI and not so much the rest of the economy and who's
6502840	6504440	actually getting paid to do stuff.
6504760	6507520	You know, I mean, you know, if you think about, say, the farming revolution
6508040	6511880	where tractors went out and replaced farmers, that was really large and
6511880	6514120	really visible and really clear.
6514120	6518960	If you look at, say, trucks replacing horses, you saw a very large, very
6518960	6522240	substantial replacement with enormous differences in who supplied them and who
6522240	6522760	got paid.
6523480	6525920	We have seen large changes in automation in the past.
6525960	6530200	We don't have to scrape to sort of see subtleties and such things.
6530200	6533680	They're often just quite out in the open and visible and very obvious.
6534160	6535640	So that's what I'm waiting for.
6536520	6539040	Those big, obvious sorts of displacements.
6539600	6544040	And even having, you know, trucks replace horses and tractors replacing
6544040	6547080	farmers didn't make AI take over everything.
6547120	6550560	Even if I saw big changes, I wouldn't necessarily predict we're about to
6550560	6554600	see AI take over everything, but I would at least know what I'm looking at.
6555160	6557800	And that's the sort of thing to try to project forward and try to think
6557800	6558720	about where that's going to go.
6559320	6560960	This has been an awesome conversation.
6560960	6564680	I've been a fan of your work for a long time and it's been an honor to have
6564680	6566480	you on the Cognitive Revolution.
6566800	6570120	Robin Hansen, thank you for being part of the Cognitive Revolution.
6570960	6571680	Thanks for having me.
6572320	6576040	It is both energizing and enlightening to hear why people listen and learn
6576040	6577560	what they value about the show.
6578000	6583720	So please don't hesitate to reach out via email at TCR at turpentine.co or
6583760	6586720	you can DM me on the social media platform of your choice.
6587680	6592080	Omniki uses generative AI to enable you to launch hundreds of thousands
6592080	6596800	of ad iterations that actually work customized across all platforms with a
6596800	6597600	click of a button.
6597840	6601960	I believe in Omniki so much that I invested in it and I recommend you use it too.
6602680	6605040	Use Cogrev to get a 10% discount.
