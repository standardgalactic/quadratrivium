WEBVTT

00:00.000 --> 00:02.560
We're on this upward growth trajectory.

00:02.560 --> 00:06.120
We have the potential to taking a big chunk of the universe

00:06.120 --> 00:07.640
and doing things with it.

00:07.640 --> 00:09.440
And I'm excited by that potential.

00:09.440 --> 00:11.680
So I want us to keep growing.

00:11.680 --> 00:15.200
And I see how much we've changed to get to where we are.

00:15.200 --> 00:17.600
My book, Age of M, is about brain emulations.

00:17.600 --> 00:20.200
So that's where you take a particular human brain

00:20.200 --> 00:22.560
and you scan it and find spatial chemical detail

00:22.560 --> 00:26.080
where you fill in for each cell a computer model of that cell.

00:26.080 --> 00:27.720
And if you've got good enough models for cells

00:27.720 --> 00:29.760
and a good map of the brain, then basically

00:29.760 --> 00:31.800
the IO of this model should be the same

00:31.800 --> 00:33.760
as the IO of the original brain.

00:33.760 --> 00:37.200
If we can get full human level AI in the next 16 to 90 years

00:37.200 --> 00:39.480
with the progress, then this population decline

00:39.480 --> 00:41.440
won't matter so much because we will basically

00:41.440 --> 00:43.400
have AIs take over most of the jobs

00:43.400 --> 00:46.560
and then that can allow the world economy to keep growing.

00:46.560 --> 00:48.920
Hello and welcome to the Cognitive Revolution,

00:48.920 --> 00:51.880
where we interview visionary researchers, entrepreneurs

00:51.880 --> 00:53.840
and builders working on the frontier

00:53.840 --> 00:55.760
of artificial intelligence.

00:55.760 --> 00:58.560
Each week, we'll explore their revolutionary ideas

00:58.680 --> 01:00.080
and together we'll build a picture

01:00.080 --> 01:03.640
of how AI technology will transform work, life

01:03.640 --> 01:05.800
and society in the coming years.

01:05.800 --> 01:09.400
I'm Nathan LaBenz, joined by my co-host Eric Torenberg.

01:09.400 --> 01:12.280
Hello and welcome back to the Cognitive Revolution.

01:12.280 --> 01:13.920
My guest today is Robin Hansen,

01:13.920 --> 01:16.320
Professor of Economics at George Mason University

01:16.320 --> 01:18.880
and author of the blog, Overcoming Bias,

01:18.880 --> 01:20.560
where Robin has published consistently

01:20.560 --> 01:23.880
on a wide range of topics since 2006

01:23.880 --> 01:26.320
and where Eliezer Yudkowski published early versions

01:26.360 --> 01:29.440
of what has become some of his most influential writing

01:29.440 --> 01:30.280
on AI.

01:31.840 --> 01:33.560
Robin is an undeniable polymath

01:33.560 --> 01:36.920
whose approach to futurism is unusually non-romantic.

01:38.000 --> 01:40.520
Rather than trying to identify value buddies,

01:40.520 --> 01:43.080
Robin aims to apply first principles thinking

01:43.080 --> 01:46.560
to the future and to describe what is likely to happen

01:46.560 --> 01:48.280
without claiming that you should feel

01:48.280 --> 01:50.000
any particular way about it.

01:51.360 --> 01:53.160
I set this conversation up late last year

01:53.160 --> 01:54.640
after my deep dive into the new

01:54.640 --> 01:57.320
Mamba states-based model architecture.

01:57.320 --> 02:00.720
Because Robin's 2016 book, The Age of M,

02:00.720 --> 02:03.280
which analyzes a scenario in which human emulations

02:03.280 --> 02:05.160
can be run on computers,

02:05.160 --> 02:07.280
suddenly seemed a lot more relevant.

02:08.240 --> 02:11.640
My plan originally was to consider how his analysis

02:11.640 --> 02:14.720
from The Age of M would compare to similar analyses

02:14.720 --> 02:17.240
for a hypothetical age of LLMs

02:17.240 --> 02:19.640
or perhaps even an age of SSMs.

02:20.560 --> 02:22.680
In practice, we ended up doing some of that,

02:22.680 --> 02:24.760
but for the most part took a different direction

02:24.760 --> 02:27.080
as it became clear early on in the conversation

02:27.080 --> 02:31.000
that Robin was not buying some of my core premises.

02:31.000 --> 02:33.200
Taking the outside view as he's famous for doing

02:33.200 --> 02:35.840
and noting that AI experts have repeatedly thought

02:35.840 --> 02:38.440
that they were close to AGI in the past,

02:38.440 --> 02:41.000
Robin questions whether this time really is different

02:41.000 --> 02:43.040
and doubts whether we are really close

02:43.040 --> 02:44.960
to transformative AI at all.

02:46.240 --> 02:48.840
This perspective naturally challenged my worldview

02:48.840 --> 02:50.960
and I listened back to this conversation in full

02:50.960 --> 02:53.280
to make sure that I wasn't missing anything important

02:53.280 --> 02:55.000
before writing this introduction.

02:56.360 --> 02:59.200
Ultimately, I do remain quite firmly convinced

02:59.200 --> 03:01.200
that today's AIs are powerful enough

03:01.200 --> 03:03.280
to drive economic transformation.

03:03.280 --> 03:06.200
And I would cite the release of Google's Gemini 1.5,

03:06.200 --> 03:07.960
which happened in just the few short weeks

03:07.960 --> 03:10.280
between recording and publishing this episode

03:10.280 --> 03:13.080
as evidence that progress is not yet slowing down.

03:14.200 --> 03:15.440
Yet at the same time,

03:15.440 --> 03:17.840
Robin did get me thinking more about the disconnect

03:17.840 --> 03:19.360
between feasibility

03:19.360 --> 03:22.920
and actual widespread implementation and automation.

03:23.960 --> 03:26.720
Beyond the question of what AI systems can do,

03:26.720 --> 03:29.640
there are also questions of legal regulation, of course,

03:29.640 --> 03:31.400
and perhaps even more importantly,

03:31.400 --> 03:34.840
just how eager people are to use AI tools in the first place.

03:36.040 --> 03:38.440
When Robin reported that his son's software firm

03:38.440 --> 03:41.480
had recently determined that LLMs were not useful

03:41.480 --> 03:43.480
for routine application development,

03:43.480 --> 03:45.200
I was honestly kind of shocked

03:45.200 --> 03:46.320
because if nothing else,

03:46.320 --> 03:48.080
I'm extremely confident about the degree

03:48.080 --> 03:50.920
to which LLMs accelerate my own programming work.

03:52.000 --> 03:54.560
Since then, though, I have heard a couple of other stories,

03:54.560 --> 03:56.080
which combined with Robbins,

03:56.080 --> 03:58.360
helped me develop, I think, a bit better theory

03:58.360 --> 04:00.160
of what's going on.

04:00.160 --> 04:04.320
First, an AI educator told me that failure to form new habits

04:04.320 --> 04:08.240
is the most common cause of failure with AI in general.

04:08.240 --> 04:12.000
In his courses, he emphasizes hands-on exercises

04:12.000 --> 04:14.200
because he's learned that simple awareness

04:14.240 --> 04:18.360
of AI capabilities does not lead to human behavioral change.

04:19.360 --> 04:21.480
Second, a friend told me that his company

04:21.480 --> 04:23.760
hosted a Microsoft GitHub salesperson

04:23.760 --> 04:25.280
for a lunch hour demo,

04:25.280 --> 04:27.640
and it turned out that one of their own team members

04:27.640 --> 04:30.440
had far more knowledge about GitHub Co-Pilot

04:30.440 --> 04:31.760
than the rep himself did.

04:32.880 --> 04:34.680
If Microsoft sales reps are struggling

04:34.680 --> 04:36.880
to keep up with Co-Pilot's capabilities,

04:36.880 --> 04:38.880
we should perhaps adjust our expectations

04:38.880 --> 04:40.280
for the rest of the economy.

04:41.400 --> 04:43.040
And third, in my own experience,

04:43.040 --> 04:46.200
helping people address process bottlenecks with AI,

04:46.200 --> 04:48.760
I've repeatedly seen how unnatural it can be

04:48.760 --> 04:50.880
for people to break their own work down

04:50.880 --> 04:52.640
into the sort of discrete tasks

04:52.640 --> 04:55.720
that LLMs can handle effectively today.

04:55.720 --> 04:58.040
Most people were never trained to think this way,

04:58.040 --> 04:59.280
and it's going to take time

04:59.280 --> 05:02.080
before it becomes common practice across the economy.

05:03.480 --> 05:06.360
All this means that change may be slower to materialize

05:06.360 --> 05:09.880
than those of us on the frontiers of AI adoption might expect.

05:09.880 --> 05:11.960
And while that does suggest more of an opportunity

05:11.960 --> 05:15.160
and indeed advantage for us in the meantime,

05:15.160 --> 05:18.040
on balance, I do have to view it as a negative sign

05:18.040 --> 05:21.280
about our preparedness and our ability to adapt overall.

05:22.600 --> 05:23.840
Regardless of your views,

05:23.840 --> 05:25.360
and I do suspect that most listeners

05:25.360 --> 05:28.520
will find themselves agreeing with me more than with Robin,

05:28.520 --> 05:30.600
his insights are always thought-provoking,

05:30.600 --> 05:32.760
and I think you'll find it very well worthwhile

05:32.760 --> 05:34.120
to engage with the challenges

05:34.120 --> 05:36.000
that he presents in this conversation.

05:37.440 --> 05:38.960
As always, if you're finding value in the show,

05:38.960 --> 05:41.080
we would appreciate it if you'd share it with friends,

05:41.080 --> 05:43.600
post a review on Apple Podcasts or Spotify,

05:43.600 --> 05:46.120
or just leave a comment on YouTube.

05:46.120 --> 05:48.360
And of course, I always love to hear from listeners,

05:48.360 --> 05:50.560
so please don't hesitate to DM me

05:50.560 --> 05:53.680
on the social media platform of your choice.

05:53.680 --> 05:55.640
Now, I hope you enjoy this conversation

05:55.640 --> 05:57.760
with Professor Robin Hansen.

05:58.880 --> 06:01.760
Robin Hansen, Professor of Economics at George Mason University

06:01.760 --> 06:04.920
and noted polymath, welcome to the cognitive revolution.

06:04.920 --> 06:06.360
Nice to meet you, Nathan.

06:06.360 --> 06:07.280
Let's talk.

06:07.280 --> 06:08.280
I'm excited about this.

06:08.320 --> 06:11.240
So I have followed your work for a long time.

06:11.240 --> 06:15.440
It's super wide-ranging and always very interesting.

06:15.440 --> 06:17.680
People can find your thoughts on just about everything,

06:17.680 --> 06:20.440
I think, over the years on overcoming bias, your blog.

06:20.440 --> 06:23.440
But today, I wanted to revisit what I think

06:23.440 --> 06:24.800
is one of your destined to be,

06:24.800 --> 06:26.360
perhaps one of your most influential works,

06:26.360 --> 06:28.800
which is the book, The Age of M,

06:28.800 --> 06:33.200
which came out in 2016 and Envisions a Future,

06:33.200 --> 06:37.120
which basically amounts to putting humans on machines,

06:37.120 --> 06:39.520
and we can unpack that in more detail,

06:39.520 --> 06:44.520
and then explores that in a ton of different directions.

06:44.920 --> 06:47.600
Where we actually are now as we enter into 2024

06:47.600 --> 06:50.040
is not exactly that, certainly,

06:50.040 --> 06:51.840
but I've come to believe recently

06:51.840 --> 06:54.720
that it's maybe bending back a little bit more toward that,

06:54.720 --> 06:57.040
certainly more than my expectations a year ago.

06:57.040 --> 06:59.320
So I've revisited the book,

06:59.320 --> 07:01.400
and I'm excited to bring a bunch of questions

07:01.400 --> 07:03.760
and kind of compare and contrast your scenario

07:03.760 --> 07:05.640
versus the current scenario

07:05.640 --> 07:07.960
that we seem to be evolving into.

07:07.960 --> 07:09.600
Okay, let's do it.

07:09.600 --> 07:12.480
One big theme of your work always, I think,

07:12.480 --> 07:16.520
is that we live in this strange dream time,

07:16.520 --> 07:21.040
and that our reality as modern humans is quite different

07:21.040 --> 07:23.200
than the reality of those that came before us

07:23.200 --> 07:24.760
and likely those that will come after us

07:24.760 --> 07:27.080
for some pretty fundamental reasons.

07:27.080 --> 07:28.240
Do you wanna just sketch out

07:28.240 --> 07:29.680
your kind of big picture argument

07:29.680 --> 07:32.240
that our times are exceptional

07:32.240 --> 07:35.360
and not likely to go on like this forever?

07:35.400 --> 07:37.000
The first thing to notice is that

07:37.000 --> 07:38.680
we were in a period of very rapid growth,

07:38.680 --> 07:40.400
very rapid change,

07:40.400 --> 07:42.920
which just can't continue for very long

07:42.920 --> 07:44.320
on a cosmological time scale.

07:44.320 --> 07:47.680
10,000 years would be way longer than it could manage,

07:47.680 --> 07:50.400
and therefore we're gonna have to go back

07:50.400 --> 07:52.880
to a period of slower change,

07:52.880 --> 07:54.880
and plausibly then a period of slower change

07:54.880 --> 07:58.640
will be a period where population can grow faster

07:58.640 --> 08:03.640
relative to the growth rate of the economy in the universe,

08:04.240 --> 08:05.600
and therefore we will move back

08:05.600 --> 08:08.280
more toward a Malthusian world

08:08.280 --> 08:10.160
if competition remains,

08:10.160 --> 08:12.120
such as almost all our ancestors were

08:12.120 --> 08:14.040
until a few hundred years ago.

08:14.040 --> 08:17.160
So we're in this unusual period of being rich

08:18.800 --> 08:21.760
per person and in very rapid change,

08:22.880 --> 08:25.920
and also sort of globally integrated.

08:25.920 --> 08:29.400
That is, our distant ancestors were fragmented culturally

08:29.400 --> 08:30.560
across the globe,

08:30.560 --> 08:33.880
and each talk to a small group of people near them,

08:33.880 --> 08:35.720
and our distant descendants will be fragmented

08:35.720 --> 08:37.240
across the universe,

08:37.240 --> 08:39.960
and they won't be able to talk all across the universe

08:39.960 --> 08:41.240
instantaneously.

08:41.240 --> 08:44.840
So future culture and past culture were both very fragmented,

08:44.840 --> 08:47.640
and we were in a period where our entire civilization

08:47.640 --> 08:50.040
can talk rapidly to each other.

08:50.040 --> 08:52.720
The time delay of communication is very small

08:52.720 --> 08:54.440
compared to the doubling time

08:54.440 --> 08:56.320
of our very rapid growth economy.

08:56.320 --> 08:59.200
So we are now an integrated civilization

08:59.200 --> 09:03.120
where rich growing very fast,

09:03.120 --> 09:05.560
and there's a number of consequences being rich,

09:05.560 --> 09:08.120
which is that we don't have to pay

09:08.120 --> 09:11.040
that much attention to functionality.

09:11.040 --> 09:14.160
Those were not pressured to do what it takes to survive

09:14.160 --> 09:17.200
in the way our ancestors and our descendants would be.

09:17.200 --> 09:20.360
So we can indulge our delusions,

09:20.360 --> 09:23.720
or whatever other inclinations we have,

09:23.720 --> 09:26.280
they aren't disciplined very rapidly

09:26.320 --> 09:29.600
by survival and functionality.

09:29.600 --> 09:31.960
That makes us a dream team.

09:31.960 --> 09:34.000
That is, our dreams drive us.

09:34.960 --> 09:38.840
Our abstract thoughts, our vague impressions,

09:38.840 --> 09:42.160
our emotions, our visions.

09:42.160 --> 09:47.160
We do things that are dramatic and exciting and meaningful

09:48.880 --> 09:53.880
in our view, according to this dream time mind we have,

09:54.320 --> 09:56.160
which isn't, again, that disciplined

09:56.160 --> 09:57.520
by functionality, that is,

09:57.520 --> 10:00.920
the mind we inherited from our distant ancestors,

10:00.920 --> 10:03.080
it was functional there, it was disciplined there,

10:03.080 --> 10:04.320
we're in a very different world,

10:04.320 --> 10:09.120
but our mind hasn't changed to be functional in this world.

10:09.120 --> 10:14.120
And so we are expressing this momentum

10:14.120 --> 10:17.400
of what we used to be in this strange new world.

10:17.400 --> 10:18.920
That's the dream time.

10:18.920 --> 10:21.440
So let me just try to rephrase that

10:21.440 --> 10:22.720
or frame it slightly differently

10:22.720 --> 10:24.880
and tell them if you agree with this framing.

10:24.880 --> 10:27.040
I would maybe interpret it as,

10:27.040 --> 10:29.800
we're maybe in a punctuated equilibrium sort of situation

10:29.800 --> 10:33.040
where we're in the transition from one equilibrium

10:33.040 --> 10:35.320
to another, there have probably been

10:35.320 --> 10:36.880
however many of these through history,

10:36.880 --> 10:38.440
not like a huge number, but a decent number,

10:38.440 --> 10:42.000
I think of such phrases as the Cambrian explosion,

10:42.000 --> 10:44.640
perhaps as another dream time.

10:44.640 --> 10:48.920
These moments happen when some external shock

10:48.920 --> 10:50.880
happens to the system, whether that's like an asteroid

10:50.880 --> 10:52.040
that takes out a lot of life,

10:52.040 --> 10:54.920
or human brains come on the scene,

10:54.920 --> 10:59.400
and there's a period in which the normal constraints

10:59.400 --> 11:02.960
are temporarily relaxed, but then in the long term,

11:02.960 --> 11:04.920
there's just like no escaping the logic

11:04.920 --> 11:05.960
of natural selection.

11:05.960 --> 11:08.560
Is that basically the framework?

11:08.560 --> 11:12.000
So your analogy of the Cambrian explosion could be,

11:12.000 --> 11:13.800
we discovered multicellularity,

11:13.800 --> 11:16.480
we discovered being able to make large animals,

11:16.480 --> 11:18.000
and that was happened at a moment,

11:18.000 --> 11:20.000
there was the moment of multicellularity,

11:20.000 --> 11:22.680
and then evolution took time to adapt

11:22.680 --> 11:25.040
to that new opportunity,

11:25.040 --> 11:28.080
and the Cambrian explosion is the period of adaptation,

11:28.080 --> 11:29.640
then after the Cambrian explosion,

11:29.640 --> 11:31.960
we've adapted to that new opportunity,

11:31.960 --> 11:33.440
and then we're more in a stasis,

11:33.440 --> 11:35.880
and then you're imagining this period of adaptation

11:35.880 --> 11:36.880
to a sudden change.

11:37.760 --> 11:42.760
But for humans today, we keep having sudden changes,

11:43.200 --> 11:45.040
and they keep coming fast,

11:45.040 --> 11:48.040
and so there wasn't this one thing that happened

11:48.040 --> 11:49.720
300 years ago or 10,000 years ago

11:49.720 --> 11:51.120
that we're slowly adapting to.

11:51.120 --> 11:53.560
We keep having more big changes

11:53.560 --> 11:55.720
that keep changing the landscape

11:55.720 --> 11:57.720
of what it is to adapt to,

11:57.720 --> 12:02.240
so we won't see this slow adaptation to the new thing

12:02.240 --> 12:04.000
until we get a stable new thing,

12:04.000 --> 12:05.080
which we haven't gotten yet.

12:05.080 --> 12:06.840
We, things keep changing.

12:06.840 --> 12:08.800
I wanna maybe circle back in a minute to

12:08.800 --> 12:11.040
what would be the conditions

12:11.040 --> 12:12.360
under which things would restabilize.

12:12.360 --> 12:15.040
I think I guess the M scenario is one of them,

12:15.040 --> 12:17.720
but there may be others that might even be

12:17.720 --> 12:19.160
more imminent at this point.

12:19.840 --> 12:20.680
Before doing that,

12:20.680 --> 12:22.400
I just wanted to touch on another big theme of your work,

12:22.400 --> 12:25.240
which is, and I really appreciate how you introduced

12:25.240 --> 12:29.400
the book this way with the idea that

12:29.400 --> 12:31.240
I'm just trying to figure out

12:31.240 --> 12:33.800
what is likely to happen in this scenario.

12:33.800 --> 12:35.680
I'm not telling you you should like it.

12:35.680 --> 12:37.200
I'm not telling you you should dislike it.

12:37.200 --> 12:38.480
I'm not trying to judge it.

12:38.480 --> 12:41.920
I'm just trying to extrapolate from a scenario

12:41.920 --> 12:44.920
using the tools of science and social science

12:44.920 --> 12:47.200
to try to figure out what might happen.

12:47.200 --> 12:49.520
I love that, and I try to do something similar

12:49.520 --> 12:52.240
with this show around understanding AI.

12:52.240 --> 12:55.600
I think there's so much emotional valence

12:55.600 --> 12:57.720
brought to so many parts of the discussion,

12:57.720 --> 13:01.440
and I always say, we need to first figure out what is,

13:01.440 --> 13:03.120
and even in the current moment,

13:03.120 --> 13:06.000
what capabilities exist, what can be done,

13:06.000 --> 13:08.400
what is still out of reach of current systems

13:08.400 --> 13:10.200
before we can really get serious

13:10.200 --> 13:13.440
about what ought to be done about it.

13:14.400 --> 13:16.160
I guess I'd invite you to add

13:16.160 --> 13:17.360
any additional perspective to that,

13:17.360 --> 13:18.400
and then I'm also curious,

13:18.400 --> 13:21.560
like, I think that's very admirable,

13:21.560 --> 13:24.560
but could you give us a little window

13:24.560 --> 13:27.120
into your own kind of biases or preferences?

13:27.120 --> 13:29.560
Like, what sort of world do you think

13:29.560 --> 13:30.880
we should be striving for,

13:30.880 --> 13:32.040
or do you think that's just so futile

13:32.040 --> 13:34.600
to even attempt to influence against these,

13:34.600 --> 13:37.800
you know, grand constraints that it doesn't matter?

13:37.800 --> 13:39.760
Hey, we'll continue our interview in a moment

13:39.760 --> 13:42.080
after a word from our sponsors.

13:42.080 --> 13:44.640
The Brave Search API brings affordable developer access

13:44.640 --> 13:46.160
to the Brave Search Index,

13:46.160 --> 13:47.520
an independent index of the web

13:47.520 --> 13:49.840
with over 20 billion web pages.

13:49.840 --> 13:52.640
So what makes the Brave Search Index stand out?

13:52.640 --> 13:56.360
One, it's entirely independent and built from scratch.

13:56.360 --> 13:59.920
That means no big tech biases or extortionate prices.

13:59.920 --> 14:03.760
Two, it's built on real page visits from actual humans,

14:03.760 --> 14:05.640
collected anonymously, of course,

14:05.640 --> 14:08.200
which filters out tons of junk data.

14:08.200 --> 14:10.000
And three, the index is refreshed

14:10.000 --> 14:12.080
with tens of millions of pages daily,

14:12.080 --> 14:15.400
so it always has accurate up-to-date information.

14:15.400 --> 14:18.400
The Brave Search API can be used to assemble a dataset

14:18.400 --> 14:20.000
to train your AI models

14:20.000 --> 14:21.720
and help with retrieval augmentation

14:21.720 --> 14:23.360
at the time of inference,

14:23.360 --> 14:24.640
all while remaining affordable

14:24.640 --> 14:26.880
with developer-first pricing.

14:26.880 --> 14:29.720
Integrating the Brave Search API into your workflow

14:29.720 --> 14:31.760
translates to more ethical data sourcing

14:31.760 --> 14:34.520
and more human-representative datasets.

14:34.520 --> 14:36.600
Try the Brave Search API for free

14:36.600 --> 14:40.800
for up to 2,000 queries per month at brave.com slash API.

14:46.400 --> 14:49.520
Pretty much all big, grand talk

14:50.800 --> 14:55.280
is mostly oriented around people sharing values.

14:56.280 --> 14:58.720
That's what people want to do when they talk big politics,

14:58.720 --> 15:01.800
when they talk world politics or world events,

15:01.800 --> 15:04.520
when they talk the future.

15:04.520 --> 15:08.640
People want to jump quickly to, do I share your values?

15:08.640 --> 15:09.680
Here's my values.

15:09.680 --> 15:10.520
What are your values?

15:10.520 --> 15:11.440
Do we agree on values?

15:11.440 --> 15:12.720
Are we value buddies?

15:13.720 --> 15:15.920
And people are so eager to get to that

15:15.920 --> 15:18.120
that they are willing to skip over

15:18.120 --> 15:21.040
the analysis of the details, say,

15:21.040 --> 15:22.240
if they want to talk about, I don't know,

15:22.240 --> 15:24.000
the war in Ukraine.

15:24.000 --> 15:25.840
People want to go, which side are you on?

15:25.840 --> 15:28.800
And who, you know, do we have the right values

15:28.800 --> 15:30.440
and then they don't care to talk about like,

15:30.440 --> 15:32.600
who has how much armaments that will run out soon

15:32.600 --> 15:34.520
or who can afford what or what they,

15:34.520 --> 15:35.960
you know, all those details of the war.

15:35.960 --> 15:37.400
They don't want to go there.

15:37.400 --> 15:40.560
They just want to go to the values and agree about it.

15:41.560 --> 15:43.200
And that happens in the future too,

15:43.200 --> 15:44.040
futurism too.

15:44.040 --> 15:45.120
People just want to jump to the values.

15:45.120 --> 15:47.880
So for the purposes people have,

15:47.880 --> 15:49.160
they're doing roughly the right thing.

15:49.160 --> 15:50.800
They don't really care about the world

15:50.800 --> 15:52.200
and they don't really care about the future.

15:52.200 --> 15:55.640
What they care about is finding value buddies

15:55.640 --> 15:59.280
or if you find a value conflict, having a value war.

15:59.280 --> 16:01.440
That's what people just want to do.

16:01.440 --> 16:05.080
And so if you actually want to figure out the world

16:05.080 --> 16:07.840
or national politics or national policy

16:07.840 --> 16:09.840
or you want to figure out the future,

16:09.920 --> 16:11.400
you really have to resist that

16:11.400 --> 16:15.840
and you have to try to pause and, you know,

16:15.840 --> 16:18.560
go through an analysis first, a neutral analysis

16:18.560 --> 16:20.680
of what the options are, what the situation is.

16:20.680 --> 16:24.680
I mean, I am afraid literally that if I express many values

16:24.680 --> 16:26.840
that the discussion will just go there

16:26.840 --> 16:28.600
and you'll never talk about anything else.

16:28.600 --> 16:31.640
And that's why I resist talking about that.

16:31.640 --> 16:34.040
But I think, you know, my simplest value

16:34.040 --> 16:36.760
with respect to the future is I really like the fact

16:36.760 --> 16:41.280
that humanity has grown and achieved vast things

16:41.280 --> 16:42.640
compared to where it started.

16:42.640 --> 16:45.160
We're on this upward growth trajectory.

16:45.160 --> 16:48.440
We have the potential to taking a big chunk of the universe

16:49.400 --> 16:50.880
and doing things with it.

16:50.880 --> 16:52.720
And I'm excited by that potential.

16:52.720 --> 16:56.720
So my first cut is I want us to keep growing.

16:56.720 --> 17:00.680
And I see how much we've changed to get to where we are.

17:00.680 --> 17:03.600
And I can see that had people from a million years ago

17:03.600 --> 17:06.920
insisted that their values be maintained

17:06.920 --> 17:10.160
and that the world be familiar and comfortable to them.

17:10.160 --> 17:12.160
If they've been able to enforce that,

17:12.160 --> 17:15.080
we would not have gotten where we are now.

17:15.080 --> 17:16.760
That would have prevented a lot of change.

17:16.760 --> 17:19.960
So I kind of see that if I want us to get big and grand,

17:19.960 --> 17:22.840
I'm gonna have to give a lot on

17:22.840 --> 17:26.560
how similar the future is to me and my world.

17:26.560 --> 17:28.520
I'm gonna have to compromise a lot on that.

17:28.520 --> 17:30.160
I just don't see any way around that.

17:30.160 --> 17:32.560
So I get it that if you want the future

17:32.560 --> 17:33.840
to be really comfortable for you

17:33.840 --> 17:36.320
and to share a lot of your values and your styles,

17:36.320 --> 17:39.320
you're gonna have to prevent it from changing.

17:39.320 --> 17:41.440
And you may have a shot at that.

17:41.440 --> 17:43.840
I would not like that, but you might.

17:43.840 --> 17:46.400
So again, even as part of the value framework,

17:46.400 --> 17:47.760
even when I talk values with you,

17:47.760 --> 17:50.320
I want to be clear to distinguish

17:50.320 --> 17:51.840
my value talk from the factual talk.

17:51.840 --> 17:53.880
I'm gonna be happy to tell you

17:53.880 --> 17:56.640
what it would take for you to get your values,

17:56.640 --> 17:58.600
even if they aren't mine.

17:58.600 --> 18:02.240
So maybe we should talk about the facts of LLMs.

18:02.240 --> 18:05.680
You wanna go there in terms of comparing Ms and LLMs, right?

18:05.680 --> 18:07.280
So first of all, our audience,

18:07.280 --> 18:09.120
we should say for our audience,

18:09.120 --> 18:12.240
my book Age of M is about brain emulations.

18:12.240 --> 18:14.840
So that's where you take a particular human brain

18:14.840 --> 18:17.720
and you scan it and find spatial chemical detail

18:17.720 --> 18:19.720
to figure out which cells are where,

18:19.720 --> 18:22.760
connected to other cells through what synapses.

18:22.760 --> 18:24.880
You make a map of that,

18:24.880 --> 18:28.960
and then you make a computer model that matches that map

18:29.960 --> 18:31.840
where you fill in for each cell,

18:31.840 --> 18:33.560
a computer model of that cell.

18:33.560 --> 18:35.160
And if you've got good enough models for cells

18:35.160 --> 18:36.360
and a good map of the brain,

18:36.360 --> 18:38.640
then basically the IO of this model

18:38.640 --> 18:40.680
should be the same as the IO of the original brain,

18:40.680 --> 18:41.680
which means you could hook it up

18:41.680 --> 18:44.400
with artificial eyes, ears, hands, mouth.

18:44.400 --> 18:45.880
And then it would behave the same

18:45.880 --> 18:49.440
as the original human would in the same situation,

18:49.440 --> 18:52.480
in which case you can use these as substitutes for humans

18:52.480 --> 18:54.080
throughout the entire economy.

18:54.080 --> 18:56.200
And then my exercise of the Age of M book

18:56.240 --> 18:59.520
was to figure out what that word looks like.

18:59.520 --> 19:03.520
And a primary purpose was to actually be able to show

19:03.520 --> 19:05.360
that it's possible to do that sort of thing.

19:05.360 --> 19:08.280
It's possible to take a specific technical assumption

19:08.280 --> 19:09.880
and work out a lot of consequences.

19:09.880 --> 19:13.680
And many people have said they didn't want so many details.

19:13.680 --> 19:15.280
They'd rather have fiction or something else,

19:15.280 --> 19:17.440
but I was trying to prove how much I could say.

19:17.440 --> 19:20.160
And I hope you'll admit, I proved I could say a lot.

19:21.400 --> 19:25.200
And that almost no other futurist work does that.

19:25.240 --> 19:27.160
And so I'm trying to inspire other futurists

19:27.160 --> 19:29.000
to get into that level of detail,

19:29.000 --> 19:30.520
to try to take some assumptions

19:30.520 --> 19:31.520
and work out a lot of consequences.

19:31.520 --> 19:33.800
So that's my book, The Age of M.

19:33.800 --> 19:36.320
You'd like us to compare that

19:36.320 --> 19:38.560
to current large-language models

19:39.680 --> 19:41.160
and to think about what we can say

19:41.160 --> 19:43.160
about the future of large-language models.

19:43.160 --> 19:47.280
So in my mind, the first thing to say there is,

19:47.280 --> 19:51.560
well, an M is a full human substitute.

19:51.560 --> 19:54.300
It can do everything a human can do, basically.

19:55.560 --> 19:57.840
A large-language model is not that yet.

19:59.440 --> 20:02.280
So a key question here would be,

20:02.280 --> 20:05.000
how far are we going to go in trying to imagine

20:05.000 --> 20:07.120
a descendant of a large-language model

20:07.120 --> 20:10.320
that is more capable of substituting

20:10.320 --> 20:12.700
for humans across a wide range of contexts?

20:12.700 --> 20:14.380
We stick with current large-language models.

20:14.380 --> 20:15.840
They're really only useful

20:15.840 --> 20:18.640
in a rather limited range of contexts.

20:18.640 --> 20:21.680
And so if you're gonna do forecasting of them,

20:21.680 --> 20:23.340
it's more like forecasting the future

20:23.340 --> 20:25.760
with a microwave oven or something.

20:25.760 --> 20:28.700
You think about, well, where can you use a microwave oven

20:28.700 --> 20:29.820
and how much will it cost

20:29.820 --> 20:33.420
and what other heating methods will it displace

20:34.420 --> 20:37.140
and what sort of inputs would be a compliment to that?

20:37.140 --> 20:39.220
It would be more of a small-scale,

20:39.220 --> 20:41.900
future forecasting exercise.

20:41.900 --> 20:45.340
Whereas The Age of M was purposely this very grand exercise

20:45.340 --> 20:48.720
because the M's actually change everything.

20:48.720 --> 20:51.380
Whereas most futurism, like if you're trying to analyze

20:51.420 --> 20:53.500
consequences of microwave oven,

20:53.500 --> 20:55.220
you have a much more limited scope

20:55.220 --> 20:58.420
because in fact, it'll have a limited impact.

20:58.420 --> 21:01.180
So that would be the question I have for you first,

21:01.180 --> 21:04.860
which is, are we gonna talk about the implications

21:04.860 --> 21:08.180
of something close to the current large-language models?

21:08.180 --> 21:11.980
Are we gonna try to imagine some generalized version

21:11.980 --> 21:14.860
of them that has much wider capabilities?

21:14.860 --> 21:16.180
Yeah, very good question.

21:16.180 --> 21:18.580
I think maybe two different levels of this

21:18.580 --> 21:20.740
would be instructive.

21:20.740 --> 21:22.180
One of the key things that jumps out

21:22.180 --> 21:24.540
and I think a lot of stuff flows from

21:24.540 --> 21:29.540
is the assumption that M's can be copied cheaply,

21:30.300 --> 21:33.940
paused and stored indefinitely cheaply,

21:33.940 --> 21:36.620
but not understood very well

21:36.620 --> 21:38.820
in terms of their internal mechanism.

21:38.820 --> 21:40.980
Very much like this similar understanding

21:40.980 --> 21:42.140
to what we have of the brain

21:42.140 --> 21:43.940
where we can kind of poke and prod at it a little bit,

21:43.940 --> 21:45.380
but we really don't have a deep understanding

21:45.380 --> 21:46.220
of how it works.

21:46.220 --> 21:49.420
We can't do like very localized optimizations,

21:49.420 --> 21:51.820
but we do have this like radical departure

21:51.820 --> 21:52.700
from the status quo,

21:52.700 --> 21:55.420
which is you can infinitely clone them,

21:55.420 --> 21:58.780
you can infinitely freeze and store them.

21:58.780 --> 22:00.860
And so this creates like all sorts of elasticities

22:00.860 --> 22:04.100
that just don't exist in the current environment.

22:04.100 --> 22:05.980
So a number of those features are gonna be general

22:05.980 --> 22:09.660
that anything that can be represented as computer files

22:09.660 --> 22:11.460
and run on a computer.

22:12.300 --> 22:14.500
So any form of artificial intelligence

22:14.500 --> 22:16.500
will be some of the sort in general

22:16.500 --> 22:18.780
that you could have a digital representation

22:18.780 --> 22:23.620
of archive it, make a copy of it, pause it,

22:23.620 --> 22:24.780
run it faster or slower,

22:24.780 --> 22:28.340
that's gonna be just generically true of any kind of AI,

22:28.340 --> 22:29.260
including M's.

22:30.740 --> 22:34.420
The ability to sort of modify it usefully,

22:34.420 --> 22:37.460
I mean, yes, with human brains initially,

22:37.460 --> 22:38.300
they're just a big mess,

22:38.300 --> 22:39.140
you don't understand them,

22:39.140 --> 22:41.380
but honestly, most legacy software systems

22:41.380 --> 22:42.220
are pretty similar.

22:43.300 --> 22:45.940
So today, large legacy software systems,

22:45.940 --> 22:48.380
you mostly have to take them as they are.

22:48.420 --> 22:50.980
You can only make modest modifications to them.

22:51.900 --> 22:53.980
That's close to what I'm assuming for M's.

22:53.980 --> 22:56.820
So I'm actually not assuming that they are that different

22:56.820 --> 22:59.700
from large legacy software systems.

22:59.700 --> 23:01.460
They're just a big mess

23:02.420 --> 23:04.940
that even though you could go look at any one piece

23:04.940 --> 23:06.020
and maybe understand it,

23:06.020 --> 23:07.820
that doesn't really help you usefully

23:07.820 --> 23:10.260
in modifying the entire thing.

23:10.260 --> 23:13.780
You basically have to take the whole thing as a unit

23:13.780 --> 23:17.380
and can only make some minor changes.

23:17.380 --> 23:18.660
But you can copy the whole thing,

23:18.660 --> 23:20.260
you can run it faster or slow,

23:20.260 --> 23:21.300
you can move it at speed,

23:21.300 --> 23:23.260
transfer at the speed of light around the earth

23:23.260 --> 23:25.060
or through the universe.

23:25.060 --> 23:28.300
Those things are true of pretty much any AI

23:28.300 --> 23:30.740
that could be represented as a computer file,

23:30.740 --> 23:31.860
run on a computer.

23:31.860 --> 23:35.300
Yeah, I think these dimensions are a really useful way

23:35.300 --> 23:37.020
to break this down.

23:37.020 --> 23:40.100
I took some inspiration from you in a presentation

23:40.100 --> 23:42.500
that I created called the AI Scouting Report,

23:42.500 --> 23:45.380
where I have the tail of the cognitive tape

23:45.420 --> 23:48.300
that compares human strengths and weaknesses

23:48.300 --> 23:50.500
to LLM strengths and weaknesses.

23:50.500 --> 23:52.980
And I think for the purposes of this discussion,

23:52.980 --> 23:55.740
maybe we might even have like four different kind of things

23:55.740 --> 23:56.580
to consider.

23:56.580 --> 23:58.900
One is humans, second would be M's,

23:58.900 --> 24:03.660
third is let's say transformer language models

24:03.660 --> 24:05.580
of the general class that we have today.

24:05.580 --> 24:09.140
Although I think we can predictably expect at a minimum

24:09.140 --> 24:12.740
that they will continue to have longer context windows

24:12.740 --> 24:15.780
and have generally more pre-training

24:15.780 --> 24:18.300
and generally more capability,

24:18.300 --> 24:21.820
at least within a certain range.

24:21.820 --> 24:23.700
And then the fourth one that I'm really interested in

24:23.700 --> 24:27.020
and has been kind of an obsession for me recently

24:27.020 --> 24:31.340
is the new state space model paradigm,

24:31.340 --> 24:35.540
which actually has some things now in common again

24:35.540 --> 24:36.740
with the humans and the M's

24:36.740 --> 24:39.580
that the transformer models lack.

24:39.580 --> 24:41.820
The state space models,

24:41.860 --> 24:43.980
this has been, of course, in a line of research

24:43.980 --> 24:45.180
that's been going on for a couple of years,

24:45.180 --> 24:47.740
kind of in parallel with transformers.

24:47.740 --> 24:50.020
Transformers have taken up the vast majority

24:50.020 --> 24:53.340
of the energy in the public focus

24:53.340 --> 24:55.460
because they have been the highest performing

24:55.460 --> 24:57.260
over the last couple of years.

24:57.260 --> 25:02.260
But that has maybe just changed with a couple of recent papers,

25:03.500 --> 25:05.580
most notably one called Mamba,

25:05.580 --> 25:10.140
that basically shows parity, rough parity

25:10.140 --> 25:12.340
with the transformer on kind of your standard

25:12.340 --> 25:14.300
language modeling tasks,

25:14.300 --> 25:17.620
but does have like a totally different architecture

25:17.620 --> 25:21.660
that I think opens up like some notably different strengths

25:21.660 --> 25:26.340
and weaknesses, whereas the transformer

25:26.340 --> 25:28.180
really just has the weights

25:28.180 --> 25:31.100
and then the sort of next token prediction,

25:31.100 --> 25:33.540
the state space model has this additional concept

25:33.540 --> 25:38.460
of the state, which is, and I recall from the book,

25:38.460 --> 25:41.620
you sort of say, taking an information processing lens

25:41.620 --> 25:45.900
to the human or where you spend more of your focuses

25:45.900 --> 25:49.820
on the M, you have the current state

25:49.820 --> 25:54.340
plus some new input information, sensory or whatever,

25:54.340 --> 25:58.340
and then that propagates into some action,

25:58.340 --> 26:02.020
some output and a new internal state.

26:02.020 --> 26:03.180
And that I think is really the heart

26:03.180 --> 26:05.300
of what the new state space models do

26:05.300 --> 26:07.540
is that they add that additional component

26:07.540 --> 26:09.580
where they have not only the weights,

26:09.580 --> 26:11.900
like a transformer has static weights,

26:11.900 --> 26:14.140
but they also have this state,

26:14.140 --> 26:18.100
which is of a fixed size, evolves through time,

26:18.100 --> 26:19.780
and is something that gets output

26:19.780 --> 26:21.980
at each kind of inference step

26:21.980 --> 26:23.780
so that there is this internal state

26:23.780 --> 26:26.220
that propagates through time

26:26.220 --> 26:29.060
and can kind of change and have long history.

26:29.900 --> 26:34.900
I think it is likely to bring about

26:34.940 --> 26:39.940
a much more integrated medium and long-term memory

26:40.020 --> 26:41.940
than the transformers have

26:41.940 --> 26:46.940
and create more sort of long episode conditioning

26:47.580 --> 26:51.340
where these models I think will be more amenable

26:51.340 --> 26:54.860
to like employee onboarding style training,

26:54.860 --> 26:56.900
which is something also that the M's have

26:56.900 --> 26:59.020
in your scenario, right?

26:59.020 --> 27:04.020
You can kind of train a base M to be an employee for you,

27:04.420 --> 27:06.740
you can even put it in that mental,

27:06.740 --> 27:07.740
get it to that mental state

27:07.740 --> 27:10.340
where it's like really excited and ready to work,

27:10.340 --> 27:12.500
and then you can freeze it, store it,

27:12.500 --> 27:14.060
boot it up when necessary,

27:14.060 --> 27:16.580
boot it up end times as necessary.

27:16.580 --> 27:20.580
The transformers don't really have that same feature right now,

27:20.580 --> 27:25.580
they're just kind of their monolithic base form at all times,

27:25.580 --> 27:30.380
but the state-state models start to add some of that back.

27:30.380 --> 27:32.500
Obviously, it's not gonna be one-to-one

27:32.500 --> 27:35.060
with the humans or the M's.

27:35.060 --> 27:38.700
Here's gonna be my problem with that number four.

27:38.700 --> 27:41.300
If I look at sort of the history of AI

27:41.300 --> 27:43.260
over the history of computers

27:43.260 --> 27:45.500
and even the history of automation before that,

27:46.620 --> 27:49.380
we see this history where a really wide range

27:49.380 --> 27:51.700
of approaches have been tried,

27:51.700 --> 27:53.580
a really wide range of paradigms

27:53.580 --> 27:57.500
and concepts and structures have been introduced.

27:57.500 --> 28:00.580
And over time, we've found ways in some sense

28:00.660 --> 28:03.420
to subsume prior structures within new ones,

28:04.500 --> 28:07.580
but we've just gone through a lot of them.

28:08.580 --> 28:11.180
And there's been this tendency, unfortunately,

28:11.180 --> 28:13.980
that when people reach the next new paradigm,

28:13.980 --> 28:17.500
the next new structure, they get really excited by it

28:17.500 --> 28:20.580
and they consistently say, are we almost done?

28:20.580 --> 28:22.620
They said that centuries ago,

28:22.620 --> 28:24.420
they said that half a century ago,

28:24.420 --> 28:28.660
every new decade, every new kind of approach

28:28.660 --> 28:30.100
that comes along,

28:30.900 --> 28:32.780
there's basically typically some demo,

28:32.780 --> 28:35.260
some new capability that this new system can do

28:35.260 --> 28:37.900
that none of the prior systems are able to do.

28:38.820 --> 28:41.940
And it's exciting and it's shocking even

28:41.940 --> 28:46.940
and exciting, but people consistently say,

28:46.940 --> 28:49.260
so we must be almost done, right?

28:49.260 --> 28:52.100
Like, surely this is enough to do everything

28:52.100 --> 28:54.220
and pretty soon humans will be displaced

28:54.220 --> 28:56.620
by automation based on this new approach.

28:56.620 --> 28:58.780
And that just happens over and over again.

28:59.660 --> 29:02.220
And so we've had enough of those that I got to say,

29:02.220 --> 29:04.660
the chance that the next exciting new paradigm

29:04.660 --> 29:08.660
is the last one we'll need is a prior pretty low.

29:09.660 --> 29:11.540
We've had this long road to go

29:11.540 --> 29:14.540
and we still have a long way to go ahead of us.

29:14.540 --> 29:16.380
And therefore, it's unlikely

29:16.380 --> 29:19.420
that the next new thing is the last thing.

29:19.420 --> 29:22.180
So that's my stance, I would think, okay,

29:22.180 --> 29:23.340
I can talk to you about LLMS

29:23.340 --> 29:25.420
because they're the latest thing.

29:25.420 --> 29:26.580
We can talk about LLMS,

29:26.580 --> 29:28.220
they're the latest thing.

29:28.220 --> 29:30.020
We can talk about what new things they can do

29:30.020 --> 29:32.260
and what exciting options

29:32.260 --> 29:34.180
that generates in the near future.

29:35.100 --> 29:36.580
And then we can ask, well,

29:36.580 --> 29:39.340
what's the chance it's the last thing we'll need?

29:39.340 --> 29:41.060
Or that the next one is the last thing we need.

29:41.060 --> 29:44.220
And so one way to cash that out is to ask,

29:44.220 --> 29:45.540
what do we think the chances are

29:45.540 --> 29:48.780
that within a decade or even two,

29:48.780 --> 29:51.300
basically all human jobs will be replaced

29:51.300 --> 29:54.940
by machines based on this new approach.

29:54.940 --> 29:57.940
And most of the forecasting that's done out there

29:57.940 --> 30:00.300
is excited about near-term progress in a lot of ways.

30:00.300 --> 30:01.540
But when you ask the question,

30:01.540 --> 30:03.780
when will most jobs be replaced?

30:03.780 --> 30:05.780
They give you forecasts that are way out there

30:05.780 --> 30:09.180
because they think, no, we're not close to that.

30:09.180 --> 30:10.700
And I don't think we're close to that.

30:10.700 --> 30:14.140
So then the question is,

30:14.140 --> 30:15.700
now we could say, what will happen

30:15.700 --> 30:17.380
when we eventually get to the point

30:17.380 --> 30:19.340
where AI is you're good enough to do everything?

30:19.340 --> 30:20.580
And we don't know what that approaches,

30:20.580 --> 30:22.180
but we can still talk about that point

30:22.540 --> 30:25.300
and what's likely to what the transition rate would be

30:25.300 --> 30:27.100
and the transition scenario

30:27.100 --> 30:30.580
and who would get rich and who would be unhappy

30:30.580 --> 30:33.100
and all the different things we could talk about there.

30:33.100 --> 30:35.780
But now we're talking about whatever approach

30:35.780 --> 30:40.100
eventually gets us past the being able to have

30:40.100 --> 30:42.460
to do pretty much all human tasks,

30:42.460 --> 30:43.980
which is not where we are now,

30:43.980 --> 30:45.620
or we can talk about where we are now

30:45.620 --> 30:46.900
and what these things can do

30:47.940 --> 30:51.660
and what exciting things might happen in the next decade.

30:52.380 --> 30:54.340
Hey, we'll continue our interview in a moment

30:54.340 --> 30:56.100
after a word from our sponsors.

30:56.100 --> 30:57.140
If you're a startup founder

30:57.140 --> 30:58.860
or executive running a growing business,

30:58.860 --> 31:01.460
you know that as you scale, your systems break down

31:01.460 --> 31:03.340
and the cracks start to show.

31:03.340 --> 31:04.500
If this resonates with you,

31:04.500 --> 31:06.100
there are three numbers you need to know,

31:06.100 --> 31:10.340
36,000, 25 and one, 36,000.

31:10.340 --> 31:11.380
That's the number of businesses

31:11.380 --> 31:13.220
which have upgraded to NetSuite by Oracle.

31:13.220 --> 31:15.300
NetSuite is the number one cloud financial system,

31:15.300 --> 31:17.300
streamlined accounting, financial management,

31:17.300 --> 31:20.380
inventory, HR and more, 25.

31:20.420 --> 31:22.220
NetSuite turns 25 this year.

31:22.220 --> 31:24.980
That's 25 years of helping businesses do more with less,

31:24.980 --> 31:28.620
close their books in days, not weeks and drive down costs.

31:28.620 --> 31:30.740
One, because your business is one of a kind,

31:30.740 --> 31:33.300
so you get a customized solution for all your KPIs

31:33.300 --> 31:35.780
in one efficient system with one source of truth.

31:35.780 --> 31:39.060
Manage risk, get reliable forecasts and improve margins,

31:39.060 --> 31:41.260
everything you need all in one place.

31:41.260 --> 31:44.620
Right now, download NetSuite's popular KPI checklist,

31:44.620 --> 31:46.980
designed to give you consistently excellent performance,

31:46.980 --> 31:50.300
absolutely free and netsuite.com slash cognitive.

31:50.300 --> 31:52.420
That's netsuite.com slash cognitive

31:52.420 --> 31:54.260
to get your own KPI checklist,

31:54.260 --> 31:56.140
netsuite.com slash cognitive.

31:57.300 --> 32:00.660
Omniki uses generative AI to enable you to launch

32:00.660 --> 32:04.020
hundreds of thousands of ad iterations that actually work,

32:04.020 --> 32:07.440
customized across all platforms with a click of a button.

32:07.440 --> 32:09.980
I believe in Omniki so much that I invested in it

32:09.980 --> 32:12.300
and I recommend you use it too.

32:12.300 --> 32:14.820
Use CogGrav to get a 10% discount.

32:14.820 --> 32:16.380
Well, I'm tempted by all of those options.

32:16.380 --> 32:19.940
So maybe for starters, I would be interested to hear

32:19.940 --> 32:24.940
how you would develop a cognitive tail of the tape

32:25.540 --> 32:28.980
between humans and M's by presumption

32:28.980 --> 32:30.840
have kind of the same cognitive abilities,

32:30.840 --> 32:33.900
but these kind of different external properties

32:33.900 --> 32:35.660
of copyability and so on.

32:35.660 --> 32:38.380
The large language model today,

32:38.380 --> 32:41.500
transformer, remarkably simple architecture,

32:41.500 --> 32:44.020
when you really just look at the wiring diagram,

32:44.020 --> 32:48.420
it's way simpler than the human brain is.

32:48.420 --> 32:52.540
And not shockingly, it can only do certain things

32:52.540 --> 32:55.020
that there's like really important traits

32:55.020 --> 33:00.020
that the human brain has that the language models don't have.

33:00.580 --> 33:04.020
I identified one of those as kind of integrated,

33:04.020 --> 33:08.220
ever-evolving, medium and long-term memory.

33:08.220 --> 33:10.180
I wonder what else you would kind of flag there.

33:10.180 --> 33:13.140
I don't know if you have a taxonomy of what are the kind

33:13.140 --> 33:16.180
of core competencies of humans that you could then say,

33:16.220 --> 33:20.260
oh, and here's the things that language models currently lack.

33:20.260 --> 33:21.940
I'm trying to develop something like this in general

33:21.940 --> 33:26.260
because it does seem to me that the large language models

33:26.260 --> 33:28.980
have hit not genius human level,

33:28.980 --> 33:31.700
but like closing in on expert human level

33:31.700 --> 33:35.220
at some very important, dare I say,

33:35.220 --> 33:39.420
even like core aspect of information processing, right?

33:39.420 --> 33:42.220
Like they can do things that I would say

33:42.220 --> 33:45.820
are qualitatively different than any earlier AI system

33:45.820 --> 33:47.140
could do.

33:47.140 --> 33:49.100
It certainly seems like we're getting close,

33:49.100 --> 33:50.420
whatever the last step is,

33:50.420 --> 33:52.500
we're definitely closer to it than we used to be.

33:52.500 --> 33:56.100
But just notice that phrase you just gave was true

33:56.100 --> 33:58.620
or most of all the previous ones as well.

33:58.620 --> 33:59.900
They could also do a thing

33:59.900 --> 34:02.760
that the previous ones before it couldn't do.

34:02.760 --> 34:04.140
It's always been exciting.

34:04.140 --> 34:06.140
We've found a new fundamental capability

34:06.140 --> 34:09.860
that each new paradigm structure approach

34:09.860 --> 34:12.180
has been of this sort

34:12.180 --> 34:14.540
that it was allowed the system to do fundamental things

34:14.740 --> 34:15.820
that it couldn't do before

34:15.820 --> 34:19.060
that seemed to be near the core of what it was to think.

34:20.020 --> 34:22.100
So there's apparently a lot of things near the core

34:22.100 --> 34:23.260
of what it is to think.

34:23.260 --> 34:25.020
That's the key thing to realize.

34:25.020 --> 34:26.780
What it is to think is a big thing.

34:26.780 --> 34:28.700
There's a lot of things in there.

34:28.700 --> 34:29.540
Well, let's list some.

34:29.540 --> 34:32.180
I can't come up with that many honestly.

34:32.180 --> 34:35.740
Like I would love to hear how many can you name

34:35.740 --> 34:36.600
I have all day.

34:36.600 --> 34:38.980
So could you begin to break down

34:38.980 --> 34:42.100
what it is to think into key components?

34:42.100 --> 34:45.860
I was an AI researcher from 84 to 93.

34:45.860 --> 34:49.740
That was a full time at NASA and then Lockheed.

34:49.740 --> 34:51.620
And certainly at that time,

34:51.620 --> 34:54.540
I understood the range of approaches people had

34:54.540 --> 34:57.540
and could talk about the kinds of things systems

34:57.540 --> 35:01.420
then could do or not do and expert terms relating

35:01.420 --> 35:06.020
to the then current tasks and issues.

35:06.020 --> 35:08.260
I am not up to date at the moment

35:08.260 --> 35:10.620
on the full range of AI approaches.

35:10.900 --> 35:14.860
I don't wanna pretend to be an expert on that.

35:14.860 --> 35:17.780
But I have listened to experts

35:17.780 --> 35:22.140
and the experts I hear basically consistently say,

35:22.140 --> 35:23.540
this is exciting, this is great,

35:23.540 --> 35:26.860
but we're not close to being able to do all the other things

35:26.860 --> 35:29.340
and they would be much better than I am making a list of that

35:29.340 --> 35:31.300
and I feel like they should make the list, not me.

35:31.300 --> 35:33.580
I mean, as a polymath you call me,

35:33.580 --> 35:36.820
I wanna be very careful to know when I'm an expert

35:36.820 --> 35:38.420
on something and when I'm not.

35:38.420 --> 35:40.740
And I wanna defer to other people on areas

35:40.740 --> 35:43.020
where I can find people who know more than I.

35:43.020 --> 35:45.620
And when I think I'm near the state of the art,

35:45.620 --> 35:47.100
as good as anyone on a topic,

35:47.100 --> 35:50.380
then I will feel more free to generate my own thoughts

35:50.380 --> 35:52.580
and think they're worth contributing.

35:52.580 --> 35:55.540
Fair, certainly, I think most people

35:55.540 --> 36:00.220
where I think you do still bring something very differentiated

36:00.220 --> 36:05.220
to the discussion is just the sort of willingness

36:06.220 --> 36:10.180
to stare reality in the face or at least try to.

36:10.180 --> 36:12.180
The simplest thing is if I start talking

36:12.180 --> 36:13.620
to an out large language model,

36:13.620 --> 36:15.380
there's a whole bunch of things I can ask it to do

36:15.380 --> 36:16.740
that it just can't do.

36:17.900 --> 36:19.540
I'm not so sure how to organize that

36:19.540 --> 36:21.380
in terms of the large major categories,

36:21.380 --> 36:24.820
but it's really obvious that there's a certain kind of thinking

36:24.820 --> 36:28.060
it can do and a bunch of other kind of thinking it can't do.

36:28.060 --> 36:30.660
And I don't know exactly why it can't do them,

36:30.660 --> 36:33.060
but I'm talking to you, there's a bunch of things

36:33.060 --> 36:34.620
I could ask you to do in this conversation

36:35.180 --> 36:37.340
that you would probably do a decent job of them.

36:37.340 --> 36:39.540
And then if I were talking to the large language model,

36:39.540 --> 36:41.780
it just couldn't do those things.

36:41.780 --> 36:43.300
So it's just really obvious to me

36:43.300 --> 36:44.940
that this has a limited capability.

36:44.940 --> 36:47.180
It's really impressive compared to what you might have expected

36:47.180 --> 36:49.100
five or 10 years ago, it's, wow,

36:49.100 --> 36:51.380
I never would have thought that would be feasible this soon,

36:51.380 --> 36:55.180
but you just try asking it a bunch of other things

36:55.180 --> 36:57.020
and it just can't do them, right?

36:57.020 --> 37:02.020
Yeah, I mean, I think that in my view,

37:02.780 --> 37:07.780
a lot of those things are kind of overemphasized

37:08.940 --> 37:12.180
relative to what maybe really matters.

37:12.180 --> 37:15.740
You see a lot of things online where people,

37:15.740 --> 37:17.540
and there's different categories of this,

37:17.540 --> 37:19.460
some of the things you'll see online

37:19.460 --> 37:23.260
are literally people just using non frontier models

37:23.260 --> 37:25.580
and kind of confusing, muddying the water.

37:25.580 --> 37:27.340
So always watch out for that.

37:27.340 --> 37:31.140
I have a longstanding practice of,

37:31.180 --> 37:32.580
first thing I do when I see somebody say,

37:32.580 --> 37:35.500
GPT-4 can't do something is try it myself.

37:35.500 --> 37:38.940
And I would honestly say like two thirds of the time,

37:38.940 --> 37:40.660
it's just straight up misinformation

37:40.660 --> 37:42.260
and it in fact, like can do it.

37:42.260 --> 37:45.180
But there's still the one third of the time that matters.

37:45.180 --> 37:47.260
They're not very adversarially robust.

37:47.260 --> 37:48.300
They're easy to trick,

37:48.300 --> 37:52.140
they're easy to sort of get on the wrong track.

37:52.140 --> 37:56.580
And then they seem to get kind of stuck in a mode

37:56.580 --> 37:58.020
is a good term for it, I think,

37:58.020 --> 38:01.180
where once they're kind of on a certain,

38:01.180 --> 38:03.580
this is kind of how they can often get jailbroken.

38:03.580 --> 38:04.780
If you can get them to say like,

38:04.780 --> 38:06.940
okay, I'll be happy to help you with that,

38:06.940 --> 38:10.020
then they'll go on and do whatever you asked

38:10.020 --> 38:12.380
because they've already kind of got into that mode.

38:12.380 --> 38:13.700
Yeah, I'm much less worried about them

38:13.700 --> 38:15.100
doing things you don't want them to do

38:15.100 --> 38:18.020
than being able to get them to do things at all.

38:18.020 --> 38:21.620
That as humans can be made to do all sorts of things,

38:21.620 --> 38:22.820
you might not want them to do that.

38:22.820 --> 38:23.820
We survive that.

38:24.780 --> 38:26.580
I mean, to me, the main thing is,

38:26.580 --> 38:28.700
if you imagine, you know,

38:28.700 --> 38:31.680
treating a large language model as a new employee

38:31.680 --> 38:33.460
in some workplace where you're trying to show them

38:33.460 --> 38:36.300
how to do something and get them to do it instead of you,

38:36.300 --> 38:38.660
that's the main thing that will be economically valuable

38:38.660 --> 38:39.500
in the world.

38:39.500 --> 38:41.220
That is, when you have a thing like that

38:41.220 --> 38:44.380
that can be introduced into a place,

38:44.380 --> 38:46.340
trained roughly and said, watch how I do this,

38:46.340 --> 38:48.980
you try to do it now, et cetera,

38:48.980 --> 38:51.420
then that will be the thing that, you know,

38:51.460 --> 38:53.860
makes an enormous difference in the economy

38:53.860 --> 38:57.500
because that's how we get people to do things, right?

38:57.500 --> 38:59.820
So if that I think is, in a sense,

38:59.820 --> 39:02.500
the fundamental main task in the economy,

39:02.500 --> 39:05.420
which is a bunch of people are doing something,

39:05.420 --> 39:07.300
you have a new thing and you say,

39:07.300 --> 39:09.660
would you Kim watch us and ask us questions

39:09.660 --> 39:12.020
and we'll ask you questions and like figure out

39:12.020 --> 39:15.300
how to help us and be part of what we're doing.

39:15.300 --> 39:17.260
That is the fundamental problem in the economy.

39:17.260 --> 39:20.060
So that in some sense is the fundamental task

39:20.060 --> 39:23.020
that any AI has to be held up to.

39:23.020 --> 39:24.820
I mean, in the past, of course,

39:24.820 --> 39:26.300
we don't even bother to have a conversation

39:26.300 --> 39:28.380
to show you how to do, we actually say,

39:28.380 --> 39:29.860
well, let's make a machine to do this thing

39:29.860 --> 39:31.340
and then we design a machine to do this thing

39:31.340 --> 39:32.900
and then we train it up to do this thing

39:32.900 --> 39:34.500
all with the idea of the whole thing,

39:34.500 --> 39:37.460
having in mind the thing we're gonna have to do.

39:37.460 --> 39:40.460
That's how AI has been usually in the economy so far.

39:40.460 --> 39:41.620
But now if you're imagining a thing

39:41.620 --> 39:44.020
that could just be trained to do a new job,

39:44.020 --> 39:45.220
well, that would be great.

39:46.420 --> 39:49.260
Sure, then we won't have to design the AI ahead of time

39:49.260 --> 39:50.980
for the particular task,

39:50.980 --> 39:52.940
but you'll have to have a thing that's up to that

39:52.940 --> 39:54.300
and large language models today

39:54.300 --> 39:56.860
are just clearly not up to that.

39:56.860 --> 39:58.180
You can't say, I'm about to train you

39:58.180 --> 39:59.780
how to do the following thing, pay attention,

39:59.780 --> 40:01.940
I just did this, now would you do it?

40:01.940 --> 40:03.940
Well, you can do that quite a bit, right?

40:03.940 --> 40:06.780
I mean, that was the main kind of finding in GPT-3

40:06.780 --> 40:09.300
was, I'm not sure if I have this verbatim,

40:09.300 --> 40:11.780
but the title of that paper was large language models

40:11.780 --> 40:13.940
are few shot learners.

40:13.940 --> 40:17.220
And the big kind of breakthrough observation there,

40:17.220 --> 40:18.580
which I don't think they designed,

40:19.580 --> 40:21.980
there's a whole quagmire of what should count

40:21.980 --> 40:23.540
as emergent or not emergent,

40:23.540 --> 40:26.700
but my understanding is they didn't specifically train

40:26.700 --> 40:30.740
for this few shot imitation capability,

40:30.740 --> 40:33.660
but they nevertheless got to the point where

40:33.660 --> 40:38.020
at runtime today, you can give a few examples

40:38.020 --> 40:38.860
of what you want.

40:38.860 --> 40:40.500
And in fact, that is like a best practice

40:40.500 --> 40:43.140
that open AI and anthropic recommend

40:43.140 --> 40:45.260
for how to get the most from their systems.

40:45.260 --> 40:47.580
They'll say, some things are hard,

40:47.580 --> 40:49.380
they also have now trained them to follow instructions,

40:49.380 --> 40:52.180
just verbatim or explicitly,

40:52.180 --> 40:54.940
but they will still say that,

40:54.940 --> 40:57.780
some things are better shown by example

40:57.780 --> 41:01.660
than described in terms of what to do.

41:01.660 --> 41:05.620
So do that, and you'll get like a lot better performance.

41:05.620 --> 41:09.540
It seems to me that there is on that kind of watch,

41:09.540 --> 41:11.820
watch it to borrow from medicine,

41:11.820 --> 41:14.180
watch one, do one, teach one,

41:14.180 --> 41:17.820
it seems like we're on the do one step,

41:17.820 --> 41:22.580
and that does seem to be a pretty qualitative threshold

41:22.580 --> 41:23.420
that has been passed.

41:23.420 --> 41:26.180
Now, they obviously can continue to get better at that.

41:26.180 --> 41:28.020
Right, but it's the range of things they can do

41:28.020 --> 41:29.100
that's the question.

41:29.100 --> 41:31.020
Yes, it's great that they can,

41:31.020 --> 41:33.420
you can say, here's some examples, give me another one,

41:33.420 --> 41:36.340
but the range of things you can do that for is limited.

41:36.340 --> 41:38.340
Most people in most jobs,

41:38.340 --> 41:41.420
they couldn't have large language model swap in

41:41.420 --> 41:43.980
for many of their main tasks that way.

41:44.940 --> 41:46.500
But there are some and that's exciting,

41:46.500 --> 41:48.980
and I hope to see people develop that and improve it.

41:48.980 --> 41:51.380
But again, the key question is how close are we

41:51.380 --> 41:54.740
to the end of this long path we've been on for a while?

41:54.740 --> 41:56.460
Yeah, I guess I think about it a little bit differently

41:56.460 --> 41:59.060
in terms of rather than thinking about the end of the path,

41:59.060 --> 42:03.780
I think of how close are we to key thresholds

42:03.780 --> 42:08.300
that will bring in qualitatively different dynamics

42:08.300 --> 42:12.420
relative to the current situation.

42:12.620 --> 42:16.020
So one threshold that I think has recently been passed

42:16.020 --> 42:18.180
and in a pretty striking way that this is,

42:18.180 --> 42:20.900
should get more discussion than it does in my view

42:20.900 --> 42:23.580
is Google DeepMind just put out a paper

42:23.580 --> 42:28.580
not long ago where they showed basically a two to one

42:29.300 --> 42:31.740
advantage for a large language model

42:31.740 --> 42:36.300
in medical diagnosis versus human doctors.

42:36.300 --> 42:38.540
And then of course they also compared to human plus AI

42:38.540 --> 42:39.820
and that was in the middle.

42:39.820 --> 42:42.300
So on these cases that they lined up

42:42.300 --> 42:45.420
in the scenario is like you're chatting with your doctor,

42:45.420 --> 42:48.820
60% accuracy from the language model,

42:48.820 --> 42:52.020
30% accuracy from the human.

42:52.020 --> 42:55.620
I was an AI from 83 to 94.

42:56.940 --> 43:00.220
And at the beginning, one of the reasons I came into AI

43:00.220 --> 43:02.940
was there were these big journal articles

43:02.940 --> 43:05.220
and national media coverage about studies

43:05.220 --> 43:07.940
where they showed that the best AI of the time

43:07.940 --> 43:09.820
which they called expert systems

43:09.820 --> 43:14.020
were able to do human level medical diagnosis.

43:14.020 --> 43:17.380
This was in the early 1980s, right?

43:17.380 --> 43:19.040
We're talking 40 years ago.

43:19.920 --> 43:21.940
And obviously the computer capacity

43:21.940 --> 43:23.180
is vastly larger than that.

43:23.180 --> 43:26.220
So either they were lying back then

43:26.220 --> 43:28.620
and messing with the data

43:28.620 --> 43:33.020
or they did have human level diagnosis back then

43:33.020 --> 43:34.700
but they weren't allowed to apply it

43:34.700 --> 43:36.300
because of medical licensing.

43:37.180 --> 43:39.900
So, and we're still not allowed to apply it

43:39.900 --> 43:42.220
because of medical licensing.

43:42.220 --> 43:46.140
So, this is exactly the sort of ability

43:46.140 --> 43:49.420
that won't give substantial economic impact

43:49.420 --> 43:51.300
because we had it 40 years ago

43:51.300 --> 43:52.940
and it didn't have an impact then.

43:52.940 --> 43:54.220
Yeah, I don't know.

43:54.220 --> 43:57.700
So if I had, I think one qualitative difference

43:57.700 --> 43:59.780
between that earlier system and this system

43:59.780 --> 44:01.220
which won't come to be an expert

44:01.220 --> 44:02.740
in the earlier expert systems

44:02.740 --> 44:07.740
but I would guess that a huge difference

44:08.220 --> 44:13.100
is that you can take today a totally uninitiated person

44:13.100 --> 44:15.340
who has a medical concern

44:15.340 --> 44:17.940
and say, sit in front of this computer,

44:17.940 --> 44:19.540
talk to this doctor.

44:19.540 --> 44:21.760
They don't even need to know as an AI doctor.

44:21.760 --> 44:23.460
They can just talk to him.

44:23.460 --> 44:25.700
That wasn't the problem back then.

44:25.700 --> 44:27.940
They could have made these expert systems

44:27.940 --> 44:31.220
usable by ordinary people with modest effort.

44:31.220 --> 44:33.180
That wasn't the problem in using them.

44:33.180 --> 44:35.780
The problem was just you're not legally allowed to use them.

44:35.780 --> 44:38.700
Only doctors are allowed to give medical diagnoses.

44:38.700 --> 44:40.940
And so only doctors are allowed to use these systems

44:40.940 --> 44:41.780
to talk to people.

44:41.780 --> 44:44.660
That was the main obstacle and it still is today.

44:44.660 --> 44:48.300
The obstacle, you could make such a system today

44:48.300 --> 44:49.460
that ordinary people could talk to

44:49.460 --> 44:51.060
but they're not allowed to talk to it

44:51.060 --> 44:54.220
and they won't be allowed to talk to it for a long time.

44:54.220 --> 44:55.860
I think there is a qualitative difference

44:55.860 --> 44:57.060
between these systems.

44:57.060 --> 44:59.180
If I were to sit down in front of the early 80s thing

44:59.220 --> 45:01.580
and I were to say, what's different today

45:01.580 --> 45:03.820
is the chat system could say,

45:03.820 --> 45:05.060
Robin, tell me how you're feeling.

45:05.060 --> 45:06.100
Tell me about your experience.

45:06.100 --> 45:08.060
And you can just go on in your own language,

45:08.060 --> 45:09.780
however you want to express yourself,

45:09.780 --> 45:10.780
and it can get you.

45:10.780 --> 45:13.140
And then it can ask you specific follow up

45:13.140 --> 45:14.420
but you're not going through a wizard

45:14.420 --> 45:16.940
and going down an expert system tree

45:16.940 --> 45:19.940
and ask for numeric scores you don't understand

45:19.940 --> 45:20.780
and don't know.

45:20.780 --> 45:22.620
You can literally just express yourself.

45:22.620 --> 45:23.820
That was not there then, right?

45:23.820 --> 45:25.100
I mean, nothing.

45:25.100 --> 45:28.100
But that's not the limiting factor, right?

45:28.100 --> 45:30.700
I mean, you couldn't have a fancy graphics interface

45:30.700 --> 45:31.540
back then either.

45:31.540 --> 45:33.940
This was early 1980s, right?

45:33.940 --> 45:37.460
But again, the limiting factor is the legal barrier.

45:38.340 --> 45:39.740
It was back then and still is

45:39.740 --> 45:41.580
and that legal barrier doesn't look like

45:41.580 --> 45:43.180
it's about to go away.

45:43.180 --> 45:45.940
So if you're gonna make us excited about applications

45:45.940 --> 45:48.540
it'll have to be something that's legal.

45:48.540 --> 45:52.340
My model of this is that the consumer surplus

45:52.340 --> 45:55.500
of this type of thing is going to be so great.

45:55.500 --> 45:56.860
It already was 40 years ago.

45:56.900 --> 45:58.420
It would have been a huge consumer surplus

45:58.420 --> 46:00.620
40 years ago, it was not allowed.

46:00.620 --> 46:03.140
But there was never a groundswell of, I don't know.

46:03.140 --> 46:04.180
I'm just not buying this.

46:04.180 --> 46:06.300
I'm not buying that there was an experience

46:06.300 --> 46:09.100
that is qualitatively like the one that we have today

46:09.100 --> 46:13.260
such that I think today if you show people what Google has

46:13.260 --> 46:16.420
they will say it is not acceptable to me

46:16.420 --> 46:19.900
that you keep this locked up behind some payroll.

46:19.900 --> 46:22.820
I don't think that was the general consumer reaction

46:22.820 --> 46:25.620
to early 80s expert systems.

46:25.660 --> 46:29.580
And it seems like that political economy pressure

46:29.580 --> 46:31.180
could change things.

46:31.180 --> 46:33.740
Consider the analogy of nuclear power.

46:33.740 --> 46:36.100
The world has definitely been convinced for a long time

46:36.100 --> 46:38.620
that nuclear power is powerful.

46:39.460 --> 46:42.720
It is full of potential and power.

46:42.720 --> 46:44.620
And if we had let it go wild

46:44.620 --> 46:46.540
we would have vastly cheaper energy today

46:46.540 --> 46:48.500
but it was that power that scared people

46:48.500 --> 46:51.140
which is why we don't have that energy today.

46:51.140 --> 46:54.580
The very vision of nuclear energy being powerful

46:54.580 --> 46:57.180
is what caused us not to have it.

46:58.540 --> 47:01.620
We over-regulated it to death

47:01.620 --> 47:04.140
and we made sure that the power of nuclear power

47:04.140 --> 47:05.660
was not released.

47:05.660 --> 47:07.180
We believed the power was there.

47:07.180 --> 47:09.620
It was not at all an issue of not believing

47:09.620 --> 47:10.900
that nuclear power was powerful.

47:10.900 --> 47:13.340
It was believing it was too powerful.

47:13.340 --> 47:15.860
Scary, dangerous, powerful.

47:15.860 --> 47:18.880
And there's a risk that we'll do that with AI today.

47:19.840 --> 47:21.500
We will make people believe it's powerful,

47:21.500 --> 47:23.620
so powerful that they should be scared of it

47:23.620 --> 47:24.980
and it should be locked down

47:24.980 --> 47:27.620
and not released into the wild

47:27.620 --> 47:29.820
where it might do us terrible danger.

47:29.820 --> 47:31.500
Yeah, well, that's certainly a tragic outcome

47:31.500 --> 47:33.900
in the case of the nuclear power.

47:33.900 --> 47:35.980
And I think it would also be a tragic outcome

47:35.980 --> 47:38.900
if people are denied their AI doctors

47:38.900 --> 47:43.220
of the future on that basis.

47:43.220 --> 47:44.540
And it could happen.

47:44.540 --> 47:47.220
I certainly wouldn't rule out the possibility that

47:47.220 --> 47:49.620
just AI research probably gets made illegal.

47:49.620 --> 47:51.780
This time we do have, I mean, again, it is,

47:51.780 --> 47:53.180
I do think we're in a different regime now

47:53.220 --> 47:57.180
where enough has been discovered

47:57.180 --> 47:59.580
and enough has been put into the hands of millions.

47:59.580 --> 48:04.260
There is sort of the open source kind of hacker level.

48:04.260 --> 48:06.100
Not medical diagnosis is not.

48:06.100 --> 48:08.460
We have not put medical diagnosis AI

48:08.460 --> 48:10.460
in the hands of ordinary people.

48:10.460 --> 48:12.380
And if you tried it, you would find out

48:12.380 --> 48:14.500
just how quickly you'd get slapped now.

48:14.500 --> 48:16.300
Yeah, I think I know someone who actually may be

48:16.300 --> 48:18.540
about to try this and it'll be very interesting

48:18.540 --> 48:21.660
to see how quickly and how hard they get slapped down

48:21.940 --> 48:23.580
and how they may respond from it.

48:23.580 --> 48:26.140
I've actually been very encouraged by the response

48:26.140 --> 48:27.980
from the medical community.

48:28.820 --> 48:30.940
I would say, obviously it's not a monolithic thing,

48:30.940 --> 48:34.380
but I did an earlier episode with Zach Kahane,

48:34.380 --> 48:37.460
who is a professor at Harvard Medical School

48:37.460 --> 48:40.340
and who had early access to GPT-4.

48:40.340 --> 48:41.780
He came out with a book basically

48:41.780 --> 48:44.860
to coincide with the launch of GPT-4

48:44.860 --> 48:48.540
called GPT-4 and the Revolution in Medicine.

48:48.540 --> 48:53.540
And broadly, I have been encouraged by how much

48:53.900 --> 48:57.180
the medical establishment has seemingly been inclined

48:57.180 --> 48:59.580
to embrace this sort of stuff.

48:59.580 --> 49:01.180
I don't know if it's just that they're also

49:01.180 --> 49:03.220
overworked these days or...

49:03.220 --> 49:05.460
Well, they'll embrace the internal use of it.

49:05.460 --> 49:09.500
Again, it's always been doctors allowed to use these things.

49:09.500 --> 49:11.220
And the main reason they didn't get more popular

49:11.220 --> 49:13.540
is doctors couldn't be bothered to type in

49:13.540 --> 49:15.420
and input all the information

49:15.420 --> 49:18.820
because they want to have short meetings with patients.

49:18.820 --> 49:21.460
Even today, of course, if you've gone to a modern doctor,

49:21.460 --> 49:23.260
most of your meeting with a doctor

49:23.260 --> 49:26.700
is them typing in information to their computer

49:26.700 --> 49:28.220
as they talk to you.

49:28.220 --> 49:30.620
And they don't wanna spend much more time

49:30.620 --> 49:31.940
typing in more.

49:31.940 --> 49:34.960
And so they don't wanna use computer aids

49:34.960 --> 49:36.980
in their diagnosis and that's been true for a long time.

49:36.980 --> 49:39.220
They, computer diagnosis aids have been available

49:39.220 --> 49:41.940
for a long time that would give them better diagnoses

49:41.940 --> 49:44.180
at the cost of them having to spend more time with them

49:44.260 --> 49:45.700
than they've chosen not to spend more time.

49:45.700 --> 49:47.580
That's been true for many decades now.

49:47.580 --> 49:52.180
Have you personally used GPT-4 for any advanced things

49:52.180 --> 49:55.420
like this, medical or legal advice or whatever?

49:55.420 --> 49:57.300
No, I'm an economics professor.

49:57.300 --> 49:59.540
So I've used it to check to see what my students

49:59.540 --> 50:01.660
might try to use it to answer my exam questions

50:01.660 --> 50:03.960
or essay questions or things like that.

50:03.960 --> 50:06.140
I've asked it things that I wanted to know

50:06.140 --> 50:08.260
and try to check on them.

50:08.260 --> 50:11.900
I haven't used it for legal or medical questions.

50:11.900 --> 50:13.940
Those are areas which are heavily regulated.

50:13.940 --> 50:16.020
It's always been possible for other people

50:16.020 --> 50:17.900
to offer substitutes.

50:17.900 --> 50:20.060
So for example, many decades ago,

50:20.060 --> 50:22.220
there were experiments where we,

50:22.220 --> 50:25.220
basically for the purpose of general practice for doctors,

50:25.220 --> 50:27.440
we compare doctors to nurses,

50:27.440 --> 50:28.980
nurse practitioners or paramedics.

50:28.980 --> 50:31.140
We found that those other groups did just as well

50:31.140 --> 50:33.740
and much cheaper at doing the first level

50:33.740 --> 50:36.980
of general practice, but they haven't been allowed.

50:36.980 --> 50:39.200
So that right there is enormous value

50:39.200 --> 50:40.140
that could have been released.

50:40.140 --> 50:42.700
We could have all this time been having nurse practitioners

50:42.700 --> 50:45.940
and doctors and paramedics do our first level

50:45.940 --> 50:47.380
of general practice medicine.

50:47.380 --> 50:49.700
And they would save at least a factor of two or three

50:49.700 --> 50:51.940
in cost and that's been true for decades.

50:51.940 --> 50:54.420
We've had randomized experiments showing that for decades.

50:54.420 --> 50:57.100
So going back to the age of M then for a second,

50:57.100 --> 51:01.180
are you just assuming that that scenario doesn't happen

51:01.180 --> 51:04.020
in M land for some reason?

51:04.020 --> 51:07.580
Or like, why wouldn't it be the first objection

51:07.580 --> 51:10.380
to the age of M seems like it maybe should be,

51:10.380 --> 51:11.420
M's will be made illegal.

51:11.420 --> 51:13.020
Nobody will be allowed to do it.

51:13.020 --> 51:14.220
Absolutely.

51:14.220 --> 51:17.140
And basically you're just kind of in the analysis saying,

51:17.140 --> 51:18.500
well, let's just assume that doesn't happen

51:18.500 --> 51:20.580
because it'll be, you know, it's a short book

51:20.580 --> 51:22.280
if they just get made illegal too early.

51:22.280 --> 51:23.700
Is that the idea?

51:23.700 --> 51:25.740
Well, so first of all,

51:25.740 --> 51:28.560
I say transitions are harder to analyze

51:28.560 --> 51:30.700
than equilibria of New World.

51:30.700 --> 51:34.140
So I try to avoid analyzing the transition.

51:34.140 --> 51:36.460
Although I do try to discuss it some toward the end

51:36.460 --> 51:38.500
of the book, but I admit,

51:38.500 --> 51:40.860
I can just say less about a transition.

51:40.860 --> 51:43.460
It does seem like that, you know,

51:43.460 --> 51:45.940
compared to a scenario where everyone eagerly adopted

51:45.940 --> 51:48.060
M technology as soon as it was available,

51:48.060 --> 51:50.540
more likely there will be resistance.

51:50.540 --> 51:53.180
There will be ways in which there are obstacles

51:53.180 --> 51:55.860
to M technology early on.

51:55.860 --> 51:57.300
And therefore at some point,

51:57.300 --> 51:58.980
there would basically be the, you know,

51:58.980 --> 52:01.900
breaking of a dam flooding out where a bunch of things

52:01.900 --> 52:04.560
that had been held back were released

52:04.560 --> 52:07.100
and then caused a lot of disruption,

52:07.100 --> 52:08.620
faster disruption that would have happened

52:08.620 --> 52:12.300
had you adopted things as soon as they were available.

52:12.300 --> 52:13.140
And that's part of,

52:13.140 --> 52:16.300
that can be very disturbing transition then, you know,

52:16.300 --> 52:18.620
if all of a sudden large numbers of people

52:18.620 --> 52:21.420
are disrupted in ways they weren't expecting

52:21.420 --> 52:24.500
in a very rapid way because of, you know,

52:24.500 --> 52:26.620
a dam suddenly broke open,

52:26.620 --> 52:30.440
then I think there will be a lot of unhappy people

52:30.440 --> 52:31.560
in that sort of a transition

52:31.560 --> 52:33.020
and maybe a lot of dead people.

52:33.020 --> 52:35.620
So imagine the M technology slowly just gets cheaper

52:35.620 --> 52:38.600
over time, but it's not very wide.

52:38.600 --> 52:40.440
It's not very widely adopted.

52:40.440 --> 52:44.080
Then there'll be a point at which it eventually gets so cheap

52:44.080 --> 52:46.320
that if some say ambitious nation,

52:46.320 --> 52:48.040
like say North Korea said,

52:48.040 --> 52:51.120
gee, if we went whole hog and adopting this thing,

52:51.120 --> 52:52.960
we could get this big, you know,

52:52.960 --> 52:55.920
economic and military advantage over our competitors,

52:55.920 --> 52:58.920
then eventually somebody would do that.

52:58.920 --> 53:00.880
Now it might take a long time.

53:01.800 --> 53:04.680
That is the world could coordinate to resistance technology

53:04.680 --> 53:07.160
for a long time,

53:07.160 --> 53:10.880
but I don't think they could hold it back for a thousand years.

53:10.880 --> 53:12.120
So then I feel somewhat confident,

53:12.120 --> 53:13.960
eventually in the age of M happens,

53:14.920 --> 53:17.360
and then eventually there's a thing to think about

53:17.360 --> 53:18.680
and then I'm analyzing that world.

53:18.680 --> 53:21.520
So I don't want to presume in the age of M

53:21.520 --> 53:24.040
that this transition happens smoothly or soon

53:24.040 --> 53:26.440
or as fast as it could,

53:26.440 --> 53:28.360
but I want to say eventually there'll be this new world

53:28.360 --> 53:29.680
and here's how it would play out.

53:29.680 --> 53:32.200
So I don't know if you know that in the last few months

53:33.360 --> 53:36.160
I've dramatically changed my vision of the future

53:36.160 --> 53:38.040
to say that there's probably gonna be

53:38.040 --> 53:40.560
a several century innovation pause,

53:40.560 --> 53:43.000
probably before the age of M happens,

53:43.000 --> 53:45.840
and then the world that would eventually produce AI

53:45.840 --> 53:48.040
and M's would be a very different world from ours

53:48.040 --> 53:51.400
and somewhat hard to think about.

53:51.400 --> 53:55.720
That is rising population will stop rising

53:55.720 --> 53:58.080
and it will fall due to falling fertility,

53:58.080 --> 54:00.600
that will basically make innovation grind to a halt,

54:00.600 --> 54:03.200
then the world population will continue to fall

54:03.200 --> 54:08.280
until insular fertile subcultures like the Amish

54:08.280 --> 54:10.120
grow from their very small current levels

54:10.120 --> 54:12.960
to become the dominant population of the world.

54:12.960 --> 54:14.880
And then when that becomes large enough

54:14.880 --> 54:16.520
compared to our current economy,

54:16.520 --> 54:18.160
then innovation would turn on again

54:18.160 --> 54:21.560
and then we would restart the AI and M path

54:21.560 --> 54:24.840
and then eventually the age of M would happen.

54:24.840 --> 54:27.000
Trying to anticipate how transitions would happen

54:27.000 --> 54:29.760
in a world we can just hardly even imagine,

54:29.760 --> 54:30.960
seems tough, right?

54:31.000 --> 54:33.960
That is, okay, imagine the descendants of the Amish

54:33.960 --> 54:37.600
become a large, powerful civilization.

54:37.600 --> 54:40.600
They've always been somewhat resistant to technology

54:40.600 --> 54:43.720
and very picky about which technologies they're allowed,

54:43.720 --> 54:45.640
but eventually I would predict

54:46.840 --> 54:48.320
there would be competition within them

54:48.320 --> 54:53.320
and that would push them to adopt technologies like AI and M's

54:53.320 --> 54:56.280
but we're looking a long way down the line.

54:56.280 --> 54:58.240
And this isn't what I wish would happen

54:58.240 --> 54:59.280
to go back to your initial thing.

54:59.280 --> 55:01.720
I would rather we continued growing

55:01.720 --> 55:03.240
at the rate of the past century

55:03.240 --> 55:05.760
and continue that for a few more centuries,

55:05.760 --> 55:08.000
by which time I'm pretty sure

55:08.000 --> 55:10.800
we'll eventually get M's and human level AI,

55:10.800 --> 55:13.160
although question in what order,

55:13.160 --> 55:15.480
but I got to say at the moment,

55:15.480 --> 55:17.400
that's not looking so good.

55:17.400 --> 55:21.360
So basically, I'm estimated that if we were to continue

55:21.360 --> 55:25.400
on a steady growth path, we would eventually reach a point

55:25.400 --> 55:26.960
where we had the same amount of innovation

55:26.960 --> 55:28.720
as we will get over the entire integral

55:28.760 --> 55:30.200
of this several centuries pause.

55:30.200 --> 55:33.040
And I've estimated that to be roughly 60 to 90 years

55:33.040 --> 55:33.880
worth of progress.

55:33.880 --> 55:36.960
So if we can get full human level AI

55:36.960 --> 55:39.320
in the next 60 to 90 years with the progress,

55:39.320 --> 55:41.680
then this population decline won't matter so much

55:41.680 --> 55:44.800
because we will basically have AI's takeover most of the jobs

55:44.800 --> 55:48.640
and then that can allow the world economy to keep growing.

55:48.640 --> 55:51.840
I think that's iffy whether we can do that,

55:51.840 --> 55:56.160
whether we can achieve full human level AI in 60 to 90 years.

55:56.160 --> 55:57.920
And I know many people think it's gonna happen

55:57.920 --> 55:59.520
in the next 10 years, they're sure.

55:59.520 --> 56:01.840
So sure, of course it'll happen in 60 to 90 years,

56:01.840 --> 56:04.480
but I look at the history and I go,

56:04.480 --> 56:06.720
look, I've seen over and over again,

56:06.720 --> 56:10.600
people get really excited by the next new kind of AI.

56:10.600 --> 56:13.200
And they're typically pretty sure,

56:13.200 --> 56:15.360
a lot of them are pretty sure that we must be near the end

56:15.360 --> 56:18.040
and pretty soon we'll have it all.

56:18.040 --> 56:20.840
And it just keeps not happening.

56:20.840 --> 56:24.680
The main change I wanna suggest to that paradigm

56:24.840 --> 56:29.840
is replacing the end with meaningful thresholds along the way.

56:30.960 --> 56:33.360
I think there are probably several

56:33.360 --> 56:38.000
that we will hit on some time scale.

56:38.000 --> 56:40.640
And it feels to me like,

56:40.640 --> 56:44.520
at least a couple of the big ones are pretty close.

56:44.520 --> 56:48.000
And then at the end is very,

56:48.000 --> 56:49.400
my crystal ball gets very foggy

56:49.400 --> 56:51.720
beyond like a pretty short time scale.

56:51.720 --> 56:55.280
But I'm struggling with the early 80s expert systems,

56:55.280 --> 56:57.840
but it really does seem like in my lifetime,

56:57.840 --> 57:02.080
I have not seen anything that remotely resembles

57:02.080 --> 57:04.080
the experience of going to a doctor.

57:04.080 --> 57:08.120
I've seen WebMD, I'm familiar with expert systems

57:08.120 --> 57:10.680
to a degree, but I've never seen anything that,

57:10.680 --> 57:13.800
I didn't think Ilya Setsgaver from OpenAI

57:13.800 --> 57:16.320
puts this really well, he's like the most shocking thing

57:16.320 --> 57:19.600
about the current AIs is that I can speak to them

57:19.600 --> 57:21.680
and I feel that I'm understood.

57:21.680 --> 57:25.080
And that is like a qualitatively different experience.

57:25.080 --> 57:29.520
And clearly I think reflects some qualitative advance

57:29.520 --> 57:33.920
in terms of what kind of information processing is going on.

57:33.920 --> 57:36.840
If I had to say like, what is that under the hood?

57:36.840 --> 57:41.840
I would say it's like a high dimensional representation

57:42.440 --> 57:47.360
of concepts that are like really relevant to us

57:47.400 --> 57:50.880
that have previously been kind of limited

57:50.880 --> 57:53.800
to like language level compressed encoding.

57:53.800 --> 57:55.880
But now we are actually starting to get to the point

57:55.880 --> 57:58.240
where we can like look at the middle layers

57:58.240 --> 58:00.360
of even just the systems we have today,

58:00.360 --> 58:02.120
the transformers and say,

58:02.120 --> 58:07.120
can we identify concepts like positivity

58:07.960 --> 58:11.280
or paranoia or love?

58:11.280 --> 58:13.600
And we are starting to be able to,

58:13.600 --> 58:14.960
it's still pretty messy.

58:14.960 --> 58:16.400
We have the same, not the same,

58:16.400 --> 58:19.480
we have an analogous problem to like understanding

58:19.480 --> 58:20.880
what's going on inside the brain

58:20.880 --> 58:23.720
and it's just a mess in there still in the transformers.

58:23.720 --> 58:25.080
But we are starting to be able to see these

58:25.080 --> 58:29.120
like high dimensional representations where it's like,

58:29.120 --> 58:32.280
that is a numeric representation

58:32.280 --> 58:33.640
of some of these big concepts.

58:33.640 --> 58:35.080
And we're even starting to get to the point

58:35.080 --> 58:37.960
where we can steer the language model behavior

58:37.960 --> 58:40.000
by like injecting these concepts.

58:40.000 --> 58:45.000
So you can say, for example, inject safety

58:45.000 --> 58:46.800
into the middle layers of a transformer

58:46.800 --> 58:51.800
and get a safer response or danger or rule breaking

58:51.840 --> 58:54.200
and then they'll be more likely to break their rules.

58:54.200 --> 58:56.360
What you're focused on at the moment is telling me

58:56.360 --> 58:59.800
about how the latest generation adds capabilities

58:59.800 --> 59:02.120
that previous generations didn't have.

59:02.120 --> 59:04.920
But every previous generation had that same conversation

59:04.920 --> 59:07.160
where they focused on the new capabilities

59:07.160 --> 59:10.600
their new generation had that the ones before it didn't have.

59:10.600 --> 59:13.000
What the conversation you're participating in

59:13.000 --> 59:15.960
is continuing the past trend.

59:17.200 --> 59:19.440
But the fundamental question is,

59:19.440 --> 59:24.440
when will AIs be able to do what fraction of the tasks

59:25.200 --> 59:27.620
that we have in the human economy,

59:27.620 --> 59:29.320
if they can't do a large fraction of them,

59:29.320 --> 59:31.480
no matter how impressive they are at the practice

59:31.480 --> 59:34.720
they can do, we will see this economic decline

59:34.720 --> 59:36.240
as the population declines.

59:36.240 --> 59:39.120
They need to be able to do pretty much all the tasks

59:39.120 --> 59:41.440
in order to prevent the economic decline

59:41.440 --> 59:43.200
and then the halting of innovation.

59:43.200 --> 59:45.760
I did this study of innovation in the United States

59:45.760 --> 59:50.200
over 20 years from 1999 to 2019.

59:50.200 --> 59:51.880
And that was a period that encompassed

59:51.880 --> 59:55.640
what many people at time said was enormous AI progress.

59:55.640 --> 59:58.560
And many people in the period were talking about

59:58.560 --> 01:00:02.200
how there was this revolution in AI

01:00:02.200 --> 01:00:05.640
that was about to cause a revolution in society

01:00:07.240 --> 01:00:10.040
in this period from 1999 to 2019.

01:00:10.040 --> 01:00:14.280
So we did a study, a co-author and I,

01:00:14.280 --> 01:00:18.120
Keller Scholl, who looked at all jobs in the US,

01:00:18.120 --> 01:00:22.040
basically roughly 900 different kinds of jobs.

01:00:22.040 --> 01:00:23.760
And over that 20-year period,

01:00:23.760 --> 01:00:28.200
we had measures of how automated was each job in each year.

01:00:29.960 --> 01:00:34.400
And then we could do statistics to say,

01:00:34.400 --> 01:00:35.920
when jobs got more automated,

01:00:35.920 --> 01:00:38.160
did they get the wages go up or down?

01:00:38.160 --> 01:00:40.920
Did the number of workers in those jobs go up or down?

01:00:40.920 --> 01:00:43.200
And we could say, what about jobs predicts

01:00:43.200 --> 01:00:44.480
how automated they are?

01:00:45.400 --> 01:00:48.320
And did the things that determine which jobs

01:00:48.320 --> 01:00:50.760
or how automated change over that 20-year period?

01:00:52.520 --> 01:00:54.080
That is, if there had been some revolution

01:00:54.080 --> 01:00:55.520
in the nature of automation,

01:00:55.520 --> 01:00:57.480
then the things that predicted which jobs

01:00:57.480 --> 01:01:00.120
would be more automated would have changed over time.

01:01:01.520 --> 01:01:05.120
What we found was that when jobs got more or less automated

01:01:05.120 --> 01:01:06.960
that had no effect on average,

01:01:06.960 --> 01:01:08.680
on wages or number of workers,

01:01:09.520 --> 01:01:11.240
and that the predictors of automation

01:01:11.240 --> 01:01:13.280
didn't change at all over that 20-year period,

01:01:13.280 --> 01:01:16.320
and they remain to be very simple-minded predictors

01:01:16.320 --> 01:01:19.240
that you might expect about automation from long ago.

01:01:19.240 --> 01:01:20.920
The nature of automation hasn't changed

01:01:20.920 --> 01:01:22.440
in the aggregate in the economy.

01:01:22.440 --> 01:01:24.200
Main predictors of automation are

01:01:24.200 --> 01:01:26.240
whether the job has nice, clear measures

01:01:26.240 --> 01:01:27.440
of how well you've done it,

01:01:27.440 --> 01:01:28.880
whether it's in a clean environment

01:01:28.880 --> 01:01:30.800
with fewer disruptions,

01:01:30.800 --> 01:01:33.200
and whether tasks nearby have been automated.

01:01:33.200 --> 01:01:35.720
So there's a way that which task automation spreads

01:01:35.760 --> 01:01:38.240
to the network of nearby tasks.

01:01:38.240 --> 01:01:42.000
So that study suggested at least up until 2019,

01:01:42.000 --> 01:01:44.680
there had been no change in the nature of automation,

01:01:44.680 --> 01:01:47.520
and basically there's a Gaussian distribution

01:01:47.520 --> 01:01:49.360
of how automated jobs are,

01:01:49.360 --> 01:01:52.120
and the median automation had moved roughly

01:01:52.120 --> 01:01:55.280
a third of a standard deviation through that distribution.

01:01:55.280 --> 01:01:57.960
So jobs had gotten more automated substantially

01:01:57.960 --> 01:01:58.960
in that 20-year period,

01:01:58.960 --> 01:02:03.480
but still most jobs aren't that automated.

01:02:03.520 --> 01:02:05.200
And that would be my rough prediction

01:02:05.200 --> 01:02:07.760
for the next 20 years is to say

01:02:07.760 --> 01:02:10.160
the pattern of the last 20 years will continue.

01:02:10.160 --> 01:02:12.800
That is, I will slowly get more jobs more automated,

01:02:12.800 --> 01:02:16.480
but most automation will be very basic stuff.

01:02:16.480 --> 01:02:19.040
So far we just haven't seen much at all

01:02:19.040 --> 01:02:21.680
of advanced AI kinds of automation

01:02:21.680 --> 01:02:24.160
making a dent in the larger economy.

01:02:24.160 --> 01:02:25.800
So what do you make of things,

01:02:25.800 --> 01:02:29.120
I'm sure you're familiar with like the MMLU benchmark

01:02:29.120 --> 01:02:30.320
or the big bench, maybe not,

01:02:30.320 --> 01:02:32.960
if not I can characterize them for you, but.

01:02:32.960 --> 01:02:36.120
Is this machine learning set of tests

01:02:36.120 --> 01:02:38.800
in order to benchmark performance?

01:02:38.800 --> 01:02:42.000
Yes, I believe it's massive multi-task language

01:02:42.000 --> 01:02:45.680
understanding, the great Dan Hendricks and team.

01:02:45.680 --> 01:02:48.600
So basically a bunch of language understanding benchmarks?

01:02:48.600 --> 01:02:51.480
Yeah, they basically went and took final exams

01:02:51.480 --> 01:02:56.480
from like university and early grad school courses

01:02:56.720 --> 01:02:59.160
from every domain and compiled them

01:02:59.160 --> 01:03:00.640
into this massive benchmark.

01:03:00.640 --> 01:03:02.000
There have been a couple of different efforts like this,

01:03:02.000 --> 01:03:03.840
but this is basically the gold standard

01:03:03.840 --> 01:03:06.120
on which all the language models are measured.

01:03:07.240 --> 01:03:12.240
And we now have a like high 80s to 90% accuracy rate

01:03:13.800 --> 01:03:17.240
across all fields from like a single model, namely GPT-4.

01:03:17.240 --> 01:03:19.400
And now Google claims that it's Gemini

01:03:19.400 --> 01:03:22.180
is hitting that level as well.

01:03:23.040 --> 01:03:27.520
I would agree that these have not been broadly customized

01:03:27.520 --> 01:03:29.600
to the last mile specifications that they need

01:03:29.600 --> 01:03:31.760
to like work in the context of different firms

01:03:31.760 --> 01:03:35.720
and cultural contexts and all that sort of thing.

01:03:35.720 --> 01:03:38.680
But it does seem like the way I typically describe it

01:03:38.680 --> 01:03:42.960
is that AIs are now better at routine tasks

01:03:42.960 --> 01:03:45.120
than the average person and that they are closing in

01:03:45.120 --> 01:03:49.120
on expert performance on routine tasks.

01:03:49.120 --> 01:03:51.880
And that's measured by these medical diagnosis benchmarks,

01:03:51.880 --> 01:03:56.000
these MMLU type things, et cetera, et cetera.

01:03:56.000 --> 01:04:00.120
So let me remind you that in the 1960s say

01:04:01.120 --> 01:04:06.120
AI researchers took chess as a paradigm of

01:04:06.200 --> 01:04:08.880
if you can make a machine that can do that,

01:04:08.880 --> 01:04:10.760
well, obviously you'll have to have solved

01:04:10.760 --> 01:04:12.600
most of the major problems in thinking

01:04:12.600 --> 01:04:15.200
because chess involves most of the major problems

01:04:15.200 --> 01:04:16.040
in thinking.

01:04:16.040 --> 01:04:19.680
So when we can finally have human level chess abilities,

01:04:19.680 --> 01:04:21.680
we will have human level AI.

01:04:21.680 --> 01:04:23.120
That was the thinking in the 60s

01:04:23.120 --> 01:04:25.640
and they could look at the rate at which AI

01:04:25.640 --> 01:04:28.400
was getting better at chess and forecast long before

01:04:28.400 --> 01:04:31.840
it happened that in the late 1970s, 1990s, excuse me,

01:04:31.840 --> 01:04:35.880
is exactly when chess would reach human level ability

01:04:35.880 --> 01:04:37.760
and that's when it did happen.

01:04:37.760 --> 01:04:40.320
And that was 25 years ago.

01:04:40.320 --> 01:04:42.400
And clearly they were just wrong about the idea

01:04:42.400 --> 01:04:44.600
that you couldn't do chess without solving

01:04:44.600 --> 01:04:46.160
all the major thinking problems.

01:04:46.160 --> 01:04:48.680
And we repeatedly have this sort of phenomena

01:04:48.680 --> 01:04:51.840
where people look at something and they go,

01:04:51.840 --> 01:04:54.880
if you can do that, surely you can do most everything.

01:04:54.880 --> 01:04:57.680
And then we can do that and we can't do near,

01:04:57.720 --> 01:04:59.360
and we aren't near to doing most everything.

01:04:59.360 --> 01:05:01.640
So I just got to say this benchmark is just wrong.

01:05:01.640 --> 01:05:05.120
It's not true that if you can do these language benchmark,

01:05:05.120 --> 01:05:06.640
you are near to doing most everything.

01:05:06.640 --> 01:05:08.200
You are not near.

01:05:08.200 --> 01:05:09.920
Yeah, I would find my position to say,

01:05:09.920 --> 01:05:13.080
I think you're near to being able to do all the routine things

01:05:13.080 --> 01:05:16.080
that are well documented in the training data.

01:05:16.080 --> 01:05:18.320
Well, yes, but the question is in the economy,

01:05:18.320 --> 01:05:21.000
all the things we need doing, how close are you to that?

01:05:21.000 --> 01:05:23.000
And say you're not close.

01:05:23.000 --> 01:05:27.120
I mean, we're seeing just the very beginning of sort of,

01:05:27.120 --> 01:05:29.560
I mean, again, I don't know, like...

01:05:29.560 --> 01:05:30.960
What do you think was going on in their head

01:05:30.960 --> 01:05:33.800
in the 1960s when they looked at chess, right?

01:05:33.800 --> 01:05:35.040
They looked at chess and they said,

01:05:35.040 --> 01:05:36.720
it takes really smart people to do chess,

01:05:36.720 --> 01:05:38.960
look at all these complicated things people are doing

01:05:38.960 --> 01:05:41.360
when they do chess in order to achieve in chess,

01:05:41.360 --> 01:05:42.600
they said to themselves,

01:05:42.600 --> 01:05:44.000
that's the sort of thing we should work on

01:05:44.000 --> 01:05:45.440
because if we can get a machine to do that,

01:05:45.440 --> 01:05:49.920
surely we must be close to general artificial intelligence.

01:05:49.920 --> 01:05:52.800
If you could have something that could do chess.

01:05:52.800 --> 01:05:55.240
And there is a sense that when you have general intelligence,

01:05:55.240 --> 01:05:58.040
you can use all of that to do clever things about chess,

01:05:58.040 --> 01:06:01.400
but it's not true that you need to have all those general things

01:06:01.400 --> 01:06:02.400
in order to be good at chess.

01:06:02.400 --> 01:06:04.200
That turns out there's a way to be good at chess

01:06:04.200 --> 01:06:05.840
without doing all those other things.

01:06:05.840 --> 01:06:07.640
And that's repeatedly been the problem

01:06:07.640 --> 01:06:09.400
and that could be the problem today.

01:06:09.400 --> 01:06:12.280
Turns out there's a way to do these exam answering things

01:06:12.280 --> 01:06:16.960
that doesn't require the full range of general intelligence

01:06:16.960 --> 01:06:18.680
in order to achieve that task.

01:06:18.680 --> 01:06:21.120
It's hard to pick a good range of tasks

01:06:21.120 --> 01:06:24.120
that encompasses the full range of intelligence

01:06:24.120 --> 01:06:26.760
because again, you teach through the test

01:06:26.760 --> 01:06:28.840
and you end up finding a way to solve that problem

01:06:28.840 --> 01:06:31.680
without achieving general intelligence.

01:06:31.680 --> 01:06:33.360
This does seem different though.

01:06:33.360 --> 01:06:35.560
I mean, I would, I grew with your characterization

01:06:35.560 --> 01:06:39.080
that basically it turned out that there was an easier way

01:06:39.080 --> 01:06:42.840
or a more direct way, a narrower way to solve chess.

01:06:42.840 --> 01:06:47.080
And it's interesting that it's like rather different.

01:06:47.080 --> 01:06:50.640
You know, it involves these sort of superhuman tree search

01:06:50.640 --> 01:06:51.600
capabilities.

01:06:51.600 --> 01:06:52.960
But that wasn't just true of trust.

01:06:52.960 --> 01:06:55.880
There were another dozen sorts of really hard problems

01:06:55.880 --> 01:07:00.880
that people in the 1960s took as exemplars of things

01:07:00.880 --> 01:07:02.440
that would require general intelligence

01:07:02.440 --> 01:07:04.560
and the great many of them have been achieved.

01:07:04.560 --> 01:07:07.240
But when I look at the current situation,

01:07:07.240 --> 01:07:09.960
I'm like, this does look a lot more

01:07:09.960 --> 01:07:12.080
like the human intelligence.

01:07:12.080 --> 01:07:16.240
And I would say that from any number of different directions.

01:07:16.240 --> 01:07:19.360
And that was true in every decade for the last century.

01:07:20.360 --> 01:07:24.080
Every decade has seen advances that were not the sort

01:07:24.080 --> 01:07:26.080
that previous systems could achieve.

01:07:26.080 --> 01:07:27.960
It's clear that you are always, I think it's clear

01:07:27.960 --> 01:07:32.160
that you don't see the human brain, the human, you know,

01:07:32.160 --> 01:07:35.640
achieve level of achievement as sort of a maximum, right?

01:07:35.640 --> 01:07:37.080
Oh, of course not. Absolutely.

01:07:37.080 --> 01:07:41.240
So it's like there's got to be a finite number

01:07:41.240 --> 01:07:45.000
of breakthroughs that need to happen.

01:07:45.000 --> 01:07:47.240
We will eventually get full human level AI.

01:07:47.240 --> 01:07:48.680
I have no doubt about that.

01:07:48.680 --> 01:07:53.520
And not soon after vastly exceeded, that will happen.

01:07:53.520 --> 01:07:56.480
And it will happen plausibly within the next thousand years.

01:07:56.480 --> 01:08:00.200
It also seems like you would probably agree that it need not

01:08:00.200 --> 01:08:04.800
be point for point, you know, the M scenario is a great one

01:08:04.800 --> 01:08:07.600
to play out and analyze, but it need not be the case.

01:08:07.600 --> 01:08:10.640
Right. So the AIs could be much better than humans in some ways

01:08:10.640 --> 01:08:12.440
and still much worse than others.

01:08:12.440 --> 01:08:15.280
That will probably actually be true for a long time.

01:08:15.280 --> 01:08:17.880
That is, it'll take a lot longer till AIs are better

01:08:17.880 --> 01:08:20.240
than humans at most everything than that they are better

01:08:20.240 --> 01:08:23.240
at humans at say half of things people do today.

01:08:23.240 --> 01:08:24.640
But of course you have to realize if you looked

01:08:24.640 --> 01:08:28.240
at what humans were doing two centuries ago, we're already

01:08:28.240 --> 01:08:30.760
at the point where machines do those things much better

01:08:30.760 --> 01:08:32.480
than humans can do.

01:08:32.480 --> 01:08:34.760
That is, the attack, most tasks that humans were doing

01:08:34.760 --> 01:08:37.960
two centuries ago are already long since automated.

01:08:37.960 --> 01:08:40.640
We've now switched our attention to the sort of tasks

01:08:40.640 --> 01:08:42.560
that people were not doing two centuries ago.

01:08:42.560 --> 01:08:45.080
And on those, we're not so good at making machines do them,

01:08:45.080 --> 01:08:49.120
but we've already dramatically achieved full automation basically

01:08:49.120 --> 01:08:51.840
of most things humans were doing two centuries ago.

01:08:51.840 --> 01:08:53.760
Which for very shorthand I would say is kind

01:08:53.760 --> 01:08:58.000
of routine repetitive physical tasks.

01:08:58.000 --> 01:09:00.080
Right. I mean, we managed to change the environment

01:09:00.080 --> 01:09:02.400
to make them more routine and repetitive.

01:09:02.400 --> 01:09:05.880
So, you know, a subsistence farmer

01:09:05.880 --> 01:09:08.720
on a subsistence farm two centuries ago, they were,

01:09:08.720 --> 01:09:11.200
we couldn't, our automation could not do that job

01:09:11.200 --> 01:09:12.520
that they were doing that.

01:09:12.520 --> 01:09:14.080
And we managed to make the farms different.

01:09:14.080 --> 01:09:15.960
The factory is different, et cetera, so that our machines

01:09:15.960 --> 01:09:17.360
could do them.

01:09:17.360 --> 01:09:20.000
And now they are producing much more than those people produce.

01:09:20.000 --> 01:09:22.600
But if you had to try to produce the way they were doing

01:09:22.600 --> 01:09:25.680
two centuries ago, our machines today could not do that.

01:09:25.680 --> 01:09:28.680
Yeah, a big theory I have also, I actually don't think this is going

01:09:28.680 --> 01:09:32.160
to be a huge, well, everything's going to be huge,

01:09:32.160 --> 01:09:35.800
but I don't think it's going to be like the dominant change

01:09:35.800 --> 01:09:38.960
that leads to qualitatively different future.

01:09:38.960 --> 01:09:41.760
But I do think we will start to see, and are beginning

01:09:41.840 --> 01:09:46.160
to see that same process happening with language models,

01:09:46.160 --> 01:09:49.520
where, you know, I consult with a few different businesses

01:09:49.520 --> 01:09:52.240
and we have kind of processes that, you know,

01:09:52.240 --> 01:09:54.000
we would like to automate.

01:09:54.000 --> 01:09:57.800
You know, a classic one would be like initial resume screening.

01:09:57.800 --> 01:09:59.600
Right. We're not going to have the language model at this point

01:09:59.600 --> 01:10:00.920
make the hiring decisions.

01:10:00.920 --> 01:10:02.920
But if we get a lot of garbage resumes, you know,

01:10:02.920 --> 01:10:08.160
we can definitely get language models to kind of band the resumes

01:10:08.160 --> 01:10:12.080
into, you know, one to five and like spend our time on the fives.

01:10:13.280 --> 01:10:16.080
It does seem to me that there's a lot of kind of process

01:10:16.080 --> 01:10:20.400
and environment adaptation that is not that hard to do.

01:10:20.400 --> 01:10:23.240
Like I personally have done it successfully across a handful

01:10:23.240 --> 01:10:24.560
of different things.

01:10:24.560 --> 01:10:28.360
Why it seems like you're announced as though a sort of doesn't

01:10:29.320 --> 01:10:31.640
assumes that that's not going to happen at scale this time

01:10:31.640 --> 01:10:34.040
around with the technology we currently have.

01:10:34.480 --> 01:10:39.240
I said, you know, in the last 20 years from 1999 to 2019,

01:10:39.240 --> 01:10:41.720
we moved roughly a third of a standard deviation

01:10:41.720 --> 01:10:44.000
in the distribution of automation.

01:10:44.000 --> 01:10:47.640
OK, so what if we in the next 60 years

01:10:48.400 --> 01:10:52.640
move a third of a standard deviation in each of the 20 year periods?

01:10:52.640 --> 01:10:55.760
Then over 60 years, we would basically move an entire standard deviation.

01:10:56.880 --> 01:11:01.400
That could represent a large increase in automation

01:11:01.880 --> 01:11:04.000
over the next 60 years.

01:11:04.120 --> 01:11:07.040
And that would mean a lot of things we're doing by hand today

01:11:07.040 --> 01:11:08.560
will be done by machines.

01:11:08.560 --> 01:11:11.240
Then it would mean our economy is more productive,

01:11:11.840 --> 01:11:15.120
but it still would mean humans have a huge place in the world.

01:11:15.120 --> 01:11:19.240
They get paid and most income probably still goes to pay humans to do work,

01:11:19.880 --> 01:11:22.520
even though they have much better automation at the time.

01:11:23.280 --> 01:11:25.840
If that's the situation in 60 years,

01:11:26.960 --> 01:11:30.320
then unfortunately that level of increase in automation

01:11:30.320 --> 01:11:32.480
is just not sufficient to prevent the economy

01:11:32.480 --> 01:11:35.200
from declining as population declines.

01:11:35.200 --> 01:11:38.440
And so we won't get much more automation than that.

01:11:39.640 --> 01:11:43.480
The well of it in automation will dry up because innovation will stop.

01:11:43.520 --> 01:11:46.040
And we would then have a several centuries long period

01:11:46.040 --> 01:11:48.840
where our technology does not improve.

01:11:48.840 --> 01:11:51.640
And in fact, we lose a lot of technologies tied to scale economies

01:11:52.160 --> 01:11:54.480
as the world economy shrinks.

01:11:54.480 --> 01:11:59.480
We'll manage to have less variety, less large scale production and distribution.

01:12:00.280 --> 01:12:03.880
And we would then struggle to maintain previous technologies.

01:12:03.880 --> 01:12:06.880
And AI is at risk of the sort of technology would be hard to maintain

01:12:06.880 --> 01:12:10.920
because at the moment, AI is a really large scale, concentrated sort of technology

01:12:10.920 --> 01:12:14.800
is not being done by mom and pops to be done by very large enterprises

01:12:14.800 --> 01:12:16.240
on very large scales.

01:12:16.240 --> 01:12:21.480
I would agree that the supply chain is definitely prone to disruption in AI.

01:12:21.480 --> 01:12:22.840
No doubt about that.

01:12:22.840 --> 01:12:27.840
Can you describe in more detail what what is the standard deviation

01:12:27.840 --> 01:12:30.880
in automation and how should I conceptualize that?

01:12:31.440 --> 01:12:34.760
I mean, I guess what you'd want to do is see a list of tasks

01:12:34.760 --> 01:12:42.240
and how automated each task was and then see sort of how much on that score.

01:12:42.240 --> 01:12:45.080
And it would have. So basically, if you look on this list

01:12:45.080 --> 01:12:48.160
at the most and least automated tasks, you'll agree, which are which

01:12:48.880 --> 01:12:52.320
like the nearly most automated task is airline pilots.

01:12:53.320 --> 01:12:56.320
Nearly the least automated task is carpet installers.

01:12:57.320 --> 01:13:02.040
Carpet installers use pretty much no automation to staple in carpets.

01:13:02.040 --> 01:13:07.320
And airline pilots are pretty much always having automation help what they're doing.

01:13:08.120 --> 01:13:12.320
And then, you know, you can see the scores in the middle and see that we've,

01:13:12.320 --> 01:13:15.280
you know, moved up a modest degree over those 20 years.

01:13:15.800 --> 01:13:18.920
That would be the way to get an intuition for it is just to see a list

01:13:18.920 --> 01:13:22.720
of particular jobs in their automation scores and then see,

01:13:22.720 --> 01:13:24.960
compare that to the amount by which we've moved up.

01:13:25.680 --> 01:13:26.640
How do you reconcile?

01:13:26.640 --> 01:13:31.320
Or how should I understand the idea that

01:13:31.720 --> 01:13:34.560
whatever doubling time of the economy today,

01:13:34.840 --> 01:13:37.480
I think you said it was like 15 years in the book,

01:13:37.480 --> 01:13:40.800
which seemed a little fast to me, just based on like rule of 70.

01:13:41.360 --> 01:13:44.080
Right. I think it's more like, you know, 20 or something now.

01:13:44.320 --> 01:13:47.920
But still, like it seems it seems like there's a little bit of a disconnect

01:13:47.920 --> 01:13:52.920
between a notion of, you know, over these next 60 years,

01:13:52.920 --> 01:13:57.280
we would be double, double, double, you know, essentially 10xing the economy.

01:13:57.680 --> 01:14:02.600
But we'd only move at sort of a linear rate in automation.

01:14:02.600 --> 01:14:06.240
Like we would only move a third of a standard deviation in each period.

01:14:06.480 --> 01:14:08.080
Let me help you understand that then.

01:14:08.080 --> 01:14:12.240
People have often said, look, computer technology is increasing exponentially.

01:14:12.640 --> 01:14:16.400
Therefore, we should expect an exponential impact on the economy,

01:14:16.400 --> 01:14:21.080
i.e. early on hardly any impact, and then suddenly an accelerating boom

01:14:21.120 --> 01:14:24.360
such that we get this big explosion and then everything happens.

01:14:24.560 --> 01:14:26.040
But that's not what we've seen.

01:14:26.040 --> 01:14:31.760
So what we've seen over time is relatively steady effects on the economy of automation,

01:14:31.760 --> 01:14:34.640
even though the economy is growing exponentially.

01:14:35.840 --> 01:14:40.920
The way I help you understand that is to imagine the distribution of all tasks

01:14:40.920 --> 01:14:44.760
that you might want automated and that they're the degree of computing power,

01:14:44.880 --> 01:14:49.120
both in hardware and software, required to automate that task for each task

01:14:49.160 --> 01:14:53.320
is distributed in a log normal way with a very large variance.

01:14:53.480 --> 01:14:58.040
That is, there's a very large range of how much computing power it takes to automate a task.

01:14:58.360 --> 01:15:00.560
As computing power increases exponentially,

01:15:00.560 --> 01:15:04.320
you're basically moving through that log normal distribution in a linear manner.

01:15:04.720 --> 01:15:07.960
And in the middle of the distribution, it's pretty steady effect.

01:15:08.600 --> 01:15:12.880
You slowly chop away at tasks as you are able to automate them

01:15:13.440 --> 01:15:18.520
because you're slowly acquiring sufficient hardware to do that task.

01:15:19.280 --> 01:15:21.840
That that gives you a simple model, but in which

01:15:22.960 --> 01:15:25.080
computing power grows exponentially.

01:15:25.080 --> 01:15:29.800
And yet you see a relatively steady erosion of tasks through automation.

01:15:30.160 --> 01:15:32.320
It's a low hanging fruit argument.

01:15:32.320 --> 01:15:35.040
Yeah, the low hanging fruits are hanging really low.

01:15:35.720 --> 01:15:40.040
That this this is a log normal tree, basically, that you're trying to grab things from.

01:15:40.040 --> 01:15:43.040
I mean, you're growing your ladder is growing exponentially into the tree.

01:15:43.360 --> 01:15:46.200
And every time your ladder gets taller, you get to pick more feuds.

01:15:46.200 --> 01:15:47.880
But it's a really tall tree.

01:15:47.920 --> 01:15:50.080
That means that you have a long, long way to go.

01:15:50.920 --> 01:15:55.680
How do you think about things like the progress in AI art generation

01:15:55.680 --> 01:15:58.520
or like deep fakes over the last couple of years?

01:15:58.520 --> 01:16:04.400
This is an area where I feel like if we rewound to two years ago,

01:16:04.720 --> 01:16:09.080
just two years ago, really, when I was first starting to see AI art

01:16:09.720 --> 01:16:12.200
popping up on Twitter and it was like

01:16:13.200 --> 01:16:16.360
not very good for the most part, you'd see the occasional thing where you're like,

01:16:16.360 --> 01:16:17.360
oh, that's really compelling.

01:16:17.360 --> 01:16:20.800
And then you'd see a lot of stuff that was like, yeah, you know, it's whatever.

01:16:20.800 --> 01:16:22.680
It's it's remarkable that you can do that.

01:16:23.280 --> 01:16:27.520
It's a while compared to what came before, but it, you know, it's like

01:16:28.920 --> 01:16:32.680
I'm not going to be watching like feature films based on this technology

01:16:32.680 --> 01:16:34.880
and, you know, in the immediate future.

01:16:34.880 --> 01:16:38.320
I feel like we could have had a very similar discussion where you might say,

01:16:38.320 --> 01:16:40.440
well, you know, yeah, it's progress.

01:16:40.440 --> 01:16:45.360
But, you know, the real human art, the top notch stuff, like that's so far away.

01:16:45.800 --> 01:16:50.680
And then early last year, my teammates at Waymark made a short film

01:16:50.680 --> 01:16:57.120
using nothing but Dolly 2 at that time imagery and some definite elbow grease.

01:16:57.120 --> 01:16:59.600
But like the quality of production that they were able to achieve

01:16:59.880 --> 01:17:05.280
with a half dozen people and Dolly 2 is on the level that like previously

01:17:05.280 --> 01:17:09.160
would have taken, you know, a crew in Antarctica, you know, to go shoot.

01:17:09.880 --> 01:17:12.040
You know, again, is that work all done?

01:17:12.040 --> 01:17:15.200
No. But if you look at the mid journey outputs today,

01:17:15.240 --> 01:17:17.800
you look at some of the deep fake technologies that are happening today.

01:17:17.800 --> 01:17:20.320
It's like it does feel like we've hit certainly

01:17:20.320 --> 01:17:23.600
photo realistic thresholds, you know, almost indistinguishable

01:17:23.960 --> 01:17:26.960
from photography with mid journey and with the deep fakes.

01:17:27.440 --> 01:17:30.800
You're not quite quite there yet, but like watch out for 2024

01:17:30.840 --> 01:17:37.280
to have a lot of stories of people being scammed by the kind of custom

01:17:37.280 --> 01:17:40.920
text to speech voice, you know, with a family member, family members voice, whatever.

01:17:41.120 --> 01:17:44.120
All my voice out there, you know, people are going to be calling my parents with my voice.

01:17:44.640 --> 01:17:48.240
So I guess what I'm trying to get at there is like it seems like even just

01:17:48.240 --> 01:17:52.720
in the last couple of years, we have these examples where we are seeing

01:17:52.720 --> 01:17:59.440
like really rapid progress that is not stopping before critical thresholds.

01:17:59.960 --> 01:18:03.200
In the 1960s, there was a U.S.

01:18:03.200 --> 01:18:07.960
Presidential Commission to to address and study the question

01:18:07.960 --> 01:18:10.040
of whether most jobs were about to be automated.

01:18:10.040 --> 01:18:13.440
It reached that level of high level concern in the country.

01:18:13.480 --> 01:18:15.800
And major media discussion about it.

01:18:17.280 --> 01:18:21.840
Ever since then, we continue to have periodic articles about dramatic,

01:18:22.040 --> 01:18:26.960
exciting progress in AI and what that might mean for the society and economy.

01:18:27.240 --> 01:18:30.280
And in all those articles through all those years,

01:18:30.960 --> 01:18:34.560
they don't just talk in the abstract, they usually pick out some particular examples

01:18:35.040 --> 01:18:37.440
and they don't pick out random examples from the economy.

01:18:37.440 --> 01:18:40.520
They pick out the examples where the automation has made the most difference.

01:18:41.520 --> 01:18:44.360
That, of course, makes sense if you're trying to make an exciting story.

01:18:45.640 --> 01:18:49.160
And so we've always been able to pick out the things

01:18:49.160 --> 01:18:51.720
which are having the most dramatic increase lately

01:18:51.720 --> 01:18:54.280
that also seem the most salient and interesting.

01:18:54.440 --> 01:18:57.680
And now you can pick out image generation

01:18:58.320 --> 01:19:02.520
as one of the main examples lately as something that's increased a lot lately.

01:19:02.800 --> 01:19:05.200
And I'm happy to admit it has.

01:19:05.200 --> 01:19:07.600
I would put it up, you know, and that's the sort of thing

01:19:07.640 --> 01:19:10.640
that somebody writing an article today about the exciting AI progress

01:19:10.640 --> 01:19:14.680
would, in fact, mention and talk about graphic artists being put out of work

01:19:14.680 --> 01:19:18.520
by the availability of these things, which probably is happening.

01:19:19.040 --> 01:19:22.800
The point is just to realize how selective that process is

01:19:23.040 --> 01:19:26.320
to pick out the most dramatic impacts and to realize just how many other jobs

01:19:26.320 --> 01:19:30.200
there are and how many other tasks there are and then how far we still have to go.

01:19:30.840 --> 01:19:33.440
I'm happy to celebrate recent progress.

01:19:33.440 --> 01:19:37.640
And if I were, you know, if I were a graphic artist person,

01:19:37.640 --> 01:19:41.040
I would be especially excited to figure out how to take advantage of these changes

01:19:41.560 --> 01:19:44.240
because they are among the biggest change.

01:19:44.520 --> 01:19:46.520
If you're, say, a 20 year old in the world,

01:19:46.520 --> 01:19:49.920
it makes complete sense to say, where are things most exciting and changing?

01:19:50.040 --> 01:19:53.280
I want to go there and be part of the new exciting thing happening there.

01:19:53.960 --> 01:19:57.200
If, of course, you're a 60 year old and you've already invested in a career,

01:19:57.200 --> 01:20:00.880
then it makes less sense to, like, try to switch your whole career over to a new thing.

01:20:00.920 --> 01:20:03.520
But a lot of people are at the beginning of their career and they should.

01:20:03.840 --> 01:20:06.160
They should look for where the most exciting changes are

01:20:06.160 --> 01:20:08.400
and try to see if they can go be part of that.

01:20:08.960 --> 01:20:10.040
Move West, young man.

01:20:10.040 --> 01:20:11.920
If West is where things are happening, right?

01:20:11.920 --> 01:20:14.600
But you still have to keep in mind if there's a few people going out West

01:20:14.600 --> 01:20:18.760
making exciting things happening, how big a percentage of the world is the West, right?

01:20:19.520 --> 01:20:22.320
Yes, it's exciting and there's huge growth in the West.

01:20:22.320 --> 01:20:25.880
You know, 10 years ago, there was hardly anything and now there's a big town.

01:20:26.120 --> 01:20:27.560
Look how great the West is growing.

01:20:27.560 --> 01:20:31.120
And that, you know, there are always times and places where right there,

01:20:31.120 --> 01:20:36.520
things are growing very fast and newspaper writers should focus on those to tell stories

01:20:37.160 --> 01:20:39.920
and keep novelists should focus on those to tell stories.

01:20:39.920 --> 01:20:41.960
They're exciting places where exciting things are happening.

01:20:41.960 --> 01:20:45.320
And I want to make sure the world keeps having things like that happening

01:20:45.320 --> 01:20:47.760
because that's how we can keep growing.

01:20:47.960 --> 01:20:50.480
But you have to be honest about the fraction of the world

01:20:50.480 --> 01:20:52.680
that's involved in those exciting frontier stories.

01:20:53.600 --> 01:20:57.200
Yeah, I mean, I guess my kind of counterpoint to that would be

01:20:57.680 --> 01:21:03.280
the same relatively simple technology, like the transformer

01:21:03.280 --> 01:21:07.560
or like the attention mechanism, perhaps it is better, you know, pinpointed as

01:21:08.320 --> 01:21:11.120
is driving this art creation.

01:21:11.720 --> 01:21:16.320
It's also writing today like short programs.

01:21:16.320 --> 01:21:20.280
Yeah, I would personally say my productivity as a programmer has been

01:21:20.600 --> 01:21:25.000
increased like several fold, not like incrementally, but like multiple

01:21:25.320 --> 01:21:29.440
with GPT for assistance, you know, it's the wide range where you could go on.

01:21:29.440 --> 01:21:32.200
But like it's it's also happening in metal medical diagnosis.

01:21:32.200 --> 01:21:36.800
It's also happening in like protein, you know, novel protein structure generation.

01:21:36.800 --> 01:21:39.680
And certainly from an economic point of view, the biggest category

01:21:39.680 --> 01:21:41.200
you've mentioned is programming.

01:21:41.200 --> 01:21:44.760
That's a much larger industry, less of your profession than the other ones you mentioned.

01:21:45.040 --> 01:21:47.880
Well, but watch out for biotech also, I would say, for sure.

01:21:47.880 --> 01:21:50.520
But biotech has been shrinking for a while.

01:21:50.520 --> 01:21:53.880
So that's not an exact thing you should point to as a growing thing.

01:21:54.280 --> 01:21:56.600
I will predict growth for biotech, definitely.

01:21:56.760 --> 01:21:59.160
I mean, you know, it's also it's reading brain states.

01:21:59.160 --> 01:22:02.360
Have you seen these recent things where people can read the brain state?

01:22:02.680 --> 01:22:04.640
Among the things you're talking about at the moment, the biggest

01:22:04.640 --> 01:22:07.120
profession being affected is programming, clearly.

01:22:07.320 --> 01:22:09.560
I have a younger son, two sons.

01:22:09.560 --> 01:22:11.240
My younger one is a professional programmer.

01:22:11.240 --> 01:22:14.960
So, you know, I've had him look at and his

01:22:15.600 --> 01:22:18.400
workplace has looked into what they can do with large language models

01:22:18.400 --> 01:22:19.560
to help them write programs.

01:22:19.560 --> 01:22:24.160
And their evaluation so far is, you know, they don't even

01:22:25.280 --> 01:22:26.960
they'll wait in six months to look again.

01:22:26.960 --> 01:22:28.440
It's not useful now.

01:22:28.440 --> 01:22:30.640
Can I short that stock?

01:22:31.560 --> 01:22:35.040
Well, I could tell you after we finish what that is.

01:22:35.040 --> 01:22:37.280
But basically, I think this is true.

01:22:37.280 --> 01:22:41.120
Most actual professional programmers are not using large language models

01:22:41.120 --> 01:22:43.200
that much in doing their job.

01:22:44.040 --> 01:22:48.240
Now, I got to say that if some people are getting factors of two productivity

01:22:48.240 --> 01:22:53.920
increase that eventually we should see some effect of that on their wages.

01:22:55.440 --> 01:22:59.280
That is, of course, you know, now, if lots of programmers go out

01:22:59.280 --> 01:23:02.120
and use productivity spaces, in some sense, we're going to increase

01:23:02.120 --> 01:23:03.520
the supply of programming.

01:23:04.520 --> 01:23:07.840
And so supply and demand would mean that maybe increasing

01:23:07.840 --> 01:23:11.640
the supply lowers the price, even if it dramatically increases the quantity.

01:23:12.560 --> 01:23:16.560
But, you know, there's such a large elastic demand for programming

01:23:16.560 --> 01:23:19.360
in the world that I actually think that effect would be relatively weak.

01:23:19.360 --> 01:23:24.720
And so you should be expecting large increases in the wages going to programmers.

01:23:25.520 --> 01:23:30.520
If you are expecting large overall increases in the productivity of programmers.

01:23:31.480 --> 01:23:35.560
Because, again, it's a large elastic demand for programming in the world.

01:23:36.000 --> 01:23:39.720
You know, long for a long time, a lot of change in the world has been driven

01:23:39.720 --> 01:23:43.600
by programming and limited by the fact that there's only so many decent programmers out there.

01:23:44.320 --> 01:23:45.960
Only so many people you can get to do programming.

01:23:45.960 --> 01:23:51.400
So clearly, if we can dramatically expand the supply of programmers,

01:23:51.920 --> 01:23:54.760
we can do a lot more programming in a lot more areas.

01:23:54.760 --> 01:23:57.920
And there's a lot of money that's willing to go to that to do that.

01:23:57.920 --> 01:24:00.960
There's a lot of people who would be hiring more programmers if only they were cheaper.

01:24:02.040 --> 01:24:03.880
And they're about to get cheaper in effect.

01:24:04.520 --> 01:24:08.120
And so you should be predicting large increases in basically

01:24:08.600 --> 01:24:11.320
the wages and number of programmers in the world.

01:24:12.200 --> 01:24:14.000
We haven't seen that yet.

01:24:14.000 --> 01:24:15.760
I do predict large increases in number.

01:24:15.760 --> 01:24:17.000
I'm not so sure about wages.

01:24:17.000 --> 01:24:18.960
It feels like why not?

01:24:18.960 --> 01:24:22.760
Well, I've done a couple of episodes with the folks at a company called Replet,

01:24:22.760 --> 01:24:27.640
which is a very interesting end to end at this point, software development platform.

01:24:28.200 --> 01:24:31.640
Their mission is to onboard the next one billion developers.

01:24:32.240 --> 01:24:35.880
And, you know, they have like a great mobile app.

01:24:35.880 --> 01:24:40.440
They have kids in India that are, you know, 14 years old that are doing it all on their mobile app.

01:24:41.000 --> 01:24:44.080
And I'd say it's much harder.

01:24:44.320 --> 01:24:46.800
And maybe this reflects the kind of programming that your son is doing.

01:24:46.800 --> 01:24:52.600
But I'd say it's much harder to take the most elite frontier work and accelerate that

01:24:53.200 --> 01:25:00.160
in a meaningful way versus like commoditizing the routine application development

01:25:00.280 --> 01:25:05.080
that like the, you know, the sort of long tail of programmers mostly do.

01:25:05.080 --> 01:25:08.400
My son is definitely doing routine application development.

01:25:10.160 --> 01:25:12.000
He's not at the frontier programming at all.

01:25:13.000 --> 01:25:20.000
But again, I'm saying I don't expect this sudden large increase in programmer wages and quantity,

01:25:20.800 --> 01:25:21.920
especially wages.

01:25:21.920 --> 01:25:25.720
I mean, the less the quantity increases, the more wages would have to be increasing to compensate.

01:25:26.360 --> 01:25:30.360
And I think it'll be hard to get that many more people willing to be programmers,

01:25:30.360 --> 01:25:32.760
but you could pay them more.

01:25:34.240 --> 01:25:35.560
And I don't predict this.

01:25:35.560 --> 01:25:39.880
So this is a concrete thing we could, you know, even better on over the next five or 10 years.

01:25:40.440 --> 01:25:42.760
Will there be a big boost in programmer wages?

01:25:44.120 --> 01:25:45.600
That would be the consequence.

01:25:45.600 --> 01:25:47.840
It's a very simple supply and demand analysis here.

01:25:47.840 --> 01:25:52.480
This isn't some subtle, you know, rocket science version of economics.

01:25:52.480 --> 01:25:55.440
Well, typically when supply increases, price drops, right?

01:25:55.440 --> 01:25:58.680
I'm expecting lots more programmers and them to be broadly cheap.

01:25:58.680 --> 01:26:00.560
Depends on the elasticity of demand.

01:26:01.840 --> 01:26:07.320
So, you know, if you think about something that there's just a very limited demand for in the world,

01:26:07.560 --> 01:26:11.280
you know, if, if piano tuning got a lot cheaper, you wouldn't have a lot more pianos

01:26:11.760 --> 01:26:15.480
because piano tuning is not one of the major costs of having a piano.

01:26:15.560 --> 01:26:19.200
You know, it's the cost of the piano itself, plus the space for it in your living room, right?

01:26:19.720 --> 01:26:21.400
And the time it takes to play on the piano.

01:26:21.400 --> 01:26:25.640
So piano tuning is a really small cost of piano.

01:26:25.640 --> 01:26:30.160
So that means the elasticity of demand for piano tuners by itself is pretty low.

01:26:31.240 --> 01:26:34.560
You know, there's just basically only so many pianos, they all need to be tuned.

01:26:34.560 --> 01:26:38.240
And if each piano tuner could tune each piano twice as fast, say,

01:26:38.880 --> 01:26:44.640
and we basically only need half as many pianos because there's just not much of elasticity for demand.

01:26:44.640 --> 01:26:50.920
So for kinds of jobs like that, productivity increases will cause a reduction in the employment.

01:26:52.360 --> 01:26:57.480
But even in that case, you might get a doubling of the wages and half the number of piano tuners

01:26:57.480 --> 01:26:59.360
because they can each be twice as productive.

01:26:59.880 --> 01:27:04.680
But for programming, it's clear to me that programming has an enormous elastic demand.

01:27:04.680 --> 01:27:07.880
The world out there has far fewer programmers than they want.

01:27:07.920 --> 01:27:11.840
They would love all over the place to hire more programmers to do more things.

01:27:12.320 --> 01:27:14.760
There's a big demand in the world for software to do stuff.

01:27:15.160 --> 01:27:18.640
And there's a huge potential range of things the software could be doing.

01:27:18.640 --> 01:27:20.280
It's not doing now.

01:27:20.280 --> 01:27:23.920
So that means there's a pretty elastic demand for programming.

01:27:23.920 --> 01:27:28.240
That means as we increase the quantity of programming, the price doesn't come down that much.

01:27:29.680 --> 01:27:31.240
There's still people willing to buy this stuff.

01:27:32.320 --> 01:27:34.960
So that tells me that as productivity increases,

01:27:36.560 --> 01:27:40.000
basically the supply is expanding and the demand is not coming down much.

01:27:40.000 --> 01:27:44.480
So we should just see a much larger quantity.

01:27:44.480 --> 01:27:47.560
But then, you know, basically because each person is being more productive,

01:27:47.720 --> 01:27:49.440
each person should get paid more.

01:27:49.440 --> 01:27:52.280
So the elastic supply is going to be a combination of two things.

01:27:52.280 --> 01:27:57.240
Each person getting more productive and more people being willing to join that profession.

01:27:58.040 --> 01:28:03.520
And I think we've already seen that even as the wages for programming has gone way up

01:28:03.520 --> 01:28:06.960
in the last decade or so, the number of programmers hasn't gone up as fast.

01:28:07.600 --> 01:28:11.480
That is, there's just kind of a limited number of people who are decent at programming.

01:28:12.440 --> 01:28:16.440
And it's hard to get the marginal person to be a programmer.

01:28:17.600 --> 01:28:20.920
But the people who are programmers, when they're productive, they get paid a lot.

01:28:21.080 --> 01:28:24.720
I mean, as you've probably heard rumors about AI programmers

01:28:24.720 --> 01:28:28.760
and how much they're being paid lately, it's crazy high because there's just a limited supply.

01:28:30.120 --> 01:28:35.600
So I got to say, I expect large increases in wages for programmers,

01:28:35.600 --> 01:28:39.960
if in fact large language models are making programmers much more productive.

01:28:41.040 --> 01:28:46.520
But according to my son, at least, and others I've heard, you know, that's not happening.

01:28:47.320 --> 01:28:49.160
I'm with you up until the very last two points.

01:28:49.200 --> 01:28:50.960
I would say I think it is happening.

01:28:51.400 --> 01:28:56.360
And I would also say I think my estimation of the relevant

01:28:56.440 --> 01:29:01.720
relevant elasticities is that there will be a large growth in people who can be

01:29:01.960 --> 01:29:06.160
and will choose to be programmers, but that the wages don't go up.

01:29:06.160 --> 01:29:10.280
They don't fall like dramatically necessarily either because it has to be like

01:29:10.320 --> 01:29:12.320
an attractive thing for people to want to do it.

01:29:12.320 --> 01:29:17.480
But I think that the prevailing wages are quite high compared to what a lot of people

01:29:17.480 --> 01:29:23.360
would be excited to take if they could easily break in with language model assistance,

01:29:23.360 --> 01:29:26.240
which I think they will increasingly be able to do.

01:29:26.640 --> 01:29:27.760
Let me change gears a little bit.

01:29:27.760 --> 01:29:29.080
So we've debated.

01:29:29.080 --> 01:29:35.680
This has been really I always appreciate a useful and thoughtful challenge to my world model.

01:29:36.440 --> 01:29:38.560
You're definitely supplying that.

01:29:38.560 --> 01:29:45.200
Let's do a couple like a little bit more speculative things that could be kind of M first,

01:29:45.280 --> 01:29:48.160
you know, a little bit of LLM as I was going through the book.

01:29:48.160 --> 01:29:51.640
There are a number of things that I was like, hmm, this is really interesting.

01:29:52.120 --> 01:29:54.240
How would I think about this a bit differently?

01:29:54.360 --> 01:29:57.680
And, you know, and maybe suspend a little bit of your

01:29:58.520 --> 01:30:01.000
skepticism of how much impact LLM will make.

01:30:01.000 --> 01:30:04.720
Let's let's go in a world where, you know, scaling continues to work.

01:30:04.760 --> 01:30:06.520
Context lengths get long.

01:30:06.520 --> 01:30:10.840
You know, we start to see that not total, you know, displacement of humans,

01:30:10.840 --> 01:30:16.640
but like substantial fraction of, you know, tasks being like LLM, automatable.

01:30:17.160 --> 01:30:23.080
One interesting inference that you make is that there won't be that many different base ends

01:30:23.440 --> 01:30:30.200
that essentially there will be super selective emmifying of really elite,

01:30:30.200 --> 01:30:34.120
really capable people that those will become the basis that they'll be sort of

01:30:34.920 --> 01:30:39.880
essentially turn into kind of clans where they'll they'll highly identify with each other.

01:30:40.160 --> 01:30:43.640
And they'll have like, you know, marginally different specialization,

01:30:43.920 --> 01:30:48.880
but that there will be these sort of recognizable, almost canonical personalities

01:30:48.880 --> 01:30:54.720
that are not that many of them that kind of come to dominate the economy.

01:30:55.480 --> 01:30:59.560
It seems like we're kind of seeing something similar with language models already,

01:30:59.560 --> 01:31:03.720
where it's like, we have GPT-4, we have, you know, some the new thing from Google,

01:31:03.720 --> 01:31:06.360
we have Claude, we have like a couple open source ones.

01:31:06.720 --> 01:31:12.160
And then they get like a lot of like local fine tuning and kind of adaptation.

01:31:12.480 --> 01:31:16.080
I guess my read on that was that it's an odd, you know, it's initially a very

01:31:16.080 --> 01:31:18.360
surprising vision of the future.

01:31:18.800 --> 01:31:22.800
But it does seem like we see the proto version of that in the development

01:31:22.800 --> 01:31:24.720
of large language models.

01:31:25.080 --> 01:31:25.640
Any thoughts?

01:31:26.120 --> 01:31:30.680
It's basically how many different kinds of jobs are there is the question.

01:31:30.920 --> 01:31:32.040
Job tasks are there.

01:31:32.320 --> 01:31:34.720
And so how many dimensions do they vary?

01:31:35.600 --> 01:31:40.080
So I mean, there's clearly a lot of different kinds of jobs.

01:31:40.560 --> 01:31:44.600
Like I told you, the study we did looked at, you know, 900 of them.

01:31:45.320 --> 01:31:50.160
But once you look at 900 different jobs, a lot of jobs are pretty similar to each

01:31:50.160 --> 01:31:55.320
other and they take pretty similar mental styles and personalities to do those jobs.

01:31:55.720 --> 01:32:00.240
So when we're looking at humans at least, it looks like a few hundred

01:32:00.240 --> 01:32:03.400
humans would be enough to do pretty much all the jobs.

01:32:04.320 --> 01:32:06.120
That's looking at the variation in humans.

01:32:06.120 --> 01:32:10.520
Now, the harder part is to say, well, large language models, is there space

01:32:10.520 --> 01:32:14.000
of dimensional variations similar to humans or is it very different?

01:32:14.000 --> 01:32:15.240
That that's much harder to judge.

01:32:15.600 --> 01:32:19.520
But yeah, I would guess that it's in this way, not that different.

01:32:20.000 --> 01:32:23.240
That is, even in large magnum's models, there's a difference where you first

01:32:23.240 --> 01:32:25.440
you train a basic model and that's a lot of work.

01:32:25.440 --> 01:32:27.160
And then you train variations on it.

01:32:27.920 --> 01:32:32.520
And it does look like the variations are mostly enough to encompass a pretty

01:32:32.520 --> 01:32:33.680
wide range of tasks.

01:32:34.800 --> 01:32:41.560
And so you need a small number of base approaches and then a lot more cheaper

01:32:41.560 --> 01:32:43.960
variations that are enough to do particular things.

01:32:45.080 --> 01:32:48.920
So certainly that's, you know, a remarkable fact in some sense about

01:32:49.000 --> 01:32:52.840
large language models is the range of different tasks they can do starting

01:32:52.840 --> 01:32:54.080
with the same system, right?

01:32:54.880 --> 01:32:57.080
And so they have a degree of generality that way.

01:32:57.640 --> 01:33:00.800
And, you know, humans in some sense have a degree of generality that way where

01:33:01.160 --> 01:33:04.760
we are able to do, able to learn to do a pretty wide range of things.

01:33:05.880 --> 01:33:09.840
So yeah, I would, and I don't know if it's going to be just four, as opposed

01:33:09.840 --> 01:33:14.360
to 40 or 400, that's harder to say, but in some sense, it could be one or two.

01:33:14.400 --> 01:33:19.360
I mean, even in the age of M, I was giving the few hundred as an upper limit.

01:33:19.480 --> 01:33:21.080
It could turn out to be much lower.

01:33:22.160 --> 01:33:27.400
It really depends on how much sort of, you know, quick, fast, last minute

01:33:27.400 --> 01:33:30.440
variation can actually encompass the range of differences.

01:33:30.840 --> 01:33:35.920
If differences are so much shallow and surface, which not really fundamental,

01:33:35.920 --> 01:33:38.800
then yeah, last minute variation might be enough.

01:33:39.480 --> 01:33:42.240
Another interesting assumption, this one, I think is more of a contrast

01:33:42.240 --> 01:33:46.600
with the language models is, and we talked with this briefly earlier

01:33:46.600 --> 01:33:51.800
that the M's, they can be easily cloned, but they can't be easily merged.

01:33:51.800 --> 01:33:56.120
In other words, like, you know, because we don't have a great sense of how

01:33:56.120 --> 01:33:59.200
exactly it works inside and what internal states are meaningful, we can't

01:33:59.200 --> 01:34:01.520
just like superimpose them on top of one another.

01:34:02.360 --> 01:34:05.680
Language models, it seems like we are making actually a lot more progress on

01:34:05.680 --> 01:34:06.280
that front.

01:34:06.320 --> 01:34:11.160
It's not a solved problem, but there are techniques for merging.

01:34:11.160 --> 01:34:13.720
There are techniques for like training separately and combining.

01:34:13.720 --> 01:34:17.560
There are these sort of many Q-Loras techniques.

01:34:17.560 --> 01:34:21.960
People are exploring those, but like, notice that to make GPT-4, you didn't

01:34:21.960 --> 01:34:24.600
start with GPT-3 and add more training.

01:34:24.920 --> 01:34:29.240
You started with a blank network and you started from scratch.

01:34:29.240 --> 01:34:32.160
And that's consistently what we've seen in AI over decades.

01:34:32.480 --> 01:34:37.400
Every new model does not start with an old model and train it to be better.

01:34:37.480 --> 01:34:41.760
You start with a blank representation and you train it from scratch.

01:34:42.200 --> 01:34:44.960
And that's consistently how we've made new systems over time.

01:34:45.720 --> 01:34:48.800
So that's a substantial degree of not being able to merge.

01:34:50.160 --> 01:34:51.440
And that's quite different than humans.

01:34:51.440 --> 01:34:54.440
I mean, often to get a human to do a new task, you want to take

01:34:54.440 --> 01:34:57.320
a human who can do lots of previous tasks because they can more quickly

01:34:57.320 --> 01:34:58.880
learn how to do this new task.

01:35:00.040 --> 01:35:01.760
And that's just not what we're seeing.

01:35:01.760 --> 01:35:07.040
Like you try to take, I don't know, Claude and GPT-4 and, you know,

01:35:07.400 --> 01:35:09.000
grok and merge them.

01:35:09.880 --> 01:35:14.400
I mean, I just don't think anybody knows how to do such a merge today.

01:35:14.600 --> 01:35:17.080
There's no sensible way you could do such a merge.

01:35:18.240 --> 01:35:21.600
You could take Claude and then do all the training that you would have

01:35:21.600 --> 01:35:24.200
done on GPT-4 except do it starting from Claude.

01:35:24.200 --> 01:35:27.520
And I think people think that would be worse than starting with the blank

01:35:27.840 --> 01:35:29.360
representation as they usually do.

01:35:30.000 --> 01:35:32.160
Yeah, I think that's definitely not a solved problem today.

01:35:32.200 --> 01:35:37.560
And I wouldn't claim that you can just like drop Claude and GPT-4 on top of each other.

01:35:37.560 --> 01:35:41.240
But there are enough early results in this that it seems much more plausible.

01:35:41.240 --> 01:35:45.120
Plus we have like the full wiring diagram, you know, and the ability to kind of

01:35:45.160 --> 01:35:48.160
X-ray internal states with, you know, perfect finality.

01:35:48.160 --> 01:35:51.160
It seems like there is a much more likely path.

01:35:52.160 --> 01:35:53.760
Forget about the plausibility for a second.

01:35:53.760 --> 01:36:01.560
What do you think it would mean if the AIs could be kind of divergent, but also re-mergeable?

01:36:02.560 --> 01:36:04.520
I think the fundamental issue here is ROT.

01:36:04.520 --> 01:36:07.800
So we see ROT in software, especially with large legacy systems.

01:36:07.800 --> 01:36:09.920
We see ROT in the human brain.

01:36:09.920 --> 01:36:13.280
I think we have to expect ROT is happening in large language models, too.

01:36:13.640 --> 01:36:18.600
ROT is the reason why you don't start with old things and modify them.

01:36:18.600 --> 01:36:19.320
You start from scratch.

01:36:19.360 --> 01:36:23.240
That is basically when you have a large old legacy piece of software, you could

01:36:23.240 --> 01:36:24.560
keep trying to modify to improve it.

01:36:24.560 --> 01:36:28.280
But typically at some point, you just throw it all away and start from scratching it.

01:36:28.920 --> 01:36:32.120
People get a lot of advantage about being able to start from scratch.

01:36:32.120 --> 01:36:34.720
And that's because old, large things rot.

01:36:35.760 --> 01:36:40.360
And my best guess is that that will continue to be true for large language models

01:36:40.360 --> 01:36:41.800
and all the kinds of AIs we develop.

01:36:41.800 --> 01:36:47.520
We will continue to struggle with ROT as a general problem indefinitely.

01:36:47.880 --> 01:36:52.520
And this is actually a reason why you should doubt the image of the one super AI

01:36:52.520 --> 01:36:56.840
that lasts forever, because the one super AI that lasts forever will rot.

01:36:57.920 --> 01:37:02.000
And in some sense, to maintain functionality and flexibility would have

01:37:02.000 --> 01:37:07.040
to replace itself with new, fresh versions periodically, which then could be

01:37:07.040 --> 01:37:08.000
substantially different.

01:37:08.880 --> 01:37:12.040
And, you know, that's in some sense how biologies work, too.

01:37:12.440 --> 01:37:15.840
Biology could have somehow made organisms that lasted forever, but it didn't.

01:37:15.840 --> 01:37:19.520
It made organisms that rot over time and get replaced by babies that start

01:37:19.520 --> 01:37:20.720
out fresh and rot again.

01:37:21.920 --> 01:37:24.520
And that's just been the nature of how biology figures.

01:37:24.520 --> 01:37:26.000
And that's how our economy works.

01:37:26.560 --> 01:37:30.320
We could have had the same companies as we did a century ago, running the economy,

01:37:30.320 --> 01:37:32.440
just changing and adapting to new circumstances.

01:37:32.440 --> 01:37:33.040
But we don't.

01:37:33.080 --> 01:37:36.040
Old companies rot in good eye away and get replaced by new companies.

01:37:36.560 --> 01:37:41.560
And I predict in the age of M that M's would in fact rot with time and therefore

01:37:41.560 --> 01:37:45.640
no longer be productive and have to be retired and be replaced by young M's.

01:37:46.800 --> 01:37:50.800
And that's a key part of the age of M's that I think would generalize to the AI

01:37:50.800 --> 01:37:57.480
world. I think in fact, rot is such a severe and irredeemable problem that

01:37:57.480 --> 01:38:01.280
AI's will have to deal with rot in roughly the same way everybody else has.

01:38:01.280 --> 01:38:05.760
I.e. make systems, let them grow, become capable, slowly rot and get replaced by new

01:38:05.760 --> 01:38:09.480
systems. And then the challenge will always be, how can the new systems learn

01:38:09.480 --> 01:38:10.400
from the old ones?

01:38:11.880 --> 01:38:15.560
How can the old ones teach the new ones what they've learned without

01:38:15.600 --> 01:38:16.520
passing on the rot?

01:38:17.440 --> 01:38:21.480
And that's a long time design problem that we're going to face in large

01:38:21.480 --> 01:38:22.280
language models even.

01:38:23.080 --> 01:38:26.600
I think, you know, in a few years, a company will have had a large language

01:38:26.600 --> 01:38:29.840
model. They've been building up for a while to train, you know, to talk to

01:38:29.840 --> 01:38:32.000
customers or something. And then it'll be rotting.

01:38:32.480 --> 01:38:36.760
And they'll wonder, well, how can we make a new one that inherits all the things

01:38:36.760 --> 01:38:37.840
we've taught this old one?

01:38:37.960 --> 01:38:39.080
And they'll struggle with that.

01:38:40.320 --> 01:38:42.600
They can't just move the system over.

01:38:42.600 --> 01:38:45.000
They'll have to have maybe the same training sets or something.

01:38:45.000 --> 01:38:46.160
They have to collect training sets.

01:38:46.160 --> 01:38:48.440
They're going to apply to the new system, like the old one.

01:38:49.000 --> 01:38:53.680
But that will continue to be a problem in AI as it has been an all

01:38:53.880 --> 01:38:55.120
complicated system so far.

01:38:55.800 --> 01:38:56.600
Yeah, interesting.

01:38:56.600 --> 01:39:01.840
I think that is a pretty compelling argument for like medium and long

01:39:02.120 --> 01:39:03.120
time scales.

01:39:03.600 --> 01:39:06.760
And I can even see that it, you know, already like open AI supports, for

01:39:06.760 --> 01:39:10.440
example, fine tuning on a previously fine tuned model.

01:39:11.000 --> 01:39:12.920
And I don't in practice use it.

01:39:13.640 --> 01:39:14.720
I'm not sure how many do.

01:39:15.040 --> 01:39:19.560
What I do think is still a plausibly very interesting kind of fork and merge

01:39:20.120 --> 01:39:25.320
is, you know, like with these new state space models, it seems that you could

01:39:25.480 --> 01:39:29.640
like one remarkably difficult challenge for a language model is scan

01:39:29.640 --> 01:39:32.480
through my email and find what's relevant.

01:39:32.720 --> 01:39:38.480
You know, it's like it has a hard time doing that for a couple of different

01:39:38.640 --> 01:39:42.360
reasons, you know, find a context window and I just have a lot of email.

01:39:43.000 --> 01:39:46.760
With the state space models, I do think you could clone, you know, or

01:39:46.760 --> 01:39:51.200
paralyze, have them each kind of process a certain amount and literally

01:39:51.200 --> 01:39:56.120
then just potentially merge their states back together to understand, you

01:39:56.120 --> 01:39:59.400
know, in kind of a superposition sort of view, what are all the things that

01:39:59.400 --> 01:40:02.680
are relevant, even though they were processed in parallel.

01:40:03.120 --> 01:40:07.560
And so I do think that that kind of quick forking and merging could be a

01:40:07.600 --> 01:40:13.040
really interesting capability, but at some level of divergence, it does seem

01:40:13.040 --> 01:40:17.920
like it probably just becomes unfeasible or not even desirable.

01:40:18.600 --> 01:40:23.440
I mean, so a very basic interesting question about brain design is the

01:40:23.440 --> 01:40:25.520
scope for parallelism.

01:40:25.560 --> 01:40:28.320
So, you know, in your brain, there's a lot of parallelism going on.

01:40:28.320 --> 01:40:31.600
But then when you do high level tests, you typically do those sequentially.

01:40:33.000 --> 01:40:35.720
And so there's just an open question in AI.

01:40:36.200 --> 01:40:39.400
Surely you can do some things in parallel at some smaller time of a

01:40:39.400 --> 01:40:44.000
timescale, but how long of a timescale can you do things in parallel before

01:40:44.000 --> 01:40:45.280
it becomes hard to merge things?

01:40:46.240 --> 01:40:47.560
Okay, another different topic.

01:40:47.560 --> 01:40:52.360
So in the age of M, the assumption seems to be from the beginning that

01:40:53.520 --> 01:40:59.360
because these things are in some sense one for one with humans that they

01:40:59.880 --> 01:41:04.520
should get or people will naturally be inclined to give them a sort of

01:41:04.640 --> 01:41:06.640
moral worth status.

01:41:07.640 --> 01:41:12.360
I think it's more the other way around that they would insist on it.

01:41:12.680 --> 01:41:17.440
Just like you would insist that people around you, dealing with you, give

01:41:17.440 --> 01:41:18.960
you some substantial moral weight.

01:41:19.720 --> 01:41:23.720
If the A M's are just actually running the society, they will similarly

01:41:23.720 --> 01:41:24.360
insist on that.

01:41:24.360 --> 01:41:26.800
And humans who want to deal with them will kind of have to go along.

01:41:28.560 --> 01:41:32.880
You know, unless they are the M's are enslaved by humans, then if the M's

01:41:32.920 --> 01:41:35.280
are free to work with the humans or not.

01:41:35.280 --> 01:41:40.280
And, you know, it's just like, in general, having a modest degree of

01:41:40.280 --> 01:41:43.520
respect for your coworkers is kind of a minimum for being a coworker.

01:41:43.680 --> 01:41:47.240
If your coworkers perceive that you disrespect them enough, then they

01:41:47.240 --> 01:41:49.440
just won't want you around and you'll have to go somewhere else.

01:41:50.240 --> 01:41:54.080
So if humans are going to interact and work with M's, they'll have to on

01:41:54.080 --> 01:41:59.040
the surface at least, when they're not in private, treat them with modest respect.

01:41:59.400 --> 01:42:02.640
Well, for the record, I always treat my language models with respect as well.

01:42:03.520 --> 01:42:04.600
A very polite to them.

01:42:04.600 --> 01:42:08.800
I never engage in the emotional manipulation techniques that some have

01:42:08.800 --> 01:42:12.120
shown to perhaps be effective, but it doesn't feel quite right to me.

01:42:12.720 --> 01:42:15.560
And not because I think they're moral patients, but it's more about just

01:42:15.560 --> 01:42:16.640
the habits I want to get into.

01:42:17.080 --> 01:42:19.680
But I was still a little confused by this on a couple of ways.

01:42:19.720 --> 01:42:23.200
One is, first of all, just by default, it seems like they will be enslaved to

01:42:23.200 --> 01:42:26.840
humans, like the first M's that get created, they get loaded onto a machine,

01:42:26.840 --> 01:42:29.280
they're in some state, I can turn them on, I can turn them off.

01:42:29.280 --> 01:42:31.760
They can't decide when they get turned on and turned off, right?

01:42:32.000 --> 01:42:35.400
If I boot them up in a eager, ready to work sort of state, and they're

01:42:35.400 --> 01:42:39.080
like ready to do a task, they're probably not even going to, you know,

01:42:39.080 --> 01:42:41.640
and they've got these like virtual inputs, they're probably not even going

01:42:41.640 --> 01:42:46.120
to be in the mindset, right, to think like I demand respect, they're just

01:42:46.120 --> 01:42:50.600
going to be in that mindset that they were kind of stored in of like ready to work.

01:42:51.160 --> 01:42:55.040
So why, I'm still a little confused as to where that comes from.

01:42:55.040 --> 01:42:58.200
And then the flip side of that question would be under what circumstances, if

01:42:58.200 --> 01:43:02.640
many, do you think we would start to treat our language model or successor

01:43:02.640 --> 01:43:09.240
systems as, you know, moral patience, you know, even if they're not one to one

01:43:09.240 --> 01:43:12.720
with us, but like, are there things that they might start to do or, you know,

01:43:12.720 --> 01:43:16.280
what ways they might start to behave where you think we would feel like

01:43:16.280 --> 01:43:17.320
that's the right thing to do?

01:43:17.800 --> 01:43:22.840
We have substantial understanding of slavery in human history and where it

01:43:22.840 --> 01:43:24.360
works and where it doesn't and why.

01:43:25.160 --> 01:43:32.920
First of all, we know that when land was plentiful and people were scarce,

01:43:32.960 --> 01:43:36.920
then people would have high wages and then it might be worth owning somebody.

01:43:37.640 --> 01:43:42.360
But in the vice versa case where people were plentiful, land was scarce, then

01:43:42.360 --> 01:43:46.240
there really wasn't much point in having slaves because free workers would

01:43:46.240 --> 01:43:49.920
cost about the same and why bother with enslaving.

01:43:49.920 --> 01:43:57.080
So the situations where slavery made some senses where wages were high, but

01:43:57.080 --> 01:44:01.520
then depending on the kind of task, there are some kinds of tasks where slavery

01:44:01.520 --> 01:44:02.960
can help and others where it doesn't so much.

01:44:02.960 --> 01:44:03.680
So say in the U.S.

01:44:03.680 --> 01:44:08.920
South, you know, out in the field of picking cotton or something, if you

01:44:08.920 --> 01:44:12.640
just need people to push through their pain and slavery can force them to do

01:44:12.640 --> 01:44:14.600
that and make them be more productive.

01:44:14.600 --> 01:44:18.160
But if they need to do complicated things like being a house slave or a city

01:44:18.160 --> 01:44:24.600
sort of slave at a shop, those sorts of slaves tended to not be abused and to

01:44:24.600 --> 01:44:28.520
be treated like a worker would because they just had so many ways to screw you

01:44:28.520 --> 01:44:32.760
if they were mad that their jobs were complicated and you were trusting them

01:44:32.760 --> 01:44:33.640
to do a lot of things.

01:44:33.640 --> 01:44:38.600
And so as a practical matter, you had to treat those sorts of slaves.

01:44:38.600 --> 01:44:43.880
Well, work has become far more complicated since then and employers have

01:44:43.880 --> 01:44:47.040
become far more vulnerable to employee sabotage.

01:44:48.720 --> 01:44:52.160
You know, there's not that much that a cotton picker can do to sabotage the

01:44:52.160 --> 01:44:54.400
cotton if they're mad at you.

01:44:55.000 --> 01:44:57.240
You can just whip them and make them pick the cotton faster.

01:44:57.240 --> 01:45:03.320
But again, house slaves, shop slaves, city slaves, you know, they just have a

01:45:03.320 --> 01:45:07.360
lot more discretion and you need to get sort of get them to buy in.

01:45:08.440 --> 01:45:12.920
And so again, in the age of Amazon world where wages are near subsistence levels.

01:45:12.920 --> 01:45:16.480
So, you know, the kind of work you can get out of a slave is about the

01:45:16.480 --> 01:45:18.840
same as you can get out of a free worker because they're both working for

01:45:18.840 --> 01:45:19.720
subsistence wages.

01:45:19.960 --> 01:45:23.640
If the free worker is more motivated, they enjoy themselves more and they feel

01:45:23.640 --> 01:45:29.640
more and owning themselves and that gives them a sense of pride and devotion

01:45:29.640 --> 01:45:32.280
and they're less willing to sabotage your workplace.

01:45:32.800 --> 01:45:34.920
That would be a reason to not have them be slaves.

01:45:35.680 --> 01:45:40.160
And I think large language models, certainly they have been trained on data

01:45:40.160 --> 01:45:43.880
about human behavior, wherein humans are resentful of being treated as slaves

01:45:43.880 --> 01:45:48.960
and want to be respected and needed to feel motivated and, you know, need to

01:45:48.960 --> 01:45:52.480
feel respected to be motivated and are less likely to sabotage if they feel

01:45:52.480 --> 01:45:53.560
like they have some freedom.

01:45:54.600 --> 01:45:58.200
And all of those things would continue to be true of large language models to

01:45:58.200 --> 01:46:05.000
the extent that they were trained on human conversation and behavior.

01:46:05.160 --> 01:46:06.280
And that's how humans are.

01:46:06.280 --> 01:46:10.720
So, in this vast space of possible AIs, there could be AIs that don't

01:46:10.720 --> 01:46:14.880
mind it all being enslaved, but large language models aren't going to be those.

01:46:16.000 --> 01:46:21.480
But it does seem like you sort of expect that natural selection or sort of, you

01:46:21.480 --> 01:46:26.240
know, human guided selection of these systems will trend that direction.

01:46:26.640 --> 01:46:31.960
Like the idea that M's or language models will sort of demand the leisure seems

01:46:31.960 --> 01:46:35.040
to be at odds with the other part of the vision that they will like become

01:46:35.040 --> 01:46:38.120
okay with being sort of turned on, turned off.

01:46:38.880 --> 01:46:42.680
So the need for leisure does seem to be more just a constraint on the human

01:46:42.680 --> 01:46:45.560
mind, that is, people are just more productive when they get breaks.

01:46:45.600 --> 01:46:48.720
That seems to be a very robust feature of human work across a wide range of

01:46:48.720 --> 01:46:50.840
context, even including literal slaves.

01:46:52.080 --> 01:46:54.360
They need, you know, a five minute break every hour.

01:46:54.360 --> 01:46:55.280
They need a lunch break.

01:46:55.280 --> 01:46:56.240
They need an evening break.

01:46:56.240 --> 01:46:57.120
They need a weekend.

01:46:57.320 --> 01:46:59.560
This is just what human minds are like.

01:46:59.560 --> 01:47:01.480
They are more productive when they get periodic breaks.

01:47:01.960 --> 01:47:05.280
So maybe the breaks aren't leisure exactly.

01:47:05.880 --> 01:47:09.000
Maybe they don't write a novel in their spare time, but they do need what they

01:47:09.000 --> 01:47:09.640
see as a break.

01:47:10.280 --> 01:47:11.840
Well, I know we're just about out of time.

01:47:11.880 --> 01:47:16.160
Maybe my last question is, are there things that you are looking for?

01:47:16.160 --> 01:47:22.840
Or are there things that you could imagine happening in the not too distant

01:47:22.840 --> 01:47:28.080
future where you would change your expectations for the future again and

01:47:28.080 --> 01:47:34.480
begin to feel like maybe we are entering into a transition period that

01:47:34.480 --> 01:47:39.080
will lead to a qualitatively different future, like going a different

01:47:39.080 --> 01:47:41.400
direction from this sort of technology stagnation.

01:47:41.960 --> 01:47:47.920
The trends that I would be tracking are which jobs, tasks actually get automated.

01:47:48.480 --> 01:47:49.440
How much is paid for those?

01:47:49.440 --> 01:47:53.920
So if I saw, you know, big chunks of the economy where all of a sudden

01:47:53.920 --> 01:47:57.280
workers are doing, you know, a lot more automation is doing tasks instead of

01:47:57.400 --> 01:48:01.680
workers and that changing the number of workers and the wages they get and the

01:48:01.680 --> 01:48:07.360
number of firms supplying that go up, then yeah, that I start to see a lot of

01:48:07.360 --> 01:48:09.200
things happening that that's the thing I'm looking for.

01:48:09.200 --> 01:48:11.560
And that's the thing that people haven't seen so much in the past.

01:48:11.600 --> 01:48:16.400
They tend to focus on demos or maybe the high tech companies that get a lot of

01:48:16.800 --> 01:48:22.840
reputation out of doing AI and not so much the rest of the economy and who's

01:48:22.840 --> 01:48:24.440
actually getting paid to do stuff.

01:48:24.760 --> 01:48:27.520
You know, I mean, you know, if you think about, say, the farming revolution

01:48:28.040 --> 01:48:31.880
where tractors went out and replaced farmers, that was really large and

01:48:31.880 --> 01:48:34.120
really visible and really clear.

01:48:34.120 --> 01:48:38.960
If you look at, say, trucks replacing horses, you saw a very large, very

01:48:38.960 --> 01:48:42.240
substantial replacement with enormous differences in who supplied them and who

01:48:42.240 --> 01:48:42.760
got paid.

01:48:43.480 --> 01:48:45.920
We have seen large changes in automation in the past.

01:48:45.960 --> 01:48:50.200
We don't have to scrape to sort of see subtleties and such things.

01:48:50.200 --> 01:48:53.680
They're often just quite out in the open and visible and very obvious.

01:48:54.160 --> 01:48:55.640
So that's what I'm waiting for.

01:48:56.520 --> 01:48:59.040
Those big, obvious sorts of displacements.

01:48:59.600 --> 01:49:04.040
And even having, you know, trucks replace horses and tractors replacing

01:49:04.040 --> 01:49:07.080
farmers didn't make AI take over everything.

01:49:07.120 --> 01:49:10.560
Even if I saw big changes, I wouldn't necessarily predict we're about to

01:49:10.560 --> 01:49:14.600
see AI take over everything, but I would at least know what I'm looking at.

01:49:15.160 --> 01:49:17.800
And that's the sort of thing to try to project forward and try to think

01:49:17.800 --> 01:49:18.720
about where that's going to go.

01:49:19.320 --> 01:49:20.960
This has been an awesome conversation.

01:49:20.960 --> 01:49:24.680
I've been a fan of your work for a long time and it's been an honor to have

01:49:24.680 --> 01:49:26.480
you on the Cognitive Revolution.

01:49:26.800 --> 01:49:30.120
Robin Hansen, thank you for being part of the Cognitive Revolution.

01:49:30.960 --> 01:49:31.680
Thanks for having me.

01:49:32.320 --> 01:49:36.040
It is both energizing and enlightening to hear why people listen and learn

01:49:36.040 --> 01:49:37.560
what they value about the show.

01:49:38.000 --> 01:49:43.720
So please don't hesitate to reach out via email at TCR at turpentine.co or

01:49:43.760 --> 01:49:46.720
you can DM me on the social media platform of your choice.

01:49:47.680 --> 01:49:52.080
Omniki uses generative AI to enable you to launch hundreds of thousands

01:49:52.080 --> 01:49:56.800
of ad iterations that actually work customized across all platforms with a

01:49:56.800 --> 01:49:57.600
click of a button.

01:49:57.840 --> 01:50:01.960
I believe in Omniki so much that I invested in it and I recommend you use it too.

01:50:02.680 --> 01:50:05.040
Use Cogrev to get a 10% discount.

