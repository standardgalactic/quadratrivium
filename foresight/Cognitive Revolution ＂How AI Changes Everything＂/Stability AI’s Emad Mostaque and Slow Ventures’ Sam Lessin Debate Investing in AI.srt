1
00:00:00,000 --> 00:00:03,200
I think a key question to this is like, you know, people say hallucinations.

2
00:00:03,840 --> 00:00:08,000
I was like, what does that mean? Well, I mean, it doesn't get every single fact completely right.

3
00:00:08,640 --> 00:00:12,800
Challenge E.P.T. is probably like 100 gigabytes down from like 10 trillion words.

4
00:00:13,440 --> 00:00:17,280
The fact you can get anything right is an absolute technical marvel that

5
00:00:17,280 --> 00:00:22,240
no one's really sure exactly how that happens. What if you had an AI tutor for every child?

6
00:00:22,880 --> 00:00:25,760
What does that look like? What if you had 100 AI tutors for every child?

7
00:00:25,760 --> 00:00:31,680
For the first time, every single person can have hundreds of characters that like and

8
00:00:31,680 --> 00:00:35,920
support them all the time. Basically, you log into social media or whatever, and you're like,

9
00:00:35,920 --> 00:00:39,840
hey, I'm Sam, and he's like, cool, what type of people, instead of who do you want to follow?

10
00:00:39,840 --> 00:00:43,440
It's like, who do you want to follow you? For example, there aren't enough therapists in the

11
00:00:43,440 --> 00:00:48,800
world, you know, and it is a regulated industry. But at the same time, there is a gap for therapists,

12
00:00:48,800 --> 00:00:52,720
just like you have the meditation apps kind of step in, and they created calm and they

13
00:00:52,720 --> 00:00:57,120
created these other things that were huge. Hello, and welcome to the Cognitive Revolution,

14
00:00:57,120 --> 00:01:01,600
where we interview visionary researchers, entrepreneurs, and builders working on the

15
00:01:01,600 --> 00:01:06,800
frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas,

16
00:01:06,800 --> 00:01:11,200
and together we'll build a picture of how AI technology will transform work,

17
00:01:11,200 --> 00:01:17,120
life, and society in the coming years. I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.

18
00:01:17,760 --> 00:01:21,760
Hello, and welcome back to the Cognitive Revolution. Today's episode is a super

19
00:01:21,760 --> 00:01:25,920
interesting one on a number of levels, as we're hosting a discussion between two

20
00:01:25,920 --> 00:01:31,520
super influential technology thinkers. Sam Lesson, former VP of product at Facebook,

21
00:01:31,520 --> 00:01:38,080
now early stage technology investor and writer, Andy Modmostock, founder and CEO of Stability AI,

22
00:01:38,080 --> 00:01:43,360
whose work at Stability, highlighted of course by Stable Diffusion, has already been incredibly

23
00:01:43,360 --> 00:01:48,800
influential, but has also come under intense scrutiny in the months since he raised $100

24
00:01:48,880 --> 00:01:55,440
million at a $1 billion valuation. This conversation started on Twitter with a short essay that Sam

25
00:01:55,440 --> 00:02:02,240
wrote, arguing that AI is mostly a bad investment for VC. Imad responded and suggested a podcast on

26
00:02:02,240 --> 00:02:09,280
the topic, and Eric and I were naturally happy to volunteer to host. Both Sam and Imad talk fast.

27
00:02:09,280 --> 00:02:15,360
This is a 1.5x speed episode for me, down from the usual 2x, and both had a lot to say,

28
00:02:15,440 --> 00:02:19,440
so we mostly let them speak directly to each other before I jumped in at the end,

29
00:02:19,440 --> 00:02:23,840
to ask some concrete question. I think regular listeners to the show will know that I definitely

30
00:02:23,840 --> 00:02:30,240
share Sam's point of view around investing in AI. AI may well disrupt society at large,

31
00:02:30,240 --> 00:02:34,880
but it doesn't seem likely to disrupt many existing SaaS markets between now and then.

32
00:02:35,840 --> 00:02:40,720
There will, as Sam says, always be exception, but for someone whose focus is on looking for

33
00:02:40,720 --> 00:02:46,000
those early-stage companies with 100x potential investment returns, I think he's quite right

34
00:02:46,000 --> 00:02:50,640
that there'll be few and far between, at least at the model and application layers.

35
00:02:52,080 --> 00:02:56,080
For what it's worth, though, I do think Sam is quite wrong to limit his thinking about LLM

36
00:02:56,080 --> 00:03:02,960
function to the no-real-intelligence-just-association-between-words paradigm. There is now ample

37
00:03:02,960 --> 00:03:08,240
mechanistic interpretability work that shows quite conclusively that AI models are indeed

38
00:03:08,240 --> 00:03:14,000
groping much more than statistical correlation, but that's a topic for another episode.

39
00:03:15,120 --> 00:03:20,400
For today, the subtext of the conversation seemed to be this question. Will stability AI

40
00:03:20,400 --> 00:03:26,480
prove to be one of those exceptional, highly successful startups deserving of its

41
00:03:26,480 --> 00:03:32,800
unicorn status and valuation? From my standpoint, the answer may depend on your definition of

42
00:03:32,800 --> 00:03:38,880
success. Stability is as much a movement as a company and has already left an indelible mark

43
00:03:38,880 --> 00:03:44,560
on AI open-source culture. Their impact goes beyond the groundbreaking stable diffusion,

44
00:03:44,560 --> 00:03:50,960
including major dataset releases such as the Lyon 5 billion image dataset, various language models

45
00:03:50,960 --> 00:03:55,600
and accompanying open-source RLHF libraries, which enable further downstream training and

46
00:03:55,600 --> 00:04:00,560
customization, and many, many other projects across a wide range of modalities.

47
00:04:00,560 --> 00:04:06,720
They've also established themselves as tremendous identifiers of and supporters of talent,

48
00:04:07,360 --> 00:04:13,680
including another upcoming guest, 19-year-old PhD Tanishk Matthew Abraham, who just published a

49
00:04:13,680 --> 00:04:21,200
literal mind-reading paper that converts fMRI data into reconstructed images of what the person saw,

50
00:04:21,200 --> 00:04:27,200
truly mind-blowing work. But perhaps more important than any of that has been

51
00:04:27,200 --> 00:04:32,880
a mod's unique ability to articulate an inspiring vision for the future of AI.

52
00:04:32,880 --> 00:04:37,520
While the positive vision of AI that we tend to hear when we hear one at all often centers

53
00:04:37,520 --> 00:04:42,880
around the possibility of large and powerful AGI's, of which there might only be a few,

54
00:04:42,880 --> 00:04:48,560
presumably built and owned by leading technology firms. A mod has not only signed the AGI pause

55
00:04:48,560 --> 00:04:54,320
letter and the extinction risk statement, but has articulated a very different positive vision

56
00:04:54,320 --> 00:04:59,920
for a panoply of smaller AI models, mostly presumably derived from the open-source standards

57
00:04:59,920 --> 00:05:05,840
that he and the team at Stability are creating, but all highly specialized for specific purposes

58
00:05:05,840 --> 00:05:11,680
and localized to specific contexts and culture. This is an extremely appealing notion to billions

59
00:05:11,680 --> 00:05:16,320
of people around the world who don't want to be beholden to American or, for that matter,

60
00:05:16,320 --> 00:05:22,080
Chinese corporations or their access to AI. A mod has been criticized recently for allegedly

61
00:05:22,080 --> 00:05:26,560
exaggerating certain claims and affiliations, and for some operational problems at stability

62
00:05:26,560 --> 00:05:32,400
that resulted in people sometimes being paid late. And while some of that may well have happened,

63
00:05:32,400 --> 00:05:36,000
I will say that I've followed a mod quite closely now for at least a year,

64
00:05:36,000 --> 00:05:41,840
and have generally found him to be very reasonable. He has, for example, always recognized the reality

65
00:05:41,840 --> 00:05:47,120
of open AI in Google's modes, and has projected that open-source models will continue to lag

66
00:05:47,120 --> 00:05:52,320
leading closed-source models by a year or more. All of this seems quite right and reasonable

67
00:05:52,320 --> 00:05:58,000
to me. Given a mod's comments about the centrality of stories, I think it's safe to say he understands

68
00:05:58,000 --> 00:06:03,120
the task of developing a positive vision for AI, a vision that others can really buy into

69
00:06:03,120 --> 00:06:09,920
as a core part of his role and strategy. This is quite different from other AI CEOs who often

70
00:06:09,920 --> 00:06:15,200
seem to be sharing their plans more for your information than for your input, and it really

71
00:06:15,200 --> 00:06:20,480
does seem to be working. I've joined the discords of many stability-affiliated projects,

72
00:06:20,480 --> 00:06:25,040
and have been very impressed with the quality of people and conversations that they contain.

73
00:06:25,680 --> 00:06:30,480
So, whether stability will ultimately deliver a great return for the investors who bought in at

74
00:06:30,480 --> 00:06:36,080
that $1 billion dollar valuation is, for me, not the most interesting question about the company.

75
00:06:37,040 --> 00:06:40,880
I'd be very surprised if they failed outright given the quality of talent that they have,

76
00:06:41,520 --> 00:06:45,760
and so the question that matters more to me is simply, what impact will they have?

77
00:06:46,400 --> 00:06:52,720
Will their push toward decentralization prove democratizing, destabilizing, or both? If you

78
00:06:52,720 --> 00:06:58,240
fear centralization of power and you want to see a rich ecology of AI's develop around the world,

79
00:06:58,240 --> 00:07:04,880
you might expect their contribution to be extremely positive. If, on the other hand, you fear chaos

80
00:07:04,880 --> 00:07:10,480
and see AI's as invasive species colonizing niche after niche and ultimately perhaps competing with

81
00:07:10,480 --> 00:07:15,120
humans, you might feel quite the opposite indeed. For my part, as you can probably guess,

82
00:07:15,120 --> 00:07:19,920
I expect the outcome will ultimately be a bit of both. Throughout this conversation,

83
00:07:19,920 --> 00:07:24,720
you'll hear just how much change both Sam and Imad take for granted as they think about the future.

84
00:07:25,520 --> 00:07:30,000
Culture, entertainment, and relationships, they agree, are in for a shock.

85
00:07:31,440 --> 00:07:35,200
The global south may well have leapfrog moments in education and even medicine.

86
00:07:36,160 --> 00:07:42,560
Online communities may come to contain AI characters that we can't even identify as non-human.

87
00:07:42,560 --> 00:07:46,960
Given the magnitude of all these changes and the resources and talent that Imad has amassed,

88
00:07:46,960 --> 00:07:52,720
the inspiration he's provided, and the tremendous global need that AI seems so well suited to fill,

89
00:07:53,680 --> 00:07:57,200
I think stability has a real chance not only to become a great company,

90
00:07:57,200 --> 00:08:04,320
but to help shape a global universal basic intelligence standard, a potentially historic

91
00:08:04,400 --> 00:08:08,960
development. How humans ultimately wield the new power that Imad and others unlock,

92
00:08:08,960 --> 00:08:13,520
and whether we can control AI long-term at all, is much harder to predict,

93
00:08:14,720 --> 00:08:22,160
but can ultimately only go one way or the other. Now, I hope you enjoy this fast-paced

94
00:08:22,160 --> 00:08:28,560
conversation with Imad Mostak and Sam Lesson. I think that large language models and a lot of

95
00:08:28,560 --> 00:08:33,200
the AI stuff that we're seeing kind of start to get consumerized right now and become real,

96
00:08:33,200 --> 00:08:38,560
it's super cool. There's no question about that. There are absolutely going to be

97
00:08:39,120 --> 00:08:45,520
great product experiences improved by it and opportunities to create more efficiency,

98
00:08:45,520 --> 00:08:51,440
create better interfaces. I am not negative on how some of the stuff will find its way

99
00:08:51,440 --> 00:08:56,720
into consumer product experiences and make things better. My wife's company,

100
00:08:56,720 --> 00:09:01,200
the publication information, we've already deployed a bunch of AI stuff that makes

101
00:09:01,200 --> 00:09:06,000
search for the information go from absolutely terrible to pretty good, and there's a bunch

102
00:09:06,000 --> 00:09:12,080
more stuff coming that will get better. I'm not against that. I do think the things that I keep

103
00:09:12,080 --> 00:09:19,360
in mind, one as an investor, is I think the case about why a bunch of this technology is going to

104
00:09:19,360 --> 00:09:26,720
make meta, and Amazon, and Google, and a bunch of big players, an assload of money are clear.

105
00:09:27,520 --> 00:09:33,920
I think the idea that it is a wedge or an angle that's going to allow a bunch of companies from

106
00:09:33,920 --> 00:09:38,800
zero to come out of nowhere and then become wildly profitable or compete with those guys,

107
00:09:38,800 --> 00:09:44,000
types of big players, I think is much more sus, as they would say. It's because I mean,

108
00:09:44,000 --> 00:09:47,760
to really take advantage of the stuff, you need a ton of distribution, you need a ton of data,

109
00:09:47,760 --> 00:09:54,880
and I really see a lot of what I've seen is opportunities to extend innovation that already

110
00:09:54,880 --> 00:10:00,560
exists versus completely reshuffled the deck. That's my big thing. I am very bullish on crypto

111
00:10:00,560 --> 00:10:06,160
long-term. Crypto is undeniably whatever you think of it, a deck reshuffled. AI, and what we're

112
00:10:06,160 --> 00:10:10,640
seeing is not a deck reshuffled from my perspective as an extender. People come pitch me, we're going

113
00:10:10,640 --> 00:10:18,640
to be the Adobe of AI. Adobe is going to be the Adobe of AI, from my deployment. I think

114
00:10:18,640 --> 00:10:21,760
it's a very tough one to see. Will there be exceptions? Of course there will be exceptions.

115
00:10:21,760 --> 00:10:26,480
There will be exceptions, but I think it's a seen thing. I think it's hard. I'd also say as an investor,

116
00:10:26,480 --> 00:10:35,040
a seed investor, which is how I earn my daily bread. I'd say that the opportunities to deploy

117
00:10:35,040 --> 00:10:40,800
a few million dollars, turn over a card and have an experience like, oh my god, there's something

118
00:10:40,800 --> 00:10:44,960
here, now let's have a series A investor put a ton more money in and see it scale up, I think are

119
00:10:44,960 --> 00:10:50,080
few and far between. And because everyone's so excited, everything's way mispriced. And so for

120
00:10:50,080 --> 00:10:55,040
me as an investor, I think it's an extremely hard market to get excited about. What else can I say?

121
00:10:55,040 --> 00:10:59,920
I mean, look, I do think that the elephant in the room, which I'm sure we can discuss or not, is

122
00:11:01,120 --> 00:11:07,760
for the companies that have gone out so far, talk about chat, GPT, I think there's huge regulatory

123
00:11:07,760 --> 00:11:12,160
problems which are becoming clearer. And it's not about the machine is going to eat us all.

124
00:11:12,160 --> 00:11:16,400
I think that's a load of crap. And it's been in the record for quite some time,

125
00:11:16,400 --> 00:11:20,800
being very, very negative and cynical about kind of a lot of those narratives. I mean,

126
00:11:20,800 --> 00:11:25,440
at the end of the day, token guessing, guess the next token is not a fundamentally dangerous

127
00:11:25,440 --> 00:11:29,840
piece of technology. I do think that the copyright issues are deeply real

128
00:11:30,480 --> 00:11:34,000
and complicated. And there's a bunch of other challenges that these guys are going to face

129
00:11:34,000 --> 00:11:37,680
that, you know, again, because the world has a general viewpoint of like,

130
00:11:37,680 --> 00:11:42,880
fool me once, shame on you, fool me twice, shame on me is, you know, the era of from social media

131
00:11:42,880 --> 00:11:47,200
to Uber to whatever, like, I think people are going to be way more quick reactive to like what's

132
00:11:47,200 --> 00:11:52,800
going on from regulatory environment here, I hope, then that historically. But I don't know,

133
00:11:52,800 --> 00:11:55,680
that's a ton of ground. And I don't know, where do you want to go?

134
00:11:56,800 --> 00:12:00,400
Yeah, no, there's a ton of ground. I think, you know, there's this question of, is this a

135
00:12:00,400 --> 00:12:04,480
disruptive or sustaining innovation? And the question of what this is, you know, you have

136
00:12:04,480 --> 00:12:09,120
the classical big data and then you extrapolate it to sell you ads. And that was good old internet.

137
00:12:09,120 --> 00:12:14,160
And it created these kind of behemoths in matter and Google in particular. But then you have the

138
00:12:14,160 --> 00:12:19,040
application of computer vision and these other things largely to the incumbents. So value was

139
00:12:19,040 --> 00:12:23,920
captured there. I was at your mobile and mobile's a great example of like, just double down, right?

140
00:12:23,920 --> 00:12:29,200
Yeah. And that's why kind of Facebook's first shift, my while was good. Next shift to matter.

141
00:12:29,200 --> 00:12:34,160
And maybe they'll rename themselves spatial or something. But, you know, this becomes very

142
00:12:34,160 --> 00:12:38,480
interesting. Because like, these models are something a bit different. So I would stable

143
00:12:38,480 --> 00:12:43,040
the fusion to 100,000 gigs of images and the app was a two gig file. And it was four of the top

144
00:12:43,040 --> 00:12:48,320
10 absolutely app store in December. We're having that as the entire backend. You put words in and

145
00:12:48,320 --> 00:12:52,720
images pop out and it makes pretty pictures in your face, right? But then they all dropped off and

146
00:12:52,720 --> 00:12:57,760
they disappeared. Because there are more features than apps, they're cool features. But they weren't

147
00:12:57,760 --> 00:13:02,160
kind of product experiences. That is exactly what happened when the apps were launched, right? You

148
00:13:02,160 --> 00:13:07,600
had like fart apps as number one for $599. There's a brief moment where it's cool and you're experimenting

149
00:13:07,600 --> 00:13:12,160
with it and you have these kind of poops, right? But they're not. Yeah, I think like poops. Yeah,

150
00:13:12,160 --> 00:13:16,880
you're right. Exactly. Literally. It's not real because you have to have the user experience

151
00:13:16,880 --> 00:13:21,280
and build products like normal. But where I feel right now is that we're at the primitive stage

152
00:13:21,280 --> 00:13:27,280
and very boring in terms of one to one interaction is very boring. I think it is again, very surface

153
00:13:27,280 --> 00:13:32,960
level without any memory. And it's ephemeral and fleeting. My thing is that probably iPhone 2G,

154
00:13:32,960 --> 00:13:37,120
iPhone 3G bit, we're just getting copy paste. Because what's happened is you've got technology

155
00:13:37,120 --> 00:13:42,240
that's gone from research and it's now starting to go into engineering. What are the design patterns

156
00:13:42,240 --> 00:13:46,640
for this? How is it implemented? Was it good for? I think a key question to this is like, you know,

157
00:13:46,640 --> 00:13:51,440
people say hallucinations. I was like, what does that mean? Well, I mean, it doesn't get every

158
00:13:51,440 --> 00:13:57,200
single fact completely right. Charge GPT is probably like 100 gigabytes down from like 10 trillion

159
00:13:57,200 --> 00:14:02,560
words. The fact you can get anything right is an absolute technical marvel that no one's really

160
00:14:02,640 --> 00:14:07,840
sure exactly how that happens. You know, it's like in a pipe pipe from Silicon Valley. Like,

161
00:14:07,840 --> 00:14:11,120
that Weisman stool would be even more intensive, even repress all that knowledge.

162
00:14:11,840 --> 00:14:14,720
Because what these really are, they're reasoning machines. They're not facts,

163
00:14:14,720 --> 00:14:17,760
because we've got two parts of our brain. Are they reasoning machines? Aren't they

164
00:14:17,760 --> 00:14:23,040
guess the next token machines? Like, that's the I think I think that's a really fundamental thing.

165
00:14:23,040 --> 00:14:27,840
Like, I think that my model and the easiest way for most consumers to think about this,

166
00:14:28,160 --> 00:14:33,120
basically accurate, right, is like, there's no actual intelligence to be system. So right,

167
00:14:33,120 --> 00:14:36,720
all they're doing is saying, okay, based on all the words I've seen in the graph of language that

168
00:14:36,720 --> 00:14:41,200
I've been able to observe, here's the most likely next token. And that's really cool to be clear.

169
00:14:41,200 --> 00:14:45,600
That's like super useful. But calling that intelligence is a real stretch in my mind.

170
00:14:45,600 --> 00:14:49,040
Well, I think it depends on your definition of intelligence, like you're applying the free

171
00:14:49,040 --> 00:14:53,200
energy principles of Carl Friston, and where everything just intelligence from energy kind

172
00:14:53,280 --> 00:14:58,160
of dropping to its last date or different definition of intelligence. I think what I

173
00:14:58,160 --> 00:15:02,560
look at it is like this. One to one is getting the next token for language models for image

174
00:15:02,560 --> 00:15:07,760
models that diffusion based and now generate all sorts of other architectures. But it's about output

175
00:15:07,760 --> 00:15:13,440
and what can it do? So one on one, it's a bit dumb, it doesn't have memory. You have the meta

176
00:15:13,440 --> 00:15:18,240
paper by Cicero, whereby they had eight language models interacting with each other, and it out

177
00:15:18,240 --> 00:15:22,800
performed humans in the game of diplomacy. You know, you just like all that good old Alpha

178
00:15:22,800 --> 00:15:27,360
Go type stuff, which he's reinforcement learning. Is that intelligence? Probably still not,

179
00:15:27,360 --> 00:15:30,880
but it can augment intelligence. That's something that we've been focusing on a lot because

180
00:15:31,440 --> 00:15:36,480
you can use it for actual intelligence augmenting things, you can use it for reasoning things.

181
00:15:36,480 --> 00:15:41,200
Give it a PDF and say, well, on Earth, this is PDF talking about, you can do that right now.

182
00:15:41,200 --> 00:15:46,960
And that's a useful thing that reduces frustration. I used to invest in video games. I used to look

183
00:15:47,120 --> 00:15:51,040
at time to fund flow and frustration. I look at things like, you know, this podcast we're doing

184
00:15:51,040 --> 00:15:56,000
any year, it'll be automatically transcribed and edited and added to our knowledge base through

185
00:15:56,000 --> 00:16:00,880
next token prediction. Does that require AGI? No. Yeah. Although interesting, let's talk about

186
00:16:00,880 --> 00:16:04,640
this podcast. It's a really interesting case. You know, in the early days of Clubhouse, when

187
00:16:04,640 --> 00:16:10,400
Clubhouse was ripping, I used to like go after Paul all the time. And I wrote about this being like,

188
00:16:10,400 --> 00:16:14,880
you are so stupid for not recording this stuff. I was like, look, here's the reality. These

189
00:16:14,880 --> 00:16:19,680
conversations in Clubhouse are dribble, right? Like 99% of them is crap. And I don't want to

190
00:16:19,680 --> 00:16:25,440
listen to it. However, if you've created a magical pump that says the internet is full of SEO,

191
00:16:25,440 --> 00:16:30,560
shit, and Wikipedia, we have a magic pump of people wanting to talk to each other live.

192
00:16:30,560 --> 00:16:35,760
Here's the thing, people want to talk, no one wants to listen. But if you transcribe and record it all,

193
00:16:35,760 --> 00:16:39,520
and you can create an index out of it, and then all of a sudden you have this meta, this next

194
00:16:39,520 --> 00:16:43,680
generation search engine, but that's fucking interesting, right? Here's the problem. We're

195
00:16:43,680 --> 00:16:46,960
Paul's side of the time, which I think turns out to be totally wrong, given where AI is coming.

196
00:16:46,960 --> 00:16:51,360
He's like, yes, Sam, but like, there's no way to index it and blah, blah, blah. There will be,

197
00:16:51,360 --> 00:16:55,520
like, there's clearly going to be, right? And it turns out, I'd like to, you know,

198
00:16:55,520 --> 00:16:59,520
because I like seeing, I told you so, like, I told you so, like, they're definitely a way to do that

199
00:16:59,520 --> 00:17:03,600
now, right? And like, that would have been super sweet. I think we're great. Here's the problem,

200
00:17:03,600 --> 00:17:10,080
though, is with a lot of these visions of like, oh, we'll just like take all the recorded podcasts,

201
00:17:10,080 --> 00:17:14,960
right? And then kind of put a front end and tie to them and like, compress them down and be done,

202
00:17:14,960 --> 00:17:19,280
is there's no economic model to that. And maybe we can get into business models for a second.

203
00:17:19,280 --> 00:17:24,000
That's going to make sense for anyone to publicly share anything, right? Like the way, the reason

204
00:17:24,000 --> 00:17:28,560
that like, people put things on the web was because they were getting paid for it in one form or

205
00:17:28,560 --> 00:17:33,200
another, because the whole ecosystem of Google, where it created was a trade, it was, okay,

206
00:17:33,200 --> 00:17:37,360
like you get to index this shit, but you're going to send me traffic and I can monetize. And like,

207
00:17:37,360 --> 00:17:42,240
you know, the publishers got snowed by that for a while, right? And like almost lost, almost went

208
00:17:42,240 --> 00:17:47,760
away until they figured out paywalls, right? We're doing this now because it's kind of fun and

209
00:17:47,760 --> 00:17:53,120
bullshit and we'll learn, right? But we're also kind of doing it, at least I'll do it. I'll post it,

210
00:17:53,120 --> 00:17:56,640
maybe someone will follow me out of it, it's a fun hour to spend with interesting people, right?

211
00:17:56,640 --> 00:18:01,920
But there's an economics to it in some form, social or financial capital. This model, I actually

212
00:18:01,920 --> 00:18:05,920
think that the interesting thing about AI, if you take that view, whatever you would think is

213
00:18:05,920 --> 00:18:11,040
interesting, is like, it's already going to crush the information economy of the web, right?

214
00:18:11,040 --> 00:18:16,320
I think that if you roll it forward, like this conversation will not be in the public domain,

215
00:18:16,320 --> 00:18:19,840
right? Going forward, because there'll be no, there'll be no social economics to it,

216
00:18:19,840 --> 00:18:24,080
just be a compression on top of it. And if anything AI, again, if you take the model of,

217
00:18:24,080 --> 00:18:29,120
oh, it'll take a bunch of podcasts and compress them down into tweets, right, will end up kind of

218
00:18:29,120 --> 00:18:34,480
collapsing on itself, if you need people, what you do, right, to ultimately be the source of truth

219
00:18:34,480 --> 00:18:38,400
and information about the world. And so that's that point of view, but I'm not sure I entirely

220
00:18:38,400 --> 00:18:43,360
agree, because you know, it's fun to shoot the shit. And Tony, you do have a podcast thing,

221
00:18:43,360 --> 00:18:46,960
they've got their ads, which is about that. But I think the attention economy is a very

222
00:18:46,960 --> 00:18:50,560
interesting element to this, particularly because these models are based on attention.

223
00:18:50,560 --> 00:18:53,600
So the differential of these models versus previous is that you have the attention of all

224
00:18:53,600 --> 00:18:57,520
you need, Peko, where it's like, from an information theory perspective, information is

225
00:18:57,520 --> 00:19:01,360
valuable in as much it changes the state. So you take this whole podcast and compare this down

226
00:19:01,440 --> 00:19:04,720
to a few tweets, that's all you need to see. But sometimes people want to see the full kind of

227
00:19:04,720 --> 00:19:10,080
thing. No one really wants the whole thing. Oh, no, they do. They do. Sometimes it's quite fun to

228
00:19:10,080 --> 00:19:15,680
kind of do it because I mean, let's say the Christianson thing of a job to be done, right?

229
00:19:15,680 --> 00:19:18,400
You have a functional component, a social component and an emotional component.

230
00:19:18,960 --> 00:19:22,080
You know, why does everyone want to go to a concert? You know, why do people want to have

231
00:19:22,080 --> 00:19:29,120
collectors items things? Products have different aspects and different elements to it. People

232
00:19:29,120 --> 00:19:32,800
still read full books. They don't kind of read the summaries of the book. They don't read the

233
00:19:32,800 --> 00:19:36,880
simulacrums of it. I mean, like, look, to me, there's there's there's two different, again,

234
00:19:36,880 --> 00:19:41,680
this gets into some old Facebook stuff. But like, I think we can talk about let's take

235
00:19:41,680 --> 00:19:45,200
financial economy out of it and just talk about like informational and social economy.

236
00:19:45,760 --> 00:19:51,760
There's the entertainment economy, right? For sure, AI is going to crush the entertainment

237
00:19:51,760 --> 00:19:56,960
economy, right? Like, there's no question about that, right? Like, you start with porn and go on

238
00:19:56,960 --> 00:20:01,360
through. And the reality is, is that, you know, we went from, you know, people magazine to your

239
00:20:01,360 --> 00:20:04,560
friends and your friends are more interesting than people magazine. And that's what's more

240
00:20:04,560 --> 00:20:08,640
impressing your friends is professional friends who are like hotter and funnier. And guess who's

241
00:20:08,640 --> 00:20:13,360
more interesting than hot, funny, professional friends, it's going to end up being actually,

242
00:20:13,360 --> 00:20:17,840
I said, there's a more tick tock was it turns out algorithmically, find the best person from the

243
00:20:17,840 --> 00:20:22,400
universe, you'll find some niche that's better. What's better than that? Synthetic, right? We will

244
00:20:22,400 --> 00:20:27,280
get to the point where we say, Hey, like, there will be like a hotter, funnier, more interesting,

245
00:20:27,280 --> 00:20:33,040
more personalized AI thing, which is derived, like, I totally buy that, right? And I think that's

246
00:20:33,040 --> 00:20:38,560
why actually some it's been funny to watch some pretty interesting influencers who are smart,

247
00:20:38,560 --> 00:20:43,280
be like, Oh, my God, this is the end of the world for us, right? I agree with that. Information is a

248
00:20:43,280 --> 00:20:48,320
very, very different beast, right? And entertainment, though, right, because the value is not like

249
00:20:48,400 --> 00:20:53,600
engagement. That is, that is actually the, in the broad sense, the attention is everything,

250
00:20:53,600 --> 00:20:59,040
where it's totally raw, right? Which is like, that is for sure true, if you're trying to optimize

251
00:20:59,040 --> 00:21:03,040
for entertainment. And it's not true, right? If you actually know what needs to know what's going

252
00:21:03,040 --> 00:21:07,920
on in the world, right? Or you need to like, you're dealing with a real world. And that interface

253
00:21:07,920 --> 00:21:12,320
between the real world, the digital world, where the systems have no knowledge of what actually is

254
00:21:12,320 --> 00:21:17,040
truth is to your, the point where I think is probably that already even falls down the most.

255
00:21:17,760 --> 00:21:22,320
Well, I mean, maybe this is why, you know, if you say that kind of hallucinations are kind of core

256
00:21:22,320 --> 00:21:26,560
and it's the creativity machine, media is where it's more impactful, where the truth is in the

257
00:21:26,560 --> 00:21:30,480
element there, right? What happened a little bit to date is a few of the AI companies wanted to

258
00:21:30,480 --> 00:21:35,360
talk about themselves as information machines. And they realized, right? And so they'll be like,

259
00:21:35,360 --> 00:21:39,200
you're like, we're not instead of we're creative, don't trust us for facts is like,

260
00:21:40,080 --> 00:21:45,040
fine. And I agree, they'll be useful entertainment machines. But I do, I think that goes into the

261
00:21:45,040 --> 00:21:49,200
whole like, what are we actually talking about here? What are the actual value is? And like,

262
00:21:49,200 --> 00:21:54,000
how scoped it is, which is not it's not zero. It's just not like every social societies are

263
00:21:54,000 --> 00:21:59,920
based on stories. You know, like, all of my view on finance, pretty much all of finance is

264
00:21:59,920 --> 00:22:04,000
securitization and leverage telling stories. And then how did you tell them? Like, and we can

265
00:22:04,000 --> 00:22:08,960
see the power of stories as they move around. So Silicon Valley Bank was a story that was true.

266
00:22:09,520 --> 00:22:13,200
And led to an $18 billion outflow like that. All of us are kind of familiar with that.

267
00:22:13,200 --> 00:22:17,280
Probably listen to this podcast. I think it's pretty cynical to say it's all in the stories.

268
00:22:17,280 --> 00:22:21,520
I mean, it's like, I think there are there's reality in the world, like the economy is not

269
00:22:21,520 --> 00:22:26,320
based just on storytelling. No, I mean, the dollar is a story. The economy is based on the dollar.

270
00:22:27,600 --> 00:22:32,080
And so you have the Fed confidence, you have confidence in the stock markets,

271
00:22:32,080 --> 00:22:36,240
it's kind of layers of these things. And then you have this technology, you need trust, that's

272
00:22:36,240 --> 00:22:40,320
for sure true. And trust, I mean, ultimately goes all the way down to like, is there a military

273
00:22:40,320 --> 00:22:43,760
behind it, which is somewhat of a story. And that I agree with. But I think that's like,

274
00:22:43,760 --> 00:22:48,800
a pretty abstract view, right? Like companies earn cash flows, they're real or not real,

275
00:22:48,800 --> 00:22:52,800
they release products, they do work, it's real or not real. It's not just storytelling.

276
00:22:52,800 --> 00:22:56,240
What is the multiple? Maybe it's because I'm a former hedge fund manager. So I always looked at

277
00:22:56,240 --> 00:23:00,400
what was the incremental story for a stock that adjusted the multiples and other things.

278
00:23:00,400 --> 00:23:05,440
Sure, I agree that if you look at the world of multiples, you say, why do you multiple expansion

279
00:23:05,440 --> 00:23:10,800
or compression, right? And that's based on people's feelings about the world in future cash flows,

280
00:23:10,800 --> 00:23:14,880
right? And in theory that that is a lot of storytelling. I don't think that's actually the

281
00:23:14,880 --> 00:23:20,160
vast majority of the economy, right? That's the stock market. So I think that separating out what

282
00:23:20,160 --> 00:23:24,720
is the stock market from what's the economy is pretty important. Hey, we'll continue our interview

283
00:23:24,720 --> 00:23:30,320
in a moment after a word from our sponsors. Omnike uses generative AI to enable you to launch

284
00:23:30,320 --> 00:23:35,760
hundreds of thousands of ad iterations that actually work, customized across all platforms

285
00:23:35,760 --> 00:23:40,480
with a click of a button. I believe in Omnike so much that I invested in it. And I recommend

286
00:23:40,480 --> 00:23:46,240
you use it too. Use CogGrav to get a 10% discount. And I think this is the important thing. We

287
00:23:46,240 --> 00:23:49,600
separate it out and we see where does this technology affect? And when does it go to the

288
00:23:49,600 --> 00:23:55,520
incumbents versus startups? All of these things kind of fundable, right? And so we have one area

289
00:23:55,520 --> 00:23:59,680
of media. We can discuss that very concretely. I think it will have a massive impact on media

290
00:23:59,680 --> 00:24:04,000
at stability. We have leading media team, right? And so we haven't agreed with that, but we can

291
00:24:04,000 --> 00:24:08,400
dig into that. The other area is a lot of these things are language models right now that chat

292
00:24:08,400 --> 00:24:15,440
bots and it's like, it's nice. But Bing is not the top search engine. It's not even top 20

293
00:24:15,440 --> 00:24:19,280
on the app store, right? Because it's still a terrible experience, rather to be speaking.

294
00:24:19,920 --> 00:24:24,160
Yeah. So even though some people like why use it for all the things, you don't really, you know,

295
00:24:24,160 --> 00:24:29,360
chat GPT rows really fast. And it's useful for things like doing your own work. But do you

296
00:24:29,360 --> 00:24:35,440
really use it that much? So where I find it interesting is really looking at where companies

297
00:24:35,440 --> 00:24:40,640
are trying to go beyond the basic search patterns and have the classical kind of feedback loops

298
00:24:40,640 --> 00:24:46,160
with engaging content and see how that grows. So I think mid journey is a good example of that,

299
00:24:46,160 --> 00:24:51,440
whereby David delivery built a community, took it to like 14 million people and is making money

300
00:24:51,440 --> 00:24:56,800
hand over fist because he built even though discord is fricking weird, a good experience on

301
00:24:56,800 --> 00:25:01,040
existing infrastructure, Facebook, App Store. But how many of those have you seen looking across

302
00:25:01,040 --> 00:25:06,480
the entire AI space? Most of this stuff right now is terrible. But again, the other question is who

303
00:25:06,480 --> 00:25:10,160
gets the value, right? And I think like, let's talk about the internet because we actually agree

304
00:25:10,160 --> 00:25:15,040
on the entertainment thing. Like, you know, world of closed loop, it's all about what's the most

305
00:25:15,040 --> 00:25:19,840
engaging thing and attention is everything, right? Yes, like, these systems are like quite

306
00:25:19,840 --> 00:25:25,680
assuming that you don't end up getting into hell, which I do think is a really big problem around

307
00:25:25,680 --> 00:25:29,920
human creativity and copyright and a bunch of other points of legal leverage on these things.

308
00:25:29,920 --> 00:25:35,680
I agree that you can make really compelling cases and it's going to hurt a lot of the human

309
00:25:35,680 --> 00:25:40,080
entertainment industry, right? That that I agree with. But the question is, who's going to win it?

310
00:25:40,080 --> 00:25:44,320
Is it going to be the Hollywood studios? Is it going to be, you know, is it going to be the

311
00:25:44,320 --> 00:25:48,960
existing publishers who just start adding incrementally more of this stuff in, etc. Or is it going to

312
00:25:48,960 --> 00:25:54,720
be new startups or new people? You know, look, there's always exceptions to the rule. But I think

313
00:25:54,720 --> 00:25:58,720
almost the entire pie is going to be the people who have the distribution, they have the IP,

314
00:25:58,720 --> 00:26:01,680
they have all the pieces they need to just plug this shit in.

315
00:26:01,680 --> 00:26:06,000
Well, but I mean, maybe we can look at it in terms of the consumption of content went to zero

316
00:26:06,000 --> 00:26:09,600
with streaming and kind of all these things that led to some winners coming

317
00:26:10,640 --> 00:26:15,440
because you have Netflix, you have Spotify, etc. The creation of content basically goes to zero

318
00:26:15,440 --> 00:26:20,720
with this technology as well. And basically, I believe in a few years who will feature films

319
00:26:20,720 --> 00:26:25,120
using this. Yeah, but I guess I own and distribute those, right? And the reality is,

320
00:26:25,120 --> 00:26:29,200
I think I'll be the Hollywood studios, because they have the distribution, right? Like,

321
00:26:29,200 --> 00:26:32,720
if you believe that's kind of the distribution mechanism, but there's a whole ecosystem that

322
00:26:32,720 --> 00:26:37,360
can build around that. Things like D-Nag, things like industrial light and magnitude,

323
00:26:37,360 --> 00:26:40,000
do you need that when you have rendering, you know, at scale?

324
00:26:40,000 --> 00:26:45,200
To be clear, I think the thing I think you could totally see changing or evolving is going to be

325
00:26:45,200 --> 00:26:51,920
the factory, right? So like, you know, meaning like, yes, are there capital investments that

326
00:26:51,920 --> 00:26:57,680
people have made that will become less relevant because of AI? Absolutely. There's no question.

327
00:26:57,680 --> 00:27:01,840
Will you almost certainly still have human writer rooms for the foreseeable future?

328
00:27:01,840 --> 00:27:05,440
For sure, right? Like, is whatever it is. So there's going to be hybrids. I just,

329
00:27:05,440 --> 00:27:10,640
I think saying that, but my basic point is that IP matters, distribution matters,

330
00:27:10,640 --> 00:27:14,800
like there are things that matter. I agree with you that the factory plumbing in some of these

331
00:27:14,800 --> 00:27:20,240
places gets a lot less valuable if you have better AI tooling. I just don't mind matters.

332
00:27:20,240 --> 00:27:23,120
Well, I think it's a bit of a disruptive innovation for that side of things,

333
00:27:23,120 --> 00:27:28,880
increasing the pace of output. So Pixar can do six movies a year rather than two.

334
00:27:28,880 --> 00:27:32,480
And so the question around the industry, so a few weeks ago I was at Cannondale gave a talk,

335
00:27:32,480 --> 00:27:36,240
so I used to be a video game investor and player. And I was like, the video game industry over the

336
00:27:36,240 --> 00:27:41,920
last 10 years has gone from 70 billion to 170 billion. The average score has gone from 69% to 74%.

337
00:27:42,720 --> 00:27:47,680
Movies are 40 billion to 50 billion. The score is 6.4 on IMDB. Are you going to be able to make

338
00:27:47,680 --> 00:27:51,520
better movies and have a bigger market then, in which case there's more rooms for people to make

339
00:27:51,520 --> 00:27:56,480
money? Or is it going to be a case of, it cannibalizes itself? There's some key questions

340
00:27:56,480 --> 00:28:01,040
around kind of media, right? And media consumption. In the end of the day, the media consumption

341
00:28:01,040 --> 00:28:04,800
thing, though, again, depending on like how you want to factor it and look at it, it really just

342
00:28:04,800 --> 00:28:10,000
comes back. There's 24 human hours in a day, right? Like, and the reality is, is like, where

343
00:28:10,080 --> 00:28:15,760
time spends, it shifts, right? As a result of this stuff, like for sure, time spent dramatically

344
00:28:15,760 --> 00:28:22,800
into social, right? Off of other things, right? When that thing. Will social get more compelling,

345
00:28:22,800 --> 00:28:28,320
right? With AI? Absolutely, right? And so will more attention shift into Instagram because of it?

346
00:28:28,320 --> 00:28:32,480
Absolutely. Do I believe there's going to be another platform that comes out of nowhere

347
00:28:32,480 --> 00:28:38,400
and swipes Instagram because the cost of production goes down? Nah, right? Like, do I believe that,

348
00:28:38,400 --> 00:28:43,200
like, some new studio is going to come out and take out Pixar? Nah, Pixel will just make a few

349
00:28:43,200 --> 00:28:47,680
more films, right? And like, that's cool. Like, I'm not against that happening. I think that's

350
00:28:47,680 --> 00:28:53,920
completely fine. And like, people will make money on that in some places. The cost of production

351
00:28:53,920 --> 00:28:58,480
and therefore the war of content gets more intense, for sure. You'll get to a point where like,

352
00:28:58,480 --> 00:29:02,400
if you don't use this stuff, you're going to get fucked. But like, just because the competition

353
00:29:02,400 --> 00:29:08,080
level rises, doesn't necessarily change the scorecard very much through that, how these things

354
00:29:08,160 --> 00:29:13,120
go. So believe this question of do you use legacy systems or do you use systems such as runway

355
00:29:13,120 --> 00:29:18,080
MLs, such as one of the dynamics, and some of these other ones that are engineered differently?

356
00:29:18,080 --> 00:29:21,680
I think there's a lot of kind of legacy stuff where you used to Photoshop and you used to continue

357
00:29:21,680 --> 00:29:26,800
to use Photoshop. And now they're introducing features like infill. But is there room for a

358
00:29:26,800 --> 00:29:32,480
ground up kind of interface? And we see that sometimes kind of a character. And my assertion is

359
00:29:32,480 --> 00:29:37,760
broadly no, but there will be exceptions. And the broadly no is going to be it's just it's

360
00:29:37,760 --> 00:29:41,280
not to your point about innovate about is it a sustaining or is it disruptive? It's like,

361
00:29:41,920 --> 00:29:46,960
Photoshop will get 95% right. They already have everyone's payment on file. They already have

362
00:29:46,960 --> 00:29:50,960
the infrastructure. This is not like the internet. People like in the internet, there was a bunch

363
00:29:50,960 --> 00:29:56,240
of companies that were fundamentally unprepared for this, right? I do not think that most of the

364
00:29:56,240 --> 00:30:00,320
incumbents are fundamentally unprepared for this. Yeah. And you know, there's a question of do you

365
00:30:00,320 --> 00:30:04,960
create brand new markets? So I was an early investment via the Chinese kind of Twitch. And

366
00:30:04,960 --> 00:30:10,320
there was two hours a day on average per user. Now on character AI, I think it's still number

367
00:30:10,320 --> 00:30:14,800
two on the app store. We're seeing two hours a day on average of usage, where she has some insane

368
00:30:14,800 --> 00:30:19,040
kind of engagement metrics. It's quite nice to have a chat with it. But there's a question,

369
00:30:19,040 --> 00:30:23,760
can that become then a product or a network? I think that we may be looking at some of the wrong

370
00:30:23,760 --> 00:30:28,560
areas here because what you have is you have the consumer experience, the media experience and

371
00:30:28,560 --> 00:30:32,560
enterprise experience. I think one of the things that's most interesting for me in terms of where

372
00:30:32,560 --> 00:30:37,440
money could potentially be made is actually the regulated experience. So at stability, we make

373
00:30:37,440 --> 00:30:42,000
open models, open source, but actually what we do is open auditable models for enterprise,

374
00:30:42,000 --> 00:30:45,920
private data, governments, et cetera. So we've got a whole bunch of stuff that doesn't have any

375
00:30:45,920 --> 00:30:51,200
web crawls, et cetera, employed via Bedrock and others. Then that's valuable data. So one of the

376
00:30:51,200 --> 00:30:56,000
things we do is kind of education. And that's where I look at some of these areas and they've

377
00:30:56,000 --> 00:31:02,240
been the main contributors to US inflation and CPI education and healthcare. And unlike you

378
00:31:02,240 --> 00:31:06,880
could do something different there. And maybe that's where a significant amount of value will be.

379
00:31:07,680 --> 00:31:12,080
I mean, I think it's sad from the Silicon Valley story if the answer is like, well, the money's

380
00:31:12,080 --> 00:31:16,640
all going to be made for regulation. I don't disagree with you for what it's worth. Disrupting

381
00:31:16,640 --> 00:31:20,640
regulated industries, which is different. I do believe that someone's going to make a lot of

382
00:31:20,640 --> 00:31:26,640
money on AI regulatory points, right? There's no question. AI insurance. There we go.

383
00:31:27,600 --> 00:31:34,800
Like, you know, there's a bunch of things that are like really sad things that you have to do and

384
00:31:34,800 --> 00:31:38,240
like, you know, people will make money on. There's no question that people will find niche markets.

385
00:31:38,240 --> 00:31:43,040
They're super boring. And not the type of thing I want to be involved in. But like, yes, like some

386
00:31:43,040 --> 00:31:49,200
enterprise investors will have will make bank on like, you know, the whatever Europe comes up with

387
00:31:49,200 --> 00:31:55,760
certifying your models are compliant and GDPR 8.0 to like deal with fucking data request removals.

388
00:31:55,840 --> 00:32:01,200
Like that will happen as kind of the stuff happens. I'm like pretty uninspired by that,

389
00:32:01,200 --> 00:32:06,240
right? Like, I think that's like, pretty sad that that's if that's if the net income of like,

390
00:32:06,240 --> 00:32:11,280
new opportunities in AI is just going to be like, opportunities to like interface with government

391
00:32:11,280 --> 00:32:16,240
and reign it in will be sad. Yeah, but like said, regulated industries. So the example that I have

392
00:32:16,240 --> 00:32:20,560
there is education and healthcare. So like, one of the things you work with a range of charities

393
00:32:20,560 --> 00:32:25,440
and multinational is deploying tablets into entire countries in Africa with AI that teaches

394
00:32:25,440 --> 00:32:30,400
and learns. You give every kid a tablet, the young ladies illustrated primer, what does that do to

395
00:32:30,400 --> 00:32:35,120
an entire nation? You know, the only thing that's been provably to work in education is the bloom

396
00:32:35,120 --> 00:32:39,680
effect, the two segment effect. Right now, our kind of sister charity Imagine Worldwide has

397
00:32:39,680 --> 00:32:44,640
been deploying the global XPRIZE for learning adaptive learning. And we're teaching 76% of kids

398
00:32:44,640 --> 00:32:48,080
literacy and numeracy in 13 months and one hour a day with older kids teaching younger kids.

399
00:32:48,800 --> 00:32:53,200
I look at this technology and I'm like, there are certain areas where there's a gap that nothing

400
00:32:53,200 --> 00:32:58,720
could fill before. What if you had an AI tutor for every child? What does that look like? What if

401
00:32:58,720 --> 00:33:03,440
you had 100 AI tutors for every child? I get it. And like, I do think that we can always go back to

402
00:33:03,440 --> 00:33:07,600
the industries that tech has been trying to disrupt for a million years and like for lots of structural

403
00:33:07,600 --> 00:33:13,520
reasons has not and say, ah, but now with this new tech will disrupt it, you know, I look forward to

404
00:33:13,520 --> 00:33:18,880
the, to the years of debate in the, we'll talk about the US between the teachers unions and people

405
00:33:18,880 --> 00:33:23,200
trying to deploy tablets for AI. We can say, Oh, no, no, no, no, we're going to do it in Africa,

406
00:33:23,200 --> 00:33:27,520
skip the regulator, like the teachers, but I'm just saying it's like, yes, there's always hope

407
00:33:27,520 --> 00:33:33,040
that the next wave of technology will somehow unstick a bunch of problems technologists hate

408
00:33:33,040 --> 00:33:38,160
because of the regulatory or the structural issues with them. But I have no confidence that this

409
00:33:38,160 --> 00:33:43,280
one is meaningfully different. But I mean, this is the question structural issues, right? Regulation

410
00:33:43,280 --> 00:33:49,360
is one thing. You look at kind of BG use some of the other Indian kind of education companies,

411
00:33:49,360 --> 00:33:54,080
you look at the Chinese ones across emerging markets, maybe it'll be the case here. I mean,

412
00:33:54,080 --> 00:34:00,160
this is what I believe that much of the productivity enhancements, aside from maybe coding and things

413
00:34:00,160 --> 00:34:05,440
like that, which we can get on to. And the biggest leaps will happen in the global south, because

414
00:34:05,440 --> 00:34:10,000
they left a mobile and there's a whole mobile economy and massive companies created from that.

415
00:34:10,000 --> 00:34:14,960
What if they make a leap to intelligence augmentation with this technology? Because right now they

416
00:34:14,960 --> 00:34:20,640
can't service that. Now they could potentially service it, given the decreased cost of creativity

417
00:34:20,640 --> 00:34:24,720
of engagement and other things from education to healthcare to other things.

418
00:34:24,720 --> 00:34:30,560
I think if your argument is that there's a bunch of countries outside of the US that have lagged

419
00:34:30,560 --> 00:34:36,160
in a bunch of infrastructure effectively or ability to like execute certain things in education,

420
00:34:36,160 --> 00:34:41,040
et cetera, it will be able to allow the cell phone have like a leap frog moment and move forward.

421
00:34:41,040 --> 00:34:46,080
Yeah, I don't object to that. I think that's like basically true. Again, I goes back to the thing

422
00:34:46,080 --> 00:34:51,760
where like, I'm excited about kind of like the US, I think lives in the future relatively speaking

423
00:34:51,760 --> 00:34:55,520
to most other people in countries. And like, I think the thing most people are excited about

424
00:34:55,520 --> 00:35:00,880
is how like we can how AI changes like the top of the top. I agree with that. So I think if your

425
00:35:00,880 --> 00:35:05,600
argument is it doesn't change the top of the top, but it does kind of catch up a bunch of the third

426
00:35:05,680 --> 00:35:08,240
world, like I do think that there are places that will be true.

427
00:35:08,240 --> 00:35:14,400
Well, so let's look at the top of the top then. So I think Microsoft put out that 50% of all code

428
00:35:14,400 --> 00:35:18,640
is AI generated on GitHub now from code products, et cetera, and there's 40% improvement in

429
00:35:19,200 --> 00:35:24,960
efficiency. I mean, my top coders really enjoy it because they train them in models. We have code

430
00:35:24,960 --> 00:35:30,240
models too, and they are showing more and better code. What do you think about it with respect

431
00:35:30,240 --> 00:35:34,000
to that industry? Because that's obviously a large industry, which is technology disrupted.

432
00:35:34,000 --> 00:35:40,720
The only thing that I actually think is fucking awesome for chat GPT effectively is, I'll call it

433
00:35:40,720 --> 00:35:46,080
Stack Overflow 2.0. It's fucking great for that. And like, if you think about it, why is it great

434
00:35:46,080 --> 00:35:52,160
for that? Like, why? I think it is the perfect problem for the existing technology we have.

435
00:35:52,160 --> 00:35:59,840
You have a shitload of open source code that these models can look at. Plus, you scrape all of Stack

436
00:35:59,840 --> 00:36:04,640
Overflow, which Cyanar is Stack Overflow, and that goes back to the whole copyright issue,

437
00:36:04,640 --> 00:36:08,960
as well as the issue of where some of the inputs come from, but most of the copyright issue.

438
00:36:08,960 --> 00:36:15,440
Plus, the nice part about computer code is that it's test driven in a lot of cases. You either

439
00:36:15,440 --> 00:36:22,160
pass it to the fucking test or it doesn't pass the test. So you have the perfect dataset of digital

440
00:36:22,160 --> 00:36:29,680
only self-contained reality, which I totally agree chat GPT is great at. And frankly, I'm

441
00:36:29,680 --> 00:36:34,400
the type of person who, like, I told, but I would never consider myself an engineer. It makes coding

442
00:36:34,400 --> 00:36:38,240
for me so much more fun because all this shit I don't want to deal with, like, what the fuck is

443
00:36:38,240 --> 00:36:42,960
this random error? What package do I have to install that manages this? It's all great. Now,

444
00:36:42,960 --> 00:36:49,360
it does lie, and it does make up wrong answers, and it's not perfect. But I fully agree that the

445
00:36:49,360 --> 00:36:55,920
co-pilot as thing is very powerful and like a really great specific use case. And I do agree that

446
00:36:56,080 --> 00:37:01,200
talking about business models or what happens is like, like Stack Overflow is the poster. Stack

447
00:37:01,200 --> 00:37:07,200
Overflow is the Yelp of this generation, right? You know how Yelp had this huge lawsuit with

448
00:37:07,200 --> 00:37:12,480
Google that's gone on forever because Google basically just sold their results, right? Stack

449
00:37:12,480 --> 00:37:18,720
Overflow is going to be that of this because they are screwed, right? And like, it is a great example

450
00:37:18,720 --> 00:37:22,320
of a place where the tech is better because it was basically lifted. Yeah. And you know, it becomes

451
00:37:22,320 --> 00:37:26,560
very interesting as well because now what you have is regulatory arbitrage, like the good old

452
00:37:26,560 --> 00:37:32,480
double Irish with the Dutch sandwich on taxation, whereby Israel and Japan have said you can scrape

453
00:37:32,480 --> 00:37:38,240
anything for any reason, which is kind of crazy, commercial or otherwise. So you maybe scrape in

454
00:37:38,240 --> 00:37:43,120
one area, trade in another, and you serve it up in a different country. So I think this technology

455
00:37:43,120 --> 00:37:48,160
is kind of inevitable. But then what is the implication of that? Like, my take is that as we

456
00:37:48,160 --> 00:37:52,240
move through the next kind of five years or something like that, the nature of coding will

457
00:37:52,240 --> 00:37:57,200
change. Like I started coding what 22 years ago, we had like assembler and subversion and stuff

458
00:37:57,200 --> 00:38:01,840
like that. And kids these days have it so easy with get up, you know, and all these libraries.

459
00:38:01,840 --> 00:38:04,880
What does it look like in a few years when you've got these technologies that you can describe

460
00:38:04,880 --> 00:38:09,280
something and start building apps? You know, what does the whole ecosystem look like again when

461
00:38:09,280 --> 00:38:14,640
the creation of these cells? It will just make them like much less valuable, right? Like is what

462
00:38:14,720 --> 00:38:18,960
I basically come down to. And what ends up remaining valuable is distribution and data,

463
00:38:18,960 --> 00:38:22,880
right? Because like right now you can be a great engineer or solve a problem, whatever, and there's

464
00:38:22,880 --> 00:38:26,480
like a value. You can create a product that's actually worth something. If everyone can make

465
00:38:26,480 --> 00:38:31,680
products, theoretically, that are like cost nothing, right, or really easily, then like there's

466
00:38:31,680 --> 00:38:35,680
just no leverage in that anymore. And again, this goes back to who wins, who wins with people with

467
00:38:35,680 --> 00:38:40,160
distribution data, right? That's the answer, like it from existing now to your point of

468
00:38:40,160 --> 00:38:44,480
a regulatory arbitrage and data, I think this is really, I think the sad part about a lot of

469
00:38:44,480 --> 00:38:49,760
this AI stuff, everything is going right, right? Like that's what the net of this is going to be

470
00:38:49,760 --> 00:38:54,240
is like any, anything that has historically been an open data set, or people are able to say like,

471
00:38:54,240 --> 00:38:58,240
okay, well, like I'll share this, but in return, I get traffic or notoriety, and that's like a

472
00:38:58,240 --> 00:39:03,600
fair economics rate over, right? And so what's going to end up happening is walls are going to go

473
00:39:03,600 --> 00:39:08,160
up everywhere, everything's going to go private. And that's going to be the interesting question

474
00:39:08,160 --> 00:39:12,960
about where you end up from all this stuff from an economics perspective in the next few years.

475
00:39:13,040 --> 00:39:17,040
But what is where this has happened many times before, right? Like this is not the first time in

476
00:39:17,040 --> 00:39:21,440
human history, this happens that you know, people, you know, if you look at news industry,

477
00:39:22,080 --> 00:39:25,360
you know, people are like, Oh, like the news industry used to be so great, and then whatever,

478
00:39:25,360 --> 00:39:29,520
it's like bullshit. It's like the number of times in the history of news, basically, you had

479
00:39:29,520 --> 00:39:35,520
growth and distribution, right? Things get super scammy. The elites retreat to private newsletters,

480
00:39:35,520 --> 00:39:39,360
like in its cycles, it's happened like six or seven times. And like, I think this is going to be a

481
00:39:39,360 --> 00:39:45,360
hard pin. In some ways, I think the biggest thing is that I'm very confident of is that AI will be

482
00:39:45,360 --> 00:39:49,520
the death of the public web, and will be the death of a lot of open information, specifically

483
00:39:49,520 --> 00:39:54,000
because of what you said, right? Which is that just will not that it's me too valuable and too,

484
00:39:54,000 --> 00:39:58,400
and too, too important. But the reality is AI doesn't need any more information,

485
00:39:58,400 --> 00:40:03,040
because of your short letter. But it does, it doesn't for entertainment. And that's why I think

486
00:40:03,040 --> 00:40:08,960
entertainment is screwed. I think it absolutely the the oracle problem in crypto, where how you

487
00:40:08,960 --> 00:40:14,640
keep a system, a digital system in sync with reality and be meaningful is exactly the same

488
00:40:14,640 --> 00:40:20,080
problem that AI has, which is it can go in any direction it wants, as long as the data is self

489
00:40:20,080 --> 00:40:24,800
contained. The second it's not, and it's trying to be synced to reality or a real world, it does

490
00:40:24,800 --> 00:40:29,040
need more data. It does need to be continuously updated or addressed in whatever direction,

491
00:40:29,040 --> 00:40:34,240
you know, cars attention. But then, you know, you have public broadcasting data, you have some

492
00:40:34,240 --> 00:40:38,480
of these other things as well, whereby the oracle problem comes a lot easier to do when you can

493
00:40:38,480 --> 00:40:43,680
do retrieval augmented models and other things like that. I mean, there are sources of verifiable

494
00:40:43,680 --> 00:40:48,080
data for leasing. Maybe it comes down to the use case. My main point is that they're going to be

495
00:40:48,080 --> 00:40:53,040
increasingly cut off, right, if there's no economic model for supporting them, and they're all getting

496
00:40:53,040 --> 00:40:57,200
abstract and scraped by model. I would disagree with this. So you know, like, I made it deliberately

497
00:40:57,280 --> 00:41:01,040
open so that we could highlight how that shapes that I think they're unsafe as well. And we're

498
00:41:01,040 --> 00:41:06,400
the only company to offer octab, but we work with multiple governments on national data sets and

499
00:41:06,400 --> 00:41:11,840
national models, using broadcasts of data and other things like that that are continuously updated

500
00:41:11,840 --> 00:41:16,400
as national infrastructure. Because I think these models are a form of infrastructure, they're a

501
00:41:16,400 --> 00:41:20,960
weird type of primitive, they're like a mega codec type thing, where stuff goes in stuff comes out,

502
00:41:20,960 --> 00:41:26,000
but people do want to have relevance and updates. So I think you will have an open version that is

503
00:41:26,080 --> 00:41:31,600
updated continuously. But then maybe again, that's where value is. Which parts of information go

504
00:41:31,600 --> 00:41:36,080
private and are served up through models and who is providing them? Is this financial data? Is it

505
00:41:36,080 --> 00:41:41,760
this? Is it that? And what is the quality of these fine-tuned models? Because what you just described

506
00:41:41,760 --> 00:41:47,840
as well is a bit of a Armageddon for consumer apps in a way, right? Because it goes down to zero.

507
00:41:47,840 --> 00:41:52,160
So then what becomes useful is that then the Apple takes a massively forward because they've got

508
00:41:52,240 --> 00:41:56,160
this identity structure, and they have all the data there, and they can do apps quicker than

509
00:41:56,160 --> 00:42:00,800
anything else. Yeah, except for the fact that Apple's entire shtick about encryption and privacy

510
00:42:00,800 --> 00:42:04,240
is going to make it literally impossible for them to play in this. I actually think Apple's role

511
00:42:04,240 --> 00:42:07,920
in the future of this stuff is going to be one of the most interesting big tech questions,

512
00:42:07,920 --> 00:42:12,720
because they have positioned themselves so hardcore against all the things you would need to get

513
00:42:12,720 --> 00:42:17,440
leveraged right from AI, that it's going to be very interesting to see how they navigate. Google,

514
00:42:17,520 --> 00:42:27,040
fine, meta, fine. But despite the fact, I am very skeptical of what Apple's AI approach is going

515
00:42:27,040 --> 00:42:32,160
to be, or I will say on the flip side, they're incredible at government relations and PR. So

516
00:42:32,160 --> 00:42:36,720
if they see you have to figure out a way to totally recant on all their encryption and their

517
00:42:36,720 --> 00:42:41,760
approaches to this type of stuff and have a new model where they somehow are the privacy heroes,

518
00:42:41,760 --> 00:42:45,040
but also doing AI, I'm very curious how that's going to work.

519
00:42:46,000 --> 00:42:49,600
They can keep a perfection and they can keep a customized rule. Because again,

520
00:42:49,600 --> 00:42:54,080
you don't need to take everyone's days to the trainer. You have a generalized model. I think

521
00:42:54,080 --> 00:43:00,320
local model, mini models on your local device, like a general model behind it. Exactly. I think

522
00:43:00,320 --> 00:43:03,920
in practice, we'll see how it plays out. I'm skeptical. It works with an embedding layer

523
00:43:03,920 --> 00:43:11,200
potentially, but it is kind of very interesting because, again, the technology doesn't matter.

524
00:43:11,200 --> 00:43:16,880
It's the use that matters. What use can you get out of it? So yesterday, they had the thing

525
00:43:16,880 --> 00:43:21,920
whereby they said, oh, it learns automatically with a little ML model in there. It learned

526
00:43:21,920 --> 00:43:26,800
through a small embedding layer. They don't talk about the technology that much because Apple

527
00:43:26,800 --> 00:43:31,360
always just talks about what the use actually is. I think the question is, what is disruptive?

528
00:43:31,360 --> 00:43:36,560
What can engage more? What could attract more? And so I think that you've got apps coming down

529
00:43:36,560 --> 00:43:41,760
there, which is why the bar generally rises. I think we see this with technology as it goes. The

530
00:43:41,760 --> 00:43:46,320
bar generally rises, and so attention becomes even more difficult, where it does come down to

531
00:43:46,320 --> 00:43:51,920
distribution. I think about that. What's your take on the nature of virality in this type of age?

532
00:43:51,920 --> 00:43:57,680
Because these things are good at optimizing for virality, potentially, right? Like, again,

533
00:43:57,680 --> 00:44:01,760
you can build better content. You can build better engagement once you get the puddles down.

534
00:44:01,760 --> 00:44:07,200
And that is the start of many of these apps. Yeah, I just think virality is a war in a lot of

535
00:44:07,200 --> 00:44:13,120
ways. So look, I think in the end of the day, will newsfeeds get more compelling for people?

536
00:44:13,120 --> 00:44:19,280
Absolutely. Will ads get more compelling for people individually? Absolutely. There's no

537
00:44:19,280 --> 00:44:24,080
question if these things are true and the existing players will get the vast majority of the pie of

538
00:44:24,080 --> 00:44:29,840
that type of stuff. I do think you'll tend towards more and more niche interests. So let's talk about

539
00:44:30,000 --> 00:44:37,440
porn for a second. Porn is always fascinating. You can go on Reddit and find the weirdest

540
00:44:37,440 --> 00:44:41,600
fucking porn in the world of all these sub communities that have filtered into these weird

541
00:44:41,600 --> 00:44:49,280
things that they're interested in. AI will make this 10 times weirder. Or if 100 times weirder.

542
00:44:49,280 --> 00:44:52,800
And people are just going to keep filtering. Now, why does this weird filtering happen?

543
00:44:53,920 --> 00:44:57,520
There's a bunch of reasons and different things. I think part of it, moving away from

544
00:44:57,520 --> 00:45:01,680
porn for a second in the broader ecosystem is people are desperate for a sense of purpose and

545
00:45:01,680 --> 00:45:07,040
place. And the reality is the internet makes you feel very small. There's millions of people just

546
00:45:07,040 --> 00:45:12,720
like you. And that encourages people to seek out right sized communities that are smaller and smaller.

547
00:45:12,720 --> 00:45:18,480
With AI, I think the interesting thing will be when it comes to attention and things like that is,

548
00:45:18,480 --> 00:45:25,200
look, for the first time, every single person can have hundreds of characters that like and

549
00:45:25,200 --> 00:45:31,680
support them all the time. The math of it all used to be, okay, you're trying to find a community

550
00:45:31,680 --> 00:45:37,600
that's the right size and knows you have a price and you're valued in. But it's hard to, you're

551
00:45:37,600 --> 00:45:41,600
not necessarily the hero. So you go find out a smaller niche or a different niche where you're

552
00:45:41,600 --> 00:45:47,280
more of a hero or you create a spit of it and try to lead that. I think a future where basically

553
00:45:47,280 --> 00:45:51,120
you log into social media or whatever and you're like, hey, I'm Sam. And it's like, cool. What

554
00:45:51,200 --> 00:45:55,200
type of people, instead of who do you want to follow? It's like, who do you want to follow you?

555
00:45:56,000 --> 00:46:00,320
And like you end up with like hundreds of AI characters or frankly, I think what's more likely

556
00:46:00,320 --> 00:46:04,800
is it's a mix of humans, AI's and you're not really sure which is which. But they're caught,

557
00:46:04,800 --> 00:46:09,040
they're the ones commenting on your posting, like, you're fucking great. Or like, here's a cool question

558
00:46:09,040 --> 00:46:12,960
or whatever. Like, I think that's the world we're gonna end up on is like more and more segmented

559
00:46:12,960 --> 00:46:18,160
niches, right? Where the ultimate end would be the her model where it's like, you just have one AI

560
00:46:18,160 --> 00:46:21,600
girlfriend. I'm not sure we'll go there. I think that's really hard to pull off. And I think like

561
00:46:21,600 --> 00:46:27,440
that's a tough thing. But if you told me that like, in the future, you know, on Twitter, good

562
00:46:27,440 --> 00:46:33,280
example, you know, everyone has 100,000 followers, right? You're not exactly sure who's a person and

563
00:46:33,280 --> 00:46:38,160
who's a robot, right? And they all fucking love you and it makes it super compelling and you feel

564
00:46:38,160 --> 00:46:43,440
great. Like that's a very plausible future. Come on, birth rates are gonna do that. Have you seen

565
00:46:43,520 --> 00:46:47,920
that child of young male virginity under 13 the US for the Washington Post? They went from 8% in

566
00:46:47,920 --> 00:46:55,760
2008 to 27% in 2018. That do you see what happened with replica on Valentine's Day this year? So

567
00:46:55,760 --> 00:46:59,920
replica was originally about that was designed to be your mental health buddy, right? Until they

568
00:46:59,920 --> 00:47:06,720
did realize you could charge $300 a year for a lot of roleplay until the 13th of February 2023,

569
00:47:06,720 --> 00:47:10,960
when they get a message from Apple saying shut this off. So on Valentine's Day, they shut that

570
00:47:10,960 --> 00:47:15,920
off. And then 68,000 people joined the Reddit the day after and said, why'd you do the boss of mine

571
00:47:15,920 --> 00:47:21,760
with my girlfriend? You know, like, it was quite a massacre. That's where we're going. And look,

572
00:47:21,760 --> 00:47:25,840
there's a whole history. I mean, again, like, we'll go back to porn for a second. Like, the whole

573
00:47:25,840 --> 00:47:30,240
it's always fascinating. It's such an interesting base human thing. But it's like, look, it's like

574
00:47:30,240 --> 00:47:36,560
the whole dynamic of like, you know, you know, how Tinder has affected sexuality and equality,

575
00:47:36,560 --> 00:47:39,680
right? It's like, fascinating. Like, there's all these really interesting studies on this,

576
00:47:39,680 --> 00:47:44,080
like technology has a deep impact on this type of stuff, right? But if people ultimately want

577
00:47:44,080 --> 00:47:48,800
care about validation, titillation, whatever it's going to be, there's no question that

578
00:47:48,800 --> 00:47:55,120
plays one place you, you and I will agree is that AI does dramatically shift the power on these

579
00:47:55,120 --> 00:47:59,200
things, they will end up with weird or sub communities. And he says, here's my question to

580
00:47:59,200 --> 00:48:03,840
you, though, we talk about power dynamics, I still think, and in this, I might be wrong about, I

581
00:48:03,840 --> 00:48:08,480
will admit, because it's a little bit of a niche, weird industry. But my bet is that porn hub is

582
00:48:08,480 --> 00:48:13,600
still the winner. I actually, I assume they're the biggest porn company, like I, or whatever Reddit

583
00:48:13,600 --> 00:48:18,800
is, it was like, the place porn is doesn't shift, the platforms don't shift. It's just going to be

584
00:48:18,800 --> 00:48:24,960
like, weirder, weirder stuff, and more and more AI generated. I don't know. I mean, this porn

585
00:48:24,960 --> 00:48:28,960
hub isn't that big. So Mind Geek is the company behind it. They were just bought by ethical

586
00:48:28,960 --> 00:48:35,120
capital partners. Because, you know, life is weird. Reddit could be a big winner of this. But I think,

587
00:48:35,840 --> 00:48:40,080
you know, I've been already, like Reddit is already just full of porn, right? So it's like,

588
00:48:40,080 --> 00:48:43,280
I just assume you are more full of porn. I'm sure they're going to be very smart about this,

589
00:48:43,280 --> 00:48:48,160
you know, and engaging porn. But really, what you're saying is go along AI voices, you know,

590
00:48:48,160 --> 00:48:53,040
like this kind of loneliness that they fill in, that could be a good investment team. Because

591
00:48:53,040 --> 00:48:57,280
again, you have the whole whole life stuff that then emerges to these engaging people.

592
00:48:57,280 --> 00:49:00,800
I think it's going to happen, but I don't think it's a good investment. And like, let me just go

593
00:49:00,800 --> 00:49:04,320
back to like, just because it's going to happen doesn't make it a good thing to invest in. And

594
00:49:04,320 --> 00:49:10,160
like, to me, it's really unclear where the leverage is in that, right? Like, it's like,

595
00:49:10,880 --> 00:49:14,800
you're, you'd have to believe that somehow you're going to have dramatically more compelling

596
00:49:14,800 --> 00:49:20,720
characters than like the next company also provide, right? Or you'd have to use it like,

597
00:49:20,720 --> 00:49:25,520
I just don't use any lock in, I think, and I don't think there's any like other and so it's

598
00:49:25,520 --> 00:49:28,960
really unclear just because it's going to happen doesn't make it an investment.

599
00:49:28,960 --> 00:49:34,000
Well, I think there is kind of, if you kind of look at hook dynamics, there's kind of that trigger

600
00:49:34,080 --> 00:49:38,960
reward kind of dopamine rush and lots of stuff that you invest into each character. So there's

601
00:49:38,960 --> 00:49:42,560
probably going to be a lot of first mood and advantage here. On the other side, you have the

602
00:49:42,560 --> 00:49:46,800
licenses, you have the IPs that can be brought to this, like not on the form side, but as a whole

603
00:49:46,800 --> 00:49:51,680
gamut from board to your mental health buddy, right? I mean, I think ultimately, if you're

604
00:49:51,680 --> 00:49:57,520
basically saying, is there a solution to loneliness and solution to making you feel good,

605
00:49:57,520 --> 00:50:00,880
there's a whole gamut of different things that can happen here, where you've got IP,

606
00:50:00,880 --> 00:50:03,920
wait for these other things. Again, the example I think that comes from that is

607
00:50:04,640 --> 00:50:07,760
the hollow life influences that going up like that.

608
00:50:07,760 --> 00:50:11,760
Not to push you with it. I mean, it sounds like you're agreeing with me, which is like the leverages

609
00:50:11,760 --> 00:50:16,720
in IP, right? Or the leverages in distribution, right? For this type of stuff, because the pure

610
00:50:16,720 --> 00:50:23,520
tech stuff to it, it's like, yes, there'll be good jillions of, you know, virtual girlfriendy,

611
00:50:23,520 --> 00:50:27,840
whatever things, but it's not, those are not platforms you can invest in. And they're like,

612
00:50:27,840 --> 00:50:31,680
they're not really valuable, even if there's a lot. I think bringing it all together is something

613
00:50:31,680 --> 00:50:35,840
that will take time. So I think there will be a lot of first-degree advantage. So like with stability,

614
00:50:35,840 --> 00:50:40,240
again, data distribution are key, right? So my thing is, take the best of open, which we stimulate

615
00:50:40,240 --> 00:50:45,120
and we fund lots of, build the stable series of models already without any data and distribution

616
00:50:45,120 --> 00:50:50,560
to it. So open data, commonsense data, national data, and then we take it through cloud system

617
00:50:50,560 --> 00:50:55,200
integrators on-prem and I take a share of all that revenue. So I agree, that's kind of cool to a good

618
00:50:55,200 --> 00:51:00,560
business. But what I'm saying is, I don't believe in this particular area, going from port at one

619
00:51:00,560 --> 00:51:05,440
end to mental health buddies at the other end, there are established distribution networks.

620
00:51:06,560 --> 00:51:11,280
I think there'll be a lot of opportunity there for first mover advantage.

621
00:51:11,280 --> 00:51:18,080
In the history of investing, first mover advantage has generally turned out to be a pretty bad

622
00:51:18,080 --> 00:51:24,720
investment. Okay, maybe not first mover advantage, just say first proper entity advantage that takes

623
00:51:24,720 --> 00:51:30,560
advantage of classical good company dynamics. There aren't big companies there yet.

624
00:51:31,120 --> 00:51:36,560
Yeah, maybe. Again, I think it's a little hard to know exactly. There's a huge spectrum here,

625
00:51:36,560 --> 00:51:41,120
it's hard to like, exactly react. But I would say, like, look, I think we're agreeing that like,

626
00:51:41,120 --> 00:51:45,280
entertainment's going to get more entertaining, right, and cheaper to produce, right?

627
00:51:46,080 --> 00:51:50,320
I think we're agreeing that IP is very valuable and maybe it's more valuable. Like, so maybe the

628
00:51:50,320 --> 00:51:56,400
answer is buy Disney stock. Because Elsa is going to be a way cooler character when like,

629
00:51:56,400 --> 00:52:01,040
that's kind of obvious, kind of obvious, right? And like, I think we can all agree on that. I think

630
00:52:01,040 --> 00:52:04,720
what is not clear to me is outside of the IP plays, outside of the distribution,

631
00:52:04,720 --> 00:52:11,040
existing distribution plays, like what IP, what AI really unlocks is a new disruptive

632
00:52:12,160 --> 00:52:17,120
vector for this type of stuff. Because I don't, I do think that there are some pure AI type things

633
00:52:17,120 --> 00:52:22,800
you can do. Again, we'll talk about the AI girlfriend thing is just unclear what the payoff

634
00:52:22,800 --> 00:52:26,880
is there, right? Because they don't have any modes. Well, I think if you look kind of, you can scale

635
00:52:26,880 --> 00:52:32,160
a certain type of human endeavours, shall we say, for example, the origin of therapists in the world,

636
00:52:32,800 --> 00:52:37,280
you know, and it is a regulated industry. But at the same time, there is a gap for therapists,

637
00:52:37,280 --> 00:52:41,200
just like you have the meditation apps kind of step in, and they created calm, and they

638
00:52:41,200 --> 00:52:46,160
created these other things that were huge. Now this is more engaging. So I think one of the areas

639
00:52:46,160 --> 00:52:50,880
to look at is where can you not find enough people that can fill in some of these things and then

640
00:52:50,880 --> 00:52:54,720
build good experiences around that if you're looking at companies that can come to the fore,

641
00:52:54,720 --> 00:52:59,200
because there isn't an existing solution. This is why, like I said, for me, I look at the global

642
00:52:59,200 --> 00:53:04,240
south, I'm like, there's lots of gaps. I look at kind of here, and there's again gaps, where are

643
00:53:04,240 --> 00:53:08,640
the gaps that you want to go because you can basically create a market need to fulfill a key

644
00:53:08,640 --> 00:53:13,920
customer need. And so again, I looked at mental health in particular, and that goes again from

645
00:53:13,920 --> 00:53:19,840
the porn AI waifens all the way through to proper mental health, kind of therapists. There's a huge

646
00:53:19,840 --> 00:53:25,040
gap in that particular market, and there's a huge chasm of loneliness, and a lot of products that

647
00:53:25,040 --> 00:53:29,920
could be built that are generally useful. And that can go quite fast enabled by this technology,

648
00:53:29,920 --> 00:53:33,920
where they were not enabled before. I think this has been fascinating. I have kind of a handful

649
00:53:33,920 --> 00:53:39,680
of concrete prediction questions that I kind of want to get you guys on record with if you're

650
00:53:39,680 --> 00:53:45,520
up for it, and see if you have similar concrete predictions are different. And then we can obviously

651
00:53:45,520 --> 00:53:52,240
check back in on in the future. How does the market for inference shape up? And for a jumping off point,

652
00:53:52,880 --> 00:53:57,760
how do you think it might look different from the current cloud infrastructure market?

653
00:53:58,640 --> 00:54:03,040
I think inference will be the vast majority, but I think it's like GPUs to assets with Bitcoin

654
00:54:03,040 --> 00:54:08,160
mining. Because these are big research artifacts that are pie torches, but the output is a little

655
00:54:08,240 --> 00:54:11,920
tiny part of binaries. And that's not a complicated thing to run inference on.

656
00:54:11,920 --> 00:54:18,160
You see in Forensia 2 on Amazon Cloud, you see kind of the TPU v5s and others. I think there'll

657
00:54:18,160 --> 00:54:22,800
be more and more customized solutions as you move from that research to engineering bit. And then

658
00:54:22,800 --> 00:54:27,600
the cost competition goes massive in a few years time. Over the next few years, I think there'll

659
00:54:27,600 --> 00:54:31,200
be a shortage because everyone will try to use this technology. There won't be enough. And then

660
00:54:31,200 --> 00:54:35,840
eventually it'll move towards the edge because I think there's just all this magnitude optimization

661
00:54:35,840 --> 00:54:39,200
that we can do from here. Yeah, I mean, this is a little bit beyond my

662
00:54:39,200 --> 00:54:43,680
direct wheelhouse. But I think in the end of the day, what I'd say is like, I highly suspect

663
00:54:43,680 --> 00:54:47,680
because the distribution is indifferent, right? And the patterns aren't different in any of this

664
00:54:47,680 --> 00:54:52,720
stuff that we're going to see is everything from chipsets all the way through to cloud providers.

665
00:54:52,720 --> 00:54:56,800
Things look basically the same as they do today. Everyone's just making more money.

666
00:54:57,360 --> 00:55:01,200
Yeah, I think inference is also interesting because in the cloud, you just move to wherever

667
00:55:01,200 --> 00:55:05,360
the cheapest inference is for these models. And so it's quite a mobile thing. So you've

668
00:55:05,360 --> 00:55:11,280
got NVIDIA coming forward for that reason. Question two, what happens to the price

669
00:55:11,280 --> 00:55:15,200
of primary care medicine in the United States over the next 10 years?

670
00:55:17,440 --> 00:55:23,760
Unfortunately, given the issues, I think it's correct. It should go down. The regulatory capture

671
00:55:23,760 --> 00:55:30,400
is far too strong. Unless something major, major happens. Question three, you guys both

672
00:55:30,480 --> 00:55:35,840
have kind of said there's a ton of junk out there. It seems like broadly, we're not expecting

673
00:55:36,960 --> 00:55:42,960
that many major incumbents to be disrupted. What would you guess would be the most likely

674
00:55:42,960 --> 00:55:49,760
incumbents to be disrupted if you had to pick some? Stack overflow. I think it's 4.1.6 billion

675
00:55:49,760 --> 00:55:56,720
by process, right? Whoever. I mean, I think the process is probably fine. You've seen disruption

676
00:55:56,800 --> 00:56:02,080
in CHEG and other things. We didn't really get into this, but I do think that some SaaS companies

677
00:56:02,080 --> 00:56:07,200
with low switching costs will be at risk from some of these higher context window companies

678
00:56:07,200 --> 00:56:12,160
where you can put 10,000 words of instructions in. Because some of them are relatively basic

679
00:56:13,040 --> 00:56:17,280
in that way. Actually, for words, I think we once again mostly agree. The only thing I think is

680
00:56:17,280 --> 00:56:23,120
at risk for things like Zapier, right? Or some of these like kind of like, and it's kind of a 50-50

681
00:56:23,120 --> 00:56:27,680
because they also get way more powerful. But I think there's a bunch of SaaS tools that

682
00:56:29,120 --> 00:56:33,120
probably end up looking more like features where they used to look maybe like companies

683
00:56:33,120 --> 00:56:39,440
because of the AI. But real incumbents, like public big multi-billion-dollar companies,

684
00:56:40,000 --> 00:56:43,120
I mean, I don't think any of them are really at risk in disruption. I think they're all just going

685
00:56:43,120 --> 00:56:48,480
to get stronger. I think a bunch of startups or series A companies are going to get swiped out

686
00:56:48,480 --> 00:56:52,160
or all of a sudden not going to be able to grow, right? Because I think the big guys will just get

687
00:56:52,160 --> 00:56:58,640
better faster. Will the big tech companies that are currently open sourcing,

688
00:56:59,440 --> 00:57:06,320
for example, Meta, Salesforce, will they continue to do so? Or will they stop?

689
00:57:07,040 --> 00:57:10,800
Well, I think Meta has moved to non-commercial open source for all their open source.

690
00:57:11,360 --> 00:57:15,840
Now, I think Salesforce has kind of continued to do full open source. I think it's just very

691
00:57:15,840 --> 00:57:20,560
difficult because the regulatory environment become tougher and tougher. And it's not called to their

692
00:57:20,560 --> 00:57:28,800
business to open source. I think that it would be 100% driven by business models, right? So like

693
00:57:28,800 --> 00:57:35,840
Meta, if you think about it, is incredibly well positioned should generally the level of AI continue

694
00:57:35,840 --> 00:57:40,000
to grow in the world, right? If you think about it, it's like the way they're going to monetize that

695
00:57:40,000 --> 00:57:46,000
is having dramatically better ads, right? And like dramatically better content in a bunch of ways.

696
00:57:46,000 --> 00:57:50,000
And so I think they have a heavy incentive to think about it, to like keep up and sourcing

697
00:57:50,000 --> 00:57:54,240
it, they want the talent, they want, you know, the reason companies also open source is like,

698
00:57:54,240 --> 00:57:59,040
there's like a real internal external interplay, right? In terms of how you build an ecosystem,

699
00:57:59,040 --> 00:58:03,600
they attract great talent. So I think they'll still keep happening. But I think you'll, I think

700
00:58:03,600 --> 00:58:07,280
the list of people who are supporting open source stuff will shrink, right? If that makes

701
00:58:07,280 --> 00:58:11,520
sense, as people get super competitive about this stuff, and the battle lines are drawn.

702
00:58:12,080 --> 00:58:19,200
If you had a billion dollar company, you know, of any, of any kind, could you come up with a story?

703
00:58:19,200 --> 00:58:24,000
Could you identify a type of company that should not, you know, where it wouldn't make sense, or

704
00:58:24,000 --> 00:58:30,880
let's even frame it more decisively, where it would be defensible to not be investing, say,

705
00:58:30,880 --> 00:58:36,720
at least a million dollars in figuring generative AI out today? In other words, is there anywhere

706
00:58:36,720 --> 00:58:41,760
where this is not relevant? I mean, I'm sure there is, but nowhere I can think of offhand.

707
00:58:41,760 --> 00:58:46,000
I think it's relevant just about everywhere, just because you always get a level of productivity

708
00:58:46,000 --> 00:58:51,360
increase. But, you know, as Sam said, for a lot of industries, is this sustaining innovation? It's

709
00:58:51,360 --> 00:58:57,200
just the next stage, as opposed to massively well changing, shall we say? What happens to

710
00:58:57,200 --> 00:59:06,080
the marriage rate and the birth rate in, say, the United States as AI companions of all sorts

711
00:59:06,720 --> 00:59:11,840
become available? It clearly goes down everywhere. I mean, like, look at South Korea, they're at

712
00:59:11,840 --> 00:59:16,320
0.8 now on their fertility rate thanks to video games and a few other factors.

713
00:59:16,320 --> 00:59:20,640
There are negative and positive ways to spin this, actually. Like, I think I personally, I

714
00:59:20,640 --> 00:59:23,920
have the negative take on this, like, I think that's the future and a bunch of other things,

715
00:59:23,920 --> 00:59:28,000
but here's the reality. It's just a simple economics thing, which is like, if the world was

716
00:59:28,000 --> 00:59:34,000
more entertaining, then like, that makes doing un-entertaining, hard, long things like having

717
00:59:34,000 --> 00:59:39,440
kids and raising them like less appealing, right? It's like, Tinder, Tinder is going to hurt the

718
00:59:39,440 --> 00:59:44,080
birth rate. Like, AI is going to hurt, again, it's just sustaining innovation, which is technology

719
00:59:44,080 --> 00:59:47,920
generally is going to hurt the birth rate. Yeah, and then you see places like Japan where you've

720
00:59:47,920 --> 00:59:51,680
got declining birth rates, really embracing this because they want the productivity increase,

721
00:59:51,680 --> 00:59:55,440
which is the other flip side of this. See, if you're more productive, you less people.

722
00:59:55,440 --> 00:59:59,520
Yeah, I mean, that's the irony that you talk about the long-lived and the diamond age you

723
00:59:59,520 --> 01:00:03,040
referenced earlier, like a really long-term sci-fi story is pretty simple, which is like,

724
01:00:03,840 --> 01:00:09,760
a highest, highest, highest level, like technology will drive there to be fewer people. And then

725
01:00:09,760 --> 01:00:14,480
because there are fewer people, we need more technology, right? And like, it becomes a symbiotic

726
01:00:14,480 --> 01:00:17,520
thing. That's the really sad part. I mean, like, it's all sort of people thinking about like, oh,

727
01:00:17,520 --> 01:00:21,920
shit, like the entire human population is going to fall off a cliff, right? It's because we're

728
01:00:21,920 --> 01:00:28,720
like entertaining ourselves to death. Do you think any AI leader, you know, open AI right now or

729
01:00:29,680 --> 01:00:34,640
somebody who takes, you know, the leading position from them in terms of having the best model,

730
01:00:35,680 --> 01:00:43,360
can sustain super high gross margins for a few years into the future?

731
01:00:43,360 --> 01:00:47,120
Based purely on the AI, no. It needs to be distribution data.

732
01:00:47,120 --> 01:00:54,560
I think the proprietary side, it's just unless it's super data unique, you're going to zero.

733
01:00:54,640 --> 01:00:59,840
I think that you have Google and open AI as uneconomic actors. And that's incredibly difficult.

734
01:00:59,840 --> 01:01:03,840
You know, so just to unpack that, you mean that basically they won't allow,

735
01:01:04,400 --> 01:01:07,840
they don't intend to make a ton of money on this and they won't allow any models to either because

736
01:01:07,840 --> 01:01:12,320
they're going to provide it at cost. They don't care about, yeah, they're probably under cost to

737
01:01:12,320 --> 01:01:16,400
get the data. You know, again, they have different business models, Google cost shifts all the time,

738
01:01:16,400 --> 01:01:21,600
right? This is why I went to the other side for open models to private data and standardizing that.

739
01:01:21,600 --> 01:01:26,320
No one's making money on open models alone. Well, I mean, there is a way, there is a way.

740
01:01:27,440 --> 01:01:31,120
So what basically what I do with my business model is standardizing it.

741
01:01:32,320 --> 01:01:36,720
And then providing all the services around it as a blueprint for my partners to take forward.

742
01:01:36,720 --> 01:01:43,520
Yeah, I mean, they're the consulting nexus version of this, like that you can probably pull off.

743
01:01:43,520 --> 01:01:47,680
Again, consulting models, I think, again, obviously saying you're pursuing, but very difficult.

744
01:01:47,680 --> 01:01:50,720
I build the models, I give it to my consulting partners and they take it forward.

745
01:01:50,720 --> 01:01:57,200
That's why this is my theory of stability has been a partial theory. It's obviously a lot of

746
01:01:57,200 --> 01:02:05,440
facets to the organization. But I kind of view stability as the the provider for like the non

747
01:02:05,440 --> 01:02:10,000
aligned countries, if you will, like those that are like, we definitely don't want to buy from

748
01:02:10,720 --> 01:02:18,160
corporate America. We want to own our own. We want control. Those folks seem like they have

749
01:02:18,160 --> 01:02:23,920
nowhere close to the resources domestically to build their own systems. But they do have kind of

750
01:02:23,920 --> 01:02:29,680
a point of pride and also just practicality, right? Like if you're an African government

751
01:02:29,680 --> 01:02:33,440
and you want to get your own legal system into a language model, you know, who's going to do that

752
01:02:33,440 --> 01:02:38,160
for you? That feels like a real sweet spot for stability. How much of the future do you think

753
01:02:38,160 --> 01:02:43,360
is kind of serving that kind of third set of countries? No, I mean, look, we're carrying

754
01:02:43,360 --> 01:02:48,000
subsidiaries and dozens of countries bringing all the top family offices with data and distribution.

755
01:02:48,400 --> 01:02:52,960
And national models and national data sets based on broadcast data, we take a subset of that make

756
01:02:52,960 --> 01:02:57,600
that open. And we've got the rest of that for our commercial side. So I think the global south

757
01:02:57,600 --> 01:03:02,800
is the focus for us. Plus, there were these big multinational companies building dedicated teams

758
01:03:02,800 --> 01:03:06,240
with them. Because we're the only company in the world that can build you a model of any

759
01:03:06,240 --> 01:03:11,280
single majority or type. Is that sustaining? Who knows, but it's a decent business. And so my

760
01:03:11,280 --> 01:03:15,360
thing was build a decent business doing decent stuff, doing something different to other people.

761
01:03:15,360 --> 01:03:18,080
I'm sure there'll be more competitors. But again, let's see how it goes.

762
01:03:18,960 --> 01:03:24,960
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually

763
01:03:24,960 --> 01:03:30,400
work customized across all platforms with a click of a button. I believe in Omniki so much

764
01:03:30,400 --> 01:03:40,160
that I invested in it. And I recommend you use it too. Use Kogrev to get a 10% discount.

