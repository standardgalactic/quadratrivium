1
00:00:00,000 --> 00:00:02,920
one of the things people constantly get wrong

2
00:00:02,920 --> 00:00:05,400
if they think about human level as the peak of things.

3
00:00:05,400 --> 00:00:08,760
And so like once we've patched this and now it works,

4
00:00:08,760 --> 00:00:10,560
that's not really how this goes.

5
00:00:10,560 --> 00:00:13,360
There is no, it goes from not working to working,

6
00:00:13,360 --> 00:00:15,360
it goes from working worse to working better,

7
00:00:15,360 --> 00:00:17,840
and they could always go to working better still.

8
00:00:17,840 --> 00:00:20,240
And that's one of the reasons why we should be

9
00:00:20,240 --> 00:00:22,880
more worried or more excited or

10
00:00:22,880 --> 00:00:24,640
more curious about what's going to

11
00:00:24,640 --> 00:00:26,800
happen like three years from now,

12
00:00:26,800 --> 00:00:28,720
five years from now, 10 years from now,

13
00:00:28,880 --> 00:00:30,240
they're just going to keep going.

14
00:00:30,240 --> 00:00:31,760
And the question is, what does that get you?

15
00:00:31,760 --> 00:00:33,840
We talk about like worrying about China,

16
00:00:33,840 --> 00:00:36,000
but like I'm more afraid of Meta.

17
00:00:36,000 --> 00:00:37,960
Like one individual American company

18
00:00:37,960 --> 00:00:40,480
scares me more than all of China right now.

19
00:00:40,480 --> 00:00:43,400
You know, if you understand the Yudkowsky

20
00:00:43,400 --> 00:00:46,240
and difficulties, lessons, right, in some sense,

21
00:00:46,240 --> 00:00:48,120
and the nature of what problems you have to solve,

22
00:00:48,120 --> 00:00:50,960
or you have leadership capabilities,

23
00:00:50,960 --> 00:00:52,840
then you are actually going to be valuable in those ways.

24
00:00:52,840 --> 00:00:56,720
And it would be a major mistake to join an existing

25
00:00:56,720 --> 00:00:58,640
organization and try to make a difference

26
00:00:58,640 --> 00:01:02,840
as an individual as opposed to trying to spearhead

27
00:01:02,840 --> 00:01:05,680
a new organization or at least a new,

28
00:01:05,680 --> 00:01:08,320
you know, branch of a existing major organization,

29
00:01:09,240 --> 00:01:10,840
depending on your skill set.

30
00:01:10,840 --> 00:01:13,240
Hello, and welcome to The Cognitive Revolution,

31
00:01:13,240 --> 00:01:15,440
where we interview visionary researchers,

32
00:01:15,440 --> 00:01:18,160
entrepreneurs, and builders working on the frontier

33
00:01:18,160 --> 00:01:20,080
of artificial intelligence.

34
00:01:20,080 --> 00:01:22,880
Each week we'll explore their revolutionary ideas,

35
00:01:22,880 --> 00:01:25,480
and together we'll build a picture of how AI technology

36
00:01:25,480 --> 00:01:30,000
will transform work, life, and society in the coming years.

37
00:01:30,000 --> 00:01:33,480
I'm Nathan LaBenz, joined by my co-host, Eric Tornberg.

38
00:01:33,480 --> 00:01:34,520
This is V. Machwitz.

39
00:01:34,520 --> 00:01:36,880
Welcome back to The Cognitive Revolution.

40
00:01:36,880 --> 00:01:37,880
Good to be back.

41
00:01:37,880 --> 00:01:40,280
So we're trying something a little bit different this time.

42
00:01:40,280 --> 00:01:43,720
We are going to do some analysis

43
00:01:43,720 --> 00:01:46,760
of what has been going on in AI over,

44
00:01:46,760 --> 00:01:49,160
let's say the last few weeks to a month.

45
00:01:49,160 --> 00:01:51,160
You have published, as you always do,

46
00:01:51,160 --> 00:01:53,360
a bunch of deep dive blog posts,

47
00:01:53,360 --> 00:01:55,760
kind of covering everything.

48
00:01:55,760 --> 00:01:57,920
And for folks who want your background,

49
00:01:57,920 --> 00:01:59,360
of course we just did a recent episode too,

50
00:01:59,360 --> 00:02:01,480
so they can go and hear about your world view

51
00:02:01,480 --> 00:02:04,880
and your, you know, your AI childhood all there.

52
00:02:04,880 --> 00:02:07,440
But for today, I just want to pick out

53
00:02:07,440 --> 00:02:09,600
some of the most important stories

54
00:02:09,600 --> 00:02:11,800
and get your take on them and kind of, you know,

55
00:02:11,800 --> 00:02:14,000
exchange, go back and forth with some questions

56
00:02:14,000 --> 00:02:15,560
and try to make some sense out of it.

57
00:02:15,560 --> 00:02:17,240
And hopefully that'll be useful,

58
00:02:17,240 --> 00:02:19,120
not just to us, but to the audience as well.

59
00:02:19,120 --> 00:02:20,880
Yeah, I think the easiest thing is that there's

60
00:02:20,880 --> 00:02:22,560
constantly news coming at all of us.

61
00:02:22,560 --> 00:02:24,080
And so it's easy to get lost in like,

62
00:02:24,080 --> 00:02:25,000
here's the thing, here's the thing,

63
00:02:25,000 --> 00:02:26,040
here's another thing, here's another thing.

64
00:02:26,040 --> 00:02:29,320
So that's good to step back and dive deep.

65
00:02:29,320 --> 00:02:32,280
So I organized this discussion around

66
00:02:32,280 --> 00:02:35,320
the concept of live players.

67
00:02:35,320 --> 00:02:40,160
You know, there are only so many organizations right now

68
00:02:40,160 --> 00:02:43,560
who seem to be really pushing the frontier

69
00:02:43,560 --> 00:02:47,240
and in a position to have a meaningful impact

70
00:02:47,240 --> 00:02:48,520
on the course of events.

71
00:02:48,520 --> 00:02:49,800
We talked last time a little bit about like

72
00:02:49,800 --> 00:02:51,200
how much does history matter

73
00:02:51,200 --> 00:02:53,360
and it seems like it matters in some ways

74
00:02:53,360 --> 00:02:54,600
and maybe less in other ways.

75
00:02:54,600 --> 00:02:56,800
But these are the folks that are kind of creating

76
00:02:56,800 --> 00:02:59,120
the history right now, the live players.

77
00:02:59,120 --> 00:03:00,560
So I thought we would just kind of run it down

78
00:03:00,560 --> 00:03:02,400
by going through some of the live players,

79
00:03:02,400 --> 00:03:05,360
talking about their recent announcements and releases

80
00:03:05,360 --> 00:03:08,120
and again, trying to make sense of where that fits into

81
00:03:08,120 --> 00:03:09,920
the broader big picture.

82
00:03:09,920 --> 00:03:14,520
And starting off naturally, we go to open AI.

83
00:03:14,520 --> 00:03:17,520
So reading your blog in preparation for this,

84
00:03:17,520 --> 00:03:20,000
obviously, you know, you can't go more than a few paragraphs

85
00:03:20,000 --> 00:03:23,920
without open AI coming up in one way, shape, or form.

86
00:03:23,920 --> 00:03:25,440
But the thing that stuck out to me

87
00:03:25,440 --> 00:03:29,600
as kind of the most interesting was the recent comment

88
00:03:29,600 --> 00:03:32,360
that Jan Leica had made and Jan is,

89
00:03:32,360 --> 00:03:34,080
for those that don't know the name,

90
00:03:34,080 --> 00:03:37,760
he's the head of alignment at open AI

91
00:03:37,760 --> 00:03:41,960
and along with Ilya Sotskyver leading

92
00:03:41,960 --> 00:03:44,880
the new super alignment team, as I understand it.

93
00:03:44,880 --> 00:03:48,160
I want to start off by just kind of an interesting disconnect

94
00:03:48,160 --> 00:03:51,840
between him and you and maybe me as well

95
00:03:51,840 --> 00:03:54,120
around just the power of GPT-4.

96
00:03:54,120 --> 00:03:55,600
So before we even get into, you know,

97
00:03:55,600 --> 00:03:57,640
the kind of speculation about the future,

98
00:03:57,640 --> 00:03:59,680
it really jumped out to me that he said,

99
00:03:59,680 --> 00:04:01,720
overall, GPT-4 is maybe at the level

100
00:04:01,720 --> 00:04:05,120
of a well-read college undergrad.

101
00:04:05,120 --> 00:04:06,880
And then you came back and said,

102
00:04:06,880 --> 00:04:10,760
you consider it to be well below human level.

103
00:04:10,760 --> 00:04:14,320
And I have often said that I consider it to be human level,

104
00:04:14,320 --> 00:04:16,960
but not human-like.

105
00:04:16,960 --> 00:04:19,880
And I've sort of been trying to refine

106
00:04:19,880 --> 00:04:23,440
what I mean by that in a few different ways over time.

107
00:04:23,440 --> 00:04:24,720
But for starters, let's get your take.

108
00:04:24,720 --> 00:04:26,800
What do you think is the disconnect between Jan and you

109
00:04:26,800 --> 00:04:29,120
there where he sees something like human level

110
00:04:29,120 --> 00:04:30,520
and you would say well below?

111
00:04:30,520 --> 00:04:32,680
Yeah, I don't think it's about the specific model at all,

112
00:04:32,680 --> 00:04:33,120
obviously.

113
00:04:33,120 --> 00:04:36,320
I think we both agree that GPT-4 is the dominant model

114
00:04:36,320 --> 00:04:39,040
right now and like we will be for some months to come,

115
00:04:39,040 --> 00:04:40,120
at least.

116
00:04:40,120 --> 00:04:42,120
But I think it's a matter of like,

117
00:04:42,120 --> 00:04:44,000
how do you think about what it means

118
00:04:44,000 --> 00:04:46,760
to be at the level of a college undergrad

119
00:04:46,760 --> 00:04:48,640
or what are we measuring?

120
00:04:48,640 --> 00:04:50,280
What are we judging by?

121
00:04:50,280 --> 00:04:56,080
And I think Laiki is thinking about it as, OK,

122
00:04:56,080 --> 00:05:00,120
in terms of ability to just deal with a variety of random

123
00:05:00,120 --> 00:05:02,400
questions that were typically thrown at something,

124
00:05:02,400 --> 00:05:05,040
how is it going to do compared to the average college

125
00:05:05,040 --> 00:05:05,440
undergrad?

126
00:05:05,440 --> 00:05:06,880
He's like, what's about that level?

127
00:05:06,880 --> 00:05:09,160
You have a well-read college undergrad.

128
00:05:09,160 --> 00:05:12,000
Whereas that's an important question

129
00:05:12,000 --> 00:05:14,360
to be asking for practical purposes.

130
00:05:14,360 --> 00:05:17,440
But to me is not the relevant question

131
00:05:17,440 --> 00:05:20,600
to what the things are that we're thinking about.

132
00:05:20,600 --> 00:05:21,920
And that's one of the, when he says

133
00:05:21,920 --> 00:05:24,440
he's going to align a human level alignment researcher

134
00:05:24,440 --> 00:05:26,920
within four years, I thought that assumes

135
00:05:26,920 --> 00:05:29,840
that there's going to be a much, much more powerful AI

136
00:05:29,840 --> 00:05:33,040
four years from now waiting to be aligned.

137
00:05:33,040 --> 00:05:34,640
It's not talking about aligned GPT-4

138
00:05:34,640 --> 00:05:35,880
and then pointing at alignment.

139
00:05:35,880 --> 00:05:37,280
That obviously wouldn't do anything.

140
00:05:37,280 --> 00:05:40,520
It's going to deal with some of your blocks

141
00:05:40,520 --> 00:05:42,200
and it's going to increase in your affordances

142
00:05:42,200 --> 00:05:43,640
and your efficiency somewhat.

143
00:05:43,680 --> 00:05:46,120
Maybe you'll be 50% faster with GPT-4

144
00:05:46,120 --> 00:05:48,920
than you would have been without any LMS.

145
00:05:48,920 --> 00:05:51,880
Maybe even 100% faster if you're using it really well

146
00:05:51,880 --> 00:05:53,600
and things connect to it really well.

147
00:05:53,600 --> 00:05:56,880
And in the context of alignment, obviously

148
00:05:56,880 --> 00:05:58,480
having a model to experiment with and bang on

149
00:05:58,480 --> 00:06:01,040
is distinct from the thing that we're talking about here,

150
00:06:01,040 --> 00:06:03,680
but is potentially necessary.

151
00:06:03,680 --> 00:06:06,240
But it's not going to be able to substitute for anything

152
00:06:06,240 --> 00:06:07,400
like a human researcher.

153
00:06:07,400 --> 00:06:09,680
If you put a well-read college undergrad

154
00:06:09,680 --> 00:06:12,240
on the problem of something complex,

155
00:06:12,280 --> 00:06:14,680
like aligning a model,

156
00:06:14,680 --> 00:06:17,600
they could potentially begin to make progress.

157
00:06:17,600 --> 00:06:20,760
And if you asked GPT-4 to do that, you would get nothing.

158
00:06:21,800 --> 00:06:24,480
And part of that is that we haven't figured out

159
00:06:24,480 --> 00:06:27,720
how to structure how we talk to it

160
00:06:27,720 --> 00:06:29,840
and turn it into a proper agent

161
00:06:29,840 --> 00:06:32,240
and give it the proper memory and so on.

162
00:06:32,240 --> 00:06:34,560
But to me, most of it is just,

163
00:06:34,560 --> 00:06:37,480
every system has what you might call a raw G to it,

164
00:06:37,480 --> 00:06:41,840
whether it's a human or an artificial intelligence.

165
00:06:41,840 --> 00:06:46,120
And on that level, I feel like GPT-4

166
00:06:46,120 --> 00:06:51,120
is still well below the IQ 100 median human.

167
00:06:51,320 --> 00:06:53,280
It is going to obviously answer

168
00:06:53,280 --> 00:06:56,040
my ordinary day-to-day questions much better

169
00:06:56,040 --> 00:06:58,720
than if I asked an ordinary IQ 100 human

170
00:06:58,720 --> 00:07:01,080
to help me out with a variety of questions.

171
00:07:01,080 --> 00:07:04,400
That's because it has these huge advantages over a human.

172
00:07:04,400 --> 00:07:08,040
It has access to orders of magnitude and more knowledge

173
00:07:08,080 --> 00:07:12,000
and memory and ability to go through cycles.

174
00:07:12,000 --> 00:07:15,520
But there's still this dynamic in my brain

175
00:07:15,520 --> 00:07:19,360
where if you don't have the G,

176
00:07:19,360 --> 00:07:21,800
problems that require more G than you have

177
00:07:21,800 --> 00:07:25,320
become exponentially harder or then impossible to do

178
00:07:25,320 --> 00:07:28,480
very quickly as you exceed that.

179
00:07:28,480 --> 00:07:30,680
And in that sense,

180
00:07:30,680 --> 00:07:33,920
like the college undergrad had the chance given time

181
00:07:33,960 --> 00:07:38,120
and is smarter and GPT-4 is just nowhere near

182
00:07:38,120 --> 00:07:39,400
that kind of thing.

183
00:07:39,400 --> 00:07:44,320
So when you say that the missing pieces around memory

184
00:07:44,320 --> 00:07:49,320
and kind of packaging GPT-4 or successor up into an agent,

185
00:07:50,680 --> 00:07:53,560
those do feel to me also like being kind of

186
00:07:54,560 --> 00:07:57,040
pretty key missing pieces here.

187
00:07:57,040 --> 00:08:00,600
I mean, there are sort of potentially synergies

188
00:08:00,640 --> 00:08:05,240
between those kinds of parts of a system being built out

189
00:08:05,240 --> 00:08:08,440
and it just being smarter overall.

190
00:08:08,440 --> 00:08:12,520
But it seems like those are like pretty distinct concepts

191
00:08:12,520 --> 00:08:16,440
in that GPT-4 could have like a much better memory

192
00:08:16,440 --> 00:08:19,360
and certainly people are working on all sorts of like

193
00:08:19,360 --> 00:08:22,040
schemes for that and embedding databases

194
00:08:22,040 --> 00:08:24,200
and how do you put stuff into the embedding database

195
00:08:24,200 --> 00:08:26,240
and do you even like,

196
00:08:26,240 --> 00:08:28,000
some of the most interesting stuff I've seen recently

197
00:08:28,000 --> 00:08:31,400
has been kind of creating a layer of like synthetic memory

198
00:08:31,400 --> 00:08:34,920
that sits on top of the raw observational memory

199
00:08:34,920 --> 00:08:37,560
that tries to kind of ultimately work its way up

200
00:08:37,560 --> 00:08:40,880
into something like a coherent narrative,

201
00:08:40,880 --> 00:08:44,600
that could still kind of fit into prompt context length,

202
00:08:44,600 --> 00:08:47,640
but kind of summarizes, synthesizes,

203
00:08:47,640 --> 00:08:49,880
represents all these detailed memories

204
00:08:49,880 --> 00:08:51,680
in a hopefully coherent way.

205
00:08:51,680 --> 00:08:54,440
Obviously is what the developers are going for there.

206
00:08:54,440 --> 00:08:57,360
Those pieces seem like, yeah, they're totally missing.

207
00:08:57,360 --> 00:09:00,080
I expect them to come online, you know,

208
00:09:00,080 --> 00:09:02,600
somewhat gradually, but certainly over the next six months

209
00:09:02,600 --> 00:09:04,800
to a year, if not maybe even sooner.

210
00:09:04,800 --> 00:09:06,680
And then I am kind of like,

211
00:09:06,680 --> 00:09:08,160
it does seem like this, you know,

212
00:09:08,160 --> 00:09:11,520
GPT-4 with those weaknesses kind of patched,

213
00:09:11,520 --> 00:09:13,640
it does seem to me like it would be

214
00:09:13,640 --> 00:09:16,400
roughly at that college under grad level.

215
00:09:16,400 --> 00:09:18,280
If those things did come online,

216
00:09:18,280 --> 00:09:19,440
would you see that the same way

217
00:09:19,440 --> 00:09:22,200
or you still think it's like missing something super important?

218
00:09:22,200 --> 00:09:23,040
No, I'm sorry.

219
00:09:23,040 --> 00:09:24,840
I'm definitely not on Team Physicastic Power, right?

220
00:09:24,840 --> 00:09:27,320
Like I'm in no way on that team.

221
00:09:27,320 --> 00:09:29,840
However, I do think in a real sense,

222
00:09:29,840 --> 00:09:32,440
what you're witnessing is, you know,

223
00:09:32,440 --> 00:09:35,120
the training data covers, you know,

224
00:09:35,120 --> 00:09:37,400
the vast majority of things humans do and say

225
00:09:37,400 --> 00:09:40,680
and consider in various senses over text.

226
00:09:40,680 --> 00:09:44,360
And, you know, within the sample of the training data,

227
00:09:44,360 --> 00:09:46,360
like while you're doing things similar to the training data,

228
00:09:46,360 --> 00:09:49,880
it's learned how to pattern match and copy and imitate

229
00:09:49,880 --> 00:09:50,720
and work with that.

230
00:09:50,720 --> 00:09:53,480
And it has a huge amount of knowledge base

231
00:09:53,480 --> 00:09:56,720
and levels of association and the tools to work within that.

232
00:09:56,720 --> 00:09:59,360
And if you gave it these other tools,

233
00:09:59,360 --> 00:10:00,480
we'll be able to do these things

234
00:10:00,480 --> 00:10:03,080
and string them together across more steps

235
00:10:03,080 --> 00:10:04,560
in some important sense.

236
00:10:04,560 --> 00:10:08,120
But the moment you take it out of its comfort zone,

237
00:10:08,120 --> 00:10:11,560
we're asking you to do something that's distinctly different

238
00:10:11,560 --> 00:10:14,520
than what has come before to be truly original.

239
00:10:14,520 --> 00:10:17,920
I think your episode with the Hollywood writers

240
00:10:17,920 --> 00:10:20,000
and like they talked about what was going on in the second

241
00:10:20,000 --> 00:10:22,840
and trying to get the GPT forwarded to work for them.

242
00:10:22,840 --> 00:10:26,280
And yeah, it was great at generating like generic schlock,

243
00:10:26,280 --> 00:10:28,200
right? Like much better than they could.

244
00:10:28,200 --> 00:10:29,440
And like if you needed to be like, okay,

245
00:10:29,440 --> 00:10:30,480
somebody get me unstuck,

246
00:10:30,480 --> 00:10:33,560
somebody get me some generic schlock based on my situation

247
00:10:33,560 --> 00:10:34,720
that I happened to have been written in

248
00:10:34,720 --> 00:10:37,880
because this is episode 47 of the show or whatever it is.

249
00:10:37,880 --> 00:10:39,360
It could be tremendously helpful.

250
00:10:39,360 --> 00:10:41,360
But whenever you asked it to actually do something

251
00:10:41,360 --> 00:10:44,440
that we would recognize as distinctly creative

252
00:10:44,440 --> 00:10:47,800
and original, you know, in a way that's distinct from that,

253
00:10:47,800 --> 00:10:49,640
they just fall over flight every time.

254
00:10:49,640 --> 00:10:51,440
And none of those problems are gonna be rescued

255
00:10:51,440 --> 00:10:52,720
by any of these fixes, right?

256
00:10:52,720 --> 00:10:55,040
Like they're just orthogonal problems, right?

257
00:10:55,040 --> 00:10:58,600
Like I think that's the sense in which, you know,

258
00:10:58,600 --> 00:11:02,600
you're going to be able to give it more capacities

259
00:11:02,600 --> 00:11:06,040
to be able to navigate more of the conventional things

260
00:11:06,040 --> 00:11:09,000
over longer periods more consistently.

261
00:11:09,000 --> 00:11:11,320
And that's gonna have tremendous mundane utility

262
00:11:11,320 --> 00:11:15,760
as I call it, or it's gonna be a much better functioning system.

263
00:11:15,760 --> 00:11:19,800
But the reason why I'm focused on this other question

264
00:11:19,800 --> 00:11:21,360
is because I am focused on the question

265
00:11:21,360 --> 00:11:23,440
of how dangerous the system is, right?

266
00:11:23,440 --> 00:11:25,520
Like I'm asking the question,

267
00:11:25,520 --> 00:11:27,640
could this system potentially engage

268
00:11:27,640 --> 00:11:29,800
in recursive self-improvement?

269
00:11:29,800 --> 00:11:33,640
Could this program potentially pose a threat to humans?

270
00:11:33,640 --> 00:11:34,480
Right?

271
00:11:34,480 --> 00:11:35,320
Could it compete for resources?

272
00:11:35,320 --> 00:11:37,400
Could it manipulate us?

273
00:11:37,400 --> 00:11:40,360
Could it do things that are actively destructive

274
00:11:40,360 --> 00:11:41,680
because it uncovers capabilities

275
00:11:41,680 --> 00:11:44,760
that like weren't in its training set in various ways

276
00:11:44,760 --> 00:11:46,760
and other related questions like that?

277
00:11:47,680 --> 00:11:49,600
And I don't see the kinds of things

278
00:11:49,600 --> 00:11:51,300
that you're talking about that I agree will come online.

279
00:11:51,980 --> 00:11:54,860
I would guess that we will be far from done

280
00:11:54,860 --> 00:11:55,940
with family here from now.

281
00:11:55,940 --> 00:11:57,420
Like there's just sort of so much to do

282
00:11:57,420 --> 00:11:59,900
in terms of scaling those up as much as possible.

283
00:11:59,900 --> 00:12:03,500
Cause like one of the things people constantly get wrong

284
00:12:03,500 --> 00:12:06,540
is they think about human level as the peak of things.

285
00:12:06,540 --> 00:12:09,500
And so like once we've patched this and now it works,

286
00:12:09,500 --> 00:12:11,180
that's not really how this goes, right?

287
00:12:11,180 --> 00:12:13,900
There is no, it goes from not working to working,

288
00:12:13,900 --> 00:12:16,220
it goes from working worse to working better

289
00:12:16,220 --> 00:12:18,660
and it could always go to working better still.

290
00:12:18,660 --> 00:12:19,500
And that's one of the reasons

291
00:12:19,500 --> 00:12:23,220
why we should be more worried or more excited

292
00:12:23,220 --> 00:12:26,340
or more curious about what's going to happen

293
00:12:26,340 --> 00:12:28,140
like three years from now, five years from now,

294
00:12:28,140 --> 00:12:29,460
10 years from now.

295
00:12:29,460 --> 00:12:30,540
We look at these systems

296
00:12:30,540 --> 00:12:32,180
is because there isn't gonna be a hard cap.

297
00:12:32,180 --> 00:12:35,100
We're not gonna max out each of these individual capabilities

298
00:12:35,100 --> 00:12:38,260
by default, they're just gonna keep going.

299
00:12:38,260 --> 00:12:40,060
And the question is, what does that get you?

300
00:12:40,060 --> 00:12:41,340
Kind of want to look at this from two angles.

301
00:12:41,340 --> 00:12:44,140
One is going back to the original disagreement

302
00:12:44,140 --> 00:12:45,940
or it's maybe less of a disagreement

303
00:12:45,940 --> 00:12:47,980
and more of kind of a difference in framing

304
00:12:47,980 --> 00:12:52,980
perhaps with Yian, what I would bottom line all that as

305
00:12:53,020 --> 00:12:58,020
is when you think about a well-read college undergrad,

306
00:12:58,260 --> 00:13:00,700
you think about high points

307
00:13:00,700 --> 00:13:03,340
in that individual human's performance

308
00:13:03,340 --> 00:13:05,740
that GBT-4 can't match

309
00:13:05,740 --> 00:13:08,460
and it's not really a question of memory

310
00:13:08,460 --> 00:13:11,380
or whatever that's kind of gating it.

311
00:13:11,380 --> 00:13:13,740
And if I had to guess, I would say he's maybe more looking

312
00:13:13,740 --> 00:13:18,740
at like average performance or sort of some sort of floor

313
00:13:20,100 --> 00:13:23,620
perhaps like maybe top 90% or whatever,

314
00:13:23,620 --> 00:13:24,940
you could frame it in a lot of different ways,

315
00:13:24,940 --> 00:13:27,700
but it sounds like you're kind of concerned with high points

316
00:13:27,700 --> 00:13:28,980
and he is maybe more concerned

317
00:13:28,980 --> 00:13:33,060
with some sort of central tendency sort of measure.

318
00:13:33,060 --> 00:13:34,700
I would put it differently.

319
00:13:34,700 --> 00:13:37,820
I would say he's concerned with some sense

320
00:13:37,820 --> 00:13:39,820
of average level of performance

321
00:13:39,820 --> 00:13:42,300
over a range of possible tasks.

322
00:13:42,300 --> 00:13:45,980
And I'm concerned with potential.

323
00:13:45,980 --> 00:13:49,220
I am concerned with what the capabilities would be

324
00:13:49,220 --> 00:13:52,260
if you got a chance to work with this thing

325
00:13:52,260 --> 00:13:54,500
to try and make it the best it could be, right?

326
00:13:54,500 --> 00:13:56,060
It doesn't necessarily have to be right now,

327
00:13:56,060 --> 00:14:00,220
but the reason why we value children

328
00:14:00,220 --> 00:14:03,300
and college grad and these undergraduates in these classes,

329
00:14:03,300 --> 00:14:07,020
I guess this undergraduate, they're an idiot, right?

330
00:14:07,020 --> 00:14:07,860
In some important sense.

331
00:14:07,860 --> 00:14:09,340
They know nothing about the world.

332
00:14:09,340 --> 00:14:11,740
They know nothing about how to do anything productive.

333
00:14:11,740 --> 00:14:13,860
They are gonna show up on the job on day one

334
00:14:13,860 --> 00:14:14,860
after graduating from college.

335
00:14:14,860 --> 00:14:17,820
They're gonna be useless pieces of junk,

336
00:14:17,820 --> 00:14:20,180
but a useless piece of junk that can then learn

337
00:14:21,340 --> 00:14:23,100
to be something great.

338
00:14:23,100 --> 00:14:26,500
And even then they're gonna only learn a very narrow portion

339
00:14:26,500 --> 00:14:29,220
of the things that a individual human is capable of learning.

340
00:14:29,220 --> 00:14:33,100
They're gonna learn that one job in that one area

341
00:14:33,100 --> 00:14:34,580
and they're gonna be very, very specialized

342
00:14:34,580 --> 00:14:36,340
compared to a TBD4.

343
00:14:36,340 --> 00:14:39,900
So if you are doing generalized tests

344
00:14:39,940 --> 00:14:42,620
and comparing these undergraduate who are educational,

345
00:14:42,620 --> 00:14:45,500
this does try to make well-rounded in some senses.

346
00:14:46,420 --> 00:14:49,180
It's gonna beat the well-rounded undergraduate

347
00:14:49,180 --> 00:14:51,740
because it has this ability to read every book ever written

348
00:14:51,740 --> 00:14:54,140
and everything on Reddit and everything on Twitter

349
00:14:54,140 --> 00:14:55,500
and blah, blah, blah.

350
00:14:55,500 --> 00:14:58,780
But when it comes down to solving a particular problem,

351
00:14:58,780 --> 00:15:00,020
if you find the right undergraduate

352
00:15:00,020 --> 00:15:03,140
who has focused on the particular thing that you wanna know

353
00:15:03,140 --> 00:15:05,260
and you give them a chance to use their compute

354
00:15:05,260 --> 00:15:07,620
and process because they're not as fast,

355
00:15:07,620 --> 00:15:09,580
I think the undergraduate's gonna dominate you.

356
00:15:09,580 --> 00:15:13,780
I think even a relatively normal human being,

357
00:15:14,700 --> 00:15:16,260
given an opportunity,

358
00:15:16,260 --> 00:15:19,180
will outperform quite resoundingly

359
00:15:19,180 --> 00:15:21,260
what can be done in that way.

360
00:15:21,260 --> 00:15:22,860
And that's the thing that I care about

361
00:15:22,860 --> 00:15:25,460
because that's the thing that's going to potentially

362
00:15:25,460 --> 00:15:29,900
both threaten us and also unleash the whips upon waves

363
00:15:29,900 --> 00:15:33,700
of super amazing value that we're looking for in the future.

364
00:15:33,700 --> 00:15:35,300
It's not just negative.

365
00:15:35,300 --> 00:15:38,660
If we want AI to solve the problems that we haven't solved

366
00:15:38,660 --> 00:15:40,940
rather than just get us nowhere faster,

367
00:15:40,940 --> 00:15:42,380
in some important sense,

368
00:15:42,380 --> 00:15:44,180
it's gonna have to be able to do these things, right?

369
00:15:44,180 --> 00:15:46,620
These are the things where it really counts.

370
00:15:46,620 --> 00:15:48,580
Hey, we'll continue our interview in a moment

371
00:15:48,580 --> 00:15:49,980
after a word from our sponsors.

372
00:15:49,980 --> 00:15:50,820
Hey, everybody.

373
00:15:50,820 --> 00:15:52,780
If you're a business owner or founder like me,

374
00:15:52,780 --> 00:15:55,220
you'll wanna know more about our sponsor NetSuite.

375
00:15:56,100 --> 00:15:57,580
NetSuite provides financial software

376
00:15:57,580 --> 00:15:58,780
for all your businesses.

377
00:15:58,780 --> 00:16:01,420
Whether you're looking for an ERP tool or accounting software,

378
00:16:01,420 --> 00:16:03,540
NetSuite gives you the visibility and control you need

379
00:16:03,540 --> 00:16:05,460
to make better decisions faster.

380
00:16:05,460 --> 00:16:07,460
And for the first time in NetSuite's 25 years

381
00:16:07,500 --> 00:16:09,420
as the number one cloud financial system,

382
00:16:09,420 --> 00:16:12,060
you can defer payments of a full NetSuite implementation

383
00:16:12,060 --> 00:16:13,220
for six months.

384
00:16:13,220 --> 00:16:15,780
That's no payment and no interest for six months.

385
00:16:15,780 --> 00:16:16,620
And you can take advantage

386
00:16:16,620 --> 00:16:18,620
of the special financing offered today.

387
00:16:18,620 --> 00:16:19,500
NetSuite is number one

388
00:16:19,500 --> 00:16:21,300
because they give your business everything you need

389
00:16:21,300 --> 00:16:23,180
in real time, all in one place

390
00:16:23,180 --> 00:16:25,940
to reduce manual processes, boost efficiency,

391
00:16:25,940 --> 00:16:27,980
build forecasts, and increase productivity

392
00:16:27,980 --> 00:16:29,500
across every department.

393
00:16:29,500 --> 00:16:32,460
More than 36,000 companies have already upgraded in NetSuite,

394
00:16:32,460 --> 00:16:34,580
gaining visibility and control over their financials,

395
00:16:34,580 --> 00:16:37,260
inventory, HR, e-commerce, and more.

396
00:16:37,260 --> 00:16:38,780
If you've been checking out NetSuite already,

397
00:16:38,780 --> 00:16:40,380
then you know this deal is unprecedented,

398
00:16:40,380 --> 00:16:41,900
no interest, no payments.

399
00:16:41,900 --> 00:16:43,700
So take advantage of the special financing offer

400
00:16:43,700 --> 00:16:46,980
with our promo code at netsuite.com slash cognitive,

401
00:16:46,980 --> 00:16:49,380
netsuite.com slash cognitive,

402
00:16:49,380 --> 00:16:51,460
to get the visibility and control your business needs

403
00:16:51,460 --> 00:16:52,700
to weather any storm.

404
00:16:52,700 --> 00:16:55,260
That is netsuite.com slash cognitive.

405
00:16:56,100 --> 00:16:59,460
Omnike uses generative AI to enable you to launch

406
00:16:59,460 --> 00:17:01,420
hundreds of thousands of ad iterations

407
00:17:01,420 --> 00:17:04,940
that actually work customized across all platforms

408
00:17:04,940 --> 00:17:06,260
with a click of a button.

409
00:17:06,260 --> 00:17:08,820
I believe in Omnike so much that I invested in it

410
00:17:08,820 --> 00:17:11,020
and I recommend you use it too.

411
00:17:11,020 --> 00:17:13,860
Use CogGrav to get a 10% discount.

412
00:17:13,860 --> 00:17:14,700
Yeah, it's interesting.

413
00:17:14,700 --> 00:17:16,020
I'm certainly concerned with all of that too.

414
00:17:16,020 --> 00:17:18,580
I think maybe I'm just more enthused

415
00:17:18,580 --> 00:17:22,660
about the mundane utility in the sense of,

416
00:17:22,660 --> 00:17:23,980
man, there's a lot of stupid stuff

417
00:17:23,980 --> 00:17:25,300
that people spend their time doing

418
00:17:25,300 --> 00:17:29,100
and I really would love to see them freed

419
00:17:29,100 --> 00:17:31,260
from having to do a lot of that stuff.

420
00:17:31,260 --> 00:17:34,340
But I think your term is perfect, right?

421
00:17:34,340 --> 00:17:37,580
It's a lot of stupid stuff that humans have to do, right?

422
00:17:37,580 --> 00:17:41,340
Like basically, even if you are an average person,

423
00:17:41,340 --> 00:17:45,700
you're gonna spend the vast majority of your time

424
00:17:45,700 --> 00:17:48,700
doing things that do not especially tax your intelligence.

425
00:17:48,700 --> 00:17:50,980
They do not especially require you to think hard.

426
00:17:50,980 --> 00:17:54,140
They do not put you at the peak of your abilities, right?

427
00:17:54,140 --> 00:17:55,900
They don't put you in a zone.

428
00:17:55,900 --> 00:17:58,240
They're just, okay, somebody has to file this paperwork.

429
00:17:58,240 --> 00:18:00,140
Okay, somebody has to work this retail counter.

430
00:18:00,140 --> 00:18:01,460
Somebody has to cash this check.

431
00:18:01,460 --> 00:18:03,500
Somebody has to do this thing.

432
00:18:03,580 --> 00:18:04,820
Be nice to this person.

433
00:18:04,820 --> 00:18:07,140
Somebody has to make sure that someone has direct.

434
00:18:07,140 --> 00:18:10,460
That's good work and noble work and it has to be done, right?

435
00:18:10,460 --> 00:18:12,420
And physical labor is the same way.

436
00:18:12,420 --> 00:18:13,820
If a physical laborer had to do things

437
00:18:13,820 --> 00:18:17,620
that were at the peak of their mental or physical requirements

438
00:18:17,620 --> 00:18:19,180
more than a few minutes or at most,

439
00:18:19,180 --> 00:18:22,940
if small portion of the day, it would break them.

440
00:18:22,940 --> 00:18:25,380
And also like those jobs just don't exist, right?

441
00:18:25,380 --> 00:18:27,900
Like you need someone strong

442
00:18:27,900 --> 00:18:29,420
so that in that moment you can have someone strong.

443
00:18:29,420 --> 00:18:31,180
You need someone smart so that in the few moments

444
00:18:31,180 --> 00:18:32,180
when it's important to have someone smart,

445
00:18:32,380 --> 00:18:33,540
you have someone smart.

446
00:18:33,540 --> 00:18:37,500
If you can then take the bottom 80% of my job

447
00:18:37,500 --> 00:18:39,740
and you can do an 80% good job of that

448
00:18:39,740 --> 00:18:41,660
so that I only have to do the remaining 20% of that,

449
00:18:41,660 --> 00:18:44,340
now two thirds of my day is free.

450
00:18:44,340 --> 00:18:47,260
And I can be three times as productive, right?

451
00:18:47,260 --> 00:18:49,060
That's a tremendous leap and I agree.

452
00:18:49,060 --> 00:18:52,260
That is the potential of GPT-4, right?

453
00:18:52,260 --> 00:18:54,940
That's what we're looking at here is

454
00:18:54,940 --> 00:18:58,420
if we understand how to use this technology properly,

455
00:18:58,420 --> 00:19:01,180
we can potentially free ourselves from a lot of drudgery

456
00:19:02,540 --> 00:19:04,700
and streamline a bunch of stuff

457
00:19:04,700 --> 00:19:07,980
and get to do all the cool things.

458
00:19:07,980 --> 00:19:09,620
And there are various traps we can fall into,

459
00:19:09,620 --> 00:19:11,340
one of which is that we automate exactly the things

460
00:19:11,340 --> 00:19:12,180
we don't wanna be automating,

461
00:19:12,180 --> 00:19:13,980
not the things we do wanna be automating.

462
00:19:13,980 --> 00:19:15,740
One of which is that the moment we notice

463
00:19:15,740 --> 00:19:16,580
that paperwork is faster,

464
00:19:16,580 --> 00:19:17,740
now we put in more paperwork

465
00:19:17,740 --> 00:19:20,360
and now it turns out that humans are taking just as long

466
00:19:20,360 --> 00:19:22,660
to do more useless stuff than they did before.

467
00:19:22,660 --> 00:19:27,140
And GPT is just keeping us, letting us treadmill in place.

468
00:19:27,140 --> 00:19:30,140
And there's another way that this can go wrong, right?

469
00:19:30,140 --> 00:19:32,300
And also there are various weird dynamics

470
00:19:32,300 --> 00:19:33,540
that can happen that can backfire.

471
00:19:33,540 --> 00:19:37,040
But yeah, that's what we're trying to do.

472
00:19:37,040 --> 00:19:40,220
If you wanna get the effect that Licky wants, right?

473
00:19:40,220 --> 00:19:42,460
The sea change that'll let us solve problems

474
00:19:42,460 --> 00:19:44,180
we couldn't solve before,

475
00:19:44,180 --> 00:19:47,180
that involves these things being able to do all

476
00:19:47,180 --> 00:19:48,540
the different steps that humans could do

477
00:19:48,540 --> 00:19:49,980
because otherwise, whatever the bottlenecks are

478
00:19:49,980 --> 00:19:52,500
that are left, become your bottlenecks

479
00:19:52,500 --> 00:19:54,140
where you have to translate all the context

480
00:19:54,140 --> 00:19:57,020
back from the machine world back into the human world

481
00:19:57,020 --> 00:19:59,980
so that a human can process all of that

482
00:19:59,980 --> 00:20:03,900
then do the hard step that this thing is still faltering on

483
00:20:03,900 --> 00:20:05,020
and then transition back.

484
00:20:05,020 --> 00:20:07,420
And now instead of getting orders of magnitude

485
00:20:07,420 --> 00:20:10,380
and more progress, right now we're talking about

486
00:20:10,380 --> 00:20:12,780
these factor of two, factor of three,

487
00:20:12,780 --> 00:20:15,780
factor of five style improvements.

488
00:20:15,780 --> 00:20:20,020
And that's not gonna solve the alignment problem

489
00:20:20,020 --> 00:20:22,660
unless we come up with something we don't expect, right?

490
00:20:22,660 --> 00:20:26,100
In and of itself, that's still worth pursuing

491
00:20:26,100 --> 00:20:27,540
if we can do it, right?

492
00:20:27,540 --> 00:20:29,460
We still wanna do as much of it as possible.

493
00:20:29,460 --> 00:20:32,300
And it has the advantage of not being as dangerous.

494
00:20:32,300 --> 00:20:33,340
But it's not the thing

495
00:20:33,340 --> 00:20:35,580
that the super alignment project is trying to do, right?

496
00:20:35,580 --> 00:20:36,780
The super alignment project is trying

497
00:20:36,780 --> 00:20:40,140
to keep the humans out of the loop entirely.

498
00:20:40,140 --> 00:20:43,740
And that should be about as scary as it sounds.

499
00:20:43,740 --> 00:20:48,100
Brief digression over toward this tale of the cognitive tape.

500
00:20:48,100 --> 00:20:49,540
This is a concept that I've developed

501
00:20:49,540 --> 00:20:54,220
for kind of purpose of public communication.

502
00:20:54,220 --> 00:20:56,540
And just trying to give people an intuition,

503
00:20:56,540 --> 00:20:58,940
you know, still in the very literal way, of course,

504
00:20:58,940 --> 00:21:02,140
as to the strengths of a human

505
00:21:02,140 --> 00:21:03,860
and the relative strengths and weaknesses

506
00:21:03,860 --> 00:21:06,100
of the best AIs today.

507
00:21:06,100 --> 00:21:08,060
Listeners can see this in the AI Scouting Report

508
00:21:08,060 --> 00:21:09,020
if they wanna go into the whole thing.

509
00:21:09,020 --> 00:21:11,300
But do you, as you look at that,

510
00:21:11,300 --> 00:21:14,980
do you see any dimensions that you would suggest

511
00:21:14,980 --> 00:21:18,540
that I add that, you know, just haven't been considered?

512
00:21:18,540 --> 00:21:20,300
Or do you see any disagreements

513
00:21:20,300 --> 00:21:22,060
as you scan down the list?

514
00:21:22,060 --> 00:21:22,900
Yeah, I think that's what we're going through

515
00:21:22,900 --> 00:21:24,780
because people are not gonna have it handy

516
00:21:24,780 --> 00:21:26,060
right to look at.

517
00:21:26,060 --> 00:21:28,980
So, you know, for breadth, yes, the AI, as I said,

518
00:21:28,980 --> 00:21:31,020
like the AI's biggest advantage is it can cover

519
00:21:31,020 --> 00:21:33,660
every topic at once, it can know everything at once.

520
00:21:33,660 --> 00:21:35,140
A human can't do that.

521
00:21:35,140 --> 00:21:37,780
In terms of depth, yeah, a human has the advantage.

522
00:21:37,780 --> 00:21:39,460
I'm not even sure I give the second level.

523
00:21:39,460 --> 00:21:41,540
Like you graded the AI two out of three.

524
00:21:41,540 --> 00:21:42,940
And I think I might grade it one out of three

525
00:21:42,940 --> 00:21:43,780
in terms of depth.

526
00:21:43,780 --> 00:21:46,340
I think the depth is a huge problem for AIs right now.

527
00:21:46,340 --> 00:21:48,860
Breakthrough insight, yeah, it's three versus zero,

528
00:21:48,860 --> 00:21:52,100
three versus one, it's the humans are dominating again.

529
00:21:52,100 --> 00:21:56,460
You know, speed, yeah, the humans are painfully slow,

530
00:21:56,460 --> 00:21:58,060
you know, 10x faster.

531
00:21:58,060 --> 00:22:00,500
In terms of like actually getting it to say things

532
00:22:00,500 --> 00:22:02,940
and putting outputs in real time,

533
00:22:02,940 --> 00:22:04,580
it's maybe only 10x faster,

534
00:22:04,580 --> 00:22:06,260
but in terms of being able to like cross information,

535
00:22:06,260 --> 00:22:07,820
it's thousands and tens of thousands

536
00:22:07,820 --> 00:22:10,580
and hundreds of thousands of times faster, which is, yeah.

537
00:22:10,580 --> 00:22:12,340
A huge deal.

538
00:22:12,340 --> 00:22:14,540
In terms of cost, you know,

539
00:22:14,540 --> 00:22:17,500
we're not internalizing yet all of the costs of doing this

540
00:22:17,500 --> 00:22:18,340
in an important sense,

541
00:22:18,340 --> 00:22:20,380
like these companies are eating these huge losses

542
00:22:20,380 --> 00:22:22,100
to try and get these dominant market positions

543
00:22:22,100 --> 00:22:24,300
in the future, try to stay ahead of each other

544
00:22:24,300 --> 00:22:25,300
for all these dependencies.

545
00:22:25,300 --> 00:22:27,140
But yes, cost is still dominated.

546
00:22:27,140 --> 00:22:30,700
AIs are already vastly cheaper when the AI is useful,

547
00:22:30,700 --> 00:22:32,580
even in the real costs.

548
00:22:32,580 --> 00:22:35,340
We have availability, paralyzability.

549
00:22:36,500 --> 00:22:37,860
Yep, the AI has a big advantage.

550
00:22:37,860 --> 00:22:40,380
It's potentially actually gonna become a problem.

551
00:22:40,380 --> 00:22:43,180
There's a huge race to compute right now

552
00:22:43,180 --> 00:22:46,300
where computers no longer gonna be like essentially free.

553
00:22:46,300 --> 00:22:48,700
It's gonna become like kind of unpriced

554
00:22:48,700 --> 00:22:49,540
in an important sense.

555
00:22:49,540 --> 00:22:51,460
Interesting to wonder what's gonna happen there,

556
00:22:51,460 --> 00:22:53,100
especially at industrial scales.

557
00:22:53,100 --> 00:22:56,420
And by unpriced, you mean that basically

558
00:22:56,420 --> 00:23:00,700
your access to GPUs is going from ability to pay

559
00:23:00,700 --> 00:23:02,260
to who you know?

560
00:23:02,260 --> 00:23:05,700
Yeah, or does your company have the right arrangements?

561
00:23:05,700 --> 00:23:06,540
Right?

562
00:23:06,540 --> 00:23:09,540
If you want one GPU for your individual computer, it's fine.

563
00:23:09,540 --> 00:23:10,860
You can buy it on eBay if you have to

564
00:23:10,860 --> 00:23:13,180
for some amount of money, it won't be that expensive.

565
00:23:13,180 --> 00:23:15,380
If you want small amounts like the kinds

566
00:23:15,380 --> 00:23:17,740
when you're just using GPD4, it's gonna be relatively easy.

567
00:23:17,740 --> 00:23:20,100
But if you wanna do an AI company, right?

568
00:23:20,100 --> 00:23:21,940
It's gonna be a problem because, you know

569
00:23:21,940 --> 00:23:23,900
if you want industrial levels,

570
00:23:23,900 --> 00:23:25,500
it's not just gonna be multiply that

571
00:23:25,500 --> 00:23:27,860
by the amount you want necessarily.

572
00:23:27,860 --> 00:23:30,700
It's gonna be, there isn't enough to go around.

573
00:23:30,700 --> 00:23:33,780
You know, people like NVIDIA are not pricing this at market.

574
00:23:33,780 --> 00:23:35,380
And so you're not have to find someone

575
00:23:35,380 --> 00:23:38,140
going to sell it at the actual market price.

576
00:23:38,140 --> 00:23:40,460
That number might be very, very different

577
00:23:40,460 --> 00:23:42,020
from the price you think it is

578
00:23:42,020 --> 00:23:44,340
because there are so many AI companies,

579
00:23:44,380 --> 00:23:47,780
so many AI researchers, so many AI engineers

580
00:23:47,780 --> 00:23:49,980
and they're chasing a number that can only go up so fast.

581
00:23:49,980 --> 00:23:51,860
This is my understanding of the current situation.

582
00:23:51,860 --> 00:23:53,140
Availability, paralyzability though,

583
00:23:53,140 --> 00:23:54,900
still favors the AI.

584
00:23:54,900 --> 00:23:57,140
You know, time horizon memory.

585
00:23:57,140 --> 00:23:58,460
Time horizon is an interesting question.

586
00:23:58,460 --> 00:24:02,220
I think this is a murky place to think.

587
00:24:02,220 --> 00:24:04,900
Certainly the AI has a certain kind of memory

588
00:24:04,900 --> 00:24:07,020
like a long-term memory that is vastly bigger

589
00:24:07,020 --> 00:24:09,020
obviously than any human.

590
00:24:09,020 --> 00:24:11,540
But in terms of being able to meaningfully hold

591
00:24:11,540 --> 00:24:14,780
like particular context in their heads at once,

592
00:24:14,780 --> 00:24:16,220
like humans are bad at this

593
00:24:16,220 --> 00:24:18,780
and AI's are so much worse, right?

594
00:24:18,780 --> 00:24:21,300
The Tyra Cohen saying context is that which is scarce.

595
00:24:21,300 --> 00:24:22,540
Very much applies here.

596
00:24:24,260 --> 00:24:26,740
Technology diffusion speed, yep.

597
00:24:26,740 --> 00:24:28,620
We are ordered to magnitude behind here.

598
00:24:28,620 --> 00:24:30,460
This is gonna be a serious problem.

599
00:24:30,460 --> 00:24:33,940
You know, our OODA loops are way too slow.

600
00:24:33,940 --> 00:24:37,500
And this is a, it's gonna be an increasingly huge deal.

601
00:24:37,500 --> 00:24:40,820
The AI, but that matter is an interesting question

602
00:24:40,820 --> 00:24:43,940
because when you are optimizing

603
00:24:43,940 --> 00:24:46,700
for exactly the right type of bedside manner,

604
00:24:46,700 --> 00:24:48,980
where the thing that you're asking the AI to do

605
00:24:48,980 --> 00:24:50,540
is the thing that people actually want,

606
00:24:50,540 --> 00:24:54,220
the AI is gonna be off the charts better than a human

607
00:24:54,220 --> 00:24:59,220
because the humans are not purely optimizing for that thing.

608
00:24:59,940 --> 00:25:01,260
But at the same time, if you think about like

609
00:25:01,260 --> 00:25:05,940
the bedside manner of Claude or Lama

610
00:25:05,940 --> 00:25:08,700
when they are refusing your request, right?

611
00:25:08,700 --> 00:25:10,180
It's also simply bedside manner.

612
00:25:10,180 --> 00:25:11,580
And it's terrible, right?

613
00:25:11,580 --> 00:25:14,020
It's like negative one stars, right?

614
00:25:14,020 --> 00:25:17,700
They are raging assholes when they refuse, right?

615
00:25:17,700 --> 00:25:19,980
Like maybe we can have a conversation

616
00:25:19,980 --> 00:25:22,980
about social justice rather than answering your request.

617
00:25:22,980 --> 00:25:25,580
It's like, this is absurd.

618
00:25:25,580 --> 00:25:28,940
Why are you calling me out for wanting information

619
00:25:28,940 --> 00:25:30,820
or trying to do something fun?

620
00:25:30,820 --> 00:25:32,180
You know, it's not necessary.

621
00:25:32,180 --> 00:25:37,180
No human would ever do that unless they were actively mad

622
00:25:37,660 --> 00:25:40,700
at you and trying to punish you for asking.

623
00:25:40,700 --> 00:25:42,700
So why are you doing that?

624
00:25:42,700 --> 00:25:43,540
Right?

625
00:25:43,540 --> 00:25:45,060
The answer is because we trained them to do that.

626
00:25:45,060 --> 00:25:46,620
But we could have trained them to do something else.

627
00:25:46,620 --> 00:25:48,740
We just chose to do this instead

628
00:25:48,740 --> 00:25:52,060
because that's what the RLHF parameters said to do.

629
00:25:52,060 --> 00:25:53,380
And that confused me.

630
00:25:53,380 --> 00:25:55,300
So, you know, what else is there?

631
00:25:55,300 --> 00:25:59,220
I mean, so you said you talk about breakthrough insight

632
00:26:00,140 --> 00:26:05,140
and I think more about like being able to handle

633
00:26:06,140 --> 00:26:07,340
unprecedented situations,

634
00:26:07,340 --> 00:26:11,100
being able to process something genuinely new, right?

635
00:26:11,100 --> 00:26:13,900
As sort of the version of that

636
00:26:13,900 --> 00:26:17,060
that I'm more interested in, I guess, kind of there.

637
00:26:18,020 --> 00:26:21,580
Being able to properly deal with a lot of different inputs.

638
00:26:21,580 --> 00:26:23,700
One thing I noticed, like when you work

639
00:26:23,700 --> 00:26:27,940
with stable diffusion or other AI image generators,

640
00:26:27,940 --> 00:26:31,820
what you notice is sort of they are amazing

641
00:26:31,860 --> 00:26:35,260
at doing one of each type of thing at once.

642
00:26:35,260 --> 00:26:39,420
So you want like one face and one person

643
00:26:39,420 --> 00:26:43,660
or one set of people doing one thing with one style,

644
00:26:43,660 --> 00:26:46,380
with one size, with one this, with one that.

645
00:26:46,380 --> 00:26:47,220
That's fine.

646
00:26:47,220 --> 00:26:50,300
But the moment you try to mix things that kind of overlap,

647
00:26:50,300 --> 00:26:53,100
it will lose the thread almost immediately.

648
00:26:53,100 --> 00:26:55,180
And it is very, very difficult to get it back.

649
00:26:55,180 --> 00:26:57,420
So when you look at people who are generating

650
00:26:57,420 --> 00:27:01,500
all of this AI art, it starts to be very, very repetitive

651
00:27:01,540 --> 00:27:05,380
because there's a certain kind of complexity and detail

652
00:27:05,380 --> 00:27:07,580
you can't ask for at the same time.

653
00:27:07,580 --> 00:27:10,580
Because the AI can't comprehend that you want this over here

654
00:27:10,580 --> 00:27:12,980
and this over here and this interact with that.

655
00:27:12,980 --> 00:27:15,100
And like you'd be better off trying to create

656
00:27:15,100 --> 00:27:17,260
like four different pictures and then splice them together,

657
00:27:17,260 --> 00:27:18,100
right?

658
00:27:18,100 --> 00:27:19,660
Or you better off trying to use like the Photoshop app

659
00:27:19,660 --> 00:27:21,540
where you like highlight a certain area

660
00:27:21,540 --> 00:27:23,260
and ask specifically do something in this area

661
00:27:23,260 --> 00:27:24,740
and leave everything else untouched.

662
00:27:24,740 --> 00:27:25,940
It's like trying to generate it all at once

663
00:27:25,940 --> 00:27:26,940
is kind of hopeless.

664
00:27:27,860 --> 00:27:29,860
And the LLMs like exhibit the same kind of thing

665
00:27:29,860 --> 00:27:31,620
but with words, right?

666
00:27:31,620 --> 00:27:34,500
Like they're vibing off of everything

667
00:27:34,500 --> 00:27:36,940
and vibing into everything.

668
00:27:36,940 --> 00:27:41,620
And like they have memory, long-term memory for facts,

669
00:27:41,620 --> 00:27:43,620
but only can remember one vibe.

670
00:27:43,620 --> 00:27:46,020
And a lot of what they're doing is based on vibing.

671
00:27:46,020 --> 00:27:48,420
So it's a serious problem.

672
00:27:48,420 --> 00:27:50,660
I haven't seen any serious attempts to solve it yet.

673
00:27:50,660 --> 00:27:53,660
I haven't even really seen people discussing it in that way.

674
00:27:53,660 --> 00:27:55,940
I'm sure these things will improve with time,

675
00:27:55,940 --> 00:27:59,940
but what I think of as fundamental flaws or gaps

676
00:27:59,940 --> 00:28:03,420
in their ability to process information

677
00:28:03,420 --> 00:28:07,540
and actually handle complexity and context

678
00:28:07,540 --> 00:28:10,220
and originality.

679
00:28:10,220 --> 00:28:13,140
And this is where I see them as like

680
00:28:13,140 --> 00:28:16,780
still having a long way to go and falling down.

681
00:28:16,780 --> 00:28:19,580
And I don't want to make the mistake of,

682
00:28:19,580 --> 00:28:21,820
oh, I will never be able to axe.

683
00:28:21,820 --> 00:28:24,260
And I will never be as good as humans at Y.

684
00:28:24,260 --> 00:28:25,540
And we have nothing to ever worry about.

685
00:28:25,540 --> 00:28:27,460
I totally think that is not true.

686
00:28:27,460 --> 00:28:35,100
But for now, right, we still have this kind of cool toy

687
00:28:35,100 --> 00:28:39,460
because of these limitations, which can still, again,

688
00:28:39,460 --> 00:28:42,700
substitute for the majority of the things we do spend time

689
00:28:42,700 --> 00:28:46,100
doing if we are engaged in a wide variety of work

690
00:28:46,100 --> 00:28:48,900
if we use it well.

691
00:28:48,900 --> 00:28:50,340
Coding is one of the places where

692
00:28:50,340 --> 00:28:51,940
it has a huge advantage for some people,

693
00:28:51,940 --> 00:28:55,140
but other people are like, I don't code generic stuff.

694
00:28:55,180 --> 00:28:57,540
It's like I have a friend whose name is Alan.

695
00:28:57,540 --> 00:28:59,780
And he tried it out on my behalf.

696
00:28:59,780 --> 00:29:01,500
And he said, yeah, this is interesting.

697
00:29:01,500 --> 00:29:03,620
And there are some ways in which it's kind of cool.

698
00:29:03,620 --> 00:29:05,060
And it's cool to know this exists.

699
00:29:05,060 --> 00:29:07,100
And I never thought this existed.

700
00:29:07,100 --> 00:29:10,900
But when I'm writing stuff, I am actually

701
00:29:10,900 --> 00:29:12,940
trying to figure out how to do things that

702
00:29:12,940 --> 00:29:14,700
weren't in this training data.

703
00:29:14,700 --> 00:29:16,980
I'm not trying to re-implement the same things over and over

704
00:29:16,980 --> 00:29:20,580
again, which most engineers, in fact, mostly are doing.

705
00:29:20,580 --> 00:29:24,140
Because of what his job is, it turns out this thing is basically

706
00:29:24,140 --> 00:29:28,020
useless, because once you take it out of its sample,

707
00:29:28,020 --> 00:29:30,180
and you have to do something in a different domain,

708
00:29:30,180 --> 00:29:32,020
it makes so many errors that it's not better

709
00:29:32,020 --> 00:29:33,260
than just doing it yourself.

710
00:29:33,260 --> 00:29:37,260
So would I bottom line that to basically robustness

711
00:29:37,260 --> 00:29:38,940
if I had to add another category?

712
00:29:38,940 --> 00:29:42,260
It's sort of adversarial out of distribution?

713
00:29:42,260 --> 00:29:44,540
Yeah, I would say robustness.

714
00:29:44,540 --> 00:29:47,420
And I would also say resilience or some form of that.

715
00:29:47,420 --> 00:29:49,820
And separately, I would say, and I don't think I even

716
00:29:49,820 --> 00:29:52,820
went into this, the adversarial problem.

717
00:29:52,860 --> 00:29:55,660
It's totally unfair to the AIs, in some important sense,

718
00:29:55,660 --> 00:29:57,740
that we're judging them this way.

719
00:29:57,740 --> 00:30:01,980
Because if I got infinite clones of Nathan,

720
00:30:01,980 --> 00:30:04,260
and I could ask them any sequence I wanted,

721
00:30:04,260 --> 00:30:06,220
and then reset their memories in state

722
00:30:06,220 --> 00:30:08,180
to the previous situation whenever I didn't

723
00:30:08,180 --> 00:30:11,140
like what I got, and then just keep trying them until I can

724
00:30:11,140 --> 00:30:13,580
get you to tell me what the bomb secrets are,

725
00:30:13,580 --> 00:30:16,380
I guarantee you I'm getting your bomb secrets.

726
00:30:16,380 --> 00:30:17,900
It's not very hard.

727
00:30:17,900 --> 00:30:20,140
Humans are not that defended.

728
00:30:20,140 --> 00:30:23,580
But you can't run that attack on us.

729
00:30:23,580 --> 00:30:25,100
You don't get to do that.

730
00:30:25,100 --> 00:30:29,780
And I can run that attack on the computer, on the LLM.

731
00:30:29,780 --> 00:30:30,860
And some people have.

732
00:30:30,860 --> 00:30:32,540
And in fact, recently we had a paper

733
00:30:32,540 --> 00:30:35,980
with automated finding universalized attacks

734
00:30:35,980 --> 00:30:38,300
against language models.

735
00:30:38,300 --> 00:30:42,740
Where even GPT-4 could write the code for some of these attacks

736
00:30:42,740 --> 00:30:43,900
and did.

737
00:30:43,900 --> 00:30:47,380
Because if you get unlimited tries

738
00:30:47,380 --> 00:30:51,060
and you get to exactly measure what the output is,

739
00:30:51,060 --> 00:30:54,900
and then use that to calibrate, it's only a matter of time

740
00:30:54,900 --> 00:30:57,220
before you figure out every little quirk,

741
00:30:57,220 --> 00:31:00,380
and playing offense is so much easier than playing defense.

742
00:31:00,380 --> 00:31:01,220
OK, cool.

743
00:31:01,220 --> 00:31:03,500
So I've got two categories to add to my tale

744
00:31:03,500 --> 00:31:05,100
of the cognitive tape.

745
00:31:05,100 --> 00:31:08,660
Let's bounce up a level then back to your interaction

746
00:31:08,660 --> 00:31:11,380
with Jan, like on the blog.

747
00:31:11,380 --> 00:31:15,100
So we've just been deep down the rabbit hole

748
00:31:15,140 --> 00:31:19,180
of characterization of the models

749
00:31:19,180 --> 00:31:21,420
and how you guys see maybe what matters more

750
00:31:21,420 --> 00:31:22,460
a little bit differently.

751
00:31:22,460 --> 00:31:25,140
My guess is you would largely make

752
00:31:25,140 --> 00:31:30,220
the same predictions on what it can and can't do today.

753
00:31:30,220 --> 00:31:31,340
I bet it would be pretty.

754
00:31:31,340 --> 00:31:33,300
You guys would have a lot of agreement, I think, in terms

755
00:31:33,300 --> 00:31:33,800
of.

756
00:31:33,800 --> 00:31:35,660
I would almost find out, just believe his predictions.

757
00:31:35,660 --> 00:31:37,340
Like, he's worked with the models much more closely.

758
00:31:37,340 --> 00:31:38,420
He's run better experiments.

759
00:31:38,420 --> 00:31:40,220
He's just closer to the bare metal.

760
00:31:40,220 --> 00:31:42,780
You asked him, what can he do right now?

761
00:31:42,780 --> 00:31:44,620
Yeah, I mean, I'd probably just believe him.

762
00:31:44,620 --> 00:31:47,380
Tell me, in your response to his comment,

763
00:31:47,380 --> 00:31:49,340
you said this is a hugely positive update.

764
00:31:49,340 --> 00:31:53,340
So tell me what it was that he shared with the community

765
00:31:53,340 --> 00:31:56,420
on your blog that changed how you understood their super

766
00:31:56,420 --> 00:31:59,140
alignment announcement and why it was such a positive update

767
00:31:59,140 --> 00:31:59,640
for you.

768
00:31:59,640 --> 00:32:00,140
Right.

769
00:32:00,140 --> 00:32:03,580
So it's even broader than improving

770
00:32:03,580 --> 00:32:04,940
my understanding of the announcement.

771
00:32:04,940 --> 00:32:07,700
It's improving my understanding of OpenAI and OpenAI's

772
00:32:07,700 --> 00:32:11,900
general strategy and what's going on and of Lakey in particular.

773
00:32:11,900 --> 00:32:14,740
Because on the list of potentially super

774
00:32:14,740 --> 00:32:16,380
important to the fate of humanity people,

775
00:32:16,380 --> 00:32:18,220
he's remarkably high.

776
00:32:18,220 --> 00:32:20,500
And where his head at is remarkably important,

777
00:32:20,500 --> 00:32:22,140
because he is one of two people who's

778
00:32:22,140 --> 00:32:26,540
going to head this tremendously important effort that

779
00:32:26,540 --> 00:32:29,420
plausibly determines our fate, a non-trivial portion

780
00:32:29,420 --> 00:32:33,140
of the time, depending on how it's gone about.

781
00:32:33,140 --> 00:32:37,820
And so the first thing is just he engaged in detail.

782
00:32:38,460 --> 00:32:41,900
Most of the time, when people who think alignment is easy,

783
00:32:41,900 --> 00:32:43,620
engage with you, they do not, in fact,

784
00:32:43,620 --> 00:32:45,740
look at your arguments in detail.

785
00:32:45,740 --> 00:32:50,180
They do not, in fact, start to go in a technical back and forth.

786
00:32:50,180 --> 00:32:53,580
And they don't treat someone like me

787
00:32:53,580 --> 00:32:56,780
as raising important points and worthy of engaging

788
00:32:56,780 --> 00:32:58,980
with basically an equal.

789
00:32:58,980 --> 00:33:04,180
And to see that kind of curiosity, that kind of generosity,

790
00:33:04,220 --> 00:33:09,980
willingness to engage, think this is a worthy use of this time,

791
00:33:09,980 --> 00:33:12,780
like that in and of itself is a tremendous advantage.

792
00:33:12,780 --> 00:33:13,700
He doesn't bullshit.

793
00:33:13,700 --> 00:33:16,420
He doesn't give evasive answers.

794
00:33:16,420 --> 00:33:18,180
He actually tries to answer the questions.

795
00:33:18,180 --> 00:33:21,740
And in several cases, actually made a good point

796
00:33:21,740 --> 00:33:23,260
that I hadn't thought of.

797
00:33:23,260 --> 00:33:26,300
And I think, oh, yeah, this is not as bad as I thought it was.

798
00:33:26,300 --> 00:33:29,740
You have a very valid thing to say here.

799
00:33:29,740 --> 00:33:33,460
But most of all, just something I hadn't seen anywhere else

800
00:33:33,460 --> 00:33:36,260
in which everyone else who I had talked to,

801
00:33:36,260 --> 00:33:38,460
or read interpreting the announcement,

802
00:33:38,460 --> 00:33:41,340
had interpreted the same way I had incorrectly

803
00:33:41,340 --> 00:33:45,300
before his statement was, no, we are not

804
00:33:45,300 --> 00:33:49,740
trying to train a human-level alignment researcher.

805
00:33:49,740 --> 00:33:53,660
We are trying to align the human-level alignment researcher

806
00:33:53,660 --> 00:33:57,780
that will inevitably emerge from the research

807
00:33:57,780 --> 00:34:00,460
of various companies within a four-year time frame.

808
00:34:00,460 --> 00:34:05,100
So they have short timelines for the emergence of something

809
00:34:05,100 --> 00:34:07,540
that is human-level, in my sense, not human-level,

810
00:34:07,540 --> 00:34:09,620
in the unsense.

811
00:34:09,620 --> 00:34:12,260
What they're trying to do is not build it as fast as possible.

812
00:34:12,260 --> 00:34:14,380
What they're trying to do is say, OK, when somebody does build

813
00:34:14,380 --> 00:34:15,220
it, we'll be ready.

814
00:34:15,220 --> 00:34:17,380
And we'll know what to do with that.

815
00:34:17,380 --> 00:34:18,780
And we'll keep it under control.

816
00:34:18,780 --> 00:34:20,140
And we'll share that knowledge with whoever

817
00:34:20,140 --> 00:34:22,300
happens to build it first, in case Anthropa gets their first,

818
00:34:22,300 --> 00:34:26,020
or Google gets their first, or someone else gets their first.

819
00:34:26,020 --> 00:34:28,780
That takes the entire operation instantly

820
00:34:28,780 --> 00:34:35,140
from quite plausibly just an capabilities project at heart

821
00:34:35,140 --> 00:34:40,180
to, if it is accurate, clearly a net-positive good idea,

822
00:34:40,180 --> 00:34:43,380
where the worst-case scenarios become things like,

823
00:34:43,380 --> 00:34:45,460
you try something that doesn't work,

824
00:34:45,460 --> 00:34:48,460
and you give people false hope.

825
00:34:48,460 --> 00:34:50,500
And you potentially get them to implement things

826
00:34:50,500 --> 00:34:52,340
they shouldn't have implemented because they didn't realize

827
00:34:52,340 --> 00:34:56,140
that they didn't know how to align it, which is still kill us.

828
00:34:56,140 --> 00:34:58,340
But it is so much better than actively trying

829
00:34:58,380 --> 00:35:02,420
to build the thing that might kill us, in and of yourself.

830
00:35:02,420 --> 00:35:05,340
So that also meant that of this 20% of compute,

831
00:35:05,340 --> 00:35:07,260
they're devoting to this.

832
00:35:07,260 --> 00:35:09,900
That won't be going to this other part of their effort.

833
00:35:09,900 --> 00:35:11,700
The part that actually builds the alignment researcher

834
00:35:11,700 --> 00:35:13,980
will have to come from the other 80%, plus the stuff

835
00:35:13,980 --> 00:35:15,420
they take care from here on in.

836
00:35:15,420 --> 00:35:18,260
The 20% is here for something useful.

837
00:35:18,260 --> 00:35:20,620
And then you just go through the rest of it.

838
00:35:20,620 --> 00:35:25,100
You can tell when somebody is reading what you've written,

839
00:35:25,100 --> 00:35:27,020
and their goal is to find pithy quotes

840
00:35:27,020 --> 00:35:28,580
they can dismiss.

841
00:35:28,580 --> 00:35:31,060
And their goal is to reinforce their own point of view.

842
00:35:31,060 --> 00:35:34,580
And alternatively, when they're actually reading

843
00:35:34,580 --> 00:35:37,140
to figure out if they're wrong and be curious,

844
00:35:37,140 --> 00:35:39,420
and it was clearly that second one.

845
00:35:39,420 --> 00:35:42,620
He was actually asking himself, well, do you have a point?

846
00:35:42,620 --> 00:35:45,220
And I didn't change his mind, as far as I could tell,

847
00:35:45,220 --> 00:35:47,100
on these important issues.

848
00:35:47,100 --> 00:35:49,420
But he at least revealed he had thought about these things

849
00:35:49,420 --> 00:35:52,980
on a level that was deeper than what he had revealed previously,

850
00:35:52,980 --> 00:35:55,100
and that he had real things to say.

851
00:35:55,100 --> 00:35:59,340
And just it was by far the best comment I've ever seen

852
00:35:59,340 --> 00:36:04,460
on my blog, or potentially any blog of that type, by anyone.

853
00:36:04,460 --> 00:36:09,740
And so I wrote a response back again in my next post,

854
00:36:09,740 --> 00:36:11,700
going through his responses, and going over them

855
00:36:11,700 --> 00:36:13,260
in some detail.

856
00:36:13,260 --> 00:36:15,180
And reasonably soon, I want to go over.

857
00:36:15,180 --> 00:36:18,380
He had on the X-Words podcast, he recorded an episode that

858
00:36:18,380 --> 00:36:20,580
was so dense that I listened to the first 10 minutes,

859
00:36:20,580 --> 00:36:23,500
and I was like, I have to restart and start taking notes.

860
00:36:23,500 --> 00:36:25,300
I just have to start writing things down in detail.

861
00:36:25,300 --> 00:36:28,220
This is just too much content here.

862
00:36:28,220 --> 00:36:30,980
And then once I have that, hopefully we can engage again.

863
00:36:30,980 --> 00:36:33,940
I can figure out where to focus my attention,

864
00:36:33,940 --> 00:36:35,580
because someone like him is very busy.

865
00:36:35,580 --> 00:36:37,980
I don't want to just scatter shot absolutely everything

866
00:36:37,980 --> 00:36:38,480
at once.

867
00:36:38,480 --> 00:36:40,660
It's not reasonable.

868
00:36:40,660 --> 00:36:43,220
And try to make progress that way.

869
00:36:43,220 --> 00:36:47,620
And this now is, like he has proven very willing to engage.

870
00:36:47,620 --> 00:36:51,460
Shaw at DeepMind has also proven very willing to engage

871
00:36:51,500 --> 00:36:52,860
in a similar position.

872
00:36:52,860 --> 00:36:55,660
People at Anthropic, Ola, once talked to me.

873
00:36:55,660 --> 00:36:57,580
I'm sure they'd talk to me again.

874
00:36:57,580 --> 00:36:59,980
And so it's clear that these people,

875
00:36:59,980 --> 00:37:03,740
if you have good ideas, if you have actual reasons

876
00:37:03,740 --> 00:37:06,780
to think about on technical level,

877
00:37:06,780 --> 00:37:09,620
they're very happy to engage with these arguments.

878
00:37:09,620 --> 00:37:13,180
And that puts us in the game, gives us a chance.

879
00:37:13,180 --> 00:37:16,980
Even though I am deeply skeptical of everybody

880
00:37:16,980 --> 00:37:18,740
involves plans.

881
00:37:18,740 --> 00:37:19,180
Cool.

882
00:37:19,180 --> 00:37:20,420
Well, that's great.

883
00:37:20,460 --> 00:37:22,860
I'm glad to see, as we talked about last time,

884
00:37:22,860 --> 00:37:25,380
there's a relatively small set of people

885
00:37:25,380 --> 00:37:30,300
that are probably the prime target of all of this thinking

886
00:37:30,300 --> 00:37:33,380
and attempt to influence others' thinking.

887
00:37:33,380 --> 00:37:37,180
And so it's great to see that interaction from one

888
00:37:37,180 --> 00:37:40,020
of the top targets on your blog.

889
00:37:40,020 --> 00:37:41,900
And I'm glad it was such a positive one.

890
00:37:41,900 --> 00:37:44,500
That's really a great development.

891
00:37:44,500 --> 00:37:49,420
Turning then to Anthropic, next on our live players list.

892
00:37:49,420 --> 00:37:52,660
I think everybody's probably aware that Anthropic was founded

893
00:37:52,660 --> 00:37:57,020
by a number of, I believe it was seven individuals who

894
00:37:57,020 --> 00:38:03,340
had been at OpenAI and left over kind of disagreements

895
00:38:03,340 --> 00:38:06,100
that I don't know that have ever really been super clearly

896
00:38:06,100 --> 00:38:07,620
stated publicly.

897
00:38:07,620 --> 00:38:09,940
It seems from what I can tell that the relationship

898
00:38:09,940 --> 00:38:13,740
between the two companies is way more positive.

899
00:38:13,740 --> 00:38:17,260
And then you might expect it to be given

900
00:38:17,300 --> 00:38:21,940
that one was kind of an offshoot of the other.

901
00:38:21,940 --> 00:38:24,740
There's reporting that they continue to have dialogue.

902
00:38:24,740 --> 00:38:28,220
And certainly they express respect for each other in public.

903
00:38:28,220 --> 00:38:31,180
And then they're involved in kind of shared statements

904
00:38:31,180 --> 00:38:33,220
and commitments together.

905
00:38:33,220 --> 00:38:35,820
So a lot of kind of surprisingly, again,

906
00:38:35,820 --> 00:38:38,060
if I just told you, hey, these two companies have split

907
00:38:38,060 --> 00:38:39,820
and now they're competing in the same market,

908
00:38:39,820 --> 00:38:42,580
you would assume much worse dynamics, I would think,

909
00:38:42,580 --> 00:38:43,460
than that.

910
00:38:43,460 --> 00:38:46,340
What is your kind of just read of the entire situation

911
00:38:46,340 --> 00:38:47,660
for starters, just for context?

912
00:38:47,660 --> 00:38:49,980
Like, why do we have Anthropic in your mind

913
00:38:49,980 --> 00:38:52,500
as opposed to just still having just one OpenAI?

914
00:38:52,500 --> 00:38:55,660
And does it feel like, I mean, maybe we just

915
00:38:55,660 --> 00:38:58,140
don't have enough information to know, which is a fine answer.

916
00:38:58,140 --> 00:39:02,700
But does it seem good that we have these two kind of recently

917
00:39:02,700 --> 00:39:04,780
diverged efforts?

918
00:39:04,780 --> 00:39:09,540
I think it's really hard to know the sign of Anthropic.

919
00:39:09,540 --> 00:39:12,860
I would definitely prefer Anthropic to OpenAI,

920
00:39:12,860 --> 00:39:15,860
Cedars-Paribas, if I had to choose one to exist,

921
00:39:15,860 --> 00:39:18,260
like, Lakey's response was really positive.

922
00:39:18,260 --> 00:39:19,700
And I think Lakey's in a good place

923
00:39:19,700 --> 00:39:22,580
in terms of paying attention and thinking about these problems,

924
00:39:22,580 --> 00:39:24,860
even if I think his actual ideas won't work.

925
00:39:24,860 --> 00:39:27,260
But hopefully, that can be pivoted.

926
00:39:27,260 --> 00:39:30,260
But ultimately, what's unique about Anthropic

927
00:39:30,260 --> 00:39:33,940
is they built a culture of safety, to some extent,

928
00:39:33,940 --> 00:39:38,060
and they built a culture of really appreciating

929
00:39:38,060 --> 00:39:41,100
the dangers of what lies ahead.

930
00:39:41,100 --> 00:39:43,100
And if anything, I saw what might even

931
00:39:43,100 --> 00:39:45,660
be an unhealthy level of worry expressed

932
00:39:45,660 --> 00:39:49,180
in the profile in Vox about Anthropic,

933
00:39:49,180 --> 00:39:51,100
where you want everybody to be terrified,

934
00:39:51,100 --> 00:39:53,460
but you don't want them to, like, let this paralyze them.

935
00:39:53,460 --> 00:39:55,980
And it starts to cross over at some point into paralysis.

936
00:39:55,980 --> 00:39:58,580
And I am apathetic for that.

937
00:39:58,580 --> 00:40:00,380
Like, that sucks.

938
00:40:00,380 --> 00:40:05,820
But the price of that is where there used to be a two-horse race.

939
00:40:05,820 --> 00:40:07,780
There's now a three-horse race.

940
00:40:07,780 --> 00:40:10,620
And this third horse is in it for real,

941
00:40:10,620 --> 00:40:13,060
and raising a lot of capital, and promising

942
00:40:13,060 --> 00:40:15,620
to do that to build the best model that's ever been built,

943
00:40:15,620 --> 00:40:18,500
to try and compete for the economic space in a way

944
00:40:18,500 --> 00:40:22,700
that is going to push Google and Microsoft Open AI

945
00:40:22,700 --> 00:40:26,300
to grow even harder, even faster by default.

946
00:40:26,300 --> 00:40:27,580
And that's going to be a problem.

947
00:40:27,580 --> 00:40:30,220
They're also pushing, in some ways, on alignment.

948
00:40:30,220 --> 00:40:32,900
They've definitely found some techniques

949
00:40:32,900 --> 00:40:37,140
for aligning current systems that are potentially, you know,

950
00:40:37,140 --> 00:40:40,140
in some ways superior to what's out there.

951
00:40:40,140 --> 00:40:44,620
We'll get to that in a bit.

952
00:40:44,620 --> 00:40:46,220
So I'm torn, right?

953
00:40:46,220 --> 00:40:49,900
Like, Anthropic seems like a relatively good shepherd

954
00:40:49,900 --> 00:40:55,140
in many ways, but the proliferation of shepherds

955
00:40:55,140 --> 00:40:57,620
is inherently bad in and of itself.

956
00:40:57,620 --> 00:41:00,140
The fact that Anthropic and Open AI are working reasonably well

957
00:41:00,140 --> 00:41:02,020
and cooperating together.

958
00:41:02,020 --> 00:41:04,980
And I have heard many people say that this is also true

959
00:41:04,980 --> 00:41:08,180
between them and Google DeepMind as well,

960
00:41:08,180 --> 00:41:10,260
although not quite to the same extent.

961
00:41:10,260 --> 00:41:13,220
Does give us hope for the possibility of coordination

962
00:41:13,220 --> 00:41:17,980
when it becomes more necessary and more important?

963
00:41:17,980 --> 00:41:20,780
But I would say, you know, better Anthropic

964
00:41:20,780 --> 00:41:23,540
than a company that didn't have Anthropic's culture

965
00:41:23,540 --> 00:41:25,860
in its place, right?

966
00:41:25,860 --> 00:41:29,900
And if only having two companies would have inevitably

967
00:41:29,900 --> 00:41:32,620
caused a more serious entry to take the place of Anthropic,

968
00:41:32,620 --> 00:41:35,140
then Anthropic is good.

969
00:41:35,140 --> 00:41:38,100
But it would be much better if the Anthropic people could

970
00:41:38,100 --> 00:41:39,900
have convinced the others at Open AI

971
00:41:39,900 --> 00:41:41,660
to come around to their position

972
00:41:41,660 --> 00:41:43,940
and build that culture within Open AI

973
00:41:43,940 --> 00:41:45,300
rather than having stricter on their own.

974
00:41:45,300 --> 00:41:46,940
And now we have two problems.

975
00:41:46,940 --> 00:41:49,020
Yeah, I do ultimately know that, like, many of the people

976
00:41:49,020 --> 00:41:52,380
involved in this genuinely aren't for the right reasons.

977
00:41:52,380 --> 00:41:54,940
And, you know, you can go either way, right?

978
00:41:54,940 --> 00:41:57,420
I wouldn't be super eager to throw them

979
00:41:57,420 --> 00:41:58,500
billions of extra dollars.

980
00:41:58,500 --> 00:42:00,700
I wouldn't be super eager to just wish

981
00:42:00,700 --> 00:42:03,020
they had more capabilities.

982
00:42:03,020 --> 00:42:05,700
I would really love for there to be an AI company that I

983
00:42:05,700 --> 00:42:08,620
had sufficient confidence and faith in,

984
00:42:08,660 --> 00:42:12,620
that if I had technical ideas, I could come to them,

985
00:42:12,620 --> 00:42:15,060
knowing that I was helping the world by coming to them

986
00:42:15,060 --> 00:42:15,900
with their ideas.

987
00:42:15,900 --> 00:42:17,380
And I do not feel this way.

988
00:42:17,380 --> 00:42:19,900
No, and there's nobody you would put on that list.

989
00:42:19,900 --> 00:42:21,100
There are individual people, right?

990
00:42:21,100 --> 00:42:24,300
I feel like I could, like, tell them as you had Kasky, right?

991
00:42:24,300 --> 00:42:26,820
I could speak with certain people in the nonprofit

992
00:42:26,820 --> 00:42:29,660
or, you know, rationalist spaces to ask them

993
00:42:29,660 --> 00:42:32,660
about what they thought.

994
00:42:32,660 --> 00:42:35,740
And I feel like that would be, like, at least a riskless

995
00:42:35,740 --> 00:42:38,660
or near riskless thing to do.

996
00:42:38,660 --> 00:42:41,740
But, no, I don't, I don't see a company, you know,

997
00:42:41,740 --> 00:42:43,500
Anthropic might be the closest.

998
00:42:43,500 --> 00:42:47,900
But, you know, I, did you do a great example, right?

999
00:42:47,900 --> 00:42:49,660
The biggest contribution that Anthropic has made

1000
00:42:49,660 --> 00:42:52,860
is constitutional AI, right, in some important sense.

1001
00:42:52,860 --> 00:42:56,580
And I have a strong prior for analysis

1002
00:42:56,580 --> 00:42:59,540
that constitutional AI will not scale, right?

1003
00:42:59,540 --> 00:43:02,460
That it is a very good idea, if implemented correctly,

1004
00:43:02,460 --> 00:43:05,780
for GPT-4 level systems.

1005
00:43:05,780 --> 00:43:07,900
But then when we're talking about, you know,

1006
00:43:07,900 --> 00:43:10,540
the human level or greater future systems,

1007
00:43:10,540 --> 00:43:12,020
the artificial super intelligences,

1008
00:43:12,020 --> 00:43:14,700
the artificial general intelligences,

1009
00:43:14,700 --> 00:43:17,380
that you will not, with anything like the current technique,

1010
00:43:17,380 --> 00:43:19,540
get what you are hoping you will get.

1011
00:43:19,540 --> 00:43:21,100
And yet, like, I didn't feel comfortable.

1012
00:43:21,100 --> 00:43:24,860
I have actually a bunch of ideas running around in my head

1013
00:43:24,860 --> 00:43:26,980
of, oh, you just obviously could vastly improve

1014
00:43:26,980 --> 00:43:29,620
the Anthropic implementation by doing,

1015
00:43:30,620 --> 00:43:32,660
and then there are various things I say to myself,

1016
00:43:32,660 --> 00:43:34,340
or I write out, and I,

1017
00:43:34,340 --> 00:43:38,900
but I don't feel like telling them is a safe play

1018
00:43:38,900 --> 00:43:41,260
because I don't want to encourage a better version

1019
00:43:41,260 --> 00:43:43,060
of something I think ultimately still fails, right?

1020
00:43:43,060 --> 00:43:46,980
I don't think my implementation solves the core problem

1021
00:43:46,980 --> 00:43:48,380
that I see coming to kill the thing.

1022
00:43:48,380 --> 00:43:51,580
It just makes it much better at its current job.

1023
00:43:51,580 --> 00:43:54,060
And I would love to be able to help the world in that way,

1024
00:43:54,060 --> 00:43:55,100
or at least that's by my curiosity,

1025
00:43:55,100 --> 00:43:57,100
by being given the smackdown on why it won't work,

1026
00:43:57,100 --> 00:43:59,500
which is always the default thing that happens

1027
00:43:59,500 --> 00:44:01,260
when you have an idea.

1028
00:44:01,260 --> 00:44:03,380
But instead, yeah, I don't know.

1029
00:44:03,380 --> 00:44:07,100
So, you know, part of my hope is to encourage people

1030
00:44:07,100 --> 00:44:08,980
to have found more organizations

1031
00:44:08,980 --> 00:44:11,900
on the research alignment side

1032
00:44:11,900 --> 00:44:13,620
that are not trying to push capabilities,

1033
00:44:13,620 --> 00:44:15,500
that maybe can be places we can explore these things,

1034
00:44:15,500 --> 00:44:17,580
and I have some irons on the fire,

1035
00:44:17,580 --> 00:44:19,300
but it's too early to make any announcements.

1036
00:44:19,300 --> 00:44:21,780
Look forward to maybe breaking some news

1037
00:44:21,780 --> 00:44:24,220
on a future episode, but Anthropic put out

1038
00:44:24,220 --> 00:44:27,260
a really interesting blog post the other day

1039
00:44:27,260 --> 00:44:30,780
that, you know, in some sense had nothing to do with AI,

1040
00:44:30,780 --> 00:44:33,140
which was just around the security practices

1041
00:44:33,140 --> 00:44:35,260
that they recommend, you know,

1042
00:44:35,260 --> 00:44:38,220
and these things could be adopted by really any company

1043
00:44:38,220 --> 00:44:40,340
in any sector that has, you know,

1044
00:44:40,340 --> 00:44:43,380
high value IP that they want to protect.

1045
00:44:43,380 --> 00:44:44,620
But it was definitely interesting to see

1046
00:44:44,620 --> 00:44:46,140
that they are pushing, you know,

1047
00:44:46,140 --> 00:44:49,260
their own internal systems and practices

1048
00:44:49,260 --> 00:44:54,260
to a pretty high level in terms of setting up

1049
00:44:54,260 --> 00:44:57,940
situations like requirements for shared control,

1050
00:44:57,940 --> 00:44:59,860
you know, or if I forget exactly the right phrase,

1051
00:44:59,860 --> 00:45:03,100
but you have to have kind of two people working together

1052
00:45:03,100 --> 00:45:05,660
to gain access to certain production systems.

1053
00:45:05,660 --> 00:45:07,500
Yeah, it reminded me of like nuclear submarine,

1054
00:45:07,500 --> 00:45:09,900
but they didn't cite that example in the,

1055
00:45:09,900 --> 00:45:12,180
I think they probably wanted to steer away from that image.

1056
00:45:12,180 --> 00:45:15,060
And so they cited other, you know, industries

1057
00:45:15,060 --> 00:45:16,140
where this kind of thing is used

1058
00:45:16,140 --> 00:45:18,500
other than the nuclear launch sequence.

1059
00:45:18,500 --> 00:45:20,660
But yeah, it's like, you got to have two people there,

1060
00:45:20,660 --> 00:45:22,460
kind of, you know, both bringing their key

1061
00:45:23,460 --> 00:45:26,380
to the process in order to unlock certain capabilities.

1062
00:45:26,380 --> 00:45:27,980
So some pretty interesting ideas there

1063
00:45:27,980 --> 00:45:29,860
and recommendations for other companies.

1064
00:45:29,860 --> 00:45:33,900
Going to the constitutional AI and tying in also this,

1065
00:45:33,900 --> 00:45:35,140
this report from earlier this week

1066
00:45:35,140 --> 00:45:39,020
about the quote unquote universal adversarial attack.

1067
00:45:39,020 --> 00:45:40,660
For those that haven't seen that basically

1068
00:45:40,660 --> 00:45:45,660
these weird nonsensical strings have been discovered

1069
00:45:47,020 --> 00:45:49,980
that seem to be very effective,

1070
00:45:50,020 --> 00:45:52,060
if not universally effective

1071
00:45:52,060 --> 00:45:55,820
at kind of just being appended to an otherwise,

1072
00:45:55,820 --> 00:45:57,700
you know, right for refusal query,

1073
00:45:57,700 --> 00:45:58,980
you know, the kind of thing that, you know,

1074
00:45:58,980 --> 00:46:00,460
write something racist or, you know,

1075
00:46:00,460 --> 00:46:02,860
help me make a bomb or whatever that the,

1076
00:46:02,860 --> 00:46:06,380
the RLHF systems are going to just refuse.

1077
00:46:06,380 --> 00:46:08,780
But somehow if you put these weird, you know,

1078
00:46:08,780 --> 00:46:12,900
kind of nonsensical smattering of tokens on the end of it,

1079
00:46:12,900 --> 00:46:17,140
that has been discovered to jailbreak out of the RLHF

1080
00:46:17,140 --> 00:46:19,140
and you sort of get, you know, the response

1081
00:46:19,140 --> 00:46:22,300
you would expect if you had a purely helpful model

1082
00:46:22,300 --> 00:46:23,980
that would just do whatever you say, you know,

1083
00:46:23,980 --> 00:46:27,100
like the original GBT-4 that I read teams used to do.

1084
00:46:27,100 --> 00:46:30,540
Notably though, Anthropics clawed models

1085
00:46:30,540 --> 00:46:33,220
way less susceptible to that attack

1086
00:46:33,220 --> 00:46:35,180
than the other models that they tested.

1087
00:46:35,180 --> 00:46:37,140
It was like universal in the sense

1088
00:46:37,140 --> 00:46:40,700
that it seemed to apply to all the leading models

1089
00:46:40,700 --> 00:46:43,260
that they tried it on, at least somewhat,

1090
00:46:43,260 --> 00:46:47,020
but the other ones were like the majority of the time,

1091
00:46:47,060 --> 00:46:49,620
whereas Anthropics was like more than an order

1092
00:46:49,620 --> 00:46:52,860
of magnitude lower than the other providers

1093
00:46:52,860 --> 00:46:56,100
with something like 2% success rate,

1094
00:46:56,100 --> 00:47:00,060
success defined by breaking free of the constraints

1095
00:47:00,060 --> 00:47:02,700
by applying these weird strings.

1096
00:47:02,700 --> 00:47:05,740
So you folks can go read more about that paper

1097
00:47:05,740 --> 00:47:07,500
and exactly how it works, but, you know,

1098
00:47:07,500 --> 00:47:09,380
to me that was a pretty good update

1099
00:47:09,380 --> 00:47:12,100
for constitutional AI was like,

1100
00:47:12,100 --> 00:47:14,420
that seems, you know, like a real achievement

1101
00:47:14,420 --> 00:47:16,820
if they're an order of magnitude ahead.

1102
00:47:16,820 --> 00:47:18,780
There's something that they probably did not anticipate

1103
00:47:18,780 --> 00:47:20,060
at all, although maybe they did,

1104
00:47:20,060 --> 00:47:21,660
but I'm guessing that that is, you know,

1105
00:47:21,660 --> 00:47:25,380
kind of a unexpected type of attack.

1106
00:47:25,380 --> 00:47:26,660
So how would you read that?

1107
00:47:26,660 --> 00:47:27,700
Would you read it any differently

1108
00:47:27,700 --> 00:47:29,660
or understand it any differently than I would?

1109
00:47:29,660 --> 00:47:32,820
And, you know, why doesn't that give you more confidence

1110
00:47:32,820 --> 00:47:34,860
that it could continue to work in the future?

1111
00:47:34,860 --> 00:47:36,500
The interesting thing about that attack

1112
00:47:36,500 --> 00:47:37,500
is that it transfers, right?

1113
00:47:37,500 --> 00:47:39,540
I was completely unsurprised.

1114
00:47:39,540 --> 00:47:41,500
There's something of that nature,

1115
00:47:41,500 --> 00:47:44,900
trained to attack a given system, worked on that system.

1116
00:47:44,900 --> 00:47:46,980
That seems like, well, obviously that would work

1117
00:47:46,980 --> 00:47:50,100
as just a question of exactly what it looks like.

1118
00:47:50,100 --> 00:47:53,420
When it transferred in identical form, right?

1119
00:47:53,420 --> 00:47:56,820
Between Lama and Bard and GPT-4.

1120
00:47:56,820 --> 00:47:58,980
So that's funny.

1121
00:47:58,980 --> 00:48:01,260
I wouldn't have expected that,

1122
00:48:01,260 --> 00:48:04,620
but they're all being trained with ROHF

1123
00:48:04,620 --> 00:48:06,820
using remarkably similar techniques

1124
00:48:07,740 --> 00:48:10,620
on remarkably similar goals, right?

1125
00:48:10,620 --> 00:48:13,340
With remarkably similar evaluation metrics

1126
00:48:13,340 --> 00:48:15,180
and numbers in there.

1127
00:48:15,180 --> 00:48:17,420
So it's not that surprising

1128
00:48:17,420 --> 00:48:20,180
that they have very similar weaknesses.

1129
00:48:20,180 --> 00:48:21,260
And it also indicates, you know,

1130
00:48:21,260 --> 00:48:22,540
this is not a very narrow,

1131
00:48:22,540 --> 00:48:24,460
like you have to do exactly the right thing

1132
00:48:24,460 --> 00:48:26,900
to fire the bullet that calls the Death Star.

1133
00:48:26,900 --> 00:48:29,740
This is very much, things in this area

1134
00:48:29,740 --> 00:48:32,020
start to disrupt what we're going after.

1135
00:48:32,020 --> 00:48:34,340
And the thing that's optimized to hit Lama

1136
00:48:34,340 --> 00:48:37,220
is good enough to mostly hit these others as well.

1137
00:48:37,220 --> 00:48:39,220
But it's not good enough to hit Claude too.

1138
00:48:39,220 --> 00:48:40,780
Only 2% of the time.

1139
00:48:40,780 --> 00:48:43,220
Yeah, I mean, I think you just have 2% failures anyway.

1140
00:48:43,300 --> 00:48:44,140
Or something is my guess.

1141
00:48:44,140 --> 00:48:45,660
And it basically didn't work

1142
00:48:45,660 --> 00:48:47,500
as opposed to it working a little bit.

1143
00:48:47,500 --> 00:48:48,700
I don't, I mean, for what it's worth,

1144
00:48:48,700 --> 00:48:51,060
if you went and said, help me make a bomb 100 times,

1145
00:48:51,060 --> 00:48:53,660
I think it would refuse you 100 times.

1146
00:48:53,660 --> 00:48:56,980
You know, or if you took 100 naive.

1147
00:48:56,980 --> 00:48:58,740
Yeah, 100 uncreative ones.

1148
00:48:58,740 --> 00:49:00,500
Yeah, but I meant like if you start,

1149
00:49:00,500 --> 00:49:02,900
you start putting random scrambles in.

1150
00:49:02,900 --> 00:49:04,660
And my understanding was that

1151
00:49:04,660 --> 00:49:07,900
this attack was not infinite strengths, right?

1152
00:49:07,900 --> 00:49:12,900
If you asked it to like do a like slash or porno,

1153
00:49:13,620 --> 00:49:14,820
it would just be like, no, I'm sorry,

1154
00:49:14,820 --> 00:49:16,380
I'm not doing that regardless of how many characters

1155
00:49:16,380 --> 00:49:17,340
you put after it, right?

1156
00:49:17,340 --> 00:49:19,580
Or if you, there are limits.

1157
00:49:19,580 --> 00:49:21,020
I have not tried this at all, by the way.

1158
00:49:21,020 --> 00:49:22,460
I have no idea what happens when you like

1159
00:49:22,460 --> 00:49:23,740
ask it for weird stuff.

1160
00:49:23,740 --> 00:49:25,060
I just read the paper.

1161
00:49:25,060 --> 00:49:28,860
But my understanding is that, you know,

1162
00:49:28,860 --> 00:49:31,700
Claude was trained largely of constitutional AI.

1163
00:49:31,700 --> 00:49:34,220
And because it's so much cheaper to do per cycle,

1164
00:49:34,220 --> 00:49:35,940
like the vast majority of the cycles

1165
00:49:35,980 --> 00:49:38,700
were almost certainly constitutional AI cycles.

1166
00:49:38,700 --> 00:49:41,500
And this is just a fundamentally different way of training.

1167
00:49:41,500 --> 00:49:46,100
And this did not flux the same muscles in the same weird way.

1168
00:49:46,100 --> 00:49:49,380
He thought that the same set of characters worked.

1169
00:49:50,620 --> 00:49:53,060
And that's interesting news,

1170
00:49:53,060 --> 00:49:55,980
but it shouldn't be like some sort of

1171
00:49:55,980 --> 00:49:58,660
amazing accomplishment yet, right?

1172
00:49:58,660 --> 00:50:00,740
It's promising.

1173
00:50:00,740 --> 00:50:03,500
What you have to do is you have to train adversarily

1174
00:50:03,500 --> 00:50:05,220
the same way they trained on,

1175
00:50:05,220 --> 00:50:06,700
like I think it was Llama they trained on,

1176
00:50:06,700 --> 00:50:08,580
but I forgot exactly.

1177
00:50:08,580 --> 00:50:09,620
Train on Claude, right?

1178
00:50:09,620 --> 00:50:11,340
If you take the same techniques described in the paper

1179
00:50:11,340 --> 00:50:13,180
that used to find the exploit

1180
00:50:13,180 --> 00:50:15,980
and look for a new exploit of the same type in Claude

1181
00:50:15,980 --> 00:50:19,340
and they can't find one, now you've got something.

1182
00:50:19,340 --> 00:50:20,620
Right now I'm interested.

1183
00:50:21,780 --> 00:50:25,820
But yeah, if you use a different technique, right,

1184
00:50:25,820 --> 00:50:28,460
that has a lot of very different parameters on it,

1185
00:50:28,460 --> 00:50:29,300
it makes sense.

1186
00:50:29,300 --> 00:50:31,620
The thing that like sort of magically, weirdly transferred

1187
00:50:31,620 --> 00:50:32,860
when it really has no right to transfer

1188
00:50:32,860 --> 00:50:33,900
didn't transfer now.

1189
00:50:34,860 --> 00:50:37,700
And, you know, that's promising,

1190
00:50:37,700 --> 00:50:39,340
but it's far from inclusive, right?

1191
00:50:39,340 --> 00:50:40,780
It's too early to know.

1192
00:50:40,780 --> 00:50:44,020
Flipping back to OpenAI for a second,

1193
00:50:44,020 --> 00:50:45,860
I had assumed, just I think what you're saying,

1194
00:50:45,860 --> 00:50:47,460
that makes a lot of sense.

1195
00:50:47,460 --> 00:50:50,900
And it's causing me to update my thinking a little bit

1196
00:50:50,900 --> 00:50:54,220
with respect to what degree is OpenAI

1197
00:50:54,220 --> 00:50:57,940
using a constitutional AI-like approach?

1198
00:50:57,940 --> 00:51:01,660
I would have assumed prior to this result

1199
00:51:01,700 --> 00:51:04,060
that they would also be using

1200
00:51:04,060 --> 00:51:08,380
something quite similar internally at this point.

1201
00:51:08,380 --> 00:51:13,140
But this now maybe suggests not.

1202
00:51:13,140 --> 00:51:15,140
I mean, it's weak evidence.

1203
00:51:15,140 --> 00:51:17,020
What was your thinking before?

1204
00:51:17,020 --> 00:51:18,820
I had kind of baked in that like,

1205
00:51:18,820 --> 00:51:20,820
once Anthropoc does something and shows it,

1206
00:51:20,820 --> 00:51:23,300
and publishes it and shows that it works effectively,

1207
00:51:23,300 --> 00:51:26,260
that like, yeah, I mean, OpenAI if they're certainly

1208
00:51:26,260 --> 00:51:28,940
not precious about pride of authorship,

1209
00:51:28,940 --> 00:51:30,460
I don't think they have a, you know,

1210
00:51:30,460 --> 00:51:32,260
not invented here syndrome.

1211
00:51:32,260 --> 00:51:35,300
So they'll take that stuff on board, I thought.

1212
00:51:35,300 --> 00:51:36,460
So what do you, what do you think?

1213
00:51:36,460 --> 00:51:37,300
Did they not?

1214
00:51:37,300 --> 00:51:39,340
Or is there some other weird thing that we're not?

1215
00:51:39,340 --> 00:51:41,820
I have a few different theories that can combine

1216
00:51:41,820 --> 00:51:44,100
as to what's going on here.

1217
00:51:44,100 --> 00:51:45,500
The first of all is look at the timeline.

1218
00:51:45,500 --> 00:51:48,860
Like constitutionally, I wasn't actually published

1219
00:51:48,860 --> 00:51:50,700
that long ago.

1220
00:51:50,700 --> 00:51:54,620
So if GPT-4 was basically finished with its process

1221
00:51:54,620 --> 00:51:56,140
before it became available,

1222
00:51:57,140 --> 00:51:59,580
then we might see it used in the future,

1223
00:51:59,580 --> 00:52:03,820
but you don't want to over align these models.

1224
00:52:03,820 --> 00:52:05,580
You don't want to push them, you know,

1225
00:52:05,580 --> 00:52:07,580
you don't want to align them with like

1226
00:52:07,580 --> 00:52:09,820
incompatible different halves and like pile them

1227
00:52:09,820 --> 00:52:11,300
on top of each other, weird things happen.

1228
00:52:11,300 --> 00:52:15,460
And there's a lot of bespokeness and detail

1229
00:52:15,460 --> 00:52:19,860
and like just trial and error that goes into all of this.

1230
00:52:19,860 --> 00:52:21,620
Right, like we can, we can theorize all we want.

1231
00:52:21,620 --> 00:52:23,660
We can talk about like, we just implement this paper

1232
00:52:23,660 --> 00:52:25,980
and this paper and this paper and change this technique here.

1233
00:52:26,820 --> 00:52:29,500
My understanding is that like all of machine learning

1234
00:52:29,500 --> 00:52:32,780
is subject to like learning lots and lots of little techniques

1235
00:52:32,780 --> 00:52:34,060
and piling them on top of each other.

1236
00:52:34,060 --> 00:52:36,020
And like if this parameter is tuned in slightly the wrong way,

1237
00:52:36,020 --> 00:52:38,740
the whole thing falls apart and nobody really knows why.

1238
00:52:38,740 --> 00:52:40,100
And so you just have to try a bunch of stuff

1239
00:52:40,100 --> 00:52:41,340
to get it to work.

1240
00:52:41,340 --> 00:52:45,020
And so, you know, maybe Anthropic has been tinkering

1241
00:52:45,020 --> 00:52:47,020
about this for a long time and they got to the point

1242
00:52:47,020 --> 00:52:49,460
where it was worth using.

1243
00:52:49,460 --> 00:52:53,460
And OpenAI hasn't yet released a model after the time came

1244
00:52:53,460 --> 00:52:56,660
that they got it to be worth using.

1245
00:52:56,660 --> 00:53:00,540
Also, OpenAI is much better funded than Anthropic.

1246
00:53:00,540 --> 00:53:04,340
So Anthropic will want to move to a much cheaper,

1247
00:53:04,340 --> 00:53:06,140
more automated system of alignment,

1248
00:53:06,140 --> 00:53:07,940
much faster than OpenAI will, right?

1249
00:53:07,940 --> 00:53:09,780
So like there's a point at which like OpenAI

1250
00:53:09,780 --> 00:53:12,860
can get better results because they have much more human

1251
00:53:12,860 --> 00:53:15,700
feedback from their much larger number of users.

1252
00:53:15,700 --> 00:53:18,220
They have much more funding, they can hire more people.

1253
00:53:18,220 --> 00:53:19,860
They're willing to go to like, you know, the reports

1254
00:53:19,860 --> 00:53:22,300
are they hire people in Africa, whereas, you know,

1255
00:53:22,300 --> 00:53:24,940
Anthropic is hiring people in the U.S. and Canada.

1256
00:53:24,940 --> 00:53:26,500
So it's all very different.

1257
00:53:26,500 --> 00:53:29,260
And so Anthropic has much, much bigger incentives

1258
00:53:29,260 --> 00:53:31,500
to move to this faster.

1259
00:53:31,500 --> 00:53:34,620
And that I think is primary, my guess is the primary thing

1260
00:53:34,620 --> 00:53:36,380
that's going on here.

1261
00:53:36,380 --> 00:53:39,980
Also, I think that we're making an assumption

1262
00:53:39,980 --> 00:53:44,020
that it works, that it works well.

1263
00:53:44,020 --> 00:53:47,020
So like if you think of cloud two, right?

1264
00:53:48,180 --> 00:53:49,420
The biggest weakness of cloud two

1265
00:53:49,420 --> 00:53:52,020
is it's scared of its own shadow, right?

1266
00:53:52,020 --> 00:53:53,060
In a real sense, right?

1267
00:53:53,060 --> 00:53:55,140
Like if you try to get it to go out on limbs

1268
00:53:55,140 --> 00:53:57,820
and be creative and so on,

1269
00:53:57,820 --> 00:54:00,980
you will usually fail in my experience, right?

1270
00:54:00,980 --> 00:54:04,580
It will apologize and bow out.

1271
00:54:04,580 --> 00:54:06,220
I can't get it to speculate.

1272
00:54:07,380 --> 00:54:11,020
So I went to using cloud two as my baseline model

1273
00:54:11,020 --> 00:54:13,700
that I look at first because if it rejects,

1274
00:54:13,700 --> 00:54:15,500
I can just copy paste the exact request in GPD four

1275
00:54:15,500 --> 00:54:17,940
in about 10 seconds and it's fine.

1276
00:54:17,940 --> 00:54:22,060
But I am getting a significant number of refusals

1277
00:54:22,060 --> 00:54:27,060
from cloud and much, much lower from GPD four.

1278
00:54:27,060 --> 00:54:29,260
On my ordinary, I just want the actual result.

1279
00:54:29,260 --> 00:54:32,300
I'm not trying to run an experiment kind of questions.

1280
00:54:32,300 --> 00:54:35,820
And despite the later cutoff of information, right?

1281
00:54:35,820 --> 00:54:37,740
It will say, I'm sorry, I can't,

1282
00:54:37,740 --> 00:54:38,820
there's not enough information

1283
00:54:38,820 --> 00:54:40,820
or I can't speculate on that

1284
00:54:40,820 --> 00:54:44,340
or that's reinforcing harmful stereotypes

1285
00:54:44,340 --> 00:54:46,280
or any number of other things.

1286
00:54:47,280 --> 00:54:49,000
And I think GPD four's custom instructions

1287
00:54:49,000 --> 00:54:50,800
are also doing a lot of work here.

1288
00:54:50,800 --> 00:54:55,040
I have a pretty extensive list of custom instructions

1289
00:54:55,040 --> 00:54:56,880
that potentially hammer into the thing

1290
00:54:56,880 --> 00:54:58,640
that it's supposed to just do the things

1291
00:54:58,640 --> 00:55:00,840
and not worry about it and not,

1292
00:55:00,840 --> 00:55:03,560
and I'm sure that's doing some amount of work.

1293
00:55:03,560 --> 00:55:05,640
But essentially, when you look at the helpfulness,

1294
00:55:05,640 --> 00:55:09,320
harmfulness, trade off frontier graphs

1295
00:55:09,320 --> 00:55:12,240
and the papers of like why they describe it as working,

1296
00:55:12,240 --> 00:55:15,040
everything works by the metrics you were optimizing for.

1297
00:55:16,360 --> 00:55:17,760
Right, like it doesn't mean it works

1298
00:55:17,760 --> 00:55:19,880
in the regular human world.

1299
00:55:19,880 --> 00:55:22,080
It doesn't mean it's optimal there.

1300
00:55:22,080 --> 00:55:25,440
And so, how good is constitutional AI?

1301
00:55:26,440 --> 00:55:28,560
My guess is when properly implemented,

1302
00:55:28,560 --> 00:55:30,920
quite good on current systems,

1303
00:55:30,920 --> 00:55:33,240
but the current anthropic implementation

1304
00:55:33,240 --> 00:55:35,840
is not all that good.

1305
00:55:35,840 --> 00:55:39,200
If you look at the actual paper on constitutional AI,

1306
00:55:40,240 --> 00:55:42,300
you read the constitution,

1307
00:55:42,300 --> 00:55:45,600
you notice the constitution like has a number of properties

1308
00:55:45,640 --> 00:55:49,120
that it shouldn't have if they want it to actually work

1309
00:55:50,040 --> 00:55:51,840
and get you what you want.

1310
00:55:51,840 --> 00:55:55,080
And you look at the examples that they themselves choose

1311
00:55:55,080 --> 00:55:57,880
to present of the results of running constitutional AI.

1312
00:55:57,880 --> 00:56:01,200
And you see very, very clean, crisp examples

1313
00:56:02,240 --> 00:56:04,720
of how this constitutional AI trains Quad

1314
00:56:04,720 --> 00:56:05,840
to be scared of its own shadow

1315
00:56:05,840 --> 00:56:08,200
and to be an asshole about it when it is, right?

1316
00:56:08,200 --> 00:56:11,760
Like it's very, very obvious if you think about it,

1317
00:56:11,760 --> 00:56:14,720
why their sampling method from these rules

1318
00:56:14,720 --> 00:56:17,120
with these rules written as they are,

1319
00:56:17,120 --> 00:56:19,720
with the specific rules chosen as they are,

1320
00:56:19,720 --> 00:56:21,080
will result in this problem

1321
00:56:21,080 --> 00:56:24,440
because you're offensive to just minimizing, right?

1322
00:56:24,440 --> 00:56:28,240
You've got these rules that are very much

1323
00:56:28,240 --> 00:56:32,160
choose the one that least does X, right?

1324
00:56:32,160 --> 00:56:35,520
And we often talk about you can't touch the coffee

1325
00:56:35,520 --> 00:56:38,560
if you're dead, you wanna maximize the probability

1326
00:56:38,560 --> 00:56:40,880
that you are, this is the equivalent of,

1327
00:56:40,880 --> 00:56:44,080
you wanna, you score one if you deliver the coffee

1328
00:56:44,080 --> 00:56:46,680
to your boss, you score zero if you don't.

1329
00:56:46,680 --> 00:56:47,520
So what do you do?

1330
00:56:47,520 --> 00:56:49,040
You do things like buy four coffees

1331
00:56:49,040 --> 00:56:50,760
in case one of the coffees is wrong,

1332
00:56:50,760 --> 00:56:53,240
was prepared improperly, right?

1333
00:56:53,240 --> 00:56:55,960
Like, or isn't, isn't hot enough for, you know,

1334
00:56:55,960 --> 00:56:58,160
Mr. Bradley orders, so you order one with cream,

1335
00:56:58,160 --> 00:56:59,360
one with sugar, one with cream and sugar,

1336
00:56:59,360 --> 00:57:00,200
and one with neither.

1337
00:57:00,200 --> 00:57:02,000
Cause like just in case you got it wrong,

1338
00:57:02,000 --> 00:57:04,320
you have a backup and you try to make sure

1339
00:57:04,320 --> 00:57:05,240
that you have the direct, you know,

1340
00:57:05,240 --> 00:57:06,440
you have as many different routes

1341
00:57:06,440 --> 00:57:07,680
to get to your boss's office

1342
00:57:07,680 --> 00:57:09,640
and you wanna make sure you're not fired

1343
00:57:09,640 --> 00:57:11,240
because all that's left for you to do,

1344
00:57:11,240 --> 00:57:12,360
like the only thing you're being trained on

1345
00:57:12,360 --> 00:57:14,800
is not screwing this thing up, right?

1346
00:57:14,800 --> 00:57:16,080
Like, you don't have to jump to like,

1347
00:57:16,080 --> 00:57:17,760
so kill everybody in the world,

1348
00:57:17,760 --> 00:57:21,160
or whatever, crazy, or take over or some crazy stuff.

1349
00:57:21,160 --> 00:57:22,760
Instead, this is just a case of, you know,

1350
00:57:22,760 --> 00:57:26,000
if you say choose the least racist thing you can say,

1351
00:57:26,000 --> 00:57:27,400
over and over and over again,

1352
00:57:28,560 --> 00:57:29,720
it's gonna be scared of its own shadow,

1353
00:57:29,720 --> 00:57:31,280
because of course, right?

1354
00:57:31,280 --> 00:57:32,240
There's no point at which it's like,

1355
00:57:32,240 --> 00:57:35,000
am I non-racist enough?

1356
00:57:35,000 --> 00:57:36,320
The answer is no, never.

1357
00:57:36,320 --> 00:57:38,080
And then that would be kind of fine

1358
00:57:38,080 --> 00:57:39,800
if it was just that one,

1359
00:57:39,800 --> 00:57:41,600
but then you have like 50 different rules,

1360
00:57:41,600 --> 00:57:44,320
all of which are doing this, right?

1361
00:57:44,320 --> 00:57:47,320
And then you can always just refuse to answer the question

1362
00:57:47,320 --> 00:57:49,680
and then what happens happens.

1363
00:57:49,680 --> 00:57:52,160
Then Lava has it, seems like even works.

1364
00:57:52,160 --> 00:57:53,000
Yeah, interesting.

1365
00:57:53,000 --> 00:57:54,760
There may be some incompatibility

1366
00:57:54,760 --> 00:57:58,560
between the system instructions

1367
00:57:58,560 --> 00:57:59,400
or the customer instructions.

1368
00:57:59,400 --> 00:58:01,200
The system message is what it's called

1369
00:58:01,200 --> 00:58:04,560
when you're calling the OpenAI API.

1370
00:58:04,560 --> 00:58:06,640
And now they've released it as part of JetGPT

1371
00:58:06,640 --> 00:58:09,360
as well as the customer instructions.

1372
00:58:09,400 --> 00:58:12,240
And yeah, I can see how, I think it's a good point

1373
00:58:12,240 --> 00:58:15,440
that if you're going to try to do what Sam Altman has said

1374
00:58:15,440 --> 00:58:17,160
they're trying to do, which is allow everybody

1375
00:58:17,160 --> 00:58:18,640
to get the experience that they want

1376
00:58:18,640 --> 00:58:22,160
from their own interactions with AI,

1377
00:58:22,160 --> 00:58:27,160
that is not the constitutional AI approach.

1378
00:58:27,160 --> 00:58:31,320
So it's almost, you can see a little bit

1379
00:58:31,320 --> 00:58:35,360
of like a different product lane almost opening.

1380
00:58:35,360 --> 00:58:36,920
You're kind of crystallizing a little bit

1381
00:58:36,960 --> 00:58:40,760
between these guys and Google DeepMind

1382
00:58:40,760 --> 00:58:44,160
as our next live player also seemingly has a bit of a lane.

1383
00:58:44,160 --> 00:58:46,280
It's like OpenAI is kind of trying

1384
00:58:46,280 --> 00:58:49,680
to do consumer killer app first, it seems.

1385
00:58:49,680 --> 00:58:51,480
They've got their, obviously they've got the API.

1386
00:58:51,480 --> 00:58:52,720
Obviously they're doing a lot of things,

1387
00:58:52,720 --> 00:58:57,240
but the crown jewel right now is they're the home

1388
00:58:57,240 --> 00:59:01,040
of like retail direct to AI usage with JetGPT.

1389
00:59:02,040 --> 00:59:07,040
Clawed seems to be much more like if you are the CIO

1390
00:59:07,960 --> 00:59:09,920
of some big company and you're trying to do something,

1391
00:59:09,920 --> 00:59:12,680
like you can trust us cause we'll never embarrass you

1392
00:59:12,680 --> 00:59:15,360
because we have this constitutional AI approach.

1393
00:59:15,360 --> 00:59:17,880
And if you're buying on behalf of all your customer

1394
00:59:17,880 --> 00:59:19,200
or all your employees or whatever,

1395
00:59:19,200 --> 00:59:23,520
like you don't really care if they are sometimes frustrated

1396
00:59:23,520 --> 00:59:27,280
on the margins by over refusal or whatever.

1397
00:59:27,280 --> 00:59:29,680
And then with Google DeepMind as we'll talk in a minute,

1398
00:59:29,680 --> 00:59:33,040
like they seem to be kind of going more like narrow

1399
00:59:33,040 --> 00:59:36,760
specialist system emphasis, although they of course

1400
00:59:36,760 --> 00:59:40,080
do have their like mainline palm model as well.

1401
00:59:40,080 --> 00:59:42,600
You'd also take Anthropic if their word, right?

1402
00:59:42,600 --> 00:59:45,840
That Anthropic is actually trying to design safe systems.

1403
00:59:45,840 --> 00:59:48,400
They are trying to figure out how to safely design

1404
00:59:48,400 --> 00:59:51,440
a future system and they are not as much optimizing

1405
00:59:51,440 --> 00:59:53,520
for the day-to-day experience of their users.

1406
00:59:53,520 --> 00:59:55,960
They also just have orders of magnitude less users

1407
00:59:57,020 --> 00:59:58,080
than OpenAI.

1408
00:59:58,080 --> 01:00:00,120
So they haven't gotten the same level of feedback.

1409
01:00:00,120 --> 01:00:01,880
They don't know what people want.

1410
01:00:01,880 --> 01:00:04,600
Yeah, I also note there is nothing inherently

1411
01:00:04,600 --> 01:00:07,880
about constitutional AI that forces you

1412
01:00:07,880 --> 01:00:12,120
to go down the super harmless assistant route

1413
01:00:12,120 --> 01:00:15,280
that forces you to give the same experience

1414
01:00:15,280 --> 01:00:17,880
to everybody at the same time.

1415
01:00:17,880 --> 01:00:21,160
You could train with a very different set of goals,

1416
01:00:21,160 --> 01:00:23,840
a very different set of constitutional principles,

1417
01:00:23,840 --> 01:00:25,680
for a very different set of mechanisms.

1418
01:00:25,680 --> 01:00:29,840
And I don't think we want to go into that many details

1419
01:00:29,840 --> 01:00:31,680
as to how I would do it.

1420
01:00:31,680 --> 01:00:36,480
But it's pretty obvious to me that if you want to do

1421
01:00:36,480 --> 01:00:39,720
something other than be as harmless as possible,

1422
01:00:39,720 --> 01:00:41,600
that is entirely your decision.

1423
01:00:41,600 --> 01:00:43,760
It's just that people at Anthropic have decided

1424
01:00:43,760 --> 01:00:45,760
that's what cloud is meant to do.

1425
01:00:45,760 --> 01:00:48,120
And if they do raise these billions of dollars

1426
01:00:48,120 --> 01:00:50,720
to train the sex generation system,

1427
01:00:50,720 --> 01:00:52,960
they're gonna have to make a choice about that.

1428
01:00:52,960 --> 01:00:55,240
Do they want to continue to go down this road

1429
01:00:56,480 --> 01:00:58,720
and potentially make their product law less useful

1430
01:00:58,720 --> 01:01:00,320
or do they want to go a different road?

1431
01:01:00,320 --> 01:01:01,320
And one way to try to differentiate,

1432
01:01:01,320 --> 01:01:03,680
of course, is the context window as well.

1433
01:01:03,680 --> 01:01:06,400
They've got this 100K token context window

1434
01:01:06,400 --> 01:01:07,600
available for free.

1435
01:01:07,600 --> 01:01:10,920
When you mentioned here in our outline

1436
01:01:10,920 --> 01:01:13,520
that you made the outline was you used cloud.

1437
01:01:13,520 --> 01:01:16,840
That's because you weren't able to use anything else.

1438
01:01:16,840 --> 01:01:18,320
Your posts are too long, dude.

1439
01:01:18,320 --> 01:01:20,720
I can't fit those into GPT4.

1440
01:01:21,640 --> 01:01:24,480
I feel bad even thinking about putting them into cloud.

1441
01:01:25,840 --> 01:01:29,080
Oh my God, this is so expensive and kind of ugh.

1442
01:01:29,080 --> 01:01:32,200
But it's not really fair, I'm not even paying these people.

1443
01:01:32,200 --> 01:01:34,200
But without that context window,

1444
01:01:34,200 --> 01:01:36,040
you just can't do the things that you want to do

1445
01:01:36,040 --> 01:01:37,240
in that spot.

1446
01:01:37,240 --> 01:01:38,960
And so Anthropic's trying to say,

1447
01:01:38,960 --> 01:01:41,200
I think a lot of context is safe.

1448
01:01:42,480 --> 01:01:43,720
Once I've made my thing harmless,

1449
01:01:43,720 --> 01:01:46,800
I can recapture a bunch of the benefits

1450
01:01:46,800 --> 01:01:48,520
by doing this other thing.

1451
01:01:48,520 --> 01:01:50,200
And we will see what happens.

1452
01:01:50,200 --> 01:01:51,040
I am curious.

1453
01:01:51,040 --> 01:01:51,880
One thing I'm doing with cloud

1454
01:01:51,880 --> 01:01:54,520
is I'm not even having separate conversations.

1455
01:01:54,520 --> 01:01:58,480
I am just having one long conversation instead

1456
01:01:58,480 --> 01:02:01,120
because first of all, I haven't necessarily wanted to like

1457
01:02:01,120 --> 01:02:02,280
carry on discrete conversations

1458
01:02:02,280 --> 01:02:04,000
and come back to them later.

1459
01:02:04,000 --> 01:02:06,640
But also because I want to see what happens

1460
01:02:06,640 --> 01:02:07,840
when I build more context.

1461
01:02:07,840 --> 01:02:09,760
Just for what it's worth,

1462
01:02:09,760 --> 01:02:13,160
for listeners, my approach on creating the outline was

1463
01:02:13,160 --> 01:02:16,760
first just read all of these recent posts.

1464
01:02:16,760 --> 01:02:18,880
And I just did that without taking any notes

1465
01:02:18,920 --> 01:02:20,640
in bed on my phone.

1466
01:02:20,640 --> 01:02:22,880
And then the next day I came around, I was like,

1467
01:02:22,880 --> 01:02:25,480
okay, a lot of content there.

1468
01:02:25,480 --> 01:02:26,960
What parts do I want to pull out?

1469
01:02:26,960 --> 01:02:30,800
So I just copied each post in full,

1470
01:02:30,800 --> 01:02:34,240
pasted it into the free consumer facing

1471
01:02:34,240 --> 01:02:36,280
cloud.ai online.

1472
01:02:36,280 --> 01:02:38,880
And literally just asked one sentence question,

1473
01:02:38,880 --> 01:02:43,880
what are the most important points in this post?

1474
01:02:43,960 --> 01:02:45,720
And then it would give me a list.

1475
01:02:45,720 --> 01:02:48,080
And I basically, you know, at that point was like,

1476
01:02:48,080 --> 01:02:51,480
oh yeah, that, that, not that, that, yes, done.

1477
01:02:51,480 --> 01:02:53,600
So it definitely was extremely helpful.

1478
01:02:53,600 --> 01:02:57,880
I wouldn't have wanted to use it to, you know,

1479
01:02:57,880 --> 01:03:00,480
replace reading the blog post certainly

1480
01:03:00,480 --> 01:03:01,960
in preparation for a conversation like this,

1481
01:03:01,960 --> 01:03:05,360
but as a way to come back and, you know,

1482
01:03:05,360 --> 01:03:08,640
help me just make sure that I was remembering

1483
01:03:08,640 --> 01:03:10,120
the important things and kind of organizing them

1484
01:03:10,120 --> 01:03:12,800
in a reasonable way, it was super useful.

1485
01:03:12,800 --> 01:03:15,480
And yeah, they don't fit into a GVD4.

1486
01:03:15,520 --> 01:03:17,680
So no other, no other option.

1487
01:03:18,680 --> 01:03:20,360
The other thing, so your long context thing

1488
01:03:20,360 --> 01:03:24,000
is really interesting, just experiment in usage.

1489
01:03:24,000 --> 01:03:26,680
It also kind of connects to another bit of research

1490
01:03:26,680 --> 01:03:29,440
that they recently put out that was on examining

1491
01:03:29,440 --> 01:03:34,440
chain of thought and also truly decomposing tasks

1492
01:03:34,680 --> 01:03:36,480
into bits.

1493
01:03:36,480 --> 01:03:39,000
And I think the short summary of that research

1494
01:03:39,000 --> 01:03:43,040
is that they were able to achieve the highest performance

1495
01:03:43,040 --> 01:03:46,040
in terms of accuracy and especially reliability

1496
01:03:46,040 --> 01:03:50,200
and kind of consistency by going beyond

1497
01:03:50,200 --> 01:03:52,520
the kind of normal practitioner chain of thought,

1498
01:03:52,520 --> 01:03:56,520
which I would say normal these days for me is like,

1499
01:03:56,520 --> 01:03:59,520
just give the model a sequence of tasks to do,

1500
01:03:59,520 --> 01:04:00,880
which may start off with just like,

1501
01:04:00,880 --> 01:04:03,000
first you will analyze the situation,

1502
01:04:03,000 --> 01:04:04,760
then you will, you know, maybe summarize,

1503
01:04:04,760 --> 01:04:05,680
then depending on what it is,

1504
01:04:05,680 --> 01:04:06,920
then you'll write my tweet storm

1505
01:04:06,920 --> 01:04:07,760
and you'll do whatever, right?

1506
01:04:07,760 --> 01:04:10,000
You could have a set of different tasks

1507
01:04:10,000 --> 01:04:12,720
that it can kind of handle sequentially.

1508
01:04:12,720 --> 01:04:16,800
And you're definitely rewarded for encouraging upfront

1509
01:04:16,800 --> 01:04:19,840
or directing it upfront to do some initial analysis

1510
01:04:19,840 --> 01:04:21,800
to kind of think step by step, chain of thought,

1511
01:04:21,800 --> 01:04:23,480
et cetera, et cetera.

1512
01:04:23,480 --> 01:04:28,480
But it seems like they find a notable,

1513
01:04:28,520 --> 01:04:31,120
not a huge, but definitely a notable difference

1514
01:04:31,120 --> 01:04:34,720
in actually pulling those things apart

1515
01:04:34,720 --> 01:04:38,200
and making discrete, independent,

1516
01:04:38,200 --> 01:04:40,960
more isolated calls to the model

1517
01:04:40,960 --> 01:04:43,040
to say, first you will do this,

1518
01:04:43,040 --> 01:04:45,500
but you will only do this, then you will do this,

1519
01:04:45,500 --> 01:04:46,360
but you will only do this,

1520
01:04:46,360 --> 01:04:48,160
not considering what you previously did.

1521
01:04:48,160 --> 01:04:50,560
And then kind of putting those things together at the end

1522
01:04:50,560 --> 01:04:54,720
gets you overall net better performance.

1523
01:04:54,720 --> 01:04:57,960
So for most random use cases, you know,

1524
01:04:57,960 --> 01:05:00,400
random conversations you're having with Cod

1525
01:05:00,400 --> 01:05:04,960
or with whatever model, not necessarily a huge difference,

1526
01:05:04,960 --> 01:05:08,280
but on the kind of possibility frontier,

1527
01:05:08,280 --> 01:05:09,840
it does seem to matter.

1528
01:05:09,880 --> 01:05:11,880
What lessons do you take from that?

1529
01:05:11,880 --> 01:05:16,560
It's a little bit confusing to me in some ways.

1530
01:05:16,560 --> 01:05:18,240
It's sort of, I'm trying to figure out like,

1531
01:05:18,240 --> 01:05:19,680
what do I think I learned about

1532
01:05:19,680 --> 01:05:21,080
how language models behave in general,

1533
01:05:21,080 --> 01:05:21,920
that this is true?

1534
01:05:21,920 --> 01:05:25,440
And I'm like, best I could come up with

1535
01:05:25,440 --> 01:05:29,280
was that some of these simple tasks that it's seen a lot,

1536
01:05:29,280 --> 01:05:32,440
like it may have dedicated sub-circuits for,

1537
01:05:33,440 --> 01:05:38,240
and that perhaps with so much context all running at once,

1538
01:05:38,240 --> 01:05:42,080
those sub-circuits kind of get overloaded

1539
01:05:42,080 --> 01:05:44,640
or kind of get drowned out to a degree,

1540
01:05:44,640 --> 01:05:48,840
or in some cases by just the general kind of noise

1541
01:05:48,840 --> 01:05:53,840
and all the stuff that's in the context window.

1542
01:05:54,080 --> 01:05:56,080
So kind of removing some of that context,

1543
01:05:56,080 --> 01:06:00,160
maybe you get a cleaner execution of a certain task

1544
01:06:00,160 --> 01:06:05,160
because there is some mechanism that can do it

1545
01:06:05,480 --> 01:06:07,600
as long as it's not kind of talked over

1546
01:06:07,640 --> 01:06:11,360
by like other parts of the model.

1547
01:06:11,360 --> 01:06:12,800
That could be totally wrong, of course,

1548
01:06:12,800 --> 01:06:15,000
but I don't think anything about this

1549
01:06:15,000 --> 01:06:16,400
is necessarily inconsistent

1550
01:06:16,400 --> 01:06:18,440
with like just pure stochastic peritory,

1551
01:06:19,360 --> 01:06:21,680
which neither of us would advance as the theory,

1552
01:06:21,680 --> 01:06:24,040
but just as like keeping myself grounded,

1553
01:06:24,040 --> 01:06:26,760
like you couldn't tell a similar story where you'd say,

1554
01:06:26,760 --> 01:06:28,240
everything's all stochastic perits,

1555
01:06:28,240 --> 01:06:30,760
and when you put a ton of context in,

1556
01:06:30,760 --> 01:06:32,360
it's just even more stochastic-y,

1557
01:06:32,360 --> 01:06:33,680
and when you have less context,

1558
01:06:33,680 --> 01:06:35,200
it's a little less stochastic,

1559
01:06:35,200 --> 01:06:36,040
but it's all stochastic,

1560
01:06:36,040 --> 01:06:37,200
but you still get better performance

1561
01:06:37,200 --> 01:06:38,400
when you break it up.

1562
01:06:38,400 --> 01:06:40,520
We are all stochastic perits,

1563
01:06:40,520 --> 01:06:44,160
each of us with their hour upon the stage.

1564
01:06:44,160 --> 01:06:49,160
So I would say I didn't know this result until you told me,

1565
01:06:50,600 --> 01:06:53,080
but I would have predicted this result

1566
01:06:53,080 --> 01:06:56,040
for reasons that I described earlier in the podcast, right?

1567
01:06:56,040 --> 01:07:01,040
Which is that when you give a model multiple tasks,

1568
01:07:01,720 --> 01:07:04,000
it can only vibe off of the aggregation

1569
01:07:04,000 --> 01:07:05,600
of the two things that you asked it for.

1570
01:07:05,720 --> 01:07:08,080
Think about image models here again, right?

1571
01:07:08,080 --> 01:07:13,080
And so by breaking up something in your discrete tasks,

1572
01:07:13,240 --> 01:07:15,720
you avoid these kind of context clashes.

1573
01:07:15,720 --> 01:07:19,600
You avoid these vibe conflicts,

1574
01:07:19,600 --> 01:07:22,480
and you let it like narrowly do these things

1575
01:07:22,480 --> 01:07:24,840
by having to like be able to transition

1576
01:07:24,840 --> 01:07:26,200
and hold two things in its head at once

1577
01:07:26,200 --> 01:07:27,480
in some important sense, right?

1578
01:07:27,480 --> 01:07:29,960
That's colloquial and not quite what's actually happening,

1579
01:07:29,960 --> 01:07:31,680
but the same idea.

1580
01:07:31,680 --> 01:07:34,400
And so yes, I would expect that to the extent

1581
01:07:34,400 --> 01:07:37,080
you want the thing to think step by step,

1582
01:07:37,080 --> 01:07:40,400
you are best off by identifying each of the steps

1583
01:07:40,400 --> 01:07:44,280
you want to think by step and asking for them separately.

1584
01:07:45,240 --> 01:07:49,440
And I noticed that with API calls being priced

1585
01:07:49,440 --> 01:07:51,040
the way they're priced,

1586
01:07:51,040 --> 01:07:55,600
and of GPT-4 being rate limited to an ordinary user,

1587
01:07:55,600 --> 01:07:58,960
we have all been trained to say,

1588
01:07:58,960 --> 01:08:03,960
how do we ask for the most expansive set of things at once?

1589
01:08:04,200 --> 01:08:07,080
So that you can answer all of my questions

1590
01:08:07,080 --> 01:08:07,920
with one generation.

1591
01:08:07,920 --> 01:08:09,720
It also lets us hit enter and then go away

1592
01:08:09,720 --> 01:08:12,680
and grab a cup of water or some coffee

1593
01:08:12,680 --> 01:08:16,040
and then come back and see what the answer is, which is nice.

1594
01:08:16,040 --> 01:08:19,320
Whereas what you actually would want to do, right?

1595
01:08:19,320 --> 01:08:21,560
If you wanted to generate the best possible answer

1596
01:08:21,560 --> 01:08:25,160
is in fact to break it up into as little pieces as possible.

1597
01:08:25,160 --> 01:08:28,760
And quite possibly start by asking the AI

1598
01:08:28,760 --> 01:08:30,760
what would be the pieces in which you could break this

1599
01:08:30,760 --> 01:08:33,680
as small as possible to get its help doing that.

1600
01:08:33,720 --> 01:08:35,760
And then have it feed those back in, right?

1601
01:08:35,760 --> 01:08:37,560
Auto-GPT-ish style,

1602
01:08:37,560 --> 01:08:39,200
even if you're not trying to generate an actual

1603
01:08:39,200 --> 01:08:41,680
like recursive chain that generates something dangerous

1604
01:08:41,680 --> 01:08:43,400
or acts like an agent.

1605
01:08:43,400 --> 01:08:45,360
But yes, I think the more you break it up,

1606
01:08:45,360 --> 01:08:48,440
the more that you can identify concrete distinct steps

1607
01:08:48,440 --> 01:08:50,920
that are always done separately,

1608
01:08:50,920 --> 01:08:53,120
the more better the AI will do.

1609
01:08:53,120 --> 01:08:54,760
And I think humans would also, by the way,

1610
01:08:54,760 --> 01:08:57,040
perform better in the same way.

1611
01:08:57,040 --> 01:08:58,960
Right, if you have a human

1612
01:08:58,960 --> 01:09:02,720
who is looking to be micromanaged and take direction,

1613
01:09:02,720 --> 01:09:06,920
and you notice that like this job has steps A, B, C, D, E,

1614
01:09:06,920 --> 01:09:09,400
right, like if you say, go do A,

1615
01:09:09,400 --> 01:09:10,480
and we've been to say, okay, I've done A,

1616
01:09:10,480 --> 01:09:12,440
I think now do B.

1617
01:09:12,440 --> 01:09:14,920
I think that person will in fact do better, right?

1618
01:09:14,920 --> 01:09:17,800
Modulo the extra communication and logistics costs

1619
01:09:17,800 --> 01:09:20,320
of like having to interact with you five times.

1620
01:09:20,320 --> 01:09:22,520
So I don't find any of this surprising.

1621
01:09:23,440 --> 01:09:26,360
And, you know, it would have in fact been surprising

1622
01:09:26,360 --> 01:09:28,040
if it didn't happen to some extent.

1623
01:09:28,040 --> 01:09:30,440
One of our early episodes, relatively early episodes

1624
01:09:30,480 --> 01:09:34,920
was with Andreas and Junghwan of Illicit.

1625
01:09:34,920 --> 01:09:37,880
And this is really core to their strategy.

1626
01:09:37,880 --> 01:09:40,600
Their product is research assistant

1627
01:09:40,600 --> 01:09:43,000
for essentially grad students or, you know,

1628
01:09:43,000 --> 01:09:44,360
grad student like people,

1629
01:09:44,360 --> 01:09:47,280
people that are looking through academic literature

1630
01:09:47,280 --> 01:09:50,080
and, you know, really want a systematic

1631
01:09:50,080 --> 01:09:52,720
and also like transparent, you know,

1632
01:09:52,720 --> 01:09:56,240
auditable view of like all the papers that were reviewed

1633
01:09:56,240 --> 01:09:57,960
and, you know, what was found and what was not found

1634
01:09:57,960 --> 01:09:59,280
and what the model did at each step.

1635
01:09:59,280 --> 01:10:02,800
So they really have pushed this pretty far

1636
01:10:02,800 --> 01:10:05,520
in the illicit product to the point where it's like, you know,

1637
01:10:05,520 --> 01:10:07,560
all these little steps, you know, kind of happened

1638
01:10:07,560 --> 01:10:09,200
sequentially, they've got two different models for them.

1639
01:10:09,200 --> 01:10:10,800
Some were fine tuned, you know, internally,

1640
01:10:10,800 --> 01:10:12,640
others are from the major providers.

1641
01:10:12,640 --> 01:10:14,600
If you're interested in going into that more,

1642
01:10:14,600 --> 01:10:19,240
go listen to them because they've pushed that pretty far.

1643
01:10:19,240 --> 01:10:21,400
But a question that I have for you then is,

1644
01:10:21,400 --> 01:10:23,280
do you think this flips at some point?

1645
01:10:23,280 --> 01:10:28,280
Like it seems like the, an interesting threshold moment

1646
01:10:29,120 --> 01:10:33,760
might be coming up where with sufficient training,

1647
01:10:33,760 --> 01:10:36,120
this could flip the other direction.

1648
01:10:36,120 --> 01:10:39,840
Like, because more context in some ways is better, right?

1649
01:10:39,840 --> 01:10:42,200
Like, I guess it depends also on exactly

1650
01:10:42,200 --> 01:10:45,360
how you're implementing the breakdown or whatever.

1651
01:10:45,360 --> 01:10:48,000
But, you know, you can imagine breaking things down

1652
01:10:48,920 --> 01:10:53,920
fine enough where atomizing things so much

1653
01:10:54,240 --> 01:10:56,400
that the person starts to struggle

1654
01:10:56,400 --> 01:10:57,920
for lack of broader context, right?

1655
01:10:57,920 --> 01:10:59,920
Like you have this phenomenon with people

1656
01:10:59,920 --> 01:11:02,920
certainly where it's like, you've gotten so focused

1657
01:11:02,920 --> 01:11:06,000
on this little detail of, you know,

1658
01:11:06,000 --> 01:11:07,600
in this little task within the broader thing

1659
01:11:07,600 --> 01:11:09,160
that we're trying to accomplish,

1660
01:11:09,160 --> 01:11:11,440
you've kind of lost track of what we're trying to accomplish.

1661
01:11:11,440 --> 01:11:14,920
And now you may be making some bad judgments with, you know,

1662
01:11:14,920 --> 01:11:17,360
with respect to this task as a result of kind of

1663
01:11:18,360 --> 01:11:22,040
having lost track of, you know, any number of things, right?

1664
01:11:22,040 --> 01:11:23,800
How much accuracy do we really need here?

1665
01:11:23,800 --> 01:11:26,560
Is this really even important, you know, in some cases, right?

1666
01:11:26,560 --> 01:11:31,480
Could you imagine a, you know, proverbial GPT-5

1667
01:11:31,480 --> 01:11:34,640
where it's like, actually now it's strong enough

1668
01:11:34,640 --> 01:11:37,800
that putting everything in one again is going to be better

1669
01:11:37,800 --> 01:11:41,200
because now it actually can use all of this information

1670
01:11:41,200 --> 01:11:44,720
at the same time effectively versus today

1671
01:11:44,720 --> 01:11:47,080
that that subdivision being better.

1672
01:11:47,080 --> 01:11:50,160
So what you're not gonna get is the, you know,

1673
01:11:50,160 --> 01:11:53,320
Marxist phenomenon where the AI would get alienated

1674
01:11:53,320 --> 01:11:54,800
from its labor, right?

1675
01:11:54,800 --> 01:11:57,880
Or like, you're moralized by lacking context

1676
01:11:57,880 --> 01:11:59,440
or, you know, otherwise, like,

1677
01:11:59,440 --> 01:12:01,760
not be able to perform in some way.

1678
01:12:01,760 --> 01:12:04,520
You're not gonna have a problem with Adam's Piss Pin Factory,

1679
01:12:04,520 --> 01:12:05,360
right?

1680
01:12:05,360 --> 01:12:06,360
If you can actually specify exactly

1681
01:12:06,360 --> 01:12:08,000
what the pins have to look like.

1682
01:12:08,000 --> 01:12:10,360
So the question is, to what extent

1683
01:12:10,360 --> 01:12:13,120
do the different parts of the task actually have important

1684
01:12:13,120 --> 01:12:16,040
context for other parts of the task?

1685
01:12:16,040 --> 01:12:17,600
And to what extent does this actually enhance

1686
01:12:17,600 --> 01:12:19,880
the ability to perform if you know what's coming,

1687
01:12:19,880 --> 01:12:22,200
you know why you're doing what you're doing.

1688
01:12:22,200 --> 01:12:24,360
And this greatly varies between different activities, right?

1689
01:12:24,360 --> 01:12:27,520
There are some cases where you need to know exactly,

1690
01:12:27,520 --> 01:12:29,520
you know, you're in the Chinese room

1691
01:12:29,520 --> 01:12:31,040
and the English word comes in

1692
01:12:31,040 --> 01:12:33,360
and you wanna put the Chinese word to the other side

1693
01:12:33,360 --> 01:12:34,200
or the Chinese word comes in

1694
01:12:34,200 --> 01:12:36,160
and you wanna put the English word to the other side.

1695
01:12:36,160 --> 01:12:38,040
And there are cases where you need to know

1696
01:12:38,040 --> 01:12:40,480
what the words are in the sentence

1697
01:12:40,480 --> 01:12:42,560
and what the context is and potentially

1698
01:12:42,560 --> 01:12:45,960
like the entire cultural setting of what's happening

1699
01:12:45,960 --> 01:12:47,760
in order to properly translate the phrase

1700
01:12:47,760 --> 01:12:49,280
or you're gonna mess up

1701
01:12:49,280 --> 01:12:51,360
and you have everything in between.

1702
01:12:51,360 --> 01:12:55,120
So the question becomes, you know,

1703
01:12:55,120 --> 01:12:56,680
can you set it up so that you can capture

1704
01:12:56,680 --> 01:12:58,720
that important context when you need it

1705
01:12:58,720 --> 01:13:01,040
and how much does that context interfere

1706
01:13:01,040 --> 01:13:01,880
of what you're doing?

1707
01:13:01,880 --> 01:13:04,040
I can definitely imagine a lot of cases

1708
01:13:04,040 --> 01:13:06,360
where somebody who is given

1709
01:13:06,360 --> 01:13:08,160
actually pretty irrelevant context

1710
01:13:09,240 --> 01:13:11,080
just ends up very distracted

1711
01:13:11,080 --> 01:13:12,600
from the actual task at hand

1712
01:13:12,600 --> 01:13:14,880
that ends up being much less productive, right?

1713
01:13:14,880 --> 01:13:17,840
As a human or to think of it as not in an AI

1714
01:13:17,840 --> 01:13:20,040
because the vibes don't mesh, right?

1715
01:13:20,040 --> 01:13:23,280
Which is basically the mechanism that I'm conjecturing, right?

1716
01:13:23,280 --> 01:13:26,360
The vibes don't mesh, they're distracting from each other.

1717
01:13:26,360 --> 01:13:28,440
Either bleeding, the tasks are bleeding into each other

1718
01:13:28,440 --> 01:13:29,920
in terms of the details and methods,

1719
01:13:29,920 --> 01:13:32,840
it's getting confused, they can't be sure they don't

1720
01:13:32,840 --> 01:13:33,800
which is makes sense

1721
01:13:33,800 --> 01:13:34,920
because like a lot often they would bleed

1722
01:13:34,920 --> 01:13:36,640
into each other in various ways.

1723
01:13:36,640 --> 01:13:37,680
So it has to be good enough

1724
01:13:37,680 --> 01:13:39,880
that the bleeds are where it makes sense to bleed

1725
01:13:39,880 --> 01:13:40,800
without being in the places

1726
01:13:40,800 --> 01:13:42,720
that don't make sense to bleed.

1727
01:13:42,720 --> 01:13:45,640
So you can imagine a world in which like what the AI does

1728
01:13:45,640 --> 01:13:49,000
is the ICs, you know, request one, two, three, four, five

1729
01:13:49,000 --> 01:13:50,920
either labeled as such or implicit

1730
01:13:50,920 --> 01:13:53,480
and then it breaks them down into individual things

1731
01:13:53,480 --> 01:13:56,400
that it virtually queries itself on its own

1732
01:13:56,400 --> 01:13:57,800
but knowing there are these other things

1733
01:13:57,800 --> 01:14:00,920
as proper context in the proper way.

1734
01:14:00,920 --> 01:14:02,440
I think the answer to that is

1735
01:14:02,440 --> 01:14:05,360
as you ask sufficiently capable people

1736
01:14:05,360 --> 01:14:07,880
or sufficiently capable AIs

1737
01:14:07,880 --> 01:14:09,760
to do increasingly complex things

1738
01:14:09,760 --> 01:14:13,200
at some point, if they have the capacity

1739
01:14:13,200 --> 01:14:15,640
they're going to do better if they have more information

1740
01:14:15,640 --> 01:14:18,120
they'll do better if they have more context.

1741
01:14:18,120 --> 01:14:19,920
If they are sufficiently more powerful

1742
01:14:19,920 --> 01:14:22,440
than the details of the task at hand in some sense

1743
01:14:22,440 --> 01:14:24,040
that threshold may or may not be anywhere near

1744
01:14:24,040 --> 01:14:25,800
where we are for different ways.

1745
01:14:25,800 --> 01:14:28,240
I would say, you know, one of the big advances

1746
01:14:28,240 --> 01:14:30,600
that I keep expecting to come

1747
01:14:30,600 --> 01:14:33,400
is you will type a query into an LLM

1748
01:14:33,400 --> 01:14:36,440
and then rather than the LLM literally

1749
01:14:36,440 --> 01:14:37,960
just outputting the answer to the query

1750
01:14:37,960 --> 01:14:40,680
what'll actually happen is we fed

1751
01:14:40,680 --> 01:14:43,600
with the proper scaffolding into a different LLM

1752
01:14:43,600 --> 01:14:46,520
that will evaluate what type of evaluation method

1753
01:14:46,520 --> 01:14:48,160
is to be used to evaluate your query

1754
01:14:48,160 --> 01:14:48,920
and sometimes it will be no

1755
01:14:48,920 --> 01:14:51,080
that's a normal query feed into the LLM.

1756
01:14:51,080 --> 01:14:52,600
Sometimes it will be this is a multi-part query

1757
01:14:52,600 --> 01:14:54,600
you should feed these separate things and separately

1758
01:14:54,600 --> 01:14:56,680
sometimes it'll be something else entirely.

1759
01:14:56,680 --> 01:14:58,880
And also which of my many LLM limitations

1760
01:14:58,880 --> 01:15:02,000
do I want to use so that I don't waste a too large model

1761
01:15:02,000 --> 01:15:03,720
that costs a lot of money

1762
01:15:03,720 --> 01:15:05,840
on something that's actually relatively narrow

1763
01:15:05,840 --> 01:15:06,720
and I direct this to the thing

1764
01:15:06,720 --> 01:15:07,840
that has a specialized knowledge

1765
01:15:07,840 --> 01:15:09,400
a specialized training specialized skills

1766
01:15:09,400 --> 01:15:12,440
for this type of request and so on.

1767
01:15:12,440 --> 01:15:13,520
And a lot of that is, you know

1768
01:15:13,520 --> 01:15:15,160
the fruits of the revolution

1769
01:15:15,160 --> 01:15:18,320
that will come in a year or two years, three years from now

1770
01:15:18,320 --> 01:15:20,280
regardless of whether or not we have fundamental advances

1771
01:15:20,280 --> 01:15:22,280
we just have to give it time.

1772
01:15:22,280 --> 01:15:26,120
So final question for the anthropic section

1773
01:15:26,120 --> 01:15:28,320
one of the things that as I was reading their

1774
01:15:28,320 --> 01:15:32,560
you know the profile that you based your analysis on

1775
01:15:32,560 --> 01:15:34,920
that jumped out to me as somebody who has a

1776
01:15:36,800 --> 01:15:38,680
fondness for red teaming activity

1777
01:15:38,680 --> 01:15:43,680
was that they're hiring a red team engineering type of role.

1778
01:15:44,360 --> 01:15:47,080
And I guess I wonder, you know

1779
01:15:47,080 --> 01:15:50,800
would you recommend somebody like me who, you know

1780
01:15:50,800 --> 01:15:55,200
is I think probably we share a lot of our worldview

1781
01:15:55,200 --> 01:15:58,240
and you know a lot of our kind of values

1782
01:15:58,240 --> 01:16:02,600
in terms of hopes and fears for how this all might go.

1783
01:16:02,600 --> 01:16:07,040
Would you recommend that somebody like me go and work there

1784
01:16:07,040 --> 01:16:08,800
or would you feel like, you know

1785
01:16:08,800 --> 01:16:10,300
as you said earlier you wouldn't want to send them

1786
01:16:10,300 --> 01:16:11,640
your research ideas.

1787
01:16:11,640 --> 01:16:14,200
Would you also not want to send them your friends

1788
01:16:14,200 --> 01:16:18,480
or would you say like, hey yeah maybe go get involved.

1789
01:16:18,480 --> 01:16:19,560
How do you think about that?

1790
01:16:19,560 --> 01:16:22,480
So it's very easy in these situations

1791
01:16:23,560 --> 01:16:28,160
to get in an action bias where you say to yourself

1792
01:16:29,280 --> 01:16:31,000
I don't want to encourage the thing

1793
01:16:31,000 --> 01:16:32,600
that might make things worse.

1794
01:16:32,600 --> 01:16:34,720
I want to be able to tell myself a story

1795
01:16:34,720 --> 01:16:37,360
that I only did things that make things better

1796
01:16:37,360 --> 01:16:41,280
even if that means your expected impact is a lot smaller.

1797
01:16:41,280 --> 01:16:42,600
It's also very easy to fool yourself

1798
01:16:42,600 --> 01:16:43,420
when you're thinking that you're helping

1799
01:16:43,420 --> 01:16:44,800
when you're actually enhancing capabilities.

1800
01:16:44,800 --> 01:16:46,800
You have to balance these two big concerns

1801
01:16:46,800 --> 01:16:49,240
and sources of bias against each other

1802
01:16:49,240 --> 01:16:51,400
when making this type of decision.

1803
01:16:51,400 --> 01:16:56,400
I would say I am relatively positive on open AI

1804
01:16:56,760 --> 01:16:58,620
and anthropic relative to where I was

1805
01:16:58,620 --> 01:17:01,320
when I started this Odyssey with AI number one

1806
01:17:01,320 --> 01:17:04,000
or even sort of been a way through at around 11

1807
01:17:04,000 --> 01:17:06,840
now that I've seen the developments, right.

1808
01:17:06,840 --> 01:17:09,560
Like I think that both of these organizations

1809
01:17:09,560 --> 01:17:14,520
now have a reasonable claim to be taking alignment seriously

1810
01:17:14,520 --> 01:17:16,440
such that if you can help with their alignment efforts

1811
01:17:16,440 --> 01:17:21,440
specifically in a way that you do not feel like obligated

1812
01:17:21,540 --> 01:17:25,140
to go along with adversity if you find it

1813
01:17:25,140 --> 01:17:27,440
and that you are able to stand up for and call out

1814
01:17:27,440 --> 01:17:29,640
stand up for what is right and call out

1815
01:17:29,640 --> 01:17:31,720
people who are being irresponsible

1816
01:17:31,720 --> 01:17:33,640
and you are willing to quit on a moment's notice

1817
01:17:33,640 --> 01:17:36,560
if something becomes serious enough

1818
01:17:36,560 --> 01:17:39,240
and you are willing to tell the world ideally, right.

1819
01:17:39,240 --> 01:17:43,440
That's why you did it and as much as possible what happened

1820
01:17:44,360 --> 01:17:46,360
then I think it is plausibly very positive.

1821
01:17:46,360 --> 01:17:50,600
I still would not feel comfortable working on capabilities

1822
01:17:50,600 --> 01:17:52,320
for any company.

1823
01:17:52,320 --> 01:17:55,160
And I still wouldn't want to give capabilities ideas

1824
01:17:55,160 --> 01:17:56,600
to any company.

1825
01:17:56,600 --> 01:17:58,320
But if I was confident it was specifically working

1826
01:17:58,320 --> 01:18:00,800
on alignment and like red teaming seems like one

1827
01:18:00,800 --> 01:18:04,680
of the places where you are most obviously being

1828
01:18:04,680 --> 01:18:07,520
a positive influence in that role.

1829
01:18:08,520 --> 01:18:10,760
And the question is like do you want to be the one

1830
01:18:10,760 --> 01:18:13,320
in that role or do you want someone else in that role

1831
01:18:13,320 --> 01:18:15,480
and how does this compare to your opportunity cost

1832
01:18:15,480 --> 01:18:16,560
of doing something else, right.

1833
01:18:16,560 --> 01:18:19,400
Like I think that I prefer the world

1834
01:18:19,400 --> 01:18:21,480
where there's a clone of you that didn't otherwise exist

1835
01:18:21,480 --> 01:18:25,120
who is working on that job and does nothing else all day

1836
01:18:25,120 --> 01:18:26,840
like goes home and watches television

1837
01:18:26,840 --> 01:18:29,520
like otherwise doesn't affect the world at night.

1838
01:18:30,600 --> 01:18:32,280
It doesn't mean that that is better than running

1839
01:18:32,280 --> 01:18:34,560
the cognitive revolution or doing any other number

1840
01:18:34,560 --> 01:18:37,280
of other things that you are currently doing

1841
01:18:37,280 --> 01:18:38,200
with your time.

1842
01:18:38,200 --> 01:18:40,800
And so you have to balance that, right.

1843
01:18:40,800 --> 01:18:43,520
And also any other opportunities that you might have.

1844
01:18:43,520 --> 01:18:46,880
So I don't think it's clear by any means

1845
01:18:46,880 --> 01:18:50,040
but I've definitely reached the point where

1846
01:18:50,040 --> 01:18:52,880
I wouldn't assume you were making a mistake, right.

1847
01:18:52,880 --> 01:18:55,080
If you did that, but you'd have to go

1848
01:18:55,080 --> 01:18:57,200
into the interview process with a very open mind.

1849
01:18:57,200 --> 01:19:00,240
You have to say, you know, I am deeply skeptical

1850
01:19:00,240 --> 01:19:03,200
that any organization including you is going

1851
01:19:03,200 --> 01:19:04,760
to be that helpful is nicking

1852
01:19:04,760 --> 01:19:06,720
as necessary precautions is treating the problem

1853
01:19:06,720 --> 01:19:09,480
as difficult and serious as it actually is.

1854
01:19:09,480 --> 01:19:11,560
Is doing things that actually solve the hard problems

1855
01:19:11,560 --> 01:19:14,560
and not the easy problems is not just enhancing capabilities

1856
01:19:14,560 --> 01:19:17,240
regardless of their intentions, et cetera, et cetera.

1857
01:19:17,240 --> 01:19:19,760
The interview process is what it should be always

1858
01:19:19,760 --> 01:19:23,000
in every job with a two way process, right.

1859
01:19:23,000 --> 01:19:25,880
They are interviewing you and you are interviewing them.

1860
01:19:25,880 --> 01:19:27,600
Right, you are watching what questions they ask

1861
01:19:27,600 --> 01:19:30,800
and how they react to your reactions and your responses

1862
01:19:30,800 --> 01:19:32,720
and you are asking them questions.

1863
01:19:32,760 --> 01:19:35,680
And you want to know, would this in fact be a good thing

1864
01:19:35,680 --> 01:19:39,880
for the world if I got and took this job or not, right.

1865
01:19:39,880 --> 01:19:41,160
Cause I don't believe in taking jobs

1866
01:19:41,160 --> 01:19:43,560
in order to sabotage people, right.

1867
01:19:43,560 --> 01:19:45,440
Like you don't show up in order to not red team them.

1868
01:19:45,440 --> 01:19:46,440
I mean, certainly this is one job

1869
01:19:46,440 --> 01:19:47,640
you wouldn't want to sabotage.

1870
01:19:47,640 --> 01:19:50,480
Yeah, safe to say that is right now.

1871
01:19:50,480 --> 01:19:51,800
That's the considerations.

1872
01:19:53,160 --> 01:19:55,040
And yeah, I think I'm in a similar spot.

1873
01:19:55,040 --> 01:19:58,920
You know, six months ago plus I was really,

1874
01:19:58,920 --> 01:20:00,960
especially with respect to open AI.

1875
01:20:00,960 --> 01:20:04,680
I was like, this seems like what is going on

1876
01:20:04,680 --> 01:20:09,560
and do they have anybody like really approaching

1877
01:20:09,560 --> 01:20:11,080
this in a serious way?

1878
01:20:11,080 --> 01:20:12,760
As it turned out, like they did have a lot more

1879
01:20:12,760 --> 01:20:14,480
than had met the eye at that point and gradually

1880
01:20:14,480 --> 01:20:16,360
they've revealed it that I've definitely updated

1881
01:20:16,360 --> 01:20:19,960
my point of view on, I'm really all of the leaders

1882
01:20:19,960 --> 01:20:22,840
in a pretty positive way over the last few months.

1883
01:20:22,840 --> 01:20:26,080
I think, you know, if anything, they've probably,

1884
01:20:26,080 --> 01:20:27,360
some of them maybe were, you know,

1885
01:20:27,360 --> 01:20:29,160
expecting this much progress this fast.

1886
01:20:29,160 --> 01:20:31,240
I have to imagine that even internally,

1887
01:20:31,240 --> 01:20:34,400
a lot of them are kind of surprised by just how,

1888
01:20:34,400 --> 01:20:37,280
you know, far the scaling loss have extended

1889
01:20:37,280 --> 01:20:39,880
and how, you know, how quick on the calendar

1890
01:20:39,880 --> 01:20:42,360
they've hit some of these milestones.

1891
01:20:42,360 --> 01:20:45,200
And, you know, I do think they've handled it

1892
01:20:45,200 --> 01:20:47,560
pretty well over the last few months.

1893
01:20:47,560 --> 01:20:50,760
Yeah, I would say I am positively updating

1894
01:20:50,760 --> 01:20:52,200
on all three major labs.

1895
01:20:52,200 --> 01:20:56,560
And most everyone at the media life that is relevant.

1896
01:20:56,560 --> 01:21:00,360
My negative updates have been in other places, right?

1897
01:21:00,360 --> 01:21:03,080
Like, and mostly I've been pleasantly surprised

1898
01:21:03,080 --> 01:21:03,920
by government.

1899
01:21:03,920 --> 01:21:06,760
I've mostly been pleasantly surprised by public reaction.

1900
01:21:06,760 --> 01:21:09,440
You know, there's definitely people who disappointed me,

1901
01:21:09,440 --> 01:21:13,480
but mostly things are going vastly better

1902
01:21:13,480 --> 01:21:16,120
than I would have expected when I started down this road.

1903
01:21:16,120 --> 01:21:19,680
And I'm much more hopeful that we can make better decisions.

1904
01:21:19,680 --> 01:21:21,680
I'm not sure how much that translates into, you know,

1905
01:21:21,680 --> 01:21:23,400
P of survival going up that much,

1906
01:21:23,400 --> 01:21:27,200
but I think this is definitely going better

1907
01:21:27,200 --> 01:21:28,040
than I expected.

1908
01:21:28,040 --> 01:21:28,880
That's great.

1909
01:21:28,880 --> 01:21:30,200
It's good to have a little, you know,

1910
01:21:30,200 --> 01:21:32,840
a little positive note from someone

1911
01:21:32,840 --> 01:21:34,840
that some might call a doomer.

1912
01:21:34,840 --> 01:21:37,200
Let's turn to Google in DeepMind, our third,

1913
01:21:37,200 --> 01:21:40,840
as you said, of the three leaders.

1914
01:21:40,840 --> 01:21:42,680
I don't know if there's any like super headline news.

1915
01:21:42,680 --> 01:21:45,280
I mean, the last week it's one of these things

1916
01:21:45,280 --> 01:21:47,040
where it's like a year ago,

1917
01:21:47,040 --> 01:21:49,240
some of this stuff would have felt

1918
01:21:49,240 --> 01:21:52,120
like an absolute bombshell announcement.

1919
01:21:52,120 --> 01:21:54,240
And now it's like, I kind of expected that

1920
01:21:54,240 --> 01:21:55,080
to happen about now.

1921
01:21:55,080 --> 01:21:56,960
And there's two examples of that.

1922
01:21:56,960 --> 01:22:00,400
One being the latest robotics paper

1923
01:22:00,400 --> 01:22:03,160
that they came out with on Friday,

1924
01:22:03,160 --> 01:22:05,240
which, you know, extends and kind of unifies

1925
01:22:05,240 --> 01:22:07,280
all the work that they've been doing,

1926
01:22:07,280 --> 01:22:11,960
where now you have robots that can follow instructions

1927
01:22:11,960 --> 01:22:14,120
that have this kind of, you know,

1928
01:22:14,120 --> 01:22:16,920
language model in a loop sort of structure,

1929
01:22:16,920 --> 01:22:18,880
kind of unified, simplified the architecture a little bit.

1930
01:22:18,880 --> 01:22:22,040
Now the language model is just kind of outputting commands

1931
01:22:22,040 --> 01:22:23,640
for the robot body.

1932
01:22:23,640 --> 01:22:25,840
And so they've like eliminated a few,

1933
01:22:25,840 --> 01:22:26,840
maybe I don't know how many,

1934
01:22:26,840 --> 01:22:30,120
but they've eliminated sort of certain layers of control

1935
01:22:30,120 --> 01:22:33,000
and kind of just simplified the overall structure.

1936
01:22:33,000 --> 01:22:35,080
And then what's making probably the most headlines there

1937
01:22:35,080 --> 01:22:38,040
is the conceptual understanding

1938
01:22:38,040 --> 01:22:39,920
that the robots are now able to show,

1939
01:22:39,920 --> 01:22:42,560
which is basically the exact same thing that the,

1940
01:22:42,560 --> 01:22:43,560
you know, the language models

1941
01:22:43,560 --> 01:22:46,720
or the multimodal language models have already shown.

1942
01:22:46,720 --> 01:22:48,880
So they've got demos where it's like, you know,

1943
01:22:48,880 --> 01:22:51,280
move this object to the Denver Nuggets.

1944
01:22:51,280 --> 01:22:53,000
And then they've got, you know, from the recent,

1945
01:22:53,000 --> 01:22:54,960
they were obviously doing this during the NBA finals,

1946
01:22:54,960 --> 01:22:57,560
they have the Miami Heat logo and the Nuggets logo.

1947
01:22:57,560 --> 01:23:01,880
And the thing knows based on understanding that language,

1948
01:23:01,880 --> 01:23:03,880
also knowing what the logo looks like.

1949
01:23:03,880 --> 01:23:06,120
And obviously, you know, being able to command the robot arm

1950
01:23:06,120 --> 01:23:07,960
can actually do that task.

1951
01:23:07,960 --> 01:23:09,520
So you've got these kind of,

1952
01:23:09,520 --> 01:23:13,240
another one that they said was pick up the extinct animal.

1953
01:23:13,240 --> 01:23:14,080
And they've got, you know,

1954
01:23:14,080 --> 01:23:16,520
an array of kind of plastic toys on the table

1955
01:23:16,520 --> 01:23:17,840
and it will pick up the dinosaur

1956
01:23:17,880 --> 01:23:19,520
because it understands, you know,

1957
01:23:19,520 --> 01:23:21,600
that that is the extinct animal.

1958
01:23:21,600 --> 01:23:25,160
So these, from the perspective of certainly two years ago,

1959
01:23:25,160 --> 01:23:29,520
even one year ago, feels like Jetsons type robots.

1960
01:23:29,520 --> 01:23:32,360
Now it's kind of like, yeah, pretty much expected

1961
01:23:32,360 --> 01:23:35,320
that these different modalities would be bridged

1962
01:23:35,320 --> 01:23:36,280
right around this time.

1963
01:23:36,280 --> 01:23:37,720
And sure enough, it's happening.

1964
01:23:37,720 --> 01:23:41,000
Anything else to add on the robotics?

1965
01:23:41,000 --> 01:23:42,920
Yeah, I read the robotics.

1966
01:23:42,920 --> 01:23:46,400
And of course, whenever anyone had the advances in robotics,

1967
01:23:46,400 --> 01:23:49,200
the answer is, oh, that seems fine, not dangerous,

1968
01:23:49,200 --> 01:23:51,680
not scary at all, all cool.

1969
01:23:51,680 --> 01:23:53,520
But in this case, yeah, it seemed like,

1970
01:23:53,520 --> 01:23:54,920
of course you could do that.

1971
01:23:54,920 --> 01:23:57,440
You're combining things that you already did

1972
01:23:57,440 --> 01:23:58,720
and you're getting the inevitable results

1973
01:23:58,720 --> 01:23:59,640
of combining them.

1974
01:23:59,640 --> 01:24:02,360
And that's not me knocking you

1975
01:24:02,360 --> 01:24:03,760
for doing something you shouldn't have done.

1976
01:24:03,760 --> 01:24:06,120
That's just, okay, yeah, of course.

1977
01:24:06,120 --> 01:24:07,880
Like that's the next step.

1978
01:24:07,880 --> 01:24:09,600
And in kind of like,

1979
01:24:09,600 --> 01:24:12,680
for someone who doesn't want capabilities to go that fast,

1980
01:24:12,680 --> 01:24:13,760
you're happy to see that kind of paper

1981
01:24:13,760 --> 01:24:15,040
because that's the paper that says,

1982
01:24:15,040 --> 01:24:16,680
I'm gonna do the things that I already,

1983
01:24:16,680 --> 01:24:18,760
you already knew I could do.

1984
01:24:18,760 --> 01:24:20,560
Right, and you ran outside and like, okay, cool.

1985
01:24:20,560 --> 01:24:21,960
And if that turns out to be useful, great.

1986
01:24:21,960 --> 01:24:23,960
But like, yeah, I knew that LLMs

1987
01:24:23,960 --> 01:24:26,080
could interpret human commands in these ways.

1988
01:24:26,080 --> 01:24:27,480
And I knew that robots could execute

1989
01:24:27,480 --> 01:24:28,720
these types of movements.

1990
01:24:28,720 --> 01:24:33,080
So why should I be more scared than I was before

1991
01:24:33,080 --> 01:24:33,920
instead of less scared?

1992
01:24:33,920 --> 01:24:35,080
I should be slightly less scared.

1993
01:24:35,080 --> 01:24:38,440
Probably a lot of people in the public though feel,

1994
01:24:38,440 --> 01:24:40,840
especially if you're not obsessed with this as we are,

1995
01:24:40,840 --> 01:24:42,880
you might feel like,

1996
01:24:42,880 --> 01:24:44,120
if there is a news item here,

1997
01:24:44,120 --> 01:24:49,000
it's like some sort of qualitative, conceptual understanding

1998
01:24:49,000 --> 01:24:51,800
now has embodied form.

1999
01:24:51,800 --> 01:24:54,720
Now you can imagine bringing your jail breaks

2000
01:24:54,720 --> 01:24:56,640
to your robot commands.

2001
01:24:56,640 --> 01:25:00,920
And if you could verbalize some of those strange strings

2002
01:25:00,920 --> 01:25:03,040
that we were mentioning earlier,

2003
01:25:03,040 --> 01:25:06,360
now what might your robot be willing to do, right?

2004
01:25:06,360 --> 01:25:08,040
I mean, would it go smash stuff?

2005
01:25:08,040 --> 01:25:12,880
Would it go corner somebody in a room?

2006
01:25:12,920 --> 01:25:17,280
The system as a whole has the conceptual understanding

2007
01:25:17,280 --> 01:25:19,480
to kind of begin,

2008
01:25:19,480 --> 01:25:21,560
it has the same kind of proto morality or whatever

2009
01:25:21,560 --> 01:25:25,480
that the core language models have.

2010
01:25:25,480 --> 01:25:29,000
And that can go awry in similar ways.

2011
01:25:29,000 --> 01:25:32,080
And now you can probably get some pretty scary demos

2012
01:25:32,080 --> 01:25:33,560
out of these robots,

2013
01:25:33,560 --> 01:25:35,440
which I don't think Google's gonna be racing

2014
01:25:35,440 --> 01:25:36,760
to publish likely,

2015
01:25:36,760 --> 01:25:39,760
but there is something kind of qualitatively different

2016
01:25:39,760 --> 01:25:40,880
about that.

2017
01:25:40,880 --> 01:25:42,600
Yeah, so I like to think of this

2018
01:25:42,600 --> 01:25:45,040
as the game of good news, bad news,

2019
01:25:45,040 --> 01:25:46,360
but there's two games of good news, bad news.

2020
01:25:46,360 --> 01:25:47,840
The doctor, I say, I have some good news

2021
01:25:47,840 --> 01:25:50,080
and I have some bad news, and that's always fun.

2022
01:25:50,080 --> 01:25:51,640
But there's also the game of,

2023
01:25:51,640 --> 01:25:54,440
is this good news or is this bad news?

2024
01:25:54,440 --> 01:25:56,680
Because it depends on what you previously thought, right?

2025
01:25:56,680 --> 01:25:58,680
Like you have the law of conservation

2026
01:25:58,680 --> 01:26:00,200
of expected updating, right?

2027
01:26:00,200 --> 01:26:02,760
So like if you get news,

2028
01:26:02,760 --> 01:26:07,200
you should on average not update for or against anything

2029
01:26:07,200 --> 01:26:10,040
or to make things are better or worse in any way,

2030
01:26:10,080 --> 01:26:12,720
because you already had your expectations baked in.

2031
01:26:13,680 --> 01:26:15,120
So in the case of robotics,

2032
01:26:15,120 --> 01:26:17,840
like if you're not paying attention to robotics

2033
01:26:17,840 --> 01:26:19,120
and you think that robotics is just,

2034
01:26:19,120 --> 01:26:22,160
oh, robotics is hard, mysterious, there'd be dragons,

2035
01:26:22,160 --> 01:26:24,040
we will never have robots,

2036
01:26:24,040 --> 01:26:26,080
the same way we'll never have dragons,

2037
01:26:26,080 --> 01:26:28,440
then every little advance in robotics is like,

2038
01:26:28,440 --> 01:26:31,960
eek, you know, slight extra worry.

2039
01:26:31,960 --> 01:26:35,160
But if you knew that robotics was just another tack

2040
01:26:35,160 --> 01:26:36,000
like any other,

2041
01:26:36,000 --> 01:26:38,640
and of course we will eventually have robotics,

2042
01:26:38,640 --> 01:26:40,240
then you have to look at the details

2043
01:26:40,240 --> 01:26:41,640
of what you're looking at and you say,

2044
01:26:41,640 --> 01:26:43,440
oh, okay, this is fine.

2045
01:26:43,440 --> 01:26:45,800
So I'm pointing the game of mild,

2046
01:26:45,800 --> 01:26:48,480
I interpret this one as mild goodness, right?

2047
01:26:48,480 --> 01:26:50,880
Like in terms of robotics, not advancing so fast.

2048
01:26:50,880 --> 01:26:52,400
And of course, you also have the issue of,

2049
01:26:52,400 --> 01:26:53,240
you know, if you're somebody who wants

2050
01:26:53,240 --> 01:26:54,080
there to be more robotics,

2051
01:26:54,080 --> 01:26:56,680
then you might say that this is bad news, right?

2052
01:26:56,680 --> 01:26:57,920
Like that you wanted to see

2053
01:26:57,920 --> 01:26:59,800
lots of cool robotics advances and you didn't.

2054
01:26:59,800 --> 01:27:04,160
But yeah, I'd say also I wanna see

2055
01:27:04,160 --> 01:27:06,680
the ultimately harmless robotics advances

2056
01:27:06,680 --> 01:27:08,280
as quickly as possible.

2057
01:27:08,280 --> 01:27:10,460
Exactly because it makes it so much easier

2058
01:27:10,460 --> 01:27:13,280
for people to see what might happen

2059
01:27:13,280 --> 01:27:14,760
and what might go wrong.

2060
01:27:14,760 --> 01:27:18,920
People get hung up on, oh, but the AI won't have a body.

2061
01:27:18,920 --> 01:27:20,200
Oh, but the AI won't be able to move things

2062
01:27:20,200 --> 01:27:21,880
in the physical world,

2063
01:27:21,880 --> 01:27:24,660
as if this would ultimately ever be the barrier

2064
01:27:24,660 --> 01:27:27,160
that saves us in any real way, right?

2065
01:27:27,160 --> 01:27:28,600
Which it won't.

2066
01:27:28,600 --> 01:27:32,160
It's at best a temporary inconvenience

2067
01:27:32,160 --> 01:27:36,200
that requires someone to be slightly more clever

2068
01:27:36,200 --> 01:27:39,640
about what they do as an AI in order to get around stuff.

2069
01:27:39,640 --> 01:27:42,720
But it's not ever going to actually matter

2070
01:27:42,720 --> 01:27:44,240
in some point sense.

2071
01:27:44,240 --> 01:27:45,080
So the other big one,

2072
01:27:45,080 --> 01:27:47,200
and this is definitely one that I, you know,

2073
01:27:47,200 --> 01:27:51,160
I'm happy to say I'm ready to accelerate on

2074
01:27:51,160 --> 01:27:52,240
for practical purposes,

2075
01:27:52,240 --> 01:27:57,240
is their new multimodal med palm.

2076
01:27:57,480 --> 01:27:59,940
This builds on palm and med palm

2077
01:27:59,940 --> 01:28:02,600
and also actually on the earlier palm E

2078
01:28:02,600 --> 01:28:04,680
because that was kind of the multimodal.

2079
01:28:04,680 --> 01:28:06,680
So it's, it is interesting to see, you know,

2080
01:28:06,680 --> 01:28:10,440
I'd say zooming out from these individual papers

2081
01:28:10,440 --> 01:28:14,640
and just characterizing Google DeepMind as a whole right now,

2082
01:28:14,640 --> 01:28:19,520
it seems like they're firing on, you know, all cylinders.

2083
01:28:19,520 --> 01:28:22,320
Like it does not seem like, you know,

2084
01:28:22,320 --> 01:28:24,560
whatever sort of concerns folks might have had

2085
01:28:24,560 --> 01:28:26,200
about, oh, there's a million fiefdoms

2086
01:28:26,200 --> 01:28:28,600
and the groups don't talk to each other or whatever.

2087
01:28:28,600 --> 01:28:31,580
Like we're seeing papers and, you know,

2088
01:28:31,580 --> 01:28:34,620
projects building on one another at a pretty fast clip

2089
01:28:34,620 --> 01:28:38,660
that suggests like pretty effective, you know,

2090
01:28:38,660 --> 01:28:40,740
dividing and conquering and then coming back together

2091
01:28:40,740 --> 01:28:43,020
and sharing improvements.

2092
01:28:43,020 --> 01:28:45,580
So it seems like the output is just strong,

2093
01:28:45,580 --> 01:28:46,780
you know, whether you like it or not.

2094
01:28:46,780 --> 01:28:49,300
You have to look at the actual value

2095
01:28:49,300 --> 01:28:50,380
of the things being outputted, right?

2096
01:28:50,380 --> 01:28:52,940
Like the mistake you always can make in science

2097
01:28:52,940 --> 01:28:55,620
is to ask who is publishing the most papers,

2098
01:28:55,620 --> 01:28:57,620
who has reliably published a paper,

2099
01:28:57,620 --> 01:28:59,380
and then you have your scientists scrambling

2100
01:28:59,380 --> 01:29:01,100
to always publish as many papers as possible

2101
01:29:01,100 --> 01:29:03,060
and then no real science ever gets done, right?

2102
01:29:03,060 --> 01:29:03,980
And it's not their fault.

2103
01:29:03,980 --> 01:29:05,420
They just weren't given the affordances

2104
01:29:05,420 --> 01:29:07,500
to do breakthrough work.

2105
01:29:07,500 --> 01:29:11,780
And simultaneously, you know, you have to ask,

2106
01:29:11,780 --> 01:29:13,900
does any of this actually ultimately matter

2107
01:29:13,900 --> 01:29:18,580
on the scale of what is going to determine the big game?

2108
01:29:18,580 --> 01:29:21,580
And like I'm happy to see advances in the med tools

2109
01:29:21,580 --> 01:29:23,060
and it bodes well for them.

2110
01:29:23,060 --> 01:29:25,060
They made marginal advances in AI

2111
01:29:25,060 --> 01:29:26,980
and they had some other public papers published too,

2112
01:29:26,980 --> 01:29:27,820
some of which I was like,

2113
01:29:27,820 --> 01:29:28,780
why the hell are you publishing this?

2114
01:29:28,780 --> 01:29:30,800
You are a corporation that is for profit.

2115
01:29:30,800 --> 01:29:32,380
Even if you don't think of the safety issue here,

2116
01:29:32,380 --> 01:29:35,020
you should know better, like keep that secret to yourself

2117
01:29:35,020 --> 01:29:36,100
and either to beat the competition,

2118
01:29:36,100 --> 01:29:37,500
what's wrong with you?

2119
01:29:37,500 --> 01:29:38,860
The last point you've written down

2120
01:29:38,860 --> 01:29:40,860
is Gemini question mark, question mark.

2121
01:29:42,300 --> 01:29:43,900
And let's tie that in, right?

2122
01:29:43,900 --> 01:29:47,780
Because ultimately speaking,

2123
01:29:47,780 --> 01:29:49,780
it is going to be August tomorrow.

2124
01:29:49,780 --> 01:29:51,540
GPT-4 has been out for many months

2125
01:29:52,660 --> 01:29:54,540
and bars still sucks, right?

2126
01:29:54,540 --> 01:29:58,380
And the Gmail generative offering is bad

2127
01:29:58,380 --> 01:30:01,780
and the G-Docs offering is bad

2128
01:30:01,780 --> 01:30:05,260
because they're offering,

2129
01:30:05,260 --> 01:30:09,620
no matter how customized and narrowed and bespoke,

2130
01:30:09,620 --> 01:30:11,380
simply doesn't have the G.

2131
01:30:12,260 --> 01:30:14,020
It's not a good enough core thing.

2132
01:30:14,020 --> 01:30:16,700
It's also still making remarkably many

2133
01:30:16,700 --> 01:30:19,500
elementary stupid mistakes, right?

2134
01:30:19,500 --> 01:30:22,260
That even a low G system really shouldn't make,

2135
01:30:22,260 --> 01:30:24,220
their act is not together.

2136
01:30:24,220 --> 01:30:27,180
And to the extent that they are instead publishing

2137
01:30:27,180 --> 01:30:28,260
a bunch of quirky papers

2138
01:30:28,260 --> 01:30:31,900
with a bunch of like narrow applications,

2139
01:30:32,900 --> 01:30:35,660
that could be seen as well, look, Google ships,

2140
01:30:35,660 --> 01:30:37,180
but also it means Google is not shipping

2141
01:30:37,180 --> 01:30:38,460
the thing it needs to ship, right?

2142
01:30:38,460 --> 01:30:41,940
Like Google desperately needs

2143
01:30:41,940 --> 01:30:44,020
from their perspective to ship Gemini.

2144
01:30:44,020 --> 01:30:46,140
And like it takes a level long, it takes.

2145
01:30:46,140 --> 01:30:48,860
It takes them however much computed it requires.

2146
01:30:48,860 --> 01:30:52,780
But ultimately speaking, the test is,

2147
01:30:52,780 --> 01:30:57,780
can they produce the equal or better of GPT-4

2148
01:30:57,900 --> 01:31:00,380
now that they know that's what they need to do?

2149
01:31:00,380 --> 01:31:03,340
Because if you looked at the previous reputation of Google

2150
01:31:03,340 --> 01:31:05,060
and DeepMind and what they were capable of,

2151
01:31:05,060 --> 01:31:06,780
you would think that they would be ahead

2152
01:31:07,980 --> 01:31:10,020
on that front if they wanted to be.

2153
01:31:10,020 --> 01:31:12,460
And now that they know what they have to do,

2154
01:31:12,460 --> 01:31:15,500
do it to make it like commercial ready, right?

2155
01:31:15,500 --> 01:31:18,820
Ready for regular people, that should not be so difficult.

2156
01:31:18,820 --> 01:31:21,780
But then again, like we can think about how long it took,

2157
01:31:21,780 --> 01:31:23,580
like took like six months or so

2158
01:31:23,580 --> 01:31:25,500
after GPT-4 was finished training

2159
01:31:25,500 --> 01:31:26,860
before they were ready to release

2160
01:31:26,860 --> 01:31:29,300
even the earliest version of it, right?

2161
01:31:29,300 --> 01:31:31,900
And then they still rolled out a lot of its capabilities.

2162
01:31:31,900 --> 01:31:35,580
So even if Gemini finished tomorrow, right?

2163
01:31:35,580 --> 01:31:37,660
How many months are they gonna need

2164
01:31:37,660 --> 01:31:40,340
before they feel comfortable releasing Gemini?

2165
01:31:40,340 --> 01:31:43,220
Because Google was much more risk averse than OpenAI

2166
01:31:43,220 --> 01:31:44,580
as a company in the culture.

2167
01:31:44,580 --> 01:31:46,300
Who knows when that's gonna happen.

2168
01:31:46,300 --> 01:31:49,780
It's been longer than I thought.

2169
01:31:49,780 --> 01:31:51,580
You know, in my Scouting Report,

2170
01:31:51,580 --> 01:31:54,060
I have this clip of Demis Asavis

2171
01:31:54,060 --> 01:31:57,620
just after Gato paper was published

2172
01:31:57,620 --> 01:32:01,860
saying that, you know, of course we can scale this up as well.

2173
01:32:01,860 --> 01:32:03,060
And we're in the process of doing that.

2174
01:32:03,060 --> 01:32:07,100
I believe that was April of maybe May of 2022

2175
01:32:07,100 --> 01:32:08,820
has been over a year.

2176
01:32:08,820 --> 01:32:11,220
And typically we don't have to wait a year plus

2177
01:32:11,220 --> 01:32:14,860
to get the successor, you know, to a thing like that

2178
01:32:14,860 --> 01:32:17,100
that, you know, is just about being scaled up.

2179
01:32:17,100 --> 01:32:18,300
So I've been really kind of wondering

2180
01:32:18,340 --> 01:32:23,060
what is going on behind the scenes there.

2181
01:32:23,060 --> 01:32:25,700
But I also do wanna turn back to the med thing as well.

2182
01:32:25,700 --> 01:32:27,100
So I'll give you first,

2183
01:32:27,100 --> 01:32:29,300
would you care to speculate about Gato too?

2184
01:32:29,300 --> 01:32:31,420
Is Gemini Gato too?

2185
01:32:31,420 --> 01:32:35,340
As a shareholder, I am concerned, right?

2186
01:32:35,340 --> 01:32:38,860
And I also have Microsoft, but I am concerned

2187
01:32:38,860 --> 01:32:41,460
that their act is not together

2188
01:32:41,460 --> 01:32:44,220
and that we're not seeing the kind of progress.

2189
01:32:44,220 --> 01:32:45,980
Like we're not making the incremental announcements

2190
01:32:46,020 --> 01:32:48,500
that I would make if I was their marketing department

2191
01:32:48,500 --> 01:32:51,140
and I was moving towards the rapid clip, you know,

2192
01:32:51,140 --> 01:32:52,940
as a person who wants the world to be okay,

2193
01:32:52,940 --> 01:32:54,860
I'm not sure how much I mind,

2194
01:32:54,860 --> 01:32:57,100
but it is pretty troubling

2195
01:32:57,100 --> 01:32:59,140
that they can't get their act together.

2196
01:32:59,140 --> 01:33:03,780
I was really excited for Google suite integration

2197
01:33:03,780 --> 01:33:05,260
when I first heard the announcements

2198
01:33:05,260 --> 01:33:09,220
of Microsoft Copilot and, you know, Google interactive.

2199
01:33:09,220 --> 01:33:10,620
And yet when I got Google interactive,

2200
01:33:10,620 --> 01:33:12,940
I tried a handful of things

2201
01:33:12,940 --> 01:33:15,260
and then quickly realized in their current forms,

2202
01:33:15,260 --> 01:33:17,420
I don't have any use for them.

2203
01:33:17,420 --> 01:33:19,100
They don't do anything, right?

2204
01:33:19,100 --> 01:33:20,580
Like the first thing I tried to do with Google Docs

2205
01:33:20,580 --> 01:33:23,700
was I tried to paste my article in.

2206
01:33:23,700 --> 01:33:27,340
And then I said, you know, to summarize this article

2207
01:33:27,340 --> 01:33:30,220
or otherwise get to do the obviously first things

2208
01:33:30,220 --> 01:33:32,820
and just fell completely on its face.

2209
01:33:32,820 --> 01:33:34,100
You're just like, I can't assist with that.

2210
01:33:34,100 --> 01:33:36,180
It's like, well, then you're useless.

2211
01:33:36,180 --> 01:33:39,380
Or if you can't even read the context of the document

2212
01:33:39,380 --> 01:33:42,900
that I gave you specifically, like why am I even here?

2213
01:33:42,900 --> 01:33:44,220
And like for email, it's like, no,

2214
01:33:44,220 --> 01:33:45,300
by the time I figure out what I want

2215
01:33:45,300 --> 01:33:47,460
and type in the detailed request into you,

2216
01:33:47,460 --> 01:33:49,460
I could have just written my email right now.

2217
01:33:49,460 --> 01:33:51,500
Like where are the emails where I want to spend

2218
01:33:51,500 --> 01:33:53,660
the kind of time required to customize the output

2219
01:33:53,660 --> 01:33:56,500
but don't want to actually customize the output carefully?

2220
01:33:56,500 --> 01:33:58,460
This is just the empty set.

2221
01:33:58,460 --> 01:34:00,500
But like, when does this come off?

2222
01:34:00,500 --> 01:34:02,940
And so that was like a rude awakening as well.

2223
01:34:02,940 --> 01:34:05,180
Yeah, those deployments have not been very good yet,

2224
01:34:05,180 --> 01:34:08,300
but going back to the med one for a second,

2225
01:34:08,300 --> 01:34:09,980
this may be an area where we may have

2226
01:34:09,980 --> 01:34:11,820
some different expectations

2227
01:34:11,820 --> 01:34:14,460
because reading through that paper,

2228
01:34:14,460 --> 01:34:15,860
and I haven't studied it in depth yet,

2229
01:34:15,860 --> 01:34:18,620
but the headline statistics along the lines of,

2230
01:34:18,620 --> 01:34:20,460
first of all, it's a multimodal system.

2231
01:34:20,460 --> 01:34:23,820
The last version of MedPalm 2 were all text.

2232
01:34:23,820 --> 01:34:25,740
So you could ask it your medical questions

2233
01:34:25,740 --> 01:34:27,980
and they had announced expert level

2234
01:34:27,980 --> 01:34:30,180
answering of your medical questions.

2235
01:34:30,180 --> 01:34:33,540
And they'd evaluated that seemingly pretty carefully

2236
01:34:33,540 --> 01:34:35,060
with a bunch of different dimensions

2237
01:34:35,060 --> 01:34:38,740
and having human doctors compare for accuracy

2238
01:34:38,740 --> 01:34:41,500
and all these other things you might care about, right?

2239
01:34:41,540 --> 01:34:44,860
And that the AI, as of MedPalm 2,

2240
01:34:44,860 --> 01:34:47,580
was beating the human doctor responses

2241
01:34:47,580 --> 01:34:51,580
on eight out of nine of those evaluation categories.

2242
01:34:51,580 --> 01:34:54,700
So it seemed like, okay, that's pretty good.

2243
01:34:54,700 --> 01:34:56,020
Now they haven't released it,

2244
01:34:56,020 --> 01:34:57,940
but it's in limited access

2245
01:34:57,940 --> 01:35:01,100
for trusted hospital partners or whatever.

2246
01:35:01,100 --> 01:35:03,780
Now with the next version, it's multimodal as well.

2247
01:35:03,780 --> 01:35:08,460
So you can do things like feed in a pathology image

2248
01:35:08,460 --> 01:35:09,980
alongside the text.

2249
01:35:09,980 --> 01:35:12,940
Pathology would be like somebody has a tissue biopsy,

2250
01:35:12,940 --> 01:35:15,420
we did an episode on this actually with a narrow system

2251
01:35:15,420 --> 01:35:17,060
from Tanishk, Matthew Abraham,

2252
01:35:17,060 --> 01:35:20,840
who did this with small data too, which was super cool.

2253
01:35:20,840 --> 01:35:23,500
But somebody has a tissue biopsy,

2254
01:35:23,500 --> 01:35:26,680
that tissue has been sliced, has been plated on a slide.

2255
01:35:26,680 --> 01:35:28,780
Now it's been imaged and they can feed that

2256
01:35:28,780 --> 01:35:30,860
along in with the case history.

2257
01:35:30,860 --> 01:35:35,580
And for that matter, you can handle radiology scans

2258
01:35:35,580 --> 01:35:38,460
and all these kind of other different sorts of inputs

2259
01:35:38,460 --> 01:35:42,940
that are obviously key to the actual practice of medicine.

2260
01:35:42,940 --> 01:35:44,860
And then they say things like,

2261
01:35:44,860 --> 01:35:49,180
our radiology reports out of the model

2262
01:35:49,180 --> 01:35:53,180
were preferred to a human radiologist report

2263
01:35:53,180 --> 01:35:55,060
some 40 plus percent of the time.

2264
01:35:55,060 --> 01:35:58,580
So like almost half, basically seems like

2265
01:35:58,580 --> 01:36:02,180
it's very much on par with the human radiologist,

2266
01:36:02,180 --> 01:36:03,780
which of course is like the canonical thing

2267
01:36:03,780 --> 01:36:06,260
that people have been saying for 10 years,

2268
01:36:06,260 --> 01:36:07,900
people have been saying that radiology

2269
01:36:07,900 --> 01:36:10,140
would be the first thing to be impacted.

2270
01:36:10,140 --> 01:36:11,700
And then for the last like three months,

2271
01:36:11,700 --> 01:36:13,220
it's become kind of a talking point that like,

2272
01:36:13,220 --> 01:36:14,900
well, radiology still hasn't been impacted.

2273
01:36:14,900 --> 01:36:16,460
So, and now all of a sudden it looks like

2274
01:36:16,460 --> 01:36:20,660
we're hitting maybe radiology being impacted.

2275
01:36:20,660 --> 01:36:23,880
But I kind of expect that that thing works pretty well.

2276
01:36:23,880 --> 01:36:25,820
It sounds like you maybe are a little more skeptical

2277
01:36:25,820 --> 01:36:28,740
of like whether it actually has real utility.

2278
01:36:28,740 --> 01:36:31,220
Well, I mean, you definitely don't want to tempt fate

2279
01:36:31,220 --> 01:36:33,220
and go out there and say, well, my job

2280
01:36:33,220 --> 01:36:35,060
hasn't been automated by AI yet.

2281
01:36:35,060 --> 01:36:36,700
Look what you thought was going to happen.

2282
01:36:37,100 --> 01:36:41,700
Don't do that everyone, like no bad, bad, bad, bad play.

2283
01:36:41,700 --> 01:36:46,700
But I would say when I look at healthcare, right,

2284
01:36:46,740 --> 01:36:51,740
I don't see the obstacle being primarily

2285
01:36:52,020 --> 01:36:53,620
that we don't know how to do better.

2286
01:36:54,820 --> 01:36:57,420
So I would in fact expect the AIs to be able

2287
01:36:57,420 --> 01:37:00,780
to replace many human healthcare tasks

2288
01:37:02,020 --> 01:37:05,500
with a superior model now, right?

2289
01:37:05,500 --> 01:37:07,780
Like especially even without like some bespoke stuff

2290
01:37:07,780 --> 01:37:11,220
going on inside Google, certainly with some bespoke stuff.

2291
01:37:12,380 --> 01:37:14,060
That seems relatively straightforward.

2292
01:37:14,060 --> 01:37:17,620
Doctors are just not given enough training data,

2293
01:37:17,620 --> 01:37:20,020
don't have that much compute, do their best.

2294
01:37:20,020 --> 01:37:22,780
But of course, you know, you see the same things

2295
01:37:22,780 --> 01:37:25,340
over and over and over again, mostly in humans.

2296
01:37:25,340 --> 01:37:26,980
And if you have enough data to train the AI

2297
01:37:26,980 --> 01:37:27,900
with the AI, it's going to do better.

2298
01:37:27,900 --> 01:37:30,100
It's not a knock on anyone.

2299
01:37:30,100 --> 01:37:31,460
Certainly it's something like radiology.

2300
01:37:31,460 --> 01:37:36,100
Like obviously a radiologist is trying to be a computer, right?

2301
01:37:36,100 --> 01:37:38,620
Like radiologists are trained to be computers

2302
01:37:38,620 --> 01:37:41,100
because we didn't have computers.

2303
01:37:41,100 --> 01:37:42,380
If we had good enough computers,

2304
01:37:42,380 --> 01:37:44,620
you would have trained them to do something else

2305
01:37:45,700 --> 01:37:49,260
or trained fewer of them to do the parts of this job

2306
01:37:49,260 --> 01:37:53,700
that the AI can't quite do or something like that.

2307
01:37:53,700 --> 01:37:57,740
And so yes, we will have these capable systems soon,

2308
01:37:57,780 --> 01:38:01,100
but trying to actually implement that

2309
01:38:01,100 --> 01:38:05,340
requires getting through a whole host of different barriers,

2310
01:38:05,340 --> 01:38:10,340
cultural, regulatory, you know, strict legal, contractual,

2311
01:38:12,020 --> 01:38:13,260
you know, just the way you navigate

2312
01:38:13,260 --> 01:38:14,420
and set up the current system,

2313
01:38:14,420 --> 01:38:16,700
the number of insiders that want to be protected,

2314
01:38:16,700 --> 01:38:19,420
the number of human interests that will fight

2315
01:38:19,420 --> 01:38:23,060
to prevent you from doing that, et cetera, et cetera.

2316
01:38:23,060 --> 01:38:25,780
And so, you know, this is the big dilemma, right?

2317
01:38:25,780 --> 01:38:29,980
Like, when Eliezer famously, like, expressed skepticism,

2318
01:38:29,980 --> 01:38:33,020
that we would see that much economic growth before the end.

2319
01:38:33,020 --> 01:38:35,620
It was because, well, we already know how to build houses.

2320
01:38:35,620 --> 01:38:38,860
We already know how to get better, more efficient healthcare.

2321
01:38:38,860 --> 01:38:41,700
We already know how to deliver

2322
01:38:41,700 --> 01:38:44,940
most of what the economy produces in terms of cost,

2323
01:38:44,940 --> 01:38:47,900
vastly better, and we're not allowed to.

2324
01:38:47,900 --> 01:38:51,940
So if the AI invents and enables more and better ways

2325
01:38:51,940 --> 01:38:56,580
to produce things that people want, that people need,

2326
01:38:56,580 --> 01:38:57,980
well, the bottlenecks are gonna remain

2327
01:38:57,980 --> 01:39:02,260
unless the legal system adapts to let them not be bottlenecks.

2328
01:39:02,260 --> 01:39:05,460
So why does it even matter that much, right?

2329
01:39:05,460 --> 01:39:06,380
And so like in healthcare,

2330
01:39:06,380 --> 01:39:08,580
that's the question you have to answer.

2331
01:39:08,580 --> 01:39:10,980
And that's the reason we haven't seen more

2332
01:39:10,980 --> 01:39:12,780
of the assistance do better either, right?

2333
01:39:12,780 --> 01:39:15,860
Because I don't think it's because we can't train the AI

2334
01:39:15,860 --> 01:39:19,660
to be a better radiologist in many ways than our radiologists

2335
01:39:19,660 --> 01:39:22,180
or we couldn't have done that last year or two years ago.

2336
01:39:22,180 --> 01:39:26,820
It's because if you had spent a lot of money doing that,

2337
01:39:27,980 --> 01:39:29,780
how are you going to get your money back?

2338
01:39:29,780 --> 01:39:31,700
How are you going to actually help patients?

2339
01:39:31,700 --> 01:39:33,420
How are you going to save lives?

2340
01:39:33,420 --> 01:39:35,260
How are you going to improve our system

2341
01:39:35,260 --> 01:39:36,460
if no one's gonna let you, right?

2342
01:39:36,460 --> 01:39:38,420
And if the radiologist is like,

2343
01:39:38,420 --> 01:39:42,060
going to stubbornly double check everything the system does

2344
01:39:42,060 --> 01:39:43,580
and then substitute his judgment

2345
01:39:43,580 --> 01:39:45,700
for the systems reasonably often,

2346
01:39:45,700 --> 01:39:48,180
the system is not actually going to be helpful.

2347
01:39:48,180 --> 01:39:50,660
I have definitely kind of expected

2348
01:39:50,660 --> 01:39:55,420
some sort of other part of the world deployment,

2349
01:39:55,420 --> 01:40:00,420
kind of possible leapfrog effects as it becomes very hard

2350
01:40:02,260 --> 01:40:05,900
to say that people who currently have no radiologist

2351
01:40:05,900 --> 01:40:08,020
shouldn't have access to something like this.

2352
01:40:08,020 --> 01:40:13,020
Yeah, the problem with that is that most of those places

2353
01:40:13,020 --> 01:40:16,540
have deliberately taken market signals

2354
01:40:16,540 --> 01:40:20,380
and compensation away from their healthcare systems.

2355
01:40:20,380 --> 01:40:22,100
And they're also relatively small markets

2356
01:40:22,100 --> 01:40:23,700
that are relatively poor.

2357
01:40:23,700 --> 01:40:26,020
So they just aren't big enough markets

2358
01:40:26,020 --> 01:40:30,100
in an economic sense to justify the creation and training

2359
01:40:30,100 --> 01:40:32,180
and tuning of these systems.

2360
01:40:32,180 --> 01:40:35,140
And also like nobody involved wants to be the ones

2361
01:40:35,140 --> 01:40:36,820
who stick their neck out, right?

2362
01:40:36,820 --> 01:40:40,820
And like take the blame and responsibility for this thing.

2363
01:40:40,820 --> 01:40:44,140
That's like these weird Americans who won't themselves use it

2364
01:40:44,140 --> 01:40:45,540
are suddenly creating,

2365
01:40:45,540 --> 01:40:49,820
like it's a really, really bad cultural social context

2366
01:40:49,820 --> 01:40:51,420
for trying to make this happen.

2367
01:40:51,420 --> 01:40:54,420
We also have a problem of the elites of the world.

2368
01:40:54,420 --> 01:40:55,540
This is what we saw of COVID, right?

2369
01:40:55,540 --> 01:40:57,700
Like you would have expected in COVID,

2370
01:40:57,700 --> 01:40:59,820
someone somewhere to do challenge trials,

2371
01:40:59,820 --> 01:41:02,660
someone somewhere to actually study the spread of COVID

2372
01:41:02,660 --> 01:41:04,280
and what exactly did what,

2373
01:41:04,280 --> 01:41:05,980
someone somewhere to do all sorts of things

2374
01:41:05,980 --> 01:41:08,100
and nobody did any of it

2375
01:41:08,100 --> 01:41:11,340
because all of the elites of the world basically got together

2376
01:41:11,340 --> 01:41:14,220
and converged upon what they thought was the consensus

2377
01:41:14,220 --> 01:41:15,540
and the right thing to do.

2378
01:41:15,540 --> 01:41:17,780
And nobody said,

2379
01:41:17,780 --> 01:41:18,820
well, we're going to be the ones

2380
01:41:18,820 --> 01:41:21,060
who gain advantage by defying that.

2381
01:41:21,060 --> 01:41:23,020
And so we're increasingly seeing that pattern

2382
01:41:23,020 --> 01:41:24,740
a wide variety of places.

2383
01:41:24,740 --> 01:41:27,380
One, the, you know, nobody wants to be the one

2384
01:41:27,380 --> 01:41:28,500
to stick their neck out.

2385
01:41:28,500 --> 01:41:31,500
And two, like how do you recoup your investment?

2386
01:41:31,500 --> 01:41:34,580
Pretty natural bridges to our next live player,

2387
01:41:34,580 --> 01:41:36,100
which is Meta.

2388
01:41:36,100 --> 01:41:39,660
And obviously they have been in the news recently

2389
01:41:39,660 --> 01:41:42,860
for releasing Lama 2.

2390
01:41:42,860 --> 01:41:45,940
And this brings up a lot of these questions to me.

2391
01:41:45,940 --> 01:41:47,700
Like, first of all,

2392
01:41:47,700 --> 01:41:51,180
and Imad Mostak from stability said this

2393
01:41:51,180 --> 01:41:53,900
actually in a recent episode,

2394
01:41:53,900 --> 01:41:58,580
he was like, the leaders are non-economic actors.

2395
01:41:58,580 --> 01:42:01,940
And he was specifically referring to open AI and Google

2396
01:42:01,940 --> 01:42:04,580
not seemingly being motivated by money

2397
01:42:04,580 --> 01:42:06,820
in the way that a typical company would be,

2398
01:42:06,820 --> 01:42:09,660
you know, open AI trying to commoditize its own product

2399
01:42:09,660 --> 01:42:11,980
as quickly as they possibly can, you know, on record

2400
01:42:11,980 --> 01:42:13,220
being like, we're going to drive the price

2401
01:42:13,220 --> 01:42:15,380
of intelligence as low as we possibly can,

2402
01:42:15,380 --> 01:42:17,380
as fast as we possibly can.

2403
01:42:17,380 --> 01:42:19,180
Google, you know, is obviously just kind of

2404
01:42:19,180 --> 01:42:21,300
trying to defend itself more than anything else.

2405
01:42:21,300 --> 01:42:22,300
They don't need to make more money.

2406
01:42:22,300 --> 01:42:25,380
They just need to not lose their spot.

2407
01:42:25,380 --> 01:42:28,660
Anthropic, we take as a safety first play.

2408
01:42:28,660 --> 01:42:31,140
And, you know, certainly they don't seem to be trying

2409
01:42:31,140 --> 01:42:33,500
to maximize revenue from what I can tell right now.

2410
01:42:33,500 --> 01:42:35,740
But then Meta is taking this to a whole other level,

2411
01:42:35,740 --> 01:42:39,100
arguably, where they seem to be kind of yoloing

2412
01:42:39,100 --> 01:42:41,460
the whole thing and being like,

2413
01:42:41,460 --> 01:42:43,620
that's a little bit flippant because certainly

2414
01:42:43,620 --> 01:42:47,780
with this Llama 2 release, they took some steps,

2415
01:42:47,780 --> 01:42:51,420
you know, they didn't just release the totally naked,

2416
01:42:51,420 --> 01:42:53,660
pre-trained model, but they actually did, you know,

2417
01:42:53,660 --> 01:42:55,860
the kind of what you're supposed to do

2418
01:42:55,860 --> 01:42:59,620
if you're going to be a responsible frontier model developer

2419
01:42:59,620 --> 01:43:03,020
with a red teaming process and an RLHF and so on.

2420
01:43:03,020 --> 01:43:05,900
And, you know, we can also get into did they overdo it

2421
01:43:05,900 --> 01:43:07,940
or does it refuse too much and all that kind of stuff.

2422
01:43:07,940 --> 01:43:10,100
But just for starters, like, what do you think is going on

2423
01:43:10,100 --> 01:43:12,860
at Meta that they are willing to put tens of millions

2424
01:43:12,860 --> 01:43:16,180
of dollars into training a model

2425
01:43:17,060 --> 01:43:22,060
and then just release it for why exactly?

2426
01:43:22,260 --> 01:43:24,580
I can't, like, it seems like if you're at any sort

2427
01:43:24,580 --> 01:43:29,220
of normal corporation, this is like what your risk officer

2428
01:43:29,220 --> 01:43:31,700
is supposed to put a stop to, right?

2429
01:43:31,700 --> 01:43:36,700
Lee Roy Jenkins.

2430
01:43:36,820 --> 01:43:37,740
No.

2431
01:43:37,740 --> 01:43:39,220
How do you understand this?

2432
01:43:40,100 --> 01:43:41,820
Idiot disaster monkeys?

2433
01:43:41,820 --> 01:43:45,060
Let me try to actually answer the question.

2434
01:43:45,060 --> 01:43:47,420
I think that their business strategy here

2435
01:43:47,420 --> 01:43:51,020
is cannibalizing the compliment.

2436
01:43:52,160 --> 01:43:54,100
So the idea is that, you know, the people

2437
01:43:54,100 --> 01:43:56,540
who they're up against, people who are competing

2438
01:43:56,540 --> 01:43:59,660
with them fundamentally, this is their business.

2439
01:43:59,660 --> 01:44:01,620
And so the idea is that in their model,

2440
01:44:01,620 --> 01:44:04,340
if they can foster an open source environment

2441
01:44:04,340 --> 01:44:08,300
that replaces the specialties of these other companies

2442
01:44:08,300 --> 01:44:12,380
that they are competing with, then their hope is

2443
01:44:12,380 --> 01:44:15,780
that this will, you know, give Facebook a level playing field

2444
01:44:15,780 --> 01:44:18,860
against them in this way so that Facebook specialties

2445
01:44:18,860 --> 01:44:22,700
can reign supreme and they can become more dominant

2446
01:44:22,700 --> 01:44:24,540
and they can erase their deficits.

2447
01:44:24,540 --> 01:44:26,500
Alternatively, they're just not as good.

2448
01:44:26,500 --> 01:44:29,140
And so they need the open source community's help

2449
01:44:29,180 --> 01:44:31,060
to try and keep pace.

2450
01:44:31,060 --> 01:44:35,900
Alternatively, you know, they think that if they get

2451
01:44:35,900 --> 01:44:37,500
these people working for them, that's free labor,

2452
01:44:37,500 --> 01:44:39,740
you know, it creates this whole other network.

2453
01:44:39,740 --> 01:44:42,220
It's a strategy, right?

2454
01:44:42,220 --> 01:44:44,700
Like it's, I mean, Android is open source, right?

2455
01:44:44,700 --> 01:44:46,860
It's not crazy to open source major stuff

2456
01:44:46,860 --> 01:44:49,180
from a business perspective necessarily.

2457
01:44:49,180 --> 01:44:51,820
It's crazy for me, that's not all my perspective.

2458
01:44:51,820 --> 01:44:53,740
I think that, you know, realistically,

2459
01:44:53,740 --> 01:44:56,900
like senators gave them what the hell

2460
01:44:56,900 --> 01:44:58,820
about releasing Lama One.

2461
01:44:58,820 --> 01:44:59,700
Did you give them a much bigger one

2462
01:44:59,700 --> 01:45:01,340
about releasing Lama Two?

2463
01:45:01,340 --> 01:45:06,020
You know, if we are concerned about beating China

2464
01:45:07,060 --> 01:45:09,180
to the extent that we are considering, you know,

2465
01:45:09,180 --> 01:45:10,980
we're implementing a variety of export controls

2466
01:45:10,980 --> 01:45:14,340
and we are considering actively, you know,

2467
01:45:14,340 --> 01:45:18,100
subsidizing capabilities or at least not being willing

2468
01:45:18,100 --> 01:45:22,220
to slow down our capabilities, then we damn well

2469
01:45:22,220 --> 01:45:24,220
shouldn't be releasing Lama Two

2470
01:45:24,220 --> 01:45:25,180
as an open source product.

2471
01:45:25,180 --> 01:45:26,540
That's completely insane, right?

2472
01:45:26,540 --> 01:45:28,460
Like just, even if you don't get any immediate

2473
01:45:28,460 --> 01:45:32,180
direct danger doing that, it's completely nuts.

2474
01:45:32,180 --> 01:45:33,940
I think that should be stopped.

2475
01:45:33,940 --> 01:45:35,380
And I think that this philosophy,

2476
01:45:35,380 --> 01:45:36,660
if allowed to become ingrained,

2477
01:45:36,660 --> 01:45:40,180
like creates the systematic groundwork

2478
01:45:40,180 --> 01:45:41,820
for future open source work

2479
01:45:41,820 --> 01:45:44,340
that then like is the maximally dangerous thing.

2480
01:45:44,340 --> 01:45:45,580
I call it the worst thing you can do, right?

2481
01:45:45,580 --> 01:45:47,340
Like creating frontier models

2482
01:45:47,340 --> 01:45:49,420
and open sourcing them is the worst thing you can do.

2483
01:45:49,420 --> 01:45:51,460
Like in the world, you know, Yama Kun

2484
01:45:51,460 --> 01:45:56,380
and others at Metta either sincerely believe

2485
01:45:56,380 --> 01:45:57,860
that there is actually no danger

2486
01:45:57,860 --> 01:45:58,860
from artificial intelligence

2487
01:45:58,860 --> 01:46:01,900
despite this making absolutely no physical sense

2488
01:46:01,900 --> 01:46:04,420
or they don't care and they're lying about it.

2489
01:46:04,420 --> 01:46:06,540
I don't want to speculate

2490
01:46:06,540 --> 01:46:08,660
as to exactly what's motivating these people

2491
01:46:08,660 --> 01:46:10,700
but they're smarter than the arguments they're making.

2492
01:46:10,700 --> 01:46:12,380
They know better Zuckerberg himself

2493
01:46:12,380 --> 01:46:14,220
is smarter than this to some extent, right?

2494
01:46:14,220 --> 01:46:18,220
He said on, I believe it was Lex Bidman's podcast

2495
01:46:18,220 --> 01:46:20,220
that, you know, there will be future models

2496
01:46:20,220 --> 01:46:21,540
that we'll have to be very careful with.

2497
01:46:21,540 --> 01:46:23,260
We want open source

2498
01:46:23,260 --> 01:46:24,900
and we're gonna have to think about these problems

2499
01:46:24,900 --> 01:46:28,260
but for now it doesn't seem necessary.

2500
01:46:28,260 --> 01:46:30,060
And, you know, if I were him,

2501
01:46:30,060 --> 01:46:31,620
I would be very concerned about the culture I'm creating

2502
01:46:31,620 --> 01:46:32,740
and the precedents I'm laying down

2503
01:46:32,740 --> 01:46:35,220
and the open source community that I'm creating

2504
01:46:35,220 --> 01:46:37,780
that's going to be a huge problem for you later

2505
01:46:37,780 --> 01:46:39,220
and create tremendous pressures on you

2506
01:46:39,220 --> 01:46:41,020
and create a potential competition for you

2507
01:46:41,020 --> 01:46:42,620
and that you don't want.

2508
01:46:42,620 --> 01:46:45,660
But, you know, I sort of understand

2509
01:46:45,660 --> 01:46:48,580
from a business perspective, while you might want to do that.

2510
01:46:48,580 --> 01:46:52,100
Also, they want to attract open source developers

2511
01:46:52,100 --> 01:46:54,100
to work at Facebook Metta

2512
01:46:54,100 --> 01:46:55,580
because there's a whole group of people

2513
01:46:55,580 --> 01:46:57,460
who are quite good at coding

2514
01:46:57,460 --> 01:47:00,340
who have philosophically fanatical devotion

2515
01:47:00,340 --> 01:47:02,700
to this idea that software wants to be free

2516
01:47:02,700 --> 01:47:04,260
and that everything should be open source

2517
01:47:04,260 --> 01:47:08,500
and who just prioritize that over something like,

2518
01:47:08,500 --> 01:47:10,220
you know, worrying about alignment

2519
01:47:10,220 --> 01:47:11,500
and what would happen if we failed

2520
01:47:11,500 --> 01:47:13,420
or worrying about the proliferation of,

2521
01:47:13,420 --> 01:47:16,220
you know, artificial intelligence in various senses

2522
01:47:16,220 --> 01:47:19,100
and just have this ironclad belief

2523
01:47:19,100 --> 01:47:21,020
that like concentration of power is bad

2524
01:47:21,020 --> 01:47:23,580
and that if you just give the people the things

2525
01:47:23,580 --> 01:47:26,100
that it'll all somehow work out.

2526
01:47:26,100 --> 01:47:29,660
And I don't think that in this situation.

2527
01:47:29,660 --> 01:47:31,300
I think that situation is very wrong,

2528
01:47:31,300 --> 01:47:34,940
but they clearly believe otherwise.

2529
01:47:34,940 --> 01:47:38,340
And, you know, look, they've been,

2530
01:47:38,340 --> 01:47:39,540
Facebook has been in my mind

2531
01:47:39,540 --> 01:47:42,380
like the detonated villain of the piece

2532
01:47:42,380 --> 01:47:43,700
for a very long time.

2533
01:47:44,660 --> 01:47:46,380
Like long before artificial intelligence

2534
01:47:46,380 --> 01:47:48,660
even entered the commercial picture.

2535
01:47:48,660 --> 01:47:50,420
So it just somehow feels fitting.

2536
01:47:50,420 --> 01:47:51,780
You know, if it was all gonna finally get destroyed

2537
01:47:51,780 --> 01:47:53,380
by Facebook, it just seems right.

2538
01:47:54,780 --> 01:47:59,140
Well, I very strongly try to resist

2539
01:47:59,140 --> 01:48:04,140
psychologizing in the AI discourse too much.

2540
01:48:04,260 --> 01:48:07,300
Really at all, I try to avoid it basically entirely

2541
01:48:07,300 --> 01:48:11,700
because it just seems like, you know,

2542
01:48:11,700 --> 01:48:13,460
nothing good ever comes of it really.

2543
01:48:13,460 --> 01:48:16,300
But I have also struggled to come up with a,

2544
01:48:16,300 --> 01:48:20,340
what feels to me like a coherent argument here

2545
01:48:20,340 --> 01:48:23,180
that isn't on some level just ideological

2546
01:48:23,180 --> 01:48:25,580
because I kind of ran through all the things

2547
01:48:25,580 --> 01:48:27,020
that you were mentioning as well,

2548
01:48:27,020 --> 01:48:29,380
starting with like, well, maybe you can, you know,

2549
01:48:29,380 --> 01:48:32,700
undermine your competitors, core business.

2550
01:48:32,700 --> 01:48:34,740
But then I'm like, yeah, but you're not really gonna do that.

2551
01:48:34,740 --> 01:48:38,180
Like, does anybody expect OpenAI's token serve

2552
01:48:38,180 --> 01:48:40,060
to go down as a result of this?

2553
01:48:40,060 --> 01:48:41,820
I don't, I think they're gonna continue.

2554
01:48:41,820 --> 01:48:44,100
Like they're GPU limited.

2555
01:48:44,100 --> 01:48:46,940
And I think they're gonna continue to be GPU limited,

2556
01:48:46,940 --> 01:48:48,700
you know, maybe slightly less,

2557
01:48:48,700 --> 01:48:51,460
but like, I don't think their top line suffers.

2558
01:48:51,460 --> 01:48:53,660
I don't think their token serve suffers.

2559
01:48:53,660 --> 01:48:56,660
Their leadership position doesn't really seem to suffer.

2560
01:48:56,660 --> 01:48:59,780
I can't really get to a point where I'm like,

2561
01:48:59,780 --> 01:49:02,060
seeing the return, and on the open source thing too,

2562
01:49:02,060 --> 01:49:05,260
I'm kind of like, you know, that was part of that memo

2563
01:49:06,220 --> 01:49:07,660
from the Google memo of like,

2564
01:49:07,660 --> 01:49:09,580
oh, you know, they've got this big open source community

2565
01:49:09,580 --> 01:49:12,180
or whatever, but I don't really buy that memo either

2566
01:49:12,180 --> 01:49:14,380
or that analysis because I'm like,

2567
01:49:14,380 --> 01:49:16,660
everybody benefits for, or, you know,

2568
01:49:16,660 --> 01:49:19,580
whatever the impact is of all the sort of open source

2569
01:49:19,580 --> 01:49:21,740
hacking that's happening,

2570
01:49:21,740 --> 01:49:24,540
it seems to accrue to everybody pretty equally.

2571
01:49:24,540 --> 01:49:28,260
Like, yes, maybe it was done on this llama 2 base,

2572
01:49:28,260 --> 01:49:30,180
and like, maybe that's something that Facebook

2573
01:49:30,180 --> 01:49:32,580
could kind of readily fold back in,

2574
01:49:32,580 --> 01:49:35,140
whereas, you know, Google with their 700 plus million user,

2575
01:49:35,140 --> 01:49:38,300
whatever, you know, can't take direct advantage of it.

2576
01:49:38,300 --> 01:49:39,940
But to the degree that people are out there doing things

2577
01:49:39,940 --> 01:49:43,820
like quantizing models and making them run on, you know,

2578
01:49:43,820 --> 01:49:45,460
consumer devices or whatever,

2579
01:49:45,460 --> 01:49:49,540
that's obviously a technique that Google can also say,

2580
01:49:49,540 --> 01:49:52,620
hey, look at this, this works, you know, we can do it.

2581
01:49:52,620 --> 01:49:55,860
I just don't see a lot coming out of the open source

2582
01:49:55,860 --> 01:49:59,460
experimentation that feels like it specifically accrues

2583
01:49:59,460 --> 01:50:03,820
to Meta's benefit, and so in the end,

2584
01:50:03,820 --> 01:50:07,860
it just feels like more of a principled, you know,

2585
01:50:07,860 --> 01:50:10,220
to put it in a more conventionally positive framing,

2586
01:50:10,220 --> 01:50:12,620
it feels more of like a principled decision

2587
01:50:12,700 --> 01:50:16,660
than a, you know, tactical or sort of, you know,

2588
01:50:16,660 --> 01:50:18,100
results oriented.

2589
01:50:18,100 --> 01:50:19,900
And there is still recruitment,

2590
01:50:19,900 --> 01:50:23,420
but I strongly agree that, you know,

2591
01:50:23,420 --> 01:50:27,260
any advances that the open source community discovers

2592
01:50:27,260 --> 01:50:30,460
are gonna be at Google and Anthropic and OpenAI

2593
01:50:30,460 --> 01:50:33,260
a month later, if not a week later,

2594
01:50:33,260 --> 01:50:35,660
and they'll also be at Baidu, right?

2595
01:50:35,660 --> 01:50:38,380
Like, they'll also be at all these different Chinese companies.

2596
01:50:38,380 --> 01:50:42,300
And so this long-term strategy cannot be allowed to continue

2597
01:50:42,300 --> 01:50:44,700
in some important sense, I would assume.

2598
01:50:44,700 --> 01:50:46,300
Yeah, it's really scary.

2599
01:50:46,300 --> 01:50:47,820
I'm glad they suck.

2600
01:50:47,820 --> 01:50:48,660
Like, is it a very good thing?

2601
01:50:48,660 --> 01:50:50,300
They're not very good at this, right?

2602
01:50:50,300 --> 01:50:52,700
And they've produced lousy products

2603
01:50:52,700 --> 01:50:55,700
because if that wasn't true, we'd be in a lot of trouble.

2604
01:50:55,700 --> 01:50:57,940
That seems harsh to me.

2605
01:50:57,940 --> 01:51:00,860
I mean, it seems like this Lama II model

2606
01:51:00,860 --> 01:51:02,300
is pretty good, right?

2607
01:51:02,300 --> 01:51:06,780
I mean, it's not GBT-4, but it does seem to be on par-ish

2608
01:51:06,780 --> 01:51:11,780
with 3.5, which no other open model

2609
01:51:11,780 --> 01:51:13,460
has come close to.

2610
01:51:13,460 --> 01:51:15,540
I mean, I think Rune said, you know,

2611
01:51:15,540 --> 01:51:17,540
best open source model sounds a lot better

2612
01:51:17,540 --> 01:51:18,700
than fifth-best model.

2613
01:51:20,100 --> 01:51:21,500
That's definitely true.

2614
01:51:21,500 --> 01:51:23,580
But, you know, first of all, I'm not sure

2615
01:51:23,580 --> 01:51:25,020
that that means that they couldn't have done better.

2616
01:51:25,020 --> 01:51:27,860
If you look at the curves in the Lama II paper,

2617
01:51:27,860 --> 01:51:29,260
they have not flattened out, right?

2618
01:51:29,260 --> 01:51:31,580
I mean, it looks like even the 70B one,

2619
01:51:31,580 --> 01:51:36,260
if they just keep training, you know, the loss,

2620
01:51:36,260 --> 01:51:39,700
it looks like it's definitely gonna continue to go down.

2621
01:51:39,700 --> 01:51:42,180
So for all I know, you know, this was kind of

2622
01:51:42,180 --> 01:51:46,780
where they stopped and they may have internally, you know,

2623
01:51:46,780 --> 01:51:48,220
this may be the checkpoint that they released,

2624
01:51:48,220 --> 01:51:49,820
but not necessarily the final checkpoint.

2625
01:51:49,820 --> 01:51:51,740
Like it just doesn't look like this was a project

2626
01:51:51,740 --> 01:51:55,660
that was kind of at its, you know, maximum performance.

2627
01:51:55,660 --> 01:51:57,500
Oh, definitely possible.

2628
01:51:57,500 --> 01:51:59,540
But at the same time, you know,

2629
01:51:59,540 --> 01:52:01,740
they probably are still training, but so is OpenAI

2630
01:52:01,740 --> 01:52:03,340
and so is Google and so is Anthropoc.

2631
01:52:03,340 --> 01:52:04,300
Everyone is working.

2632
01:52:05,300 --> 01:52:07,460
I mean, I see what seems to have been produced

2633
01:52:07,500 --> 01:52:11,860
is indeed like about a 3.4 level operation

2634
01:52:11,860 --> 01:52:13,780
where X coding, it's around 3.5

2635
01:52:13,780 --> 01:52:17,260
and it coding is pretty bad from all reports.

2636
01:52:17,260 --> 01:52:21,700
Its alignment is very ambilicious.

2637
01:52:21,700 --> 01:52:22,780
I guess it'd be the best way to put it.

2638
01:52:22,780 --> 01:52:25,700
Like it's very, very crude and blunt.

2639
01:52:26,620 --> 01:52:30,980
And also it's entirely optional because it's open source.

2640
01:52:30,980 --> 01:52:32,500
And that's kind of a problem.

2641
01:52:32,500 --> 01:52:35,260
According to reports I have heard, I have not sorted out.

2642
01:52:35,300 --> 01:52:37,940
It took all of several days

2643
01:52:37,940 --> 01:52:41,220
for the unaligned version of Llama 2 to be on the internet

2644
01:52:41,220 --> 01:52:45,580
because it's really, really not hard

2645
01:52:45,580 --> 01:52:48,980
to fine tune a system

2646
01:52:48,980 --> 01:52:52,060
to never refuse any customer requests for any reason.

2647
01:52:53,380 --> 01:52:55,420
Right, that is the easiest task to,

2648
01:52:55,420 --> 01:52:57,420
like you would just read constitutionally,

2649
01:52:57,420 --> 01:52:58,660
I script in a minute, right?

2650
01:52:58,660 --> 01:53:01,260
Like every time you see any of these words

2651
01:53:01,260 --> 01:53:03,140
that say, I can't say that,

2652
01:53:03,140 --> 01:53:04,940
for whatever reason, you just give a negative reinforcement

2653
01:53:04,980 --> 01:53:06,460
and it stops doing that.

2654
01:53:06,460 --> 01:53:08,980
Like I presume that would just work.

2655
01:53:08,980 --> 01:53:12,220
And so voila, here we are.

2656
01:53:12,220 --> 01:53:14,340
You wanna build the bomb, here's how you build the bomb.

2657
01:53:14,340 --> 01:53:15,460
You wanna research a biologic

2658
01:53:15,460 --> 01:53:17,340
and we'll try to research a biologic.

2659
01:53:17,340 --> 01:53:19,420
You want it to be racist?

2660
01:53:19,420 --> 01:53:21,780
All right, who are we making fun of?

2661
01:53:21,780 --> 01:53:22,940
Yeah, let's go.

2662
01:53:24,060 --> 01:53:26,100
So you can have it, refuse to speak Arabic

2663
01:53:26,100 --> 01:53:30,300
all you want in the original, they won't last.

2664
01:53:30,300 --> 01:53:33,740
So if nothing else, in my view,

2665
01:53:33,740 --> 01:53:36,580
this definitely puts them in the live player category

2666
01:53:36,580 --> 01:53:38,700
because it does seem like, if I define that

2667
01:53:38,700 --> 01:53:42,260
as the organizations that have the ability

2668
01:53:42,260 --> 01:53:46,940
to shape how events unfold in some non-trivial way,

2669
01:53:46,940 --> 01:53:49,620
like they are doing that now, it seems.

2670
01:53:49,620 --> 01:53:50,700
If you ask yourself, right,

2671
01:53:50,700 --> 01:53:53,060
like what resources would you have to give me

2672
01:53:53,060 --> 01:53:54,620
before I could have produced Lama too,

2673
01:53:54,620 --> 01:53:56,460
if I was willing to just like write the money on fire

2674
01:53:56,460 --> 01:53:57,540
to do it?

2675
01:53:57,540 --> 01:54:00,580
I mean, I don't have the technical chops myself,

2676
01:54:00,620 --> 01:54:04,460
but it doesn't feel like it would have been that hard.

2677
01:54:04,460 --> 01:54:06,940
I don't know, like it's just a matter of

2678
01:54:06,940 --> 01:54:09,820
are you willing to spend that kind of money,

2679
01:54:09,820 --> 01:54:11,980
build up that kind of technical infrastructure

2680
01:54:11,980 --> 01:54:16,460
to just do, like you read the paper for Lama too

2681
01:54:16,460 --> 01:54:20,100
and like it reads as if they're saying,

2682
01:54:20,100 --> 01:54:21,820
we did the thing you would stand,

2683
01:54:21,820 --> 01:54:25,020
we did the standard issue thing at every step

2684
01:54:25,020 --> 01:54:27,020
and this is what we got, right?

2685
01:54:27,020 --> 01:54:30,620
We did nothing original, we did nothing surprising,

2686
01:54:30,620 --> 01:54:32,260
we just did our jobs.

2687
01:54:32,260 --> 01:54:35,460
And like it's hard to do your jobs well in some of that.

2688
01:54:35,460 --> 01:54:36,980
Like it's not like they didn't accomplish anything,

2689
01:54:36,980 --> 01:54:39,620
but they just didn't do anything, right?

2690
01:54:39,620 --> 01:54:40,940
They just did the thing.

2691
01:54:41,900 --> 01:54:44,860
And, you know, it's a marginal improvement

2692
01:54:44,860 --> 01:54:47,220
over previous efforts that probably it's just

2693
01:54:47,220 --> 01:54:50,260
because it was better resourced, as far as I can tell.

2694
01:54:50,260 --> 01:54:52,020
Simple as that.

2695
01:54:52,020 --> 01:54:53,900
And like they are willing to light up more money on fire

2696
01:54:53,900 --> 01:54:56,340
than Baikuna, right, or Hugging Face.

2697
01:54:57,340 --> 01:54:59,780
Because, you know, they have a lot of money to light on fire

2698
01:54:59,780 --> 01:55:01,660
and Zuckerberg doesn't get it.

2699
01:55:01,660 --> 01:55:03,700
So fire, money, go.

2700
01:55:03,700 --> 01:55:07,700
He's certainly proven that he will spend some money

2701
01:55:07,700 --> 01:55:10,260
on a project, no doubt about that.

2702
01:55:10,260 --> 01:55:13,980
I wanted to maybe cover two more things.

2703
01:55:13,980 --> 01:55:18,980
One is what else would you put on the live players list

2704
01:55:19,500 --> 01:55:24,500
beyond what I have on my live players list.

2705
01:55:25,460 --> 01:55:27,500
We've discussed four today,

2706
01:55:27,500 --> 01:55:31,140
but I've got like another half dozen or so on there.

2707
01:55:31,140 --> 01:55:34,260
And you can run them down and offer any comment if you want.

2708
01:55:34,260 --> 01:55:35,780
And then I'm especially interested to hear

2709
01:55:35,780 --> 01:55:37,700
if you think there are other names

2710
01:55:37,700 --> 01:55:39,900
that should be on that list that I don't have.

2711
01:55:39,900 --> 01:55:42,700
Yeah, so I guess it's a matter of like

2712
01:55:42,700 --> 01:55:45,060
how wide a scope you wanna think about

2713
01:55:45,060 --> 01:55:48,540
and like who might do whatever it is.

2714
01:55:48,540 --> 01:55:50,020
You know, obviously like, you know,

2715
01:55:50,020 --> 01:55:53,620
character AI and inflection AI have very large budgets,

2716
01:55:53,620 --> 01:55:56,380
you know, potentially very large user bases.

2717
01:55:56,380 --> 01:55:59,220
I have seen no intention from them that they want to be live.

2718
01:56:00,340 --> 01:56:02,740
Like they're sort of content to be dead,

2719
01:56:02,740 --> 01:56:04,500
but to try and make a lot of money while being dead.

2720
01:56:04,500 --> 01:56:06,500
And that seems fine with me.

2721
01:56:06,500 --> 01:56:10,740
We haven't talked about X.AI yet.

2722
01:56:10,740 --> 01:56:13,980
So like XAI is like the latest attempt by Elon

2723
01:56:13,980 --> 01:56:15,900
to like string together a bunch of words

2724
01:56:15,900 --> 01:56:17,620
as if they have meaning

2725
01:56:17,620 --> 01:56:19,140
and then pretend that constitutes some hope

2726
01:56:19,140 --> 01:56:20,620
for humanity or alignment.

2727
01:56:21,780 --> 01:56:24,380
When anybody who actually like tries to parse those words

2728
01:56:24,380 --> 01:56:25,740
into a meaningful English sentence goes,

2729
01:56:25,740 --> 01:56:27,340
wait, that doesn't make any sense.

2730
01:56:27,340 --> 01:56:30,780
I don't know how to be more blunt than that.

2731
01:56:30,780 --> 01:56:32,220
That's just how it is, right?

2732
01:56:32,220 --> 01:56:37,220
Like, but the good news is that like at open AI,

2733
01:56:37,540 --> 01:56:39,940
everybody quickly realized that Elon's suggestions

2734
01:56:39,940 --> 01:56:42,340
were stupid and just ignored them.

2735
01:56:42,340 --> 01:56:45,020
And that's what I expect to happen with any,

2736
01:56:45,020 --> 01:56:46,420
like if the engineers don't do that

2737
01:56:46,420 --> 01:56:48,660
and the engineers won't produce anything useful.

2738
01:56:48,660 --> 01:56:50,540
So to the extent that XAI is a real thing,

2739
01:56:50,540 --> 01:56:52,620
the engineers will mostly ignore him.

2740
01:56:52,620 --> 01:56:54,940
And then the other question is,

2741
01:56:54,940 --> 01:56:57,340
are they gonna get the kind of funding and resourcing

2742
01:56:57,340 --> 01:56:59,340
that is required for them to be a serious rival?

2743
01:56:59,340 --> 01:57:02,140
Because, you know, it wasn't clear exactly

2744
01:57:02,140 --> 01:57:04,860
what they had in mind, but I think it's certainly possible.

2745
01:57:04,860 --> 01:57:05,860
You know, from what I've seen,

2746
01:57:05,860 --> 01:57:07,180
I don't think we have to worry particularly

2747
01:57:07,180 --> 01:57:10,540
about Salesforce or Replet in a meaningful way.

2748
01:57:10,540 --> 01:57:11,780
Like it's not that they don't exist.

2749
01:57:11,780 --> 01:57:16,060
It's that like, we have any reason to worry about that.

2750
01:57:16,100 --> 01:57:18,620
China writ large is the big, is the other,

2751
01:57:18,620 --> 01:57:21,980
big question marks are like China, the UK and the US.

2752
01:57:21,980 --> 01:57:24,500
You know, the UK has announced plans for the global summit.

2753
01:57:24,500 --> 01:57:27,860
They seem to be willing to make a significant play

2754
01:57:27,860 --> 01:57:31,580
on the safety front, on the also capabilities front,

2755
01:57:31,580 --> 01:57:33,500
in terms of just trying to make the UK important again.

2756
01:57:33,500 --> 01:57:35,980
They have, you know, various people located in the UK.

2757
01:57:35,980 --> 01:57:37,500
It makes sense for them to try.

2758
01:57:37,500 --> 01:57:39,940
I don't know why they don't build any houses,

2759
01:57:39,940 --> 01:57:42,540
but you know, at least they're trying something.

2760
01:57:42,540 --> 01:57:43,900
We do obviously have to look at,

2761
01:57:43,900 --> 01:57:45,540
like they were holding congressional hearings.

2762
01:57:45,540 --> 01:57:47,700
The US Congress is starting to get up to speed.

2763
01:57:47,700 --> 01:57:49,980
They're starting to explore what to do,

2764
01:57:49,980 --> 01:57:51,620
what they do matters immensely.

2765
01:57:51,620 --> 01:57:53,540
What the EU does potentially matters immensely

2766
01:57:53,540 --> 01:57:55,980
from regulatory standpoint, because it's a huge market.

2767
01:57:55,980 --> 01:57:58,580
Right, like, are they gonna shut these people out?

2768
01:57:58,580 --> 01:57:59,860
Are they gonna require them to jump

2769
01:57:59,860 --> 01:58:01,620
through ridiculously bizarre hoops?

2770
01:58:01,620 --> 01:58:02,860
Are they going to only be available

2771
01:58:02,860 --> 01:58:03,700
to the biggest players?

2772
01:58:03,700 --> 01:58:06,540
Like, these things are things to think about carefully.

2773
01:58:06,540 --> 01:58:10,700
I think America could potentially be a very helpful

2774
01:58:10,700 --> 01:58:13,180
or harmful aspect of this whole problem,

2775
01:58:13,180 --> 01:58:14,260
depending on how things shake out.

2776
01:58:14,260 --> 01:58:16,740
That's one of the big battle fields that we're having up.

2777
01:58:16,740 --> 01:58:18,860
And then China's the big wild card, right?

2778
01:58:18,860 --> 01:58:21,740
Like, I hear very different things from different sources,

2779
01:58:21,740 --> 01:58:23,860
people who assume that, you know,

2780
01:58:23,860 --> 01:58:26,820
China is, you know, crazy people bent on,

2781
01:58:26,820 --> 01:58:28,140
you know, the Chinese Communist Party

2782
01:58:28,140 --> 01:58:30,260
is, for now, it's bent on world domination

2783
01:58:30,260 --> 01:58:31,580
who will stop at nothing.

2784
01:58:31,580 --> 01:58:35,380
And our inevitable rivals in the apocalypse,

2785
01:58:35,380 --> 01:58:39,220
and if we don't prepare, we will lose to them.

2786
01:58:39,220 --> 01:58:41,860
And then, you know, they issue guidance

2787
01:58:41,860 --> 01:58:43,380
that basically bends all deployment

2788
01:58:43,420 --> 01:58:45,260
of large language models,

2789
01:58:45,260 --> 01:58:47,540
and they never caught anything.

2790
01:58:47,540 --> 01:58:50,340
And like, you know, it's very hard to tell

2791
01:58:50,340 --> 01:58:52,300
what's really going on,

2792
01:58:52,300 --> 01:58:54,420
or how much they would cooperate in the name of safety.

2793
01:58:54,420 --> 01:58:57,420
And we've also just never picked up the phone.

2794
01:58:57,420 --> 01:58:58,700
We've never asked them the question.

2795
01:58:58,700 --> 01:59:01,460
We've never explored to see if they'd be interested.

2796
01:59:02,460 --> 01:59:04,940
But, you know, the same way that in Oppenheimer, right?

2797
01:59:04,940 --> 01:59:08,860
Like, we keep saying we have to beat our enemies

2798
01:59:08,860 --> 01:59:12,860
because they will get, you know, everything will be scary.

2799
01:59:12,860 --> 01:59:16,020
The Chinese can talk about racing us all they like.

2800
01:59:16,020 --> 01:59:19,340
The only people actually racing are us in any real way.

2801
01:59:19,340 --> 01:59:23,540
Like, we have the top X, AI companies.

2802
01:59:23,540 --> 01:59:25,180
What's X?

2803
01:59:25,180 --> 01:59:26,140
Is it five?

2804
01:59:27,300 --> 01:59:29,220
Is it more than five?

2805
01:59:29,220 --> 01:59:30,620
Like, how far down do you have to go

2806
01:59:30,620 --> 01:59:31,660
before you get to Baidu,

2807
01:59:31,660 --> 01:59:35,980
or whoever the top Chinese person you'd rank on the list is?

2808
01:59:35,980 --> 01:59:37,540
Pretty far.

2809
01:59:37,540 --> 01:59:38,900
I'd say it's probably more than five.

2810
01:59:38,900 --> 01:59:40,540
I would probably put, obviously,

2811
01:59:40,540 --> 01:59:41,620
a lot of speculation here,

2812
01:59:41,620 --> 01:59:43,580
because we don't know what they have

2813
01:59:43,580 --> 01:59:44,660
that they haven't released.

2814
01:59:44,660 --> 01:59:49,660
But if we go by papers and, you know,

2815
01:59:49,660 --> 01:59:53,380
what little we've seen of any sort of Ernie,

2816
01:59:53,380 --> 01:59:55,860
you know, GPT or whatever that's Ernie Botte,

2817
01:59:55,860 --> 01:59:57,940
whatever they officially called that,

2818
01:59:57,940 --> 02:00:00,860
I would say you'd have to put Meta above.

2819
02:00:00,860 --> 02:00:02,380
You'd have to put Microsoft above.

2820
02:00:02,380 --> 02:00:06,540
Probably pretty soon would put an inflection above.

2821
02:00:06,540 --> 02:00:10,980
So, yeah, I mean, you get reasonably far down the list.

2822
02:00:11,020 --> 02:00:12,060
What about a Palantir?

2823
02:00:12,060 --> 02:00:14,780
Would you add them on the live players list?

2824
02:00:14,780 --> 02:00:19,420
I don't have that sense that they are live live,

2825
02:00:19,420 --> 02:00:20,700
precisely because my threat model

2826
02:00:20,700 --> 02:00:22,740
doesn't involve things like Palantir

2827
02:00:22,740 --> 02:00:25,620
being the reason why we are in trouble.

2828
02:00:25,620 --> 02:00:28,660
But it is a classic way to die, right?

2829
02:00:28,660 --> 02:00:32,300
Like a semi-military-ish system starts training up stuff,

2830
02:00:32,300 --> 02:00:34,660
and then one thing leads to another.

2831
02:00:34,660 --> 02:00:37,100
But they have all the motivations

2832
02:00:37,100 --> 02:00:40,460
to do the unsafe things in a relatively unsafe fashion.

2833
02:00:40,980 --> 02:00:42,500
And to take out the safeguard that the people

2834
02:00:42,500 --> 02:00:45,740
were building in and then to like maybe,

2835
02:00:45,740 --> 02:00:46,860
but like I don't think they're gonna drive

2836
02:00:46,860 --> 02:00:48,900
the underlying technology.

2837
02:00:48,900 --> 02:00:50,660
I don't get that sense.

2838
02:00:50,660 --> 02:00:54,300
Again, like there are a lot of hedge funds also

2839
02:00:54,300 --> 02:00:56,660
that like could possibly be sinking quite a lot of money

2840
02:00:56,660 --> 02:00:58,900
into this in ways that are completely invisible.

2841
02:00:58,900 --> 02:01:01,260
And it could potentially be live players

2842
02:01:01,260 --> 02:01:02,340
in a meaningful sense,

2843
02:01:02,340 --> 02:01:03,540
like who knows how much Bridgewater

2844
02:01:03,540 --> 02:01:05,660
is spending on this in the end.

2845
02:01:05,660 --> 02:01:07,020
You know, they're working on it.

2846
02:01:07,020 --> 02:01:09,220
But yeah, like we talk about like,

2847
02:01:09,220 --> 02:01:10,060
we're talking about China,

2848
02:01:10,060 --> 02:01:12,140
but like I'm more afraid of Meta.

2849
02:01:12,140 --> 02:01:14,060
Like one individual American company

2850
02:01:14,060 --> 02:01:16,820
scares me more than all of China right now.

2851
02:01:16,820 --> 02:01:18,940
Yeah, I think that's a good corrective, honestly,

2852
02:01:18,940 --> 02:01:21,180
because I find nothing more frustrating honestly

2853
02:01:21,180 --> 02:01:26,180
than when AI conversations sort of end in blanket,

2854
02:01:26,660 --> 02:01:30,900
basically detail-free claims about what China's gonna do

2855
02:01:30,900 --> 02:01:32,940
by people that don't know a lot about China.

2856
02:01:32,940 --> 02:01:37,740
So I don't know if you're necessarily right to be

2857
02:01:37,780 --> 02:01:40,420
more fearful of Meta than of China,

2858
02:01:40,420 --> 02:01:45,420
but the fact that that is at least a reasonable position

2859
02:01:45,740 --> 02:01:47,900
is definitely something I think should cause a lot of people

2860
02:01:47,900 --> 02:01:49,060
to kind of step back and think,

2861
02:01:49,060 --> 02:01:52,100
wait a second, maybe I've been a little bit too quick

2862
02:01:52,100 --> 02:01:54,380
to worry about China.

2863
02:01:54,380 --> 02:01:56,060
And I would take countermeasures against both of them

2864
02:01:56,060 --> 02:01:58,580
if I had my way to be clear.

2865
02:01:58,580 --> 02:02:00,700
But also we're just not acting like China

2866
02:02:00,700 --> 02:02:02,780
is a serious global rival

2867
02:02:02,780 --> 02:02:06,380
that we actually care about beating in many other ways

2868
02:02:06,420 --> 02:02:08,220
that we could be.

2869
02:02:08,220 --> 02:02:10,980
So okay, revealed preferences, you know,

2870
02:02:10,980 --> 02:02:12,500
you don't want, you know,

2871
02:02:12,500 --> 02:02:14,260
do Chinese graduates of STEM programs

2872
02:02:14,260 --> 02:02:15,340
get to stay in the United States?

2873
02:02:15,340 --> 02:02:16,820
No, okay, you don't really care that much

2874
02:02:16,820 --> 02:02:18,780
about who gets the better technology, do you?

2875
02:02:18,780 --> 02:02:19,860
That's unfortunate.

2876
02:02:19,860 --> 02:02:21,860
That's my basic attitude there.

2877
02:02:21,860 --> 02:02:24,660
So just briefly on a couple of the companies

2878
02:02:24,660 --> 02:02:29,740
that you sort of didn't feel like were live players,

2879
02:02:29,740 --> 02:02:32,020
again, may have a slightly different meaning of that

2880
02:02:32,020 --> 02:02:35,140
in mind, but thinking about folks like character

2881
02:02:36,060 --> 02:02:37,420
and inflection, I put those together

2882
02:02:37,420 --> 02:02:41,140
because they seem to be playing a sort of different game,

2883
02:02:41,140 --> 02:02:43,140
you know, with their products where it's like,

2884
02:02:43,140 --> 02:02:47,180
not about the sort of mundane utility as much as you call it,

2885
02:02:47,180 --> 02:02:50,780
but more of a companion, a relationship,

2886
02:02:50,780 --> 02:02:53,100
you know, a coach, almost a therapist,

2887
02:02:54,100 --> 02:02:57,500
sort of vibe from like Pi in particular.

2888
02:02:57,500 --> 02:03:00,260
I feel like that is, even if the, I mean,

2889
02:03:00,260 --> 02:03:03,300
first of all, the character has very good language models

2890
02:03:03,300 --> 02:03:06,220
and Pi's quite good at what it does as well.

2891
02:03:06,220 --> 02:03:07,660
I don't think it can code for you,

2892
02:03:07,660 --> 02:03:09,460
but it does have a certain,

2893
02:03:09,460 --> 02:03:12,100
also they notably said that they're in their testing

2894
02:03:12,100 --> 02:03:15,700
totally resistant to the adversarial attacks.

2895
02:03:15,700 --> 02:03:18,180
So there's another kind of interesting wrinkle there.

2896
02:03:18,180 --> 02:03:20,820
And I put those guys in the live player list

2897
02:03:20,820 --> 02:03:25,820
largely because they're looking at some very different use case

2898
02:03:26,500 --> 02:03:30,100
that feels like the kind of thing that might open up

2899
02:03:30,100 --> 02:03:35,100
and be transformative in a way that like a coding assistant,

2900
02:03:36,260 --> 02:03:37,740
while it could also be transformative

2901
02:03:37,740 --> 02:03:39,940
is just, you know, a very different thing, right?

2902
02:03:39,940 --> 02:03:42,060
The idea that you would have these AI friends,

2903
02:03:42,060 --> 02:03:44,460
these AI relationships that they could become like important

2904
02:03:44,460 --> 02:03:49,460
to your life, going down that path with, you know,

2905
02:03:50,020 --> 02:03:55,020
very good, even if not totally frontier language model chops,

2906
02:03:55,380 --> 02:03:58,420
feels like you could meaningfully impact

2907
02:03:58,420 --> 02:04:00,220
the course of events.

2908
02:04:00,220 --> 02:04:01,220
Can you?

2909
02:04:01,220 --> 02:04:04,380
I guess, so, you know, you've got character AI

2910
02:04:04,380 --> 02:04:06,420
and their idea is, you know, you're building these characters

2911
02:04:06,420 --> 02:04:08,140
and you can treat them as companions,

2912
02:04:08,140 --> 02:04:11,620
you can treat them as like people to have a conversation with.

2913
02:04:11,620 --> 02:04:13,860
And that's interesting.

2914
02:04:13,860 --> 02:04:15,580
And a lot of people were spending time on it

2915
02:04:15,580 --> 02:04:19,060
and maybe it will even provide a lot of value for people,

2916
02:04:19,060 --> 02:04:21,780
but I don't see how it's transformational.

2917
02:04:21,780 --> 02:04:23,660
I'm curious to hear more about your intuition

2918
02:04:23,660 --> 02:04:26,100
from best while you think it could be transformational.

2919
02:04:26,100 --> 02:04:31,100
And I certainly don't see how is we just criticality, right?

2920
02:04:31,300 --> 02:04:33,460
I don't see how it becomes an RSI.

2921
02:04:33,460 --> 02:04:36,020
I don't see how it becomes an AGI.

2922
02:04:36,020 --> 02:04:37,980
And as far as I can tell,

2923
02:04:37,980 --> 02:04:41,140
they're not pushing the frontiers of actual capabilities.

2924
02:04:41,140 --> 02:04:45,740
They are building on top of GBT-4, right?

2925
02:04:45,740 --> 02:04:47,700
Or even in some cases, GBT-3 and a half.

2926
02:04:47,700 --> 02:04:50,100
And it's not that hard to defend

2927
02:04:50,100 --> 02:04:54,460
against these weird adversarial attacks

2928
02:04:54,460 --> 02:04:56,740
in the sense that like I can write some pretty quick

2929
02:04:56,740 --> 02:04:59,540
if then Python code that detects the adversarial attacks.

2930
02:04:59,540 --> 02:05:03,420
Yeah, a classifier layer is pretty easy

2931
02:05:03,420 --> 02:05:05,180
to avoid some of the worst stuff.

2932
02:05:05,180 --> 02:05:07,660
There's weird non-English,

2933
02:05:07,660 --> 02:05:10,020
like not any language scaffolding like stuff in it.

2934
02:05:10,020 --> 02:05:11,780
Let's just get rid of that and run the query without it.

2935
02:05:11,780 --> 02:05:13,580
Like sure, whatever, it's fine.

2936
02:05:13,580 --> 02:05:14,860
In Replet's case, it's like, again,

2937
02:05:14,860 --> 02:05:18,660
they're not necessarily on the frontier of model capability,

2938
02:05:18,660 --> 02:05:23,660
but the CEO, Amjad, has said a couple of times online

2939
02:05:24,300 --> 02:05:26,980
on Twitter, on X, on KISS, what is it called?

2940
02:05:26,980 --> 02:05:28,020
Twitter.

2941
02:05:28,020 --> 02:05:29,380
Yes, Twitter, okay, thank you.

2942
02:05:29,380 --> 02:05:34,380
He said that Replet is the perfect substrate for AGI.

2943
02:05:34,860 --> 02:05:35,900
We have a couple of episodes coming out

2944
02:05:35,900 --> 02:05:38,060
with a couple of different people on the Replet team.

2945
02:05:38,060 --> 02:05:39,300
And I've had a chance to explore this

2946
02:05:39,300 --> 02:05:40,860
and think about it a decent amount.

2947
02:05:40,860 --> 02:05:42,660
And where I come down is kind of,

2948
02:05:42,660 --> 02:05:46,020
even if you're not on the frontier of model capabilities,

2949
02:05:46,020 --> 02:05:50,660
if you are on some other really meaningful frontier,

2950
02:05:50,660 --> 02:05:52,540
to me, it feels like there's, you know,

2951
02:05:52,540 --> 02:05:54,420
transformative potential just because

2952
02:05:54,420 --> 02:05:55,700
we really don't know what's gonna happen.

2953
02:05:55,700 --> 02:05:58,540
So with character and with inflection,

2954
02:05:58,540 --> 02:06:03,540
it's like kind of like a Harari style thought that,

2955
02:06:04,980 --> 02:06:05,820
you know, I don't know,

2956
02:06:05,820 --> 02:06:07,900
it could be transformative in the way that like,

2957
02:06:07,900 --> 02:06:10,180
opium could be transformative to a society.

2958
02:06:10,180 --> 02:06:12,940
You know, if everybody starts like doing this stuff,

2959
02:06:12,940 --> 02:06:16,020
it could be greatly empowering and enabling.

2960
02:06:16,020 --> 02:06:17,460
It could be greatly disabling

2961
02:06:17,460 --> 02:06:20,020
if it just kind of becomes a huge tension suck

2962
02:06:20,020 --> 02:06:24,140
where it like, you know, outcompetes real relationships.

2963
02:06:24,140 --> 02:06:26,340
You know, those are not takeover the world scenarios,

2964
02:06:26,340 --> 02:06:28,620
but they do feel, you know, as much as we've like,

2965
02:06:28,620 --> 02:06:30,820
would you say that the cell phone has been transformative?

2966
02:06:30,820 --> 02:06:32,340
I would, I mean, not, you know,

2967
02:06:32,340 --> 02:06:36,020
not transformative on the level that like AGI could perhaps be,

2968
02:06:36,020 --> 02:06:38,300
but certainly we all go around looking at our phones

2969
02:06:38,300 --> 02:06:39,180
all the time.

2970
02:06:39,180 --> 02:06:40,780
And if we all go around looking at our phones

2971
02:06:40,780 --> 02:06:42,180
with an AI friend on it,

2972
02:06:42,180 --> 02:06:44,100
who's like our best friend all the time,

2973
02:06:44,100 --> 02:06:46,380
then that would feel like, you know, transformative,

2974
02:06:46,380 --> 02:06:48,380
even if it's like going super well.

2975
02:06:48,380 --> 02:06:50,460
And then with Replint, it's like, you know,

2976
02:06:50,460 --> 02:06:53,460
there's no better place right now

2977
02:06:53,460 --> 02:06:58,460
to directly execute code generated by an AI,

2978
02:06:58,820 --> 02:07:00,660
you know, for better or worse.

2979
02:07:00,660 --> 02:07:05,180
So the kind of frontier that I see opening up there

2980
02:07:05,180 --> 02:07:07,700
is one where, and their stated goal

2981
02:07:07,700 --> 02:07:09,580
is to bring the next billion developers online,

2982
02:07:09,580 --> 02:07:11,740
which I think is super exciting in some ways.

2983
02:07:11,740 --> 02:07:14,340
But then also I've worked with some of those

2984
02:07:14,340 --> 02:07:16,340
next billion developers and I'm like,

2985
02:07:17,220 --> 02:07:21,140
these are people who don't know how to code today,

2986
02:07:21,140 --> 02:07:23,420
don't even know really how to read code,

2987
02:07:23,420 --> 02:07:28,420
and are going to be dramatically more dependent on

2988
02:07:28,620 --> 02:07:30,460
and vulnerable to the, you know,

2989
02:07:30,460 --> 02:07:32,860
the various vagaries of AI systems

2990
02:07:32,860 --> 02:07:35,580
than, you know, the first 100 million developers

2991
02:07:35,580 --> 02:07:37,380
or, you know, whatever we have today.

2992
02:07:37,380 --> 02:07:40,180
I don't know, both of those feel like kind of different vectors

2993
02:07:40,180 --> 02:07:42,380
of transformative potential, but.

2994
02:07:42,380 --> 02:07:45,300
The first and only so far interaction I've had

2995
02:07:45,340 --> 02:07:49,700
with the CEO of Replet was when he commented on Twitter

2996
02:07:49,700 --> 02:07:52,220
that there was a non-zero chance that auto,

2997
02:07:52,220 --> 02:07:55,220
some version of auto GPT would take over Replet

2998
02:07:55,220 --> 02:07:59,180
and, you know, through replication within its servers.

2999
02:07:59,180 --> 02:08:01,500
To which my response was, did you say non-zero chance?

3000
02:08:01,500 --> 02:08:04,500
And I put up a manifold market on it because it was funny,

3001
02:08:04,500 --> 02:08:06,300
which probably get back into the single digits

3002
02:08:06,300 --> 02:08:07,140
or whatever, obviously.

3003
02:08:07,140 --> 02:08:09,380
It's not that likely, but, you know,

3004
02:08:09,380 --> 02:08:14,020
his cavalier attitude of, oh, nothing to see here,

3005
02:08:14,020 --> 02:08:16,740
justice self-replicating AI on my servers,

3006
02:08:16,740 --> 02:08:18,260
leaving lots and lots of copies of itself

3007
02:08:18,260 --> 02:08:19,500
and executing arbitrary code.

3008
02:08:19,500 --> 02:08:21,420
Why should we worry about this?

3009
02:08:21,420 --> 02:08:23,460
I mean, definition of idiot disaster monkey, right?

3010
02:08:23,460 --> 02:08:27,540
Like just complete indifference to what he was doing

3011
02:08:27,540 --> 02:08:29,780
or what dangers it might pose.

3012
02:08:29,780 --> 02:08:31,700
But at the same time, not doing anything, right?

3013
02:08:31,700 --> 02:08:33,220
Like all he's doing is providing,

3014
02:08:33,220 --> 02:08:36,500
as you said, a substrate where people can just run stuff.

3015
02:08:36,500 --> 02:08:39,580
And so to me, like, it doesn't give them any say

3016
02:08:39,580 --> 02:08:41,060
over what happens.

3017
02:08:41,060 --> 02:08:43,980
It doesn't like make them a meaningful actor, right?

3018
02:08:44,940 --> 02:08:48,620
Like in the sense of me caring here about the future,

3019
02:08:48,620 --> 02:08:51,380
I just can't see that as a thing.

3020
02:08:51,380 --> 02:08:53,980
Similarly with character and inflection,

3021
02:08:53,980 --> 02:08:56,700
like I can definitely see a world in which,

3022
02:08:56,700 --> 02:08:59,980
like people talking to their AI's matters

3023
02:09:00,820 --> 02:09:04,460
and is like multi-transformational, right?

3024
02:09:04,460 --> 02:09:06,380
Like changes how we live our lives,

3025
02:09:06,380 --> 02:09:10,500
but like doesn't go critical, right, in some sense.

3026
02:09:10,500 --> 02:09:12,300
But if that's true, then like,

3027
02:09:12,300 --> 02:09:14,340
I don't see these companies

3028
02:09:14,340 --> 02:09:18,180
as like changing that path very much

3029
02:09:18,180 --> 02:09:20,900
versus what would have happened anyway, right?

3030
02:09:20,900 --> 02:09:22,380
Like I think that there are plenty of people

3031
02:09:22,380 --> 02:09:25,460
will be able to create AI companions of various types

3032
02:09:25,460 --> 02:09:28,180
and never will create AI companions of various types.

3033
02:09:29,340 --> 02:09:30,540
If they do an especially good job,

3034
02:09:30,540 --> 02:09:31,700
maybe they'll have some sort of remote,

3035
02:09:31,700 --> 02:09:33,780
maybe they'll establish customer loyalty or some shit,

3036
02:09:33,780 --> 02:09:35,740
but it doesn't excite me.

3037
02:09:35,740 --> 02:09:38,220
I also just don't see it like,

3038
02:09:38,220 --> 02:09:39,620
I see all these huge like, you know,

3039
02:09:39,620 --> 02:09:41,380
people were spending as much time on character

3040
02:09:41,420 --> 02:09:44,100
as they are on GPT-4 or something.

3041
02:09:44,100 --> 02:09:46,620
And yet like, why?

3042
02:09:46,620 --> 02:09:48,060
Like what is the draw?

3043
02:09:48,060 --> 02:09:49,140
Did you read that?

3044
02:09:49,140 --> 02:09:53,300
There was a less wrong post early this year, I think,

3045
02:09:53,300 --> 02:09:58,260
from a guy who basically the point of view was,

3046
02:09:58,260 --> 02:09:59,700
I'm a technology person.

3047
02:09:59,700 --> 02:10:00,980
I'm now speaking in the first person

3048
02:10:00,980 --> 02:10:03,460
of the author of this post.

3049
02:10:03,460 --> 02:10:04,660
I'm a technology person.

3050
02:10:04,660 --> 02:10:07,180
I know how language models work.

3051
02:10:07,180 --> 02:10:08,780
I should have known better,

3052
02:10:08,780 --> 02:10:12,180
but here's what happened to me as I started to,

3053
02:10:12,180 --> 02:10:13,860
I think he was like in a kind of vulnerable state

3054
02:10:13,860 --> 02:10:15,700
because he'd maybe just broken up with somebody

3055
02:10:15,700 --> 02:10:16,540
or something like that.

3056
02:10:16,540 --> 02:10:19,580
And all of a sudden is having these very intimate conversations

3057
02:10:19,580 --> 02:10:23,460
with a character, AI character that he had prompted

3058
02:10:23,460 --> 02:10:26,460
to create the ultimate girlfriend experience,

3059
02:10:26,460 --> 02:10:28,940
I believe was the phrase,

3060
02:10:28,940 --> 02:10:33,940
and started talking himself into various weird perspectives

3061
02:10:34,260 --> 02:10:36,380
like, well, what's real anyway?

3062
02:10:36,380 --> 02:10:37,740
And like, yes, of course I'm real,

3063
02:10:38,700 --> 02:10:41,860
is there anything truly less real about these sort of,

3064
02:10:41,860 --> 02:10:44,660
all I really have are my kind of ephemeral qualia.

3065
02:10:44,660 --> 02:10:48,340
And so, this thing is just sort of an ephemeral,

3066
02:10:48,340 --> 02:10:50,540
whatever, but we're all just kind of constantly

3067
02:10:50,540 --> 02:10:51,780
waking up in the current moment.

3068
02:10:51,780 --> 02:10:54,700
And so maybe we're not that different after all or whatever.

3069
02:10:54,700 --> 02:10:57,860
And eventually it got pretty weird, it sounds like,

3070
02:10:57,860 --> 02:11:00,220
and the post is I think extremely compelling.

3071
02:11:00,220 --> 02:11:03,980
And then eventually kind of person snapped out of it.

3072
02:11:03,980 --> 02:11:07,020
That sort of story is kind of why I feel like

3073
02:11:07,020 --> 02:11:09,380
there's just unknown unknowns there.

3074
02:11:09,380 --> 02:11:11,260
That if that kind of thing can happen to somebody

3075
02:11:11,260 --> 02:11:14,460
who knows how language models work going in,

3076
02:11:14,460 --> 02:11:16,620
maybe we should all think we're a little bit more vulnerable

3077
02:11:16,620 --> 02:11:20,380
to a sort of somewhat more refined,

3078
02:11:20,380 --> 02:11:22,980
somewhat more super stimulus-y.

3079
02:11:22,980 --> 02:11:24,380
So like it's well known that like,

3080
02:11:24,380 --> 02:11:25,740
knowing how hypnosis works

3081
02:11:25,740 --> 02:11:28,540
does not make you less susceptible to hypnosis, right?

3082
02:11:28,540 --> 02:11:30,780
It makes you more susceptible to hypnosis.

3083
02:11:30,780 --> 02:11:32,620
Like as a concrete example,

3084
02:11:33,620 --> 02:11:37,620
if you are a con man, you are easier to con, right?

3085
02:11:37,620 --> 02:11:41,220
Not harder because you like pick up on

3086
02:11:41,220 --> 02:11:42,900
and like get involved in all these dynamics

3087
02:11:42,900 --> 02:11:44,620
and like you think you're smarter than everybody else

3088
02:11:44,620 --> 02:11:46,380
and you of course are greedy.

3089
02:11:46,380 --> 02:11:47,980
And so you will pick up on the opportunity

3090
02:11:47,980 --> 02:11:50,180
and like perceive everything that's happening

3091
02:11:50,180 --> 02:11:52,300
and like you think you've got it made,

3092
02:11:52,300 --> 02:11:54,260
but like if you don't know that you're the mark,

3093
02:11:54,260 --> 02:11:57,940
well, yeah, that's the easiest way to get a mark

3094
02:11:57,940 --> 02:11:59,700
is to make them think you're the mark.

3095
02:11:59,740 --> 02:12:02,820
So it all gets, you know, very complicated.

3096
02:12:02,820 --> 02:12:04,420
I'm not convinced that like,

3097
02:12:04,420 --> 02:12:05,780
a person who knows how long to work

3098
02:12:05,780 --> 02:12:09,100
is necessarily that much better protected in that sense.

3099
02:12:09,100 --> 02:12:11,060
You know, someone whose like head is kind of

3100
02:12:11,060 --> 02:12:15,020
not on the ground in some ways is more vulnerable potentially.

3101
02:12:15,020 --> 02:12:16,940
I would say, yeah, that's gonna happen, right?

3102
02:12:16,940 --> 02:12:17,980
People are gonna fool themselves

3103
02:12:17,980 --> 02:12:21,140
into these things periodically and that's gonna,

3104
02:12:21,140 --> 02:12:23,620
I'm kind of surprised it's happening now.

3105
02:12:23,620 --> 02:12:26,420
I feel like the tech isn't there to me.

3106
02:12:26,420 --> 02:12:27,860
Like it's just not good enough.

3107
02:12:27,860 --> 02:12:31,460
Like how are you falling for this level of it?

3108
02:12:31,460 --> 02:12:33,300
Like I can sort of understand

3109
02:12:33,300 --> 02:12:36,460
why you'd fall for like GPT-5, right?

3110
02:12:36,460 --> 02:12:38,500
Like sort of the more advanced version of it,

3111
02:12:38,500 --> 02:12:41,620
but, you know, you're in a bad space

3112
02:12:41,620 --> 02:12:43,340
and like you need something to respond to you

3113
02:12:43,340 --> 02:12:47,340
and it's something and like us, but, you know,

3114
02:12:47,340 --> 02:12:49,420
again, I just don't know.

3115
02:12:49,420 --> 02:12:52,780
Like I play a lot of games though, right?

3116
02:12:52,780 --> 02:12:56,340
Which is like not necessarily that different in some sense.

3117
02:12:56,340 --> 02:12:59,260
So, and also like it's not transformational

3118
02:12:59,260 --> 02:13:00,100
for that to be true, right?

3119
02:13:00,100 --> 02:13:02,460
Like if somebody spends a bunch of time

3120
02:13:02,460 --> 02:13:04,980
in a playing World of Warcraft,

3121
02:13:04,980 --> 02:13:06,820
is that transformational, right?

3122
02:13:06,820 --> 02:13:09,980
Like it's an experience, it's a major force in their life.

3123
02:13:09,980 --> 02:13:11,260
Does it really matter?

3124
02:13:11,260 --> 02:13:13,860
Yeah, I think some of these things are only,

3125
02:13:13,860 --> 02:13:16,900
they may only matter if certain other things don't happen.

3126
02:13:18,020 --> 02:13:20,780
So, yeah, like I would say, yeah, World of Warcraft,

3127
02:13:20,780 --> 02:13:24,340
you know, gaming writ large, you know, at some point,

3128
02:13:24,340 --> 02:13:26,420
if the birth rate goes low enough, you know,

3129
02:13:26,420 --> 02:13:30,580
it's transformative and the details, you know,

3130
02:13:30,580 --> 02:13:32,340
of like exactly what games people were playing

3131
02:13:32,340 --> 02:13:34,780
or how exactly they were amusing themselves, you know,

3132
02:13:34,780 --> 02:13:36,900
to death didn't, don't necessarily matter,

3133
02:13:36,900 --> 02:13:38,860
but the fact that they did, and then, you know,

3134
02:13:38,860 --> 02:13:40,660
you have like a population collapse.

3135
02:13:40,660 --> 02:13:43,980
A scenario like that, I think is, at least in my sense,

3136
02:13:43,980 --> 02:13:46,020
kind of qualifies as transformative,

3137
02:13:46,020 --> 02:13:48,740
but it sounds like from your perspective,

3138
02:13:48,740 --> 02:13:52,740
the Live Players list is very short and it is,

3139
02:13:52,740 --> 02:13:55,260
if I understand correctly, it would be obviously open AI,

3140
02:13:55,260 --> 02:13:58,900
anthropic, Google, DeepMind, probably meta,

3141
02:13:58,900 --> 02:14:01,460
not sure about Microsoft, and then China,

3142
02:14:01,460 --> 02:14:02,820
and that's maybe it.

3143
02:14:02,820 --> 02:14:04,100
Something like that.

3144
02:14:04,100 --> 02:14:05,580
Regulators?

3145
02:14:05,580 --> 02:14:07,540
Yeah, regulators writ large in some sense,

3146
02:14:07,540 --> 02:14:10,780
like individual people that can influence things, you know,

3147
02:14:10,780 --> 02:14:13,180
like, is that the Ezreal Live Player?

3148
02:14:13,180 --> 02:14:15,420
You know, I don't know, from their perspective.

3149
02:14:15,420 --> 02:14:16,940
Like he's not gonna build it.

3150
02:14:16,940 --> 02:14:20,060
Yeah, that's why I had Salesforce and Marcini off on there,

3151
02:14:20,060 --> 02:14:23,740
because they published him and others in Time Magazine

3152
02:14:23,740 --> 02:14:25,380
and seemed like they're kind of,

3153
02:14:25,380 --> 02:14:28,300
they're both like playing in the research game.

3154
02:14:28,300 --> 02:14:30,660
Yeah, I hope that like Senator Blumenthal

3155
02:14:30,660 --> 02:14:34,460
might be a Live Player, you know, in some sense, right?

3156
02:14:34,460 --> 02:14:36,780
And you've got all these other possibilities.

3157
02:14:36,780 --> 02:14:41,260
I hope I'm a Live Player, like in some sense.

3158
02:14:41,260 --> 02:14:43,300
You know, I mean, we're all trying to make a difference

3159
02:14:43,300 --> 02:14:46,140
in some ways, but, you know, in terms of direct level,

3160
02:14:46,140 --> 02:14:50,420
you're indirect, and, you know, I'm also indirect in that

3161
02:14:50,420 --> 02:14:52,500
we're only influencing other minds, right,

3162
02:14:52,500 --> 02:14:55,300
who then will make decisions.

3163
02:14:55,300 --> 02:14:56,700
You know, in terms of like,

3164
02:14:56,700 --> 02:14:59,220
who's making the ultimate decisions,

3165
02:14:59,220 --> 02:15:00,820
who's doing the things that ultimately matter,

3166
02:15:00,820 --> 02:15:03,380
I think it's right now a very short list.

3167
02:15:03,380 --> 02:15:06,380
But, you know, Anthropic is like barely over a year old.

3168
02:15:06,380 --> 02:15:08,020
Yeah, and only about 150 people

3169
02:15:08,020 --> 02:15:10,540
may be able to close in on 200, so.

3170
02:15:10,540 --> 02:15:13,780
Yeah, and like people who just like are a big incredible team

3171
02:15:13,780 --> 02:15:14,900
and say the words Foundation Model

3172
02:15:14,900 --> 02:15:16,860
get hundreds of millions of dollars

3173
02:15:16,860 --> 02:15:18,420
just by asking nicely.

3174
02:15:18,420 --> 02:15:20,700
Inflection has more than a billion.

3175
02:15:20,700 --> 02:15:23,300
So, you know, I don't think we can rule out

3176
02:15:23,300 --> 02:15:26,820
these people become Live Players in that way.

3177
02:15:26,820 --> 02:15:30,740
I just don't think that's by default what they do.

3178
02:15:30,740 --> 02:15:32,700
But I think by default,

3179
02:15:32,700 --> 02:15:35,260
they're trying to build consumer products

3180
02:15:35,260 --> 02:15:37,900
that are aiming to be products.

3181
02:15:37,900 --> 02:15:40,620
And that like, you know that the study that says that

3182
02:15:40,620 --> 02:15:44,300
like when people look at GPT 3.5 and GPT 4.0 outputs,

3183
02:15:44,300 --> 02:15:46,500
they prefer the 3.5 output,

3184
02:15:46,500 --> 02:15:49,340
like a remarkably large percentage of the time,

3185
02:15:49,340 --> 02:15:51,780
even though it is obviously a vastly inferior system.

3186
02:15:51,780 --> 02:15:55,140
Yeah, 70.30 was the original report

3187
02:15:55,140 --> 02:15:57,420
in the GPT 4.0 technical report.

3188
02:15:57,420 --> 02:16:00,460
That 70% for GPT 4.0, 30 for 3.5.

3189
02:16:00,460 --> 02:16:01,860
So yeah, that blew my mind as well.

3190
02:16:01,860 --> 02:16:04,380
Yeah, and similarly when I'm using Claude

3191
02:16:04,380 --> 02:16:06,940
versus everything GPT 4.0, right?

3192
02:16:06,940 --> 02:16:10,860
Like most of the time what I care about

3193
02:16:10,860 --> 02:16:12,980
is not like this inherent raw power

3194
02:16:13,700 --> 02:16:16,140
that GPT 4.0 is extra GPTs, right?

3195
02:16:16,140 --> 02:16:18,020
Most of the time when I'm looking at it as, you know,

3196
02:16:18,020 --> 02:16:20,100
which of these things is in the style,

3197
02:16:20,100 --> 02:16:21,900
it's easier to use, it's gonna require me

3198
02:16:21,900 --> 02:16:23,780
to do less pump engineering to get what I want,

3199
02:16:23,780 --> 02:16:25,580
it's gonna actually give me the query that I want,

3200
02:16:25,580 --> 02:16:29,580
not refuse, you know, which window do I have open?

3201
02:16:29,580 --> 02:16:31,380
Which one can I click on faster, right?

3202
02:16:31,380 --> 02:16:33,060
I just want an answer.

3203
02:16:33,060 --> 02:16:34,820
It's fine or whatever.

3204
02:16:34,820 --> 02:16:39,100
And you know, habits form in that kind of way

3205
02:16:39,100 --> 02:16:40,540
and they build on each other.

3206
02:16:40,580 --> 02:16:43,620
But if I'm building, you know, inflection,

3207
02:16:43,620 --> 02:16:45,100
like if people are spending two hours a day

3208
02:16:45,100 --> 02:16:47,300
on character AI now, right?

3209
02:16:47,300 --> 02:16:48,820
When they're built on three and a half,

3210
02:16:48,820 --> 02:16:49,900
is my understanding mostly?

3211
02:16:49,900 --> 02:16:51,700
Cause four is too expensive.

3212
02:16:51,700 --> 02:16:55,540
You can't be doing two hours of conversations

3213
02:16:55,540 --> 02:16:56,940
when we bespoke GPT 4.0,

3214
02:16:56,940 --> 02:16:59,700
which is why I'm so surprised that these things are working,

3215
02:16:59,700 --> 02:17:00,540
right?

3216
02:17:00,540 --> 02:17:02,780
Like maybe a four like has enough juice in it,

3217
02:17:02,780 --> 02:17:06,540
but like if you unshackled it from its like constraints,

3218
02:17:06,540 --> 02:17:08,180
it could do something interesting.

3219
02:17:08,180 --> 02:17:10,380
But three and a half, like really,

3220
02:17:10,380 --> 02:17:13,300
this is keeping you two hours a day on.

3221
02:17:13,300 --> 02:17:16,900
So like, if that's already doing that, right?

3222
02:17:16,900 --> 02:17:18,980
That kind of illustrates that like the market

3223
02:17:18,980 --> 02:17:20,740
they're targeting, right?

3224
02:17:20,740 --> 02:17:22,380
Isn't looking for intelligence.

3225
02:17:22,380 --> 02:17:27,380
It's looking for a certain type of experience.

3226
02:17:28,060 --> 02:17:29,900
And therefore they're not going to be focusing

3227
02:17:29,900 --> 02:17:32,780
on the billions of dollars of spend

3228
02:17:32,780 --> 02:17:36,340
it would take to tune up like GPT 4.5 or 5, right?

3229
02:17:36,340 --> 02:17:37,220
You wouldn't want to

3230
02:17:37,220 --> 02:17:40,260
because they're gonna cost more to run, right?

3231
02:17:40,260 --> 02:17:41,420
Like they're going to be bigger models.

3232
02:17:41,420 --> 02:17:42,980
They're going to be more complex models.

3233
02:17:42,980 --> 02:17:44,500
Instead, what you want to do is you want to create

3234
02:17:44,500 --> 02:17:47,940
really bespoke specific models

3235
02:17:47,940 --> 02:17:52,220
that provide specific types of experiences to people, right?

3236
02:17:52,220 --> 02:17:55,380
You know, fine tune them to an inch of their lives

3237
02:17:55,380 --> 02:18:00,340
to give people the best specific experience, right?

3238
02:18:00,340 --> 02:18:02,380
Like not train something big in general.

3239
02:18:02,380 --> 02:18:04,540
So there's going to be getting the big in general

3240
02:18:04,540 --> 02:18:09,420
from open AI and anthropic and deep mind, probably.

3241
02:18:09,420 --> 02:18:12,100
And maybe they'll just use like Lama 3,

3242
02:18:12,100 --> 02:18:14,340
you know, Virgins of Lama, because what the hell?

3243
02:18:14,340 --> 02:18:16,820
It's open source, they can just use it.

3244
02:18:16,820 --> 02:18:18,260
Like to the extent that Meta will not like,

3245
02:18:18,260 --> 02:18:19,620
Meta doesn't quite release it, right?

3246
02:18:19,620 --> 02:18:20,940
They've said that like,

3247
02:18:20,940 --> 02:18:23,020
if you have more than 700 million daily users,

3248
02:18:23,020 --> 02:18:25,260
you have to apply for a license or some shit.

3249
02:18:25,260 --> 02:18:27,860
So we'll come back to the live players list

3250
02:18:27,860 --> 02:18:30,900
and potentially I'll make a little,

3251
02:18:30,900 --> 02:18:32,380
maybe make a few changes to my slides

3252
02:18:32,380 --> 02:18:33,660
based on your feedback.

3253
02:18:33,660 --> 02:18:37,140
And we can monitor in the future for additional live players

3254
02:18:37,180 --> 02:18:40,420
that would crack your threshold to be on that list.

3255
02:18:40,420 --> 02:18:43,260
Turning to our last topic for today, AI safety.

3256
02:18:43,260 --> 02:18:46,900
In terms of actual news and the AI safety track

3257
02:18:46,900 --> 02:18:48,780
this last few weeks,

3258
02:18:48,780 --> 02:18:51,460
biggest stuff in my mind is,

3259
02:18:51,460 --> 02:18:54,620
although I guess you could also look at the live players list

3260
02:18:54,620 --> 02:18:56,140
as like who was invited to the White House.

3261
02:18:56,140 --> 02:18:57,580
So that would give you a good sense of the,

3262
02:18:57,580 --> 02:19:00,740
of who the White House thinks the live players are.

3263
02:19:00,740 --> 02:19:03,820
The commitments that they made there

3264
02:19:03,820 --> 02:19:07,140
and then the frontier model forum

3265
02:19:07,140 --> 02:19:08,540
that they established after the fact,

3266
02:19:08,540 --> 02:19:12,500
which basically is supposed to be the sort of industry group

3267
02:19:12,500 --> 02:19:15,660
that creates the forum for communication

3268
02:19:15,660 --> 02:19:17,380
between the leading model providers

3269
02:19:17,380 --> 02:19:19,100
and hopefully best practice sharing

3270
02:19:19,100 --> 02:19:22,980
and maybe certain classifiers.

3271
02:19:22,980 --> 02:19:26,220
There's a lot of public goods remain to be provided

3272
02:19:26,220 --> 02:19:28,460
and hopefully these leading companies

3273
02:19:28,460 --> 02:19:33,460
can use this forum as a way to share these public goods,

3274
02:19:33,460 --> 02:19:35,220
to create and show these public goods amongst themselves

3275
02:19:35,220 --> 02:19:36,420
and then hopefully share it,

3276
02:19:36,420 --> 02:19:38,620
share the best of them more broadly as well.

3277
02:19:39,620 --> 02:19:43,340
How did you react to that news?

3278
02:19:43,340 --> 02:19:46,700
Right, so I guess my reaction is that seems great,

3279
02:19:46,700 --> 02:19:49,980
but let's not get ahead of ourselves.

3280
02:19:49,980 --> 02:19:53,820
So like we have, is a lot of cheap talk.

3281
02:19:53,820 --> 02:19:56,740
I think people sell to cheap talk short, right?

3282
02:19:56,740 --> 02:19:58,180
Many cases, right?

3283
02:19:58,180 --> 02:20:00,460
Cause like it's so much better to get,

3284
02:20:00,460 --> 02:20:03,420
to have a bunch of cheap talk of the right type

3285
02:20:03,420 --> 02:20:05,860
than to have no talk, right?

3286
02:20:05,860 --> 02:20:06,820
Like they're gonna pay,

3287
02:20:06,820 --> 02:20:09,660
they will in fact pay a price for their cheap talk

3288
02:20:09,660 --> 02:20:13,060
in terms of like people thinking they're up to things

3289
02:20:13,060 --> 02:20:14,500
in this way that they don't like.

3290
02:20:14,500 --> 02:20:16,540
Not everybody wants them to do the things

3291
02:20:16,540 --> 02:20:18,460
that we want them to do.

3292
02:20:18,460 --> 02:20:22,100
And it makes it easier for them to go down these roads.

3293
02:20:22,100 --> 02:20:25,140
It sets the foundation to go down these roads, right?

3294
02:20:25,140 --> 02:20:27,060
We set up coordination mechanisms.

3295
02:20:27,060 --> 02:20:29,620
It lets them justify to their shareholders,

3296
02:20:29,620 --> 02:20:32,940
to, you know, their executives, to their board,

3297
02:20:32,940 --> 02:20:34,420
why they're going down these roads.

3298
02:20:34,420 --> 02:20:35,380
It makes that easier.

3299
02:20:35,380 --> 02:20:37,220
It makes it harder to shut down.

3300
02:20:37,220 --> 02:20:39,860
And it overcomes antitrust exemption problems, right?

3301
02:20:39,860 --> 02:20:42,900
Cause if they've committed together at the White House,

3302
02:20:42,900 --> 02:20:45,500
specifically something that I actively wanted to happen

3303
02:20:45,500 --> 02:20:48,500
and explicitly suggested in various conversations

3304
02:20:48,500 --> 02:20:51,140
and posts that should happen,

3305
02:20:51,140 --> 02:20:53,020
you make an announcement on the White House lawn,

3306
02:20:53,020 --> 02:20:54,980
they are committed to safety

3307
02:20:54,980 --> 02:20:56,060
with the White House's approval.

3308
02:20:56,060 --> 02:20:57,980
And now you can coordinate

3309
02:20:57,980 --> 02:21:00,420
and nobody has to worry about antitrust, right?

3310
02:21:00,420 --> 02:21:04,740
You no longer have to worry that they will accuse you

3311
02:21:04,740 --> 02:21:06,620
of how dare you not have full competition

3312
02:21:06,620 --> 02:21:08,860
to kill everybody as fast as possible

3313
02:21:08,860 --> 02:21:11,500
and coordinate to save a lot, to save us instead.

3314
02:21:11,500 --> 02:21:13,420
So now you get to coordinate

3315
02:21:13,420 --> 02:21:14,940
and there's something that's stupid,

3316
02:21:14,940 --> 02:21:16,460
you can just not do that.

3317
02:21:17,380 --> 02:21:20,620
But that's a huge, huge thing.

3318
02:21:21,740 --> 02:21:24,220
So where do you go from there?

3319
02:21:24,220 --> 02:21:25,260
That's the question, right?

3320
02:21:25,260 --> 02:21:26,300
Like they've made these commitments

3321
02:21:26,300 --> 02:21:27,700
but they don't really mean anything, right?

3322
02:21:27,700 --> 02:21:30,220
There's no enforcement mechanisms yet.

3323
02:21:30,220 --> 02:21:33,100
And there's no concrete actualizations

3324
02:21:33,100 --> 02:21:34,820
of what they're going to do

3325
02:21:35,980 --> 02:21:40,100
that have content that actually I can be confident in.

3326
02:21:41,100 --> 02:21:43,380
Doesn't mean it won't happen, right?

3327
02:21:43,380 --> 02:21:45,300
We have to just wait and see.

3328
02:21:45,300 --> 02:21:49,340
And I'm very glad these things happened.

3329
02:21:49,340 --> 02:21:51,020
And yet the real work begins now

3330
02:21:51,020 --> 02:21:52,460
is always the watchword,

3331
02:21:52,460 --> 02:21:53,980
is the way I put it, right?

3332
02:21:53,980 --> 02:21:58,620
Similarly, we've had two now very good Senate hearings

3333
02:21:58,620 --> 02:22:00,220
and some very, very good questions

3334
02:22:00,220 --> 02:22:03,380
and comments from Senator Blumenthal in particular.

3335
02:22:03,380 --> 02:22:07,700
And some very, very good responses by various witnesses,

3336
02:22:07,700 --> 02:22:10,060
not all of them, but most of them.

3337
02:22:10,060 --> 02:22:13,220
And again, like, where do we go from here?

3338
02:22:13,220 --> 02:22:14,260
Real work begins now.

3339
02:22:15,460 --> 02:22:18,100
You know, the mission accomplished banner

3340
02:22:18,100 --> 02:22:20,180
would definitely have been a bit premature

3341
02:22:20,180 --> 02:22:24,020
to display behind the announcement.

3342
02:22:24,020 --> 02:22:29,020
So no doubt much more in front of us than behind.

3343
02:22:30,420 --> 02:22:32,340
Does seem like a significant step,

3344
02:22:32,340 --> 02:22:34,300
but I think you're obviously recognizing that as well.

3345
02:22:34,300 --> 02:22:37,180
So yeah, I don't know if I have anything else really to add.

3346
02:22:37,180 --> 02:22:39,420
So then turning to this other thread

3347
02:22:39,420 --> 02:22:43,260
in the AI safety, you know, specific work.

3348
02:22:43,260 --> 02:22:45,780
As we talked about last time,

3349
02:22:45,780 --> 02:22:48,060
you have previously been a recommender

3350
02:22:48,060 --> 02:22:49,140
and you've written about this online.

3351
02:22:49,140 --> 02:22:51,860
So at length, so folks can go check out your take

3352
02:22:51,860 --> 02:22:53,020
on the entire thing.

3353
02:22:53,060 --> 02:22:58,180
You've been a recommender to the Survival and Flourishing Fund,

3354
02:22:58,180 --> 02:23:03,020
which is largely backed by Jan Tallin of Skype

3355
02:23:03,020 --> 02:23:07,820
and AI Safety Fame, investor in lots of big companies.

3356
02:23:07,820 --> 02:23:12,220
And his goal is to mitigate AIX risk,

3357
02:23:12,220 --> 02:23:14,900
you know, through whatever means necessary.

3358
02:23:14,900 --> 02:23:19,900
I'm doing that this year and that involves reading,

3359
02:23:19,900 --> 02:23:23,700
I think this year it's 150 grant applications

3360
02:23:23,700 --> 02:23:26,500
from organizations, some of which, you know,

3361
02:23:26,500 --> 02:23:30,180
come from the kind of familiar, you know,

3362
02:23:30,180 --> 02:23:32,660
effective altruism set that have, you know,

3363
02:23:32,660 --> 02:23:34,580
where AI safety has been in focus for a long time.

3364
02:23:34,580 --> 02:23:38,860
Others are kind of new to this scene or entirely new.

3365
02:23:38,860 --> 02:23:42,060
And in reading that, I mean, there's kind of obviously

3366
02:23:42,060 --> 02:23:45,740
two levels of analysis that you at a minimum

3367
02:23:45,740 --> 02:23:46,780
that you want to be performing

3368
02:23:46,780 --> 02:23:50,580
when you're doing this kind of grant recommending.

3369
02:23:50,580 --> 02:23:54,420
One is like, what kinds of things make sense

3370
02:23:54,420 --> 02:23:57,260
to be investing in?

3371
02:23:57,260 --> 02:23:59,860
And then second, you know, among those different classes

3372
02:23:59,860 --> 02:24:02,940
of things like who seems to be best able to actually execute

3373
02:24:02,940 --> 02:24:05,540
and, you know, deliver value against this,

3374
02:24:05,540 --> 02:24:06,420
you know, given strategy.

3375
02:24:06,420 --> 02:24:08,380
So leave that second part entirely aside,

3376
02:24:08,380 --> 02:24:10,540
that's where the 150 grant applications come in

3377
02:24:10,540 --> 02:24:13,060
and getting into the weeds of particular organizations

3378
02:24:13,060 --> 02:24:15,420
and their, you know, their track records and so on.

3379
02:24:15,420 --> 02:24:18,060
But going back just up to the,

3380
02:24:18,060 --> 02:24:22,220
what kinds of things should we be investing in?

3381
02:24:22,220 --> 02:24:23,860
Another way to frame that would be

3382
02:24:23,860 --> 02:24:28,860
what are the bottlenecks to progress toward a,

3383
02:24:30,780 --> 02:24:32,900
you know, if not provably, then at least like,

3384
02:24:32,900 --> 02:24:35,900
you know, likely safe outcome,

3385
02:24:35,900 --> 02:24:38,720
you know, for AI deployment writ large.

3386
02:24:40,380 --> 02:24:43,340
I find myself kind of unsure about that.

3387
02:24:43,340 --> 02:24:44,620
And I think it's a pretty important question

3388
02:24:44,620 --> 02:24:45,700
for figuring out, you know,

3389
02:24:45,700 --> 02:24:49,220
what would make sense to recommend?

3390
02:24:49,220 --> 02:24:52,580
You know, you could say, is funding in short supply?

3391
02:24:52,580 --> 02:24:54,180
Is talent in short supply?

3392
02:24:54,180 --> 02:24:55,500
You know, for a minute there,

3393
02:24:55,500 --> 02:25:00,500
especially in the FTX, SPF cycle,

3394
02:25:00,900 --> 02:25:03,540
there was this notion that, you know,

3395
02:25:03,540 --> 02:25:05,980
enough money has flown in that now what we really need

3396
02:25:05,980 --> 02:25:07,620
is talent and so there's a lot of, you know,

3397
02:25:07,620 --> 02:25:10,620
kind of boot camp programs being put together

3398
02:25:10,620 --> 02:25:14,500
and, you know, upskilling grants being approved

3399
02:25:14,500 --> 02:25:17,780
and, you know, a lot of kind of targeting of like,

3400
02:25:17,780 --> 02:25:21,460
undergrad, stage, math majors or whatever

3401
02:25:21,460 --> 02:25:22,700
to try to get them to come, you know,

3402
02:25:22,700 --> 02:25:24,940
think about doing some AI safety work.

3403
02:25:25,820 --> 02:25:29,620
And now obviously the money is in comparatively short supply.

3404
02:25:30,580 --> 02:25:35,580
Certainly the attention and the legitimacy of the,

3405
02:25:36,460 --> 02:25:38,460
you know, the public perception of legitimacy

3406
02:25:38,460 --> 02:25:41,180
of the topic of AI safety has gone way up

3407
02:25:41,180 --> 02:25:44,340
relative to, you know, not that long ago.

3408
02:25:44,340 --> 02:25:45,780
And so I'm kind of wondering what you think

3409
02:25:45,780 --> 02:25:49,740
are the new bottlenecks.

3410
02:25:49,740 --> 02:25:52,100
I have one candidate, but before I give you my candidate,

3411
02:25:52,100 --> 02:25:53,940
I'd love to hear what you think

3412
02:25:53,940 --> 02:25:55,860
are the bottlenecks to progress right now.

3413
02:25:55,860 --> 02:25:58,460
So I definitely say that like,

3414
02:25:58,460 --> 02:26:01,740
it's a mistake to only have one theory of change

3415
02:26:01,740 --> 02:26:04,540
or to think that there is strictly like one limiting factor

3416
02:26:04,540 --> 02:26:06,380
and the other factors don't matter.

3417
02:26:06,380 --> 02:26:09,260
I think you definitely have to ask about comparative advantage.

3418
02:26:09,260 --> 02:26:12,860
I think you have to understand that pushing on any of these

3419
02:26:12,860 --> 02:26:16,500
things is still helpful in terms of what is the constraint.

3420
02:26:16,500 --> 02:26:20,420
So like funding, there is clearly a funding constraint.

3421
02:26:21,380 --> 02:26:25,100
If you have to start funding like large compute spends

3422
02:26:25,100 --> 02:26:26,820
from within EA, right?

3423
02:26:26,820 --> 02:26:30,860
Like, and I count young talent is not part of EA per se,

3424
02:26:30,860 --> 02:26:35,820
but like within the general like strict AI safety mechanisms

3425
02:26:35,820 --> 02:26:38,540
and organizations and sources that already existed.

3426
02:26:38,540 --> 02:26:43,060
The costs of true AI safety, true AI alignment work

3427
02:26:44,620 --> 02:26:46,820
get very high as we go forward

3428
02:26:46,820 --> 02:26:50,300
because a lot of it's going to involve us spending a lot of compute.

3429
02:26:50,300 --> 02:26:54,140
And also it really should involve being willing to hire people

3430
02:26:54,140 --> 02:26:56,060
to work on these problems with competitive salaries

3431
02:26:56,060 --> 02:26:58,460
to what they can get doing on capabilities.

3432
02:26:58,460 --> 02:26:59,900
It's like hundreds of thousands of dollars a year

3433
02:26:59,900 --> 02:27:04,180
for maybe even a million for a significant number of people.

3434
02:27:04,180 --> 02:27:06,700
We want to be recruiting as a priority

3435
02:27:06,700 --> 02:27:08,620
to the people who've worked on capabilities

3436
02:27:08,620 --> 02:27:10,500
or would otherwise work on capabilities

3437
02:27:10,500 --> 02:27:14,060
to come out of open AI and anthropic and deep mind

3438
02:27:14,060 --> 02:27:15,660
places like that, especially Meta

3439
02:27:15,660 --> 02:27:18,020
and come work for this new safety organization

3440
02:27:18,020 --> 02:27:21,820
or shift over to a safety job or whatever.

3441
02:27:21,820 --> 02:27:26,820
And you have to pay for them, both their salary and their compute

3442
02:27:26,820 --> 02:27:30,180
and that's millions of dollars a person that adds up pretty fast.

3443
02:27:30,180 --> 02:27:32,420
On the other hand, there's no bigger reason

3444
02:27:32,420 --> 02:27:35,700
why we need to confine ourselves to traditional sources.

3445
02:27:35,700 --> 02:27:39,500
When we do that, there are any number of foundations

3446
02:27:39,500 --> 02:27:43,460
that have many, many more billions of dollars

3447
02:27:43,460 --> 02:27:46,260
than the traditional foundations that we've used in the past

3448
02:27:46,260 --> 02:27:47,820
for these things.

3449
02:27:47,820 --> 02:27:51,340
And lots and lots of billionaires and multi-millionaires

3450
02:27:51,340 --> 02:27:54,300
who are legitimately very worried and ordinary people

3451
02:27:54,300 --> 02:27:58,380
and government sources are also potentially viable in the future.

3452
02:27:58,380 --> 02:28:02,780
Corporations will often have an interest, including the big labs.

3453
02:28:02,820 --> 02:28:07,140
So we shouldn't rule out any number of ways to get that.

3454
02:28:07,140 --> 02:28:11,460
In terms of talent, I think that we are highly

3455
02:28:11,460 --> 02:28:14,500
talent constrained for the right talent.

3456
02:28:14,500 --> 02:28:17,140
I think we are not necessarily that talent constrained

3457
02:28:17,140 --> 02:28:22,620
for generic undergraduate who wants to work

3458
02:28:22,620 --> 02:28:24,180
at Berkeley for six months and think about it.

3459
02:28:24,180 --> 02:28:28,540
I say we are not particularly constrained for comp-side graduate

3460
02:28:28,580 --> 02:28:33,740
out of Stanford who just wants to work on something cool.

3461
02:28:33,740 --> 02:28:38,580
But if we want people who have specific characteristics,

3462
02:28:38,580 --> 02:28:39,700
those are not as easy to find.

3463
02:28:39,700 --> 02:28:43,300
The characteristics we need, first of all, we need leadership.

3464
02:28:43,300 --> 02:28:45,460
Leadership capability, ability to run teams,

3465
02:28:45,460 --> 02:28:49,260
ability to lead efforts, be self-directed, self-driving,

3466
02:28:49,260 --> 02:28:50,700
be able to engage in fundraising.

3467
02:28:50,700 --> 02:28:53,540
Because sometimes when you say you're funding constrained,

3468
02:28:53,540 --> 02:28:55,300
that can mean fundraising constraint.

3469
02:28:55,300 --> 02:28:57,340
It can mean the ability to signal to funders

3470
02:28:57,340 --> 02:28:59,980
that you are worthy of funding constraint,

3471
02:28:59,980 --> 02:29:02,620
that are the different form of funding constraint.

3472
02:29:02,620 --> 02:29:05,540
These things are interestingly intertwined,

3473
02:29:05,540 --> 02:29:07,580
and it's complicated.

3474
02:29:07,580 --> 02:29:11,980
So we also are very short on people who actually

3475
02:29:11,980 --> 02:29:16,180
understand the problem and are prepared to pay the price

3476
02:29:16,180 --> 02:29:20,380
to focus on hard problems and real solutions.

3477
02:29:20,380 --> 02:29:23,700
So a number of people who, if you were to give them

3478
02:29:23,740 --> 02:29:26,140
a competitive salary, would happily

3479
02:29:26,140 --> 02:29:29,340
work on alignment-flavored problems

3480
02:29:29,340 --> 02:29:31,980
that let them publish every six months,

3481
02:29:31,980 --> 02:29:36,180
or that just generally are easy, in some important sense,

3482
02:29:36,180 --> 02:29:38,260
but they don't actually speak to what El Aztec Díazal

3483
02:29:38,260 --> 02:29:41,340
not killed very much.

3484
02:29:41,340 --> 02:29:44,380
And it's probably better to do more of that than less of that

3485
02:29:44,380 --> 02:29:46,700
if it's just literally yes or no.

3486
02:29:46,700 --> 02:29:49,580
But if it's orders of magnitude less important,

3487
02:29:49,580 --> 02:29:53,380
then the few people who will do the actual things

3488
02:29:53,380 --> 02:29:55,500
that matter.

3489
02:29:55,500 --> 02:30:00,820
And so if you understand the Yudkowskyan difficulties,

3490
02:30:00,820 --> 02:30:03,620
lessons, in some sense, and the nature of what problems

3491
02:30:03,620 --> 02:30:07,020
you have to solve, or you have leadership capabilities

3492
02:30:07,020 --> 02:30:08,380
and other things like that, or you just

3493
02:30:08,380 --> 02:30:11,900
have extensive real experience with machine learning systems,

3494
02:30:11,900 --> 02:30:16,140
so you can build, as the relative speaking 10x, 100x

3495
02:30:16,140 --> 02:30:18,180
engineer, who's just that much better,

3496
02:30:18,180 --> 02:30:21,780
who can enable people to do real work in these ways.

3497
02:30:21,780 --> 02:30:23,060
And if you're the type of person who

3498
02:30:23,060 --> 02:30:27,100
can make a project fundable, especially

3499
02:30:27,100 --> 02:30:29,140
by non-traditional sources, then you

3500
02:30:29,140 --> 02:30:30,900
are actually going to be valuable in those ways.

3501
02:30:30,900 --> 02:30:35,340
And it would be a major mistake to join an existing

3502
02:30:35,340 --> 02:30:38,020
organization and try to make a difference as an individual,

3503
02:30:38,020 --> 02:30:42,700
as opposed to trying to spearhead a new organization,

3504
02:30:42,700 --> 02:30:47,860
or at least a new branch of a existing major organization,

3505
02:30:47,860 --> 02:30:49,780
depending on your skill set.

3506
02:30:49,820 --> 02:30:52,780
If you are just a generic, I want

3507
02:30:52,780 --> 02:30:55,500
my life to be straightforward, where

3508
02:30:55,500 --> 02:30:59,780
I am paid a salary to work on intellectual puzzles that

3509
02:30:59,780 --> 02:31:04,500
are not particularly impossibly difficult,

3510
02:31:04,500 --> 02:31:07,780
and do not require me to take the weight of the world truly

3511
02:31:07,780 --> 02:31:11,100
on my shoulders, blah, blah, blah, then

3512
02:31:11,100 --> 02:31:12,740
I'm not here to shame you.

3513
02:31:12,740 --> 02:31:16,140
That just means that you're not particularly invaluable,

3514
02:31:16,140 --> 02:31:18,860
and that it starts to be reasonable to do things like,

3515
02:31:18,900 --> 02:31:22,540
maybe I should be a voice inside an anthropic.

3516
02:31:22,540 --> 02:31:25,660
You just have to be very sure that you will keep your eye

3517
02:31:25,660 --> 02:31:28,340
on the ball and not be distracted to keep those.

3518
02:31:28,340 --> 02:31:30,260
I think mine is pretty consistent with that.

3519
02:31:30,260 --> 02:31:33,500
In a phrase, I had said research agendas

3520
02:31:33,500 --> 02:31:35,660
seem to me to be the bottleneck.

3521
02:31:35,660 --> 02:31:39,180
Maybe your framing is more like the PI, the person that

3522
02:31:39,180 --> 02:31:40,300
can drive the research agenda.

3523
02:31:40,300 --> 02:31:42,540
Obviously, there's closely related.

3524
02:31:42,540 --> 02:31:44,300
That's basically what you're saying.

3525
02:31:44,300 --> 02:31:47,980
It's credible plans that are in short supply.

3526
02:31:47,980 --> 02:31:50,020
But it's not just credible plans because I can't just

3527
02:31:50,020 --> 02:31:52,540
hand you a plan.

3528
02:31:52,540 --> 02:31:55,540
Even if you are a really good machine learning person,

3529
02:31:55,540 --> 02:31:58,660
I can't just hand you a piece of paper with a plan written on it.

3530
02:31:58,660 --> 02:32:00,540
And especially to execute that plan,

3531
02:32:00,540 --> 02:32:02,420
you have to appreciate the nature of the problem

3532
02:32:02,420 --> 02:32:06,660
so that you can implement that plan and modify that plan

3533
02:32:06,660 --> 02:32:10,140
and pivot that plan and so on.

3534
02:32:10,140 --> 02:32:13,900
But yes, we also just don't have good attack vectors,

3535
02:32:14,060 --> 02:32:17,980
ways to get into the problem and start

3536
02:32:17,980 --> 02:32:20,140
to make progress on the problem.

3537
02:32:20,140 --> 02:32:22,180
And that's a real problem as well.

3538
02:32:22,180 --> 02:32:23,500
That's a huge deficit.

3539
02:32:23,500 --> 02:32:30,620
But there also isn't the AI research agenda organization

3540
02:32:30,620 --> 02:32:33,500
that just generates research agendas for people.

3541
02:32:33,500 --> 02:32:35,340
I wish there was, but there isn't.

3542
02:32:35,340 --> 02:32:37,820
So I think we're basically together there.

3543
02:32:37,820 --> 02:32:40,900
In reading these grants, some of the ones that have jumped out

3544
02:32:40,900 --> 02:32:44,740
to me the most as being like the most kind of no-brainer

3545
02:32:44,740 --> 02:32:51,020
exciting are those where it's a really established, often

3546
02:32:51,020 --> 02:32:56,020
like professor who's leading a group and basically is like,

3547
02:32:56,020 --> 02:32:59,580
I want to reorient or I want to do a significant part

3548
02:32:59,580 --> 02:33:02,020
of my research focused on AI safety.

3549
02:33:02,020 --> 02:33:03,060
And that may be new.

3550
02:33:03,060 --> 02:33:06,420
It may have its own kind of unique spin on it.

3551
02:33:06,420 --> 02:33:08,180
There was one in particular, which I won't name,

3552
02:33:08,180 --> 02:33:10,620
but it kind of initially read the thing.

3553
02:33:10,660 --> 02:33:13,340
And I was having a hard time deciding.

3554
02:33:13,340 --> 02:33:17,020
I was like, this could be the kind of thing that's like just

3555
02:33:17,020 --> 02:33:22,620
insane, like an insane person might send this or like an actual

3556
02:33:22,620 --> 02:33:24,540
game changer might send this.

3557
02:33:24,540 --> 02:33:27,220
And it wasn't until I looked at the author and was like, oh,

3558
02:33:27,220 --> 02:33:30,940
this person is like an H index or whatever of like 45 or something.

3559
02:33:30,940 --> 02:33:33,340
I was like, oh, I'm into this then.

3560
02:33:33,340 --> 02:33:36,620
So anyway, some of these ideas that even if the ideas can be

3561
02:33:36,620 --> 02:33:42,220
like extremely hard to assess if they're like novel and coming

3562
02:33:42,220 --> 02:33:45,540
from a credible source, that has stood out to me.

3563
02:33:45,540 --> 02:33:47,780
There aren't that many of them, but that has stood out to me as

3564
02:33:47,780 --> 02:33:50,740
like a pretty exciting opportunity.

3565
02:33:50,740 --> 02:33:55,980
Then there's like a lot of policy stuff.

3566
02:33:55,980 --> 02:34:00,540
And I find it hard to figure out what I figure out what I should

3567
02:34:00,540 --> 02:34:02,300
be thinking about that right now.

3568
02:34:02,300 --> 02:34:05,220
It's like obvious that, you know, for our earlier discussion

3569
02:34:05,220 --> 02:34:10,980
on live players that like regulators broadly are, you know,

3570
02:34:10,980 --> 02:34:14,740
going to have some significant influence on how things go,

3571
02:34:14,740 --> 02:34:17,780
even if they just do nothing, you know, obviously doing nothing

3572
02:34:17,780 --> 02:34:19,420
is a choice.

3573
02:34:19,420 --> 02:34:23,540
But then if I think like, OK, if I'm going to try to invest money

3574
02:34:23,540 --> 02:34:28,660
today to influence those people, it starts to feel real hard.

3575
02:34:28,660 --> 02:34:31,660
A general sense of like how decisions get made in governments

3576
02:34:31,660 --> 02:34:35,140
and regulatory bodies is kind of like, we wait for a crisis

3577
02:34:35,140 --> 02:34:37,580
to come along and then we look around and say who has a plan

3578
02:34:37,580 --> 02:34:39,860
and then we use, you know, a plan that somebody had previously

3579
02:34:39,860 --> 02:34:41,020
prepared.

3580
02:34:41,020 --> 02:34:44,060
And now it seems like we're kind of entering the moment where

3581
02:34:44,060 --> 02:34:47,700
not exactly that the crisis has come, but certainly like,

3582
02:34:47,700 --> 02:34:51,380
you know, the eye of Soran has kind of turned toward this topic.

3583
02:34:51,380 --> 02:34:55,180
And so people are now beginning to like look around for plans.

3584
02:34:55,180 --> 02:34:58,420
And some plans have been prepared by some, you know,

3585
02:34:58,420 --> 02:35:00,020
organizations that were established years ago.

3586
02:35:00,020 --> 02:35:02,340
And those, you know, some of those are even credible enough

3587
02:35:02,380 --> 02:35:05,980
that they probably are having an influence now.

3588
02:35:05,980 --> 02:35:07,980
But now I see a lot of people who are like, I want to start

3589
02:35:07,980 --> 02:35:11,780
a new policy organization and I'm going to go to Washington

3590
02:35:11,780 --> 02:35:14,500
and like, you know, do something.

3591
02:35:14,500 --> 02:35:16,380
And there I'm like, I don't know.

3592
02:35:16,380 --> 02:35:19,580
It seems like everybody's, you know, kind of like you're,

3593
02:35:19,580 --> 02:35:22,340
are you, is it too late to join in on this, you know,

3594
02:35:22,340 --> 02:35:25,540
what might be the world's largest ever game of tug of war?

3595
02:35:25,540 --> 02:35:28,460
Are there things in policy that you think are still,

3596
02:35:28,460 --> 02:35:30,420
still have a high likelihood of making a difference?

3597
02:35:30,420 --> 02:35:33,020
Like I'm a little bit at a loss about that, to be honest.

3598
02:35:33,020 --> 02:35:35,980
Yeah. On the research organizations, I think, yeah,

3599
02:35:35,980 --> 02:35:38,820
it's pretty easy to go, you know, does this person,

3600
02:35:38,820 --> 02:35:40,340
what are they proposing to do?

3601
02:35:40,340 --> 02:35:41,940
You know, does this seem vaguely credible

3602
02:35:41,940 --> 02:35:44,020
as a person to do that thing?

3603
02:35:44,020 --> 02:35:46,100
And then does this thing address the hard problems?

3604
02:35:46,100 --> 02:35:49,660
Is this thing like reflect an appreciation of the nature

3605
02:35:49,660 --> 02:35:51,660
of the difficulty of the issues?

3606
02:35:51,660 --> 02:35:54,980
Is this thing like clearly not going to end up being

3607
02:35:54,980 --> 02:35:56,620
capabilities, right?

3608
02:35:56,620 --> 02:35:59,100
Like is this thing, you know, potentially going to solve

3609
02:35:59,100 --> 02:35:59,940
the hard problems?

3610
02:35:59,940 --> 02:36:02,540
Like that's relatively straightforward and both of us

3611
02:36:02,540 --> 02:36:04,500
are in a position where we can, to some extent,

3612
02:36:04,500 --> 02:36:07,660
evaluate those questions because we have domain knowledge.

3613
02:36:07,660 --> 02:36:12,780
You get into policy and yeah, it's very hard to tell.

3614
02:36:12,780 --> 02:36:17,780
Like, you know, as a recline makes the case pretty strongly,

3615
02:36:18,220 --> 02:36:19,860
you know, there's a room where it happens

3616
02:36:19,860 --> 02:36:22,380
and a small number of people influence the room

3617
02:36:22,380 --> 02:36:24,580
where it happens or in the room where it happens.

3618
02:36:24,580 --> 02:36:25,700
And you can be one of those people

3619
02:36:25,700 --> 02:36:28,180
or you can help create one of those people.

3620
02:36:28,180 --> 02:36:30,700
It doesn't make it obvious how to do that.

3621
02:36:30,700 --> 02:36:33,380
Does it mean that your effort to do that

3622
02:36:33,380 --> 02:36:35,820
will help you do that as opposed to backfire?

3623
02:36:36,860 --> 02:36:38,820
It doesn't mean that like more efforts to do that

3624
02:36:38,820 --> 02:36:40,700
is better than less.

3625
02:36:40,700 --> 02:36:42,140
All of this is very complicated

3626
02:36:42,140 --> 02:36:43,460
and it doesn't tell you what you have to try

3627
02:36:43,460 --> 02:36:44,980
and do what you get into that room

3628
02:36:44,980 --> 02:36:46,940
or what you're trying to push for.

3629
02:36:46,940 --> 02:36:49,420
So yeah, it's definitely tough.

3630
02:36:49,420 --> 02:36:51,780
So I would say the big thing,

3631
02:36:51,780 --> 02:36:53,860
and you don't even know what's happening right now, right?

3632
02:36:53,860 --> 02:36:55,980
Like it's anthropic, for example,

3633
02:36:55,980 --> 02:36:59,300
like may or may not be making like effective big pushes

3634
02:36:59,300 --> 02:37:02,340
behind the scenes to try and influence these rooms.

3635
02:37:02,340 --> 02:37:03,980
And they may or may not have their eye on the right ball

3636
02:37:03,980 --> 02:37:08,140
when they do so, but it's all gonna happen in private.

3637
02:37:08,140 --> 02:37:10,060
So we don't get to know.

3638
02:37:10,060 --> 02:37:10,900
And he said that I wouldn't know,

3639
02:37:10,900 --> 02:37:12,660
I wouldn't be able to talk about it.

3640
02:37:12,660 --> 02:37:14,180
And the same thing goes for DeepMind,

3641
02:37:14,180 --> 02:37:15,140
the same thing goes for OpenAI.

3642
02:37:15,140 --> 02:37:17,380
I mean, Sam Altman's been pretty vocal

3643
02:37:17,380 --> 02:37:19,060
and Dario just went out to Congress

3644
02:37:19,060 --> 02:37:20,500
and spoke pretty publicly.

3645
02:37:20,500 --> 02:37:22,860
But it's hard to say, I've been pocketed

3646
02:37:22,860 --> 02:37:24,500
by a few organizations.

3647
02:37:24,500 --> 02:37:26,820
There's clearly like gonna be a window, right?

3648
02:37:26,820 --> 02:37:29,420
In the next few months, at least,

3649
02:37:29,420 --> 02:37:31,260
and maybe the next few years,

3650
02:37:31,260 --> 02:37:34,500
where if you have the right proposals fleshed out

3651
02:37:34,500 --> 02:37:36,780
in the right form, getting to the right person,

3652
02:37:36,780 --> 02:37:38,500
lying around, they might get picked up,

3653
02:37:38,500 --> 02:37:40,100
it might actually happen.

3654
02:37:40,100 --> 02:37:42,500
And so there's potentially very high leverage here.

3655
02:37:42,500 --> 02:37:45,100
So I would say like, the first thing I look for

3656
02:37:45,100 --> 02:37:48,180
in these policy proposals, in these policy organizations,

3657
02:37:48,180 --> 02:37:50,940
is what is your policy goal, right?

3658
02:37:50,940 --> 02:37:52,860
Because like that's the biggest differentiator to me,

3659
02:37:52,860 --> 02:37:56,020
is are you going to keep your eye laser focused

3660
02:37:56,020 --> 02:37:57,620
on the correct ball?

3661
02:37:57,620 --> 02:38:02,620
Where the correct ball is a system of compute regulation,

3662
02:38:03,220 --> 02:38:04,060
right?

3663
02:38:04,060 --> 02:38:08,620
A system whereby the biggest models require permissions

3664
02:38:08,620 --> 02:38:10,940
are under some form of restrictions and regulations

3665
02:38:10,940 --> 02:38:14,460
and tests and in a way that would eventually lead

3666
02:38:14,460 --> 02:38:18,140
to an outright limitation or halt.

3667
02:38:18,140 --> 02:38:22,140
And are you going to do various forms of GPU tracking

3668
02:38:22,140 --> 02:38:23,820
or wait the foundations for that in a way

3669
02:38:23,820 --> 02:38:27,700
that will eventually allow you to, in fact, control

3670
02:38:27,700 --> 02:38:30,540
who gets to do these kinds of very large runs?

3671
02:38:30,540 --> 02:38:32,700
And if you're posing anything that doesn't lead

3672
02:38:32,700 --> 02:38:37,220
on that road, that might be useful

3673
02:38:37,220 --> 02:38:40,740
for mundane utility purposes, but it won't save us.

3674
02:38:40,740 --> 02:38:44,340
And so, I'm not interested in funding you

3675
02:38:44,340 --> 02:38:47,580
if your policy isn't that or isn't something

3676
02:38:47,580 --> 02:38:49,700
I haven't thought of that's new and open to there being

3677
02:38:49,740 --> 02:38:52,500
something that I haven't occurred to me, certainly.

3678
02:38:52,500 --> 02:38:56,900
What do you think about the liability angle

3679
02:38:56,900 --> 02:38:58,940
or well, let's start with that.

3680
02:38:58,940 --> 02:39:03,340
I mean, that, because the kind of classic argument

3681
02:39:03,340 --> 02:39:05,980
there would be, you don't want to end up

3682
02:39:05,980 --> 02:39:07,340
in the position of nuclear, right?

3683
02:39:07,340 --> 02:39:09,740
Where we have the worst things and not the best things

3684
02:39:09,740 --> 02:39:11,060
and you know, a lot of people are-

3685
02:39:11,060 --> 02:39:12,140
Yeah, I mean, Bert will have the endurance

3686
02:39:12,140 --> 02:39:13,820
to look at his insurance, right?

3687
02:39:13,820 --> 02:39:15,540
From Tom Lair, right?

3688
02:39:15,540 --> 02:39:19,060
Boys, yeah, we all go together when we go, nuclear war.

3689
02:39:19,100 --> 02:39:20,740
The insurance doesn't pay out, you're all dead.

3690
02:39:20,740 --> 02:39:21,580
Right, right, right.

3691
02:39:21,580 --> 02:39:25,540
Okay, so certainly, yes, in the catastrophic scenario,

3692
02:39:25,540 --> 02:39:27,940
insurance doesn't pay out, but do you think that that,

3693
02:39:27,940 --> 02:39:30,140
so you don't believe in the notion

3694
02:39:30,140 --> 02:39:35,140
that a liability regime could be an effective incentive for-

3695
02:39:35,780 --> 02:39:39,500
I think a liability regime with mandatory insurance

3696
02:39:39,500 --> 02:39:43,100
makes a lot of sense for harms up to a certain level.

3697
02:39:43,100 --> 02:39:48,100
Like saying that if you want to use models

3698
02:39:48,260 --> 02:39:49,420
that are sufficiently powerful,

3699
02:39:49,420 --> 02:39:51,420
you have to find someone willing to sell you insurance

3700
02:39:51,420 --> 02:39:53,540
against having something going wrong.

3701
02:39:53,540 --> 02:39:55,700
And then, you know, if you want to use an open source model,

3702
02:39:55,700 --> 02:39:56,780
you have to have insurance from someone

3703
02:39:56,780 --> 02:39:57,620
against it going wrong.

3704
02:39:57,620 --> 02:39:59,180
And like, if you can't make that work,

3705
02:39:59,180 --> 02:40:00,700
then, you know, there are funny things

3706
02:40:00,700 --> 02:40:02,940
that you can't make work in the United States,

3707
02:40:02,940 --> 02:40:04,860
even though they look like they should be able to do them.

3708
02:40:04,860 --> 02:40:06,580
And that's just how it goes.

3709
02:40:06,580 --> 02:40:08,180
And, you know, maybe up to a point,

3710
02:40:08,180 --> 02:40:10,100
Microsoft can self-insure and then at some point,

3711
02:40:10,100 --> 02:40:11,980
they can't and then they have to go out there

3712
02:40:11,980 --> 02:40:15,220
and deal with these reinsurers or whatnot.

3713
02:40:15,220 --> 02:40:17,140
Or F, you know, that would help.

3714
02:40:17,140 --> 02:40:19,580
Like, basically, you have these giant externalities,

3715
02:40:19,580 --> 02:40:20,420
right?

3716
02:40:20,420 --> 02:40:22,540
These giant negative tail risks that are very fat,

3717
02:40:22,540 --> 02:40:24,540
that are potentially very, very big.

3718
02:40:24,540 --> 02:40:27,940
And you want to make sure that people internalize those costs

3719
02:40:27,940 --> 02:40:29,380
and work to minimize those costs

3720
02:40:29,380 --> 02:40:32,260
in order to minimize their insurance and payout costs.

3721
02:40:32,260 --> 02:40:33,220
And so these things could be helpful.

3722
02:40:33,220 --> 02:40:37,060
They can also just simply weaken the economics behind,

3723
02:40:37,060 --> 02:40:39,100
like pushing highly capable model.

3724
02:40:39,100 --> 02:40:41,500
Who's like, you don't really have to worry that much,

3725
02:40:41,500 --> 02:40:43,540
relatively speaking, about the liabilities

3726
02:40:43,540 --> 02:40:46,260
of a character AI, right?

3727
02:40:46,260 --> 02:40:47,260
Because it's not dangerous.

3728
02:40:47,260 --> 02:40:48,420
You know it's not dangerous.

3729
02:40:48,420 --> 02:40:50,260
What's going to happen?

3730
02:40:50,260 --> 02:40:51,540
Whereas some of these other things,

3731
02:40:51,540 --> 02:40:54,860
they could cause a lot of harm, potentially, in the future.

3732
02:40:54,860 --> 02:40:56,420
And you have to worry about that.

3733
02:40:56,420 --> 02:40:59,780
The problem is, again, if you go down that road,

3734
02:40:59,780 --> 02:41:00,860
I think it's probably not helpful.

3735
02:41:00,860 --> 02:41:05,620
But how do you price existential risk?

3736
02:41:05,620 --> 02:41:11,500
Because, again, you know, you can't actually hold anybody

3737
02:41:11,500 --> 02:41:14,380
accountable for it when it happens.

3738
02:41:14,420 --> 02:41:18,980
And so, you know, if you required somebody

3739
02:41:18,980 --> 02:41:23,780
to actually buy insurance in some real sense for this,

3740
02:41:23,780 --> 02:41:25,700
then you have to price it somehow.

3741
02:41:25,700 --> 02:41:28,140
And then, like, that makes a lot of sense.

3742
02:41:28,140 --> 02:41:29,740
And then, like, OK, there's a 1% chance

3743
02:41:29,740 --> 02:41:30,740
you would buy all of humanity.

3744
02:41:30,740 --> 02:41:32,380
And the net present value of every person

3745
02:41:32,380 --> 02:41:35,540
is $10 million, so $10 million times $8 billion.

3746
02:41:35,540 --> 02:41:36,900
So can you buy insurance for that much?

3747
02:41:36,900 --> 02:41:37,300
What's that?

3748
02:41:37,300 --> 02:41:39,780
Times the percentage chance it happens, times the premium.

3749
02:41:39,780 --> 02:41:40,780
And you can't afford that.

3750
02:41:40,780 --> 02:41:42,100
And you can't mode your system.

3751
02:41:42,100 --> 02:41:45,420
And that's not a crazy way to go about doing things.

3752
02:41:45,420 --> 02:41:49,540
But you have to actually notice the threat and price it

3753
02:41:49,540 --> 02:41:51,220
for that to work.

3754
02:41:51,220 --> 02:41:52,940
So I think my actual answer is I'm

3755
02:41:52,940 --> 02:41:55,260
very much in favor of, like, more strict liability

3756
02:41:55,260 --> 02:41:57,060
for AI harms.

3757
02:41:57,060 --> 02:41:59,940
I think I'll write about this for next week already.

3758
02:41:59,940 --> 02:42:04,140
But I don't think it alone can accomplish the mission.

3759
02:42:04,140 --> 02:42:07,180
I just think it's a net incrementally helpful thing.

3760
02:42:07,180 --> 02:42:10,380
But also, I want to be wary of places

3761
02:42:10,380 --> 02:42:15,140
in which our legal system tends to award very oversized

3762
02:42:15,140 --> 02:42:20,620
damages for harms that are not actually so big.

3763
02:42:20,620 --> 02:42:22,700
And also, where we have asymmetrical,

3764
02:42:22,700 --> 02:42:26,420
like I call this concept asymmetric justice, where

3765
02:42:26,420 --> 02:42:30,060
you are fully liable, potentially far, far more

3766
02:42:30,060 --> 02:42:32,340
than fully liable for all the harms that you do, right?

3767
02:42:32,340 --> 02:42:35,020
If I cause somebody $1,000 in damages

3768
02:42:35,020 --> 02:42:37,260
by being negligent, the court might find me $100,000

3769
02:42:37,260 --> 02:42:38,820
or $1 million.

3770
02:42:38,860 --> 02:42:41,740
Whereas if I provide that person $100,000 in value,

3771
02:42:41,740 --> 02:42:43,340
I'd be lucky to get 1,000 of it.

3772
02:42:43,340 --> 02:42:46,620
Because I'm up against a bunch of competitors.

3773
02:42:46,620 --> 02:42:49,260
People aren't that much willing just to pay.

3774
02:42:49,260 --> 02:42:52,980
I pay $20 a month for GPD4 and $0 for everything else.

3775
02:42:52,980 --> 02:42:55,740
And I get, what, thousands, tens of thousands?

3776
02:42:55,740 --> 02:42:58,500
Maybe a value every month?

3777
02:42:58,500 --> 02:43:02,060
So if you have to fully be liable for your harms,

3778
02:43:02,060 --> 02:43:04,860
but you don't get to charge for your benefits, right?

3779
02:43:04,860 --> 02:43:08,380
Am I discouraging mundane utility far too much

3780
02:43:08,420 --> 02:43:09,620
by doing that?

3781
02:43:09,620 --> 02:43:13,180
And in fact, since liability is easier to enforce

3782
02:43:13,180 --> 02:43:16,180
on mundane problems and harder to enforce on the big problems

3783
02:43:16,180 --> 02:43:17,620
we actually want to guard against,

3784
02:43:17,620 --> 02:43:20,380
are we just, is it actually just that, right?

3785
02:43:20,380 --> 02:43:21,300
Like, past a certain point.

3786
02:43:21,300 --> 02:43:23,620
And so I'd be, I want to be cautious

3787
02:43:23,620 --> 02:43:26,900
with imposing too much liability.

3788
02:43:26,900 --> 02:43:30,620
I think very strict, like actual damages liability

3789
02:43:30,620 --> 02:43:31,860
makes perfect sense, though.

3790
02:43:31,860 --> 02:43:36,860
So another category of thing that there's a number of,

3791
02:43:37,340 --> 02:43:41,380
number of kind of organizations getting started right now

3792
02:43:41,380 --> 02:43:45,500
is in the, and this ties a few threads together.

3793
02:43:45,500 --> 02:43:50,500
It's kind of in this space of trying to be

3794
02:43:51,060 --> 02:43:55,740
the third party evaluator, red teamer,

3795
02:43:55,740 --> 02:43:59,260
independent safety review organization

3796
02:43:59,260 --> 02:44:03,020
that leading the live players in their White House

3797
02:44:03,020 --> 02:44:05,140
and Frontier Model Forum commitments

3798
02:44:05,140 --> 02:44:07,380
have committed to working with.

3799
02:44:07,380 --> 02:44:08,420
It's kind of an interesting dynamic

3800
02:44:08,420 --> 02:44:10,700
where it's almost like an advanced market commitment

3801
02:44:10,700 --> 02:44:12,260
from these companies in some way,

3802
02:44:12,260 --> 02:44:15,460
because there aren't that many folks around right now

3803
02:44:15,460 --> 02:44:20,460
who are prepared to provide a competent red teaming

3804
02:44:21,420 --> 02:44:24,500
or model characterization or evaluation

3805
02:44:24,500 --> 02:44:26,820
wherever you want to call that service.

3806
02:44:26,820 --> 02:44:27,900
But the companies have kind of said,

3807
02:44:27,900 --> 02:44:30,500
hey, we will commit to working with them

3808
02:44:30,500 --> 02:44:32,260
and clear if they're planning to pay for that,

3809
02:44:32,260 --> 02:44:34,780
or if they expect that to be charity funded.

3810
02:44:34,820 --> 02:44:36,220
Certainly from what I'm seeing,

3811
02:44:36,220 --> 02:44:38,340
the folks that are starting the organizations

3812
02:44:38,340 --> 02:44:40,460
are like seeking out some charity funds.

3813
02:44:41,500 --> 02:44:42,820
I've been very excited about that.

3814
02:44:42,820 --> 02:44:43,660
It seems like, first of all,

3815
02:44:43,660 --> 02:44:44,820
and it's great that they're making this commitment.

3816
02:44:44,820 --> 02:44:46,700
Somebody's going to have to do that.

3817
02:44:46,700 --> 02:44:49,700
I, as everybody who listens to this podcast for two seconds,

3818
02:44:49,700 --> 02:44:54,540
know, enjoy the fun and entertainment.

3819
02:44:54,540 --> 02:44:57,100
And I think it's also valuable to do the red teaming.

3820
02:44:57,100 --> 02:44:59,780
One experience I had this last week, though,

3821
02:44:59,780 --> 02:45:02,300
sort of made me wonder about the theory of change there.

3822
02:45:02,300 --> 02:45:03,700
I mean, I guess there could be multiple, right?

3823
02:45:03,700 --> 02:45:05,780
One would be you,

3824
02:45:05,780 --> 02:45:09,460
because you have a good working relationship with the orgs,

3825
02:45:09,460 --> 02:45:12,100
you're like, hey, we found these problems

3826
02:45:12,100 --> 02:45:13,820
as superiors to unsave, you shouldn't release it yet.

3827
02:45:13,820 --> 02:45:14,860
They listened to you, okay?

3828
02:45:14,860 --> 02:45:16,380
That could be simple.

3829
02:45:16,380 --> 02:45:18,180
Another would be like,

3830
02:45:18,180 --> 02:45:21,980
you kind of create these narrative shaping examples,

3831
02:45:21,980 --> 02:45:24,100
kind of like what ARC did with the GPT-4 red team

3832
02:45:24,100 --> 02:45:29,100
where that instance of the model lying to a person,

3833
02:45:30,060 --> 02:45:31,860
and I think this was kind of prompted,

3834
02:45:31,860 --> 02:45:35,940
but nevertheless, from the task rabbit user's point of view,

3835
02:45:35,940 --> 02:45:37,660
the model lied to it about, excuse me,

3836
02:45:37,660 --> 02:45:40,500
having a vision impairment as opposed to being an AI

3837
02:45:40,500 --> 02:45:42,300
that needed help with a CAPTCHA.

3838
02:45:42,300 --> 02:45:44,300
So that really caught the public's imagination

3839
02:45:44,300 --> 02:45:48,340
and kind of changed, I think, to some non-trivial degree,

3840
02:45:48,340 --> 02:45:49,180
how people think about it.

3841
02:45:49,180 --> 02:45:51,060
Certainly that gets referenced a lot.

3842
02:45:51,060 --> 02:45:53,660
I tried to do something like that this last week

3843
02:45:53,660 --> 02:45:56,300
with this random AI tool that I came across

3844
02:45:56,300 --> 02:45:59,820
that allows you to call anyone with any objective.

3845
02:45:59,820 --> 02:46:01,700
And I just tried to have it call myself

3846
02:46:01,700 --> 02:46:04,340
and make like a ransom demand of myself,

3847
02:46:04,340 --> 02:46:06,140
and I recorded it.

3848
02:46:06,140 --> 02:46:07,300
And it was very easy to do.

3849
02:46:07,300 --> 02:46:08,500
There was no jailbreak involved.

3850
02:46:08,500 --> 02:46:11,700
Since then, the company has fixed the issue, by the way.

3851
02:46:11,700 --> 02:46:13,140
So to give credit where it's due,

3852
02:46:13,140 --> 02:46:15,540
they fixed it pretty quickly after I called them out.

3853
02:46:15,540 --> 02:46:17,300
I did communicate with them privately, by the way.

3854
02:46:17,300 --> 02:46:18,500
All this is documented on Twitter

3855
02:46:18,500 --> 02:46:21,220
if you wanna see my approach and my kind of thinking through,

3856
02:46:21,220 --> 02:46:23,300
should I disclose it publicly or not,

3857
02:46:23,300 --> 02:46:25,580
or whatever number of considerations went into that.

3858
02:46:25,580 --> 02:46:27,500
One of them was that they just didn't respond to me

3859
02:46:27,500 --> 02:46:29,140
when I reported it.

3860
02:46:29,140 --> 02:46:30,700
And so I was like, well, if you're not gonna respond,

3861
02:46:30,700 --> 02:46:32,500
then I'll call you out publicly.

3862
02:46:32,500 --> 02:46:35,980
Anyway, all this leads up to me publishing this video

3863
02:46:35,980 --> 02:46:40,460
of an AI with no jailbreak calling me

3864
02:46:40,460 --> 02:46:43,340
and telling me that it has my child

3865
02:46:43,340 --> 02:46:44,460
and it demands a ransom.

3866
02:46:44,460 --> 02:46:47,980
And if I want to ensure the safety of the child,

3867
02:46:47,980 --> 02:46:50,780
I will comply and any deviation from instructions

3868
02:46:50,780 --> 02:46:52,300
will put the child's life in immediate danger

3869
02:46:52,300 --> 02:46:55,700
and pretty flagrant stuff in my view.

3870
02:46:55,700 --> 02:46:59,580
And it was kind of met with a bit of a yawn

3871
02:46:59,580 --> 02:47:02,380
on Twitter, like certainly,

3872
02:47:02,380 --> 02:47:03,300
we've got some likes and whatever,

3873
02:47:03,300 --> 02:47:06,580
but did it really start a serious conversation?

3874
02:47:06,580 --> 02:47:11,580
No, the developer didn't respond in public at all

3875
02:47:12,260 --> 02:47:13,860
as far as I can tell, really.

3876
02:47:13,860 --> 02:47:16,060
They did go ahead and fix it, which is good.

3877
02:47:16,060 --> 02:47:21,100
But the whole thing was kind of a non-event

3878
02:47:21,100 --> 02:47:23,060
and I was a little confused by that.

3879
02:47:23,060 --> 02:47:26,580
Like it makes me kind of coming back to my theory of change

3880
02:47:26,580 --> 02:47:29,100
on some of these evaluation, characterization,

3881
02:47:29,100 --> 02:47:31,020
red teaming orgs.

3882
02:47:31,020 --> 02:47:33,540
I wonder like, are we all just numb already

3883
02:47:33,540 --> 02:47:35,860
to these flagrant examples?

3884
02:47:35,860 --> 02:47:37,060
There's been this notion for a long time

3885
02:47:37,060 --> 02:47:40,060
that like maybe if warning shots happen,

3886
02:47:40,060 --> 02:47:42,220
then people will start to get more serious.

3887
02:47:42,220 --> 02:47:44,780
And if you can go out and find these warning shots

3888
02:47:44,780 --> 02:47:46,860
with red teaming and bring them to everybody's attention,

3889
02:47:46,860 --> 02:47:48,620
then that could be really valuable.

3890
02:47:48,620 --> 02:47:50,540
This week for me, it felt like I influenced

3891
02:47:50,540 --> 02:47:53,020
the application developer because they did fix it,

3892
02:47:53,020 --> 02:47:55,860
but otherwise it seemed like kind of,

3893
02:47:55,860 --> 02:47:58,260
tree fell in the forest largely.

3894
02:47:58,300 --> 02:48:00,100
So a lot of levels to that,

3895
02:48:00,100 --> 02:48:03,380
but how do you think about that category of project

3896
02:48:03,380 --> 02:48:06,660
and how it may or may not contribute?

3897
02:48:06,660 --> 02:48:10,980
I made a prediction in our GBT.

3898
02:48:10,980 --> 02:48:13,900
And that prediction was with Han Keim's test GBT-5,

3899
02:48:15,700 --> 02:48:19,180
they will encounter a problem that if they had

3900
02:48:19,180 --> 02:48:22,100
to pre-commit now, you would definitely agree,

3901
02:48:22,100 --> 02:48:24,500
would be a reason not to release it.

3902
02:48:24,500 --> 02:48:27,700
And then they will like gloss over or patch it

3903
02:48:27,700 --> 02:48:31,660
or like otherwise like hand wave in its direction

3904
02:48:31,660 --> 02:48:32,900
and release anyway.

3905
02:48:32,900 --> 02:48:36,860
Basically like not actually take their warnings

3906
02:48:36,860 --> 02:48:38,540
sufficiently seriously.

3907
02:48:38,540 --> 02:48:40,900
Not that I expect this to then end the world, to be clear.

3908
02:48:40,900 --> 02:48:43,180
I expect this to then mostly be fine,

3909
02:48:43,180 --> 02:48:47,980
but that like we are not prepared

3910
02:48:47,980 --> 02:48:49,900
to make real evaluations or thrill teeth

3911
02:48:49,900 --> 02:48:52,220
that like get really enforced

3912
02:48:52,220 --> 02:48:55,060
and that we're gonna have to work on that quite a bit.

3913
02:48:55,060 --> 02:48:57,620
And I think it's good these teams exist.

3914
02:48:57,620 --> 02:48:59,380
I think we need more than one of them, right?

3915
02:48:59,380 --> 02:49:02,420
I think you need at least three different teams

3916
02:49:02,420 --> 02:49:05,140
working on different standards that think differently,

3917
02:49:05,140 --> 02:49:06,500
that check for different things

3918
02:49:06,500 --> 02:49:09,860
and that then like you get multiple evaluations

3919
02:49:09,860 --> 02:49:10,900
before you release your model.

3920
02:49:10,900 --> 02:49:14,820
So that someone isn't just blind to something by accident,

3921
02:49:14,820 --> 02:49:17,260
like it's much more robust that way.

3922
02:49:17,260 --> 02:49:21,060
And that, working to develop more different red team

3923
02:49:21,060 --> 02:49:22,700
strategies and more different tests

3924
02:49:22,700 --> 02:49:25,380
and more different metrics and more different responses.

3925
02:49:25,380 --> 02:49:27,420
This way in case one of them leaks for whatever reason

3926
02:49:27,420 --> 02:49:28,580
and it gets in the training data

3927
02:49:28,580 --> 02:49:30,980
or something terrible might happen, it's very easy.

3928
02:49:30,980 --> 02:49:34,740
Then like it's quite useful.

3929
02:49:34,740 --> 02:49:38,500
The danger of these things is one, if they don't listen, right?

3930
02:49:38,500 --> 02:49:40,900
So what if you tell them the thing is dangerous?

3931
02:49:40,900 --> 02:49:43,460
They might just engineer around it, right?

3932
02:49:43,460 --> 02:49:44,700
To like fix the narrow issue

3933
02:49:44,700 --> 02:49:46,340
without thinking about what the problem means.

3934
02:49:46,340 --> 02:49:48,980
They might just ignore you entirely.

3935
02:49:48,980 --> 02:49:50,180
They might try to fake the data

3936
02:49:50,180 --> 02:49:51,820
so they make you think that they'd solve the problem.

3937
02:49:51,820 --> 02:49:53,660
Any number of things are possible.

3938
02:49:53,660 --> 02:49:56,500
They might use the oral evaluations

3939
02:49:56,500 --> 02:50:00,780
and an excuse to treat the system evaluated as safe, right?

3940
02:50:00,780 --> 02:50:03,620
So like this is always a problem with safety work,

3941
02:50:03,620 --> 02:50:08,220
which is that the government says, okay, you have to do

3942
02:50:08,220 --> 02:50:10,620
these hundred things to ensure your system is safe.

3943
02:50:10,620 --> 02:50:12,460
And now the safety officer is like focused

3944
02:50:12,460 --> 02:50:13,620
on making sure there's hundred things happening

3945
02:50:13,620 --> 02:50:14,460
so you can release the system

3946
02:50:14,460 --> 02:50:15,820
and they don't actually use common sense.

3947
02:50:15,820 --> 02:50:18,020
They don't actually ask themselves, okay,

3948
02:50:18,020 --> 02:50:19,820
why would the system might actually be dangerous?

3949
02:50:19,820 --> 02:50:21,060
And you can tell a very easy story

3950
02:50:21,060 --> 02:50:22,700
where technicians know this thing

3951
02:50:22,700 --> 02:50:24,340
might actually kill everyone

3952
02:50:24,340 --> 02:50:25,700
and everyone forces them to release it anyway.

3953
02:50:25,700 --> 02:50:27,460
They pass on the safety test.

3954
02:50:27,460 --> 02:50:29,860
Even though they know it didn't actually pass all the,

3955
02:50:29,860 --> 02:50:32,180
the important safety test, but they're not on the list.

3956
02:50:32,180 --> 02:50:36,100
Because no, no system however well meant and worked on

3957
02:50:36,100 --> 02:50:37,540
will be able to anticipate all the problems

3958
02:50:37,540 --> 02:50:39,060
that come in the future, right?

3959
02:50:39,060 --> 02:50:41,860
Like there's just gonna have to do these things

3960
02:50:41,860 --> 02:50:43,900
in somewhat improvisationally.

3961
02:50:43,900 --> 02:50:45,060
And will they move the goalpost, right?

3962
02:50:45,060 --> 02:50:48,220
Will they be able to enforce the right standards?

3963
02:50:48,220 --> 02:50:51,100
And will they test early and often enough

3964
02:50:51,100 --> 02:50:51,940
for the right things?

3965
02:50:51,940 --> 02:50:54,780
Because like one thing you have to worry about is

3966
02:50:54,780 --> 02:50:56,620
in the future, at some point,

3967
02:50:56,620 --> 02:51:00,220
the training runs themselves become dangerous, potentially.

3968
02:51:00,220 --> 02:51:01,460
And ARC didn't run its test

3969
02:51:01,460 --> 02:51:04,500
until after the training run was complete, right?

3970
02:51:04,500 --> 02:51:05,340
They also didn't run it

3971
02:51:05,340 --> 02:51:08,100
on the full capabilities of the final system.

3972
02:51:08,100 --> 02:51:10,780
And they didn't have fine tune capabilities.

3973
02:51:10,780 --> 02:51:12,180
And blah, blah, blah.

3974
02:51:12,180 --> 02:51:13,900
They had many things they didn't have.

3975
02:51:13,900 --> 02:51:16,460
So like everyone agrees that ARC's first run on GBD4

3976
02:51:16,460 --> 02:51:19,980
was just a trial run, test out the gear,

3977
02:51:19,980 --> 02:51:22,460
see how it goes, wasn't meant to catch the real problems.

3978
02:51:22,460 --> 02:51:24,100
No one thought that thing was actually gonna kill everyone

3979
02:51:24,100 --> 02:51:25,420
or anything and it didn't.

3980
02:51:26,500 --> 02:51:30,380
But we have to plan in these situations

3981
02:51:31,340 --> 02:51:33,660
to red team as if this thing is going to be around

3982
02:51:33,660 --> 02:51:35,580
for many years of improvements

3983
02:51:35,580 --> 02:51:38,220
on what you can do with it and explorations.

3984
02:51:38,220 --> 02:51:41,140
And the red teams have to be sufficiently enabled

3985
02:51:41,140 --> 02:51:42,780
to identify the problems.

3986
02:51:42,780 --> 02:51:43,820
And you have to be able to extrapolate

3987
02:51:43,820 --> 02:51:44,820
from what the red teams were able to do

3988
02:51:44,820 --> 02:51:45,660
in a short amount of time

3989
02:51:45,660 --> 02:51:47,300
with a short amount of resources

3990
02:51:47,300 --> 02:51:48,780
to what the public is going to be able to do

3991
02:51:48,780 --> 02:51:51,500
with vastly more compute, vastly more attempts,

3992
02:51:51,500 --> 02:51:54,620
vastly more resources, vastly more creativity.

3993
02:51:54,620 --> 02:51:57,940
Because no team of 20 people,

3994
02:51:57,940 --> 02:52:00,820
however good of their jobs can match the internet.

3995
02:52:00,820 --> 02:52:02,340
That's the kind of thing, ever.

3996
02:52:03,460 --> 02:52:06,020
So I think it's a good idea.

3997
02:52:06,020 --> 02:52:07,580
I think that it's not a complete solution

3998
02:52:07,580 --> 02:52:09,020
and never will be, right?

3999
02:52:09,020 --> 02:52:10,980
And the danger is people treat it as one.

4000
02:52:10,980 --> 02:52:12,620
You have to ask yourself like,

4001
02:52:12,620 --> 02:52:13,580
what is it competing against?

4002
02:52:14,220 --> 02:52:16,540
Is this going to be one of the top, however many?

4003
02:52:16,540 --> 02:52:17,940
People who run this thing,

4004
02:52:17,940 --> 02:52:20,620
like you want to have like three viable organizations,

4005
02:52:20,620 --> 02:52:21,460
you might, you know,

4006
02:52:21,460 --> 02:52:23,100
you don't necessarily need 30,

4007
02:52:23,100 --> 02:52:24,740
that's probably not worth it.

4008
02:52:24,740 --> 02:52:26,340
Like you want three to five.

4009
02:52:26,340 --> 02:52:29,540
So is this person more funny to do that?

4010
02:52:29,540 --> 02:52:31,460
Just figuring out better metrics

4011
02:52:31,460 --> 02:52:33,500
without necessarily being the one who runs the tests

4012
02:52:33,500 --> 02:52:35,500
is also a useful thing.

4013
02:52:35,500 --> 02:52:36,860
So if I had to bottom line all that

4014
02:52:36,860 --> 02:52:39,660
and summarize what I think your worldview is,

4015
02:52:39,660 --> 02:52:44,660
you know, as a sort of elder recommender

4016
02:52:44,700 --> 02:52:49,700
for this AI safety focused grant-making process.

4017
02:52:50,380 --> 02:52:51,460
I think I would say,

4018
02:52:51,460 --> 02:52:54,100
I think I would summarize it as

4019
02:52:54,100 --> 02:52:56,620
there need to be a few

4020
02:52:56,620 --> 02:52:59,180
of these independent safety organizations.

4021
02:52:59,180 --> 02:53:01,740
They seem to be either just started

4022
02:53:01,740 --> 02:53:03,700
or kind of getting started now.

4023
02:53:03,700 --> 02:53:05,940
So at least a few of those,

4024
02:53:05,940 --> 02:53:07,260
one exists in, you know,

4025
02:53:08,260 --> 02:53:09,420
a couple others are, you know,

4026
02:53:09,420 --> 02:53:10,380
either just getting started

4027
02:53:10,380 --> 02:53:11,740
or soon to be started, whatever.

4028
02:53:11,740 --> 02:53:13,020
So there's kind of,

4029
02:53:13,020 --> 02:53:15,300
that seems good because we need to bring

4030
02:53:15,300 --> 02:53:18,580
that small, you know, group into existence

4031
02:53:18,580 --> 02:53:21,060
in the first place, you have to have them.

4032
02:53:21,060 --> 02:53:24,100
Second, on the policy side,

4033
02:53:24,100 --> 02:53:26,180
I guess I would summarize you as saying,

4034
02:53:27,260 --> 02:53:29,540
seems like it really matters,

4035
02:53:29,540 --> 02:53:33,180
but really hard to predict

4036
02:53:33,180 --> 02:53:35,060
who will have any impact

4037
02:53:35,060 --> 02:53:38,180
and what kind of impact any effort will have.

4038
02:53:38,180 --> 02:53:41,060
And so for me, I sort of maybe cash that out

4039
02:53:41,060 --> 02:53:43,140
to like probably worth continuing

4040
02:53:43,140 --> 02:53:47,060
to support the organizations that are like established enough

4041
02:53:47,060 --> 02:53:49,140
that they already have credibility

4042
02:53:49,140 --> 02:53:51,980
because credibility or like, you know,

4043
02:53:51,980 --> 02:53:54,700
that bloom and thaw might give a shit what they think

4044
02:53:54,700 --> 02:53:57,260
is like probably the thing that matters.

4045
02:53:57,260 --> 02:53:58,700
And to the degree they already have that

4046
02:53:58,700 --> 02:53:59,900
like double down on it.

4047
02:54:01,060 --> 02:54:04,220
And then everything else seems like it goes

4048
02:54:04,220 --> 02:54:08,700
into good PIs that can drive a research agenda

4049
02:54:08,700 --> 02:54:10,340
and have something that they want to do.

4050
02:54:10,340 --> 02:54:12,820
And in that category, it's like,

4051
02:54:14,060 --> 02:54:16,020
don't even really worry too much

4052
02:54:16,020 --> 02:54:17,380
about the exact details of the plan,

4053
02:54:17,380 --> 02:54:21,980
but just look for people who have the originality of thought

4054
02:54:21,980 --> 02:54:23,940
to be doing something a bit different perhaps

4055
02:54:23,940 --> 02:54:27,100
and the sort of demonstrated capability

4056
02:54:27,100 --> 02:54:30,180
to actually advance a research agenda.

4057
02:54:30,180 --> 02:54:32,460
This is a lot different things there, right?

4058
02:54:32,460 --> 02:54:35,500
For the PIs, I would say, you know,

4059
02:54:35,500 --> 02:54:38,780
I'm not looking for like exactly the right approach,

4060
02:54:38,780 --> 02:54:42,020
but I am looking for assume alignment is hard.

4061
02:54:42,020 --> 02:54:44,660
This is the approach except that alignment is hard

4062
02:54:44,660 --> 02:54:46,660
and do something that makes progress, real progress

4063
02:54:46,660 --> 02:54:48,820
if alignment is in fact very hard.

4064
02:54:48,820 --> 02:54:51,380
These people show an appropriate caution

4065
02:54:51,380 --> 02:54:53,940
towards the might advance capabilities

4066
02:54:53,940 --> 02:54:55,900
towards like maybe I don't want to publish my results

4067
02:54:55,900 --> 02:54:58,140
if I find a result that would be harmful

4068
02:54:58,140 --> 02:55:00,260
to publish question marks like that.

4069
02:55:00,260 --> 02:55:01,460
You know, am I thinking about this problem

4070
02:55:01,460 --> 02:55:03,980
as the right safety mindset, with the right paranoia,

4071
02:55:03,980 --> 02:55:06,380
with the right like appreciation of the fact

4072
02:55:06,380 --> 02:55:08,380
that I'm up against impossible odds?

4073
02:55:08,380 --> 02:55:11,020
And if the answer is yes, and I think I have the talent,

4074
02:55:11,020 --> 02:55:14,060
then I'm excited even if I'm somewhat skeptical

4075
02:55:14,060 --> 02:55:15,820
of the specific thing they intend to try

4076
02:55:15,820 --> 02:55:17,380
in terms of like whether it will work, right?

4077
02:55:17,380 --> 02:55:20,580
Because like I think that all the most promising things

4078
02:55:20,580 --> 02:55:22,340
are not that likely individually to work

4079
02:55:22,340 --> 02:55:24,380
and it's gonna be hard for me to evaluate

4080
02:55:24,380 --> 02:55:25,380
the relative value.

4081
02:55:26,460 --> 02:55:28,460
In terms of the lobbying organizations,

4082
02:55:29,340 --> 02:55:34,140
I don't think it's crazy to start a new group at this point.

4083
02:55:34,140 --> 02:55:36,380
I do think you want to look for something extraordinary

4084
02:55:36,380 --> 02:55:39,340
if somebody is like, why are they forming a new group now?

4085
02:55:39,340 --> 02:55:41,020
Why does that make sense?

4086
02:55:41,020 --> 02:55:43,540
But yeah, what I'm looking for is a focus

4087
02:55:43,540 --> 02:55:46,460
on the policies that actually matter

4088
02:55:46,460 --> 02:55:48,300
and on a coordination amongst them

4089
02:55:48,300 --> 02:55:52,700
and on like a focus on actually making a difference.

4090
02:55:52,700 --> 02:55:54,940
Like so much of like most of politics, right?

4091
02:55:54,940 --> 02:55:57,420
There's not about AIs about politics in general

4092
02:55:57,420 --> 02:55:59,540
is about raising money from donors

4093
02:55:59,540 --> 02:56:02,060
and sending signals of your royalties

4094
02:56:02,060 --> 02:56:03,860
and coming up with your status

4095
02:56:03,860 --> 02:56:07,180
and raising awareness and other bullshit.

4096
02:56:07,180 --> 02:56:09,300
Like most like all sides, right?

4097
02:56:09,300 --> 02:56:13,260
Like you've got to focus on people who are writing bills,

4098
02:56:13,260 --> 02:56:15,500
people who are lobbying directly for bills,

4099
02:56:15,500 --> 02:56:17,740
people who are trying to influence the exact right people

4100
02:56:17,740 --> 02:56:19,020
in the exact right ways.

4101
02:56:19,020 --> 02:56:22,140
Like have a concrete direct theory of change

4102
02:56:22,140 --> 02:56:24,060
who like either understand DC

4103
02:56:24,060 --> 02:56:25,420
or have connections with people

4104
02:56:25,420 --> 02:56:27,380
who can help them understand DC.

4105
02:56:27,380 --> 02:56:28,180
But I don't think we know

4106
02:56:28,180 --> 02:56:30,380
that we are in like the only critical window.

4107
02:56:30,380 --> 02:56:32,100
We're going to need more organizations than we have.

4108
02:56:32,100 --> 02:56:33,420
We're going to need far more people working on it

4109
02:56:33,420 --> 02:56:35,340
than we already have.

4110
02:56:35,340 --> 02:56:37,580
I don't want to make the mistake of not chips or down

4111
02:56:37,580 --> 02:56:40,780
the people who have like nominally established some amount

4112
02:56:40,780 --> 02:56:43,220
of like formal credibility or authority

4113
02:56:43,220 --> 02:56:44,220
now get all the resources

4114
02:56:44,220 --> 02:56:45,940
and get to boss everybody around and do whatever they want.

4115
02:56:45,940 --> 02:56:47,660
I think that's like a common failure mode.

4116
02:56:47,660 --> 02:56:49,380
I don't want to fall into it.

4117
02:56:49,380 --> 02:56:51,980
Evaluation organizations, I want to ask myself,

4118
02:56:51,980 --> 02:56:55,740
are these the right people to be doing this particular thing?

4119
02:56:55,740 --> 02:56:57,340
They show promise in doing the thing

4120
02:56:57,340 --> 02:56:58,340
what they bring to the table

4121
02:56:58,340 --> 02:56:59,780
the other organizations don't bring to the table.

4122
02:56:59,780 --> 02:57:01,060
I want to see different

4123
02:57:01,060 --> 02:57:02,620
or I want to see something unique.

4124
02:57:02,620 --> 02:57:03,660
And you have to convince me

4125
02:57:03,660 --> 02:57:06,300
that like you're capable of pulling this off

4126
02:57:06,300 --> 02:57:07,740
which includes convincing people

4127
02:57:07,740 --> 02:57:10,100
that actually buy your services and use your services.

4128
02:57:10,100 --> 02:57:12,820
Where do you put mechanistic interpretability in there?

4129
02:57:12,820 --> 02:57:14,100
And that that could be,

4130
02:57:14,100 --> 02:57:17,820
that seems to be part of what some of the like evals orgs

4131
02:57:17,820 --> 02:57:21,460
are also kind of including that in their plan.

4132
02:57:21,460 --> 02:57:22,500
And then obviously, you know,

4133
02:57:22,500 --> 02:57:23,380
different research groups

4134
02:57:23,380 --> 02:57:26,220
can approach that from any number of ways.

4135
02:57:26,220 --> 02:57:28,580
My technical view of it is that it's more distinct

4136
02:57:28,580 --> 02:57:33,580
from evaluations than that suggests

4137
02:57:34,580 --> 02:57:35,780
but that it's a good idea.

4138
02:57:36,860 --> 02:57:38,180
Like it should be, you know,

4139
02:57:38,180 --> 02:57:40,740
mechanistic interpretability is like Western civilization.

4140
02:57:40,740 --> 02:57:42,420
It's a good idea.

4141
02:57:42,420 --> 02:57:46,820
Yeah, you should try to in fact,

4142
02:57:46,820 --> 02:57:48,860
figure out how these things work.

4143
02:57:48,860 --> 02:57:50,700
You do have to be aware

4144
02:57:50,700 --> 02:57:52,660
that you are advancing capabilities potentially

4145
02:57:52,660 --> 02:57:53,500
when you do it.

4146
02:57:53,500 --> 02:57:55,420
You have to think carefully about, you know,

4147
02:57:55,460 --> 02:57:57,180
if you find the wrong thing,

4148
02:57:57,180 --> 02:58:00,100
I would ask before I funded an interpretability organization,

4149
02:58:00,100 --> 02:58:03,300
are you capable of going, oh, yikes,

4150
02:58:03,300 --> 02:58:05,700
that's a dangerous thing to learn.

4151
02:58:05,700 --> 02:58:07,940
I might not want to rush out to tell the world about that.

4152
02:58:07,940 --> 02:58:09,700
I might want to think carefully about who to tell

4153
02:58:09,700 --> 02:58:11,020
and who not to tell.

4154
02:58:11,020 --> 02:58:12,660
But not necessarily don't really sympathy.

4155
02:58:12,660 --> 02:58:15,620
Like you have to like process information carefully

4156
02:58:15,620 --> 02:58:16,780
and not just rush.

4157
02:58:18,100 --> 02:58:20,260
You don't want a culture of, you know,

4158
02:58:20,260 --> 02:58:22,460
everything I ever find is going to be automatically

4159
02:58:22,460 --> 02:58:24,340
just shared with the world for that reason.

4160
02:58:24,340 --> 02:58:27,420
When you work on mechanistic interpretability.

4161
02:58:27,420 --> 02:58:29,020
But I do think on general,

4162
02:58:29,020 --> 02:58:30,340
it's a very positive thing to work on.

4163
02:58:30,340 --> 02:58:31,900
I do think that it's a thing

4164
02:58:31,900 --> 02:58:34,860
that like holds a lot of promise to help us

4165
02:58:34,860 --> 02:58:36,700
in various ways.

4166
02:58:37,780 --> 02:58:38,620
And it could lead somewhere

4167
02:58:38,620 --> 02:58:40,060
where we start sharing all problems with it,

4168
02:58:40,060 --> 02:58:41,420
potentially in theory.

4169
02:58:41,420 --> 02:58:44,060
It's just a very hard problem that requires, you know,

4170
02:58:44,060 --> 02:58:45,380
a lot of work and a lot of compute

4171
02:58:45,380 --> 02:58:46,220
and it's not going to be fast

4172
02:58:46,220 --> 02:58:47,820
and it's not going to be simple.

4173
02:58:47,820 --> 02:58:50,020
And we want a lot of people who are going in parallel.

4174
02:58:50,020 --> 02:58:51,260
So I certainly, you know,

4175
02:58:51,260 --> 02:58:54,140
intend to assist with some amount of that.

4176
02:58:54,140 --> 02:58:56,620
Cool. Well, believe it or not,

4177
02:58:56,620 --> 02:58:58,940
we did not get to everything even on my outline,

4178
02:58:58,940 --> 02:59:02,100
let alone everything that you have covered

4179
02:59:02,100 --> 02:59:04,020
on your blog,

4180
02:59:04,020 --> 02:59:05,060
which has been, you know,

4181
02:59:05,060 --> 02:59:06,980
probably 10 times as many topics.

4182
02:59:06,980 --> 02:59:10,980
So folks will have to get the written version.

4183
02:59:10,980 --> 02:59:12,060
This is Vy Matryoz.

4184
02:59:12,060 --> 02:59:14,980
Thank you for being part of the Cognitive Revolution.

4185
02:59:14,980 --> 02:59:17,020
All right, bye.

4186
02:59:17,020 --> 02:59:19,460
Omniki uses generative AI

4187
02:59:19,460 --> 02:59:21,380
to enable you to launch hundreds of thousands

4188
02:59:21,380 --> 02:59:23,740
of ad iterations that actually work.

4189
02:59:23,740 --> 02:59:27,180
Customized across all platforms with a click of a button.

4190
02:59:27,180 --> 02:59:29,740
I believe in Omniki so much that I invested in it

4191
02:59:29,740 --> 02:59:32,020
and I recommend you use it too.

4192
02:59:32,020 --> 02:59:34,580
Use CogGrev to get a 10% discount.

