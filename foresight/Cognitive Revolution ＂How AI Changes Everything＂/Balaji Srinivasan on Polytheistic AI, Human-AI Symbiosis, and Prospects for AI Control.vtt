WEBVTT

00:00.000 --> 00:01.520
They don't care about AI safety.

00:01.520 --> 00:02.880
What they care about is AI control.

00:03.440 --> 00:05.680
Do I think we eventually get to a configuration like that?

00:05.680 --> 00:06.320
Maybe.

00:06.320 --> 00:10.080
Where you have an AI brain is at the center of civilization,

00:10.080 --> 00:12.160
and it's coordinating all the people around it.

00:12.160 --> 00:14.960
And every civilization that makes it

00:14.960 --> 00:17.440
is capable of crowdfunding and operating its own AI.

00:17.440 --> 00:19.760
You know, our background culture influences things

00:19.760 --> 00:21.440
in ways we don't even think about.

00:21.440 --> 00:23.680
So much of the paperclip thinking

00:23.680 --> 00:27.120
is like a vengeful god will turn you into pillars of salt.

00:27.120 --> 00:30.560
The polytheistic model of many gods, as opposed to one god,

00:30.560 --> 00:32.880
is we're all going to have our own AI gods,

00:32.880 --> 00:34.000
and there'll be war of the gods.

00:34.640 --> 00:36.880
Man-machine symbiosis is not some new thing.

00:37.440 --> 00:40.560
It's actually the old thing that broke us away

00:40.560 --> 00:43.200
from other primate lineages that weren't using tools.

00:43.760 --> 00:45.840
Then the question is, what's the next step?

00:45.840 --> 00:48.480
Which is AI is amplified intelligence.

00:48.480 --> 00:51.120
It is that the AI human fusion

00:51.840 --> 00:53.920
means there's another 20 Elon Musk's,

00:53.920 --> 00:54.960
or whatever the number is.

00:55.680 --> 00:56.560
That's good.

00:56.640 --> 00:58.960
Hello, and welcome to The Cognitive Revolution,

00:58.960 --> 01:02.000
where we interview visionary researchers, entrepreneurs,

01:02.000 --> 01:03.920
and builders working on the frontier

01:03.920 --> 01:05.120
of artificial intelligence.

01:05.840 --> 01:08.640
Each week, we'll explore their revolutionary ideas,

01:08.640 --> 01:10.080
and together, we'll build a picture

01:10.080 --> 01:13.600
of how AI technology will transform work, life,

01:13.600 --> 01:15.200
and society in the coming years.

01:15.760 --> 01:19.280
I'm Nathan LaBenz, joined by my co-host, Eric Torenberg.

01:19.280 --> 01:22.000
Hello, and welcome back to The Cognitive Revolution.

01:22.960 --> 01:25.200
Today, my guest is biology, Srinivasan.

01:25.920 --> 01:28.880
In tech circles, biology needs no introduction.

01:28.880 --> 01:30.800
But for folks from other backgrounds,

01:30.800 --> 01:32.960
biology is a serial startup entrepreneur

01:32.960 --> 01:35.120
who's founded and ultimately sold

01:35.120 --> 01:37.360
highly dissimilar technology companies,

01:37.360 --> 01:38.800
including Teleport,

01:38.800 --> 01:40.560
which helped people move around the world

01:40.560 --> 01:41.680
to realize opportunities,

01:42.560 --> 01:44.880
Council, which provided genetic testing

01:44.880 --> 01:46.320
for couples planning to have children,

01:46.880 --> 01:51.200
and Earn.com, a paid email on the blockchain startup,

01:51.200 --> 01:53.360
which ultimately sold to Coinbase,

01:53.360 --> 01:55.200
where biology became CTO.

01:56.560 --> 01:59.120
Along the way, he's also taught statistics at Stanford

01:59.120 --> 02:01.920
and been a general partner at Andreessen Horowitz as well.

02:03.200 --> 02:05.440
Today, as an independent thinker, investor,

02:05.440 --> 02:06.960
and author of the network state,

02:06.960 --> 02:08.640
biology is extremely prolific

02:08.640 --> 02:11.040
in both text and audio formats.

02:11.040 --> 02:11.760
And as you'll hear,

02:11.760 --> 02:13.840
whether for the first time or the 50th,

02:13.840 --> 02:15.840
he is an incredibly creative thinker

02:15.840 --> 02:18.240
who relentlessly develops and iterates

02:18.240 --> 02:20.320
on new paradigms for understanding

02:20.320 --> 02:22.800
a fast-changing, often chaotic world.

02:23.760 --> 02:25.120
He's also a very associative

02:25.120 --> 02:26.720
and interdisciplinary thinker

02:26.720 --> 02:29.520
who constantly adds dimensions to any analysis.

02:31.200 --> 02:32.720
Such horsepower can be hard

02:32.720 --> 02:34.800
for a podcast host to rein in,

02:34.800 --> 02:37.360
but I personally find it extremely stimulating.

02:37.360 --> 02:38.240
So in this conversation,

02:38.240 --> 02:39.280
I tried to strike a balance

02:39.280 --> 02:41.760
between letting biology go off as only he can do,

02:42.480 --> 02:44.720
contributing what I hope are worthy versions

02:44.720 --> 02:46.400
of core AI safety arguments

02:46.400 --> 02:48.400
and supporting results from recent research,

02:49.120 --> 02:50.160
and occasionally,

02:50.160 --> 02:51.920
steering us back toward what I see

02:51.920 --> 02:54.960
as the most critical questions for the AI big picture.

02:56.720 --> 02:58.800
If there's one area where biology and I

02:58.800 --> 03:00.640
disagree most consequentially,

03:00.640 --> 03:03.760
it's on the question of how independent AI systems

03:03.760 --> 03:06.560
are likely to become over the next five to 10 years.

03:07.440 --> 03:08.960
Biology thinks that AI systems

03:08.960 --> 03:11.600
need to be at least symbiotic with humans,

03:11.600 --> 03:13.040
because physical computers

03:13.040 --> 03:15.600
can't replicate themselves without human support.

03:16.720 --> 03:19.280
While I think there's at least a significant chance

03:19.360 --> 03:22.640
that we get AIs that are so independent of humans

03:22.640 --> 03:24.880
that their behaviors and interactions

03:24.880 --> 03:27.360
become the primary drivers of world history.

03:28.960 --> 03:30.400
In biology's own words,

03:30.400 --> 03:32.960
he does expect massive economic

03:32.960 --> 03:34.960
and social disruption from AI,

03:35.600 --> 03:37.520
but doesn't think that quote-unquote

03:37.520 --> 03:40.800
can't turn the killer AI off scenarios,

03:40.800 --> 03:42.960
are likely, at least for a long while,

03:42.960 --> 03:45.760
due to factors like the existence of adversarial inputs

03:45.760 --> 03:47.520
that can paralyze AIs,

03:47.520 --> 03:49.440
particularly those with open model weights,

03:50.320 --> 03:52.800
the observation that even decentralized programs

03:52.800 --> 03:55.040
like the Bitcoin network can't run independently

03:55.040 --> 03:56.800
without continuous human support,

03:57.520 --> 04:00.160
and the premise that to control the physical world,

04:00.160 --> 04:02.960
AIs will need to direct either large numbers of humans

04:02.960 --> 04:05.040
who are notoriously difficult to control

04:05.040 --> 04:08.640
or highly agile robots which don't yet exist.

04:10.160 --> 04:11.120
With all that in mind,

04:11.120 --> 04:12.640
in the first half of this conversation,

04:12.640 --> 04:14.320
you'll hear biology's analysis

04:14.320 --> 04:16.240
of the likely impact of AI

04:16.240 --> 04:19.760
in a world where powerful AI systems do come to exist,

04:19.760 --> 04:21.120
but humans retain control,

04:22.160 --> 04:24.560
resulting in a human AI symbiosis

04:24.560 --> 04:27.360
similar to how believers relate to their gods

04:27.360 --> 04:29.200
or citizens relate to their governments.

04:30.160 --> 04:31.280
Then in the second half,

04:31.280 --> 04:32.640
we really dig into the question

04:32.640 --> 04:34.640
of just how confident we should be

04:34.640 --> 04:38.480
that AI won't prove to be even more revolutionary than that.

04:39.920 --> 04:42.080
After more than two hours of recording,

04:42.080 --> 04:44.480
I was the one who ran out of time today,

04:44.560 --> 04:47.040
but I really enjoyed this conversation with biology.

04:47.040 --> 04:50.560
He is as good-natured and curious as he is opinionated,

04:50.560 --> 04:53.280
and we have continued to exchange links and arguments offline,

04:53.280 --> 04:55.040
such that I hope we'll have another episode

04:55.040 --> 04:57.200
to share with you in the future as well.

04:58.720 --> 05:00.560
As always, if you're enjoying the show,

05:00.560 --> 05:03.120
we'd ask that you take a moment to share it with a friend.

05:03.760 --> 05:06.960
And with that, here's part one of an all-angles look

05:06.960 --> 05:10.720
at how AI will shape the future with biology, Srinivasan.

05:11.680 --> 05:15.440
Biology, Srinivasan, welcome to the Cognitive Revolution.

05:15.440 --> 05:17.360
All right, I feel welcome.

05:17.360 --> 05:19.200
Well, we've got a ton to talk about.

05:19.200 --> 05:22.080
Obviously, you bring a lot of different perspectives

05:22.080 --> 05:25.200
to everything that you think about and work on.

05:25.840 --> 05:28.960
And today, I want to just try to muster

05:28.960 --> 05:31.280
all those different perspectives onto this.

05:31.280 --> 05:34.240
What I see is really the defining question of our time,

05:34.240 --> 05:36.000
which is like, what's up with AI?

05:36.000 --> 05:38.000
And how's it going to turn out?

05:38.880 --> 05:40.320
I thought maybe for starters,

05:40.320 --> 05:42.320
I would love to just get your baseline kind of table

05:42.320 --> 05:48.480
setting on how much more AI progress do you expect us to see

05:48.480 --> 05:50.080
over the next few years?

05:50.080 --> 05:53.040
Like how powerful are AI systems going to become

05:53.600 --> 05:55.920
in, again, kind of a relatively short timeline?

05:55.920 --> 05:58.800
And then maybe if you want to take a bigger stab at it,

05:58.800 --> 06:01.200
you could answer that same question for a longer timeline

06:01.200 --> 06:02.880
like the rest of our lives or whatever.

06:02.880 --> 06:04.400
Sure. Let me give an abstract answer,

06:04.400 --> 06:05.840
then let me give a technical answer.

06:06.800 --> 06:10.800
If you look at evolution, we've seen something as complex

06:10.800 --> 06:15.360
as flight evolve independently in birds, bats, and bees.

06:16.560 --> 06:22.080
And even intelligence, we've seen fairly high intelligence

06:22.080 --> 06:25.600
in dolphins, in whales, in octopuses.

06:27.120 --> 06:29.200
Octopus in particular can do like tool manipulation.

06:29.200 --> 06:32.080
They've got things that are a lot like hands with tentacles.

06:32.080 --> 06:35.200
And so that indicates that it is plausible

06:35.840 --> 06:39.120
that you could have multiple pathways to intelligence,

06:39.920 --> 06:42.400
whether we have carbon-based intelligence,

06:42.400 --> 06:43.920
so we could have silicon-based intelligence

06:43.920 --> 06:45.280
that just has a totally different form

06:45.280 --> 06:47.680
where the fundamental thing is an electromagnetic wave

06:47.680 --> 06:51.440
and data storage as opposed to DNA and so on.

06:51.440 --> 06:52.960
So that's like a plausibility argument

06:52.960 --> 06:55.680
in terms of evolution is being so resourceful

06:55.680 --> 06:58.560
that it's invented really complicated things

06:58.560 --> 06:59.360
in different ways.

07:00.480 --> 07:02.320
Then in terms of the technical point,

07:02.320 --> 07:04.960
I think as of right now, I should probably date it,

07:05.440 --> 07:08.480
December 11, 2023, because this field moves so fast, right?

07:09.120 --> 07:12.240
My view is, and maybe you'll have a different view,

07:12.240 --> 07:14.400
is that the breakthroughs that are really needed

07:14.400 --> 07:16.720
for something that's like true artificial intelligence

07:16.720 --> 07:20.240
that is human-independent, right?

07:20.240 --> 07:22.560
Maybe the next step after the Turing test,

07:22.560 --> 07:24.720
I've got an article that we're writing

07:24.720 --> 07:25.920
called the Turing Thresholds,

07:26.640 --> 07:28.400
which tries to generalize the Turing test

07:28.400 --> 07:30.240
to like the Kardashev scale.

07:30.240 --> 07:31.680
Have you got energy thresholds?

07:31.680 --> 07:33.280
What are useful scales beyond that?

07:33.840 --> 07:37.920
And right now, I think that what we call AI

07:37.920 --> 07:40.480
is absolutely amazing for environments

07:40.480 --> 07:42.480
that are not time-varying or rule-varying.

07:43.840 --> 07:46.960
And what I mean by that is, so you kind of have,

07:46.960 --> 07:48.880
let's say two large schools of AI,

07:48.880 --> 07:50.240
and obviously there's overlap

07:50.240 --> 07:51.840
in terms of the personnel and so on,

07:51.840 --> 07:53.760
but there's like the DeepMind School,

07:53.760 --> 07:55.200
which has gotten less press recently,

07:55.200 --> 07:57.520
but got more press a few years ago,

07:57.520 --> 08:00.160
and that is game-playing, right?

08:00.160 --> 08:04.720
It is super human-playing of go without go.

08:04.720 --> 08:07.920
It is all the video game stuff they've done

08:07.920 --> 08:09.280
where they learn at the pixel level

08:09.280 --> 08:11.200
and they just teach the very basic rules

08:11.200 --> 08:12.640
and it figures it out from there.

08:12.640 --> 08:14.960
And it's also the protein folding stuff

08:14.960 --> 08:15.920
and what have you, right?

08:16.800 --> 08:18.160
But in general, I think they're known

08:18.160 --> 08:20.880
for reinforcement learning and those kinds of approaches.

08:20.880 --> 08:21.840
I mean, they're good at a lot of things,

08:21.840 --> 08:23.360
but that's what I think DeepMind is known for.

08:23.360 --> 08:25.680
Of course, they put out this new model recently,

08:25.680 --> 08:27.040
the Gemini model.

08:27.040 --> 08:29.040
So I'm not saying that they're not good at everything,

08:29.040 --> 08:31.440
but that's just kind of what they're maybe most known for.

08:31.440 --> 08:35.120
And then you have the OpenAI ChatGBT School of Generative AI,

08:35.120 --> 08:39.040
and it includes stable diffusion and just as a pioneer,

08:39.040 --> 08:40.320
even if they're not,

08:40.320 --> 08:41.840
I don't know how much they're used right now,

08:41.840 --> 08:45.040
but basically, you have the diffusion models for images

08:45.040 --> 08:47.280
and you have large language models

08:47.280 --> 08:49.120
and now you have the multimodals that integrate them.

08:49.120 --> 08:51.280
And so the difference, I think with these,

08:51.280 --> 08:54.800
is the reinforcement learning approaches

08:54.800 --> 08:58.160
are based on an assumption of static rules,

08:58.160 --> 09:00.080
like the rules of chess, the rules that go,

09:00.080 --> 09:02.240
the rules of a video game are not changing with time.

09:02.240 --> 09:02.960
They are discoverable.

09:02.960 --> 09:04.080
They're like the laws of physics.

09:04.640 --> 09:08.800
And similarly, like the body of language

09:08.800 --> 09:09.920
where you're learning it,

09:09.920 --> 09:11.840
English is not rapidly time varying.

09:12.480 --> 09:14.480
That is to say, the rules of grammar

09:14.480 --> 09:15.920
that are implicit aren't changing.

09:15.920 --> 09:18.560
The meanings of words aren't changing very rapidly.

09:18.560 --> 09:20.800
You can argue they're changing over the span of decades

09:20.800 --> 09:23.200
or centuries, but not extremely rapidly, right?

09:23.200 --> 09:26.880
So therefore, when you generate a new result,

09:26.880 --> 09:29.200
training data from five years ago for English

09:29.200 --> 09:31.120
is actually still fairly valuable

09:31.120 --> 09:33.840
and the same input roughly gives the same output.

09:33.840 --> 09:35.760
Now, of course, there are facts that change with time,

09:36.720 --> 09:39.280
like who is the ruler of England, right?

09:39.280 --> 09:40.720
The queen of England is passed away now.

09:40.720 --> 09:41.840
It's the king of England, right?

09:41.840 --> 09:43.280
It's just facts that change with time.

09:43.280 --> 09:44.480
But I think more fundamentally

09:44.480 --> 09:46.400
is when there's rules that change with time.

09:47.040 --> 09:51.040
You have, for example, changes in law in countries, right?

09:51.040 --> 09:53.040
But most interestingly, perhaps changes in markets

09:54.000 --> 09:54.880
because the same input

09:54.880 --> 09:56.480
does not give the same output in a market.

09:56.480 --> 09:58.080
If you try that, then what will happen

09:58.080 --> 10:00.400
is there's adversarial behavior on the other side.

10:00.400 --> 10:01.840
And once people see it enough times,

10:01.840 --> 10:03.200
they'll see your strategy

10:03.200 --> 10:05.360
and they're going to trade against you on that, right?

10:05.360 --> 10:07.200
And I can get to other technical examples on that,

10:07.200 --> 10:08.960
but I think, and probably people in the space

10:08.960 --> 10:09.840
are aware of this,

10:09.840 --> 10:11.760
but I think that is a true frontier

10:11.760 --> 10:15.120
is dealing with time-varying, rule-varying systems,

10:15.120 --> 10:18.240
as opposed to systems where the implicit rules are static.

10:18.240 --> 10:19.120
Let me pause there.

10:19.120 --> 10:20.160
Yeah, I think that makes sense.

10:20.160 --> 10:22.960
I think in the very practical,

10:23.920 --> 10:25.920
just trying to get, as Zvi calls it,

10:25.920 --> 10:27.360
mundane utility from AI,

10:28.000 --> 10:31.440
that is often cashed out to AI is good at tasks,

10:31.440 --> 10:33.680
but it's not good at whole jobs.

10:33.680 --> 10:35.920
It can handle these small things

10:35.920 --> 10:38.240
where you can define what good looks like

10:38.240 --> 10:40.480
and tell it exactly what to do.

10:40.480 --> 10:45.200
But in the broader context of handling things

10:45.200 --> 10:46.480
that come up as they come up,

10:47.040 --> 10:48.560
it's definitely not there yet.

10:49.440 --> 10:54.000
And I agree that there's likely to be some synthesis,

10:54.000 --> 10:58.560
which is kind of the subject of all the Q-star rumors recently,

10:58.560 --> 11:01.920
I would say is kind of the prospect

11:01.920 --> 11:05.280
that there could be already within the labs

11:05.280 --> 11:07.600
a beginning of a synthesis between the,

11:08.240 --> 11:11.520
I kind of think of it as like harder-edged reinforcement

11:11.520 --> 11:15.680
learning systems that are like small, efficient, and deadly,

11:15.680 --> 11:17.840
versus the language model systems

11:17.840 --> 11:20.800
that are like kind of slow and soft,

11:20.800 --> 11:22.960
and but have a sense of our values,

11:22.960 --> 11:25.280
which is really a remarkable accomplishment

11:25.280 --> 11:29.040
that they're able to have even an approximation of our values

11:29.040 --> 11:30.480
that seems like reasonably good.

11:31.200 --> 11:36.160
So yeah, I think I agree with that framing,

11:36.720 --> 11:40.080
but I guess I would still wonder like,

11:40.720 --> 11:44.560
how far do you think this goes in the near term?

11:44.560 --> 11:46.720
Because I have a lot of uncertainty about that,

11:46.720 --> 11:48.160
and I think the field has a lot of uncertainty.

11:48.160 --> 11:50.080
You hear people say, well,

11:50.080 --> 11:52.480
it's never going to get smarter than its training data.

11:52.480 --> 11:54.800
It'll kind of level out where humans are,

11:54.800 --> 11:55.840
but we certainly don't see that

11:55.840 --> 11:58.480
in the reinforcement learning side, right?

11:58.480 --> 12:01.760
Like once, it usually don't take too long

12:01.760 --> 12:03.600
at human level of these games,

12:03.600 --> 12:05.440
and then it like blows past human level.

12:05.440 --> 12:08.400
Interestingly, you do still see some adversarial vulnerability,

12:08.400 --> 12:12.160
like there's a great paper from the team at FAR AI,

12:12.160 --> 12:14.560
and I'm planning to have Adam Gleave,

12:14.560 --> 12:16.000
the head of that organization on soon,

12:16.000 --> 12:17.360
to talk about that and other things,

12:17.360 --> 12:19.840
where they found like a, basically a hack

12:19.840 --> 12:23.920
where a really simple, but unexpected attack

12:23.920 --> 12:27.280
on the superhuman go player can defeat it.

12:27.280 --> 12:30.160
So you do have these like very interesting vulnerabilities

12:30.160 --> 12:33.120
or kind of lack of adversarial robustness.

12:33.120 --> 12:34.640
Still kind of wondering like,

12:34.640 --> 12:36.240
where do you think that leaves us

12:36.240 --> 12:38.240
in say a three to five years time?

12:38.240 --> 12:40.160
Obviously, huge uncertainty on that.

12:40.160 --> 12:42.240
It's really hard to predict something like this.

12:42.800 --> 12:46.480
Just to your point, generative AI is generic AI, right?

12:46.480 --> 12:48.320
It's like generically smart,

12:48.320 --> 12:49.920
but doesn't have specific intelligence

12:49.920 --> 12:51.520
or creativity or facts.

12:51.520 --> 12:53.520
And as you're saying, just like we have,

12:53.520 --> 12:57.760
you know, adversarial images back in full programs

12:57.760 --> 13:00.480
that are trained on a certain set of data

13:00.480 --> 13:02.880
and they just give some weird, you know, pattern

13:02.880 --> 13:04.080
that looks like a giraffe,

13:04.080 --> 13:06.320
but the algorithm thinks it's a dog.

13:06.320 --> 13:07.680
You can do the same thing for game playing

13:07.680 --> 13:09.360
and you can have out of sample input

13:09.360 --> 13:11.440
that can beat, you know,

13:11.440 --> 13:14.560
these very sophisticated reinforcement learners.

13:16.000 --> 13:17.520
And an interesting question is whether

13:17.520 --> 13:19.440
that is a fundamental thing

13:19.440 --> 13:23.520
or whether it is a work aroundable thing.

13:24.160 --> 13:26.640
And you'd think it was work aroundable, you know,

13:27.520 --> 13:29.920
because there's probably some robustification

13:29.920 --> 13:32.560
because these pictures look like giraffes, you know,

13:33.200 --> 13:35.760
and yet they're being recognized as dogs.

13:35.760 --> 13:39.600
So you would think that the right proximity metric

13:39.600 --> 13:42.480
would group it with giraffes, you know,

13:43.040 --> 13:45.200
but maybe there's some, I don't know,

13:45.200 --> 13:46.800
maybe there's some result there.

13:46.800 --> 13:51.120
My intuition would be we can probably robustify these systems

13:51.120 --> 13:54.320
so that they are less vulnerable to adversarial input.

13:55.920 --> 13:57.360
But if we can't, then that leads us

13:57.360 --> 13:58.880
in a totally different direction,

13:58.880 --> 14:01.840
where these systems are fragile in a fundamental way.

14:02.800 --> 14:07.040
So that's one big branch point is how fragile these systems are

14:07.040 --> 14:09.200
because if they're fragile in a certain way,

14:09.200 --> 14:12.080
then it's almost like you can always kill them,

14:12.080 --> 14:13.920
which is kind of good, right?

14:13.920 --> 14:16.320
In a sense, that there's that, you know,

14:16.320 --> 14:20.480
almost like the, you know, the 50 IQ, 100 IQ, 150 IQ thing.

14:20.480 --> 14:22.080
Like the meme?

14:22.080 --> 14:23.280
Yeah, the meme, right?

14:23.280 --> 14:25.600
So the 50 IQ guys like these machines

14:25.600 --> 14:28.080
will never be as creative as humans or whatever.

14:28.080 --> 14:30.160
100 IQ is look at all the things they can do.

14:30.240 --> 14:34.800
The 150 IQ is like, well, there's some like equivalent result,

14:34.800 --> 14:37.280
you know, that's like some impossibility proof

14:37.280 --> 14:39.920
that shows that we, the dimensional space of a giraffe

14:39.920 --> 14:43.200
is too high and we can't actually learn what a true giraffe.

14:43.200 --> 14:46.080
I don't think that's true, but maybe it's true

14:46.080 --> 14:48.960
from the perspective of how these learners are working

14:48.960 --> 14:51.200
because my understanding is people have been trying,

14:51.200 --> 14:52.800
and I mean, I'm not on the cutting edge of this.

14:52.800 --> 14:54.080
So, you know, maybe someone,

14:54.080 --> 14:56.720
but my understanding is we haven't yet been able to

14:56.720 --> 15:00.880
robustify these models against adversarial input.

15:00.880 --> 15:02.320
Am I wrong about that?

15:02.320 --> 15:03.600
Yeah, that's definitely right.

15:03.600 --> 15:05.600
Hey, we'll continue our interview in a moment

15:05.600 --> 15:06.960
after a word from our sponsors.

15:26.720 --> 15:50.480
I've used it in the past at the companies I've founded,

15:50.480 --> 15:52.560
and when we launch Merch here at Turpentine,

15:52.560 --> 15:54.000
Shopify will be our go-to.

15:54.640 --> 15:56.640
Shopify helps turn browsers into buyers

15:56.640 --> 15:58.880
with the internet's best converting checkout

15:58.880 --> 16:02.400
up to 36% better compared to other leading commerce platforms.

16:02.400 --> 16:05.040
And Shopify helps you sell more with less effort

16:05.040 --> 16:08.000
thanks to Shopify Magic, your AI-powered all-star.

16:08.000 --> 16:10.720
With Shopify Magic, whip up captivating content

16:10.720 --> 16:13.760
that converts from blog posts to product descriptions.

16:13.760 --> 16:15.840
Generate instant FAQ answers.

16:15.840 --> 16:17.920
Pick the perfect email send time.

16:17.920 --> 16:21.120
Plus, Shopify Magic is free for every Shopify seller.

16:21.840 --> 16:23.920
Businesses that grow, grow with Shopify.

16:24.560 --> 16:26.720
Sign up for a $1 per month trial period

16:26.720 --> 16:29.280
at Shopify.com slash cognitive.

16:29.280 --> 16:31.600
Go to Shopify.com slash cognitive now

16:31.600 --> 16:34.080
to grow your business no matter what stage you're in.

16:34.080 --> 16:36.160
Shopify.com slash cognitive.

16:51.280 --> 17:01.440
There's no single architecture as far as I know

17:01.440 --> 17:04.320
that is demonstrably robust.

17:04.320 --> 17:06.400
And on the contrary, even with language models,

17:06.400 --> 17:07.120
there's a...

17:07.120 --> 17:09.440
We did a whole episode on the universal jailbreak

17:10.000 --> 17:13.280
where especially if you have access to the weights,

17:13.280 --> 17:14.240
not to change the weights,

17:14.240 --> 17:17.040
but just to probe around in the weights,

17:17.040 --> 17:19.120
then you have a really hard time

17:19.600 --> 17:21.360
guaranteeing any sort of robustness.

17:21.920 --> 17:24.800
The conjecture is, see, for humans,

17:24.800 --> 17:28.240
you can't mirror their brain and analyze it.

17:28.240 --> 17:28.880
Okay?

17:28.880 --> 17:30.960
But we have enough humans that we've got things

17:30.960 --> 17:33.680
like optical illusions, stuff like that,

17:33.680 --> 17:35.840
that works on enough humans

17:36.720 --> 17:38.400
and our brains aren't changing enough, right?

17:38.960 --> 17:42.160
A conjecture is, as you said, open weights.

17:42.880 --> 17:46.560
Open weights mean safety because if you have open weights,

17:46.560 --> 17:48.960
you can always reverse engineer adversarial input

17:49.360 --> 17:51.120
and then you can always break the system.

17:51.680 --> 17:52.240
Conjecture.

17:52.960 --> 17:56.000
Yeah, that's again with Adam from Far AI.

17:56.000 --> 17:57.760
I'm really interested to get into that

17:57.760 --> 17:58.960
because they are starting to study,

17:58.960 --> 17:59.840
as I understand it,

18:00.800 --> 18:05.040
kind of proto-scaling laws for adversarial robustness.

18:05.600 --> 18:07.600
And I think a huge question there is,

18:08.480 --> 18:12.160
what are the kind of frontiers of possibility there?

18:14.320 --> 18:15.680
How do the orders of magnitude work?

18:15.760 --> 18:19.440
Do you need another 10x as much adversarial training

18:19.440 --> 18:23.360
to half the rate of your adversarial failures?

18:23.360 --> 18:25.520
And if so, can we generate that many?

18:25.520 --> 18:28.080
It may always sort of be fleeting.

18:28.640 --> 18:33.200
So, Far AI and they are working on cutting edge

18:33.200 --> 18:34.640
of adversarial input.

18:34.640 --> 18:36.640
Yeah, they're the group that did the attack

18:36.640 --> 18:40.400
on the Alpha Go model and found that, like, you know,

18:40.400 --> 18:42.080
and what was really interesting about that,

18:42.080 --> 18:43.280
I mean, multiple things, right?

18:43.280 --> 18:45.520
First, that they could beat a super human Go player at all.

18:45.680 --> 18:47.680
But second, that the technique that they used

18:47.680 --> 18:50.720
would not work at all if playing a quality human.

18:50.720 --> 18:53.680
Or is, you know, it's a strategy that is trivial to beat

18:53.680 --> 18:55.840
if you're a quality human Go player,

18:55.840 --> 18:58.880
but the Alpha Go is just totally blind to it.

18:58.880 --> 19:00.480
You know, that's why I say the conjecture is,

19:01.200 --> 19:02.240
if you have the model,

19:03.520 --> 19:05.200
then you can generate the adversarial input.

19:05.760 --> 19:08.080
And then, so if that is true,

19:08.080 --> 19:11.200
and that itself is an important conjecture about AI safety,

19:11.200 --> 19:16.160
right? Because if open weights are inherently something

19:16.160 --> 19:18.080
where you can generate adversarial input from that

19:18.080 --> 19:20.800
and break or crash or defeat the AI,

19:22.160 --> 19:25.520
then that AI is not omnipotent, right?

19:25.520 --> 19:27.920
You have some power words you can speak to

19:27.920 --> 19:30.320
at almost like magical words that'll just make it

19:31.520 --> 19:34.560
power down, so to speak, right?

19:34.560 --> 19:37.200
It's like those movies where the monsters can't see you

19:37.200 --> 19:40.400
if you stand really still or if you don't make a noise

19:40.400 --> 19:41.680
or something like that, right?

19:41.680 --> 19:43.520
They're very powerful on Dimension X,

19:43.520 --> 19:44.880
but they're very weak on Dimension Y.

19:45.440 --> 19:47.440
A kind of an obvious point, but, you know,

19:47.440 --> 19:49.680
I'm not sure how important it's going to be in the future.

19:51.120 --> 19:53.040
Your next question was on like, you know,

19:53.040 --> 19:55.200
humanoid robots and so on, and before we get to that,

19:55.760 --> 19:59.600
maybe obviously, but all of these models are trained

19:59.600 --> 20:01.760
on things that we can easily record,

20:01.760 --> 20:04.960
which are sights and sounds, right?

20:05.600 --> 20:08.000
But touch and taste and smell,

20:08.960 --> 20:12.320
we don't have amazing data sets on those.

20:12.320 --> 20:14.480
Well, I mean, there's some haptic stuff, right?

20:15.760 --> 20:17.840
There's probably some, you know,

20:17.840 --> 20:19.440
some work on taste and smell and so on,

20:19.440 --> 20:21.760
but there's five senses, right?

20:21.760 --> 20:26.640
I wonder if there's something like that where you might be like,

20:26.640 --> 20:29.440
okay, how are you going to outsmell a robot

20:29.440 --> 20:30.480
or something like that?

20:30.480 --> 20:32.800
Well, dogs actually have a very powerful sense of smell,

20:32.800 --> 20:34.640
and that's being very important for them, you know?

20:35.280 --> 20:36.960
And it may turn out that there's,

20:37.040 --> 20:38.800
maybe it's just that we just haven't collected the data,

20:38.800 --> 20:41.280
and it could become a much better smeller or whatever,

20:41.280 --> 20:43.040
or, you know, taster than anything else.

20:43.040 --> 20:43.840
I wouldn't be surprised.

20:43.840 --> 20:45.600
It could be a much better wine taster,

20:45.600 --> 20:47.600
because you can do molecular diagnostics.

20:47.600 --> 20:51.440
But it's just kind of, I just use that as an analogy to say,

20:51.440 --> 20:52.880
there's areas of the human experience

20:52.880 --> 20:54.240
that we haven't yet quantified,

20:55.040 --> 20:57.840
and maybe it's just the opera term is yet, okay?

20:57.840 --> 20:58.960
But there's areas of the human experience

20:58.960 --> 21:00.480
we haven't yet quantified,

21:00.480 --> 21:02.880
which are also an area that AIs at least

21:02.880 --> 21:04.320
are not yet capable of.

21:04.320 --> 21:07.760
Yeah, I guess maybe my expectation boils down to,

21:09.360 --> 21:12.080
I think the really powerful systems

21:12.080 --> 21:16.560
are probably likely to mix architectures

21:16.560 --> 21:18.080
in some sort of ensemble.

21:18.080 --> 21:20.400
You know, when you think about just the structure of the brain,

21:20.400 --> 21:22.880
it's not, I mean, there certainly are aspects of it

21:22.880 --> 21:23.840
that are repeated, right?

21:23.840 --> 21:25.760
You look at the frontal cortex,

21:25.760 --> 21:28.480
and it's like there is kind of this, you know,

21:28.480 --> 21:30.560
unit that gets repeated over and over again,

21:30.560 --> 21:32.560
in a sense that's kind of analogous to say,

21:32.560 --> 21:34.160
the transformer block that just gets,

21:34.160 --> 21:35.920
you know, stacked layer on layer.

21:35.920 --> 21:37.920
But it is striking in a transformer

21:37.920 --> 21:40.480
that it's basically the same exact mechanism

21:40.480 --> 21:42.400
at every layer that's doing kind of

21:42.400 --> 21:44.320
all the different kinds of processing.

21:44.880 --> 21:47.680
And so whatever weaknesses that structure has,

21:47.680 --> 21:48.800
and you know, with the transformer

21:48.800 --> 21:49.840
and the attention mechanism,

21:49.840 --> 21:51.440
there's like some pretty profound ones,

21:51.440 --> 21:55.760
like finite context window, you know, you kind of need,

21:55.760 --> 21:58.080
I would think, a different sort of architecture

21:58.080 --> 21:59.680
with a little bit of a different strength

21:59.760 --> 22:04.880
and weakness profile to complement that in such a way that,

22:05.440 --> 22:07.680
you know, kind of more similar to like a biological system

22:07.680 --> 22:10.400
where you kind of have this like dynamic feedback

22:10.400 --> 22:12.400
where, you know, we have obviously, you know,

22:12.400 --> 22:14.560
thinking fast and slow and all sorts of different modules

22:14.560 --> 22:17.040
in the brain and they kind of cross regulate each other

22:17.600 --> 22:21.440
and don't let any one system, you know,

22:22.080 --> 22:25.200
go totally, you know, down the wrong path on its own, right?

22:25.200 --> 22:26.480
Without something kind of coming back

22:26.480 --> 22:27.760
and trying to override that.

22:28.240 --> 22:29.680
It seems to me like that's a big part

22:29.680 --> 22:33.680
of what is missing from the current crop of AIs

22:33.680 --> 22:35.200
in terms of their robustness.

22:35.840 --> 22:38.240
And I don't know how long that takes to show up,

22:38.800 --> 22:43.440
but we are starting to see some, you know, possibly,

22:43.440 --> 22:44.560
you know, I think people are maybe thinking

22:44.560 --> 22:45.680
about this a little bit the wrong way.

22:45.680 --> 22:47.360
They're just in the last couple of weeks,

22:47.360 --> 22:48.560
there's been a number of papers

22:49.280 --> 22:52.640
that are really looking at the state space model

22:53.200 --> 22:54.080
kind of alternative.

22:54.080 --> 22:56.800
It's being framed as an alternative to the transformer.

22:56.800 --> 22:58.560
But when I see that I'm much more like,

22:59.280 --> 23:01.920
it's probably a compliment to the transformer

23:01.920 --> 23:04.400
or, you know, these two things probably get integrated

23:04.400 --> 23:06.480
in some form because to the degree

23:06.480 --> 23:08.560
that they do have very different strengths and weaknesses,

23:08.560 --> 23:10.480
ultimately you're going to want the best of both

23:10.480 --> 23:11.840
in a robust system.

23:11.840 --> 23:13.040
Certainly if you're trying to make an agent,

23:13.040 --> 23:14.480
certainly if you're trying to make, you know,

23:14.480 --> 23:16.560
a humanoid robot that can go around your house

23:16.560 --> 23:18.080
and like do useful work,

23:18.080 --> 23:20.800
but also be robust enough that it doesn't,

23:20.800 --> 23:22.640
you get tricked into attacking your kid

23:22.640 --> 23:24.000
or your dog or, you know, whatever,

23:24.480 --> 23:26.720
you're going to want to have more checks and balances

23:26.720 --> 23:29.360
than just kind of a single stack of, you know,

23:29.360 --> 23:31.680
the same block over and over again.

23:31.680 --> 23:35.120
Well, so I know Boston Dynamics with their legged robots

23:35.120 --> 23:38.480
is all control theory and it's not classical ML.

23:38.480 --> 23:41.360
It's really interesting to see how they've accomplished it.

23:41.360 --> 23:44.160
And they do have essentially a state space model

23:44.160 --> 23:46.880
where they have a big position vector

23:46.880 --> 23:48.800
that's got all the coordinates of all the joints

23:48.800 --> 23:51.040
and then a bunch of matrix algebra to figure out

23:51.040 --> 23:53.760
how this thing is moving and all the feedback control

23:53.760 --> 23:54.880
and so on there.

23:54.880 --> 23:55.920
And it's more complicated than that,

23:55.920 --> 23:58.240
but that's, you know, I think the V1 of it.

23:58.240 --> 23:59.360
Sorry, it was there.

23:59.360 --> 24:00.880
I wasn't following this though.

24:00.880 --> 24:02.480
Are you saying that there's papers

24:02.480 --> 24:05.360
that are integrating that with the kind of

24:05.360 --> 24:07.280
generator transformer model?

24:07.280 --> 24:09.360
You know, like what's a good citation for me to look at?

24:09.360 --> 24:12.320
Yeah, starting to, we did an episode, for example,

24:12.320 --> 24:15.760
with one of the technology leads at Skydio,

24:15.760 --> 24:19.040
the, you know, the U.S. is champion drone maker.

24:19.840 --> 24:22.400
And they have kind of a similar thing

24:22.400 --> 24:26.240
where they have built over, you know, a decade, right?

24:26.240 --> 24:31.920
A fully explicit multiple orders of, you know,

24:31.920 --> 24:34.480
spanning multiple orders of magnitude control stack.

24:35.280 --> 24:38.160
And now over the top of that,

24:38.160 --> 24:40.480
they're starting to layer this kind of, you know,

24:40.480 --> 24:42.400
it's not exactly generative AI in their case

24:42.400 --> 24:44.720
because they're not like generating content,

24:44.720 --> 24:47.280
but it's kind of the high level, you know,

24:47.280 --> 24:50.080
can I give the thing verbal instructions?

24:50.080 --> 24:52.080
Have it go out and kind of understand,

24:52.080 --> 24:54.720
okay, like this is a bridge, I'm supposed to kind of,

24:54.720 --> 24:58.160
you know, survey the bridge and translate

24:58.160 --> 25:01.120
those high level instructions to a plan

25:01.120 --> 25:04.480
and then use the lower level explicit code

25:04.480 --> 25:07.040
that is fully deterministic and, you know,

25:07.040 --> 25:09.360
runs on control theory and all that kind of stuff

25:09.360 --> 25:12.160
to actually execute the plan at a low level.

25:12.160 --> 25:14.560
But also, you know, at times like surface errors

25:14.560 --> 25:16.320
up to the top and say like, hey, we've got a problem,

25:16.320 --> 25:17.840
you know, whatever, I'm not able to do it.

25:17.840 --> 25:20.960
You know, can you now, at the higher level,

25:21.040 --> 25:23.120
the semantic layer adjust the plan?

25:23.920 --> 25:26.640
That stuff is starting to happen in multiple domains,

25:26.640 --> 25:27.360
I would say.

25:27.360 --> 25:28.880
Yeah, and so I think that makes sense,

25:28.880 --> 25:32.080
is basically it's like generative AI is almost the front end,

25:32.080 --> 25:34.080
and then you have almost like an assembly,

25:34.080 --> 25:36.480
like you give instructions to Figma

25:36.480 --> 25:39.680
and the objects there are their shapes

25:39.680 --> 25:42.160
and their images and so on, it's not text.

25:42.160 --> 25:43.520
You give instructions to a drone

25:43.520 --> 25:48.000
and the objects are like GPS coordinates and paths and so on.

25:48.640 --> 25:52.480
And so you are generating structures

25:52.480 --> 25:53.840
that are in a different domain,

25:53.840 --> 25:56.400
or it's like in VR, you're generating 3D structures,

25:56.400 --> 26:00.000
again, as opposed to text, and then that compute engine

26:00.000 --> 26:01.200
takes those three structures

26:01.200 --> 26:04.000
and does something with them in a much more rules-based way.

26:04.000 --> 26:06.960
So you have like a statistical user-friendly front end

26:06.960 --> 26:10.720
with a generative AI, and then you have a more deterministic

26:10.720 --> 26:12.160
or usually totally deterministic,

26:13.280 --> 26:14.800
almost like assembly language back end

26:14.800 --> 26:16.160
that actually takes that and does something.

26:16.160 --> 26:17.120
That's what you're saying, right?

26:17.120 --> 26:19.120
Yeah, pretty much, and I would say there's another analogy

26:19.120 --> 26:21.360
to just, again, our biological experience

26:21.360 --> 26:26.080
where it's like I'm sort of in a semi-conscious level, right?

26:26.080 --> 26:28.160
I kind of think about what I want to do,

26:28.160 --> 26:30.640
but the low-level movements of the hand

26:30.640 --> 26:32.320
are both like not conscious,

26:32.320 --> 26:38.320
and also if I do encounter some pain or hit some hot item

26:38.320 --> 26:40.480
or whatever, there's a quick reaction

26:40.480 --> 26:44.400
that's sort of mediated by a lower level control system,

26:44.400 --> 26:45.920
and then that fires back up to the brain

26:45.920 --> 26:48.240
and is like, hey, we need a new plan here.

26:48.240 --> 26:53.440
So that is only starting to come into focus, I think,

26:53.440 --> 26:56.240
with, because obviously these, I mean, it's amazing,

26:56.240 --> 26:57.920
as you said, it's all moving so fast.

26:58.800 --> 27:01.200
What is always striking to me,

27:01.200 --> 27:03.760
and I kind of like recite timelines to myself

27:03.760 --> 27:05.120
almost as like a mantra, right?

27:05.120 --> 27:07.760
Like the first instruction following AI

27:07.760 --> 27:10.640
that hit the public was just January 2022.

27:10.640 --> 27:12.800
That was OpenAI's Text of Ingee 002.

27:12.800 --> 27:14.560
It was the first one where you could say like do X,

27:14.560 --> 27:17.200
and it would do X as opposed to having

27:17.200 --> 27:19.360
an elaborate prompt engineering type of setup.

27:20.160 --> 27:23.280
GPT-4 just a little over a year ago finished training,

27:23.280 --> 27:24.960
not even a year that it's been in the public,

27:25.680 --> 27:29.280
and it has been amazing to see how quickly

27:29.920 --> 27:32.640
this kind of technology is being integrated into those systems,

27:32.640 --> 27:34.800
but it's definitely still very much a work in progress.

27:35.360 --> 27:40.080
Yeah, I mean, the tricky part is the training data and so on.

27:40.320 --> 27:46.320
Like a large existing scale company like a Figma or DJI

27:46.320 --> 27:49.760
that has millions or billions of user sessions

27:49.760 --> 27:51.680
will have a much easier time training,

27:52.400 --> 27:55.040
and they have a unique data set,

27:55.040 --> 27:58.400
and then everybody else will not be able to do that.

27:58.400 --> 27:59.600
So there is actually almost like,

28:00.400 --> 28:03.360
I mean, a return on scale where the massive data set,

28:03.360 --> 28:04.800
if you've got a massive clean data set

28:04.800 --> 28:07.520
and a unique domain that lots of people are using,

28:07.520 --> 28:09.040
then you can crush it.

28:09.760 --> 28:12.080
And if you don't, I suppose,

28:12.080 --> 28:14.080
I mean, there's lots of people who work on zero shot stuff

28:14.080 --> 28:15.920
and sort of sort of, but it still strikes me

28:15.920 --> 28:19.040
that there'll probably be an advantage to see those sessions.

28:20.400 --> 28:23.120
I find it hard to believe that you could generate

28:23.120 --> 28:27.280
a really good drone command language

28:27.280 --> 28:30.000
without lots of drone flight paths, but you can see.

28:30.000 --> 28:31.440
And where it doesn't exist, people are,

28:32.080 --> 28:33.520
obviously you need deep pockets for this,

28:33.520 --> 28:36.400
but the likes of Google are starting to just

28:37.360 --> 28:39.120
grind out the generation of that, right?

28:39.120 --> 28:41.760
They've got their kind of test kitchen,

28:41.760 --> 28:44.640
which is a literal physical kitchen at Google

28:44.640 --> 28:47.040
where the robots go around and do tasks.

28:47.040 --> 28:48.880
And when they get stuck, my understanding

28:48.880 --> 28:52.080
of their kind of critical path, as I understand,

28:52.080 --> 28:55.920
they understand it, is robots gonna get stuck,

28:56.560 --> 29:01.120
will have a human operator remotely operate the robot

29:01.680 --> 29:05.280
to show what to do, and then that data becomes

29:05.280 --> 29:07.520
the bridge from what the robot can't do

29:07.520 --> 29:09.520
to what it's supposed to learn to do next time.

29:10.160 --> 29:12.800
And they're gonna need a lot of that, for sure.

29:13.440 --> 29:15.360
But they increasingly have,

29:15.360 --> 29:16.960
I don't know exactly how many robots they have now,

29:16.960 --> 29:18.400
but last I talked to someone there,

29:18.400 --> 29:20.320
it was like into the dozens,

29:20.960 --> 29:24.560
and presumably they're continuing to scale that.

29:24.560 --> 29:29.360
I think they just view that they can probably brute force it

29:29.360 --> 29:32.640
to the point where it's good enough to put out into the world,

29:32.640 --> 29:34.560
and then very much like a Waymo or a cruise

29:34.560 --> 29:36.800
or whatever, they probably still have remote operators,

29:36.800 --> 29:38.960
even when the robot is in your home,

29:40.080 --> 29:40.960
when it encounters something

29:40.960 --> 29:43.760
that it doesn't know what to do about, raise that alarm,

29:43.760 --> 29:46.000
get the human supervision to help it over the hump,

29:46.000 --> 29:49.600
and then obviously that's where you really get the scale

29:49.600 --> 29:50.880
that you're talking about.

29:50.880 --> 29:53.200
And this raises a couple of questions I wanted to ask

29:53.200 --> 29:54.320
that are conceptual.

29:54.320 --> 29:57.840
So obviously there's huge questions around like,

29:59.120 --> 30:01.920
again, highest level, how is all this gonna play out?

30:01.920 --> 30:06.560
One big debate is to what degree does AI favor the incumbents?

30:06.560 --> 30:09.200
To what degree does it enable startups?

30:09.200 --> 30:10.160
Obviously it's both,

30:10.160 --> 30:12.640
but I'm interested in your perspective on that.

30:12.640 --> 30:13.920
Also really interested in your perspective

30:13.920 --> 30:15.840
on like offense versus defense.

30:15.840 --> 30:17.200
That's something that a lot of people

30:18.000 --> 30:19.360
now and in the future,

30:19.360 --> 30:21.600
that seems like it probably really matters a lot,

30:21.600 --> 30:23.440
whether it's a more offense enabling

30:23.440 --> 30:24.720
or defense enabling technology.

30:24.720 --> 30:28.000
So I love your take on those two dimensions.

30:28.000 --> 30:29.920
Hey, we'll continue our interview in a moment

30:29.920 --> 30:31.280
after a word from our sponsors.

30:32.080 --> 30:34.480
Omniki uses generative AI

30:34.480 --> 30:36.400
to enable you to launch hundreds of thousands

30:36.400 --> 30:38.800
of ad iterations that actually work,

30:38.800 --> 30:42.160
customized across all platforms with a click of a button.

30:42.160 --> 30:44.720
I believe in Omniki so much that I invested in it

30:44.720 --> 30:46.320
and I recommend you use it too.

30:47.040 --> 30:49.680
Use Cogrev to get a 10% discount.

30:49.680 --> 30:51.280
If you're a startup founder or executive

30:51.280 --> 30:52.400
running a growing business,

30:52.400 --> 30:53.680
you know that as you scale,

30:53.680 --> 30:56.320
your systems break down and the cracks start to show.

30:56.880 --> 30:58.000
If this resonates with you,

30:58.000 --> 30:59.600
there are three numbers you need to know,

30:59.680 --> 31:03.840
36,000, 25, and one, 36,000.

31:03.840 --> 31:04.880
That's the number of businesses

31:04.880 --> 31:06.800
which have upgraded to NetSuite by Oracle.

31:06.800 --> 31:08.880
NetSuite is the number one cloud financial system,

31:08.880 --> 31:10.880
streamline accounting, financial management,

31:10.880 --> 31:13.920
inventory, HR, and more, 25.

31:13.920 --> 31:15.680
NetSuite turns 25 this year.

31:15.680 --> 31:18.480
That's 25 years of helping businesses do more with less,

31:18.480 --> 31:21.440
close their books in days, not weeks, and drive down costs.

31:22.160 --> 31:24.240
One, because your business is one of a kind,

31:24.240 --> 31:26.800
so you get a customized solution for all your KPIs

31:26.800 --> 31:29.280
in one efficient system with one source of truth.

31:29.360 --> 31:32.640
Manage risk, get reliable forecasts, and improve margins.

31:32.640 --> 31:34.240
Everything you need, all in one place.

31:34.800 --> 31:38.160
Right now, download NetSuite's popular KPI checklist,

31:38.160 --> 31:40.560
designed to give you consistently excellent performance,

31:40.560 --> 31:43.760
absolutely free, and netsuite.com slash cognitive.

31:43.760 --> 31:45.920
That's netsuite.com slash cognitive

31:45.920 --> 31:47.760
to get your own KPI checklist.

31:47.760 --> 31:49.600
NetSuite.com slash cognitive.

31:50.400 --> 31:51.920
So like offense or defense

31:51.920 --> 31:54.400
in the sense of disenabled disruptors or incumbents?

31:54.960 --> 31:56.480
Both in business and in like,

31:56.480 --> 31:58.240
you know, potentially outright conflict.

31:58.240 --> 32:00.880
I'd be interested to hear your analysis on both.

32:00.880 --> 32:02.080
All right, a lot of views on this.

32:02.080 --> 32:07.120
So obviously, if you've got a competent, existing tech CEO,

32:07.120 --> 32:10.240
you know, like who's still in their prime,

32:10.240 --> 32:14.880
like Amjad of Replit, or, you know, Dillon Field of Figma,

32:16.640 --> 32:19.520
or, you know, those are two who have thought of,

32:19.520 --> 32:22.720
who are very good and, you know, will be on top of it.

32:22.720 --> 32:25.920
Amjad is very early on integrating AI into Replit,

32:25.920 --> 32:28.320
and it's basically built that into an AI first company,

32:28.320 --> 32:29.280
which is really impressive.

32:30.000 --> 32:34.000
Those are folks who cleanly made a pivot.

32:34.000 --> 32:37.520
It's as big or bigger than, comparable to, I would say,

32:37.520 --> 32:40.640
the pivot from desktop to mobile

32:40.640 --> 32:42.080
that broke a bunch of companies

32:42.080 --> 32:44.080
in the late 2000s and early 2010s.

32:44.080 --> 32:48.080
Like Facebook in 2012 had no mobile revenue, roughly,

32:48.080 --> 32:49.680
at the time of their IPO,

32:49.680 --> 32:51.440
and then they had to like redo the whole thing.

32:51.440 --> 32:54.400
And it's hard to turn a company 90 degrees

32:54.400 --> 32:56.480
when something new like that hits, you know?

32:57.040 --> 32:59.440
Those that are run by kind of tech CEOs in their prime

33:00.400 --> 33:03.760
will adapt and will AI-ify their existing services.

33:04.320 --> 33:05.680
And the question is, obviously,

33:05.680 --> 33:06.880
there's new things that are coming out,

33:06.880 --> 33:08.800
like pika and character.ai.

33:08.800 --> 33:11.440
There's some like really good stuff that's out there.

33:11.440 --> 33:13.760
The question is, you know,

33:14.320 --> 33:16.000
will the disruption be allowed to happen

33:16.000 --> 33:17.520
in the U.S. regulatory environment?

33:18.320 --> 33:21.680
And so my view is actually that, you know,

33:21.680 --> 33:24.320
so this is from like the network state book, right?

33:25.280 --> 33:27.040
You know, people talk about a multipolar world

33:27.040 --> 33:28.400
or unipolar world.

33:28.400 --> 33:30.880
The political axis is actually really important

33:30.880 --> 33:32.480
in my view for thinking about

33:32.480 --> 33:35.920
whether AI will be allowed to disrupt, okay?

33:35.920 --> 33:37.840
Because we'll get to this probably later,

33:37.840 --> 33:40.240
but the 640K of compute is enough

33:40.240 --> 33:42.000
for everyone executive order.

33:42.000 --> 33:44.720
You know, 640K of memory, the apocryphal,

33:44.720 --> 33:46.160
he didn't delegate to actually say it,

33:46.160 --> 33:48.800
but that quote kind of gives a certain mindset

33:48.800 --> 33:49.600
about computing.

33:49.600 --> 33:50.960
That should be enough for everybody.

33:50.960 --> 33:52.640
So the 10 to the 26 of compute

33:52.640 --> 33:53.920
should be enough for everyone bill.

33:55.040 --> 33:56.400
I actually think it's very bad

33:56.400 --> 33:57.600
and I think it's just the beginning

33:57.600 --> 34:01.040
of their attempts to build like a software FDA, okay?

34:01.040 --> 34:04.480
To decelerate, control, regulate, red tape,

34:04.480 --> 34:07.760
the entire space, just like how, you know,

34:07.760 --> 34:10.480
the threat of nuclear terrorism got turned into the TSA.

34:11.360 --> 34:14.160
The threat of, you know, terminators and AGI

34:14.160 --> 34:16.480
gets turned into a million rules

34:16.480 --> 34:18.080
on whether you can set up servers

34:18.080 --> 34:20.400
and this last free sector of the economy

34:20.400 --> 34:22.640
is strangled or at least controlled

34:23.200 --> 34:25.920
within the territory control by Washington DC.

34:26.880 --> 34:30.000
Now, why does this relate to the political?

34:30.000 --> 34:32.160
Well, obviously this, you know,

34:32.160 --> 34:33.840
you can just spend your entire life

34:33.840 --> 34:35.120
just tracking AI papers

34:35.120 --> 34:37.680
and that's moving like at the speed of light like this, right?

34:38.320 --> 34:39.440
What's also happening

34:39.440 --> 34:41.200
as you can kind of see in your peripheral vision

34:41.200 --> 34:43.840
is there's political developments

34:43.840 --> 34:45.040
that are happening at the speed of light

34:45.040 --> 34:47.040
much faster than they've happened in our lifespans.

34:47.040 --> 34:48.640
Like there's more, you just noticed,

34:48.640 --> 34:51.120
more wars, more serious online conflicts

34:51.120 --> 34:53.440
like, you know, there's a sovereign debt crisis.

34:53.440 --> 34:55.440
All of those things that can show graph after graph

34:55.440 --> 34:58.880
of things looking like their own types of singularities, you know,

34:59.440 --> 35:00.960
like military debts are way up, you know,

35:00.960 --> 35:03.360
the long piece that Steven Pinker showed

35:03.360 --> 35:04.320
that's looking like a you

35:04.320 --> 35:06.080
that suddenly way up after Ukraine

35:06.080 --> 35:07.920
and some of these other wars are happening, unfortunately, right?

35:08.800 --> 35:10.960
Interest payments, whoosh, way up to the side.

35:10.960 --> 35:11.920
What's my point?

35:11.920 --> 35:15.600
Point is, I think that the world is going to become

35:15.600 --> 35:17.760
from the Pax Americana world

35:17.760 --> 35:20.320
of just like basically one superpower,

35:20.320 --> 35:23.600
a hyperpower that we grew up in from 91 to 2021, roughly,

35:24.160 --> 35:28.160
that we're going to get a specifically tripolar world,

35:28.160 --> 35:31.680
not unipolar, not bipolar, not multipolar, but tripolar.

35:31.680 --> 35:36.560
And those three poles, I kind of think of as NYT, CCP, BTC,

35:36.560 --> 35:38.080
or you could think of them as,

35:38.080 --> 35:40.720
and those are just certain labels that are associated with them,

35:40.720 --> 35:44.480
but they're roughly U.S. tech, the U.S. environment,

35:45.200 --> 35:46.880
China tech and China environment,

35:46.880 --> 35:48.640
and global tech and the global environment.

35:49.280 --> 35:53.360
And why do I identify BTC and crypto and so on with global tech?

35:53.360 --> 35:55.520
Because that's a tech that decentralized out of the U.S.

35:56.240 --> 35:59.200
And right now people think of crypto as finance,

35:59.200 --> 36:00.560
but it's also financiers.

36:02.080 --> 36:03.680
Okay, and in this next run-up,

36:04.560 --> 36:08.400
it is, I think, quite likely about, depending on how you count,

36:08.400 --> 36:10.960
between a third to a half of the world's billionaires will be crypto.

36:12.320 --> 36:15.360
Okay, around, you know, I calculated this a while back around,

36:15.360 --> 36:16.960
Bitcoin at a few hundred thousands,

36:16.960 --> 36:19.600
around a third to a half the world's billionaires are crypto.

36:19.600 --> 36:21.520
That's the unlocked pool of capital.

36:22.080 --> 36:26.800
And those are the people who do not bow to D.C. or Beijing.

36:26.800 --> 36:29.520
And they might, by the way, be Indians or Israelis

36:29.520 --> 36:31.360
or every other demographic in the world,

36:31.360 --> 36:33.360
or they could be American libertarians,

36:33.360 --> 36:35.520
or they could be Chinese liberals like Jack Ma,

36:35.520 --> 36:37.280
who are pushed out of Beijing sphere.

36:37.280 --> 36:38.640
Okay, or the next Jack Ma.

36:38.640 --> 36:41.920
You know, Jack Ma himself may not be able to do too much, okay?

36:42.560 --> 36:46.320
That group of people who are, let's say, the dissident technologists

36:46.320 --> 36:49.760
who are not going to just kneel to anything

36:49.760 --> 36:51.600
that comes out of Washington D.C. or Beijing,

36:52.160 --> 36:54.400
that is the, that's decentralized AI.

36:55.040 --> 36:56.240
That's crypto.

36:56.240 --> 36:57.680
That's decentralized social media.

36:57.680 --> 36:59.280
So you can think of it as, you know,

36:59.280 --> 37:02.080
where we talked about in the recent Pirate Warriors podcast,

37:02.080 --> 37:06.000
freedom to speak with decentralized censorship-resistant social media,

37:06.000 --> 37:08.560
freedom to transact with cryptocurrency,

37:08.560 --> 37:11.600
freedom to compute with open source AI.

37:11.680 --> 37:13.440
And no compute limits, okay?

37:14.080 --> 37:15.280
That's a freedom movement.

37:16.320 --> 37:18.880
And that's like the same spirit as a pirate bay,

37:18.880 --> 37:20.240
the same spirit as BitTorrent,

37:20.240 --> 37:22.080
the same spirit as Bitcoin,

37:22.080 --> 37:25.520
the same spirit as peer-to-peer and end-to-end encryption.

37:25.520 --> 37:28.160
That's a very different spirit than

37:28.160 --> 37:30.880
having Kamala Harris regulate a superintelligence

37:30.880 --> 37:33.760
or signing it over to Xi Jinping thought.

37:34.480 --> 37:39.200
And the reason I say this is, I think that that group of people,

37:39.200 --> 37:42.480
of which I think Indians and Israelis will be a very prominent,

37:42.480 --> 37:44.080
maybe a plurality, right?

37:44.080 --> 37:45.520
Just because the sheer quantity of Indians

37:45.520 --> 37:48.400
are like the third sort of big group that's kind of coming up.

37:48.400 --> 37:49.920
And they're relatively underpriced.

37:49.920 --> 37:53.600
You know, China is, I don't say it's price to perfection,

37:53.600 --> 37:56.160
but it's something that people, when I say priced,

37:56.160 --> 38:00.080
I mean, people were dismissive of China even up until 2019.

38:00.960 --> 38:02.480
And then it was after 2020,

38:02.480 --> 38:04.480
if you look that people started to take China seriously.

38:05.040 --> 38:07.440
And I mean, that is the West Coast tech people

38:07.440 --> 38:10.080
knew that China actually had A plus tech companies

38:10.080 --> 38:11.760
and was a very strong competitor.

38:11.760 --> 38:14.240
But the East Coast still thought of them as a third-world country

38:14.240 --> 38:17.360
until after COVID, when now, you know,

38:17.360 --> 38:19.760
the East Coast was sort of threatened by them politically.

38:19.760 --> 38:23.440
And it wasn't just blue collars, but blue America

38:23.440 --> 38:24.400
that was threatened by China.

38:25.040 --> 38:27.600
And so that's why the reaction to China went from,

38:27.600 --> 38:30.320
oh, who cares, it's just taking some manufacturing jobs to,

38:30.320 --> 38:32.160
this is an empire that can contend with us

38:32.160 --> 38:33.440
for control of the world.

38:33.440 --> 38:35.440
That's why the hostility is ramped up, in my view.

38:35.440 --> 38:36.480
There's a lot of other dimensions to it,

38:36.480 --> 38:37.440
but that's a big part of it.

38:38.560 --> 38:41.440
So India is also kind of there, but it's like the third.

38:41.440 --> 38:43.440
And India is not going to play for number one or number two.

38:44.160 --> 38:46.560
But India and Israel, if you look at like tech founders,

38:47.840 --> 38:50.320
depending on how you count, especially if you include diasporas,

38:50.320 --> 38:53.600
it's on the order of 30 to 50% of tech founders, right?

38:53.600 --> 38:55.680
And it's obviously some, you know, very good tech CEOs

38:55.680 --> 38:58.720
and, you know, Satya and Sundar and investors and whatnot.

38:58.720 --> 39:02.960
Those are folks Indians do not want to bow to DC or to Beijing,

39:02.960 --> 39:04.560
neither do Israelis for all kinds of reasons,

39:04.640 --> 39:06.640
even if Israel has to, you know,

39:06.640 --> 39:08.160
take some direction from the U.S.

39:08.160 --> 39:09.440
Now they're bristling at it, right?

39:10.080 --> 39:12.080
And then a bunch of other countries don't.

39:12.080 --> 39:14.480
So the question is, who breaks away?

39:14.480 --> 39:16.320
And now we get to your point on,

39:16.320 --> 39:19.760
the reason I had to say that is that that's preface,

39:19.760 --> 39:23.040
the political environment, this tripolar thing of U.S. tech

39:23.760 --> 39:26.880
and U.S. regulated, Chinese tech and China regulated,

39:26.880 --> 39:28.240
and global tech that's free.

39:29.120 --> 39:32.080
Okay, of course there's, even though I identify those three polls,

39:32.080 --> 39:33.760
there's of course boundary regions.

39:33.840 --> 39:36.880
EAC is actually on the boundary of U.S. tech

39:36.880 --> 39:38.800
and decentralized tech, you know?

39:38.800 --> 39:41.440
And I'm sure there'll be some Chinese thing that comes out

39:41.440 --> 39:42.640
that is also on the boundary there.

39:42.640 --> 39:45.680
For example, Binance is on the boundary of Chinese tech

39:45.680 --> 39:47.440
and global and decentralized tech,

39:47.440 --> 39:48.640
if that makes any sense, right?

39:48.640 --> 39:50.160
And there's probably others, Apple is actually

39:50.160 --> 39:52.080
on the boundary of U.S. tech and Chinese tech

39:52.080 --> 39:54.000
because they make all of their stuff in China, right?

39:54.000 --> 39:56.880
So these are not totally disjoint groups,

39:56.880 --> 39:58.960
but there's boundary areas, but you can think of them.

39:59.520 --> 40:01.600
Why is this third group so important in my view?

40:02.320 --> 40:05.120
Both the Chinese group and the decentralized group

40:05.120 --> 40:08.640
will be very strong competition for the American group

40:08.640 --> 40:09.920
for totally different reasons.

40:10.880 --> 40:14.480
China has things like WeChat, these super apps,

40:14.480 --> 40:15.840
I mean, obviously not likely,

40:17.040 --> 40:19.440
WeChat is a super app, but they also have,

40:19.440 --> 40:21.520
for example, their digital yuan, right?

40:21.520 --> 40:25.520
They have the largest, cleanest data sets in the world

40:25.520 --> 40:27.040
that are constantly updated in real time

40:27.040 --> 40:29.440
that they can mandate their entire population opt into,

40:30.000 --> 40:35.120
and most of the Chinese language speaking people

40:35.120 --> 40:37.840
are under their ambit, right?

40:37.840 --> 40:40.560
So that doesn't include Taiwan, doesn't include Singapore,

40:40.560 --> 40:43.920
doesn't include some of the Chinese diaspora,

40:43.920 --> 40:45.920
but basically anything that's happening in Chinese

40:45.920 --> 40:50.240
for 99% of it, 95, whatever the ratio is, they can see it

40:50.240 --> 40:52.560
and they can coerce it and they can control it.

40:52.560 --> 40:55.600
So they can tell all of their people,

40:55.600 --> 40:59.440
okay, here's five bucks in digital yuan,

40:59.440 --> 41:01.120
do this micro task, okay?

41:01.840 --> 41:03.920
All of these digital blue collar jobs,

41:03.920 --> 41:04.960
both China and India, I think,

41:04.960 --> 41:07.040
can do quite a lot with that and they'll come back to it.

41:07.040 --> 41:09.040
So they can make their people do immense amounts

41:09.040 --> 41:11.520
of training data, clean up lots of data sets,

41:11.520 --> 41:13.440
once it's clear that you have to build this and do this,

41:13.440 --> 41:15.120
they can just kind of execute on that.

41:15.120 --> 41:18.240
And they can also deploy, I mean, in many ways,

41:18.240 --> 41:21.040
the US is still very strong in digital technology,

41:21.040 --> 41:22.960
but in the physical world, it's terrible

41:23.840 --> 41:24.960
because of all the regulations,

41:24.960 --> 41:26.960
it causes all the nimbyism and so on.

41:26.960 --> 41:28.640
It's not like that in China.

41:28.640 --> 41:31.040
So anything which kind of works in the US

41:31.040 --> 41:33.200
at a physical level, like the Boston Dynamic stuff,

41:33.200 --> 41:34.880
they're already cloning it in China

41:34.880 --> 41:36.800
and they can scale it out in the physical world.

41:36.800 --> 41:40.000
You already have drones, little sidewalk drone things

41:40.000 --> 41:42.160
that come to your hotel room and drop things off.

41:42.160 --> 41:44.480
That's already very common in China.

41:44.480 --> 41:45.600
In many ways, it's already ahead

41:45.600 --> 41:47.120
if you go to the Chinese cities.

41:47.120 --> 41:49.440
So the Chinese version of AI is ultra centralized,

41:49.440 --> 41:52.800
more centralized, more monitoring, less privacy

41:52.800 --> 41:54.320
and so on than the American version,

41:54.320 --> 41:56.960
and therefore they will have potentially better data sets,

41:56.960 --> 41:58.800
at least for the Chinese population.

41:58.800 --> 42:01.520
And so we chat AI, I don't even know what it's gonna be,

42:01.520 --> 42:03.360
but it'll be probably really good.

42:03.360 --> 42:04.960
It'll also be really dangerous in other ways.

42:06.160 --> 42:10.480
Then the decentralized sphere has power for a different reason

42:10.480 --> 42:11.920
because the decentralized sphere

42:11.920 --> 42:14.000
can train on full Hollywood movies.

42:15.040 --> 42:19.120
It can train on all books, all MP3s.

42:19.520 --> 42:22.560
And just say, screw all this copyright stuff, right?

42:22.560 --> 42:25.600
Like what Psyhub and Libgen are doing.

42:25.600 --> 42:28.160
Because all the copyright, first of all,

42:28.160 --> 42:31.200
it's not, it's like Disney lobbying politicians

42:31.200 --> 42:33.600
to put like another 60 or 70 or 90.

42:33.600 --> 42:34.400
I don't even know what it is,

42:34.400 --> 42:35.920
some crazy amount on copyrights.

42:35.920 --> 42:37.440
So you can keep milking this stuff

42:37.440 --> 42:39.440
and it doesn't go into public domain, number one.

42:39.440 --> 42:40.960
And second, you know how Hollywood is built

42:40.960 --> 42:41.840
in the first place?

42:41.840 --> 42:44.080
It was all patent copyright and IP violation.

42:44.080 --> 42:45.920
Essentially Edison had all the patents.

42:45.920 --> 42:48.960
He's in New Jersey-ish, okay, that East Coast area.

42:48.960 --> 42:51.760
And Neil Gabler has this great book

42:51.760 --> 42:53.600
called An Empire of Their Own

42:53.600 --> 42:57.200
where he talks about how immigrant populations,

42:57.200 --> 42:58.480
the Jewish community in particular,

42:58.480 --> 43:01.760
and also others went to Southern California in part

43:01.760 --> 43:03.440
so they could just make movies

43:03.440 --> 43:04.960
so that Edison coming and suing them

43:04.960 --> 43:06.720
for all the patents and so on and so forth.

43:06.720 --> 43:07.520
And they made enough money

43:07.520 --> 43:09.280
that they could fight those battles in court

43:09.280 --> 43:11.920
and that's how they built Hollywood, okay?

43:11.920 --> 43:15.840
So one of my big thesis is history is running in reverse

43:15.840 --> 43:16.960
and I can get to why,

43:16.960 --> 43:18.800
but it's like 1950s, a mirror moment

43:18.800 --> 43:20.640
and you go more decentralized backwards

43:20.640 --> 43:22.480
and forwards in time is like these,

43:22.480 --> 43:23.920
you have these huge centralized states

43:23.920 --> 43:26.960
like the US and USSR and China, you know,

43:26.960 --> 43:29.600
all these things exist and then their fist relaxes

43:29.600 --> 43:31.600
as you go forwards and backwards in time.

43:31.600 --> 43:33.040
For example, backwards in time,

43:33.040 --> 43:34.640
the Western frontier closed

43:35.360 --> 43:37.280
and forwards in time, the Eastern frontier opens.

43:37.280 --> 43:39.040
Backwards in time, you have the robber barons,

43:39.040 --> 43:40.640
forwards in time, you have the tech billionaires.

43:40.640 --> 43:42.240
Backwards in time, you have Spanish flu,

43:42.240 --> 43:43.600
forwards in time, you have COVID-19.

43:43.600 --> 43:46.400
And I've got dozens of examples of this in the book.

43:46.400 --> 43:49.120
The point is that if you go backwards in time,

43:49.120 --> 43:51.840
the ability to enforce patents and copyrights and so on

43:51.840 --> 43:53.520
starts dropping off, right?

43:53.520 --> 43:56.480
You have much more of a Grand Theft Auto environment

43:56.480 --> 43:58.480
and you go forwards in time and that's happening again.

43:59.360 --> 44:02.240
So India in particular, for many years,

44:02.240 --> 44:06.160
basically just didn't obey Western patent protections

44:06.160 --> 44:08.720
and all these stupid rules basically, you know,

44:09.440 --> 44:11.920
it's a combination of artificial scarcity on the patent side

44:11.920 --> 44:14.080
and artificial regulation on the FDI side.

44:14.080 --> 44:16.240
That's a big part of what jacksup drug costs,

44:16.240 --> 44:17.440
where these things cost, you know,

44:17.440 --> 44:18.480
only cents to manufacturing,

44:18.480 --> 44:19.920
they sell them for so much money.

44:20.800 --> 44:23.360
All the delays, of course, that are imposed on the process,

44:23.360 --> 44:24.800
the only way they can pay for it,

44:24.800 --> 44:26.880
the manufacturers, is to take it out of your hide.

44:26.880 --> 44:28.160
What India did is they just said,

44:28.160 --> 44:29.520
we're not going to obey any of that stuff.

44:30.160 --> 44:33.120
So they have a whole massive generic drugs

44:33.120 --> 44:35.120
and biotech industry that arose

44:35.120 --> 44:36.400
because they built all the skills for that.

44:36.400 --> 44:38.720
That's why they could do their own vaccine during COVID

44:38.720 --> 44:41.840
and they're one of the biggest biotech industries in the world

44:41.840 --> 44:45.280
because they said screw Western restrictive IPs

44:45.280 --> 44:46.560
and other stuff, right?

44:46.560 --> 44:49.120
So I was actually talking with the founder of Flipkart,

44:49.120 --> 44:50.640
that's India's largest exit.

44:50.640 --> 44:52.400
And we were talking about this a few months ago,

44:53.040 --> 44:56.560
and what we want is for India and other countries like it,

44:56.560 --> 44:58.960
do something similar, not just generic drugs,

44:58.960 --> 45:04.720
but generic AI, meaning just let people train on Hollywood movies,

45:04.720 --> 45:06.880
let them train on full songs,

45:06.880 --> 45:08.560
let them train on every book,

45:09.120 --> 45:11.200
let them train on anything.

45:11.200 --> 45:14.240
And you know what, sue them in India, right?

45:14.240 --> 45:15.760
And have the servers in India

45:15.760 --> 45:18.080
and let people also train models in India

45:18.720 --> 45:22.560
because that's something that can build up a domestic industry

45:22.560 --> 45:24.240
with skills that the rest of the world,

45:24.960 --> 45:26.240
people will want the model output,

45:26.240 --> 45:28.400
they'll want to use the software service there,

45:28.400 --> 45:29.920
and they'll be fighting in court on the back end.

45:29.920 --> 45:32.640
This is similar to how all of the record companies

45:32.640 --> 45:35.520
fought Napster and Kazaa and so on,

45:35.520 --> 45:36.880
but they couldn't take down Spotify.

45:36.880 --> 45:38.320
Do you know that story? Do you remember that?

45:38.320 --> 45:42.800
Basically, because Spotify was legitimately a European company

45:42.800 --> 45:46.000
and that a combination of execution and negotiation,

45:46.720 --> 45:47.840
they couldn't take them down.

45:47.840 --> 45:50.640
They did take down Napster, they took down Limewire,

45:50.640 --> 45:52.080
they took down Groove Shark,

45:52.080 --> 45:53.920
and Kazaa had Estonians,

45:53.920 --> 45:55.200
I don't know exactly how it was incorporated,

45:55.200 --> 45:56.960
but it was probably two U.S. proximal,

45:56.960 --> 45:58.480
and that's where they were able to get them.

45:58.480 --> 46:00.240
But Spotify was far enough away

46:00.240 --> 46:01.680
that they couldn't just sue them

46:01.680 --> 46:03.920
and they actually genuinely had European traction.

46:03.920 --> 46:05.920
That's why the RA had to negotiate.

46:06.000 --> 46:09.040
So being far away from San Francisco

46:09.680 --> 46:12.000
may also be an advantage in AI,

46:12.000 --> 46:14.240
because it means you're far away from the blue city

46:14.240 --> 46:16.080
in the blue state in the Union.

46:16.080 --> 46:17.760
This relates to another really important point.

46:18.640 --> 46:20.800
When you actually think about deploying AI,

46:20.800 --> 46:22.000
there's those jobs you can disrupt

46:22.000 --> 46:23.680
that are not regulated jobs.

46:23.680 --> 46:27.440
Like, obviously, programmers are not,

46:28.320 --> 46:30.880
thank God, you don't need a license to be a programmer,

46:30.880 --> 46:33.520
but programmers adopt this kind of stuff naturally.

46:33.520 --> 46:35.520
So get up, co-pilot, replete.

46:35.520 --> 46:36.720
We just, boom, use it,

46:36.720 --> 46:38.160
and now it's amplified intelligence.

46:39.280 --> 46:40.480
But a lot of other jobs,

46:41.120 --> 46:42.240
there's some that are unionized

46:42.240 --> 46:43.840
and then some that are licensed.

46:43.840 --> 46:46.720
So Hollywood screenwriters are complaining.

46:46.720 --> 46:47.840
Journalists are complaining.

46:47.840 --> 46:49.040
Artists are complaining.

46:49.040 --> 46:51.280
This is a good chunk of Blue America.

46:51.280 --> 46:53.440
If you add in licensed jobs,

46:53.440 --> 46:56.480
like lawyers and doctors and bureaucrats,

46:58.400 --> 46:59.440
especially lawyers and doctors

46:59.440 --> 47:00.480
are very politically powerful,

47:00.480 --> 47:01.360
MDs and JDs.

47:01.360 --> 47:03.520
They have strong lobbying organizations,

47:03.600 --> 47:05.760
AMA and APN and so on.

47:07.280 --> 47:09.200
Basically, AI is part

47:09.200 --> 47:11.760
of the economic apocalypse for Blue America.

47:13.440 --> 47:17.920
It just attacks these overpriced jobs.

47:17.920 --> 47:19.520
And they say overpriced relative to

47:20.320 --> 47:22.320
what an Indian could do with an Android phone,

47:22.320 --> 47:24.320
what a South American could do with an Android phone,

47:24.320 --> 47:25.600
what someone in the Middle East

47:25.600 --> 47:27.520
or the Midwest could do with an Android phone.

47:28.240 --> 47:31.920
Now, those folks have been armed

47:32.480 --> 47:33.440
with generative AI.

47:34.000 --> 47:35.200
They can do way more.

47:35.840 --> 47:36.800
They're ready to work.

47:36.800 --> 47:38.400
They're ready to work for much less money.

47:38.400 --> 47:42.000
And they're a massive threat to Blue America.

47:42.000 --> 47:43.280
Blue America is now feeling

47:43.280 --> 47:45.120
like the blue collars of 10 or 20 years ago,

47:45.680 --> 47:48.720
where the blue collars had their jobs,

47:48.720 --> 47:50.640
going to China and other places,

47:50.640 --> 47:51.600
and they were mad about that.

47:51.600 --> 47:53.200
Factories got shut down and so on.

47:53.200 --> 47:54.880
That's about to happen to Blue America,

47:54.880 --> 47:55.840
already happening.

47:56.800 --> 47:58.480
And so that's going to mean

47:58.480 --> 48:01.680
a political backlash by Blue America of protectionism.

48:02.160 --> 48:03.760
Again, already happening.

48:03.760 --> 48:06.080
And the AI safety stuff,

48:06.800 --> 48:07.920
that's a whole separate thing,

48:07.920 --> 48:09.040
but it's going to be used.

48:09.680 --> 48:10.640
I'm going to use a phrase,

48:11.200 --> 48:12.960
and I hope you won't be offended by this.

48:12.960 --> 48:14.160
Have you heard the phrase,

48:14.160 --> 48:16.720
useful idiots, like by Lenin or whatever?

48:16.720 --> 48:17.280
Okay.

48:17.280 --> 48:18.480
It basically means like,

48:18.480 --> 48:21.120
okay, those guys, they're useful idiots

48:21.120 --> 48:22.320
for communism and so on.

48:22.320 --> 48:24.720
So let me put it like naive people

48:25.440 --> 48:28.800
who think that the US government

48:28.800 --> 48:30.080
is interested in AI safety

48:30.720 --> 48:32.880
are trying to give a lot of power to the US government.

48:33.440 --> 48:34.080
And the reason is,

48:34.080 --> 48:36.000
they haven't actually thought through from first principles

48:36.000 --> 48:37.920
what is the most powerful action in the world to connect them.

48:37.920 --> 48:39.040
They're trying to give power to the US government

48:39.040 --> 48:40.080
to regulate AI safety.

48:40.560 --> 48:42.800
But the government doesn't care about safety of anything.

48:42.800 --> 48:45.760
They literally funded the COVID virus

48:46.400 --> 48:47.360
in Wuhan,

48:47.360 --> 48:48.640
credibly alleged, right?

48:48.640 --> 48:50.160
There's at least,

48:50.160 --> 48:53.120
it is a reasonable hypothesis based on a lot of the data.

48:53.120 --> 48:54.880
Matt Ridley wrote a whole book on this.

48:54.880 --> 48:56.480
There's a lot of data that indicates,

48:56.480 --> 48:57.440
a lot of scientists believe it.

48:57.440 --> 48:59.760
I'm actually like a bioinformatics genomics guy,

48:59.760 --> 49:01.280
if you look at the sequences,

49:01.280 --> 49:02.960
there is a gap and a jump

49:02.960 --> 49:04.960
where it looks like this thing could have been engineered

49:04.960 --> 49:06.800
or partially engineered or evolved.

49:06.800 --> 49:08.480
There's Peter Dazak,

49:08.480 --> 49:09.760
there's Zengli Xi,

49:09.760 --> 49:11.200
there's actually a lot of evidence here.

49:11.200 --> 49:13.600
So the US government and the Chinese government

49:13.600 --> 49:15.600
are responsible for an existential risk.

49:16.480 --> 49:18.080
By studying it, they created it.

49:18.080 --> 49:18.880
Okay.

49:18.880 --> 49:21.840
They're responsible for risking nuclear war with Russia

49:21.840 --> 49:24.240
over this piece of land in eastern Ukraine,

49:24.240 --> 49:26.640
which probably is going to get wound down.

49:26.640 --> 49:27.280
Okay.

49:27.280 --> 49:28.960
So they don't care about your safety

49:30.240 --> 49:30.640
at all.

49:31.440 --> 49:32.160
They're not like,

49:32.160 --> 49:34.160
these are immediate things where we can show

49:34.160 --> 49:35.760
and there's nobody who's punished for this,

49:35.760 --> 49:36.880
nobody who's fired for this,

49:37.840 --> 49:41.680
literally rolling the dice on millions,

49:41.680 --> 49:42.960
hundreds of millions of people's lives

49:43.520 --> 49:44.560
has not been punished.

49:44.560 --> 49:46.000
In fact, it's like,

49:46.000 --> 49:47.120
it's not even talked about

49:47.760 --> 49:50.160
we're past the pandemic and these institutions

49:50.160 --> 49:50.880
can't be punished.

49:52.240 --> 49:54.800
So they don't care about AI safety.

49:54.800 --> 49:56.160
What they care about is AI control.

49:57.760 --> 49:59.520
And so the people in tech who are like,

49:59.600 --> 50:02.480
well, the government will guarantee AI safety.

50:02.480 --> 50:04.640
That's actually what we're going to actually get

50:04.640 --> 50:06.320
is something on the current path,

50:06.320 --> 50:08.160
like what happened with nuclear technology,

50:08.160 --> 50:10.320
where you got nuclear weapons,

50:10.320 --> 50:11.680
but not nuclear power,

50:11.680 --> 50:13.680
or at least not to the scale that we could have had it.

50:13.680 --> 50:16.320
We could have had much cheaper energy for everything.

50:16.320 --> 50:18.240
Instead, we got the militarization

50:18.240 --> 50:20.960
and the regulation and the deceleration,

50:20.960 --> 50:22.160
worst of all worlds,

50:22.160 --> 50:24.080
where you can blow people up,

50:24.080 --> 50:26.320
but you can't build nuclear power plants.

50:26.880 --> 50:29.920
And like even getting into nuclear technology,

50:29.920 --> 50:31.440
forget about just nuclear power plants.

50:31.440 --> 50:32.640
We don't have nuclear submarines.

50:32.640 --> 50:34.000
We don't have nuclear planes,

50:34.000 --> 50:34.800
all that kind of stuff.

50:34.800 --> 50:36.000
I don't know if nuclear planes are possible,

50:36.000 --> 50:37.520
but I do know nuclear submarines are possible.

50:37.520 --> 50:39.840
You can do a lot more cruise ships,

50:39.840 --> 50:40.720
a lot more stuff like that.

50:40.720 --> 50:42.000
You could probably have nuclear trains.

50:42.560 --> 50:44.240
You have to look at exactly how big those are.

50:45.280 --> 50:46.960
I don't know exactly how big those engines are

50:46.960 --> 50:47.440
and what the spies,

50:47.440 --> 50:48.800
but I wouldn't be surprised if you could.

50:49.680 --> 50:50.480
We don't have that.

50:50.480 --> 50:51.360
Why don't we have that?

50:51.360 --> 50:54.800
Because we had the wrong fear driven regulation

50:54.800 --> 50:55.600
in the early 70s.

50:56.400 --> 50:57.360
Putting it all together,

50:58.960 --> 51:01.520
I think that the current AI safety stuff

51:01.520 --> 51:03.040
is similar to nuclear safety stuff,

51:04.160 --> 51:06.160
that the US government has a terrible track record

51:06.160 --> 51:07.520
on safety in general.

51:07.520 --> 51:08.560
It doesn't care about it.

51:08.560 --> 51:10.400
It funded the COVID virus,

51:10.400 --> 51:11.680
incredibly alleged.

51:11.680 --> 51:14.880
It definitely risked nuclear war with Russia recently.

51:14.880 --> 51:16.480
Hot war with Russia was the red line

51:16.480 --> 51:17.600
we were not supposed to cross,

51:17.600 --> 51:19.360
and we're now like way into that.

51:19.920 --> 51:21.120
So it doesn't care about AI safety.

51:21.120 --> 51:22.240
It doesn't care about your safety.

51:23.200 --> 51:25.280
And it's also not even good at regulating.

51:26.080 --> 51:28.480
And so what it cares about is control,

51:28.480 --> 51:30.800
and we are going to have potentially a bad outcome

51:30.800 --> 51:33.040
where Silicon Valley and San Francisco

51:33.040 --> 51:34.560
is the Xerox Park of AI.

51:36.000 --> 51:37.040
Maybe that's too strong,

51:37.040 --> 51:40.080
okay, but basically it develops it,

51:40.080 --> 51:42.160
and there's a lot of things it can't do

51:42.160 --> 51:44.160
because it lobbied for this regulation

51:44.160 --> 51:46.160
that is going to come back and choke it.

51:46.160 --> 51:49.040
And then the other two spheres will push ahead

51:49.040 --> 51:50.960
because it's not about the technology.

51:50.960 --> 51:52.640
It's also about the political layer.

51:52.640 --> 51:53.760
You know the Steve Jobs saying,

51:54.640 --> 51:56.560
actually Alan Kay by way of Steve Jobs,

51:56.560 --> 51:59.040
if you're really serious about software,

51:59.040 --> 52:00.400
you need your own hardware, right?

52:00.960 --> 52:03.040
So if you're really serious about technology,

52:03.040 --> 52:04.240
you need your own sovereignty.

52:05.920 --> 52:09.040
Because like what the AI people haven't thought about is

52:09.040 --> 52:11.120
there's a platform beneath you,

52:11.120 --> 52:14.560
which is not just compute, it is regulate.

52:14.560 --> 52:16.400
It's a law, okay?

52:16.400 --> 52:18.560
And if the law doesn't allow you to compute

52:18.560 --> 52:20.160
so much for all of your stuff above that.

52:20.880 --> 52:22.320
And I know you're saying,

52:22.320 --> 52:24.880
oh, it's only a 10 to the 26 compute band

52:24.880 --> 52:25.680
and so on and so forth.

52:25.680 --> 52:27.520
Have you seen the first IRS tax form?

52:28.080 --> 52:31.760
It's always, always super simple.

52:31.760 --> 52:33.760
It's only the super, super, super rich

52:33.760 --> 52:35.280
who's we're going to get in at first.

52:35.280 --> 52:36.240
Doesn't matter to you.

52:36.240 --> 52:39.120
So that's called boiling the frog slowly.

52:39.120 --> 52:40.560
There's a million, you know, slippery slope.

52:40.560 --> 52:41.920
Slippery slope isn't a fallacy.

52:41.920 --> 52:43.600
It's literally how things work, right?

52:44.160 --> 52:47.280
Apple, one of the reasons they talk about

52:47.280 --> 52:48.720
not setting a precedent.

52:48.720 --> 52:51.600
Zuck starts a very hard line on setting precedents

52:51.600 --> 52:53.760
because he understands the long-term equivalent

52:53.760 --> 52:54.960
of setting a precedent, right?

52:55.600 --> 52:56.800
The precedent setting is that

52:56.800 --> 52:58.640
they're setting up a software FDA.

52:58.640 --> 53:01.360
And DC is so energized on this

53:01.360 --> 53:04.080
because they know how much social media disrupted them.

53:04.080 --> 53:06.560
That's why they're on the attack on crypto and AI.

53:06.560 --> 53:08.640
That's why they're on the attack on self-driving cars.

53:08.640 --> 53:11.760
They want to freeze the current social order in amber

53:11.760 --> 53:13.280
domestically and globally.

53:13.280 --> 53:15.200
So they think they can sanction China

53:15.200 --> 53:16.800
and stop it from developing chips.

53:16.800 --> 53:19.600
They think they can impose regulations on the U.S.

53:19.600 --> 53:21.280
and stop it from developing AI.

53:21.280 --> 53:22.400
But they can't.

53:22.400 --> 53:25.680
And also, by the way, they're totally schizophrenic on this

53:25.680 --> 53:27.440
where when they're talking about China,

53:27.440 --> 53:29.120
they're like, we're going to stop their chips

53:29.120 --> 53:31.120
to make sure America is a global leader.

53:31.120 --> 53:33.120
This is this Gina Raimondo who's saying this.

53:33.120 --> 53:34.960
And then domestically, they're like,

53:34.960 --> 53:37.760
we're going to regulate you so you stop accelerating AI.

53:37.760 --> 53:39.200
We're not about AI acceleration.

53:39.200 --> 53:41.360
EAC is weird over there, okay?

53:41.360 --> 53:42.880
So think about how schizophrenic that is.

53:43.600 --> 53:45.440
Okay, you're going to be far ahead of China.

53:45.440 --> 53:47.760
We're also going to be make sure to control the U.S.

53:47.760 --> 53:49.840
So they want to try and slow.

53:49.840 --> 53:51.440
What they actually want is to freeze the current system

53:51.440 --> 53:54.720
in amber, try to go back to pre-2007

53:54.720 --> 53:56.400
before all these tech guys disrupted everything.

53:56.960 --> 53:58.080
But that's not what's going to happen.

53:58.960 --> 54:00.560
So, but they're going to try to do it.

54:00.560 --> 54:03.440
And so everybody who's still loyal to the DC sphere,

54:04.000 --> 54:06.080
which includes an enormous chunk of AI people.

54:06.880 --> 54:08.560
And because they're all in,

54:08.560 --> 54:10.560
a lot of them are in San Francisco, right?

54:10.560 --> 54:15.040
And the political chaos of the last few years

54:15.600 --> 54:18.880
was not sufficient for them to relocate yet.

54:20.000 --> 54:20.560
Not all of them.

54:20.560 --> 54:22.240
I mean, Elon is in Texas.

54:22.240 --> 54:25.200
And it may turn out that Grock, for example,

54:25.200 --> 54:26.800
and what they're doing there, because he's a very legit,

54:26.800 --> 54:29.600
I mean, he's Elon, so he's capable of doing a lot.

54:29.600 --> 54:30.800
He was very early on OpenAI.

54:30.800 --> 54:33.200
He understands, right?

54:33.200 --> 54:36.080
It may turn out that Grock becomes red AI,

54:36.640 --> 54:38.000
or the community around that.

54:39.040 --> 54:41.200
And OpenAI in DeepMind are still blue AI.

54:41.200 --> 54:43.200
And we have Chinese AI and we're going to have decentralized AI.

54:43.200 --> 54:43.840
Okay, let me pause there.

54:43.840 --> 54:45.280
I know there's a big download.

54:45.280 --> 54:47.040
Well, for starters, I would say,

54:47.920 --> 54:54.160
broadly, I have a pretty similar intellectual tendency as you.

54:54.160 --> 54:56.880
I would broadly describe myself as a techno-optimist

54:56.880 --> 54:59.120
libertarian just about every issue.

54:59.920 --> 55:05.440
And I think your analysis of the dynamics is super interesting.

55:05.440 --> 55:07.360
And I think a lot of it sounds pretty plausible,

55:07.360 --> 55:09.360
although I'll kind of float a couple of things

55:09.360 --> 55:12.320
that I think may be bucking the trend.

55:12.320 --> 55:15.040
But I think it's maybe useful to kind of try to separate this

55:15.040 --> 55:19.760
into scenarios, because all the analysis that you're describing

55:20.960 --> 55:23.120
here seem, if I understand it correctly,

55:23.120 --> 55:29.760
it seems to have the implicit assumption that the AI itself

55:29.760 --> 55:32.720
is not going to get super powerful or hard to control.

55:33.680 --> 55:38.000
It's like, if we assume that it's kind of a normal technology,

55:38.800 --> 55:40.480
then you're off to the races on this analysis,

55:40.480 --> 55:42.240
and then we can get into the fine points.

55:42.240 --> 55:45.600
But I do want to take at least one moment and say,

55:46.400 --> 55:47.840
how confident are you on that?

55:47.840 --> 55:51.840
Because if it's a totally different kind of technology

55:51.840 --> 55:53.600
from other technologies that we've seen,

55:54.720 --> 55:58.720
you raise the gain of function research example.

55:58.720 --> 56:02.480
If it's that sort of technology that has these sort of

56:03.280 --> 56:09.360
non-local possible impacts or self-reinforcing kind of dynamics,

56:09.360 --> 56:13.600
which need not be like an Eliezer-style snap of the fingers fume,

56:13.600 --> 56:18.480
but even over, say, a decade, let's imagine that over the next 10 years,

56:18.480 --> 56:22.080
that AI's kind of multiple architectures develop,

56:22.080 --> 56:23.280
and they sort of get integrated,

56:23.280 --> 56:26.000
and we have something that kind of looks like robust,

56:26.000 --> 56:28.960
silicon-based intelligence, maybe not totally robust,

56:28.960 --> 56:32.400
but as robust or more robust than us, and running faster,

56:32.400 --> 56:36.720
and the kind of thing that can do lots of full jobs,

56:36.720 --> 56:38.480
or maybe even be tech CEOs,

56:39.040 --> 56:42.000
then it kind of feels like a lot of this analysis

56:42.720 --> 56:47.600
probably doesn't hold, because we're just in a totally different regime

56:47.600 --> 56:51.200
that is just extremely hard to predict.

56:51.920 --> 56:54.480
And I guess I wonder, first of all, do you agree with that?

56:55.440 --> 56:57.840
There seems to be a big fork in the road there that's like,

56:57.840 --> 57:03.120
just how fast and how powerful do these AI's become super powerful,

57:03.120 --> 57:03.760
or do they not?

57:03.760 --> 57:04.960
And if they don't, then yeah,

57:04.960 --> 57:07.440
I think we're much more into real politic type of analysis.

57:07.440 --> 57:09.680
But I'm not at all confident in that.

57:09.680 --> 57:11.840
To me, it feels like there's a very real chance

57:12.480 --> 57:15.520
that AI of 10 years from now is...

57:16.160 --> 57:18.320
And by the way, this is like what the leaders are saying, right?

57:18.320 --> 57:21.520
I mean, open AI is saying this, Anthropic is saying this,

57:22.160 --> 57:26.400
Demis and Shane Legge are certainly saying things like this.

57:26.400 --> 57:30.080
It seems like they expect that we will have AI's

57:30.080 --> 57:33.200
that are more powerful than any individual human,

57:33.200 --> 57:36.480
and that becomes like the bigger question

57:37.360 --> 57:38.800
than anything else.

57:38.800 --> 57:44.480
So do you agree with that kind of division of scenarios, first of all?

57:44.480 --> 57:46.000
And then maybe you could kind of say like,

57:46.000 --> 57:47.600
how likely you think each one is.

57:47.600 --> 57:49.520
And obviously that one where it takes off

57:49.520 --> 57:51.120
is like super hard to analyze.

57:51.120 --> 57:53.440
And I also definitely think it is worth analyzing

57:53.440 --> 57:55.440
the scenario where it doesn't take off.

57:55.440 --> 57:59.680
But I just wanted to flag that it seems like there's a big...

57:59.680 --> 58:01.120
If you talk to the AI safety people,

58:02.000 --> 58:06.480
any world in which it's like we're suing Indian AI firms

58:06.480 --> 58:10.960
in Indian court over IP is like a normal world in their mind, right?

58:10.960 --> 58:12.960
And that's not the kind of world that they're most worried about.

58:13.520 --> 58:17.440
I think that there have been some plausible sounding things

58:17.440 --> 58:18.640
that have been said,

58:18.640 --> 58:22.560
but I want to just kind of talk about a few technical counter-arguments,

58:23.280 --> 58:27.760
mathematical or physical, that constrain what is possible.

58:28.000 --> 58:32.560
And actually Martin Casado and Vijay and I are working on a long thing on this

58:32.560 --> 58:34.720
where Vijay did folding at home.

58:34.720 --> 58:35.600
He's a physicist.

58:35.600 --> 58:38.400
Martin sold in the Syrah for a billion dollars

58:38.400 --> 58:43.040
and knows a lot about how a Stuxnet-like thing could work at the systems level.

58:43.040 --> 58:44.960
And I've thought about it from other angles

58:44.960 --> 58:48.640
and some of the math stuff that I'll get to.

58:48.640 --> 58:51.200
So for example, one thing...

58:51.200 --> 58:53.440
And I'm going to give a bunch of different technical arguments

58:53.440 --> 58:55.200
and then let's kind of combine them.

58:56.160 --> 58:59.680
One thing that's being talked about is if you have a superintelligence,

58:59.680 --> 59:02.800
it can double it for a million years

59:02.800 --> 59:04.080
and then it can make one move

59:04.080 --> 59:06.880
and it's going to outthink you all the time and so on and so forth.

59:07.920 --> 59:12.160
Well, if you're familiar with the math of chaos or the math of turbulence,

59:12.800 --> 59:17.040
there are limits to even very simple systems that you can set up

59:17.040 --> 59:21.120
where they can become very unpredictable quite quickly.

59:21.440 --> 59:25.520
Okay. And so you can, if you want to, engineer a system

59:25.520 --> 59:29.600
where you have very rapid diversions of predictability

59:29.600 --> 59:32.560
so that, I don't know, it's like the heat depth of the universe

59:32.560 --> 59:34.720
before you can predict out in timestamps.

59:35.760 --> 59:36.800
Do you understand what I'm saying?

59:36.800 --> 59:37.680
Right?

59:37.680 --> 59:40.800
This is sort of akin to like a wolf from like simple...

59:40.800 --> 59:43.280
Even simple rules can generate patterns

59:43.280 --> 59:46.240
such that you can't know them without literally computing them.

59:46.800 --> 59:47.840
Yeah, exactly, right?

59:47.840 --> 59:50.640
So at least right now, with chaos and turbulence,

59:50.640 --> 59:56.400
you can get things that are extremely provably difficult to forecast

59:56.400 --> 59:58.800
without actually doing it, okay?

59:59.360 --> 01:00:00.800
You know, I can make that argument quantitative

01:00:00.800 --> 01:00:02.640
but that's just something to look at, right?

01:00:02.640 --> 01:00:05.200
It's almost like a Delta Epsilon challenge from calculus.

01:00:05.200 --> 01:00:07.680
Like, okay, how hard do you want me to make this to predict?

01:00:07.680 --> 01:00:10.800
Okay, I can set up a problem that is like that, right?

01:00:10.800 --> 01:00:12.880
It's basically extreme sensitivity to initial conditions

01:00:12.880 --> 01:00:15.200
lead to extreme divergence in outcomes.

01:00:16.160 --> 01:00:18.720
So you could design systems to be chaotic

01:00:18.720 --> 01:00:22.080
that might be AI immune because they can't be forecasted that well.

01:00:22.080 --> 01:00:23.520
You have to kind of react to them in real time.

01:00:24.240 --> 01:00:26.240
The ultimate version of this is not even a chaotic system.

01:00:26.240 --> 01:00:30.000
It's a cryptographic system where I've got a whole slide deck on this,

01:00:30.000 --> 01:00:33.600
how AI makes everything fake, easy to fake.

01:00:33.600 --> 01:00:37.440
Crypto makes it hard to fake again, right?

01:00:37.440 --> 01:00:40.160
Because crypto in the broader sense of cryptography,

01:00:40.160 --> 01:00:41.520
but also in the narrower sense,

01:00:41.520 --> 01:00:46.720
I think crypto is cryptography as the internet is to computer science.

01:00:46.800 --> 01:00:49.360
It's like the primary place where all this stuff is applied,

01:00:49.360 --> 01:00:51.360
but obviously it's not the equivalent, okay?

01:00:51.360 --> 01:00:54.400
And AI can fake an image, but it can't fake a digital signature

01:00:54.400 --> 01:00:57.200
unless it can break certain math, you know,

01:00:57.200 --> 01:00:59.040
and so it's sort of like a, you know,

01:00:59.040 --> 01:01:00.880
solve factors each problem or something like that.

01:01:00.880 --> 01:01:03.920
So cryptography is another mathematical thing that constrains AI.

01:01:03.920 --> 01:01:05.680
Similar to chaos and turbulence,

01:01:05.680 --> 01:01:10.560
it constrains how much an AI can infer things.

01:01:10.560 --> 01:01:13.120
You can't statistically infer it, okay?

01:01:13.120 --> 01:01:15.920
You need to actually have the private key to solve that equation.

01:01:15.920 --> 01:01:17.120
So that is another math.

01:01:17.120 --> 01:01:19.440
So I'm going to rules of math, right?

01:01:20.160 --> 01:01:22.800
Math is very powerful because you can make proofs

01:01:22.800 --> 01:01:26.160
that will work no matter what devices we come up with, okay?

01:01:26.160 --> 01:01:28.080
You start to put an AI in a cage.

01:01:28.080 --> 01:01:29.520
It can't predict beyond a certain amount

01:01:29.520 --> 01:01:31.760
because of chaos and turbulence, math.

01:01:31.760 --> 01:01:35.600
It cannot solve certain equations unless it has a private key

01:01:35.600 --> 01:01:38.480
is because of what we know about cryptography, math, okay?

01:01:39.040 --> 01:01:41.280
Again, if somebody proves P equals NP,

01:01:41.280 --> 01:01:42.480
some of this stuff breaks down,

01:01:42.480 --> 01:01:44.800
but this is within the bounds of our mathematical knowledge right now.

01:01:45.360 --> 01:01:48.240
Physics wise, physical friction exists.

01:01:49.200 --> 01:01:50.800
A lot of physical friction exists.

01:01:51.600 --> 01:01:58.880
And a huge amount of the writing on AI assumes by guys like Elias

01:01:58.880 --> 01:02:01.120
or who I like, I don't dislike it, you know,

01:02:01.120 --> 01:02:04.320
but it is extremely,

01:02:04.320 --> 01:02:07.280
there's two things that really stick out to me about it.

01:02:07.280 --> 01:02:09.840
First is extremely theoretical and not empirical.

01:02:10.720 --> 01:02:13.920
And second, extremely Abrahamic rather than Darmic or Sinic.

01:02:15.040 --> 01:02:17.120
Okay, why theoretical and not empirical?

01:02:17.760 --> 01:02:22.000
It's not trivial to turn something from the computer

01:02:22.000 --> 01:02:24.080
into a real world thing, okay?

01:02:24.080 --> 01:02:27.760
One of the biggest gaps in all of this thinking

01:02:27.760 --> 01:02:29.920
is what are the sensors in actuators?

01:02:30.800 --> 01:02:33.600
Okay, because like if you actually build, you know,

01:02:33.600 --> 01:02:36.560
I've built in industrial robot systems that, you know,

01:02:36.560 --> 01:02:40.080
10 years ago, you know, a genome sequencing lab with robots,

01:02:40.080 --> 01:02:41.120
that's hard.

01:02:41.120 --> 01:02:43.200
That's physical friction, okay?

01:02:43.200 --> 01:02:48.720
And a lot of the AI scenarios seem to basically say,

01:02:48.720 --> 01:02:51.280
oh, it's going to be a self-programming Stuxnet

01:02:51.280 --> 01:02:54.160
that's going to escape and live off the land

01:02:54.160 --> 01:02:57.440
and hypnotize people into doing things, okay?

01:02:58.080 --> 01:03:00.960
Now, each of those is actually really, really difficult steps.

01:03:01.520 --> 01:03:04.000
First is self-programming Stuxnet.

01:03:04.000 --> 01:03:07.280
Like this would have to be a computer virus

01:03:07.280 --> 01:03:10.640
that can live on any device, despite the fact

01:03:10.640 --> 01:03:13.200
that Apple or Google can push a software update

01:03:13.200 --> 01:03:15.600
to a billion devices, right?

01:03:15.600 --> 01:03:18.880
A few executives coordinating almost certainly can,

01:03:18.880 --> 01:03:21.760
I mean, the off switch exists, right?

01:03:21.760 --> 01:03:24.080
Like this is actually like the core thing.

01:03:24.080 --> 01:03:27.600
Lots of AI safety guys get themselves into the mindset

01:03:27.600 --> 01:03:29.520
that the off switch doesn't exist.

01:03:30.080 --> 01:03:30.640
But guess what?

01:03:30.640 --> 01:03:32.800
There's almost nothing living that we haven't been able to kill,

01:03:34.160 --> 01:03:34.640
right?

01:03:34.640 --> 01:03:36.080
Like can we kill it?

01:03:36.080 --> 01:03:38.800
This thing exists and this is getting back to living off the land.

01:03:39.680 --> 01:03:41.680
Even if you had like something that could solve

01:03:41.680 --> 01:03:43.280
some other technical problems that it'll get to,

01:03:44.560 --> 01:03:47.120
it exists as an electromagnetic wave kind of thing

01:03:47.120 --> 01:03:50.560
on a certain, you know, on chips and so on and so forth.

01:03:50.560 --> 01:03:52.240
It's taking it out in the environment

01:03:52.240 --> 01:03:55.440
is like putting a really smart human into outer space, right?

01:03:55.440 --> 01:03:57.120
Your body just explodes and you die.

01:03:57.680 --> 01:03:58.960
It doesn't matter how smart you are.

01:03:59.600 --> 01:04:01.600
That strength on this axis,

01:04:01.600 --> 01:04:03.680
where you're weak on this axis and, you know,

01:04:03.680 --> 01:04:04.800
it's just strength on the X axis,

01:04:04.800 --> 01:04:06.720
not strength on the Y or the Z axis.

01:04:06.800 --> 01:04:08.880
An AI outside, you know, pour water on it.

01:04:08.880 --> 01:04:12.720
This is, you know, this is why I mean the 50 IQ, 150 IQ thing.

01:04:13.280 --> 01:04:15.040
You know, the 150 IQ way of saying it is,

01:04:15.040 --> 01:04:16.640
it's strong on this X and weak on this X.

01:04:16.640 --> 01:04:20.000
And the 50 IQ way is pour water on it, disconnect it,

01:04:20.000 --> 01:04:22.160
you know, turn the power off.

01:04:22.160 --> 01:04:23.040
Okay, right?

01:04:23.600 --> 01:04:26.560
Like it'll, it'll be very difficult to build a system

01:04:26.560 --> 01:04:28.480
where you literally cannot turn it off.

01:04:28.480 --> 01:04:31.440
The closest thing we have to that is actually not Stuxnet.

01:04:31.440 --> 01:04:32.160
It's Bitcoin.

01:04:32.800 --> 01:04:37.920
And Bitcoin only exists because millions of humans keep it going.

01:04:39.200 --> 01:04:41.600
So you, you need, so that gets the second point,

01:04:41.600 --> 01:04:44.640
living off the land for an AI to live off the land,

01:04:45.440 --> 01:04:47.200
meaning without human cooperation.

01:04:48.080 --> 01:04:50.480
Okay, that's the next Turing threshold.

01:04:50.480 --> 01:04:52.880
An AI to live without human cooperation.

01:04:52.880 --> 01:04:59.680
It would need to be able to control robots sufficient to dig or out of the ground,

01:04:59.680 --> 01:05:02.960
set up data centers and generators and connect them

01:05:02.960 --> 01:05:06.720
and defend that against human attack, literally a terminator scenario.

01:05:06.720 --> 01:05:09.120
Okay, that's a big leap in terms of, I mean,

01:05:09.120 --> 01:05:10.320
is it completely impossible?

01:05:10.320 --> 01:05:13.040
I can't say it's completely impossible, but it's not happening tomorrow.

01:05:13.600 --> 01:05:15.440
No matter what your AI timelines are,

01:05:15.440 --> 01:05:19.440
you would need to have like a billion or hundreds of millions

01:05:19.440 --> 01:05:25.600
of internet connected autonomous robots that this Stuxnet AI could hijack.

01:05:25.600 --> 01:05:31.440
They were sufficient to carve or out of the earth and set up data centers

01:05:31.440 --> 01:05:33.200
and make the AI duplicate.

01:05:33.200 --> 01:05:34.240
We're not there.

01:05:34.240 --> 01:05:36.240
That's a huge amount of physical friction.

01:05:36.240 --> 01:05:39.440
That's AI operating without a human to make itself propagate, right?

01:05:39.440 --> 01:05:43.760
A human doesn't need the cooperation of a lizard to self-replicate.

01:05:44.560 --> 01:05:46.240
For an AI to replicate right now,

01:05:46.240 --> 01:05:50.000
it would need the cooperation of a human in some sense,

01:05:50.000 --> 01:05:52.080
because otherwise those humans can kill it

01:05:52.080 --> 01:05:56.160
because there's not that many different pieces of operating systems around the world.

01:05:56.160 --> 01:05:59.200
I'm just talking about the practical constraints of our current world, right?

01:05:59.200 --> 01:06:04.960
Actually existing reality, not AI safety guys reality where all these things don't exist.

01:06:04.960 --> 01:06:07.360
There's just a few operating systems, just a few countries.

01:06:07.360 --> 01:06:11.760
If everybody is going with torches and search lights through the internet,

01:06:11.760 --> 01:06:13.680
it's very hard for a virus to continue.

01:06:14.880 --> 01:06:21.760
So A, on the practicalities, there's the technical stuff with chaos and turbulence

01:06:21.760 --> 01:06:25.600
and with cryptography itself where AI can't predict and it can't solve certain equations.

01:06:26.400 --> 01:06:30.240
B, on the physical difficulties, it probably...

01:06:30.240 --> 01:06:31.920
I mean, like to be a Stuxnet,

01:06:31.920 --> 01:06:33.600
Microsoft and Google and so on could kill it.

01:06:33.600 --> 01:06:34.960
The off switch exists.

01:06:34.960 --> 01:06:36.160
Can it live off the land?

01:06:36.160 --> 01:06:40.640
No, it cannot because it doesn't have drones to mine or stuff out of the ground.

01:06:44.400 --> 01:06:46.560
Can it exist without humans?

01:06:46.560 --> 01:06:48.400
Can it be this hypnotizing thing?

01:06:48.400 --> 01:06:50.400
Okay, so the hypnotizing thing, by the way,

01:06:50.400 --> 01:06:53.680
this is one of the things that's the most hilarious self-fulfilling prophecy in my view.

01:06:55.520 --> 01:06:58.000
And no offense to anybody who's listening to this podcast,

01:06:58.000 --> 01:07:02.240
but I think the absolutely dumbest kind of tweet that I've seen on AI is,

01:07:02.800 --> 01:07:05.440
I typed this in and, oh my God, it told me this.

01:07:06.160 --> 01:07:10.640
Like, I asked it how to make sarin gas and it told me X or whatever, right?

01:07:10.640 --> 01:07:12.800
That's just a search engine, okay?

01:07:14.000 --> 01:07:17.520
What basically a lot of these people are doing is they're saying,

01:07:17.600 --> 01:07:20.720
what if there were people out there that were so impressionable

01:07:21.360 --> 01:07:27.040
that they would type things into an AI and follow it as if they were hearing voices?

01:07:27.040 --> 01:07:30.960
And that's actually not the model or whatever that's doing it.

01:07:30.960 --> 01:07:33.760
That's like this AI cult that has evolved around the world,

01:07:33.760 --> 01:07:37.680
like a Aum Shinrikyo that hears voices and does the sarin gas.

01:07:39.200 --> 01:07:42.560
The point is an AI can't just hypnotize people.

01:07:42.560 --> 01:07:44.480
Those people have to participate in it.

01:07:44.480 --> 01:07:47.040
They're typing things into the machine or whatever, okay?

01:07:47.520 --> 01:07:50.240
Now, you might say, all right, let's project out a few years.

01:07:50.240 --> 01:07:54.960
In a few years, what you have is you have an AI that is not just text,

01:07:54.960 --> 01:07:56.880
but it appears as Jesus.

01:07:56.880 --> 01:07:58.720
What would AI Jesus do?

01:07:58.720 --> 01:08:00.240
What would AI Lee Kuan Yew do?

01:08:00.240 --> 01:08:01.680
What would AI George Washington do?

01:08:01.680 --> 01:08:03.680
So it appears as 3D, okay?

01:08:03.680 --> 01:08:04.800
So it's generating that.

01:08:05.360 --> 01:08:09.520
It speaks in your language and in a voice.

01:08:09.520 --> 01:08:12.560
It knows the history of your whole culture, okay?

01:08:12.560 --> 01:08:14.160
That would be very convincing.

01:08:14.160 --> 01:08:15.280
Absolutely be very convincing.

01:08:15.920 --> 01:08:18.000
But it still can't exist without human programmers

01:08:18.880 --> 01:08:21.120
who are like the priests tending this AI God,

01:08:21.120 --> 01:08:24.240
whether it's AI Jesus or AI Lee Kuan Yew or something like that.

01:08:24.240 --> 01:08:27.280
The thing about the hypnotization thing that I really want to poke on that,

01:08:27.280 --> 01:08:29.280
are you familiar with the concept of the principal agent problem?

01:08:29.840 --> 01:08:34.960
Basically, every time you've got like a CEO and a worker,

01:08:34.960 --> 01:08:37.840
or you have an LP and a VC,

01:08:37.840 --> 01:08:42.320
or you have an employer and a contractor,

01:08:42.320 --> 01:08:45.040
every edge there, there are four possibilities.

01:08:45.040 --> 01:08:47.040
In a 2x2 matrix.

01:08:47.040 --> 01:08:51.040
Win-win, win-lose, lose-win, lose-lose, okay?

01:08:52.240 --> 01:08:56.400
So for example, when somebody joins a tech startup,

01:08:56.400 --> 01:08:59.520
the CEO makes a lot of money and so does a worker, okay?

01:08:59.520 --> 01:09:00.640
That's win-win.

01:09:00.640 --> 01:09:02.560
Lose-lose is they both lose money.

01:09:02.560 --> 01:09:04.960
Win-lose is the CEO makes money and the employee doesn't.

01:09:05.600 --> 01:09:07.600
Lose-win is the company fails,

01:09:07.600 --> 01:09:09.600
but the employee got paid a very high salary.

01:09:09.600 --> 01:09:12.160
So what equity does is it aligns people.

01:09:12.160 --> 01:09:14.240
That's where the concept of alignment comes from.

01:09:14.240 --> 01:09:16.960
It aligns people to the upper left corner of win-win.

01:09:16.960 --> 01:09:19.520
That's when you have one CEO and one employee.

01:09:19.520 --> 01:09:22.640
When you have one CEO and two employees,

01:09:22.640 --> 01:09:24.480
you don't have two squared outcomes,

01:09:24.480 --> 01:09:25.920
you have two cubed outcomes,

01:09:25.920 --> 01:09:28.400
because you have win-win-win, win-win-lose,

01:09:28.400 --> 01:09:30.240
win-lose-lose, et cetera, right?

01:09:30.240 --> 01:09:32.720
Because all three people can be win or lose.

01:09:32.720 --> 01:09:35.520
Because CEO can be win or lose, employee can be win or lose,

01:09:35.520 --> 01:09:37.280
employee number two can be win or lose.

01:09:37.280 --> 01:09:39.040
If you have n people, rather than three people,

01:09:39.040 --> 01:09:40.320
you have two to the n possible outcomes

01:09:40.320 --> 01:09:43.440
and you have essentially a 2x2x2x2x2x2xn

01:09:43.440 --> 01:09:44.880
hyper-cube of possibilities.

01:09:44.880 --> 01:09:47.040
Okay, it's literally just two dimensions on each axis.

01:09:48.000 --> 01:09:50.880
There's tons of possible defecting kinds of things that happen there.

01:09:50.880 --> 01:09:52.400
So that's why in a large company,

01:09:52.400 --> 01:09:54.800
there's lose-win coalitions that happen,

01:09:54.800 --> 01:09:57.040
where m people gang up on the other k people

01:09:57.040 --> 01:09:58.320
and they win what the other people lose.

01:09:58.320 --> 01:09:59.920
That's how politics happens.

01:09:59.920 --> 01:10:02.160
When you've got a startup that's driven by equity

01:10:02.160 --> 01:10:03.440
and the biggest payoff,

01:10:03.440 --> 01:10:04.480
people don't have to try to think,

01:10:04.480 --> 01:10:06.240
okay, well, I make more money by politics,

01:10:06.240 --> 01:10:08.240
we'll make more money by the win-win-win-win-win column

01:10:08.240 --> 01:10:10.320
because the exit makes everybody make the most money.

01:10:10.320 --> 01:10:12.160
That's actually how the open AI people

01:10:12.160 --> 01:10:13.440
were able to coordinate around,

01:10:13.440 --> 01:10:15.200
we want an $80 billion company,

01:10:15.200 --> 01:10:16.960
the economics help find the sell

01:10:16.960 --> 01:10:18.560
that was actually the most beneficial to all of them,

01:10:18.560 --> 01:10:19.840
help them coordinate, okay?

01:10:19.840 --> 01:10:21.520
So you search that hyper-cube, okay.

01:10:21.520 --> 01:10:24.160
That's a point of equity as lining.

01:10:24.160 --> 01:10:26.320
Still, despite all of this,

01:10:27.040 --> 01:10:29.440
that's one of our best mechanisms for coordinating

01:10:29.440 --> 01:10:32.400
large numbers of people in the principal agent problem.

01:10:32.400 --> 01:10:35.600
Despite all of this, the possibility exists

01:10:36.400 --> 01:10:39.600
for any of these people to win while the others lose,

01:10:39.600 --> 01:10:40.480
right, with me so far?

01:10:40.480 --> 01:10:41.840
And I'll explain why this is important.

01:10:41.920 --> 01:10:45.520
What that means is those 1,000 employees of the CEO

01:10:46.160 --> 01:10:49.840
are their own agents with their own payoff functions

01:10:49.840 --> 01:10:52.560
that are not perfectly aligned with the CEO's payoff function.

01:10:53.120 --> 01:10:55.120
As such, there are scenarios

01:10:55.120 --> 01:10:58.240
under which they will defect and do other things, okay?

01:10:58.240 --> 01:11:02.560
The only way they become like actual limbs,

01:11:02.560 --> 01:11:06.960
see, my hand is not an agent of its own.

01:11:06.960 --> 01:11:08.160
It lives or dies with me.

01:11:08.720 --> 01:11:11.440
Therefore, it does exactly what I'm saying at this time.

01:11:11.440 --> 01:11:12.480
I tell it to go up, it goes up.

01:11:12.480 --> 01:11:13.760
I tell it to go down, it goes down.

01:11:13.760 --> 01:11:14.880
Sideway is sideways, right?

01:11:15.520 --> 01:11:17.360
An employee is not like that.

01:11:17.360 --> 01:11:19.840
They will do this and this and sideways, sideways,

01:11:19.840 --> 01:11:21.360
up to a certain point.

01:11:21.360 --> 01:11:23.520
And if you have them do something

01:11:23.520 --> 01:11:24.880
that's extremely against their interests,

01:11:24.880 --> 01:11:26.400
they will not do your action.

01:11:26.400 --> 01:11:27.680
Did you understand my point?

01:11:27.680 --> 01:11:31.120
Okay, that is the difference between an AI hypnotizing humans

01:11:31.760 --> 01:11:33.200
versus an AI controlling drones.

01:11:33.920 --> 01:11:36.160
AI controlling drones is like your hands.

01:11:36.160 --> 01:11:37.520
They're actually pieces of your body.

01:11:37.520 --> 01:11:38.800
There's no defecting.

01:11:38.800 --> 01:11:40.160
There's no loose wind.

01:11:40.160 --> 01:11:41.280
They have no mind of their own.

01:11:41.280 --> 01:11:43.120
They're literally taking instructions, okay?

01:11:43.120 --> 01:11:43.920
They have no payoff function.

01:11:43.920 --> 01:11:46.240
They will kill themselves for the hoard, right?

01:11:47.200 --> 01:11:49.440
An AI hypnotizing humans has a thousand

01:11:49.440 --> 01:11:51.920
principal Asian problems for every thousand humans.

01:11:52.560 --> 01:11:55.040
And it has to incentivize them to continue

01:11:55.040 --> 01:11:56.320
and has to generate huge payoffs.

01:11:56.320 --> 01:11:57.200
It's like an AI CEO.

01:11:57.200 --> 01:11:59.600
That's really hard to do, right?

01:11:59.600 --> 01:12:02.880
The history of evolution shows us how hard it is

01:12:02.880 --> 01:12:04.480
to coordinate multicellular organisms.

01:12:04.480 --> 01:12:06.320
You have to make them all live or die as one.

01:12:06.320 --> 01:12:08.160
Then you get something along these lines.

01:12:08.160 --> 01:12:10.240
Like an ant colony can coordinate like that

01:12:10.240 --> 01:12:13.280
because if the queen doesn't reproduce all the ants,

01:12:13.280 --> 01:12:15.280
it doesn't matter what they're having sort of genetic material.

01:12:15.280 --> 01:12:15.600
Okay?

01:12:16.160 --> 01:12:18.960
We are not currently set up for those humans

01:12:18.960 --> 01:12:21.360
to not be able to reproduce unless the AI reproduces.

01:12:22.720 --> 01:12:25.040
Do I think we eventually get to a configuration like that?

01:12:25.040 --> 01:12:25.360
Maybe.

01:12:26.640 --> 01:12:30.320
Where you have an AI brain is at the center of civilization

01:12:30.320 --> 01:12:32.400
and it's coordinating all the people around it.

01:12:32.400 --> 01:12:35.280
And every civilization that makes it

01:12:35.280 --> 01:12:37.520
is capable of crowdfunding and operating its own AI.

01:12:38.000 --> 01:12:40.800
That gets me to my other critique of the AI safety guys.

01:12:40.800 --> 01:12:42.080
I mentioned that the first critique

01:12:42.080 --> 01:12:43.920
is very theoretical rather than empirical.

01:12:43.920 --> 01:12:45.600
And the second critique is they're Abrahamic

01:12:45.600 --> 01:12:48.000
rather than Darmic or Sinic.

01:12:48.000 --> 01:12:48.320
Okay?

01:12:48.960 --> 01:12:51.600
And, you know, our background culture influences things

01:12:51.600 --> 01:12:53.280
in ways we don't even think about.

01:12:53.280 --> 01:12:55.520
So much of the paperclip thinking

01:12:55.520 --> 01:12:58.880
is like a vengeful God will turn you into pillars of salt,

01:12:58.880 --> 01:13:01.120
except it's a vengeful, you know, AI God

01:13:01.120 --> 01:13:02.800
will turn you into paperclips.

01:13:02.800 --> 01:13:03.040
Okay?

01:13:03.680 --> 01:13:06.320
The polytheistic model of many gods,

01:13:06.320 --> 01:13:07.520
as opposed to one God is,

01:13:07.520 --> 01:13:09.440
we're all going to have our own AI gods

01:13:09.440 --> 01:13:10.560
and there'll be war of the gods,

01:13:11.600 --> 01:13:13.040
like Zeus and Hera and so on.

01:13:13.040 --> 01:13:14.560
That's the closest Western version,

01:13:14.560 --> 01:13:16.400
you know, the paganism that predated,

01:13:16.400 --> 01:13:17.760
you know, Abrahamic religions.

01:13:17.760 --> 01:13:19.120
But that's still there in India.

01:13:19.120 --> 01:13:20.560
That's still how Indians think.

01:13:20.560 --> 01:13:21.920
That's why India is sort of,

01:13:21.920 --> 01:13:23.600
people have gotten so woke that they don't even make

01:13:23.600 --> 01:13:25.520
large scale cultural generalizations anymore.

01:13:25.520 --> 01:13:29.520
But it's true that India is just culturally more amenable

01:13:29.520 --> 01:13:32.480
to decentralization, to, you know,

01:13:32.480 --> 01:13:35.360
multiple gods rather than one God and one state.

01:13:35.360 --> 01:13:35.600
Okay?

01:13:36.880 --> 01:13:38.800
And then the Chinese model is yet the opposite.

01:13:38.800 --> 01:13:40.240
Like they have like, I mean, of course,

01:13:40.240 --> 01:13:41.920
they have their tech entrepreneurs and so on.

01:13:41.920 --> 01:13:43.840
But they're, if India is more decentralized,

01:13:43.840 --> 01:13:44.720
China is more centralized.

01:13:44.720 --> 01:13:46.800
They have like one government and one leader

01:13:46.800 --> 01:13:48.080
for the entire civilization.

01:13:48.080 --> 01:13:48.320
Okay?

01:13:49.200 --> 01:13:52.160
And that the biggest thing that China has done

01:13:52.160 --> 01:13:55.520
over the last 20 or 30 years is they've taken various,

01:13:55.520 --> 01:13:57.440
you know, U.S. things and they've made sure

01:13:57.440 --> 01:13:59.040
that they have their own Chinese version

01:13:59.040 --> 01:13:59.760
where they have root.

01:14:00.320 --> 01:14:01.760
So they take U.S. social media

01:14:01.760 --> 01:14:04.000
and they made sure they had root over Sina Weibo.

01:14:04.000 --> 01:14:05.440
Okay?

01:14:05.440 --> 01:14:06.960
They make sure they have their own Chinese version

01:14:06.960 --> 01:14:08.400
of electric cars, the most Chinese version.

01:14:08.400 --> 01:14:11.440
So the private keys, in a sense, are with G.

01:14:11.440 --> 01:14:14.720
So that means that they also, at a minimum,

01:14:14.720 --> 01:14:16.080
you combine these two things,

01:14:16.080 --> 01:14:18.560
you're at a minimum going to get polytheistic AI

01:14:18.560 --> 01:14:20.000
of the U.S. and Chinese varieties.

01:14:20.960 --> 01:14:23.120
And then you add the Indian version on it

01:14:23.120 --> 01:14:24.320
and you're going to get quite a few

01:14:24.320 --> 01:14:26.080
of these different AIs around there.

01:14:26.080 --> 01:14:27.360
And then you have War of the Gods

01:14:27.360 --> 01:14:30.880
where maybe they are good at coordinating humans

01:14:30.880 --> 01:14:34.000
who take instructions from them,

01:14:34.000 --> 01:14:35.440
but they can't live without the humans

01:14:36.080 --> 01:14:37.600
and the humans are giving input to them.

01:14:38.240 --> 01:14:39.120
That's a series of things.

01:14:39.120 --> 01:14:40.240
I could probably make that clearer

01:14:40.240 --> 01:14:42.560
if I just laid it out in bullets in an essay,

01:14:42.560 --> 01:14:45.120
but just to recap it, A, technical reasons

01:14:45.120 --> 01:14:48.640
like chaos, turbulence, cryptography,

01:14:48.640 --> 01:14:51.440
why AI is limited in its ability to predict time frames

01:14:51.440 --> 01:14:54.560
and to solve equations, B, practical limits.

01:14:54.560 --> 01:14:57.120
And AI cannot easily be a Stuxnet

01:14:57.120 --> 01:14:59.920
because Microsoft and Google and Apple

01:14:59.920 --> 01:15:02.560
can install software on a billion devices

01:15:02.560 --> 01:15:03.920
and just kill it, right?

01:15:03.920 --> 01:15:06.800
Like basically guys with torches come, all right?

01:15:06.800 --> 01:15:08.960
It can't easily live off the land without humans

01:15:08.960 --> 01:15:10.880
because they would need hundreds of millions

01:15:10.880 --> 01:15:12.720
of autonomous robots out there to control,

01:15:12.720 --> 01:15:15.040
to mine the ore and set the data centers.

01:15:15.600 --> 01:15:17.520
It can't just hypnotize humans

01:15:17.520 --> 01:15:19.120
like it can control drones

01:15:19.120 --> 01:15:20.640
because of the principal agent problem

01:15:20.640 --> 01:15:22.240
and the degree of human defection.

01:15:22.240 --> 01:15:23.600
To make those humans do that,

01:15:23.600 --> 01:15:26.160
you'd have to have such massive alignment

01:15:26.160 --> 01:15:27.440
between the AI and humans

01:15:27.440 --> 01:15:29.040
that the humans all know they'll die

01:15:29.040 --> 01:15:30.400
if the AI dies and vies versa.

01:15:30.400 --> 01:15:31.280
We're not there.

01:15:31.280 --> 01:15:32.960
Maybe we'll be there in like, I don't know,

01:15:32.960 --> 01:15:34.480
end number of years, but not for a while.

01:15:34.480 --> 01:15:38.240
That's a total change in like how states are organized, okay?

01:15:39.200 --> 01:15:41.440
Finally, let me just talk about the physics a little bit more.

01:15:42.720 --> 01:15:44.320
There's a lot of stuff which is talked about

01:15:44.320 --> 01:15:46.720
at a very sci-fi book level of,

01:15:46.720 --> 01:15:49.040
it'll just invent nanomedicine and nanotech

01:15:49.040 --> 01:15:51.120
and kill us all and so on and so forth.

01:15:51.120 --> 01:15:52.960
Now look, I like Robert Freitas,

01:15:52.960 --> 01:15:55.200
obviously Richard Feynman's a genius and so on and so forth,

01:15:55.760 --> 01:15:58.080
but nanotech somehow hasn't been invented yet.

01:16:00.000 --> 01:16:02.560
Meaning that there's a lot of chemists

01:16:02.560 --> 01:16:03.760
that have worked in this area.

01:16:05.680 --> 01:16:08.320
A lot of nanotech is like rebranded chemistry

01:16:08.320 --> 01:16:10.400
because those are the molecular machines.

01:16:10.400 --> 01:16:14.240
For example, DNA polymerase or ribosome,

01:16:14.240 --> 01:16:15.280
those are molecular machines

01:16:15.280 --> 01:16:18.000
that we can get to work at that scale, the evolved ones.

01:16:18.640 --> 01:16:20.480
To my knowledge, and I may be wrong about this,

01:16:20.480 --> 01:16:22.160
I haven't looked at it very, very recently,

01:16:22.800 --> 01:16:26.640
we haven't actually been able to make artificial replicators

01:16:26.640 --> 01:16:28.080
of the stuff that they're talking about,

01:16:28.160 --> 01:16:31.280
which means it's possible that there's some practical difficulty

01:16:31.280 --> 01:16:34.160
that intervened between Feynman and Freitas

01:16:34.160 --> 01:16:35.440
and so on's calculations.

01:16:36.000 --> 01:16:39.120
Just a sheer fact that those books have came out decades ago

01:16:39.120 --> 01:16:40.720
and no progress has been made,

01:16:40.720 --> 01:16:42.160
indicates that maybe there's a roadblock

01:16:42.160 --> 01:16:43.680
that wasn't contemplated.

01:16:43.680 --> 01:16:46.160
So you can't just click your fingers and say, boom,

01:16:46.160 --> 01:16:47.840
nanomess, and it's sort of like clicking your fingers

01:16:47.840 --> 01:16:49.280
and saying, boom, time travel.

01:16:50.480 --> 01:16:52.880
Nanomess and exist, that was a good poke

01:16:52.880 --> 01:16:54.960
that I had a while ago in a conversation like this

01:16:54.960 --> 01:16:57.440
where the AI guy, AI safety guy on the other side was like,

01:16:57.440 --> 01:16:59.120
well, time travel, that's too implausible.

01:16:59.120 --> 01:17:02.320
I'm like, yeah, but you're waiting on the nanotech thing

01:17:02.320 --> 01:17:04.000
you're thinking is like here,

01:17:04.000 --> 01:17:06.240
and you're making so many assumptions there

01:17:06.240 --> 01:17:08.800
that I want to actually see some more work there.

01:17:08.800 --> 01:17:10.000
I want to actually see that nanotech

01:17:10.000 --> 01:17:12.080
is actually more possible than you think it is.

01:17:13.040 --> 01:17:15.360
As for, oh, we just need to mix things in a beaker

01:17:15.360 --> 01:17:17.440
and make a virus and so on.

01:17:17.440 --> 01:17:18.960
You know what is really, really good

01:17:18.960 --> 01:17:22.080
at defending against novel viruses?

01:17:22.080 --> 01:17:24.640
Like the human immune, that's something that's within envelope.

01:17:24.720 --> 01:17:27.680
Right, like you have evolved to not die

01:17:27.680 --> 01:17:29.120
and to fight off viruses.

01:17:29.120 --> 01:17:31.920
Is it possible that maybe you could make some super virus?

01:17:31.920 --> 01:17:36.000
I mean, maybe, but again, like humans are really good

01:17:36.000 --> 01:17:38.560
and the immune system is really good at that kind of thing.

01:17:38.560 --> 01:17:40.160
That is what we're set up to do, right,

01:17:40.160 --> 01:17:42.560
to adapt to that billions of years of evolution

01:17:42.560 --> 01:17:43.680
being set up for them.

01:17:43.680 --> 01:17:47.680
Physical constraints are not really contemplated

01:17:47.680 --> 01:17:49.200
when people talk about these super powerful

01:17:49.200 --> 01:17:51.200
mathematical constraints, practical constraints

01:17:51.200 --> 01:17:52.480
are not contemplated.

01:17:52.480 --> 01:17:53.440
And I could give more,

01:17:53.440 --> 01:17:54.480
but I think that was a lot right there.

01:17:54.480 --> 01:17:55.680
Let me pause here.

01:17:55.680 --> 01:17:58.240
Yeah, let me try to steal man a few things.

01:17:58.880 --> 01:18:02.400
And then I do think it's before too long,

01:18:02.400 --> 01:18:06.000
I want to kind of get back to the somewhat less,

01:18:06.000 --> 01:18:07.760
you know, radically transformative scenarios

01:18:07.760 --> 01:18:09.520
and ask a few follow up questions on that too.

01:18:09.520 --> 01:18:12.800
But I think for starters, I would say the sort of Eliezer,

01:18:12.800 --> 01:18:15.200
you know, he's updated his thinking over time as well.

01:18:15.200 --> 01:18:17.360
And I would say probably doesn't get quite enough credit for it

01:18:17.360 --> 01:18:19.680
because he's definitely on record, you know,

01:18:19.680 --> 01:18:22.320
repeatedly saying, yeah, I was kind of expecting

01:18:22.320 --> 01:18:24.960
more something from like the deep mind school to pop out

01:18:24.960 --> 01:18:29.360
and be, you know, wildly overpowered very quickly.

01:18:29.360 --> 01:18:32.960
And on the contrary, it seems like we're in more of a slow

01:18:32.960 --> 01:18:36.160
takeoff type of scenario where, you know, we've got these,

01:18:36.160 --> 01:18:39.040
again, like super high surface area kind of suck up

01:18:39.040 --> 01:18:41.840
all the knowledge, gradually get better at everything.

01:18:41.840 --> 01:18:43.120
Some surprises in there, you know,

01:18:43.120 --> 01:18:45.200
certainly some emergent properties,

01:18:45.200 --> 01:18:46.960
if you will accept that term, you know,

01:18:46.960 --> 01:18:49.440
surprise surprises to the developers of nothing else,

01:18:49.440 --> 01:18:51.440
right, that are definitely things

01:18:51.440 --> 01:18:52.880
we don't fully understand.

01:18:52.880 --> 01:18:54.880
But it does seem to be a, you know,

01:18:54.880 --> 01:18:58.240
more gradual turning up of capability versus some like,

01:18:58.240 --> 01:19:01.520
you know, super sudden surprise.

01:19:01.520 --> 01:19:05.280
But okay, so then what is the alternative?

01:19:05.280 --> 01:19:07.520
I'm going to try to kind of give you the,

01:19:07.520 --> 01:19:13.920
what I think of as the most consensus strongest scenario

01:19:13.920 --> 01:19:16.640
where humans lose track of the future

01:19:17.360 --> 01:19:19.200
and or lose control of the future,

01:19:19.200 --> 01:19:21.440
maybe starting by kind of losing track of the present

01:19:21.440 --> 01:19:23.040
and then having that kind of, you know,

01:19:23.040 --> 01:19:24.960
give way to losing control of the future.

01:19:24.960 --> 01:19:26.320
And I think within that, by the way, the,

01:19:27.200 --> 01:19:30.080
I'm not really one who cares that much about like,

01:19:30.080 --> 01:19:32.400
whether AIs say something offensive today,

01:19:33.040 --> 01:19:35.440
I'm not easily offended and like whatever.

01:19:35.440 --> 01:19:36.960
That's not, that's not world ending.

01:19:36.960 --> 01:19:37.680
I understand your point.

01:19:37.680 --> 01:19:39.440
That's not like, who cares, whatever.

01:19:39.440 --> 01:19:41.120
That's within scope, that's within envelope.

01:19:41.120 --> 01:19:43.200
Within this bigger kind of, you know,

01:19:43.200 --> 01:19:47.440
what is the real, you know, most likely path

01:19:47.440 --> 01:19:50.640
to like AI disaster as understood,

01:19:50.640 --> 01:19:52.160
I think by the smartest people today,

01:19:52.160 --> 01:19:54.320
I think that is still a useful leading indicator

01:19:54.320 --> 01:19:57.920
because it's like, okay, the developers,

01:19:57.920 --> 01:19:59.440
you know, whether you agree with their politics,

01:19:59.440 --> 01:20:00.320
whether you agree with their,

01:20:00.320 --> 01:20:02.080
whether you think their commercial reasons are,

01:20:02.080 --> 01:20:03.840
their sincere reasons are not,

01:20:03.840 --> 01:20:06.800
they have made it a goal to get the AI

01:20:06.800 --> 01:20:08.240
to not say certain things, right?

01:20:08.240 --> 01:20:09.840
They don't want it to be offensive.

01:20:09.840 --> 01:20:11.920
The most naive, you know, kind of down the fairway

01:20:11.920 --> 01:20:12.880
interpretation of that is like,

01:20:12.880 --> 01:20:14.720
Hey, they want to sell it to corporate customers.

01:20:14.720 --> 01:20:16.560
They know that their corporate customers don't want,

01:20:16.560 --> 01:20:19.120
you know, to have their AI saying offensive things.

01:20:19.120 --> 01:20:21.360
So they don't want to say offensive things.

01:20:21.360 --> 01:20:24.800
And yet they can't really control it.

01:20:24.800 --> 01:20:27.040
It's like still pretty easy to break.

01:20:27.040 --> 01:20:30.640
So I view that as just kind of a leading indicator of,

01:20:30.640 --> 01:20:34.560
okay, we've seen GPT-2, 3 and 4 over the last four years.

01:20:35.680 --> 01:20:39.280
And that's, you know, a big Delta in capability.

01:20:39.840 --> 01:20:44.560
How much control have we seen developed in that time

01:20:44.560 --> 01:20:46.160
and does it seem to be keeping pace?

01:20:46.720 --> 01:20:48.800
And my answer would be on the face of it,

01:20:48.800 --> 01:20:50.640
it seems like the answer is no.

01:20:50.640 --> 01:20:56.000
You know, we, we don't have the ability to really dial in

01:20:56.000 --> 01:20:58.960
the behavior such that we can say, okay, you're going to,

01:20:58.960 --> 01:21:02.000
you know, you can expect, you can trust that these AIs

01:21:02.000 --> 01:21:04.080
will like not do, you know, A, B, and C.

01:21:04.720 --> 01:21:07.040
On the contrary, it's like, if you're a little clever,

01:21:07.040 --> 01:21:08.320
you know, you can get them to do it.

01:21:08.320 --> 01:21:09.840
You can break out of the sandbox on it.

01:21:10.400 --> 01:21:12.000
Yeah. And it's, it's not even like, I mean,

01:21:12.000 --> 01:21:13.920
we've talked about, you know, things where you have access

01:21:13.920 --> 01:21:16.160
to the weights and you're doing like counter optimizations,

01:21:16.160 --> 01:21:17.200
but you don't even need that.

01:21:17.200 --> 01:21:20.080
You know, the kind of stuff I do in like my red teaming

01:21:20.080 --> 01:21:24.640
in public is literally just like feed the AI a couple of words,

01:21:24.640 --> 01:21:26.160
put a couple of words in its mouth, you know,

01:21:26.160 --> 01:21:27.760
and it will kind of carry on from there.

01:21:28.400 --> 01:21:30.320
So with that in mind is just the leading indicator.

01:21:30.880 --> 01:21:32.880
You know, I don't know how powerful the most powerful

01:21:32.880 --> 01:21:34.560
AI systems get over the next few years,

01:21:34.560 --> 01:21:37.840
but it seems very plausible to me that it might be

01:21:37.840 --> 01:21:40.880
as powerful as like an Elon Musk type figure, you know,

01:21:40.880 --> 01:21:43.040
somebody who's like really good at thinking

01:21:43.040 --> 01:21:45.920
from first principles, really smart, you know,

01:21:45.920 --> 01:21:49.040
really dynamic across a wide range of different contexts.

01:21:49.760 --> 01:21:53.200
And, you know, he's not powerful enough to like,

01:21:53.200 --> 01:21:54.720
in and of himself take over the world,

01:21:55.280 --> 01:21:57.760
but he is kind of becoming transformative.

01:21:58.480 --> 01:22:00.400
Now imagine that you have that kind of system

01:22:00.960 --> 01:22:04.080
and it's trivial to replicate it.

01:22:04.080 --> 01:22:06.080
So, you know, if you have like one Elon Musk,

01:22:06.080 --> 01:22:08.080
all of a sudden you can have arbitrary,

01:22:08.080 --> 01:22:09.520
you know, functionally arbitrary numbers

01:22:09.520 --> 01:22:13.440
of Elon Musk power things that are clones of each other.

01:22:14.080 --> 01:22:15.040
Maybe I can pause you there.

01:22:15.040 --> 01:22:17.440
So that's my polytheistic AI scenario,

01:22:17.440 --> 01:22:20.400
but here's the thing that is, this is background,

01:22:20.400 --> 01:22:21.760
but I want to push it to foreground.

01:22:23.040 --> 01:22:26.000
You still have a human typing in things into that thing.

01:22:26.000 --> 01:22:27.920
The human is doing the jailbreak, right?

01:22:27.920 --> 01:22:31.680
What we're talking about is not artificial intelligence

01:22:31.680 --> 01:22:33.520
in the sense of something separate from a human,

01:22:33.520 --> 01:22:34.880
but amplified intelligence.

01:22:36.320 --> 01:22:38.240
Amplified intelligence, I very much believe in.

01:22:38.240 --> 01:22:40.160
The reason is amplified intelligence.

01:22:40.160 --> 01:22:43.120
So here's something that people may not know about humans.

01:22:43.840 --> 01:22:46.640
There's this great book, Cooking Made Us Human.

01:22:48.080 --> 01:22:52.480
Tool use has shifted your biology in the following way.

01:22:52.480 --> 01:22:55.840
For example, I'll map it to the present day.

01:22:55.840 --> 01:22:57.840
This book by Richard Rang, I'm Cooking Made Us Human,

01:22:57.840 --> 01:23:01.440
where the fact that we started cooking and using fire

01:23:02.080 --> 01:23:05.360
meant that we could do metabolism outside the body,

01:23:05.360 --> 01:23:10.800
which meant it freed up energy for more brain development.

01:23:11.760 --> 01:23:13.680
Similarly, developing clothes

01:23:13.680 --> 01:23:15.920
meant that we didn't have to evolve as much fur,

01:23:16.640 --> 01:23:18.640
again, more energy for brain development.

01:23:18.640 --> 01:23:21.600
Evolving tools meant we didn't have as much fangs

01:23:21.600 --> 01:23:23.280
and claws and muscles,

01:23:23.280 --> 01:23:25.280
again, more energy for brain development, right?

01:23:25.280 --> 01:23:29.040
So encephalization quotient rose as tool use

01:23:29.040 --> 01:23:31.760
meant that we didn't have to do as much natively

01:23:31.760 --> 01:23:34.000
and we could push more to the machines.

01:23:34.000 --> 01:23:37.760
In a very real sense, we have been a man-machine symbiosis

01:23:37.760 --> 01:23:41.680
since the invention of fire and the stone axe and clothes.

01:23:42.640 --> 01:23:46.080
You do not exist as a human being on your own.

01:23:46.080 --> 01:23:50.480
The entire Ted Kojinski concept of living in nature by itself,

01:23:50.480 --> 01:23:53.680
humans are social organisms that are adapted

01:23:53.680 --> 01:23:56.480
to working with other humans and using tools.

01:23:57.760 --> 01:23:59.600
And we have been for millennia.

01:23:59.600 --> 01:24:02.000
This goes back, not just human history,

01:24:02.000 --> 01:24:03.840
but hundreds of thousands years before,

01:24:03.840 --> 01:24:05.120
100 gatherers are using tools.

01:24:05.840 --> 01:24:08.720
So what that means is,

01:24:08.720 --> 01:24:11.040
man-machine symbiosis is not some new thing.

01:24:11.600 --> 01:24:14.640
It's actually the old thing that broke us away

01:24:14.640 --> 01:24:17.600
from other primate lineages that weren't using tools.

01:24:17.600 --> 01:24:18.240
Okay?

01:24:18.240 --> 01:24:20.400
This is the fundamental difference between

01:24:20.400 --> 01:24:22.880
what I call Uncle Ted and Uncle Fred.

01:24:23.440 --> 01:24:25.120
Uncle Ted is Ted Kojinski.

01:24:25.120 --> 01:24:26.720
It's a unabomber, it's a doomer,

01:24:26.720 --> 01:24:29.280
it's a decelerator, the de-grocer who thinks

01:24:29.280 --> 01:24:32.000
we need to go back to Gaia and Eden and become monkeys

01:24:32.000 --> 01:24:35.040
and live in the jungle like Ted Kojinski, right?

01:24:35.680 --> 01:24:37.360
The unabomber cell.

01:24:37.360 --> 01:24:40.640
Uncle Fred is Friedrich Nietzsche, right?

01:24:40.640 --> 01:24:43.440
Nietzsche and we must get to the stars

01:24:43.440 --> 01:24:45.600
and become ubermen and so on and so forth.

01:24:45.600 --> 01:24:47.280
This, I think, is going to become,

01:24:47.280 --> 01:24:48.800
and I actually tweeted about this years ago

01:24:48.800 --> 01:24:50.400
before the current AI debates,

01:24:51.600 --> 01:24:55.920
between anarcho-primitivism, de-growth, deceleration,

01:24:55.920 --> 01:24:57.040
okay, on the one hand,

01:24:57.760 --> 01:25:01.360
and transhumanism and acceleration

01:25:01.920 --> 01:25:04.240
and human 2.0 and human self-improvement

01:25:04.240 --> 01:25:06.480
and make it to the stars, on the other hand.

01:25:06.480 --> 01:25:09.280
This is the future political axis, the current one.

01:25:10.000 --> 01:25:11.760
And roughly speaking, you can,

01:25:11.760 --> 01:25:13.600
it's not really left and right

01:25:13.600 --> 01:25:16.240
because you'll have both left status

01:25:16.240 --> 01:25:18.640
and right conservatives go over here.

01:25:18.640 --> 01:25:21.120
You know, left status will say it's against the state

01:25:21.120 --> 01:25:22.240
and the right status will say,

01:25:22.240 --> 01:25:24.800
the right conservatives say it's against God, okay?

01:25:24.800 --> 01:25:26.560
And you'll have left libertarians

01:25:26.560 --> 01:25:28.080
and right libertarians over here

01:25:28.080 --> 01:25:30.080
where the left libertarians say it's my body

01:25:30.080 --> 01:25:33.520
and the right libertarians say it's my money, right?

01:25:34.560 --> 01:25:37.120
And so that is a re-architecting of the political axis

01:25:37.120 --> 01:25:39.200
where Uncle Ted and Uncle Fred,

01:25:39.200 --> 01:25:40.960
which is a kind of clever way of putting it, okay?

01:25:42.240 --> 01:25:44.800
And the problem with the Uncle Ted guys, in my view,

01:25:44.800 --> 01:25:49.040
is, as I said, yeah, if they go and want to live in the woods,

01:25:49.040 --> 01:25:50.240
fine, go get them.

01:25:50.240 --> 01:25:52.800
But once you start having even like a thousand,

01:25:52.800 --> 01:25:55.120
forget a thousand, 100 people doing that,

01:25:55.120 --> 01:25:58.320
your trees will very quickly get exfoliated,

01:25:58.320 --> 01:26:00.400
the leaves are going to get all picked off of them.

01:26:00.400 --> 01:26:02.640
Humans are not set up to just literally live

01:26:02.640 --> 01:26:03.840
in the jungle right now.

01:26:03.840 --> 01:26:06.160
You've had hundreds of thousands of years of evolution

01:26:06.160 --> 01:26:08.560
that have driven you in the direction of tool use,

01:26:08.560 --> 01:26:11.360
social organisms, farming, et cetera, et cetera.

01:26:11.360 --> 01:26:13.600
The man machine symbiosis is not today,

01:26:13.600 --> 01:26:15.280
it's yesterday and the day before

01:26:15.280 --> 01:26:18.080
and 10,000 years ago and 100,000 years ago.

01:26:18.080 --> 01:26:20.400
And how do we know we've got man machine symbiosis?

01:26:20.400 --> 01:26:23.920
Can you live without, even if you're not living,

01:26:23.920 --> 01:26:25.680
even if you're not using the stove,

01:26:25.680 --> 01:26:28.480
somebody's using a stove to make you food, right?

01:26:28.480 --> 01:26:30.160
Can you live without the tractors

01:26:30.160 --> 01:26:32.320
that are digging up the grains?

01:26:32.320 --> 01:26:34.880
Can you live without indoor heating?

01:26:34.880 --> 01:26:36.560
Can you live without your clothes?

01:26:36.560 --> 01:26:38.480
Frankly, can you do your work without your phone,

01:26:38.480 --> 01:26:39.840
without your computer?

01:26:39.840 --> 01:26:40.800
No, you can't.

01:26:40.800 --> 01:26:42.880
You are already a man machine symbiosis.

01:26:43.440 --> 01:26:45.920
Once we accept that, then the question is,

01:26:45.920 --> 01:26:46.800
what's the next step?

01:26:47.520 --> 01:26:50.240
And right now, we're in the middle of that next step,

01:26:50.240 --> 01:26:52.480
which is AI is amplified intelligence.

01:26:53.120 --> 01:26:56.720
So what you're talking about is not that the AI is Elon Musk,

01:26:56.720 --> 01:27:00.320
it is that the AI human fusion means

01:27:00.320 --> 01:27:01.840
there's another 20 Elon Musk's.

01:27:02.320 --> 01:27:03.920
Or whatever the number is, okay?

01:27:04.560 --> 01:27:06.880
And that's good.

01:27:06.880 --> 01:27:07.600
That's fine.

01:27:07.600 --> 01:27:09.120
That's within envelope.

01:27:09.120 --> 01:27:11.520
That's just a bunch of smarter humans on the planet.

01:27:11.520 --> 01:27:13.120
That is amplified intelligence.

01:27:13.120 --> 01:27:17.200
That is more like, I mentioned the tool thing, okay?

01:27:17.200 --> 01:27:18.960
The other analogy would be like a dog.

01:27:18.960 --> 01:27:21.920
You know, a dog is man's best friend, right?

01:27:21.920 --> 01:27:24.560
So that AI does not live without you.

01:27:25.200 --> 01:27:26.960
Humans can turn it off.

01:27:26.960 --> 01:27:28.320
They have to power it.

01:27:28.320 --> 01:27:30.240
They have to give it subsidence, right?

01:27:30.240 --> 01:27:32.160
Eventually, that might become like a ceremonial thing,

01:27:32.160 --> 01:27:35.280
like this is our God that we pray to, right?

01:27:35.280 --> 01:27:37.120
Because it's wiser and smarter than us,

01:27:37.120 --> 01:27:39.120
and it appears in an image.

01:27:39.120 --> 01:27:40.800
But the priests maintain it.

01:27:40.800 --> 01:27:42.880
You know, just like you go to a Hindu temple

01:27:42.880 --> 01:27:43.840
or something like that,

01:27:43.840 --> 01:27:45.600
and the priests will pour out the ghee,

01:27:45.600 --> 01:27:47.360
you know, for the fires and so on and so forth.

01:27:47.360 --> 01:27:49.360
And then everybody comes in and prays, okay?

01:27:49.360 --> 01:27:51.680
The priests believe in the whole thing,

01:27:51.680 --> 01:27:53.600
but they also maintain the back of the house.

01:27:53.600 --> 01:27:56.160
They do the system administration for the temple.

01:27:56.160 --> 01:27:58.720
Same, you know, in a Christian church, right?

01:27:59.440 --> 01:28:02.960
The, you know, it's not like it appears out of nowhere.

01:28:02.960 --> 01:28:08.080
Somebody, you know, went and assembled this cathedral, right?

01:28:08.080 --> 01:28:09.840
They saw the back of the house,

01:28:09.840 --> 01:28:11.600
the fact that it was just woods and rocks

01:28:11.600 --> 01:28:12.800
and so on that came together.

01:28:12.800 --> 01:28:13.760
But then when people come there,

01:28:13.760 --> 01:28:15.200
it feels like a spiritual experience.

01:28:15.200 --> 01:28:16.720
You see what I'm saying, okay?

01:28:16.720 --> 01:28:18.080
So the equivalent of that,

01:28:18.080 --> 01:28:21.040
the priests or the people

01:28:21.040 --> 01:28:23.520
maintaining temples, cathedrals, mosques, whatever,

01:28:23.520 --> 01:28:27.600
is engineers who are maintaining

01:28:27.600 --> 01:28:30.800
these future AIs, which appear to you as Jesus.

01:28:30.800 --> 01:28:32.960
They appear to you, maybe even a hologram, okay?

01:28:32.960 --> 01:28:35.200
You come there, you ask it for guidance as an oracle.

01:28:35.200 --> 01:28:37.440
You've also got the personal version on your phone.

01:28:37.440 --> 01:28:39.520
You ask it for guidance, but guess what?

01:28:40.880 --> 01:28:44.640
You're still a human AI symbiosis until,

01:28:44.640 --> 01:28:47.280
and unless, that AI actually has the terminator scenario

01:28:47.280 --> 01:28:48.320
where it's got lots of robots

01:28:48.320 --> 01:28:49.520
and it can live on its own.

01:28:49.520 --> 01:28:51.600
I'm not saying that's physically impossible.

01:28:51.600 --> 01:28:53.760
I did give some constraints on it earlier,

01:28:53.760 --> 01:28:55.680
but for a while, we're not going to be there.

01:28:55.680 --> 01:28:57.840
So that alone means it's not fume

01:28:57.840 --> 01:28:59.520
because we don't have lots of drones running around.

01:28:59.520 --> 01:29:00.800
The AI has to be with the human.

01:29:00.800 --> 01:29:02.320
It's a human AI symbiosis.

01:29:02.320 --> 01:29:04.000
It's not AI Elon Musk.

01:29:04.000 --> 01:29:07.280
It is human AI fusion that becomes Elon Musk.

01:29:07.280 --> 01:29:08.640
And frankly, that's not that different

01:29:08.640 --> 01:29:09.840
from what Elon Musk himself is.

01:29:09.840 --> 01:29:11.920
Elon Musk would not be Elon Musk without the internet.

01:29:11.920 --> 01:29:13.040
Without the internet, you can't tweet

01:29:13.040 --> 01:29:14.880
and reach 150 million people.

01:29:14.880 --> 01:29:18.080
The internet itself made Elon what he is, right?

01:29:18.080 --> 01:29:20.640
And so this is like the next version of that.

01:29:20.640 --> 01:29:21.840
Maybe there's now 30 Elons

01:29:21.840 --> 01:29:24.000
because the AI makes the next 30 Elons.

01:29:24.000 --> 01:29:26.320
Yeah, I mean, again, I think I'm largely with you

01:29:26.320 --> 01:29:30.720
with just this one very important nagging worry

01:29:30.720 --> 01:29:33.120
that's like, what if this time is different?

01:29:33.120 --> 01:29:37.440
Because what if these systems are getting so powerful,

01:29:37.440 --> 01:29:39.520
so quickly that we don't really have time

01:29:39.520 --> 01:29:44.960
for that techno human fusion to really work out?

01:29:44.960 --> 01:29:46.880
And I'll just give you kind of a couple of data points on that.

01:29:46.880 --> 01:29:49.040
Like, you know, you said like,

01:29:49.040 --> 01:29:51.200
it's still somebody putting something into the AI.

01:29:51.200 --> 01:29:52.240
Well, sort of, right?

01:29:52.240 --> 01:29:54.960
I mean, already we have these proto agents

01:29:54.960 --> 01:29:57.440
and the like super simple scaffolding of an agent

01:29:57.440 --> 01:30:00.800
is just run it in a loop, give it a goal,

01:30:00.800 --> 01:30:04.560
and have it kind of pursue some like plan, act,

01:30:04.560 --> 01:30:08.080
get feedback and loop type of structure, right?

01:30:08.080 --> 01:30:09.840
It doesn't seem to take a lot.

01:30:09.840 --> 01:30:11.600
Now, they're not smart enough yet

01:30:11.600 --> 01:30:13.920
to accomplish big things in the world,

01:30:13.920 --> 01:30:19.040
but it seems like the language model to agent switch

01:30:19.120 --> 01:30:23.680
is less one right now that is gated by the structure

01:30:23.680 --> 01:30:25.680
or the architecture and more one that's just gated

01:30:25.680 --> 01:30:27.440
by the fact that like the language models,

01:30:27.440 --> 01:30:30.560
when framed as agents, just aren't that successful

01:30:30.560 --> 01:30:32.880
at like doing practical things and getting over hump,

01:30:32.880 --> 01:30:34.080
so they tend to get stuck.

01:30:35.040 --> 01:30:37.040
But it doesn't seem that hard to imagine that like,

01:30:37.040 --> 01:30:39.040
you know, if you had something that is sort of

01:30:39.040 --> 01:30:41.680
that next level that you put it into a loop,

01:30:41.680 --> 01:30:44.240
you say, okay, you're Elon Musk, LLM,

01:30:44.240 --> 01:30:47.120
and your job is to like make, you know, us,

01:30:47.120 --> 01:30:51.200
whatever us exactly is a, you know, multi-planetary species,

01:30:51.200 --> 01:30:55.120
and then you just kind of keep updating your status,

01:30:55.120 --> 01:30:57.200
keep updating your plans, keep trying stuff,

01:30:57.200 --> 01:31:00.240
keep getting feedback, and you know,

01:31:00.240 --> 01:31:02.880
like what really limits that?

01:31:03.760 --> 01:31:05.360
There may be like a really good program,

01:31:06.000 --> 01:31:09.600
but the whole AI kills everyone thing is so,

01:31:09.600 --> 01:31:11.840
it's like, where's the actuator?

01:31:11.840 --> 01:31:13.040
Okay, I hit enter.

01:31:14.320 --> 01:31:16.480
What kills me, right?

01:31:16.560 --> 01:31:19.920
Is it a hypnotized human who's being hypnotized by an AI

01:31:19.920 --> 01:31:23.200
that he's typed into and he's radicalized himself

01:31:23.200 --> 01:31:24.560
by typing into a computer?

01:31:25.120 --> 01:31:27.200
Okay, that's not that different from a lot of other things

01:31:27.200 --> 01:31:29.040
that have happened in the past, right?

01:31:29.040 --> 01:31:31.280
So who is actually striking me, right?

01:31:31.280 --> 01:31:32.560
Who's striking the human?

01:31:32.560 --> 01:31:35.280
It's another human within acts that he's been radicalized

01:31:35.280 --> 01:31:36.720
by an AI, okay?

01:31:36.720 --> 01:31:38.560
He's not, actually, that's not even the right term.

01:31:38.560 --> 01:31:42.080
We're giving agency to the AI when it's not really an agent.

01:31:42.080 --> 01:31:44.080
It is a human who's self-radicalized

01:31:44.080 --> 01:31:45.840
by typing into a computer screen.

01:31:46.560 --> 01:31:48.800
And has hit another human.

01:31:48.800 --> 01:31:49.760
That's one scenario.

01:31:49.760 --> 01:31:50.720
The other scenario is,

01:31:50.720 --> 01:31:52.880
it's literally a Skynet drone that's hitting you.

01:31:53.680 --> 01:31:54.560
Those are the only two,

01:31:54.560 --> 01:31:56.720
how else is it going to be physical, right?

01:31:56.720 --> 01:31:59.360
How does the, the actuation step is a part

01:31:59.360 --> 01:32:01.280
that is skipped over and it's a non-trivial step.

01:32:02.160 --> 01:32:04.160
Well, I think it could be lots of things, right?

01:32:04.160 --> 01:32:05.760
I mean, if it's not one of those two,

01:32:05.760 --> 01:32:08.640
if it's not another human or a drone hitting you,

01:32:08.640 --> 01:32:09.120
what is it?

01:32:10.080 --> 01:32:11.680
Just habitat degradation, right?

01:32:11.680 --> 01:32:13.360
I mean, how do we kill most of the other species

01:32:13.360 --> 01:32:14.480
that we drive to extinction?

01:32:14.480 --> 01:32:18.160
We don't go out and like hunt them down with axes one by one.

01:32:18.160 --> 01:32:20.720
We just like change the environment more broadly

01:32:20.720 --> 01:32:23.440
to the point where it's not suitable for them anymore

01:32:23.440 --> 01:32:25.920
and they don't have enough space and they kind of die out, right?

01:32:25.920 --> 01:32:28.400
Like, so we did hunt down some of the megafauna,

01:32:28.400 --> 01:32:31.360
like literally one by one with, with spears and stuff.

01:32:31.360 --> 01:32:34.400
But like most of the recent loss of species

01:32:34.960 --> 01:32:38.080
is just like, we're out there just extracting resources

01:32:38.080 --> 01:32:39.120
for our own purposes.

01:32:39.680 --> 01:32:41.840
And in the course of doing that, you know,

01:32:41.840 --> 01:32:45.680
whatever bird or whatever, you know, thing just kind of loses its place

01:32:45.680 --> 01:32:46.720
and then it's no more.

01:32:47.280 --> 01:32:49.680
And I don't think that's like totally implausible.

01:32:50.240 --> 01:32:54.080
Wait, so that is though, I think, within normal world, right?

01:32:54.080 --> 01:32:54.880
What does that mean?

01:32:54.880 --> 01:32:59.680
That means that some people, some, some amplified intelligence,

01:32:59.680 --> 01:33:02.000
and maybe we might call it HAI.

01:33:02.000 --> 01:33:04.720
Okay, human plus AI combination, right?

01:33:04.720 --> 01:33:08.640
Some HAIs out compete others economically

01:33:08.640 --> 01:33:09.680
and they lose their jobs.

01:33:09.680 --> 01:33:10.560
Is that what you're talking about?

01:33:11.040 --> 01:33:14.480
I think also the humans potentially become unnecessary

01:33:14.480 --> 01:33:16.000
in a lot of the configurations.

01:33:16.000 --> 01:33:19.120
Like just a recent paper from DeepMind.

01:33:19.120 --> 01:33:20.560
It's your marginal product workers.

01:33:20.560 --> 01:33:21.280
Or negative.

01:33:21.280 --> 01:33:25.680
Yeah, I mean, so the last, you know, DeepMind has been on Google,

01:33:25.680 --> 01:33:31.920
Google DeepMind has been on a tear of increasingly impressive medical AI's.

01:33:31.920 --> 01:33:36.800
Their most recent one takes a bunch of difficult case studies

01:33:36.800 --> 01:33:37.440
from the literature.

01:33:37.440 --> 01:33:38.240
I mean, case studies, you know,

01:33:38.240 --> 01:33:40.720
this is like rare diseases, hard to diagnose stuff.

01:33:41.520 --> 01:33:46.480
And asks an AI to do the differential diagnosis, compares that to human,

01:33:46.480 --> 01:33:48.640
and compares it to human plus AI.

01:33:49.280 --> 01:33:52.160
And they've phrased their results like in a very understated way.

01:33:52.160 --> 01:33:56.400
But the headline is, the AI blows away the human plus AI.

01:33:56.400 --> 01:33:58.000
The human makes the AI worse.

01:33:58.800 --> 01:34:02.080
So here's the thing, I'll say something provocative maybe.

01:34:02.080 --> 01:34:03.520
Okay, like I have in a very fine.

01:34:04.080 --> 01:34:08.400
I do think that the ABC's of Economic Apocalypse for Blue America

01:34:08.400 --> 01:34:10.160
are AI, Bitcoin, and China.

01:34:10.160 --> 01:34:13.280
Where AI takes away their, a lot of the revenue streams,

01:34:13.280 --> 01:34:18.320
the licensures that have made medical and legal costs and other things so high.

01:34:18.320 --> 01:34:20.080
Bitcoin takes away their power over money,

01:34:20.080 --> 01:34:22.000
and China takes away their military power.

01:34:22.000 --> 01:34:25.760
So I've perceived total meltdown for Blue America

01:34:26.880 --> 01:34:31.600
in the years and, you know, maybe decade to come already kind of happening.

01:34:31.600 --> 01:34:33.360
But that's different than being at the end of the world.

01:34:34.000 --> 01:34:34.640
Right?

01:34:34.640 --> 01:34:37.760
Like, Blue America had a really great time for a long time,

01:34:37.760 --> 01:34:39.520
and they've got these licensure locks.

01:34:39.520 --> 01:34:43.520
But because of that, they've hyperinflated the cost of medicine.

01:34:44.320 --> 01:34:47.360
It's like, how much, so what you're talking about is,

01:34:48.240 --> 01:34:49.760
wow, we have infinite free medicine.

01:34:50.320 --> 01:34:53.040
Man, Dr. Billing events are going to get ahead.

01:34:53.040 --> 01:34:53.920
That's the point.

01:34:54.640 --> 01:34:57.200
Yeah, and to be clear, I'm really with you on that too.

01:34:57.200 --> 01:35:01.360
Like, I want to see, when people say like, what is good about AI?

01:35:01.360 --> 01:35:03.920
You know, why should we pursue this?

01:35:05.200 --> 01:35:12.880
My standard answer is high quality medical advice for everyone at pennies per visit, right?

01:35:12.880 --> 01:35:14.720
It is orders of magnitude cheaper.

01:35:14.720 --> 01:35:17.280
We're already starting to see that in some ways it's better.

01:35:17.280 --> 01:35:19.680
People prefer it, you know, that AI is more patient.

01:35:19.680 --> 01:35:21.040
It has better bedside manner.

01:35:21.680 --> 01:35:26.000
I wouldn't say, you know, if I was giving my own family advice today,

01:35:26.000 --> 01:35:28.320
I would say use both a human doctor and an AI,

01:35:28.320 --> 01:35:30.960
but definitely use the AI as part of your mix.

01:35:31.680 --> 01:35:32.880
Absolutely. That's right.

01:35:32.880 --> 01:35:34.800
That's right. But you're prompting it still, right?

01:35:34.800 --> 01:35:37.520
The smarter you are, the smarter the AI is.

01:35:37.520 --> 01:35:40.560
You notice this immediately with your vocabulary, right?

01:35:40.560 --> 01:35:42.560
The more sophisticated your vocabulary,

01:35:42.560 --> 01:35:44.400
the finer the distinctions you can have,

01:35:44.400 --> 01:35:46.320
the better your own ability to spot errors.

01:35:47.040 --> 01:35:49.280
You can generate a basic program with it, right?

01:35:49.280 --> 01:35:51.280
But really amplified intelligence is, I think,

01:35:51.280 --> 01:35:52.880
a much better way of thinking about it.

01:35:52.880 --> 01:35:58.160
Because whatever your IQ is, it surges it upward by a factor of three or whatever the number.

01:35:58.160 --> 01:36:00.800
And maybe the amplifier increases with your intelligence.

01:36:00.800 --> 01:36:04.240
But that internal intelligence difference still exists.

01:36:04.240 --> 01:36:05.600
It's just like what a computer is.

01:36:05.600 --> 01:36:07.760
A computer is an amplifier for intelligence.

01:36:07.760 --> 01:36:11.120
If you're smart, you can hit enter and programs can go to...

01:36:11.120 --> 01:36:13.520
Like thinking about the Minecraft guy, right?

01:36:13.520 --> 01:36:14.320
Or Satoshi.

01:36:14.880 --> 01:36:19.600
One person built a billion, or in Satoshi's case, a trillion dollar thing, you know?

01:36:19.600 --> 01:36:22.560
Obviously other people continued Bitcoin and so on and so forth, right?

01:36:23.440 --> 01:36:25.760
So what I feel, though, is this is what I mean

01:36:25.760 --> 01:36:29.680
by going from nuclear terrorism to the TSA, okay?

01:36:29.680 --> 01:36:32.160
We went from AI will kill everyone.

01:36:32.160 --> 01:36:35.360
And I'm like, what's the actuator to, okay,

01:36:35.360 --> 01:36:36.640
it'll gradually degrade our environment.

01:36:36.640 --> 01:36:37.200
What does that mean?

01:36:37.200 --> 01:36:38.320
Okay, some people will lose their jobs.

01:36:38.320 --> 01:36:39.600
But then we're back in normal world.

01:36:40.240 --> 01:36:40.800
Well, hold on.

01:36:40.800 --> 01:36:42.480
Let me paint a little bit more complete picture,

01:36:42.480 --> 01:36:44.160
because I don't think we're quite there yet.

01:36:44.160 --> 01:36:48.480
So I think the differential diagnosis,

01:36:48.480 --> 01:36:52.800
recent paper, that's just a data point where it's kind of like chess.

01:36:52.800 --> 01:36:54.400
So this came long before, right?

01:36:54.400 --> 01:36:57.120
There was a period where humans were the best chess players.

01:36:57.120 --> 01:36:59.120
Then there was a period where the best were the hybrid,

01:36:59.120 --> 01:37:00.880
human AI systems.

01:37:00.880 --> 01:37:03.680
And now as far as I understand it, we're in a regime where

01:37:03.680 --> 01:37:06.480
the human can't really help the AI anymore.

01:37:06.480 --> 01:37:10.240
And so the best chess players are just pure AIs.

01:37:10.240 --> 01:37:13.360
We're not there in medicine, but we're starting to see examples where,

01:37:13.360 --> 01:37:16.480
hey, in a pretty defined study, differential diagnosis,

01:37:16.480 --> 01:37:18.880
the AI is beating, not just beating the humans,

01:37:18.880 --> 01:37:21.440
but also beating the AI human hybrid,

01:37:21.440 --> 01:37:23.040
or the human with access to AI.

01:37:23.760 --> 01:37:25.920
So, okay, that's not it, right?

01:37:25.920 --> 01:37:28.880
There's a paper recently called Eureka.

01:37:28.880 --> 01:37:35.520
Out of NVIDIA, this is Jim Fan's lab where they use GPT-4

01:37:35.520 --> 01:37:39.520
to write the reward functions to train a robot.

01:37:39.520 --> 01:37:42.720
So you want to train a robot to twirl a pencil in fingers.

01:37:43.920 --> 01:37:44.560
Hard for me to do.

01:37:45.280 --> 01:37:47.280
Robots definitely can't do it.

01:37:47.280 --> 01:37:48.160
How do you train that?

01:37:48.160 --> 01:37:49.520
Well, you need a reward function.

01:37:49.520 --> 01:37:52.640
The reward function, basically while you're in the early process

01:37:52.640 --> 01:37:54.080
of learning and failing all the time,

01:37:54.640 --> 01:37:57.280
the reward function gives you encouragement

01:37:57.280 --> 01:37:58.480
when you're on the right track, right?

01:37:58.960 --> 01:38:01.600
There are people who have developed this skill,

01:38:01.600 --> 01:38:02.720
and you might do something like,

01:38:02.720 --> 01:38:04.880
well, if the pencil has angular momentum,

01:38:05.520 --> 01:38:07.600
then that seems like you're on maybe the right track,

01:38:07.600 --> 01:38:08.960
so give that a reward.

01:38:08.960 --> 01:38:11.600
Even though, at the beginning, you're just failing all the time.

01:38:11.600 --> 01:38:14.640
It turns out GPT-4 is way better than humans at this, right?

01:38:14.640 --> 01:38:16.800
So it's better at training robots.

01:38:17.360 --> 01:38:19.120
So all of that is awesome, and it's great.

01:38:19.920 --> 01:38:24.640
But here's the thing, is there's a huge difference between AI

01:38:24.640 --> 01:38:27.120
is going to kill everybody and turn everybody into paperclips.

01:38:28.560 --> 01:38:33.360
Versus some humans with some AI are going to make a lot more money,

01:38:33.920 --> 01:38:35.440
and some people are going to lose their jobs.

01:38:36.320 --> 01:38:37.520
Yeah, I'm not scared of that.

01:38:37.520 --> 01:38:38.480
I'm not scared of that, Snare.

01:38:38.480 --> 01:38:39.840
I mean, it could be disruptive,

01:38:39.840 --> 01:38:43.680
it could be disruptive, but it's not existential under itself.

01:38:44.640 --> 01:38:45.120
Big deal.

01:38:45.120 --> 01:38:46.240
Okay, so that's why I went, right.

01:38:46.800 --> 01:38:49.360
There's a, the, the, to me, it comes, if I,

01:38:49.360 --> 01:38:51.840
if I ask just one question is, what is the actuator?

01:38:52.560 --> 01:38:52.960
Right?

01:38:52.960 --> 01:38:54.640
You know, sensors and actuators, right?

01:38:54.640 --> 01:38:56.240
What is the thing that's actually going to

01:38:56.960 --> 01:39:00.080
plunge a knife or a bullet into you and kill you?

01:39:00.800 --> 01:39:08.480
It is either a human who has hypnotized themselves by typing into a computer,

01:39:08.480 --> 01:39:10.400
like basically an AI terrorist, you know,

01:39:10.400 --> 01:39:14.320
which is kind of where some of the EAs are going in my view,

01:39:14.320 --> 01:39:20.080
or it is like an autonomous drone that is controlled in a

01:39:20.080 --> 01:39:22.240
starcraft or terminator like way.

01:39:22.880 --> 01:39:27.600
We are not there yet in terms of having enough humanoid or autonomous drones

01:39:27.600 --> 01:39:29.600
that are internet connected and programmable.

01:39:29.600 --> 01:39:31.120
That won't be there for some time.

01:39:31.120 --> 01:39:31.360
Okay.

01:39:32.000 --> 01:39:33.840
So that alone means fast takeoff is,

01:39:33.840 --> 01:39:35.680
and what I think by the time we get there,

01:39:36.480 --> 01:39:40.080
you will have a cryptographic control over them.

01:39:40.080 --> 01:39:41.440
That's a crucial thing.

01:39:41.440 --> 01:39:44.720
Cryptography fragments the whole space in a very fundamental way.

01:39:45.280 --> 01:39:46.720
If you don't have the private keys,

01:39:46.720 --> 01:39:48.320
you do not have control over it.

01:39:48.320 --> 01:39:52.720
So long as that piece of hardware, the cryptographic controller,

01:39:52.720 --> 01:39:54.240
you've nailed the equations on that,

01:39:54.240 --> 01:39:56.720
and frankly, you can use AI to attack that as well

01:39:56.720 --> 01:39:58.960
to make sure the code is perfect, right?

01:40:00.000 --> 01:40:01.840
Remember you talked about attack and defense?

01:40:01.840 --> 01:40:05.120
AI is attack cryptos defense, right?

01:40:05.120 --> 01:40:08.080
Because one of the things that crypto has done,

01:40:08.640 --> 01:40:11.440
do you know the PKI problem is public key infrastructure?

01:40:11.440 --> 01:40:14.480
I'll say no on behalf of the audience.

01:40:14.480 --> 01:40:14.960
This is good.

01:40:14.960 --> 01:40:16.080
We should do more of these actually.

01:40:16.080 --> 01:40:19.200
I feel it's a good fusion of things or whatever, right?

01:40:19.200 --> 01:40:27.360
But the public key infrastructure problem is something that was sort of,

01:40:27.360 --> 01:40:30.640
lots of cryptography papers and computer science papers

01:40:30.640 --> 01:40:34.560
in the 90s and 2000s assumed that this could exist

01:40:34.560 --> 01:40:36.640
and essentially meant if you could assume

01:40:36.640 --> 01:40:42.400
that everybody on the internet had a public key that was public

01:40:42.400 --> 01:40:46.480
and a private key that was kept both secure and available at all times,

01:40:46.480 --> 01:40:48.800
then there's all kinds of amazing things you can do

01:40:48.800 --> 01:40:51.840
with privacy preserving, messaging, and authentication, and so on.

01:40:52.480 --> 01:40:55.760
The problem is that for many years,

01:40:55.760 --> 01:40:59.040
what cryptographers try to do is they try to nag people

01:40:59.040 --> 01:41:01.520
into keeping their private keys secure and available.

01:41:01.520 --> 01:41:05.040
And the issue is it's trivial to keep it secure and unavailable

01:41:05.040 --> 01:41:07.040
where you write it down and you put it into a lockbox

01:41:07.040 --> 01:41:08.080
and you lose the lockbox.

01:41:08.720 --> 01:41:11.200
It's trivial to keep it available and not secure,

01:41:11.200 --> 01:41:14.080
okay, where you put it on your public website

01:41:14.080 --> 01:41:17.120
and it's available all the time, you never lose it,

01:41:17.120 --> 01:41:20.320
but it's not secure because anybody can see it.

01:41:21.120 --> 01:41:22.080
When you actually ask,

01:41:22.080 --> 01:41:24.560
what does it mean to keep something secure and available?

01:41:25.680 --> 01:41:27.440
That's actually a very high cost.

01:41:27.440 --> 01:41:31.360
It's precious space because it's based on your wallet, right?

01:41:31.360 --> 01:41:34.320
Your wallet is on your person at all times, so it's available,

01:41:35.040 --> 01:41:38.960
but it's not available to everybody else, so it's secure.

01:41:38.960 --> 01:41:41.760
So you actually have to touch it constantly, yes, right?

01:41:41.760 --> 01:41:44.400
So it turns out that the crypto wallet,

01:41:45.200 --> 01:41:50.320
by adding a literal incentive to keep your private keys secure and available,

01:41:50.320 --> 01:41:52.640
because if they're not available, you've lost your money.

01:41:52.640 --> 01:41:55.200
If they're not secure, you've lost your money, okay?

01:41:55.200 --> 01:41:58.320
To have both of them, that was what solved the PKI problem.

01:41:59.520 --> 01:42:00.960
Now we have hundreds of millions of people

01:42:00.960 --> 01:42:04.480
with public private key pairs where the private keys are secure and available.

01:42:04.480 --> 01:42:07.840
That means all kinds of cryptographic schemes,

01:42:07.840 --> 01:42:08.720
zero-knowledge stuff.

01:42:08.720 --> 01:42:12.000
There's this amazing universe of things that is happening now.

01:42:12.000 --> 01:42:15.040
Zero-knowledge in particular has made cryptography much more programmable.

01:42:15.040 --> 01:42:18.640
There's a whole topic which is, if you want something that's kind of,

01:42:18.640 --> 01:42:21.200
you know, like AI was creeping for a while

01:42:21.200 --> 01:42:23.120
and people, specialists were paying attention to it

01:42:23.120 --> 01:42:24.640
and then just burst out on the scene.

01:42:25.280 --> 01:42:27.440
Zero-knowledge is kind of like that for cryptography.

01:42:27.440 --> 01:42:30.560
Thanks to the, you know, you've probably heard of zero-knowledge before.

01:42:30.560 --> 01:42:37.440
Yeah, we did one episode with Daniel Kong on the use of zero-knowledge proofs

01:42:37.840 --> 01:42:43.280
to basically prove without revealing like the weights

01:42:43.280 --> 01:42:45.680
that you actually ran the model you said you were going to run

01:42:45.680 --> 01:42:47.600
and things like that I think are super interesting.

01:42:48.240 --> 01:42:48.880
Exactly, right?

01:42:48.880 --> 01:42:52.560
So what kinds of stuff, why is that useful in the AI space?

01:42:52.560 --> 01:42:55.680
Well, first is you can use it, for example,

01:42:55.680 --> 01:42:58.880
for training on medical records while keeping them both private

01:42:58.880 --> 01:43:01.840
but also getting the data you wanted.

01:43:01.840 --> 01:43:05.760
For example, let's say you've got a collection of genomes, okay?

01:43:06.400 --> 01:43:12.240
And you want to ask, okay, how many Gs were in this data set?

01:43:12.240 --> 01:43:13.520
How many Cs?

01:43:13.520 --> 01:43:14.080
How many A's?

01:43:14.080 --> 01:43:14.880
How many T's?

01:43:14.880 --> 01:43:16.960
Okay, like you just said, like that's a very simple down.

01:43:16.960 --> 01:43:21.120
So what's the ACG T content of this, you know, the sequence data set?

01:43:21.840 --> 01:43:24.400
You could get those numbers, you could prove they were correct

01:43:24.400 --> 01:43:27.040
without giving any information about the individual sequences, right?

01:43:27.040 --> 01:43:29.440
Or more specifically, you do it at one locus and you say,

01:43:29.440 --> 01:43:31.920
how many Gs and how many Cs are at this particular locus

01:43:31.920 --> 01:43:33.840
and you get the SNP distribution, okay?

01:43:33.840 --> 01:43:37.920
So it's useful for what you just said,

01:43:37.920 --> 01:43:40.240
which is like showing that you ran a particular model

01:43:40.240 --> 01:43:41.760
without giving anything else away.

01:43:41.760 --> 01:43:45.440
It's useful for certain kinds of data analysis.

01:43:45.440 --> 01:43:47.520
There's a lot of overhead on compute on this right now.

01:43:47.520 --> 01:43:49.520
So it's not something that you do trivially, okay?

01:43:49.520 --> 01:43:50.800
But it'll probably come down with time.

01:43:51.680 --> 01:43:55.360
But what is perhaps most interestingly useful for

01:43:55.360 --> 01:44:00.880
is in the context of AI is coming up with things that AI can't fake.

01:44:00.880 --> 01:44:02.880
So what we talked about earlier, right?

01:44:02.880 --> 01:44:07.680
Like an AI can come up with all kinds of plausible sounding images,

01:44:07.680 --> 01:44:12.480
but if it wasn't cryptographically signed by the sender,

01:44:13.680 --> 01:44:18.960
then, you know, it should be signed by the sender and put on chain.

01:44:18.960 --> 01:44:22.160
And then at least you know that this person or this entity

01:44:22.160 --> 01:44:28.000
with this private key asserted that this object existed at this time

01:44:28.000 --> 01:44:30.080
in a way that'd be extremely expensive to falsify

01:44:30.080 --> 01:44:31.600
because it's either on the Bitcoin blockchain

01:44:31.600 --> 01:44:33.680
or another blockchain that's very expensive to rewind, okay?

01:44:34.320 --> 01:44:37.440
This starts to be a bunch of facts that an AI can't fake.

01:44:38.400 --> 01:44:43.440
You know, so going back to the kind of big picture loss of control story,

01:44:43.440 --> 01:44:45.520
I was just kind of trying to build up a few of these data points

01:44:45.520 --> 01:44:47.840
that like, hey, look at this differential diagnosis.

01:44:47.840 --> 01:44:52.000
We already see like humans are not really adding value to AIs anymore.

01:44:52.000 --> 01:44:53.440
That's kind of striking.

01:44:53.440 --> 01:44:55.760
And like similarly with training robot hands,

01:44:55.760 --> 01:44:58.240
GPT-4 is outperforming human experts.

01:44:58.800 --> 01:45:02.560
And by the way, all of the sort of latent spaces

01:45:02.560 --> 01:45:04.240
are like totally bridgeable, right?

01:45:04.240 --> 01:45:06.320
I mean, one of the most striking observations

01:45:06.320 --> 01:45:08.320
of the last couple of years of study is that

01:45:09.120 --> 01:45:11.680
AIs can talk to each other in high dimensional space,

01:45:12.800 --> 01:45:16.080
which we don't really have a way of understanding natively, right?

01:45:16.080 --> 01:45:18.240
It takes a lot of work for us to decode.

01:45:19.040 --> 01:45:20.880
This is like the language thing?

01:45:20.880 --> 01:45:23.200
We're starting to see AIs kind of develop

01:45:23.200 --> 01:45:26.000
not obviously totally on their own as of now,

01:45:26.080 --> 01:45:27.120
but we are...

01:45:27.120 --> 01:45:32.480
There is becoming an increasingly reliable go-to set of techniques

01:45:33.040 --> 01:45:36.880
if you want to bridge different modalities

01:45:36.880 --> 01:45:39.680
with like a pretty small parameter adapter.

01:45:39.680 --> 01:45:40.320
That's interesting.

01:45:40.320 --> 01:45:41.760
Actually, what's a good paper on that?

01:45:41.760 --> 01:45:42.800
I actually hadn't seen that.

01:45:42.800 --> 01:45:45.680
The blip family of models out of Salesforce research

01:45:45.680 --> 01:45:46.480
is really interesting.

01:45:46.480 --> 01:45:48.400
And I've used that in production at...

01:45:48.400 --> 01:45:49.440
Salesforce, really?

01:45:49.440 --> 01:45:50.480
Yeah, Salesforce research.

01:45:50.480 --> 01:45:54.080
They have a crack team that has open sourced a ton of stuff

01:45:54.080 --> 01:45:59.280
in the language model, computer vision, joint space.

01:46:00.160 --> 01:46:01.200
And this...

01:46:01.200 --> 01:46:02.560
You see this all over the place now,

01:46:02.560 --> 01:46:06.400
but basically what they did in a paper called blip2,

01:46:06.400 --> 01:46:09.120
and they've had like five of these with a bunch of different techniques,

01:46:09.680 --> 01:46:13.600
but in blip2, they took a pre-trained language model

01:46:14.320 --> 01:46:16.080
and then a pre-trained computer vision model,

01:46:16.640 --> 01:46:19.040
and they were able to train just a very small model

01:46:19.040 --> 01:46:20.160
that kind of connects the two.

01:46:20.640 --> 01:46:24.080
So you could take an image, put it into the image space,

01:46:24.960 --> 01:46:28.960
then have their little bridge that over to language space.

01:46:29.520 --> 01:46:31.680
And that everything else, the two big models are frozen.

01:46:31.680 --> 01:46:33.280
So they were able to do this on just like

01:46:33.280 --> 01:46:35.360
a couple days worth of GPU time,

01:46:36.080 --> 01:46:37.280
which I do think goes to show

01:46:37.280 --> 01:46:40.400
how it is going to be very difficult to contain proliferation.

01:46:40.400 --> 01:46:41.360
It was just good.

01:46:41.360 --> 01:46:42.880
In my view, that's really good.

01:46:42.880 --> 01:46:44.080
As long as it doesn't get out of control,

01:46:44.080 --> 01:46:45.920
I'm probably with you on that too.

01:46:46.880 --> 01:46:50.080
But by bridging this vision space into the language space,

01:46:50.080 --> 01:46:52.880
then the language model would be able to converse with you

01:46:52.880 --> 01:46:55.520
about the image, even though the language model

01:46:56.080 --> 01:46:57.760
was never trained on images,

01:46:57.760 --> 01:47:01.440
but you just had this connector that kind of bridges those modalities.

01:47:02.240 --> 01:47:03.680
It's like another layer of the network

01:47:03.680 --> 01:47:05.200
that just bridges two networks, almost.

01:47:05.920 --> 01:47:07.840
Yeah, it bridges the spaces.

01:47:07.840 --> 01:47:10.240
It like it bridges the conceptual spaces

01:47:10.240 --> 01:47:12.960
between something that has only understood images

01:47:12.960 --> 01:47:15.040
and something that has only understood language,

01:47:15.040 --> 01:47:17.040
but now you can kind of bring those together.

01:47:17.040 --> 01:47:19.040
As I think about it, it's not that surprising

01:47:19.120 --> 01:47:24.320
because that's what, for example, text image models are basically that.

01:47:24.320 --> 01:47:27.200
They're bridging two spaces in a sense, right?

01:47:27.200 --> 01:47:28.480
But I'll check this paper out.

01:47:28.480 --> 01:47:31.440
So on the one hand, it's not that surprising.

01:47:31.440 --> 01:47:33.280
On their hand, I should see how they implement it

01:47:33.280 --> 01:47:34.480
or whatever, so blip to.

01:47:34.480 --> 01:47:34.960
Okay.

01:47:34.960 --> 01:47:36.960
Yeah, I think the most striking thing about that

01:47:36.960 --> 01:47:38.800
is just how small it is.

01:47:38.800 --> 01:47:42.640
You took these two off-the-shelf models that were trained

01:47:43.760 --> 01:47:45.440
independently for other purposes,

01:47:46.000 --> 01:47:50.880
and you're able to bridge them with a relatively small connector.

01:47:51.600 --> 01:47:54.640
And that seems to be kind of happening all over the place.

01:47:54.640 --> 01:47:57.520
I would also look at the Flamingo architecture,

01:47:57.520 --> 01:48:00.640
which is like a year and a half ago now out of DeepMind.

01:48:01.440 --> 01:48:04.000
That was one for me where I was like, oh my,

01:48:04.000 --> 01:48:06.080
and it's also a language to vision

01:48:06.080 --> 01:48:08.240
where they keep the language model frozen,

01:48:08.240 --> 01:48:10.800
and then they kind of, in my mind,

01:48:10.800 --> 01:48:13.040
it's like I can see the person in their garage

01:48:13.040 --> 01:48:14.800
like tinkering with their soldering iron,

01:48:14.800 --> 01:48:16.080
because it's just like, wow,

01:48:16.080 --> 01:48:18.240
you took this whole language thing that was frozen,

01:48:18.240 --> 01:48:21.280
and you kind of injected some vision stuff here,

01:48:21.280 --> 01:48:22.320
and you added a couple layers,

01:48:22.320 --> 01:48:24.720
and you kind of Frankensteined it, and it works.

01:48:24.720 --> 01:48:26.160
And it's like, wow, that's not really,

01:48:26.720 --> 01:48:29.440
it wasn't like super principled.

01:48:29.440 --> 01:48:31.600
It was just kind of hack a few things together

01:48:31.600 --> 01:48:32.800
and try training it.

01:48:32.800 --> 01:48:34.400
And I don't want to diminish what they did,

01:48:34.400 --> 01:48:36.640
because I'm sure there were more insights to it than that.

01:48:37.200 --> 01:48:41.760
But it seems like we are kind of seeing a reliable pattern

01:48:41.840 --> 01:48:46.640
of the key point here being model-to-model communication

01:48:46.640 --> 01:48:47.920
through high-dimensional space,

01:48:47.920 --> 01:48:51.120
which is not mediated by human language,

01:48:52.240 --> 01:48:55.840
is I think one of the reasons that I would expect,

01:48:55.840 --> 01:48:57.120
and by the way, there's lots of papers too.

01:48:57.120 --> 01:49:00.080
I'm like, language models are human level

01:49:00.080 --> 01:49:01.920
or even superhuman prompt engineers.

01:49:01.920 --> 01:49:05.360
They're self-prompting techniques are getting pretty good.

01:49:06.160 --> 01:49:08.480
So if I'm imagining the big picture of like,

01:49:09.520 --> 01:49:10.800
and we can get back to like,

01:49:10.800 --> 01:49:12.640
okay, well, how do we use any techniques,

01:49:12.640 --> 01:49:14.320
crypto or otherwise, to keep this under control?

01:49:15.440 --> 01:49:18.080
And then I would say this is kind of the newer school

01:49:18.080 --> 01:49:20.880
of the big picture AI safety worry.

01:49:21.520 --> 01:49:23.360
Obviously, there's a lot of flavors,

01:49:23.360 --> 01:49:26.080
but if you were to go look at like a Jay Acotra,

01:49:26.080 --> 01:49:28.080
for example, I think is a really good writer on this.

01:49:29.360 --> 01:49:31.520
Her worldview is less that we're going to have this fume

01:49:31.520 --> 01:49:33.600
and more that over a period of time,

01:49:33.600 --> 01:49:34.880
and it may not be a long period of time.

01:49:34.880 --> 01:49:37.040
Maybe it's like a generation, maybe it's 10 years,

01:49:37.040 --> 01:49:38.400
maybe it's 100 years,

01:49:38.400 --> 01:49:39.760
but obviously those are all small

01:49:39.760 --> 01:49:42.000
in the sort of grand scheme of the future.

01:49:43.120 --> 01:49:45.440
We have in all likelihood,

01:49:46.080 --> 01:49:53.120
the development of AI centric schemes of production,

01:49:53.120 --> 01:49:55.520
where you've got kind of your high level executive function

01:49:55.520 --> 01:49:56.880
is like your language model.

01:49:56.880 --> 01:49:58.720
You've got all these like lower level models.

01:49:58.720 --> 01:49:59.840
They're all bridgeable.

01:49:59.840 --> 01:50:03.200
All the spaces are bridgeable in high dimensional form,

01:50:03.200 --> 01:50:05.360
where they're not really mediated by language,

01:50:05.360 --> 01:50:06.560
unless we enforce that.

01:50:06.560 --> 01:50:10.480
I mean, we could say it must always be mediated by language

01:50:10.480 --> 01:50:12.160
so we can read the logs,

01:50:12.720 --> 01:50:15.360
but there's a tax to that,

01:50:15.360 --> 01:50:18.000
because going through language is like highly compressed

01:50:18.800 --> 01:50:20.880
compared to the high dimensional space to space.

01:50:21.600 --> 01:50:21.920
All right.

01:50:21.920 --> 01:50:24.720
So let me see if I can steal man or articulate your case.

01:50:24.720 --> 01:50:27.280
You're saying AIs are going to get good enough.

01:50:27.280 --> 01:50:28.560
They're going to be able to communicate with each other

01:50:28.560 --> 01:50:31.280
good enough, and they'll be able to do enough tasks

01:50:31.280 --> 01:50:33.680
that more and more humans will be rendered economically

01:50:33.680 --> 01:50:35.120
marginal and unnecessary.

01:50:35.120 --> 01:50:36.800
I'm not saying I think that will happen.

01:50:36.800 --> 01:50:38.480
I'm just saying I think there's a good enough chance

01:50:38.480 --> 01:50:39.200
that that will happen,

01:50:39.200 --> 01:50:41.200
but it's worth taking really seriously.

01:50:41.200 --> 01:50:42.720
I actually think that will happen,

01:50:42.720 --> 01:50:44.400
something along those lines,

01:50:44.400 --> 01:50:47.040
in the sense of at least massive economic disruption.

01:50:47.040 --> 01:50:47.680
Definitely.

01:50:47.680 --> 01:50:48.400
Okay.

01:50:48.400 --> 01:50:50.240
But I'll give an answer to that,

01:50:50.240 --> 01:50:53.040
which is both maybe fun and not fun.

01:50:53.040 --> 01:50:57.120
Have you seen the graph of the percentage of America

01:50:57.120 --> 01:50:58.320
that was involved in farming?

01:50:59.040 --> 01:50:59.280
Yeah.

01:50:59.280 --> 01:51:01.120
I tweeted a version of that once.

01:51:01.680 --> 01:51:02.240
Oh, you did.

01:51:02.240 --> 01:51:02.480
Okay.

01:51:02.480 --> 01:51:02.800
Great.

01:51:02.800 --> 01:51:03.280
Good.

01:51:03.280 --> 01:51:04.480
So you're familiar with this,

01:51:04.480 --> 01:51:07.280
and you're familiar with what I mean by the implication of it,

01:51:07.280 --> 01:51:10.480
where basically Americans used to identify themselves

01:51:10.480 --> 01:51:15.280
as farmers, and manufacturing rose

01:51:15.280 --> 01:51:17.040
as agriculture collapsed.

01:51:17.760 --> 01:51:20.560
And here is the graph on that.

01:51:20.560 --> 01:51:23.040
But from like 40% in the year 1900

01:51:23.760 --> 01:51:26.720
to a total collapse of agriculture,

01:51:26.720 --> 01:51:28.960
and then also more recently a collapse of manufacturing

01:51:28.960 --> 01:51:32.080
into bureaucracy, paperwork, legal work,

01:51:32.080 --> 01:51:34.240
what is up into the right since then

01:51:34.320 --> 01:51:38.640
is the lawyers.

01:51:38.640 --> 01:51:39.680
What is up into the right?

01:51:39.680 --> 01:51:40.800
What is replacing that?

01:51:41.440 --> 01:51:43.520
Starting in around the 1970s,

01:51:44.480 --> 01:51:46.160
we used to be adding energy production

01:51:46.160 --> 01:51:47.760
and energy production flatlined

01:51:47.760 --> 01:51:50.720
once people got angry about nuclear power.

01:51:50.720 --> 01:51:52.320
So this is a future that could have been.

01:51:52.320 --> 01:51:53.600
We could be on Mars by now,

01:51:53.600 --> 01:51:54.800
but we got flatlined.

01:51:54.800 --> 01:51:55.120
Right?

01:51:55.840 --> 01:51:57.600
What did go up into the right?

01:51:57.600 --> 01:51:58.560
So construction costs,

01:51:58.560 --> 01:51:59.760
this is the bad scenario,

01:51:59.760 --> 01:52:02.160
where the miracle energy got destroyed

01:52:02.160 --> 01:52:05.280
because regulations, the cost was flat.

01:52:05.280 --> 01:52:07.840
And then when vertical, when regulations were imposed,

01:52:07.840 --> 01:52:10.960
all the progress was stopped by decels and de-growthers.

01:52:10.960 --> 01:52:12.720
And then Alara was implemented,

01:52:12.720 --> 01:52:16.800
which said nuclear energy has to be as low risk

01:52:16.800 --> 01:52:19.600
as reasonably necessary, as reasonably achievable.

01:52:19.600 --> 01:52:21.840
And that meant that you just keep adding safety to it

01:52:21.840 --> 01:52:23.440
until it's as same as cost as everything else,

01:52:23.440 --> 01:52:25.840
which means you destroyed the value of it.

01:52:27.040 --> 01:52:28.320
But you know what was up into the right?

01:52:28.320 --> 01:52:30.560
What replaced those agriculture and manufacturing jobs?

01:52:30.560 --> 01:52:31.600
Look at this, you see this graph?

01:52:32.960 --> 01:52:34.080
We will put this on YouTube.

01:52:34.080 --> 01:52:37.200
So if you want to see the graph do the YouTube version of this,

01:52:37.200 --> 01:52:38.880
for the audio only group,

01:52:38.880 --> 01:52:41.120
it's an exponential curve in the number of lawyers

01:52:41.120 --> 01:52:42.560
in the United States from,

01:52:42.560 --> 01:52:44.560
looks like maybe two thirds of a million

01:52:44.560 --> 01:52:47.280
to 13 million over the last 140 years.

01:52:47.280 --> 01:52:51.040
Yeah. And in 1880, it was like sub 100,000

01:52:51.040 --> 01:52:52.480
or something like that, right?

01:52:52.480 --> 01:52:55.440
And then it's just like, especially that 1970 point,

01:52:55.440 --> 01:52:57.280
that's when it went totally vertical, okay?

01:52:57.920 --> 01:52:59.760
And it's probably even more sensitive.

01:52:59.760 --> 01:53:03.120
So, you know, if you add paperwork jobs, bureaucratic jobs,

01:53:03.120 --> 01:53:07.040
you know, every lawyer is like, you know, sorry lawyers,

01:53:07.040 --> 01:53:08.880
but you're basically negative value add, right?

01:53:08.880 --> 01:53:11.200
Because it should, the fact that you have a lawyer

01:53:11.200 --> 01:53:15.040
means that you couldn't just self serve a form, right?

01:53:15.040 --> 01:53:16.400
Basic government is platformers

01:53:16.400 --> 01:53:18.480
where you can just self serve and you fill it out.

01:53:18.480 --> 01:53:20.240
And you don't have to have somebody

01:53:20.240 --> 01:53:22.560
like code something for you custom, you know,

01:53:22.560 --> 01:53:23.840
lawyers that's doing custom code

01:53:23.840 --> 01:53:26.320
is because the legal code is so complicated.

01:53:26.320 --> 01:53:28.800
So, you know, the whole Shakespeare thing,

01:53:28.800 --> 01:53:31.120
like first thing we do, let's, you know, kill all the lawyers.

01:53:31.120 --> 01:53:33.760
First thing we do, let's automate all the lawyers, right?

01:53:33.760 --> 01:53:37.440
Only something that's the hammer blow of AI

01:53:37.440 --> 01:53:40.320
can break the backbone and it will.

01:53:41.200 --> 01:53:43.360
It's going to break the backbone of Blue America, right?

01:53:43.360 --> 01:53:45.680
It's going to cause, that's why the political layer

01:53:45.680 --> 01:53:48.960
and the sovereignty layer is not what AI people think about.

01:53:48.960 --> 01:53:51.360
But it's like crucial for thinking about AI

01:53:51.360 --> 01:53:53.600
because what tribes does AI benefit?

01:53:54.400 --> 01:53:59.040
And again, we got away from, why does AI kill everybody?

01:53:59.040 --> 01:54:00.240
Well, it's going to need actuators.

01:54:00.240 --> 01:54:02.000
Who's going to stab you? Who's going to shoot you?

01:54:02.000 --> 01:54:03.680
It's got to be a human hypnotized by AI

01:54:03.680 --> 01:54:05.280
or a drone that AI controls.

01:54:05.280 --> 01:54:08.320
A human hypnotized by AI is actually a conventional threat.

01:54:08.320 --> 01:54:09.360
It looks like a terrorist cell.

01:54:09.360 --> 01:54:10.880
We know how to deal with that, right?

01:54:10.880 --> 01:54:13.120
It's just like radicalized humans that worship some AI

01:54:13.120 --> 01:54:13.920
that stab you.

01:54:13.920 --> 01:54:16.560
It's like the pause AI people are one step, I think,

01:54:16.560 --> 01:54:17.760
away from that, all right?

01:54:17.760 --> 01:54:19.360
But that's just like Aum Shinriko.

01:54:19.360 --> 01:54:20.320
That's like allocated.

01:54:20.320 --> 01:54:22.320
That's like basically terrorists

01:54:22.400 --> 01:54:25.120
who think that the AI is telling them what to do, fine?

01:54:25.120 --> 01:54:29.600
If it's not a human that's stabbing you, it is a drone.

01:54:29.600 --> 01:54:33.520
And that's like a very different future where like five or 10

01:54:33.520 --> 01:54:35.200
or 15 years up, maybe we have enough

01:54:35.200 --> 01:54:36.400
internet connected drones out there,

01:54:36.400 --> 01:54:38.160
but even then they'll have private keys.

01:54:38.160 --> 01:54:41.360
So there's going to be fragmentation of address space.

01:54:41.360 --> 01:54:44.560
Not all drones be controlled by everybody in my view, okay?

01:54:44.560 --> 01:54:46.000
That's what AI safety is.

01:54:46.000 --> 01:54:48.240
AI safety is can you turn it off?

01:54:48.240 --> 01:54:49.120
Can you kill it?

01:54:49.120 --> 01:54:51.760
Can you stop it from controlling drones?

01:54:51.760 --> 01:54:53.200
That's what AI safety is.

01:54:53.200 --> 01:54:54.960
Can you also open the model weights

01:54:54.960 --> 01:54:56.480
so you can generate adversarial inputs?

01:54:57.440 --> 01:54:59.840
Can you open the model weights and proliferate it?

01:54:59.840 --> 01:55:01.040
You're saying, oh, proliferation is bad.

01:55:01.040 --> 01:55:04.720
I'm saying proliferation is good because if everybody has one,

01:55:04.720 --> 01:55:07.360
then nobody has an advantage on it, right?

01:55:07.360 --> 01:55:09.440
Not relatively speaking, okay?

01:55:09.440 --> 01:55:11.920
I have very few super confident positions.

01:55:11.920 --> 01:55:17.040
So I wouldn't necessarily say I think that proliferation is bad.

01:55:17.040 --> 01:55:18.640
I'd say so far it's good.

01:55:19.360 --> 01:55:21.840
It has, and even most of the AI safety people,

01:55:23.120 --> 01:55:26.960
I would say if I could speak on the behalf of the AI safety

01:55:27.920 --> 01:55:31.040
consensus, I would say most people would say

01:55:31.040 --> 01:55:37.040
even that the Llama 2 release has proven good for AI safety

01:55:37.040 --> 01:55:38.080
for the reasons that you're saying.

01:55:38.080 --> 01:55:39.760
But they opposed it.

01:55:39.760 --> 01:55:41.040
Well, some didn't, some didn't.

01:55:41.040 --> 01:55:44.880
I would say the main posture that I see AI safety people taking

01:55:44.880 --> 01:55:49.280
is that we're getting really close to,

01:55:49.280 --> 01:55:50.800
or we might be getting really close.

01:55:51.760 --> 01:55:54.880
Certainly if we just kind of naively extrapolate out recent progress,

01:55:54.880 --> 01:55:58.400
it would seem that we're getting really close to systems

01:55:58.400 --> 01:56:03.360
that are sufficiently powerful that it's very hard to predict

01:56:03.360 --> 01:56:05.680
what happens if they proliferate.

01:56:05.680 --> 01:56:06.800
Llama 2, not there.

01:56:07.600 --> 01:56:12.240
And so, yes, it has enabled a lot of interpretability work.

01:56:12.240 --> 01:56:14.640
It has enabled things like representation engineering,

01:56:15.280 --> 01:56:17.920
which there isn't a lot of good stuff that has come from it.

01:56:17.920 --> 01:56:20.000
The big thing that I want to kind of establish is

01:56:21.120 --> 01:56:23.440
you agree with me on the actuation point or not.

01:56:24.080 --> 01:56:26.160
Like, the thing is this thing, like,

01:56:26.960 --> 01:56:29.600
oh Llama 2 proliferates and so businesses are disrupted

01:56:29.600 --> 01:56:32.400
and people, you know, maybe they paid a lot of money

01:56:32.400 --> 01:56:35.200
for their MD degree and they can't make us a bunch of money.

01:56:35.200 --> 01:56:38.320
That's within the realm of what I call conventional warfare.

01:56:38.320 --> 01:56:39.120
You know what I mean?

01:56:39.120 --> 01:56:41.600
That's like we're still in normal world as we were talking about.

01:56:42.560 --> 01:56:45.200
Unconventional warfare is, you know,

01:56:45.200 --> 01:56:47.040
Skynet arises and kills everybody.

01:56:47.040 --> 01:56:47.360
Okay.

01:56:48.000 --> 01:56:49.840
And that is what is being sold over here.

01:56:50.800 --> 01:56:52.720
And when you think about the actuators,

01:56:52.720 --> 01:56:54.160
we don't have the drones out there.

01:56:54.160 --> 01:56:56.320
We don't have the humanoid robots at control.

01:56:56.320 --> 01:56:59.200
And hypnotized humans are a very tiny subset of humans,

01:56:59.200 --> 01:57:00.000
probably.

01:57:00.000 --> 01:57:02.320
And if they aren't, that just looks like a religion

01:57:02.320 --> 01:57:04.160
or a cult or a terrorist cell.

01:57:04.160 --> 01:57:05.760
And we know how to deal with that as well.

01:57:05.760 --> 01:57:08.480
The super intelligent AI with, you know,

01:57:08.480 --> 01:57:11.840
lots of robots that control in a Starcraft form,

01:57:11.840 --> 01:57:14.640
I would agree is something that humans haven't faced yet.

01:57:14.640 --> 01:57:17.760
But by the time we get that many robots out there,

01:57:17.760 --> 01:57:19.520
you won't be able to control all of them at once

01:57:19.520 --> 01:57:21.120
because of the private key things I mentioned.

01:57:21.920 --> 01:57:24.080
So that's why I'm like, okay,

01:57:24.080 --> 01:57:25.920
everything else we're talking about is in normal world.

01:57:25.920 --> 01:57:28.480
That is the single biggest thing that I wanted to get.

01:57:28.480 --> 01:57:31.840
Like economic disruption, people losing jobs,

01:57:31.840 --> 01:57:34.880
proliferation so that the balance of power is redistributed.

01:57:34.880 --> 01:57:35.920
All that's fine.

01:57:35.920 --> 01:57:38.080
The other reason I say this is people keep trying

01:57:38.080 --> 01:57:40.080
to link AI to existential risk.

01:57:40.080 --> 01:57:42.480
A great example is one of the things you actually had in here.

01:57:42.480 --> 01:57:44.800
This is similar to the AI policy and two things.

01:57:44.800 --> 01:57:45.840
It's a totally reasonable question,

01:57:45.840 --> 01:57:48.160
but then I'm going to, in my view, deconstruct the question.

01:57:48.800 --> 01:57:50.240
What would you think about putting the limit on the right

01:57:50.240 --> 01:57:52.560
to compute or their capabilities an AI system might demonstrate

01:57:52.560 --> 01:57:54.720
that you make you think open access no longer wise?

01:57:54.720 --> 01:57:56.880
Most common near term answer here to be seems to be related

01:57:56.880 --> 01:57:59.440
to risk of pandemic via novel pathogen engineering.

01:57:59.440 --> 01:58:00.240
So guess what?

01:58:00.240 --> 01:58:02.720
You know who the novel pathogen engineers are?

01:58:02.720 --> 01:58:05.200
The US and Chinese governments, right?

01:58:05.200 --> 01:58:07.360
They did it or probably did it,

01:58:07.360 --> 01:58:09.680
credibly did it, credibly being accused of doing it.

01:58:10.240 --> 01:58:11.760
They haven't been punished for COVID-19.

01:58:11.760 --> 01:58:13.360
In fact, they covered up their culpability

01:58:13.360 --> 01:58:15.520
and pointed everywhere other than themselves.

01:58:15.520 --> 01:58:18.800
They used it to gain more power in both the US and China

01:58:18.800 --> 01:58:21.440
with both lockdown in China and in the US

01:58:21.440 --> 01:58:22.960
and all kinds of COVID era.

01:58:23.520 --> 01:58:26.720
Trillions of dollars was printed and spent and so on and so forth.

01:58:26.720 --> 01:58:29.440
They did everything other than actually solve the problem.

01:58:29.440 --> 01:58:33.440
That was actually getting the vaccines in the private sector.

01:58:33.440 --> 01:58:36.160
And they studied the existential risk only to generate it.

01:58:36.160 --> 01:58:38.800
And they're even paid to generate pandemic prevention and failed.

01:58:39.360 --> 01:58:42.160
So this would be the ultimate Fox guarding the hen house.

01:58:42.800 --> 01:58:45.760
Okay, the only reason that the two organizations responsible

01:58:45.760 --> 01:58:47.680
for killing millions of people novel pathogen

01:58:47.680 --> 01:58:53.040
are going to prevent people from doing this by restricting compute.

01:58:53.040 --> 01:58:55.760
No, you know what it is actually what's happening here is

01:58:57.040 --> 01:58:59.040
one of the concepts I have in the network state

01:58:59.040 --> 01:59:01.760
is this idea of God, state and network.

01:59:01.760 --> 01:59:04.480
Okay, meaning what do you think is the most powerful force in the world?

01:59:04.480 --> 01:59:05.520
Is it almighty God?

01:59:06.080 --> 01:59:09.440
Is it the US government or is it encryption?

01:59:10.240 --> 01:59:12.400
Right, or eventually maybe an AGI, right?

01:59:13.040 --> 01:59:18.240
If what's happening here is a lot of people are implicitly

01:59:19.120 --> 01:59:21.840
without realizing it, even if they are secular atheists,

01:59:21.840 --> 01:59:24.160
they're treating GOV as GOD.

01:59:24.160 --> 01:59:27.680
Okay, they treat the US government as God as the final mover.

01:59:28.400 --> 01:59:31.280
No, I appreciate your little I take inspiration from you actually

01:59:31.280 --> 01:59:37.680
in terms of trying to come up with these little quips that are memorable.

01:59:37.680 --> 01:59:41.440
So I was just smiling at that because I think you do a great job of that.

01:59:41.440 --> 01:59:46.560
And I try to encourage, I have less success coining terms than you have,

01:59:46.560 --> 01:59:50.240
but certainly try to follow your example on that front.

01:59:50.240 --> 01:59:53.200
It's like a helpful, if you can compress it down,

01:59:53.200 --> 01:59:54.080
it's like more memorable.

01:59:54.080 --> 01:59:55.280
So that's what I try to do, right?

01:59:55.280 --> 01:59:58.400
So exactly a lot of these people who are secular,

01:59:58.400 --> 01:59:59.840
think of themselves as atheists,

01:59:59.840 --> 02:00:02.560
have just replaced GOD with GOV.

02:00:02.560 --> 02:00:04.480
They worship the US government as God.

02:00:04.480 --> 02:00:05.600
And there's two versions of this.

02:00:05.600 --> 02:00:08.400
You know how like God has put the male and female version, right?

02:00:08.400 --> 02:00:13.840
The female version is the Democrat God within the USA that has infinite money

02:00:13.840 --> 02:00:16.000
and can take care of everybody and care for everybody.

02:00:16.000 --> 02:00:19.280
And the Republican God is the US military that can blow up anybody,

02:00:19.280 --> 02:00:22.320
and it's the biggest and strongest and most powerful America F. Yeah.

02:00:22.320 --> 02:00:23.200
Okay.

02:00:23.200 --> 02:00:30.400
And everybody who thinks of the US government as being able to stop something

02:00:30.400 --> 02:00:32.000
is praying to a dead God.

02:00:32.880 --> 02:00:33.360
Okay.

02:00:33.360 --> 02:00:34.400
When you say this,

02:00:34.400 --> 02:00:37.600
you actually get an interesting reaction from AI safety people

02:00:37.600 --> 02:00:40.080
where you've actually hit their true solar plexus.

02:00:41.680 --> 02:00:42.080
All right.

02:00:42.080 --> 02:00:45.280
The true solar plexus is not that they believe in AI.

02:00:45.280 --> 02:00:46.960
It's that they believe in the US government.

02:00:48.800 --> 02:00:50.400
That's a true solar plexus

02:00:50.400 --> 02:00:52.000
because they are appealing to,

02:00:52.000 --> 02:00:53.520
they're praying to this dead God

02:00:53.520 --> 02:00:57.760
that can't even clean the poop off the streets in San Francisco, right?

02:00:57.760 --> 02:01:00.720
That is losing wars or fighting them to sell me.

02:01:00.720 --> 02:01:03.520
It has lost all these wars around the world

02:01:03.520 --> 02:01:04.800
that spent trillions of dollars

02:01:04.800 --> 02:01:06.800
has been through financial crisis, coronavirus,

02:01:06.800 --> 02:01:10.080
Iraq war, you know, total meltdown politically.

02:01:10.080 --> 02:01:10.640
Okay.

02:01:10.640 --> 02:01:14.320
That is now has interest payments more than the defense budget

02:01:14.880 --> 02:01:16.160
that is, you know,

02:01:16.160 --> 02:01:18.720
that spent $100 billion on the California train

02:01:18.720 --> 02:01:20.080
without laying a single track.

02:01:20.800 --> 02:01:23.440
It's like that, you know, that Morgan Freeman thing for,

02:01:23.440 --> 02:01:24.960
you know, the clip from Batman,

02:01:24.960 --> 02:01:28.720
where he's like, so this man has a billionaire,

02:01:28.720 --> 02:01:29.600
blah, blah, blah, this and that,

02:01:29.600 --> 02:01:32.080
and your plan is to threaten him, right?

02:01:32.080 --> 02:01:34.560
And so you're going to create this super intelligence

02:01:34.560 --> 02:01:36.720
and have Kamala Harris regulate it.

02:01:36.720 --> 02:01:39.120
Come on, man, so to speak, right?

02:01:39.120 --> 02:01:43.520
Like these people are praying to a blind, deaf and dumb God

02:01:43.520 --> 02:01:47.760
that was powerful in 1945, right?

02:01:47.760 --> 02:01:50.640
That's why, by the way, all the popular movies,

02:01:50.640 --> 02:01:51.120
what are they?

02:01:51.120 --> 02:01:53.680
It's Barbie, it's Oppenheimer, right?

02:01:53.680 --> 02:01:55.680
It's, it's Top Gun.

02:01:55.680 --> 02:01:58.640
They're all throwbacks the 80s or the 50s

02:01:58.640 --> 02:02:01.040
when the USA was really big and strong.

02:02:01.760 --> 02:02:03.520
And the future is a black mirror.

02:02:03.520 --> 02:02:04.640
Yeah, I think that's tragic.

02:02:05.600 --> 02:02:06.880
One of the projects that I do like,

02:02:06.880 --> 02:02:07.840
and you might appreciate this,

02:02:07.840 --> 02:02:08.560
I don't know if you've seen it,

02:02:08.560 --> 02:02:12.560
is the, from the future of Life Institute,

02:02:13.120 --> 02:02:16.240
a project called Imagine a World,

02:02:16.240 --> 02:02:17.520
I think is the name of it.

02:02:18.160 --> 02:02:21.200
And they basically challenged, you know,

02:02:21.200 --> 02:02:25.200
their audience and the public to come up with

02:02:25.920 --> 02:02:29.120
positive visions of a future,

02:02:29.120 --> 02:02:31.280
you know, where technology changes a lot.

02:02:31.280 --> 02:02:33.440
And obviously AI pretty central to a lot of those stories.

02:02:34.400 --> 02:02:37.840
And, you know, one of the challenges that people go through

02:02:37.840 --> 02:02:39.520
and how do we get there and whatever,

02:02:39.520 --> 02:02:44.960
but a purposeful effort to imagine positive futures.

02:02:45.840 --> 02:02:47.760
Super under provided.

02:02:47.760 --> 02:02:50.640
And I really liked the,

02:02:50.640 --> 02:02:51.920
the investment that they made in that.

02:02:52.480 --> 02:02:54.000
You know, one of the things I've got

02:02:54.000 --> 02:02:56.320
in the Never See It book is there's certain megatrends

02:02:56.320 --> 02:02:57.840
that are happening, right?

02:02:57.840 --> 02:03:00.240
And megatrends, I mean, it's possible for,

02:03:00.960 --> 02:03:03.760
like one miraculous human maybe to reverse them, okay?

02:03:04.560 --> 02:03:07.520
Because I think both the impersonal force of history theory

02:03:07.520 --> 02:03:09.440
and the great man theory of history have some truth to them.

02:03:10.640 --> 02:03:14.080
But the megatrends are the decline of Washington DC

02:03:14.960 --> 02:03:15.920
the rise of the internet,

02:03:15.920 --> 02:03:17.280
the rise of India, the rise of China.

02:03:18.160 --> 02:03:19.840
That is like my worldview.

02:03:19.840 --> 02:03:23.680
And I can give a thousand graphs and charts and so on for that.

02:03:23.680 --> 02:03:25.280
But that's basically the last 30 years.

02:03:26.000 --> 02:03:27.840
And maybe the next X, right?

02:03:27.840 --> 02:03:29.360
I'm not saying there can't be trend reversal.

02:03:29.360 --> 02:03:30.400
Of course it can be trend reversal,

02:03:30.400 --> 02:03:32.480
as I just mentioned, some hammer blow could hit it,

02:03:32.480 --> 02:03:33.360
but that's what's happening.

02:03:33.920 --> 02:03:35.280
And so because of that,

02:03:35.280 --> 02:03:37.520
the people who are optimistic about the future

02:03:37.520 --> 02:03:39.680
are aligned with either the internet, India or China.

02:03:40.320 --> 02:03:42.880
And the people who are not optimistic about the future

02:03:42.880 --> 02:03:46.480
are blue Americans or left out red Americans, okay?

02:03:46.480 --> 02:03:50.720
Or Westerners in general who are not tech people, okay?

02:03:50.720 --> 02:03:53.120
If they're not tech people, they're not up into the right,

02:03:53.680 --> 02:03:55.200
basically, because the internet's,

02:03:55.200 --> 02:03:57.920
if you, I mean, one of the things is we have a misnomer,

02:03:57.920 --> 02:03:59.280
as I was saying earlier,

02:03:59.280 --> 02:04:00.720
of calling it the United States,

02:04:00.720 --> 02:04:02.400
because it's the dis-United States.

02:04:02.400 --> 02:04:03.920
It's like talking about,

02:04:03.920 --> 02:04:05.680
you know, talking about America is like talking about Korea.

02:04:05.680 --> 02:04:06.880
There's North Korea and South Korea,

02:04:06.880 --> 02:04:08.720
and they're totally different populations.

02:04:08.720 --> 02:04:10.880
And, you know, communism and capitalism

02:04:10.880 --> 02:04:12.400
are totally different systems.

02:04:12.400 --> 02:04:14.560
And the thing that is good for one

02:04:14.560 --> 02:04:16.480
is bad for another and vice versa.

02:04:16.480 --> 02:04:18.080
And so like America doesn't exist.

02:04:18.080 --> 02:04:19.600
There's only, just like there's no Korea,

02:04:19.600 --> 02:04:21.040
there's only North Korea and South Korea,

02:04:21.040 --> 02:04:22.160
there's no America.

02:04:22.160 --> 02:04:23.920
There is blue America and red America

02:04:23.920 --> 02:04:25.840
and also gray America, tech America.

02:04:25.840 --> 02:04:29.120
And blue America is harmed,

02:04:29.120 --> 02:04:30.160
or they think they're harmed,

02:04:30.160 --> 02:04:32.720
or they've gotten themselves into a spot where they're harmed,

02:04:32.720 --> 02:04:34.480
by every technological development,

02:04:34.480 --> 02:04:37.120
which is why they hate it so much, right?

02:04:37.120 --> 02:04:38.800
AI versus journalist jobs,

02:04:38.800 --> 02:04:41.440
crypto takes away banking jobs, you know,

02:04:41.440 --> 02:04:42.960
everything, you know, self-driving cars,

02:04:42.960 --> 02:04:45.520
they just take away regulator control, right?

02:04:45.520 --> 02:04:48.240
Anything that reduces their power, they hate,

02:04:48.240 --> 02:04:50.560
and they're just trying to freeze an amber with regulations.

02:04:50.560 --> 02:04:52.800
Red America got crushed a long time ago

02:04:52.800 --> 02:04:54.720
by offshoring to China and so on.

02:04:54.720 --> 02:04:56.880
They're making, you know, inroads ally

02:04:56.880 --> 02:04:58.800
with tech America or gray America.

02:04:58.800 --> 02:05:01.360
Tech America is like the one piece of America

02:05:01.360 --> 02:05:03.760
that's actually still functional and globally competitive.

02:05:03.760 --> 02:05:06.240
And people always do this fallacy of aggregation,

02:05:06.240 --> 02:05:08.000
where they talk about the USA,

02:05:08.000 --> 02:05:10.720
and it's really this component that's up and to the right,

02:05:10.720 --> 02:05:12.640
and the others that are down and to the right,

02:05:12.640 --> 02:05:15.520
or at best flat, like red, but they're like down, right?

02:05:15.520 --> 02:05:17.760
Like red is like okay functional, blue is down.

02:05:18.880 --> 02:05:21.440
Point is, tech America, I think we're gonna find,

02:05:21.440 --> 02:05:27.440
is not even truly, or how American is tech America,

02:05:27.440 --> 02:05:30.160
because it's like 50% immigrants, right?

02:05:30.160 --> 02:05:31.680
And like a lot of children immigrants,

02:05:31.680 --> 02:05:33.840
and most of their customers are overseas,

02:05:33.840 --> 02:05:36.720
and their users are overseas,

02:05:36.720 --> 02:05:40.320
and their vantage point is global, right?

02:05:40.320 --> 02:05:41.920
And they're basically not,

02:05:43.040 --> 02:05:45.680
I know we're in this ultra-nationalist kick right now,

02:05:45.680 --> 02:05:46.960
and I know that there's gonna be,

02:05:47.840 --> 02:05:49.840
there's a degree of a fork here,

02:05:49.840 --> 02:05:54.160
where you fork technology into Silicon Valley

02:05:54.160 --> 02:05:56.320
and the internet, okay?

02:05:56.320 --> 02:05:58.880
Where Silicon Valley is American,

02:05:58.880 --> 02:06:01.040
and they'll be making like American military equipment,

02:06:01.040 --> 02:06:03.040
and so on and so forth, and they're signaling USA,

02:06:03.040 --> 02:06:04.640
which is fine, okay?

02:06:04.640 --> 02:06:08.560
And then the internet is international global capitalism,

02:06:08.560 --> 02:06:13.040
and the difference is Silicon Valley, or let's say US tech,

02:06:13.040 --> 02:06:16.480
let me be less, you know, US tech says ban TikTok,

02:06:16.480 --> 02:06:17.680
build military equipment, et cetera,

02:06:17.680 --> 02:06:19.280
it's really identifying itself as American,

02:06:19.840 --> 02:06:22.320
and it's thinking of being anti-China, okay?

02:06:22.320 --> 02:06:24.640
But there's, US and China are only 20% of the world,

02:06:24.640 --> 02:06:26.720
80% of the world is neither American nor Chinese.

02:06:27.280 --> 02:06:30.240
So the internet is for everybody else

02:06:30.240 --> 02:06:34.080
who wants actual global rule of law, right?

02:06:34.080 --> 02:06:36.240
When as the US decays as a rules-based order,

02:06:36.240 --> 02:06:37.920
and people don't wanna be under China,

02:06:38.000 --> 02:06:40.960
people wanna be under something like blockchains,

02:06:40.960 --> 02:06:43.520
where you've got like property rights contract law

02:06:43.520 --> 02:06:46.960
across borders that are enforced by an impartial authority, okay?

02:06:46.960 --> 02:06:49.280
That's also the kind of laws that can bind AIs,

02:06:49.280 --> 02:06:50.480
like AIs across borders,

02:06:50.480 --> 02:06:52.160
if you wanna make sure they're gonna do something,

02:06:52.160 --> 02:06:55.280
cryptography can bind an AI in such a way that it can't fake it.

02:06:55.280 --> 02:06:57.360
It can't, an AI can't mint more Bitcoin, you know?

02:06:58.160 --> 02:06:59.360
Here's my last question for you.

02:07:00.000 --> 02:07:04.320
AI discourse right now does seem to be polarizing into camps.

02:07:04.320 --> 02:07:06.640
Obviously a big way that you think about the world

02:07:06.640 --> 02:07:09.360
is by trying to figure out, you know,

02:07:09.360 --> 02:07:10.320
what are the different camps?

02:07:10.320 --> 02:07:11.600
How do they relate to each other?

02:07:11.600 --> 02:07:12.400
So on and so forth.

02:07:13.600 --> 02:07:18.000
I have the view that AI is so weird,

02:07:18.560 --> 02:07:21.360
and so unlike other things that we've encountered in the past,

02:07:21.360 --> 02:07:23.360
including just like, unlike humans, right?

02:07:23.360 --> 02:07:25.440
I always say AI, alien intelligence,

02:07:26.080 --> 02:07:30.160
that I feel like it's really important to borrow a phrase

02:07:30.160 --> 02:07:32.640
from Paul Graham, keep our identities small,

02:07:33.440 --> 02:07:36.160
and try to have a scout mindset

02:07:36.880 --> 02:07:40.640
to really just take things on their own terms, right?

02:07:40.640 --> 02:07:42.480
And not necessarily put them through a prism

02:07:42.480 --> 02:07:43.760
of like, whose team am I on?

02:07:43.760 --> 02:07:46.640
Or, you know, is this benefit my team

02:07:46.640 --> 02:07:47.920
or hurt the other team or whatever?

02:07:48.800 --> 02:07:52.880
But, you know, just try to be as kind of directly engaged

02:07:52.880 --> 02:07:55.040
with the things themselves as we can

02:07:55.040 --> 02:07:57.360
without mediating it through all these lenses.

02:07:57.360 --> 02:07:58.800
You know, I think about, you mentioned like,

02:07:59.600 --> 02:08:01.440
the gain of function, right?

02:08:01.440 --> 02:08:04.240
And I don't know for sure what happened,

02:08:04.240 --> 02:08:06.400
but it certainly does seem like there's a

02:08:06.400 --> 02:08:08.640
very significant chance that it was a lab leak.

02:08:08.640 --> 02:08:10.560
Certainly there's a long history of lab leaks,

02:08:11.200 --> 02:08:13.120
but it would be like, you know,

02:08:13.120 --> 02:08:15.760
it would seem to me a failure to say,

02:08:15.760 --> 02:08:18.800
okay, well, what's the opposite

02:08:18.800 --> 02:08:20.880
of just having like a couple of government labs?

02:08:20.880 --> 02:08:23.200
Like everybody gets their own gain of function lab, right?

02:08:23.200 --> 02:08:24.640
Like if we could, and this is kind of what we're doing

02:08:24.640 --> 02:08:25.680
with AI, we're like,

02:08:25.680 --> 02:08:28.160
let's compress this power down to as small as we can.

02:08:28.160 --> 02:08:30.160
Let's make a kit that can run in everybody's home.

02:08:31.040 --> 02:08:34.960
Would we want to send out these like gain of function,

02:08:34.960 --> 02:08:38.800
you know, wet lab research kits to like every home in the world

02:08:38.800 --> 02:08:41.360
and be like, hope you find something interesting,

02:08:41.360 --> 02:08:45.280
you know, like let us know if you find any new pathogens

02:08:45.280 --> 02:08:46.880
or hey, maybe you'll find life-saving drugs,

02:08:46.880 --> 02:08:48.720
like whatever, we'll see what you find,

02:08:48.720 --> 02:08:50.160
you know, all eight billion of you.

02:08:50.800 --> 02:08:54.160
That to me seems like it would be definitely a big misstep.

02:08:54.160 --> 02:08:57.280
And that's the kind of thing that I see coming out of

02:08:58.640 --> 02:09:02.400
ideologically motivated reasoning or like,

02:09:02.400 --> 02:09:03.680
you know, tribal reasoning.

02:09:03.680 --> 02:09:06.560
And so I guess I wonder how you think about the role

02:09:07.120 --> 02:09:10.960
that tribalism and ideology is playing

02:09:10.960 --> 02:09:14.400
and should or shouldn't play as we try to understand AI.

02:09:15.040 --> 02:09:17.760
Okay, so first is you're absolutely right

02:09:17.760 --> 02:09:22.960
that just because A is bad does not mean that B is good, right?

02:09:22.960 --> 02:09:26.320
So A could be a bad option, B could be a bad option,

02:09:26.320 --> 02:09:27.280
C could be a bad option.

02:09:27.840 --> 02:09:30.320
There might be, you have to go down to option G

02:09:30.320 --> 02:09:31.520
before you find a good option.

02:09:31.520 --> 02:09:33.600
Or there might be three good options and seven bad options,

02:09:33.600 --> 02:09:34.800
for example, right?

02:09:34.800 --> 02:09:37.600
So to map that here, in my view,

02:09:37.600 --> 02:09:40.480
an extremely bad option is to ask the U.S.

02:09:40.480 --> 02:09:42.560
and Chinese governments to do something.

02:09:43.200 --> 02:09:46.640
Anything the U.S. government does at the federal level,

02:09:46.640 --> 02:09:49.120
at the state level, in blue states, at the city level,

02:09:49.680 --> 02:09:50.640
has been a failure.

02:09:51.200 --> 02:09:53.440
And the way, here's a metaway of thinking about it.

02:09:53.440 --> 02:09:54.880
You invest in companies, right?

02:09:54.880 --> 02:09:57.600
So as an investor, here's a really important thing.

02:09:58.400 --> 02:10:00.080
You might have 10 people who come to you

02:10:00.080 --> 02:10:02.000
with the same words in their pitch.

02:10:02.000 --> 02:10:04.080
They're all, for example, building social networks.

02:10:04.800 --> 02:10:06.400
But one of them is Facebook,

02:10:06.400 --> 02:10:09.600
and the others are Friendster and whatever, okay?

02:10:09.600 --> 02:10:10.960
And no offense to Friendster, you know,

02:10:10.960 --> 02:10:13.840
these guys were like, you know, pioneers in their own way,

02:10:14.400 --> 02:10:16.640
but they just got outmatched by Facebook.

02:10:16.640 --> 02:10:18.480
So the point is that the words were the same

02:10:19.040 --> 02:10:20.560
on each of these packages,

02:10:20.560 --> 02:10:22.240
but the execution was completely different.

02:10:22.960 --> 02:10:26.880
So could I imagine a highly competent government

02:10:26.880 --> 02:10:31.200
that could execute and that actually did, you know,

02:10:32.000 --> 02:10:34.800
like, you know, make the right balance of things and so on?

02:10:34.800 --> 02:10:36.400
I can't say it's impossible,

02:10:36.400 --> 02:10:39.040
but I can say that it wouldn't be this government.

02:10:40.640 --> 02:10:44.000
Okay, and so you are talking about the words,

02:10:44.000 --> 02:10:45.680
and I'm talking about the substance.

02:10:45.680 --> 02:10:48.640
The words are, we will protect you from AI, right?

02:10:48.640 --> 02:10:49.760
In my view, the substances,

02:10:49.760 --> 02:10:51.920
they aren't protecting you from anything, right?

02:10:51.920 --> 02:10:53.440
You're basically giving money and power

02:10:53.440 --> 02:10:56.480
to a completely incompetent and, in fact, malicious organization,

02:10:56.480 --> 02:10:59.040
which is Washington DC, which is the U.S. government,

02:10:59.040 --> 02:11:01.600
that has basically over the last 30 years

02:11:01.600 --> 02:11:05.360
gone from a hyperpower that wins everywhere without fighting

02:11:05.360 --> 02:11:07.680
to a declining power that fights everywhere without winning.

02:11:08.960 --> 02:11:12.320
Okay, like just literally burn trillions of dollars doing this,

02:11:12.320 --> 02:11:14.480
take maybe the greatest decline in fortunes

02:11:14.480 --> 02:11:16.400
in 30 years and maybe human history.

02:11:16.400 --> 02:11:18.400
Not even the Roman Empire went down this fast

02:11:18.400 --> 02:11:21.440
on this many power dimensions this quickly, right?

02:11:21.440 --> 02:11:25.040
So giving that guy, let's trust him.

02:11:25.040 --> 02:11:27.840
That's just people running an old script in their heads

02:11:27.840 --> 02:11:28.880
that they inherited.

02:11:28.880 --> 02:11:31.200
They are not thinking about it from first principles that

02:11:31.200 --> 02:11:34.400
this state is a failure, okay?

02:11:34.400 --> 02:11:35.600
And like how much of a failure it is,

02:11:35.600 --> 02:11:36.960
you have to look at the sovereign debt crisis,

02:11:36.960 --> 02:11:39.360
you have to look at graphs that other people aren't looking at,

02:11:39.360 --> 02:11:44.320
but like, you know, the domain of what Blue America can regulate

02:11:44.320 --> 02:11:48.720
is already collapsing because it can't regulate Russia anymore.

02:11:48.720 --> 02:11:50.240
It can't regulate China anymore.

02:11:50.240 --> 02:11:52.160
It's less able to regulate India.

02:11:52.160 --> 02:11:54.720
It's less able even to regulate Florida and Texas.

02:11:54.720 --> 02:11:56.720
States are breaking away from it domestically.

02:11:56.720 --> 02:11:58.000
So this gets to your other point.

02:11:58.000 --> 02:12:02.640
Why is the tribal lens not something that we can put in the back,

02:12:02.640 --> 02:12:04.800
we have to put in the absolute front?

02:12:04.800 --> 02:12:06.880
Because the world is retribalizing.

02:12:07.680 --> 02:12:11.760
Like basically your tribe determines what law you're bound by.

02:12:11.760 --> 02:12:15.440
If you think you can pass some policy that binds the whole world,

02:12:15.440 --> 02:12:18.480
well, there have to be guys with guns who enforce that policy.

02:12:18.480 --> 02:12:20.880
And if I have guys with guns on their side that say,

02:12:20.880 --> 02:12:23.280
we're not enforcing that policy, then you have no policy.

02:12:23.280 --> 02:12:24.880
You've only bound your own people.

02:12:24.880 --> 02:12:26.240
Does that make sense, right?

02:12:26.240 --> 02:12:31.440
And so Blue America will probably succeed in choking the life out of AI

02:12:31.440 --> 02:12:32.800
within Blue America.

02:12:32.800 --> 02:12:35.440
But Blue America controls less and less of the world.

02:12:36.080 --> 02:12:37.920
So it'll have more power over fewer people.

02:12:38.880 --> 02:12:41.440
I can go into why this is, but essentially, you know,

02:12:42.160 --> 02:12:44.400
a financial Berlin Wall is arising.

02:12:44.400 --> 02:12:46.960
There's a lot of taxation and regulation

02:12:47.520 --> 02:12:51.120
and effectively financial repression de facto confiscation

02:12:51.120 --> 02:12:53.680
that will have to happen for the level of debt service

02:12:53.680 --> 02:12:55.120
that the US is being taking on.

02:12:55.120 --> 02:12:58.160
OK, just there's one graph just to make the point.

02:12:58.160 --> 02:13:01.200
And if you want to dig into this, you can.

02:13:02.000 --> 02:13:06.000
But the reason this impacts things is when you're talking about AI safety,

02:13:06.000 --> 02:13:08.080
you're talking about AI regulation.

02:13:08.080 --> 02:13:10.640
You're talking about the US government, right?

02:13:10.640 --> 02:13:12.960
And you have to ask, what does that actually mean?

02:13:13.680 --> 02:13:18.080
And it's like, in my view, it's like asking the Soviet Union in 1989

02:13:18.080 --> 02:13:20.000
to regulate the internet, right?

02:13:20.000 --> 02:13:22.560
That's going to outlive, you know, the country.

02:13:22.560 --> 02:13:25.280
US interest payment on federal debt versus defense spending.

02:13:25.280 --> 02:13:26.960
The white line is defense spending.

02:13:26.960 --> 02:13:27.840
Look at the red line.

02:13:27.840 --> 02:13:29.200
That's just gone absolutely vertical.

02:13:29.200 --> 02:13:30.320
That's interest.

02:13:30.320 --> 02:13:33.440
And it's going to go more vertical next year

02:13:33.440 --> 02:13:35.840
because all of this debt is getting refinanced

02:13:36.720 --> 02:13:37.840
at much higher interest rates.

02:13:37.840 --> 02:13:39.600
That's why look at this.

02:13:39.600 --> 02:13:41.920
You want you want AI timelines, right?

02:13:41.920 --> 02:13:43.680
The question for me is DC's timeline.

02:13:44.240 --> 02:13:45.920
What is DC's time left to live?

02:13:46.560 --> 02:13:49.920
OK, this is the kind of thing that kills empires

02:13:50.000 --> 02:13:53.440
and you either have this just go to the absolute moon

02:13:54.000 --> 02:13:56.480
or they cut rates and they print a lot.

02:13:56.480 --> 02:13:59.920
And either way, you know, the fundamental assumption

02:13:59.920 --> 02:14:03.040
underpinning all the AI safety, all the AI regulation work

02:14:03.040 --> 02:14:06.720
is that they have a functional golem in Washington DC

02:14:06.720 --> 02:14:08.240
where if they convince it to do something,

02:14:08.240 --> 02:14:11.360
it has enough power to control enough of the world.

02:14:11.360 --> 02:14:12.720
When that assumption is broken,

02:14:14.000 --> 02:14:18.000
then a lot of assumptions are broken, right?

02:14:18.000 --> 02:14:21.600
And so in my view, you have to you must think

02:14:21.600 --> 02:14:23.280
about a polytheistic AI world

02:14:23.920 --> 02:14:26.640
because other tribes are already into this.

02:14:26.640 --> 02:14:29.120
They're already funding their own, right?

02:14:29.120 --> 02:14:30.800
The proliferation is already happening

02:14:31.520 --> 02:14:33.120
and they're not going to bow to blue tribes.

02:14:33.120 --> 02:14:36.960
So that's why I think the tribal lens is not secondary.

02:14:36.960 --> 02:14:40.080
It's not some, you know, totally separate thing.

02:14:40.080 --> 02:14:42.800
It is an absolutely primary way in which to look at this.

02:14:42.800 --> 02:14:44.880
And in a sense, it's almost like a, you know,

02:14:44.880 --> 02:14:46.800
in a well done movie.

02:14:47.520 --> 02:14:50.640
All the plot lines come together at the end.

02:14:51.760 --> 02:14:52.560
Okay.

02:14:52.560 --> 02:14:54.560
And all the disruptions that are happening,

02:14:54.560 --> 02:14:57.200
the China disruption, the rise of India,

02:14:57.200 --> 02:15:00.240
the rise of the internet, the rise of crypto,

02:15:00.240 --> 02:15:02.880
the rise of AI and the decline of DC

02:15:02.880 --> 02:15:04.560
and the internal political conflict

02:15:05.120 --> 02:15:06.640
and, you know, various other theaters

02:15:06.640 --> 02:15:10.240
like what's happening in Europe and, you know, and Middle East.

02:15:10.240 --> 02:15:13.360
All of those come together into a crescendo of,

02:15:13.360 --> 02:15:16.240
oh, there's a lot of those graphs are all having the same time.

02:15:16.960 --> 02:15:19.520
And it's not something you can analyze by just, I think,

02:15:19.520 --> 02:15:21.440
looking at one of these curves on its own.

02:15:21.440 --> 02:15:22.960
I think that's a great note to wrap on.

02:15:22.960 --> 02:15:26.080
I am always lamenting the fact that so many people

02:15:26.080 --> 02:15:29.040
are thinking about this AI moment

02:15:29.040 --> 02:15:32.400
in just fundamentally too small of terms.

02:15:32.960 --> 02:15:37.120
But I don't think you're one that will easily be accused of that.

02:15:37.120 --> 02:15:40.560
So with an invitation to come back

02:15:40.560 --> 02:15:42.640
and continue in the not too distant future,

02:15:42.640 --> 02:15:45.680
for now, I will say apology, Srinivasan.

02:15:45.680 --> 02:15:47.840
Thank you for being part of the Cognitive Revolution.

02:15:48.960 --> 02:15:50.640
Thank you, Nathan. Good to be here.

02:15:50.640 --> 02:15:52.640
It is both energizing and enlightening

02:15:52.640 --> 02:15:53.920
to hear why people listen

02:15:53.920 --> 02:15:56.320
and learn what they value about the show.

02:15:56.320 --> 02:16:01.760
So please don't hesitate to reach out via email at TCR at turpentine.co

02:16:01.760 --> 02:16:05.040
or you can DM me on the social media platform of your choice.

02:16:06.080 --> 02:16:08.480
Omnike uses generative AI

02:16:08.480 --> 02:16:11.360
to enable you to launch hundreds of thousands of ad iterations

02:16:11.360 --> 02:16:14.880
that actually work customized across all platforms

02:16:14.880 --> 02:16:16.160
with a click of a button.

02:16:16.160 --> 02:16:18.720
I believe in Omnike so much that I invested in it

02:16:18.720 --> 02:16:20.320
and I recommend you use it too.

02:16:21.040 --> 02:16:27.280
Use COGRAV to get a 10% discount.

