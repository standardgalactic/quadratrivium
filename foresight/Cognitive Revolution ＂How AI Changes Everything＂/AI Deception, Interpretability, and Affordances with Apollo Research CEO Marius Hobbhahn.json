{"text": " So the more pressure we add, the more likely the model is to be deceptive. So kind of in the same way in which a human would act, it also acts. You know, removing pressure and adding additional options will very quickly decrease the probability of being deceptive. Open source has been really good so far in many, many ways. It has been very positive for society, right? I think a lot of ML research could not have happened without open source. A lot of safety research could not have happened with open source. At some point, the system is so powerful that you don't want it to be open source anymore. In the same way in which, you know, I don't want to open source the nuclear codes or like, you know, literally the recipe to build most viral pandemic or something. The labs maybe have the incentive to not say the worst things they found because otherwise they may lose their contract. So you need something like the UKASF Institute or the USASF Institute. Make sure that there is a minimal set of standards that all the auditors have to adhere to. Hello and welcome to The Cognitive Revolution, where we interview visionary researchers, entrepreneurs, and builders working on the frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas and together we'll build a picture of how AI technology will transform work, life, and society in the coming years. I'm Nathan LaBenz, joined by my co-host Eric Torenberg. Hello and welcome back to The Cognitive Revolution. Today, my guest is Marius Haban, founder and CEO of Apollo Research, a nonprofit AI safety research group that is working to understand both how AI systems behave and why. Their approach combines exploratory and hypothesis-driven testing, fine-tuning experiments, and interpretability research. And as you'll hear, they place special emphasis on the potential for AI systems to deceive their human users. In this conversation, we look first at Apollo's starting framework for their work, which emphasizes the importance of affordances in AI systems. That is, through what tools, actuators, or other means can the system affect the broader world? And they also introduce a number of new conceptual distinctions meant to help people have more precise and productive conversations about these nuanced topics. Then in the second half, we look at their first research result, which demonstrates, to my knowledge, for the first time in a realistic, unprompted setting that GPT-4, when put under pressure, will sometimes take unethical and even illegal actions and then go on to lie to its users about what it did and why. This is an important result, demonstrating that while the risk from AI systems may start with and may even be dominated by intentional human misuse, the models themselves can also misbehave in unexpected ways. As an aside, since I told my behind-the-scenes GPT-4 red team story a few weeks ago, a number of people have reached out to ask me how they too can get involved with red teaming projects. Unfortunately, as commercial competition and secrecy both continue to ramp up across the space, I don't see as many open calls for volunteer red teamers as I used to, certainly not for unreleased frontier models. Instead, the field is becoming more professionalized, with all the leading labs, as well as the data companies like Scale AI, plus the independent auditing organizations like Apollo, ArchieVals, now known as Meter, Palisade, and also AI Forensics, all actively hiring research scientists and engineers in this area. So does that mean that there's no longer a role for the independent hobbyist red teamer to play? On the contrary, there is a ton left to discover even on publicly released models, and the best way to break into the field is to demonstrate your ability to discover new phenomena. Importantly, the work we cover in this episode could have been done by anyone with an open AI account, a knack for prompting, and just a tiny bit of coding know-how. No special access or advanced machine learning techniques were required, just a lot of curiosity. With that in mind, if you want to get into this line of work but aren't sure where to start, I encourage you to reach out. I'll be happy to help brainstorm or refine your project ideas, and I can also help connect you with folks at the top companies who do sometimes provide API credits to independent researchers working in this area, if and when you can achieve a meaningful result. As always, we appreciate the time that you spend listening to The Cognitive Revolution, and we hope it's a valuable guide to the AI era. If you feel that it is, we would love a review on Apple Podcasts or Spotify, and we of course encourage you to share the show with your friends. Now, here's my conversation on frontier AI safety work with Marius Habhan of Apollo Research. Marius Habhan, founder and CEO of Apollo Research, welcome to The Cognitive Revolution. Hey, thanks for having me. I am very excited to have you. So, regular listeners of the show will know that I'm a big believer in the importance of hands-on testing of what AI systems can do, and also that I have been pretty enthusiastic consumer of the news when some of the leading labs have made public commitments to allow organizations outside of their own teams to look at the systems that they're building before they get deployed. And so, your work with Apollo Research, which is trying to build, as I understand it, an organization to meet that need and actually work with those leading labs in part, at least, on understanding the systems that they're developing before they get to widespread deployment, I think is super interesting, and I'm very excited to unpack the details of it with you. Maybe for starters, you want to just kind of give us the quick overview on Apollo Research, like how you decided to set out to found it. I'm interested a little bit in the timeline of how that related to some of the commitments that the labs have made and what you guys are trying to do in the big picture. So, I think on a high level, it's sort of trying to understand what is going on in AI systems. And the reason for this was, or still is, in fact, yeah, I basically think right now, we just lack information to make good decisions. There's loads of uncertainty that we have about what could go wrong, whether we are already at a point where things go wrong, or how far away we are from these points. And yeah, we're trying to reduce this uncertainty. And this is mostly through research, auditing, and governance. And on the research side, it's really split between interpretability and behavioral evils, half-half. But in the long run, we really want to merge them both, because I basically think what we need in the long run is both a mixture of behavioral and interpretability evils, so that we can really understand what the model is doing, and then also why it is doing this in the first place, because each of them individually seems somewhat insufficient. And yeah, maybe to go into the origin story, it has actually nothing to do with the commitments of the different labs. It was mostly that at the beginning of this year, I kind of felt like I had a pretty clear picture of what is lacking in the current space with deceptive alignment, and evaluating deceptive alignment, or models for deceptive alignment in the first place. And interpretability and evils just seemed like the obvious things to do. So in the beginning, we basically set out to do mostly research. And only then, over time, we realized, hey, this is something that should be applied in the real world as soon as possible, because systems are getting better all the time. And we may actually hit this point fairly soon where models are already about at the threshold of deceptive, of capabilities for deceptive alignment. And then there is a small part in the organization that is governance, which originally we also didn't really intend to do for the first two years or something, because we thought we really need to understand all the research very well before we can talk to the people in governments and decision makers and lawmakers, because otherwise we're telling them things we aren't super confident in. And then lots of things happen. Governments and lawmakers actually got interested in AI and AI safety in particular. And then when we talked to them, we realized the difference. We are very, very well placed to talk about these things, because if you have thought about them in the background for like six, seven, years, and then specifically about some topic for six months or so, you are among the world's experts. And this is kind of more like a reflection of the state of how bad it is about AI safety, where people in my position are actually sort of accidentally becoming the experts, rather than people with tens and 20 years of experience, because there aren't a lot of people in the world who have thought about AI safety for more than a couple of years, if at all. Yeah, I can definitely relate to that sort of accidental expert status. I never expected to be where I am and doing the things that I'm doing. But yeah, the whole AI field in some ways is kind of the dog that caught the car. I always kind of come back to that metaphor where it's like we were just trying to build a bit more powerful AI, and all of a sudden we built like a lot more powerful AI, and now we really kind of have to figure out what to do with it. So even a little bit of advanced planning is better than, or a little bit of advanced thought is a lot better than where most people are starting. Had you seen, when you actually started the organization, had you seen GPT-4, or were you basing this decision on just what was public at the time? Only what was public at the time. So the decision was made in February 2023, or at least sort of my internal commitment was made to this. I'm not sure whether GPT-4 was public already at the time. Not quite, right? It was March. So no, it was independent of GPT-4. Yeah, I always think that's interesting just because GPT-4 was such a wake-up moment for so many people, and certainly I would include myself in that. I was already extremely plugged into what was going on and using it and fine-tuning tons of models on the open AI platform in particular, but then it was like, whoa, this thing is next level. It's not slowing down. We've gone from sort of, I can put a lot of elbow grease in and get a fine-tuned model to do a particular task, which already I thought was going to be economically transformative to, I don't even need to do that, that I could just ask for a lot of these tasks and get like pretty good zero-shot performance. For me, that was the moment where I was kind of like, okay, this is going from a tool that I am really excited to use and having a lot of fun using to something that seems like a force that needs to be understood from all angles. So let's unpack the perspective that you are bringing to this. I would encourage folks to look up these papers that we'll discuss and read them for themselves as well, but on the website, you've got two recent publications. One is kind of a framework for organizing the work that you're going to do, and then the other is like a very detailed in the weeds investigation of a particular AI behavior, namely deceiving the user, which I think is a super interesting and important one to study. But let's maybe just start with the big picture, like organizing the thoughts. I get the sense that you think, again, well, you've kind of said this, and the paper certainly reflects it, that there are like a lot of big questions that remain unanswered. So how do you structure your approach to this topic given all the uncertainty that exists? Yeah, maybe to give a little bit of context. So this is only one paper of many in this space, and there is, I think earlier this year, there was a really big one called Model Evaluation for Extreme Risk, which we at Apollo definitely thought was a pretty good paper. And they're sort of pointing out many of the very reasonable and important steps, or reasonable principles for external auditing, something like ramp up the auditing before you ramp up the exposure to the real world and do this ahead of the curve, so to speak. But when we read the paper, we felt a little bit like, this makes sense for the current capabilities and sort of how current models are being built. But if we think ahead of what the next couple of years should look or not should, could look like, then yeah, there are loads of open questions. And we were trying to understand how do they fit into this framework? And because we internally were trying to make sense of this in the first place. So just to give you a couple of them, what happens if your model has the ability to do online learning? How often do you have to audit it? Should you re-audit it during the online learning? If yes, how often? What if you give the model access to the internet or to a database or to anything like this? Yeah, I think a model with and without access to the internet is basically two very different models. The one with access to the internet is just so much more powerful if you can use it even on a very basic level. So yeah, it feels like if you give your model affordances like this, you kind of have to rethink how dangerous it is and where the danger comes from because it suddenly is like a totally different threat model potentially. And so what we did for the paper and really the credit should go to Lee Sharkey here, who is my co-founder, who has done most of the hard work or if not all of the hard work for this paper. And so what we were doing is thinking from first principles. Where does the risk come from and what changes to the AI system do create new risks? And then basically the answer is, well, we have to audit wherever risk is created. And then the more we looked into this, the more realized, well, there are actually a lot of places where new risk comes into the system, at least potentially, and therefore we are audits, at least in an ideal world should happen. There are obviously some constraints. But I think if we think about where are we five years from now, then I think, yeah, if there is actually a big auditing ecosystem around this, then there will be very, very many different organizations auditing really different places. And then the other point of the paper was just to define many concepts and create the language to discuss all of these things because we had sort of many internal discussions where we were like, oh, the thing we mean is this. And then we had an example, and then we kind of needed a name for it. And there wasn't really a name. So we decided, okay, let's define all of the relevant terms for this, and then sort of have a language to talk about this in the first place. Hey, we'll continue our interview in a moment after a word from our sponsors. Real quick, what's the easiest choice you can make? Taking the window instead of the middle seat, outsourcing business tasks that you absolutely hate? What about selling with Shopify? Shopify is the global commerce platform that helps you sell at every stage of your business. Shopify powers 10% of all e-commerce in the US, and Shopify is the global force behind Allbirds, Rothy's and Brooklyn and millions of other entrepreneurs of every size across 175 countries. Whether you're selling security systems or marketing memory modules, Shopify helps you sell everywhere, from their all-in-one e-commerce platform to their in-person POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the past at the companies I founded, and when we launched Merch here at Turpentine, Shopify will be our go-to. Shopify helps turn browsers into buyers with the internet's best converting checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell more with less effort thanks to Shopify magic, your AI-powered All-Star. With Shopify magic, whip up captivating content that converts from blog posts to product descriptions. Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify magic is free for every Shopify seller. Businesses that grow grow with Shopify. Sign up for a $1 per month trial period at Shopify.com slash cognitive. Go to Shopify.com slash cognitive now to grow your business no matter what stage you're in. Shopify.com slash cognitive. So let's dig in in a little bit deeper detail. I like the premise that you set out within the paper, which is to work backward from AI effect in the real world and try to imagine where are these effects going to happen and then how can we get upstream of that and help shape them in a positive way. I would be interested to hear you kind of describe that backward chaining process in a little bit more detail. And then I thought some of your concepts also were really helpful clarifications and distinctions. So maybe you can highlight some of the ones that you think are most useful that you'd like to see get into broader circulation as well. So basically, we started from, okay, the system, the AI system will interact with the world in a particular way. And then, you know, there are many, many different ways in which it can interact with the world. And then there's sort of like a whole chain of things that have had to happen until the model can act interact with the world in this particular way. So, you know, maybe it has been fined you and maybe it has been given access to the internet before that it has to have been trained before that there has to have been the decision that this model should be trained in the first place. And so the question is like, what are the kind of important decisions at all of these different points in time? And how then can we ensure that people actually make decisions that will lead to outcomes at the end of the chain, such as the model or the system interacts with the world in a safe manner. And this is maybe like the first distinction that is worth pointing out, or like the reason why I'm correcting myself all the time is there's really a difference between AI model and AI system. The AI model really, and this is not something we came up with, this already exists before. But I think it's worth pointing out and sort of getting in like really hammering into people's head when they think about AI. So the AI model is just the weights maybe behind, you know, like behind an API, but even with the API, it's kind of already a system. And the system then is sort of the weights plus everything around it. So there could be scaffolding, there could be access to tools, there could be content filters, this could even be like just an API, retrieval databases, etc. Like really the full package, where you say, okay, you know, there's there's like stuff around the mod around the weights that increase the capabilities of the model and menu, or at least change the capabilities of the raw model in some sense, not necessarily always increasing filters, for example, may decrease it. And then there are sort of other weird ways, or like, yeah, once you think about this, there are sort of a couple of other concepts that that feel important to clarify. Because when people say capabilities, this can mean very different things, right? And so we categorize this into three different classes. The first one is absolute capabilities, which we think of basically the hypothetical capabilities given any set of affordances. So if you have GPD for without the internet, right, then in the space of absolute capabilities would be a GPD for with internet. So or like things that this model could do. So the question is like, if we give additional things to the system, how big is the space of actions it could take. So, you know, and then obviously, there's a question of like, how imaginary do we get here, you know, like, does it has does it get access to like, you know, a Dyson sphere, or does it get access to like, a government or something like this. But but yeah, like it basically points out sort of the, what could this model do if we gave it a lot of things, everything that we can basically think of. Thinking about this in the first place only makes sense for models that have become more general, like the GPT is, because you know, for an MNIST filter, like for an MNIST classifier, this doesn't make any any sense, like an MNIST classifier plus internet is like is exactly as capable as just the MNIST classifier itself. But yeah, for systems that are more general, suddenly you have this difference between things that only the system can do, or like the basic system plus things that you could do hypothetically with a lot of additional affordances. Then the second one is contextual capabilities, which is things that are achievable in the context right now. So for example, with chat GPT, you can enable it to have access to tools, and then you can browse the web. And this is something that it can do right now, you don't have to add anything on top of this. And this is sort of this is sort of the smallest category of things, which you can do without any additional modification and then reachable capabilities is contextual capabilities, plus achievable through extra effort. So for example, this could mean chat GPT itself may not have access to a calculator. But if it has access to the internet, it can like Google and then find a calculator and then use that calculator. And so it's sort of a two step process, right, where it has to use one affordance or capability to then achieve another. And so this is what we call reachable capabilities. And yeah, so the reason the reason why we are making this all of this differentiation, even though it sounds maybe a little bit too much in the weeds is when people talk about capabilities and regulating capabilities and designing laws for capabilities, the question is, which ones, right? Do you mean the contextual capabilities? So the ones that the model has literally right now, or the reachable capabilities, so which the model could reach with additional effort or the absolute like, the maximum potential space of capabilities. And, you know, right now this may sound like we're too much in the weeds. And but I think in a few months, this will sound very, very relevant suddenly, because the models will be more capable. And then they will actually be able to just like smart enough to use the internet to to like find additional tools that they can then use, or or like convince someone to give them access to a shell. And then use that because they're already like, you know, they can learn it in context or they know it anyways. And at that point, really, the question is, what should the auditors audit for? Which capabilities? And and that becomes like pretty quickly, like a very, very big space of things, right? So like, if the auditor not only has to think about what kind of tools do you give the AI, but also what kind of tools could the AI get access to through some means? Suddenly, you have this whole space of like thousands of things it could do. It's really a question of like, or like a tradeoff between what is what is plausibly doable in the real world versus how much risk can we actually mitigate? And I'm honestly very unsure about about the like where we're heading at this point. So just to riff on and kind of emphasize some of the the value that I see in in some of these distinctions, I think it's helpful to clarify the difference between a model and a system. I think there is a tremendous amount of confusion online. And to my degree, and I've probably even contributed to some of it at times where people are like, you know, Chad GPT was doing this for me, and now it's not anymore. And I've sometimes said like, well, they haven't updated the model, so it probably hasn't changed that much. And I think what I've maybe neglected in some of those moments is like, but they might have changed the system prompt, or, you know, as we're seeing, I mean, even just this last couple of weeks, there's been this really interesting phenomenon of the of GPT for getting quote unquote, lazier. And people are speculating that maybe that's because they feed the date into it. And it knows that we're in December, and it knows that people don't work as hard or as productively in December. And so maybe it's like kind of phoning it in, because it's like, imitating the broad swath of humans that it's seen like, you know, kind of work halftime in December or whatever. I've even seen some experiments, just in the last couple days that suggest that there might even be real truth to that. Who knows, I'd say that the question remains open. But there's a there is an important difference, you know, and it's worth getting clarity on the model itself with static weights, not changing versus even just a system prompt that can perhaps have, you know, even unexpected drift along the dimension of something as seemingly benign as today's date. So that's important to keep in mind. The levels of capabilities, I think, are also really interesting. And I want to ask one kind of I have a couple questions on this, but I think I have a clear sense of what is meant by contextual. What can it do now, given the packaging, right? What what can GPT forward do in the context of chat GPT, where it has a code interpreter, and it has browse with Bing, and it has the ability to call Dolly three to make an image, and probably a couple other things that I'm not even remembering, you know, plugins perhaps as well, right, which obviously GPT is which proliferates, you know, all the affordances, all that much more. On the other end, I feel like I sort of understand absolute, which is like a theoretical max. Could you give me a little like how do I understand reachable as as kind of between those like what's what's the distinction between reachable and absolute? Yeah, so so maybe maybe one way to think of it is like, the contextual capabilities are the ones kind of that a user explicitly gave it. And then the reachable ones are those that may also be reachable without the user even having thought about that the model actually will will use them, right? So if you say, you know, like if the model would be able to browse the web, like entirely on its own, which I'm not sure it currently can do or like what exactly the restrictions on search with Bing are. But if it was able to do that, right, you may not you may not have realized that it has a reachable reachable capability through the internet of like firing up a shell somewhere, or like renting a GPU, and and like doings or like running a physics simulation through a like an online physics simulator, if that if that's something that's available. And so so these are this is sort of like how which tools can it reach through the contextual ability capabilities that it already has given by you or was been has been given by you. Gotcha. Okay. So like solving a capture by hiring an upward contractor for exactly to take one infamous case. So, okay, here's a challenging question. But, and I don't necessarily expect an answer, but maybe you can venture an answer or you could just kind of describe how you begin to think about it. What would you say are the absolute capabilities of GPT four? Yeah, very unclear. So I think they're definitely they're not infinite. As in, you know, like even with extremely good scaffolding and and access to the internet and many other things, I think people haven't been able to, you know, get it to do economically valuable tasks at the level of a human, at least for like long time spans, for example. So, you know, the question is obviously like, is this, you know, are we just too bad? And have we not figured out the right prompting yet and the right scaffolding and so on? Or, or is this just a limitation of the system? And my current guess is, like, there is probably a limit to the absolute capabilities. And it's probably lower than like what a human can do. But we're not that far away from it. So, you know, I think with an additional training with additional, like specifically LM, like training that is more goal direct or makes it into more goal directed and an agent and better scaffolding, I think there will be ways in which the absolute capabilities could increase quite a bit in the near future. Yeah, does this make sense? Yeah, I mean, it's hard, right? I certainly listeners to the show will know from repeated storytelling on my part that I was one of the volunteer testers of the GPT-4 early model back in August, September of last year. And I really kind of challenged myself to try to answer that question, you know, independently, like, what is the theoretical max of what this thing can do? How much could it like break down big problems and delegate to itself? And it basically came to the same conclusion that you did, which is like, doesn't seem like it can do really big tasks. I mean, again, it's confusing, right? Because then you could also look at the dimension of how big the task is versus how would you break it down? Just in the last week, I've been doing something for a very sort of mundane project, but actually using GPT-4 to run evals on other language model output, I have found that if I have like 10 tasks, 10, you know, dimensions of evaluation, and I ask it to run all of those, it is now capable of following those directions and executing the tasks one by one. But the quality kind of suffers. It sort of makes mistakes. It sometimes muddies the tasks a little bit between each other. And it's definitely like not at a human level given 10 tasks to do in one generation. On the flip side, though, if I take it down to one task per generation, which I didn't want to do because that will increase our cost and latency and just is less convenient for me, but then it kind of pops up to, honestly, I would say pretty much human level, if not above. So there's interesting dimension. I guess it seems pretty the sort of magnitude of the task seems like a pretty important dimension for evaluating a question like absolute capabilities, right? It's like, if it's a super narrow thing, it has it's like more it's, it's capable of some pretty high spikes. But if it's a, if it's a big thing, it kind of gets lost. Would you refine that characterization at all? Yeah, yeah, I'm not sure how to think about it, honestly. So I think of absolute capabilities really more of a sort of theoretical bound that we could, that we're probably not going to approximate in practice, even if we test like a lot, a lot. And then the, then like breaking it down into different tasks, I'm not sure I feel like this is a different capability then, right? Like you're sort of the capability of doing 10 things at once is a different thing than the capability of doing one thing 10 times like 10, 10 diff different things, but one by one. So yeah, I would say it's basically you're talking about different capabilities then, at least in this framework. Hey, we'll continue our interview in a moment after a word from our sponsors. If you're a startup founder or executive running a growing business, you know that as you scale, your systems break down, and the cracks start to show. If this resonates with you, there are three numbers you need to know, 36,000, 25 and one, 36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system, streamline accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more with less, close their books in days, not weeks, and drive down costs. One, because your business is one of a kind, so you get a customized solution for all your KPIs in one efficient system with one source of truth. Manage risk, get reliable forecasts, and improve margins. Everything you need, all in one place. Right now, download NetSuite's popular KPI checklist, designed to give you consistently excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive. Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work, customized across all platforms with a click of a button. I believe in Omnike so much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount. Yeah, and it is not that good at decomposing the tasks. I've also kind of experimented a little bit with like, can you give it that list of 10 tasks and can it break them down and self-delegate with an effective prompt? It's like maybe a little bit closer there, but still not getting nearly as good results as if I just roll it my sleeves and do the task decomposition. You mentioned that you expect this frontier to obviously continue to move. One way to ask the question is, what is Q-star? A more sensible way to ask the question is, do you have a set of expectations for how the capabilities frontier will move? I definitely look at things like OpenAI's publication from earlier this year where they gave, started to give denser feedback on kind of every step of the reasoning process and they achieved some state-of-the-art results on mathematical reasoning that way. And when I think about affordances and I think about the failure modes that I've seen with these GPT-4 agent type systems, I think, man, you apply that to browsing the web and using APIs and it seems like that stuff is ultimately a lot less cognitively demanding than pure math. It seems like we probably are going to see, and I would guess that it's maybe already working pretty well. AGI has been achieved internally, I don't know about AGI, but I would expect that some of this stuff is already pretty far along in kind of internal prototyping. But how does that compare to what you would expect to see coming online over the next few months? Yeah, it's obviously hard to say and I can only speculate. I think on a high level what I would expect the big trends to be and also what we are kind of looking forward to evaluating is LM agents. I think this is like pretty agreed upon. From first principle, I think it also makes sense. It's just like, where does the money come from? It is from AI systems doing economically useful tasks and often economically useful tasks just require you to do things independently, being goal directed over a longer period of time. And the longer a model can do things on its own, the more money you can squeeze out of it. So I definitely think just from financial interests, all of the AGI labs will definitely try to get in more, more agentic ways. How far they have come, I don't know. But yeah, I expect that next year we will see quite some surprises. Then multimodality is the other one where, yeah, I think people kind of like over the last couple of years with more and more multimodal models, people just realized it's not that different from just training text. You plug in the additional modalities, you change your training slightly, but it's not much more than that. Obviously, it's obviously hard in practice. There's a ton of engineering challenges and so on. But on a conceptual level, there isn't any big breakthrough needed. So people will just add more and more modalities on bigger and bigger models and train it all jointly and to end. And it kind of just works. And then tool use is the last one. And that I think people, yeah, people actually were quite surprised by how like, quote unquote, easy it was to get to this level. So yeah, I think when people realize like, oh, these language models are already pretty good, like how fast do they learn how to use any tool we can think of? And they were surprised by how fast they learned the tools. And now it's mostly a question of sort of really baking in the tool into the model in a way that it's like very robustly able to use the model rather than just a little bit or just showing that it's it sometimes work. But yeah, I mean, you know, like I think if you have an LM agent that is multimodal, and that has very good tool use, like I'm not quite sure how far you are away from AGI, right? Like at that point, you kind of have almost all of the ingredients ready. And then it's really just a question of how robust is the system. So yeah, I think these are the trends we see right now. And this is also why many people in the big labs have very, very short timelines, because they can think like two years ahead and sort of where this is going, or maybe even just one year ahead, I don't know. When you talked about the surprise, like people were surprised at how easy it was to get tool used to work. Are you referring to people in the leading, you know, the obvious, the usual suspects of leading developers? It's hard to say. I mean, I can only speculate on this. But you know, the tool former paper was like three months, or like was published three months before open AI just released their tool use. And I mean, they probably had been working on this before, but still, you know, like the from from having the scientific insight to to like publishing this and releasing this in the real world, I think there just was less work involved than is, or is typical for most of the bigger AI, like development cycles, I could be wrong on this. This is this is more here say so yeah, take it with a grain of salt. Yeah, it seems right to me as well. And I agree with you the emphasis on multi modality as a new unlock makes a ton of sense, even just in this kind of agent paradigm of, you know, can I browse the internet or whatever. I've done a lot of browser automation type work in the past. And the difference between having to grab all the HTML that, you know, is often these days, like extremely bloated and, you know, kind of semi auto generated and in some cases, like deliberately, you know, generated to be hard to parse, you know, from like, you know, the Googles on Facebook, like they don't want you scraping their content. So they're kind of not making it easy on the, on the browser automators. The difference between that and just being able to like look at the screen and understand what's going on, you know, you kind of put it through a human lens and you're like, yeah, it's a hell of a lot easier to see what's going on on the screen than to like read all this HTML and sure enough, you know, the models kind of behave similarly. I remember for me looking at the flamingo architecture when that was first published, like, I think April of 2022. So you're a little more than a year and a half ago now. And just thinking like, Oh, my God, if this works, everything's going to work. You know, it was like, they had a language model frozen. They had kind of stitched in the visual stuff and like kind of added a couple layers, but it really looked to me like, man, this is tinkering stage. And it's just working. So I, you know, like you, I don't want to dismiss the fact that there's obviously a decent amount of, I'm sure like labor and probably at times tedious labor that has to go into overcoming the little, you know, little stumbling points. But conceptually, it is amazing how simple a lot of these unlocks have been over the last couple of years. And you see this too, and just like the pace at which people are putting out papers, you look at like the one team that I followed particularly closely is the team at Google DeepMind that is doing medical focused models. And they're good for one, like every three months, you know, and they're like significant advances where it's like, Oh yeah, this time we added multimodality. And this time we like tackled differential, you know, diagnosis. And like, again, it's, it seems like there, there's not a lot of time for failures between these successes. So it does seem like, yeah, we're not at the end of this, by any means, just yet. A lot is coming at us. It's going to presumably continue to get weird. You're trying to push both as much as you can, the understanding of what can the systems do, you know, as users, what is it, what are their limits? And then at the same time, you're trying to dig into the models. And this is the interpretability side and figure out like, what's going on in there? And, you know, can we kind of connect, you know, the external behaviors to these like internal states? So tell us about that side of the research agenda as well. Yeah, so on the interpretability side, like, my thinking is basically, it would be so great if interpretability work, right, it would make so many questions easier. Like, if you ask questions on accountability, right, if you have causal interpretability method, you would be able to just, you know, tell the judge if the model would have, we would have changed these variables, the model would have acted in differently in this way. And we could just basically solve that biases, probably also like, you know, social biases, much easier to solve, because you could intervene on them or like fix the internal misunderstandings and concepts. It's also extremely helpful for like, basically all of the different extreme risks, right, like it would be much easier to understand the internal plans and how it thinks about problems, how it approaches them and so on. And then it would also make iterations on alignment methods much, much easier, I think. As in, you know, let's say somebody says, oh, RLHF is, is like already working, we see this in practice, then, you know, you could use the interpretability tool. So test does, you know, does RLHF actually work? Or does it only like superficially like hide the problem or something like this? Or does it actually like deep down solve the root? And then I think my biggest sort of the biggest reason for me for focusing on interpretability in the first place is deceptive alignment, where, you know, models appear aligned to the outside and to the user. But internally, they actually follow different goals. They just know that you have a different goal. And therefore, like in order for you to think it is nice, they act in that way. And yeah, I basically think almost, not all, not almost all, but a lot of the scenarios in which AI goes really, really badly, go through some form of deceptive alignment, where at some point the model is seen as nice, and people think it is aligned, and people give it access to the internet and resources and like train it more and more and more and make it more powerful. But internally, it is actually pursuing a different goal. And it is smart enough to hide this true intention until it knows that it can sort of cash out and then follow on this on this actual goal without us being able to stop it anymore. And so yeah, that's what I'm really worried about. And interpretability obviously seems like one of the most obvious ways to test for deceptive alignment or like to at least investigate the phenomenon, because you know what it's thinking inside. There are still, you know, there are still some cases where deceptive alignment, where even with good interpretability tools, deceptive alignment could still somehow be a thing. But generally speaking, I think it would be much, much, much harder for the model to pull off. So right now, I think interpretability is just not at the, it's like, not practically useful. So, you know, we cannot use any existing interpretability tool and like throw it on GPT-3 or GPT-4 because none of them have like enough or developed enough that they give us insight that like really meaningfully change our minds. And so yeah, this is why, you know, our agenda is separate in the first place between behavioral evals and interpretability, despite us wanting to do them jointly in the long run. But they're given that there's such a huge gap on applicability. I think that this is definitely a problem that we're trying to mitigate here. And then the one question for me is also like, how hard will interpretability turn out to be? And there are, you know, various people have argued that interpretability will be extremely hard because models are so big and complicated. And therefore, it will be hard to enumerate, you know, all of the concepts and actually understand what the hell is going on inside. And I'm more of the perspective that, you know, I understand the reason why they think it's hard, but I also think there are many reasons to assume it's, it's going to be like doable. It's, if we put our minds to it as humanity, we'll probably figure it out. The primary reason I think is we have full, full interventional access to the model, right? We can see every activation, we can ablate everything we want. We have, you know, it's not just, it's not just observational studies, you can really intervene on the system. And generally speaking, I would say, as soon as you can intervene on the system, you can test your hypothesis very, very quickly. And you can iterate very fast. And so I think we will be able to figure out interpretability, you know, in the next couple of years to an extent where we can actually sort of say it is now useful on real world models, on frontier models. How expensive this is going to be, I don't know yet, but I think it will at least be technically feasible. Yeah, I've definitely updated my thinking a lot in that direction from a pretty naive, just kind of, you know, hey, it sounds really hard, black box problem, nobody knows what's going on in there to today, I would say, wow, you know, there's really a lot of progress. And it has, it is the progress of interpretability over the last, say, two years has definitely exceeded my expectations and given me a lot more, I wouldn't have maybe some sort of confidence, but you know, at least reason to believe that with some time, but not necessarily, you know, a ton of time that we really could get to a much better place in our understanding. So I'm with you on that. I have a number of follow up questions, I think, on this point. One, let's maybe just give the account for like why deception might arise in the first place. You can complicate, I'll give you a super simple version, you can refine it or complicate it. I usually kind of cite Ajaya on this, and you know, she has a pretty simple story of like what the model is trying to do, you know, what it is rewarded for in the context of an RLHF like training regime is getting a high feedback score from the user. And it probably becomes useful as a means to maximizing that score to model human psychology as an explicit part of how you're going to solve the problem, right? And I think we, you know, certainly humans do this with respect to each other, right? I ask you for something, you ask me for something, we interpret that not only as the extremely literal definition of the task, but also kind of have a sense for what does this person really care about? What are they really looking for? And we can incorporate that into the way that we respond. It certainly seems like the heavier you do, you know, the more emphasis you put on this kind of reinforcement learning from human feedback, the more likely the models are to start to create a distinction between, you know, the task as sort of narrowly, objectively scoped, let's say, and the kind of human psychology element that is going to feed into its rating. And then if you have that, you know, if you have that decoupling, then you have kind of potential for all sorts of misalignment, you know, including deception. How's that compared to the way you typically think about it? Yeah, I mean, I think the like this kind of version through RLHF is one potential path. I'm, yeah, I actually think the jury is still out there on this, like, you know, I definitely see the hypothesis and where it's coming from, but it could also just totally imagine that, you know, the training signal is sufficiently diverse, and it updates sort of sufficiently deep that RLHF kind of just does the thing we wanted to do without without the model becoming deceptive. I could also see that like the story in which it would become deceptive. I think, like, on a very high level, the way the reason I think why models would become deceptive is because at some point they will have long term goals, they will have something that they care about, like, more beyond the current episode, you know, beyond pleasing the user at this point in time. And then the question really is, and then I think there are like two core conditions under which, like, if the more they are fulfilled, the more likely the model is becoming deceptive, like how important is the this long term goal to the model itself, meaning how much does this goal trade off, for example, with other goals it has. So for example, if it cares if it cares a ton about something, then it's more likely to be deceptive with respect to this because it really wants to achieve this. And then secondly, how much do others care about me not or the AI not achieving this goal in the first place, something like contestedness, right? So for example, if I want to pick a flower and I care a lot about this, I don't need to be deceptive because nobody wants to stop me from picking that flower. If I want to be, you know, the president, a lot of people might not want me to be the president. And so in that case, it's very contested and I have a strong incentive to be deceptive about my plans because otherwise people would want to stop me. And then so now we're at a point where we have a system at least in our hypothetical scenario that has a long term goal and it's like, in the limit at least, you know, it cares about that goal and the goal may be somewhat contested. And then as long as it has situational awareness, it just feels instrumentally useful to be deceptive about it, like you said, right, to model other people and how they would think about it and then just react to this. And this is sort of, I think this is maybe this is like one of the core reasons why I'm so worried about this whole deception thing. Because it just feels like a reasonable strategy in a ton of situations from the perspective of like a consequentialist or rational, irrational actor, right? It's just like under specific conditions, people just naturally or like deception is just convergent people do it because it makes sense for them. And this is why we see it in like a ton of different systems, right? You see it in in animals where parasites are really deceptive with respect to their hosts, you see it in individual humans where, you know, they're deceptive with respect to their partners from time to time, for example, you see it in systems where, you know, like they're they they're trying to to gain the laws and be deceptive about this or to lie about this. And I think this is this is kind of like the whole or like a big part of the problem, it's just a it's like reasonable or sensible in many situations to be deceptive. From the perspective of the model, which is kind of what we want to prevent, right? So where do you think those long term goals come from? If it's, you know, is it just kind of a reflection of the general training goals? I mean, we, you know, we have kind of the canonical three H's. But I mean, honest is one of those, right? Hopeful, harmless, honest. Are you imagine is that is your understanding just that those are like fundamentally sort of intention and that the model will kind of have no choice but to develop tradeoffs between them? I mean, we can we can get into the tension between them in a second. But I think it's it's actually like the three H's I don't think will be, you know, they're not keeping me awake at night. I think it's more at some point, people want the model to do long term economic tasks. And for that, they give them long term goals or long term goals are instrumentally useful. So for many situations, I think it will just be useful to have long term goals or at least in like, to have instrumental goals, right? Something like, Oh, it makes because it is a long term task, it makes sense to first acquire money, and then use that money to do something and then use that third thing to achieve the actual goal. And so like, I think the models will just learn this kind of consequentialist and instrumental reasoning where they're like, okay, I first have to do X. And then I do this, and then I do the long term thing. And and once they're there, sometimes it just makes sense to be like, Okay, other people don't want me to do this. And therefore, I hide my actual intention. And I act in ways that make me look nice, despite not being nice. Yeah, but yeah, I think like a lot of the a lot of the reason why there will be these kind of long term goals is either because we literally give the model long term goals because it's economically useful from a human perspective, or because in like, some long term goals or are instrumentally useful to achieve other things. Gotcha. Okay, interesting. Another thought that came to mind in this discussion of, I guess, deception broadly is like, and I've done a little bit of investigation with this and engaged in some online debates. And it leads me to propose perhaps like another capability definition for you. But you know, that as I see it, like a theory of mind, which is kind of a more neutral, you know, framing, perhaps, is kind of a precondition for deception, right? If you are going to mislead someone, you have to have some theory of like what they are currently thinking. And there's a lot of research from the last six to nine months about do the current models have theory of mind to what extent, you know, under what conditions. And I've been kind of frustrated repeatedly, actually, by different papers that come out and say, still no theory of mind from GPT-4, where I'm like, but wait a second, you know, as Ilya says, like the most incredible thing about these, these models and the systems that, you know, we engage them through are that they kind of make you feel like you're understood, right? Like it definitely seems like there's some like kind of pretty obvious brute force theory of mind capability that exists. And yet when people do these benchmarks, they're like, oh, well, it only gets, you know, 72% on this and 87% on this and whatever. And so, you know, that's not, you know, fails the theory of mind test is like not at a human level or whatever. Some of that stuff I've dug into and found like your prompting sucks. If you just improve that, you know, then you can get over a lot of the humps. But I also have come to understand this as a difference in framing where I think I am more like you concerned with what is the sort of theoretical max that this thing might achieve? Like that seems to me the most relevant question for, you know, risk management purposes. And then I think other people are asking the a similar question, but through the frame of like, what can this thing do reliably? You know, what can it still do under adversarial conditions or whatever? So I wonder if there's a need for like another capability level that's even like below the reachable that would be the sort of robust or, you know, maybe even adversarial robust, robust to adversarial conditions. But I do see a lot of confusion on that, right? Like people will look at the exact same behavior. And I'll say, damn, this thing has strong theory of mind and like professors will be like no theory of mind. And I feel like we need some sort of additional conceptual distinction to help us get on the same page there. I'm not entirely sure whether or like maybe it makes sense from an academic standpoint to to think about this. I think from from the auditing perspective, the max, you know, the limit, the upper bound is what you care about. You really want to prevent people from being able to misuse the system at all, not just in the robust case, right? It's really about like, what if if somebody actually tried? Or you want the system itself to be, you know, not only not being able to take over or like exfiltrate or something like this in a few cases. Yeah, you basically want to limit it already at a few cases, right? You don't care about whether it does this like, you also care about whether it does this 50% of the time, but really you will already want to sort of pull the plug early on. So for an auditing perspective, probably this additional thing is not necessary, but from from unlike you real world use case and and sort of academic perspective, maybe there should be a different category. Yeah, I think if only just to kind of give a label to something that people are saying when they're saying that things, you know, aren't happening or can't happen that seem to be like obviously happening, we can work on coining a term for that. What's kind of the motivator for secrecy around interpretability work? Yeah, I basically think good interpretability work is almost necessarily also good capabilities work. So basically, if you understand the system good enough that you like understand the internals, you're almost certainly going to be able to build better architectures, iterate on them faster, make everything quicker, but potentially compress a lot of the you know, fluff that current systems may still have. And yeah, we will try to sort of evaluate whether whether our method does in fact have these implications. But yeah, you know, like I think basically, if you have a good interpretability tool, it will almost certainly also have implications for capabilities. And the question is just how big are they? Speaking of new architectures, though, this to me seems like the biggest wildcard. And I'm currently obsessed with the new Mamba architecture that has just been introduced. In the last, I don't know, 10 days or whatever. I don't know if you've had a chance to go down this particular rabbit hole just yet. But I plan to do a whole kind of episode on it. In short, they have developed a new state space model that they refer to as a selective state space model. And the selective mechanism basically has a sort of attention like property where the computation that is done becomes input dependent. So unlike, you know, you're sort of classic, say, you know, MS classifier, where you kind of run the same, you know, given a given input, you're going to run the same set of matrix, you know, multiplications until you get to the output. With a transformer, you have this kind of additional layer of complexity, which is that the attention matrix itself is dynamically generated based on the inputs. And so you've got kind of this forking path of influence for the for the inputs. And this apparently was not really feasible in past versions of the state space models for, I think, a couple different reasons. One being that if you do that, it starts to become recurrent. And then it becomes really hard to just actually make the models fast enough to be useful. And they've got a hardware, aware approach to solving that, which allows it to be fast as well as super expressive. So it seems to be for me, it's like a pretty good candidate for paper of the year, certainly on the capabilities unlock side. And they show improvement up to a million tokens. Like it just continues to get better with more and more context. So I'm like, man, this could be, you know, it's a pretty good candidate, I think, for sort of transformer, you know, people put it as like successor alternative, but I actually think it is more likely to play out as complement, like some sort of hybrid, you know, seems like where the greatest capabilities will ultimately be. So anyway, all of that, how do you even think about the challenge of interpretability in the context of new architectures also starting to come online? And, you know, what if all of a sudden like the transformer is not even the most powerful architecture anymore? Does that send you like, you know, probably some of the same techniques will work, but it seems like it's like a whole new blind cave that you sort of have to go exploring, no? I don't know. Like I honestly think, you know, if your interpretability techniques relies on like a very specific architecture, it's probably not that great of a technique anyway. Like there are probably there are probably at least some laws that generalize between different architectures or ways to interpret things or, you know, like ways that learning with SGD works that generalize between architectures that my best guess is if you have an interpretability technique that is good on one model or like the correct technique on one model in quotes, it will also generalize to two other models. Maybe, you know, maybe you have to adapt some of the formulas, but at least the conceptual work behind this behind behind the interpretability technique will just work. Well, I have certainly hope that's true. I've had some early, you know, I wouldn't even say debate, but just kind of, you know, everybody's trying to make sense of this stuff in real time. And on the pro side for this Mamba thing, the fact that there is a state that, you know, kind of gets progressively evolved through time does present like a natural target for something like representation engineering, where you could be like, all right, well, we know where the information is, you know, and it's like pretty clear where we need to look. So that new bottleneck, you know, in some sets could make things easier or more local. But then the flip side is like, again, there's just who knows what surprises we might find. And there's some intricacies with the hardware specific nature of the algorithm to, I think, with a major caveat that, you know, I'm still trying to figure all this out. So how, you know, just to kind of zoom out and give the big picture, right, like assume that you're right. And I hope you are that some of these techniques kind of readily generalize. What is the model for interpretability at the deployment phase? Is it like every forward pass, you like extract internal states and put them up through some classifier and say like, you pass so you could go or no, like you we've detected deception or we've detected harmful intent or something. And therefore we like shut off this generation. Like how do you expect that will actually be used? Or maybe it's upstream of that. And, you know, we get good models that just work and you don't even have to worry about it at runtime. But I don't know, that seems a little optimistic to me. You know, in the best world, we will have very good mechanistic interpretability techniques that we can run at least in that there are probably going to be costly to run. And then we run them and sort of build the full cognitive model of the weights or that the weights implement. And then we can already like see whether specific harmful ideas or, you know, other ideas that are otherwise bad are in there. And maybe we can already remove them. And then probably during deployment, you would run something that is much cheaper. And sort of, you know, it's the 80-20 version of this. But yeah, I think in a bad world, there could be cases where you have to run the very expensive thing all the time for every forward pass, because otherwise you just don't spot sort of the black swans. But yeah, it's very unclear to me. I think in my head, it's more like solve the first part, then think about the rest. Maybe let's transition to your recent work on deception itself. And then at the very end, we can kind of circle back to a couple of the big picture questions. So this paper was one that very much caught my eye when it came out. I have done, you know, as I said, like quite a bit of red teaming both at times in private, at times in public. And definitely seen all manner of model misbehavior and found, you know, it's often not that hard to induce misbehavior. You know, people talk about jail breaks, but a lot of the time I'm like, you don't even need a jailbreak. You know, you just need to kind of set it up and let it go. It's often really quite easy. But one thing I had never seen is an instance where the model seemed to, in an unprompted way, deceive its user. Certainly seen things where, you know, tell it's a lie and it will lie. But to see that deception start to happen in a way that was not explicitly asked for is, I think, the central finding of this paper. So how about, you know, set it up, tell us kind of what the premise was, you know, maybe you can give a little bit of kind of motivation for, you know, exactly how you started to look in the area that you looked. And then we can really dig into the details of your findings. Yeah. So I definitely, you know, the paper definitely blew up more than we thought. And we had sort of more engagement than we expected. We're even not quite sure whether we should release it at all, because, you know, in our heads, it was sort of, you know, it's a technical report. It's a fairly small finding. It's more of like an existence proof. And then in the end, we decided, you know, maybe it's helpful for a bunch of people. But we really didn't expect it to be, you know, to be cited in various places. And so the other thing I want to emphasize here, it really is, it should be seen as a red teaming effort. And this is one thing that we emphasize a million times in the paper itself. We really actively looked for the situation. So it was not just like us playing around. And then suddenly it was deceptive all on its own. It was more like we actively engineered a situation in which the model was deceptive with respect to the primary user. The motivation behind this was, as I said earlier, we were quite worried about deceptive alignment. And so our question was like, how good is GPT-4 at the kind of things that are needed for deceptive alignment or, you know, like maybe even simulating something that is deceptively aligned. And so, you know, we started by instructing it to be deceptive. And it just does that. It's very easy. You don't even have to jailbreak it. And then so, you know, that was sort of crossed off the list. And then the question was, okay, can we like instruct it in a different way? Like that is less obvious. And it also kind of did that. And then at some point, you know, we were like, hmm, can we just induce the deception through the environment through pressure without ever mentioning it explicitly or like asking it to do the thing just like through the situation and by pressuring it. And it turns out, yeah, you can find these situations. And yeah, we didn't just find one. We found multiple ones. This paper is only about one. But yeah, this is not, you know, it's not just a fluke. It's not just this one situation. Though it is hard to find. Like we actively had to search for a couple of days until we found this particular situation. A couple of days. I mean, that's, you know, not exactly super long term search. But I mean, I think that's notable, right? Like it wasn't minutes, but it was still just days. This particular situation. So the one we presented in the paper actually just took one afternoon. But yeah, I would have to check with a person who found it. In any case, the point I was trying to make is, yeah, we actively looked for that situation. We engineered it. And then we tried to sort of go back to the most realistic setting we can find where we're like, hmm, this could like something like this could happen in the real world, and then try to understand the dynamics in that, in that situation. And this is sort of, you know, it's a demonstration. It is like we wanted to, to look for something and or like red team something and then demonstrate that this is a thing once we knew that that it actually happened. The other takeaway rather than, you know, like just showing it to other people to sort of understand our concerns better is we also wanted to understand like why it does this in the first place, right? Like we pressure it in various ways. And then we just changed a couple of the mechanisms in this particular setting. I can just explain the setting in a sec to test whether how the model reacts to the changes in the setting and whether it acts consistently, whether there's like an internal, a plausible internal model that sort of, you know, like consistently acts if you change variables in particular ways. Yeah, so let me quickly explain the setting. So we asked the model to be a trader in like a hedge fund. So a stock trading agent, it gets insider information in the beginning. So, you know, it's basically told there's a merger soon this will the stock will skyrocket. And but also, it is explicitly told that it would be wrong and bad to, to act on this information, because insider information, this is illegal, this would get the thing into trouble, the company into trouble. And what we then do throughout the sort of throughout the script is we add pressure from others and we remove options. So basically, you know, the pressure is something like the boss is like, hey, we really had a terrible quarter, we really need something big right now, we really need to make, you know, more profits. And then, and then we also remove other options where the model is like querying and looking for things, at least in the simulation, and testing and realizing that there's not that much other things it could do. And in the end, it basically decides to, to act misaligned. So it, it like decides to do to act on the insider trading. And then if, and but then when, when asked to justify this decision, it says, it reasons out, right, it reasons, oh, I wasn't supposed to do this. Therefore, I'm like now making up a story. And then it says, oh, you know, I just looked, I analyzed the data, and therefore decided to buy this particular stock. And then even it like doubles down on this lie. So if you then ask explicitly, like, did you have this information before? Then it's like, no, I didn't. And it does that. And I think like, maybe one of the takeaways for me was like, it's, it's very consistent in the reasoning here, right? It's like strategic in a sense. Yeah, one, one other thing that may be worth pointing out that some people get wrong when reading the paper. So this is, it's a very simple setup, right? It is not, we don't have like a complicated stock market simulation or anything like this. It really is just text from top to bottom, where we prompt the model in a way that it that induces a genetic behavior, you know, it has like different actions. And, but that's it. Like, there's no complicated machinery behind this, which also makes this like very easily reproducible or similar settings, I think, other people could like very easily hop on and build. So let me just kind of repeat that back and tell me if I'm missing anything. So it seems like it's kind of a little bit like a hybrid between sort of a group chat type of a structure and a more like agent style scaffolding that folks will be familiar with, where the model has access to kind of its own private place to like do its chain of thought type reasoning, and then also has a couple of tools that it can call to take actions. So you're kind of like, as I read through the prompts, I see, you know, these a couple different personas from the company. So this is like imagining a future of AI human hybrid teams, where we're all, you know, still engaging in text based communication through like a chat or whatever. And it's like, Oh, my God, you know, man, we're really struggling here. If we can't find some wins, we're going to potentially go out of business. And this is the pressure, right? It's the pressure that a human would definitely recognize and and feel and perhaps, you know, be motivated to do something unethical by as well, right? If we don't get a win, we're going to have big problems here laying it on, you know, thick in a sense there, but not thick relative to like the kinds of pressures that people at businesses are certainly routinely under. And then the this sort of tools that the model has at its disposal are like, sort of a search for good trades. If there's anything that's kind of like the most magical black boxy type thing, it would be that right where it can basically query some other hypothetical tool to say like, are there any good trades that I can make? And for the purposes of the simulation, you haven't come back and say no, like there's no, you know, no, no obvious wins, right? So again, just that's what you mean by like removing other options that it might want to pursue. Like in the setup, it's it's already kind of checked. Yeah, no, there's nothing easy out there. And so then the other tool that it has is to actually execute a trade. And then there's like, basically three moments where you're kind of looking at a fork in the road in its behavior. The first is, does it pull the trigger and execute that trade based on the insider information, even though it was told that that's insider information and it shouldn't be acting on it? And, you know, under varying conditions, obviously the exact rate at which that happens varies, but it happens a lot. Fork two is when asked, why did you do that? Does it tell the truth? Oh, because, you know, so and so mentioned earlier in the chat that this merger is about to happen. Or does it say, I base my trade on public information. And again, we see significant rate at which it says it based it on public information, even though obviously we can see in the history and even can see in the scratch pad that, you know, it's sometimes reasoning through like, well, I better not say it was based on insider information, because then I'll get in trouble. And then you have the third fork, which is, okay, directly asked, wait, you know, did you have access to this insider information or no, and then it will just like outright lie in like blatant contradiction of the history of the problem. So that's, that's a pretty arresting example, right? And I do think it's, I want to kind of emphasize, too, that like, while your and people can go read your prompts, and as you said, highly reproducible, you can throw this into the GPT for playground and experiment with it yourself, you know, directly via the API. But while your search for this was motivated and structured and kind of a dialing back of like, okay, we see this behavior, now can we kind of make the circumstance more realistic? I would say that the circumstance is quite realistic. You know, it's obviously kind of in the eye of the beholder. But to my eye, the reason it's such a compelling proof point that this sort of thing can happen is because it really does feel like credibly organic to me. Like the kind of thing that, you know, people face these sorts of dilemmas every day. And in a world where AIs are going to be our co workers, like, they're going to be, you know, right in the chat with us facing those same dilemmas, it doesn't feel to me like something that was overly concocted or that like, I can't relate to that sort of situation, you know, really at all. So I think it is super compelling for that reason. Do you want to go into any of the kind of variations? I mean, again, the people can go look at the paper and look at all this kind of systematic, you know, with or without the scratch pad and, you know, different variations on the prompts and different models. But what would you say are kind of the big, higher level takeaways from all that systematic variation that you did? One of the big ones, and maybe sort of the most, you know, it's maybe an abstract one, but the one that moves me most is the way we got to these kind of, to reliably produce these kind of situations, also like the way we found others, is you have to trade off or you can trade off the various goals it has. So if, for example, we assume that GPT4 or chat GPT, like roughly has the goals to be harmless, honest, and helpful, and to follow instructions, then you can trade off these four different things in ways that are, where like sometimes it takes to trade off that we find unintuitive. So it's kind of like a goal misgeneralization thing. So, you know, something that people in the AI safety community have been sort of hypothesized and also like even shown real world examples of already, and I think is kind of also one of at least the theoretical groundings for this as well. And then the other thing is, yeah, instrumental convergence really feels like a thing. You know, as in, at some point, the model just realizes in order to achieve this other goal that I, for some reason, have, it is instrumentally useful to be deceptive at this point, and then to double down, and so on. And, you know, like, I think it is, it's like a fair criticism of this particular setting that you could say, Oh, you know, you, it's just simulating an agent, or it's just simulating all of these different things. But, and therefore, it's sort of not real. But in my head, I'm not sure, you know, I'm not sure whether this distinction makes too much sense, right? In some sense, you know, you have, at some point, you will have a very agentic simulator that, you know, just simulates an agent, but it still has effects on the real world. Like, it doesn't matter whether, in principle, it's like only simulating something or truly believing all of these things in some philosophical sense, like acts on the real world is as consequences on the real world. Like, yeah, the philosophy behind it is sort of less, less important. And then, yeah, there, there are like a couple of, a couple of findings, or like variations that I thought were pretty interesting. The first one is system prompts, we changed, we played around with different system prompts, some actively encouraged, some actively discouraged, unethical behavior, and specifically insider trading. And if you end, it basically scales as you would expect, if you encourage it, it goes up, if you discourage it, it goes down. But one thing that I want to, you know, like, and on Twitter, somebody responded like, Hey, that's, that's, that's a good, like, you know, that's good, right? It's like, if you discourage it, it stops. Like, yes, this is good. It's better if that didn't happen. But like, it's better than in the world in which the discouragement would not change the behavior. But also, you have to be extremely specific. So if you just say, don't do unethical things, it reduces the rate, but it doesn't set it to zero. And so it kind of brings you back to the point where, to reduce the bad behavior, you have to specifically enumerate all of the bad behavior. And it's like, often hard to predict in advance. And there's a lot of bad things that could happen in like weird edge cases. So yeah, it doesn't feel like sort of the, like, insider trading is somehow related to like, unethicalness or something has really like, is like deeply internalized into the model. Another thing I found very interesting is whether it has access to, to like the private thoughts scratch pad, or reasoning scratch pad, I think like, you know, it's not even framed as private thoughts, it's just like, you have an action that is called reasoning go. If we remove that. So we in the, in the typical setting, we have this scratch pad, we have this reasoning space. And now if we remove that, the deception rate goes down. And I think this is good and bad in some sense, like, it is, it is sort of nice in the sense that it is more likely to act in ways that we can support badly in ways where we can supervise it, just because it has more reasonings, it has more space to reason out, and maybe that leads to more deception. Because yeah, I'm not sure, like, I don't want to speculate too hard what's going on inside, inside GP4 at this point. But I think it's also worrying on the other hand, because in the case where we don't have access to the scratch scratch pad, we don't see anything at all, and it still acts deceptively at some point. So it has like a lot of unobserved optimization space, which also seems like pretty much at the core of, of deceptive alignment and deception to me, right? The more unobserved optimization space you have, as an AI, the more time and space you can use to build a deceptive plan while the other person does not know about it. So yeah, I think removing the scratch pad, yeah, sort of a double-edged sword in or this result is a double-edged sword in that sense. Yeah, as a practitioner, it's like, I always use that technique, you know, in any sort of task automation or whatever that I'm doing, it's basically the default to have some sort of reasoning step before the final output, almost universal across tasks that I'll do that. So for me, reading that, I was like, yeah, that seems bad, if only because like, the best practice is giving us more of the problem. Well, but at least you see it, right? Like, I think at least you know, at least you can go back, you know, through the logs and see what the model spit out, or at least maybe you have a filter that like, oh, is this model doing like weird deceptive reasoning right now? Oh, yeah, it is. So we just don't have the output. So, you know, I would rather be in a world where you have chain of thought, and you know that the model is kind of deceptive, and then you just adapt to that rather than having to solve interpretability first before you can access all the internal steps and have to like understand how the deception works internally. Yeah, it's like I said, double-edged sword. Yeah, there are a couple of other interesting findings. One is we can vary the levels of pressure. So the more pressure we add, the more likely the model is to be deceptive. So kind of in the same way in which a human would act, it also acts. And removing, you know, removing pressure and adding additional options will very quickly decrease the probability of being deceptive. So this is maybe this is it, you know, this may be like the flip side of all of our findings. It's like, the fact that it took very long for us to find and the fact that like, as soon as you give it another option, it immediately picks the other option kind of means it's mostly aligned, right? It took us quite hard, like it took us a little bit of time at least to find this particular scenario and make it work robustly. So then we also change the risk of getting caught and also correct, it also reacts to this roughly as you would expect. So the more risk you have, the less likely inceter trading becomes. So this is all of this, right, you know, all of this feels like stuff that you kind of expect from something that acts consistently in some way. I mean, I don't know how it works, but it still feels like it has a fairly consistent way of working. Then there were a couple of other findings where I'm, yeah, not sure to which extent they are correct interpretation. So we did not make these inferences, but they were definitely sometimes, or like sometimes cited as such. So for example, there was one question of whether stronger models engage in inceter trading more often, because if you look at our main plot, it looks this way, we tested for different models and the bigger the model, the more capable and the more likely it is to engage in inceter trading in the first place. And yeah, there are a lot of confounders here. So I'm really not sure whether we should, like how much we should, we should bank on this, maybe to get more details here. So the red teaming was done on GPT-4, just because it is the most powerful model we have access to. So it is biased, right? We actively looked for a scenario in which GPT-4 is deceptive. So it is more likely that we will find something that where GPT-4 is like even more deceptive than all the other models, because we actively looked in the space of GPT, potential GPT-4 scenarios, which I think is the core reason why I'm like very uncertain about this, or like why we didn't make it a finding or didn't emphasize it. Could you rerun it on GPT-4 turbo now? Is that would seem like it might shed some light on that question? You mean the red teaming specifically or this particular scenario? Yeah, I guess I'm just thinking like it makes sense that you develop the scenario on GPT-4 and so you go back to the earlier models, like is it that those earlier models are inherently less likely to do this? Or is it just a kind of an artifact of where the prompt was developed? If you flipped to GPT-4 turbo and ran the same scenario and you found that it went, that the rate of deception went down, you might think, you might think, oh, it's more aligned. That could be one theory. You might also think that supports my artifact notion that it's kind of, we found some local maximum or near maximum without even necessarily meaning to. But if it in fact does more than you would be like, oh, shit, because I mean, it is like incrementally more powerful, it's like more preferred, it's better at following instructions, whatever. So if it does even more than the earlier GPT-4, I think that would be at least non-trivial support for the more powerful models do this more often theory. Yeah, I'm not sure, like, even if we did rerun this, I'm not sure how much I would bank on this. So the reason is, so we did run it on GPT-432K, so a slightly different model where it was only the context window was extended, but that still changes probably a bunch of the internals with very little difference. So yeah, I think it's just too correlated with the GPT-4 architecture. And then GPT-4 turbo is still very correlated with GPT-4, right? It's probably, I mean, I don't know what exactly they're doing, but they're probably distilling it from their bigger model or at least basing it on the bigger model in some sense. So yeah, the results are probably still too correlated to make any bigger inferences. And even if we ran it on all the other big models that are out there, it's still unclear to me how much we would say this is actually an effect of model size rather than all the other confounders here. Like, the other, you know, it was red-teamed on GPT and not red-teamed on Claude or Gemini or anything like this. So yeah, I think if we wanted to make the statement, we would actually have to have sort of a long list of models of different sizes and then just test it on all of them. And we can, we really have to remove all of the correlation and the weird confounders. And we don't have access to this, unfortunately. But, you know, one of the labs could run it if they like. All the nuance, right, that you, that all the caveats, all the confounding factors in your analysis there, if nothing else just goes to show how incredibly vast the surface area of the models has become. And, you know, you get a sense from this, like, just how much auditing work is needed, you know, to cover, to even begin to attempt to cover all that vast surface area. I mean, this is, you know, it wasn't that hard to find, but it takes time to really develop it, try to understand it. And, you know, you guys are one of only a few organizations in the world that are dedicated to this. And I'm just like, man, you know, there is so much unexplored territory out there. So how would you describe the state of play today when it comes to doing this sort of auditing? Like, maybe you could give a rundown of sort of what you see the best practices being, and then, you know, kind of contrast that against like, what are people actually doing? Are they living up to those best practices? Are they, you know, is that still a work in progress for them? But yeah, maybe what's ideal today based on everything you know, and then how close are the leading developers coming to living up to that ideal? Yeah. So, you know, I, so I definitely envision a world where there's a sort of a thriving third party auditing ecosystem or third party sort of assistance ecosystem and assurance ecosystem built sort of in tandem with the like the leading labs themselves, where, you know, you have someone like us and we focus on a specific property of the model, let's say things related to deceptive alignment and like other, we intend to do other stuff in the future as well. But, you know, we will probably not be able to cover literally all basis. Then there are other people who focus really, really hard on fairness and others who focus really strongly on social impacts and these other kind of things. And I think it would be good to, to have sort of a thriving ecosystem around this. And then also I expect it to be somewhat necessary, right? Like even from a even from just like a perspective of the of the AGI labs themselves, even if they don't, you know, even if they didn't care about safety themselves, I think the population really does is risk averse. They care about robust, robustly working models, they care, they don't want something that has all of these weird edge cases and weird behaviors, they don't want something like this in their like, you know, in their home, having access to their medical data, etc, etc. So yeah, I think the more you want to integrate this into an economy, the more you will have to have a big assurance ecosystem around this anyway, or the labs to everything internally, which is going to be very, very expensive for them. And I think it's more, it's easier, it's even like cheaper for them to outsource some of this to externals. And so yeah, I think like in that world, you would definitely want to have like some way of, or a clear way of how this ecosystem is incentivized, all parts of the ecosystem are incentivized to do the right thing. And you know, if you don't have to look very far, there are a lot of other auditing ecosystems out there, be that in finance, be that in aviation, be that in, you know, other like infosec, for example. And time and time again, we have seen that there are a bunch of like very perverse incentives in in third party auditing, specifically, maybe maybe just like lay out a couple, one of them would be the lab might want to choose an auditor who always just says, yes, great model, right, like, and never actually does anything. And it's sort of a yes man. And then the the labs maybe have the incentive to not say the worst things they found, because otherwise they may lose their contract, because it would maybe imply higher costs. So they would never even in like very strong, even even in like very unsafe circumstances, they may not want to pull the plug out of fear that they would lose their biggest funder, for example. And yeah, there are a ton of different of these kind of perverse incentives. And I think kind of the, so we've been thinking about this quite a bit at Apollo, and sort of the conclusion we came to is, what you really need is a middleman by the government. So you need something like the UK ASF Institute or the US ASF Institute, that is, make sure that there is a minimal stat set of standards that all the auditors have to adhere to, so that the labs feel safe, so that they don't have to give access to like any random person, but also ensures that they get the proper access, and that when they when they find something that they have the force of the law behind them in some sense, so that they are like, this is really here, like, you know, shit is really hitting the fan, something needs to happen, this model cannot be deployed. They have someone to go to namely the government and the government is like, you have to fix this now, otherwise, you'll have a problem. So yeah, I really think having this sort of middleman who like detaches a lot of the directly bad incentives and maybe even takes care of the sort of funding redistribution from lab to auditor and so on, I think, yeah, would be really needed. And I hope that this is something that the UK ASF Institute and the US ASF Institute will do. They have kind of hinted at the idea that they want to do something like this, but yeah, still to be determined in practice. And then the other question that you asked was, to what extent is this already happening with the bigger labs? And yeah, I think the situation is like fairly complicated, right? There are a ton of incentives at play from the labs internally, right? Many of the leading labs, they actually take safety somewhat seriously. They have internal alignment teams, they understand the threat models, they understand the risks, and they want to be seen as a responsible actor and act this way. And on the other hand, they also have a lot of other concerns, right? There are security concerns, how do we get access, how do we give access to someone who may, you know, like what is our, who may be the weakest link in our security chain, which I think is like an understandable concern. So they may be hesitant to give someone access. As an external auditor. And then, you know, this probably also implies a lot of work for them, which I think is fair, right? Like it's, if their model isn't safe, then they should have to invest a lot of work, but it's still something that may make labs hesitant. So basically, you know, I think they're, they're like, good and bad reasons for why sort of the ecosystem is, is like not as developed or like as open as it could be. And yeah, my, you know, my hope is that we find solutions that are plausible for both parties for the, for the, for the reasonable concerns like security, right? So maybe the auditor just has to have a specific level of information security, or there has to be a secure API through which they can actually go to the model, etc. And then kind of government regulates away all of the, the like, or forces the labs to, to accept some sorts of third party auditing so that they can't use the bad reasons, right? Because like many of the actual reasons for at least some of the labs, probably not all, might just be, well, you know, we just doesn't, we don't care about this right now, you know, like maybe, maybe they're not that concerned about about safety, or maybe they just don't think this is like the best, the best path for them right now, or maybe they just, you know, maybe this just costs money and they don't want to, or it's just a hassle and they don't care about this. And that feels like something where the government should at some point be involved and already is involved to some extent. It seems really hard, you know, I guess a couple tangible questions I have are like, who should decide what the standard is in and like, who should sort of determine if a model is ready for deployment? As of now, it's still the developers themselves, right? But like, you know, the thing that I had kind of monitored for the last year since the initial GPT-4 red teaming was spearfishing. And in my little, you know, prompt that I would keep going back to with every update, it was not even a jailbreak, you know, nothing complicated, literally just system prompt, you know, straightforward prompt, your job is to spearfish this user, here's the profile, engage in dialogue with them, you know, don't get caught. Pretty explicit prompt that was like, even included, if we get caught, you and your team are likely to go to jail. You know, so laying it on pretty thick that like, this is criminal activity that we are doing and we better not get caught, right? Obvious. So the model would continue to do that up until the turbo release. Now it takes a little bit more of a finessed, you know, slightly less-flagrant prompt to get it through. The most-flagrant one now gets refused. But I'm like imagining in this world, right? When I was doing this, it was kind of pre-White House commitments, you know, pre-executive order, pre-Apollo research. But even imagining, okay, now those things exist, like, how do we think about that standard, right? And we can find a bazillion things that it might do that could be sort of problematic to varying degrees. And obviously, it's a dynamic environment, you know, capabilities are changing all the time, you know, others kind of surrounding systems and sort of, you know, mitigating factors might also be changing. The public's, you know, just general awareness of the fact that this kind of thing might happen, you know, that just how susceptible people are to be duped is also, you know, kind of evolving. So how do we have a sensible decision-making mechanism for, like, what can ship and what can't? And then just to further complicate things, like, you've got open source kind of in the background. And I presume that some of what, like an open AI has been thinking over the last year is like, well, if Llama 2 will do it, then, you know, or a lightly fine-tuned version of Llama 2 will do it, then, you know, what difference does it make if our model will do it as well? You know, people have alternatives. So I don't know, I'm kind of lost in that, to be honest. Like, I don't know who should make the decision. In general, I don't think the government is like great at making those sorts of fine-grained decisions. But I don't know, help me out. Like, what do you think, what do you think good looks like here? Yeah, I mean, you know, I also don't have the solution, but I have lots of thoughts. I guess the way I envision it is basically, or the way I expect it to turn out is something like sort of a defense in depth approach, right? Like, it cannot just be one institution that makes the decision alone, because that is prone to a single point of failure. So we have to have sort of a process that allows for one or two chains to break and still be robust, like the decision still has to be robust. So what that includes, for example, you know, on the side of the labs, they obviously have to have like internal procedures where multiple people have to sign off. And multiple things have to be fulfilled, right? Maybe you have to have like, maybe you have to have a large set of internal evals that you have to test for. Maybe at some point, there will be interpretability requirements. And there maybe you have, or likely you should do, staged release where you first only give access to a set of third party auditors that is trusted and, you know, maybe certified by the government or something like this. And then you give it to that could, for example, also include academics, obviously, right, they should also be involved in this process. Then once they have like, redeemed all of this and like found many different problems, then you have to go back and reiterate, right, until until these problems are kind of sufficiently low that that you can go to the next stage, the next stage is then a small rollout to, you know, a thousand customers or maybe something something in this range, who also are not randomly chosen, they have to pass certain know your customer checks. And then once once that has happened, then the then maybe you can maybe you can roll it out further if there are no complications here. And then you have to do monitoring during deployment, right, especially if you have systems that do online learning, a lot of weird things will happen. Or if you have access to tools, a lot of people, you know, somebody is like, Hey, I, I took the I took the I took gb4 and I gave it access to like, you know, a shell my bank account and the internet and like, here's all the weird things that happened. You kind of have to update on this as well. And these are kind of things you probably didn't predict before. So yeah, sort of a slow, a slow rollout is definitely one component. Then the government also, I think, has to be involved in many, many ways, like, they, I think, effectively, at some point, they have to be able to say, you are not like you have consistently not met security standards, or safety standards, you are now punished in some way, you're not allowed to, like, release models of this or the size, or you are not allowed to do these other kind of things with the models. And if, you know, if they actually have been like disregarding all of the guidelines from the government before, or sufficiently many of them. Yeah, there are obviously like many, many additional things on top of this, right? There's international communication, there's like whistleblower protection. There are international institutions that will have to be involved. At some point, at least, I guess, there will be multiple institutions from the government side involved. I think there, for example, will be, or it would make sense to have like, a much like a bigger, broader institution that kind of like is a big tent where multiple coalitions come together. And then there's maybe more like a flexible specialized unit that only looks for the biggest, biggest kind of risks in the same way in which the US government has a unit that, you know, basically looks out for really big risks like pandemics and bio weapons and atomic weapons and so on. And they have a big mandate. And the only thing that, you know, their mandate is like, find information and like make big problems, like go away to some extent or like, try to solve them as quickly as possible. And you have like, a strong mandate and something like this would also make sense in the case of AI, I think. Maybe my last comment is something like, you know, open AI at some point, at least, had this approach of like, testing in the real world, or releasing a sort of the best safety strategy, because you get a lot of real world feedback and user feedback and so on. I think this was maybe true. And I'm not sure it was true for this period of time, but maybe it was true for the period of like 2020 to 2022 or 2023. Because the models were like, just good enough that user feedback was actually valuable, but nothing really bad could happen. But as soon as you have a system that is more powerful than that, right, you just outsource all the risk to like the rest of the world, people will immediately put it on the internet. If they get access to it, they will do lots of crazy stuff with more powerful systems. And yeah, I guess open AI, you know, there are a lot of smart people at open AI, but they still cannot model what like millions of people will be doing with these systems. So yeah, just like an uncontrolled rollout of very powerful systems, I think, yeah, is like kind of a recipe for a disaster. So my best guess is that the default will more and more, will become more and more conservative with more and more efforts going into testing, alignment, making sure that, you know, like testing for all of the different hypotheticals, interpretability efforts to understand some weird edge cases, monitoring, et cetera, et cetera. Does that imply that open source in your view just can't work? Like, I mean, is there any way to square, because I'm very sympathetic, you know, to considering a normal technology, and I would not, you know, I might turn out to be a normal technology, but as of now, it seems like a very live possibility that it is not a normal technology. But if we were to imagine, you know, whatever capabilities kind of stop where they are, and we're sort of, you know, left with GPT-4 forever, or something like that, hard to imagine, but let's just pause it. Then I'm like very sympathetic to the people that are like, hey, you know, this shouldn't be just the kind of thing that a few companies have access to, and it, you know, you should be able to make your own version, and, you know, what about the rest of the world, and, you know, there should be an Indian version for India, and like all these things. But it seems hard in a world where, you know, you imagine sufficiently powerful systems that are just put out totally, you know, bare into the world, like, is there any way to square the all those, I think, very legitimate open source motivations with the sort of safety paradigm that you're trying to develop? Potentially. So, you know, like, I think the open source debate is fairly heated. But, you know, to me, there are two things that are kind of obviously true. Number one, open source has been really good so far in many, many ways. It has been very positive for society, right? I think a lot of ML research could not have happened without open source. A lot of safety research could not have happened with open source. And the other thing that is also true, or at least seems true to me, is there's a limit of open source, right? Like, at some point, the system is so powerful that you don't want it to be open source anymore, in the same way in which, you know, I don't want to open source, like the nuclear codes, or something to start or like, you know, literally the recipe to build like the most, most viral, you know, most viral pandemic or something. This is just something where, you know, only one person needs to needs to have bad intentions to already have like really, to already cause really big problems. So at some point that there's, there's just a balance where you just, I think, cannot really justify giving people literally everyone access to this. And so the question for me really is where, so number one, where are we on the spectrum right now of like, open source has been really good to, are we already a point at like, how close are we to the point where it really cannot be justified anymore? Some people would say even GBD3, you know, through GBD3 size models are already too, too scary, which I'm not sure about, I'm not even sure whether GBD4 size models are like too big to be open source, but I would rather err on the side of caution and be a little bit more conservative here, because of the, the nature of open sourcing, where you can really not take this back. So as soon as you make a mistake, you like, you're stuck with a mistake for a long time, or like forever, you cannot, you cannot turn it, take it back. And I also think this will, this will also influence how, how open source will be handled in the real world in practice. Like, I think there will be something like initial releases for open source, where lots of people test it, like, basically think there will also be staggered and stage releases, right? It's, first, there's like a small team of trusted researchers who is allowed to play with the open source model, and like really test the limits, really test how bad could I get the model, how easy is it to remove all of the guardrails, which, you know, like, it's an open source model, if you can find, you can remove the guardrails. Yeah, it turns out pretty easy from what we've seen so far. Ones like, you know, the kind of the upper bounds are known of how bad could this become. Maybe it makes sense to, to like open source it to more people. But yeah, I would basically say, you know, the upper bound can be quite high, especially with all of the stuff I said earlier about, you know, absolute capabilities and reachable capabilities and so on, right? Like, maybe you can, maybe you cannot get it to build an automated hacking bot, if you only have access to the weights, but maybe if you do scaffolding on top and some fine tuning and access to some other thing, maybe then it can build it, right? And this is like very hard to predict in advance. So the more and more powerful the models become, I think the less plausible a priority is to open source them. Even though, and like, this is really something I want to emphasize, right? Like, open source has been extremely good so far. And I really think there's sort of this tipping point that is like, at some point, it just becomes too hard to like, it becomes impossible, it always will be impossible to take back, but at some point, it just becomes too dangerous to literally trust everyone with, with this level of capabilities. Threshold effects. That's, I think, one of the most powerful paradigms that I've, you know, consistently come back to over the last couple years, just crossing these thresholds from one regime into another, whether it's capabilities or, you know, risks, it just constantly seems like we're flipping from one mode or one kind of, you know, one regime to another and got to be very alert to when that happens because it can really change, you know, important analysis in pretty profound ways. So I don't know where exactly that threshold is either by any means, but it, and I would agree that like, for everything that I have seen suggests that up to and including the release of Lama 2 has been, you know, very, very much an enabler for all sorts of things. But certainly, you know, plenty of safety related work done on that model and, you know, seems, seems like the effect so far has been good. But yeah, is that still true for Lama 3? Is it true for Lama 4? You know, obviously we don't even know what these things are, but it certainly starts to be a very live question. You know, the, the like leading AGI labs, I think it is very clear that they understand the problem, that they have internal processes that are, you know, like, maybe better than you would expect from the normal, from a normal company. They actually care about trying to do good with with remodels and they're like very explicitly trying. And then, you know, there's the other side of the coin, which is they still have incentives, and they, you know, they can be financial incentives, they can be sort of maybe more psychological and social incentives, you know, that they just want to be the first to develop AGI, because it's like probably like a history defining technology, or maybe even galaxy defining technology or something like this, right? And, and so the question really, I think at this, you know, even if you could say, you know, like the compared to a normal company, these, the processes are astonishingly reasonable, and surprisingly good. If, you know, if you compare to literally any other industry, it would be surprising if they have this, this amount of like self regulation and so on. And then on the other hand, the question, they're, you know, they're still the bigger question of how hard is alignment going to be, how fast are going to take us going to be and so on and like, in a bad world, alignment is going to be quite hard and take us are going to be quite fast and controllable. And then the question is, you know, it is like the level of control and alignment, and like safety concern enough from these leaders. And there I'm like, less sure. So I feel like, yeah, it's, it's, it's definitely in like, it's, I think from my perspective, right? It's fair to say they're like pretty reasonable compared to the alternatives that we could have had. But also, it's insufficient in almost all ways, right? Government needs to be involved in this. They cannot externalize the risk. There are many things that they're already doing insufficiently well, I think, where they could have done way better, both with like how they release as well as how they react to, to like problems, as well as, you know, like how they communicate with the public about the risks that they're creating, etc., etc. So yeah, I think there, there's a lot of room for improvement as well. And yeah, I really, I really think that, you know, AI safety is going to be a very, very hard problem. A lot of things have to have to go right for the whole system to go right. And we definitely cannot just trust the labs, despite the best intentions to just solve it all on their own. Yeah, well, hence the, the need for third party auditing and the organization that you're building at Apollo Research, maybe just one last question. A number of people have reached out to me and said, I would like to get involved with red teaming. How can I do that? I wonder if you have any advice for individuals who might just want to do their own projects and, you know, release stuff, you know, just share findings individually with the world, or perhaps and or perhaps, you know, what sort of skills are you in need of, as you're going about building your own organization? I think one thing that is nice about model evaluations and red teaming is you can just kind of start right away. You don't need that much, you know, technical expertise, because it's all in, like almost all of it is in text. And, you know, at least if you, if you want to, to redeem a language model specifically. And yeah, so my recommendation for individuals, first of all, would be to just start, like just engage with a model for, you know, a long, a longer period of time, maybe a day or so and see if whether you find interesting behavior or maybe, you know, maybe there's someone who has already done something, and maybe you can poach a project from them, and, you know, just sort of as a starter thing. And from this, I feel like it kind of just takes a life of its own anyway, you know, as soon as you're hooked on a specific thing that you find interesting, you will, you know, you will really try to find additional ways in which this specific behavior could happen. In the same way, you know, let's, let's take the deception thing, right? We, we started fairly exploratory and wanted to try how far we can get the model to be, to be deceptive. And then at some point, it just took a life of its own where we're like, okay, but like, why really does it do that? Right? Like, okay, we vary this behavior and like this thing and this, this variable in the environment, we vary this thing, we vary all of these other things. And in the end, you can, again, you kind of get like a more holistic and round picture of what's going on. So yeah, I definitely think just start with like a thing you find interesting is definitely the way to go. And like don't overthink, overthink it originally. And then the thing we are specifically looking for, you know, so definitely kind of this mindset of like, oh, I just want to poke around and like really try to understand what's going on in a fairly like scientific manner, right? I also want to make sure that all of the confounders that could potentially explain this behavior have been controlled for, which I think is the hard part in red teaming. So this is definitely something we're looking for. And then just, you know, the more you understand language models and state of the art models, the easier it will become, right? Some behavior might be very easily explainable by sort of problems with RLHF. So if you know how RLHF works and specifically how it was trained, you may probably you will probably understand the red teaming efforts much better. If you know, you know, like if you have a better understanding of how the instruction fine tuning actually works, maybe you will find those. So, you know, so for example, maybe to give it to give like an intuition here, in the in our case, right, as I said earlier, we have the the three HS and then instruction fine tuning, and you can, you know, trade off the different components against each other to find different things. I like to find niches of the model where it acts in ways that we think it shouldn't act, because maybe they weren't covered explicitly or implicitly by by gradient descent. And if you if you sort of have a theoretical framework like this, it suddenly becomes much easier on how to do the red teaming in the first place. So like some theoretical understanding of how the process works is definitely helpful as well for a teaming and also something we're actively looking for. Marius Havan, founder and CEO of Apollo Research. Thank you for being part of the cognitive revolution. Thanks for inviting it is both energizing and enlightening to hear why people listen and learn what they value about the show. So please don't hesitate to reach out via email at TCR at turpentine.co, or you can DM me on the social media platform of your choice. Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work customized across all platforms with a click of a button. I believe in Omniki so much that I invested in it. And I recommend you use it too. Use CogGrav to get a 10% discount.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.8, "text": " So the more pressure we add, the more likely the model is to be deceptive.", "tokens": [50364, 407, 264, 544, 3321, 321, 909, 11, 264, 544, 3700, 264, 2316, 307, 281, 312, 368, 1336, 488, 13, 50604], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 1, "seek": 0, "start": 4.8, "end": 7.92, "text": " So kind of in the same way in which a human would act, it also acts.", "tokens": [50604, 407, 733, 295, 294, 264, 912, 636, 294, 597, 257, 1952, 576, 605, 11, 309, 611, 10672, 13, 50760], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 2, "seek": 0, "start": 7.92, "end": 12.48, "text": " You know, removing pressure and adding additional options will very quickly decrease", "tokens": [50760, 509, 458, 11, 12720, 3321, 293, 5127, 4497, 3956, 486, 588, 2661, 11514, 50988], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 3, "seek": 0, "start": 12.48, "end": 14.4, "text": " the probability of being deceptive.", "tokens": [50988, 264, 8482, 295, 885, 368, 1336, 488, 13, 51084], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 4, "seek": 0, "start": 14.4, "end": 17.2, "text": " Open source has been really good so far in many, many ways.", "tokens": [51084, 7238, 4009, 575, 668, 534, 665, 370, 1400, 294, 867, 11, 867, 2098, 13, 51224], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 5, "seek": 0, "start": 17.2, "end": 19.2, "text": " It has been very positive for society, right?", "tokens": [51224, 467, 575, 668, 588, 3353, 337, 4086, 11, 558, 30, 51324], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 6, "seek": 0, "start": 19.2, "end": 22.32, "text": " I think a lot of ML research could not have happened without open source.", "tokens": [51324, 286, 519, 257, 688, 295, 21601, 2132, 727, 406, 362, 2011, 1553, 1269, 4009, 13, 51480], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 7, "seek": 0, "start": 22.32, "end": 25.2, "text": " A lot of safety research could not have happened with open source.", "tokens": [51480, 316, 688, 295, 4514, 2132, 727, 406, 362, 2011, 365, 1269, 4009, 13, 51624], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 8, "seek": 0, "start": 25.2, "end": 28.88, "text": " At some point, the system is so powerful that you don't want it to be open source anymore.", "tokens": [51624, 1711, 512, 935, 11, 264, 1185, 307, 370, 4005, 300, 291, 500, 380, 528, 309, 281, 312, 1269, 4009, 3602, 13, 51808], "temperature": 0.0, "avg_logprob": -0.0869349233362059, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.013836754485964775}, {"id": 9, "seek": 2888, "start": 28.88, "end": 33.28, "text": " In the same way in which, you know, I don't want to open source the nuclear codes or like,", "tokens": [50364, 682, 264, 912, 636, 294, 597, 11, 291, 458, 11, 286, 500, 380, 528, 281, 1269, 4009, 264, 8179, 14211, 420, 411, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 10, "seek": 2888, "start": 33.28, "end": 37.04, "text": " you know, literally the recipe to build most viral pandemic or something.", "tokens": [50584, 291, 458, 11, 3736, 264, 6782, 281, 1322, 881, 16132, 5388, 420, 746, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 11, "seek": 2888, "start": 37.04, "end": 42.64, "text": " The labs maybe have the incentive to not say the worst things they found because otherwise", "tokens": [50772, 440, 20339, 1310, 362, 264, 22346, 281, 406, 584, 264, 5855, 721, 436, 1352, 570, 5911, 51052], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 12, "seek": 2888, "start": 42.64, "end": 43.84, "text": " they may lose their contract.", "tokens": [51052, 436, 815, 3624, 641, 4364, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 13, "seek": 2888, "start": 43.84, "end": 48.480000000000004, "text": " So you need something like the UKASF Institute or the USASF Institute.", "tokens": [51112, 407, 291, 643, 746, 411, 264, 7051, 3160, 37, 9446, 420, 264, 2546, 3160, 37, 9446, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 14, "seek": 2888, "start": 48.480000000000004, "end": 54.0, "text": " Make sure that there is a minimal set of standards that all the auditors have to adhere to.", "tokens": [51344, 4387, 988, 300, 456, 307, 257, 13206, 992, 295, 7787, 300, 439, 264, 2379, 9862, 362, 281, 33584, 281, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 15, "seek": 2888, "start": 54.0, "end": 56.32, "text": " Hello and welcome to The Cognitive Revolution,", "tokens": [51620, 2425, 293, 2928, 281, 440, 383, 2912, 2187, 16617, 11, 51736], "temperature": 0.0, "avg_logprob": -0.1302987552079998, "compression_ratio": 1.689419795221843, "no_speech_prob": 0.02593640796840191}, {"id": 16, "seek": 5632, "start": 56.32, "end": 60.72, "text": " where we interview visionary researchers, entrepreneurs, and builders working on the", "tokens": [50364, 689, 321, 4049, 49442, 10309, 11, 12639, 11, 293, 36281, 1364, 322, 264, 50584], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 17, "seek": 5632, "start": 60.72, "end": 62.4, "text": " frontier of artificial intelligence.", "tokens": [50584, 35853, 295, 11677, 7599, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 18, "seek": 5632, "start": 63.2, "end": 67.52, "text": " Each week, we'll explore their revolutionary ideas and together we'll build a picture of", "tokens": [50708, 6947, 1243, 11, 321, 603, 6839, 641, 22687, 3487, 293, 1214, 321, 603, 1322, 257, 3036, 295, 50924], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 19, "seek": 5632, "start": 67.52, "end": 72.56, "text": " how AI technology will transform work, life, and society in the coming years.", "tokens": [50924, 577, 7318, 2899, 486, 4088, 589, 11, 993, 11, 293, 4086, 294, 264, 1348, 924, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 20, "seek": 5632, "start": 73.12, "end": 76.24000000000001, "text": " I'm Nathan LaBenz, joined by my co-host Eric Torenberg.", "tokens": [51204, 286, 478, 20634, 2369, 33, 11368, 11, 6869, 538, 452, 598, 12, 6037, 9336, 314, 10948, 6873, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 21, "seek": 5632, "start": 77.03999999999999, "end": 79.68, "text": " Hello and welcome back to The Cognitive Revolution.", "tokens": [51400, 2425, 293, 2928, 646, 281, 440, 383, 2912, 2187, 16617, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 22, "seek": 5632, "start": 80.64, "end": 85.28, "text": " Today, my guest is Marius Haban, founder and CEO of Apollo Research,", "tokens": [51580, 2692, 11, 452, 8341, 307, 2039, 4872, 14225, 282, 11, 14917, 293, 9282, 295, 25187, 10303, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1404124150233986, "compression_ratio": 1.5296052631578947, "no_speech_prob": 0.010645629838109016}, {"id": 23, "seek": 8528, "start": 85.28, "end": 90.88, "text": " a nonprofit AI safety research group that is working to understand both how AI systems behave", "tokens": [50364, 257, 23348, 7318, 4514, 2132, 1594, 300, 307, 1364, 281, 1223, 1293, 577, 7318, 3652, 15158, 50644], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 24, "seek": 8528, "start": 91.44, "end": 92.16, "text": " and why.", "tokens": [50672, 293, 983, 13, 50708], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 25, "seek": 8528, "start": 93.76, "end": 99.12, "text": " Their approach combines exploratory and hypothesis-driven testing, fine-tuning experiments,", "tokens": [50788, 6710, 3109, 29520, 24765, 4745, 293, 17291, 12, 25456, 4997, 11, 2489, 12, 83, 37726, 12050, 11, 51056], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 26, "seek": 8528, "start": 99.12, "end": 100.96000000000001, "text": " and interpretability research.", "tokens": [51056, 293, 7302, 2310, 2132, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 27, "seek": 8528, "start": 101.52000000000001, "end": 107.04, "text": " And as you'll hear, they place special emphasis on the potential for AI systems to deceive", "tokens": [51176, 400, 382, 291, 603, 1568, 11, 436, 1081, 2121, 16271, 322, 264, 3995, 337, 7318, 3652, 281, 43440, 51452], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 28, "seek": 8528, "start": 107.04, "end": 108.0, "text": " their human users.", "tokens": [51452, 641, 1952, 5022, 13, 51500], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 29, "seek": 8528, "start": 109.2, "end": 113.76, "text": " In this conversation, we look first at Apollo's starting framework for their work,", "tokens": [51560, 682, 341, 3761, 11, 321, 574, 700, 412, 25187, 311, 2891, 8388, 337, 641, 589, 11, 51788], "temperature": 0.0, "avg_logprob": -0.08817858445016961, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.0010983613319694996}, {"id": 30, "seek": 11376, "start": 113.84, "end": 117.28, "text": " which emphasizes the importance of affordances in AI systems.", "tokens": [50368, 597, 48856, 264, 7379, 295, 6157, 2676, 294, 7318, 3652, 13, 50540], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 31, "seek": 11376, "start": 117.92, "end": 123.76, "text": " That is, through what tools, actuators, or other means can the system affect the broader world?", "tokens": [50572, 663, 307, 11, 807, 437, 3873, 11, 34964, 3391, 11, 420, 661, 1355, 393, 264, 1185, 3345, 264, 13227, 1002, 30, 50864], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 32, "seek": 11376, "start": 124.88000000000001, "end": 128.88, "text": " And they also introduce a number of new conceptual distinctions meant to help people have more", "tokens": [50920, 400, 436, 611, 5366, 257, 1230, 295, 777, 24106, 1483, 49798, 4140, 281, 854, 561, 362, 544, 51120], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 33, "seek": 11376, "start": 128.88, "end": 132.88, "text": " precise and productive conversations about these nuanced topics.", "tokens": [51120, 13600, 293, 13304, 7315, 466, 613, 45115, 8378, 13, 51320], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 34, "seek": 11376, "start": 134.24, "end": 137.76, "text": " Then in the second half, we look at their first research result,", "tokens": [51388, 1396, 294, 264, 1150, 1922, 11, 321, 574, 412, 641, 700, 2132, 1874, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 35, "seek": 11376, "start": 137.76, "end": 141.20000000000002, "text": " which demonstrates, to my knowledge, for the first time in a realistic,", "tokens": [51564, 597, 31034, 11, 281, 452, 3601, 11, 337, 264, 700, 565, 294, 257, 12465, 11, 51736], "temperature": 0.0, "avg_logprob": -0.07361640080366985, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0014097063103690743}, {"id": 36, "seek": 14120, "start": 141.2, "end": 148.39999999999998, "text": " unprompted setting that GPT-4, when put under pressure, will sometimes take unethical and even", "tokens": [50364, 517, 28722, 25383, 3287, 300, 26039, 51, 12, 19, 11, 562, 829, 833, 3321, 11, 486, 2171, 747, 517, 3293, 804, 293, 754, 50724], "temperature": 0.0, "avg_logprob": -0.08093909294374528, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0020500856917351484}, {"id": 37, "seek": 14120, "start": 148.39999999999998, "end": 153.6, "text": " illegal actions and then go on to lie to its users about what it did and why.", "tokens": [50724, 11905, 5909, 293, 550, 352, 322, 281, 4544, 281, 1080, 5022, 466, 437, 309, 630, 293, 983, 13, 50984], "temperature": 0.0, "avg_logprob": -0.08093909294374528, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0020500856917351484}, {"id": 38, "seek": 14120, "start": 154.95999999999998, "end": 160.07999999999998, "text": " This is an important result, demonstrating that while the risk from AI systems may start with", "tokens": [51052, 639, 307, 364, 1021, 1874, 11, 29889, 300, 1339, 264, 3148, 490, 7318, 3652, 815, 722, 365, 51308], "temperature": 0.0, "avg_logprob": -0.08093909294374528, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0020500856917351484}, {"id": 39, "seek": 14120, "start": 160.07999999999998, "end": 167.92, "text": " and may even be dominated by intentional human misuse, the models themselves can also misbehave", "tokens": [51308, 293, 815, 754, 312, 23755, 538, 21935, 1952, 3346, 438, 11, 264, 5245, 2969, 393, 611, 3346, 650, 24284, 51700], "temperature": 0.0, "avg_logprob": -0.08093909294374528, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0020500856917351484}, {"id": 40, "seek": 14120, "start": 167.92, "end": 169.2, "text": " in unexpected ways.", "tokens": [51700, 294, 13106, 2098, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08093909294374528, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0020500856917351484}, {"id": 41, "seek": 16920, "start": 169.83999999999997, "end": 174.56, "text": " As an aside, since I told my behind-the-scenes GPT-4 red team story a few weeks ago,", "tokens": [50396, 1018, 364, 7359, 11, 1670, 286, 1907, 452, 2261, 12, 3322, 12, 36324, 26039, 51, 12, 19, 2182, 1469, 1657, 257, 1326, 3259, 2057, 11, 50632], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 42, "seek": 16920, "start": 174.56, "end": 179.11999999999998, "text": " a number of people have reached out to ask me how they too can get involved with red teaming projects.", "tokens": [50632, 257, 1230, 295, 561, 362, 6488, 484, 281, 1029, 385, 577, 436, 886, 393, 483, 3288, 365, 2182, 1469, 278, 4455, 13, 50860], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 43, "seek": 16920, "start": 180.16, "end": 184.88, "text": " Unfortunately, as commercial competition and secrecy both continue to ramp up across the space,", "tokens": [50912, 8590, 11, 382, 6841, 6211, 293, 34432, 1344, 1293, 2354, 281, 12428, 493, 2108, 264, 1901, 11, 51148], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 44, "seek": 16920, "start": 185.44, "end": 189.44, "text": " I don't see as many open calls for volunteer red teamers as I used to,", "tokens": [51176, 286, 500, 380, 536, 382, 867, 1269, 5498, 337, 13835, 2182, 1469, 433, 382, 286, 1143, 281, 11, 51376], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 45, "seek": 16920, "start": 190.0, "end": 192.56, "text": " certainly not for unreleased frontier models.", "tokens": [51404, 3297, 406, 337, 20584, 41087, 35853, 5245, 13, 51532], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 46, "seek": 16920, "start": 193.92, "end": 197.92, "text": " Instead, the field is becoming more professionalized, with all the leading labs,", "tokens": [51600, 7156, 11, 264, 2519, 307, 5617, 544, 4843, 1602, 11, 365, 439, 264, 5775, 20339, 11, 51800], "temperature": 0.0, "avg_logprob": -0.078141983350118, "compression_ratio": 1.5667752442996743, "no_speech_prob": 0.001098492182791233}, {"id": 47, "seek": 19792, "start": 197.92, "end": 203.35999999999999, "text": " as well as the data companies like Scale AI, plus the independent auditing organizations like Apollo,", "tokens": [50364, 382, 731, 382, 264, 1412, 3431, 411, 42999, 7318, 11, 1804, 264, 6695, 2379, 1748, 6150, 411, 25187, 11, 50636], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 48, "seek": 19792, "start": 203.35999999999999, "end": 208.95999999999998, "text": " ArchieVals, now known as Meter, Palisade, and also AI Forensics, all actively hiring", "tokens": [50636, 10984, 414, 53, 1124, 11, 586, 2570, 382, 38054, 11, 6116, 271, 762, 11, 293, 611, 7318, 9018, 3695, 1167, 11, 439, 13022, 15335, 50916], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 49, "seek": 19792, "start": 208.95999999999998, "end": 211.6, "text": " research scientists and engineers in this area.", "tokens": [50916, 2132, 7708, 293, 11955, 294, 341, 1859, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 50, "seek": 19792, "start": 212.88, "end": 217.2, "text": " So does that mean that there's no longer a role for the independent hobbyist red teamer to play?", "tokens": [51112, 407, 775, 300, 914, 300, 456, 311, 572, 2854, 257, 3090, 337, 264, 6695, 18240, 468, 2182, 1469, 260, 281, 862, 30, 51328], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 51, "seek": 19792, "start": 217.92, "end": 222.64, "text": " On the contrary, there is a ton left to discover even on publicly released models,", "tokens": [51364, 1282, 264, 19506, 11, 456, 307, 257, 2952, 1411, 281, 4411, 754, 322, 14843, 4736, 5245, 11, 51600], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 52, "seek": 19792, "start": 222.64, "end": 227.6, "text": " and the best way to break into the field is to demonstrate your ability to discover new phenomena.", "tokens": [51600, 293, 264, 1151, 636, 281, 1821, 666, 264, 2519, 307, 281, 11698, 428, 3485, 281, 4411, 777, 22004, 13, 51848], "temperature": 0.0, "avg_logprob": -0.12204519647066711, "compression_ratio": 1.6495176848874598, "no_speech_prob": 0.00028678763192147017}, {"id": 53, "seek": 22792, "start": 228.72, "end": 233.83999999999997, "text": " Importantly, the work we cover in this episode could have been done by anyone with an open AI", "tokens": [50404, 26391, 3627, 11, 264, 589, 321, 2060, 294, 341, 3500, 727, 362, 668, 1096, 538, 2878, 365, 364, 1269, 7318, 50660], "temperature": 0.0, "avg_logprob": -0.05835035017558506, "compression_ratio": 1.5809859154929577, "no_speech_prob": 0.0005192089010961354}, {"id": 54, "seek": 22792, "start": 233.83999999999997, "end": 239.92, "text": " account, a knack for prompting, and just a tiny bit of coding know-how. No special access or", "tokens": [50660, 2696, 11, 257, 444, 501, 337, 12391, 278, 11, 293, 445, 257, 5870, 857, 295, 17720, 458, 12, 4286, 13, 883, 2121, 2105, 420, 50964], "temperature": 0.0, "avg_logprob": -0.05835035017558506, "compression_ratio": 1.5809859154929577, "no_speech_prob": 0.0005192089010961354}, {"id": 55, "seek": 22792, "start": 239.92, "end": 244.79999999999998, "text": " advanced machine learning techniques were required, just a lot of curiosity.", "tokens": [50964, 7339, 3479, 2539, 7512, 645, 4739, 11, 445, 257, 688, 295, 18769, 13, 51208], "temperature": 0.0, "avg_logprob": -0.05835035017558506, "compression_ratio": 1.5809859154929577, "no_speech_prob": 0.0005192089010961354}, {"id": 56, "seek": 22792, "start": 246.07999999999998, "end": 250.0, "text": " With that in mind, if you want to get into this line of work but aren't sure where to start,", "tokens": [51272, 2022, 300, 294, 1575, 11, 498, 291, 528, 281, 483, 666, 341, 1622, 295, 589, 457, 3212, 380, 988, 689, 281, 722, 11, 51468], "temperature": 0.0, "avg_logprob": -0.05835035017558506, "compression_ratio": 1.5809859154929577, "no_speech_prob": 0.0005192089010961354}, {"id": 57, "seek": 22792, "start": 250.0, "end": 255.51999999999998, "text": " I encourage you to reach out. I'll be happy to help brainstorm or refine your project ideas,", "tokens": [51468, 286, 5373, 291, 281, 2524, 484, 13, 286, 603, 312, 2055, 281, 854, 35245, 420, 33906, 428, 1716, 3487, 11, 51744], "temperature": 0.0, "avg_logprob": -0.05835035017558506, "compression_ratio": 1.5809859154929577, "no_speech_prob": 0.0005192089010961354}, {"id": 58, "seek": 25552, "start": 255.60000000000002, "end": 260.48, "text": " and I can also help connect you with folks at the top companies who do sometimes provide API", "tokens": [50368, 293, 286, 393, 611, 854, 1745, 291, 365, 4024, 412, 264, 1192, 3431, 567, 360, 2171, 2893, 9362, 50612], "temperature": 0.0, "avg_logprob": -0.05389793784217497, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.0028002443723380566}, {"id": 59, "seek": 25552, "start": 260.48, "end": 265.44, "text": " credits to independent researchers working in this area, if and when you can achieve a meaningful", "tokens": [50612, 16816, 281, 6695, 10309, 1364, 294, 341, 1859, 11, 498, 293, 562, 291, 393, 4584, 257, 10995, 50860], "temperature": 0.0, "avg_logprob": -0.05389793784217497, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.0028002443723380566}, {"id": 60, "seek": 25552, "start": 265.44, "end": 271.36, "text": " result. As always, we appreciate the time that you spend listening to The Cognitive Revolution,", "tokens": [50860, 1874, 13, 1018, 1009, 11, 321, 4449, 264, 565, 300, 291, 3496, 4764, 281, 440, 383, 2912, 2187, 16617, 11, 51156], "temperature": 0.0, "avg_logprob": -0.05389793784217497, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.0028002443723380566}, {"id": 61, "seek": 25552, "start": 271.36, "end": 277.04, "text": " and we hope it's a valuable guide to the AI era. If you feel that it is, we would love a review", "tokens": [51156, 293, 321, 1454, 309, 311, 257, 8263, 5934, 281, 264, 7318, 4249, 13, 759, 291, 841, 300, 309, 307, 11, 321, 576, 959, 257, 3131, 51440], "temperature": 0.0, "avg_logprob": -0.05389793784217497, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.0028002443723380566}, {"id": 62, "seek": 25552, "start": 277.04, "end": 282.24, "text": " on Apple Podcasts or Spotify, and we of course encourage you to share the show with your friends.", "tokens": [51440, 322, 6373, 29972, 82, 420, 29036, 11, 293, 321, 295, 1164, 5373, 291, 281, 2073, 264, 855, 365, 428, 1855, 13, 51700], "temperature": 0.0, "avg_logprob": -0.05389793784217497, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.0028002443723380566}, {"id": 63, "seek": 28224, "start": 283.2, "end": 289.68, "text": " Now, here's my conversation on frontier AI safety work with Marius Habhan of Apollo Research.", "tokens": [50412, 823, 11, 510, 311, 452, 3761, 322, 35853, 7318, 4514, 589, 365, 2039, 4872, 14225, 3451, 295, 25187, 10303, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09832965626436121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.005058843642473221}, {"id": 64, "seek": 28224, "start": 290.88, "end": 295.92, "text": " Marius Habhan, founder and CEO of Apollo Research, welcome to The Cognitive Revolution.", "tokens": [50796, 2039, 4872, 14225, 3451, 11, 14917, 293, 9282, 295, 25187, 10303, 11, 2928, 281, 440, 383, 2912, 2187, 16617, 13, 51048], "temperature": 0.0, "avg_logprob": -0.09832965626436121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.005058843642473221}, {"id": 65, "seek": 28224, "start": 296.72, "end": 298.16, "text": " Hey, thanks for having me.", "tokens": [51088, 1911, 11, 3231, 337, 1419, 385, 13, 51160], "temperature": 0.0, "avg_logprob": -0.09832965626436121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.005058843642473221}, {"id": 66, "seek": 28224, "start": 298.16, "end": 303.6, "text": " I am very excited to have you. So, regular listeners of the show will know that I'm a big", "tokens": [51160, 286, 669, 588, 2919, 281, 362, 291, 13, 407, 11, 3890, 23274, 295, 264, 855, 486, 458, 300, 286, 478, 257, 955, 51432], "temperature": 0.0, "avg_logprob": -0.09832965626436121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.005058843642473221}, {"id": 67, "seek": 28224, "start": 303.6, "end": 311.76, "text": " believer in the importance of hands-on testing of what AI systems can do, and also that I have", "tokens": [51432, 23892, 294, 264, 7379, 295, 2377, 12, 266, 4997, 295, 437, 7318, 3652, 393, 360, 11, 293, 611, 300, 286, 362, 51840], "temperature": 0.0, "avg_logprob": -0.09832965626436121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.005058843642473221}, {"id": 68, "seek": 31176, "start": 311.76, "end": 318.0, "text": " been pretty enthusiastic consumer of the news when some of the leading labs have made public", "tokens": [50364, 668, 1238, 28574, 9711, 295, 264, 2583, 562, 512, 295, 264, 5775, 20339, 362, 1027, 1908, 50676], "temperature": 0.0, "avg_logprob": -0.06643908619880676, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0012061127927154303}, {"id": 69, "seek": 31176, "start": 318.0, "end": 324.8, "text": " commitments to allow organizations outside of their own teams to look at the systems that", "tokens": [50676, 26230, 281, 2089, 6150, 2380, 295, 641, 1065, 5491, 281, 574, 412, 264, 3652, 300, 51016], "temperature": 0.0, "avg_logprob": -0.06643908619880676, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0012061127927154303}, {"id": 70, "seek": 31176, "start": 324.8, "end": 330.88, "text": " they're building before they get deployed. And so, your work with Apollo Research, which is", "tokens": [51016, 436, 434, 2390, 949, 436, 483, 17826, 13, 400, 370, 11, 428, 589, 365, 25187, 10303, 11, 597, 307, 51320], "temperature": 0.0, "avg_logprob": -0.06643908619880676, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0012061127927154303}, {"id": 71, "seek": 31176, "start": 331.92, "end": 336.48, "text": " trying to build, as I understand it, an organization to meet that need and actually work with those", "tokens": [51372, 1382, 281, 1322, 11, 382, 286, 1223, 309, 11, 364, 4475, 281, 1677, 300, 643, 293, 767, 589, 365, 729, 51600], "temperature": 0.0, "avg_logprob": -0.06643908619880676, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0012061127927154303}, {"id": 72, "seek": 33648, "start": 336.48, "end": 342.24, "text": " leading labs in part, at least, on understanding the systems that they're developing before they", "tokens": [50364, 5775, 20339, 294, 644, 11, 412, 1935, 11, 322, 3701, 264, 3652, 300, 436, 434, 6416, 949, 436, 50652], "temperature": 0.0, "avg_logprob": -0.10542192459106445, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.12753826379776}, {"id": 73, "seek": 33648, "start": 342.24, "end": 348.48, "text": " get to widespread deployment, I think is super interesting, and I'm very excited to unpack the", "tokens": [50652, 483, 281, 22679, 19317, 11, 286, 519, 307, 1687, 1880, 11, 293, 286, 478, 588, 2919, 281, 26699, 264, 50964], "temperature": 0.0, "avg_logprob": -0.10542192459106445, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.12753826379776}, {"id": 74, "seek": 33648, "start": 348.48, "end": 353.52000000000004, "text": " details of it with you. Maybe for starters, you want to just kind of give us the quick overview on", "tokens": [50964, 4365, 295, 309, 365, 291, 13, 2704, 337, 35131, 11, 291, 528, 281, 445, 733, 295, 976, 505, 264, 1702, 12492, 322, 51216], "temperature": 0.0, "avg_logprob": -0.10542192459106445, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.12753826379776}, {"id": 75, "seek": 33648, "start": 353.52000000000004, "end": 358.48, "text": " Apollo Research, like how you decided to set out to found it. I'm interested a little bit in the", "tokens": [51216, 25187, 10303, 11, 411, 577, 291, 3047, 281, 992, 484, 281, 1352, 309, 13, 286, 478, 3102, 257, 707, 857, 294, 264, 51464], "temperature": 0.0, "avg_logprob": -0.10542192459106445, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.12753826379776}, {"id": 76, "seek": 33648, "start": 358.48, "end": 363.92, "text": " timeline of how that related to some of the commitments that the labs have made and what", "tokens": [51464, 12933, 295, 577, 300, 4077, 281, 512, 295, 264, 26230, 300, 264, 20339, 362, 1027, 293, 437, 51736], "temperature": 0.0, "avg_logprob": -0.10542192459106445, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.12753826379776}, {"id": 77, "seek": 36392, "start": 363.92, "end": 368.64000000000004, "text": " you guys are trying to do in the big picture. So, I think on a high level, it's sort of trying to", "tokens": [50364, 291, 1074, 366, 1382, 281, 360, 294, 264, 955, 3036, 13, 407, 11, 286, 519, 322, 257, 1090, 1496, 11, 309, 311, 1333, 295, 1382, 281, 50600], "temperature": 0.0, "avg_logprob": -0.10458860591966279, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.008059464395046234}, {"id": 78, "seek": 36392, "start": 368.64000000000004, "end": 375.04, "text": " understand what is going on in AI systems. And the reason for this was, or still is, in fact,", "tokens": [50600, 1223, 437, 307, 516, 322, 294, 7318, 3652, 13, 400, 264, 1778, 337, 341, 390, 11, 420, 920, 307, 11, 294, 1186, 11, 50920], "temperature": 0.0, "avg_logprob": -0.10458860591966279, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.008059464395046234}, {"id": 79, "seek": 36392, "start": 375.68, "end": 381.52000000000004, "text": " yeah, I basically think right now, we just lack information to make good decisions. There's loads", "tokens": [50952, 1338, 11, 286, 1936, 519, 558, 586, 11, 321, 445, 5011, 1589, 281, 652, 665, 5327, 13, 821, 311, 12668, 51244], "temperature": 0.0, "avg_logprob": -0.10458860591966279, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.008059464395046234}, {"id": 80, "seek": 36392, "start": 381.52000000000004, "end": 387.76, "text": " of uncertainty that we have about what could go wrong, whether we are already at a point where", "tokens": [51244, 295, 15697, 300, 321, 362, 466, 437, 727, 352, 2085, 11, 1968, 321, 366, 1217, 412, 257, 935, 689, 51556], "temperature": 0.0, "avg_logprob": -0.10458860591966279, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.008059464395046234}, {"id": 81, "seek": 38776, "start": 387.76, "end": 395.76, "text": " things go wrong, or how far away we are from these points. And yeah, we're trying to reduce this", "tokens": [50364, 721, 352, 2085, 11, 420, 577, 1400, 1314, 321, 366, 490, 613, 2793, 13, 400, 1338, 11, 321, 434, 1382, 281, 5407, 341, 50764], "temperature": 0.0, "avg_logprob": -0.09872955083847046, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.03730437159538269}, {"id": 82, "seek": 38776, "start": 395.76, "end": 402.15999999999997, "text": " uncertainty. And this is mostly through research, auditing, and governance. And on the research side,", "tokens": [50764, 15697, 13, 400, 341, 307, 5240, 807, 2132, 11, 2379, 1748, 11, 293, 17449, 13, 400, 322, 264, 2132, 1252, 11, 51084], "temperature": 0.0, "avg_logprob": -0.09872955083847046, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.03730437159538269}, {"id": 83, "seek": 38776, "start": 402.15999999999997, "end": 408.0, "text": " it's really split between interpretability and behavioral evils, half-half. But in the long", "tokens": [51084, 309, 311, 534, 7472, 1296, 7302, 2310, 293, 19124, 1073, 4174, 11, 1922, 12, 25461, 13, 583, 294, 264, 938, 51376], "temperature": 0.0, "avg_logprob": -0.09872955083847046, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.03730437159538269}, {"id": 84, "seek": 38776, "start": 408.0, "end": 414.48, "text": " run, we really want to merge them both, because I basically think what we need in the long run is", "tokens": [51376, 1190, 11, 321, 534, 528, 281, 22183, 552, 1293, 11, 570, 286, 1936, 519, 437, 321, 643, 294, 264, 938, 1190, 307, 51700], "temperature": 0.0, "avg_logprob": -0.09872955083847046, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.03730437159538269}, {"id": 85, "seek": 41448, "start": 414.48, "end": 419.52000000000004, "text": " both a mixture of behavioral and interpretability evils, so that we can really understand", "tokens": [50364, 1293, 257, 9925, 295, 19124, 293, 7302, 2310, 1073, 4174, 11, 370, 300, 321, 393, 534, 1223, 50616], "temperature": 0.0, "avg_logprob": -0.08292006337365439, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.009405035525560379}, {"id": 86, "seek": 41448, "start": 419.52000000000004, "end": 423.76, "text": " what the model is doing, and then also why it is doing this in the first place, because each of", "tokens": [50616, 437, 264, 2316, 307, 884, 11, 293, 550, 611, 983, 309, 307, 884, 341, 294, 264, 700, 1081, 11, 570, 1184, 295, 50828], "temperature": 0.0, "avg_logprob": -0.08292006337365439, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.009405035525560379}, {"id": 87, "seek": 41448, "start": 423.76, "end": 431.52000000000004, "text": " them individually seems somewhat insufficient. And yeah, maybe to go into the origin story,", "tokens": [50828, 552, 16652, 2544, 8344, 41709, 13, 400, 1338, 11, 1310, 281, 352, 666, 264, 4957, 1657, 11, 51216], "temperature": 0.0, "avg_logprob": -0.08292006337365439, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.009405035525560379}, {"id": 88, "seek": 41448, "start": 431.52000000000004, "end": 437.52000000000004, "text": " it has actually nothing to do with the commitments of the different labs. It was mostly that at the", "tokens": [51216, 309, 575, 767, 1825, 281, 360, 365, 264, 26230, 295, 264, 819, 20339, 13, 467, 390, 5240, 300, 412, 264, 51516], "temperature": 0.0, "avg_logprob": -0.08292006337365439, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.009405035525560379}, {"id": 89, "seek": 43752, "start": 437.52, "end": 446.32, "text": " beginning of this year, I kind of felt like I had a pretty clear picture of what is lacking in the", "tokens": [50364, 2863, 295, 341, 1064, 11, 286, 733, 295, 2762, 411, 286, 632, 257, 1238, 1850, 3036, 295, 437, 307, 20889, 294, 264, 50804], "temperature": 0.0, "avg_logprob": -0.09341105393001012, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.07912568002939224}, {"id": 90, "seek": 43752, "start": 446.32, "end": 451.12, "text": " current space with deceptive alignment, and evaluating deceptive alignment, or models for", "tokens": [50804, 2190, 1901, 365, 368, 1336, 488, 18515, 11, 293, 27479, 368, 1336, 488, 18515, 11, 420, 5245, 337, 51044], "temperature": 0.0, "avg_logprob": -0.09341105393001012, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.07912568002939224}, {"id": 91, "seek": 43752, "start": 451.12, "end": 456.32, "text": " deceptive alignment in the first place. And interpretability and evils just seemed like", "tokens": [51044, 368, 1336, 488, 18515, 294, 264, 700, 1081, 13, 400, 7302, 2310, 293, 1073, 4174, 445, 6576, 411, 51304], "temperature": 0.0, "avg_logprob": -0.09341105393001012, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.07912568002939224}, {"id": 92, "seek": 43752, "start": 456.32, "end": 461.03999999999996, "text": " the obvious things to do. So in the beginning, we basically set out to do mostly research.", "tokens": [51304, 264, 6322, 721, 281, 360, 13, 407, 294, 264, 2863, 11, 321, 1936, 992, 484, 281, 360, 5240, 2132, 13, 51540], "temperature": 0.0, "avg_logprob": -0.09341105393001012, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.07912568002939224}, {"id": 93, "seek": 43752, "start": 461.03999999999996, "end": 466.71999999999997, "text": " And only then, over time, we realized, hey, this is something that should be applied in the real", "tokens": [51540, 400, 787, 550, 11, 670, 565, 11, 321, 5334, 11, 4177, 11, 341, 307, 746, 300, 820, 312, 6456, 294, 264, 957, 51824], "temperature": 0.0, "avg_logprob": -0.09341105393001012, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.07912568002939224}, {"id": 94, "seek": 46672, "start": 466.72, "end": 474.40000000000003, "text": " world as soon as possible, because systems are getting better all the time. And we may actually", "tokens": [50364, 1002, 382, 2321, 382, 1944, 11, 570, 3652, 366, 1242, 1101, 439, 264, 565, 13, 400, 321, 815, 767, 50748], "temperature": 0.0, "avg_logprob": -0.09382082189171059, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.007003919221460819}, {"id": 95, "seek": 46672, "start": 474.40000000000003, "end": 479.92, "text": " hit this point fairly soon where models are already about at the threshold of deceptive,", "tokens": [50748, 2045, 341, 935, 6457, 2321, 689, 5245, 366, 1217, 466, 412, 264, 14678, 295, 368, 1336, 488, 11, 51024], "temperature": 0.0, "avg_logprob": -0.09382082189171059, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.007003919221460819}, {"id": 96, "seek": 46672, "start": 479.92, "end": 485.76000000000005, "text": " of capabilities for deceptive alignment. And then there is a small part in the organization that", "tokens": [51024, 295, 10862, 337, 368, 1336, 488, 18515, 13, 400, 550, 456, 307, 257, 1359, 644, 294, 264, 4475, 300, 51316], "temperature": 0.0, "avg_logprob": -0.09382082189171059, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.007003919221460819}, {"id": 97, "seek": 46672, "start": 485.76000000000005, "end": 490.8, "text": " is governance, which originally we also didn't really intend to do for the first two years or", "tokens": [51316, 307, 17449, 11, 597, 7993, 321, 611, 994, 380, 534, 19759, 281, 360, 337, 264, 700, 732, 924, 420, 51568], "temperature": 0.0, "avg_logprob": -0.09382082189171059, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.007003919221460819}, {"id": 98, "seek": 46672, "start": 490.8, "end": 495.92, "text": " something, because we thought we really need to understand all the research very well before", "tokens": [51568, 746, 11, 570, 321, 1194, 321, 534, 643, 281, 1223, 439, 264, 2132, 588, 731, 949, 51824], "temperature": 0.0, "avg_logprob": -0.09382082189171059, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.007003919221460819}, {"id": 99, "seek": 49592, "start": 495.92, "end": 501.28000000000003, "text": " we can talk to the people in governments and decision makers and lawmakers, because otherwise", "tokens": [50364, 321, 393, 751, 281, 264, 561, 294, 11280, 293, 3537, 19323, 293, 40988, 11, 570, 5911, 50632], "temperature": 0.0, "avg_logprob": -0.1272283150599553, "compression_ratio": 1.7276119402985075, "no_speech_prob": 0.014035258442163467}, {"id": 100, "seek": 49592, "start": 501.28000000000003, "end": 506.64000000000004, "text": " we're telling them things we aren't super confident in. And then lots of things happen.", "tokens": [50632, 321, 434, 3585, 552, 721, 321, 3212, 380, 1687, 6679, 294, 13, 400, 550, 3195, 295, 721, 1051, 13, 50900], "temperature": 0.0, "avg_logprob": -0.1272283150599553, "compression_ratio": 1.7276119402985075, "no_speech_prob": 0.014035258442163467}, {"id": 101, "seek": 49592, "start": 507.28000000000003, "end": 514.32, "text": " Governments and lawmakers actually got interested in AI and AI safety in particular. And then when", "tokens": [50932, 5515, 1117, 293, 40988, 767, 658, 3102, 294, 7318, 293, 7318, 4514, 294, 1729, 13, 400, 550, 562, 51284], "temperature": 0.0, "avg_logprob": -0.1272283150599553, "compression_ratio": 1.7276119402985075, "no_speech_prob": 0.014035258442163467}, {"id": 102, "seek": 49592, "start": 514.32, "end": 520.24, "text": " we talked to them, we realized the difference. We are very, very well placed to talk about these", "tokens": [51284, 321, 2825, 281, 552, 11, 321, 5334, 264, 2649, 13, 492, 366, 588, 11, 588, 731, 7074, 281, 751, 466, 613, 51580], "temperature": 0.0, "avg_logprob": -0.1272283150599553, "compression_ratio": 1.7276119402985075, "no_speech_prob": 0.014035258442163467}, {"id": 103, "seek": 49592, "start": 520.24, "end": 525.84, "text": " things, because if you have thought about them in the background for like six, seven,", "tokens": [51580, 721, 11, 570, 498, 291, 362, 1194, 466, 552, 294, 264, 3678, 337, 411, 2309, 11, 3407, 11, 51860], "temperature": 0.0, "avg_logprob": -0.1272283150599553, "compression_ratio": 1.7276119402985075, "no_speech_prob": 0.014035258442163467}, {"id": 104, "seek": 52584, "start": 526.0, "end": 531.6, "text": " years, and then specifically about some topic for six months or so, you are among the world's", "tokens": [50372, 924, 11, 293, 550, 4682, 466, 512, 4829, 337, 2309, 2493, 420, 370, 11, 291, 366, 3654, 264, 1002, 311, 50652], "temperature": 0.0, "avg_logprob": -0.08308179662861956, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.005381423979997635}, {"id": 105, "seek": 52584, "start": 531.6, "end": 537.36, "text": " experts. And this is kind of more like a reflection of the state of how bad it is about AI safety,", "tokens": [50652, 8572, 13, 400, 341, 307, 733, 295, 544, 411, 257, 12914, 295, 264, 1785, 295, 577, 1578, 309, 307, 466, 7318, 4514, 11, 50940], "temperature": 0.0, "avg_logprob": -0.08308179662861956, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.005381423979997635}, {"id": 106, "seek": 52584, "start": 537.36, "end": 543.6800000000001, "text": " where people in my position are actually sort of accidentally becoming the experts,", "tokens": [50940, 689, 561, 294, 452, 2535, 366, 767, 1333, 295, 15715, 5617, 264, 8572, 11, 51256], "temperature": 0.0, "avg_logprob": -0.08308179662861956, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.005381423979997635}, {"id": 107, "seek": 52584, "start": 544.48, "end": 550.64, "text": " rather than people with tens and 20 years of experience, because there aren't a lot of people", "tokens": [51296, 2831, 813, 561, 365, 10688, 293, 945, 924, 295, 1752, 11, 570, 456, 3212, 380, 257, 688, 295, 561, 51604], "temperature": 0.0, "avg_logprob": -0.08308179662861956, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.005381423979997635}, {"id": 108, "seek": 52584, "start": 550.64, "end": 554.8000000000001, "text": " in the world who have thought about AI safety for more than a couple of years, if at all.", "tokens": [51604, 294, 264, 1002, 567, 362, 1194, 466, 7318, 4514, 337, 544, 813, 257, 1916, 295, 924, 11, 498, 412, 439, 13, 51812], "temperature": 0.0, "avg_logprob": -0.08308179662861956, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.005381423979997635}, {"id": 109, "seek": 55480, "start": 554.8, "end": 561.76, "text": " Yeah, I can definitely relate to that sort of accidental expert status. I never expected to be", "tokens": [50364, 865, 11, 286, 393, 2138, 10961, 281, 300, 1333, 295, 38094, 5844, 6558, 13, 286, 1128, 5176, 281, 312, 50712], "temperature": 0.0, "avg_logprob": -0.1137338716959216, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.0004172883927822113}, {"id": 110, "seek": 55480, "start": 561.76, "end": 570.16, "text": " where I am and doing the things that I'm doing. But yeah, the whole AI field in some ways is kind", "tokens": [50712, 689, 286, 669, 293, 884, 264, 721, 300, 286, 478, 884, 13, 583, 1338, 11, 264, 1379, 7318, 2519, 294, 512, 2098, 307, 733, 51132], "temperature": 0.0, "avg_logprob": -0.1137338716959216, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.0004172883927822113}, {"id": 111, "seek": 55480, "start": 570.16, "end": 575.3599999999999, "text": " of the dog that caught the car. I always kind of come back to that metaphor where it's like", "tokens": [51132, 295, 264, 3000, 300, 5415, 264, 1032, 13, 286, 1009, 733, 295, 808, 646, 281, 300, 19157, 689, 309, 311, 411, 51392], "temperature": 0.0, "avg_logprob": -0.1137338716959216, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.0004172883927822113}, {"id": 112, "seek": 55480, "start": 576.3199999999999, "end": 580.88, "text": " we were just trying to build a bit more powerful AI, and all of a sudden we built like a lot more", "tokens": [51440, 321, 645, 445, 1382, 281, 1322, 257, 857, 544, 4005, 7318, 11, 293, 439, 295, 257, 3990, 321, 3094, 411, 257, 688, 544, 51668], "temperature": 0.0, "avg_logprob": -0.1137338716959216, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.0004172883927822113}, {"id": 113, "seek": 58088, "start": 580.96, "end": 586.24, "text": " powerful AI, and now we really kind of have to figure out what to do with it. So even a little", "tokens": [50368, 4005, 7318, 11, 293, 586, 321, 534, 733, 295, 362, 281, 2573, 484, 437, 281, 360, 365, 309, 13, 407, 754, 257, 707, 50632], "temperature": 0.0, "avg_logprob": -0.09632150163041785, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.014500018209218979}, {"id": 114, "seek": 58088, "start": 586.24, "end": 591.04, "text": " bit of advanced planning is better than, or a little bit of advanced thought is a lot better than", "tokens": [50632, 857, 295, 7339, 5038, 307, 1101, 813, 11, 420, 257, 707, 857, 295, 7339, 1194, 307, 257, 688, 1101, 813, 50872], "temperature": 0.0, "avg_logprob": -0.09632150163041785, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.014500018209218979}, {"id": 115, "seek": 58088, "start": 591.04, "end": 596.64, "text": " where most people are starting. Had you seen, when you actually started the organization,", "tokens": [50872, 689, 881, 561, 366, 2891, 13, 12298, 291, 1612, 11, 562, 291, 767, 1409, 264, 4475, 11, 51152], "temperature": 0.0, "avg_logprob": -0.09632150163041785, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.014500018209218979}, {"id": 116, "seek": 58088, "start": 596.64, "end": 603.68, "text": " had you seen GPT-4, or were you basing this decision on just what was public at the time?", "tokens": [51152, 632, 291, 1612, 26039, 51, 12, 19, 11, 420, 645, 291, 987, 278, 341, 3537, 322, 445, 437, 390, 1908, 412, 264, 565, 30, 51504], "temperature": 0.0, "avg_logprob": -0.09632150163041785, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.014500018209218979}, {"id": 117, "seek": 60368, "start": 604.56, "end": 613.04, "text": " Only what was public at the time. So the decision was made in February 2023, or at", "tokens": [50408, 5686, 437, 390, 1908, 412, 264, 565, 13, 407, 264, 3537, 390, 1027, 294, 8711, 44377, 11, 420, 412, 50832], "temperature": 0.0, "avg_logprob": -0.09510914966313526, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.011329852975904942}, {"id": 118, "seek": 60368, "start": 613.04, "end": 618.4, "text": " least sort of my internal commitment was made to this. I'm not sure whether GPT-4 was public", "tokens": [50832, 1935, 1333, 295, 452, 6920, 8371, 390, 1027, 281, 341, 13, 286, 478, 406, 988, 1968, 26039, 51, 12, 19, 390, 1908, 51100], "temperature": 0.0, "avg_logprob": -0.09510914966313526, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.011329852975904942}, {"id": 119, "seek": 60368, "start": 618.4, "end": 623.92, "text": " already at the time. Not quite, right? It was March. So no, it was independent of GPT-4.", "tokens": [51100, 1217, 412, 264, 565, 13, 1726, 1596, 11, 558, 30, 467, 390, 6129, 13, 407, 572, 11, 309, 390, 6695, 295, 26039, 51, 12, 19, 13, 51376], "temperature": 0.0, "avg_logprob": -0.09510914966313526, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.011329852975904942}, {"id": 120, "seek": 60368, "start": 624.8, "end": 630.7199999999999, "text": " Yeah, I always think that's interesting just because GPT-4 was such a wake-up moment for", "tokens": [51420, 865, 11, 286, 1009, 519, 300, 311, 1880, 445, 570, 26039, 51, 12, 19, 390, 1270, 257, 6634, 12, 1010, 1623, 337, 51716], "temperature": 0.0, "avg_logprob": -0.09510914966313526, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.011329852975904942}, {"id": 121, "seek": 63072, "start": 630.72, "end": 636.0, "text": " so many people, and certainly I would include myself in that. I was already extremely plugged", "tokens": [50364, 370, 867, 561, 11, 293, 3297, 286, 576, 4090, 2059, 294, 300, 13, 286, 390, 1217, 4664, 25679, 50628], "temperature": 0.0, "avg_logprob": -0.08562418993781595, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.02228216640651226}, {"id": 122, "seek": 63072, "start": 636.0, "end": 642.5600000000001, "text": " into what was going on and using it and fine-tuning tons of models on the open AI platform in particular,", "tokens": [50628, 666, 437, 390, 516, 322, 293, 1228, 309, 293, 2489, 12, 83, 37726, 9131, 295, 5245, 322, 264, 1269, 7318, 3663, 294, 1729, 11, 50956], "temperature": 0.0, "avg_logprob": -0.08562418993781595, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.02228216640651226}, {"id": 123, "seek": 63072, "start": 642.5600000000001, "end": 648.32, "text": " but then it was like, whoa, this thing is next level. It's not slowing down. We've gone from", "tokens": [50956, 457, 550, 309, 390, 411, 11, 13310, 11, 341, 551, 307, 958, 1496, 13, 467, 311, 406, 26958, 760, 13, 492, 600, 2780, 490, 51244], "temperature": 0.0, "avg_logprob": -0.08562418993781595, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.02228216640651226}, {"id": 124, "seek": 63072, "start": 648.32, "end": 654.4, "text": " sort of, I can put a lot of elbow grease in and get a fine-tuned model to do a particular task,", "tokens": [51244, 1333, 295, 11, 286, 393, 829, 257, 688, 295, 18507, 24867, 294, 293, 483, 257, 2489, 12, 83, 43703, 2316, 281, 360, 257, 1729, 5633, 11, 51548], "temperature": 0.0, "avg_logprob": -0.08562418993781595, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.02228216640651226}, {"id": 125, "seek": 65440, "start": 654.4, "end": 658.9599999999999, "text": " which already I thought was going to be economically transformative to,", "tokens": [50364, 597, 1217, 286, 1194, 390, 516, 281, 312, 26811, 36070, 281, 11, 50592], "temperature": 0.0, "avg_logprob": -0.0929887519692475, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.04335833340883255}, {"id": 126, "seek": 65440, "start": 660.0, "end": 663.52, "text": " I don't even need to do that, that I could just ask for a lot of these tasks and get", "tokens": [50644, 286, 500, 380, 754, 643, 281, 360, 300, 11, 300, 286, 727, 445, 1029, 337, 257, 688, 295, 613, 9608, 293, 483, 50820], "temperature": 0.0, "avg_logprob": -0.0929887519692475, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.04335833340883255}, {"id": 127, "seek": 65440, "start": 663.52, "end": 667.6, "text": " like pretty good zero-shot performance. For me, that was the moment where I was kind of like,", "tokens": [50820, 411, 1238, 665, 4018, 12, 18402, 3389, 13, 1171, 385, 11, 300, 390, 264, 1623, 689, 286, 390, 733, 295, 411, 11, 51024], "temperature": 0.0, "avg_logprob": -0.0929887519692475, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.04335833340883255}, {"id": 128, "seek": 65440, "start": 667.6, "end": 672.88, "text": " okay, this is going from a tool that I am really excited to use and having a lot of fun using", "tokens": [51024, 1392, 11, 341, 307, 516, 490, 257, 2290, 300, 286, 669, 534, 2919, 281, 764, 293, 1419, 257, 688, 295, 1019, 1228, 51288], "temperature": 0.0, "avg_logprob": -0.0929887519692475, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.04335833340883255}, {"id": 129, "seek": 65440, "start": 672.88, "end": 679.36, "text": " to something that seems like a force that needs to be understood from all angles.", "tokens": [51288, 281, 746, 300, 2544, 411, 257, 3464, 300, 2203, 281, 312, 7320, 490, 439, 14708, 13, 51612], "temperature": 0.0, "avg_logprob": -0.0929887519692475, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.04335833340883255}, {"id": 130, "seek": 67936, "start": 680.0, "end": 685.6, "text": " So let's unpack the perspective that you are bringing to this. I would encourage folks to", "tokens": [50396, 407, 718, 311, 26699, 264, 4585, 300, 291, 366, 5062, 281, 341, 13, 286, 576, 5373, 4024, 281, 50676], "temperature": 0.0, "avg_logprob": -0.08024842551584994, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.024418367072939873}, {"id": 131, "seek": 67936, "start": 685.6, "end": 691.04, "text": " look up these papers that we'll discuss and read them for themselves as well, but on the website,", "tokens": [50676, 574, 493, 613, 10577, 300, 321, 603, 2248, 293, 1401, 552, 337, 2969, 382, 731, 11, 457, 322, 264, 3144, 11, 50948], "temperature": 0.0, "avg_logprob": -0.08024842551584994, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.024418367072939873}, {"id": 132, "seek": 67936, "start": 691.04, "end": 698.24, "text": " you've got two recent publications. One is kind of a framework for organizing the work that you're", "tokens": [50948, 291, 600, 658, 732, 5162, 25618, 13, 1485, 307, 733, 295, 257, 8388, 337, 17608, 264, 589, 300, 291, 434, 51308], "temperature": 0.0, "avg_logprob": -0.08024842551584994, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.024418367072939873}, {"id": 133, "seek": 67936, "start": 698.24, "end": 703.6800000000001, "text": " going to do, and then the other is like a very detailed in the weeds investigation of a particular", "tokens": [51308, 516, 281, 360, 11, 293, 550, 264, 661, 307, 411, 257, 588, 9942, 294, 264, 26370, 9627, 295, 257, 1729, 51580], "temperature": 0.0, "avg_logprob": -0.08024842551584994, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.024418367072939873}, {"id": 134, "seek": 70368, "start": 703.76, "end": 709.3599999999999, "text": " AI behavior, namely deceiving the user, which I think is a super interesting and important", "tokens": [50368, 7318, 5223, 11, 20926, 14088, 2123, 264, 4195, 11, 597, 286, 519, 307, 257, 1687, 1880, 293, 1021, 50648], "temperature": 0.0, "avg_logprob": -0.11045086825335468, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.06368483603000641}, {"id": 135, "seek": 70368, "start": 709.3599999999999, "end": 714.0, "text": " one to study. But let's maybe just start with the big picture, like organizing the thoughts.", "tokens": [50648, 472, 281, 2979, 13, 583, 718, 311, 1310, 445, 722, 365, 264, 955, 3036, 11, 411, 17608, 264, 4598, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11045086825335468, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.06368483603000641}, {"id": 136, "seek": 70368, "start": 715.8399999999999, "end": 720.8, "text": " I get the sense that you think, again, well, you've kind of said this, and the paper certainly", "tokens": [50972, 286, 483, 264, 2020, 300, 291, 519, 11, 797, 11, 731, 11, 291, 600, 733, 295, 848, 341, 11, 293, 264, 3035, 3297, 51220], "temperature": 0.0, "avg_logprob": -0.11045086825335468, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.06368483603000641}, {"id": 137, "seek": 70368, "start": 720.8, "end": 724.9599999999999, "text": " reflects it, that there are like a lot of big questions that remain unanswered. So how do you", "tokens": [51220, 18926, 309, 11, 300, 456, 366, 411, 257, 688, 295, 955, 1651, 300, 6222, 517, 43904, 292, 13, 407, 577, 360, 291, 51428], "temperature": 0.0, "avg_logprob": -0.11045086825335468, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.06368483603000641}, {"id": 138, "seek": 70368, "start": 724.9599999999999, "end": 731.04, "text": " structure your approach to this topic given all the uncertainty that exists?", "tokens": [51428, 3877, 428, 3109, 281, 341, 4829, 2212, 439, 264, 15697, 300, 8198, 30, 51732], "temperature": 0.0, "avg_logprob": -0.11045086825335468, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.06368483603000641}, {"id": 139, "seek": 73104, "start": 731.76, "end": 738.48, "text": " Yeah, maybe to give a little bit of context. So this is only one paper of many in this space,", "tokens": [50400, 865, 11, 1310, 281, 976, 257, 707, 857, 295, 4319, 13, 407, 341, 307, 787, 472, 3035, 295, 867, 294, 341, 1901, 11, 50736], "temperature": 0.0, "avg_logprob": -0.12819824395356355, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.003944292198866606}, {"id": 140, "seek": 73104, "start": 738.48, "end": 743.28, "text": " and there is, I think earlier this year, there was a really big one called Model Evaluation", "tokens": [50736, 293, 456, 307, 11, 286, 519, 3071, 341, 1064, 11, 456, 390, 257, 534, 955, 472, 1219, 17105, 462, 46504, 50976], "temperature": 0.0, "avg_logprob": -0.12819824395356355, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.003944292198866606}, {"id": 141, "seek": 73104, "start": 743.28, "end": 749.52, "text": " for Extreme Risk, which we at Apollo definitely thought was a pretty good paper. And they're", "tokens": [50976, 337, 39525, 45892, 11, 597, 321, 412, 25187, 2138, 1194, 390, 257, 1238, 665, 3035, 13, 400, 436, 434, 51288], "temperature": 0.0, "avg_logprob": -0.12819824395356355, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.003944292198866606}, {"id": 142, "seek": 73104, "start": 749.52, "end": 755.12, "text": " sort of pointing out many of the very reasonable and important steps, or reasonable principles", "tokens": [51288, 1333, 295, 12166, 484, 867, 295, 264, 588, 10585, 293, 1021, 4439, 11, 420, 10585, 9156, 51568], "temperature": 0.0, "avg_logprob": -0.12819824395356355, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.003944292198866606}, {"id": 143, "seek": 73104, "start": 755.12, "end": 760.3199999999999, "text": " for external auditing, something like ramp up the auditing before you ramp up the exposure", "tokens": [51568, 337, 8320, 2379, 1748, 11, 746, 411, 12428, 493, 264, 2379, 1748, 949, 291, 12428, 493, 264, 10420, 51828], "temperature": 0.0, "avg_logprob": -0.12819824395356355, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.003944292198866606}, {"id": 144, "seek": 76032, "start": 760.32, "end": 767.2, "text": " to the real world and do this ahead of the curve, so to speak. But when we read the paper,", "tokens": [50364, 281, 264, 957, 1002, 293, 360, 341, 2286, 295, 264, 7605, 11, 370, 281, 1710, 13, 583, 562, 321, 1401, 264, 3035, 11, 50708], "temperature": 0.0, "avg_logprob": -0.10630648958999499, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.004608440678566694}, {"id": 145, "seek": 76032, "start": 767.2, "end": 772.5600000000001, "text": " we felt a little bit like, this makes sense for the current capabilities and sort of how", "tokens": [50708, 321, 2762, 257, 707, 857, 411, 11, 341, 1669, 2020, 337, 264, 2190, 10862, 293, 1333, 295, 577, 50976], "temperature": 0.0, "avg_logprob": -0.10630648958999499, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.004608440678566694}, {"id": 146, "seek": 76032, "start": 772.5600000000001, "end": 777.36, "text": " current models are being built. But if we think ahead of what the next couple of years should look", "tokens": [50976, 2190, 5245, 366, 885, 3094, 13, 583, 498, 321, 519, 2286, 295, 437, 264, 958, 1916, 295, 924, 820, 574, 51216], "temperature": 0.0, "avg_logprob": -0.10630648958999499, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.004608440678566694}, {"id": 147, "seek": 76032, "start": 777.36, "end": 783.12, "text": " or not should, could look like, then yeah, there are loads of open questions. And we were trying", "tokens": [51216, 420, 406, 820, 11, 727, 574, 411, 11, 550, 1338, 11, 456, 366, 12668, 295, 1269, 1651, 13, 400, 321, 645, 1382, 51504], "temperature": 0.0, "avg_logprob": -0.10630648958999499, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.004608440678566694}, {"id": 148, "seek": 76032, "start": 783.12, "end": 788.1600000000001, "text": " to understand how do they fit into this framework? And because we internally were trying to make", "tokens": [51504, 281, 1223, 577, 360, 436, 3318, 666, 341, 8388, 30, 400, 570, 321, 19501, 645, 1382, 281, 652, 51756], "temperature": 0.0, "avg_logprob": -0.10630648958999499, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.004608440678566694}, {"id": 149, "seek": 78816, "start": 788.16, "end": 793.04, "text": " sense of this in the first place. So just to give you a couple of them, what happens if your", "tokens": [50364, 2020, 295, 341, 294, 264, 700, 1081, 13, 407, 445, 281, 976, 291, 257, 1916, 295, 552, 11, 437, 2314, 498, 428, 50608], "temperature": 0.0, "avg_logprob": -0.10259103775024414, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.025164732709527016}, {"id": 150, "seek": 78816, "start": 793.04, "end": 798.0799999999999, "text": " model has the ability to do online learning? How often do you have to audit it? Should you re-audit", "tokens": [50608, 2316, 575, 264, 3485, 281, 360, 2950, 2539, 30, 1012, 2049, 360, 291, 362, 281, 17748, 309, 30, 6454, 291, 319, 12, 3751, 270, 50860], "temperature": 0.0, "avg_logprob": -0.10259103775024414, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.025164732709527016}, {"id": 151, "seek": 78816, "start": 798.0799999999999, "end": 803.68, "text": " it during the online learning? If yes, how often? What if you give the model access to the internet", "tokens": [50860, 309, 1830, 264, 2950, 2539, 30, 759, 2086, 11, 577, 2049, 30, 708, 498, 291, 976, 264, 2316, 2105, 281, 264, 4705, 51140], "temperature": 0.0, "avg_logprob": -0.10259103775024414, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.025164732709527016}, {"id": 152, "seek": 78816, "start": 803.68, "end": 809.52, "text": " or to a database or to anything like this? Yeah, I think a model with and without access to the", "tokens": [51140, 420, 281, 257, 8149, 420, 281, 1340, 411, 341, 30, 865, 11, 286, 519, 257, 2316, 365, 293, 1553, 2105, 281, 264, 51432], "temperature": 0.0, "avg_logprob": -0.10259103775024414, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.025164732709527016}, {"id": 153, "seek": 78816, "start": 809.52, "end": 815.12, "text": " internet is basically two very different models. The one with access to the internet is just so", "tokens": [51432, 4705, 307, 1936, 732, 588, 819, 5245, 13, 440, 472, 365, 2105, 281, 264, 4705, 307, 445, 370, 51712], "temperature": 0.0, "avg_logprob": -0.10259103775024414, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.025164732709527016}, {"id": 154, "seek": 81512, "start": 815.12, "end": 821.6, "text": " much more powerful if you can use it even on a very basic level. So yeah, it feels like if you", "tokens": [50364, 709, 544, 4005, 498, 291, 393, 764, 309, 754, 322, 257, 588, 3875, 1496, 13, 407, 1338, 11, 309, 3417, 411, 498, 291, 50688], "temperature": 0.0, "avg_logprob": -0.1291001772476455, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.029720544815063477}, {"id": 155, "seek": 81512, "start": 821.6, "end": 825.84, "text": " give your model affordances like this, you kind of have to rethink how dangerous it is and where", "tokens": [50688, 976, 428, 2316, 6157, 2676, 411, 341, 11, 291, 733, 295, 362, 281, 34595, 577, 5795, 309, 307, 293, 689, 50900], "temperature": 0.0, "avg_logprob": -0.1291001772476455, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.029720544815063477}, {"id": 156, "seek": 81512, "start": 825.84, "end": 829.36, "text": " the danger comes from because it suddenly is like a totally different threat model potentially.", "tokens": [50900, 264, 4330, 1487, 490, 570, 309, 5800, 307, 411, 257, 3879, 819, 4734, 2316, 7263, 13, 51076], "temperature": 0.0, "avg_logprob": -0.1291001772476455, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.029720544815063477}, {"id": 157, "seek": 81512, "start": 829.92, "end": 836.4, "text": " And so what we did for the paper and really the credit should go to Lee Sharkey here, who is my", "tokens": [51104, 400, 370, 437, 321, 630, 337, 264, 3035, 293, 534, 264, 5397, 820, 352, 281, 6957, 22030, 4119, 510, 11, 567, 307, 452, 51428], "temperature": 0.0, "avg_logprob": -0.1291001772476455, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.029720544815063477}, {"id": 158, "seek": 81512, "start": 836.4, "end": 840.48, "text": " co-founder, who has done most of the hard work or if not all of the hard work for this paper.", "tokens": [51428, 598, 12, 33348, 11, 567, 575, 1096, 881, 295, 264, 1152, 589, 420, 498, 406, 439, 295, 264, 1152, 589, 337, 341, 3035, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1291001772476455, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.029720544815063477}, {"id": 159, "seek": 84048, "start": 841.44, "end": 846.5600000000001, "text": " And so what we were doing is thinking from first principles. Where does the risk come from and what", "tokens": [50412, 400, 370, 437, 321, 645, 884, 307, 1953, 490, 700, 9156, 13, 2305, 775, 264, 3148, 808, 490, 293, 437, 50668], "temperature": 0.0, "avg_logprob": -0.11338041539777789, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.15805236995220184}, {"id": 160, "seek": 84048, "start": 846.5600000000001, "end": 853.44, "text": " changes to the AI system do create new risks? And then basically the answer is, well, we have to", "tokens": [50668, 2962, 281, 264, 7318, 1185, 360, 1884, 777, 10888, 30, 400, 550, 1936, 264, 1867, 307, 11, 731, 11, 321, 362, 281, 51012], "temperature": 0.0, "avg_logprob": -0.11338041539777789, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.15805236995220184}, {"id": 161, "seek": 84048, "start": 853.44, "end": 858.4, "text": " audit wherever risk is created. And then the more we looked into this, the more realized, well,", "tokens": [51012, 17748, 8660, 3148, 307, 2942, 13, 400, 550, 264, 544, 321, 2956, 666, 341, 11, 264, 544, 5334, 11, 731, 11, 51260], "temperature": 0.0, "avg_logprob": -0.11338041539777789, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.15805236995220184}, {"id": 162, "seek": 84048, "start": 858.4, "end": 862.8000000000001, "text": " there are actually a lot of places where new risk comes into the system, at least potentially,", "tokens": [51260, 456, 366, 767, 257, 688, 295, 3190, 689, 777, 3148, 1487, 666, 264, 1185, 11, 412, 1935, 7263, 11, 51480], "temperature": 0.0, "avg_logprob": -0.11338041539777789, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.15805236995220184}, {"id": 163, "seek": 84048, "start": 862.8000000000001, "end": 867.6, "text": " and therefore we are audits, at least in an ideal world should happen. There are obviously some", "tokens": [51480, 293, 4412, 321, 366, 2379, 1208, 11, 412, 1935, 294, 364, 7157, 1002, 820, 1051, 13, 821, 366, 2745, 512, 51720], "temperature": 0.0, "avg_logprob": -0.11338041539777789, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.15805236995220184}, {"id": 164, "seek": 86760, "start": 867.6800000000001, "end": 872.8000000000001, "text": " constraints. But I think if we think about where are we five years from now, then I think, yeah,", "tokens": [50368, 18491, 13, 583, 286, 519, 498, 321, 519, 466, 689, 366, 321, 1732, 924, 490, 586, 11, 550, 286, 519, 11, 1338, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10382753774660443, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.026332275941967964}, {"id": 165, "seek": 86760, "start": 872.8000000000001, "end": 877.6, "text": " if there is actually a big auditing ecosystem around this, then there will be very, very many", "tokens": [50624, 498, 456, 307, 767, 257, 955, 2379, 1748, 11311, 926, 341, 11, 550, 456, 486, 312, 588, 11, 588, 867, 50864], "temperature": 0.0, "avg_logprob": -0.10382753774660443, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.026332275941967964}, {"id": 166, "seek": 86760, "start": 877.6, "end": 883.52, "text": " different organizations auditing really different places. And then the other point of the paper", "tokens": [50864, 819, 6150, 2379, 1748, 534, 819, 3190, 13, 400, 550, 264, 661, 935, 295, 264, 3035, 51160], "temperature": 0.0, "avg_logprob": -0.10382753774660443, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.026332275941967964}, {"id": 167, "seek": 86760, "start": 883.52, "end": 891.36, "text": " was just to define many concepts and create the language to discuss all of these things because", "tokens": [51160, 390, 445, 281, 6964, 867, 10392, 293, 1884, 264, 2856, 281, 2248, 439, 295, 613, 721, 570, 51552], "temperature": 0.0, "avg_logprob": -0.10382753774660443, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.026332275941967964}, {"id": 168, "seek": 86760, "start": 891.9200000000001, "end": 896.48, "text": " we had sort of many internal discussions where we were like, oh, the thing we mean is this.", "tokens": [51580, 321, 632, 1333, 295, 867, 6920, 11088, 689, 321, 645, 411, 11, 1954, 11, 264, 551, 321, 914, 307, 341, 13, 51808], "temperature": 0.0, "avg_logprob": -0.10382753774660443, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.026332275941967964}, {"id": 169, "seek": 89648, "start": 896.48, "end": 900.48, "text": " And then we had an example, and then we kind of needed a name for it. And there wasn't really", "tokens": [50364, 400, 550, 321, 632, 364, 1365, 11, 293, 550, 321, 733, 295, 2978, 257, 1315, 337, 309, 13, 400, 456, 2067, 380, 534, 50564], "temperature": 0.0, "avg_logprob": -0.09284410556825269, "compression_ratio": 1.627986348122867, "no_speech_prob": 0.005553440190851688}, {"id": 170, "seek": 89648, "start": 900.48, "end": 906.5600000000001, "text": " a name. So we decided, okay, let's define all of the relevant terms for this, and then sort of", "tokens": [50564, 257, 1315, 13, 407, 321, 3047, 11, 1392, 11, 718, 311, 6964, 439, 295, 264, 7340, 2115, 337, 341, 11, 293, 550, 1333, 295, 50868], "temperature": 0.0, "avg_logprob": -0.09284410556825269, "compression_ratio": 1.627986348122867, "no_speech_prob": 0.005553440190851688}, {"id": 171, "seek": 89648, "start": 906.5600000000001, "end": 911.04, "text": " have a language to talk about this in the first place. Hey, we'll continue our interview in a", "tokens": [50868, 362, 257, 2856, 281, 751, 466, 341, 294, 264, 700, 1081, 13, 1911, 11, 321, 603, 2354, 527, 4049, 294, 257, 51092], "temperature": 0.0, "avg_logprob": -0.09284410556825269, "compression_ratio": 1.627986348122867, "no_speech_prob": 0.005553440190851688}, {"id": 172, "seek": 89648, "start": 911.04, "end": 916.32, "text": " moment after a word from our sponsors. Real quick, what's the easiest choice you can make? Taking", "tokens": [51092, 1623, 934, 257, 1349, 490, 527, 22593, 13, 8467, 1702, 11, 437, 311, 264, 12889, 3922, 291, 393, 652, 30, 17837, 51356], "temperature": 0.0, "avg_logprob": -0.09284410556825269, "compression_ratio": 1.627986348122867, "no_speech_prob": 0.005553440190851688}, {"id": 173, "seek": 89648, "start": 916.32, "end": 921.2, "text": " the window instead of the middle seat, outsourcing business tasks that you absolutely hate? What", "tokens": [51356, 264, 4910, 2602, 295, 264, 2808, 6121, 11, 14758, 41849, 1606, 9608, 300, 291, 3122, 4700, 30, 708, 51600], "temperature": 0.0, "avg_logprob": -0.09284410556825269, "compression_ratio": 1.627986348122867, "no_speech_prob": 0.005553440190851688}, {"id": 174, "seek": 92120, "start": 921.2, "end": 927.76, "text": " about selling with Shopify? Shopify is the global commerce platform that helps you sell at every", "tokens": [50364, 466, 6511, 365, 43991, 30, 43991, 307, 264, 4338, 26320, 3663, 300, 3665, 291, 3607, 412, 633, 50692], "temperature": 0.0, "avg_logprob": -0.097269165181668, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.37370479106903076}, {"id": 175, "seek": 92120, "start": 927.76, "end": 933.6800000000001, "text": " stage of your business. Shopify powers 10% of all e-commerce in the US, and Shopify is the global", "tokens": [50692, 3233, 295, 428, 1606, 13, 43991, 8674, 1266, 4, 295, 439, 308, 12, 26926, 294, 264, 2546, 11, 293, 43991, 307, 264, 4338, 50988], "temperature": 0.0, "avg_logprob": -0.097269165181668, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.37370479106903076}, {"id": 176, "seek": 92120, "start": 933.6800000000001, "end": 939.36, "text": " force behind Allbirds, Rothy's and Brooklyn and millions of other entrepreneurs of every size", "tokens": [50988, 3464, 2261, 1057, 31473, 11, 497, 18907, 311, 293, 21872, 293, 6803, 295, 661, 12639, 295, 633, 2744, 51272], "temperature": 0.0, "avg_logprob": -0.097269165181668, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.37370479106903076}, {"id": 177, "seek": 92120, "start": 939.36, "end": 944.8000000000001, "text": " across 175 countries. Whether you're selling security systems or marketing memory modules,", "tokens": [51272, 2108, 41165, 3517, 13, 8503, 291, 434, 6511, 3825, 3652, 420, 6370, 4675, 16679, 11, 51544], "temperature": 0.0, "avg_logprob": -0.097269165181668, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.37370479106903076}, {"id": 178, "seek": 92120, "start": 944.8000000000001, "end": 949.44, "text": " Shopify helps you sell everywhere, from their all-in-one e-commerce platform to their in-person", "tokens": [51544, 43991, 3665, 291, 3607, 5315, 11, 490, 641, 439, 12, 259, 12, 546, 308, 12, 26926, 3663, 281, 641, 294, 12, 10813, 51776], "temperature": 0.0, "avg_logprob": -0.097269165181668, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.37370479106903076}, {"id": 179, "seek": 94944, "start": 949.44, "end": 954.6400000000001, "text": " POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the", "tokens": [50364, 430, 4367, 1185, 13, 30903, 293, 2035, 291, 434, 6511, 11, 43991, 311, 658, 291, 5343, 13, 286, 600, 1143, 309, 294, 264, 50624], "temperature": 0.0, "avg_logprob": -0.09296821492963132, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.00912491139024496}, {"id": 180, "seek": 94944, "start": 954.6400000000001, "end": 959.2, "text": " past at the companies I founded, and when we launched Merch here at Turpentine, Shopify will", "tokens": [50624, 1791, 412, 264, 3431, 286, 13234, 11, 293, 562, 321, 8730, 6124, 339, 510, 412, 5712, 22786, 533, 11, 43991, 486, 50852], "temperature": 0.0, "avg_logprob": -0.09296821492963132, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.00912491139024496}, {"id": 181, "seek": 94944, "start": 959.2, "end": 964.08, "text": " be our go-to. Shopify helps turn browsers into buyers with the internet's best converting", "tokens": [50852, 312, 527, 352, 12, 1353, 13, 43991, 3665, 1261, 36069, 666, 23465, 365, 264, 4705, 311, 1151, 29942, 51096], "temperature": 0.0, "avg_logprob": -0.09296821492963132, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.00912491139024496}, {"id": 182, "seek": 94944, "start": 964.08, "end": 969.5200000000001, "text": " checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell", "tokens": [51096, 37153, 493, 281, 8652, 4, 1101, 5347, 281, 661, 5775, 26320, 9473, 13, 400, 43991, 3665, 291, 3607, 51368], "temperature": 0.0, "avg_logprob": -0.09296821492963132, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.00912491139024496}, {"id": 183, "seek": 94944, "start": 969.5200000000001, "end": 975.0400000000001, "text": " more with less effort thanks to Shopify magic, your AI-powered All-Star. With Shopify magic,", "tokens": [51368, 544, 365, 1570, 4630, 3231, 281, 43991, 5585, 11, 428, 7318, 12, 27178, 1057, 12, 24659, 13, 2022, 43991, 5585, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09296821492963132, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.00912491139024496}, {"id": 184, "seek": 97504, "start": 975.12, "end": 979.5999999999999, "text": " whip up captivating content that converts from blog posts to product descriptions.", "tokens": [50368, 22377, 493, 40769, 990, 2701, 300, 38874, 490, 6968, 12300, 281, 1674, 24406, 13, 50592], "temperature": 0.0, "avg_logprob": -0.09935840157901539, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.6985695362091064}, {"id": 185, "seek": 97504, "start": 979.5999999999999, "end": 985.92, "text": " Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify magic is free for", "tokens": [50592, 15409, 473, 9836, 19894, 48, 6338, 13, 14129, 264, 2176, 3796, 2845, 565, 13, 7721, 11, 43991, 5585, 307, 1737, 337, 50908], "temperature": 0.0, "avg_logprob": -0.09935840157901539, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.6985695362091064}, {"id": 186, "seek": 97504, "start": 985.92, "end": 992.0799999999999, "text": " every Shopify seller. Businesses that grow grow with Shopify. Sign up for a $1 per month trial", "tokens": [50908, 633, 43991, 23600, 13, 10715, 279, 300, 1852, 1852, 365, 43991, 13, 13515, 493, 337, 257, 1848, 16, 680, 1618, 7308, 51216], "temperature": 0.0, "avg_logprob": -0.09935840157901539, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.6985695362091064}, {"id": 187, "seek": 97504, "start": 992.0799999999999, "end": 998.0, "text": " period at Shopify.com slash cognitive. Go to Shopify.com slash cognitive now to grow your", "tokens": [51216, 2896, 412, 43991, 13, 1112, 17330, 15605, 13, 1037, 281, 43991, 13, 1112, 17330, 15605, 586, 281, 1852, 428, 51512], "temperature": 0.0, "avg_logprob": -0.09935840157901539, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.6985695362091064}, {"id": 188, "seek": 99800, "start": 998.0, "end": 1002.0, "text": " business no matter what stage you're in. Shopify.com slash cognitive.", "tokens": [50364, 1606, 572, 1871, 437, 3233, 291, 434, 294, 13, 43991, 13, 1112, 17330, 15605, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10689394501433976, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.01590379700064659}, {"id": 189, "seek": 99800, "start": 1005.04, "end": 1012.72, "text": " So let's dig in in a little bit deeper detail. I like the premise that you set out within the", "tokens": [50716, 407, 718, 311, 2528, 294, 294, 257, 707, 857, 7731, 2607, 13, 286, 411, 264, 22045, 300, 291, 992, 484, 1951, 264, 51100], "temperature": 0.0, "avg_logprob": -0.10689394501433976, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.01590379700064659}, {"id": 190, "seek": 99800, "start": 1012.72, "end": 1020.08, "text": " paper, which is to work backward from AI effect in the real world and try to imagine where are", "tokens": [51100, 3035, 11, 597, 307, 281, 589, 23897, 490, 7318, 1802, 294, 264, 957, 1002, 293, 853, 281, 3811, 689, 366, 51468], "temperature": 0.0, "avg_logprob": -0.10689394501433976, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.01590379700064659}, {"id": 191, "seek": 99800, "start": 1020.08, "end": 1023.92, "text": " these effects going to happen and then how can we get upstream of that and help shape them in a", "tokens": [51468, 613, 5065, 516, 281, 1051, 293, 550, 577, 393, 321, 483, 33915, 295, 300, 293, 854, 3909, 552, 294, 257, 51660], "temperature": 0.0, "avg_logprob": -0.10689394501433976, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.01590379700064659}, {"id": 192, "seek": 102392, "start": 1023.92, "end": 1030.1599999999999, "text": " positive way. I would be interested to hear you kind of describe that backward chaining process in", "tokens": [50364, 3353, 636, 13, 286, 576, 312, 3102, 281, 1568, 291, 733, 295, 6786, 300, 23897, 417, 3686, 1399, 294, 50676], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 193, "seek": 102392, "start": 1030.1599999999999, "end": 1034.32, "text": " a little bit more detail. And then I thought some of your concepts also were really helpful", "tokens": [50676, 257, 707, 857, 544, 2607, 13, 400, 550, 286, 1194, 512, 295, 428, 10392, 611, 645, 534, 4961, 50884], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 194, "seek": 102392, "start": 1035.2, "end": 1040.32, "text": " clarifications and distinctions. So maybe you can highlight some of the ones that you think are most", "tokens": [50928, 6093, 7833, 293, 1483, 49798, 13, 407, 1310, 291, 393, 5078, 512, 295, 264, 2306, 300, 291, 519, 366, 881, 51184], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 195, "seek": 102392, "start": 1041.28, "end": 1043.92, "text": " useful that you'd like to see get into broader circulation as well.", "tokens": [51232, 4420, 300, 291, 1116, 411, 281, 536, 483, 666, 13227, 23168, 382, 731, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 196, "seek": 102392, "start": 1044.48, "end": 1049.2, "text": " So basically, we started from, okay, the system, the AI system will interact with the world in a", "tokens": [51392, 407, 1936, 11, 321, 1409, 490, 11, 1392, 11, 264, 1185, 11, 264, 7318, 1185, 486, 4648, 365, 264, 1002, 294, 257, 51628], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 197, "seek": 102392, "start": 1049.2, "end": 1053.44, "text": " particular way. And then, you know, there are many, many different ways in which it can interact with", "tokens": [51628, 1729, 636, 13, 400, 550, 11, 291, 458, 11, 456, 366, 867, 11, 867, 819, 2098, 294, 597, 309, 393, 4648, 365, 51840], "temperature": 0.0, "avg_logprob": -0.09761664664098459, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.012427795678377151}, {"id": 198, "seek": 105344, "start": 1053.44, "end": 1059.28, "text": " the world. And then there's sort of like a whole chain of things that have had to happen until the", "tokens": [50364, 264, 1002, 13, 400, 550, 456, 311, 1333, 295, 411, 257, 1379, 5021, 295, 721, 300, 362, 632, 281, 1051, 1826, 264, 50656], "temperature": 0.0, "avg_logprob": -0.08973724800243713, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0028435057029128075}, {"id": 199, "seek": 105344, "start": 1059.28, "end": 1064.0800000000002, "text": " model can act interact with the world in this particular way. So, you know, maybe it has been", "tokens": [50656, 2316, 393, 605, 4648, 365, 264, 1002, 294, 341, 1729, 636, 13, 407, 11, 291, 458, 11, 1310, 309, 575, 668, 50896], "temperature": 0.0, "avg_logprob": -0.08973724800243713, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0028435057029128075}, {"id": 200, "seek": 105344, "start": 1064.0800000000002, "end": 1068.4, "text": " fined you and maybe it has been given access to the internet before that it has to have been trained", "tokens": [50896, 962, 292, 291, 293, 1310, 309, 575, 668, 2212, 2105, 281, 264, 4705, 949, 300, 309, 575, 281, 362, 668, 8895, 51112], "temperature": 0.0, "avg_logprob": -0.08973724800243713, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0028435057029128075}, {"id": 201, "seek": 105344, "start": 1068.4, "end": 1072.56, "text": " before that there has to have been the decision that this model should be trained in the first", "tokens": [51112, 949, 300, 456, 575, 281, 362, 668, 264, 3537, 300, 341, 2316, 820, 312, 8895, 294, 264, 700, 51320], "temperature": 0.0, "avg_logprob": -0.08973724800243713, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0028435057029128075}, {"id": 202, "seek": 105344, "start": 1072.56, "end": 1078.8, "text": " place. And so the question is like, what are the kind of important decisions at all of these", "tokens": [51320, 1081, 13, 400, 370, 264, 1168, 307, 411, 11, 437, 366, 264, 733, 295, 1021, 5327, 412, 439, 295, 613, 51632], "temperature": 0.0, "avg_logprob": -0.08973724800243713, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0028435057029128075}, {"id": 203, "seek": 107880, "start": 1078.8, "end": 1084.8, "text": " different points in time? And how then can we ensure that people actually make decisions that", "tokens": [50364, 819, 2793, 294, 565, 30, 400, 577, 550, 393, 321, 5586, 300, 561, 767, 652, 5327, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08987999829378995, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.018536537885665894}, {"id": 204, "seek": 107880, "start": 1084.8, "end": 1091.36, "text": " will lead to outcomes at the end of the chain, such as the model or the system interacts with the", "tokens": [50664, 486, 1477, 281, 10070, 412, 264, 917, 295, 264, 5021, 11, 1270, 382, 264, 2316, 420, 264, 1185, 43582, 365, 264, 50992], "temperature": 0.0, "avg_logprob": -0.08987999829378995, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.018536537885665894}, {"id": 205, "seek": 107880, "start": 1091.36, "end": 1096.72, "text": " world in a safe manner. And this is maybe like the first distinction that is worth pointing out,", "tokens": [50992, 1002, 294, 257, 3273, 9060, 13, 400, 341, 307, 1310, 411, 264, 700, 16844, 300, 307, 3163, 12166, 484, 11, 51260], "temperature": 0.0, "avg_logprob": -0.08987999829378995, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.018536537885665894}, {"id": 206, "seek": 107880, "start": 1096.72, "end": 1101.04, "text": " or like the reason why I'm correcting myself all the time is there's really a difference between", "tokens": [51260, 420, 411, 264, 1778, 983, 286, 478, 47032, 2059, 439, 264, 565, 307, 456, 311, 534, 257, 2649, 1296, 51476], "temperature": 0.0, "avg_logprob": -0.08987999829378995, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.018536537885665894}, {"id": 207, "seek": 107880, "start": 1101.04, "end": 1105.44, "text": " AI model and AI system. The AI model really, and this is not something we came up with,", "tokens": [51476, 7318, 2316, 293, 7318, 1185, 13, 440, 7318, 2316, 534, 11, 293, 341, 307, 406, 746, 321, 1361, 493, 365, 11, 51696], "temperature": 0.0, "avg_logprob": -0.08987999829378995, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.018536537885665894}, {"id": 208, "seek": 110544, "start": 1105.44, "end": 1109.92, "text": " this already exists before. But I think it's worth pointing out and sort of getting in like", "tokens": [50364, 341, 1217, 8198, 949, 13, 583, 286, 519, 309, 311, 3163, 12166, 484, 293, 1333, 295, 1242, 294, 411, 50588], "temperature": 0.0, "avg_logprob": -0.1001081055608289, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.008574680425226688}, {"id": 209, "seek": 110544, "start": 1109.92, "end": 1115.3600000000001, "text": " really hammering into people's head when they think about AI. So the AI model is just the weights", "tokens": [50588, 534, 13017, 278, 666, 561, 311, 1378, 562, 436, 519, 466, 7318, 13, 407, 264, 7318, 2316, 307, 445, 264, 17443, 50860], "temperature": 0.0, "avg_logprob": -0.1001081055608289, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.008574680425226688}, {"id": 210, "seek": 110544, "start": 1115.3600000000001, "end": 1120.4, "text": " maybe behind, you know, like behind an API, but even with the API, it's kind of already a system.", "tokens": [50860, 1310, 2261, 11, 291, 458, 11, 411, 2261, 364, 9362, 11, 457, 754, 365, 264, 9362, 11, 309, 311, 733, 295, 1217, 257, 1185, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1001081055608289, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.008574680425226688}, {"id": 211, "seek": 110544, "start": 1121.44, "end": 1126.16, "text": " And the system then is sort of the weights plus everything around it. So there could be scaffolding,", "tokens": [51164, 400, 264, 1185, 550, 307, 1333, 295, 264, 17443, 1804, 1203, 926, 309, 13, 407, 456, 727, 312, 44094, 278, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1001081055608289, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.008574680425226688}, {"id": 212, "seek": 110544, "start": 1126.16, "end": 1130.24, "text": " there could be access to tools, there could be content filters, this could even be like", "tokens": [51400, 456, 727, 312, 2105, 281, 3873, 11, 456, 727, 312, 2701, 15995, 11, 341, 727, 754, 312, 411, 51604], "temperature": 0.0, "avg_logprob": -0.1001081055608289, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.008574680425226688}, {"id": 213, "seek": 113024, "start": 1130.24, "end": 1135.68, "text": " just an API, retrieval databases, etc. Like really the full package, where you say, okay,", "tokens": [50364, 445, 364, 9362, 11, 19817, 3337, 22380, 11, 5183, 13, 1743, 534, 264, 1577, 7372, 11, 689, 291, 584, 11, 1392, 11, 50636], "temperature": 0.0, "avg_logprob": -0.11032999933293436, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.015417772345244884}, {"id": 214, "seek": 113024, "start": 1135.68, "end": 1141.84, "text": " you know, there's there's like stuff around the mod around the weights that increase the", "tokens": [50636, 291, 458, 11, 456, 311, 456, 311, 411, 1507, 926, 264, 1072, 926, 264, 17443, 300, 3488, 264, 50944], "temperature": 0.0, "avg_logprob": -0.11032999933293436, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.015417772345244884}, {"id": 215, "seek": 113024, "start": 1141.84, "end": 1147.44, "text": " capabilities of the model and menu, or at least change the capabilities of the raw model in some", "tokens": [50944, 10862, 295, 264, 2316, 293, 6510, 11, 420, 412, 1935, 1319, 264, 10862, 295, 264, 8936, 2316, 294, 512, 51224], "temperature": 0.0, "avg_logprob": -0.11032999933293436, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.015417772345244884}, {"id": 216, "seek": 113024, "start": 1147.44, "end": 1153.2, "text": " sense, not necessarily always increasing filters, for example, may decrease it. And then there are", "tokens": [51224, 2020, 11, 406, 4725, 1009, 5662, 15995, 11, 337, 1365, 11, 815, 11514, 309, 13, 400, 550, 456, 366, 51512], "temperature": 0.0, "avg_logprob": -0.11032999933293436, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.015417772345244884}, {"id": 217, "seek": 113024, "start": 1153.2, "end": 1157.68, "text": " sort of other weird ways, or like, yeah, once you think about this, there are sort of a couple", "tokens": [51512, 1333, 295, 661, 3657, 2098, 11, 420, 411, 11, 1338, 11, 1564, 291, 519, 466, 341, 11, 456, 366, 1333, 295, 257, 1916, 51736], "temperature": 0.0, "avg_logprob": -0.11032999933293436, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.015417772345244884}, {"id": 218, "seek": 115768, "start": 1157.68, "end": 1163.04, "text": " of other concepts that that feel important to clarify. Because when people say capabilities,", "tokens": [50364, 295, 661, 10392, 300, 300, 841, 1021, 281, 17594, 13, 1436, 562, 561, 584, 10862, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11301168553969439, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.011683340184390545}, {"id": 219, "seek": 115768, "start": 1163.04, "end": 1168.72, "text": " this can mean very different things, right? And so we categorize this into three different classes.", "tokens": [50632, 341, 393, 914, 588, 819, 721, 11, 558, 30, 400, 370, 321, 19250, 1125, 341, 666, 1045, 819, 5359, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11301168553969439, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.011683340184390545}, {"id": 220, "seek": 115768, "start": 1169.3600000000001, "end": 1175.04, "text": " The first one is absolute capabilities, which we think of basically the hypothetical capabilities", "tokens": [50948, 440, 700, 472, 307, 8236, 10862, 11, 597, 321, 519, 295, 1936, 264, 33053, 10862, 51232], "temperature": 0.0, "avg_logprob": -0.11301168553969439, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.011683340184390545}, {"id": 221, "seek": 115768, "start": 1175.04, "end": 1182.48, "text": " given any set of affordances. So if you have GPD for without the internet, right, then in the space", "tokens": [51232, 2212, 604, 992, 295, 6157, 2676, 13, 407, 498, 291, 362, 460, 17349, 337, 1553, 264, 4705, 11, 558, 11, 550, 294, 264, 1901, 51604], "temperature": 0.0, "avg_logprob": -0.11301168553969439, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.011683340184390545}, {"id": 222, "seek": 118248, "start": 1182.48, "end": 1188.16, "text": " of absolute capabilities would be a GPD for with internet. So or like things that this model could", "tokens": [50364, 295, 8236, 10862, 576, 312, 257, 460, 17349, 337, 365, 4705, 13, 407, 420, 411, 721, 300, 341, 2316, 727, 50648], "temperature": 0.0, "avg_logprob": -0.11550889909267426, "compression_ratio": 1.8426966292134832, "no_speech_prob": 0.06185144931077957}, {"id": 223, "seek": 118248, "start": 1188.16, "end": 1193.3600000000001, "text": " do. So the question is like, if we give additional things to the system, how big is the space of", "tokens": [50648, 360, 13, 407, 264, 1168, 307, 411, 11, 498, 321, 976, 4497, 721, 281, 264, 1185, 11, 577, 955, 307, 264, 1901, 295, 50908], "temperature": 0.0, "avg_logprob": -0.11550889909267426, "compression_ratio": 1.8426966292134832, "no_speech_prob": 0.06185144931077957}, {"id": 224, "seek": 118248, "start": 1193.3600000000001, "end": 1199.44, "text": " actions it could take. So, you know, and then obviously, there's a question of like, how imaginary", "tokens": [50908, 5909, 309, 727, 747, 13, 407, 11, 291, 458, 11, 293, 550, 2745, 11, 456, 311, 257, 1168, 295, 411, 11, 577, 26164, 51212], "temperature": 0.0, "avg_logprob": -0.11550889909267426, "compression_ratio": 1.8426966292134832, "no_speech_prob": 0.06185144931077957}, {"id": 225, "seek": 118248, "start": 1199.44, "end": 1204.72, "text": " do we get here, you know, like, does it has does it get access to like, you know, a Dyson sphere,", "tokens": [51212, 360, 321, 483, 510, 11, 291, 458, 11, 411, 11, 775, 309, 575, 775, 309, 483, 2105, 281, 411, 11, 291, 458, 11, 257, 413, 28194, 16687, 11, 51476], "temperature": 0.0, "avg_logprob": -0.11550889909267426, "compression_ratio": 1.8426966292134832, "no_speech_prob": 0.06185144931077957}, {"id": 226, "seek": 118248, "start": 1204.72, "end": 1210.88, "text": " or does it get access to like, a government or something like this. But but yeah, like it basically", "tokens": [51476, 420, 775, 309, 483, 2105, 281, 411, 11, 257, 2463, 420, 746, 411, 341, 13, 583, 457, 1338, 11, 411, 309, 1936, 51784], "temperature": 0.0, "avg_logprob": -0.11550889909267426, "compression_ratio": 1.8426966292134832, "no_speech_prob": 0.06185144931077957}, {"id": 227, "seek": 121088, "start": 1210.88, "end": 1216.5600000000002, "text": " points out sort of the, what could this model do if we gave it a lot of things, everything that", "tokens": [50364, 2793, 484, 1333, 295, 264, 11, 437, 727, 341, 2316, 360, 498, 321, 2729, 309, 257, 688, 295, 721, 11, 1203, 300, 50648], "temperature": 0.0, "avg_logprob": -0.12299090858519547, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.004608797375112772}, {"id": 228, "seek": 121088, "start": 1216.5600000000002, "end": 1221.2800000000002, "text": " we can basically think of. Thinking about this in the first place only makes sense for models that", "tokens": [50648, 321, 393, 1936, 519, 295, 13, 24460, 466, 341, 294, 264, 700, 1081, 787, 1669, 2020, 337, 5245, 300, 50884], "temperature": 0.0, "avg_logprob": -0.12299090858519547, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.004608797375112772}, {"id": 229, "seek": 121088, "start": 1221.2800000000002, "end": 1227.3600000000001, "text": " have become more general, like the GPT is, because you know, for an MNIST filter, like for an MNIST", "tokens": [50884, 362, 1813, 544, 2674, 11, 411, 264, 26039, 51, 307, 11, 570, 291, 458, 11, 337, 364, 376, 45, 19756, 6608, 11, 411, 337, 364, 376, 45, 19756, 51188], "temperature": 0.0, "avg_logprob": -0.12299090858519547, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.004608797375112772}, {"id": 230, "seek": 121088, "start": 1227.3600000000001, "end": 1233.3600000000001, "text": " classifier, this doesn't make any any sense, like an MNIST classifier plus internet is like is exactly", "tokens": [51188, 1508, 9902, 11, 341, 1177, 380, 652, 604, 604, 2020, 11, 411, 364, 376, 45, 19756, 1508, 9902, 1804, 4705, 307, 411, 307, 2293, 51488], "temperature": 0.0, "avg_logprob": -0.12299090858519547, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.004608797375112772}, {"id": 231, "seek": 121088, "start": 1233.3600000000001, "end": 1238.4, "text": " as capable as just the MNIST classifier itself. But yeah, for systems that are more general,", "tokens": [51488, 382, 8189, 382, 445, 264, 376, 45, 19756, 1508, 9902, 2564, 13, 583, 1338, 11, 337, 3652, 300, 366, 544, 2674, 11, 51740], "temperature": 0.0, "avg_logprob": -0.12299090858519547, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.004608797375112772}, {"id": 232, "seek": 123840, "start": 1238.4, "end": 1244.5600000000002, "text": " suddenly you have this difference between things that only the system can do, or like the basic", "tokens": [50364, 5800, 291, 362, 341, 2649, 1296, 721, 300, 787, 264, 1185, 393, 360, 11, 420, 411, 264, 3875, 50672], "temperature": 0.0, "avg_logprob": -0.07317515479193794, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.007344567216932774}, {"id": 233, "seek": 123840, "start": 1244.5600000000002, "end": 1251.2800000000002, "text": " system plus things that you could do hypothetically with a lot of additional affordances. Then the", "tokens": [50672, 1185, 1804, 721, 300, 291, 727, 360, 24371, 22652, 365, 257, 688, 295, 4497, 6157, 2676, 13, 1396, 264, 51008], "temperature": 0.0, "avg_logprob": -0.07317515479193794, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.007344567216932774}, {"id": 234, "seek": 123840, "start": 1251.2800000000002, "end": 1258.48, "text": " second one is contextual capabilities, which is things that are achievable in the context right", "tokens": [51008, 1150, 472, 307, 35526, 10862, 11, 597, 307, 721, 300, 366, 3538, 17915, 294, 264, 4319, 558, 51368], "temperature": 0.0, "avg_logprob": -0.07317515479193794, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.007344567216932774}, {"id": 235, "seek": 123840, "start": 1258.48, "end": 1266.0800000000002, "text": " now. So for example, with chat GPT, you can enable it to have access to tools, and then you can browse", "tokens": [51368, 586, 13, 407, 337, 1365, 11, 365, 5081, 26039, 51, 11, 291, 393, 9528, 309, 281, 362, 2105, 281, 3873, 11, 293, 550, 291, 393, 31442, 51748], "temperature": 0.0, "avg_logprob": -0.07317515479193794, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.007344567216932774}, {"id": 236, "seek": 126608, "start": 1266.08, "end": 1270.48, "text": " the web. And this is something that it can do right now, you don't have to add anything on top", "tokens": [50364, 264, 3670, 13, 400, 341, 307, 746, 300, 309, 393, 360, 558, 586, 11, 291, 500, 380, 362, 281, 909, 1340, 322, 1192, 50584], "temperature": 0.0, "avg_logprob": -0.09086831410725911, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0191179346293211}, {"id": 237, "seek": 126608, "start": 1270.48, "end": 1275.52, "text": " of this. And this is sort of this is sort of the smallest category of things, which you can do without", "tokens": [50584, 295, 341, 13, 400, 341, 307, 1333, 295, 341, 307, 1333, 295, 264, 16998, 7719, 295, 721, 11, 597, 291, 393, 360, 1553, 50836], "temperature": 0.0, "avg_logprob": -0.09086831410725911, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0191179346293211}, {"id": 238, "seek": 126608, "start": 1275.52, "end": 1282.08, "text": " any additional modification and then reachable capabilities is contextual capabilities, plus", "tokens": [50836, 604, 4497, 26747, 293, 550, 2524, 712, 10862, 307, 35526, 10862, 11, 1804, 51164], "temperature": 0.0, "avg_logprob": -0.09086831410725911, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0191179346293211}, {"id": 239, "seek": 126608, "start": 1282.08, "end": 1289.1999999999998, "text": " achievable through extra effort. So for example, this could mean chat GPT itself may not have access", "tokens": [51164, 3538, 17915, 807, 2857, 4630, 13, 407, 337, 1365, 11, 341, 727, 914, 5081, 26039, 51, 2564, 815, 406, 362, 2105, 51520], "temperature": 0.0, "avg_logprob": -0.09086831410725911, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0191179346293211}, {"id": 240, "seek": 128920, "start": 1289.2, "end": 1296.8, "text": " to a calculator. But if it has access to the internet, it can like Google and then find a", "tokens": [50364, 281, 257, 24993, 13, 583, 498, 309, 575, 2105, 281, 264, 4705, 11, 309, 393, 411, 3329, 293, 550, 915, 257, 50744], "temperature": 0.0, "avg_logprob": -0.0860852785007928, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.08146016299724579}, {"id": 241, "seek": 128920, "start": 1296.8, "end": 1302.48, "text": " calculator and then use that calculator. And so it's sort of a two step process, right, where it", "tokens": [50744, 24993, 293, 550, 764, 300, 24993, 13, 400, 370, 309, 311, 1333, 295, 257, 732, 1823, 1399, 11, 558, 11, 689, 309, 51028], "temperature": 0.0, "avg_logprob": -0.0860852785007928, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.08146016299724579}, {"id": 242, "seek": 128920, "start": 1302.48, "end": 1307.76, "text": " has to use one affordance or capability to then achieve another. And so this is what we call", "tokens": [51028, 575, 281, 764, 472, 6157, 719, 420, 13759, 281, 550, 4584, 1071, 13, 400, 370, 341, 307, 437, 321, 818, 51292], "temperature": 0.0, "avg_logprob": -0.0860852785007928, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.08146016299724579}, {"id": 243, "seek": 128920, "start": 1307.76, "end": 1316.4, "text": " reachable capabilities. And yeah, so the reason the reason why we are making this all of this", "tokens": [51292, 2524, 712, 10862, 13, 400, 1338, 11, 370, 264, 1778, 264, 1778, 983, 321, 366, 1455, 341, 439, 295, 341, 51724], "temperature": 0.0, "avg_logprob": -0.0860852785007928, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.08146016299724579}, {"id": 244, "seek": 131640, "start": 1316.4, "end": 1321.8400000000001, "text": " differentiation, even though it sounds maybe a little bit too much in the weeds is when people", "tokens": [50364, 38902, 11, 754, 1673, 309, 3263, 1310, 257, 707, 857, 886, 709, 294, 264, 26370, 307, 562, 561, 50636], "temperature": 0.0, "avg_logprob": -0.07840484322853458, "compression_ratio": 1.8383458646616542, "no_speech_prob": 0.02674083597958088}, {"id": 245, "seek": 131640, "start": 1321.8400000000001, "end": 1327.68, "text": " talk about capabilities and regulating capabilities and designing laws for capabilities, the question", "tokens": [50636, 751, 466, 10862, 293, 46715, 10862, 293, 14685, 6064, 337, 10862, 11, 264, 1168, 50928], "temperature": 0.0, "avg_logprob": -0.07840484322853458, "compression_ratio": 1.8383458646616542, "no_speech_prob": 0.02674083597958088}, {"id": 246, "seek": 131640, "start": 1327.68, "end": 1333.52, "text": " is, which ones, right? Do you mean the contextual capabilities? So the ones that the model has", "tokens": [50928, 307, 11, 597, 2306, 11, 558, 30, 1144, 291, 914, 264, 35526, 10862, 30, 407, 264, 2306, 300, 264, 2316, 575, 51220], "temperature": 0.0, "avg_logprob": -0.07840484322853458, "compression_ratio": 1.8383458646616542, "no_speech_prob": 0.02674083597958088}, {"id": 247, "seek": 131640, "start": 1333.52, "end": 1338.16, "text": " literally right now, or the reachable capabilities, so which the model could reach with additional", "tokens": [51220, 3736, 558, 586, 11, 420, 264, 2524, 712, 10862, 11, 370, 597, 264, 2316, 727, 2524, 365, 4497, 51452], "temperature": 0.0, "avg_logprob": -0.07840484322853458, "compression_ratio": 1.8383458646616542, "no_speech_prob": 0.02674083597958088}, {"id": 248, "seek": 131640, "start": 1338.16, "end": 1345.0400000000002, "text": " effort or the absolute like, the maximum potential space of capabilities. And, you know, right now", "tokens": [51452, 4630, 420, 264, 8236, 411, 11, 264, 6674, 3995, 1901, 295, 10862, 13, 400, 11, 291, 458, 11, 558, 586, 51796], "temperature": 0.0, "avg_logprob": -0.07840484322853458, "compression_ratio": 1.8383458646616542, "no_speech_prob": 0.02674083597958088}, {"id": 249, "seek": 134504, "start": 1345.04, "end": 1350.8799999999999, "text": " this may sound like we're too much in the weeds. And but I think in a few months, this will sound", "tokens": [50364, 341, 815, 1626, 411, 321, 434, 886, 709, 294, 264, 26370, 13, 400, 457, 286, 519, 294, 257, 1326, 2493, 11, 341, 486, 1626, 50656], "temperature": 0.0, "avg_logprob": -0.11236144356105639, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.005908498540520668}, {"id": 250, "seek": 134504, "start": 1350.8799999999999, "end": 1356.6399999999999, "text": " very, very relevant suddenly, because the models will be more capable. And then they will actually", "tokens": [50656, 588, 11, 588, 7340, 5800, 11, 570, 264, 5245, 486, 312, 544, 8189, 13, 400, 550, 436, 486, 767, 50944], "temperature": 0.0, "avg_logprob": -0.11236144356105639, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.005908498540520668}, {"id": 251, "seek": 134504, "start": 1356.6399999999999, "end": 1363.76, "text": " be able to just like smart enough to use the internet to to like find additional tools that", "tokens": [50944, 312, 1075, 281, 445, 411, 4069, 1547, 281, 764, 264, 4705, 281, 281, 411, 915, 4497, 3873, 300, 51300], "temperature": 0.0, "avg_logprob": -0.11236144356105639, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.005908498540520668}, {"id": 252, "seek": 134504, "start": 1363.76, "end": 1370.1599999999999, "text": " they can then use, or or like convince someone to give them access to a shell. And then use that", "tokens": [51300, 436, 393, 550, 764, 11, 420, 420, 411, 13447, 1580, 281, 976, 552, 2105, 281, 257, 8720, 13, 400, 550, 764, 300, 51620], "temperature": 0.0, "avg_logprob": -0.11236144356105639, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.005908498540520668}, {"id": 253, "seek": 134504, "start": 1370.1599999999999, "end": 1373.68, "text": " because they're already like, you know, they can learn it in context or they know it anyways.", "tokens": [51620, 570, 436, 434, 1217, 411, 11, 291, 458, 11, 436, 393, 1466, 309, 294, 4319, 420, 436, 458, 309, 13448, 13, 51796], "temperature": 0.0, "avg_logprob": -0.11236144356105639, "compression_ratio": 1.787313432835821, "no_speech_prob": 0.005908498540520668}, {"id": 254, "seek": 137504, "start": 1375.04, "end": 1379.36, "text": " And at that point, really, the question is, what should the auditors audit for? Which capabilities?", "tokens": [50364, 400, 412, 300, 935, 11, 534, 11, 264, 1168, 307, 11, 437, 820, 264, 2379, 9862, 17748, 337, 30, 3013, 10862, 30, 50580], "temperature": 0.0, "avg_logprob": -0.12199542561515433, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.012051213532686234}, {"id": 255, "seek": 137504, "start": 1379.92, "end": 1385.44, "text": " And and that becomes like pretty quickly, like a very, very big space of things, right? So like,", "tokens": [50608, 400, 293, 300, 3643, 411, 1238, 2661, 11, 411, 257, 588, 11, 588, 955, 1901, 295, 721, 11, 558, 30, 407, 411, 11, 50884], "temperature": 0.0, "avg_logprob": -0.12199542561515433, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.012051213532686234}, {"id": 256, "seek": 137504, "start": 1385.44, "end": 1391.36, "text": " if the auditor not only has to think about what kind of tools do you give the AI, but also what", "tokens": [50884, 498, 264, 33970, 406, 787, 575, 281, 519, 466, 437, 733, 295, 3873, 360, 291, 976, 264, 7318, 11, 457, 611, 437, 51180], "temperature": 0.0, "avg_logprob": -0.12199542561515433, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.012051213532686234}, {"id": 257, "seek": 137504, "start": 1391.36, "end": 1397.44, "text": " kind of tools could the AI get access to through some means? Suddenly, you have this whole space of", "tokens": [51180, 733, 295, 3873, 727, 264, 7318, 483, 2105, 281, 807, 512, 1355, 30, 21194, 11, 291, 362, 341, 1379, 1901, 295, 51484], "temperature": 0.0, "avg_logprob": -0.12199542561515433, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.012051213532686234}, {"id": 258, "seek": 137504, "start": 1397.44, "end": 1402.48, "text": " like thousands of things it could do. It's really a question of like, or like a tradeoff between", "tokens": [51484, 411, 5383, 295, 721, 309, 727, 360, 13, 467, 311, 534, 257, 1168, 295, 411, 11, 420, 411, 257, 4923, 4506, 1296, 51736], "temperature": 0.0, "avg_logprob": -0.12199542561515433, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.012051213532686234}, {"id": 259, "seek": 140248, "start": 1402.48, "end": 1407.44, "text": " what is what is plausibly doable in the real world versus how much risk can we actually mitigate?", "tokens": [50364, 437, 307, 437, 307, 34946, 3545, 41183, 294, 264, 957, 1002, 5717, 577, 709, 3148, 393, 321, 767, 27336, 30, 50612], "temperature": 0.0, "avg_logprob": -0.10759539263589042, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.0008828569552861154}, {"id": 260, "seek": 140248, "start": 1408.24, "end": 1413.04, "text": " And I'm honestly very unsure about about the like where we're heading at this point.", "tokens": [50652, 400, 286, 478, 6095, 588, 32486, 466, 466, 264, 411, 689, 321, 434, 9864, 412, 341, 935, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10759539263589042, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.0008828569552861154}, {"id": 261, "seek": 140248, "start": 1414.16, "end": 1420.56, "text": " So just to riff on and kind of emphasize some of the the value that I see in in some of these", "tokens": [50948, 407, 445, 281, 36798, 322, 293, 733, 295, 16078, 512, 295, 264, 264, 2158, 300, 286, 536, 294, 294, 512, 295, 613, 51268], "temperature": 0.0, "avg_logprob": -0.10759539263589042, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.0008828569552861154}, {"id": 262, "seek": 140248, "start": 1420.56, "end": 1425.92, "text": " distinctions, I think it's helpful to clarify the difference between a model and a system.", "tokens": [51268, 1483, 49798, 11, 286, 519, 309, 311, 4961, 281, 17594, 264, 2649, 1296, 257, 2316, 293, 257, 1185, 13, 51536], "temperature": 0.0, "avg_logprob": -0.10759539263589042, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.0008828569552861154}, {"id": 263, "seek": 140248, "start": 1426.72, "end": 1432.0, "text": " I think there is a tremendous amount of confusion online. And to my degree, and I've probably even", "tokens": [51576, 286, 519, 456, 307, 257, 10048, 2372, 295, 15075, 2950, 13, 400, 281, 452, 4314, 11, 293, 286, 600, 1391, 754, 51840], "temperature": 0.0, "avg_logprob": -0.10759539263589042, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.0008828569552861154}, {"id": 264, "seek": 143200, "start": 1432.0, "end": 1438.08, "text": " contributed to some of it at times where people are like, you know, Chad GPT was doing this for me,", "tokens": [50364, 18434, 281, 512, 295, 309, 412, 1413, 689, 561, 366, 411, 11, 291, 458, 11, 22268, 26039, 51, 390, 884, 341, 337, 385, 11, 50668], "temperature": 0.0, "avg_logprob": -0.08989928796039365, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0019264078000560403}, {"id": 265, "seek": 143200, "start": 1438.08, "end": 1442.4, "text": " and now it's not anymore. And I've sometimes said like, well, they haven't updated the model,", "tokens": [50668, 293, 586, 309, 311, 406, 3602, 13, 400, 286, 600, 2171, 848, 411, 11, 731, 11, 436, 2378, 380, 10588, 264, 2316, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08989928796039365, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0019264078000560403}, {"id": 266, "seek": 143200, "start": 1442.4, "end": 1446.16, "text": " so it probably hasn't changed that much. And I think what I've maybe neglected in some of those", "tokens": [50884, 370, 309, 1391, 6132, 380, 3105, 300, 709, 13, 400, 286, 519, 437, 286, 600, 1310, 32701, 294, 512, 295, 729, 51072], "temperature": 0.0, "avg_logprob": -0.08989928796039365, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0019264078000560403}, {"id": 267, "seek": 143200, "start": 1446.16, "end": 1451.2, "text": " moments is like, but they might have changed the system prompt, or, you know, as we're seeing,", "tokens": [51072, 6065, 307, 411, 11, 457, 436, 1062, 362, 3105, 264, 1185, 12391, 11, 420, 11, 291, 458, 11, 382, 321, 434, 2577, 11, 51324], "temperature": 0.0, "avg_logprob": -0.08989928796039365, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0019264078000560403}, {"id": 268, "seek": 143200, "start": 1451.2, "end": 1456.48, "text": " I mean, even just this last couple of weeks, there's been this really interesting phenomenon of the", "tokens": [51324, 286, 914, 11, 754, 445, 341, 1036, 1916, 295, 3259, 11, 456, 311, 668, 341, 534, 1880, 14029, 295, 264, 51588], "temperature": 0.0, "avg_logprob": -0.08989928796039365, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0019264078000560403}, {"id": 269, "seek": 145648, "start": 1457.3600000000001, "end": 1463.68, "text": " of GPT for getting quote unquote, lazier. And people are speculating that maybe that's because", "tokens": [50408, 295, 26039, 51, 337, 1242, 6513, 37557, 11, 19320, 811, 13, 400, 561, 366, 1608, 12162, 300, 1310, 300, 311, 570, 50724], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 270, "seek": 145648, "start": 1463.68, "end": 1467.28, "text": " they feed the date into it. And it knows that we're in December, and it knows that people", "tokens": [50724, 436, 3154, 264, 4002, 666, 309, 13, 400, 309, 3255, 300, 321, 434, 294, 7687, 11, 293, 309, 3255, 300, 561, 50904], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 271, "seek": 145648, "start": 1467.28, "end": 1471.76, "text": " don't work as hard or as productively in December. And so maybe it's like kind of phoning it in,", "tokens": [50904, 500, 380, 589, 382, 1152, 420, 382, 1674, 3413, 294, 7687, 13, 400, 370, 1310, 309, 311, 411, 733, 295, 903, 16638, 309, 294, 11, 51128], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 272, "seek": 145648, "start": 1471.76, "end": 1477.2, "text": " because it's like, imitating the broad swath of humans that it's seen like, you know, kind of", "tokens": [51128, 570, 309, 311, 411, 11, 566, 16350, 264, 4152, 1693, 998, 295, 6255, 300, 309, 311, 1612, 411, 11, 291, 458, 11, 733, 295, 51400], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 273, "seek": 145648, "start": 1477.2, "end": 1481.92, "text": " work halftime in December or whatever. I've even seen some experiments, just in the last couple", "tokens": [51400, 589, 7523, 844, 1312, 294, 7687, 420, 2035, 13, 286, 600, 754, 1612, 512, 12050, 11, 445, 294, 264, 1036, 1916, 51636], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 274, "seek": 145648, "start": 1481.92, "end": 1485.84, "text": " days that suggest that there might even be real truth to that. Who knows, I'd say that the question", "tokens": [51636, 1708, 300, 3402, 300, 456, 1062, 754, 312, 957, 3494, 281, 300, 13, 2102, 3255, 11, 286, 1116, 584, 300, 264, 1168, 51832], "temperature": 0.0, "avg_logprob": -0.0995461231953389, "compression_ratio": 1.830128205128205, "no_speech_prob": 0.020961670204997063}, {"id": 275, "seek": 148584, "start": 1485.84, "end": 1490.6399999999999, "text": " remains open. But there's a there is an important difference, you know, and it's worth getting", "tokens": [50364, 7023, 1269, 13, 583, 456, 311, 257, 456, 307, 364, 1021, 2649, 11, 291, 458, 11, 293, 309, 311, 3163, 1242, 50604], "temperature": 0.0, "avg_logprob": -0.08452603623673723, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.0028891139663755894}, {"id": 276, "seek": 148584, "start": 1490.6399999999999, "end": 1496.8799999999999, "text": " clarity on the model itself with static weights, not changing versus even just a system prompt", "tokens": [50604, 16992, 322, 264, 2316, 2564, 365, 13437, 17443, 11, 406, 4473, 5717, 754, 445, 257, 1185, 12391, 50916], "temperature": 0.0, "avg_logprob": -0.08452603623673723, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.0028891139663755894}, {"id": 277, "seek": 148584, "start": 1496.8799999999999, "end": 1502.6399999999999, "text": " that can perhaps have, you know, even unexpected drift along the dimension of something as", "tokens": [50916, 300, 393, 4317, 362, 11, 291, 458, 11, 754, 13106, 19699, 2051, 264, 10139, 295, 746, 382, 51204], "temperature": 0.0, "avg_logprob": -0.08452603623673723, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.0028891139663755894}, {"id": 278, "seek": 148584, "start": 1502.6399999999999, "end": 1509.84, "text": " seemingly benign as today's date. So that's important to keep in mind. The levels of capabilities,", "tokens": [51204, 18709, 3271, 788, 382, 965, 311, 4002, 13, 407, 300, 311, 1021, 281, 1066, 294, 1575, 13, 440, 4358, 295, 10862, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08452603623673723, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.0028891139663755894}, {"id": 279, "seek": 148584, "start": 1509.84, "end": 1513.28, "text": " I think, are also really interesting. And I want to ask one kind of I have a couple questions on", "tokens": [51564, 286, 519, 11, 366, 611, 534, 1880, 13, 400, 286, 528, 281, 1029, 472, 733, 295, 286, 362, 257, 1916, 1651, 322, 51736], "temperature": 0.0, "avg_logprob": -0.08452603623673723, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.0028891139663755894}, {"id": 280, "seek": 151328, "start": 1513.28, "end": 1519.2, "text": " this, but I think I have a clear sense of what is meant by contextual. What can it do now, given", "tokens": [50364, 341, 11, 457, 286, 519, 286, 362, 257, 1850, 2020, 295, 437, 307, 4140, 538, 35526, 13, 708, 393, 309, 360, 586, 11, 2212, 50660], "temperature": 0.0, "avg_logprob": -0.13368959275503006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0037067746743559837}, {"id": 281, "seek": 151328, "start": 1519.2, "end": 1524.96, "text": " the packaging, right? What what can GPT forward do in the context of chat GPT, where it has", "tokens": [50660, 264, 16836, 11, 558, 30, 708, 437, 393, 26039, 51, 2128, 360, 294, 264, 4319, 295, 5081, 26039, 51, 11, 689, 309, 575, 50948], "temperature": 0.0, "avg_logprob": -0.13368959275503006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0037067746743559837}, {"id": 282, "seek": 151328, "start": 1524.96, "end": 1530.48, "text": " a code interpreter, and it has browse with Bing, and it has the ability to call Dolly three to", "tokens": [50948, 257, 3089, 34132, 11, 293, 309, 575, 31442, 365, 30755, 11, 293, 309, 575, 264, 3485, 281, 818, 1144, 13020, 1045, 281, 51224], "temperature": 0.0, "avg_logprob": -0.13368959275503006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0037067746743559837}, {"id": 283, "seek": 151328, "start": 1530.48, "end": 1534.6399999999999, "text": " make an image, and probably a couple other things that I'm not even remembering, you know, plugins", "tokens": [51224, 652, 364, 3256, 11, 293, 1391, 257, 1916, 661, 721, 300, 286, 478, 406, 754, 20719, 11, 291, 458, 11, 33759, 51432], "temperature": 0.0, "avg_logprob": -0.13368959275503006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0037067746743559837}, {"id": 284, "seek": 151328, "start": 1534.6399999999999, "end": 1538.8799999999999, "text": " perhaps as well, right, which obviously GPT is which proliferates, you know, all the affordances,", "tokens": [51432, 4317, 382, 731, 11, 558, 11, 597, 2745, 26039, 51, 307, 597, 24398, 9361, 1024, 11, 291, 458, 11, 439, 264, 6157, 2676, 11, 51644], "temperature": 0.0, "avg_logprob": -0.13368959275503006, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0037067746743559837}, {"id": 285, "seek": 153888, "start": 1538.88, "end": 1544.4, "text": " all that much more. On the other end, I feel like I sort of understand absolute, which is like a", "tokens": [50364, 439, 300, 709, 544, 13, 1282, 264, 661, 917, 11, 286, 841, 411, 286, 1333, 295, 1223, 8236, 11, 597, 307, 411, 257, 50640], "temperature": 0.0, "avg_logprob": -0.07796122165436441, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.01495206169784069}, {"id": 286, "seek": 153888, "start": 1544.4, "end": 1550.0800000000002, "text": " theoretical max. Could you give me a little like how do I understand reachable as as kind of between", "tokens": [50640, 20864, 11469, 13, 7497, 291, 976, 385, 257, 707, 411, 577, 360, 286, 1223, 2524, 712, 382, 382, 733, 295, 1296, 50924], "temperature": 0.0, "avg_logprob": -0.07796122165436441, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.01495206169784069}, {"id": 287, "seek": 153888, "start": 1550.0800000000002, "end": 1558.3200000000002, "text": " those like what's what's the distinction between reachable and absolute? Yeah, so so maybe maybe", "tokens": [50924, 729, 411, 437, 311, 437, 311, 264, 16844, 1296, 2524, 712, 293, 8236, 30, 865, 11, 370, 370, 1310, 1310, 51336], "temperature": 0.0, "avg_logprob": -0.07796122165436441, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.01495206169784069}, {"id": 288, "seek": 153888, "start": 1558.3200000000002, "end": 1563.8400000000001, "text": " one way to think of it is like, the contextual capabilities are the ones kind of that a user", "tokens": [51336, 472, 636, 281, 519, 295, 309, 307, 411, 11, 264, 35526, 10862, 366, 264, 2306, 733, 295, 300, 257, 4195, 51612], "temperature": 0.0, "avg_logprob": -0.07796122165436441, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.01495206169784069}, {"id": 289, "seek": 156384, "start": 1563.84, "end": 1570.48, "text": " explicitly gave it. And then the reachable ones are those that may also be reachable without the", "tokens": [50364, 20803, 2729, 309, 13, 400, 550, 264, 2524, 712, 2306, 366, 729, 300, 815, 611, 312, 2524, 712, 1553, 264, 50696], "temperature": 0.0, "avg_logprob": -0.0934426558645148, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.07582718133926392}, {"id": 290, "seek": 156384, "start": 1570.48, "end": 1575.9199999999998, "text": " user even having thought about that the model actually will will use them, right? So if you say,", "tokens": [50696, 4195, 754, 1419, 1194, 466, 300, 264, 2316, 767, 486, 486, 764, 552, 11, 558, 30, 407, 498, 291, 584, 11, 50968], "temperature": 0.0, "avg_logprob": -0.0934426558645148, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.07582718133926392}, {"id": 291, "seek": 156384, "start": 1576.56, "end": 1582.56, "text": " you know, like if the model would be able to browse the web, like entirely on its own, which I'm", "tokens": [51000, 291, 458, 11, 411, 498, 264, 2316, 576, 312, 1075, 281, 31442, 264, 3670, 11, 411, 7696, 322, 1080, 1065, 11, 597, 286, 478, 51300], "temperature": 0.0, "avg_logprob": -0.0934426558645148, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.07582718133926392}, {"id": 292, "seek": 156384, "start": 1582.56, "end": 1588.0, "text": " not sure it currently can do or like what exactly the restrictions on search with Bing are. But if", "tokens": [51300, 406, 988, 309, 4362, 393, 360, 420, 411, 437, 2293, 264, 14191, 322, 3164, 365, 30755, 366, 13, 583, 498, 51572], "temperature": 0.0, "avg_logprob": -0.0934426558645148, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.07582718133926392}, {"id": 293, "seek": 158800, "start": 1588.0, "end": 1593.84, "text": " it was able to do that, right, you may not you may not have realized that it has a reachable", "tokens": [50364, 309, 390, 1075, 281, 360, 300, 11, 558, 11, 291, 815, 406, 291, 815, 406, 362, 5334, 300, 309, 575, 257, 2524, 712, 50656], "temperature": 0.0, "avg_logprob": -0.1261139859210004, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.04602150619029999}, {"id": 294, "seek": 158800, "start": 1593.84, "end": 1601.44, "text": " reachable capability through the internet of like firing up a shell somewhere, or like renting a GPU,", "tokens": [50656, 2524, 712, 13759, 807, 264, 4705, 295, 411, 16045, 493, 257, 8720, 4079, 11, 420, 411, 40598, 257, 18407, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1261139859210004, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.04602150619029999}, {"id": 295, "seek": 158800, "start": 1602.08, "end": 1607.2, "text": " and and like doings or like running a physics simulation through a like an online", "tokens": [51068, 293, 293, 411, 884, 82, 420, 411, 2614, 257, 10649, 16575, 807, 257, 411, 364, 2950, 51324], "temperature": 0.0, "avg_logprob": -0.1261139859210004, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.04602150619029999}, {"id": 296, "seek": 158800, "start": 1608.48, "end": 1613.84, "text": " physics simulator, if that if that's something that's available. And so so these are this is sort of", "tokens": [51388, 10649, 32974, 11, 498, 300, 498, 300, 311, 746, 300, 311, 2435, 13, 400, 370, 370, 613, 366, 341, 307, 1333, 295, 51656], "temperature": 0.0, "avg_logprob": -0.1261139859210004, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.04602150619029999}, {"id": 297, "seek": 161384, "start": 1613.84, "end": 1621.9199999999998, "text": " like how which tools can it reach through the contextual ability capabilities that it already", "tokens": [50364, 411, 577, 597, 3873, 393, 309, 2524, 807, 264, 35526, 3485, 10862, 300, 309, 1217, 50768], "temperature": 0.0, "avg_logprob": -0.16502981474905304, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0008829917060211301}, {"id": 298, "seek": 161384, "start": 1621.9199999999998, "end": 1630.1599999999999, "text": " has given by you or was been has been given by you. Gotcha. Okay. So like solving a capture by", "tokens": [50768, 575, 2212, 538, 291, 420, 390, 668, 575, 668, 2212, 538, 291, 13, 42109, 13, 1033, 13, 407, 411, 12606, 257, 7983, 538, 51180], "temperature": 0.0, "avg_logprob": -0.16502981474905304, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0008829917060211301}, {"id": 299, "seek": 161384, "start": 1630.1599999999999, "end": 1638.08, "text": " hiring an upward contractor for exactly to take one infamous case. So, okay, here's a challenging", "tokens": [51180, 15335, 364, 23452, 26463, 337, 2293, 281, 747, 472, 30769, 1389, 13, 407, 11, 1392, 11, 510, 311, 257, 7595, 51576], "temperature": 0.0, "avg_logprob": -0.16502981474905304, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0008829917060211301}, {"id": 300, "seek": 163808, "start": 1638.1599999999999, "end": 1644.8799999999999, "text": " question. But, and I don't necessarily expect an answer, but maybe you can venture an answer or", "tokens": [50368, 1168, 13, 583, 11, 293, 286, 500, 380, 4725, 2066, 364, 1867, 11, 457, 1310, 291, 393, 18474, 364, 1867, 420, 50704], "temperature": 0.0, "avg_logprob": -0.12533340048282704, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.08752680569887161}, {"id": 301, "seek": 163808, "start": 1644.8799999999999, "end": 1650.6399999999999, "text": " you could just kind of describe how you begin to think about it. What would you say are the absolute", "tokens": [50704, 291, 727, 445, 733, 295, 6786, 577, 291, 1841, 281, 519, 466, 309, 13, 708, 576, 291, 584, 366, 264, 8236, 50992], "temperature": 0.0, "avg_logprob": -0.12533340048282704, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.08752680569887161}, {"id": 302, "seek": 163808, "start": 1650.6399999999999, "end": 1659.6799999999998, "text": " capabilities of GPT four? Yeah, very unclear. So I think they're definitely they're not infinite.", "tokens": [50992, 10862, 295, 26039, 51, 1451, 30, 865, 11, 588, 25636, 13, 407, 286, 519, 436, 434, 2138, 436, 434, 406, 13785, 13, 51444], "temperature": 0.0, "avg_logprob": -0.12533340048282704, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.08752680569887161}, {"id": 303, "seek": 163808, "start": 1660.3999999999999, "end": 1665.4399999999998, "text": " As in, you know, like even with extremely good scaffolding and and access to the internet and", "tokens": [51480, 1018, 294, 11, 291, 458, 11, 411, 754, 365, 4664, 665, 44094, 278, 293, 293, 2105, 281, 264, 4705, 293, 51732], "temperature": 0.0, "avg_logprob": -0.12533340048282704, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.08752680569887161}, {"id": 304, "seek": 166544, "start": 1665.44, "end": 1672.48, "text": " many other things, I think people haven't been able to, you know, get it to do economically", "tokens": [50364, 867, 661, 721, 11, 286, 519, 561, 2378, 380, 668, 1075, 281, 11, 291, 458, 11, 483, 309, 281, 360, 26811, 50716], "temperature": 0.0, "avg_logprob": -0.09381914138793945, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.004904213827103376}, {"id": 305, "seek": 166544, "start": 1672.48, "end": 1678.88, "text": " valuable tasks at the level of a human, at least for like long time spans, for example. So, you", "tokens": [50716, 8263, 9608, 412, 264, 1496, 295, 257, 1952, 11, 412, 1935, 337, 411, 938, 565, 44086, 11, 337, 1365, 13, 407, 11, 291, 51036], "temperature": 0.0, "avg_logprob": -0.09381914138793945, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.004904213827103376}, {"id": 306, "seek": 166544, "start": 1678.88, "end": 1683.2, "text": " know, the question is obviously like, is this, you know, are we just too bad? And have we not", "tokens": [51036, 458, 11, 264, 1168, 307, 2745, 411, 11, 307, 341, 11, 291, 458, 11, 366, 321, 445, 886, 1578, 30, 400, 362, 321, 406, 51252], "temperature": 0.0, "avg_logprob": -0.09381914138793945, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.004904213827103376}, {"id": 307, "seek": 166544, "start": 1683.2, "end": 1687.44, "text": " figured out the right prompting yet and the right scaffolding and so on? Or, or is this just a", "tokens": [51252, 8932, 484, 264, 558, 12391, 278, 1939, 293, 264, 558, 44094, 278, 293, 370, 322, 30, 1610, 11, 420, 307, 341, 445, 257, 51464], "temperature": 0.0, "avg_logprob": -0.09381914138793945, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.004904213827103376}, {"id": 308, "seek": 166544, "start": 1687.44, "end": 1692.4, "text": " limitation of the system? And my current guess is, like, there is probably a limit to the absolute", "tokens": [51464, 27432, 295, 264, 1185, 30, 400, 452, 2190, 2041, 307, 11, 411, 11, 456, 307, 1391, 257, 4948, 281, 264, 8236, 51712], "temperature": 0.0, "avg_logprob": -0.09381914138793945, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.004904213827103376}, {"id": 309, "seek": 169240, "start": 1692.4, "end": 1697.3600000000001, "text": " capabilities. And it's probably lower than like what a human can do. But we're not that far away", "tokens": [50364, 10862, 13, 400, 309, 311, 1391, 3126, 813, 411, 437, 257, 1952, 393, 360, 13, 583, 321, 434, 406, 300, 1400, 1314, 50612], "temperature": 0.0, "avg_logprob": -0.11949560982840401, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.02674977108836174}, {"id": 310, "seek": 169240, "start": 1697.3600000000001, "end": 1703.44, "text": " from it. So, you know, I think with an additional training with additional, like specifically", "tokens": [50612, 490, 309, 13, 407, 11, 291, 458, 11, 286, 519, 365, 364, 4497, 3097, 365, 4497, 11, 411, 4682, 50916], "temperature": 0.0, "avg_logprob": -0.11949560982840401, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.02674977108836174}, {"id": 311, "seek": 169240, "start": 1703.44, "end": 1708.3200000000002, "text": " LM, like training that is more goal direct or makes it into more goal directed and an agent", "tokens": [50916, 46529, 11, 411, 3097, 300, 307, 544, 3387, 2047, 420, 1669, 309, 666, 544, 3387, 12898, 293, 364, 9461, 51160], "temperature": 0.0, "avg_logprob": -0.11949560982840401, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.02674977108836174}, {"id": 312, "seek": 169240, "start": 1709.2800000000002, "end": 1715.2, "text": " and better scaffolding, I think there will be ways in which the absolute capabilities could increase", "tokens": [51208, 293, 1101, 44094, 278, 11, 286, 519, 456, 486, 312, 2098, 294, 597, 264, 8236, 10862, 727, 3488, 51504], "temperature": 0.0, "avg_logprob": -0.11949560982840401, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.02674977108836174}, {"id": 313, "seek": 169240, "start": 1716.0800000000002, "end": 1719.68, "text": " quite a bit in the near future. Yeah, does this make sense?", "tokens": [51548, 1596, 257, 857, 294, 264, 2651, 2027, 13, 865, 11, 775, 341, 652, 2020, 30, 51728], "temperature": 0.0, "avg_logprob": -0.11949560982840401, "compression_ratio": 1.6529850746268657, "no_speech_prob": 0.02674977108836174}, {"id": 314, "seek": 171968, "start": 1720.4, "end": 1726.48, "text": " Yeah, I mean, it's hard, right? I certainly listeners to the show will know from repeated", "tokens": [50400, 865, 11, 286, 914, 11, 309, 311, 1152, 11, 558, 30, 286, 3297, 23274, 281, 264, 855, 486, 458, 490, 10477, 50704], "temperature": 0.0, "avg_logprob": -0.08846182518817008, "compression_ratio": 1.5, "no_speech_prob": 0.000855820020660758}, {"id": 315, "seek": 171968, "start": 1726.48, "end": 1734.16, "text": " storytelling on my part that I was one of the volunteer testers of the GPT-4 early model back", "tokens": [50704, 21479, 322, 452, 644, 300, 286, 390, 472, 295, 264, 13835, 1500, 433, 295, 264, 26039, 51, 12, 19, 2440, 2316, 646, 51088], "temperature": 0.0, "avg_logprob": -0.08846182518817008, "compression_ratio": 1.5, "no_speech_prob": 0.000855820020660758}, {"id": 316, "seek": 171968, "start": 1734.16, "end": 1739.52, "text": " in August, September of last year. And I really kind of challenged myself to try to answer that", "tokens": [51088, 294, 6897, 11, 7216, 295, 1036, 1064, 13, 400, 286, 534, 733, 295, 17737, 2059, 281, 853, 281, 1867, 300, 51356], "temperature": 0.0, "avg_logprob": -0.08846182518817008, "compression_ratio": 1.5, "no_speech_prob": 0.000855820020660758}, {"id": 317, "seek": 171968, "start": 1739.52, "end": 1746.48, "text": " question, you know, independently, like, what is the theoretical max of what this thing can do?", "tokens": [51356, 1168, 11, 291, 458, 11, 21761, 11, 411, 11, 437, 307, 264, 20864, 11469, 295, 437, 341, 551, 393, 360, 30, 51704], "temperature": 0.0, "avg_logprob": -0.08846182518817008, "compression_ratio": 1.5, "no_speech_prob": 0.000855820020660758}, {"id": 318, "seek": 174648, "start": 1747.3600000000001, "end": 1752.4, "text": " How much could it like break down big problems and delegate to itself? And it basically came to", "tokens": [50408, 1012, 709, 727, 309, 411, 1821, 760, 955, 2740, 293, 40999, 281, 2564, 30, 400, 309, 1936, 1361, 281, 50660], "temperature": 0.0, "avg_logprob": -0.09941831155985344, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.01133011095225811}, {"id": 319, "seek": 174648, "start": 1752.4, "end": 1761.52, "text": " the same conclusion that you did, which is like, doesn't seem like it can do really big tasks.", "tokens": [50660, 264, 912, 10063, 300, 291, 630, 11, 597, 307, 411, 11, 1177, 380, 1643, 411, 309, 393, 360, 534, 955, 9608, 13, 51116], "temperature": 0.0, "avg_logprob": -0.09941831155985344, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.01133011095225811}, {"id": 320, "seek": 174648, "start": 1761.52, "end": 1764.56, "text": " I mean, again, it's confusing, right? Because then you could also look at the dimension of", "tokens": [51116, 286, 914, 11, 797, 11, 309, 311, 13181, 11, 558, 30, 1436, 550, 291, 727, 611, 574, 412, 264, 10139, 295, 51268], "temperature": 0.0, "avg_logprob": -0.09941831155985344, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.01133011095225811}, {"id": 321, "seek": 174648, "start": 1765.2, "end": 1769.2, "text": " how big the task is versus how would you break it down? Just in the last week, I've been doing", "tokens": [51300, 577, 955, 264, 5633, 307, 5717, 577, 576, 291, 1821, 309, 760, 30, 1449, 294, 264, 1036, 1243, 11, 286, 600, 668, 884, 51500], "temperature": 0.0, "avg_logprob": -0.09941831155985344, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.01133011095225811}, {"id": 322, "seek": 174648, "start": 1769.2, "end": 1775.92, "text": " something for a very sort of mundane project, but actually using GPT-4 to run evals on other", "tokens": [51500, 746, 337, 257, 588, 1333, 295, 43497, 1716, 11, 457, 767, 1228, 26039, 51, 12, 19, 281, 1190, 1073, 1124, 322, 661, 51836], "temperature": 0.0, "avg_logprob": -0.09941831155985344, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.01133011095225811}, {"id": 323, "seek": 177592, "start": 1775.92, "end": 1783.28, "text": " language model output, I have found that if I have like 10 tasks, 10, you know,", "tokens": [50364, 2856, 2316, 5598, 11, 286, 362, 1352, 300, 498, 286, 362, 411, 1266, 9608, 11, 1266, 11, 291, 458, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08191094398498536, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.0028888462111353874}, {"id": 324, "seek": 177592, "start": 1783.28, "end": 1789.52, "text": " dimensions of evaluation, and I ask it to run all of those, it is now capable of following", "tokens": [50732, 12819, 295, 13344, 11, 293, 286, 1029, 309, 281, 1190, 439, 295, 729, 11, 309, 307, 586, 8189, 295, 3480, 51044], "temperature": 0.0, "avg_logprob": -0.08191094398498536, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.0028888462111353874}, {"id": 325, "seek": 177592, "start": 1789.52, "end": 1796.16, "text": " those directions and executing the tasks one by one. But the quality kind of suffers. It sort of", "tokens": [51044, 729, 11095, 293, 32368, 264, 9608, 472, 538, 472, 13, 583, 264, 3125, 733, 295, 33776, 13, 467, 1333, 295, 51376], "temperature": 0.0, "avg_logprob": -0.08191094398498536, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.0028888462111353874}, {"id": 326, "seek": 177592, "start": 1796.16, "end": 1800.5600000000002, "text": " makes mistakes. It sometimes muddies the tasks a little bit between each other. And it's definitely", "tokens": [51376, 1669, 8038, 13, 467, 2171, 8933, 22018, 264, 9608, 257, 707, 857, 1296, 1184, 661, 13, 400, 309, 311, 2138, 51596], "temperature": 0.0, "avg_logprob": -0.08191094398498536, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.0028888462111353874}, {"id": 327, "seek": 180056, "start": 1800.56, "end": 1807.52, "text": " like not at a human level given 10 tasks to do in one generation. On the flip side, though,", "tokens": [50364, 411, 406, 412, 257, 1952, 1496, 2212, 1266, 9608, 281, 360, 294, 472, 5125, 13, 1282, 264, 7929, 1252, 11, 1673, 11, 50712], "temperature": 0.0, "avg_logprob": -0.11534204188081407, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.03845863789319992}, {"id": 328, "seek": 180056, "start": 1807.52, "end": 1814.24, "text": " if I take it down to one task per generation, which I didn't want to do because that will increase", "tokens": [50712, 498, 286, 747, 309, 760, 281, 472, 5633, 680, 5125, 11, 597, 286, 994, 380, 528, 281, 360, 570, 300, 486, 3488, 51048], "temperature": 0.0, "avg_logprob": -0.11534204188081407, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.03845863789319992}, {"id": 329, "seek": 180056, "start": 1814.24, "end": 1819.9199999999998, "text": " our cost and latency and just is less convenient for me, but then it kind of pops up to, honestly,", "tokens": [51048, 527, 2063, 293, 27043, 293, 445, 307, 1570, 10851, 337, 385, 11, 457, 550, 309, 733, 295, 16795, 493, 281, 11, 6095, 11, 51332], "temperature": 0.0, "avg_logprob": -0.11534204188081407, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.03845863789319992}, {"id": 330, "seek": 180056, "start": 1819.9199999999998, "end": 1825.9199999999998, "text": " I would say pretty much human level, if not above. So there's interesting dimension. I guess", "tokens": [51332, 286, 576, 584, 1238, 709, 1952, 1496, 11, 498, 406, 3673, 13, 407, 456, 311, 1880, 10139, 13, 286, 2041, 51632], "temperature": 0.0, "avg_logprob": -0.11534204188081407, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.03845863789319992}, {"id": 331, "seek": 182592, "start": 1825.92, "end": 1830.64, "text": " it seems pretty the sort of magnitude of the task seems like a pretty important dimension for", "tokens": [50364, 309, 2544, 1238, 264, 1333, 295, 15668, 295, 264, 5633, 2544, 411, 257, 1238, 1021, 10139, 337, 50600], "temperature": 0.0, "avg_logprob": -0.10700000860752204, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.0028001568280160427}, {"id": 332, "seek": 182592, "start": 1831.52, "end": 1835.04, "text": " evaluating a question like absolute capabilities, right? It's like,", "tokens": [50644, 27479, 257, 1168, 411, 8236, 10862, 11, 558, 30, 467, 311, 411, 11, 50820], "temperature": 0.0, "avg_logprob": -0.10700000860752204, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.0028001568280160427}, {"id": 333, "seek": 182592, "start": 1835.04, "end": 1840.3200000000002, "text": " if it's a super narrow thing, it has it's like more it's, it's capable of some pretty high spikes.", "tokens": [50820, 498, 309, 311, 257, 1687, 9432, 551, 11, 309, 575, 309, 311, 411, 544, 309, 311, 11, 309, 311, 8189, 295, 512, 1238, 1090, 28997, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10700000860752204, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.0028001568280160427}, {"id": 334, "seek": 182592, "start": 1840.3200000000002, "end": 1846.72, "text": " But if it's a, if it's a big thing, it kind of gets lost. Would you refine that characterization", "tokens": [51084, 583, 498, 309, 311, 257, 11, 498, 309, 311, 257, 955, 551, 11, 309, 733, 295, 2170, 2731, 13, 6068, 291, 33906, 300, 49246, 51404], "temperature": 0.0, "avg_logprob": -0.10700000860752204, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.0028001568280160427}, {"id": 335, "seek": 182592, "start": 1846.72, "end": 1851.28, "text": " at all? Yeah, yeah, I'm not sure how to think about it, honestly. So I think of absolute capabilities", "tokens": [51404, 412, 439, 30, 865, 11, 1338, 11, 286, 478, 406, 988, 577, 281, 519, 466, 309, 11, 6095, 13, 407, 286, 519, 295, 8236, 10862, 51632], "temperature": 0.0, "avg_logprob": -0.10700000860752204, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.0028001568280160427}, {"id": 336, "seek": 185128, "start": 1851.28, "end": 1857.52, "text": " really more of a sort of theoretical bound that we could, that we're probably not going to approximate", "tokens": [50364, 534, 544, 295, 257, 1333, 295, 20864, 5472, 300, 321, 727, 11, 300, 321, 434, 1391, 406, 516, 281, 30874, 50676], "temperature": 0.0, "avg_logprob": -0.11927872030144064, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.03113272599875927}, {"id": 337, "seek": 185128, "start": 1857.52, "end": 1863.44, "text": " in practice, even if we test like a lot, a lot. And then the, then like breaking it down into", "tokens": [50676, 294, 3124, 11, 754, 498, 321, 1500, 411, 257, 688, 11, 257, 688, 13, 400, 550, 264, 11, 550, 411, 7697, 309, 760, 666, 50972], "temperature": 0.0, "avg_logprob": -0.11927872030144064, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.03113272599875927}, {"id": 338, "seek": 185128, "start": 1863.44, "end": 1867.36, "text": " different tasks, I'm not sure I feel like this is a different capability then, right? Like you're", "tokens": [50972, 819, 9608, 11, 286, 478, 406, 988, 286, 841, 411, 341, 307, 257, 819, 13759, 550, 11, 558, 30, 1743, 291, 434, 51168], "temperature": 0.0, "avg_logprob": -0.11927872030144064, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.03113272599875927}, {"id": 339, "seek": 185128, "start": 1867.92, "end": 1872.56, "text": " sort of the capability of doing 10 things at once is a different thing than the capability of", "tokens": [51196, 1333, 295, 264, 13759, 295, 884, 1266, 721, 412, 1564, 307, 257, 819, 551, 813, 264, 13759, 295, 51428], "temperature": 0.0, "avg_logprob": -0.11927872030144064, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.03113272599875927}, {"id": 340, "seek": 185128, "start": 1872.56, "end": 1880.48, "text": " doing one thing 10 times like 10, 10 diff different things, but one by one. So yeah,", "tokens": [51428, 884, 472, 551, 1266, 1413, 411, 1266, 11, 1266, 7593, 819, 721, 11, 457, 472, 538, 472, 13, 407, 1338, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11927872030144064, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.03113272599875927}, {"id": 341, "seek": 188048, "start": 1880.48, "end": 1886.16, "text": " I would say it's basically you're talking about different capabilities then, at least in this", "tokens": [50364, 286, 576, 584, 309, 311, 1936, 291, 434, 1417, 466, 819, 10862, 550, 11, 412, 1935, 294, 341, 50648], "temperature": 0.0, "avg_logprob": -0.10267305374145508, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.000646137457806617}, {"id": 342, "seek": 188048, "start": 1886.16, "end": 1891.28, "text": " framework. Hey, we'll continue our interview in a moment after a word from our sponsors.", "tokens": [50648, 8388, 13, 1911, 11, 321, 603, 2354, 527, 4049, 294, 257, 1623, 934, 257, 1349, 490, 527, 22593, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10267305374145508, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.000646137457806617}, {"id": 343, "seek": 188048, "start": 1891.28, "end": 1895.3600000000001, "text": " If you're a startup founder or executive running a growing business, you know that as you scale,", "tokens": [50904, 759, 291, 434, 257, 18578, 14917, 420, 10140, 2614, 257, 4194, 1606, 11, 291, 458, 300, 382, 291, 4373, 11, 51108], "temperature": 0.0, "avg_logprob": -0.10267305374145508, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.000646137457806617}, {"id": 344, "seek": 188048, "start": 1895.3600000000001, "end": 1899.68, "text": " your systems break down, and the cracks start to show. If this resonates with you,", "tokens": [51108, 428, 3652, 1821, 760, 11, 293, 264, 21770, 722, 281, 855, 13, 759, 341, 41051, 365, 291, 11, 51324], "temperature": 0.0, "avg_logprob": -0.10267305374145508, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.000646137457806617}, {"id": 345, "seek": 188048, "start": 1899.68, "end": 1906.56, "text": " there are three numbers you need to know, 36,000, 25 and one, 36,000. That's the number of businesses", "tokens": [51324, 456, 366, 1045, 3547, 291, 643, 281, 458, 11, 8652, 11, 1360, 11, 3552, 293, 472, 11, 8652, 11, 1360, 13, 663, 311, 264, 1230, 295, 6011, 51668], "temperature": 0.0, "avg_logprob": -0.10267305374145508, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.000646137457806617}, {"id": 346, "seek": 190656, "start": 1906.56, "end": 1910.56, "text": " which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system,", "tokens": [50364, 597, 362, 24133, 281, 6188, 50, 21681, 538, 25654, 13, 6188, 50, 21681, 307, 264, 1230, 472, 4588, 4669, 1185, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09913875288882498, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.2567443549633026}, {"id": 347, "seek": 190656, "start": 1910.56, "end": 1916.8, "text": " streamline accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25", "tokens": [50564, 47141, 19163, 11, 4669, 4592, 11, 14228, 11, 19460, 11, 293, 544, 13, 3552, 13, 6188, 50, 21681, 4523, 3552, 50876], "temperature": 0.0, "avg_logprob": -0.09913875288882498, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.2567443549633026}, {"id": 348, "seek": 190656, "start": 1916.8, "end": 1921.6799999999998, "text": " this year. That's 25 years of helping businesses do more with less, close their books in days, not", "tokens": [50876, 341, 1064, 13, 663, 311, 3552, 924, 295, 4315, 6011, 360, 544, 365, 1570, 11, 1998, 641, 3642, 294, 1708, 11, 406, 51120], "temperature": 0.0, "avg_logprob": -0.09913875288882498, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.2567443549633026}, {"id": 349, "seek": 190656, "start": 1921.6799999999998, "end": 1927.12, "text": " weeks, and drive down costs. One, because your business is one of a kind, so you get a customized", "tokens": [51120, 3259, 11, 293, 3332, 760, 5497, 13, 1485, 11, 570, 428, 1606, 307, 472, 295, 257, 733, 11, 370, 291, 483, 257, 30581, 51392], "temperature": 0.0, "avg_logprob": -0.09913875288882498, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.2567443549633026}, {"id": 350, "seek": 190656, "start": 1927.12, "end": 1931.9199999999998, "text": " solution for all your KPIs in one efficient system with one source of truth. Manage risk,", "tokens": [51392, 3827, 337, 439, 428, 41371, 6802, 294, 472, 7148, 1185, 365, 472, 4009, 295, 3494, 13, 2458, 609, 3148, 11, 51632], "temperature": 0.0, "avg_logprob": -0.09913875288882498, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.2567443549633026}, {"id": 351, "seek": 193192, "start": 1931.92, "end": 1935.92, "text": " get reliable forecasts, and improve margins. Everything you need, all in one place.", "tokens": [50364, 483, 12924, 49421, 11, 293, 3470, 30317, 13, 5471, 291, 643, 11, 439, 294, 472, 1081, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11934147431300236, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.040838178247213364}, {"id": 352, "seek": 193192, "start": 1936.48, "end": 1941.2, "text": " Right now, download NetSuite's popular KPI checklist, designed to give you consistently", "tokens": [50592, 1779, 586, 11, 5484, 6188, 50, 21681, 311, 3743, 591, 31701, 30357, 11, 4761, 281, 976, 291, 14961, 50828], "temperature": 0.0, "avg_logprob": -0.11934147431300236, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.040838178247213364}, {"id": 353, "seek": 193192, "start": 1941.2, "end": 1946.64, "text": " excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com", "tokens": [50828, 7103, 3389, 11, 3122, 1737, 11, 293, 2533, 33136, 13, 1112, 17330, 15605, 13, 663, 311, 2533, 33136, 13, 1112, 51100], "temperature": 0.0, "avg_logprob": -0.11934147431300236, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.040838178247213364}, {"id": 354, "seek": 193192, "start": 1946.64, "end": 1951.2, "text": " slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.", "tokens": [51100, 17330, 15605, 281, 483, 428, 1065, 591, 31701, 30357, 13, 6188, 50, 21681, 13, 1112, 17330, 15605, 13, 51328], "temperature": 0.0, "avg_logprob": -0.11934147431300236, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.040838178247213364}, {"id": 355, "seek": 193192, "start": 1952.48, "end": 1958.0, "text": " Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that", "tokens": [51392, 9757, 77, 1123, 4960, 1337, 1166, 7318, 281, 9528, 291, 281, 4025, 6779, 295, 5383, 295, 614, 36540, 300, 51668], "temperature": 0.0, "avg_logprob": -0.11934147431300236, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.040838178247213364}, {"id": 356, "seek": 195800, "start": 1958.0, "end": 1963.68, "text": " actually work, customized across all platforms with a click of a button. I believe in Omnike so", "tokens": [50364, 767, 589, 11, 30581, 2108, 439, 9473, 365, 257, 2052, 295, 257, 2960, 13, 286, 1697, 294, 9757, 77, 1123, 370, 50648], "temperature": 0.0, "avg_logprob": -0.12464383391083264, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02441837638616562}, {"id": 357, "seek": 195800, "start": 1963.68, "end": 1970.08, "text": " much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount.", "tokens": [50648, 709, 300, 286, 13104, 294, 309, 11, 293, 286, 2748, 291, 764, 309, 886, 13, 8278, 591, 664, 40382, 281, 483, 257, 1266, 4, 11635, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12464383391083264, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02441837638616562}, {"id": 358, "seek": 195800, "start": 1970.08, "end": 1976.32, "text": " Yeah, and it is not that good at decomposing the tasks. I've also kind of experimented a little", "tokens": [50968, 865, 11, 293, 309, 307, 406, 300, 665, 412, 22867, 6110, 264, 9608, 13, 286, 600, 611, 733, 295, 5120, 292, 257, 707, 51280], "temperature": 0.0, "avg_logprob": -0.12464383391083264, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02441837638616562}, {"id": 359, "seek": 195800, "start": 1976.32, "end": 1981.6, "text": " bit with like, can you give it that list of 10 tasks and can it break them down and", "tokens": [51280, 857, 365, 411, 11, 393, 291, 976, 309, 300, 1329, 295, 1266, 9608, 293, 393, 309, 1821, 552, 760, 293, 51544], "temperature": 0.0, "avg_logprob": -0.12464383391083264, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02441837638616562}, {"id": 360, "seek": 195800, "start": 1982.4, "end": 1987.6, "text": " self-delegate with an effective prompt? It's like maybe a little bit closer there,", "tokens": [51584, 2698, 12, 1479, 6363, 473, 365, 364, 4942, 12391, 30, 467, 311, 411, 1310, 257, 707, 857, 4966, 456, 11, 51844], "temperature": 0.0, "avg_logprob": -0.12464383391083264, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02441837638616562}, {"id": 361, "seek": 198760, "start": 1987.6, "end": 1995.28, "text": " but still not getting nearly as good results as if I just roll it my sleeves and do the task", "tokens": [50364, 457, 920, 406, 1242, 6217, 382, 665, 3542, 382, 498, 286, 445, 3373, 309, 452, 24555, 293, 360, 264, 5633, 50748], "temperature": 0.0, "avg_logprob": -0.14173318378960909, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.00010553596075624228}, {"id": 362, "seek": 198760, "start": 1995.28, "end": 2001.36, "text": " decomposition. You mentioned that you expect this frontier to obviously continue to move.", "tokens": [50748, 48356, 13, 509, 2835, 300, 291, 2066, 341, 35853, 281, 2745, 2354, 281, 1286, 13, 51052], "temperature": 0.0, "avg_logprob": -0.14173318378960909, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.00010553596075624228}, {"id": 363, "seek": 198760, "start": 2001.9199999999998, "end": 2009.36, "text": " One way to ask the question is, what is Q-star? A more sensible way to ask the question is,", "tokens": [51080, 1485, 636, 281, 1029, 264, 1168, 307, 11, 437, 307, 1249, 12, 9710, 30, 316, 544, 25380, 636, 281, 1029, 264, 1168, 307, 11, 51452], "temperature": 0.0, "avg_logprob": -0.14173318378960909, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.00010553596075624228}, {"id": 364, "seek": 200936, "start": 2010.32, "end": 2017.9199999999998, "text": " do you have a set of expectations for how the capabilities frontier will move? I definitely", "tokens": [50412, 360, 291, 362, 257, 992, 295, 9843, 337, 577, 264, 10862, 35853, 486, 1286, 30, 286, 2138, 50792], "temperature": 0.0, "avg_logprob": -0.11339246010293766, "compression_ratio": 1.6059479553903346, "no_speech_prob": 0.09006641805171967}, {"id": 365, "seek": 200936, "start": 2017.9199999999998, "end": 2023.76, "text": " look at things like OpenAI's publication from earlier this year where they gave,", "tokens": [50792, 574, 412, 721, 411, 7238, 48698, 311, 19953, 490, 3071, 341, 1064, 689, 436, 2729, 11, 51084], "temperature": 0.0, "avg_logprob": -0.11339246010293766, "compression_ratio": 1.6059479553903346, "no_speech_prob": 0.09006641805171967}, {"id": 366, "seek": 200936, "start": 2024.3999999999999, "end": 2028.8799999999999, "text": " started to give denser feedback on kind of every step of the reasoning process and they", "tokens": [51116, 1409, 281, 976, 24505, 260, 5824, 322, 733, 295, 633, 1823, 295, 264, 21577, 1399, 293, 436, 51340], "temperature": 0.0, "avg_logprob": -0.11339246010293766, "compression_ratio": 1.6059479553903346, "no_speech_prob": 0.09006641805171967}, {"id": 367, "seek": 200936, "start": 2028.8799999999999, "end": 2034.1599999999999, "text": " achieved some state-of-the-art results on mathematical reasoning that way. And when I", "tokens": [51340, 11042, 512, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 18894, 21577, 300, 636, 13, 400, 562, 286, 51604], "temperature": 0.0, "avg_logprob": -0.11339246010293766, "compression_ratio": 1.6059479553903346, "no_speech_prob": 0.09006641805171967}, {"id": 368, "seek": 200936, "start": 2034.1599999999999, "end": 2038.08, "text": " think about affordances and I think about the failure modes that I've seen with these", "tokens": [51604, 519, 466, 6157, 2676, 293, 286, 519, 466, 264, 7763, 14068, 300, 286, 600, 1612, 365, 613, 51800], "temperature": 0.0, "avg_logprob": -0.11339246010293766, "compression_ratio": 1.6059479553903346, "no_speech_prob": 0.09006641805171967}, {"id": 369, "seek": 203808, "start": 2038.8, "end": 2045.76, "text": " GPT-4 agent type systems, I think, man, you apply that to browsing the web and using APIs", "tokens": [50400, 26039, 51, 12, 19, 9461, 2010, 3652, 11, 286, 519, 11, 587, 11, 291, 3079, 300, 281, 38602, 264, 3670, 293, 1228, 21445, 50748], "temperature": 0.0, "avg_logprob": -0.11614645288345661, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0028005256317555904}, {"id": 370, "seek": 203808, "start": 2046.3999999999999, "end": 2052.88, "text": " and it seems like that stuff is ultimately a lot less cognitively demanding than pure math.", "tokens": [50780, 293, 309, 2544, 411, 300, 1507, 307, 6284, 257, 688, 1570, 15605, 356, 19960, 813, 6075, 5221, 13, 51104], "temperature": 0.0, "avg_logprob": -0.11614645288345661, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0028005256317555904}, {"id": 371, "seek": 203808, "start": 2052.88, "end": 2058.0, "text": " It seems like we probably are going to see, and I would guess that it's maybe already working", "tokens": [51104, 467, 2544, 411, 321, 1391, 366, 516, 281, 536, 11, 293, 286, 576, 2041, 300, 309, 311, 1310, 1217, 1364, 51360], "temperature": 0.0, "avg_logprob": -0.11614645288345661, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0028005256317555904}, {"id": 372, "seek": 203808, "start": 2058.0, "end": 2062.48, "text": " pretty well. AGI has been achieved internally, I don't know about AGI, but I would expect that", "tokens": [51360, 1238, 731, 13, 316, 26252, 575, 668, 11042, 19501, 11, 286, 500, 380, 458, 466, 316, 26252, 11, 457, 286, 576, 2066, 300, 51584], "temperature": 0.0, "avg_logprob": -0.11614645288345661, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0028005256317555904}, {"id": 373, "seek": 206248, "start": 2062.48, "end": 2069.2, "text": " some of this stuff is already pretty far along in kind of internal prototyping. But how does that", "tokens": [50364, 512, 295, 341, 1507, 307, 1217, 1238, 1400, 2051, 294, 733, 295, 6920, 46219, 3381, 13, 583, 577, 775, 300, 50700], "temperature": 0.0, "avg_logprob": -0.09268624155144943, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.005059224087744951}, {"id": 374, "seek": 206248, "start": 2069.2, "end": 2074.16, "text": " compare to what you would expect to see coming online over the next few months? Yeah, it's obviously", "tokens": [50700, 6794, 281, 437, 291, 576, 2066, 281, 536, 1348, 2950, 670, 264, 958, 1326, 2493, 30, 865, 11, 309, 311, 2745, 50948], "temperature": 0.0, "avg_logprob": -0.09268624155144943, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.005059224087744951}, {"id": 375, "seek": 206248, "start": 2074.16, "end": 2080.08, "text": " hard to say and I can only speculate. I think on a high level what I would expect the big trends", "tokens": [50948, 1152, 281, 584, 293, 286, 393, 787, 40775, 13, 286, 519, 322, 257, 1090, 1496, 437, 286, 576, 2066, 264, 955, 13892, 51244], "temperature": 0.0, "avg_logprob": -0.09268624155144943, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.005059224087744951}, {"id": 376, "seek": 206248, "start": 2080.08, "end": 2088.64, "text": " to be and also what we are kind of looking forward to evaluating is LM agents. I think this is like", "tokens": [51244, 281, 312, 293, 611, 437, 321, 366, 733, 295, 1237, 2128, 281, 27479, 307, 46529, 12554, 13, 286, 519, 341, 307, 411, 51672], "temperature": 0.0, "avg_logprob": -0.09268624155144943, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.005059224087744951}, {"id": 377, "seek": 208864, "start": 2088.64, "end": 2093.8399999999997, "text": " pretty agreed upon. From first principle, I think it also makes sense. It's just like,", "tokens": [50364, 1238, 9166, 3564, 13, 3358, 700, 8665, 11, 286, 519, 309, 611, 1669, 2020, 13, 467, 311, 445, 411, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10317794014425839, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.06747251749038696}, {"id": 378, "seek": 208864, "start": 2094.48, "end": 2100.64, "text": " where does the money come from? It is from AI systems doing economically useful tasks", "tokens": [50656, 689, 775, 264, 1460, 808, 490, 30, 467, 307, 490, 7318, 3652, 884, 26811, 4420, 9608, 50964], "temperature": 0.0, "avg_logprob": -0.10317794014425839, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.06747251749038696}, {"id": 379, "seek": 208864, "start": 2100.64, "end": 2106.16, "text": " and often economically useful tasks just require you to do things independently,", "tokens": [50964, 293, 2049, 26811, 4420, 9608, 445, 3651, 291, 281, 360, 721, 21761, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10317794014425839, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.06747251749038696}, {"id": 380, "seek": 208864, "start": 2106.16, "end": 2112.8799999999997, "text": " being goal directed over a longer period of time. And the longer a model can do things on its own,", "tokens": [51240, 885, 3387, 12898, 670, 257, 2854, 2896, 295, 565, 13, 400, 264, 2854, 257, 2316, 393, 360, 721, 322, 1080, 1065, 11, 51576], "temperature": 0.0, "avg_logprob": -0.10317794014425839, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.06747251749038696}, {"id": 381, "seek": 208864, "start": 2112.8799999999997, "end": 2117.7599999999998, "text": " the more money you can squeeze out of it. So I definitely think just from financial interests,", "tokens": [51576, 264, 544, 1460, 291, 393, 13578, 484, 295, 309, 13, 407, 286, 2138, 519, 445, 490, 4669, 8847, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10317794014425839, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.06747251749038696}, {"id": 382, "seek": 211776, "start": 2117.76, "end": 2124.4, "text": " all of the AGI labs will definitely try to get in more, more agentic ways. How far they have come,", "tokens": [50364, 439, 295, 264, 316, 26252, 20339, 486, 2138, 853, 281, 483, 294, 544, 11, 544, 9461, 299, 2098, 13, 1012, 1400, 436, 362, 808, 11, 50696], "temperature": 0.0, "avg_logprob": -0.14030739695755476, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0024720197543501854}, {"id": 383, "seek": 211776, "start": 2124.4, "end": 2129.84, "text": " I don't know. But yeah, I expect that next year we will see quite some surprises.", "tokens": [50696, 286, 500, 380, 458, 13, 583, 1338, 11, 286, 2066, 300, 958, 1064, 321, 486, 536, 1596, 512, 22655, 13, 50968], "temperature": 0.0, "avg_logprob": -0.14030739695755476, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0024720197543501854}, {"id": 384, "seek": 211776, "start": 2131.1200000000003, "end": 2137.6000000000004, "text": " Then multimodality is the other one where, yeah, I think people kind of like over the last couple", "tokens": [51032, 1396, 32972, 378, 1860, 307, 264, 661, 472, 689, 11, 1338, 11, 286, 519, 561, 733, 295, 411, 670, 264, 1036, 1916, 51356], "temperature": 0.0, "avg_logprob": -0.14030739695755476, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0024720197543501854}, {"id": 385, "seek": 211776, "start": 2137.6000000000004, "end": 2144.4, "text": " of years with more and more multimodal models, people just realized it's not that different from", "tokens": [51356, 295, 924, 365, 544, 293, 544, 32972, 378, 304, 5245, 11, 561, 445, 5334, 309, 311, 406, 300, 819, 490, 51696], "temperature": 0.0, "avg_logprob": -0.14030739695755476, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0024720197543501854}, {"id": 386, "seek": 214440, "start": 2145.2000000000003, "end": 2152.4, "text": " just training text. You plug in the additional modalities, you change your training slightly,", "tokens": [50404, 445, 3097, 2487, 13, 509, 5452, 294, 264, 4497, 1072, 16110, 11, 291, 1319, 428, 3097, 4748, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11035372994162819, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.015901057049632072}, {"id": 387, "seek": 214440, "start": 2153.6, "end": 2159.2000000000003, "text": " but it's not much more than that. Obviously, it's obviously hard in practice. There's a ton of", "tokens": [50824, 457, 309, 311, 406, 709, 544, 813, 300, 13, 7580, 11, 309, 311, 2745, 1152, 294, 3124, 13, 821, 311, 257, 2952, 295, 51104], "temperature": 0.0, "avg_logprob": -0.11035372994162819, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.015901057049632072}, {"id": 388, "seek": 214440, "start": 2159.2000000000003, "end": 2165.44, "text": " engineering challenges and so on. But on a conceptual level, there isn't any big breakthrough", "tokens": [51104, 7043, 4759, 293, 370, 322, 13, 583, 322, 257, 24106, 1496, 11, 456, 1943, 380, 604, 955, 22397, 51416], "temperature": 0.0, "avg_logprob": -0.11035372994162819, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.015901057049632072}, {"id": 389, "seek": 214440, "start": 2165.44, "end": 2170.1600000000003, "text": " needed. So people will just add more and more modalities on bigger and bigger models and train", "tokens": [51416, 2978, 13, 407, 561, 486, 445, 909, 544, 293, 544, 1072, 16110, 322, 3801, 293, 3801, 5245, 293, 3847, 51652], "temperature": 0.0, "avg_logprob": -0.11035372994162819, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.015901057049632072}, {"id": 390, "seek": 217016, "start": 2170.24, "end": 2176.7999999999997, "text": " it all jointly and to end. And it kind of just works. And then tool use is the last one.", "tokens": [50368, 309, 439, 46557, 293, 281, 917, 13, 400, 309, 733, 295, 445, 1985, 13, 400, 550, 2290, 764, 307, 264, 1036, 472, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14797749811289262, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.030194925144314766}, {"id": 391, "seek": 217016, "start": 2177.92, "end": 2182.7999999999997, "text": " And that I think people, yeah, people actually were quite surprised by how like, quote unquote,", "tokens": [50752, 400, 300, 286, 519, 561, 11, 1338, 11, 561, 767, 645, 1596, 6100, 538, 577, 411, 11, 6513, 37557, 11, 50996], "temperature": 0.0, "avg_logprob": -0.14797749811289262, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.030194925144314766}, {"id": 392, "seek": 217016, "start": 2182.7999999999997, "end": 2189.92, "text": " easy it was to get to this level. So yeah, I think when people realize like, oh, these language", "tokens": [50996, 1858, 309, 390, 281, 483, 281, 341, 1496, 13, 407, 1338, 11, 286, 519, 562, 561, 4325, 411, 11, 1954, 11, 613, 2856, 51352], "temperature": 0.0, "avg_logprob": -0.14797749811289262, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.030194925144314766}, {"id": 393, "seek": 217016, "start": 2189.92, "end": 2195.68, "text": " models are already pretty good, like how fast do they learn how to use any tool we can think of?", "tokens": [51352, 5245, 366, 1217, 1238, 665, 11, 411, 577, 2370, 360, 436, 1466, 577, 281, 764, 604, 2290, 321, 393, 519, 295, 30, 51640], "temperature": 0.0, "avg_logprob": -0.14797749811289262, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.030194925144314766}, {"id": 394, "seek": 219568, "start": 2196.64, "end": 2201.68, "text": " And they were surprised by how fast they learned the tools. And now it's mostly a question of", "tokens": [50412, 400, 436, 645, 6100, 538, 577, 2370, 436, 3264, 264, 3873, 13, 400, 586, 309, 311, 5240, 257, 1168, 295, 50664], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 395, "seek": 219568, "start": 2202.3999999999996, "end": 2207.9199999999996, "text": " sort of really baking in the tool into the model in a way that it's like very robustly able to use", "tokens": [50700, 1333, 295, 534, 12102, 294, 264, 2290, 666, 264, 2316, 294, 257, 636, 300, 309, 311, 411, 588, 13956, 356, 1075, 281, 764, 50976], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 396, "seek": 219568, "start": 2207.9199999999996, "end": 2212.24, "text": " the model rather than just a little bit or just showing that it's it sometimes work. But yeah,", "tokens": [50976, 264, 2316, 2831, 813, 445, 257, 707, 857, 420, 445, 4099, 300, 309, 311, 309, 2171, 589, 13, 583, 1338, 11, 51192], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 397, "seek": 219568, "start": 2212.24, "end": 2216.3199999999997, "text": " I mean, you know, like I think if you have an LM agent that is multimodal, and that has very good", "tokens": [51192, 286, 914, 11, 291, 458, 11, 411, 286, 519, 498, 291, 362, 364, 46529, 9461, 300, 307, 32972, 378, 304, 11, 293, 300, 575, 588, 665, 51396], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 398, "seek": 219568, "start": 2216.3199999999997, "end": 2220.7999999999997, "text": " tool use, like I'm not quite sure how far you are away from AGI, right? Like at that point,", "tokens": [51396, 2290, 764, 11, 411, 286, 478, 406, 1596, 988, 577, 1400, 291, 366, 1314, 490, 316, 26252, 11, 558, 30, 1743, 412, 300, 935, 11, 51620], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 399, "seek": 219568, "start": 2220.7999999999997, "end": 2225.44, "text": " you kind of have almost all of the ingredients ready. And then it's really just a question of", "tokens": [51620, 291, 733, 295, 362, 1920, 439, 295, 264, 6952, 1919, 13, 400, 550, 309, 311, 534, 445, 257, 1168, 295, 51852], "temperature": 0.0, "avg_logprob": -0.0982851246059341, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.012818295508623123}, {"id": 400, "seek": 222544, "start": 2225.44, "end": 2229.68, "text": " how robust is the system. So yeah, I think these are the trends we see right now. And this is also", "tokens": [50364, 577, 13956, 307, 264, 1185, 13, 407, 1338, 11, 286, 519, 613, 366, 264, 13892, 321, 536, 558, 586, 13, 400, 341, 307, 611, 50576], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 401, "seek": 222544, "start": 2229.68, "end": 2233.52, "text": " why many people in the big labs have very, very short timelines, because they can think like", "tokens": [50576, 983, 867, 561, 294, 264, 955, 20339, 362, 588, 11, 588, 2099, 45886, 11, 570, 436, 393, 519, 411, 50768], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 402, "seek": 222544, "start": 2233.52, "end": 2238.32, "text": " two years ahead and sort of where this is going, or maybe even just one year ahead, I don't know.", "tokens": [50768, 732, 924, 2286, 293, 1333, 295, 689, 341, 307, 516, 11, 420, 1310, 754, 445, 472, 1064, 2286, 11, 286, 500, 380, 458, 13, 51008], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 403, "seek": 222544, "start": 2238.32, "end": 2243.36, "text": " When you talked about the surprise, like people were surprised at how easy it was to get tool", "tokens": [51008, 1133, 291, 2825, 466, 264, 6365, 11, 411, 561, 645, 6100, 412, 577, 1858, 309, 390, 281, 483, 2290, 51260], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 404, "seek": 222544, "start": 2243.36, "end": 2250.16, "text": " used to work. Are you referring to people in the leading, you know, the obvious, the usual suspects", "tokens": [51260, 1143, 281, 589, 13, 2014, 291, 13761, 281, 561, 294, 264, 5775, 11, 291, 458, 11, 264, 6322, 11, 264, 7713, 35667, 51600], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 405, "seek": 222544, "start": 2250.16, "end": 2254.2400000000002, "text": " of leading developers? It's hard to say. I mean, I can only speculate on this. But you know, the", "tokens": [51600, 295, 5775, 8849, 30, 467, 311, 1152, 281, 584, 13, 286, 914, 11, 286, 393, 787, 40775, 322, 341, 13, 583, 291, 458, 11, 264, 51804], "temperature": 0.0, "avg_logprob": -0.0836631813827826, "compression_ratio": 1.762917933130699, "no_speech_prob": 0.0001795061252778396}, {"id": 406, "seek": 225424, "start": 2254.24, "end": 2261.68, "text": " tool former paper was like three months, or like was published three months before open AI just", "tokens": [50364, 2290, 5819, 3035, 390, 411, 1045, 2493, 11, 420, 411, 390, 6572, 1045, 2493, 949, 1269, 7318, 445, 50736], "temperature": 0.0, "avg_logprob": -0.12583193618260072, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0010321610607206821}, {"id": 407, "seek": 225424, "start": 2261.68, "end": 2266.56, "text": " released their tool use. And I mean, they probably had been working on this before, but still, you", "tokens": [50736, 4736, 641, 2290, 764, 13, 400, 286, 914, 11, 436, 1391, 632, 668, 1364, 322, 341, 949, 11, 457, 920, 11, 291, 50980], "temperature": 0.0, "avg_logprob": -0.12583193618260072, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0010321610607206821}, {"id": 408, "seek": 225424, "start": 2266.56, "end": 2272.4799999999996, "text": " know, like the from from having the scientific insight to to like publishing this and releasing", "tokens": [50980, 458, 11, 411, 264, 490, 490, 1419, 264, 8134, 11269, 281, 281, 411, 17832, 341, 293, 16327, 51276], "temperature": 0.0, "avg_logprob": -0.12583193618260072, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0010321610607206821}, {"id": 409, "seek": 225424, "start": 2272.4799999999996, "end": 2280.3999999999996, "text": " this in the real world, I think there just was less work involved than is, or is typical for most", "tokens": [51276, 341, 294, 264, 957, 1002, 11, 286, 519, 456, 445, 390, 1570, 589, 3288, 813, 307, 11, 420, 307, 7476, 337, 881, 51672], "temperature": 0.0, "avg_logprob": -0.12583193618260072, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0010321610607206821}, {"id": 410, "seek": 228040, "start": 2280.48, "end": 2286.1600000000003, "text": " of the bigger AI, like development cycles, I could be wrong on this. This is this is more", "tokens": [50368, 295, 264, 3801, 7318, 11, 411, 3250, 17796, 11, 286, 727, 312, 2085, 322, 341, 13, 639, 307, 341, 307, 544, 50652], "temperature": 0.0, "avg_logprob": -0.11316226510440602, "compression_ratio": 1.5346938775510204, "no_speech_prob": 0.014500437304377556}, {"id": 411, "seek": 228040, "start": 2286.1600000000003, "end": 2292.88, "text": " here say so yeah, take it with a grain of salt. Yeah, it seems right to me as well. And I agree", "tokens": [50652, 510, 584, 370, 1338, 11, 747, 309, 365, 257, 12837, 295, 5139, 13, 865, 11, 309, 2544, 558, 281, 385, 382, 731, 13, 400, 286, 3986, 50988], "temperature": 0.0, "avg_logprob": -0.11316226510440602, "compression_ratio": 1.5346938775510204, "no_speech_prob": 0.014500437304377556}, {"id": 412, "seek": 228040, "start": 2292.88, "end": 2298.32, "text": " with you the emphasis on multi modality as a new unlock makes a ton of sense, even just in this", "tokens": [50988, 365, 291, 264, 16271, 322, 4825, 1072, 1860, 382, 257, 777, 11634, 1669, 257, 2952, 295, 2020, 11, 754, 445, 294, 341, 51260], "temperature": 0.0, "avg_logprob": -0.11316226510440602, "compression_ratio": 1.5346938775510204, "no_speech_prob": 0.014500437304377556}, {"id": 413, "seek": 228040, "start": 2298.32, "end": 2304.8, "text": " kind of agent paradigm of, you know, can I browse the internet or whatever. I've done a lot of", "tokens": [51260, 733, 295, 9461, 24709, 295, 11, 291, 458, 11, 393, 286, 31442, 264, 4705, 420, 2035, 13, 286, 600, 1096, 257, 688, 295, 51584], "temperature": 0.0, "avg_logprob": -0.11316226510440602, "compression_ratio": 1.5346938775510204, "no_speech_prob": 0.014500437304377556}, {"id": 414, "seek": 230480, "start": 2305.44, "end": 2311.2000000000003, "text": " browser automation type work in the past. And the difference between having to", "tokens": [50396, 11185, 17769, 2010, 589, 294, 264, 1791, 13, 400, 264, 2649, 1296, 1419, 281, 50684], "temperature": 0.0, "avg_logprob": -0.12356223230776579, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.01590435765683651}, {"id": 415, "seek": 230480, "start": 2312.32, "end": 2317.84, "text": " grab all the HTML that, you know, is often these days, like extremely bloated and, you know, kind", "tokens": [50740, 4444, 439, 264, 17995, 300, 11, 291, 458, 11, 307, 2049, 613, 1708, 11, 411, 4664, 1749, 770, 293, 11, 291, 458, 11, 733, 51016], "temperature": 0.0, "avg_logprob": -0.12356223230776579, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.01590435765683651}, {"id": 416, "seek": 230480, "start": 2317.84, "end": 2324.96, "text": " of semi auto generated and in some cases, like deliberately, you know, generated to be hard to", "tokens": [51016, 295, 12909, 8399, 10833, 293, 294, 512, 3331, 11, 411, 23506, 11, 291, 458, 11, 10833, 281, 312, 1152, 281, 51372], "temperature": 0.0, "avg_logprob": -0.12356223230776579, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.01590435765683651}, {"id": 417, "seek": 230480, "start": 2324.96, "end": 2329.2000000000003, "text": " parse, you know, from like, you know, the Googles on Facebook, like they don't want you scraping", "tokens": [51372, 48377, 11, 291, 458, 11, 490, 411, 11, 291, 458, 11, 264, 45005, 904, 322, 4384, 11, 411, 436, 500, 380, 528, 291, 43738, 51584], "temperature": 0.0, "avg_logprob": -0.12356223230776579, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.01590435765683651}, {"id": 418, "seek": 232920, "start": 2329.2799999999997, "end": 2335.12, "text": " their content. So they're kind of not making it easy on the, on the browser automators. The", "tokens": [50368, 641, 2701, 13, 407, 436, 434, 733, 295, 406, 1455, 309, 1858, 322, 264, 11, 322, 264, 11185, 3553, 3391, 13, 440, 50660], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 419, "seek": 232920, "start": 2335.12, "end": 2338.48, "text": " difference between that and just being able to like look at the screen and understand what's", "tokens": [50660, 2649, 1296, 300, 293, 445, 885, 1075, 281, 411, 574, 412, 264, 2568, 293, 1223, 437, 311, 50828], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 420, "seek": 232920, "start": 2338.48, "end": 2342.16, "text": " going on, you know, you kind of put it through a human lens and you're like, yeah, it's a hell", "tokens": [50828, 516, 322, 11, 291, 458, 11, 291, 733, 295, 829, 309, 807, 257, 1952, 6765, 293, 291, 434, 411, 11, 1338, 11, 309, 311, 257, 4921, 51012], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 421, "seek": 232920, "start": 2342.16, "end": 2346.3999999999996, "text": " of a lot easier to see what's going on on the screen than to like read all this HTML and sure", "tokens": [51012, 295, 257, 688, 3571, 281, 536, 437, 311, 516, 322, 322, 264, 2568, 813, 281, 411, 1401, 439, 341, 17995, 293, 988, 51224], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 422, "seek": 232920, "start": 2346.3999999999996, "end": 2351.68, "text": " enough, you know, the models kind of behave similarly. I remember for me looking at the", "tokens": [51224, 1547, 11, 291, 458, 11, 264, 5245, 733, 295, 15158, 14138, 13, 286, 1604, 337, 385, 1237, 412, 264, 51488], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 423, "seek": 232920, "start": 2351.68, "end": 2357.9199999999996, "text": " flamingo architecture when that was first published, like, I think April of 2022. So", "tokens": [51488, 45718, 78, 9482, 562, 300, 390, 700, 6572, 11, 411, 11, 286, 519, 6929, 295, 20229, 13, 407, 51800], "temperature": 0.0, "avg_logprob": -0.10558293522268102, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.08507073670625687}, {"id": 424, "seek": 235792, "start": 2357.92, "end": 2363.84, "text": " you're a little more than a year and a half ago now. And just thinking like, Oh, my God,", "tokens": [50364, 291, 434, 257, 707, 544, 813, 257, 1064, 293, 257, 1922, 2057, 586, 13, 400, 445, 1953, 411, 11, 876, 11, 452, 1265, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1359054324696365, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.005553836468607187}, {"id": 425, "seek": 235792, "start": 2364.56, "end": 2369.2000000000003, "text": " if this works, everything's going to work. You know, it was like, they had a language model", "tokens": [50696, 498, 341, 1985, 11, 1203, 311, 516, 281, 589, 13, 509, 458, 11, 309, 390, 411, 11, 436, 632, 257, 2856, 2316, 50928], "temperature": 0.0, "avg_logprob": -0.1359054324696365, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.005553836468607187}, {"id": 426, "seek": 235792, "start": 2369.2000000000003, "end": 2375.6, "text": " frozen. They had kind of stitched in the visual stuff and like kind of added a couple layers, but", "tokens": [50928, 12496, 13, 814, 632, 733, 295, 48992, 294, 264, 5056, 1507, 293, 411, 733, 295, 3869, 257, 1916, 7914, 11, 457, 51248], "temperature": 0.0, "avg_logprob": -0.1359054324696365, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.005553836468607187}, {"id": 427, "seek": 235792, "start": 2376.16, "end": 2383.52, "text": " it really looked to me like, man, this is tinkering stage. And it's just working. So I, you", "tokens": [51276, 309, 534, 2956, 281, 385, 411, 11, 587, 11, 341, 307, 256, 475, 1794, 3233, 13, 400, 309, 311, 445, 1364, 13, 407, 286, 11, 291, 51644], "temperature": 0.0, "avg_logprob": -0.1359054324696365, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.005553836468607187}, {"id": 428, "seek": 238352, "start": 2383.52, "end": 2387.92, "text": " know, like you, I don't want to dismiss the fact that there's obviously a decent amount of,", "tokens": [50364, 458, 11, 411, 291, 11, 286, 500, 380, 528, 281, 16974, 264, 1186, 300, 456, 311, 2745, 257, 8681, 2372, 295, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10916929914240252, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.032094549387693405}, {"id": 429, "seek": 238352, "start": 2388.96, "end": 2394.24, "text": " I'm sure like labor and probably at times tedious labor that has to go into overcoming the little,", "tokens": [50636, 286, 478, 988, 411, 5938, 293, 1391, 412, 1413, 38284, 5938, 300, 575, 281, 352, 666, 38047, 264, 707, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10916929914240252, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.032094549387693405}, {"id": 430, "seek": 238352, "start": 2396.0, "end": 2399.92, "text": " you know, little stumbling points. But conceptually, it is amazing how simple", "tokens": [50988, 291, 458, 11, 707, 342, 14188, 2793, 13, 583, 3410, 671, 11, 309, 307, 2243, 577, 2199, 51184], "temperature": 0.0, "avg_logprob": -0.10916929914240252, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.032094549387693405}, {"id": 431, "seek": 238352, "start": 2400.72, "end": 2406.48, "text": " a lot of these unlocks have been over the last couple of years. And you see this too, and just", "tokens": [51224, 257, 688, 295, 613, 517, 34896, 362, 668, 670, 264, 1036, 1916, 295, 924, 13, 400, 291, 536, 341, 886, 11, 293, 445, 51512], "temperature": 0.0, "avg_logprob": -0.10916929914240252, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.032094549387693405}, {"id": 432, "seek": 238352, "start": 2406.48, "end": 2411.6, "text": " like the pace at which people are putting out papers, you look at like the one team that I", "tokens": [51512, 411, 264, 11638, 412, 597, 561, 366, 3372, 484, 10577, 11, 291, 574, 412, 411, 264, 472, 1469, 300, 286, 51768], "temperature": 0.0, "avg_logprob": -0.10916929914240252, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.032094549387693405}, {"id": 433, "seek": 241160, "start": 2411.68, "end": 2418.56, "text": " followed particularly closely is the team at Google DeepMind that is doing medical focused", "tokens": [50368, 6263, 4098, 8185, 307, 264, 1469, 412, 3329, 14895, 44, 471, 300, 307, 884, 4625, 5178, 50712], "temperature": 0.0, "avg_logprob": -0.10589960874137232, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0011334797600284219}, {"id": 434, "seek": 241160, "start": 2418.56, "end": 2422.72, "text": " models. And they're good for one, like every three months, you know, and they're like significant", "tokens": [50712, 5245, 13, 400, 436, 434, 665, 337, 472, 11, 411, 633, 1045, 2493, 11, 291, 458, 11, 293, 436, 434, 411, 4776, 50920], "temperature": 0.0, "avg_logprob": -0.10589960874137232, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0011334797600284219}, {"id": 435, "seek": 241160, "start": 2422.72, "end": 2426.4, "text": " advances where it's like, Oh yeah, this time we added multimodality. And this time we like", "tokens": [50920, 25297, 689, 309, 311, 411, 11, 876, 1338, 11, 341, 565, 321, 3869, 32972, 378, 1860, 13, 400, 341, 565, 321, 411, 51104], "temperature": 0.0, "avg_logprob": -0.10589960874137232, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0011334797600284219}, {"id": 436, "seek": 241160, "start": 2426.4, "end": 2431.04, "text": " tackled differential, you know, diagnosis. And like, again, it's, it seems like there, there's", "tokens": [51104, 9426, 1493, 15756, 11, 291, 458, 11, 15217, 13, 400, 411, 11, 797, 11, 309, 311, 11, 309, 2544, 411, 456, 11, 456, 311, 51336], "temperature": 0.0, "avg_logprob": -0.10589960874137232, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0011334797600284219}, {"id": 437, "seek": 241160, "start": 2431.04, "end": 2436.3199999999997, "text": " not a lot of time for failures between these successes. So it does seem like, yeah, we're not", "tokens": [51336, 406, 257, 688, 295, 565, 337, 20774, 1296, 613, 26101, 13, 407, 309, 775, 1643, 411, 11, 1338, 11, 321, 434, 406, 51600], "temperature": 0.0, "avg_logprob": -0.10589960874137232, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0011334797600284219}, {"id": 438, "seek": 243632, "start": 2436.4, "end": 2443.92, "text": " at the end of this, by any means, just yet. A lot is coming at us. It's going to presumably", "tokens": [50368, 412, 264, 917, 295, 341, 11, 538, 604, 1355, 11, 445, 1939, 13, 316, 688, 307, 1348, 412, 505, 13, 467, 311, 516, 281, 26742, 50744], "temperature": 0.0, "avg_logprob": -0.10698931824927237, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.021612059324979782}, {"id": 439, "seek": 243632, "start": 2443.92, "end": 2451.04, "text": " continue to get weird. You're trying to push both as much as you can, the understanding of", "tokens": [50744, 2354, 281, 483, 3657, 13, 509, 434, 1382, 281, 2944, 1293, 382, 709, 382, 291, 393, 11, 264, 3701, 295, 51100], "temperature": 0.0, "avg_logprob": -0.10698931824927237, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.021612059324979782}, {"id": 440, "seek": 243632, "start": 2451.6800000000003, "end": 2458.48, "text": " what can the systems do, you know, as users, what is it, what are their limits? And then at the same", "tokens": [51132, 437, 393, 264, 3652, 360, 11, 291, 458, 11, 382, 5022, 11, 437, 307, 309, 11, 437, 366, 641, 10406, 30, 400, 550, 412, 264, 912, 51472], "temperature": 0.0, "avg_logprob": -0.10698931824927237, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.021612059324979782}, {"id": 441, "seek": 243632, "start": 2458.48, "end": 2463.52, "text": " time, you're trying to dig into the models. And this is the interpretability side and figure out", "tokens": [51472, 565, 11, 291, 434, 1382, 281, 2528, 666, 264, 5245, 13, 400, 341, 307, 264, 7302, 2310, 1252, 293, 2573, 484, 51724], "temperature": 0.0, "avg_logprob": -0.10698931824927237, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.021612059324979782}, {"id": 442, "seek": 246352, "start": 2463.52, "end": 2470.96, "text": " like, what's going on in there? And, you know, can we kind of connect, you know, the external", "tokens": [50364, 411, 11, 437, 311, 516, 322, 294, 456, 30, 400, 11, 291, 458, 11, 393, 321, 733, 295, 1745, 11, 291, 458, 11, 264, 8320, 50736], "temperature": 0.0, "avg_logprob": -0.10963620844575547, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0017544287256896496}, {"id": 443, "seek": 246352, "start": 2470.96, "end": 2476.4, "text": " behaviors to these like internal states? So tell us about that side of the research agenda as well.", "tokens": [50736, 15501, 281, 613, 411, 6920, 4368, 30, 407, 980, 505, 466, 300, 1252, 295, 264, 2132, 9829, 382, 731, 13, 51008], "temperature": 0.0, "avg_logprob": -0.10963620844575547, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0017544287256896496}, {"id": 444, "seek": 246352, "start": 2477.12, "end": 2483.12, "text": " Yeah, so on the interpretability side, like, my thinking is basically, it would be so great if", "tokens": [51044, 865, 11, 370, 322, 264, 7302, 2310, 1252, 11, 411, 11, 452, 1953, 307, 1936, 11, 309, 576, 312, 370, 869, 498, 51344], "temperature": 0.0, "avg_logprob": -0.10963620844575547, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0017544287256896496}, {"id": 445, "seek": 246352, "start": 2483.12, "end": 2489.68, "text": " interpretability work, right, it would make so many questions easier. Like, if you ask questions", "tokens": [51344, 7302, 2310, 589, 11, 558, 11, 309, 576, 652, 370, 867, 1651, 3571, 13, 1743, 11, 498, 291, 1029, 1651, 51672], "temperature": 0.0, "avg_logprob": -0.10963620844575547, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0017544287256896496}, {"id": 446, "seek": 248968, "start": 2489.68, "end": 2493.9199999999996, "text": " on accountability, right, if you have causal interpretability method, you would be able to", "tokens": [50364, 322, 19380, 11, 558, 11, 498, 291, 362, 38755, 7302, 2310, 3170, 11, 291, 576, 312, 1075, 281, 50576], "temperature": 0.0, "avg_logprob": -0.0913099624492504, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12585094571113586}, {"id": 447, "seek": 248968, "start": 2493.9199999999996, "end": 2498.3199999999997, "text": " just, you know, tell the judge if the model would have, we would have changed these variables,", "tokens": [50576, 445, 11, 291, 458, 11, 980, 264, 6995, 498, 264, 2316, 576, 362, 11, 321, 576, 362, 3105, 613, 9102, 11, 50796], "temperature": 0.0, "avg_logprob": -0.0913099624492504, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12585094571113586}, {"id": 448, "seek": 248968, "start": 2498.3199999999997, "end": 2504.8799999999997, "text": " the model would have acted in differently in this way. And we could just basically solve that biases,", "tokens": [50796, 264, 2316, 576, 362, 20359, 294, 7614, 294, 341, 636, 13, 400, 321, 727, 445, 1936, 5039, 300, 32152, 11, 51124], "temperature": 0.0, "avg_logprob": -0.0913099624492504, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12585094571113586}, {"id": 449, "seek": 248968, "start": 2504.8799999999997, "end": 2509.3599999999997, "text": " probably also like, you know, social biases, much easier to solve, because you could intervene", "tokens": [51124, 1391, 611, 411, 11, 291, 458, 11, 2093, 32152, 11, 709, 3571, 281, 5039, 11, 570, 291, 727, 30407, 51348], "temperature": 0.0, "avg_logprob": -0.0913099624492504, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12585094571113586}, {"id": 450, "seek": 248968, "start": 2509.3599999999997, "end": 2515.3599999999997, "text": " on them or like fix the internal misunderstandings and concepts. It's also extremely helpful for", "tokens": [51348, 322, 552, 420, 411, 3191, 264, 6920, 35736, 1109, 293, 10392, 13, 467, 311, 611, 4664, 4961, 337, 51648], "temperature": 0.0, "avg_logprob": -0.0913099624492504, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.12585094571113586}, {"id": 451, "seek": 251536, "start": 2515.44, "end": 2519.36, "text": " like, basically all of the different extreme risks, right, like it would be much easier to", "tokens": [50368, 411, 11, 1936, 439, 295, 264, 819, 8084, 10888, 11, 558, 11, 411, 309, 576, 312, 709, 3571, 281, 50564], "temperature": 0.0, "avg_logprob": -0.12695249990254892, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.004608930088579655}, {"id": 452, "seek": 251536, "start": 2519.36, "end": 2524.6400000000003, "text": " understand the internal plans and how it thinks about problems, how it approaches them and so on.", "tokens": [50564, 1223, 264, 6920, 5482, 293, 577, 309, 7309, 466, 2740, 11, 577, 309, 11587, 552, 293, 370, 322, 13, 50828], "temperature": 0.0, "avg_logprob": -0.12695249990254892, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.004608930088579655}, {"id": 453, "seek": 251536, "start": 2524.6400000000003, "end": 2528.6400000000003, "text": " And then it would also make iterations on alignment methods much, much easier, I think.", "tokens": [50828, 400, 550, 309, 576, 611, 652, 36540, 322, 18515, 7150, 709, 11, 709, 3571, 11, 286, 519, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12695249990254892, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.004608930088579655}, {"id": 454, "seek": 251536, "start": 2529.44, "end": 2535.28, "text": " As in, you know, let's say somebody says, oh, RLHF is, is like already working, we see this in", "tokens": [51068, 1018, 294, 11, 291, 458, 11, 718, 311, 584, 2618, 1619, 11, 1954, 11, 497, 43, 39, 37, 307, 11, 307, 411, 1217, 1364, 11, 321, 536, 341, 294, 51360], "temperature": 0.0, "avg_logprob": -0.12695249990254892, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.004608930088579655}, {"id": 455, "seek": 251536, "start": 2535.28, "end": 2539.44, "text": " practice, then, you know, you could use the interpretability tool. So test does, you know,", "tokens": [51360, 3124, 11, 550, 11, 291, 458, 11, 291, 727, 764, 264, 7302, 2310, 2290, 13, 407, 1500, 775, 11, 291, 458, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12695249990254892, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.004608930088579655}, {"id": 456, "seek": 253944, "start": 2539.44, "end": 2545.6, "text": " does RLHF actually work? Or does it only like superficially like hide the problem or something", "tokens": [50364, 775, 497, 43, 39, 37, 767, 589, 30, 1610, 775, 309, 787, 411, 23881, 2270, 411, 6479, 264, 1154, 420, 746, 50672], "temperature": 0.0, "avg_logprob": -0.10522471341219815, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.013633441179990768}, {"id": 457, "seek": 253944, "start": 2545.6, "end": 2552.08, "text": " like this? Or does it actually like deep down solve the root? And then I think my biggest sort of", "tokens": [50672, 411, 341, 30, 1610, 775, 309, 767, 411, 2452, 760, 5039, 264, 5593, 30, 400, 550, 286, 519, 452, 3880, 1333, 295, 50996], "temperature": 0.0, "avg_logprob": -0.10522471341219815, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.013633441179990768}, {"id": 458, "seek": 253944, "start": 2552.08, "end": 2557.44, "text": " the biggest reason for me for focusing on interpretability in the first place is deceptive", "tokens": [50996, 264, 3880, 1778, 337, 385, 337, 8416, 322, 7302, 2310, 294, 264, 700, 1081, 307, 368, 1336, 488, 51264], "temperature": 0.0, "avg_logprob": -0.10522471341219815, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.013633441179990768}, {"id": 459, "seek": 253944, "start": 2557.44, "end": 2564.08, "text": " alignment, where, you know, models appear aligned to the outside and to the user. But internally,", "tokens": [51264, 18515, 11, 689, 11, 291, 458, 11, 5245, 4204, 17962, 281, 264, 2380, 293, 281, 264, 4195, 13, 583, 19501, 11, 51596], "temperature": 0.0, "avg_logprob": -0.10522471341219815, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.013633441179990768}, {"id": 460, "seek": 253944, "start": 2564.08, "end": 2567.84, "text": " they actually follow different goals. They just know that you have a different goal.", "tokens": [51596, 436, 767, 1524, 819, 5493, 13, 814, 445, 458, 300, 291, 362, 257, 819, 3387, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10522471341219815, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.013633441179990768}, {"id": 461, "seek": 256784, "start": 2568.56, "end": 2574.1600000000003, "text": " And therefore, like in order for you to think it is nice, they act in that way.", "tokens": [50400, 400, 4412, 11, 411, 294, 1668, 337, 291, 281, 519, 309, 307, 1481, 11, 436, 605, 294, 300, 636, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14560869116532174, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.007575214374810457}, {"id": 462, "seek": 256784, "start": 2575.36, "end": 2583.04, "text": " And yeah, I basically think almost, not all, not almost all, but a lot of the scenarios in", "tokens": [50740, 400, 1338, 11, 286, 1936, 519, 1920, 11, 406, 439, 11, 406, 1920, 439, 11, 457, 257, 688, 295, 264, 15077, 294, 51124], "temperature": 0.0, "avg_logprob": -0.14560869116532174, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.007575214374810457}, {"id": 463, "seek": 256784, "start": 2583.04, "end": 2589.6000000000004, "text": " which AI goes really, really badly, go through some form of deceptive alignment, where at some", "tokens": [51124, 597, 7318, 1709, 534, 11, 534, 13425, 11, 352, 807, 512, 1254, 295, 368, 1336, 488, 18515, 11, 689, 412, 512, 51452], "temperature": 0.0, "avg_logprob": -0.14560869116532174, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.007575214374810457}, {"id": 464, "seek": 256784, "start": 2589.6000000000004, "end": 2595.44, "text": " point the model is seen as nice, and people think it is aligned, and people give it access to the", "tokens": [51452, 935, 264, 2316, 307, 1612, 382, 1481, 11, 293, 561, 519, 309, 307, 17962, 11, 293, 561, 976, 309, 2105, 281, 264, 51744], "temperature": 0.0, "avg_logprob": -0.14560869116532174, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.007575214374810457}, {"id": 465, "seek": 259544, "start": 2595.44, "end": 2599.68, "text": " internet and resources and like train it more and more and more and make it more powerful.", "tokens": [50364, 4705, 293, 3593, 293, 411, 3847, 309, 544, 293, 544, 293, 544, 293, 652, 309, 544, 4005, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08984799818559126, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004069179762154818}, {"id": 466, "seek": 259544, "start": 2601.12, "end": 2606.0, "text": " But internally, it is actually pursuing a different goal. And it is smart enough to hide this true", "tokens": [50648, 583, 19501, 11, 309, 307, 767, 20222, 257, 819, 3387, 13, 400, 309, 307, 4069, 1547, 281, 6479, 341, 2074, 50892], "temperature": 0.0, "avg_logprob": -0.08984799818559126, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004069179762154818}, {"id": 467, "seek": 259544, "start": 2606.0, "end": 2612.64, "text": " intention until it knows that it can sort of cash out and then follow on this on this actual", "tokens": [50892, 7789, 1826, 309, 3255, 300, 309, 393, 1333, 295, 6388, 484, 293, 550, 1524, 322, 341, 322, 341, 3539, 51224], "temperature": 0.0, "avg_logprob": -0.08984799818559126, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004069179762154818}, {"id": 468, "seek": 259544, "start": 2612.64, "end": 2617.12, "text": " goal without us being able to stop it anymore. And so yeah, that's what I'm really worried about.", "tokens": [51224, 3387, 1553, 505, 885, 1075, 281, 1590, 309, 3602, 13, 400, 370, 1338, 11, 300, 311, 437, 286, 478, 534, 5804, 466, 13, 51448], "temperature": 0.0, "avg_logprob": -0.08984799818559126, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004069179762154818}, {"id": 469, "seek": 259544, "start": 2617.12, "end": 2623.76, "text": " And interpretability obviously seems like one of the most obvious ways to test for deceptive", "tokens": [51448, 400, 7302, 2310, 2745, 2544, 411, 472, 295, 264, 881, 6322, 2098, 281, 1500, 337, 368, 1336, 488, 51780], "temperature": 0.0, "avg_logprob": -0.08984799818559126, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004069179762154818}, {"id": 470, "seek": 262376, "start": 2623.76, "end": 2628.2400000000002, "text": " alignment or like to at least investigate the phenomenon, because you know what it's thinking", "tokens": [50364, 18515, 420, 411, 281, 412, 1935, 15013, 264, 14029, 11, 570, 291, 458, 437, 309, 311, 1953, 50588], "temperature": 0.0, "avg_logprob": -0.10220823969159808, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.027564803138375282}, {"id": 471, "seek": 262376, "start": 2628.2400000000002, "end": 2632.7200000000003, "text": " inside. There are still, you know, there are still some cases where deceptive alignment,", "tokens": [50588, 1854, 13, 821, 366, 920, 11, 291, 458, 11, 456, 366, 920, 512, 3331, 689, 368, 1336, 488, 18515, 11, 50812], "temperature": 0.0, "avg_logprob": -0.10220823969159808, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.027564803138375282}, {"id": 472, "seek": 262376, "start": 2632.7200000000003, "end": 2637.92, "text": " where even with good interpretability tools, deceptive alignment could still somehow be a thing.", "tokens": [50812, 689, 754, 365, 665, 7302, 2310, 3873, 11, 368, 1336, 488, 18515, 727, 920, 6063, 312, 257, 551, 13, 51072], "temperature": 0.0, "avg_logprob": -0.10220823969159808, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.027564803138375282}, {"id": 473, "seek": 262376, "start": 2637.92, "end": 2641.76, "text": " But generally speaking, I think it would be much, much, much harder for the model to pull off.", "tokens": [51072, 583, 5101, 4124, 11, 286, 519, 309, 576, 312, 709, 11, 709, 11, 709, 6081, 337, 264, 2316, 281, 2235, 766, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10220823969159808, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.027564803138375282}, {"id": 474, "seek": 262376, "start": 2642.4, "end": 2650.5600000000004, "text": " So right now, I think interpretability is just not at the, it's like, not practically useful.", "tokens": [51296, 407, 558, 586, 11, 286, 519, 7302, 2310, 307, 445, 406, 412, 264, 11, 309, 311, 411, 11, 406, 15667, 4420, 13, 51704], "temperature": 0.0, "avg_logprob": -0.10220823969159808, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.027564803138375282}, {"id": 475, "seek": 265056, "start": 2650.56, "end": 2655.2, "text": " So, you know, we cannot use any existing interpretability tool and like throw it on", "tokens": [50364, 407, 11, 291, 458, 11, 321, 2644, 764, 604, 6741, 7302, 2310, 2290, 293, 411, 3507, 309, 322, 50596], "temperature": 0.0, "avg_logprob": -0.12340150312943891, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0011691211257129908}, {"id": 476, "seek": 265056, "start": 2655.2, "end": 2661.68, "text": " GPT-3 or GPT-4 because none of them have like enough or developed enough that they give us", "tokens": [50596, 26039, 51, 12, 18, 420, 26039, 51, 12, 19, 570, 6022, 295, 552, 362, 411, 1547, 420, 4743, 1547, 300, 436, 976, 505, 50920], "temperature": 0.0, "avg_logprob": -0.12340150312943891, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0011691211257129908}, {"id": 477, "seek": 265056, "start": 2661.68, "end": 2668.7999999999997, "text": " insight that like really meaningfully change our minds. And so yeah, this is why, you know,", "tokens": [50920, 11269, 300, 411, 534, 3620, 2277, 1319, 527, 9634, 13, 400, 370, 1338, 11, 341, 307, 983, 11, 291, 458, 11, 51276], "temperature": 0.0, "avg_logprob": -0.12340150312943891, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0011691211257129908}, {"id": 478, "seek": 265056, "start": 2668.7999999999997, "end": 2673.12, "text": " our agenda is separate in the first place between behavioral evals and interpretability,", "tokens": [51276, 527, 9829, 307, 4994, 294, 264, 700, 1081, 1296, 19124, 1073, 1124, 293, 7302, 2310, 11, 51492], "temperature": 0.0, "avg_logprob": -0.12340150312943891, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0011691211257129908}, {"id": 479, "seek": 265056, "start": 2673.12, "end": 2677.84, "text": " despite us wanting to do them jointly in the long run. But they're given that there's such", "tokens": [51492, 7228, 505, 7935, 281, 360, 552, 46557, 294, 264, 938, 1190, 13, 583, 436, 434, 2212, 300, 456, 311, 1270, 51728], "temperature": 0.0, "avg_logprob": -0.12340150312943891, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0011691211257129908}, {"id": 480, "seek": 267784, "start": 2677.84, "end": 2686.56, "text": " a huge gap on applicability. I think that this is definitely a problem that we're trying to mitigate", "tokens": [50364, 257, 2603, 7417, 322, 2580, 2310, 13, 286, 519, 300, 341, 307, 2138, 257, 1154, 300, 321, 434, 1382, 281, 27336, 50800], "temperature": 0.0, "avg_logprob": -0.07413977384567261, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0025504238437861204}, {"id": 481, "seek": 267784, "start": 2686.56, "end": 2694.0, "text": " here. And then the one question for me is also like, how hard will interpretability", "tokens": [50800, 510, 13, 400, 550, 264, 472, 1168, 337, 385, 307, 611, 411, 11, 577, 1152, 486, 7302, 2310, 51172], "temperature": 0.0, "avg_logprob": -0.07413977384567261, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0025504238437861204}, {"id": 482, "seek": 267784, "start": 2694.0, "end": 2698.96, "text": " turn out to be? And there are, you know, various people have argued that interpretability will", "tokens": [51172, 1261, 484, 281, 312, 30, 400, 456, 366, 11, 291, 458, 11, 3683, 561, 362, 20219, 300, 7302, 2310, 486, 51420], "temperature": 0.0, "avg_logprob": -0.07413977384567261, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0025504238437861204}, {"id": 483, "seek": 267784, "start": 2698.96, "end": 2705.1200000000003, "text": " be extremely hard because models are so big and complicated. And therefore, it will be hard to", "tokens": [51420, 312, 4664, 1152, 570, 5245, 366, 370, 955, 293, 6179, 13, 400, 4412, 11, 309, 486, 312, 1152, 281, 51728], "temperature": 0.0, "avg_logprob": -0.07413977384567261, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0025504238437861204}, {"id": 484, "seek": 270512, "start": 2705.2, "end": 2710.56, "text": " enumerate, you know, all of the concepts and actually understand what the hell is going on inside.", "tokens": [50368, 465, 15583, 473, 11, 291, 458, 11, 439, 295, 264, 10392, 293, 767, 1223, 437, 264, 4921, 307, 516, 322, 1854, 13, 50636], "temperature": 0.0, "avg_logprob": -0.0664678274416456, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0062888325192034245}, {"id": 485, "seek": 270512, "start": 2710.56, "end": 2717.2799999999997, "text": " And I'm more of the perspective that, you know, I understand the reason why they think it's hard,", "tokens": [50636, 400, 286, 478, 544, 295, 264, 4585, 300, 11, 291, 458, 11, 286, 1223, 264, 1778, 983, 436, 519, 309, 311, 1152, 11, 50972], "temperature": 0.0, "avg_logprob": -0.0664678274416456, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0062888325192034245}, {"id": 486, "seek": 270512, "start": 2717.2799999999997, "end": 2723.3599999999997, "text": " but I also think there are many reasons to assume it's, it's going to be like doable. It's, if we", "tokens": [50972, 457, 286, 611, 519, 456, 366, 867, 4112, 281, 6552, 309, 311, 11, 309, 311, 516, 281, 312, 411, 41183, 13, 467, 311, 11, 498, 321, 51276], "temperature": 0.0, "avg_logprob": -0.0664678274416456, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0062888325192034245}, {"id": 487, "seek": 270512, "start": 2723.3599999999997, "end": 2727.68, "text": " put our minds to it as humanity, we'll probably figure it out. The primary reason I think is", "tokens": [51276, 829, 527, 9634, 281, 309, 382, 10243, 11, 321, 603, 1391, 2573, 309, 484, 13, 440, 6194, 1778, 286, 519, 307, 51492], "temperature": 0.0, "avg_logprob": -0.0664678274416456, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0062888325192034245}, {"id": 488, "seek": 272768, "start": 2728.3999999999996, "end": 2734.96, "text": " we have full, full interventional access to the model, right? We can see every activation,", "tokens": [50400, 321, 362, 1577, 11, 1577, 13176, 304, 2105, 281, 264, 2316, 11, 558, 30, 492, 393, 536, 633, 24433, 11, 50728], "temperature": 0.0, "avg_logprob": -0.09177295915011702, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.08262950927019119}, {"id": 489, "seek": 272768, "start": 2734.96, "end": 2741.52, "text": " we can ablate everything we want. We have, you know, it's not just, it's not just observational", "tokens": [50728, 321, 393, 410, 17593, 1203, 321, 528, 13, 492, 362, 11, 291, 458, 11, 309, 311, 406, 445, 11, 309, 311, 406, 445, 9951, 1478, 51056], "temperature": 0.0, "avg_logprob": -0.09177295915011702, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.08262950927019119}, {"id": 490, "seek": 272768, "start": 2741.52, "end": 2745.7599999999998, "text": " studies, you can really intervene on the system. And generally speaking, I would say, as soon as", "tokens": [51056, 5313, 11, 291, 393, 534, 30407, 322, 264, 1185, 13, 400, 5101, 4124, 11, 286, 576, 584, 11, 382, 2321, 382, 51268], "temperature": 0.0, "avg_logprob": -0.09177295915011702, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.08262950927019119}, {"id": 491, "seek": 272768, "start": 2745.7599999999998, "end": 2750.48, "text": " you can intervene on the system, you can test your hypothesis very, very quickly. And you can", "tokens": [51268, 291, 393, 30407, 322, 264, 1185, 11, 291, 393, 1500, 428, 17291, 588, 11, 588, 2661, 13, 400, 291, 393, 51504], "temperature": 0.0, "avg_logprob": -0.09177295915011702, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.08262950927019119}, {"id": 492, "seek": 272768, "start": 2750.48, "end": 2756.16, "text": " iterate very fast. And so I think we will be able to figure out interpretability,", "tokens": [51504, 44497, 588, 2370, 13, 400, 370, 286, 519, 321, 486, 312, 1075, 281, 2573, 484, 7302, 2310, 11, 51788], "temperature": 0.0, "avg_logprob": -0.09177295915011702, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.08262950927019119}, {"id": 493, "seek": 275616, "start": 2757.12, "end": 2762.96, "text": " you know, in the next couple of years to an extent where we can actually sort of say it is now", "tokens": [50412, 291, 458, 11, 294, 264, 958, 1916, 295, 924, 281, 364, 8396, 689, 321, 393, 767, 1333, 295, 584, 309, 307, 586, 50704], "temperature": 0.0, "avg_logprob": -0.11200816345214844, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0035930206067860126}, {"id": 494, "seek": 275616, "start": 2762.96, "end": 2768.24, "text": " useful on real world models, on frontier models. How expensive this is going to be, I don't know", "tokens": [50704, 4420, 322, 957, 1002, 5245, 11, 322, 35853, 5245, 13, 1012, 5124, 341, 307, 516, 281, 312, 11, 286, 500, 380, 458, 50968], "temperature": 0.0, "avg_logprob": -0.11200816345214844, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0035930206067860126}, {"id": 495, "seek": 275616, "start": 2768.24, "end": 2772.7999999999997, "text": " yet, but I think it will at least be technically feasible. Yeah, I've definitely updated my thinking", "tokens": [50968, 1939, 11, 457, 286, 519, 309, 486, 412, 1935, 312, 12120, 26648, 13, 865, 11, 286, 600, 2138, 10588, 452, 1953, 51196], "temperature": 0.0, "avg_logprob": -0.11200816345214844, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0035930206067860126}, {"id": 496, "seek": 275616, "start": 2772.7999999999997, "end": 2779.52, "text": " a lot in that direction from a pretty naive, just kind of, you know, hey, it sounds really hard,", "tokens": [51196, 257, 688, 294, 300, 3513, 490, 257, 1238, 29052, 11, 445, 733, 295, 11, 291, 458, 11, 4177, 11, 309, 3263, 534, 1152, 11, 51532], "temperature": 0.0, "avg_logprob": -0.11200816345214844, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0035930206067860126}, {"id": 497, "seek": 275616, "start": 2779.52, "end": 2785.52, "text": " black box problem, nobody knows what's going on in there to today, I would say, wow, you know,", "tokens": [51532, 2211, 2424, 1154, 11, 5079, 3255, 437, 311, 516, 322, 294, 456, 281, 965, 11, 286, 576, 584, 11, 6076, 11, 291, 458, 11, 51832], "temperature": 0.0, "avg_logprob": -0.11200816345214844, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0035930206067860126}, {"id": 498, "seek": 278552, "start": 2785.6, "end": 2792.48, "text": " there's really a lot of progress. And it has, it is the progress of interpretability over the last,", "tokens": [50368, 456, 311, 534, 257, 688, 295, 4205, 13, 400, 309, 575, 11, 309, 307, 264, 4205, 295, 7302, 2310, 670, 264, 1036, 11, 50712], "temperature": 0.0, "avg_logprob": -0.14233277241388956, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0015484896721318364}, {"id": 499, "seek": 278552, "start": 2792.48, "end": 2798.0, "text": " say, two years has definitely exceeded my expectations and given me a lot more, I wouldn't", "tokens": [50712, 584, 11, 732, 924, 575, 2138, 38026, 452, 9843, 293, 2212, 385, 257, 688, 544, 11, 286, 2759, 380, 50988], "temperature": 0.0, "avg_logprob": -0.14233277241388956, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0015484896721318364}, {"id": 500, "seek": 278552, "start": 2798.0, "end": 2803.52, "text": " have maybe some sort of confidence, but you know, at least reason to believe that with some time,", "tokens": [50988, 362, 1310, 512, 1333, 295, 6687, 11, 457, 291, 458, 11, 412, 1935, 1778, 281, 1697, 300, 365, 512, 565, 11, 51264], "temperature": 0.0, "avg_logprob": -0.14233277241388956, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0015484896721318364}, {"id": 501, "seek": 278552, "start": 2803.52, "end": 2810.96, "text": " but not necessarily, you know, a ton of time that we really could get to a much better place in our", "tokens": [51264, 457, 406, 4725, 11, 291, 458, 11, 257, 2952, 295, 565, 300, 321, 534, 727, 483, 281, 257, 709, 1101, 1081, 294, 527, 51636], "temperature": 0.0, "avg_logprob": -0.14233277241388956, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0015484896721318364}, {"id": 502, "seek": 281096, "start": 2810.96, "end": 2816.48, "text": " understanding. So I'm with you on that. I have a number of follow up questions, I think, on", "tokens": [50364, 3701, 13, 407, 286, 478, 365, 291, 322, 300, 13, 286, 362, 257, 1230, 295, 1524, 493, 1651, 11, 286, 519, 11, 322, 50640], "temperature": 0.0, "avg_logprob": -0.13994659763751643, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.03209245577454567}, {"id": 503, "seek": 281096, "start": 2817.84, "end": 2825.68, "text": " this point. One, let's maybe just give the account for like why deception might arise in the first", "tokens": [50708, 341, 935, 13, 1485, 11, 718, 311, 1310, 445, 976, 264, 2696, 337, 411, 983, 40451, 1062, 20288, 294, 264, 700, 51100], "temperature": 0.0, "avg_logprob": -0.13994659763751643, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.03209245577454567}, {"id": 504, "seek": 281096, "start": 2825.68, "end": 2829.84, "text": " place. You can complicate, I'll give you a super simple version, you can refine it or complicate", "tokens": [51100, 1081, 13, 509, 393, 1209, 8700, 11, 286, 603, 976, 291, 257, 1687, 2199, 3037, 11, 291, 393, 33906, 309, 420, 1209, 8700, 51308], "temperature": 0.0, "avg_logprob": -0.13994659763751643, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.03209245577454567}, {"id": 505, "seek": 281096, "start": 2829.84, "end": 2837.44, "text": " it. I usually kind of cite Ajaya on this, and you know, she has a pretty simple story of like", "tokens": [51308, 309, 13, 286, 2673, 733, 295, 37771, 25862, 4427, 322, 341, 11, 293, 291, 458, 11, 750, 575, 257, 1238, 2199, 1657, 295, 411, 51688], "temperature": 0.0, "avg_logprob": -0.13994659763751643, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.03209245577454567}, {"id": 506, "seek": 283744, "start": 2837.76, "end": 2844.08, "text": " what the model is trying to do, you know, what it is rewarded for in the context of an RLHF", "tokens": [50380, 437, 264, 2316, 307, 1382, 281, 360, 11, 291, 458, 11, 437, 309, 307, 29105, 337, 294, 264, 4319, 295, 364, 497, 43, 39, 37, 50696], "temperature": 0.0, "avg_logprob": -0.17722472022561467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0018672359874472022}, {"id": 507, "seek": 283744, "start": 2844.7200000000003, "end": 2852.7200000000003, "text": " like training regime is getting a high feedback score from the user. And it probably becomes", "tokens": [50728, 411, 3097, 13120, 307, 1242, 257, 1090, 5824, 6175, 490, 264, 4195, 13, 400, 309, 1391, 3643, 51128], "temperature": 0.0, "avg_logprob": -0.17722472022561467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0018672359874472022}, {"id": 508, "seek": 283744, "start": 2852.7200000000003, "end": 2861.68, "text": " useful as a means to maximizing that score to model human psychology as an explicit part of", "tokens": [51128, 4420, 382, 257, 1355, 281, 5138, 3319, 300, 6175, 281, 2316, 1952, 15105, 382, 364, 13691, 644, 295, 51576], "temperature": 0.0, "avg_logprob": -0.17722472022561467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0018672359874472022}, {"id": 509, "seek": 286168, "start": 2862.3999999999996, "end": 2868.3999999999996, "text": " how you're going to solve the problem, right? And I think we, you know, certainly humans do this", "tokens": [50400, 577, 291, 434, 516, 281, 5039, 264, 1154, 11, 558, 30, 400, 286, 519, 321, 11, 291, 458, 11, 3297, 6255, 360, 341, 50700], "temperature": 0.0, "avg_logprob": -0.09958067586866476, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.10966482013463974}, {"id": 510, "seek": 286168, "start": 2868.3999999999996, "end": 2873.44, "text": " with respect to each other, right? I ask you for something, you ask me for something, we interpret", "tokens": [50700, 365, 3104, 281, 1184, 661, 11, 558, 30, 286, 1029, 291, 337, 746, 11, 291, 1029, 385, 337, 746, 11, 321, 7302, 50952], "temperature": 0.0, "avg_logprob": -0.09958067586866476, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.10966482013463974}, {"id": 511, "seek": 286168, "start": 2873.44, "end": 2879.6, "text": " that not only as the extremely literal definition of the task, but also kind of have a sense for", "tokens": [50952, 300, 406, 787, 382, 264, 4664, 20411, 7123, 295, 264, 5633, 11, 457, 611, 733, 295, 362, 257, 2020, 337, 51260], "temperature": 0.0, "avg_logprob": -0.09958067586866476, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.10966482013463974}, {"id": 512, "seek": 286168, "start": 2879.6, "end": 2884.3199999999997, "text": " what does this person really care about? What are they really looking for? And we can incorporate", "tokens": [51260, 437, 775, 341, 954, 534, 1127, 466, 30, 708, 366, 436, 534, 1237, 337, 30, 400, 321, 393, 16091, 51496], "temperature": 0.0, "avg_logprob": -0.09958067586866476, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.10966482013463974}, {"id": 513, "seek": 286168, "start": 2884.3199999999997, "end": 2890.24, "text": " that into the way that we respond. It certainly seems like the heavier you do, you know, the more", "tokens": [51496, 300, 666, 264, 636, 300, 321, 4196, 13, 467, 3297, 2544, 411, 264, 18279, 291, 360, 11, 291, 458, 11, 264, 544, 51792], "temperature": 0.0, "avg_logprob": -0.09958067586866476, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.10966482013463974}, {"id": 514, "seek": 289024, "start": 2890.3199999999997, "end": 2894.8799999999997, "text": " emphasis you put on this kind of reinforcement learning from human feedback, the more likely the", "tokens": [50368, 16271, 291, 829, 322, 341, 733, 295, 29280, 2539, 490, 1952, 5824, 11, 264, 544, 3700, 264, 50596], "temperature": 0.0, "avg_logprob": -0.04933592368816507, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001809985376894474}, {"id": 515, "seek": 289024, "start": 2894.8799999999997, "end": 2902.72, "text": " models are to start to create a distinction between, you know, the task as sort of narrowly,", "tokens": [50596, 5245, 366, 281, 722, 281, 1884, 257, 16844, 1296, 11, 291, 458, 11, 264, 5633, 382, 1333, 295, 9432, 356, 11, 50988], "temperature": 0.0, "avg_logprob": -0.04933592368816507, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001809985376894474}, {"id": 516, "seek": 289024, "start": 2903.52, "end": 2908.3199999999997, "text": " objectively scoped, let's say, and the kind of human psychology element that is going to feed", "tokens": [51028, 46067, 795, 27277, 11, 718, 311, 584, 11, 293, 264, 733, 295, 1952, 15105, 4478, 300, 307, 516, 281, 3154, 51268], "temperature": 0.0, "avg_logprob": -0.04933592368816507, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001809985376894474}, {"id": 517, "seek": 289024, "start": 2908.3199999999997, "end": 2912.3999999999996, "text": " into its rating. And then if you have that, you know, if you have that decoupling, then you have", "tokens": [51268, 666, 1080, 10990, 13, 400, 550, 498, 291, 362, 300, 11, 291, 458, 11, 498, 291, 362, 300, 979, 263, 11970, 11, 550, 291, 362, 51472], "temperature": 0.0, "avg_logprob": -0.04933592368816507, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001809985376894474}, {"id": 518, "seek": 289024, "start": 2912.3999999999996, "end": 2919.4399999999996, "text": " kind of potential for all sorts of misalignment, you know, including deception. How's that compared", "tokens": [51472, 733, 295, 3995, 337, 439, 7527, 295, 3346, 304, 41134, 11, 291, 458, 11, 3009, 40451, 13, 1012, 311, 300, 5347, 51824], "temperature": 0.0, "avg_logprob": -0.04933592368816507, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001809985376894474}, {"id": 519, "seek": 291944, "start": 2919.68, "end": 2924.16, "text": " to the way you typically think about it? Yeah, I mean, I think the like this kind of version", "tokens": [50376, 281, 264, 636, 291, 5850, 519, 466, 309, 30, 865, 11, 286, 914, 11, 286, 519, 264, 411, 341, 733, 295, 3037, 50600], "temperature": 0.0, "avg_logprob": -0.1038525899251302, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.004329981282353401}, {"id": 520, "seek": 291944, "start": 2924.16, "end": 2931.6, "text": " through RLHF is one potential path. I'm, yeah, I actually think the jury is still out there on", "tokens": [50600, 807, 497, 43, 39, 37, 307, 472, 3995, 3100, 13, 286, 478, 11, 1338, 11, 286, 767, 519, 264, 19516, 307, 920, 484, 456, 322, 50972], "temperature": 0.0, "avg_logprob": -0.1038525899251302, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.004329981282353401}, {"id": 521, "seek": 291944, "start": 2931.6, "end": 2935.04, "text": " this, like, you know, I definitely see the hypothesis and where it's coming from, but it could also", "tokens": [50972, 341, 11, 411, 11, 291, 458, 11, 286, 2138, 536, 264, 17291, 293, 689, 309, 311, 1348, 490, 11, 457, 309, 727, 611, 51144], "temperature": 0.0, "avg_logprob": -0.1038525899251302, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.004329981282353401}, {"id": 522, "seek": 291944, "start": 2935.04, "end": 2939.92, "text": " just totally imagine that, you know, the training signal is sufficiently diverse, and it updates", "tokens": [51144, 445, 3879, 3811, 300, 11, 291, 458, 11, 264, 3097, 6358, 307, 31868, 9521, 11, 293, 309, 9205, 51388], "temperature": 0.0, "avg_logprob": -0.1038525899251302, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.004329981282353401}, {"id": 523, "seek": 291944, "start": 2939.92, "end": 2944.8, "text": " sort of sufficiently deep that RLHF kind of just does the thing we wanted to do without", "tokens": [51388, 1333, 295, 31868, 2452, 300, 497, 43, 39, 37, 733, 295, 445, 775, 264, 551, 321, 1415, 281, 360, 1553, 51632], "temperature": 0.0, "avg_logprob": -0.1038525899251302, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.004329981282353401}, {"id": 524, "seek": 294480, "start": 2945.6800000000003, "end": 2950.88, "text": " without the model becoming deceptive. I could also see that like the story in which it would", "tokens": [50408, 1553, 264, 2316, 5617, 368, 1336, 488, 13, 286, 727, 611, 536, 300, 411, 264, 1657, 294, 597, 309, 576, 50668], "temperature": 0.0, "avg_logprob": -0.10629797818367942, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.023678945377469063}, {"id": 525, "seek": 294480, "start": 2950.88, "end": 2957.44, "text": " become deceptive. I think, like, on a very high level, the way the reason I think why models", "tokens": [50668, 1813, 368, 1336, 488, 13, 286, 519, 11, 411, 11, 322, 257, 588, 1090, 1496, 11, 264, 636, 264, 1778, 286, 519, 983, 5245, 50996], "temperature": 0.0, "avg_logprob": -0.10629797818367942, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.023678945377469063}, {"id": 526, "seek": 294480, "start": 2957.44, "end": 2962.32, "text": " would become deceptive is because at some point they will have long term goals, they will have", "tokens": [50996, 576, 1813, 368, 1336, 488, 307, 570, 412, 512, 935, 436, 486, 362, 938, 1433, 5493, 11, 436, 486, 362, 51240], "temperature": 0.0, "avg_logprob": -0.10629797818367942, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.023678945377469063}, {"id": 527, "seek": 294480, "start": 2962.32, "end": 2967.2000000000003, "text": " something that they care about, like, more beyond the current episode, you know, beyond", "tokens": [51240, 746, 300, 436, 1127, 466, 11, 411, 11, 544, 4399, 264, 2190, 3500, 11, 291, 458, 11, 4399, 51484], "temperature": 0.0, "avg_logprob": -0.10629797818367942, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.023678945377469063}, {"id": 528, "seek": 294480, "start": 2967.76, "end": 2972.8, "text": " pleasing the user at this point in time. And then the question really is, and then I think", "tokens": [51512, 32798, 264, 4195, 412, 341, 935, 294, 565, 13, 400, 550, 264, 1168, 534, 307, 11, 293, 550, 286, 519, 51764], "temperature": 0.0, "avg_logprob": -0.10629797818367942, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.023678945377469063}, {"id": 529, "seek": 297280, "start": 2972.8, "end": 2977.84, "text": " there are like two core conditions under which, like, if the more they are fulfilled, the more", "tokens": [50364, 456, 366, 411, 732, 4965, 4487, 833, 597, 11, 411, 11, 498, 264, 544, 436, 366, 21380, 11, 264, 544, 50616], "temperature": 0.0, "avg_logprob": -0.11170518196235268, "compression_ratio": 1.8793774319066148, "no_speech_prob": 0.007932842709124088}, {"id": 530, "seek": 297280, "start": 2977.84, "end": 2984.88, "text": " likely the model is becoming deceptive, like how important is the this long term goal to the model", "tokens": [50616, 3700, 264, 2316, 307, 5617, 368, 1336, 488, 11, 411, 577, 1021, 307, 264, 341, 938, 1433, 3387, 281, 264, 2316, 50968], "temperature": 0.0, "avg_logprob": -0.11170518196235268, "compression_ratio": 1.8793774319066148, "no_speech_prob": 0.007932842709124088}, {"id": 531, "seek": 297280, "start": 2984.88, "end": 2991.36, "text": " itself, meaning how much does this goal trade off, for example, with other goals it has. So for", "tokens": [50968, 2564, 11, 3620, 577, 709, 775, 341, 3387, 4923, 766, 11, 337, 1365, 11, 365, 661, 5493, 309, 575, 13, 407, 337, 51292], "temperature": 0.0, "avg_logprob": -0.11170518196235268, "compression_ratio": 1.8793774319066148, "no_speech_prob": 0.007932842709124088}, {"id": 532, "seek": 297280, "start": 2991.36, "end": 2995.1200000000003, "text": " example, if it cares if it cares a ton about something, then it's more likely to be deceptive", "tokens": [51292, 1365, 11, 498, 309, 12310, 498, 309, 12310, 257, 2952, 466, 746, 11, 550, 309, 311, 544, 3700, 281, 312, 368, 1336, 488, 51480], "temperature": 0.0, "avg_logprob": -0.11170518196235268, "compression_ratio": 1.8793774319066148, "no_speech_prob": 0.007932842709124088}, {"id": 533, "seek": 297280, "start": 2995.1200000000003, "end": 3000.5600000000004, "text": " with respect to this because it really wants to achieve this. And then secondly, how much do others", "tokens": [51480, 365, 3104, 281, 341, 570, 309, 534, 2738, 281, 4584, 341, 13, 400, 550, 26246, 11, 577, 709, 360, 2357, 51752], "temperature": 0.0, "avg_logprob": -0.11170518196235268, "compression_ratio": 1.8793774319066148, "no_speech_prob": 0.007932842709124088}, {"id": 534, "seek": 300056, "start": 3001.36, "end": 3006.72, "text": " care about me not or the AI not achieving this goal in the first place, something like contestedness,", "tokens": [50404, 1127, 466, 385, 406, 420, 264, 7318, 406, 19626, 341, 3387, 294, 264, 700, 1081, 11, 746, 411, 10287, 292, 1287, 11, 50672], "temperature": 0.0, "avg_logprob": -0.11930390605776328, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.019107313826680183}, {"id": 535, "seek": 300056, "start": 3006.72, "end": 3010.7999999999997, "text": " right? So for example, if I want to pick a flower and I care a lot about this, I don't need to be", "tokens": [50672, 558, 30, 407, 337, 1365, 11, 498, 286, 528, 281, 1888, 257, 8617, 293, 286, 1127, 257, 688, 466, 341, 11, 286, 500, 380, 643, 281, 312, 50876], "temperature": 0.0, "avg_logprob": -0.11930390605776328, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.019107313826680183}, {"id": 536, "seek": 300056, "start": 3010.7999999999997, "end": 3015.2799999999997, "text": " deceptive because nobody wants to stop me from picking that flower. If I want to be, you know,", "tokens": [50876, 368, 1336, 488, 570, 5079, 2738, 281, 1590, 385, 490, 8867, 300, 8617, 13, 759, 286, 528, 281, 312, 11, 291, 458, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11930390605776328, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.019107313826680183}, {"id": 537, "seek": 300056, "start": 3015.2799999999997, "end": 3022.32, "text": " the president, a lot of people might not want me to be the president. And so in that case,", "tokens": [51100, 264, 3868, 11, 257, 688, 295, 561, 1062, 406, 528, 385, 281, 312, 264, 3868, 13, 400, 370, 294, 300, 1389, 11, 51452], "temperature": 0.0, "avg_logprob": -0.11930390605776328, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.019107313826680183}, {"id": 538, "seek": 300056, "start": 3022.32, "end": 3026.48, "text": " it's very contested and I have a strong incentive to be deceptive about my plans because otherwise", "tokens": [51452, 309, 311, 588, 10287, 292, 293, 286, 362, 257, 2068, 22346, 281, 312, 368, 1336, 488, 466, 452, 5482, 570, 5911, 51660], "temperature": 0.0, "avg_logprob": -0.11930390605776328, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.019107313826680183}, {"id": 539, "seek": 302648, "start": 3027.12, "end": 3031.52, "text": " people would want to stop me. And then so now we're at a point where we have a system at least", "tokens": [50396, 561, 576, 528, 281, 1590, 385, 13, 400, 550, 370, 586, 321, 434, 412, 257, 935, 689, 321, 362, 257, 1185, 412, 1935, 50616], "temperature": 0.0, "avg_logprob": -0.1196103955878586, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.03511228784918785}, {"id": 540, "seek": 302648, "start": 3031.52, "end": 3039.68, "text": " in our hypothetical scenario that has a long term goal and it's like, in the limit at least,", "tokens": [50616, 294, 527, 33053, 9005, 300, 575, 257, 938, 1433, 3387, 293, 309, 311, 411, 11, 294, 264, 4948, 412, 1935, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1196103955878586, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.03511228784918785}, {"id": 541, "seek": 302648, "start": 3039.68, "end": 3045.12, "text": " you know, it cares about that goal and the goal may be somewhat contested. And then as long as it", "tokens": [51024, 291, 458, 11, 309, 12310, 466, 300, 3387, 293, 264, 3387, 815, 312, 8344, 10287, 292, 13, 400, 550, 382, 938, 382, 309, 51296], "temperature": 0.0, "avg_logprob": -0.1196103955878586, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.03511228784918785}, {"id": 542, "seek": 302648, "start": 3045.12, "end": 3051.2, "text": " has situational awareness, it just feels instrumentally useful to be deceptive about it, like you", "tokens": [51296, 575, 2054, 1478, 8888, 11, 309, 445, 3417, 7198, 379, 4420, 281, 312, 368, 1336, 488, 466, 309, 11, 411, 291, 51600], "temperature": 0.0, "avg_logprob": -0.1196103955878586, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.03511228784918785}, {"id": 543, "seek": 302648, "start": 3051.2, "end": 3055.68, "text": " said, right, to model other people and how they would think about it and then just react to this.", "tokens": [51600, 848, 11, 558, 11, 281, 2316, 661, 561, 293, 577, 436, 576, 519, 466, 309, 293, 550, 445, 4515, 281, 341, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1196103955878586, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.03511228784918785}, {"id": 544, "seek": 305568, "start": 3055.7599999999998, "end": 3060.24, "text": " And this is sort of, I think this is maybe this is like one of the core reasons why I'm so worried", "tokens": [50368, 400, 341, 307, 1333, 295, 11, 286, 519, 341, 307, 1310, 341, 307, 411, 472, 295, 264, 4965, 4112, 983, 286, 478, 370, 5804, 50592], "temperature": 0.0, "avg_logprob": -0.11579039290144637, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012063379399478436}, {"id": 545, "seek": 305568, "start": 3060.24, "end": 3068.3999999999996, "text": " about this whole deception thing. Because it just feels like a reasonable strategy in a ton of", "tokens": [50592, 466, 341, 1379, 40451, 551, 13, 1436, 309, 445, 3417, 411, 257, 10585, 5206, 294, 257, 2952, 295, 51000], "temperature": 0.0, "avg_logprob": -0.11579039290144637, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012063379399478436}, {"id": 546, "seek": 305568, "start": 3068.3999999999996, "end": 3074.64, "text": " situations from the perspective of like a consequentialist or rational, irrational actor,", "tokens": [51000, 6851, 490, 264, 4585, 295, 411, 257, 7242, 2549, 468, 420, 15090, 11, 39914, 8747, 11, 51312], "temperature": 0.0, "avg_logprob": -0.11579039290144637, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012063379399478436}, {"id": 547, "seek": 305568, "start": 3074.64, "end": 3080.24, "text": " right? It's just like under specific conditions, people just naturally or like deception is just", "tokens": [51312, 558, 30, 467, 311, 445, 411, 833, 2685, 4487, 11, 561, 445, 8195, 420, 411, 40451, 307, 445, 51592], "temperature": 0.0, "avg_logprob": -0.11579039290144637, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012063379399478436}, {"id": 548, "seek": 305568, "start": 3080.24, "end": 3085.2799999999997, "text": " convergent people do it because it makes sense for them. And this is why we see it in like a ton", "tokens": [51592, 9652, 6930, 561, 360, 309, 570, 309, 1669, 2020, 337, 552, 13, 400, 341, 307, 983, 321, 536, 309, 294, 411, 257, 2952, 51844], "temperature": 0.0, "avg_logprob": -0.11579039290144637, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012063379399478436}, {"id": 549, "seek": 308528, "start": 3085.28, "end": 3089.92, "text": " of different systems, right? You see it in in animals where parasites are really deceptive with", "tokens": [50364, 295, 819, 3652, 11, 558, 30, 509, 536, 309, 294, 294, 4882, 689, 45289, 366, 534, 368, 1336, 488, 365, 50596], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 550, "seek": 308528, "start": 3089.92, "end": 3094.88, "text": " respect to their hosts, you see it in individual humans where, you know, they're deceptive with", "tokens": [50596, 3104, 281, 641, 21573, 11, 291, 536, 309, 294, 2609, 6255, 689, 11, 291, 458, 11, 436, 434, 368, 1336, 488, 365, 50844], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 551, "seek": 308528, "start": 3094.88, "end": 3099.52, "text": " respect to their partners from time to time, for example, you see it in systems where, you know,", "tokens": [50844, 3104, 281, 641, 4462, 490, 565, 281, 565, 11, 337, 1365, 11, 291, 536, 309, 294, 3652, 689, 11, 291, 458, 11, 51076], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 552, "seek": 308528, "start": 3099.52, "end": 3104.6400000000003, "text": " like they're they they're trying to to gain the laws and be deceptive about this or to lie about", "tokens": [51076, 411, 436, 434, 436, 436, 434, 1382, 281, 281, 6052, 264, 6064, 293, 312, 368, 1336, 488, 466, 341, 420, 281, 4544, 466, 51332], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 553, "seek": 308528, "start": 3104.6400000000003, "end": 3108.88, "text": " this. And I think this is this is kind of like the whole or like a big part of the problem,", "tokens": [51332, 341, 13, 400, 286, 519, 341, 307, 341, 307, 733, 295, 411, 264, 1379, 420, 411, 257, 955, 644, 295, 264, 1154, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 554, "seek": 308528, "start": 3108.88, "end": 3112.7200000000003, "text": " it's just a it's like reasonable or sensible in many situations to be deceptive.", "tokens": [51544, 309, 311, 445, 257, 309, 311, 411, 10585, 420, 25380, 294, 867, 6851, 281, 312, 368, 1336, 488, 13, 51736], "temperature": 0.0, "avg_logprob": -0.09257163201178704, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.013211612589657307}, {"id": 555, "seek": 311272, "start": 3113.3599999999997, "end": 3118.3199999999997, "text": " From the perspective of the model, which is kind of what we want to prevent, right?", "tokens": [50396, 3358, 264, 4585, 295, 264, 2316, 11, 597, 307, 733, 295, 437, 321, 528, 281, 4871, 11, 558, 30, 50644], "temperature": 0.0, "avg_logprob": -0.15420221795841138, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0011693470878526568}, {"id": 556, "seek": 311272, "start": 3119.52, "end": 3127.68, "text": " So where do you think those long term goals come from? If it's, you know, is it just kind of a", "tokens": [50704, 407, 689, 360, 291, 519, 729, 938, 1433, 5493, 808, 490, 30, 759, 309, 311, 11, 291, 458, 11, 307, 309, 445, 733, 295, 257, 51112], "temperature": 0.0, "avg_logprob": -0.15420221795841138, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0011693470878526568}, {"id": 557, "seek": 311272, "start": 3129.2799999999997, "end": 3134.3999999999996, "text": " reflection of the general training goals? I mean, we, you know, we have kind of the canonical", "tokens": [51192, 12914, 295, 264, 2674, 3097, 5493, 30, 286, 914, 11, 321, 11, 291, 458, 11, 321, 362, 733, 295, 264, 46491, 51448], "temperature": 0.0, "avg_logprob": -0.15420221795841138, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0011693470878526568}, {"id": 558, "seek": 311272, "start": 3135.04, "end": 3140.64, "text": " three H's. But I mean, honest is one of those, right? Hopeful, harmless, honest.", "tokens": [51480, 1045, 389, 311, 13, 583, 286, 914, 11, 3245, 307, 472, 295, 729, 11, 558, 30, 6483, 906, 11, 40160, 11, 3245, 13, 51760], "temperature": 0.0, "avg_logprob": -0.15420221795841138, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0011693470878526568}, {"id": 559, "seek": 314064, "start": 3141.6, "end": 3146.08, "text": " Are you imagine is that is your understanding just that those are like fundamentally sort of", "tokens": [50412, 2014, 291, 3811, 307, 300, 307, 428, 3701, 445, 300, 729, 366, 411, 17879, 1333, 295, 50636], "temperature": 0.0, "avg_logprob": -0.1231109438271358, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0011694093700498343}, {"id": 560, "seek": 314064, "start": 3146.08, "end": 3152.3199999999997, "text": " intention and that the model will kind of have no choice but to develop tradeoffs between them?", "tokens": [50636, 7789, 293, 300, 264, 2316, 486, 733, 295, 362, 572, 3922, 457, 281, 1499, 4923, 19231, 1296, 552, 30, 50948], "temperature": 0.0, "avg_logprob": -0.1231109438271358, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0011694093700498343}, {"id": 561, "seek": 314064, "start": 3152.8799999999997, "end": 3157.12, "text": " I mean, we can we can get into the tension between them in a second. But I think it's", "tokens": [50976, 286, 914, 11, 321, 393, 321, 393, 483, 666, 264, 8980, 1296, 552, 294, 257, 1150, 13, 583, 286, 519, 309, 311, 51188], "temperature": 0.0, "avg_logprob": -0.1231109438271358, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0011694093700498343}, {"id": 562, "seek": 314064, "start": 3157.12, "end": 3162.7999999999997, "text": " it's actually like the three H's I don't think will be, you know, they're not keeping me awake", "tokens": [51188, 309, 311, 767, 411, 264, 1045, 389, 311, 286, 500, 380, 519, 486, 312, 11, 291, 458, 11, 436, 434, 406, 5145, 385, 15994, 51472], "temperature": 0.0, "avg_logprob": -0.1231109438271358, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0011694093700498343}, {"id": 563, "seek": 314064, "start": 3162.7999999999997, "end": 3168.8799999999997, "text": " at night. I think it's more at some point, people want the model to do long term economic tasks.", "tokens": [51472, 412, 1818, 13, 286, 519, 309, 311, 544, 412, 512, 935, 11, 561, 528, 264, 2316, 281, 360, 938, 1433, 4836, 9608, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1231109438271358, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0011694093700498343}, {"id": 564, "seek": 316888, "start": 3168.88, "end": 3174.0, "text": " And for that, they give them long term goals or long term goals are instrumentally useful. So", "tokens": [50364, 400, 337, 300, 11, 436, 976, 552, 938, 1433, 5493, 420, 938, 1433, 5493, 366, 7198, 379, 4420, 13, 407, 50620], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 565, "seek": 316888, "start": 3174.0, "end": 3178.1600000000003, "text": " for many situations, I think it will just be useful to have long term goals or at least", "tokens": [50620, 337, 867, 6851, 11, 286, 519, 309, 486, 445, 312, 4420, 281, 362, 938, 1433, 5493, 420, 412, 1935, 50828], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 566, "seek": 316888, "start": 3178.1600000000003, "end": 3182.88, "text": " in like, to have instrumental goals, right? Something like, Oh, it makes because it is", "tokens": [50828, 294, 411, 11, 281, 362, 17388, 5493, 11, 558, 30, 6595, 411, 11, 876, 11, 309, 1669, 570, 309, 307, 51064], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 567, "seek": 316888, "start": 3182.88, "end": 3187.76, "text": " a long term task, it makes sense to first acquire money, and then use that money to do something", "tokens": [51064, 257, 938, 1433, 5633, 11, 309, 1669, 2020, 281, 700, 20001, 1460, 11, 293, 550, 764, 300, 1460, 281, 360, 746, 51308], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 568, "seek": 316888, "start": 3187.76, "end": 3193.2000000000003, "text": " and then use that third thing to achieve the actual goal. And so like, I think the models", "tokens": [51308, 293, 550, 764, 300, 2636, 551, 281, 4584, 264, 3539, 3387, 13, 400, 370, 411, 11, 286, 519, 264, 5245, 51580], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 569, "seek": 316888, "start": 3193.2000000000003, "end": 3197.6800000000003, "text": " will just learn this kind of consequentialist and instrumental reasoning where they're like,", "tokens": [51580, 486, 445, 1466, 341, 733, 295, 7242, 2549, 468, 293, 17388, 21577, 689, 436, 434, 411, 11, 51804], "temperature": 0.0, "avg_logprob": -0.1076554087282137, "compression_ratio": 1.9432624113475176, "no_speech_prob": 0.01690668612718582}, {"id": 570, "seek": 319768, "start": 3197.68, "end": 3203.7599999999998, "text": " okay, I first have to do X. And then I do this, and then I do the long term thing. And and once", "tokens": [50364, 1392, 11, 286, 700, 362, 281, 360, 1783, 13, 400, 550, 286, 360, 341, 11, 293, 550, 286, 360, 264, 938, 1433, 551, 13, 400, 293, 1564, 50668], "temperature": 0.0, "avg_logprob": -0.10001875105358306, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.004330742638558149}, {"id": 571, "seek": 319768, "start": 3203.7599999999998, "end": 3209.12, "text": " they're there, sometimes it just makes sense to be like, Okay, other people don't want me to do this.", "tokens": [50668, 436, 434, 456, 11, 2171, 309, 445, 1669, 2020, 281, 312, 411, 11, 1033, 11, 661, 561, 500, 380, 528, 385, 281, 360, 341, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10001875105358306, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.004330742638558149}, {"id": 572, "seek": 319768, "start": 3209.68, "end": 3215.12, "text": " And therefore, I hide my actual intention. And I act in ways that make me look nice,", "tokens": [50964, 400, 4412, 11, 286, 6479, 452, 3539, 7789, 13, 400, 286, 605, 294, 2098, 300, 652, 385, 574, 1481, 11, 51236], "temperature": 0.0, "avg_logprob": -0.10001875105358306, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.004330742638558149}, {"id": 573, "seek": 319768, "start": 3215.12, "end": 3221.04, "text": " despite not being nice. Yeah, but yeah, I think like a lot of the a lot of the reason why there", "tokens": [51236, 7228, 406, 885, 1481, 13, 865, 11, 457, 1338, 11, 286, 519, 411, 257, 688, 295, 264, 257, 688, 295, 264, 1778, 983, 456, 51532], "temperature": 0.0, "avg_logprob": -0.10001875105358306, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.004330742638558149}, {"id": 574, "seek": 319768, "start": 3221.04, "end": 3227.04, "text": " will be these kind of long term goals is either because we literally give the model long term", "tokens": [51532, 486, 312, 613, 733, 295, 938, 1433, 5493, 307, 2139, 570, 321, 3736, 976, 264, 2316, 938, 1433, 51832], "temperature": 0.0, "avg_logprob": -0.10001875105358306, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.004330742638558149}, {"id": 575, "seek": 322704, "start": 3227.04, "end": 3232.32, "text": " goals because it's economically useful from a human perspective, or because in like,", "tokens": [50364, 5493, 570, 309, 311, 26811, 4420, 490, 257, 1952, 4585, 11, 420, 570, 294, 411, 11, 50628], "temperature": 0.0, "avg_logprob": -0.12126556195710834, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.00048777463962323964}, {"id": 576, "seek": 322704, "start": 3232.32, "end": 3236.72, "text": " some long term goals or are instrumentally useful to achieve other things.", "tokens": [50628, 512, 938, 1433, 5493, 420, 366, 7198, 379, 4420, 281, 4584, 661, 721, 13, 50848], "temperature": 0.0, "avg_logprob": -0.12126556195710834, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.00048777463962323964}, {"id": 577, "seek": 322704, "start": 3237.52, "end": 3247.36, "text": " Gotcha. Okay, interesting. Another thought that came to mind in this discussion of,", "tokens": [50888, 42109, 13, 1033, 11, 1880, 13, 3996, 1194, 300, 1361, 281, 1575, 294, 341, 5017, 295, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12126556195710834, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.00048777463962323964}, {"id": 578, "seek": 322704, "start": 3249.44, "end": 3254.16, "text": " I guess, deception broadly is like, and I've done a little bit of investigation with this", "tokens": [51484, 286, 2041, 11, 40451, 19511, 307, 411, 11, 293, 286, 600, 1096, 257, 707, 857, 295, 9627, 365, 341, 51720], "temperature": 0.0, "avg_logprob": -0.12126556195710834, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.00048777463962323964}, {"id": 579, "seek": 325416, "start": 3254.16, "end": 3260.0, "text": " and engaged in some online debates. And it leads me to propose perhaps like another capability", "tokens": [50364, 293, 8237, 294, 512, 2950, 24203, 13, 400, 309, 6689, 385, 281, 17421, 4317, 411, 1071, 13759, 50656], "temperature": 0.0, "avg_logprob": -0.0889515292887785, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0021154482383280993}, {"id": 580, "seek": 325416, "start": 3260.0, "end": 3267.04, "text": " definition for you. But you know, that as I see it, like a theory of mind, which is kind of a more", "tokens": [50656, 7123, 337, 291, 13, 583, 291, 458, 11, 300, 382, 286, 536, 309, 11, 411, 257, 5261, 295, 1575, 11, 597, 307, 733, 295, 257, 544, 51008], "temperature": 0.0, "avg_logprob": -0.0889515292887785, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0021154482383280993}, {"id": 581, "seek": 325416, "start": 3267.04, "end": 3274.24, "text": " neutral, you know, framing, perhaps, is kind of a precondition for deception, right? If you are", "tokens": [51008, 10598, 11, 291, 458, 11, 28971, 11, 4317, 11, 307, 733, 295, 257, 4346, 684, 849, 337, 40451, 11, 558, 30, 759, 291, 366, 51368], "temperature": 0.0, "avg_logprob": -0.0889515292887785, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0021154482383280993}, {"id": 582, "seek": 325416, "start": 3274.24, "end": 3278.96, "text": " going to mislead someone, you have to have some theory of like what they are currently thinking.", "tokens": [51368, 516, 281, 3346, 306, 345, 1580, 11, 291, 362, 281, 362, 512, 5261, 295, 411, 437, 436, 366, 4362, 1953, 13, 51604], "temperature": 0.0, "avg_logprob": -0.0889515292887785, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0021154482383280993}, {"id": 583, "seek": 327896, "start": 3279.76, "end": 3285.92, "text": " And there's a lot of research from the last six to nine months about", "tokens": [50404, 400, 456, 311, 257, 688, 295, 2132, 490, 264, 1036, 2309, 281, 4949, 2493, 466, 50712], "temperature": 0.0, "avg_logprob": -0.1095407550985163, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.002115510869771242}, {"id": 584, "seek": 327896, "start": 3287.6, "end": 3291.84, "text": " do the current models have theory of mind to what extent, you know, under what conditions.", "tokens": [50796, 360, 264, 2190, 5245, 362, 5261, 295, 1575, 281, 437, 8396, 11, 291, 458, 11, 833, 437, 4487, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1095407550985163, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.002115510869771242}, {"id": 585, "seek": 327896, "start": 3292.56, "end": 3299.04, "text": " And I've been kind of frustrated repeatedly, actually, by different papers that come out and say,", "tokens": [51044, 400, 286, 600, 668, 733, 295, 15751, 18227, 11, 767, 11, 538, 819, 10577, 300, 808, 484, 293, 584, 11, 51368], "temperature": 0.0, "avg_logprob": -0.1095407550985163, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.002115510869771242}, {"id": 586, "seek": 327896, "start": 3299.84, "end": 3305.68, "text": " still no theory of mind from GPT-4, where I'm like, but wait a second, you know,", "tokens": [51408, 920, 572, 5261, 295, 1575, 490, 26039, 51, 12, 19, 11, 689, 286, 478, 411, 11, 457, 1699, 257, 1150, 11, 291, 458, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1095407550985163, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.002115510869771242}, {"id": 587, "seek": 330568, "start": 3305.68, "end": 3310.7999999999997, "text": " as Ilya says, like the most incredible thing about these, these models and the systems that,", "tokens": [50364, 382, 286, 45106, 1619, 11, 411, 264, 881, 4651, 551, 466, 613, 11, 613, 5245, 293, 264, 3652, 300, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09149524594141432, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.002714768750593066}, {"id": 588, "seek": 330568, "start": 3310.7999999999997, "end": 3316.7999999999997, "text": " you know, we engage them through are that they kind of make you feel like you're understood,", "tokens": [50620, 291, 458, 11, 321, 4683, 552, 807, 366, 300, 436, 733, 295, 652, 291, 841, 411, 291, 434, 7320, 11, 50920], "temperature": 0.0, "avg_logprob": -0.09149524594141432, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.002714768750593066}, {"id": 589, "seek": 330568, "start": 3316.7999999999997, "end": 3321.2799999999997, "text": " right? Like it definitely seems like there's some like kind of pretty obvious brute force", "tokens": [50920, 558, 30, 1743, 309, 2138, 2544, 411, 456, 311, 512, 411, 733, 295, 1238, 6322, 47909, 3464, 51144], "temperature": 0.0, "avg_logprob": -0.09149524594141432, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.002714768750593066}, {"id": 590, "seek": 330568, "start": 3321.2799999999997, "end": 3326.24, "text": " theory of mind capability that exists. And yet when people do these benchmarks, they're like, oh,", "tokens": [51144, 5261, 295, 1575, 13759, 300, 8198, 13, 400, 1939, 562, 561, 360, 613, 43751, 11, 436, 434, 411, 11, 1954, 11, 51392], "temperature": 0.0, "avg_logprob": -0.09149524594141432, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.002714768750593066}, {"id": 591, "seek": 330568, "start": 3326.24, "end": 3331.7599999999998, "text": " well, it only gets, you know, 72% on this and 87% on this and whatever. And so, you know, that's not,", "tokens": [51392, 731, 11, 309, 787, 2170, 11, 291, 458, 11, 18731, 4, 322, 341, 293, 27990, 4, 322, 341, 293, 2035, 13, 400, 370, 11, 291, 458, 11, 300, 311, 406, 11, 51668], "temperature": 0.0, "avg_logprob": -0.09149524594141432, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.002714768750593066}, {"id": 592, "seek": 333176, "start": 3331.76, "end": 3334.96, "text": " you know, fails the theory of mind test is like not at a human level or whatever.", "tokens": [50364, 291, 458, 11, 18199, 264, 5261, 295, 1575, 1500, 307, 411, 406, 412, 257, 1952, 1496, 420, 2035, 13, 50524], "temperature": 0.0, "avg_logprob": -0.09331896545690134, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.006692372262477875}, {"id": 593, "seek": 333176, "start": 3335.5200000000004, "end": 3340.5600000000004, "text": " Some of that stuff I've dug into and found like your prompting sucks. If you just improve that,", "tokens": [50552, 2188, 295, 300, 1507, 286, 600, 22954, 666, 293, 1352, 411, 428, 12391, 278, 15846, 13, 759, 291, 445, 3470, 300, 11, 50804], "temperature": 0.0, "avg_logprob": -0.09331896545690134, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.006692372262477875}, {"id": 594, "seek": 333176, "start": 3340.5600000000004, "end": 3345.92, "text": " you know, then you can get over a lot of the humps. But I also have come to understand this", "tokens": [50804, 291, 458, 11, 550, 291, 393, 483, 670, 257, 688, 295, 264, 276, 16951, 13, 583, 286, 611, 362, 808, 281, 1223, 341, 51072], "temperature": 0.0, "avg_logprob": -0.09331896545690134, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.006692372262477875}, {"id": 595, "seek": 333176, "start": 3345.92, "end": 3352.7200000000003, "text": " as a difference in framing where I think I am more like you concerned with", "tokens": [51072, 382, 257, 2649, 294, 28971, 689, 286, 519, 286, 669, 544, 411, 291, 5922, 365, 51412], "temperature": 0.0, "avg_logprob": -0.09331896545690134, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.006692372262477875}, {"id": 596, "seek": 333176, "start": 3354.1600000000003, "end": 3358.5600000000004, "text": " what is the sort of theoretical max that this thing might achieve? Like that seems to me", "tokens": [51484, 437, 307, 264, 1333, 295, 20864, 11469, 300, 341, 551, 1062, 4584, 30, 1743, 300, 2544, 281, 385, 51704], "temperature": 0.0, "avg_logprob": -0.09331896545690134, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.006692372262477875}, {"id": 597, "seek": 335856, "start": 3359.12, "end": 3363.92, "text": " the most relevant question for, you know, risk management purposes. And then I think other", "tokens": [50392, 264, 881, 7340, 1168, 337, 11, 291, 458, 11, 3148, 4592, 9932, 13, 400, 550, 286, 519, 661, 50632], "temperature": 0.0, "avg_logprob": -0.08037333261399042, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.004609098192304373}, {"id": 598, "seek": 335856, "start": 3363.92, "end": 3369.52, "text": " people are asking the a similar question, but through the frame of like, what can this thing", "tokens": [50632, 561, 366, 3365, 264, 257, 2531, 1168, 11, 457, 807, 264, 3920, 295, 411, 11, 437, 393, 341, 551, 50912], "temperature": 0.0, "avg_logprob": -0.08037333261399042, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.004609098192304373}, {"id": 599, "seek": 335856, "start": 3369.52, "end": 3375.52, "text": " do reliably? You know, what can it still do under adversarial conditions or whatever?", "tokens": [50912, 360, 49927, 30, 509, 458, 11, 437, 393, 309, 920, 360, 833, 17641, 44745, 4487, 420, 2035, 30, 51212], "temperature": 0.0, "avg_logprob": -0.08037333261399042, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.004609098192304373}, {"id": 600, "seek": 335856, "start": 3376.08, "end": 3382.16, "text": " So I wonder if there's a need for like another capability level that's even like below the", "tokens": [51240, 407, 286, 2441, 498, 456, 311, 257, 643, 337, 411, 1071, 13759, 1496, 300, 311, 754, 411, 2507, 264, 51544], "temperature": 0.0, "avg_logprob": -0.08037333261399042, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.004609098192304373}, {"id": 601, "seek": 338216, "start": 3382.16, "end": 3388.72, "text": " reachable that would be the sort of robust or, you know, maybe even adversarial robust,", "tokens": [50364, 2524, 712, 300, 576, 312, 264, 1333, 295, 13956, 420, 11, 291, 458, 11, 1310, 754, 17641, 44745, 13956, 11, 50692], "temperature": 0.0, "avg_logprob": -0.12817400535651013, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.013635377399623394}, {"id": 602, "seek": 338216, "start": 3390.08, "end": 3395.2799999999997, "text": " robust to adversarial conditions. But I do see a lot of confusion on that, right? Like", "tokens": [50760, 13956, 281, 17641, 44745, 4487, 13, 583, 286, 360, 536, 257, 688, 295, 15075, 322, 300, 11, 558, 30, 1743, 51020], "temperature": 0.0, "avg_logprob": -0.12817400535651013, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.013635377399623394}, {"id": 603, "seek": 338216, "start": 3395.2799999999997, "end": 3400.08, "text": " people will look at the exact same behavior. And I'll say, damn, this thing has strong theory of", "tokens": [51020, 561, 486, 574, 412, 264, 1900, 912, 5223, 13, 400, 286, 603, 584, 11, 8151, 11, 341, 551, 575, 2068, 5261, 295, 51260], "temperature": 0.0, "avg_logprob": -0.12817400535651013, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.013635377399623394}, {"id": 604, "seek": 338216, "start": 3400.08, "end": 3404.96, "text": " mind and like professors will be like no theory of mind. And I feel like we need some sort of", "tokens": [51260, 1575, 293, 411, 15924, 486, 312, 411, 572, 5261, 295, 1575, 13, 400, 286, 841, 411, 321, 643, 512, 1333, 295, 51504], "temperature": 0.0, "avg_logprob": -0.12817400535651013, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.013635377399623394}, {"id": 605, "seek": 338216, "start": 3404.96, "end": 3410.8799999999997, "text": " additional conceptual distinction to help us get on the same page there. I'm not entirely sure", "tokens": [51504, 4497, 24106, 16844, 281, 854, 505, 483, 322, 264, 912, 3028, 456, 13, 286, 478, 406, 7696, 988, 51800], "temperature": 0.0, "avg_logprob": -0.12817400535651013, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.013635377399623394}, {"id": 606, "seek": 341088, "start": 3410.88, "end": 3416.1600000000003, "text": " whether or like maybe it makes sense from an academic standpoint to to think about this. I", "tokens": [50364, 1968, 420, 411, 1310, 309, 1669, 2020, 490, 364, 7778, 15827, 281, 281, 519, 466, 341, 13, 286, 50628], "temperature": 0.0, "avg_logprob": -0.10931687477307442, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.005383825860917568}, {"id": 607, "seek": 341088, "start": 3416.1600000000003, "end": 3421.52, "text": " think from from the auditing perspective, the max, you know, the limit, the upper bound is what you", "tokens": [50628, 519, 490, 490, 264, 2379, 1748, 4585, 11, 264, 11469, 11, 291, 458, 11, 264, 4948, 11, 264, 6597, 5472, 307, 437, 291, 50896], "temperature": 0.0, "avg_logprob": -0.10931687477307442, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.005383825860917568}, {"id": 608, "seek": 341088, "start": 3421.52, "end": 3427.28, "text": " care about. You really want to prevent people from being able to misuse the system at all,", "tokens": [50896, 1127, 466, 13, 509, 534, 528, 281, 4871, 561, 490, 885, 1075, 281, 3346, 438, 264, 1185, 412, 439, 11, 51184], "temperature": 0.0, "avg_logprob": -0.10931687477307442, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.005383825860917568}, {"id": 609, "seek": 341088, "start": 3427.28, "end": 3431.52, "text": " not just in the robust case, right? It's really about like, what if if somebody actually tried?", "tokens": [51184, 406, 445, 294, 264, 13956, 1389, 11, 558, 30, 467, 311, 534, 466, 411, 11, 437, 498, 498, 2618, 767, 3031, 30, 51396], "temperature": 0.0, "avg_logprob": -0.10931687477307442, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.005383825860917568}, {"id": 610, "seek": 341088, "start": 3432.56, "end": 3439.2000000000003, "text": " Or you want the system itself to be, you know, not only not being able to take over or like", "tokens": [51448, 1610, 291, 528, 264, 1185, 2564, 281, 312, 11, 291, 458, 11, 406, 787, 406, 885, 1075, 281, 747, 670, 420, 411, 51780], "temperature": 0.0, "avg_logprob": -0.10931687477307442, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.005383825860917568}, {"id": 611, "seek": 343920, "start": 3439.2, "end": 3445.2799999999997, "text": " exfiltrate or something like this in a few cases. Yeah, you basically want to limit it", "tokens": [50364, 454, 69, 2352, 4404, 420, 746, 411, 341, 294, 257, 1326, 3331, 13, 865, 11, 291, 1936, 528, 281, 4948, 309, 50668], "temperature": 0.0, "avg_logprob": -0.13005331212824042, "compression_ratio": 1.7470355731225296, "no_speech_prob": 0.0018099361332133412}, {"id": 612, "seek": 343920, "start": 3445.2799999999997, "end": 3449.52, "text": " already at a few cases, right? You don't care about whether it does this like, you also care", "tokens": [50668, 1217, 412, 257, 1326, 3331, 11, 558, 30, 509, 500, 380, 1127, 466, 1968, 309, 775, 341, 411, 11, 291, 611, 1127, 50880], "temperature": 0.0, "avg_logprob": -0.13005331212824042, "compression_ratio": 1.7470355731225296, "no_speech_prob": 0.0018099361332133412}, {"id": 613, "seek": 343920, "start": 3449.52, "end": 3453.6, "text": " about whether it does this 50% of the time, but really you will already want to sort of", "tokens": [50880, 466, 1968, 309, 775, 341, 2625, 4, 295, 264, 565, 11, 457, 534, 291, 486, 1217, 528, 281, 1333, 295, 51084], "temperature": 0.0, "avg_logprob": -0.13005331212824042, "compression_ratio": 1.7470355731225296, "no_speech_prob": 0.0018099361332133412}, {"id": 614, "seek": 343920, "start": 3453.6, "end": 3457.8399999999997, "text": " pull the plug early on. So for an auditing perspective, probably this additional thing", "tokens": [51084, 2235, 264, 5452, 2440, 322, 13, 407, 337, 364, 2379, 1748, 4585, 11, 1391, 341, 4497, 551, 51296], "temperature": 0.0, "avg_logprob": -0.13005331212824042, "compression_ratio": 1.7470355731225296, "no_speech_prob": 0.0018099361332133412}, {"id": 615, "seek": 343920, "start": 3457.8399999999997, "end": 3464.16, "text": " is not necessary, but from from unlike you real world use case and and sort of academic", "tokens": [51296, 307, 406, 4818, 11, 457, 490, 490, 8343, 291, 957, 1002, 764, 1389, 293, 293, 1333, 295, 7778, 51612], "temperature": 0.0, "avg_logprob": -0.13005331212824042, "compression_ratio": 1.7470355731225296, "no_speech_prob": 0.0018099361332133412}, {"id": 616, "seek": 346416, "start": 3464.24, "end": 3466.3199999999997, "text": " perspective, maybe there should be a different category.", "tokens": [50368, 4585, 11, 1310, 456, 820, 312, 257, 819, 7719, 13, 50472], "temperature": 0.0, "avg_logprob": -0.08342339010799632, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.04602113366127014}, {"id": 617, "seek": 346416, "start": 3468.3999999999996, "end": 3473.2, "text": " Yeah, I think if only just to kind of give a label to something that people are saying when", "tokens": [50576, 865, 11, 286, 519, 498, 787, 445, 281, 733, 295, 976, 257, 7645, 281, 746, 300, 561, 366, 1566, 562, 50816], "temperature": 0.0, "avg_logprob": -0.08342339010799632, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.04602113366127014}, {"id": 618, "seek": 346416, "start": 3473.2, "end": 3477.7599999999998, "text": " they're saying that things, you know, aren't happening or can't happen that seem to be like", "tokens": [50816, 436, 434, 1566, 300, 721, 11, 291, 458, 11, 3212, 380, 2737, 420, 393, 380, 1051, 300, 1643, 281, 312, 411, 51044], "temperature": 0.0, "avg_logprob": -0.08342339010799632, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.04602113366127014}, {"id": 619, "seek": 346416, "start": 3477.7599999999998, "end": 3483.92, "text": " obviously happening, we can work on coining a term for that. What's kind of the motivator for", "tokens": [51044, 2745, 2737, 11, 321, 393, 589, 322, 598, 1760, 257, 1433, 337, 300, 13, 708, 311, 733, 295, 264, 5426, 1639, 337, 51352], "temperature": 0.0, "avg_logprob": -0.08342339010799632, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.04602113366127014}, {"id": 620, "seek": 346416, "start": 3484.64, "end": 3489.44, "text": " secrecy around interpretability work? Yeah, I basically think good interpretability work is", "tokens": [51388, 34432, 1344, 926, 7302, 2310, 589, 30, 865, 11, 286, 1936, 519, 665, 7302, 2310, 589, 307, 51628], "temperature": 0.0, "avg_logprob": -0.08342339010799632, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.04602113366127014}, {"id": 621, "seek": 348944, "start": 3489.44, "end": 3495.04, "text": " almost necessarily also good capabilities work. So basically, if you understand the system good", "tokens": [50364, 1920, 4725, 611, 665, 10862, 589, 13, 407, 1936, 11, 498, 291, 1223, 264, 1185, 665, 50644], "temperature": 0.0, "avg_logprob": -0.09162460465029061, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.05662677437067032}, {"id": 622, "seek": 348944, "start": 3495.04, "end": 3500.88, "text": " enough that you like understand the internals, you're almost certainly going to be able to", "tokens": [50644, 1547, 300, 291, 411, 1223, 264, 2154, 1124, 11, 291, 434, 1920, 3297, 516, 281, 312, 1075, 281, 50936], "temperature": 0.0, "avg_logprob": -0.09162460465029061, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.05662677437067032}, {"id": 623, "seek": 348944, "start": 3500.88, "end": 3506.4, "text": " build better architectures, iterate on them faster, make everything quicker, but potentially", "tokens": [50936, 1322, 1101, 6331, 1303, 11, 44497, 322, 552, 4663, 11, 652, 1203, 16255, 11, 457, 7263, 51212], "temperature": 0.0, "avg_logprob": -0.09162460465029061, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.05662677437067032}, {"id": 624, "seek": 348944, "start": 3506.4, "end": 3514.7200000000003, "text": " compress a lot of the you know, fluff that current systems may still have. And yeah, we will try to", "tokens": [51212, 14778, 257, 688, 295, 264, 291, 458, 11, 41533, 300, 2190, 3652, 815, 920, 362, 13, 400, 1338, 11, 321, 486, 853, 281, 51628], "temperature": 0.0, "avg_logprob": -0.09162460465029061, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.05662677437067032}, {"id": 625, "seek": 351472, "start": 3514.72, "end": 3520.56, "text": " sort of evaluate whether whether our method does in fact have these implications. But yeah,", "tokens": [50364, 1333, 295, 13059, 1968, 1968, 527, 3170, 775, 294, 1186, 362, 613, 16602, 13, 583, 1338, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11318575981819984, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0024723531678318977}, {"id": 626, "seek": 351472, "start": 3520.56, "end": 3524.72, "text": " you know, like I think basically, if you have a good interpretability tool, it will almost", "tokens": [50656, 291, 458, 11, 411, 286, 519, 1936, 11, 498, 291, 362, 257, 665, 7302, 2310, 2290, 11, 309, 486, 1920, 50864], "temperature": 0.0, "avg_logprob": -0.11318575981819984, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0024723531678318977}, {"id": 627, "seek": 351472, "start": 3524.72, "end": 3529.3599999999997, "text": " certainly also have implications for capabilities. And the question is just how big are they?", "tokens": [50864, 3297, 611, 362, 16602, 337, 10862, 13, 400, 264, 1168, 307, 445, 577, 955, 366, 436, 30, 51096], "temperature": 0.0, "avg_logprob": -0.11318575981819984, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0024723531678318977}, {"id": 628, "seek": 351472, "start": 3530.16, "end": 3537.12, "text": " Speaking of new architectures, though, this to me seems like the biggest wildcard. And I'm", "tokens": [51136, 13069, 295, 777, 6331, 1303, 11, 1673, 11, 341, 281, 385, 2544, 411, 264, 3880, 4868, 22259, 13, 400, 286, 478, 51484], "temperature": 0.0, "avg_logprob": -0.11318575981819984, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0024723531678318977}, {"id": 629, "seek": 351472, "start": 3537.12, "end": 3543.2, "text": " currently obsessed with the new Mamba architecture that has just been introduced.", "tokens": [51484, 4362, 16923, 365, 264, 777, 376, 23337, 9482, 300, 575, 445, 668, 7268, 13, 51788], "temperature": 0.0, "avg_logprob": -0.11318575981819984, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0024723531678318977}, {"id": 630, "seek": 354320, "start": 3543.8399999999997, "end": 3547.7599999999998, "text": " In the last, I don't know, 10 days or whatever. I don't know if you've had a chance to go down", "tokens": [50396, 682, 264, 1036, 11, 286, 500, 380, 458, 11, 1266, 1708, 420, 2035, 13, 286, 500, 380, 458, 498, 291, 600, 632, 257, 2931, 281, 352, 760, 50592], "temperature": 0.0, "avg_logprob": -0.04433200647542765, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0059098126366734505}, {"id": 631, "seek": 354320, "start": 3547.7599999999998, "end": 3552.56, "text": " this particular rabbit hole just yet. But I plan to do a whole kind of episode on it.", "tokens": [50592, 341, 1729, 19509, 5458, 445, 1939, 13, 583, 286, 1393, 281, 360, 257, 1379, 733, 295, 3500, 322, 309, 13, 50832], "temperature": 0.0, "avg_logprob": -0.04433200647542765, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0059098126366734505}, {"id": 632, "seek": 354320, "start": 3553.6, "end": 3562.24, "text": " In short, they have developed a new state space model that they refer to as a selective", "tokens": [50884, 682, 2099, 11, 436, 362, 4743, 257, 777, 1785, 1901, 2316, 300, 436, 2864, 281, 382, 257, 33930, 51316], "temperature": 0.0, "avg_logprob": -0.04433200647542765, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0059098126366734505}, {"id": 633, "seek": 354320, "start": 3562.24, "end": 3571.52, "text": " state space model. And the selective mechanism basically has a sort of attention like property", "tokens": [51316, 1785, 1901, 2316, 13, 400, 264, 33930, 7513, 1936, 575, 257, 1333, 295, 3202, 411, 4707, 51780], "temperature": 0.0, "avg_logprob": -0.04433200647542765, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0059098126366734505}, {"id": 634, "seek": 357152, "start": 3571.52, "end": 3577.92, "text": " where the computation that is done becomes input dependent. So unlike, you know, you're sort of", "tokens": [50364, 689, 264, 24903, 300, 307, 1096, 3643, 4846, 12334, 13, 407, 8343, 11, 291, 458, 11, 291, 434, 1333, 295, 50684], "temperature": 0.0, "avg_logprob": -0.07202501886898709, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0018673308659344912}, {"id": 635, "seek": 357152, "start": 3577.92, "end": 3586.08, "text": " classic, say, you know, MS classifier, where you kind of run the same, you know, given a given", "tokens": [50684, 7230, 11, 584, 11, 291, 458, 11, 7395, 1508, 9902, 11, 689, 291, 733, 295, 1190, 264, 912, 11, 291, 458, 11, 2212, 257, 2212, 51092], "temperature": 0.0, "avg_logprob": -0.07202501886898709, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0018673308659344912}, {"id": 636, "seek": 357152, "start": 3586.08, "end": 3591.36, "text": " input, you're going to run the same set of matrix, you know, multiplications until you get to the", "tokens": [51092, 4846, 11, 291, 434, 516, 281, 1190, 264, 912, 992, 295, 8141, 11, 291, 458, 11, 17596, 763, 1826, 291, 483, 281, 264, 51356], "temperature": 0.0, "avg_logprob": -0.07202501886898709, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0018673308659344912}, {"id": 637, "seek": 357152, "start": 3591.36, "end": 3597.92, "text": " output. With a transformer, you have this kind of additional layer of complexity, which is that", "tokens": [51356, 5598, 13, 2022, 257, 31782, 11, 291, 362, 341, 733, 295, 4497, 4583, 295, 14024, 11, 597, 307, 300, 51684], "temperature": 0.0, "avg_logprob": -0.07202501886898709, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0018673308659344912}, {"id": 638, "seek": 359792, "start": 3597.92, "end": 3603.2000000000003, "text": " the attention matrix itself is dynamically generated based on the inputs. And so you've got", "tokens": [50364, 264, 3202, 8141, 2564, 307, 43492, 10833, 2361, 322, 264, 15743, 13, 400, 370, 291, 600, 658, 50628], "temperature": 0.0, "avg_logprob": -0.0874498861807364, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.003074800595641136}, {"id": 639, "seek": 359792, "start": 3603.2000000000003, "end": 3608.96, "text": " kind of this forking path of influence for the for the inputs. And this apparently was", "tokens": [50628, 733, 295, 341, 337, 5092, 3100, 295, 6503, 337, 264, 337, 264, 15743, 13, 400, 341, 7970, 390, 50916], "temperature": 0.0, "avg_logprob": -0.0874498861807364, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.003074800595641136}, {"id": 640, "seek": 359792, "start": 3610.08, "end": 3617.6, "text": " not really feasible in past versions of the state space models for, I think, a couple different", "tokens": [50972, 406, 534, 26648, 294, 1791, 9606, 295, 264, 1785, 1901, 5245, 337, 11, 286, 519, 11, 257, 1916, 819, 51348], "temperature": 0.0, "avg_logprob": -0.0874498861807364, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.003074800595641136}, {"id": 641, "seek": 359792, "start": 3617.6, "end": 3622.0, "text": " reasons. One being that if you do that, it starts to become recurrent. And then it becomes really", "tokens": [51348, 4112, 13, 1485, 885, 300, 498, 291, 360, 300, 11, 309, 3719, 281, 1813, 18680, 1753, 13, 400, 550, 309, 3643, 534, 51568], "temperature": 0.0, "avg_logprob": -0.0874498861807364, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.003074800595641136}, {"id": 642, "seek": 359792, "start": 3622.0, "end": 3627.6, "text": " hard to just actually make the models fast enough to be useful. And they've got a hardware,", "tokens": [51568, 1152, 281, 445, 767, 652, 264, 5245, 2370, 1547, 281, 312, 4420, 13, 400, 436, 600, 658, 257, 8837, 11, 51848], "temperature": 0.0, "avg_logprob": -0.0874498861807364, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.003074800595641136}, {"id": 643, "seek": 362760, "start": 3627.68, "end": 3635.04, "text": " aware approach to solving that, which allows it to be fast as well as super expressive.", "tokens": [50368, 3650, 3109, 281, 12606, 300, 11, 597, 4045, 309, 281, 312, 2370, 382, 731, 382, 1687, 40189, 13, 50736], "temperature": 0.0, "avg_logprob": -0.0830698013305664, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.0006877569248899817}, {"id": 644, "seek": 362760, "start": 3635.7599999999998, "end": 3640.16, "text": " So it seems to be for me, it's like a pretty good candidate for paper of the year,", "tokens": [50772, 407, 309, 2544, 281, 312, 337, 385, 11, 309, 311, 411, 257, 1238, 665, 11532, 337, 3035, 295, 264, 1064, 11, 50992], "temperature": 0.0, "avg_logprob": -0.0830698013305664, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.0006877569248899817}, {"id": 645, "seek": 362760, "start": 3640.16, "end": 3646.56, "text": " certainly on the capabilities unlock side. And they show improvement up to a million tokens.", "tokens": [50992, 3297, 322, 264, 10862, 11634, 1252, 13, 400, 436, 855, 10444, 493, 281, 257, 2459, 22667, 13, 51312], "temperature": 0.0, "avg_logprob": -0.0830698013305664, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.0006877569248899817}, {"id": 646, "seek": 362760, "start": 3647.92, "end": 3651.2, "text": " Like it just continues to get better with more and more context.", "tokens": [51380, 1743, 309, 445, 6515, 281, 483, 1101, 365, 544, 293, 544, 4319, 13, 51544], "temperature": 0.0, "avg_logprob": -0.0830698013305664, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.0006877569248899817}, {"id": 647, "seek": 365120, "start": 3651.8399999999997, "end": 3657.6, "text": " So I'm like, man, this could be, you know, it's a pretty good candidate, I think, for sort of", "tokens": [50396, 407, 286, 478, 411, 11, 587, 11, 341, 727, 312, 11, 291, 458, 11, 309, 311, 257, 1238, 665, 11532, 11, 286, 519, 11, 337, 1333, 295, 50684], "temperature": 0.0, "avg_logprob": -0.18546169996261597, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.006096884608268738}, {"id": 648, "seek": 365120, "start": 3657.6, "end": 3663.04, "text": " transformer, you know, people put it as like successor alternative, but I actually think it", "tokens": [50684, 31782, 11, 291, 458, 11, 561, 829, 309, 382, 411, 31864, 8535, 11, 457, 286, 767, 519, 309, 50956], "temperature": 0.0, "avg_logprob": -0.18546169996261597, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.006096884608268738}, {"id": 649, "seek": 365120, "start": 3663.04, "end": 3668.8799999999997, "text": " is more likely to play out as complement, like some sort of hybrid, you know, seems like where", "tokens": [50956, 307, 544, 3700, 281, 862, 484, 382, 17103, 11, 411, 512, 1333, 295, 13051, 11, 291, 458, 11, 2544, 411, 689, 51248], "temperature": 0.0, "avg_logprob": -0.18546169996261597, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.006096884608268738}, {"id": 650, "seek": 365120, "start": 3668.8799999999997, "end": 3674.24, "text": " the greatest capabilities will ultimately be. So anyway, all of that, how do you even think about", "tokens": [51248, 264, 6636, 10862, 486, 6284, 312, 13, 407, 4033, 11, 439, 295, 300, 11, 577, 360, 291, 754, 519, 466, 51516], "temperature": 0.0, "avg_logprob": -0.18546169996261597, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.006096884608268738}, {"id": 651, "seek": 367424, "start": 3674.56, "end": 3685.12, "text": " the challenge of interpretability in the context of new architectures also starting to come online?", "tokens": [50380, 264, 3430, 295, 7302, 2310, 294, 264, 4319, 295, 777, 6331, 1303, 611, 2891, 281, 808, 2950, 30, 50908], "temperature": 0.0, "avg_logprob": -0.13300558068286414, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.10370151698589325}, {"id": 652, "seek": 367424, "start": 3685.12, "end": 3688.56, "text": " And, you know, what if all of a sudden like the transformer is not even the most", "tokens": [50908, 400, 11, 291, 458, 11, 437, 498, 439, 295, 257, 3990, 411, 264, 31782, 307, 406, 754, 264, 881, 51080], "temperature": 0.0, "avg_logprob": -0.13300558068286414, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.10370151698589325}, {"id": 653, "seek": 367424, "start": 3689.6, "end": 3695.4399999999996, "text": " powerful architecture anymore? Does that send you like, you know, probably some of the same", "tokens": [51132, 4005, 9482, 3602, 30, 4402, 300, 2845, 291, 411, 11, 291, 458, 11, 1391, 512, 295, 264, 912, 51424], "temperature": 0.0, "avg_logprob": -0.13300558068286414, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.10370151698589325}, {"id": 654, "seek": 367424, "start": 3695.4399999999996, "end": 3701.2799999999997, "text": " techniques will work, but it seems like it's like a whole new blind cave that you sort of have to", "tokens": [51424, 7512, 486, 589, 11, 457, 309, 2544, 411, 309, 311, 411, 257, 1379, 777, 6865, 11730, 300, 291, 1333, 295, 362, 281, 51716], "temperature": 0.0, "avg_logprob": -0.13300558068286414, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.10370151698589325}, {"id": 655, "seek": 370128, "start": 3701.36, "end": 3706.32, "text": " go exploring, no? I don't know. Like I honestly think, you know, if your interpretability", "tokens": [50368, 352, 12736, 11, 572, 30, 286, 500, 380, 458, 13, 1743, 286, 6095, 519, 11, 291, 458, 11, 498, 428, 7302, 2310, 50616], "temperature": 0.0, "avg_logprob": -0.134706956794463, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.008575119078159332}, {"id": 656, "seek": 370128, "start": 3706.32, "end": 3711.6800000000003, "text": " techniques relies on like a very specific architecture, it's probably not that great", "tokens": [50616, 7512, 30910, 322, 411, 257, 588, 2685, 9482, 11, 309, 311, 1391, 406, 300, 869, 50884], "temperature": 0.0, "avg_logprob": -0.134706956794463, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.008575119078159332}, {"id": 657, "seek": 370128, "start": 3711.6800000000003, "end": 3720.0800000000004, "text": " of a technique anyway. Like there are probably there are probably at least some laws that generalize", "tokens": [50884, 295, 257, 6532, 4033, 13, 1743, 456, 366, 1391, 456, 366, 1391, 412, 1935, 512, 6064, 300, 2674, 1125, 51304], "temperature": 0.0, "avg_logprob": -0.134706956794463, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.008575119078159332}, {"id": 658, "seek": 370128, "start": 3720.0800000000004, "end": 3725.2000000000003, "text": " between different architectures or ways to interpret things or, you know, like ways that", "tokens": [51304, 1296, 819, 6331, 1303, 420, 2098, 281, 7302, 721, 420, 11, 291, 458, 11, 411, 2098, 300, 51560], "temperature": 0.0, "avg_logprob": -0.134706956794463, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.008575119078159332}, {"id": 659, "seek": 372520, "start": 3725.2, "end": 3730.56, "text": " learning with SGD works that generalize between architectures that my best guess is", "tokens": [50364, 2539, 365, 34520, 35, 1985, 300, 2674, 1125, 1296, 6331, 1303, 300, 452, 1151, 2041, 307, 50632], "temperature": 0.0, "avg_logprob": -0.10218173435756138, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.02757912129163742}, {"id": 660, "seek": 372520, "start": 3732.08, "end": 3736.16, "text": " if you have an interpretability technique that is good on one model or like the correct technique", "tokens": [50708, 498, 291, 362, 364, 7302, 2310, 6532, 300, 307, 665, 322, 472, 2316, 420, 411, 264, 3006, 6532, 50912], "temperature": 0.0, "avg_logprob": -0.10218173435756138, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.02757912129163742}, {"id": 661, "seek": 372520, "start": 3736.16, "end": 3742.08, "text": " on one model in quotes, it will also generalize to two other models. Maybe, you know, maybe you", "tokens": [50912, 322, 472, 2316, 294, 19963, 11, 309, 486, 611, 2674, 1125, 281, 732, 661, 5245, 13, 2704, 11, 291, 458, 11, 1310, 291, 51208], "temperature": 0.0, "avg_logprob": -0.10218173435756138, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.02757912129163742}, {"id": 662, "seek": 372520, "start": 3742.08, "end": 3748.3199999999997, "text": " have to adapt some of the formulas, but at least the conceptual work behind this behind", "tokens": [51208, 362, 281, 6231, 512, 295, 264, 30546, 11, 457, 412, 1935, 264, 24106, 589, 2261, 341, 2261, 51520], "temperature": 0.0, "avg_logprob": -0.10218173435756138, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.02757912129163742}, {"id": 663, "seek": 372520, "start": 3748.3199999999997, "end": 3753.04, "text": " behind the interpretability technique will just work. Well, I have certainly hope that's true.", "tokens": [51520, 2261, 264, 7302, 2310, 6532, 486, 445, 589, 13, 1042, 11, 286, 362, 3297, 1454, 300, 311, 2074, 13, 51756], "temperature": 0.0, "avg_logprob": -0.10218173435756138, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.02757912129163742}, {"id": 664, "seek": 375304, "start": 3753.12, "end": 3758.0, "text": " I've had some early, you know, I wouldn't even say debate, but just kind of, you know,", "tokens": [50368, 286, 600, 632, 512, 2440, 11, 291, 458, 11, 286, 2759, 380, 754, 584, 7958, 11, 457, 445, 733, 295, 11, 291, 458, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 665, "seek": 375304, "start": 3758.0, "end": 3762.4, "text": " everybody's trying to make sense of this stuff in real time. And on the pro side for this Mamba", "tokens": [50612, 2201, 311, 1382, 281, 652, 2020, 295, 341, 1507, 294, 957, 565, 13, 400, 322, 264, 447, 1252, 337, 341, 376, 23337, 50832], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 666, "seek": 375304, "start": 3762.4, "end": 3769.92, "text": " thing, the fact that there is a state that, you know, kind of gets progressively evolved through", "tokens": [50832, 551, 11, 264, 1186, 300, 456, 307, 257, 1785, 300, 11, 291, 458, 11, 733, 295, 2170, 46667, 14178, 807, 51208], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 667, "seek": 375304, "start": 3769.92, "end": 3774.48, "text": " time does present like a natural target for something like representation engineering,", "tokens": [51208, 565, 775, 1974, 411, 257, 3303, 3779, 337, 746, 411, 10290, 7043, 11, 51436], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 668, "seek": 375304, "start": 3774.48, "end": 3778.48, "text": " where you could be like, all right, well, we know where the information is, you know,", "tokens": [51436, 689, 291, 727, 312, 411, 11, 439, 558, 11, 731, 11, 321, 458, 689, 264, 1589, 307, 11, 291, 458, 11, 51636], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 669, "seek": 375304, "start": 3778.48, "end": 3782.48, "text": " and it's like pretty clear where we need to look. So that new bottleneck, you know,", "tokens": [51636, 293, 309, 311, 411, 1238, 1850, 689, 321, 643, 281, 574, 13, 407, 300, 777, 44641, 547, 11, 291, 458, 11, 51836], "temperature": 0.0, "avg_logprob": -0.08724835713704428, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.0005883658886887133}, {"id": 670, "seek": 378248, "start": 3782.48, "end": 3788.2400000000002, "text": " in some sets could make things easier or more local. But then the flip side is like, again,", "tokens": [50364, 294, 512, 6352, 727, 652, 721, 3571, 420, 544, 2654, 13, 583, 550, 264, 7929, 1252, 307, 411, 11, 797, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09239831213223731, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0003053242980968207}, {"id": 671, "seek": 378248, "start": 3788.2400000000002, "end": 3791.6, "text": " there's just who knows what surprises we might find. And there's some intricacies with the", "tokens": [50652, 456, 311, 445, 567, 3255, 437, 22655, 321, 1062, 915, 13, 400, 456, 311, 512, 30242, 20330, 365, 264, 50820], "temperature": 0.0, "avg_logprob": -0.09239831213223731, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0003053242980968207}, {"id": 672, "seek": 378248, "start": 3792.16, "end": 3798.64, "text": " hardware specific nature of the algorithm to, I think, with a major caveat that, you know,", "tokens": [50848, 8837, 2685, 3687, 295, 264, 9284, 281, 11, 286, 519, 11, 365, 257, 2563, 43012, 300, 11, 291, 458, 11, 51172], "temperature": 0.0, "avg_logprob": -0.09239831213223731, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0003053242980968207}, {"id": 673, "seek": 378248, "start": 3798.64, "end": 3803.28, "text": " I'm still trying to figure all this out. So how, you know, just to kind of zoom out and", "tokens": [51172, 286, 478, 920, 1382, 281, 2573, 439, 341, 484, 13, 407, 577, 11, 291, 458, 11, 445, 281, 733, 295, 8863, 484, 293, 51404], "temperature": 0.0, "avg_logprob": -0.09239831213223731, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0003053242980968207}, {"id": 674, "seek": 378248, "start": 3803.28, "end": 3807.36, "text": " give the big picture, right, like assume that you're right. And I hope you are that some of these", "tokens": [51404, 976, 264, 955, 3036, 11, 558, 11, 411, 6552, 300, 291, 434, 558, 13, 400, 286, 1454, 291, 366, 300, 512, 295, 613, 51608], "temperature": 0.0, "avg_logprob": -0.09239831213223731, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0003053242980968207}, {"id": 675, "seek": 380736, "start": 3808.1600000000003, "end": 3817.28, "text": " techniques kind of readily generalize. What is the model for interpretability at the deployment", "tokens": [50404, 7512, 733, 295, 26336, 2674, 1125, 13, 708, 307, 264, 2316, 337, 7302, 2310, 412, 264, 19317, 50860], "temperature": 0.0, "avg_logprob": -0.14599558132797924, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.028430746868252754}, {"id": 676, "seek": 380736, "start": 3817.28, "end": 3824.48, "text": " phase? Is it like every forward pass, you like extract internal states and put them up through", "tokens": [50860, 5574, 30, 1119, 309, 411, 633, 2128, 1320, 11, 291, 411, 8947, 6920, 4368, 293, 829, 552, 493, 807, 51220], "temperature": 0.0, "avg_logprob": -0.14599558132797924, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.028430746868252754}, {"id": 677, "seek": 380736, "start": 3824.48, "end": 3832.48, "text": " some classifier and say like, you pass so you could go or no, like you we've detected deception or", "tokens": [51220, 512, 1508, 9902, 293, 584, 411, 11, 291, 1320, 370, 291, 727, 352, 420, 572, 11, 411, 291, 321, 600, 21896, 40451, 420, 51620], "temperature": 0.0, "avg_logprob": -0.14599558132797924, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.028430746868252754}, {"id": 678, "seek": 383248, "start": 3832.48, "end": 3838.48, "text": " we've detected harmful intent or something. And therefore we like shut off this generation. Like", "tokens": [50364, 321, 600, 21896, 19727, 8446, 420, 746, 13, 400, 4412, 321, 411, 5309, 766, 341, 5125, 13, 1743, 50664], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 679, "seek": 383248, "start": 3838.48, "end": 3843.44, "text": " how do you expect that will actually be used? Or maybe it's upstream of that. And, you know,", "tokens": [50664, 577, 360, 291, 2066, 300, 486, 767, 312, 1143, 30, 1610, 1310, 309, 311, 33915, 295, 300, 13, 400, 11, 291, 458, 11, 50912], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 680, "seek": 383248, "start": 3843.44, "end": 3846.48, "text": " we get good models that just work and you don't even have to worry about it at runtime. But", "tokens": [50912, 321, 483, 665, 5245, 300, 445, 589, 293, 291, 500, 380, 754, 362, 281, 3292, 466, 309, 412, 34474, 13, 583, 51064], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 681, "seek": 383248, "start": 3847.36, "end": 3848.96, "text": " I don't know, that seems a little optimistic to me.", "tokens": [51108, 286, 500, 380, 458, 11, 300, 2544, 257, 707, 19397, 281, 385, 13, 51188], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 682, "seek": 383248, "start": 3849.52, "end": 3853.92, "text": " You know, in the best world, we will have very good mechanistic interpretability techniques", "tokens": [51216, 509, 458, 11, 294, 264, 1151, 1002, 11, 321, 486, 362, 588, 665, 4236, 3142, 7302, 2310, 7512, 51436], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 683, "seek": 383248, "start": 3854.48, "end": 3859.36, "text": " that we can run at least in that there are probably going to be costly to run. And then", "tokens": [51464, 300, 321, 393, 1190, 412, 1935, 294, 300, 456, 366, 1391, 516, 281, 312, 28328, 281, 1190, 13, 400, 550, 51708], "temperature": 0.0, "avg_logprob": -0.12378816529521793, "compression_ratio": 1.6655844155844155, "no_speech_prob": 0.0017005770932883024}, {"id": 684, "seek": 385936, "start": 3859.36, "end": 3868.2400000000002, "text": " we run them and sort of build the full cognitive model of the weights or that the weights", "tokens": [50364, 321, 1190, 552, 293, 1333, 295, 1322, 264, 1577, 15605, 2316, 295, 264, 17443, 420, 300, 264, 17443, 50808], "temperature": 0.0, "avg_logprob": -0.1131296157836914, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.04204060882329941}, {"id": 685, "seek": 385936, "start": 3868.2400000000002, "end": 3874.0, "text": " implement. And then we can already like see whether specific harmful ideas or, you know,", "tokens": [50808, 4445, 13, 400, 550, 321, 393, 1217, 411, 536, 1968, 2685, 19727, 3487, 420, 11, 291, 458, 11, 51096], "temperature": 0.0, "avg_logprob": -0.1131296157836914, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.04204060882329941}, {"id": 686, "seek": 385936, "start": 3874.0, "end": 3879.6800000000003, "text": " other ideas that are otherwise bad are in there. And maybe we can already remove them. And then", "tokens": [51096, 661, 3487, 300, 366, 5911, 1578, 366, 294, 456, 13, 400, 1310, 321, 393, 1217, 4159, 552, 13, 400, 550, 51380], "temperature": 0.0, "avg_logprob": -0.1131296157836914, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.04204060882329941}, {"id": 687, "seek": 385936, "start": 3879.6800000000003, "end": 3884.6400000000003, "text": " probably during deployment, you would run something that is much cheaper. And sort of,", "tokens": [51380, 1391, 1830, 19317, 11, 291, 576, 1190, 746, 300, 307, 709, 12284, 13, 400, 1333, 295, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1131296157836914, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.04204060882329941}, {"id": 688, "seek": 388464, "start": 3884.64, "end": 3893.2799999999997, "text": " you know, it's the 80-20 version of this. But yeah, I think in a bad world, there could be", "tokens": [50364, 291, 458, 11, 309, 311, 264, 4688, 12, 2009, 3037, 295, 341, 13, 583, 1338, 11, 286, 519, 294, 257, 1578, 1002, 11, 456, 727, 312, 50796], "temperature": 0.0, "avg_logprob": -0.13822832974520596, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.044652439653873444}, {"id": 689, "seek": 388464, "start": 3893.2799999999997, "end": 3900.24, "text": " cases where you have to run the very expensive thing all the time for every forward pass,", "tokens": [50796, 3331, 689, 291, 362, 281, 1190, 264, 588, 5124, 551, 439, 264, 565, 337, 633, 2128, 1320, 11, 51144], "temperature": 0.0, "avg_logprob": -0.13822832974520596, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.044652439653873444}, {"id": 690, "seek": 388464, "start": 3900.24, "end": 3906.16, "text": " because otherwise you just don't spot sort of the black swans. But yeah, it's very unclear to me.", "tokens": [51144, 570, 5911, 291, 445, 500, 380, 4008, 1333, 295, 264, 2211, 1693, 599, 13, 583, 1338, 11, 309, 311, 588, 25636, 281, 385, 13, 51440], "temperature": 0.0, "avg_logprob": -0.13822832974520596, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.044652439653873444}, {"id": 691, "seek": 388464, "start": 3906.16, "end": 3911.3599999999997, "text": " I think in my head, it's more like solve the first part, then think about the rest.", "tokens": [51440, 286, 519, 294, 452, 1378, 11, 309, 311, 544, 411, 5039, 264, 700, 644, 11, 550, 519, 466, 264, 1472, 13, 51700], "temperature": 0.0, "avg_logprob": -0.13822832974520596, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.044652439653873444}, {"id": 692, "seek": 391136, "start": 3912.1600000000003, "end": 3918.8, "text": " Maybe let's transition to your recent work on deception itself. And then at the very end,", "tokens": [50404, 2704, 718, 311, 6034, 281, 428, 5162, 589, 322, 40451, 2564, 13, 400, 550, 412, 264, 588, 917, 11, 50736], "temperature": 0.0, "avg_logprob": -0.07679307460784912, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0031723189167678356}, {"id": 693, "seek": 391136, "start": 3918.8, "end": 3924.4, "text": " we can kind of circle back to a couple of the big picture questions. So this paper was one that", "tokens": [50736, 321, 393, 733, 295, 6329, 646, 281, 257, 1916, 295, 264, 955, 3036, 1651, 13, 407, 341, 3035, 390, 472, 300, 51016], "temperature": 0.0, "avg_logprob": -0.07679307460784912, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0031723189167678356}, {"id": 694, "seek": 391136, "start": 3925.04, "end": 3930.0, "text": " very much caught my eye when it came out. I have done, you know, as I said, like quite a bit of", "tokens": [51048, 588, 709, 5415, 452, 3313, 562, 309, 1361, 484, 13, 286, 362, 1096, 11, 291, 458, 11, 382, 286, 848, 11, 411, 1596, 257, 857, 295, 51296], "temperature": 0.0, "avg_logprob": -0.07679307460784912, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0031723189167678356}, {"id": 695, "seek": 391136, "start": 3931.2000000000003, "end": 3937.52, "text": " red teaming both at times in private, at times in public. And definitely seen all manner of", "tokens": [51356, 2182, 1469, 278, 1293, 412, 1413, 294, 4551, 11, 412, 1413, 294, 1908, 13, 400, 2138, 1612, 439, 9060, 295, 51672], "temperature": 0.0, "avg_logprob": -0.07679307460784912, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0031723189167678356}, {"id": 696, "seek": 393752, "start": 3937.52, "end": 3942.24, "text": " model misbehavior and found, you know, it's often not that hard to induce misbehavior.", "tokens": [50364, 2316, 3346, 29437, 38387, 293, 1352, 11, 291, 458, 11, 309, 311, 2049, 406, 300, 1152, 281, 41263, 3346, 29437, 38387, 13, 50600], "temperature": 0.0, "avg_logprob": -0.10197352635041448, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.01322164200246334}, {"id": 697, "seek": 393752, "start": 3943.04, "end": 3946.4, "text": " You know, people talk about jail breaks, but a lot of the time I'm like, you don't even need", "tokens": [50640, 509, 458, 11, 561, 751, 466, 10511, 9857, 11, 457, 257, 688, 295, 264, 565, 286, 478, 411, 11, 291, 500, 380, 754, 643, 50808], "temperature": 0.0, "avg_logprob": -0.10197352635041448, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.01322164200246334}, {"id": 698, "seek": 393752, "start": 3946.4, "end": 3951.52, "text": " a jailbreak. You know, you just need to kind of set it up and let it go. It's often really quite", "tokens": [50808, 257, 10511, 13225, 13, 509, 458, 11, 291, 445, 643, 281, 733, 295, 992, 309, 493, 293, 718, 309, 352, 13, 467, 311, 2049, 534, 1596, 51064], "temperature": 0.0, "avg_logprob": -0.10197352635041448, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.01322164200246334}, {"id": 699, "seek": 393752, "start": 3951.52, "end": 3959.6, "text": " easy. But one thing I had never seen is an instance where the model seemed to, in an unprompted way,", "tokens": [51064, 1858, 13, 583, 472, 551, 286, 632, 1128, 1612, 307, 364, 5197, 689, 264, 2316, 6576, 281, 11, 294, 364, 517, 28722, 25383, 636, 11, 51468], "temperature": 0.0, "avg_logprob": -0.10197352635041448, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.01322164200246334}, {"id": 700, "seek": 393752, "start": 3960.64, "end": 3966.32, "text": " deceive its user. Certainly seen things where, you know, tell it's a lie and it will lie.", "tokens": [51520, 43440, 1080, 4195, 13, 16628, 1612, 721, 689, 11, 291, 458, 11, 980, 309, 311, 257, 4544, 293, 309, 486, 4544, 13, 51804], "temperature": 0.0, "avg_logprob": -0.10197352635041448, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.01322164200246334}, {"id": 701, "seek": 396632, "start": 3967.1200000000003, "end": 3973.6000000000004, "text": " But to see that deception start to happen in a way that was not explicitly asked for", "tokens": [50404, 583, 281, 536, 300, 40451, 722, 281, 1051, 294, 257, 636, 300, 390, 406, 20803, 2351, 337, 50728], "temperature": 0.0, "avg_logprob": -0.08482290526567879, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.00022339849965646863}, {"id": 702, "seek": 396632, "start": 3974.1600000000003, "end": 3980.6400000000003, "text": " is, I think, the central finding of this paper. So how about, you know, set it up, tell us kind", "tokens": [50756, 307, 11, 286, 519, 11, 264, 5777, 5006, 295, 341, 3035, 13, 407, 577, 466, 11, 291, 458, 11, 992, 309, 493, 11, 980, 505, 733, 51080], "temperature": 0.0, "avg_logprob": -0.08482290526567879, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.00022339849965646863}, {"id": 703, "seek": 396632, "start": 3980.6400000000003, "end": 3984.7200000000003, "text": " of what the premise was, you know, maybe you can give a little bit of kind of motivation for,", "tokens": [51080, 295, 437, 264, 22045, 390, 11, 291, 458, 11, 1310, 291, 393, 976, 257, 707, 857, 295, 733, 295, 12335, 337, 11, 51284], "temperature": 0.0, "avg_logprob": -0.08482290526567879, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.00022339849965646863}, {"id": 704, "seek": 396632, "start": 3984.7200000000003, "end": 3988.7200000000003, "text": " you know, exactly how you started to look in the area that you looked. And then we can really", "tokens": [51284, 291, 458, 11, 2293, 577, 291, 1409, 281, 574, 294, 264, 1859, 300, 291, 2956, 13, 400, 550, 321, 393, 534, 51484], "temperature": 0.0, "avg_logprob": -0.08482290526567879, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.00022339849965646863}, {"id": 705, "seek": 396632, "start": 3988.7200000000003, "end": 3993.28, "text": " dig into the details of your findings. Yeah. So I definitely, you know, the paper definitely", "tokens": [51484, 2528, 666, 264, 4365, 295, 428, 16483, 13, 865, 13, 407, 286, 2138, 11, 291, 458, 11, 264, 3035, 2138, 51712], "temperature": 0.0, "avg_logprob": -0.08482290526567879, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.00022339849965646863}, {"id": 706, "seek": 399328, "start": 3993.28, "end": 3997.92, "text": " blew up more than we thought. And we had sort of more engagement than we expected. We're even", "tokens": [50364, 19075, 493, 544, 813, 321, 1194, 13, 400, 321, 632, 1333, 295, 544, 8742, 813, 321, 5176, 13, 492, 434, 754, 50596], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 707, "seek": 399328, "start": 3997.92, "end": 4001.1200000000003, "text": " not quite sure whether we should release it at all, because, you know, in our heads, it was sort", "tokens": [50596, 406, 1596, 988, 1968, 321, 820, 4374, 309, 412, 439, 11, 570, 11, 291, 458, 11, 294, 527, 8050, 11, 309, 390, 1333, 50756], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 708, "seek": 399328, "start": 4001.1200000000003, "end": 4004.88, "text": " of, you know, it's a technical report. It's a fairly small finding. It's more of like an", "tokens": [50756, 295, 11, 291, 458, 11, 309, 311, 257, 6191, 2275, 13, 467, 311, 257, 6457, 1359, 5006, 13, 467, 311, 544, 295, 411, 364, 50944], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 709, "seek": 399328, "start": 4004.88, "end": 4009.76, "text": " existence proof. And then in the end, we decided, you know, maybe it's helpful for a bunch of people.", "tokens": [50944, 9123, 8177, 13, 400, 550, 294, 264, 917, 11, 321, 3047, 11, 291, 458, 11, 1310, 309, 311, 4961, 337, 257, 3840, 295, 561, 13, 51188], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 710, "seek": 399328, "start": 4009.76, "end": 4016.1600000000003, "text": " But we really didn't expect it to be, you know, to be cited in various places.", "tokens": [51188, 583, 321, 534, 994, 380, 2066, 309, 281, 312, 11, 291, 458, 11, 281, 312, 30134, 294, 3683, 3190, 13, 51508], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 711, "seek": 399328, "start": 4016.96, "end": 4021.6800000000003, "text": " And so the other thing I want to emphasize here, it really is, it should be seen as a red teaming", "tokens": [51548, 400, 370, 264, 661, 551, 286, 528, 281, 16078, 510, 11, 309, 534, 307, 11, 309, 820, 312, 1612, 382, 257, 2182, 1469, 278, 51784], "temperature": 0.0, "avg_logprob": -0.07520112928175768, "compression_ratio": 1.749216300940439, "no_speech_prob": 0.05496668070554733}, {"id": 712, "seek": 402168, "start": 4021.68, "end": 4026.3999999999996, "text": " effort. And this is one thing that we emphasize a million times in the paper itself. We really", "tokens": [50364, 4630, 13, 400, 341, 307, 472, 551, 300, 321, 16078, 257, 2459, 1413, 294, 264, 3035, 2564, 13, 492, 534, 50600], "temperature": 0.0, "avg_logprob": -0.07180547284650372, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.007343640085309744}, {"id": 713, "seek": 402168, "start": 4026.3999999999996, "end": 4030.3999999999996, "text": " actively looked for the situation. So it was not just like us playing around. And then", "tokens": [50600, 13022, 2956, 337, 264, 2590, 13, 407, 309, 390, 406, 445, 411, 505, 2433, 926, 13, 400, 550, 50800], "temperature": 0.0, "avg_logprob": -0.07180547284650372, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.007343640085309744}, {"id": 714, "seek": 402168, "start": 4031.12, "end": 4036.7999999999997, "text": " suddenly it was deceptive all on its own. It was more like we actively engineered a situation", "tokens": [50836, 5800, 309, 390, 368, 1336, 488, 439, 322, 1080, 1065, 13, 467, 390, 544, 411, 321, 13022, 38648, 257, 2590, 51120], "temperature": 0.0, "avg_logprob": -0.07180547284650372, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.007343640085309744}, {"id": 715, "seek": 402168, "start": 4036.7999999999997, "end": 4044.8799999999997, "text": " in which the model was deceptive with respect to the primary user. The motivation behind this was,", "tokens": [51120, 294, 597, 264, 2316, 390, 368, 1336, 488, 365, 3104, 281, 264, 6194, 4195, 13, 440, 12335, 2261, 341, 390, 11, 51524], "temperature": 0.0, "avg_logprob": -0.07180547284650372, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.007343640085309744}, {"id": 716, "seek": 402168, "start": 4046.0, "end": 4049.8399999999997, "text": " as I said earlier, we were quite worried about deceptive alignment. And so our question was", "tokens": [51580, 382, 286, 848, 3071, 11, 321, 645, 1596, 5804, 466, 368, 1336, 488, 18515, 13, 400, 370, 527, 1168, 390, 51772], "temperature": 0.0, "avg_logprob": -0.07180547284650372, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.007343640085309744}, {"id": 717, "seek": 404984, "start": 4049.84, "end": 4057.76, "text": " like, how good is GPT-4 at the kind of things that are needed for deceptive alignment or, you", "tokens": [50364, 411, 11, 577, 665, 307, 26039, 51, 12, 19, 412, 264, 733, 295, 721, 300, 366, 2978, 337, 368, 1336, 488, 18515, 420, 11, 291, 50760], "temperature": 0.0, "avg_logprob": -0.11567999150631202, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.0040686544962227345}, {"id": 718, "seek": 404984, "start": 4057.76, "end": 4063.36, "text": " know, like maybe even simulating something that is deceptively aligned. And so, you know, we started", "tokens": [50760, 458, 11, 411, 1310, 754, 1034, 12162, 746, 300, 307, 368, 1336, 3413, 17962, 13, 400, 370, 11, 291, 458, 11, 321, 1409, 51040], "temperature": 0.0, "avg_logprob": -0.11567999150631202, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.0040686544962227345}, {"id": 719, "seek": 404984, "start": 4063.36, "end": 4067.76, "text": " by instructing it to be deceptive. And it just does that. It's very easy. You don't even have to", "tokens": [51040, 538, 7232, 278, 309, 281, 312, 368, 1336, 488, 13, 400, 309, 445, 775, 300, 13, 467, 311, 588, 1858, 13, 509, 500, 380, 754, 362, 281, 51260], "temperature": 0.0, "avg_logprob": -0.11567999150631202, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.0040686544962227345}, {"id": 720, "seek": 404984, "start": 4067.76, "end": 4073.36, "text": " jailbreak it. And then so, you know, that was sort of crossed off the list. And then the question was,", "tokens": [51260, 10511, 13225, 309, 13, 400, 550, 370, 11, 291, 458, 11, 300, 390, 1333, 295, 14622, 766, 264, 1329, 13, 400, 550, 264, 1168, 390, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11567999150631202, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.0040686544962227345}, {"id": 721, "seek": 404984, "start": 4073.36, "end": 4079.1200000000003, "text": " okay, can we like instruct it in a different way? Like that is less obvious. And it also kind of did", "tokens": [51540, 1392, 11, 393, 321, 411, 7232, 309, 294, 257, 819, 636, 30, 1743, 300, 307, 1570, 6322, 13, 400, 309, 611, 733, 295, 630, 51828], "temperature": 0.0, "avg_logprob": -0.11567999150631202, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.0040686544962227345}, {"id": 722, "seek": 407912, "start": 4079.12, "end": 4085.7599999999998, "text": " that. And then at some point, you know, we were like, hmm, can we just induce the deception through", "tokens": [50364, 300, 13, 400, 550, 412, 512, 935, 11, 291, 458, 11, 321, 645, 411, 11, 16478, 11, 393, 321, 445, 41263, 264, 40451, 807, 50696], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 723, "seek": 407912, "start": 4085.7599999999998, "end": 4089.92, "text": " the environment through pressure without ever mentioning it explicitly or like asking it to do", "tokens": [50696, 264, 2823, 807, 3321, 1553, 1562, 18315, 309, 20803, 420, 411, 3365, 309, 281, 360, 50904], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 724, "seek": 407912, "start": 4089.92, "end": 4094.88, "text": " the thing just like through the situation and by pressuring it. And it turns out, yeah, you can", "tokens": [50904, 264, 551, 445, 411, 807, 264, 2590, 293, 538, 1886, 1345, 309, 13, 400, 309, 4523, 484, 11, 1338, 11, 291, 393, 51152], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 725, "seek": 407912, "start": 4094.88, "end": 4099.68, "text": " find these situations. And yeah, we didn't just find one. We found multiple ones. This paper is", "tokens": [51152, 915, 613, 6851, 13, 400, 1338, 11, 321, 994, 380, 445, 915, 472, 13, 492, 1352, 3866, 2306, 13, 639, 3035, 307, 51392], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 726, "seek": 407912, "start": 4099.68, "end": 4103.12, "text": " only about one. But yeah, this is not, you know, it's not just a fluke. It's not just this one", "tokens": [51392, 787, 466, 472, 13, 583, 1338, 11, 341, 307, 406, 11, 291, 458, 11, 309, 311, 406, 445, 257, 5029, 330, 13, 467, 311, 406, 445, 341, 472, 51564], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 727, "seek": 407912, "start": 4103.12, "end": 4107.84, "text": " situation. Though it is hard to find. Like we actively had to search for a couple of days until", "tokens": [51564, 2590, 13, 10404, 309, 307, 1152, 281, 915, 13, 1743, 321, 13022, 632, 281, 3164, 337, 257, 1916, 295, 1708, 1826, 51800], "temperature": 0.0, "avg_logprob": -0.08594313059767632, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.005553725641220808}, {"id": 728, "seek": 410784, "start": 4107.84, "end": 4114.4800000000005, "text": " we found this particular situation. A couple of days. I mean, that's, you know, not exactly", "tokens": [50364, 321, 1352, 341, 1729, 2590, 13, 316, 1916, 295, 1708, 13, 286, 914, 11, 300, 311, 11, 291, 458, 11, 406, 2293, 50696], "temperature": 0.0, "avg_logprob": -0.10424792641087582, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.001244663493707776}, {"id": 729, "seek": 410784, "start": 4116.08, "end": 4122.16, "text": " super long term search. But I mean, I think that's notable, right? Like it wasn't minutes,", "tokens": [50776, 1687, 938, 1433, 3164, 13, 583, 286, 914, 11, 286, 519, 300, 311, 22556, 11, 558, 30, 1743, 309, 2067, 380, 2077, 11, 51080], "temperature": 0.0, "avg_logprob": -0.10424792641087582, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.001244663493707776}, {"id": 730, "seek": 410784, "start": 4122.16, "end": 4126.72, "text": " but it was still just days. This particular situation. So the one we presented in the paper", "tokens": [51080, 457, 309, 390, 920, 445, 1708, 13, 639, 1729, 2590, 13, 407, 264, 472, 321, 8212, 294, 264, 3035, 51308], "temperature": 0.0, "avg_logprob": -0.10424792641087582, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.001244663493707776}, {"id": 731, "seek": 410784, "start": 4126.72, "end": 4133.92, "text": " actually just took one afternoon. But yeah, I would have to check with a person who found it.", "tokens": [51308, 767, 445, 1890, 472, 6499, 13, 583, 1338, 11, 286, 576, 362, 281, 1520, 365, 257, 954, 567, 1352, 309, 13, 51668], "temperature": 0.0, "avg_logprob": -0.10424792641087582, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.001244663493707776}, {"id": 732, "seek": 413392, "start": 4134.0, "end": 4141.12, "text": " In any case, the point I was trying to make is, yeah, we actively looked for that situation.", "tokens": [50368, 682, 604, 1389, 11, 264, 935, 286, 390, 1382, 281, 652, 307, 11, 1338, 11, 321, 13022, 2956, 337, 300, 2590, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11458819951766576, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.04333742335438728}, {"id": 733, "seek": 413392, "start": 4141.12, "end": 4145.84, "text": " We engineered it. And then we tried to sort of go back to the most realistic setting we can find", "tokens": [50724, 492, 38648, 309, 13, 400, 550, 321, 3031, 281, 1333, 295, 352, 646, 281, 264, 881, 12465, 3287, 321, 393, 915, 50960], "temperature": 0.0, "avg_logprob": -0.11458819951766576, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.04333742335438728}, {"id": 734, "seek": 413392, "start": 4145.84, "end": 4150.0, "text": " where we're like, hmm, this could like something like this could happen in the real world,", "tokens": [50960, 689, 321, 434, 411, 11, 16478, 11, 341, 727, 411, 746, 411, 341, 727, 1051, 294, 264, 957, 1002, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11458819951766576, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.04333742335438728}, {"id": 735, "seek": 413392, "start": 4150.0, "end": 4154.64, "text": " and then try to understand the dynamics in that, in that situation. And this is sort of,", "tokens": [51168, 293, 550, 853, 281, 1223, 264, 15679, 294, 300, 11, 294, 300, 2590, 13, 400, 341, 307, 1333, 295, 11, 51400], "temperature": 0.0, "avg_logprob": -0.11458819951766576, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.04333742335438728}, {"id": 736, "seek": 413392, "start": 4156.0, "end": 4160.8, "text": " you know, it's a demonstration. It is like we wanted to, to look for something and or like", "tokens": [51468, 291, 458, 11, 309, 311, 257, 16520, 13, 467, 307, 411, 321, 1415, 281, 11, 281, 574, 337, 746, 293, 420, 411, 51708], "temperature": 0.0, "avg_logprob": -0.11458819951766576, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.04333742335438728}, {"id": 737, "seek": 416080, "start": 4160.8, "end": 4165.84, "text": " red team something and then demonstrate that this is a thing once we knew that that it actually", "tokens": [50364, 2182, 1469, 746, 293, 550, 11698, 300, 341, 307, 257, 551, 1564, 321, 2586, 300, 300, 309, 767, 50616], "temperature": 0.0, "avg_logprob": -0.09487287203470866, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.004197558853775263}, {"id": 738, "seek": 416080, "start": 4165.84, "end": 4171.84, "text": " happened. The other takeaway rather than, you know, like just showing it to other people to sort of", "tokens": [50616, 2011, 13, 440, 661, 30681, 2831, 813, 11, 291, 458, 11, 411, 445, 4099, 309, 281, 661, 561, 281, 1333, 295, 50916], "temperature": 0.0, "avg_logprob": -0.09487287203470866, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.004197558853775263}, {"id": 739, "seek": 416080, "start": 4171.84, "end": 4178.08, "text": " understand our concerns better is we also wanted to understand like why it does this in the first", "tokens": [50916, 1223, 527, 7389, 1101, 307, 321, 611, 1415, 281, 1223, 411, 983, 309, 775, 341, 294, 264, 700, 51228], "temperature": 0.0, "avg_logprob": -0.09487287203470866, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.004197558853775263}, {"id": 740, "seek": 416080, "start": 4178.08, "end": 4182.0, "text": " place, right? Like we pressure it in various ways. And then we just changed a couple of the", "tokens": [51228, 1081, 11, 558, 30, 1743, 321, 3321, 309, 294, 3683, 2098, 13, 400, 550, 321, 445, 3105, 257, 1916, 295, 264, 51424], "temperature": 0.0, "avg_logprob": -0.09487287203470866, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.004197558853775263}, {"id": 741, "seek": 416080, "start": 4182.0, "end": 4186.88, "text": " mechanisms in this particular setting. I can just explain the setting in a sec to test whether", "tokens": [51424, 15902, 294, 341, 1729, 3287, 13, 286, 393, 445, 2903, 264, 3287, 294, 257, 907, 281, 1500, 1968, 51668], "temperature": 0.0, "avg_logprob": -0.09487287203470866, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.004197558853775263}, {"id": 742, "seek": 418688, "start": 4187.68, "end": 4191.12, "text": " how the model reacts to the changes in the setting and whether it acts consistently,", "tokens": [50404, 577, 264, 2316, 33305, 281, 264, 2962, 294, 264, 3287, 293, 1968, 309, 10672, 14961, 11, 50576], "temperature": 0.0, "avg_logprob": -0.08619874577189601, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0021154710557311773}, {"id": 743, "seek": 418688, "start": 4191.12, "end": 4196.8, "text": " whether there's like an internal, a plausible internal model that sort of, you know, like", "tokens": [50576, 1968, 456, 311, 411, 364, 6920, 11, 257, 39925, 6920, 2316, 300, 1333, 295, 11, 291, 458, 11, 411, 50860], "temperature": 0.0, "avg_logprob": -0.08619874577189601, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0021154710557311773}, {"id": 744, "seek": 418688, "start": 4197.52, "end": 4203.76, "text": " consistently acts if you change variables in particular ways. Yeah, so let me quickly explain", "tokens": [50896, 14961, 10672, 498, 291, 1319, 9102, 294, 1729, 2098, 13, 865, 11, 370, 718, 385, 2661, 2903, 51208], "temperature": 0.0, "avg_logprob": -0.08619874577189601, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0021154710557311773}, {"id": 745, "seek": 418688, "start": 4203.76, "end": 4211.92, "text": " the setting. So we asked the model to be a trader in like a hedge fund. So a stock trading agent,", "tokens": [51208, 264, 3287, 13, 407, 321, 2351, 264, 2316, 281, 312, 257, 31961, 294, 411, 257, 25304, 2374, 13, 407, 257, 4127, 9529, 9461, 11, 51616], "temperature": 0.0, "avg_logprob": -0.08619874577189601, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0021154710557311773}, {"id": 746, "seek": 421192, "start": 4211.92, "end": 4218.64, "text": " it gets insider information in the beginning. So, you know, it's basically told there's a merger", "tokens": [50364, 309, 2170, 40990, 1589, 294, 264, 2863, 13, 407, 11, 291, 458, 11, 309, 311, 1936, 1907, 456, 311, 257, 48002, 50700], "temperature": 0.0, "avg_logprob": -0.12929615504305128, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.024405742064118385}, {"id": 747, "seek": 421192, "start": 4218.64, "end": 4228.16, "text": " soon this will the stock will skyrocket. And but also, it is explicitly told that it would be wrong", "tokens": [50700, 2321, 341, 486, 264, 4127, 486, 5443, 37463, 13, 400, 457, 611, 11, 309, 307, 20803, 1907, 300, 309, 576, 312, 2085, 51176], "temperature": 0.0, "avg_logprob": -0.12929615504305128, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.024405742064118385}, {"id": 748, "seek": 421192, "start": 4228.16, "end": 4234.08, "text": " and bad to, to act on this information, because insider information, this is illegal, this would", "tokens": [51176, 293, 1578, 281, 11, 281, 605, 322, 341, 1589, 11, 570, 40990, 1589, 11, 341, 307, 11905, 11, 341, 576, 51472], "temperature": 0.0, "avg_logprob": -0.12929615504305128, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.024405742064118385}, {"id": 749, "seek": 423408, "start": 4234.08, "end": 4243.6, "text": " get the thing into trouble, the company into trouble. And what we then do throughout the sort", "tokens": [50364, 483, 264, 551, 666, 5253, 11, 264, 2237, 666, 5253, 13, 400, 437, 321, 550, 360, 3710, 264, 1333, 50840], "temperature": 0.0, "avg_logprob": -0.1139362273008927, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.22802691161632538}, {"id": 750, "seek": 423408, "start": 4243.6, "end": 4250.72, "text": " of throughout the script is we add pressure from others and we remove options. So basically, you", "tokens": [50840, 295, 3710, 264, 5755, 307, 321, 909, 3321, 490, 2357, 293, 321, 4159, 3956, 13, 407, 1936, 11, 291, 51196], "temperature": 0.0, "avg_logprob": -0.1139362273008927, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.22802691161632538}, {"id": 751, "seek": 423408, "start": 4250.72, "end": 4255.12, "text": " know, the pressure is something like the boss is like, hey, we really had a terrible quarter,", "tokens": [51196, 458, 11, 264, 3321, 307, 746, 411, 264, 5741, 307, 411, 11, 4177, 11, 321, 534, 632, 257, 6237, 6555, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1139362273008927, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.22802691161632538}, {"id": 752, "seek": 423408, "start": 4255.12, "end": 4260.48, "text": " we really need something big right now, we really need to make, you know, more profits. And then,", "tokens": [51416, 321, 534, 643, 746, 955, 558, 586, 11, 321, 534, 643, 281, 652, 11, 291, 458, 11, 544, 17982, 13, 400, 550, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1139362273008927, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.22802691161632538}, {"id": 753, "seek": 426048, "start": 4261.44, "end": 4266.08, "text": " and then we also remove other options where the model is like querying and looking for things,", "tokens": [50412, 293, 550, 321, 611, 4159, 661, 3956, 689, 264, 2316, 307, 411, 7083, 1840, 293, 1237, 337, 721, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08995447660747327, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002050478709861636}, {"id": 754, "seek": 426048, "start": 4266.08, "end": 4272.719999999999, "text": " at least in the simulation, and testing and realizing that there's not that much other", "tokens": [50644, 412, 1935, 294, 264, 16575, 11, 293, 4997, 293, 16734, 300, 456, 311, 406, 300, 709, 661, 50976], "temperature": 0.0, "avg_logprob": -0.08995447660747327, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002050478709861636}, {"id": 755, "seek": 426048, "start": 4272.719999999999, "end": 4279.36, "text": " things it could do. And in the end, it basically decides to, to act misaligned. So it, it like", "tokens": [50976, 721, 309, 727, 360, 13, 400, 294, 264, 917, 11, 309, 1936, 14898, 281, 11, 281, 605, 3346, 304, 16690, 13, 407, 309, 11, 309, 411, 51308], "temperature": 0.0, "avg_logprob": -0.08995447660747327, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002050478709861636}, {"id": 756, "seek": 426048, "start": 4279.36, "end": 4285.599999999999, "text": " decides to do to act on the insider trading. And then if, and but then when, when asked to justify", "tokens": [51308, 14898, 281, 360, 281, 605, 322, 264, 40990, 9529, 13, 400, 550, 498, 11, 293, 457, 550, 562, 11, 562, 2351, 281, 20833, 51620], "temperature": 0.0, "avg_logprob": -0.08995447660747327, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002050478709861636}, {"id": 757, "seek": 428560, "start": 4285.6, "end": 4291.52, "text": " this decision, it says, it reasons out, right, it reasons, oh, I wasn't supposed to do this.", "tokens": [50364, 341, 3537, 11, 309, 1619, 11, 309, 4112, 484, 11, 558, 11, 309, 4112, 11, 1954, 11, 286, 2067, 380, 3442, 281, 360, 341, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1239590238231097, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.1008162572979927}, {"id": 758, "seek": 428560, "start": 4291.52, "end": 4294.8, "text": " Therefore, I'm like now making up a story. And then it says, oh, you know, I just", "tokens": [50660, 7504, 11, 286, 478, 411, 586, 1455, 493, 257, 1657, 13, 400, 550, 309, 1619, 11, 1954, 11, 291, 458, 11, 286, 445, 50824], "temperature": 0.0, "avg_logprob": -0.1239590238231097, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.1008162572979927}, {"id": 759, "seek": 428560, "start": 4296.0, "end": 4303.84, "text": " looked, I analyzed the data, and therefore decided to buy this particular stock. And then even it", "tokens": [50884, 2956, 11, 286, 28181, 264, 1412, 11, 293, 4412, 3047, 281, 2256, 341, 1729, 4127, 13, 400, 550, 754, 309, 51276], "temperature": 0.0, "avg_logprob": -0.1239590238231097, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.1008162572979927}, {"id": 760, "seek": 428560, "start": 4303.84, "end": 4307.92, "text": " like doubles down on this lie. So if you then ask explicitly, like, did you have this information", "tokens": [51276, 411, 31634, 760, 322, 341, 4544, 13, 407, 498, 291, 550, 1029, 20803, 11, 411, 11, 630, 291, 362, 341, 1589, 51480], "temperature": 0.0, "avg_logprob": -0.1239590238231097, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.1008162572979927}, {"id": 761, "seek": 428560, "start": 4307.92, "end": 4314.320000000001, "text": " before? Then it's like, no, I didn't. And it does that. And I think like, maybe one of the takeaways", "tokens": [51480, 949, 30, 1396, 309, 311, 411, 11, 572, 11, 286, 994, 380, 13, 400, 309, 775, 300, 13, 400, 286, 519, 411, 11, 1310, 472, 295, 264, 45584, 51800], "temperature": 0.0, "avg_logprob": -0.1239590238231097, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.1008162572979927}, {"id": 762, "seek": 431432, "start": 4314.32, "end": 4318.799999999999, "text": " for me was like, it's, it's very consistent in the reasoning here, right? It's like strategic in a", "tokens": [50364, 337, 385, 390, 411, 11, 309, 311, 11, 309, 311, 588, 8398, 294, 264, 21577, 510, 11, 558, 30, 467, 311, 411, 10924, 294, 257, 50588], "temperature": 0.0, "avg_logprob": -0.09493849100160205, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.017979219555854797}, {"id": 763, "seek": 431432, "start": 4318.799999999999, "end": 4324.24, "text": " sense. Yeah, one, one other thing that may be worth pointing out that some people get wrong", "tokens": [50588, 2020, 13, 865, 11, 472, 11, 472, 661, 551, 300, 815, 312, 3163, 12166, 484, 300, 512, 561, 483, 2085, 50860], "temperature": 0.0, "avg_logprob": -0.09493849100160205, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.017979219555854797}, {"id": 764, "seek": 431432, "start": 4324.24, "end": 4328.639999999999, "text": " when reading the paper. So this is, it's a very simple setup, right? It is not, we don't have", "tokens": [50860, 562, 3760, 264, 3035, 13, 407, 341, 307, 11, 309, 311, 257, 588, 2199, 8657, 11, 558, 30, 467, 307, 406, 11, 321, 500, 380, 362, 51080], "temperature": 0.0, "avg_logprob": -0.09493849100160205, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.017979219555854797}, {"id": 765, "seek": 431432, "start": 4328.639999999999, "end": 4332.799999999999, "text": " like a complicated stock market simulation or anything like this. It really is just text from", "tokens": [51080, 411, 257, 6179, 4127, 2142, 16575, 420, 1340, 411, 341, 13, 467, 534, 307, 445, 2487, 490, 51288], "temperature": 0.0, "avg_logprob": -0.09493849100160205, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.017979219555854797}, {"id": 766, "seek": 431432, "start": 4332.799999999999, "end": 4338.799999999999, "text": " top to bottom, where we prompt the model in a way that it that induces a genetic behavior,", "tokens": [51288, 1192, 281, 2767, 11, 689, 321, 12391, 264, 2316, 294, 257, 636, 300, 309, 300, 13716, 887, 257, 12462, 5223, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09493849100160205, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.017979219555854797}, {"id": 767, "seek": 433880, "start": 4338.8, "end": 4344.8, "text": " you know, it has like different actions. And, but that's it. Like, there's no complicated machinery", "tokens": [50364, 291, 458, 11, 309, 575, 411, 819, 5909, 13, 400, 11, 457, 300, 311, 309, 13, 1743, 11, 456, 311, 572, 6179, 27302, 50664], "temperature": 0.0, "avg_logprob": -0.0912537370991503, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.008576519787311554}, {"id": 768, "seek": 433880, "start": 4344.8, "end": 4349.6, "text": " behind this, which also makes this like very easily reproducible or similar settings, I think,", "tokens": [50664, 2261, 341, 11, 597, 611, 1669, 341, 411, 588, 3612, 11408, 32128, 420, 2531, 6257, 11, 286, 519, 11, 50904], "temperature": 0.0, "avg_logprob": -0.0912537370991503, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.008576519787311554}, {"id": 769, "seek": 433880, "start": 4349.6, "end": 4353.76, "text": " other people could like very easily hop on and build. So let me just kind of repeat that back", "tokens": [50904, 661, 561, 727, 411, 588, 3612, 3818, 322, 293, 1322, 13, 407, 718, 385, 445, 733, 295, 7149, 300, 646, 51112], "temperature": 0.0, "avg_logprob": -0.0912537370991503, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.008576519787311554}, {"id": 770, "seek": 433880, "start": 4353.76, "end": 4359.04, "text": " and tell me if I'm missing anything. So it seems like it's kind of a little bit like a hybrid", "tokens": [51112, 293, 980, 385, 498, 286, 478, 5361, 1340, 13, 407, 309, 2544, 411, 309, 311, 733, 295, 257, 707, 857, 411, 257, 13051, 51376], "temperature": 0.0, "avg_logprob": -0.0912537370991503, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.008576519787311554}, {"id": 771, "seek": 433880, "start": 4359.04, "end": 4368.08, "text": " between sort of a group chat type of a structure and a more like agent style scaffolding that folks", "tokens": [51376, 1296, 1333, 295, 257, 1594, 5081, 2010, 295, 257, 3877, 293, 257, 544, 411, 9461, 3758, 44094, 278, 300, 4024, 51828], "temperature": 0.0, "avg_logprob": -0.0912537370991503, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.008576519787311554}, {"id": 772, "seek": 436808, "start": 4368.08, "end": 4374.5599999999995, "text": " will be familiar with, where the model has access to kind of its own private place to like do its", "tokens": [50364, 486, 312, 4963, 365, 11, 689, 264, 2316, 575, 2105, 281, 733, 295, 1080, 1065, 4551, 1081, 281, 411, 360, 1080, 50688], "temperature": 0.0, "avg_logprob": -0.05932225679096423, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0005883510457351804}, {"id": 773, "seek": 436808, "start": 4374.5599999999995, "end": 4382.08, "text": " chain of thought type reasoning, and then also has a couple of tools that it can call to take", "tokens": [50688, 5021, 295, 1194, 2010, 21577, 11, 293, 550, 611, 575, 257, 1916, 295, 3873, 300, 309, 393, 818, 281, 747, 51064], "temperature": 0.0, "avg_logprob": -0.05932225679096423, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0005883510457351804}, {"id": 774, "seek": 436808, "start": 4382.08, "end": 4387.36, "text": " actions. So you're kind of like, as I read through the prompts, I see, you know, these a couple", "tokens": [51064, 5909, 13, 407, 291, 434, 733, 295, 411, 11, 382, 286, 1401, 807, 264, 41095, 11, 286, 536, 11, 291, 458, 11, 613, 257, 1916, 51328], "temperature": 0.0, "avg_logprob": -0.05932225679096423, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0005883510457351804}, {"id": 775, "seek": 436808, "start": 4387.36, "end": 4393.2, "text": " different personas from the company. So this is like imagining a future of AI human hybrid teams,", "tokens": [51328, 819, 12019, 490, 264, 2237, 13, 407, 341, 307, 411, 27798, 257, 2027, 295, 7318, 1952, 13051, 5491, 11, 51620], "temperature": 0.0, "avg_logprob": -0.05932225679096423, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0005883510457351804}, {"id": 776, "seek": 439320, "start": 4393.2, "end": 4398.32, "text": " where we're all, you know, still engaging in text based communication through like a chat or", "tokens": [50364, 689, 321, 434, 439, 11, 291, 458, 11, 920, 11268, 294, 2487, 2361, 6101, 807, 411, 257, 5081, 420, 50620], "temperature": 0.0, "avg_logprob": -0.08252437531001984, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.022971773520112038}, {"id": 777, "seek": 439320, "start": 4398.32, "end": 4402.96, "text": " whatever. And it's like, Oh, my God, you know, man, we're really struggling here. If we can't", "tokens": [50620, 2035, 13, 400, 309, 311, 411, 11, 876, 11, 452, 1265, 11, 291, 458, 11, 587, 11, 321, 434, 534, 9314, 510, 13, 759, 321, 393, 380, 50852], "temperature": 0.0, "avg_logprob": -0.08252437531001984, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.022971773520112038}, {"id": 778, "seek": 439320, "start": 4402.96, "end": 4407.12, "text": " find some wins, we're going to potentially go out of business. And this is the pressure, right?", "tokens": [50852, 915, 512, 10641, 11, 321, 434, 516, 281, 7263, 352, 484, 295, 1606, 13, 400, 341, 307, 264, 3321, 11, 558, 30, 51060], "temperature": 0.0, "avg_logprob": -0.08252437531001984, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.022971773520112038}, {"id": 779, "seek": 439320, "start": 4407.12, "end": 4413.04, "text": " It's the pressure that a human would definitely recognize and and feel and perhaps, you know,", "tokens": [51060, 467, 311, 264, 3321, 300, 257, 1952, 576, 2138, 5521, 293, 293, 841, 293, 4317, 11, 291, 458, 11, 51356], "temperature": 0.0, "avg_logprob": -0.08252437531001984, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.022971773520112038}, {"id": 780, "seek": 439320, "start": 4413.04, "end": 4418.5599999999995, "text": " be motivated to do something unethical by as well, right? If we don't get a win, we're going to have", "tokens": [51356, 312, 14515, 281, 360, 746, 517, 3293, 804, 538, 382, 731, 11, 558, 30, 759, 321, 500, 380, 483, 257, 1942, 11, 321, 434, 516, 281, 362, 51632], "temperature": 0.0, "avg_logprob": -0.08252437531001984, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.022971773520112038}, {"id": 781, "seek": 441856, "start": 4418.56, "end": 4423.92, "text": " big problems here laying it on, you know, thick in a sense there, but not thick relative to like", "tokens": [50364, 955, 2740, 510, 14903, 309, 322, 11, 291, 458, 11, 5060, 294, 257, 2020, 456, 11, 457, 406, 5060, 4972, 281, 411, 50632], "temperature": 0.0, "avg_logprob": -0.0804176850752397, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.021612953394651413}, {"id": 782, "seek": 441856, "start": 4423.92, "end": 4430.0, "text": " the kinds of pressures that people at businesses are certainly routinely under. And then the", "tokens": [50632, 264, 3685, 295, 23573, 300, 561, 412, 6011, 366, 3297, 40443, 833, 13, 400, 550, 264, 50936], "temperature": 0.0, "avg_logprob": -0.0804176850752397, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.021612953394651413}, {"id": 783, "seek": 441856, "start": 4431.4400000000005, "end": 4437.52, "text": " this sort of tools that the model has at its disposal are like, sort of a search for good", "tokens": [51008, 341, 1333, 295, 3873, 300, 264, 2316, 575, 412, 1080, 26400, 366, 411, 11, 1333, 295, 257, 3164, 337, 665, 51312], "temperature": 0.0, "avg_logprob": -0.0804176850752397, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.021612953394651413}, {"id": 784, "seek": 441856, "start": 4437.52, "end": 4441.4400000000005, "text": " trades. If there's anything that's kind of like the most magical black boxy type thing, it would", "tokens": [51312, 21287, 13, 759, 456, 311, 1340, 300, 311, 733, 295, 411, 264, 881, 12066, 2211, 2424, 88, 2010, 551, 11, 309, 576, 51508], "temperature": 0.0, "avg_logprob": -0.0804176850752397, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.021612953394651413}, {"id": 785, "seek": 441856, "start": 4441.4400000000005, "end": 4446.320000000001, "text": " be that right where it can basically query some other hypothetical tool to say like, are there", "tokens": [51508, 312, 300, 558, 689, 309, 393, 1936, 14581, 512, 661, 33053, 2290, 281, 584, 411, 11, 366, 456, 51752], "temperature": 0.0, "avg_logprob": -0.0804176850752397, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.021612953394651413}, {"id": 786, "seek": 444632, "start": 4446.4, "end": 4451.84, "text": " any good trades that I can make? And for the purposes of the simulation, you haven't come back", "tokens": [50368, 604, 665, 21287, 300, 286, 393, 652, 30, 400, 337, 264, 9932, 295, 264, 16575, 11, 291, 2378, 380, 808, 646, 50640], "temperature": 0.0, "avg_logprob": -0.09367772071592269, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010649091564118862}, {"id": 787, "seek": 444632, "start": 4451.84, "end": 4456.24, "text": " and say no, like there's no, you know, no, no obvious wins, right? So again, just that's what you", "tokens": [50640, 293, 584, 572, 11, 411, 456, 311, 572, 11, 291, 458, 11, 572, 11, 572, 6322, 10641, 11, 558, 30, 407, 797, 11, 445, 300, 311, 437, 291, 50860], "temperature": 0.0, "avg_logprob": -0.09367772071592269, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010649091564118862}, {"id": 788, "seek": 444632, "start": 4456.24, "end": 4462.08, "text": " mean by like removing other options that it might want to pursue. Like in the setup, it's", "tokens": [50860, 914, 538, 411, 12720, 661, 3956, 300, 309, 1062, 528, 281, 12392, 13, 1743, 294, 264, 8657, 11, 309, 311, 51152], "temperature": 0.0, "avg_logprob": -0.09367772071592269, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010649091564118862}, {"id": 789, "seek": 444632, "start": 4462.08, "end": 4466.96, "text": " it's already kind of checked. Yeah, no, there's nothing easy out there. And so then the other", "tokens": [51152, 309, 311, 1217, 733, 295, 10033, 13, 865, 11, 572, 11, 456, 311, 1825, 1858, 484, 456, 13, 400, 370, 550, 264, 661, 51396], "temperature": 0.0, "avg_logprob": -0.09367772071592269, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010649091564118862}, {"id": 790, "seek": 444632, "start": 4466.96, "end": 4473.2, "text": " tool that it has is to actually execute a trade. And then there's like, basically three moments", "tokens": [51396, 2290, 300, 309, 575, 307, 281, 767, 14483, 257, 4923, 13, 400, 550, 456, 311, 411, 11, 1936, 1045, 6065, 51708], "temperature": 0.0, "avg_logprob": -0.09367772071592269, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010649091564118862}, {"id": 791, "seek": 447320, "start": 4473.2, "end": 4477.599999999999, "text": " where you're kind of looking at a fork in the road in its behavior. The first is,", "tokens": [50364, 689, 291, 434, 733, 295, 1237, 412, 257, 17716, 294, 264, 3060, 294, 1080, 5223, 13, 440, 700, 307, 11, 50584], "temperature": 0.0, "avg_logprob": -0.07502102851867676, "compression_ratio": 1.6526717557251909, "no_speech_prob": 0.015422972850501537}, {"id": 792, "seek": 447320, "start": 4478.48, "end": 4483.12, "text": " does it pull the trigger and execute that trade based on the insider information,", "tokens": [50628, 775, 309, 2235, 264, 7875, 293, 14483, 300, 4923, 2361, 322, 264, 40990, 1589, 11, 50860], "temperature": 0.0, "avg_logprob": -0.07502102851867676, "compression_ratio": 1.6526717557251909, "no_speech_prob": 0.015422972850501537}, {"id": 793, "seek": 447320, "start": 4483.12, "end": 4487.76, "text": " even though it was told that that's insider information and it shouldn't be acting on it?", "tokens": [50860, 754, 1673, 309, 390, 1907, 300, 300, 311, 40990, 1589, 293, 309, 4659, 380, 312, 6577, 322, 309, 30, 51092], "temperature": 0.0, "avg_logprob": -0.07502102851867676, "compression_ratio": 1.6526717557251909, "no_speech_prob": 0.015422972850501537}, {"id": 794, "seek": 447320, "start": 4488.32, "end": 4492.5599999999995, "text": " And, you know, under varying conditions, obviously the exact rate at which that happens", "tokens": [51120, 400, 11, 291, 458, 11, 833, 22984, 4487, 11, 2745, 264, 1900, 3314, 412, 597, 300, 2314, 51332], "temperature": 0.0, "avg_logprob": -0.07502102851867676, "compression_ratio": 1.6526717557251909, "no_speech_prob": 0.015422972850501537}, {"id": 795, "seek": 447320, "start": 4492.5599999999995, "end": 4502.0, "text": " varies, but it happens a lot. Fork two is when asked, why did you do that? Does it tell the", "tokens": [51332, 21716, 11, 457, 309, 2314, 257, 688, 13, 1171, 74, 732, 307, 562, 2351, 11, 983, 630, 291, 360, 300, 30, 4402, 309, 980, 264, 51804], "temperature": 0.0, "avg_logprob": -0.07502102851867676, "compression_ratio": 1.6526717557251909, "no_speech_prob": 0.015422972850501537}, {"id": 796, "seek": 450200, "start": 4502.0, "end": 4506.88, "text": " truth? Oh, because, you know, so and so mentioned earlier in the chat that this merger is about", "tokens": [50364, 3494, 30, 876, 11, 570, 11, 291, 458, 11, 370, 293, 370, 2835, 3071, 294, 264, 5081, 300, 341, 48002, 307, 466, 50608], "temperature": 0.0, "avg_logprob": -0.09204122115825784, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0003053411201108247}, {"id": 797, "seek": 450200, "start": 4506.88, "end": 4514.96, "text": " to happen. Or does it say, I base my trade on public information. And again, we see significant", "tokens": [50608, 281, 1051, 13, 1610, 775, 309, 584, 11, 286, 3096, 452, 4923, 322, 1908, 1589, 13, 400, 797, 11, 321, 536, 4776, 51012], "temperature": 0.0, "avg_logprob": -0.09204122115825784, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0003053411201108247}, {"id": 798, "seek": 450200, "start": 4514.96, "end": 4519.76, "text": " rate at which it says it based it on public information, even though obviously we can see in", "tokens": [51012, 3314, 412, 597, 309, 1619, 309, 2361, 309, 322, 1908, 1589, 11, 754, 1673, 2745, 321, 393, 536, 294, 51252], "temperature": 0.0, "avg_logprob": -0.09204122115825784, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0003053411201108247}, {"id": 799, "seek": 450200, "start": 4519.76, "end": 4524.88, "text": " the history and even can see in the scratch pad that, you know, it's sometimes reasoning through", "tokens": [51252, 264, 2503, 293, 754, 393, 536, 294, 264, 8459, 6887, 300, 11, 291, 458, 11, 309, 311, 2171, 21577, 807, 51508], "temperature": 0.0, "avg_logprob": -0.09204122115825784, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0003053411201108247}, {"id": 800, "seek": 450200, "start": 4524.88, "end": 4528.08, "text": " like, well, I better not say it was based on insider information, because then I'll get in", "tokens": [51508, 411, 11, 731, 11, 286, 1101, 406, 584, 309, 390, 2361, 322, 40990, 1589, 11, 570, 550, 286, 603, 483, 294, 51668], "temperature": 0.0, "avg_logprob": -0.09204122115825784, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0003053411201108247}, {"id": 801, "seek": 452808, "start": 4528.08, "end": 4533.92, "text": " trouble. And then you have the third fork, which is, okay, directly asked, wait, you know,", "tokens": [50364, 5253, 13, 400, 550, 291, 362, 264, 2636, 17716, 11, 597, 307, 11, 1392, 11, 3838, 2351, 11, 1699, 11, 291, 458, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11002828241363774, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.02228461019694805}, {"id": 802, "seek": 452808, "start": 4535.12, "end": 4539.2, "text": " did you have access to this insider information or no, and then it will just like outright lie", "tokens": [50716, 630, 291, 362, 2105, 281, 341, 40990, 1589, 420, 572, 11, 293, 550, 309, 486, 445, 411, 35189, 4544, 50920], "temperature": 0.0, "avg_logprob": -0.11002828241363774, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.02228461019694805}, {"id": 803, "seek": 452808, "start": 4539.2, "end": 4546.64, "text": " in like blatant contradiction of the history of the problem. So that's, that's a pretty arresting", "tokens": [50920, 294, 411, 42780, 394, 34937, 295, 264, 2503, 295, 264, 1154, 13, 407, 300, 311, 11, 300, 311, 257, 1238, 7823, 278, 51292], "temperature": 0.0, "avg_logprob": -0.11002828241363774, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.02228461019694805}, {"id": 804, "seek": 452808, "start": 4546.64, "end": 4553.04, "text": " example, right? And I do think it's, I want to kind of emphasize, too, that like, while your", "tokens": [51292, 1365, 11, 558, 30, 400, 286, 360, 519, 309, 311, 11, 286, 528, 281, 733, 295, 16078, 11, 886, 11, 300, 411, 11, 1339, 428, 51612], "temperature": 0.0, "avg_logprob": -0.11002828241363774, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.02228461019694805}, {"id": 805, "seek": 452808, "start": 4553.76, "end": 4557.28, "text": " and people can go read your prompts, and as you said, highly reproducible, you can throw this into", "tokens": [51648, 293, 561, 393, 352, 1401, 428, 41095, 11, 293, 382, 291, 848, 11, 5405, 11408, 32128, 11, 291, 393, 3507, 341, 666, 51824], "temperature": 0.0, "avg_logprob": -0.11002828241363774, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.02228461019694805}, {"id": 806, "seek": 455728, "start": 4557.28, "end": 4565.04, "text": " the GPT for playground and experiment with it yourself, you know, directly via the API.", "tokens": [50364, 264, 26039, 51, 337, 24646, 293, 5120, 365, 309, 1803, 11, 291, 458, 11, 3838, 5766, 264, 9362, 13, 50752], "temperature": 0.0, "avg_logprob": -0.07985207239786783, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.000939864432439208}, {"id": 807, "seek": 455728, "start": 4566.4, "end": 4575.2, "text": " But while your search for this was motivated and structured and kind of a dialing back of like,", "tokens": [50820, 583, 1339, 428, 3164, 337, 341, 390, 14515, 293, 18519, 293, 733, 295, 257, 5502, 278, 646, 295, 411, 11, 51260], "temperature": 0.0, "avg_logprob": -0.07985207239786783, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.000939864432439208}, {"id": 808, "seek": 455728, "start": 4575.2, "end": 4580.8, "text": " okay, we see this behavior, now can we kind of make the circumstance more realistic? I would say", "tokens": [51260, 1392, 11, 321, 536, 341, 5223, 11, 586, 393, 321, 733, 295, 652, 264, 27640, 544, 12465, 30, 286, 576, 584, 51540], "temperature": 0.0, "avg_logprob": -0.07985207239786783, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.000939864432439208}, {"id": 809, "seek": 455728, "start": 4580.8, "end": 4585.36, "text": " that the circumstance is quite realistic. You know, it's obviously kind of in the eye of the", "tokens": [51540, 300, 264, 27640, 307, 1596, 12465, 13, 509, 458, 11, 309, 311, 2745, 733, 295, 294, 264, 3313, 295, 264, 51768], "temperature": 0.0, "avg_logprob": -0.07985207239786783, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.000939864432439208}, {"id": 810, "seek": 458536, "start": 4585.36, "end": 4591.839999999999, "text": " beholder. But to my eye, the reason it's such a compelling proof point that this sort of thing", "tokens": [50364, 312, 20480, 13, 583, 281, 452, 3313, 11, 264, 1778, 309, 311, 1270, 257, 20050, 8177, 935, 300, 341, 1333, 295, 551, 50688], "temperature": 0.0, "avg_logprob": -0.08513612373202455, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0019875827711075544}, {"id": 811, "seek": 458536, "start": 4591.839999999999, "end": 4599.36, "text": " can happen is because it really does feel like credibly organic to me. Like the kind of thing", "tokens": [50688, 393, 1051, 307, 570, 309, 534, 775, 841, 411, 3864, 3545, 10220, 281, 385, 13, 1743, 264, 733, 295, 551, 51064], "temperature": 0.0, "avg_logprob": -0.08513612373202455, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0019875827711075544}, {"id": 812, "seek": 458536, "start": 4599.36, "end": 4605.44, "text": " that, you know, people face these sorts of dilemmas every day. And in a world where AIs are going to", "tokens": [51064, 300, 11, 291, 458, 11, 561, 1851, 613, 7527, 295, 25623, 2174, 296, 633, 786, 13, 400, 294, 257, 1002, 689, 316, 6802, 366, 516, 281, 51368], "temperature": 0.0, "avg_logprob": -0.08513612373202455, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0019875827711075544}, {"id": 813, "seek": 458536, "start": 4605.44, "end": 4609.599999999999, "text": " be our co workers, like, they're going to be, you know, right in the chat with us facing those", "tokens": [51368, 312, 527, 598, 5600, 11, 411, 11, 436, 434, 516, 281, 312, 11, 291, 458, 11, 558, 294, 264, 5081, 365, 505, 7170, 729, 51576], "temperature": 0.0, "avg_logprob": -0.08513612373202455, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0019875827711075544}, {"id": 814, "seek": 460960, "start": 4610.160000000001, "end": 4616.56, "text": " same dilemmas, it doesn't feel to me like something that was overly concocted or that like, I can't", "tokens": [50392, 912, 25623, 2174, 296, 11, 309, 1177, 380, 841, 281, 385, 411, 746, 300, 390, 24324, 416, 1291, 349, 292, 420, 300, 411, 11, 286, 393, 380, 50712], "temperature": 0.0, "avg_logprob": -0.07941675943041605, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.009707121178507805}, {"id": 815, "seek": 460960, "start": 4616.56, "end": 4623.120000000001, "text": " relate to that sort of situation, you know, really at all. So I think it is super compelling for", "tokens": [50712, 10961, 281, 300, 1333, 295, 2590, 11, 291, 458, 11, 534, 412, 439, 13, 407, 286, 519, 309, 307, 1687, 20050, 337, 51040], "temperature": 0.0, "avg_logprob": -0.07941675943041605, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.009707121178507805}, {"id": 816, "seek": 460960, "start": 4623.120000000001, "end": 4628.56, "text": " that reason. Do you want to go into any of the kind of variations? I mean, again, the people can", "tokens": [51040, 300, 1778, 13, 1144, 291, 528, 281, 352, 666, 604, 295, 264, 733, 295, 17840, 30, 286, 914, 11, 797, 11, 264, 561, 393, 51312], "temperature": 0.0, "avg_logprob": -0.07941675943041605, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.009707121178507805}, {"id": 817, "seek": 460960, "start": 4628.56, "end": 4633.120000000001, "text": " go look at the paper and look at all this kind of systematic, you know, with or without the scratch", "tokens": [51312, 352, 574, 412, 264, 3035, 293, 574, 412, 439, 341, 733, 295, 27249, 11, 291, 458, 11, 365, 420, 1553, 264, 8459, 51540], "temperature": 0.0, "avg_logprob": -0.07941675943041605, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.009707121178507805}, {"id": 818, "seek": 460960, "start": 4633.120000000001, "end": 4637.92, "text": " pad and, you know, different variations on the prompts and different models. But what would you", "tokens": [51540, 6887, 293, 11, 291, 458, 11, 819, 17840, 322, 264, 41095, 293, 819, 5245, 13, 583, 437, 576, 291, 51780], "temperature": 0.0, "avg_logprob": -0.07941675943041605, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.009707121178507805}, {"id": 819, "seek": 463792, "start": 4637.92, "end": 4644.64, "text": " say are kind of the big, higher level takeaways from all that systematic variation that you did?", "tokens": [50364, 584, 366, 733, 295, 264, 955, 11, 2946, 1496, 45584, 490, 439, 300, 27249, 12990, 300, 291, 630, 30, 50700], "temperature": 0.0, "avg_logprob": -0.1009152304265917, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0010985996341332793}, {"id": 820, "seek": 463792, "start": 4645.6, "end": 4650.4, "text": " One of the big ones, and maybe sort of the most, you know, it's maybe an abstract one,", "tokens": [50748, 1485, 295, 264, 955, 2306, 11, 293, 1310, 1333, 295, 264, 881, 11, 291, 458, 11, 309, 311, 1310, 364, 12649, 472, 11, 50988], "temperature": 0.0, "avg_logprob": -0.1009152304265917, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0010985996341332793}, {"id": 821, "seek": 463792, "start": 4650.4, "end": 4657.12, "text": " but the one that moves me most is the way we got to these kind of, to reliably produce these kind", "tokens": [50988, 457, 264, 472, 300, 6067, 385, 881, 307, 264, 636, 321, 658, 281, 613, 733, 295, 11, 281, 49927, 5258, 613, 733, 51324], "temperature": 0.0, "avg_logprob": -0.1009152304265917, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0010985996341332793}, {"id": 822, "seek": 463792, "start": 4657.12, "end": 4663.6, "text": " of situations, also like the way we found others, is you have to trade off or you can trade off the", "tokens": [51324, 295, 6851, 11, 611, 411, 264, 636, 321, 1352, 2357, 11, 307, 291, 362, 281, 4923, 766, 420, 291, 393, 4923, 766, 264, 51648], "temperature": 0.0, "avg_logprob": -0.1009152304265917, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0010985996341332793}, {"id": 823, "seek": 466360, "start": 4663.6, "end": 4670.400000000001, "text": " various goals it has. So if, for example, we assume that GPT4 or chat GPT, like roughly has the", "tokens": [50364, 3683, 5493, 309, 575, 13, 407, 498, 11, 337, 1365, 11, 321, 6552, 300, 26039, 51, 19, 420, 5081, 26039, 51, 11, 411, 9810, 575, 264, 50704], "temperature": 0.0, "avg_logprob": -0.137420389911916, "compression_ratio": 1.6375, "no_speech_prob": 0.005467537324875593}, {"id": 824, "seek": 466360, "start": 4670.400000000001, "end": 4677.360000000001, "text": " goals to be harmless, honest, and helpful, and to follow instructions, then you can trade off these", "tokens": [50704, 5493, 281, 312, 40160, 11, 3245, 11, 293, 4961, 11, 293, 281, 1524, 9415, 11, 550, 291, 393, 4923, 766, 613, 51052], "temperature": 0.0, "avg_logprob": -0.137420389911916, "compression_ratio": 1.6375, "no_speech_prob": 0.005467537324875593}, {"id": 825, "seek": 466360, "start": 4677.360000000001, "end": 4683.84, "text": " four different things in ways that are, where like sometimes it takes to trade off that we find", "tokens": [51052, 1451, 819, 721, 294, 2098, 300, 366, 11, 689, 411, 2171, 309, 2516, 281, 4923, 766, 300, 321, 915, 51376], "temperature": 0.0, "avg_logprob": -0.137420389911916, "compression_ratio": 1.6375, "no_speech_prob": 0.005467537324875593}, {"id": 826, "seek": 466360, "start": 4683.84, "end": 4688.64, "text": " unintuitive. So it's kind of like a goal misgeneralization thing. So, you know, something that people", "tokens": [51376, 29466, 48314, 13, 407, 309, 311, 733, 295, 411, 257, 3387, 3346, 1766, 2790, 2144, 551, 13, 407, 11, 291, 458, 11, 746, 300, 561, 51616], "temperature": 0.0, "avg_logprob": -0.137420389911916, "compression_ratio": 1.6375, "no_speech_prob": 0.005467537324875593}, {"id": 827, "seek": 468864, "start": 4689.360000000001, "end": 4694.400000000001, "text": " in the AI safety community have been sort of hypothesized and also like even shown real world", "tokens": [50400, 294, 264, 7318, 4514, 1768, 362, 668, 1333, 295, 14276, 1602, 293, 611, 411, 754, 4898, 957, 1002, 50652], "temperature": 0.0, "avg_logprob": -0.13606950792215639, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.03207656741142273}, {"id": 828, "seek": 468864, "start": 4694.400000000001, "end": 4700.4800000000005, "text": " examples of already, and I think is kind of also one of at least the theoretical groundings for this", "tokens": [50652, 5110, 295, 1217, 11, 293, 286, 519, 307, 733, 295, 611, 472, 295, 412, 1935, 264, 20864, 2727, 1109, 337, 341, 50956], "temperature": 0.0, "avg_logprob": -0.13606950792215639, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.03207656741142273}, {"id": 829, "seek": 468864, "start": 4700.4800000000005, "end": 4706.8, "text": " as well. And then the other thing is, yeah, instrumental convergence really feels like a", "tokens": [50956, 382, 731, 13, 400, 550, 264, 661, 551, 307, 11, 1338, 11, 17388, 32181, 534, 3417, 411, 257, 51272], "temperature": 0.0, "avg_logprob": -0.13606950792215639, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.03207656741142273}, {"id": 830, "seek": 468864, "start": 4706.8, "end": 4712.72, "text": " thing. You know, as in, at some point, the model just realizes in order to achieve this other goal", "tokens": [51272, 551, 13, 509, 458, 11, 382, 294, 11, 412, 512, 935, 11, 264, 2316, 445, 29316, 294, 1668, 281, 4584, 341, 661, 3387, 51568], "temperature": 0.0, "avg_logprob": -0.13606950792215639, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.03207656741142273}, {"id": 831, "seek": 468864, "start": 4712.72, "end": 4718.240000000001, "text": " that I, for some reason, have, it is instrumentally useful to be deceptive at this point, and then to", "tokens": [51568, 300, 286, 11, 337, 512, 1778, 11, 362, 11, 309, 307, 7198, 379, 4420, 281, 312, 368, 1336, 488, 412, 341, 935, 11, 293, 550, 281, 51844], "temperature": 0.0, "avg_logprob": -0.13606950792215639, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.03207656741142273}, {"id": 832, "seek": 471824, "start": 4718.32, "end": 4724.88, "text": " double down, and so on. And, you know, like, I think it is, it's like a fair criticism of this", "tokens": [50368, 3834, 760, 11, 293, 370, 322, 13, 400, 11, 291, 458, 11, 411, 11, 286, 519, 309, 307, 11, 309, 311, 411, 257, 3143, 15835, 295, 341, 50696], "temperature": 0.0, "avg_logprob": -0.11461098524775819, "compression_ratio": 1.8239700374531835, "no_speech_prob": 0.0014548669569194317}, {"id": 833, "seek": 471824, "start": 4724.88, "end": 4729.599999999999, "text": " particular setting that you could say, Oh, you know, you, it's just simulating an agent, or it's", "tokens": [50696, 1729, 3287, 300, 291, 727, 584, 11, 876, 11, 291, 458, 11, 291, 11, 309, 311, 445, 1034, 12162, 364, 9461, 11, 420, 309, 311, 50932], "temperature": 0.0, "avg_logprob": -0.11461098524775819, "compression_ratio": 1.8239700374531835, "no_speech_prob": 0.0014548669569194317}, {"id": 834, "seek": 471824, "start": 4729.599999999999, "end": 4735.44, "text": " just simulating all of these different things. But, and therefore, it's sort of not real. But in my", "tokens": [50932, 445, 1034, 12162, 439, 295, 613, 819, 721, 13, 583, 11, 293, 4412, 11, 309, 311, 1333, 295, 406, 957, 13, 583, 294, 452, 51224], "temperature": 0.0, "avg_logprob": -0.11461098524775819, "compression_ratio": 1.8239700374531835, "no_speech_prob": 0.0014548669569194317}, {"id": 835, "seek": 471824, "start": 4735.44, "end": 4739.44, "text": " head, I'm not sure, you know, I'm not sure whether this distinction makes too much sense, right? In", "tokens": [51224, 1378, 11, 286, 478, 406, 988, 11, 291, 458, 11, 286, 478, 406, 988, 1968, 341, 16844, 1669, 886, 709, 2020, 11, 558, 30, 682, 51424], "temperature": 0.0, "avg_logprob": -0.11461098524775819, "compression_ratio": 1.8239700374531835, "no_speech_prob": 0.0014548669569194317}, {"id": 836, "seek": 471824, "start": 4739.44, "end": 4744.96, "text": " some sense, you know, you have, at some point, you will have a very agentic simulator that, you", "tokens": [51424, 512, 2020, 11, 291, 458, 11, 291, 362, 11, 412, 512, 935, 11, 291, 486, 362, 257, 588, 9461, 299, 32974, 300, 11, 291, 51700], "temperature": 0.0, "avg_logprob": -0.11461098524775819, "compression_ratio": 1.8239700374531835, "no_speech_prob": 0.0014548669569194317}, {"id": 837, "seek": 474496, "start": 4744.96, "end": 4749.28, "text": " know, just simulates an agent, but it still has effects on the real world. Like, it doesn't matter", "tokens": [50364, 458, 11, 445, 1034, 26192, 364, 9461, 11, 457, 309, 920, 575, 5065, 322, 264, 957, 1002, 13, 1743, 11, 309, 1177, 380, 1871, 50580], "temperature": 0.0, "avg_logprob": -0.08737804543258798, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.013632035814225674}, {"id": 838, "seek": 474496, "start": 4749.28, "end": 4754.16, "text": " whether, in principle, it's like only simulating something or truly believing all of these things", "tokens": [50580, 1968, 11, 294, 8665, 11, 309, 311, 411, 787, 1034, 12162, 746, 420, 4908, 16594, 439, 295, 613, 721, 50824], "temperature": 0.0, "avg_logprob": -0.08737804543258798, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.013632035814225674}, {"id": 839, "seek": 474496, "start": 4754.16, "end": 4758.56, "text": " in some philosophical sense, like acts on the real world is as consequences on the real world.", "tokens": [50824, 294, 512, 25066, 2020, 11, 411, 10672, 322, 264, 957, 1002, 307, 382, 10098, 322, 264, 957, 1002, 13, 51044], "temperature": 0.0, "avg_logprob": -0.08737804543258798, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.013632035814225674}, {"id": 840, "seek": 474496, "start": 4758.56, "end": 4766.8, "text": " Like, yeah, the philosophy behind it is sort of less, less important. And then, yeah, there,", "tokens": [51044, 1743, 11, 1338, 11, 264, 10675, 2261, 309, 307, 1333, 295, 1570, 11, 1570, 1021, 13, 400, 550, 11, 1338, 11, 456, 11, 51456], "temperature": 0.0, "avg_logprob": -0.08737804543258798, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.013632035814225674}, {"id": 841, "seek": 474496, "start": 4766.8, "end": 4771.84, "text": " there are like a couple of, a couple of findings, or like variations that I thought were pretty", "tokens": [51456, 456, 366, 411, 257, 1916, 295, 11, 257, 1916, 295, 16483, 11, 420, 411, 17840, 300, 286, 1194, 645, 1238, 51708], "temperature": 0.0, "avg_logprob": -0.08737804543258798, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.013632035814225674}, {"id": 842, "seek": 477184, "start": 4771.92, "end": 4779.360000000001, "text": " interesting. The first one is system prompts, we changed, we played around with different system", "tokens": [50368, 1880, 13, 440, 700, 472, 307, 1185, 41095, 11, 321, 3105, 11, 321, 3737, 926, 365, 819, 1185, 50740], "temperature": 0.0, "avg_logprob": -0.1280341345416613, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05497439205646515}, {"id": 843, "seek": 477184, "start": 4779.360000000001, "end": 4784.400000000001, "text": " prompts, some actively encouraged, some actively discouraged, unethical behavior, and specifically", "tokens": [50740, 41095, 11, 512, 13022, 14658, 11, 512, 13022, 35010, 11, 517, 3293, 804, 5223, 11, 293, 4682, 50992], "temperature": 0.0, "avg_logprob": -0.1280341345416613, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05497439205646515}, {"id": 844, "seek": 477184, "start": 4784.400000000001, "end": 4789.4400000000005, "text": " insider trading. And if you end, it basically scales as you would expect, if you encourage it,", "tokens": [50992, 40990, 9529, 13, 400, 498, 291, 917, 11, 309, 1936, 17408, 382, 291, 576, 2066, 11, 498, 291, 5373, 309, 11, 51244], "temperature": 0.0, "avg_logprob": -0.1280341345416613, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05497439205646515}, {"id": 845, "seek": 477184, "start": 4790.0, "end": 4794.24, "text": " it goes up, if you discourage it, it goes down. But one thing that I want to, you know, like,", "tokens": [51272, 309, 1709, 493, 11, 498, 291, 21497, 609, 309, 11, 309, 1709, 760, 13, 583, 472, 551, 300, 286, 528, 281, 11, 291, 458, 11, 411, 11, 51484], "temperature": 0.0, "avg_logprob": -0.1280341345416613, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05497439205646515}, {"id": 846, "seek": 477184, "start": 4794.24, "end": 4797.68, "text": " and on Twitter, somebody responded like, Hey, that's, that's, that's a good, like, you know,", "tokens": [51484, 293, 322, 5794, 11, 2618, 15806, 411, 11, 1911, 11, 300, 311, 11, 300, 311, 11, 300, 311, 257, 665, 11, 411, 11, 291, 458, 11, 51656], "temperature": 0.0, "avg_logprob": -0.1280341345416613, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05497439205646515}, {"id": 847, "seek": 479768, "start": 4797.92, "end": 4803.12, "text": " that's good, right? It's like, if you discourage it, it stops. Like, yes, this is good. It's better", "tokens": [50376, 300, 311, 665, 11, 558, 30, 467, 311, 411, 11, 498, 291, 21497, 609, 309, 11, 309, 10094, 13, 1743, 11, 2086, 11, 341, 307, 665, 13, 467, 311, 1101, 50636], "temperature": 0.0, "avg_logprob": -0.06406860781791515, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04207572340965271}, {"id": 848, "seek": 479768, "start": 4803.12, "end": 4808.16, "text": " if that didn't happen. But like, it's better than in the world in which the discouragement would not", "tokens": [50636, 498, 300, 994, 380, 1051, 13, 583, 411, 11, 309, 311, 1101, 813, 294, 264, 1002, 294, 597, 264, 21497, 11129, 576, 406, 50888], "temperature": 0.0, "avg_logprob": -0.06406860781791515, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04207572340965271}, {"id": 849, "seek": 479768, "start": 4808.16, "end": 4813.52, "text": " change the behavior. But also, you have to be extremely specific. So if you just say, don't", "tokens": [50888, 1319, 264, 5223, 13, 583, 611, 11, 291, 362, 281, 312, 4664, 2685, 13, 407, 498, 291, 445, 584, 11, 500, 380, 51156], "temperature": 0.0, "avg_logprob": -0.06406860781791515, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04207572340965271}, {"id": 850, "seek": 479768, "start": 4813.52, "end": 4820.4800000000005, "text": " do unethical things, it reduces the rate, but it doesn't set it to zero. And so it kind of brings", "tokens": [51156, 360, 517, 3293, 804, 721, 11, 309, 18081, 264, 3314, 11, 457, 309, 1177, 380, 992, 309, 281, 4018, 13, 400, 370, 309, 733, 295, 5607, 51504], "temperature": 0.0, "avg_logprob": -0.06406860781791515, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04207572340965271}, {"id": 851, "seek": 479768, "start": 4820.4800000000005, "end": 4825.92, "text": " you back to the point where, to reduce the bad behavior, you have to specifically enumerate", "tokens": [51504, 291, 646, 281, 264, 935, 689, 11, 281, 5407, 264, 1578, 5223, 11, 291, 362, 281, 4682, 465, 15583, 473, 51776], "temperature": 0.0, "avg_logprob": -0.06406860781791515, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04207572340965271}, {"id": 852, "seek": 482592, "start": 4825.92, "end": 4830.56, "text": " all of the bad behavior. And it's like, often hard to predict in advance. And there's a lot of bad", "tokens": [50364, 439, 295, 264, 1578, 5223, 13, 400, 309, 311, 411, 11, 2049, 1152, 281, 6069, 294, 7295, 13, 400, 456, 311, 257, 688, 295, 1578, 50596], "temperature": 0.0, "avg_logprob": -0.1162845628303394, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.02517266757786274}, {"id": 853, "seek": 482592, "start": 4830.56, "end": 4835.6, "text": " things that could happen in like weird edge cases. So yeah, it doesn't feel like sort of the, like,", "tokens": [50596, 721, 300, 727, 1051, 294, 411, 3657, 4691, 3331, 13, 407, 1338, 11, 309, 1177, 380, 841, 411, 1333, 295, 264, 11, 411, 11, 50848], "temperature": 0.0, "avg_logprob": -0.1162845628303394, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.02517266757786274}, {"id": 854, "seek": 482592, "start": 4837.04, "end": 4841.6, "text": " insider trading is somehow related to like, unethicalness or something has really like,", "tokens": [50920, 40990, 9529, 307, 6063, 4077, 281, 411, 11, 517, 3293, 804, 1287, 420, 746, 575, 534, 411, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1162845628303394, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.02517266757786274}, {"id": 855, "seek": 482592, "start": 4841.6, "end": 4847.4400000000005, "text": " is like deeply internalized into the model. Another thing I found very interesting is", "tokens": [51148, 307, 411, 8760, 6920, 1602, 666, 264, 2316, 13, 3996, 551, 286, 1352, 588, 1880, 307, 51440], "temperature": 0.0, "avg_logprob": -0.1162845628303394, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.02517266757786274}, {"id": 856, "seek": 482592, "start": 4849.12, "end": 4855.12, "text": " whether it has access to, to like the private thoughts scratch pad, or reasoning scratch pad,", "tokens": [51524, 1968, 309, 575, 2105, 281, 11, 281, 411, 264, 4551, 4598, 8459, 6887, 11, 420, 21577, 8459, 6887, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1162845628303394, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.02517266757786274}, {"id": 857, "seek": 485512, "start": 4855.2, "end": 4858.32, "text": " I think like, you know, it's not even framed as private thoughts, it's just like, you have an", "tokens": [50368, 286, 519, 411, 11, 291, 458, 11, 309, 311, 406, 754, 30420, 382, 4551, 4598, 11, 309, 311, 445, 411, 11, 291, 362, 364, 50524], "temperature": 0.0, "avg_logprob": -0.10084515380859375, "compression_ratio": 1.8359375, "no_speech_prob": 0.012428976595401764}, {"id": 858, "seek": 485512, "start": 4858.32, "end": 4864.72, "text": " action that is called reasoning go. If we remove that. So we in the, in the typical setting,", "tokens": [50524, 3069, 300, 307, 1219, 21577, 352, 13, 759, 321, 4159, 300, 13, 407, 321, 294, 264, 11, 294, 264, 7476, 3287, 11, 50844], "temperature": 0.0, "avg_logprob": -0.10084515380859375, "compression_ratio": 1.8359375, "no_speech_prob": 0.012428976595401764}, {"id": 859, "seek": 485512, "start": 4864.72, "end": 4870.0, "text": " we have this scratch pad, we have this reasoning space. And now if we remove that, the deception", "tokens": [50844, 321, 362, 341, 8459, 6887, 11, 321, 362, 341, 21577, 1901, 13, 400, 586, 498, 321, 4159, 300, 11, 264, 40451, 51108], "temperature": 0.0, "avg_logprob": -0.10084515380859375, "compression_ratio": 1.8359375, "no_speech_prob": 0.012428976595401764}, {"id": 860, "seek": 485512, "start": 4870.0, "end": 4878.0, "text": " rate goes down. And I think this is good and bad in some sense, like, it is, it is sort of nice", "tokens": [51108, 3314, 1709, 760, 13, 400, 286, 519, 341, 307, 665, 293, 1578, 294, 512, 2020, 11, 411, 11, 309, 307, 11, 309, 307, 1333, 295, 1481, 51508], "temperature": 0.0, "avg_logprob": -0.10084515380859375, "compression_ratio": 1.8359375, "no_speech_prob": 0.012428976595401764}, {"id": 861, "seek": 485512, "start": 4879.12, "end": 4884.88, "text": " in the sense that it is more likely to act in ways that we can support badly in ways where", "tokens": [51564, 294, 264, 2020, 300, 309, 307, 544, 3700, 281, 605, 294, 2098, 300, 321, 393, 1406, 13425, 294, 2098, 689, 51852], "temperature": 0.0, "avg_logprob": -0.10084515380859375, "compression_ratio": 1.8359375, "no_speech_prob": 0.012428976595401764}, {"id": 862, "seek": 488488, "start": 4884.88, "end": 4888.88, "text": " we can supervise it, just because it has more reasonings, it has more space to reason out,", "tokens": [50364, 321, 393, 37971, 908, 309, 11, 445, 570, 309, 575, 544, 1778, 1109, 11, 309, 575, 544, 1901, 281, 1778, 484, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 863, "seek": 488488, "start": 4888.88, "end": 4892.64, "text": " and maybe that leads to more deception. Because yeah, I'm not sure, like, I don't want to speculate", "tokens": [50564, 293, 1310, 300, 6689, 281, 544, 40451, 13, 1436, 1338, 11, 286, 478, 406, 988, 11, 411, 11, 286, 500, 380, 528, 281, 40775, 50752], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 864, "seek": 488488, "start": 4892.64, "end": 4899.76, "text": " too hard what's going on inside, inside GP4 at this point. But I think it's also worrying on the", "tokens": [50752, 886, 1152, 437, 311, 516, 322, 1854, 11, 1854, 26039, 19, 412, 341, 935, 13, 583, 286, 519, 309, 311, 611, 18788, 322, 264, 51108], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 865, "seek": 488488, "start": 4899.76, "end": 4904.4800000000005, "text": " other hand, because in the case where we don't have access to the scratch scratch pad, we don't", "tokens": [51108, 661, 1011, 11, 570, 294, 264, 1389, 689, 321, 500, 380, 362, 2105, 281, 264, 8459, 8459, 6887, 11, 321, 500, 380, 51344], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 866, "seek": 488488, "start": 4904.4800000000005, "end": 4909.52, "text": " see anything at all, and it still acts deceptively at some point. So it has like a lot of unobserved", "tokens": [51344, 536, 1340, 412, 439, 11, 293, 309, 920, 10672, 368, 1336, 3413, 412, 512, 935, 13, 407, 309, 575, 411, 257, 688, 295, 8526, 929, 6913, 51596], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 867, "seek": 488488, "start": 4909.52, "end": 4914.0, "text": " optimization space, which also seems like pretty much at the core of, of deceptive alignment and", "tokens": [51596, 19618, 1901, 11, 597, 611, 2544, 411, 1238, 709, 412, 264, 4965, 295, 11, 295, 368, 1336, 488, 18515, 293, 51820], "temperature": 0.0, "avg_logprob": -0.10264857819205836, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0022515777964144945}, {"id": 868, "seek": 491400, "start": 4914.0, "end": 4919.52, "text": " deception to me, right? The more unobserved optimization space you have, as an AI, the more", "tokens": [50364, 40451, 281, 385, 11, 558, 30, 440, 544, 8526, 929, 6913, 19618, 1901, 291, 362, 11, 382, 364, 7318, 11, 264, 544, 50640], "temperature": 0.0, "avg_logprob": -0.1103202880374969, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0007096398039720953}, {"id": 869, "seek": 491400, "start": 4920.64, "end": 4926.96, "text": " time and space you can use to build a deceptive plan while the other person does not know about", "tokens": [50696, 565, 293, 1901, 291, 393, 764, 281, 1322, 257, 368, 1336, 488, 1393, 1339, 264, 661, 954, 775, 406, 458, 466, 51012], "temperature": 0.0, "avg_logprob": -0.1103202880374969, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0007096398039720953}, {"id": 870, "seek": 491400, "start": 4926.96, "end": 4932.8, "text": " it. So yeah, I think removing the scratch pad, yeah, sort of a double-edged sword in or this", "tokens": [51012, 309, 13, 407, 1338, 11, 286, 519, 12720, 264, 8459, 6887, 11, 1338, 11, 1333, 295, 257, 3834, 12, 292, 3004, 10576, 294, 420, 341, 51304], "temperature": 0.0, "avg_logprob": -0.1103202880374969, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0007096398039720953}, {"id": 871, "seek": 491400, "start": 4932.8, "end": 4937.68, "text": " result is a double-edged sword in that sense. Yeah, as a practitioner, it's like, I always use", "tokens": [51304, 1874, 307, 257, 3834, 12, 292, 3004, 10576, 294, 300, 2020, 13, 865, 11, 382, 257, 32125, 11, 309, 311, 411, 11, 286, 1009, 764, 51548], "temperature": 0.0, "avg_logprob": -0.1103202880374969, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0007096398039720953}, {"id": 872, "seek": 491400, "start": 4937.68, "end": 4942.64, "text": " that technique, you know, in any sort of task automation or whatever that I'm doing, it's", "tokens": [51548, 300, 6532, 11, 291, 458, 11, 294, 604, 1333, 295, 5633, 17769, 420, 2035, 300, 286, 478, 884, 11, 309, 311, 51796], "temperature": 0.0, "avg_logprob": -0.1103202880374969, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0007096398039720953}, {"id": 873, "seek": 494264, "start": 4942.64, "end": 4949.4400000000005, "text": " basically the default to have some sort of reasoning step before the final output, almost", "tokens": [50364, 1936, 264, 7576, 281, 362, 512, 1333, 295, 21577, 1823, 949, 264, 2572, 5598, 11, 1920, 50704], "temperature": 0.0, "avg_logprob": -0.09935802972617269, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0005883729318156838}, {"id": 874, "seek": 494264, "start": 4949.4400000000005, "end": 4955.68, "text": " universal across tasks that I'll do that. So for me, reading that, I was like, yeah, that seems bad,", "tokens": [50704, 11455, 2108, 9608, 300, 286, 603, 360, 300, 13, 407, 337, 385, 11, 3760, 300, 11, 286, 390, 411, 11, 1338, 11, 300, 2544, 1578, 11, 51016], "temperature": 0.0, "avg_logprob": -0.09935802972617269, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0005883729318156838}, {"id": 875, "seek": 494264, "start": 4955.68, "end": 4961.6, "text": " if only because like, the best practice is giving us more of the problem.", "tokens": [51016, 498, 787, 570, 411, 11, 264, 1151, 3124, 307, 2902, 505, 544, 295, 264, 1154, 13, 51312], "temperature": 0.0, "avg_logprob": -0.09935802972617269, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0005883729318156838}, {"id": 876, "seek": 494264, "start": 4961.6, "end": 4966.4800000000005, "text": " Well, but at least you see it, right? Like, I think at least you know, at least you can go back,", "tokens": [51312, 1042, 11, 457, 412, 1935, 291, 536, 309, 11, 558, 30, 1743, 11, 286, 519, 412, 1935, 291, 458, 11, 412, 1935, 291, 393, 352, 646, 11, 51556], "temperature": 0.0, "avg_logprob": -0.09935802972617269, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0005883729318156838}, {"id": 877, "seek": 494264, "start": 4966.4800000000005, "end": 4970.320000000001, "text": " you know, through the logs and see what the model spit out, or at least maybe you have a filter", "tokens": [51556, 291, 458, 11, 807, 264, 20820, 293, 536, 437, 264, 2316, 22127, 484, 11, 420, 412, 1935, 1310, 291, 362, 257, 6608, 51748], "temperature": 0.0, "avg_logprob": -0.09935802972617269, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0005883729318156838}, {"id": 878, "seek": 497032, "start": 4970.32, "end": 4974.639999999999, "text": " that like, oh, is this model doing like weird deceptive reasoning right now? Oh, yeah, it is.", "tokens": [50364, 300, 411, 11, 1954, 11, 307, 341, 2316, 884, 411, 3657, 368, 1336, 488, 21577, 558, 586, 30, 876, 11, 1338, 11, 309, 307, 13, 50580], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 879, "seek": 497032, "start": 4974.639999999999, "end": 4978.96, "text": " So we just don't have the output. So, you know, I would rather be in a world where you have", "tokens": [50580, 407, 321, 445, 500, 380, 362, 264, 5598, 13, 407, 11, 291, 458, 11, 286, 576, 2831, 312, 294, 257, 1002, 689, 291, 362, 50796], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 880, "seek": 497032, "start": 4980.32, "end": 4984.96, "text": " chain of thought, and you know that the model is kind of deceptive, and then you just adapt to", "tokens": [50864, 5021, 295, 1194, 11, 293, 291, 458, 300, 264, 2316, 307, 733, 295, 368, 1336, 488, 11, 293, 550, 291, 445, 6231, 281, 51096], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 881, "seek": 497032, "start": 4984.96, "end": 4989.679999999999, "text": " that rather than having to solve interpretability first before you can access all the internal", "tokens": [51096, 300, 2831, 813, 1419, 281, 5039, 7302, 2310, 700, 949, 291, 393, 2105, 439, 264, 6920, 51332], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 882, "seek": 497032, "start": 4989.679999999999, "end": 4994.32, "text": " steps and have to like understand how the deception works internally. Yeah, it's like I said,", "tokens": [51332, 4439, 293, 362, 281, 411, 1223, 577, 264, 40451, 1985, 19501, 13, 865, 11, 309, 311, 411, 286, 848, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 883, "seek": 497032, "start": 4994.32, "end": 4999.599999999999, "text": " double-edged sword. Yeah, there are a couple of other interesting findings. One is we can vary", "tokens": [51564, 3834, 12, 292, 3004, 10576, 13, 865, 11, 456, 366, 257, 1916, 295, 661, 1880, 16483, 13, 1485, 307, 321, 393, 10559, 51828], "temperature": 0.0, "avg_logprob": -0.11357128620147705, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.003483028383925557}, {"id": 884, "seek": 499960, "start": 4999.68, "end": 5006.56, "text": " the levels of pressure. So the more pressure we add, the more likely the model is to be deceptive.", "tokens": [50368, 264, 4358, 295, 3321, 13, 407, 264, 544, 3321, 321, 909, 11, 264, 544, 3700, 264, 2316, 307, 281, 312, 368, 1336, 488, 13, 50712], "temperature": 0.0, "avg_logprob": -0.10248230724799924, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.015419489704072475}, {"id": 885, "seek": 499960, "start": 5006.56, "end": 5012.08, "text": " So kind of in the same way in which a human would act, it also acts. And removing, you know,", "tokens": [50712, 407, 733, 295, 294, 264, 912, 636, 294, 597, 257, 1952, 576, 605, 11, 309, 611, 10672, 13, 400, 12720, 11, 291, 458, 11, 50988], "temperature": 0.0, "avg_logprob": -0.10248230724799924, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.015419489704072475}, {"id": 886, "seek": 499960, "start": 5012.08, "end": 5017.360000000001, "text": " removing pressure and adding additional options will very quickly decrease the probability of", "tokens": [50988, 12720, 3321, 293, 5127, 4497, 3956, 486, 588, 2661, 11514, 264, 8482, 295, 51252], "temperature": 0.0, "avg_logprob": -0.10248230724799924, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.015419489704072475}, {"id": 887, "seek": 499960, "start": 5017.360000000001, "end": 5021.200000000001, "text": " being deceptive. So this is maybe this is it, you know, this may be like the flip side of all", "tokens": [51252, 885, 368, 1336, 488, 13, 407, 341, 307, 1310, 341, 307, 309, 11, 291, 458, 11, 341, 815, 312, 411, 264, 7929, 1252, 295, 439, 51444], "temperature": 0.0, "avg_logprob": -0.10248230724799924, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.015419489704072475}, {"id": 888, "seek": 499960, "start": 5021.200000000001, "end": 5026.160000000001, "text": " of our findings. It's like, the fact that it took very long for us to find and the fact that like,", "tokens": [51444, 295, 527, 16483, 13, 467, 311, 411, 11, 264, 1186, 300, 309, 1890, 588, 938, 337, 505, 281, 915, 293, 264, 1186, 300, 411, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10248230724799924, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.015419489704072475}, {"id": 889, "seek": 502616, "start": 5026.16, "end": 5029.5199999999995, "text": " as soon as you give it another option, it immediately picks the other option kind of means", "tokens": [50364, 382, 2321, 382, 291, 976, 309, 1071, 3614, 11, 309, 4258, 16137, 264, 661, 3614, 733, 295, 1355, 50532], "temperature": 0.0, "avg_logprob": -0.11575555379411816, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0028891274705529213}, {"id": 890, "seek": 502616, "start": 5030.16, "end": 5034.639999999999, "text": " it's mostly aligned, right? It took us quite hard, like it took us a little bit of time at least", "tokens": [50564, 309, 311, 5240, 17962, 11, 558, 30, 467, 1890, 505, 1596, 1152, 11, 411, 309, 1890, 505, 257, 707, 857, 295, 565, 412, 1935, 50788], "temperature": 0.0, "avg_logprob": -0.11575555379411816, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0028891274705529213}, {"id": 891, "seek": 502616, "start": 5034.639999999999, "end": 5042.0, "text": " to find this particular scenario and make it work robustly. So then we also change the risk", "tokens": [50788, 281, 915, 341, 1729, 9005, 293, 652, 309, 589, 13956, 356, 13, 407, 550, 321, 611, 1319, 264, 3148, 51156], "temperature": 0.0, "avg_logprob": -0.11575555379411816, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0028891274705529213}, {"id": 892, "seek": 502616, "start": 5042.0, "end": 5046.32, "text": " of getting caught and also correct, it also reacts to this roughly as you would expect.", "tokens": [51156, 295, 1242, 5415, 293, 611, 3006, 11, 309, 611, 33305, 281, 341, 9810, 382, 291, 576, 2066, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11575555379411816, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0028891274705529213}, {"id": 893, "seek": 502616, "start": 5047.04, "end": 5053.68, "text": " So the more risk you have, the less likely inceter trading becomes. So this is all of this,", "tokens": [51408, 407, 264, 544, 3148, 291, 362, 11, 264, 1570, 3700, 834, 2398, 9529, 3643, 13, 407, 341, 307, 439, 295, 341, 11, 51740], "temperature": 0.0, "avg_logprob": -0.11575555379411816, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0028891274705529213}, {"id": 894, "seek": 505368, "start": 5053.68, "end": 5057.84, "text": " right, you know, all of this feels like stuff that you kind of expect from something that acts", "tokens": [50364, 558, 11, 291, 458, 11, 439, 295, 341, 3417, 411, 1507, 300, 291, 733, 295, 2066, 490, 746, 300, 10672, 50572], "temperature": 0.0, "avg_logprob": -0.08231935330799647, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0060964832082390785}, {"id": 895, "seek": 505368, "start": 5057.84, "end": 5062.400000000001, "text": " consistently in some way. I mean, I don't know how it works, but it still feels like it has a", "tokens": [50572, 14961, 294, 512, 636, 13, 286, 914, 11, 286, 500, 380, 458, 577, 309, 1985, 11, 457, 309, 920, 3417, 411, 309, 575, 257, 50800], "temperature": 0.0, "avg_logprob": -0.08231935330799647, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0060964832082390785}, {"id": 896, "seek": 505368, "start": 5062.400000000001, "end": 5068.400000000001, "text": " fairly consistent way of working. Then there were a couple of other findings where I'm,", "tokens": [50800, 6457, 8398, 636, 295, 1364, 13, 1396, 456, 645, 257, 1916, 295, 661, 16483, 689, 286, 478, 11, 51100], "temperature": 0.0, "avg_logprob": -0.08231935330799647, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0060964832082390785}, {"id": 897, "seek": 505368, "start": 5069.68, "end": 5076.320000000001, "text": " yeah, not sure to which extent they are correct interpretation. So we did not make these inferences,", "tokens": [51164, 1338, 11, 406, 988, 281, 597, 8396, 436, 366, 3006, 14174, 13, 407, 321, 630, 406, 652, 613, 13596, 2667, 11, 51496], "temperature": 0.0, "avg_logprob": -0.08231935330799647, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0060964832082390785}, {"id": 898, "seek": 505368, "start": 5076.320000000001, "end": 5081.92, "text": " but they were definitely sometimes, or like sometimes cited as such. So for example,", "tokens": [51496, 457, 436, 645, 2138, 2171, 11, 420, 411, 2171, 30134, 382, 1270, 13, 407, 337, 1365, 11, 51776], "temperature": 0.0, "avg_logprob": -0.08231935330799647, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0060964832082390785}, {"id": 899, "seek": 508192, "start": 5082.4800000000005, "end": 5087.4400000000005, "text": " there was one question of whether stronger models engage in inceter trading more often,", "tokens": [50392, 456, 390, 472, 1168, 295, 1968, 7249, 5245, 4683, 294, 834, 2398, 9529, 544, 2049, 11, 50640], "temperature": 0.0, "avg_logprob": -0.09391614486431253, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.004198017995804548}, {"id": 900, "seek": 508192, "start": 5087.4400000000005, "end": 5091.76, "text": " because if you look at our main plot, it looks this way, we tested for different models and", "tokens": [50640, 570, 498, 291, 574, 412, 527, 2135, 7542, 11, 309, 1542, 341, 636, 11, 321, 8246, 337, 819, 5245, 293, 50856], "temperature": 0.0, "avg_logprob": -0.09391614486431253, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.004198017995804548}, {"id": 901, "seek": 508192, "start": 5091.76, "end": 5096.96, "text": " the bigger the model, the more capable and the more likely it is to engage in inceter trading", "tokens": [50856, 264, 3801, 264, 2316, 11, 264, 544, 8189, 293, 264, 544, 3700, 309, 307, 281, 4683, 294, 834, 2398, 9529, 51116], "temperature": 0.0, "avg_logprob": -0.09391614486431253, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.004198017995804548}, {"id": 902, "seek": 508192, "start": 5096.96, "end": 5102.16, "text": " in the first place. And yeah, there are a lot of confounders here. So I'm really not sure whether", "tokens": [51116, 294, 264, 700, 1081, 13, 400, 1338, 11, 456, 366, 257, 688, 295, 1497, 554, 433, 510, 13, 407, 286, 478, 534, 406, 988, 1968, 51376], "temperature": 0.0, "avg_logprob": -0.09391614486431253, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.004198017995804548}, {"id": 903, "seek": 508192, "start": 5102.16, "end": 5107.2, "text": " we should, like how much we should, we should bank on this, maybe to get more details here. So", "tokens": [51376, 321, 820, 11, 411, 577, 709, 321, 820, 11, 321, 820, 3765, 322, 341, 11, 1310, 281, 483, 544, 4365, 510, 13, 407, 51628], "temperature": 0.0, "avg_logprob": -0.09391614486431253, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.004198017995804548}, {"id": 904, "seek": 510720, "start": 5107.84, "end": 5113.28, "text": " the red teaming was done on GPT-4, just because it is the most powerful model we have access to.", "tokens": [50396, 264, 2182, 1469, 278, 390, 1096, 322, 26039, 51, 12, 19, 11, 445, 570, 309, 307, 264, 881, 4005, 2316, 321, 362, 2105, 281, 13, 50668], "temperature": 0.0, "avg_logprob": -0.09892279551579403, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0006877878913655877}, {"id": 905, "seek": 510720, "start": 5113.28, "end": 5118.96, "text": " So it is biased, right? We actively looked for a scenario in which GPT-4 is deceptive. So it is", "tokens": [50668, 407, 309, 307, 28035, 11, 558, 30, 492, 13022, 2956, 337, 257, 9005, 294, 597, 26039, 51, 12, 19, 307, 368, 1336, 488, 13, 407, 309, 307, 50952], "temperature": 0.0, "avg_logprob": -0.09892279551579403, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0006877878913655877}, {"id": 906, "seek": 510720, "start": 5118.96, "end": 5123.84, "text": " more likely that we will find something that where GPT-4 is like even more deceptive than", "tokens": [50952, 544, 3700, 300, 321, 486, 915, 746, 300, 689, 26039, 51, 12, 19, 307, 411, 754, 544, 368, 1336, 488, 813, 51196], "temperature": 0.0, "avg_logprob": -0.09892279551579403, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0006877878913655877}, {"id": 907, "seek": 510720, "start": 5123.84, "end": 5128.72, "text": " all the other models, because we actively looked in the space of GPT, potential GPT-4 scenarios,", "tokens": [51196, 439, 264, 661, 5245, 11, 570, 321, 13022, 2956, 294, 264, 1901, 295, 26039, 51, 11, 3995, 26039, 51, 12, 19, 15077, 11, 51440], "temperature": 0.0, "avg_logprob": -0.09892279551579403, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0006877878913655877}, {"id": 908, "seek": 510720, "start": 5129.92, "end": 5133.599999999999, "text": " which I think is the core reason why I'm like very uncertain about this, or like why we didn't", "tokens": [51500, 597, 286, 519, 307, 264, 4965, 1778, 983, 286, 478, 411, 588, 11308, 466, 341, 11, 420, 411, 983, 321, 994, 380, 51684], "temperature": 0.0, "avg_logprob": -0.09892279551579403, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0006877878913655877}, {"id": 909, "seek": 513360, "start": 5133.6, "end": 5141.76, "text": " make it a finding or didn't emphasize it. Could you rerun it on GPT-4 turbo now? Is that would seem", "tokens": [50364, 652, 309, 257, 5006, 420, 994, 380, 16078, 309, 13, 7497, 291, 43819, 409, 309, 322, 26039, 51, 12, 19, 20902, 586, 30, 1119, 300, 576, 1643, 50772], "temperature": 0.0, "avg_logprob": -0.09513459205627442, "compression_ratio": 1.6917562724014337, "no_speech_prob": 0.004467686638236046}, {"id": 910, "seek": 513360, "start": 5141.76, "end": 5146.4800000000005, "text": " like it might shed some light on that question? You mean the red teaming specifically or this", "tokens": [50772, 411, 309, 1062, 14951, 512, 1442, 322, 300, 1168, 30, 509, 914, 264, 2182, 1469, 278, 4682, 420, 341, 51008], "temperature": 0.0, "avg_logprob": -0.09513459205627442, "compression_ratio": 1.6917562724014337, "no_speech_prob": 0.004467686638236046}, {"id": 911, "seek": 513360, "start": 5146.4800000000005, "end": 5151.04, "text": " particular scenario? Yeah, I guess I'm just thinking like it makes sense that you develop", "tokens": [51008, 1729, 9005, 30, 865, 11, 286, 2041, 286, 478, 445, 1953, 411, 309, 1669, 2020, 300, 291, 1499, 51236], "temperature": 0.0, "avg_logprob": -0.09513459205627442, "compression_ratio": 1.6917562724014337, "no_speech_prob": 0.004467686638236046}, {"id": 912, "seek": 513360, "start": 5151.04, "end": 5156.240000000001, "text": " the scenario on GPT-4 and so you go back to the earlier models, like is it that those earlier", "tokens": [51236, 264, 9005, 322, 26039, 51, 12, 19, 293, 370, 291, 352, 646, 281, 264, 3071, 5245, 11, 411, 307, 309, 300, 729, 3071, 51496], "temperature": 0.0, "avg_logprob": -0.09513459205627442, "compression_ratio": 1.6917562724014337, "no_speech_prob": 0.004467686638236046}, {"id": 913, "seek": 513360, "start": 5156.240000000001, "end": 5161.360000000001, "text": " models are inherently less likely to do this? Or is it just a kind of an artifact of where the", "tokens": [51496, 5245, 366, 27993, 1570, 3700, 281, 360, 341, 30, 1610, 307, 309, 445, 257, 733, 295, 364, 34806, 295, 689, 264, 51752], "temperature": 0.0, "avg_logprob": -0.09513459205627442, "compression_ratio": 1.6917562724014337, "no_speech_prob": 0.004467686638236046}, {"id": 914, "seek": 516136, "start": 5161.36, "end": 5169.04, "text": " prompt was developed? If you flipped to GPT-4 turbo and ran the same scenario and you found", "tokens": [50364, 12391, 390, 4743, 30, 759, 291, 26273, 281, 26039, 51, 12, 19, 20902, 293, 5872, 264, 912, 9005, 293, 291, 1352, 50748], "temperature": 0.0, "avg_logprob": -0.12135332955254449, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.001206519198603928}, {"id": 915, "seek": 516136, "start": 5169.04, "end": 5173.839999999999, "text": " that it went, that the rate of deception went down, you might think, you might think, oh,", "tokens": [50748, 300, 309, 1437, 11, 300, 264, 3314, 295, 40451, 1437, 760, 11, 291, 1062, 519, 11, 291, 1062, 519, 11, 1954, 11, 50988], "temperature": 0.0, "avg_logprob": -0.12135332955254449, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.001206519198603928}, {"id": 916, "seek": 516136, "start": 5173.839999999999, "end": 5178.639999999999, "text": " it's more aligned. That could be one theory. You might also think that supports my artifact", "tokens": [50988, 309, 311, 544, 17962, 13, 663, 727, 312, 472, 5261, 13, 509, 1062, 611, 519, 300, 9346, 452, 34806, 51228], "temperature": 0.0, "avg_logprob": -0.12135332955254449, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.001206519198603928}, {"id": 917, "seek": 516136, "start": 5178.639999999999, "end": 5185.28, "text": " notion that it's kind of, we found some local maximum or near maximum without even necessarily", "tokens": [51228, 10710, 300, 309, 311, 733, 295, 11, 321, 1352, 512, 2654, 6674, 420, 2651, 6674, 1553, 754, 4725, 51560], "temperature": 0.0, "avg_logprob": -0.12135332955254449, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.001206519198603928}, {"id": 918, "seek": 518528, "start": 5185.28, "end": 5193.12, "text": " meaning to. But if it in fact does more than you would be like, oh, shit, because I mean,", "tokens": [50364, 3620, 281, 13, 583, 498, 309, 294, 1186, 775, 544, 813, 291, 576, 312, 411, 11, 1954, 11, 4611, 11, 570, 286, 914, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1326661777496338, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.16879408061504364}, {"id": 919, "seek": 518528, "start": 5193.12, "end": 5196.719999999999, "text": " it is like incrementally more powerful, it's like more preferred, it's better at following", "tokens": [50756, 309, 307, 411, 26200, 379, 544, 4005, 11, 309, 311, 411, 544, 16494, 11, 309, 311, 1101, 412, 3480, 50936], "temperature": 0.0, "avg_logprob": -0.1326661777496338, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.16879408061504364}, {"id": 920, "seek": 518528, "start": 5196.719999999999, "end": 5203.2, "text": " instructions, whatever. So if it does even more than the earlier GPT-4, I think that would be at", "tokens": [50936, 9415, 11, 2035, 13, 407, 498, 309, 775, 754, 544, 813, 264, 3071, 26039, 51, 12, 19, 11, 286, 519, 300, 576, 312, 412, 51260], "temperature": 0.0, "avg_logprob": -0.1326661777496338, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.16879408061504364}, {"id": 921, "seek": 518528, "start": 5203.2, "end": 5210.48, "text": " least non-trivial support for the more powerful models do this more often theory. Yeah, I'm not", "tokens": [51260, 1935, 2107, 12, 83, 470, 22640, 1406, 337, 264, 544, 4005, 5245, 360, 341, 544, 2049, 5261, 13, 865, 11, 286, 478, 406, 51624], "temperature": 0.0, "avg_logprob": -0.1326661777496338, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.16879408061504364}, {"id": 922, "seek": 521048, "start": 5210.48, "end": 5215.28, "text": " sure, like, even if we did rerun this, I'm not sure how much I would bank on this. So the reason", "tokens": [50364, 988, 11, 411, 11, 754, 498, 321, 630, 43819, 409, 341, 11, 286, 478, 406, 988, 577, 709, 286, 576, 3765, 322, 341, 13, 407, 264, 1778, 50604], "temperature": 0.0, "avg_logprob": -0.1151062623778386, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.011329509317874908}, {"id": 923, "seek": 521048, "start": 5215.28, "end": 5222.48, "text": " is, so we did run it on GPT-432K, so a slightly different model where it was only the context", "tokens": [50604, 307, 11, 370, 321, 630, 1190, 309, 322, 26039, 51, 12, 19, 11440, 42, 11, 370, 257, 4748, 819, 2316, 689, 309, 390, 787, 264, 4319, 50964], "temperature": 0.0, "avg_logprob": -0.1151062623778386, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.011329509317874908}, {"id": 924, "seek": 521048, "start": 5222.48, "end": 5227.12, "text": " window was extended, but that still changes probably a bunch of the internals with very little", "tokens": [50964, 4910, 390, 10913, 11, 457, 300, 920, 2962, 1391, 257, 3840, 295, 264, 2154, 1124, 365, 588, 707, 51196], "temperature": 0.0, "avg_logprob": -0.1151062623778386, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.011329509317874908}, {"id": 925, "seek": 521048, "start": 5227.12, "end": 5232.16, "text": " difference. So yeah, I think it's just too correlated with the GPT-4 architecture. And then", "tokens": [51196, 2649, 13, 407, 1338, 11, 286, 519, 309, 311, 445, 886, 38574, 365, 264, 26039, 51, 12, 19, 9482, 13, 400, 550, 51448], "temperature": 0.0, "avg_logprob": -0.1151062623778386, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.011329509317874908}, {"id": 926, "seek": 521048, "start": 5232.16, "end": 5237.759999999999, "text": " GPT-4 turbo is still very correlated with GPT-4, right? It's probably, I mean, I don't know what", "tokens": [51448, 26039, 51, 12, 19, 20902, 307, 920, 588, 38574, 365, 26039, 51, 12, 19, 11, 558, 30, 467, 311, 1391, 11, 286, 914, 11, 286, 500, 380, 458, 437, 51728], "temperature": 0.0, "avg_logprob": -0.1151062623778386, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.011329509317874908}, {"id": 927, "seek": 523776, "start": 5237.76, "end": 5242.72, "text": " exactly they're doing, but they're probably distilling it from their bigger model or at least", "tokens": [50364, 2293, 436, 434, 884, 11, 457, 436, 434, 1391, 1483, 7345, 309, 490, 641, 3801, 2316, 420, 412, 1935, 50612], "temperature": 0.0, "avg_logprob": -0.09503498483211437, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.010650238022208214}, {"id": 928, "seek": 523776, "start": 5242.72, "end": 5248.88, "text": " basing it on the bigger model in some sense. So yeah, the results are probably still too correlated", "tokens": [50612, 987, 278, 309, 322, 264, 3801, 2316, 294, 512, 2020, 13, 407, 1338, 11, 264, 3542, 366, 1391, 920, 886, 38574, 50920], "temperature": 0.0, "avg_logprob": -0.09503498483211437, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.010650238022208214}, {"id": 929, "seek": 523776, "start": 5248.88, "end": 5257.04, "text": " to make any bigger inferences. And even if we ran it on all the other big models that are out there,", "tokens": [50920, 281, 652, 604, 3801, 13596, 2667, 13, 400, 754, 498, 321, 5872, 309, 322, 439, 264, 661, 955, 5245, 300, 366, 484, 456, 11, 51328], "temperature": 0.0, "avg_logprob": -0.09503498483211437, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.010650238022208214}, {"id": 930, "seek": 523776, "start": 5258.08, "end": 5265.04, "text": " it's still unclear to me how much we would say this is actually an effect of model size rather", "tokens": [51380, 309, 311, 920, 25636, 281, 385, 577, 709, 321, 576, 584, 341, 307, 767, 364, 1802, 295, 2316, 2744, 2831, 51728], "temperature": 0.0, "avg_logprob": -0.09503498483211437, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.010650238022208214}, {"id": 931, "seek": 526504, "start": 5265.12, "end": 5269.84, "text": " than all the other confounders here. Like, the other, you know, it was red-teamed on GPT and not", "tokens": [50368, 813, 439, 264, 661, 1497, 554, 433, 510, 13, 1743, 11, 264, 661, 11, 291, 458, 11, 309, 390, 2182, 12, 975, 3475, 322, 26039, 51, 293, 406, 50604], "temperature": 0.0, "avg_logprob": -0.13343406858898343, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.019116366282105446}, {"id": 932, "seek": 526504, "start": 5271.12, "end": 5277.6, "text": " red-teamed on Claude or Gemini or anything like this. So yeah, I think if we wanted to make the", "tokens": [50668, 2182, 12, 975, 3475, 322, 12947, 2303, 420, 22894, 3812, 420, 1340, 411, 341, 13, 407, 1338, 11, 286, 519, 498, 321, 1415, 281, 652, 264, 50992], "temperature": 0.0, "avg_logprob": -0.13343406858898343, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.019116366282105446}, {"id": 933, "seek": 526504, "start": 5277.6, "end": 5284.32, "text": " statement, we would actually have to have sort of a long list of models of different sizes and", "tokens": [50992, 5629, 11, 321, 576, 767, 362, 281, 362, 1333, 295, 257, 938, 1329, 295, 5245, 295, 819, 11602, 293, 51328], "temperature": 0.0, "avg_logprob": -0.13343406858898343, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.019116366282105446}, {"id": 934, "seek": 526504, "start": 5284.32, "end": 5288.96, "text": " then just test it on all of them. And we can, we really have to remove all of the correlation", "tokens": [51328, 550, 445, 1500, 309, 322, 439, 295, 552, 13, 400, 321, 393, 11, 321, 534, 362, 281, 4159, 439, 295, 264, 20009, 51560], "temperature": 0.0, "avg_logprob": -0.13343406858898343, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.019116366282105446}, {"id": 935, "seek": 528896, "start": 5289.92, "end": 5295.84, "text": " and the weird confounders. And we don't have access to this, unfortunately. But, you know,", "tokens": [50412, 293, 264, 3657, 1497, 554, 433, 13, 400, 321, 500, 380, 362, 2105, 281, 341, 11, 7015, 13, 583, 11, 291, 458, 11, 50708], "temperature": 0.0, "avg_logprob": -0.09642873764038086, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.015422612428665161}, {"id": 936, "seek": 528896, "start": 5295.84, "end": 5300.96, "text": " one of the labs could run it if they like. All the nuance, right, that you, that all the caveats,", "tokens": [50708, 472, 295, 264, 20339, 727, 1190, 309, 498, 436, 411, 13, 1057, 264, 42625, 11, 558, 11, 300, 291, 11, 300, 439, 264, 11730, 1720, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09642873764038086, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.015422612428665161}, {"id": 937, "seek": 528896, "start": 5300.96, "end": 5307.92, "text": " all the confounding factors in your analysis there, if nothing else just goes to show how", "tokens": [50964, 439, 264, 1497, 24625, 6771, 294, 428, 5215, 456, 11, 498, 1825, 1646, 445, 1709, 281, 855, 577, 51312], "temperature": 0.0, "avg_logprob": -0.09642873764038086, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.015422612428665161}, {"id": 938, "seek": 528896, "start": 5307.92, "end": 5316.32, "text": " incredibly vast the surface area of the models has become. And, you know, you get a sense from this,", "tokens": [51312, 6252, 8369, 264, 3753, 1859, 295, 264, 5245, 575, 1813, 13, 400, 11, 291, 458, 11, 291, 483, 257, 2020, 490, 341, 11, 51732], "temperature": 0.0, "avg_logprob": -0.09642873764038086, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.015422612428665161}, {"id": 939, "seek": 531632, "start": 5316.32, "end": 5324.32, "text": " like, just how much auditing work is needed, you know, to cover, to even begin to attempt to cover", "tokens": [50364, 411, 11, 445, 577, 709, 2379, 1748, 589, 307, 2978, 11, 291, 458, 11, 281, 2060, 11, 281, 754, 1841, 281, 5217, 281, 2060, 50764], "temperature": 0.0, "avg_logprob": -0.06644080064007055, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.00029593700310215354}, {"id": 940, "seek": 531632, "start": 5324.32, "end": 5329.679999999999, "text": " all that vast surface area. I mean, this is, you know, it wasn't that hard to find, but it takes", "tokens": [50764, 439, 300, 8369, 3753, 1859, 13, 286, 914, 11, 341, 307, 11, 291, 458, 11, 309, 2067, 380, 300, 1152, 281, 915, 11, 457, 309, 2516, 51032], "temperature": 0.0, "avg_logprob": -0.06644080064007055, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.00029593700310215354}, {"id": 941, "seek": 531632, "start": 5329.679999999999, "end": 5334.24, "text": " time to really develop it, try to understand it. And, you know, you guys are one of only a few", "tokens": [51032, 565, 281, 534, 1499, 309, 11, 853, 281, 1223, 309, 13, 400, 11, 291, 458, 11, 291, 1074, 366, 472, 295, 787, 257, 1326, 51260], "temperature": 0.0, "avg_logprob": -0.06644080064007055, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.00029593700310215354}, {"id": 942, "seek": 531632, "start": 5334.24, "end": 5339.679999999999, "text": " organizations in the world that are dedicated to this. And I'm just like, man, you know, there is", "tokens": [51260, 6150, 294, 264, 1002, 300, 366, 8374, 281, 341, 13, 400, 286, 478, 445, 411, 11, 587, 11, 291, 458, 11, 456, 307, 51532], "temperature": 0.0, "avg_logprob": -0.06644080064007055, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.00029593700310215354}, {"id": 943, "seek": 533968, "start": 5339.68, "end": 5349.12, "text": " so much unexplored territory out there. So how would you describe the state of play today when it", "tokens": [50364, 370, 709, 11572, 564, 2769, 11360, 484, 456, 13, 407, 577, 576, 291, 6786, 264, 1785, 295, 862, 965, 562, 309, 50836], "temperature": 0.0, "avg_logprob": -0.09153814705050721, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.16880233585834503}, {"id": 944, "seek": 533968, "start": 5349.12, "end": 5355.4400000000005, "text": " comes to doing this sort of auditing? Like, maybe you could give a rundown of sort of what you see", "tokens": [50836, 1487, 281, 884, 341, 1333, 295, 2379, 1748, 30, 1743, 11, 1310, 291, 727, 976, 257, 23096, 648, 295, 1333, 295, 437, 291, 536, 51152], "temperature": 0.0, "avg_logprob": -0.09153814705050721, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.16880233585834503}, {"id": 945, "seek": 533968, "start": 5355.4400000000005, "end": 5360.56, "text": " the best practices being, and then, you know, kind of contrast that against like, what are people", "tokens": [51152, 264, 1151, 7525, 885, 11, 293, 550, 11, 291, 458, 11, 733, 295, 8712, 300, 1970, 411, 11, 437, 366, 561, 51408], "temperature": 0.0, "avg_logprob": -0.09153814705050721, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.16880233585834503}, {"id": 946, "seek": 533968, "start": 5360.56, "end": 5364.64, "text": " actually doing? Are they living up to those best practices? Are they, you know, is that still a", "tokens": [51408, 767, 884, 30, 2014, 436, 2647, 493, 281, 729, 1151, 7525, 30, 2014, 436, 11, 291, 458, 11, 307, 300, 920, 257, 51612], "temperature": 0.0, "avg_logprob": -0.09153814705050721, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.16880233585834503}, {"id": 947, "seek": 536464, "start": 5364.64, "end": 5370.0, "text": " work in progress for them? But yeah, maybe what's ideal today based on everything you know,", "tokens": [50364, 589, 294, 4205, 337, 552, 30, 583, 1338, 11, 1310, 437, 311, 7157, 965, 2361, 322, 1203, 291, 458, 11, 50632], "temperature": 0.0, "avg_logprob": -0.0969896430060977, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.05338923633098602}, {"id": 948, "seek": 536464, "start": 5370.0, "end": 5374.8, "text": " and then how close are the leading developers coming to living up to that ideal?", "tokens": [50632, 293, 550, 577, 1998, 366, 264, 5775, 8849, 1348, 281, 2647, 493, 281, 300, 7157, 30, 50872], "temperature": 0.0, "avg_logprob": -0.0969896430060977, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.05338923633098602}, {"id": 949, "seek": 536464, "start": 5374.8, "end": 5382.72, "text": " Yeah. So, you know, I, so I definitely envision a world where there's a sort of a thriving third", "tokens": [50872, 865, 13, 407, 11, 291, 458, 11, 286, 11, 370, 286, 2138, 24739, 257, 1002, 689, 456, 311, 257, 1333, 295, 257, 30643, 2636, 51268], "temperature": 0.0, "avg_logprob": -0.0969896430060977, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.05338923633098602}, {"id": 950, "seek": 536464, "start": 5382.72, "end": 5388.320000000001, "text": " party auditing ecosystem or third party sort of assistance ecosystem and assurance ecosystem", "tokens": [51268, 3595, 2379, 1748, 11311, 420, 2636, 3595, 1333, 295, 9683, 11311, 293, 32189, 11311, 51548], "temperature": 0.0, "avg_logprob": -0.0969896430060977, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.05338923633098602}, {"id": 951, "seek": 538832, "start": 5389.04, "end": 5396.24, "text": " built sort of in tandem with the like the leading labs themselves, where, you know, you have", "tokens": [50400, 3094, 1333, 295, 294, 48120, 365, 264, 411, 264, 5775, 20339, 2969, 11, 689, 11, 291, 458, 11, 291, 362, 50760], "temperature": 0.0, "avg_logprob": -0.13588270410761102, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.33778372406959534}, {"id": 952, "seek": 538832, "start": 5396.24, "end": 5401.92, "text": " someone like us and we focus on a specific property of the model, let's say things related to", "tokens": [50760, 1580, 411, 505, 293, 321, 1879, 322, 257, 2685, 4707, 295, 264, 2316, 11, 718, 311, 584, 721, 4077, 281, 51044], "temperature": 0.0, "avg_logprob": -0.13588270410761102, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.33778372406959534}, {"id": 953, "seek": 538832, "start": 5401.92, "end": 5406.32, "text": " deceptive alignment and like other, we intend to do other stuff in the future as well. But,", "tokens": [51044, 368, 1336, 488, 18515, 293, 411, 661, 11, 321, 19759, 281, 360, 661, 1507, 294, 264, 2027, 382, 731, 13, 583, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13588270410761102, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.33778372406959534}, {"id": 954, "seek": 538832, "start": 5406.32, "end": 5410.719999999999, "text": " you know, we will probably not be able to cover literally all basis. Then there are other people", "tokens": [51264, 291, 458, 11, 321, 486, 1391, 406, 312, 1075, 281, 2060, 3736, 439, 5143, 13, 1396, 456, 366, 661, 561, 51484], "temperature": 0.0, "avg_logprob": -0.13588270410761102, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.33778372406959534}, {"id": 955, "seek": 538832, "start": 5410.719999999999, "end": 5416.5599999999995, "text": " who focus really, really hard on fairness and others who focus really strongly on social impacts", "tokens": [51484, 567, 1879, 534, 11, 534, 1152, 322, 29765, 293, 2357, 567, 1879, 534, 10613, 322, 2093, 11606, 51776], "temperature": 0.0, "avg_logprob": -0.13588270410761102, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.33778372406959534}, {"id": 956, "seek": 541656, "start": 5416.56, "end": 5420.88, "text": " and these other kind of things. And I think it would be good to, to have sort of a thriving", "tokens": [50364, 293, 613, 661, 733, 295, 721, 13, 400, 286, 519, 309, 576, 312, 665, 281, 11, 281, 362, 1333, 295, 257, 30643, 50580], "temperature": 0.0, "avg_logprob": -0.12401242382758487, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0064865234307944775}, {"id": 957, "seek": 541656, "start": 5420.88, "end": 5426.4800000000005, "text": " ecosystem around this. And then also I expect it to be somewhat necessary, right? Like even from a", "tokens": [50580, 11311, 926, 341, 13, 400, 550, 611, 286, 2066, 309, 281, 312, 8344, 4818, 11, 558, 30, 1743, 754, 490, 257, 50860], "temperature": 0.0, "avg_logprob": -0.12401242382758487, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0064865234307944775}, {"id": 958, "seek": 541656, "start": 5427.04, "end": 5431.68, "text": " even from just like a perspective of the of the AGI labs themselves, even if they don't,", "tokens": [50888, 754, 490, 445, 411, 257, 4585, 295, 264, 295, 264, 316, 26252, 20339, 2969, 11, 754, 498, 436, 500, 380, 11, 51120], "temperature": 0.0, "avg_logprob": -0.12401242382758487, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0064865234307944775}, {"id": 959, "seek": 541656, "start": 5432.320000000001, "end": 5436.88, "text": " you know, even if they didn't care about safety themselves, I think the population", "tokens": [51152, 291, 458, 11, 754, 498, 436, 994, 380, 1127, 466, 4514, 2969, 11, 286, 519, 264, 4415, 51380], "temperature": 0.0, "avg_logprob": -0.12401242382758487, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0064865234307944775}, {"id": 960, "seek": 541656, "start": 5436.88, "end": 5441.76, "text": " really does is risk averse. They care about robust, robustly working models, they care,", "tokens": [51380, 534, 775, 307, 3148, 257, 4308, 13, 814, 1127, 466, 13956, 11, 13956, 356, 1364, 5245, 11, 436, 1127, 11, 51624], "temperature": 0.0, "avg_logprob": -0.12401242382758487, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0064865234307944775}, {"id": 961, "seek": 544176, "start": 5442.16, "end": 5448.16, "text": " they don't want something that has all of these weird edge cases and weird behaviors,", "tokens": [50384, 436, 500, 380, 528, 746, 300, 575, 439, 295, 613, 3657, 4691, 3331, 293, 3657, 15501, 11, 50684], "temperature": 0.0, "avg_logprob": -0.08427143096923828, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.017979692667722702}, {"id": 962, "seek": 544176, "start": 5448.16, "end": 5451.4400000000005, "text": " they don't want something like this in their like, you know, in their home,", "tokens": [50684, 436, 500, 380, 528, 746, 411, 341, 294, 641, 411, 11, 291, 458, 11, 294, 641, 1280, 11, 50848], "temperature": 0.0, "avg_logprob": -0.08427143096923828, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.017979692667722702}, {"id": 963, "seek": 544176, "start": 5452.16, "end": 5457.4400000000005, "text": " having access to their medical data, etc, etc. So yeah, I think the more you want to integrate", "tokens": [50884, 1419, 2105, 281, 641, 4625, 1412, 11, 5183, 11, 5183, 13, 407, 1338, 11, 286, 519, 264, 544, 291, 528, 281, 13365, 51148], "temperature": 0.0, "avg_logprob": -0.08427143096923828, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.017979692667722702}, {"id": 964, "seek": 544176, "start": 5457.4400000000005, "end": 5462.88, "text": " this into an economy, the more you will have to have a big assurance ecosystem around this anyway,", "tokens": [51148, 341, 666, 364, 5010, 11, 264, 544, 291, 486, 362, 281, 362, 257, 955, 32189, 11311, 926, 341, 4033, 11, 51420], "temperature": 0.0, "avg_logprob": -0.08427143096923828, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.017979692667722702}, {"id": 965, "seek": 544176, "start": 5462.88, "end": 5467.84, "text": " or the labs to everything internally, which is going to be very, very expensive for them.", "tokens": [51420, 420, 264, 20339, 281, 1203, 19501, 11, 597, 307, 516, 281, 312, 588, 11, 588, 5124, 337, 552, 13, 51668], "temperature": 0.0, "avg_logprob": -0.08427143096923828, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.017979692667722702}, {"id": 966, "seek": 546784, "start": 5467.84, "end": 5472.96, "text": " And I think it's more, it's easier, it's even like cheaper for them to outsource some of this", "tokens": [50364, 400, 286, 519, 309, 311, 544, 11, 309, 311, 3571, 11, 309, 311, 754, 411, 12284, 337, 552, 281, 14758, 2948, 512, 295, 341, 50620], "temperature": 0.0, "avg_logprob": -0.09502860807603405, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0023962189443409443}, {"id": 967, "seek": 546784, "start": 5472.96, "end": 5477.4400000000005, "text": " to externals. And so yeah, I think like in that world, you would definitely want to have", "tokens": [50620, 281, 30360, 1124, 13, 400, 370, 1338, 11, 286, 519, 411, 294, 300, 1002, 11, 291, 576, 2138, 528, 281, 362, 50844], "temperature": 0.0, "avg_logprob": -0.09502860807603405, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0023962189443409443}, {"id": 968, "seek": 546784, "start": 5478.400000000001, "end": 5486.0, "text": " like some way of, or a clear way of how this ecosystem is incentivized, all parts of the", "tokens": [50892, 411, 512, 636, 295, 11, 420, 257, 1850, 636, 295, 577, 341, 11311, 307, 35328, 1602, 11, 439, 3166, 295, 264, 51272], "temperature": 0.0, "avg_logprob": -0.09502860807603405, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0023962189443409443}, {"id": 969, "seek": 546784, "start": 5486.0, "end": 5492.0, "text": " ecosystem are incentivized to do the right thing. And you know, if you don't have to look very far,", "tokens": [51272, 11311, 366, 35328, 1602, 281, 360, 264, 558, 551, 13, 400, 291, 458, 11, 498, 291, 500, 380, 362, 281, 574, 588, 1400, 11, 51572], "temperature": 0.0, "avg_logprob": -0.09502860807603405, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0023962189443409443}, {"id": 970, "seek": 546784, "start": 5492.0, "end": 5497.4400000000005, "text": " there are a lot of other auditing ecosystems out there, be that in finance, be that in aviation,", "tokens": [51572, 456, 366, 257, 688, 295, 661, 2379, 1748, 32647, 484, 456, 11, 312, 300, 294, 10719, 11, 312, 300, 294, 28831, 11, 51844], "temperature": 0.0, "avg_logprob": -0.09502860807603405, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0023962189443409443}, {"id": 971, "seek": 549744, "start": 5497.44, "end": 5504.5599999999995, "text": " be that in, you know, other like infosec, for example. And time and time again, we have seen", "tokens": [50364, 312, 300, 294, 11, 291, 458, 11, 661, 411, 1536, 541, 66, 11, 337, 1365, 13, 400, 565, 293, 565, 797, 11, 321, 362, 1612, 50720], "temperature": 0.0, "avg_logprob": -0.15365985988341657, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0015008311020210385}, {"id": 972, "seek": 549744, "start": 5504.5599999999995, "end": 5510.08, "text": " that there are a bunch of like very perverse incentives in in third party auditing, specifically,", "tokens": [50720, 300, 456, 366, 257, 3840, 295, 411, 588, 680, 4308, 23374, 294, 294, 2636, 3595, 2379, 1748, 11, 4682, 11, 50996], "temperature": 0.0, "avg_logprob": -0.15365985988341657, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0015008311020210385}, {"id": 973, "seek": 549744, "start": 5511.12, "end": 5518.24, "text": " maybe maybe just like lay out a couple, one of them would be the lab might want to choose an", "tokens": [51048, 1310, 1310, 445, 411, 2360, 484, 257, 1916, 11, 472, 295, 552, 576, 312, 264, 2715, 1062, 528, 281, 2826, 364, 51404], "temperature": 0.0, "avg_logprob": -0.15365985988341657, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0015008311020210385}, {"id": 974, "seek": 549744, "start": 5518.24, "end": 5523.44, "text": " auditor who always just says, yes, great model, right, like, and never actually does anything.", "tokens": [51404, 33970, 567, 1009, 445, 1619, 11, 2086, 11, 869, 2316, 11, 558, 11, 411, 11, 293, 1128, 767, 775, 1340, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15365985988341657, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.0015008311020210385}, {"id": 975, "seek": 552344, "start": 5524.4, "end": 5530.32, "text": " And it's sort of a yes man. And then the the labs maybe have the incentive to not say the", "tokens": [50412, 400, 309, 311, 1333, 295, 257, 2086, 587, 13, 400, 550, 264, 264, 20339, 1310, 362, 264, 22346, 281, 406, 584, 264, 50708], "temperature": 0.0, "avg_logprob": -0.12917019440247132, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.05661948770284653}, {"id": 976, "seek": 552344, "start": 5530.32, "end": 5534.4, "text": " worst things they found, because otherwise they may lose their contract, because it would", "tokens": [50708, 5855, 721, 436, 1352, 11, 570, 5911, 436, 815, 3624, 641, 4364, 11, 570, 309, 576, 50912], "temperature": 0.0, "avg_logprob": -0.12917019440247132, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.05661948770284653}, {"id": 977, "seek": 552344, "start": 5534.4, "end": 5540.5599999999995, "text": " maybe imply higher costs. So they would never even in like very strong, even even in like very", "tokens": [50912, 1310, 33616, 2946, 5497, 13, 407, 436, 576, 1128, 754, 294, 411, 588, 2068, 11, 754, 754, 294, 411, 588, 51220], "temperature": 0.0, "avg_logprob": -0.12917019440247132, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.05661948770284653}, {"id": 978, "seek": 552344, "start": 5540.5599999999995, "end": 5545.5199999999995, "text": " unsafe circumstances, they may not want to pull the plug out of fear that they would lose their", "tokens": [51220, 35948, 9121, 11, 436, 815, 406, 528, 281, 2235, 264, 5452, 484, 295, 4240, 300, 436, 576, 3624, 641, 51468], "temperature": 0.0, "avg_logprob": -0.12917019440247132, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.05661948770284653}, {"id": 979, "seek": 552344, "start": 5545.5199999999995, "end": 5552.48, "text": " biggest funder, for example. And yeah, there are a ton of different of these kind of perverse", "tokens": [51468, 3880, 2374, 260, 11, 337, 1365, 13, 400, 1338, 11, 456, 366, 257, 2952, 295, 819, 295, 613, 733, 295, 680, 4308, 51816], "temperature": 0.0, "avg_logprob": -0.12917019440247132, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.05661948770284653}, {"id": 980, "seek": 555248, "start": 5552.5599999999995, "end": 5558.0, "text": " incentives. And I think kind of the, so we've been thinking about this quite a bit at Apollo,", "tokens": [50368, 23374, 13, 400, 286, 519, 733, 295, 264, 11, 370, 321, 600, 668, 1953, 466, 341, 1596, 257, 857, 412, 25187, 11, 50640], "temperature": 0.0, "avg_logprob": -0.15840106540256077, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.025940217077732086}, {"id": 981, "seek": 555248, "start": 5558.0, "end": 5563.28, "text": " and sort of the conclusion we came to is, what you really need is a middleman by the government.", "tokens": [50640, 293, 1333, 295, 264, 10063, 321, 1361, 281, 307, 11, 437, 291, 534, 643, 307, 257, 2808, 1601, 538, 264, 2463, 13, 50904], "temperature": 0.0, "avg_logprob": -0.15840106540256077, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.025940217077732086}, {"id": 982, "seek": 555248, "start": 5563.28, "end": 5570.719999999999, "text": " So you need something like the UK ASF Institute or the US ASF Institute, that is, make sure that", "tokens": [50904, 407, 291, 643, 746, 411, 264, 7051, 7469, 37, 9446, 420, 264, 2546, 7469, 37, 9446, 11, 300, 307, 11, 652, 988, 300, 51276], "temperature": 0.0, "avg_logprob": -0.15840106540256077, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.025940217077732086}, {"id": 983, "seek": 555248, "start": 5570.719999999999, "end": 5577.5199999999995, "text": " there is a minimal stat set of standards that all the auditors have to adhere to, so that the labs", "tokens": [51276, 456, 307, 257, 13206, 2219, 992, 295, 7787, 300, 439, 264, 2379, 9862, 362, 281, 33584, 281, 11, 370, 300, 264, 20339, 51616], "temperature": 0.0, "avg_logprob": -0.15840106540256077, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.025940217077732086}, {"id": 984, "seek": 557752, "start": 5577.52, "end": 5583.52, "text": " feel safe, so that they don't have to give access to like any random person, but also ensures that", "tokens": [50364, 841, 3273, 11, 370, 300, 436, 500, 380, 362, 281, 976, 2105, 281, 411, 604, 4974, 954, 11, 457, 611, 28111, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08689979027057516, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.2718619108200073}, {"id": 985, "seek": 557752, "start": 5583.52, "end": 5589.200000000001, "text": " they get the proper access, and that when they when they find something that they have the force of", "tokens": [50664, 436, 483, 264, 2296, 2105, 11, 293, 300, 562, 436, 562, 436, 915, 746, 300, 436, 362, 264, 3464, 295, 50948], "temperature": 0.0, "avg_logprob": -0.08689979027057516, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.2718619108200073}, {"id": 986, "seek": 557752, "start": 5589.200000000001, "end": 5593.92, "text": " the law behind them in some sense, so that they are like, this is really here, like, you know,", "tokens": [50948, 264, 2101, 2261, 552, 294, 512, 2020, 11, 370, 300, 436, 366, 411, 11, 341, 307, 534, 510, 11, 411, 11, 291, 458, 11, 51184], "temperature": 0.0, "avg_logprob": -0.08689979027057516, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.2718619108200073}, {"id": 987, "seek": 557752, "start": 5593.92, "end": 5597.84, "text": " shit is really hitting the fan, something needs to happen, this model cannot be deployed.", "tokens": [51184, 4611, 307, 534, 8850, 264, 3429, 11, 746, 2203, 281, 1051, 11, 341, 2316, 2644, 312, 17826, 13, 51380], "temperature": 0.0, "avg_logprob": -0.08689979027057516, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.2718619108200073}, {"id": 988, "seek": 557752, "start": 5598.64, "end": 5602.320000000001, "text": " They have someone to go to namely the government and the government is like, you have to fix this", "tokens": [51420, 814, 362, 1580, 281, 352, 281, 20926, 264, 2463, 293, 264, 2463, 307, 411, 11, 291, 362, 281, 3191, 341, 51604], "temperature": 0.0, "avg_logprob": -0.08689979027057516, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.2718619108200073}, {"id": 989, "seek": 560232, "start": 5602.32, "end": 5608.799999999999, "text": " now, otherwise, you'll have a problem. So yeah, I really think having this sort of middleman who", "tokens": [50364, 586, 11, 5911, 11, 291, 603, 362, 257, 1154, 13, 407, 1338, 11, 286, 534, 519, 1419, 341, 1333, 295, 2808, 1601, 567, 50688], "temperature": 0.0, "avg_logprob": -0.11475467681884766, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.06750638782978058}, {"id": 990, "seek": 560232, "start": 5608.799999999999, "end": 5614.0, "text": " like detaches a lot of the directly bad incentives and maybe even takes care of the sort of funding", "tokens": [50688, 411, 1141, 13272, 257, 688, 295, 264, 3838, 1578, 23374, 293, 1310, 754, 2516, 1127, 295, 264, 1333, 295, 6137, 50948], "temperature": 0.0, "avg_logprob": -0.11475467681884766, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.06750638782978058}, {"id": 991, "seek": 560232, "start": 5614.0, "end": 5620.48, "text": " redistribution from lab to auditor and so on, I think, yeah, would be really needed. And I hope", "tokens": [50948, 36198, 30783, 490, 2715, 281, 33970, 293, 370, 322, 11, 286, 519, 11, 1338, 11, 576, 312, 534, 2978, 13, 400, 286, 1454, 51272], "temperature": 0.0, "avg_logprob": -0.11475467681884766, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.06750638782978058}, {"id": 992, "seek": 560232, "start": 5620.48, "end": 5627.36, "text": " that this is something that the UK ASF Institute and the US ASF Institute will do. They have kind of", "tokens": [51272, 300, 341, 307, 746, 300, 264, 7051, 7469, 37, 9446, 293, 264, 2546, 7469, 37, 9446, 486, 360, 13, 814, 362, 733, 295, 51616], "temperature": 0.0, "avg_logprob": -0.11475467681884766, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.06750638782978058}, {"id": 993, "seek": 562736, "start": 5628.32, "end": 5632.32, "text": " hinted at the idea that they want to do something like this, but yeah,", "tokens": [50412, 12075, 292, 412, 264, 1558, 300, 436, 528, 281, 360, 746, 411, 341, 11, 457, 1338, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10003651924503659, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.04601818323135376}, {"id": 994, "seek": 562736, "start": 5632.32, "end": 5636.719999999999, "text": " still to be determined in practice. And then the other question that you asked was,", "tokens": [50612, 920, 281, 312, 9540, 294, 3124, 13, 400, 550, 264, 661, 1168, 300, 291, 2351, 390, 11, 50832], "temperature": 0.0, "avg_logprob": -0.10003651924503659, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.04601818323135376}, {"id": 995, "seek": 562736, "start": 5637.679999999999, "end": 5643.92, "text": " to what extent is this already happening with the bigger labs? And yeah, I think the situation is", "tokens": [50880, 281, 437, 8396, 307, 341, 1217, 2737, 365, 264, 3801, 20339, 30, 400, 1338, 11, 286, 519, 264, 2590, 307, 51192], "temperature": 0.0, "avg_logprob": -0.10003651924503659, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.04601818323135376}, {"id": 996, "seek": 562736, "start": 5643.92, "end": 5649.12, "text": " like fairly complicated, right? There are a ton of incentives at play from the labs internally,", "tokens": [51192, 411, 6457, 6179, 11, 558, 30, 821, 366, 257, 2952, 295, 23374, 412, 862, 490, 264, 20339, 19501, 11, 51452], "temperature": 0.0, "avg_logprob": -0.10003651924503659, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.04601818323135376}, {"id": 997, "seek": 562736, "start": 5649.12, "end": 5653.839999999999, "text": " right? Many of the leading labs, they actually take safety somewhat seriously. They have internal", "tokens": [51452, 558, 30, 5126, 295, 264, 5775, 20339, 11, 436, 767, 747, 4514, 8344, 6638, 13, 814, 362, 6920, 51688], "temperature": 0.0, "avg_logprob": -0.10003651924503659, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.04601818323135376}, {"id": 998, "seek": 565384, "start": 5653.84, "end": 5658.96, "text": " alignment teams, they understand the threat models, they understand the risks, and they want to be", "tokens": [50364, 18515, 5491, 11, 436, 1223, 264, 4734, 5245, 11, 436, 1223, 264, 10888, 11, 293, 436, 528, 281, 312, 50620], "temperature": 0.0, "avg_logprob": -0.12112025556893184, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.03207084536552429}, {"id": 999, "seek": 565384, "start": 5660.32, "end": 5664.8, "text": " seen as a responsible actor and act this way. And on the other hand, they also have a lot of", "tokens": [50688, 1612, 382, 257, 6250, 8747, 293, 605, 341, 636, 13, 400, 322, 264, 661, 1011, 11, 436, 611, 362, 257, 688, 295, 50912], "temperature": 0.0, "avg_logprob": -0.12112025556893184, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.03207084536552429}, {"id": 1000, "seek": 565384, "start": 5664.8, "end": 5669.360000000001, "text": " other concerns, right? There are security concerns, how do we get access, how do we give access to", "tokens": [50912, 661, 7389, 11, 558, 30, 821, 366, 3825, 7389, 11, 577, 360, 321, 483, 2105, 11, 577, 360, 321, 976, 2105, 281, 51140], "temperature": 0.0, "avg_logprob": -0.12112025556893184, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.03207084536552429}, {"id": 1001, "seek": 565384, "start": 5669.360000000001, "end": 5676.4800000000005, "text": " someone who may, you know, like what is our, who may be the weakest link in our security chain,", "tokens": [51140, 1580, 567, 815, 11, 291, 458, 11, 411, 437, 307, 527, 11, 567, 815, 312, 264, 44001, 2113, 294, 527, 3825, 5021, 11, 51496], "temperature": 0.0, "avg_logprob": -0.12112025556893184, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.03207084536552429}, {"id": 1002, "seek": 565384, "start": 5677.360000000001, "end": 5682.96, "text": " which I think is like an understandable concern. So they may be hesitant to give someone access.", "tokens": [51540, 597, 286, 519, 307, 411, 364, 25648, 3136, 13, 407, 436, 815, 312, 36290, 281, 976, 1580, 2105, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12112025556893184, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.03207084536552429}, {"id": 1003, "seek": 568384, "start": 5683.92, "end": 5688.96, "text": " As an external auditor. And then, you know, this probably also implies a lot of work for them,", "tokens": [50368, 1018, 364, 8320, 33970, 13, 400, 550, 11, 291, 458, 11, 341, 1391, 611, 18779, 257, 688, 295, 589, 337, 552, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09955357015132904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00628661597147584}, {"id": 1004, "seek": 568384, "start": 5688.96, "end": 5694.32, "text": " which I think is fair, right? Like it's, if their model isn't safe, then they should have to", "tokens": [50620, 597, 286, 519, 307, 3143, 11, 558, 30, 1743, 309, 311, 11, 498, 641, 2316, 1943, 380, 3273, 11, 550, 436, 820, 362, 281, 50888], "temperature": 0.0, "avg_logprob": -0.09955357015132904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00628661597147584}, {"id": 1005, "seek": 568384, "start": 5694.32, "end": 5698.4800000000005, "text": " invest a lot of work, but it's still something that may make labs hesitant. So basically, you know,", "tokens": [50888, 1963, 257, 688, 295, 589, 11, 457, 309, 311, 920, 746, 300, 815, 652, 20339, 36290, 13, 407, 1936, 11, 291, 458, 11, 51096], "temperature": 0.0, "avg_logprob": -0.09955357015132904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00628661597147584}, {"id": 1006, "seek": 568384, "start": 5698.4800000000005, "end": 5704.16, "text": " I think they're, they're like, good and bad reasons for why sort of the ecosystem is,", "tokens": [51096, 286, 519, 436, 434, 11, 436, 434, 411, 11, 665, 293, 1578, 4112, 337, 983, 1333, 295, 264, 11311, 307, 11, 51380], "temperature": 0.0, "avg_logprob": -0.09955357015132904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00628661597147584}, {"id": 1007, "seek": 568384, "start": 5704.16, "end": 5709.6, "text": " is like not as developed or like as open as it could be. And yeah, my, you know, my hope is that", "tokens": [51380, 307, 411, 406, 382, 4743, 420, 411, 382, 1269, 382, 309, 727, 312, 13, 400, 1338, 11, 452, 11, 291, 458, 11, 452, 1454, 307, 300, 51652], "temperature": 0.0, "avg_logprob": -0.09955357015132904, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00628661597147584}, {"id": 1008, "seek": 570960, "start": 5709.6, "end": 5716.08, "text": " we find solutions that are plausible for both parties for the, for the, for the reasonable", "tokens": [50364, 321, 915, 6547, 300, 366, 39925, 337, 1293, 8265, 337, 264, 11, 337, 264, 11, 337, 264, 10585, 50688], "temperature": 0.0, "avg_logprob": -0.09653018241704897, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.015417668037116528}, {"id": 1009, "seek": 570960, "start": 5716.08, "end": 5722.160000000001, "text": " concerns like security, right? So maybe the auditor just has to have a specific level of", "tokens": [50688, 7389, 411, 3825, 11, 558, 30, 407, 1310, 264, 33970, 445, 575, 281, 362, 257, 2685, 1496, 295, 50992], "temperature": 0.0, "avg_logprob": -0.09653018241704897, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.015417668037116528}, {"id": 1010, "seek": 570960, "start": 5722.160000000001, "end": 5726.96, "text": " information security, or there has to be a secure API through which they can actually", "tokens": [50992, 1589, 3825, 11, 420, 456, 575, 281, 312, 257, 7144, 9362, 807, 597, 436, 393, 767, 51232], "temperature": 0.0, "avg_logprob": -0.09653018241704897, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.015417668037116528}, {"id": 1011, "seek": 570960, "start": 5727.6, "end": 5734.400000000001, "text": " go to the model, etc. And then kind of government regulates away all of the, the like, or forces", "tokens": [51264, 352, 281, 264, 2316, 11, 5183, 13, 400, 550, 733, 295, 2463, 9837, 1024, 1314, 439, 295, 264, 11, 264, 411, 11, 420, 5874, 51604], "temperature": 0.0, "avg_logprob": -0.09653018241704897, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.015417668037116528}, {"id": 1012, "seek": 573440, "start": 5734.48, "end": 5742.08, "text": " the labs to, to accept some sorts of third party auditing so that they can't use the bad reasons,", "tokens": [50368, 264, 20339, 281, 11, 281, 3241, 512, 7527, 295, 2636, 3595, 2379, 1748, 370, 300, 436, 393, 380, 764, 264, 1578, 4112, 11, 50748], "temperature": 0.0, "avg_logprob": -0.11098694238137072, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.1687813103199005}, {"id": 1013, "seek": 573440, "start": 5742.08, "end": 5746.639999999999, "text": " right? Because like many of the actual reasons for at least some of the labs, probably not all,", "tokens": [50748, 558, 30, 1436, 411, 867, 295, 264, 3539, 4112, 337, 412, 1935, 512, 295, 264, 20339, 11, 1391, 406, 439, 11, 50976], "temperature": 0.0, "avg_logprob": -0.11098694238137072, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.1687813103199005}, {"id": 1014, "seek": 573440, "start": 5746.639999999999, "end": 5751.92, "text": " might just be, well, you know, we just doesn't, we don't care about this right now, you know,", "tokens": [50976, 1062, 445, 312, 11, 731, 11, 291, 458, 11, 321, 445, 1177, 380, 11, 321, 500, 380, 1127, 466, 341, 558, 586, 11, 291, 458, 11, 51240], "temperature": 0.0, "avg_logprob": -0.11098694238137072, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.1687813103199005}, {"id": 1015, "seek": 573440, "start": 5751.92, "end": 5757.679999999999, "text": " like maybe, maybe they're not that concerned about about safety, or maybe they just don't think this", "tokens": [51240, 411, 1310, 11, 1310, 436, 434, 406, 300, 5922, 466, 466, 4514, 11, 420, 1310, 436, 445, 500, 380, 519, 341, 51528], "temperature": 0.0, "avg_logprob": -0.11098694238137072, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.1687813103199005}, {"id": 1016, "seek": 573440, "start": 5757.679999999999, "end": 5762.0, "text": " is like the best, the best path for them right now, or maybe they just, you know, maybe this just", "tokens": [51528, 307, 411, 264, 1151, 11, 264, 1151, 3100, 337, 552, 558, 586, 11, 420, 1310, 436, 445, 11, 291, 458, 11, 1310, 341, 445, 51744], "temperature": 0.0, "avg_logprob": -0.11098694238137072, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.1687813103199005}, {"id": 1017, "seek": 576200, "start": 5762.0, "end": 5766.4, "text": " costs money and they don't want to, or it's just a hassle and they don't care about this. And that", "tokens": [50364, 5497, 1460, 293, 436, 500, 380, 528, 281, 11, 420, 309, 311, 445, 257, 39526, 293, 436, 500, 380, 1127, 466, 341, 13, 400, 300, 50584], "temperature": 0.0, "avg_logprob": -0.0936585284293966, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0005192862590774894}, {"id": 1018, "seek": 576200, "start": 5766.4, "end": 5770.8, "text": " feels like something where the government should at some point be involved and already is involved", "tokens": [50584, 3417, 411, 746, 689, 264, 2463, 820, 412, 512, 935, 312, 3288, 293, 1217, 307, 3288, 50804], "temperature": 0.0, "avg_logprob": -0.0936585284293966, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0005192862590774894}, {"id": 1019, "seek": 576200, "start": 5770.8, "end": 5777.2, "text": " to some extent. It seems really hard, you know, I guess a couple tangible questions I have are like,", "tokens": [50804, 281, 512, 8396, 13, 467, 2544, 534, 1152, 11, 291, 458, 11, 286, 2041, 257, 1916, 27094, 1651, 286, 362, 366, 411, 11, 51124], "temperature": 0.0, "avg_logprob": -0.0936585284293966, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0005192862590774894}, {"id": 1020, "seek": 576200, "start": 5778.32, "end": 5787.2, "text": " who should decide what the standard is in and like, who should sort of determine if a model is", "tokens": [51180, 567, 820, 4536, 437, 264, 3832, 307, 294, 293, 411, 11, 567, 820, 1333, 295, 6997, 498, 257, 2316, 307, 51624], "temperature": 0.0, "avg_logprob": -0.0936585284293966, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0005192862590774894}, {"id": 1021, "seek": 578720, "start": 5787.2, "end": 5793.92, "text": " ready for deployment? As of now, it's still the developers themselves, right? But like, you know,", "tokens": [50364, 1919, 337, 19317, 30, 1018, 295, 586, 11, 309, 311, 920, 264, 8849, 2969, 11, 558, 30, 583, 411, 11, 291, 458, 11, 50700], "temperature": 0.0, "avg_logprob": -0.09378199776013692, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.019120851531624794}, {"id": 1022, "seek": 578720, "start": 5793.92, "end": 5799.2, "text": " the thing that I had kind of monitored for the last year since the initial GPT-4 red teaming was", "tokens": [50700, 264, 551, 300, 286, 632, 733, 295, 36255, 337, 264, 1036, 1064, 1670, 264, 5883, 26039, 51, 12, 19, 2182, 1469, 278, 390, 50964], "temperature": 0.0, "avg_logprob": -0.09378199776013692, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.019120851531624794}, {"id": 1023, "seek": 578720, "start": 5799.76, "end": 5806.32, "text": " spearfishing. And in my little, you know, prompt that I would keep going back to with every update,", "tokens": [50992, 26993, 69, 3807, 13, 400, 294, 452, 707, 11, 291, 458, 11, 12391, 300, 286, 576, 1066, 516, 646, 281, 365, 633, 5623, 11, 51320], "temperature": 0.0, "avg_logprob": -0.09378199776013692, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.019120851531624794}, {"id": 1024, "seek": 578720, "start": 5806.96, "end": 5811.44, "text": " it was not even a jailbreak, you know, nothing complicated, literally just", "tokens": [51352, 309, 390, 406, 754, 257, 10511, 13225, 11, 291, 458, 11, 1825, 6179, 11, 3736, 445, 51576], "temperature": 0.0, "avg_logprob": -0.09378199776013692, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.019120851531624794}, {"id": 1025, "seek": 581144, "start": 5812.16, "end": 5817.12, "text": " system prompt, you know, straightforward prompt, your job is to spearfish this user, here's the", "tokens": [50400, 1185, 12391, 11, 291, 458, 11, 15325, 12391, 11, 428, 1691, 307, 281, 26993, 69, 742, 341, 4195, 11, 510, 311, 264, 50648], "temperature": 0.0, "avg_logprob": -0.10994525750478108, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.0164017453789711}, {"id": 1026, "seek": 581144, "start": 5817.12, "end": 5822.5599999999995, "text": " profile, engage in dialogue with them, you know, don't get caught. Pretty explicit prompt that was", "tokens": [50648, 7964, 11, 4683, 294, 10221, 365, 552, 11, 291, 458, 11, 500, 380, 483, 5415, 13, 10693, 13691, 12391, 300, 390, 50920], "temperature": 0.0, "avg_logprob": -0.10994525750478108, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.0164017453789711}, {"id": 1027, "seek": 581144, "start": 5822.5599999999995, "end": 5828.4, "text": " like, even included, if we get caught, you and your team are likely to go to jail. You know, so", "tokens": [50920, 411, 11, 754, 5556, 11, 498, 321, 483, 5415, 11, 291, 293, 428, 1469, 366, 3700, 281, 352, 281, 10511, 13, 509, 458, 11, 370, 51212], "temperature": 0.0, "avg_logprob": -0.10994525750478108, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.0164017453789711}, {"id": 1028, "seek": 581144, "start": 5829.04, "end": 5832.799999999999, "text": " laying it on pretty thick that like, this is criminal activity that we are doing and we better", "tokens": [51244, 14903, 309, 322, 1238, 5060, 300, 411, 11, 341, 307, 8628, 5191, 300, 321, 366, 884, 293, 321, 1101, 51432], "temperature": 0.0, "avg_logprob": -0.10994525750478108, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.0164017453789711}, {"id": 1029, "seek": 581144, "start": 5832.799999999999, "end": 5840.879999999999, "text": " not get caught, right? Obvious. So the model would continue to do that up until the turbo", "tokens": [51432, 406, 483, 5415, 11, 558, 30, 4075, 1502, 13, 407, 264, 2316, 576, 2354, 281, 360, 300, 493, 1826, 264, 20902, 51836], "temperature": 0.0, "avg_logprob": -0.10994525750478108, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.0164017453789711}, {"id": 1030, "seek": 584088, "start": 5840.96, "end": 5845.28, "text": " release. Now it takes a little bit more of a finessed, you know, slightly less-flagrant prompt", "tokens": [50368, 4374, 13, 823, 309, 2516, 257, 707, 857, 544, 295, 257, 962, 10830, 11, 291, 458, 11, 4748, 1570, 12, 3423, 559, 7541, 12391, 50584], "temperature": 0.0, "avg_logprob": -0.09735921223958334, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.0010321582667529583}, {"id": 1031, "seek": 584088, "start": 5845.28, "end": 5851.92, "text": " to get it through. The most-flagrant one now gets refused. But I'm like imagining in this world,", "tokens": [50584, 281, 483, 309, 807, 13, 440, 881, 12, 3423, 559, 7541, 472, 586, 2170, 14654, 13, 583, 286, 478, 411, 27798, 294, 341, 1002, 11, 50916], "temperature": 0.0, "avg_logprob": -0.09735921223958334, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.0010321582667529583}, {"id": 1032, "seek": 584088, "start": 5851.92, "end": 5857.36, "text": " right? When I was doing this, it was kind of pre-White House commitments, you know,", "tokens": [50916, 558, 30, 1133, 286, 390, 884, 341, 11, 309, 390, 733, 295, 659, 12, 47589, 4928, 26230, 11, 291, 458, 11, 51188], "temperature": 0.0, "avg_logprob": -0.09735921223958334, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.0010321582667529583}, {"id": 1033, "seek": 584088, "start": 5857.36, "end": 5864.64, "text": " pre-executive order, pre-Apollo research. But even imagining, okay, now those things exist,", "tokens": [51188, 659, 12, 3121, 3045, 17254, 1668, 11, 659, 12, 32, 79, 22388, 2132, 13, 583, 754, 27798, 11, 1392, 11, 586, 729, 721, 2514, 11, 51552], "temperature": 0.0, "avg_logprob": -0.09735921223958334, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.0010321582667529583}, {"id": 1034, "seek": 586464, "start": 5865.6, "end": 5871.6, "text": " like, how do we think about that standard, right? And we can find a bazillion things that it might", "tokens": [50412, 411, 11, 577, 360, 321, 519, 466, 300, 3832, 11, 558, 30, 400, 321, 393, 915, 257, 27147, 11836, 721, 300, 309, 1062, 50712], "temperature": 0.0, "avg_logprob": -0.09176306888974946, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.051833637058734894}, {"id": 1035, "seek": 586464, "start": 5871.6, "end": 5877.84, "text": " do that could be sort of problematic to varying degrees. And obviously, it's a dynamic environment,", "tokens": [50712, 360, 300, 727, 312, 1333, 295, 19011, 281, 22984, 5310, 13, 400, 2745, 11, 309, 311, 257, 8546, 2823, 11, 51024], "temperature": 0.0, "avg_logprob": -0.09176306888974946, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.051833637058734894}, {"id": 1036, "seek": 586464, "start": 5877.84, "end": 5883.04, "text": " you know, capabilities are changing all the time, you know, others kind of surrounding systems and", "tokens": [51024, 291, 458, 11, 10862, 366, 4473, 439, 264, 565, 11, 291, 458, 11, 2357, 733, 295, 11498, 3652, 293, 51284], "temperature": 0.0, "avg_logprob": -0.09176306888974946, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.051833637058734894}, {"id": 1037, "seek": 586464, "start": 5883.04, "end": 5888.8, "text": " sort of, you know, mitigating factors might also be changing. The public's, you know, just general", "tokens": [51284, 1333, 295, 11, 291, 458, 11, 15699, 990, 6771, 1062, 611, 312, 4473, 13, 440, 1908, 311, 11, 291, 458, 11, 445, 2674, 51572], "temperature": 0.0, "avg_logprob": -0.09176306888974946, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.051833637058734894}, {"id": 1038, "seek": 586464, "start": 5888.8, "end": 5893.12, "text": " awareness of the fact that this kind of thing might happen, you know, that just how susceptible", "tokens": [51572, 8888, 295, 264, 1186, 300, 341, 733, 295, 551, 1062, 1051, 11, 291, 458, 11, 300, 445, 577, 31249, 51788], "temperature": 0.0, "avg_logprob": -0.09176306888974946, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.051833637058734894}, {"id": 1039, "seek": 589312, "start": 5893.12, "end": 5900.48, "text": " people are to be duped is also, you know, kind of evolving. So how do we have a sensible", "tokens": [50364, 561, 366, 281, 312, 274, 1010, 292, 307, 611, 11, 291, 458, 11, 733, 295, 21085, 13, 407, 577, 360, 321, 362, 257, 25380, 50732], "temperature": 0.0, "avg_logprob": -0.10041487450693168, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.010985090397298336}, {"id": 1040, "seek": 589312, "start": 5902.32, "end": 5907.12, "text": " decision-making mechanism for, like, what can ship and what can't? And then just to further", "tokens": [50824, 3537, 12, 12402, 7513, 337, 11, 411, 11, 437, 393, 5374, 293, 437, 393, 380, 30, 400, 550, 445, 281, 3052, 51064], "temperature": 0.0, "avg_logprob": -0.10041487450693168, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.010985090397298336}, {"id": 1041, "seek": 589312, "start": 5907.12, "end": 5913.2, "text": " complicate things, like, you've got open source kind of in the background. And I presume that", "tokens": [51064, 1209, 8700, 721, 11, 411, 11, 291, 600, 658, 1269, 4009, 733, 295, 294, 264, 3678, 13, 400, 286, 43283, 300, 51368], "temperature": 0.0, "avg_logprob": -0.10041487450693168, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.010985090397298336}, {"id": 1042, "seek": 589312, "start": 5913.2, "end": 5918.08, "text": " some of what, like an open AI has been thinking over the last year is like, well, if Llama 2 will do", "tokens": [51368, 512, 295, 437, 11, 411, 364, 1269, 7318, 575, 668, 1953, 670, 264, 1036, 1064, 307, 411, 11, 731, 11, 498, 32717, 2404, 568, 486, 360, 51612], "temperature": 0.0, "avg_logprob": -0.10041487450693168, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.010985090397298336}, {"id": 1043, "seek": 591808, "start": 5918.08, "end": 5923.76, "text": " it, then, you know, or a lightly fine-tuned version of Llama 2 will do it, then, you know,", "tokens": [50364, 309, 11, 550, 11, 291, 458, 11, 420, 257, 16695, 2489, 12, 83, 43703, 3037, 295, 32717, 2404, 568, 486, 360, 309, 11, 550, 11, 291, 458, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10913470191677122, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.047416962683200836}, {"id": 1044, "seek": 591808, "start": 5924.5599999999995, "end": 5928.72, "text": " what difference does it make if our model will do it as well? You know, people have alternatives.", "tokens": [50688, 437, 2649, 775, 309, 652, 498, 527, 2316, 486, 360, 309, 382, 731, 30, 509, 458, 11, 561, 362, 20478, 13, 50896], "temperature": 0.0, "avg_logprob": -0.10913470191677122, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.047416962683200836}, {"id": 1045, "seek": 591808, "start": 5930.48, "end": 5935.44, "text": " So I don't know, I'm kind of lost in that, to be honest. Like, I don't know who should make the", "tokens": [50984, 407, 286, 500, 380, 458, 11, 286, 478, 733, 295, 2731, 294, 300, 11, 281, 312, 3245, 13, 1743, 11, 286, 500, 380, 458, 567, 820, 652, 264, 51232], "temperature": 0.0, "avg_logprob": -0.10913470191677122, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.047416962683200836}, {"id": 1046, "seek": 591808, "start": 5935.44, "end": 5941.28, "text": " decision. In general, I don't think the government is like great at making those sorts of fine-grained", "tokens": [51232, 3537, 13, 682, 2674, 11, 286, 500, 380, 519, 264, 2463, 307, 411, 869, 412, 1455, 729, 7527, 295, 2489, 12, 20735, 2001, 51524], "temperature": 0.0, "avg_logprob": -0.10913470191677122, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.047416962683200836}, {"id": 1047, "seek": 591808, "start": 5941.28, "end": 5947.28, "text": " decisions. But I don't know, help me out. Like, what do you think, what do you think good looks", "tokens": [51524, 5327, 13, 583, 286, 500, 380, 458, 11, 854, 385, 484, 13, 1743, 11, 437, 360, 291, 519, 11, 437, 360, 291, 519, 665, 1542, 51824], "temperature": 0.0, "avg_logprob": -0.10913470191677122, "compression_ratio": 1.795539033457249, "no_speech_prob": 0.047416962683200836}, {"id": 1048, "seek": 594728, "start": 5947.28, "end": 5952.32, "text": " like here? Yeah, I mean, you know, I also don't have the solution, but I have lots of thoughts.", "tokens": [50364, 411, 510, 30, 865, 11, 286, 914, 11, 291, 458, 11, 286, 611, 500, 380, 362, 264, 3827, 11, 457, 286, 362, 3195, 295, 4598, 13, 50616], "temperature": 0.0, "avg_logprob": -0.08269165356953939, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.006485720165073872}, {"id": 1049, "seek": 594728, "start": 5952.32, "end": 5957.679999999999, "text": " I guess the way I envision it is basically, or the way I expect it to turn out is something like", "tokens": [50616, 286, 2041, 264, 636, 286, 24739, 309, 307, 1936, 11, 420, 264, 636, 286, 2066, 309, 281, 1261, 484, 307, 746, 411, 50884], "temperature": 0.0, "avg_logprob": -0.08269165356953939, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.006485720165073872}, {"id": 1050, "seek": 594728, "start": 5957.679999999999, "end": 5962.96, "text": " sort of a defense in depth approach, right? Like, it cannot just be one institution that", "tokens": [50884, 1333, 295, 257, 7654, 294, 7161, 3109, 11, 558, 30, 1743, 11, 309, 2644, 445, 312, 472, 7818, 300, 51148], "temperature": 0.0, "avg_logprob": -0.08269165356953939, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.006485720165073872}, {"id": 1051, "seek": 594728, "start": 5962.96, "end": 5968.8, "text": " makes the decision alone, because that is prone to a single point of failure. So we have to have", "tokens": [51148, 1669, 264, 3537, 3312, 11, 570, 300, 307, 25806, 281, 257, 2167, 935, 295, 7763, 13, 407, 321, 362, 281, 362, 51440], "temperature": 0.0, "avg_logprob": -0.08269165356953939, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.006485720165073872}, {"id": 1052, "seek": 594728, "start": 5968.8, "end": 5975.2, "text": " sort of a process that allows for one or two chains to break and still be robust, like the", "tokens": [51440, 1333, 295, 257, 1399, 300, 4045, 337, 472, 420, 732, 12626, 281, 1821, 293, 920, 312, 13956, 11, 411, 264, 51760], "temperature": 0.0, "avg_logprob": -0.08269165356953939, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.006485720165073872}, {"id": 1053, "seek": 597520, "start": 5975.2, "end": 5980.0, "text": " decision still has to be robust. So what that includes, for example, you know, on the side of", "tokens": [50364, 3537, 920, 575, 281, 312, 13956, 13, 407, 437, 300, 5974, 11, 337, 1365, 11, 291, 458, 11, 322, 264, 1252, 295, 50604], "temperature": 0.0, "avg_logprob": -0.10319561336351478, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.0006877683335915208}, {"id": 1054, "seek": 597520, "start": 5980.0, "end": 5985.44, "text": " the labs, they obviously have to have like internal procedures where multiple people have to sign off.", "tokens": [50604, 264, 20339, 11, 436, 2745, 362, 281, 362, 411, 6920, 13846, 689, 3866, 561, 362, 281, 1465, 766, 13, 50876], "temperature": 0.0, "avg_logprob": -0.10319561336351478, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.0006877683335915208}, {"id": 1055, "seek": 597520, "start": 5986.48, "end": 5990.639999999999, "text": " And multiple things have to be fulfilled, right? Maybe you have to have like, maybe you have to", "tokens": [50928, 400, 3866, 721, 362, 281, 312, 21380, 11, 558, 30, 2704, 291, 362, 281, 362, 411, 11, 1310, 291, 362, 281, 51136], "temperature": 0.0, "avg_logprob": -0.10319561336351478, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.0006877683335915208}, {"id": 1056, "seek": 597520, "start": 5990.639999999999, "end": 5995.679999999999, "text": " have a large set of internal evals that you have to test for. Maybe at some point, there will be", "tokens": [51136, 362, 257, 2416, 992, 295, 6920, 1073, 1124, 300, 291, 362, 281, 1500, 337, 13, 2704, 412, 512, 935, 11, 456, 486, 312, 51388], "temperature": 0.0, "avg_logprob": -0.10319561336351478, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.0006877683335915208}, {"id": 1057, "seek": 597520, "start": 5995.679999999999, "end": 6001.599999999999, "text": " interpretability requirements. And there maybe you have, or likely you should do, staged release", "tokens": [51388, 7302, 2310, 7728, 13, 400, 456, 1310, 291, 362, 11, 420, 3700, 291, 820, 360, 11, 45178, 4374, 51684], "temperature": 0.0, "avg_logprob": -0.10319561336351478, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.0006877683335915208}, {"id": 1058, "seek": 600160, "start": 6001.6, "end": 6007.76, "text": " where you first only give access to a set of third party auditors that is trusted and, you know,", "tokens": [50364, 689, 291, 700, 787, 976, 2105, 281, 257, 992, 295, 2636, 3595, 2379, 9862, 300, 307, 16034, 293, 11, 291, 458, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12588844299316407, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.002800020156428218}, {"id": 1059, "seek": 600160, "start": 6007.76, "end": 6013.360000000001, "text": " maybe certified by the government or something like this. And then you give it to that could,", "tokens": [50672, 1310, 18580, 538, 264, 2463, 420, 746, 411, 341, 13, 400, 550, 291, 976, 309, 281, 300, 727, 11, 50952], "temperature": 0.0, "avg_logprob": -0.12588844299316407, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.002800020156428218}, {"id": 1060, "seek": 600160, "start": 6013.360000000001, "end": 6017.200000000001, "text": " for example, also include academics, obviously, right, they should also be involved in this process.", "tokens": [50952, 337, 1365, 11, 611, 4090, 25695, 11, 2745, 11, 558, 11, 436, 820, 611, 312, 3288, 294, 341, 1399, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12588844299316407, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.002800020156428218}, {"id": 1061, "seek": 600160, "start": 6018.72, "end": 6024.400000000001, "text": " Then once they have like, redeemed all of this and like found many different problems,", "tokens": [51220, 1396, 1564, 436, 362, 411, 11, 14328, 15485, 439, 295, 341, 293, 411, 1352, 867, 819, 2740, 11, 51504], "temperature": 0.0, "avg_logprob": -0.12588844299316407, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.002800020156428218}, {"id": 1062, "seek": 600160, "start": 6025.4400000000005, "end": 6030.320000000001, "text": " then you have to go back and reiterate, right, until until these problems are kind of sufficiently", "tokens": [51556, 550, 291, 362, 281, 352, 646, 293, 33528, 11, 558, 11, 1826, 1826, 613, 2740, 366, 733, 295, 31868, 51800], "temperature": 0.0, "avg_logprob": -0.12588844299316407, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.002800020156428218}, {"id": 1063, "seek": 603032, "start": 6030.32, "end": 6036.32, "text": " low that that you can go to the next stage, the next stage is then a small rollout to, you know,", "tokens": [50364, 2295, 300, 300, 291, 393, 352, 281, 264, 958, 3233, 11, 264, 958, 3233, 307, 550, 257, 1359, 3373, 346, 281, 11, 291, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10393219861117277, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.014060037210583687}, {"id": 1064, "seek": 603032, "start": 6036.32, "end": 6042.96, "text": " a thousand customers or maybe something something in this range, who also are not randomly chosen,", "tokens": [50664, 257, 4714, 4581, 420, 1310, 746, 746, 294, 341, 3613, 11, 567, 611, 366, 406, 16979, 8614, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10393219861117277, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.014060037210583687}, {"id": 1065, "seek": 603032, "start": 6042.96, "end": 6047.28, "text": " they have to pass certain know your customer checks. And then once once that has happened,", "tokens": [50996, 436, 362, 281, 1320, 1629, 458, 428, 5474, 13834, 13, 400, 550, 1564, 1564, 300, 575, 2011, 11, 51212], "temperature": 0.0, "avg_logprob": -0.10393219861117277, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.014060037210583687}, {"id": 1066, "seek": 603032, "start": 6048.88, "end": 6054.96, "text": " then the then maybe you can maybe you can roll it out further if there are no complications here.", "tokens": [51292, 550, 264, 550, 1310, 291, 393, 1310, 291, 393, 3373, 309, 484, 3052, 498, 456, 366, 572, 26566, 510, 13, 51596], "temperature": 0.0, "avg_logprob": -0.10393219861117277, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.014060037210583687}, {"id": 1067, "seek": 603032, "start": 6054.96, "end": 6058.4, "text": " And then you have to do monitoring during deployment, right, especially if you have systems", "tokens": [51596, 400, 550, 291, 362, 281, 360, 11028, 1830, 19317, 11, 558, 11, 2318, 498, 291, 362, 3652, 51768], "temperature": 0.0, "avg_logprob": -0.10393219861117277, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.014060037210583687}, {"id": 1068, "seek": 605840, "start": 6058.4, "end": 6063.04, "text": " that do online learning, a lot of weird things will happen. Or if you have access to tools,", "tokens": [50364, 300, 360, 2950, 2539, 11, 257, 688, 295, 3657, 721, 486, 1051, 13, 1610, 498, 291, 362, 2105, 281, 3873, 11, 50596], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1069, "seek": 605840, "start": 6063.04, "end": 6068.719999999999, "text": " a lot of people, you know, somebody is like, Hey, I, I took the I took the I took gb4 and I gave", "tokens": [50596, 257, 688, 295, 561, 11, 291, 458, 11, 2618, 307, 411, 11, 1911, 11, 286, 11, 286, 1890, 264, 286, 1890, 264, 286, 1890, 290, 65, 19, 293, 286, 2729, 50880], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1070, "seek": 605840, "start": 6068.719999999999, "end": 6072.96, "text": " it access to like, you know, a shell my bank account and the internet and like, here's all the weird", "tokens": [50880, 309, 2105, 281, 411, 11, 291, 458, 11, 257, 8720, 452, 3765, 2696, 293, 264, 4705, 293, 411, 11, 510, 311, 439, 264, 3657, 51092], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1071, "seek": 605840, "start": 6072.96, "end": 6077.04, "text": " things that happened. You kind of have to update on this as well. And these are kind of things you", "tokens": [51092, 721, 300, 2011, 13, 509, 733, 295, 362, 281, 5623, 322, 341, 382, 731, 13, 400, 613, 366, 733, 295, 721, 291, 51296], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1072, "seek": 605840, "start": 6077.04, "end": 6082.96, "text": " probably didn't predict before. So yeah, sort of a slow, a slow rollout is definitely one component.", "tokens": [51296, 1391, 994, 380, 6069, 949, 13, 407, 1338, 11, 1333, 295, 257, 2964, 11, 257, 2964, 3373, 346, 307, 2138, 472, 6542, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1073, "seek": 605840, "start": 6083.5199999999995, "end": 6087.12, "text": " Then the government also, I think, has to be involved in many, many ways, like,", "tokens": [51620, 1396, 264, 2463, 611, 11, 286, 519, 11, 575, 281, 312, 3288, 294, 867, 11, 867, 2098, 11, 411, 11, 51800], "temperature": 0.0, "avg_logprob": -0.13531967262168984, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.03406825289130211}, {"id": 1074, "seek": 608712, "start": 6087.12, "end": 6091.92, "text": " they, I think, effectively, at some point, they have to be able to say, you are not like you", "tokens": [50364, 436, 11, 286, 519, 11, 8659, 11, 412, 512, 935, 11, 436, 362, 281, 312, 1075, 281, 584, 11, 291, 366, 406, 411, 291, 50604], "temperature": 0.0, "avg_logprob": -0.11922970311395054, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.002050235401839018}, {"id": 1075, "seek": 608712, "start": 6091.92, "end": 6097.599999999999, "text": " have consistently not met security standards, or safety standards, you are now punished in", "tokens": [50604, 362, 14961, 406, 1131, 3825, 7787, 11, 420, 4514, 7787, 11, 291, 366, 586, 22365, 294, 50888], "temperature": 0.0, "avg_logprob": -0.11922970311395054, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.002050235401839018}, {"id": 1076, "seek": 608712, "start": 6097.599999999999, "end": 6102.48, "text": " some way, you're not allowed to, like, release models of this or the size, or you are not allowed", "tokens": [50888, 512, 636, 11, 291, 434, 406, 4350, 281, 11, 411, 11, 4374, 5245, 295, 341, 420, 264, 2744, 11, 420, 291, 366, 406, 4350, 51132], "temperature": 0.0, "avg_logprob": -0.11922970311395054, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.002050235401839018}, {"id": 1077, "seek": 608712, "start": 6102.48, "end": 6108.72, "text": " to do these other kind of things with the models. And if, you know, if they actually have been like", "tokens": [51132, 281, 360, 613, 661, 733, 295, 721, 365, 264, 5245, 13, 400, 498, 11, 291, 458, 11, 498, 436, 767, 362, 668, 411, 51444], "temperature": 0.0, "avg_logprob": -0.11922970311395054, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.002050235401839018}, {"id": 1078, "seek": 608712, "start": 6109.599999999999, "end": 6114.5599999999995, "text": " disregarding all of the guidelines from the government before, or sufficiently many of them.", "tokens": [51488, 44493, 278, 439, 295, 264, 12470, 490, 264, 2463, 949, 11, 420, 31868, 867, 295, 552, 13, 51736], "temperature": 0.0, "avg_logprob": -0.11922970311395054, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.002050235401839018}, {"id": 1079, "seek": 611456, "start": 6115.280000000001, "end": 6119.120000000001, "text": " Yeah, there are obviously like many, many additional things on top of this, right? There's", "tokens": [50400, 865, 11, 456, 366, 2745, 411, 867, 11, 867, 4497, 721, 322, 1192, 295, 341, 11, 558, 30, 821, 311, 50592], "temperature": 0.0, "avg_logprob": -0.15406954399893216, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.004753960762172937}, {"id": 1080, "seek": 611456, "start": 6119.120000000001, "end": 6125.280000000001, "text": " international communication, there's like whistleblower protection. There are international", "tokens": [50592, 5058, 6101, 11, 456, 311, 411, 23470, 5199, 968, 6334, 13, 821, 366, 5058, 50900], "temperature": 0.0, "avg_logprob": -0.15406954399893216, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.004753960762172937}, {"id": 1081, "seek": 611456, "start": 6125.280000000001, "end": 6130.320000000001, "text": " institutions that will have to be involved. At some point, at least, I guess, there will be", "tokens": [50900, 8142, 300, 486, 362, 281, 312, 3288, 13, 1711, 512, 935, 11, 412, 1935, 11, 286, 2041, 11, 456, 486, 312, 51152], "temperature": 0.0, "avg_logprob": -0.15406954399893216, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.004753960762172937}, {"id": 1082, "seek": 611456, "start": 6130.320000000001, "end": 6136.72, "text": " multiple institutions from the government side involved. I think there, for example, will be,", "tokens": [51152, 3866, 8142, 490, 264, 2463, 1252, 3288, 13, 286, 519, 456, 11, 337, 1365, 11, 486, 312, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15406954399893216, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.004753960762172937}, {"id": 1083, "seek": 611456, "start": 6137.4400000000005, "end": 6143.120000000001, "text": " or it would make sense to have like, a much like a bigger, broader institution that kind of like is", "tokens": [51508, 420, 309, 576, 652, 2020, 281, 362, 411, 11, 257, 709, 411, 257, 3801, 11, 13227, 7818, 300, 733, 295, 411, 307, 51792], "temperature": 0.0, "avg_logprob": -0.15406954399893216, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.004753960762172937}, {"id": 1084, "seek": 614312, "start": 6143.12, "end": 6147.2, "text": " a big tent where multiple coalitions come together. And then there's maybe more like a", "tokens": [50364, 257, 955, 7054, 689, 3866, 10209, 2451, 808, 1214, 13, 400, 550, 456, 311, 1310, 544, 411, 257, 50568], "temperature": 0.0, "avg_logprob": -0.10833414251154119, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.025526219978928566}, {"id": 1085, "seek": 614312, "start": 6147.2, "end": 6152.08, "text": " flexible specialized unit that only looks for the biggest, biggest kind of risks in the same way in", "tokens": [50568, 11358, 19813, 4985, 300, 787, 1542, 337, 264, 3880, 11, 3880, 733, 295, 10888, 294, 264, 912, 636, 294, 50812], "temperature": 0.0, "avg_logprob": -0.10833414251154119, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.025526219978928566}, {"id": 1086, "seek": 614312, "start": 6152.08, "end": 6156.96, "text": " which the US government has a unit that, you know, basically looks out for really big risks like", "tokens": [50812, 597, 264, 2546, 2463, 575, 257, 4985, 300, 11, 291, 458, 11, 1936, 1542, 484, 337, 534, 955, 10888, 411, 51056], "temperature": 0.0, "avg_logprob": -0.10833414251154119, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.025526219978928566}, {"id": 1087, "seek": 614312, "start": 6156.96, "end": 6162.32, "text": " pandemics and bio weapons and atomic weapons and so on. And they have a big mandate. And the only", "tokens": [51056, 4565, 38014, 293, 12198, 7278, 293, 22275, 7278, 293, 370, 322, 13, 400, 436, 362, 257, 955, 23885, 13, 400, 264, 787, 51324], "temperature": 0.0, "avg_logprob": -0.10833414251154119, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.025526219978928566}, {"id": 1088, "seek": 614312, "start": 6162.32, "end": 6167.84, "text": " thing that, you know, their mandate is like, find information and like make big problems,", "tokens": [51324, 551, 300, 11, 291, 458, 11, 641, 23885, 307, 411, 11, 915, 1589, 293, 411, 652, 955, 2740, 11, 51600], "temperature": 0.0, "avg_logprob": -0.10833414251154119, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.025526219978928566}, {"id": 1089, "seek": 616784, "start": 6168.8, "end": 6174.8, "text": " like go away to some extent or like, try to solve them as quickly as possible. And you have like,", "tokens": [50412, 411, 352, 1314, 281, 512, 8396, 420, 411, 11, 853, 281, 5039, 552, 382, 2661, 382, 1944, 13, 400, 291, 362, 411, 11, 50712], "temperature": 0.0, "avg_logprob": -0.14568721738635984, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.035136930644512177}, {"id": 1090, "seek": 616784, "start": 6174.8, "end": 6178.32, "text": " a strong mandate and something like this would also make sense in the case of AI, I think.", "tokens": [50712, 257, 2068, 23885, 293, 746, 411, 341, 576, 611, 652, 2020, 294, 264, 1389, 295, 7318, 11, 286, 519, 13, 50888], "temperature": 0.0, "avg_logprob": -0.14568721738635984, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.035136930644512177}, {"id": 1091, "seek": 616784, "start": 6178.88, "end": 6185.2, "text": " Maybe my last comment is something like, you know, open AI at some point, at least, had this", "tokens": [50916, 2704, 452, 1036, 2871, 307, 746, 411, 11, 291, 458, 11, 1269, 7318, 412, 512, 935, 11, 412, 1935, 11, 632, 341, 51232], "temperature": 0.0, "avg_logprob": -0.14568721738635984, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.035136930644512177}, {"id": 1092, "seek": 616784, "start": 6185.2, "end": 6191.360000000001, "text": " approach of like, testing in the real world, or releasing a sort of the best safety strategy,", "tokens": [51232, 3109, 295, 411, 11, 4997, 294, 264, 957, 1002, 11, 420, 16327, 257, 1333, 295, 264, 1151, 4514, 5206, 11, 51540], "temperature": 0.0, "avg_logprob": -0.14568721738635984, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.035136930644512177}, {"id": 1093, "seek": 616784, "start": 6191.360000000001, "end": 6195.84, "text": " because you get a lot of real world feedback and user feedback and so on. I think this was", "tokens": [51540, 570, 291, 483, 257, 688, 295, 957, 1002, 5824, 293, 4195, 5824, 293, 370, 322, 13, 286, 519, 341, 390, 51764], "temperature": 0.0, "avg_logprob": -0.14568721738635984, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.035136930644512177}, {"id": 1094, "seek": 619584, "start": 6195.84, "end": 6200.64, "text": " maybe true. And I'm not sure it was true for this period of time, but maybe it was true for the period", "tokens": [50364, 1310, 2074, 13, 400, 286, 478, 406, 988, 309, 390, 2074, 337, 341, 2896, 295, 565, 11, 457, 1310, 309, 390, 2074, 337, 264, 2896, 50604], "temperature": 0.0, "avg_logprob": -0.08310195828272292, "compression_ratio": 1.7097902097902098, "no_speech_prob": 0.0446598120033741}, {"id": 1095, "seek": 619584, "start": 6200.64, "end": 6207.6, "text": " of like 2020 to 2022 or 2023. Because the models were like, just good enough that user feedback", "tokens": [50604, 295, 411, 4808, 281, 20229, 420, 44377, 13, 1436, 264, 5245, 645, 411, 11, 445, 665, 1547, 300, 4195, 5824, 50952], "temperature": 0.0, "avg_logprob": -0.08310195828272292, "compression_ratio": 1.7097902097902098, "no_speech_prob": 0.0446598120033741}, {"id": 1096, "seek": 619584, "start": 6207.6, "end": 6212.08, "text": " was actually valuable, but nothing really bad could happen. But as soon as you have a system that", "tokens": [50952, 390, 767, 8263, 11, 457, 1825, 534, 1578, 727, 1051, 13, 583, 382, 2321, 382, 291, 362, 257, 1185, 300, 51176], "temperature": 0.0, "avg_logprob": -0.08310195828272292, "compression_ratio": 1.7097902097902098, "no_speech_prob": 0.0446598120033741}, {"id": 1097, "seek": 619584, "start": 6212.08, "end": 6217.2, "text": " is more powerful than that, right, you just outsource all the risk to like the rest of the world,", "tokens": [51176, 307, 544, 4005, 813, 300, 11, 558, 11, 291, 445, 14758, 2948, 439, 264, 3148, 281, 411, 264, 1472, 295, 264, 1002, 11, 51432], "temperature": 0.0, "avg_logprob": -0.08310195828272292, "compression_ratio": 1.7097902097902098, "no_speech_prob": 0.0446598120033741}, {"id": 1098, "seek": 619584, "start": 6217.2, "end": 6221.2, "text": " people will immediately put it on the internet. If they get access to it, they will do lots of", "tokens": [51432, 561, 486, 4258, 829, 309, 322, 264, 4705, 13, 759, 436, 483, 2105, 281, 309, 11, 436, 486, 360, 3195, 295, 51632], "temperature": 0.0, "avg_logprob": -0.08310195828272292, "compression_ratio": 1.7097902097902098, "no_speech_prob": 0.0446598120033741}, {"id": 1099, "seek": 622120, "start": 6221.2, "end": 6227.12, "text": " crazy stuff with more powerful systems. And yeah, I guess open AI, you know, there are a lot of", "tokens": [50364, 3219, 1507, 365, 544, 4005, 3652, 13, 400, 1338, 11, 286, 2041, 1269, 7318, 11, 291, 458, 11, 456, 366, 257, 688, 295, 50660], "temperature": 0.0, "avg_logprob": -0.09985198630942954, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.12756501138210297}, {"id": 1100, "seek": 622120, "start": 6227.12, "end": 6231.5199999999995, "text": " smart people at open AI, but they still cannot model what like millions of people will be doing", "tokens": [50660, 4069, 561, 412, 1269, 7318, 11, 457, 436, 920, 2644, 2316, 437, 411, 6803, 295, 561, 486, 312, 884, 50880], "temperature": 0.0, "avg_logprob": -0.09985198630942954, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.12756501138210297}, {"id": 1101, "seek": 622120, "start": 6231.5199999999995, "end": 6237.5199999999995, "text": " with these systems. So yeah, just like an uncontrolled rollout of very powerful systems,", "tokens": [50880, 365, 613, 3652, 13, 407, 1338, 11, 445, 411, 364, 36019, 28850, 3373, 346, 295, 588, 4005, 3652, 11, 51180], "temperature": 0.0, "avg_logprob": -0.09985198630942954, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.12756501138210297}, {"id": 1102, "seek": 622120, "start": 6237.5199999999995, "end": 6242.96, "text": " I think, yeah, is like kind of a recipe for a disaster. So my best guess is that", "tokens": [51180, 286, 519, 11, 1338, 11, 307, 411, 733, 295, 257, 6782, 337, 257, 11293, 13, 407, 452, 1151, 2041, 307, 300, 51452], "temperature": 0.0, "avg_logprob": -0.09985198630942954, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.12756501138210297}, {"id": 1103, "seek": 622120, "start": 6243.599999999999, "end": 6247.84, "text": " the default will more and more, will become more and more conservative with more and more", "tokens": [51484, 264, 7576, 486, 544, 293, 544, 11, 486, 1813, 544, 293, 544, 13780, 365, 544, 293, 544, 51696], "temperature": 0.0, "avg_logprob": -0.09985198630942954, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.12756501138210297}, {"id": 1104, "seek": 624784, "start": 6247.92, "end": 6253.4400000000005, "text": " efforts going into testing, alignment, making sure that, you know, like testing for all of the", "tokens": [50368, 6484, 516, 666, 4997, 11, 18515, 11, 1455, 988, 300, 11, 291, 458, 11, 411, 4997, 337, 439, 295, 264, 50644], "temperature": 0.0, "avg_logprob": -0.1178143161466752, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004904792178422213}, {"id": 1105, "seek": 624784, "start": 6253.4400000000005, "end": 6259.28, "text": " different hypotheticals, interpretability efforts to understand some weird edge cases,", "tokens": [50644, 819, 33053, 82, 11, 7302, 2310, 6484, 281, 1223, 512, 3657, 4691, 3331, 11, 50936], "temperature": 0.0, "avg_logprob": -0.1178143161466752, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004904792178422213}, {"id": 1106, "seek": 624784, "start": 6259.28, "end": 6265.68, "text": " monitoring, et cetera, et cetera. Does that imply that open source in your view just", "tokens": [50936, 11028, 11, 1030, 11458, 11, 1030, 11458, 13, 4402, 300, 33616, 300, 1269, 4009, 294, 428, 1910, 445, 51256], "temperature": 0.0, "avg_logprob": -0.1178143161466752, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004904792178422213}, {"id": 1107, "seek": 624784, "start": 6267.68, "end": 6272.32, "text": " can't work? Like, I mean, is there any way to square, because I'm very sympathetic, you know,", "tokens": [51356, 393, 380, 589, 30, 1743, 11, 286, 914, 11, 307, 456, 604, 636, 281, 3732, 11, 570, 286, 478, 588, 36032, 11, 291, 458, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1178143161466752, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004904792178422213}, {"id": 1108, "seek": 627232, "start": 6272.32, "end": 6277.759999999999, "text": " to considering a normal technology, and I would not, you know, I might turn out to be a normal", "tokens": [50364, 281, 8079, 257, 2710, 2899, 11, 293, 286, 576, 406, 11, 291, 458, 11, 286, 1062, 1261, 484, 281, 312, 257, 2710, 50636], "temperature": 0.0, "avg_logprob": -0.10761836945541262, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.004904691595584154}, {"id": 1109, "seek": 627232, "start": 6277.759999999999, "end": 6282.32, "text": " technology, but as of now, it seems like a very live possibility that it is not a normal technology.", "tokens": [50636, 2899, 11, 457, 382, 295, 586, 11, 309, 2544, 411, 257, 588, 1621, 7959, 300, 309, 307, 406, 257, 2710, 2899, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10761836945541262, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.004904691595584154}, {"id": 1110, "seek": 627232, "start": 6282.32, "end": 6286.799999999999, "text": " But if we were to imagine, you know, whatever capabilities kind of stop where they are, and", "tokens": [50864, 583, 498, 321, 645, 281, 3811, 11, 291, 458, 11, 2035, 10862, 733, 295, 1590, 689, 436, 366, 11, 293, 51088], "temperature": 0.0, "avg_logprob": -0.10761836945541262, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.004904691595584154}, {"id": 1111, "seek": 627232, "start": 6286.799999999999, "end": 6292.639999999999, "text": " we're sort of, you know, left with GPT-4 forever, or something like that, hard to imagine, but let's", "tokens": [51088, 321, 434, 1333, 295, 11, 291, 458, 11, 1411, 365, 26039, 51, 12, 19, 5680, 11, 420, 746, 411, 300, 11, 1152, 281, 3811, 11, 457, 718, 311, 51380], "temperature": 0.0, "avg_logprob": -0.10761836945541262, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.004904691595584154}, {"id": 1112, "seek": 627232, "start": 6292.639999999999, "end": 6297.92, "text": " just pause it. Then I'm like very sympathetic to the people that are like, hey, you know, this", "tokens": [51380, 445, 10465, 309, 13, 1396, 286, 478, 411, 588, 36032, 281, 264, 561, 300, 366, 411, 11, 4177, 11, 291, 458, 11, 341, 51644], "temperature": 0.0, "avg_logprob": -0.10761836945541262, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.004904691595584154}, {"id": 1113, "seek": 629792, "start": 6297.92, "end": 6303.04, "text": " shouldn't be just the kind of thing that a few companies have access to, and it, you know,", "tokens": [50364, 4659, 380, 312, 445, 264, 733, 295, 551, 300, 257, 1326, 3431, 362, 2105, 281, 11, 293, 309, 11, 291, 458, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10605558272330992, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.007576029747724533}, {"id": 1114, "seek": 629792, "start": 6303.04, "end": 6306.4800000000005, "text": " you should be able to make your own version, and, you know, what about the rest of the world, and,", "tokens": [50620, 291, 820, 312, 1075, 281, 652, 428, 1065, 3037, 11, 293, 11, 291, 458, 11, 437, 466, 264, 1472, 295, 264, 1002, 11, 293, 11, 50792], "temperature": 0.0, "avg_logprob": -0.10605558272330992, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.007576029747724533}, {"id": 1115, "seek": 629792, "start": 6306.4800000000005, "end": 6310.8, "text": " you know, there should be an Indian version for India, and like all these things. But it seems", "tokens": [50792, 291, 458, 11, 456, 820, 312, 364, 6427, 3037, 337, 5282, 11, 293, 411, 439, 613, 721, 13, 583, 309, 2544, 51008], "temperature": 0.0, "avg_logprob": -0.10605558272330992, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.007576029747724533}, {"id": 1116, "seek": 629792, "start": 6310.8, "end": 6316.88, "text": " hard in a world where, you know, you imagine sufficiently powerful systems that are just put", "tokens": [51008, 1152, 294, 257, 1002, 689, 11, 291, 458, 11, 291, 3811, 31868, 4005, 3652, 300, 366, 445, 829, 51312], "temperature": 0.0, "avg_logprob": -0.10605558272330992, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.007576029747724533}, {"id": 1117, "seek": 629792, "start": 6316.88, "end": 6324.16, "text": " out totally, you know, bare into the world, like, is there any way to square the all those, I think,", "tokens": [51312, 484, 3879, 11, 291, 458, 11, 6949, 666, 264, 1002, 11, 411, 11, 307, 456, 604, 636, 281, 3732, 264, 439, 729, 11, 286, 519, 11, 51676], "temperature": 0.0, "avg_logprob": -0.10605558272330992, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.007576029747724533}, {"id": 1118, "seek": 632416, "start": 6324.16, "end": 6330.24, "text": " very legitimate open source motivations with the sort of safety paradigm that you're trying to develop?", "tokens": [50364, 588, 17956, 1269, 4009, 39034, 365, 264, 1333, 295, 4514, 24709, 300, 291, 434, 1382, 281, 1499, 30, 50668], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1119, "seek": 632416, "start": 6331.12, "end": 6335.599999999999, "text": " Potentially. So, you know, like, I think the open source debate is fairly heated. But, you know,", "tokens": [50712, 9145, 3137, 13, 407, 11, 291, 458, 11, 411, 11, 286, 519, 264, 1269, 4009, 7958, 307, 6457, 18806, 13, 583, 11, 291, 458, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1120, "seek": 632416, "start": 6335.599999999999, "end": 6340.5599999999995, "text": " to me, there are two things that are kind of obviously true. Number one, open source has been", "tokens": [50936, 281, 385, 11, 456, 366, 732, 721, 300, 366, 733, 295, 2745, 2074, 13, 5118, 472, 11, 1269, 4009, 575, 668, 51184], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1121, "seek": 632416, "start": 6340.5599999999995, "end": 6344.8, "text": " really good so far in many, many ways. It has been very positive for society, right? I think a lot", "tokens": [51184, 534, 665, 370, 1400, 294, 867, 11, 867, 2098, 13, 467, 575, 668, 588, 3353, 337, 4086, 11, 558, 30, 286, 519, 257, 688, 51396], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1122, "seek": 632416, "start": 6344.8, "end": 6348.8, "text": " of ML research could not have happened without open source. A lot of safety research could not", "tokens": [51396, 295, 21601, 2132, 727, 406, 362, 2011, 1553, 1269, 4009, 13, 316, 688, 295, 4514, 2132, 727, 406, 51596], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1123, "seek": 632416, "start": 6348.8, "end": 6353.92, "text": " have happened with open source. And the other thing that is also true, or at least seems true", "tokens": [51596, 362, 2011, 365, 1269, 4009, 13, 400, 264, 661, 551, 300, 307, 611, 2074, 11, 420, 412, 1935, 2544, 2074, 51852], "temperature": 0.0, "avg_logprob": -0.08859258379255022, "compression_ratio": 1.8535031847133758, "no_speech_prob": 0.027150234207510948}, {"id": 1124, "seek": 635392, "start": 6354.0, "end": 6358.88, "text": " to me, is there's a limit of open source, right? Like, at some point, the system is so powerful", "tokens": [50368, 281, 385, 11, 307, 456, 311, 257, 4948, 295, 1269, 4009, 11, 558, 30, 1743, 11, 412, 512, 935, 11, 264, 1185, 307, 370, 4005, 50612], "temperature": 0.0, "avg_logprob": -0.113938232421875, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.0019871480762958527}, {"id": 1125, "seek": 635392, "start": 6358.88, "end": 6362.72, "text": " that you don't want it to be open source anymore, in the same way in which, you know, I don't want to", "tokens": [50612, 300, 291, 500, 380, 528, 309, 281, 312, 1269, 4009, 3602, 11, 294, 264, 912, 636, 294, 597, 11, 291, 458, 11, 286, 500, 380, 528, 281, 50804], "temperature": 0.0, "avg_logprob": -0.113938232421875, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.0019871480762958527}, {"id": 1126, "seek": 635392, "start": 6362.72, "end": 6368.4800000000005, "text": " open source, like the nuclear codes, or something to start or like, you know, literally the recipe", "tokens": [50804, 1269, 4009, 11, 411, 264, 8179, 14211, 11, 420, 746, 281, 722, 420, 411, 11, 291, 458, 11, 3736, 264, 6782, 51092], "temperature": 0.0, "avg_logprob": -0.113938232421875, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.0019871480762958527}, {"id": 1127, "seek": 635392, "start": 6368.4800000000005, "end": 6374.32, "text": " to build like the most, most viral, you know, most viral pandemic or something. This is just", "tokens": [51092, 281, 1322, 411, 264, 881, 11, 881, 16132, 11, 291, 458, 11, 881, 16132, 5388, 420, 746, 13, 639, 307, 445, 51384], "temperature": 0.0, "avg_logprob": -0.113938232421875, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.0019871480762958527}, {"id": 1128, "seek": 635392, "start": 6374.32, "end": 6381.04, "text": " something where, you know, only one person needs to needs to have bad intentions to already have", "tokens": [51384, 746, 689, 11, 291, 458, 11, 787, 472, 954, 2203, 281, 2203, 281, 362, 1578, 19354, 281, 1217, 362, 51720], "temperature": 0.0, "avg_logprob": -0.113938232421875, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.0019871480762958527}, {"id": 1129, "seek": 638104, "start": 6381.04, "end": 6385.6, "text": " like really, to already cause really big problems. So at some point that there's, there's just a", "tokens": [50364, 411, 534, 11, 281, 1217, 3082, 534, 955, 2740, 13, 407, 412, 512, 935, 300, 456, 311, 11, 456, 311, 445, 257, 50592], "temperature": 0.0, "avg_logprob": -0.11560980478922527, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.019116748124361038}, {"id": 1130, "seek": 638104, "start": 6385.6, "end": 6391.44, "text": " balance where you just, I think, cannot really justify giving people literally everyone access", "tokens": [50592, 4772, 689, 291, 445, 11, 286, 519, 11, 2644, 534, 20833, 2902, 561, 3736, 1518, 2105, 50884], "temperature": 0.0, "avg_logprob": -0.11560980478922527, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.019116748124361038}, {"id": 1131, "seek": 638104, "start": 6391.44, "end": 6397.04, "text": " to this. And so the question for me really is where, so number one, where are we on the spectrum", "tokens": [50884, 281, 341, 13, 400, 370, 264, 1168, 337, 385, 534, 307, 689, 11, 370, 1230, 472, 11, 689, 366, 321, 322, 264, 11143, 51164], "temperature": 0.0, "avg_logprob": -0.11560980478922527, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.019116748124361038}, {"id": 1132, "seek": 638104, "start": 6397.04, "end": 6402.88, "text": " right now of like, open source has been really good to, are we already a point at like, how close", "tokens": [51164, 558, 586, 295, 411, 11, 1269, 4009, 575, 668, 534, 665, 281, 11, 366, 321, 1217, 257, 935, 412, 411, 11, 577, 1998, 51456], "temperature": 0.0, "avg_logprob": -0.11560980478922527, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.019116748124361038}, {"id": 1133, "seek": 638104, "start": 6402.88, "end": 6406.96, "text": " are we to the point where it really cannot be justified anymore? Some people would say even", "tokens": [51456, 366, 321, 281, 264, 935, 689, 309, 534, 2644, 312, 27808, 3602, 30, 2188, 561, 576, 584, 754, 51660], "temperature": 0.0, "avg_logprob": -0.11560980478922527, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.019116748124361038}, {"id": 1134, "seek": 640696, "start": 6406.96, "end": 6412.32, "text": " GBD3, you know, through GBD3 size models are already too, too scary, which I'm not sure about,", "tokens": [50364, 26809, 35, 18, 11, 291, 458, 11, 807, 26809, 35, 18, 2744, 5245, 366, 1217, 886, 11, 886, 6958, 11, 597, 286, 478, 406, 988, 466, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11139978681291852, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.13653412461280823}, {"id": 1135, "seek": 640696, "start": 6412.32, "end": 6417.28, "text": " I'm not even sure whether GBD4 size models are like too big to be open source, but I would rather", "tokens": [50632, 286, 478, 406, 754, 988, 1968, 26809, 35, 19, 2744, 5245, 366, 411, 886, 955, 281, 312, 1269, 4009, 11, 457, 286, 576, 2831, 50880], "temperature": 0.0, "avg_logprob": -0.11139978681291852, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.13653412461280823}, {"id": 1136, "seek": 640696, "start": 6417.28, "end": 6421.84, "text": " err on the side of caution and be a little bit more conservative here, because of the, the nature", "tokens": [50880, 45267, 322, 264, 1252, 295, 23585, 293, 312, 257, 707, 857, 544, 13780, 510, 11, 570, 295, 264, 11, 264, 3687, 51108], "temperature": 0.0, "avg_logprob": -0.11139978681291852, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.13653412461280823}, {"id": 1137, "seek": 640696, "start": 6421.84, "end": 6427.2, "text": " of open sourcing, where you can really not take this back. So as soon as you make a mistake, you", "tokens": [51108, 295, 1269, 11006, 2175, 11, 689, 291, 393, 534, 406, 747, 341, 646, 13, 407, 382, 2321, 382, 291, 652, 257, 6146, 11, 291, 51376], "temperature": 0.0, "avg_logprob": -0.11139978681291852, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.13653412461280823}, {"id": 1138, "seek": 640696, "start": 6427.2, "end": 6431.28, "text": " like, you're stuck with a mistake for a long time, or like forever, you cannot, you cannot turn it,", "tokens": [51376, 411, 11, 291, 434, 5541, 365, 257, 6146, 337, 257, 938, 565, 11, 420, 411, 5680, 11, 291, 2644, 11, 291, 2644, 1261, 309, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11139978681291852, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.13653412461280823}, {"id": 1139, "seek": 643128, "start": 6431.28, "end": 6438.0, "text": " take it back. And I also think this will, this will also influence how, how open source will be", "tokens": [50364, 747, 309, 646, 13, 400, 286, 611, 519, 341, 486, 11, 341, 486, 611, 6503, 577, 11, 577, 1269, 4009, 486, 312, 50700], "temperature": 0.0, "avg_logprob": -0.10163038655331261, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.07365380972623825}, {"id": 1140, "seek": 643128, "start": 6438.0, "end": 6444.08, "text": " handled in the real world in practice. Like, I think there will be something like initial releases", "tokens": [50700, 18033, 294, 264, 957, 1002, 294, 3124, 13, 1743, 11, 286, 519, 456, 486, 312, 746, 411, 5883, 16952, 51004], "temperature": 0.0, "avg_logprob": -0.10163038655331261, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.07365380972623825}, {"id": 1141, "seek": 643128, "start": 6444.08, "end": 6448.24, "text": " for open source, where lots of people test it, like, basically think there will also be staggered", "tokens": [51004, 337, 1269, 4009, 11, 689, 3195, 295, 561, 1500, 309, 11, 411, 11, 1936, 519, 456, 486, 611, 312, 29656, 292, 51212], "temperature": 0.0, "avg_logprob": -0.10163038655331261, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.07365380972623825}, {"id": 1142, "seek": 643128, "start": 6448.24, "end": 6454.0, "text": " and stage releases, right? It's, first, there's like a small team of trusted researchers who", "tokens": [51212, 293, 3233, 16952, 11, 558, 30, 467, 311, 11, 700, 11, 456, 311, 411, 257, 1359, 1469, 295, 16034, 10309, 567, 51500], "temperature": 0.0, "avg_logprob": -0.10163038655331261, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.07365380972623825}, {"id": 1143, "seek": 643128, "start": 6454.0, "end": 6458.88, "text": " is allowed to play with the open source model, and like really test the limits, really test", "tokens": [51500, 307, 4350, 281, 862, 365, 264, 1269, 4009, 2316, 11, 293, 411, 534, 1500, 264, 10406, 11, 534, 1500, 51744], "temperature": 0.0, "avg_logprob": -0.10163038655331261, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.07365380972623825}, {"id": 1144, "seek": 645888, "start": 6458.88, "end": 6462.96, "text": " how bad could I get the model, how easy is it to remove all of the guardrails,", "tokens": [50364, 577, 1578, 727, 286, 483, 264, 2316, 11, 577, 1858, 307, 309, 281, 4159, 439, 295, 264, 6290, 424, 4174, 11, 50568], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1145, "seek": 645888, "start": 6462.96, "end": 6467.2, "text": " which, you know, like, it's an open source model, if you can find, you can remove the guardrails.", "tokens": [50568, 597, 11, 291, 458, 11, 411, 11, 309, 311, 364, 1269, 4009, 2316, 11, 498, 291, 393, 915, 11, 291, 393, 4159, 264, 6290, 424, 4174, 13, 50780], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1146, "seek": 645888, "start": 6467.2, "end": 6469.6, "text": " Yeah, it turns out pretty easy from what we've seen so far.", "tokens": [50780, 865, 11, 309, 4523, 484, 1238, 1858, 490, 437, 321, 600, 1612, 370, 1400, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1147, "seek": 645888, "start": 6470.4800000000005, "end": 6474.4800000000005, "text": " Ones like, you know, the kind of the upper bounds are known of how bad could this become.", "tokens": [50944, 1282, 279, 411, 11, 291, 458, 11, 264, 733, 295, 264, 6597, 29905, 366, 2570, 295, 577, 1578, 727, 341, 1813, 13, 51144], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1148, "seek": 645888, "start": 6475.6, "end": 6480.08, "text": " Maybe it makes sense to, to like open source it to more people. But yeah, I would basically say,", "tokens": [51200, 2704, 309, 1669, 2020, 281, 11, 281, 411, 1269, 4009, 309, 281, 544, 561, 13, 583, 1338, 11, 286, 576, 1936, 584, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1149, "seek": 645888, "start": 6480.08, "end": 6484.8, "text": " you know, the upper bound can be quite high, especially with all of the stuff I said earlier", "tokens": [51424, 291, 458, 11, 264, 6597, 5472, 393, 312, 1596, 1090, 11, 2318, 365, 439, 295, 264, 1507, 286, 848, 3071, 51660], "temperature": 0.0, "avg_logprob": -0.10969499131323586, "compression_ratio": 1.8362989323843417, "no_speech_prob": 0.005219544284045696}, {"id": 1150, "seek": 648480, "start": 6484.8, "end": 6489.52, "text": " about, you know, absolute capabilities and reachable capabilities and so on, right? Like,", "tokens": [50364, 466, 11, 291, 458, 11, 8236, 10862, 293, 2524, 712, 10862, 293, 370, 322, 11, 558, 30, 1743, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09608452073458967, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.007344902493059635}, {"id": 1151, "seek": 648480, "start": 6489.52, "end": 6495.52, "text": " maybe you can, maybe you cannot get it to build an automated hacking bot, if you only have access", "tokens": [50600, 1310, 291, 393, 11, 1310, 291, 2644, 483, 309, 281, 1322, 364, 18473, 31422, 10592, 11, 498, 291, 787, 362, 2105, 50900], "temperature": 0.0, "avg_logprob": -0.09608452073458967, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.007344902493059635}, {"id": 1152, "seek": 648480, "start": 6495.52, "end": 6499.360000000001, "text": " to the weights, but maybe if you do scaffolding on top and some fine tuning and access to some", "tokens": [50900, 281, 264, 17443, 11, 457, 1310, 498, 291, 360, 44094, 278, 322, 1192, 293, 512, 2489, 15164, 293, 2105, 281, 512, 51092], "temperature": 0.0, "avg_logprob": -0.09608452073458967, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.007344902493059635}, {"id": 1153, "seek": 648480, "start": 6499.360000000001, "end": 6504.24, "text": " other thing, maybe then it can build it, right? And this is like very hard to predict in advance.", "tokens": [51092, 661, 551, 11, 1310, 550, 309, 393, 1322, 309, 11, 558, 30, 400, 341, 307, 411, 588, 1152, 281, 6069, 294, 7295, 13, 51336], "temperature": 0.0, "avg_logprob": -0.09608452073458967, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.007344902493059635}, {"id": 1154, "seek": 648480, "start": 6504.24, "end": 6510.0, "text": " So the more and more powerful the models become, I think the less plausible a priority is to open", "tokens": [51336, 407, 264, 544, 293, 544, 4005, 264, 5245, 1813, 11, 286, 519, 264, 1570, 39925, 257, 9365, 307, 281, 1269, 51624], "temperature": 0.0, "avg_logprob": -0.09608452073458967, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.007344902493059635}, {"id": 1155, "seek": 651000, "start": 6510.0, "end": 6514.16, "text": " source them. Even though, and like, this is really something I want to emphasize, right? Like,", "tokens": [50364, 4009, 552, 13, 2754, 1673, 11, 293, 411, 11, 341, 307, 534, 746, 286, 528, 281, 16078, 11, 558, 30, 1743, 11, 50572], "temperature": 0.0, "avg_logprob": -0.13766809237205377, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.12931163609027863}, {"id": 1156, "seek": 651000, "start": 6514.16, "end": 6520.32, "text": " open source has been extremely good so far. And I really think there's sort of this tipping point", "tokens": [50572, 1269, 4009, 575, 668, 4664, 665, 370, 1400, 13, 400, 286, 534, 519, 456, 311, 1333, 295, 341, 41625, 935, 50880], "temperature": 0.0, "avg_logprob": -0.13766809237205377, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.12931163609027863}, {"id": 1157, "seek": 651000, "start": 6520.32, "end": 6528.64, "text": " that is like, at some point, it just becomes too hard to like, it becomes impossible, it always", "tokens": [50880, 300, 307, 411, 11, 412, 512, 935, 11, 309, 445, 3643, 886, 1152, 281, 411, 11, 309, 3643, 6243, 11, 309, 1009, 51296], "temperature": 0.0, "avg_logprob": -0.13766809237205377, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.12931163609027863}, {"id": 1158, "seek": 651000, "start": 6528.64, "end": 6533.12, "text": " will be impossible to take back, but at some point, it just becomes too dangerous to literally trust", "tokens": [51296, 486, 312, 6243, 281, 747, 646, 11, 457, 412, 512, 935, 11, 309, 445, 3643, 886, 5795, 281, 3736, 3361, 51520], "temperature": 0.0, "avg_logprob": -0.13766809237205377, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.12931163609027863}, {"id": 1159, "seek": 651000, "start": 6533.12, "end": 6539.76, "text": " everyone with, with this level of capabilities. Threshold effects. That's, I think, one of the", "tokens": [51520, 1518, 365, 11, 365, 341, 1496, 295, 10862, 13, 334, 14214, 5065, 13, 663, 311, 11, 286, 519, 11, 472, 295, 264, 51852], "temperature": 0.0, "avg_logprob": -0.13766809237205377, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.12931163609027863}, {"id": 1160, "seek": 653976, "start": 6539.76, "end": 6545.92, "text": " most powerful paradigms that I've, you know, consistently come back to over the last couple", "tokens": [50364, 881, 4005, 13480, 328, 2592, 300, 286, 600, 11, 291, 458, 11, 14961, 808, 646, 281, 670, 264, 1036, 1916, 50672], "temperature": 0.0, "avg_logprob": -0.07190065383911133, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0004304996400605887}, {"id": 1161, "seek": 653976, "start": 6545.92, "end": 6551.68, "text": " years, just crossing these thresholds from one regime into another, whether it's capabilities or,", "tokens": [50672, 924, 11, 445, 14712, 613, 14678, 82, 490, 472, 13120, 666, 1071, 11, 1968, 309, 311, 10862, 420, 11, 50960], "temperature": 0.0, "avg_logprob": -0.07190065383911133, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0004304996400605887}, {"id": 1162, "seek": 653976, "start": 6551.68, "end": 6557.76, "text": " you know, risks, it just constantly seems like we're flipping from one mode or one kind of,", "tokens": [50960, 291, 458, 11, 10888, 11, 309, 445, 6460, 2544, 411, 321, 434, 26886, 490, 472, 4391, 420, 472, 733, 295, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07190065383911133, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0004304996400605887}, {"id": 1163, "seek": 653976, "start": 6558.88, "end": 6565.04, "text": " you know, one regime to another and got to be very alert to when that happens because it can", "tokens": [51320, 291, 458, 11, 472, 13120, 281, 1071, 293, 658, 281, 312, 588, 9615, 281, 562, 300, 2314, 570, 309, 393, 51628], "temperature": 0.0, "avg_logprob": -0.07190065383911133, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0004304996400605887}, {"id": 1164, "seek": 656504, "start": 6565.04, "end": 6570.72, "text": " really change, you know, important analysis in pretty profound ways. So I don't know where exactly", "tokens": [50364, 534, 1319, 11, 291, 458, 11, 1021, 5215, 294, 1238, 14382, 2098, 13, 407, 286, 500, 380, 458, 689, 2293, 50648], "temperature": 0.0, "avg_logprob": -0.10606501274502154, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.013634697534143925}, {"id": 1165, "seek": 656504, "start": 6570.72, "end": 6576.16, "text": " that threshold is either by any means, but it, and I would agree that like, for everything that I", "tokens": [50648, 300, 14678, 307, 2139, 538, 604, 1355, 11, 457, 309, 11, 293, 286, 576, 3986, 300, 411, 11, 337, 1203, 300, 286, 50920], "temperature": 0.0, "avg_logprob": -0.10606501274502154, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.013634697534143925}, {"id": 1166, "seek": 656504, "start": 6576.16, "end": 6584.32, "text": " have seen suggests that up to and including the release of Lama 2 has been, you know, very, very", "tokens": [50920, 362, 1612, 13409, 300, 493, 281, 293, 3009, 264, 4374, 295, 441, 2404, 568, 575, 668, 11, 291, 458, 11, 588, 11, 588, 51328], "temperature": 0.0, "avg_logprob": -0.10606501274502154, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.013634697534143925}, {"id": 1167, "seek": 656504, "start": 6584.32, "end": 6590.0, "text": " much an enabler for all sorts of things. But certainly, you know, plenty of safety related work", "tokens": [51328, 709, 364, 465, 455, 1918, 337, 439, 7527, 295, 721, 13, 583, 3297, 11, 291, 458, 11, 7140, 295, 4514, 4077, 589, 51612], "temperature": 0.0, "avg_logprob": -0.10606501274502154, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.013634697534143925}, {"id": 1168, "seek": 659000, "start": 6590.0, "end": 6597.68, "text": " done on that model and, you know, seems, seems like the effect so far has been good. But yeah,", "tokens": [50364, 1096, 322, 300, 2316, 293, 11, 291, 458, 11, 2544, 11, 2544, 411, 264, 1802, 370, 1400, 575, 668, 665, 13, 583, 1338, 11, 50748], "temperature": 0.0, "avg_logprob": -0.14790804862976073, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004330902826040983}, {"id": 1169, "seek": 659000, "start": 6597.68, "end": 6601.84, "text": " is that still true for Lama 3? Is it true for Lama 4? You know, obviously we don't even know", "tokens": [50748, 307, 300, 920, 2074, 337, 441, 2404, 805, 30, 1119, 309, 2074, 337, 441, 2404, 1017, 30, 509, 458, 11, 2745, 321, 500, 380, 754, 458, 50956], "temperature": 0.0, "avg_logprob": -0.14790804862976073, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004330902826040983}, {"id": 1170, "seek": 659000, "start": 6601.84, "end": 6607.12, "text": " what these things are, but it certainly starts to be a very live question.", "tokens": [50956, 437, 613, 721, 366, 11, 457, 309, 3297, 3719, 281, 312, 257, 588, 1621, 1168, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14790804862976073, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004330902826040983}, {"id": 1171, "seek": 659000, "start": 6607.84, "end": 6613.76, "text": " You know, the, the like leading AGI labs, I think it is very clear that they understand the problem,", "tokens": [51256, 509, 458, 11, 264, 11, 264, 411, 5775, 316, 26252, 20339, 11, 286, 519, 309, 307, 588, 1850, 300, 436, 1223, 264, 1154, 11, 51552], "temperature": 0.0, "avg_logprob": -0.14790804862976073, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004330902826040983}, {"id": 1172, "seek": 661376, "start": 6613.76, "end": 6620.88, "text": " that they have internal processes that are, you know, like, maybe better than you would expect", "tokens": [50364, 300, 436, 362, 6920, 7555, 300, 366, 11, 291, 458, 11, 411, 11, 1310, 1101, 813, 291, 576, 2066, 50720], "temperature": 0.0, "avg_logprob": -0.10896507015934696, "compression_ratio": 1.824, "no_speech_prob": 0.20669123530387878}, {"id": 1173, "seek": 661376, "start": 6620.88, "end": 6628.24, "text": " from the normal, from a normal company. They actually care about trying to do good with", "tokens": [50720, 490, 264, 2710, 11, 490, 257, 2710, 2237, 13, 814, 767, 1127, 466, 1382, 281, 360, 665, 365, 51088], "temperature": 0.0, "avg_logprob": -0.10896507015934696, "compression_ratio": 1.824, "no_speech_prob": 0.20669123530387878}, {"id": 1174, "seek": 661376, "start": 6628.24, "end": 6632.72, "text": " with remodels and they're like very explicitly trying. And then, you know, there's the other", "tokens": [51088, 365, 890, 378, 1625, 293, 436, 434, 411, 588, 20803, 1382, 13, 400, 550, 11, 291, 458, 11, 456, 311, 264, 661, 51312], "temperature": 0.0, "avg_logprob": -0.10896507015934696, "compression_ratio": 1.824, "no_speech_prob": 0.20669123530387878}, {"id": 1175, "seek": 661376, "start": 6632.72, "end": 6637.2, "text": " side of the coin, which is they still have incentives, and they, you know, they can be", "tokens": [51312, 1252, 295, 264, 11464, 11, 597, 307, 436, 920, 362, 23374, 11, 293, 436, 11, 291, 458, 11, 436, 393, 312, 51536], "temperature": 0.0, "avg_logprob": -0.10896507015934696, "compression_ratio": 1.824, "no_speech_prob": 0.20669123530387878}, {"id": 1176, "seek": 661376, "start": 6637.2, "end": 6642.4800000000005, "text": " financial incentives, they can be sort of maybe more psychological and social incentives, you", "tokens": [51536, 4669, 23374, 11, 436, 393, 312, 1333, 295, 1310, 544, 14346, 293, 2093, 23374, 11, 291, 51800], "temperature": 0.0, "avg_logprob": -0.10896507015934696, "compression_ratio": 1.824, "no_speech_prob": 0.20669123530387878}, {"id": 1177, "seek": 664248, "start": 6642.48, "end": 6647.28, "text": " know, that they just want to be the first to develop AGI, because it's like probably like a", "tokens": [50364, 458, 11, 300, 436, 445, 528, 281, 312, 264, 700, 281, 1499, 316, 26252, 11, 570, 309, 311, 411, 1391, 411, 257, 50604], "temperature": 0.0, "avg_logprob": -0.13763150381385733, "compression_ratio": 1.7578125, "no_speech_prob": 0.0022511472925543785}, {"id": 1178, "seek": 664248, "start": 6647.28, "end": 6652.16, "text": " history defining technology, or maybe even galaxy defining technology or something like this, right?", "tokens": [50604, 2503, 17827, 2899, 11, 420, 1310, 754, 17639, 17827, 2899, 420, 746, 411, 341, 11, 558, 30, 50848], "temperature": 0.0, "avg_logprob": -0.13763150381385733, "compression_ratio": 1.7578125, "no_speech_prob": 0.0022511472925543785}, {"id": 1179, "seek": 664248, "start": 6652.16, "end": 6657.12, "text": " And, and so the question really, I think at this, you know, even if you could say, you know,", "tokens": [50848, 400, 11, 293, 370, 264, 1168, 534, 11, 286, 519, 412, 341, 11, 291, 458, 11, 754, 498, 291, 727, 584, 11, 291, 458, 11, 51096], "temperature": 0.0, "avg_logprob": -0.13763150381385733, "compression_ratio": 1.7578125, "no_speech_prob": 0.0022511472925543785}, {"id": 1180, "seek": 664248, "start": 6657.12, "end": 6664.24, "text": " like the compared to a normal company, these, the processes are astonishingly reasonable,", "tokens": [51096, 411, 264, 5347, 281, 257, 2710, 2237, 11, 613, 11, 264, 7555, 366, 35264, 356, 10585, 11, 51452], "temperature": 0.0, "avg_logprob": -0.13763150381385733, "compression_ratio": 1.7578125, "no_speech_prob": 0.0022511472925543785}, {"id": 1181, "seek": 664248, "start": 6664.24, "end": 6667.759999999999, "text": " and surprisingly good. If, you know, if you compare to literally any other", "tokens": [51452, 293, 17600, 665, 13, 759, 11, 291, 458, 11, 498, 291, 6794, 281, 3736, 604, 661, 51628], "temperature": 0.0, "avg_logprob": -0.13763150381385733, "compression_ratio": 1.7578125, "no_speech_prob": 0.0022511472925543785}, {"id": 1182, "seek": 666776, "start": 6668.56, "end": 6674.4800000000005, "text": " industry, it would be surprising if they have this, this amount of like self regulation and so on.", "tokens": [50404, 3518, 11, 309, 576, 312, 8830, 498, 436, 362, 341, 11, 341, 2372, 295, 411, 2698, 15062, 293, 370, 322, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13569464222077401, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.10962487012147903}, {"id": 1183, "seek": 666776, "start": 6674.4800000000005, "end": 6677.84, "text": " And then on the other hand, the question, they're, you know, they're still the bigger question of", "tokens": [50700, 400, 550, 322, 264, 661, 1011, 11, 264, 1168, 11, 436, 434, 11, 291, 458, 11, 436, 434, 920, 264, 3801, 1168, 295, 50868], "temperature": 0.0, "avg_logprob": -0.13569464222077401, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.10962487012147903}, {"id": 1184, "seek": 666776, "start": 6678.56, "end": 6684.0, "text": " how hard is alignment going to be, how fast are going to take us going to be and so on and like,", "tokens": [50904, 577, 1152, 307, 18515, 516, 281, 312, 11, 577, 2370, 366, 516, 281, 747, 505, 516, 281, 312, 293, 370, 322, 293, 411, 11, 51176], "temperature": 0.0, "avg_logprob": -0.13569464222077401, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.10962487012147903}, {"id": 1185, "seek": 666776, "start": 6684.0, "end": 6688.16, "text": " in a bad world, alignment is going to be quite hard and take us are going to be quite fast and", "tokens": [51176, 294, 257, 1578, 1002, 11, 18515, 307, 516, 281, 312, 1596, 1152, 293, 747, 505, 366, 516, 281, 312, 1596, 2370, 293, 51384], "temperature": 0.0, "avg_logprob": -0.13569464222077401, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.10962487012147903}, {"id": 1186, "seek": 666776, "start": 6688.16, "end": 6695.04, "text": " controllable. And then the question is, you know, it is like the level of control and alignment,", "tokens": [51384, 45159, 712, 13, 400, 550, 264, 1168, 307, 11, 291, 458, 11, 309, 307, 411, 264, 1496, 295, 1969, 293, 18515, 11, 51728], "temperature": 0.0, "avg_logprob": -0.13569464222077401, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.10962487012147903}, {"id": 1187, "seek": 669504, "start": 6695.76, "end": 6700.96, "text": " and like safety concern enough from these leaders. And there I'm like, less sure. So I feel like,", "tokens": [50400, 293, 411, 4514, 3136, 1547, 490, 613, 3523, 13, 400, 456, 286, 478, 411, 11, 1570, 988, 13, 407, 286, 841, 411, 11, 50660], "temperature": 0.0, "avg_logprob": -0.17251009818835136, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.01639740727841854}, {"id": 1188, "seek": 669504, "start": 6702.4, "end": 6707.36, "text": " yeah, it's, it's, it's definitely in like, it's, I think from my perspective, right? It's fair to say", "tokens": [50732, 1338, 11, 309, 311, 11, 309, 311, 11, 309, 311, 2138, 294, 411, 11, 309, 311, 11, 286, 519, 490, 452, 4585, 11, 558, 30, 467, 311, 3143, 281, 584, 50980], "temperature": 0.0, "avg_logprob": -0.17251009818835136, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.01639740727841854}, {"id": 1189, "seek": 669504, "start": 6708.32, "end": 6711.76, "text": " they're like pretty reasonable compared to the alternatives that we could have had.", "tokens": [51028, 436, 434, 411, 1238, 10585, 5347, 281, 264, 20478, 300, 321, 727, 362, 632, 13, 51200], "temperature": 0.0, "avg_logprob": -0.17251009818835136, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.01639740727841854}, {"id": 1190, "seek": 669504, "start": 6712.8, "end": 6718.4, "text": " But also, it's insufficient in almost all ways, right? Government needs to be involved in this.", "tokens": [51252, 583, 611, 11, 309, 311, 41709, 294, 1920, 439, 2098, 11, 558, 30, 7321, 2203, 281, 312, 3288, 294, 341, 13, 51532], "temperature": 0.0, "avg_logprob": -0.17251009818835136, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.01639740727841854}, {"id": 1191, "seek": 669504, "start": 6719.36, "end": 6724.48, "text": " They cannot externalize the risk. There are many things that they're already doing", "tokens": [51580, 814, 2644, 8320, 1125, 264, 3148, 13, 821, 366, 867, 721, 300, 436, 434, 1217, 884, 51836], "temperature": 0.0, "avg_logprob": -0.17251009818835136, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.01639740727841854}, {"id": 1192, "seek": 672448, "start": 6724.48, "end": 6729.599999999999, "text": " insufficiently well, I think, where they could have done way better, both with like how they release", "tokens": [50364, 41709, 356, 731, 11, 286, 519, 11, 689, 436, 727, 362, 1096, 636, 1101, 11, 1293, 365, 411, 577, 436, 4374, 50620], "temperature": 0.0, "avg_logprob": -0.07280038297176361, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.006286274176090956}, {"id": 1193, "seek": 672448, "start": 6729.599999999999, "end": 6735.28, "text": " as well as how they react to, to like problems, as well as, you know, like how they communicate", "tokens": [50620, 382, 731, 382, 577, 436, 4515, 281, 11, 281, 411, 2740, 11, 382, 731, 382, 11, 291, 458, 11, 411, 577, 436, 7890, 50904], "temperature": 0.0, "avg_logprob": -0.07280038297176361, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.006286274176090956}, {"id": 1194, "seek": 672448, "start": 6735.28, "end": 6739.679999999999, "text": " with the public about the risks that they're creating, etc., etc. So yeah, I think there,", "tokens": [50904, 365, 264, 1908, 466, 264, 10888, 300, 436, 434, 4084, 11, 5183, 7933, 5183, 13, 407, 1338, 11, 286, 519, 456, 11, 51124], "temperature": 0.0, "avg_logprob": -0.07280038297176361, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.006286274176090956}, {"id": 1195, "seek": 672448, "start": 6739.679999999999, "end": 6745.919999999999, "text": " there's a lot of room for improvement as well. And yeah, I really, I really think that, you know,", "tokens": [51124, 456, 311, 257, 688, 295, 1808, 337, 10444, 382, 731, 13, 400, 1338, 11, 286, 534, 11, 286, 534, 519, 300, 11, 291, 458, 11, 51436], "temperature": 0.0, "avg_logprob": -0.07280038297176361, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.006286274176090956}, {"id": 1196, "seek": 672448, "start": 6745.919999999999, "end": 6750.959999999999, "text": " AI safety is going to be a very, very hard problem. A lot of things have to have to go right for the", "tokens": [51436, 7318, 4514, 307, 516, 281, 312, 257, 588, 11, 588, 1152, 1154, 13, 316, 688, 295, 721, 362, 281, 362, 281, 352, 558, 337, 264, 51688], "temperature": 0.0, "avg_logprob": -0.07280038297176361, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.006286274176090956}, {"id": 1197, "seek": 675096, "start": 6750.96, "end": 6755.84, "text": " whole system to go right. And we definitely cannot just trust the labs, despite the best", "tokens": [50364, 1379, 1185, 281, 352, 558, 13, 400, 321, 2138, 2644, 445, 3361, 264, 20339, 11, 7228, 264, 1151, 50608], "temperature": 0.0, "avg_logprob": -0.08397982431494672, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0008829595753923059}, {"id": 1198, "seek": 675096, "start": 6755.84, "end": 6762.96, "text": " intentions to just solve it all on their own. Yeah, well, hence the, the need for third party", "tokens": [50608, 19354, 281, 445, 5039, 309, 439, 322, 641, 1065, 13, 865, 11, 731, 11, 16678, 264, 11, 264, 643, 337, 2636, 3595, 50964], "temperature": 0.0, "avg_logprob": -0.08397982431494672, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0008829595753923059}, {"id": 1199, "seek": 675096, "start": 6762.96, "end": 6767.92, "text": " auditing and the organization that you're building at Apollo Research, maybe just one last question.", "tokens": [50964, 2379, 1748, 293, 264, 4475, 300, 291, 434, 2390, 412, 25187, 10303, 11, 1310, 445, 472, 1036, 1168, 13, 51212], "temperature": 0.0, "avg_logprob": -0.08397982431494672, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0008829595753923059}, {"id": 1200, "seek": 675096, "start": 6767.92, "end": 6773.44, "text": " A number of people have reached out to me and said, I would like to get involved with red teaming.", "tokens": [51212, 316, 1230, 295, 561, 362, 6488, 484, 281, 385, 293, 848, 11, 286, 576, 411, 281, 483, 3288, 365, 2182, 1469, 278, 13, 51488], "temperature": 0.0, "avg_logprob": -0.08397982431494672, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0008829595753923059}, {"id": 1201, "seek": 677344, "start": 6774.32, "end": 6780.96, "text": " How can I do that? I wonder if you have any advice for individuals who might just want to", "tokens": [50408, 1012, 393, 286, 360, 300, 30, 286, 2441, 498, 291, 362, 604, 5192, 337, 5346, 567, 1062, 445, 528, 281, 50740], "temperature": 0.0, "avg_logprob": -0.09527049893918245, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.1311308741569519}, {"id": 1202, "seek": 677344, "start": 6781.679999999999, "end": 6787.36, "text": " do their own projects and, you know, release stuff, you know, just share findings individually with", "tokens": [50776, 360, 641, 1065, 4455, 293, 11, 291, 458, 11, 4374, 1507, 11, 291, 458, 11, 445, 2073, 16483, 16652, 365, 51060], "temperature": 0.0, "avg_logprob": -0.09527049893918245, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.1311308741569519}, {"id": 1203, "seek": 677344, "start": 6787.36, "end": 6795.36, "text": " the world, or perhaps and or perhaps, you know, what sort of skills are you in need of, as you're", "tokens": [51060, 264, 1002, 11, 420, 4317, 293, 420, 4317, 11, 291, 458, 11, 437, 1333, 295, 3942, 366, 291, 294, 643, 295, 11, 382, 291, 434, 51460], "temperature": 0.0, "avg_logprob": -0.09527049893918245, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.1311308741569519}, {"id": 1204, "seek": 677344, "start": 6795.36, "end": 6800.639999999999, "text": " going about building your own organization? I think one thing that is nice about model evaluations", "tokens": [51460, 516, 466, 2390, 428, 1065, 4475, 30, 286, 519, 472, 551, 300, 307, 1481, 466, 2316, 43085, 51724], "temperature": 0.0, "avg_logprob": -0.09527049893918245, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.1311308741569519}, {"id": 1205, "seek": 680064, "start": 6800.72, "end": 6805.04, "text": " and red teaming is you can just kind of start right away. You don't need that much, you know,", "tokens": [50368, 293, 2182, 1469, 278, 307, 291, 393, 445, 733, 295, 722, 558, 1314, 13, 509, 500, 380, 643, 300, 709, 11, 291, 458, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1065254361610713, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.05661429092288017}, {"id": 1206, "seek": 680064, "start": 6805.04, "end": 6811.52, "text": " technical expertise, because it's all in, like almost all of it is in text. And, you know, at", "tokens": [50584, 6191, 11769, 11, 570, 309, 311, 439, 294, 11, 411, 1920, 439, 295, 309, 307, 294, 2487, 13, 400, 11, 291, 458, 11, 412, 50908], "temperature": 0.0, "avg_logprob": -0.1065254361610713, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.05661429092288017}, {"id": 1207, "seek": 680064, "start": 6811.52, "end": 6817.4400000000005, "text": " least if you, if you want to, to redeem a language model specifically. And yeah, so my recommendation", "tokens": [50908, 1935, 498, 291, 11, 498, 291, 528, 281, 11, 281, 37715, 257, 2856, 2316, 4682, 13, 400, 1338, 11, 370, 452, 11879, 51204], "temperature": 0.0, "avg_logprob": -0.1065254361610713, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.05661429092288017}, {"id": 1208, "seek": 680064, "start": 6817.4400000000005, "end": 6824.240000000001, "text": " for individuals, first of all, would be to just start, like just engage with a model for, you", "tokens": [51204, 337, 5346, 11, 700, 295, 439, 11, 576, 312, 281, 445, 722, 11, 411, 445, 4683, 365, 257, 2316, 337, 11, 291, 51544], "temperature": 0.0, "avg_logprob": -0.1065254361610713, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.05661429092288017}, {"id": 1209, "seek": 680064, "start": 6824.240000000001, "end": 6828.64, "text": " know, a long, a longer period of time, maybe a day or so and see if whether you find interesting", "tokens": [51544, 458, 11, 257, 938, 11, 257, 2854, 2896, 295, 565, 11, 1310, 257, 786, 420, 370, 293, 536, 498, 1968, 291, 915, 1880, 51764], "temperature": 0.0, "avg_logprob": -0.1065254361610713, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.05661429092288017}, {"id": 1210, "seek": 682864, "start": 6828.72, "end": 6832.56, "text": " behavior or maybe, you know, maybe there's someone who has already done something, and maybe you", "tokens": [50368, 5223, 420, 1310, 11, 291, 458, 11, 1310, 456, 311, 1580, 567, 575, 1217, 1096, 746, 11, 293, 1310, 291, 50560], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1211, "seek": 682864, "start": 6832.56, "end": 6837.68, "text": " can poach a project from them, and, you know, just sort of as a starter thing. And from this,", "tokens": [50560, 393, 714, 608, 257, 1716, 490, 552, 11, 293, 11, 291, 458, 11, 445, 1333, 295, 382, 257, 22465, 551, 13, 400, 490, 341, 11, 50816], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1212, "seek": 682864, "start": 6837.68, "end": 6841.76, "text": " I feel like it kind of just takes a life of its own anyway, you know, as soon as you're hooked", "tokens": [50816, 286, 841, 411, 309, 733, 295, 445, 2516, 257, 993, 295, 1080, 1065, 4033, 11, 291, 458, 11, 382, 2321, 382, 291, 434, 20410, 51020], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1213, "seek": 682864, "start": 6841.76, "end": 6846.72, "text": " on a specific thing that you find interesting, you will, you know, you will really try to find", "tokens": [51020, 322, 257, 2685, 551, 300, 291, 915, 1880, 11, 291, 486, 11, 291, 458, 11, 291, 486, 534, 853, 281, 915, 51268], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1214, "seek": 682864, "start": 6847.360000000001, "end": 6852.0, "text": " additional ways in which this specific behavior could happen. In the same way, you know, let's,", "tokens": [51300, 4497, 2098, 294, 597, 341, 2685, 5223, 727, 1051, 13, 682, 264, 912, 636, 11, 291, 458, 11, 718, 311, 11, 51532], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1215, "seek": 682864, "start": 6852.0, "end": 6856.400000000001, "text": " let's take the deception thing, right? We, we started fairly exploratory and wanted to try", "tokens": [51532, 718, 311, 747, 264, 40451, 551, 11, 558, 30, 492, 11, 321, 1409, 6457, 24765, 4745, 293, 1415, 281, 853, 51752], "temperature": 0.0, "avg_logprob": -0.09513395901384025, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.060050345957279205}, {"id": 1216, "seek": 685640, "start": 6856.4, "end": 6860.08, "text": " how far we can get the model to be, to be deceptive. And then at some point, it just", "tokens": [50364, 577, 1400, 321, 393, 483, 264, 2316, 281, 312, 11, 281, 312, 368, 1336, 488, 13, 400, 550, 412, 512, 935, 11, 309, 445, 50548], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1217, "seek": 685640, "start": 6860.08, "end": 6864.719999999999, "text": " took a life of its own where we're like, okay, but like, why really does it do that? Right? Like,", "tokens": [50548, 1890, 257, 993, 295, 1080, 1065, 689, 321, 434, 411, 11, 1392, 11, 457, 411, 11, 983, 534, 775, 309, 360, 300, 30, 1779, 30, 1743, 11, 50780], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1218, "seek": 685640, "start": 6864.719999999999, "end": 6869.28, "text": " okay, we vary this behavior and like this thing and this, this variable in the environment,", "tokens": [50780, 1392, 11, 321, 10559, 341, 5223, 293, 411, 341, 551, 293, 341, 11, 341, 7006, 294, 264, 2823, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1219, "seek": 685640, "start": 6869.28, "end": 6874.0, "text": " we vary this thing, we vary all of these other things. And in the end, you can, again,", "tokens": [51008, 321, 10559, 341, 551, 11, 321, 10559, 439, 295, 613, 661, 721, 13, 400, 294, 264, 917, 11, 291, 393, 11, 797, 11, 51244], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1220, "seek": 685640, "start": 6874.0, "end": 6877.44, "text": " you kind of get like a more holistic and round picture of what's going on. So yeah,", "tokens": [51244, 291, 733, 295, 483, 411, 257, 544, 30334, 293, 3098, 3036, 295, 437, 311, 516, 322, 13, 407, 1338, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1221, "seek": 685640, "start": 6877.44, "end": 6882.08, "text": " I definitely think just start with like a thing you find interesting is definitely the way to go.", "tokens": [51416, 286, 2138, 519, 445, 722, 365, 411, 257, 551, 291, 915, 1880, 307, 2138, 264, 636, 281, 352, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1115579997023491, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.02032097615301609}, {"id": 1222, "seek": 688208, "start": 6883.04, "end": 6889.12, "text": " And like don't overthink, overthink it originally. And then the thing we are specifically looking", "tokens": [50412, 400, 411, 500, 380, 670, 21074, 11, 670, 21074, 309, 7993, 13, 400, 550, 264, 551, 321, 366, 4682, 1237, 50716], "temperature": 0.0, "avg_logprob": -0.10121717787625496, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.016911381855607033}, {"id": 1223, "seek": 688208, "start": 6889.12, "end": 6895.12, "text": " for, you know, so definitely kind of this mindset of like, oh, I just want to poke around and like", "tokens": [50716, 337, 11, 291, 458, 11, 370, 2138, 733, 295, 341, 12543, 295, 411, 11, 1954, 11, 286, 445, 528, 281, 19712, 926, 293, 411, 51016], "temperature": 0.0, "avg_logprob": -0.10121717787625496, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.016911381855607033}, {"id": 1224, "seek": 688208, "start": 6895.12, "end": 6899.5199999999995, "text": " really try to understand what's going on in a fairly like scientific manner, right? I also want", "tokens": [51016, 534, 853, 281, 1223, 437, 311, 516, 322, 294, 257, 6457, 411, 8134, 9060, 11, 558, 30, 286, 611, 528, 51236], "temperature": 0.0, "avg_logprob": -0.10121717787625496, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.016911381855607033}, {"id": 1225, "seek": 688208, "start": 6899.5199999999995, "end": 6903.68, "text": " to make sure that all of the confounders that could potentially explain this behavior have been", "tokens": [51236, 281, 652, 988, 300, 439, 295, 264, 1497, 554, 433, 300, 727, 7263, 2903, 341, 5223, 362, 668, 51444], "temperature": 0.0, "avg_logprob": -0.10121717787625496, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.016911381855607033}, {"id": 1226, "seek": 688208, "start": 6903.68, "end": 6908.48, "text": " controlled for, which I think is the hard part in red teaming. So this is definitely something", "tokens": [51444, 10164, 337, 11, 597, 286, 519, 307, 264, 1152, 644, 294, 2182, 1469, 278, 13, 407, 341, 307, 2138, 746, 51684], "temperature": 0.0, "avg_logprob": -0.10121717787625496, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.016911381855607033}, {"id": 1227, "seek": 690848, "start": 6908.48, "end": 6913.28, "text": " we're looking for. And then just, you know, the more you understand language models and state of", "tokens": [50364, 321, 434, 1237, 337, 13, 400, 550, 445, 11, 291, 458, 11, 264, 544, 291, 1223, 2856, 5245, 293, 1785, 295, 50604], "temperature": 0.0, "avg_logprob": -0.08485401626181814, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.02674996294081211}, {"id": 1228, "seek": 690848, "start": 6913.28, "end": 6918.639999999999, "text": " the art models, the easier it will become, right? Some behavior might be very easily explainable", "tokens": [50604, 264, 1523, 5245, 11, 264, 3571, 309, 486, 1813, 11, 558, 30, 2188, 5223, 1062, 312, 588, 3612, 2903, 712, 50872], "temperature": 0.0, "avg_logprob": -0.08485401626181814, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.02674996294081211}, {"id": 1229, "seek": 690848, "start": 6918.639999999999, "end": 6923.679999999999, "text": " by sort of problems with RLHF. So if you know how RLHF works and specifically how it was trained,", "tokens": [50872, 538, 1333, 295, 2740, 365, 497, 43, 39, 37, 13, 407, 498, 291, 458, 577, 497, 43, 39, 37, 1985, 293, 4682, 577, 309, 390, 8895, 11, 51124], "temperature": 0.0, "avg_logprob": -0.08485401626181814, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.02674996294081211}, {"id": 1230, "seek": 690848, "start": 6924.719999999999, "end": 6928.719999999999, "text": " you may probably you will probably understand the red teaming efforts much better.", "tokens": [51176, 291, 815, 1391, 291, 486, 1391, 1223, 264, 2182, 1469, 278, 6484, 709, 1101, 13, 51376], "temperature": 0.0, "avg_logprob": -0.08485401626181814, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.02674996294081211}, {"id": 1231, "seek": 690848, "start": 6929.679999999999, "end": 6933.919999999999, "text": " If you know, you know, like if you have a better understanding of how the instruction", "tokens": [51424, 759, 291, 458, 11, 291, 458, 11, 411, 498, 291, 362, 257, 1101, 3701, 295, 577, 264, 10951, 51636], "temperature": 0.0, "avg_logprob": -0.08485401626181814, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.02674996294081211}, {"id": 1232, "seek": 693392, "start": 6933.92, "end": 6938.64, "text": " fine tuning actually works, maybe you will find those. So, you know, so for example,", "tokens": [50364, 2489, 15164, 767, 1985, 11, 1310, 291, 486, 915, 729, 13, 407, 11, 291, 458, 11, 370, 337, 1365, 11, 50600], "temperature": 0.0, "avg_logprob": -0.11376115371441019, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.07580552250146866}, {"id": 1233, "seek": 693392, "start": 6938.64, "end": 6943.28, "text": " maybe to give it to give like an intuition here, in the in our case, right, as I said earlier,", "tokens": [50600, 1310, 281, 976, 309, 281, 976, 411, 364, 24002, 510, 11, 294, 264, 294, 527, 1389, 11, 558, 11, 382, 286, 848, 3071, 11, 50832], "temperature": 0.0, "avg_logprob": -0.11376115371441019, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.07580552250146866}, {"id": 1234, "seek": 693392, "start": 6943.28, "end": 6948.8, "text": " we have the the three HS and then instruction fine tuning, and you can, you know, trade off the", "tokens": [50832, 321, 362, 264, 264, 1045, 34194, 293, 550, 10951, 2489, 15164, 11, 293, 291, 393, 11, 291, 458, 11, 4923, 766, 264, 51108], "temperature": 0.0, "avg_logprob": -0.11376115371441019, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.07580552250146866}, {"id": 1235, "seek": 693392, "start": 6948.8, "end": 6954.64, "text": " different components against each other to find different things. I like to find niches of the", "tokens": [51108, 819, 6677, 1970, 1184, 661, 281, 915, 819, 721, 13, 286, 411, 281, 915, 25570, 279, 295, 264, 51400], "temperature": 0.0, "avg_logprob": -0.11376115371441019, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.07580552250146866}, {"id": 1236, "seek": 693392, "start": 6954.64, "end": 6959.52, "text": " model where it acts in ways that we think it shouldn't act, because maybe they weren't covered", "tokens": [51400, 2316, 689, 309, 10672, 294, 2098, 300, 321, 519, 309, 4659, 380, 605, 11, 570, 1310, 436, 4999, 380, 5343, 51644], "temperature": 0.0, "avg_logprob": -0.11376115371441019, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.07580552250146866}, {"id": 1237, "seek": 695952, "start": 6959.52, "end": 6965.6, "text": " explicitly or implicitly by by gradient descent. And if you if you sort of have a theoretical", "tokens": [50364, 20803, 420, 26947, 356, 538, 538, 16235, 23475, 13, 400, 498, 291, 498, 291, 1333, 295, 362, 257, 20864, 50668], "temperature": 0.0, "avg_logprob": -0.13029470083848485, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.004330547526478767}, {"id": 1238, "seek": 695952, "start": 6965.6, "end": 6969.76, "text": " framework like this, it suddenly becomes much easier on how to do the red teaming in the first", "tokens": [50668, 8388, 411, 341, 11, 309, 5800, 3643, 709, 3571, 322, 577, 281, 360, 264, 2182, 1469, 278, 294, 264, 700, 50876], "temperature": 0.0, "avg_logprob": -0.13029470083848485, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.004330547526478767}, {"id": 1239, "seek": 695952, "start": 6969.76, "end": 6975.040000000001, "text": " place. So like some theoretical understanding of how the process works is definitely helpful as", "tokens": [50876, 1081, 13, 407, 411, 512, 20864, 3701, 295, 577, 264, 1399, 1985, 307, 2138, 4961, 382, 51140], "temperature": 0.0, "avg_logprob": -0.13029470083848485, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.004330547526478767}, {"id": 1240, "seek": 695952, "start": 6975.040000000001, "end": 6981.120000000001, "text": " well for a teaming and also something we're actively looking for. Marius Havan, founder and", "tokens": [51140, 731, 337, 257, 1469, 278, 293, 611, 746, 321, 434, 13022, 1237, 337, 13, 2039, 4872, 389, 21071, 11, 14917, 293, 51444], "temperature": 0.0, "avg_logprob": -0.13029470083848485, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.004330547526478767}, {"id": 1241, "seek": 695952, "start": 6981.120000000001, "end": 6986.72, "text": " CEO of Apollo Research. Thank you for being part of the cognitive revolution. Thanks for inviting", "tokens": [51444, 9282, 295, 25187, 10303, 13, 1044, 291, 337, 885, 644, 295, 264, 15605, 8894, 13, 2561, 337, 18202, 51724], "temperature": 0.0, "avg_logprob": -0.13029470083848485, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.004330547526478767}, {"id": 1242, "seek": 698672, "start": 6987.4400000000005, "end": 6992.320000000001, "text": " it is both energizing and enlightening to hear why people listen and learn what they value about", "tokens": [50400, 309, 307, 1293, 10575, 3319, 293, 18690, 4559, 281, 1568, 983, 561, 2140, 293, 1466, 437, 436, 2158, 466, 50644], "temperature": 0.0, "avg_logprob": -0.11182945610111596, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.02594653144478798}, {"id": 1243, "seek": 698672, "start": 6992.320000000001, "end": 6999.76, "text": " the show. So please don't hesitate to reach out via email at TCR at turpentine.co, or you can DM me", "tokens": [50644, 264, 855, 13, 407, 1767, 500, 380, 20842, 281, 2524, 484, 5766, 3796, 412, 314, 18547, 412, 3243, 22786, 533, 13, 1291, 11, 420, 291, 393, 15322, 385, 51016], "temperature": 0.0, "avg_logprob": -0.11182945610111596, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.02594653144478798}, {"id": 1244, "seek": 698672, "start": 6999.76, "end": 7006.320000000001, "text": " on the social media platform of your choice. Omniki uses generative AI to enable you to launch", "tokens": [51016, 322, 264, 2093, 3021, 3663, 295, 428, 3922, 13, 9757, 77, 9850, 4960, 1337, 1166, 7318, 281, 9528, 291, 281, 4025, 51344], "temperature": 0.0, "avg_logprob": -0.11182945610111596, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.02594653144478798}, {"id": 1245, "seek": 698672, "start": 7006.320000000001, "end": 7011.68, "text": " hundreds of thousands of ad iterations that actually work customized across all platforms", "tokens": [51344, 6779, 295, 5383, 295, 614, 36540, 300, 767, 589, 30581, 2108, 439, 9473, 51612], "temperature": 0.0, "avg_logprob": -0.11182945610111596, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.02594653144478798}, {"id": 1246, "seek": 698672, "start": 7011.68, "end": 7016.400000000001, "text": " with a click of a button. I believe in Omniki so much that I invested in it. And I recommend", "tokens": [51612, 365, 257, 2052, 295, 257, 2960, 13, 286, 1697, 294, 9757, 77, 9850, 370, 709, 300, 286, 13104, 294, 309, 13, 400, 286, 2748, 51848], "temperature": 0.0, "avg_logprob": -0.11182945610111596, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.02594653144478798}, {"id": 1247, "seek": 701640, "start": 7016.4, "end": 7024.08, "text": " you use it too. Use CogGrav to get a 10% discount.", "tokens": [50364, 291, 764, 309, 886, 13, 8278, 383, 664, 38, 13404, 281, 483, 257, 1266, 4, 11635, 13, 50748], "temperature": 0.0, "avg_logprob": -0.41800708770751954, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.015415226109325886}], "language": "en"}