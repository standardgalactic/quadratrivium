1
00:00:00,000 --> 00:00:01,520
They don't care about AI safety.

2
00:00:01,520 --> 00:00:02,880
What they care about is AI control.

3
00:00:03,440 --> 00:00:05,680
Do I think we eventually get to a configuration like that?

4
00:00:05,680 --> 00:00:06,320
Maybe.

5
00:00:06,320 --> 00:00:10,080
Where you have an AI brain is at the center of civilization,

6
00:00:10,080 --> 00:00:12,160
and it's coordinating all the people around it.

7
00:00:12,160 --> 00:00:14,960
And every civilization that makes it

8
00:00:14,960 --> 00:00:17,440
is capable of crowdfunding and operating its own AI.

9
00:00:17,440 --> 00:00:19,760
You know, our background culture influences things

10
00:00:19,760 --> 00:00:21,440
in ways we don't even think about.

11
00:00:21,440 --> 00:00:23,680
So much of the paperclip thinking

12
00:00:23,680 --> 00:00:27,120
is like a vengeful god will turn you into pillars of salt.

13
00:00:27,120 --> 00:00:30,560
The polytheistic model of many gods, as opposed to one god,

14
00:00:30,560 --> 00:00:32,880
is we're all going to have our own AI gods,

15
00:00:32,880 --> 00:00:34,000
and there'll be war of the gods.

16
00:00:34,640 --> 00:00:36,880
Man-machine symbiosis is not some new thing.

17
00:00:37,440 --> 00:00:40,560
It's actually the old thing that broke us away

18
00:00:40,560 --> 00:00:43,200
from other primate lineages that weren't using tools.

19
00:00:43,760 --> 00:00:45,840
Then the question is, what's the next step?

20
00:00:45,840 --> 00:00:48,480
Which is AI is amplified intelligence.

21
00:00:48,480 --> 00:00:51,120
It is that the AI human fusion

22
00:00:51,840 --> 00:00:53,920
means there's another 20 Elon Musk's,

23
00:00:53,920 --> 00:00:54,960
or whatever the number is.

24
00:00:55,680 --> 00:00:56,560
That's good.

25
00:00:56,640 --> 00:00:58,960
Hello, and welcome to The Cognitive Revolution,

26
00:00:58,960 --> 00:01:02,000
where we interview visionary researchers, entrepreneurs,

27
00:01:02,000 --> 00:01:03,920
and builders working on the frontier

28
00:01:03,920 --> 00:01:05,120
of artificial intelligence.

29
00:01:05,840 --> 00:01:08,640
Each week, we'll explore their revolutionary ideas,

30
00:01:08,640 --> 00:01:10,080
and together, we'll build a picture

31
00:01:10,080 --> 00:01:13,600
of how AI technology will transform work, life,

32
00:01:13,600 --> 00:01:15,200
and society in the coming years.

33
00:01:15,760 --> 00:01:19,280
I'm Nathan LaBenz, joined by my co-host, Eric Torenberg.

34
00:01:19,280 --> 00:01:22,000
Hello, and welcome back to The Cognitive Revolution.

35
00:01:22,960 --> 00:01:25,200
Today, my guest is biology, Srinivasan.

36
00:01:25,920 --> 00:01:28,880
In tech circles, biology needs no introduction.

37
00:01:28,880 --> 00:01:30,800
But for folks from other backgrounds,

38
00:01:30,800 --> 00:01:32,960
biology is a serial startup entrepreneur

39
00:01:32,960 --> 00:01:35,120
who's founded and ultimately sold

40
00:01:35,120 --> 00:01:37,360
highly dissimilar technology companies,

41
00:01:37,360 --> 00:01:38,800
including Teleport,

42
00:01:38,800 --> 00:01:40,560
which helped people move around the world

43
00:01:40,560 --> 00:01:41,680
to realize opportunities,

44
00:01:42,560 --> 00:01:44,880
Council, which provided genetic testing

45
00:01:44,880 --> 00:01:46,320
for couples planning to have children,

46
00:01:46,880 --> 00:01:51,200
and Earn.com, a paid email on the blockchain startup,

47
00:01:51,200 --> 00:01:53,360
which ultimately sold to Coinbase,

48
00:01:53,360 --> 00:01:55,200
where biology became CTO.

49
00:01:56,560 --> 00:01:59,120
Along the way, he's also taught statistics at Stanford

50
00:01:59,120 --> 00:02:01,920
and been a general partner at Andreessen Horowitz as well.

51
00:02:03,200 --> 00:02:05,440
Today, as an independent thinker, investor,

52
00:02:05,440 --> 00:02:06,960
and author of the network state,

53
00:02:06,960 --> 00:02:08,640
biology is extremely prolific

54
00:02:08,640 --> 00:02:11,040
in both text and audio formats.

55
00:02:11,040 --> 00:02:11,760
And as you'll hear,

56
00:02:11,760 --> 00:02:13,840
whether for the first time or the 50th,

57
00:02:13,840 --> 00:02:15,840
he is an incredibly creative thinker

58
00:02:15,840 --> 00:02:18,240
who relentlessly develops and iterates

59
00:02:18,240 --> 00:02:20,320
on new paradigms for understanding

60
00:02:20,320 --> 00:02:22,800
a fast-changing, often chaotic world.

61
00:02:23,760 --> 00:02:25,120
He's also a very associative

62
00:02:25,120 --> 00:02:26,720
and interdisciplinary thinker

63
00:02:26,720 --> 00:02:29,520
who constantly adds dimensions to any analysis.

64
00:02:31,200 --> 00:02:32,720
Such horsepower can be hard

65
00:02:32,720 --> 00:02:34,800
for a podcast host to rein in,

66
00:02:34,800 --> 00:02:37,360
but I personally find it extremely stimulating.

67
00:02:37,360 --> 00:02:38,240
So in this conversation,

68
00:02:38,240 --> 00:02:39,280
I tried to strike a balance

69
00:02:39,280 --> 00:02:41,760
between letting biology go off as only he can do,

70
00:02:42,480 --> 00:02:44,720
contributing what I hope are worthy versions

71
00:02:44,720 --> 00:02:46,400
of core AI safety arguments

72
00:02:46,400 --> 00:02:48,400
and supporting results from recent research,

73
00:02:49,120 --> 00:02:50,160
and occasionally,

74
00:02:50,160 --> 00:02:51,920
steering us back toward what I see

75
00:02:51,920 --> 00:02:54,960
as the most critical questions for the AI big picture.

76
00:02:56,720 --> 00:02:58,800
If there's one area where biology and I

77
00:02:58,800 --> 00:03:00,640
disagree most consequentially,

78
00:03:00,640 --> 00:03:03,760
it's on the question of how independent AI systems

79
00:03:03,760 --> 00:03:06,560
are likely to become over the next five to 10 years.

80
00:03:07,440 --> 00:03:08,960
Biology thinks that AI systems

81
00:03:08,960 --> 00:03:11,600
need to be at least symbiotic with humans,

82
00:03:11,600 --> 00:03:13,040
because physical computers

83
00:03:13,040 --> 00:03:15,600
can't replicate themselves without human support.

84
00:03:16,720 --> 00:03:19,280
While I think there's at least a significant chance

85
00:03:19,360 --> 00:03:22,640
that we get AIs that are so independent of humans

86
00:03:22,640 --> 00:03:24,880
that their behaviors and interactions

87
00:03:24,880 --> 00:03:27,360
become the primary drivers of world history.

88
00:03:28,960 --> 00:03:30,400
In biology's own words,

89
00:03:30,400 --> 00:03:32,960
he does expect massive economic

90
00:03:32,960 --> 00:03:34,960
and social disruption from AI,

91
00:03:35,600 --> 00:03:37,520
but doesn't think that quote-unquote

92
00:03:37,520 --> 00:03:40,800
can't turn the killer AI off scenarios,

93
00:03:40,800 --> 00:03:42,960
are likely, at least for a long while,

94
00:03:42,960 --> 00:03:45,760
due to factors like the existence of adversarial inputs

95
00:03:45,760 --> 00:03:47,520
that can paralyze AIs,

96
00:03:47,520 --> 00:03:49,440
particularly those with open model weights,

97
00:03:50,320 --> 00:03:52,800
the observation that even decentralized programs

98
00:03:52,800 --> 00:03:55,040
like the Bitcoin network can't run independently

99
00:03:55,040 --> 00:03:56,800
without continuous human support,

100
00:03:57,520 --> 00:04:00,160
and the premise that to control the physical world,

101
00:04:00,160 --> 00:04:02,960
AIs will need to direct either large numbers of humans

102
00:04:02,960 --> 00:04:05,040
who are notoriously difficult to control

103
00:04:05,040 --> 00:04:08,640
or highly agile robots which don't yet exist.

104
00:04:10,160 --> 00:04:11,120
With all that in mind,

105
00:04:11,120 --> 00:04:12,640
in the first half of this conversation,

106
00:04:12,640 --> 00:04:14,320
you'll hear biology's analysis

107
00:04:14,320 --> 00:04:16,240
of the likely impact of AI

108
00:04:16,240 --> 00:04:19,760
in a world where powerful AI systems do come to exist,

109
00:04:19,760 --> 00:04:21,120
but humans retain control,

110
00:04:22,160 --> 00:04:24,560
resulting in a human AI symbiosis

111
00:04:24,560 --> 00:04:27,360
similar to how believers relate to their gods

112
00:04:27,360 --> 00:04:29,200
or citizens relate to their governments.

113
00:04:30,160 --> 00:04:31,280
Then in the second half,

114
00:04:31,280 --> 00:04:32,640
we really dig into the question

115
00:04:32,640 --> 00:04:34,640
of just how confident we should be

116
00:04:34,640 --> 00:04:38,480
that AI won't prove to be even more revolutionary than that.

117
00:04:39,920 --> 00:04:42,080
After more than two hours of recording,

118
00:04:42,080 --> 00:04:44,480
I was the one who ran out of time today,

119
00:04:44,560 --> 00:04:47,040
but I really enjoyed this conversation with biology.

120
00:04:47,040 --> 00:04:50,560
He is as good-natured and curious as he is opinionated,

121
00:04:50,560 --> 00:04:53,280
and we have continued to exchange links and arguments offline,

122
00:04:53,280 --> 00:04:55,040
such that I hope we'll have another episode

123
00:04:55,040 --> 00:04:57,200
to share with you in the future as well.

124
00:04:58,720 --> 00:05:00,560
As always, if you're enjoying the show,

125
00:05:00,560 --> 00:05:03,120
we'd ask that you take a moment to share it with a friend.

126
00:05:03,760 --> 00:05:06,960
And with that, here's part one of an all-angles look

127
00:05:06,960 --> 00:05:10,720
at how AI will shape the future with biology, Srinivasan.

128
00:05:11,680 --> 00:05:15,440
Biology, Srinivasan, welcome to the Cognitive Revolution.

129
00:05:15,440 --> 00:05:17,360
All right, I feel welcome.

130
00:05:17,360 --> 00:05:19,200
Well, we've got a ton to talk about.

131
00:05:19,200 --> 00:05:22,080
Obviously, you bring a lot of different perspectives

132
00:05:22,080 --> 00:05:25,200
to everything that you think about and work on.

133
00:05:25,840 --> 00:05:28,960
And today, I want to just try to muster

134
00:05:28,960 --> 00:05:31,280
all those different perspectives onto this.

135
00:05:31,280 --> 00:05:34,240
What I see is really the defining question of our time,

136
00:05:34,240 --> 00:05:36,000
which is like, what's up with AI?

137
00:05:36,000 --> 00:05:38,000
And how's it going to turn out?

138
00:05:38,880 --> 00:05:40,320
I thought maybe for starters,

139
00:05:40,320 --> 00:05:42,320
I would love to just get your baseline kind of table

140
00:05:42,320 --> 00:05:48,480
setting on how much more AI progress do you expect us to see

141
00:05:48,480 --> 00:05:50,080
over the next few years?

142
00:05:50,080 --> 00:05:53,040
Like how powerful are AI systems going to become

143
00:05:53,600 --> 00:05:55,920
in, again, kind of a relatively short timeline?

144
00:05:55,920 --> 00:05:58,800
And then maybe if you want to take a bigger stab at it,

145
00:05:58,800 --> 00:06:01,200
you could answer that same question for a longer timeline

146
00:06:01,200 --> 00:06:02,880
like the rest of our lives or whatever.

147
00:06:02,880 --> 00:06:04,400
Sure. Let me give an abstract answer,

148
00:06:04,400 --> 00:06:05,840
then let me give a technical answer.

149
00:06:06,800 --> 00:06:10,800
If you look at evolution, we've seen something as complex

150
00:06:10,800 --> 00:06:15,360
as flight evolve independently in birds, bats, and bees.

151
00:06:16,560 --> 00:06:22,080
And even intelligence, we've seen fairly high intelligence

152
00:06:22,080 --> 00:06:25,600
in dolphins, in whales, in octopuses.

153
00:06:27,120 --> 00:06:29,200
Octopus in particular can do like tool manipulation.

154
00:06:29,200 --> 00:06:32,080
They've got things that are a lot like hands with tentacles.

155
00:06:32,080 --> 00:06:35,200
And so that indicates that it is plausible

156
00:06:35,840 --> 00:06:39,120
that you could have multiple pathways to intelligence,

157
00:06:39,920 --> 00:06:42,400
whether we have carbon-based intelligence,

158
00:06:42,400 --> 00:06:43,920
so we could have silicon-based intelligence

159
00:06:43,920 --> 00:06:45,280
that just has a totally different form

160
00:06:45,280 --> 00:06:47,680
where the fundamental thing is an electromagnetic wave

161
00:06:47,680 --> 00:06:51,440
and data storage as opposed to DNA and so on.

162
00:06:51,440 --> 00:06:52,960
So that's like a plausibility argument

163
00:06:52,960 --> 00:06:55,680
in terms of evolution is being so resourceful

164
00:06:55,680 --> 00:06:58,560
that it's invented really complicated things

165
00:06:58,560 --> 00:06:59,360
in different ways.

166
00:07:00,480 --> 00:07:02,320
Then in terms of the technical point,

167
00:07:02,320 --> 00:07:04,960
I think as of right now, I should probably date it,

168
00:07:05,440 --> 00:07:08,480
December 11, 2023, because this field moves so fast, right?

169
00:07:09,120 --> 00:07:12,240
My view is, and maybe you'll have a different view,

170
00:07:12,240 --> 00:07:14,400
is that the breakthroughs that are really needed

171
00:07:14,400 --> 00:07:16,720
for something that's like true artificial intelligence

172
00:07:16,720 --> 00:07:20,240
that is human-independent, right?

173
00:07:20,240 --> 00:07:22,560
Maybe the next step after the Turing test,

174
00:07:22,560 --> 00:07:24,720
I've got an article that we're writing

175
00:07:24,720 --> 00:07:25,920
called the Turing Thresholds,

176
00:07:26,640 --> 00:07:28,400
which tries to generalize the Turing test

177
00:07:28,400 --> 00:07:30,240
to like the Kardashev scale.

178
00:07:30,240 --> 00:07:31,680
Have you got energy thresholds?

179
00:07:31,680 --> 00:07:33,280
What are useful scales beyond that?

180
00:07:33,840 --> 00:07:37,920
And right now, I think that what we call AI

181
00:07:37,920 --> 00:07:40,480
is absolutely amazing for environments

182
00:07:40,480 --> 00:07:42,480
that are not time-varying or rule-varying.

183
00:07:43,840 --> 00:07:46,960
And what I mean by that is, so you kind of have,

184
00:07:46,960 --> 00:07:48,880
let's say two large schools of AI,

185
00:07:48,880 --> 00:07:50,240
and obviously there's overlap

186
00:07:50,240 --> 00:07:51,840
in terms of the personnel and so on,

187
00:07:51,840 --> 00:07:53,760
but there's like the DeepMind School,

188
00:07:53,760 --> 00:07:55,200
which has gotten less press recently,

189
00:07:55,200 --> 00:07:57,520
but got more press a few years ago,

190
00:07:57,520 --> 00:08:00,160
and that is game-playing, right?

191
00:08:00,160 --> 00:08:04,720
It is super human-playing of go without go.

192
00:08:04,720 --> 00:08:07,920
It is all the video game stuff they've done

193
00:08:07,920 --> 00:08:09,280
where they learn at the pixel level

194
00:08:09,280 --> 00:08:11,200
and they just teach the very basic rules

195
00:08:11,200 --> 00:08:12,640
and it figures it out from there.

196
00:08:12,640 --> 00:08:14,960
And it's also the protein folding stuff

197
00:08:14,960 --> 00:08:15,920
and what have you, right?

198
00:08:16,800 --> 00:08:18,160
But in general, I think they're known

199
00:08:18,160 --> 00:08:20,880
for reinforcement learning and those kinds of approaches.

200
00:08:20,880 --> 00:08:21,840
I mean, they're good at a lot of things,

201
00:08:21,840 --> 00:08:23,360
but that's what I think DeepMind is known for.

202
00:08:23,360 --> 00:08:25,680
Of course, they put out this new model recently,

203
00:08:25,680 --> 00:08:27,040
the Gemini model.

204
00:08:27,040 --> 00:08:29,040
So I'm not saying that they're not good at everything,

205
00:08:29,040 --> 00:08:31,440
but that's just kind of what they're maybe most known for.

206
00:08:31,440 --> 00:08:35,120
And then you have the OpenAI ChatGBT School of Generative AI,

207
00:08:35,120 --> 00:08:39,040
and it includes stable diffusion and just as a pioneer,

208
00:08:39,040 --> 00:08:40,320
even if they're not,

209
00:08:40,320 --> 00:08:41,840
I don't know how much they're used right now,

210
00:08:41,840 --> 00:08:45,040
but basically, you have the diffusion models for images

211
00:08:45,040 --> 00:08:47,280
and you have large language models

212
00:08:47,280 --> 00:08:49,120
and now you have the multimodals that integrate them.

213
00:08:49,120 --> 00:08:51,280
And so the difference, I think with these,

214
00:08:51,280 --> 00:08:54,800
is the reinforcement learning approaches

215
00:08:54,800 --> 00:08:58,160
are based on an assumption of static rules,

216
00:08:58,160 --> 00:09:00,080
like the rules of chess, the rules that go,

217
00:09:00,080 --> 00:09:02,240
the rules of a video game are not changing with time.

218
00:09:02,240 --> 00:09:02,960
They are discoverable.

219
00:09:02,960 --> 00:09:04,080
They're like the laws of physics.

220
00:09:04,640 --> 00:09:08,800
And similarly, like the body of language

221
00:09:08,800 --> 00:09:09,920
where you're learning it,

222
00:09:09,920 --> 00:09:11,840
English is not rapidly time varying.

223
00:09:12,480 --> 00:09:14,480
That is to say, the rules of grammar

224
00:09:14,480 --> 00:09:15,920
that are implicit aren't changing.

225
00:09:15,920 --> 00:09:18,560
The meanings of words aren't changing very rapidly.

226
00:09:18,560 --> 00:09:20,800
You can argue they're changing over the span of decades

227
00:09:20,800 --> 00:09:23,200
or centuries, but not extremely rapidly, right?

228
00:09:23,200 --> 00:09:26,880
So therefore, when you generate a new result,

229
00:09:26,880 --> 00:09:29,200
training data from five years ago for English

230
00:09:29,200 --> 00:09:31,120
is actually still fairly valuable

231
00:09:31,120 --> 00:09:33,840
and the same input roughly gives the same output.

232
00:09:33,840 --> 00:09:35,760
Now, of course, there are facts that change with time,

233
00:09:36,720 --> 00:09:39,280
like who is the ruler of England, right?

234
00:09:39,280 --> 00:09:40,720
The queen of England is passed away now.

235
00:09:40,720 --> 00:09:41,840
It's the king of England, right?

236
00:09:41,840 --> 00:09:43,280
It's just facts that change with time.

237
00:09:43,280 --> 00:09:44,480
But I think more fundamentally

238
00:09:44,480 --> 00:09:46,400
is when there's rules that change with time.

239
00:09:47,040 --> 00:09:51,040
You have, for example, changes in law in countries, right?

240
00:09:51,040 --> 00:09:53,040
But most interestingly, perhaps changes in markets

241
00:09:54,000 --> 00:09:54,880
because the same input

242
00:09:54,880 --> 00:09:56,480
does not give the same output in a market.

243
00:09:56,480 --> 00:09:58,080
If you try that, then what will happen

244
00:09:58,080 --> 00:10:00,400
is there's adversarial behavior on the other side.

245
00:10:00,400 --> 00:10:01,840
And once people see it enough times,

246
00:10:01,840 --> 00:10:03,200
they'll see your strategy

247
00:10:03,200 --> 00:10:05,360
and they're going to trade against you on that, right?

248
00:10:05,360 --> 00:10:07,200
And I can get to other technical examples on that,

249
00:10:07,200 --> 00:10:08,960
but I think, and probably people in the space

250
00:10:08,960 --> 00:10:09,840
are aware of this,

251
00:10:09,840 --> 00:10:11,760
but I think that is a true frontier

252
00:10:11,760 --> 00:10:15,120
is dealing with time-varying, rule-varying systems,

253
00:10:15,120 --> 00:10:18,240
as opposed to systems where the implicit rules are static.

254
00:10:18,240 --> 00:10:19,120
Let me pause there.

255
00:10:19,120 --> 00:10:20,160
Yeah, I think that makes sense.

256
00:10:20,160 --> 00:10:22,960
I think in the very practical,

257
00:10:23,920 --> 00:10:25,920
just trying to get, as Zvi calls it,

258
00:10:25,920 --> 00:10:27,360
mundane utility from AI,

259
00:10:28,000 --> 00:10:31,440
that is often cashed out to AI is good at tasks,

260
00:10:31,440 --> 00:10:33,680
but it's not good at whole jobs.

261
00:10:33,680 --> 00:10:35,920
It can handle these small things

262
00:10:35,920 --> 00:10:38,240
where you can define what good looks like

263
00:10:38,240 --> 00:10:40,480
and tell it exactly what to do.

264
00:10:40,480 --> 00:10:45,200
But in the broader context of handling things

265
00:10:45,200 --> 00:10:46,480
that come up as they come up,

266
00:10:47,040 --> 00:10:48,560
it's definitely not there yet.

267
00:10:49,440 --> 00:10:54,000
And I agree that there's likely to be some synthesis,

268
00:10:54,000 --> 00:10:58,560
which is kind of the subject of all the Q-star rumors recently,

269
00:10:58,560 --> 00:11:01,920
I would say is kind of the prospect

270
00:11:01,920 --> 00:11:05,280
that there could be already within the labs

271
00:11:05,280 --> 00:11:07,600
a beginning of a synthesis between the,

272
00:11:08,240 --> 00:11:11,520
I kind of think of it as like harder-edged reinforcement

273
00:11:11,520 --> 00:11:15,680
learning systems that are like small, efficient, and deadly,

274
00:11:15,680 --> 00:11:17,840
versus the language model systems

275
00:11:17,840 --> 00:11:20,800
that are like kind of slow and soft,

276
00:11:20,800 --> 00:11:22,960
and but have a sense of our values,

277
00:11:22,960 --> 00:11:25,280
which is really a remarkable accomplishment

278
00:11:25,280 --> 00:11:29,040
that they're able to have even an approximation of our values

279
00:11:29,040 --> 00:11:30,480
that seems like reasonably good.

280
00:11:31,200 --> 00:11:36,160
So yeah, I think I agree with that framing,

281
00:11:36,720 --> 00:11:40,080
but I guess I would still wonder like,

282
00:11:40,720 --> 00:11:44,560
how far do you think this goes in the near term?

283
00:11:44,560 --> 00:11:46,720
Because I have a lot of uncertainty about that,

284
00:11:46,720 --> 00:11:48,160
and I think the field has a lot of uncertainty.

285
00:11:48,160 --> 00:11:50,080
You hear people say, well,

286
00:11:50,080 --> 00:11:52,480
it's never going to get smarter than its training data.

287
00:11:52,480 --> 00:11:54,800
It'll kind of level out where humans are,

288
00:11:54,800 --> 00:11:55,840
but we certainly don't see that

289
00:11:55,840 --> 00:11:58,480
in the reinforcement learning side, right?

290
00:11:58,480 --> 00:12:01,760
Like once, it usually don't take too long

291
00:12:01,760 --> 00:12:03,600
at human level of these games,

292
00:12:03,600 --> 00:12:05,440
and then it like blows past human level.

293
00:12:05,440 --> 00:12:08,400
Interestingly, you do still see some adversarial vulnerability,

294
00:12:08,400 --> 00:12:12,160
like there's a great paper from the team at FAR AI,

295
00:12:12,160 --> 00:12:14,560
and I'm planning to have Adam Gleave,

296
00:12:14,560 --> 00:12:16,000
the head of that organization on soon,

297
00:12:16,000 --> 00:12:17,360
to talk about that and other things,

298
00:12:17,360 --> 00:12:19,840
where they found like a, basically a hack

299
00:12:19,840 --> 00:12:23,920
where a really simple, but unexpected attack

300
00:12:23,920 --> 00:12:27,280
on the superhuman go player can defeat it.

301
00:12:27,280 --> 00:12:30,160
So you do have these like very interesting vulnerabilities

302
00:12:30,160 --> 00:12:33,120
or kind of lack of adversarial robustness.

303
00:12:33,120 --> 00:12:34,640
Still kind of wondering like,

304
00:12:34,640 --> 00:12:36,240
where do you think that leaves us

305
00:12:36,240 --> 00:12:38,240
in say a three to five years time?

306
00:12:38,240 --> 00:12:40,160
Obviously, huge uncertainty on that.

307
00:12:40,160 --> 00:12:42,240
It's really hard to predict something like this.

308
00:12:42,800 --> 00:12:46,480
Just to your point, generative AI is generic AI, right?

309
00:12:46,480 --> 00:12:48,320
It's like generically smart,

310
00:12:48,320 --> 00:12:49,920
but doesn't have specific intelligence

311
00:12:49,920 --> 00:12:51,520
or creativity or facts.

312
00:12:51,520 --> 00:12:53,520
And as you're saying, just like we have,

313
00:12:53,520 --> 00:12:57,760
you know, adversarial images back in full programs

314
00:12:57,760 --> 00:13:00,480
that are trained on a certain set of data

315
00:13:00,480 --> 00:13:02,880
and they just give some weird, you know, pattern

316
00:13:02,880 --> 00:13:04,080
that looks like a giraffe,

317
00:13:04,080 --> 00:13:06,320
but the algorithm thinks it's a dog.

318
00:13:06,320 --> 00:13:07,680
You can do the same thing for game playing

319
00:13:07,680 --> 00:13:09,360
and you can have out of sample input

320
00:13:09,360 --> 00:13:11,440
that can beat, you know,

321
00:13:11,440 --> 00:13:14,560
these very sophisticated reinforcement learners.

322
00:13:16,000 --> 00:13:17,520
And an interesting question is whether

323
00:13:17,520 --> 00:13:19,440
that is a fundamental thing

324
00:13:19,440 --> 00:13:23,520
or whether it is a work aroundable thing.

325
00:13:24,160 --> 00:13:26,640
And you'd think it was work aroundable, you know,

326
00:13:27,520 --> 00:13:29,920
because there's probably some robustification

327
00:13:29,920 --> 00:13:32,560
because these pictures look like giraffes, you know,

328
00:13:33,200 --> 00:13:35,760
and yet they're being recognized as dogs.

329
00:13:35,760 --> 00:13:39,600
So you would think that the right proximity metric

330
00:13:39,600 --> 00:13:42,480
would group it with giraffes, you know,

331
00:13:43,040 --> 00:13:45,200
but maybe there's some, I don't know,

332
00:13:45,200 --> 00:13:46,800
maybe there's some result there.

333
00:13:46,800 --> 00:13:51,120
My intuition would be we can probably robustify these systems

334
00:13:51,120 --> 00:13:54,320
so that they are less vulnerable to adversarial input.

335
00:13:55,920 --> 00:13:57,360
But if we can't, then that leads us

336
00:13:57,360 --> 00:13:58,880
in a totally different direction,

337
00:13:58,880 --> 00:14:01,840
where these systems are fragile in a fundamental way.

338
00:14:02,800 --> 00:14:07,040
So that's one big branch point is how fragile these systems are

339
00:14:07,040 --> 00:14:09,200
because if they're fragile in a certain way,

340
00:14:09,200 --> 00:14:12,080
then it's almost like you can always kill them,

341
00:14:12,080 --> 00:14:13,920
which is kind of good, right?

342
00:14:13,920 --> 00:14:16,320
In a sense, that there's that, you know,

343
00:14:16,320 --> 00:14:20,480
almost like the, you know, the 50 IQ, 100 IQ, 150 IQ thing.

344
00:14:20,480 --> 00:14:22,080
Like the meme?

345
00:14:22,080 --> 00:14:23,280
Yeah, the meme, right?

346
00:14:23,280 --> 00:14:25,600
So the 50 IQ guys like these machines

347
00:14:25,600 --> 00:14:28,080
will never be as creative as humans or whatever.

348
00:14:28,080 --> 00:14:30,160
100 IQ is look at all the things they can do.

349
00:14:30,240 --> 00:14:34,800
The 150 IQ is like, well, there's some like equivalent result,

350
00:14:34,800 --> 00:14:37,280
you know, that's like some impossibility proof

351
00:14:37,280 --> 00:14:39,920
that shows that we, the dimensional space of a giraffe

352
00:14:39,920 --> 00:14:43,200
is too high and we can't actually learn what a true giraffe.

353
00:14:43,200 --> 00:14:46,080
I don't think that's true, but maybe it's true

354
00:14:46,080 --> 00:14:48,960
from the perspective of how these learners are working

355
00:14:48,960 --> 00:14:51,200
because my understanding is people have been trying,

356
00:14:51,200 --> 00:14:52,800
and I mean, I'm not on the cutting edge of this.

357
00:14:52,800 --> 00:14:54,080
So, you know, maybe someone,

358
00:14:54,080 --> 00:14:56,720
but my understanding is we haven't yet been able to

359
00:14:56,720 --> 00:15:00,880
robustify these models against adversarial input.

360
00:15:00,880 --> 00:15:02,320
Am I wrong about that?

361
00:15:02,320 --> 00:15:03,600
Yeah, that's definitely right.

362
00:15:03,600 --> 00:15:05,600
Hey, we'll continue our interview in a moment

363
00:15:05,600 --> 00:15:06,960
after a word from our sponsors.

364
00:15:26,720 --> 00:15:50,480
I've used it in the past at the companies I've founded,

365
00:15:50,480 --> 00:15:52,560
and when we launch Merch here at Turpentine,

366
00:15:52,560 --> 00:15:54,000
Shopify will be our go-to.

367
00:15:54,640 --> 00:15:56,640
Shopify helps turn browsers into buyers

368
00:15:56,640 --> 00:15:58,880
with the internet's best converting checkout

369
00:15:58,880 --> 00:16:02,400
up to 36% better compared to other leading commerce platforms.

370
00:16:02,400 --> 00:16:05,040
And Shopify helps you sell more with less effort

371
00:16:05,040 --> 00:16:08,000
thanks to Shopify Magic, your AI-powered all-star.

372
00:16:08,000 --> 00:16:10,720
With Shopify Magic, whip up captivating content

373
00:16:10,720 --> 00:16:13,760
that converts from blog posts to product descriptions.

374
00:16:13,760 --> 00:16:15,840
Generate instant FAQ answers.

375
00:16:15,840 --> 00:16:17,920
Pick the perfect email send time.

376
00:16:17,920 --> 00:16:21,120
Plus, Shopify Magic is free for every Shopify seller.

377
00:16:21,840 --> 00:16:23,920
Businesses that grow, grow with Shopify.

378
00:16:24,560 --> 00:16:26,720
Sign up for a $1 per month trial period

379
00:16:26,720 --> 00:16:29,280
at Shopify.com slash cognitive.

380
00:16:29,280 --> 00:16:31,600
Go to Shopify.com slash cognitive now

381
00:16:31,600 --> 00:16:34,080
to grow your business no matter what stage you're in.

382
00:16:34,080 --> 00:16:36,160
Shopify.com slash cognitive.

383
00:16:51,280 --> 00:17:01,440
There's no single architecture as far as I know

384
00:17:01,440 --> 00:17:04,320
that is demonstrably robust.

385
00:17:04,320 --> 00:17:06,400
And on the contrary, even with language models,

386
00:17:06,400 --> 00:17:07,120
there's a...

387
00:17:07,120 --> 00:17:09,440
We did a whole episode on the universal jailbreak

388
00:17:10,000 --> 00:17:13,280
where especially if you have access to the weights,

389
00:17:13,280 --> 00:17:14,240
not to change the weights,

390
00:17:14,240 --> 00:17:17,040
but just to probe around in the weights,

391
00:17:17,040 --> 00:17:19,120
then you have a really hard time

392
00:17:19,600 --> 00:17:21,360
guaranteeing any sort of robustness.

393
00:17:21,920 --> 00:17:24,800
The conjecture is, see, for humans,

394
00:17:24,800 --> 00:17:28,240
you can't mirror their brain and analyze it.

395
00:17:28,240 --> 00:17:28,880
Okay?

396
00:17:28,880 --> 00:17:30,960
But we have enough humans that we've got things

397
00:17:30,960 --> 00:17:33,680
like optical illusions, stuff like that,

398
00:17:33,680 --> 00:17:35,840
that works on enough humans

399
00:17:36,720 --> 00:17:38,400
and our brains aren't changing enough, right?

400
00:17:38,960 --> 00:17:42,160
A conjecture is, as you said, open weights.

401
00:17:42,880 --> 00:17:46,560
Open weights mean safety because if you have open weights,

402
00:17:46,560 --> 00:17:48,960
you can always reverse engineer adversarial input

403
00:17:49,360 --> 00:17:51,120
and then you can always break the system.

404
00:17:51,680 --> 00:17:52,240
Conjecture.

405
00:17:52,960 --> 00:17:56,000
Yeah, that's again with Adam from Far AI.

406
00:17:56,000 --> 00:17:57,760
I'm really interested to get into that

407
00:17:57,760 --> 00:17:58,960
because they are starting to study,

408
00:17:58,960 --> 00:17:59,840
as I understand it,

409
00:18:00,800 --> 00:18:05,040
kind of proto-scaling laws for adversarial robustness.

410
00:18:05,600 --> 00:18:07,600
And I think a huge question there is,

411
00:18:08,480 --> 00:18:12,160
what are the kind of frontiers of possibility there?

412
00:18:14,320 --> 00:18:15,680
How do the orders of magnitude work?

413
00:18:15,760 --> 00:18:19,440
Do you need another 10x as much adversarial training

414
00:18:19,440 --> 00:18:23,360
to half the rate of your adversarial failures?

415
00:18:23,360 --> 00:18:25,520
And if so, can we generate that many?

416
00:18:25,520 --> 00:18:28,080
It may always sort of be fleeting.

417
00:18:28,640 --> 00:18:33,200
So, Far AI and they are working on cutting edge

418
00:18:33,200 --> 00:18:34,640
of adversarial input.

419
00:18:34,640 --> 00:18:36,640
Yeah, they're the group that did the attack

420
00:18:36,640 --> 00:18:40,400
on the Alpha Go model and found that, like, you know,

421
00:18:40,400 --> 00:18:42,080
and what was really interesting about that,

422
00:18:42,080 --> 00:18:43,280
I mean, multiple things, right?

423
00:18:43,280 --> 00:18:45,520
First, that they could beat a super human Go player at all.

424
00:18:45,680 --> 00:18:47,680
But second, that the technique that they used

425
00:18:47,680 --> 00:18:50,720
would not work at all if playing a quality human.

426
00:18:50,720 --> 00:18:53,680
Or is, you know, it's a strategy that is trivial to beat

427
00:18:53,680 --> 00:18:55,840
if you're a quality human Go player,

428
00:18:55,840 --> 00:18:58,880
but the Alpha Go is just totally blind to it.

429
00:18:58,880 --> 00:19:00,480
You know, that's why I say the conjecture is,

430
00:19:01,200 --> 00:19:02,240
if you have the model,

431
00:19:03,520 --> 00:19:05,200
then you can generate the adversarial input.

432
00:19:05,760 --> 00:19:08,080
And then, so if that is true,

433
00:19:08,080 --> 00:19:11,200
and that itself is an important conjecture about AI safety,

434
00:19:11,200 --> 00:19:16,160
right? Because if open weights are inherently something

435
00:19:16,160 --> 00:19:18,080
where you can generate adversarial input from that

436
00:19:18,080 --> 00:19:20,800
and break or crash or defeat the AI,

437
00:19:22,160 --> 00:19:25,520
then that AI is not omnipotent, right?

438
00:19:25,520 --> 00:19:27,920
You have some power words you can speak to

439
00:19:27,920 --> 00:19:30,320
at almost like magical words that'll just make it

440
00:19:31,520 --> 00:19:34,560
power down, so to speak, right?

441
00:19:34,560 --> 00:19:37,200
It's like those movies where the monsters can't see you

442
00:19:37,200 --> 00:19:40,400
if you stand really still or if you don't make a noise

443
00:19:40,400 --> 00:19:41,680
or something like that, right?

444
00:19:41,680 --> 00:19:43,520
They're very powerful on Dimension X,

445
00:19:43,520 --> 00:19:44,880
but they're very weak on Dimension Y.

446
00:19:45,440 --> 00:19:47,440
A kind of an obvious point, but, you know,

447
00:19:47,440 --> 00:19:49,680
I'm not sure how important it's going to be in the future.

448
00:19:51,120 --> 00:19:53,040
Your next question was on like, you know,

449
00:19:53,040 --> 00:19:55,200
humanoid robots and so on, and before we get to that,

450
00:19:55,760 --> 00:19:59,600
maybe obviously, but all of these models are trained

451
00:19:59,600 --> 00:20:01,760
on things that we can easily record,

452
00:20:01,760 --> 00:20:04,960
which are sights and sounds, right?

453
00:20:05,600 --> 00:20:08,000
But touch and taste and smell,

454
00:20:08,960 --> 00:20:12,320
we don't have amazing data sets on those.

455
00:20:12,320 --> 00:20:14,480
Well, I mean, there's some haptic stuff, right?

456
00:20:15,760 --> 00:20:17,840
There's probably some, you know,

457
00:20:17,840 --> 00:20:19,440
some work on taste and smell and so on,

458
00:20:19,440 --> 00:20:21,760
but there's five senses, right?

459
00:20:21,760 --> 00:20:26,640
I wonder if there's something like that where you might be like,

460
00:20:26,640 --> 00:20:29,440
okay, how are you going to outsmell a robot

461
00:20:29,440 --> 00:20:30,480
or something like that?

462
00:20:30,480 --> 00:20:32,800
Well, dogs actually have a very powerful sense of smell,

463
00:20:32,800 --> 00:20:34,640
and that's being very important for them, you know?

464
00:20:35,280 --> 00:20:36,960
And it may turn out that there's,

465
00:20:37,040 --> 00:20:38,800
maybe it's just that we just haven't collected the data,

466
00:20:38,800 --> 00:20:41,280
and it could become a much better smeller or whatever,

467
00:20:41,280 --> 00:20:43,040
or, you know, taster than anything else.

468
00:20:43,040 --> 00:20:43,840
I wouldn't be surprised.

469
00:20:43,840 --> 00:20:45,600
It could be a much better wine taster,

470
00:20:45,600 --> 00:20:47,600
because you can do molecular diagnostics.

471
00:20:47,600 --> 00:20:51,440
But it's just kind of, I just use that as an analogy to say,

472
00:20:51,440 --> 00:20:52,880
there's areas of the human experience

473
00:20:52,880 --> 00:20:54,240
that we haven't yet quantified,

474
00:20:55,040 --> 00:20:57,840
and maybe it's just the opera term is yet, okay?

475
00:20:57,840 --> 00:20:58,960
But there's areas of the human experience

476
00:20:58,960 --> 00:21:00,480
we haven't yet quantified,

477
00:21:00,480 --> 00:21:02,880
which are also an area that AIs at least

478
00:21:02,880 --> 00:21:04,320
are not yet capable of.

479
00:21:04,320 --> 00:21:07,760
Yeah, I guess maybe my expectation boils down to,

480
00:21:09,360 --> 00:21:12,080
I think the really powerful systems

481
00:21:12,080 --> 00:21:16,560
are probably likely to mix architectures

482
00:21:16,560 --> 00:21:18,080
in some sort of ensemble.

483
00:21:18,080 --> 00:21:20,400
You know, when you think about just the structure of the brain,

484
00:21:20,400 --> 00:21:22,880
it's not, I mean, there certainly are aspects of it

485
00:21:22,880 --> 00:21:23,840
that are repeated, right?

486
00:21:23,840 --> 00:21:25,760
You look at the frontal cortex,

487
00:21:25,760 --> 00:21:28,480
and it's like there is kind of this, you know,

488
00:21:28,480 --> 00:21:30,560
unit that gets repeated over and over again,

489
00:21:30,560 --> 00:21:32,560
in a sense that's kind of analogous to say,

490
00:21:32,560 --> 00:21:34,160
the transformer block that just gets,

491
00:21:34,160 --> 00:21:35,920
you know, stacked layer on layer.

492
00:21:35,920 --> 00:21:37,920
But it is striking in a transformer

493
00:21:37,920 --> 00:21:40,480
that it's basically the same exact mechanism

494
00:21:40,480 --> 00:21:42,400
at every layer that's doing kind of

495
00:21:42,400 --> 00:21:44,320
all the different kinds of processing.

496
00:21:44,880 --> 00:21:47,680
And so whatever weaknesses that structure has,

497
00:21:47,680 --> 00:21:48,800
and you know, with the transformer

498
00:21:48,800 --> 00:21:49,840
and the attention mechanism,

499
00:21:49,840 --> 00:21:51,440
there's like some pretty profound ones,

500
00:21:51,440 --> 00:21:55,760
like finite context window, you know, you kind of need,

501
00:21:55,760 --> 00:21:58,080
I would think, a different sort of architecture

502
00:21:58,080 --> 00:21:59,680
with a little bit of a different strength

503
00:21:59,760 --> 00:22:04,880
and weakness profile to complement that in such a way that,

504
00:22:05,440 --> 00:22:07,680
you know, kind of more similar to like a biological system

505
00:22:07,680 --> 00:22:10,400
where you kind of have this like dynamic feedback

506
00:22:10,400 --> 00:22:12,400
where, you know, we have obviously, you know,

507
00:22:12,400 --> 00:22:14,560
thinking fast and slow and all sorts of different modules

508
00:22:14,560 --> 00:22:17,040
in the brain and they kind of cross regulate each other

509
00:22:17,600 --> 00:22:21,440
and don't let any one system, you know,

510
00:22:22,080 --> 00:22:25,200
go totally, you know, down the wrong path on its own, right?

511
00:22:25,200 --> 00:22:26,480
Without something kind of coming back

512
00:22:26,480 --> 00:22:27,760
and trying to override that.

513
00:22:28,240 --> 00:22:29,680
It seems to me like that's a big part

514
00:22:29,680 --> 00:22:33,680
of what is missing from the current crop of AIs

515
00:22:33,680 --> 00:22:35,200
in terms of their robustness.

516
00:22:35,840 --> 00:22:38,240
And I don't know how long that takes to show up,

517
00:22:38,800 --> 00:22:43,440
but we are starting to see some, you know, possibly,

518
00:22:43,440 --> 00:22:44,560
you know, I think people are maybe thinking

519
00:22:44,560 --> 00:22:45,680
about this a little bit the wrong way.

520
00:22:45,680 --> 00:22:47,360
They're just in the last couple of weeks,

521
00:22:47,360 --> 00:22:48,560
there's been a number of papers

522
00:22:49,280 --> 00:22:52,640
that are really looking at the state space model

523
00:22:53,200 --> 00:22:54,080
kind of alternative.

524
00:22:54,080 --> 00:22:56,800
It's being framed as an alternative to the transformer.

525
00:22:56,800 --> 00:22:58,560
But when I see that I'm much more like,

526
00:22:59,280 --> 00:23:01,920
it's probably a compliment to the transformer

527
00:23:01,920 --> 00:23:04,400
or, you know, these two things probably get integrated

528
00:23:04,400 --> 00:23:06,480
in some form because to the degree

529
00:23:06,480 --> 00:23:08,560
that they do have very different strengths and weaknesses,

530
00:23:08,560 --> 00:23:10,480
ultimately you're going to want the best of both

531
00:23:10,480 --> 00:23:11,840
in a robust system.

532
00:23:11,840 --> 00:23:13,040
Certainly if you're trying to make an agent,

533
00:23:13,040 --> 00:23:14,480
certainly if you're trying to make, you know,

534
00:23:14,480 --> 00:23:16,560
a humanoid robot that can go around your house

535
00:23:16,560 --> 00:23:18,080
and like do useful work,

536
00:23:18,080 --> 00:23:20,800
but also be robust enough that it doesn't,

537
00:23:20,800 --> 00:23:22,640
you get tricked into attacking your kid

538
00:23:22,640 --> 00:23:24,000
or your dog or, you know, whatever,

539
00:23:24,480 --> 00:23:26,720
you're going to want to have more checks and balances

540
00:23:26,720 --> 00:23:29,360
than just kind of a single stack of, you know,

541
00:23:29,360 --> 00:23:31,680
the same block over and over again.

542
00:23:31,680 --> 00:23:35,120
Well, so I know Boston Dynamics with their legged robots

543
00:23:35,120 --> 00:23:38,480
is all control theory and it's not classical ML.

544
00:23:38,480 --> 00:23:41,360
It's really interesting to see how they've accomplished it.

545
00:23:41,360 --> 00:23:44,160
And they do have essentially a state space model

546
00:23:44,160 --> 00:23:46,880
where they have a big position vector

547
00:23:46,880 --> 00:23:48,800
that's got all the coordinates of all the joints

548
00:23:48,800 --> 00:23:51,040
and then a bunch of matrix algebra to figure out

549
00:23:51,040 --> 00:23:53,760
how this thing is moving and all the feedback control

550
00:23:53,760 --> 00:23:54,880
and so on there.

551
00:23:54,880 --> 00:23:55,920
And it's more complicated than that,

552
00:23:55,920 --> 00:23:58,240
but that's, you know, I think the V1 of it.

553
00:23:58,240 --> 00:23:59,360
Sorry, it was there.

554
00:23:59,360 --> 00:24:00,880
I wasn't following this though.

555
00:24:00,880 --> 00:24:02,480
Are you saying that there's papers

556
00:24:02,480 --> 00:24:05,360
that are integrating that with the kind of

557
00:24:05,360 --> 00:24:07,280
generator transformer model?

558
00:24:07,280 --> 00:24:09,360
You know, like what's a good citation for me to look at?

559
00:24:09,360 --> 00:24:12,320
Yeah, starting to, we did an episode, for example,

560
00:24:12,320 --> 00:24:15,760
with one of the technology leads at Skydio,

561
00:24:15,760 --> 00:24:19,040
the, you know, the U.S. is champion drone maker.

562
00:24:19,840 --> 00:24:22,400
And they have kind of a similar thing

563
00:24:22,400 --> 00:24:26,240
where they have built over, you know, a decade, right?

564
00:24:26,240 --> 00:24:31,920
A fully explicit multiple orders of, you know,

565
00:24:31,920 --> 00:24:34,480
spanning multiple orders of magnitude control stack.

566
00:24:35,280 --> 00:24:38,160
And now over the top of that,

567
00:24:38,160 --> 00:24:40,480
they're starting to layer this kind of, you know,

568
00:24:40,480 --> 00:24:42,400
it's not exactly generative AI in their case

569
00:24:42,400 --> 00:24:44,720
because they're not like generating content,

570
00:24:44,720 --> 00:24:47,280
but it's kind of the high level, you know,

571
00:24:47,280 --> 00:24:50,080
can I give the thing verbal instructions?

572
00:24:50,080 --> 00:24:52,080
Have it go out and kind of understand,

573
00:24:52,080 --> 00:24:54,720
okay, like this is a bridge, I'm supposed to kind of,

574
00:24:54,720 --> 00:24:58,160
you know, survey the bridge and translate

575
00:24:58,160 --> 00:25:01,120
those high level instructions to a plan

576
00:25:01,120 --> 00:25:04,480
and then use the lower level explicit code

577
00:25:04,480 --> 00:25:07,040
that is fully deterministic and, you know,

578
00:25:07,040 --> 00:25:09,360
runs on control theory and all that kind of stuff

579
00:25:09,360 --> 00:25:12,160
to actually execute the plan at a low level.

580
00:25:12,160 --> 00:25:14,560
But also, you know, at times like surface errors

581
00:25:14,560 --> 00:25:16,320
up to the top and say like, hey, we've got a problem,

582
00:25:16,320 --> 00:25:17,840
you know, whatever, I'm not able to do it.

583
00:25:17,840 --> 00:25:20,960
You know, can you now, at the higher level,

584
00:25:21,040 --> 00:25:23,120
the semantic layer adjust the plan?

585
00:25:23,920 --> 00:25:26,640
That stuff is starting to happen in multiple domains,

586
00:25:26,640 --> 00:25:27,360
I would say.

587
00:25:27,360 --> 00:25:28,880
Yeah, and so I think that makes sense,

588
00:25:28,880 --> 00:25:32,080
is basically it's like generative AI is almost the front end,

589
00:25:32,080 --> 00:25:34,080
and then you have almost like an assembly,

590
00:25:34,080 --> 00:25:36,480
like you give instructions to Figma

591
00:25:36,480 --> 00:25:39,680
and the objects there are their shapes

592
00:25:39,680 --> 00:25:42,160
and their images and so on, it's not text.

593
00:25:42,160 --> 00:25:43,520
You give instructions to a drone

594
00:25:43,520 --> 00:25:48,000
and the objects are like GPS coordinates and paths and so on.

595
00:25:48,640 --> 00:25:52,480
And so you are generating structures

596
00:25:52,480 --> 00:25:53,840
that are in a different domain,

597
00:25:53,840 --> 00:25:56,400
or it's like in VR, you're generating 3D structures,

598
00:25:56,400 --> 00:26:00,000
again, as opposed to text, and then that compute engine

599
00:26:00,000 --> 00:26:01,200
takes those three structures

600
00:26:01,200 --> 00:26:04,000
and does something with them in a much more rules-based way.

601
00:26:04,000 --> 00:26:06,960
So you have like a statistical user-friendly front end

602
00:26:06,960 --> 00:26:10,720
with a generative AI, and then you have a more deterministic

603
00:26:10,720 --> 00:26:12,160
or usually totally deterministic,

604
00:26:13,280 --> 00:26:14,800
almost like assembly language back end

605
00:26:14,800 --> 00:26:16,160
that actually takes that and does something.

606
00:26:16,160 --> 00:26:17,120
That's what you're saying, right?

607
00:26:17,120 --> 00:26:19,120
Yeah, pretty much, and I would say there's another analogy

608
00:26:19,120 --> 00:26:21,360
to just, again, our biological experience

609
00:26:21,360 --> 00:26:26,080
where it's like I'm sort of in a semi-conscious level, right?

610
00:26:26,080 --> 00:26:28,160
I kind of think about what I want to do,

611
00:26:28,160 --> 00:26:30,640
but the low-level movements of the hand

612
00:26:30,640 --> 00:26:32,320
are both like not conscious,

613
00:26:32,320 --> 00:26:38,320
and also if I do encounter some pain or hit some hot item

614
00:26:38,320 --> 00:26:40,480
or whatever, there's a quick reaction

615
00:26:40,480 --> 00:26:44,400
that's sort of mediated by a lower level control system,

616
00:26:44,400 --> 00:26:45,920
and then that fires back up to the brain

617
00:26:45,920 --> 00:26:48,240
and is like, hey, we need a new plan here.

618
00:26:48,240 --> 00:26:53,440
So that is only starting to come into focus, I think,

619
00:26:53,440 --> 00:26:56,240
with, because obviously these, I mean, it's amazing,

620
00:26:56,240 --> 00:26:57,920
as you said, it's all moving so fast.

621
00:26:58,800 --> 00:27:01,200
What is always striking to me,

622
00:27:01,200 --> 00:27:03,760
and I kind of like recite timelines to myself

623
00:27:03,760 --> 00:27:05,120
almost as like a mantra, right?

624
00:27:05,120 --> 00:27:07,760
Like the first instruction following AI

625
00:27:07,760 --> 00:27:10,640
that hit the public was just January 2022.

626
00:27:10,640 --> 00:27:12,800
That was OpenAI's Text of Ingee 002.

627
00:27:12,800 --> 00:27:14,560
It was the first one where you could say like do X,

628
00:27:14,560 --> 00:27:17,200
and it would do X as opposed to having

629
00:27:17,200 --> 00:27:19,360
an elaborate prompt engineering type of setup.

630
00:27:20,160 --> 00:27:23,280
GPT-4 just a little over a year ago finished training,

631
00:27:23,280 --> 00:27:24,960
not even a year that it's been in the public,

632
00:27:25,680 --> 00:27:29,280
and it has been amazing to see how quickly

633
00:27:29,920 --> 00:27:32,640
this kind of technology is being integrated into those systems,

634
00:27:32,640 --> 00:27:34,800
but it's definitely still very much a work in progress.

635
00:27:35,360 --> 00:27:40,080
Yeah, I mean, the tricky part is the training data and so on.

636
00:27:40,320 --> 00:27:46,320
Like a large existing scale company like a Figma or DJI

637
00:27:46,320 --> 00:27:49,760
that has millions or billions of user sessions

638
00:27:49,760 --> 00:27:51,680
will have a much easier time training,

639
00:27:52,400 --> 00:27:55,040
and they have a unique data set,

640
00:27:55,040 --> 00:27:58,400
and then everybody else will not be able to do that.

641
00:27:58,400 --> 00:27:59,600
So there is actually almost like,

642
00:28:00,400 --> 00:28:03,360
I mean, a return on scale where the massive data set,

643
00:28:03,360 --> 00:28:04,800
if you've got a massive clean data set

644
00:28:04,800 --> 00:28:07,520
and a unique domain that lots of people are using,

645
00:28:07,520 --> 00:28:09,040
then you can crush it.

646
00:28:09,760 --> 00:28:12,080
And if you don't, I suppose,

647
00:28:12,080 --> 00:28:14,080
I mean, there's lots of people who work on zero shot stuff

648
00:28:14,080 --> 00:28:15,920
and sort of sort of, but it still strikes me

649
00:28:15,920 --> 00:28:19,040
that there'll probably be an advantage to see those sessions.

650
00:28:20,400 --> 00:28:23,120
I find it hard to believe that you could generate

651
00:28:23,120 --> 00:28:27,280
a really good drone command language

652
00:28:27,280 --> 00:28:30,000
without lots of drone flight paths, but you can see.

653
00:28:30,000 --> 00:28:31,440
And where it doesn't exist, people are,

654
00:28:32,080 --> 00:28:33,520
obviously you need deep pockets for this,

655
00:28:33,520 --> 00:28:36,400
but the likes of Google are starting to just

656
00:28:37,360 --> 00:28:39,120
grind out the generation of that, right?

657
00:28:39,120 --> 00:28:41,760
They've got their kind of test kitchen,

658
00:28:41,760 --> 00:28:44,640
which is a literal physical kitchen at Google

659
00:28:44,640 --> 00:28:47,040
where the robots go around and do tasks.

660
00:28:47,040 --> 00:28:48,880
And when they get stuck, my understanding

661
00:28:48,880 --> 00:28:52,080
of their kind of critical path, as I understand,

662
00:28:52,080 --> 00:28:55,920
they understand it, is robots gonna get stuck,

663
00:28:56,560 --> 00:29:01,120
will have a human operator remotely operate the robot

664
00:29:01,680 --> 00:29:05,280
to show what to do, and then that data becomes

665
00:29:05,280 --> 00:29:07,520
the bridge from what the robot can't do

666
00:29:07,520 --> 00:29:09,520
to what it's supposed to learn to do next time.

667
00:29:10,160 --> 00:29:12,800
And they're gonna need a lot of that, for sure.

668
00:29:13,440 --> 00:29:15,360
But they increasingly have,

669
00:29:15,360 --> 00:29:16,960
I don't know exactly how many robots they have now,

670
00:29:16,960 --> 00:29:18,400
but last I talked to someone there,

671
00:29:18,400 --> 00:29:20,320
it was like into the dozens,

672
00:29:20,960 --> 00:29:24,560
and presumably they're continuing to scale that.

673
00:29:24,560 --> 00:29:29,360
I think they just view that they can probably brute force it

674
00:29:29,360 --> 00:29:32,640
to the point where it's good enough to put out into the world,

675
00:29:32,640 --> 00:29:34,560
and then very much like a Waymo or a cruise

676
00:29:34,560 --> 00:29:36,800
or whatever, they probably still have remote operators,

677
00:29:36,800 --> 00:29:38,960
even when the robot is in your home,

678
00:29:40,080 --> 00:29:40,960
when it encounters something

679
00:29:40,960 --> 00:29:43,760
that it doesn't know what to do about, raise that alarm,

680
00:29:43,760 --> 00:29:46,000
get the human supervision to help it over the hump,

681
00:29:46,000 --> 00:29:49,600
and then obviously that's where you really get the scale

682
00:29:49,600 --> 00:29:50,880
that you're talking about.

683
00:29:50,880 --> 00:29:53,200
And this raises a couple of questions I wanted to ask

684
00:29:53,200 --> 00:29:54,320
that are conceptual.

685
00:29:54,320 --> 00:29:57,840
So obviously there's huge questions around like,

686
00:29:59,120 --> 00:30:01,920
again, highest level, how is all this gonna play out?

687
00:30:01,920 --> 00:30:06,560
One big debate is to what degree does AI favor the incumbents?

688
00:30:06,560 --> 00:30:09,200
To what degree does it enable startups?

689
00:30:09,200 --> 00:30:10,160
Obviously it's both,

690
00:30:10,160 --> 00:30:12,640
but I'm interested in your perspective on that.

691
00:30:12,640 --> 00:30:13,920
Also really interested in your perspective

692
00:30:13,920 --> 00:30:15,840
on like offense versus defense.

693
00:30:15,840 --> 00:30:17,200
That's something that a lot of people

694
00:30:18,000 --> 00:30:19,360
now and in the future,

695
00:30:19,360 --> 00:30:21,600
that seems like it probably really matters a lot,

696
00:30:21,600 --> 00:30:23,440
whether it's a more offense enabling

697
00:30:23,440 --> 00:30:24,720
or defense enabling technology.

698
00:30:24,720 --> 00:30:28,000
So I love your take on those two dimensions.

699
00:30:28,000 --> 00:30:29,920
Hey, we'll continue our interview in a moment

700
00:30:29,920 --> 00:30:31,280
after a word from our sponsors.

701
00:30:32,080 --> 00:30:34,480
Omniki uses generative AI

702
00:30:34,480 --> 00:30:36,400
to enable you to launch hundreds of thousands

703
00:30:36,400 --> 00:30:38,800
of ad iterations that actually work,

704
00:30:38,800 --> 00:30:42,160
customized across all platforms with a click of a button.

705
00:30:42,160 --> 00:30:44,720
I believe in Omniki so much that I invested in it

706
00:30:44,720 --> 00:30:46,320
and I recommend you use it too.

707
00:30:47,040 --> 00:30:49,680
Use Cogrev to get a 10% discount.

708
00:30:49,680 --> 00:30:51,280
If you're a startup founder or executive

709
00:30:51,280 --> 00:30:52,400
running a growing business,

710
00:30:52,400 --> 00:30:53,680
you know that as you scale,

711
00:30:53,680 --> 00:30:56,320
your systems break down and the cracks start to show.

712
00:30:56,880 --> 00:30:58,000
If this resonates with you,

713
00:30:58,000 --> 00:30:59,600
there are three numbers you need to know,

714
00:30:59,680 --> 00:31:03,840
36,000, 25, and one, 36,000.

715
00:31:03,840 --> 00:31:04,880
That's the number of businesses

716
00:31:04,880 --> 00:31:06,800
which have upgraded to NetSuite by Oracle.

717
00:31:06,800 --> 00:31:08,880
NetSuite is the number one cloud financial system,

718
00:31:08,880 --> 00:31:10,880
streamline accounting, financial management,

719
00:31:10,880 --> 00:31:13,920
inventory, HR, and more, 25.

720
00:31:13,920 --> 00:31:15,680
NetSuite turns 25 this year.

721
00:31:15,680 --> 00:31:18,480
That's 25 years of helping businesses do more with less,

722
00:31:18,480 --> 00:31:21,440
close their books in days, not weeks, and drive down costs.

723
00:31:22,160 --> 00:31:24,240
One, because your business is one of a kind,

724
00:31:24,240 --> 00:31:26,800
so you get a customized solution for all your KPIs

725
00:31:26,800 --> 00:31:29,280
in one efficient system with one source of truth.

726
00:31:29,360 --> 00:31:32,640
Manage risk, get reliable forecasts, and improve margins.

727
00:31:32,640 --> 00:31:34,240
Everything you need, all in one place.

728
00:31:34,800 --> 00:31:38,160
Right now, download NetSuite's popular KPI checklist,

729
00:31:38,160 --> 00:31:40,560
designed to give you consistently excellent performance,

730
00:31:40,560 --> 00:31:43,760
absolutely free, and netsuite.com slash cognitive.

731
00:31:43,760 --> 00:31:45,920
That's netsuite.com slash cognitive

732
00:31:45,920 --> 00:31:47,760
to get your own KPI checklist.

733
00:31:47,760 --> 00:31:49,600
NetSuite.com slash cognitive.

734
00:31:50,400 --> 00:31:51,920
So like offense or defense

735
00:31:51,920 --> 00:31:54,400
in the sense of disenabled disruptors or incumbents?

736
00:31:54,960 --> 00:31:56,480
Both in business and in like,

737
00:31:56,480 --> 00:31:58,240
you know, potentially outright conflict.

738
00:31:58,240 --> 00:32:00,880
I'd be interested to hear your analysis on both.

739
00:32:00,880 --> 00:32:02,080
All right, a lot of views on this.

740
00:32:02,080 --> 00:32:07,120
So obviously, if you've got a competent, existing tech CEO,

741
00:32:07,120 --> 00:32:10,240
you know, like who's still in their prime,

742
00:32:10,240 --> 00:32:14,880
like Amjad of Replit, or, you know, Dillon Field of Figma,

743
00:32:16,640 --> 00:32:19,520
or, you know, those are two who have thought of,

744
00:32:19,520 --> 00:32:22,720
who are very good and, you know, will be on top of it.

745
00:32:22,720 --> 00:32:25,920
Amjad is very early on integrating AI into Replit,

746
00:32:25,920 --> 00:32:28,320
and it's basically built that into an AI first company,

747
00:32:28,320 --> 00:32:29,280
which is really impressive.

748
00:32:30,000 --> 00:32:34,000
Those are folks who cleanly made a pivot.

749
00:32:34,000 --> 00:32:37,520
It's as big or bigger than, comparable to, I would say,

750
00:32:37,520 --> 00:32:40,640
the pivot from desktop to mobile

751
00:32:40,640 --> 00:32:42,080
that broke a bunch of companies

752
00:32:42,080 --> 00:32:44,080
in the late 2000s and early 2010s.

753
00:32:44,080 --> 00:32:48,080
Like Facebook in 2012 had no mobile revenue, roughly,

754
00:32:48,080 --> 00:32:49,680
at the time of their IPO,

755
00:32:49,680 --> 00:32:51,440
and then they had to like redo the whole thing.

756
00:32:51,440 --> 00:32:54,400
And it's hard to turn a company 90 degrees

757
00:32:54,400 --> 00:32:56,480
when something new like that hits, you know?

758
00:32:57,040 --> 00:32:59,440
Those that are run by kind of tech CEOs in their prime

759
00:33:00,400 --> 00:33:03,760
will adapt and will AI-ify their existing services.

760
00:33:04,320 --> 00:33:05,680
And the question is, obviously,

761
00:33:05,680 --> 00:33:06,880
there's new things that are coming out,

762
00:33:06,880 --> 00:33:08,800
like pika and character.ai.

763
00:33:08,800 --> 00:33:11,440
There's some like really good stuff that's out there.

764
00:33:11,440 --> 00:33:13,760
The question is, you know,

765
00:33:14,320 --> 00:33:16,000
will the disruption be allowed to happen

766
00:33:16,000 --> 00:33:17,520
in the U.S. regulatory environment?

767
00:33:18,320 --> 00:33:21,680
And so my view is actually that, you know,

768
00:33:21,680 --> 00:33:24,320
so this is from like the network state book, right?

769
00:33:25,280 --> 00:33:27,040
You know, people talk about a multipolar world

770
00:33:27,040 --> 00:33:28,400
or unipolar world.

771
00:33:28,400 --> 00:33:30,880
The political axis is actually really important

772
00:33:30,880 --> 00:33:32,480
in my view for thinking about

773
00:33:32,480 --> 00:33:35,920
whether AI will be allowed to disrupt, okay?

774
00:33:35,920 --> 00:33:37,840
Because we'll get to this probably later,

775
00:33:37,840 --> 00:33:40,240
but the 640K of compute is enough

776
00:33:40,240 --> 00:33:42,000
for everyone executive order.

777
00:33:42,000 --> 00:33:44,720
You know, 640K of memory, the apocryphal,

778
00:33:44,720 --> 00:33:46,160
he didn't delegate to actually say it,

779
00:33:46,160 --> 00:33:48,800
but that quote kind of gives a certain mindset

780
00:33:48,800 --> 00:33:49,600
about computing.

781
00:33:49,600 --> 00:33:50,960
That should be enough for everybody.

782
00:33:50,960 --> 00:33:52,640
So the 10 to the 26 of compute

783
00:33:52,640 --> 00:33:53,920
should be enough for everyone bill.

784
00:33:55,040 --> 00:33:56,400
I actually think it's very bad

785
00:33:56,400 --> 00:33:57,600
and I think it's just the beginning

786
00:33:57,600 --> 00:34:01,040
of their attempts to build like a software FDA, okay?

787
00:34:01,040 --> 00:34:04,480
To decelerate, control, regulate, red tape,

788
00:34:04,480 --> 00:34:07,760
the entire space, just like how, you know,

789
00:34:07,760 --> 00:34:10,480
the threat of nuclear terrorism got turned into the TSA.

790
00:34:11,360 --> 00:34:14,160
The threat of, you know, terminators and AGI

791
00:34:14,160 --> 00:34:16,480
gets turned into a million rules

792
00:34:16,480 --> 00:34:18,080
on whether you can set up servers

793
00:34:18,080 --> 00:34:20,400
and this last free sector of the economy

794
00:34:20,400 --> 00:34:22,640
is strangled or at least controlled

795
00:34:23,200 --> 00:34:25,920
within the territory control by Washington DC.

796
00:34:26,880 --> 00:34:30,000
Now, why does this relate to the political?

797
00:34:30,000 --> 00:34:32,160
Well, obviously this, you know,

798
00:34:32,160 --> 00:34:33,840
you can just spend your entire life

799
00:34:33,840 --> 00:34:35,120
just tracking AI papers

800
00:34:35,120 --> 00:34:37,680
and that's moving like at the speed of light like this, right?

801
00:34:38,320 --> 00:34:39,440
What's also happening

802
00:34:39,440 --> 00:34:41,200
as you can kind of see in your peripheral vision

803
00:34:41,200 --> 00:34:43,840
is there's political developments

804
00:34:43,840 --> 00:34:45,040
that are happening at the speed of light

805
00:34:45,040 --> 00:34:47,040
much faster than they've happened in our lifespans.

806
00:34:47,040 --> 00:34:48,640
Like there's more, you just noticed,

807
00:34:48,640 --> 00:34:51,120
more wars, more serious online conflicts

808
00:34:51,120 --> 00:34:53,440
like, you know, there's a sovereign debt crisis.

809
00:34:53,440 --> 00:34:55,440
All of those things that can show graph after graph

810
00:34:55,440 --> 00:34:58,880
of things looking like their own types of singularities, you know,

811
00:34:59,440 --> 00:35:00,960
like military debts are way up, you know,

812
00:35:00,960 --> 00:35:03,360
the long piece that Steven Pinker showed

813
00:35:03,360 --> 00:35:04,320
that's looking like a you

814
00:35:04,320 --> 00:35:06,080
that suddenly way up after Ukraine

815
00:35:06,080 --> 00:35:07,920
and some of these other wars are happening, unfortunately, right?

816
00:35:08,800 --> 00:35:10,960
Interest payments, whoosh, way up to the side.

817
00:35:10,960 --> 00:35:11,920
What's my point?

818
00:35:11,920 --> 00:35:15,600
Point is, I think that the world is going to become

819
00:35:15,600 --> 00:35:17,760
from the Pax Americana world

820
00:35:17,760 --> 00:35:20,320
of just like basically one superpower,

821
00:35:20,320 --> 00:35:23,600
a hyperpower that we grew up in from 91 to 2021, roughly,

822
00:35:24,160 --> 00:35:28,160
that we're going to get a specifically tripolar world,

823
00:35:28,160 --> 00:35:31,680
not unipolar, not bipolar, not multipolar, but tripolar.

824
00:35:31,680 --> 00:35:36,560
And those three poles, I kind of think of as NYT, CCP, BTC,

825
00:35:36,560 --> 00:35:38,080
or you could think of them as,

826
00:35:38,080 --> 00:35:40,720
and those are just certain labels that are associated with them,

827
00:35:40,720 --> 00:35:44,480
but they're roughly U.S. tech, the U.S. environment,

828
00:35:45,200 --> 00:35:46,880
China tech and China environment,

829
00:35:46,880 --> 00:35:48,640
and global tech and the global environment.

830
00:35:49,280 --> 00:35:53,360
And why do I identify BTC and crypto and so on with global tech?

831
00:35:53,360 --> 00:35:55,520
Because that's a tech that decentralized out of the U.S.

832
00:35:56,240 --> 00:35:59,200
And right now people think of crypto as finance,

833
00:35:59,200 --> 00:36:00,560
but it's also financiers.

834
00:36:02,080 --> 00:36:03,680
Okay, and in this next run-up,

835
00:36:04,560 --> 00:36:08,400
it is, I think, quite likely about, depending on how you count,

836
00:36:08,400 --> 00:36:10,960
between a third to a half of the world's billionaires will be crypto.

837
00:36:12,320 --> 00:36:15,360
Okay, around, you know, I calculated this a while back around,

838
00:36:15,360 --> 00:36:16,960
Bitcoin at a few hundred thousands,

839
00:36:16,960 --> 00:36:19,600
around a third to a half the world's billionaires are crypto.

840
00:36:19,600 --> 00:36:21,520
That's the unlocked pool of capital.

841
00:36:22,080 --> 00:36:26,800
And those are the people who do not bow to D.C. or Beijing.

842
00:36:26,800 --> 00:36:29,520
And they might, by the way, be Indians or Israelis

843
00:36:29,520 --> 00:36:31,360
or every other demographic in the world,

844
00:36:31,360 --> 00:36:33,360
or they could be American libertarians,

845
00:36:33,360 --> 00:36:35,520
or they could be Chinese liberals like Jack Ma,

846
00:36:35,520 --> 00:36:37,280
who are pushed out of Beijing sphere.

847
00:36:37,280 --> 00:36:38,640
Okay, or the next Jack Ma.

848
00:36:38,640 --> 00:36:41,920
You know, Jack Ma himself may not be able to do too much, okay?

849
00:36:42,560 --> 00:36:46,320
That group of people who are, let's say, the dissident technologists

850
00:36:46,320 --> 00:36:49,760
who are not going to just kneel to anything

851
00:36:49,760 --> 00:36:51,600
that comes out of Washington D.C. or Beijing,

852
00:36:52,160 --> 00:36:54,400
that is the, that's decentralized AI.

853
00:36:55,040 --> 00:36:56,240
That's crypto.

854
00:36:56,240 --> 00:36:57,680
That's decentralized social media.

855
00:36:57,680 --> 00:36:59,280
So you can think of it as, you know,

856
00:36:59,280 --> 00:37:02,080
where we talked about in the recent Pirate Warriors podcast,

857
00:37:02,080 --> 00:37:06,000
freedom to speak with decentralized censorship-resistant social media,

858
00:37:06,000 --> 00:37:08,560
freedom to transact with cryptocurrency,

859
00:37:08,560 --> 00:37:11,600
freedom to compute with open source AI.

860
00:37:11,680 --> 00:37:13,440
And no compute limits, okay?

861
00:37:14,080 --> 00:37:15,280
That's a freedom movement.

862
00:37:16,320 --> 00:37:18,880
And that's like the same spirit as a pirate bay,

863
00:37:18,880 --> 00:37:20,240
the same spirit as BitTorrent,

864
00:37:20,240 --> 00:37:22,080
the same spirit as Bitcoin,

865
00:37:22,080 --> 00:37:25,520
the same spirit as peer-to-peer and end-to-end encryption.

866
00:37:25,520 --> 00:37:28,160
That's a very different spirit than

867
00:37:28,160 --> 00:37:30,880
having Kamala Harris regulate a superintelligence

868
00:37:30,880 --> 00:37:33,760
or signing it over to Xi Jinping thought.

869
00:37:34,480 --> 00:37:39,200
And the reason I say this is, I think that that group of people,

870
00:37:39,200 --> 00:37:42,480
of which I think Indians and Israelis will be a very prominent,

871
00:37:42,480 --> 00:37:44,080
maybe a plurality, right?

872
00:37:44,080 --> 00:37:45,520
Just because the sheer quantity of Indians

873
00:37:45,520 --> 00:37:48,400
are like the third sort of big group that's kind of coming up.

874
00:37:48,400 --> 00:37:49,920
And they're relatively underpriced.

875
00:37:49,920 --> 00:37:53,600
You know, China is, I don't say it's price to perfection,

876
00:37:53,600 --> 00:37:56,160
but it's something that people, when I say priced,

877
00:37:56,160 --> 00:38:00,080
I mean, people were dismissive of China even up until 2019.

878
00:38:00,960 --> 00:38:02,480
And then it was after 2020,

879
00:38:02,480 --> 00:38:04,480
if you look that people started to take China seriously.

880
00:38:05,040 --> 00:38:07,440
And I mean, that is the West Coast tech people

881
00:38:07,440 --> 00:38:10,080
knew that China actually had A plus tech companies

882
00:38:10,080 --> 00:38:11,760
and was a very strong competitor.

883
00:38:11,760 --> 00:38:14,240
But the East Coast still thought of them as a third-world country

884
00:38:14,240 --> 00:38:17,360
until after COVID, when now, you know,

885
00:38:17,360 --> 00:38:19,760
the East Coast was sort of threatened by them politically.

886
00:38:19,760 --> 00:38:23,440
And it wasn't just blue collars, but blue America

887
00:38:23,440 --> 00:38:24,400
that was threatened by China.

888
00:38:25,040 --> 00:38:27,600
And so that's why the reaction to China went from,

889
00:38:27,600 --> 00:38:30,320
oh, who cares, it's just taking some manufacturing jobs to,

890
00:38:30,320 --> 00:38:32,160
this is an empire that can contend with us

891
00:38:32,160 --> 00:38:33,440
for control of the world.

892
00:38:33,440 --> 00:38:35,440
That's why the hostility is ramped up, in my view.

893
00:38:35,440 --> 00:38:36,480
There's a lot of other dimensions to it,

894
00:38:36,480 --> 00:38:37,440
but that's a big part of it.

895
00:38:38,560 --> 00:38:41,440
So India is also kind of there, but it's like the third.

896
00:38:41,440 --> 00:38:43,440
And India is not going to play for number one or number two.

897
00:38:44,160 --> 00:38:46,560
But India and Israel, if you look at like tech founders,

898
00:38:47,840 --> 00:38:50,320
depending on how you count, especially if you include diasporas,

899
00:38:50,320 --> 00:38:53,600
it's on the order of 30 to 50% of tech founders, right?

900
00:38:53,600 --> 00:38:55,680
And it's obviously some, you know, very good tech CEOs

901
00:38:55,680 --> 00:38:58,720
and, you know, Satya and Sundar and investors and whatnot.

902
00:38:58,720 --> 00:39:02,960
Those are folks Indians do not want to bow to DC or to Beijing,

903
00:39:02,960 --> 00:39:04,560
neither do Israelis for all kinds of reasons,

904
00:39:04,640 --> 00:39:06,640
even if Israel has to, you know,

905
00:39:06,640 --> 00:39:08,160
take some direction from the U.S.

906
00:39:08,160 --> 00:39:09,440
Now they're bristling at it, right?

907
00:39:10,080 --> 00:39:12,080
And then a bunch of other countries don't.

908
00:39:12,080 --> 00:39:14,480
So the question is, who breaks away?

909
00:39:14,480 --> 00:39:16,320
And now we get to your point on,

910
00:39:16,320 --> 00:39:19,760
the reason I had to say that is that that's preface,

911
00:39:19,760 --> 00:39:23,040
the political environment, this tripolar thing of U.S. tech

912
00:39:23,760 --> 00:39:26,880
and U.S. regulated, Chinese tech and China regulated,

913
00:39:26,880 --> 00:39:28,240
and global tech that's free.

914
00:39:29,120 --> 00:39:32,080
Okay, of course there's, even though I identify those three polls,

915
00:39:32,080 --> 00:39:33,760
there's of course boundary regions.

916
00:39:33,840 --> 00:39:36,880
EAC is actually on the boundary of U.S. tech

917
00:39:36,880 --> 00:39:38,800
and decentralized tech, you know?

918
00:39:38,800 --> 00:39:41,440
And I'm sure there'll be some Chinese thing that comes out

919
00:39:41,440 --> 00:39:42,640
that is also on the boundary there.

920
00:39:42,640 --> 00:39:45,680
For example, Binance is on the boundary of Chinese tech

921
00:39:45,680 --> 00:39:47,440
and global and decentralized tech,

922
00:39:47,440 --> 00:39:48,640
if that makes any sense, right?

923
00:39:48,640 --> 00:39:50,160
And there's probably others, Apple is actually

924
00:39:50,160 --> 00:39:52,080
on the boundary of U.S. tech and Chinese tech

925
00:39:52,080 --> 00:39:54,000
because they make all of their stuff in China, right?

926
00:39:54,000 --> 00:39:56,880
So these are not totally disjoint groups,

927
00:39:56,880 --> 00:39:58,960
but there's boundary areas, but you can think of them.

928
00:39:59,520 --> 00:40:01,600
Why is this third group so important in my view?

929
00:40:02,320 --> 00:40:05,120
Both the Chinese group and the decentralized group

930
00:40:05,120 --> 00:40:08,640
will be very strong competition for the American group

931
00:40:08,640 --> 00:40:09,920
for totally different reasons.

932
00:40:10,880 --> 00:40:14,480
China has things like WeChat, these super apps,

933
00:40:14,480 --> 00:40:15,840
I mean, obviously not likely,

934
00:40:17,040 --> 00:40:19,440
WeChat is a super app, but they also have,

935
00:40:19,440 --> 00:40:21,520
for example, their digital yuan, right?

936
00:40:21,520 --> 00:40:25,520
They have the largest, cleanest data sets in the world

937
00:40:25,520 --> 00:40:27,040
that are constantly updated in real time

938
00:40:27,040 --> 00:40:29,440
that they can mandate their entire population opt into,

939
00:40:30,000 --> 00:40:35,120
and most of the Chinese language speaking people

940
00:40:35,120 --> 00:40:37,840
are under their ambit, right?

941
00:40:37,840 --> 00:40:40,560
So that doesn't include Taiwan, doesn't include Singapore,

942
00:40:40,560 --> 00:40:43,920
doesn't include some of the Chinese diaspora,

943
00:40:43,920 --> 00:40:45,920
but basically anything that's happening in Chinese

944
00:40:45,920 --> 00:40:50,240
for 99% of it, 95, whatever the ratio is, they can see it

945
00:40:50,240 --> 00:40:52,560
and they can coerce it and they can control it.

946
00:40:52,560 --> 00:40:55,600
So they can tell all of their people,

947
00:40:55,600 --> 00:40:59,440
okay, here's five bucks in digital yuan,

948
00:40:59,440 --> 00:41:01,120
do this micro task, okay?

949
00:41:01,840 --> 00:41:03,920
All of these digital blue collar jobs,

950
00:41:03,920 --> 00:41:04,960
both China and India, I think,

951
00:41:04,960 --> 00:41:07,040
can do quite a lot with that and they'll come back to it.

952
00:41:07,040 --> 00:41:09,040
So they can make their people do immense amounts

953
00:41:09,040 --> 00:41:11,520
of training data, clean up lots of data sets,

954
00:41:11,520 --> 00:41:13,440
once it's clear that you have to build this and do this,

955
00:41:13,440 --> 00:41:15,120
they can just kind of execute on that.

956
00:41:15,120 --> 00:41:18,240
And they can also deploy, I mean, in many ways,

957
00:41:18,240 --> 00:41:21,040
the US is still very strong in digital technology,

958
00:41:21,040 --> 00:41:22,960
but in the physical world, it's terrible

959
00:41:23,840 --> 00:41:24,960
because of all the regulations,

960
00:41:24,960 --> 00:41:26,960
it causes all the nimbyism and so on.

961
00:41:26,960 --> 00:41:28,640
It's not like that in China.

962
00:41:28,640 --> 00:41:31,040
So anything which kind of works in the US

963
00:41:31,040 --> 00:41:33,200
at a physical level, like the Boston Dynamic stuff,

964
00:41:33,200 --> 00:41:34,880
they're already cloning it in China

965
00:41:34,880 --> 00:41:36,800
and they can scale it out in the physical world.

966
00:41:36,800 --> 00:41:40,000
You already have drones, little sidewalk drone things

967
00:41:40,000 --> 00:41:42,160
that come to your hotel room and drop things off.

968
00:41:42,160 --> 00:41:44,480
That's already very common in China.

969
00:41:44,480 --> 00:41:45,600
In many ways, it's already ahead

970
00:41:45,600 --> 00:41:47,120
if you go to the Chinese cities.

971
00:41:47,120 --> 00:41:49,440
So the Chinese version of AI is ultra centralized,

972
00:41:49,440 --> 00:41:52,800
more centralized, more monitoring, less privacy

973
00:41:52,800 --> 00:41:54,320
and so on than the American version,

974
00:41:54,320 --> 00:41:56,960
and therefore they will have potentially better data sets,

975
00:41:56,960 --> 00:41:58,800
at least for the Chinese population.

976
00:41:58,800 --> 00:42:01,520
And so we chat AI, I don't even know what it's gonna be,

977
00:42:01,520 --> 00:42:03,360
but it'll be probably really good.

978
00:42:03,360 --> 00:42:04,960
It'll also be really dangerous in other ways.

979
00:42:06,160 --> 00:42:10,480
Then the decentralized sphere has power for a different reason

980
00:42:10,480 --> 00:42:11,920
because the decentralized sphere

981
00:42:11,920 --> 00:42:14,000
can train on full Hollywood movies.

982
00:42:15,040 --> 00:42:19,120
It can train on all books, all MP3s.

983
00:42:19,520 --> 00:42:22,560
And just say, screw all this copyright stuff, right?

984
00:42:22,560 --> 00:42:25,600
Like what Psyhub and Libgen are doing.

985
00:42:25,600 --> 00:42:28,160
Because all the copyright, first of all,

986
00:42:28,160 --> 00:42:31,200
it's not, it's like Disney lobbying politicians

987
00:42:31,200 --> 00:42:33,600
to put like another 60 or 70 or 90.

988
00:42:33,600 --> 00:42:34,400
I don't even know what it is,

989
00:42:34,400 --> 00:42:35,920
some crazy amount on copyrights.

990
00:42:35,920 --> 00:42:37,440
So you can keep milking this stuff

991
00:42:37,440 --> 00:42:39,440
and it doesn't go into public domain, number one.

992
00:42:39,440 --> 00:42:40,960
And second, you know how Hollywood is built

993
00:42:40,960 --> 00:42:41,840
in the first place?

994
00:42:41,840 --> 00:42:44,080
It was all patent copyright and IP violation.

995
00:42:44,080 --> 00:42:45,920
Essentially Edison had all the patents.

996
00:42:45,920 --> 00:42:48,960
He's in New Jersey-ish, okay, that East Coast area.

997
00:42:48,960 --> 00:42:51,760
And Neil Gabler has this great book

998
00:42:51,760 --> 00:42:53,600
called An Empire of Their Own

999
00:42:53,600 --> 00:42:57,200
where he talks about how immigrant populations,

1000
00:42:57,200 --> 00:42:58,480
the Jewish community in particular,

1001
00:42:58,480 --> 00:43:01,760
and also others went to Southern California in part

1002
00:43:01,760 --> 00:43:03,440
so they could just make movies

1003
00:43:03,440 --> 00:43:04,960
so that Edison coming and suing them

1004
00:43:04,960 --> 00:43:06,720
for all the patents and so on and so forth.

1005
00:43:06,720 --> 00:43:07,520
And they made enough money

1006
00:43:07,520 --> 00:43:09,280
that they could fight those battles in court

1007
00:43:09,280 --> 00:43:11,920
and that's how they built Hollywood, okay?

1008
00:43:11,920 --> 00:43:15,840
So one of my big thesis is history is running in reverse

1009
00:43:15,840 --> 00:43:16,960
and I can get to why,

1010
00:43:16,960 --> 00:43:18,800
but it's like 1950s, a mirror moment

1011
00:43:18,800 --> 00:43:20,640
and you go more decentralized backwards

1012
00:43:20,640 --> 00:43:22,480
and forwards in time is like these,

1013
00:43:22,480 --> 00:43:23,920
you have these huge centralized states

1014
00:43:23,920 --> 00:43:26,960
like the US and USSR and China, you know,

1015
00:43:26,960 --> 00:43:29,600
all these things exist and then their fist relaxes

1016
00:43:29,600 --> 00:43:31,600
as you go forwards and backwards in time.

1017
00:43:31,600 --> 00:43:33,040
For example, backwards in time,

1018
00:43:33,040 --> 00:43:34,640
the Western frontier closed

1019
00:43:35,360 --> 00:43:37,280
and forwards in time, the Eastern frontier opens.

1020
00:43:37,280 --> 00:43:39,040
Backwards in time, you have the robber barons,

1021
00:43:39,040 --> 00:43:40,640
forwards in time, you have the tech billionaires.

1022
00:43:40,640 --> 00:43:42,240
Backwards in time, you have Spanish flu,

1023
00:43:42,240 --> 00:43:43,600
forwards in time, you have COVID-19.

1024
00:43:43,600 --> 00:43:46,400
And I've got dozens of examples of this in the book.

1025
00:43:46,400 --> 00:43:49,120
The point is that if you go backwards in time,

1026
00:43:49,120 --> 00:43:51,840
the ability to enforce patents and copyrights and so on

1027
00:43:51,840 --> 00:43:53,520
starts dropping off, right?

1028
00:43:53,520 --> 00:43:56,480
You have much more of a Grand Theft Auto environment

1029
00:43:56,480 --> 00:43:58,480
and you go forwards in time and that's happening again.

1030
00:43:59,360 --> 00:44:02,240
So India in particular, for many years,

1031
00:44:02,240 --> 00:44:06,160
basically just didn't obey Western patent protections

1032
00:44:06,160 --> 00:44:08,720
and all these stupid rules basically, you know,

1033
00:44:09,440 --> 00:44:11,920
it's a combination of artificial scarcity on the patent side

1034
00:44:11,920 --> 00:44:14,080
and artificial regulation on the FDI side.

1035
00:44:14,080 --> 00:44:16,240
That's a big part of what jacksup drug costs,

1036
00:44:16,240 --> 00:44:17,440
where these things cost, you know,

1037
00:44:17,440 --> 00:44:18,480
only cents to manufacturing,

1038
00:44:18,480 --> 00:44:19,920
they sell them for so much money.

1039
00:44:20,800 --> 00:44:23,360
All the delays, of course, that are imposed on the process,

1040
00:44:23,360 --> 00:44:24,800
the only way they can pay for it,

1041
00:44:24,800 --> 00:44:26,880
the manufacturers, is to take it out of your hide.

1042
00:44:26,880 --> 00:44:28,160
What India did is they just said,

1043
00:44:28,160 --> 00:44:29,520
we're not going to obey any of that stuff.

1044
00:44:30,160 --> 00:44:33,120
So they have a whole massive generic drugs

1045
00:44:33,120 --> 00:44:35,120
and biotech industry that arose

1046
00:44:35,120 --> 00:44:36,400
because they built all the skills for that.

1047
00:44:36,400 --> 00:44:38,720
That's why they could do their own vaccine during COVID

1048
00:44:38,720 --> 00:44:41,840
and they're one of the biggest biotech industries in the world

1049
00:44:41,840 --> 00:44:45,280
because they said screw Western restrictive IPs

1050
00:44:45,280 --> 00:44:46,560
and other stuff, right?

1051
00:44:46,560 --> 00:44:49,120
So I was actually talking with the founder of Flipkart,

1052
00:44:49,120 --> 00:44:50,640
that's India's largest exit.

1053
00:44:50,640 --> 00:44:52,400
And we were talking about this a few months ago,

1054
00:44:53,040 --> 00:44:56,560
and what we want is for India and other countries like it,

1055
00:44:56,560 --> 00:44:58,960
do something similar, not just generic drugs,

1056
00:44:58,960 --> 00:45:04,720
but generic AI, meaning just let people train on Hollywood movies,

1057
00:45:04,720 --> 00:45:06,880
let them train on full songs,

1058
00:45:06,880 --> 00:45:08,560
let them train on every book,

1059
00:45:09,120 --> 00:45:11,200
let them train on anything.

1060
00:45:11,200 --> 00:45:14,240
And you know what, sue them in India, right?

1061
00:45:14,240 --> 00:45:15,760
And have the servers in India

1062
00:45:15,760 --> 00:45:18,080
and let people also train models in India

1063
00:45:18,720 --> 00:45:22,560
because that's something that can build up a domestic industry

1064
00:45:22,560 --> 00:45:24,240
with skills that the rest of the world,

1065
00:45:24,960 --> 00:45:26,240
people will want the model output,

1066
00:45:26,240 --> 00:45:28,400
they'll want to use the software service there,

1067
00:45:28,400 --> 00:45:29,920
and they'll be fighting in court on the back end.

1068
00:45:29,920 --> 00:45:32,640
This is similar to how all of the record companies

1069
00:45:32,640 --> 00:45:35,520
fought Napster and Kazaa and so on,

1070
00:45:35,520 --> 00:45:36,880
but they couldn't take down Spotify.

1071
00:45:36,880 --> 00:45:38,320
Do you know that story? Do you remember that?

1072
00:45:38,320 --> 00:45:42,800
Basically, because Spotify was legitimately a European company

1073
00:45:42,800 --> 00:45:46,000
and that a combination of execution and negotiation,

1074
00:45:46,720 --> 00:45:47,840
they couldn't take them down.

1075
00:45:47,840 --> 00:45:50,640
They did take down Napster, they took down Limewire,

1076
00:45:50,640 --> 00:45:52,080
they took down Groove Shark,

1077
00:45:52,080 --> 00:45:53,920
and Kazaa had Estonians,

1078
00:45:53,920 --> 00:45:55,200
I don't know exactly how it was incorporated,

1079
00:45:55,200 --> 00:45:56,960
but it was probably two U.S. proximal,

1080
00:45:56,960 --> 00:45:58,480
and that's where they were able to get them.

1081
00:45:58,480 --> 00:46:00,240
But Spotify was far enough away

1082
00:46:00,240 --> 00:46:01,680
that they couldn't just sue them

1083
00:46:01,680 --> 00:46:03,920
and they actually genuinely had European traction.

1084
00:46:03,920 --> 00:46:05,920
That's why the RA had to negotiate.

1085
00:46:06,000 --> 00:46:09,040
So being far away from San Francisco

1086
00:46:09,680 --> 00:46:12,000
may also be an advantage in AI,

1087
00:46:12,000 --> 00:46:14,240
because it means you're far away from the blue city

1088
00:46:14,240 --> 00:46:16,080
in the blue state in the Union.

1089
00:46:16,080 --> 00:46:17,760
This relates to another really important point.

1090
00:46:18,640 --> 00:46:20,800
When you actually think about deploying AI,

1091
00:46:20,800 --> 00:46:22,000
there's those jobs you can disrupt

1092
00:46:22,000 --> 00:46:23,680
that are not regulated jobs.

1093
00:46:23,680 --> 00:46:27,440
Like, obviously, programmers are not,

1094
00:46:28,320 --> 00:46:30,880
thank God, you don't need a license to be a programmer,

1095
00:46:30,880 --> 00:46:33,520
but programmers adopt this kind of stuff naturally.

1096
00:46:33,520 --> 00:46:35,520
So get up, co-pilot, replete.

1097
00:46:35,520 --> 00:46:36,720
We just, boom, use it,

1098
00:46:36,720 --> 00:46:38,160
and now it's amplified intelligence.

1099
00:46:39,280 --> 00:46:40,480
But a lot of other jobs,

1100
00:46:41,120 --> 00:46:42,240
there's some that are unionized

1101
00:46:42,240 --> 00:46:43,840
and then some that are licensed.

1102
00:46:43,840 --> 00:46:46,720
So Hollywood screenwriters are complaining.

1103
00:46:46,720 --> 00:46:47,840
Journalists are complaining.

1104
00:46:47,840 --> 00:46:49,040
Artists are complaining.

1105
00:46:49,040 --> 00:46:51,280
This is a good chunk of Blue America.

1106
00:46:51,280 --> 00:46:53,440
If you add in licensed jobs,

1107
00:46:53,440 --> 00:46:56,480
like lawyers and doctors and bureaucrats,

1108
00:46:58,400 --> 00:46:59,440
especially lawyers and doctors

1109
00:46:59,440 --> 00:47:00,480
are very politically powerful,

1110
00:47:00,480 --> 00:47:01,360
MDs and JDs.

1111
00:47:01,360 --> 00:47:03,520
They have strong lobbying organizations,

1112
00:47:03,600 --> 00:47:05,760
AMA and APN and so on.

1113
00:47:07,280 --> 00:47:09,200
Basically, AI is part

1114
00:47:09,200 --> 00:47:11,760
of the economic apocalypse for Blue America.

1115
00:47:13,440 --> 00:47:17,920
It just attacks these overpriced jobs.

1116
00:47:17,920 --> 00:47:19,520
And they say overpriced relative to

1117
00:47:20,320 --> 00:47:22,320
what an Indian could do with an Android phone,

1118
00:47:22,320 --> 00:47:24,320
what a South American could do with an Android phone,

1119
00:47:24,320 --> 00:47:25,600
what someone in the Middle East

1120
00:47:25,600 --> 00:47:27,520
or the Midwest could do with an Android phone.

1121
00:47:28,240 --> 00:47:31,920
Now, those folks have been armed

1122
00:47:32,480 --> 00:47:33,440
with generative AI.

1123
00:47:34,000 --> 00:47:35,200
They can do way more.

1124
00:47:35,840 --> 00:47:36,800
They're ready to work.

1125
00:47:36,800 --> 00:47:38,400
They're ready to work for much less money.

1126
00:47:38,400 --> 00:47:42,000
And they're a massive threat to Blue America.

1127
00:47:42,000 --> 00:47:43,280
Blue America is now feeling

1128
00:47:43,280 --> 00:47:45,120
like the blue collars of 10 or 20 years ago,

1129
00:47:45,680 --> 00:47:48,720
where the blue collars had their jobs,

1130
00:47:48,720 --> 00:47:50,640
going to China and other places,

1131
00:47:50,640 --> 00:47:51,600
and they were mad about that.

1132
00:47:51,600 --> 00:47:53,200
Factories got shut down and so on.

1133
00:47:53,200 --> 00:47:54,880
That's about to happen to Blue America,

1134
00:47:54,880 --> 00:47:55,840
already happening.

1135
00:47:56,800 --> 00:47:58,480
And so that's going to mean

1136
00:47:58,480 --> 00:48:01,680
a political backlash by Blue America of protectionism.

1137
00:48:02,160 --> 00:48:03,760
Again, already happening.

1138
00:48:03,760 --> 00:48:06,080
And the AI safety stuff,

1139
00:48:06,800 --> 00:48:07,920
that's a whole separate thing,

1140
00:48:07,920 --> 00:48:09,040
but it's going to be used.

1141
00:48:09,680 --> 00:48:10,640
I'm going to use a phrase,

1142
00:48:11,200 --> 00:48:12,960
and I hope you won't be offended by this.

1143
00:48:12,960 --> 00:48:14,160
Have you heard the phrase,

1144
00:48:14,160 --> 00:48:16,720
useful idiots, like by Lenin or whatever?

1145
00:48:16,720 --> 00:48:17,280
Okay.

1146
00:48:17,280 --> 00:48:18,480
It basically means like,

1147
00:48:18,480 --> 00:48:21,120
okay, those guys, they're useful idiots

1148
00:48:21,120 --> 00:48:22,320
for communism and so on.

1149
00:48:22,320 --> 00:48:24,720
So let me put it like naive people

1150
00:48:25,440 --> 00:48:28,800
who think that the US government

1151
00:48:28,800 --> 00:48:30,080
is interested in AI safety

1152
00:48:30,720 --> 00:48:32,880
are trying to give a lot of power to the US government.

1153
00:48:33,440 --> 00:48:34,080
And the reason is,

1154
00:48:34,080 --> 00:48:36,000
they haven't actually thought through from first principles

1155
00:48:36,000 --> 00:48:37,920
what is the most powerful action in the world to connect them.

1156
00:48:37,920 --> 00:48:39,040
They're trying to give power to the US government

1157
00:48:39,040 --> 00:48:40,080
to regulate AI safety.

1158
00:48:40,560 --> 00:48:42,800
But the government doesn't care about safety of anything.

1159
00:48:42,800 --> 00:48:45,760
They literally funded the COVID virus

1160
00:48:46,400 --> 00:48:47,360
in Wuhan,

1161
00:48:47,360 --> 00:48:48,640
credibly alleged, right?

1162
00:48:48,640 --> 00:48:50,160
There's at least,

1163
00:48:50,160 --> 00:48:53,120
it is a reasonable hypothesis based on a lot of the data.

1164
00:48:53,120 --> 00:48:54,880
Matt Ridley wrote a whole book on this.

1165
00:48:54,880 --> 00:48:56,480
There's a lot of data that indicates,

1166
00:48:56,480 --> 00:48:57,440
a lot of scientists believe it.

1167
00:48:57,440 --> 00:48:59,760
I'm actually like a bioinformatics genomics guy,

1168
00:48:59,760 --> 00:49:01,280
if you look at the sequences,

1169
00:49:01,280 --> 00:49:02,960
there is a gap and a jump

1170
00:49:02,960 --> 00:49:04,960
where it looks like this thing could have been engineered

1171
00:49:04,960 --> 00:49:06,800
or partially engineered or evolved.

1172
00:49:06,800 --> 00:49:08,480
There's Peter Dazak,

1173
00:49:08,480 --> 00:49:09,760
there's Zengli Xi,

1174
00:49:09,760 --> 00:49:11,200
there's actually a lot of evidence here.

1175
00:49:11,200 --> 00:49:13,600
So the US government and the Chinese government

1176
00:49:13,600 --> 00:49:15,600
are responsible for an existential risk.

1177
00:49:16,480 --> 00:49:18,080
By studying it, they created it.

1178
00:49:18,080 --> 00:49:18,880
Okay.

1179
00:49:18,880 --> 00:49:21,840
They're responsible for risking nuclear war with Russia

1180
00:49:21,840 --> 00:49:24,240
over this piece of land in eastern Ukraine,

1181
00:49:24,240 --> 00:49:26,640
which probably is going to get wound down.

1182
00:49:26,640 --> 00:49:27,280
Okay.

1183
00:49:27,280 --> 00:49:28,960
So they don't care about your safety

1184
00:49:30,240 --> 00:49:30,640
at all.

1185
00:49:31,440 --> 00:49:32,160
They're not like,

1186
00:49:32,160 --> 00:49:34,160
these are immediate things where we can show

1187
00:49:34,160 --> 00:49:35,760
and there's nobody who's punished for this,

1188
00:49:35,760 --> 00:49:36,880
nobody who's fired for this,

1189
00:49:37,840 --> 00:49:41,680
literally rolling the dice on millions,

1190
00:49:41,680 --> 00:49:42,960
hundreds of millions of people's lives

1191
00:49:43,520 --> 00:49:44,560
has not been punished.

1192
00:49:44,560 --> 00:49:46,000
In fact, it's like,

1193
00:49:46,000 --> 00:49:47,120
it's not even talked about

1194
00:49:47,760 --> 00:49:50,160
we're past the pandemic and these institutions

1195
00:49:50,160 --> 00:49:50,880
can't be punished.

1196
00:49:52,240 --> 00:49:54,800
So they don't care about AI safety.

1197
00:49:54,800 --> 00:49:56,160
What they care about is AI control.

1198
00:49:57,760 --> 00:49:59,520
And so the people in tech who are like,

1199
00:49:59,600 --> 00:50:02,480
well, the government will guarantee AI safety.

1200
00:50:02,480 --> 00:50:04,640
That's actually what we're going to actually get

1201
00:50:04,640 --> 00:50:06,320
is something on the current path,

1202
00:50:06,320 --> 00:50:08,160
like what happened with nuclear technology,

1203
00:50:08,160 --> 00:50:10,320
where you got nuclear weapons,

1204
00:50:10,320 --> 00:50:11,680
but not nuclear power,

1205
00:50:11,680 --> 00:50:13,680
or at least not to the scale that we could have had it.

1206
00:50:13,680 --> 00:50:16,320
We could have had much cheaper energy for everything.

1207
00:50:16,320 --> 00:50:18,240
Instead, we got the militarization

1208
00:50:18,240 --> 00:50:20,960
and the regulation and the deceleration,

1209
00:50:20,960 --> 00:50:22,160
worst of all worlds,

1210
00:50:22,160 --> 00:50:24,080
where you can blow people up,

1211
00:50:24,080 --> 00:50:26,320
but you can't build nuclear power plants.

1212
00:50:26,880 --> 00:50:29,920
And like even getting into nuclear technology,

1213
00:50:29,920 --> 00:50:31,440
forget about just nuclear power plants.

1214
00:50:31,440 --> 00:50:32,640
We don't have nuclear submarines.

1215
00:50:32,640 --> 00:50:34,000
We don't have nuclear planes,

1216
00:50:34,000 --> 00:50:34,800
all that kind of stuff.

1217
00:50:34,800 --> 00:50:36,000
I don't know if nuclear planes are possible,

1218
00:50:36,000 --> 00:50:37,520
but I do know nuclear submarines are possible.

1219
00:50:37,520 --> 00:50:39,840
You can do a lot more cruise ships,

1220
00:50:39,840 --> 00:50:40,720
a lot more stuff like that.

1221
00:50:40,720 --> 00:50:42,000
You could probably have nuclear trains.

1222
00:50:42,560 --> 00:50:44,240
You have to look at exactly how big those are.

1223
00:50:45,280 --> 00:50:46,960
I don't know exactly how big those engines are

1224
00:50:46,960 --> 00:50:47,440
and what the spies,

1225
00:50:47,440 --> 00:50:48,800
but I wouldn't be surprised if you could.

1226
00:50:49,680 --> 00:50:50,480
We don't have that.

1227
00:50:50,480 --> 00:50:51,360
Why don't we have that?

1228
00:50:51,360 --> 00:50:54,800
Because we had the wrong fear driven regulation

1229
00:50:54,800 --> 00:50:55,600
in the early 70s.

1230
00:50:56,400 --> 00:50:57,360
Putting it all together,

1231
00:50:58,960 --> 00:51:01,520
I think that the current AI safety stuff

1232
00:51:01,520 --> 00:51:03,040
is similar to nuclear safety stuff,

1233
00:51:04,160 --> 00:51:06,160
that the US government has a terrible track record

1234
00:51:06,160 --> 00:51:07,520
on safety in general.

1235
00:51:07,520 --> 00:51:08,560
It doesn't care about it.

1236
00:51:08,560 --> 00:51:10,400
It funded the COVID virus,

1237
00:51:10,400 --> 00:51:11,680
incredibly alleged.

1238
00:51:11,680 --> 00:51:14,880
It definitely risked nuclear war with Russia recently.

1239
00:51:14,880 --> 00:51:16,480
Hot war with Russia was the red line

1240
00:51:16,480 --> 00:51:17,600
we were not supposed to cross,

1241
00:51:17,600 --> 00:51:19,360
and we're now like way into that.

1242
00:51:19,920 --> 00:51:21,120
So it doesn't care about AI safety.

1243
00:51:21,120 --> 00:51:22,240
It doesn't care about your safety.

1244
00:51:23,200 --> 00:51:25,280
And it's also not even good at regulating.

1245
00:51:26,080 --> 00:51:28,480
And so what it cares about is control,

1246
00:51:28,480 --> 00:51:30,800
and we are going to have potentially a bad outcome

1247
00:51:30,800 --> 00:51:33,040
where Silicon Valley and San Francisco

1248
00:51:33,040 --> 00:51:34,560
is the Xerox Park of AI.

1249
00:51:36,000 --> 00:51:37,040
Maybe that's too strong,

1250
00:51:37,040 --> 00:51:40,080
okay, but basically it develops it,

1251
00:51:40,080 --> 00:51:42,160
and there's a lot of things it can't do

1252
00:51:42,160 --> 00:51:44,160
because it lobbied for this regulation

1253
00:51:44,160 --> 00:51:46,160
that is going to come back and choke it.

1254
00:51:46,160 --> 00:51:49,040
And then the other two spheres will push ahead

1255
00:51:49,040 --> 00:51:50,960
because it's not about the technology.

1256
00:51:50,960 --> 00:51:52,640
It's also about the political layer.

1257
00:51:52,640 --> 00:51:53,760
You know the Steve Jobs saying,

1258
00:51:54,640 --> 00:51:56,560
actually Alan Kay by way of Steve Jobs,

1259
00:51:56,560 --> 00:51:59,040
if you're really serious about software,

1260
00:51:59,040 --> 00:52:00,400
you need your own hardware, right?

1261
00:52:00,960 --> 00:52:03,040
So if you're really serious about technology,

1262
00:52:03,040 --> 00:52:04,240
you need your own sovereignty.

1263
00:52:05,920 --> 00:52:09,040
Because like what the AI people haven't thought about is

1264
00:52:09,040 --> 00:52:11,120
there's a platform beneath you,

1265
00:52:11,120 --> 00:52:14,560
which is not just compute, it is regulate.

1266
00:52:14,560 --> 00:52:16,400
It's a law, okay?

1267
00:52:16,400 --> 00:52:18,560
And if the law doesn't allow you to compute

1268
00:52:18,560 --> 00:52:20,160
so much for all of your stuff above that.

1269
00:52:20,880 --> 00:52:22,320
And I know you're saying,

1270
00:52:22,320 --> 00:52:24,880
oh, it's only a 10 to the 26 compute band

1271
00:52:24,880 --> 00:52:25,680
and so on and so forth.

1272
00:52:25,680 --> 00:52:27,520
Have you seen the first IRS tax form?

1273
00:52:28,080 --> 00:52:31,760
It's always, always super simple.

1274
00:52:31,760 --> 00:52:33,760
It's only the super, super, super rich

1275
00:52:33,760 --> 00:52:35,280
who's we're going to get in at first.

1276
00:52:35,280 --> 00:52:36,240
Doesn't matter to you.

1277
00:52:36,240 --> 00:52:39,120
So that's called boiling the frog slowly.

1278
00:52:39,120 --> 00:52:40,560
There's a million, you know, slippery slope.

1279
00:52:40,560 --> 00:52:41,920
Slippery slope isn't a fallacy.

1280
00:52:41,920 --> 00:52:43,600
It's literally how things work, right?

1281
00:52:44,160 --> 00:52:47,280
Apple, one of the reasons they talk about

1282
00:52:47,280 --> 00:52:48,720
not setting a precedent.

1283
00:52:48,720 --> 00:52:51,600
Zuck starts a very hard line on setting precedents

1284
00:52:51,600 --> 00:52:53,760
because he understands the long-term equivalent

1285
00:52:53,760 --> 00:52:54,960
of setting a precedent, right?

1286
00:52:55,600 --> 00:52:56,800
The precedent setting is that

1287
00:52:56,800 --> 00:52:58,640
they're setting up a software FDA.

1288
00:52:58,640 --> 00:53:01,360
And DC is so energized on this

1289
00:53:01,360 --> 00:53:04,080
because they know how much social media disrupted them.

1290
00:53:04,080 --> 00:53:06,560
That's why they're on the attack on crypto and AI.

1291
00:53:06,560 --> 00:53:08,640
That's why they're on the attack on self-driving cars.

1292
00:53:08,640 --> 00:53:11,760
They want to freeze the current social order in amber

1293
00:53:11,760 --> 00:53:13,280
domestically and globally.

1294
00:53:13,280 --> 00:53:15,200
So they think they can sanction China

1295
00:53:15,200 --> 00:53:16,800
and stop it from developing chips.

1296
00:53:16,800 --> 00:53:19,600
They think they can impose regulations on the U.S.

1297
00:53:19,600 --> 00:53:21,280
and stop it from developing AI.

1298
00:53:21,280 --> 00:53:22,400
But they can't.

1299
00:53:22,400 --> 00:53:25,680
And also, by the way, they're totally schizophrenic on this

1300
00:53:25,680 --> 00:53:27,440
where when they're talking about China,

1301
00:53:27,440 --> 00:53:29,120
they're like, we're going to stop their chips

1302
00:53:29,120 --> 00:53:31,120
to make sure America is a global leader.

1303
00:53:31,120 --> 00:53:33,120
This is this Gina Raimondo who's saying this.

1304
00:53:33,120 --> 00:53:34,960
And then domestically, they're like,

1305
00:53:34,960 --> 00:53:37,760
we're going to regulate you so you stop accelerating AI.

1306
00:53:37,760 --> 00:53:39,200
We're not about AI acceleration.

1307
00:53:39,200 --> 00:53:41,360
EAC is weird over there, okay?

1308
00:53:41,360 --> 00:53:42,880
So think about how schizophrenic that is.

1309
00:53:43,600 --> 00:53:45,440
Okay, you're going to be far ahead of China.

1310
00:53:45,440 --> 00:53:47,760
We're also going to be make sure to control the U.S.

1311
00:53:47,760 --> 00:53:49,840
So they want to try and slow.

1312
00:53:49,840 --> 00:53:51,440
What they actually want is to freeze the current system

1313
00:53:51,440 --> 00:53:54,720
in amber, try to go back to pre-2007

1314
00:53:54,720 --> 00:53:56,400
before all these tech guys disrupted everything.

1315
00:53:56,960 --> 00:53:58,080
But that's not what's going to happen.

1316
00:53:58,960 --> 00:54:00,560
So, but they're going to try to do it.

1317
00:54:00,560 --> 00:54:03,440
And so everybody who's still loyal to the DC sphere,

1318
00:54:04,000 --> 00:54:06,080
which includes an enormous chunk of AI people.

1319
00:54:06,880 --> 00:54:08,560
And because they're all in,

1320
00:54:08,560 --> 00:54:10,560
a lot of them are in San Francisco, right?

1321
00:54:10,560 --> 00:54:15,040
And the political chaos of the last few years

1322
00:54:15,600 --> 00:54:18,880
was not sufficient for them to relocate yet.

1323
00:54:20,000 --> 00:54:20,560
Not all of them.

1324
00:54:20,560 --> 00:54:22,240
I mean, Elon is in Texas.

1325
00:54:22,240 --> 00:54:25,200
And it may turn out that Grock, for example,

1326
00:54:25,200 --> 00:54:26,800
and what they're doing there, because he's a very legit,

1327
00:54:26,800 --> 00:54:29,600
I mean, he's Elon, so he's capable of doing a lot.

1328
00:54:29,600 --> 00:54:30,800
He was very early on OpenAI.

1329
00:54:30,800 --> 00:54:33,200
He understands, right?

1330
00:54:33,200 --> 00:54:36,080
It may turn out that Grock becomes red AI,

1331
00:54:36,640 --> 00:54:38,000
or the community around that.

1332
00:54:39,040 --> 00:54:41,200
And OpenAI in DeepMind are still blue AI.

1333
00:54:41,200 --> 00:54:43,200
And we have Chinese AI and we're going to have decentralized AI.

1334
00:54:43,200 --> 00:54:43,840
Okay, let me pause there.

1335
00:54:43,840 --> 00:54:45,280
I know there's a big download.

1336
00:54:45,280 --> 00:54:47,040
Well, for starters, I would say,

1337
00:54:47,920 --> 00:54:54,160
broadly, I have a pretty similar intellectual tendency as you.

1338
00:54:54,160 --> 00:54:56,880
I would broadly describe myself as a techno-optimist

1339
00:54:56,880 --> 00:54:59,120
libertarian just about every issue.

1340
00:54:59,920 --> 00:55:05,440
And I think your analysis of the dynamics is super interesting.

1341
00:55:05,440 --> 00:55:07,360
And I think a lot of it sounds pretty plausible,

1342
00:55:07,360 --> 00:55:09,360
although I'll kind of float a couple of things

1343
00:55:09,360 --> 00:55:12,320
that I think may be bucking the trend.

1344
00:55:12,320 --> 00:55:15,040
But I think it's maybe useful to kind of try to separate this

1345
00:55:15,040 --> 00:55:19,760
into scenarios, because all the analysis that you're describing

1346
00:55:20,960 --> 00:55:23,120
here seem, if I understand it correctly,

1347
00:55:23,120 --> 00:55:29,760
it seems to have the implicit assumption that the AI itself

1348
00:55:29,760 --> 00:55:32,720
is not going to get super powerful or hard to control.

1349
00:55:33,680 --> 00:55:38,000
It's like, if we assume that it's kind of a normal technology,

1350
00:55:38,800 --> 00:55:40,480
then you're off to the races on this analysis,

1351
00:55:40,480 --> 00:55:42,240
and then we can get into the fine points.

1352
00:55:42,240 --> 00:55:45,600
But I do want to take at least one moment and say,

1353
00:55:46,400 --> 00:55:47,840
how confident are you on that?

1354
00:55:47,840 --> 00:55:51,840
Because if it's a totally different kind of technology

1355
00:55:51,840 --> 00:55:53,600
from other technologies that we've seen,

1356
00:55:54,720 --> 00:55:58,720
you raise the gain of function research example.

1357
00:55:58,720 --> 00:56:02,480
If it's that sort of technology that has these sort of

1358
00:56:03,280 --> 00:56:09,360
non-local possible impacts or self-reinforcing kind of dynamics,

1359
00:56:09,360 --> 00:56:13,600
which need not be like an Eliezer-style snap of the fingers fume,

1360
00:56:13,600 --> 00:56:18,480
but even over, say, a decade, let's imagine that over the next 10 years,

1361
00:56:18,480 --> 00:56:22,080
that AI's kind of multiple architectures develop,

1362
00:56:22,080 --> 00:56:23,280
and they sort of get integrated,

1363
00:56:23,280 --> 00:56:26,000
and we have something that kind of looks like robust,

1364
00:56:26,000 --> 00:56:28,960
silicon-based intelligence, maybe not totally robust,

1365
00:56:28,960 --> 00:56:32,400
but as robust or more robust than us, and running faster,

1366
00:56:32,400 --> 00:56:36,720
and the kind of thing that can do lots of full jobs,

1367
00:56:36,720 --> 00:56:38,480
or maybe even be tech CEOs,

1368
00:56:39,040 --> 00:56:42,000
then it kind of feels like a lot of this analysis

1369
00:56:42,720 --> 00:56:47,600
probably doesn't hold, because we're just in a totally different regime

1370
00:56:47,600 --> 00:56:51,200
that is just extremely hard to predict.

1371
00:56:51,920 --> 00:56:54,480
And I guess I wonder, first of all, do you agree with that?

1372
00:56:55,440 --> 00:56:57,840
There seems to be a big fork in the road there that's like,

1373
00:56:57,840 --> 00:57:03,120
just how fast and how powerful do these AI's become super powerful,

1374
00:57:03,120 --> 00:57:03,760
or do they not?

1375
00:57:03,760 --> 00:57:04,960
And if they don't, then yeah,

1376
00:57:04,960 --> 00:57:07,440
I think we're much more into real politic type of analysis.

1377
00:57:07,440 --> 00:57:09,680
But I'm not at all confident in that.

1378
00:57:09,680 --> 00:57:11,840
To me, it feels like there's a very real chance

1379
00:57:12,480 --> 00:57:15,520
that AI of 10 years from now is...

1380
00:57:16,160 --> 00:57:18,320
And by the way, this is like what the leaders are saying, right?

1381
00:57:18,320 --> 00:57:21,520
I mean, open AI is saying this, Anthropic is saying this,

1382
00:57:22,160 --> 00:57:26,400
Demis and Shane Legge are certainly saying things like this.

1383
00:57:26,400 --> 00:57:30,080
It seems like they expect that we will have AI's

1384
00:57:30,080 --> 00:57:33,200
that are more powerful than any individual human,

1385
00:57:33,200 --> 00:57:36,480
and that becomes like the bigger question

1386
00:57:37,360 --> 00:57:38,800
than anything else.

1387
00:57:38,800 --> 00:57:44,480
So do you agree with that kind of division of scenarios, first of all?

1388
00:57:44,480 --> 00:57:46,000
And then maybe you could kind of say like,

1389
00:57:46,000 --> 00:57:47,600
how likely you think each one is.

1390
00:57:47,600 --> 00:57:49,520
And obviously that one where it takes off

1391
00:57:49,520 --> 00:57:51,120
is like super hard to analyze.

1392
00:57:51,120 --> 00:57:53,440
And I also definitely think it is worth analyzing

1393
00:57:53,440 --> 00:57:55,440
the scenario where it doesn't take off.

1394
00:57:55,440 --> 00:57:59,680
But I just wanted to flag that it seems like there's a big...

1395
00:57:59,680 --> 00:58:01,120
If you talk to the AI safety people,

1396
00:58:02,000 --> 00:58:06,480
any world in which it's like we're suing Indian AI firms

1397
00:58:06,480 --> 00:58:10,960
in Indian court over IP is like a normal world in their mind, right?

1398
00:58:10,960 --> 00:58:12,960
And that's not the kind of world that they're most worried about.

1399
00:58:13,520 --> 00:58:17,440
I think that there have been some plausible sounding things

1400
00:58:17,440 --> 00:58:18,640
that have been said,

1401
00:58:18,640 --> 00:58:22,560
but I want to just kind of talk about a few technical counter-arguments,

1402
00:58:23,280 --> 00:58:27,760
mathematical or physical, that constrain what is possible.

1403
00:58:28,000 --> 00:58:32,560
And actually Martin Casado and Vijay and I are working on a long thing on this

1404
00:58:32,560 --> 00:58:34,720
where Vijay did folding at home.

1405
00:58:34,720 --> 00:58:35,600
He's a physicist.

1406
00:58:35,600 --> 00:58:38,400
Martin sold in the Syrah for a billion dollars

1407
00:58:38,400 --> 00:58:43,040
and knows a lot about how a Stuxnet-like thing could work at the systems level.

1408
00:58:43,040 --> 00:58:44,960
And I've thought about it from other angles

1409
00:58:44,960 --> 00:58:48,640
and some of the math stuff that I'll get to.

1410
00:58:48,640 --> 00:58:51,200
So for example, one thing...

1411
00:58:51,200 --> 00:58:53,440
And I'm going to give a bunch of different technical arguments

1412
00:58:53,440 --> 00:58:55,200
and then let's kind of combine them.

1413
00:58:56,160 --> 00:58:59,680
One thing that's being talked about is if you have a superintelligence,

1414
00:58:59,680 --> 00:59:02,800
it can double it for a million years

1415
00:59:02,800 --> 00:59:04,080
and then it can make one move

1416
00:59:04,080 --> 00:59:06,880
and it's going to outthink you all the time and so on and so forth.

1417
00:59:07,920 --> 00:59:12,160
Well, if you're familiar with the math of chaos or the math of turbulence,

1418
00:59:12,800 --> 00:59:17,040
there are limits to even very simple systems that you can set up

1419
00:59:17,040 --> 00:59:21,120
where they can become very unpredictable quite quickly.

1420
00:59:21,440 --> 00:59:25,520
Okay. And so you can, if you want to, engineer a system

1421
00:59:25,520 --> 00:59:29,600
where you have very rapid diversions of predictability

1422
00:59:29,600 --> 00:59:32,560
so that, I don't know, it's like the heat depth of the universe

1423
00:59:32,560 --> 00:59:34,720
before you can predict out in timestamps.

1424
00:59:35,760 --> 00:59:36,800
Do you understand what I'm saying?

1425
00:59:36,800 --> 00:59:37,680
Right?

1426
00:59:37,680 --> 00:59:40,800
This is sort of akin to like a wolf from like simple...

1427
00:59:40,800 --> 00:59:43,280
Even simple rules can generate patterns

1428
00:59:43,280 --> 00:59:46,240
such that you can't know them without literally computing them.

1429
00:59:46,800 --> 00:59:47,840
Yeah, exactly, right?

1430
00:59:47,840 --> 00:59:50,640
So at least right now, with chaos and turbulence,

1431
00:59:50,640 --> 00:59:56,400
you can get things that are extremely provably difficult to forecast

1432
00:59:56,400 --> 00:59:58,800
without actually doing it, okay?

1433
00:59:59,360 --> 01:00:00,800
You know, I can make that argument quantitative

1434
01:00:00,800 --> 01:00:02,640
but that's just something to look at, right?

1435
01:00:02,640 --> 01:00:05,200
It's almost like a Delta Epsilon challenge from calculus.

1436
01:00:05,200 --> 01:00:07,680
Like, okay, how hard do you want me to make this to predict?

1437
01:00:07,680 --> 01:00:10,800
Okay, I can set up a problem that is like that, right?

1438
01:00:10,800 --> 01:00:12,880
It's basically extreme sensitivity to initial conditions

1439
01:00:12,880 --> 01:00:15,200
lead to extreme divergence in outcomes.

1440
01:00:16,160 --> 01:00:18,720
So you could design systems to be chaotic

1441
01:00:18,720 --> 01:00:22,080
that might be AI immune because they can't be forecasted that well.

1442
01:00:22,080 --> 01:00:23,520
You have to kind of react to them in real time.

1443
01:00:24,240 --> 01:00:26,240
The ultimate version of this is not even a chaotic system.

1444
01:00:26,240 --> 01:00:30,000
It's a cryptographic system where I've got a whole slide deck on this,

1445
01:00:30,000 --> 01:00:33,600
how AI makes everything fake, easy to fake.

1446
01:00:33,600 --> 01:00:37,440
Crypto makes it hard to fake again, right?

1447
01:00:37,440 --> 01:00:40,160
Because crypto in the broader sense of cryptography,

1448
01:00:40,160 --> 01:00:41,520
but also in the narrower sense,

1449
01:00:41,520 --> 01:00:46,720
I think crypto is cryptography as the internet is to computer science.

1450
01:00:46,800 --> 01:00:49,360
It's like the primary place where all this stuff is applied,

1451
01:00:49,360 --> 01:00:51,360
but obviously it's not the equivalent, okay?

1452
01:00:51,360 --> 01:00:54,400
And AI can fake an image, but it can't fake a digital signature

1453
01:00:54,400 --> 01:00:57,200
unless it can break certain math, you know,

1454
01:00:57,200 --> 01:00:59,040
and so it's sort of like a, you know,

1455
01:00:59,040 --> 01:01:00,880
solve factors each problem or something like that.

1456
01:01:00,880 --> 01:01:03,920
So cryptography is another mathematical thing that constrains AI.

1457
01:01:03,920 --> 01:01:05,680
Similar to chaos and turbulence,

1458
01:01:05,680 --> 01:01:10,560
it constrains how much an AI can infer things.

1459
01:01:10,560 --> 01:01:13,120
You can't statistically infer it, okay?

1460
01:01:13,120 --> 01:01:15,920
You need to actually have the private key to solve that equation.

1461
01:01:15,920 --> 01:01:17,120
So that is another math.

1462
01:01:17,120 --> 01:01:19,440
So I'm going to rules of math, right?

1463
01:01:20,160 --> 01:01:22,800
Math is very powerful because you can make proofs

1464
01:01:22,800 --> 01:01:26,160
that will work no matter what devices we come up with, okay?

1465
01:01:26,160 --> 01:01:28,080
You start to put an AI in a cage.

1466
01:01:28,080 --> 01:01:29,520
It can't predict beyond a certain amount

1467
01:01:29,520 --> 01:01:31,760
because of chaos and turbulence, math.

1468
01:01:31,760 --> 01:01:35,600
It cannot solve certain equations unless it has a private key

1469
01:01:35,600 --> 01:01:38,480
is because of what we know about cryptography, math, okay?

1470
01:01:39,040 --> 01:01:41,280
Again, if somebody proves P equals NP,

1471
01:01:41,280 --> 01:01:42,480
some of this stuff breaks down,

1472
01:01:42,480 --> 01:01:44,800
but this is within the bounds of our mathematical knowledge right now.

1473
01:01:45,360 --> 01:01:48,240
Physics wise, physical friction exists.

1474
01:01:49,200 --> 01:01:50,800
A lot of physical friction exists.

1475
01:01:51,600 --> 01:01:58,880
And a huge amount of the writing on AI assumes by guys like Elias

1476
01:01:58,880 --> 01:02:01,120
or who I like, I don't dislike it, you know,

1477
01:02:01,120 --> 01:02:04,320
but it is extremely,

1478
01:02:04,320 --> 01:02:07,280
there's two things that really stick out to me about it.

1479
01:02:07,280 --> 01:02:09,840
First is extremely theoretical and not empirical.

1480
01:02:10,720 --> 01:02:13,920
And second, extremely Abrahamic rather than Darmic or Sinic.

1481
01:02:15,040 --> 01:02:17,120
Okay, why theoretical and not empirical?

1482
01:02:17,760 --> 01:02:22,000
It's not trivial to turn something from the computer

1483
01:02:22,000 --> 01:02:24,080
into a real world thing, okay?

1484
01:02:24,080 --> 01:02:27,760
One of the biggest gaps in all of this thinking

1485
01:02:27,760 --> 01:02:29,920
is what are the sensors in actuators?

1486
01:02:30,800 --> 01:02:33,600
Okay, because like if you actually build, you know,

1487
01:02:33,600 --> 01:02:36,560
I've built in industrial robot systems that, you know,

1488
01:02:36,560 --> 01:02:40,080
10 years ago, you know, a genome sequencing lab with robots,

1489
01:02:40,080 --> 01:02:41,120
that's hard.

1490
01:02:41,120 --> 01:02:43,200
That's physical friction, okay?

1491
01:02:43,200 --> 01:02:48,720
And a lot of the AI scenarios seem to basically say,

1492
01:02:48,720 --> 01:02:51,280
oh, it's going to be a self-programming Stuxnet

1493
01:02:51,280 --> 01:02:54,160
that's going to escape and live off the land

1494
01:02:54,160 --> 01:02:57,440
and hypnotize people into doing things, okay?

1495
01:02:58,080 --> 01:03:00,960
Now, each of those is actually really, really difficult steps.

1496
01:03:01,520 --> 01:03:04,000
First is self-programming Stuxnet.

1497
01:03:04,000 --> 01:03:07,280
Like this would have to be a computer virus

1498
01:03:07,280 --> 01:03:10,640
that can live on any device, despite the fact

1499
01:03:10,640 --> 01:03:13,200
that Apple or Google can push a software update

1500
01:03:13,200 --> 01:03:15,600
to a billion devices, right?

1501
01:03:15,600 --> 01:03:18,880
A few executives coordinating almost certainly can,

1502
01:03:18,880 --> 01:03:21,760
I mean, the off switch exists, right?

1503
01:03:21,760 --> 01:03:24,080
Like this is actually like the core thing.

1504
01:03:24,080 --> 01:03:27,600
Lots of AI safety guys get themselves into the mindset

1505
01:03:27,600 --> 01:03:29,520
that the off switch doesn't exist.

1506
01:03:30,080 --> 01:03:30,640
But guess what?

1507
01:03:30,640 --> 01:03:32,800
There's almost nothing living that we haven't been able to kill,

1508
01:03:34,160 --> 01:03:34,640
right?

1509
01:03:34,640 --> 01:03:36,080
Like can we kill it?

1510
01:03:36,080 --> 01:03:38,800
This thing exists and this is getting back to living off the land.

1511
01:03:39,680 --> 01:03:41,680
Even if you had like something that could solve

1512
01:03:41,680 --> 01:03:43,280
some other technical problems that it'll get to,

1513
01:03:44,560 --> 01:03:47,120
it exists as an electromagnetic wave kind of thing

1514
01:03:47,120 --> 01:03:50,560
on a certain, you know, on chips and so on and so forth.

1515
01:03:50,560 --> 01:03:52,240
It's taking it out in the environment

1516
01:03:52,240 --> 01:03:55,440
is like putting a really smart human into outer space, right?

1517
01:03:55,440 --> 01:03:57,120
Your body just explodes and you die.

1518
01:03:57,680 --> 01:03:58,960
It doesn't matter how smart you are.

1519
01:03:59,600 --> 01:04:01,600
That strength on this axis,

1520
01:04:01,600 --> 01:04:03,680
where you're weak on this axis and, you know,

1521
01:04:03,680 --> 01:04:04,800
it's just strength on the X axis,

1522
01:04:04,800 --> 01:04:06,720
not strength on the Y or the Z axis.

1523
01:04:06,800 --> 01:04:08,880
An AI outside, you know, pour water on it.

1524
01:04:08,880 --> 01:04:12,720
This is, you know, this is why I mean the 50 IQ, 150 IQ thing.

1525
01:04:13,280 --> 01:04:15,040
You know, the 150 IQ way of saying it is,

1526
01:04:15,040 --> 01:04:16,640
it's strong on this X and weak on this X.

1527
01:04:16,640 --> 01:04:20,000
And the 50 IQ way is pour water on it, disconnect it,

1528
01:04:20,000 --> 01:04:22,160
you know, turn the power off.

1529
01:04:22,160 --> 01:04:23,040
Okay, right?

1530
01:04:23,600 --> 01:04:26,560
Like it'll, it'll be very difficult to build a system

1531
01:04:26,560 --> 01:04:28,480
where you literally cannot turn it off.

1532
01:04:28,480 --> 01:04:31,440
The closest thing we have to that is actually not Stuxnet.

1533
01:04:31,440 --> 01:04:32,160
It's Bitcoin.

1534
01:04:32,800 --> 01:04:37,920
And Bitcoin only exists because millions of humans keep it going.

1535
01:04:39,200 --> 01:04:41,600
So you, you need, so that gets the second point,

1536
01:04:41,600 --> 01:04:44,640
living off the land for an AI to live off the land,

1537
01:04:45,440 --> 01:04:47,200
meaning without human cooperation.

1538
01:04:48,080 --> 01:04:50,480
Okay, that's the next Turing threshold.

1539
01:04:50,480 --> 01:04:52,880
An AI to live without human cooperation.

1540
01:04:52,880 --> 01:04:59,680
It would need to be able to control robots sufficient to dig or out of the ground,

1541
01:04:59,680 --> 01:05:02,960
set up data centers and generators and connect them

1542
01:05:02,960 --> 01:05:06,720
and defend that against human attack, literally a terminator scenario.

1543
01:05:06,720 --> 01:05:09,120
Okay, that's a big leap in terms of, I mean,

1544
01:05:09,120 --> 01:05:10,320
is it completely impossible?

1545
01:05:10,320 --> 01:05:13,040
I can't say it's completely impossible, but it's not happening tomorrow.

1546
01:05:13,600 --> 01:05:15,440
No matter what your AI timelines are,

1547
01:05:15,440 --> 01:05:19,440
you would need to have like a billion or hundreds of millions

1548
01:05:19,440 --> 01:05:25,600
of internet connected autonomous robots that this Stuxnet AI could hijack.

1549
01:05:25,600 --> 01:05:31,440
They were sufficient to carve or out of the earth and set up data centers

1550
01:05:31,440 --> 01:05:33,200
and make the AI duplicate.

1551
01:05:33,200 --> 01:05:34,240
We're not there.

1552
01:05:34,240 --> 01:05:36,240
That's a huge amount of physical friction.

1553
01:05:36,240 --> 01:05:39,440
That's AI operating without a human to make itself propagate, right?

1554
01:05:39,440 --> 01:05:43,760
A human doesn't need the cooperation of a lizard to self-replicate.

1555
01:05:44,560 --> 01:05:46,240
For an AI to replicate right now,

1556
01:05:46,240 --> 01:05:50,000
it would need the cooperation of a human in some sense,

1557
01:05:50,000 --> 01:05:52,080
because otherwise those humans can kill it

1558
01:05:52,080 --> 01:05:56,160
because there's not that many different pieces of operating systems around the world.

1559
01:05:56,160 --> 01:05:59,200
I'm just talking about the practical constraints of our current world, right?

1560
01:05:59,200 --> 01:06:04,960
Actually existing reality, not AI safety guys reality where all these things don't exist.

1561
01:06:04,960 --> 01:06:07,360
There's just a few operating systems, just a few countries.

1562
01:06:07,360 --> 01:06:11,760
If everybody is going with torches and search lights through the internet,

1563
01:06:11,760 --> 01:06:13,680
it's very hard for a virus to continue.

1564
01:06:14,880 --> 01:06:21,760
So A, on the practicalities, there's the technical stuff with chaos and turbulence

1565
01:06:21,760 --> 01:06:25,600
and with cryptography itself where AI can't predict and it can't solve certain equations.

1566
01:06:26,400 --> 01:06:30,240
B, on the physical difficulties, it probably...

1567
01:06:30,240 --> 01:06:31,920
I mean, like to be a Stuxnet,

1568
01:06:31,920 --> 01:06:33,600
Microsoft and Google and so on could kill it.

1569
01:06:33,600 --> 01:06:34,960
The off switch exists.

1570
01:06:34,960 --> 01:06:36,160
Can it live off the land?

1571
01:06:36,160 --> 01:06:40,640
No, it cannot because it doesn't have drones to mine or stuff out of the ground.

1572
01:06:44,400 --> 01:06:46,560
Can it exist without humans?

1573
01:06:46,560 --> 01:06:48,400
Can it be this hypnotizing thing?

1574
01:06:48,400 --> 01:06:50,400
Okay, so the hypnotizing thing, by the way,

1575
01:06:50,400 --> 01:06:53,680
this is one of the things that's the most hilarious self-fulfilling prophecy in my view.

1576
01:06:55,520 --> 01:06:58,000
And no offense to anybody who's listening to this podcast,

1577
01:06:58,000 --> 01:07:02,240
but I think the absolutely dumbest kind of tweet that I've seen on AI is,

1578
01:07:02,800 --> 01:07:05,440
I typed this in and, oh my God, it told me this.

1579
01:07:06,160 --> 01:07:10,640
Like, I asked it how to make sarin gas and it told me X or whatever, right?

1580
01:07:10,640 --> 01:07:12,800
That's just a search engine, okay?

1581
01:07:14,000 --> 01:07:17,520
What basically a lot of these people are doing is they're saying,

1582
01:07:17,600 --> 01:07:20,720
what if there were people out there that were so impressionable

1583
01:07:21,360 --> 01:07:27,040
that they would type things into an AI and follow it as if they were hearing voices?

1584
01:07:27,040 --> 01:07:30,960
And that's actually not the model or whatever that's doing it.

1585
01:07:30,960 --> 01:07:33,760
That's like this AI cult that has evolved around the world,

1586
01:07:33,760 --> 01:07:37,680
like a Aum Shinrikyo that hears voices and does the sarin gas.

1587
01:07:39,200 --> 01:07:42,560
The point is an AI can't just hypnotize people.

1588
01:07:42,560 --> 01:07:44,480
Those people have to participate in it.

1589
01:07:44,480 --> 01:07:47,040
They're typing things into the machine or whatever, okay?

1590
01:07:47,520 --> 01:07:50,240
Now, you might say, all right, let's project out a few years.

1591
01:07:50,240 --> 01:07:54,960
In a few years, what you have is you have an AI that is not just text,

1592
01:07:54,960 --> 01:07:56,880
but it appears as Jesus.

1593
01:07:56,880 --> 01:07:58,720
What would AI Jesus do?

1594
01:07:58,720 --> 01:08:00,240
What would AI Lee Kuan Yew do?

1595
01:08:00,240 --> 01:08:01,680
What would AI George Washington do?

1596
01:08:01,680 --> 01:08:03,680
So it appears as 3D, okay?

1597
01:08:03,680 --> 01:08:04,800
So it's generating that.

1598
01:08:05,360 --> 01:08:09,520
It speaks in your language and in a voice.

1599
01:08:09,520 --> 01:08:12,560
It knows the history of your whole culture, okay?

1600
01:08:12,560 --> 01:08:14,160
That would be very convincing.

1601
01:08:14,160 --> 01:08:15,280
Absolutely be very convincing.

1602
01:08:15,920 --> 01:08:18,000
But it still can't exist without human programmers

1603
01:08:18,880 --> 01:08:21,120
who are like the priests tending this AI God,

1604
01:08:21,120 --> 01:08:24,240
whether it's AI Jesus or AI Lee Kuan Yew or something like that.

1605
01:08:24,240 --> 01:08:27,280
The thing about the hypnotization thing that I really want to poke on that,

1606
01:08:27,280 --> 01:08:29,280
are you familiar with the concept of the principal agent problem?

1607
01:08:29,840 --> 01:08:34,960
Basically, every time you've got like a CEO and a worker,

1608
01:08:34,960 --> 01:08:37,840
or you have an LP and a VC,

1609
01:08:37,840 --> 01:08:42,320
or you have an employer and a contractor,

1610
01:08:42,320 --> 01:08:45,040
every edge there, there are four possibilities.

1611
01:08:45,040 --> 01:08:47,040
In a 2x2 matrix.

1612
01:08:47,040 --> 01:08:51,040
Win-win, win-lose, lose-win, lose-lose, okay?

1613
01:08:52,240 --> 01:08:56,400
So for example, when somebody joins a tech startup,

1614
01:08:56,400 --> 01:08:59,520
the CEO makes a lot of money and so does a worker, okay?

1615
01:08:59,520 --> 01:09:00,640
That's win-win.

1616
01:09:00,640 --> 01:09:02,560
Lose-lose is they both lose money.

1617
01:09:02,560 --> 01:09:04,960
Win-lose is the CEO makes money and the employee doesn't.

1618
01:09:05,600 --> 01:09:07,600
Lose-win is the company fails,

1619
01:09:07,600 --> 01:09:09,600
but the employee got paid a very high salary.

1620
01:09:09,600 --> 01:09:12,160
So what equity does is it aligns people.

1621
01:09:12,160 --> 01:09:14,240
That's where the concept of alignment comes from.

1622
01:09:14,240 --> 01:09:16,960
It aligns people to the upper left corner of win-win.

1623
01:09:16,960 --> 01:09:19,520
That's when you have one CEO and one employee.

1624
01:09:19,520 --> 01:09:22,640
When you have one CEO and two employees,

1625
01:09:22,640 --> 01:09:24,480
you don't have two squared outcomes,

1626
01:09:24,480 --> 01:09:25,920
you have two cubed outcomes,

1627
01:09:25,920 --> 01:09:28,400
because you have win-win-win, win-win-lose,

1628
01:09:28,400 --> 01:09:30,240
win-lose-lose, et cetera, right?

1629
01:09:30,240 --> 01:09:32,720
Because all three people can be win or lose.

1630
01:09:32,720 --> 01:09:35,520
Because CEO can be win or lose, employee can be win or lose,

1631
01:09:35,520 --> 01:09:37,280
employee number two can be win or lose.

1632
01:09:37,280 --> 01:09:39,040
If you have n people, rather than three people,

1633
01:09:39,040 --> 01:09:40,320
you have two to the n possible outcomes

1634
01:09:40,320 --> 01:09:43,440
and you have essentially a 2x2x2x2x2x2xn

1635
01:09:43,440 --> 01:09:44,880
hyper-cube of possibilities.

1636
01:09:44,880 --> 01:09:47,040
Okay, it's literally just two dimensions on each axis.

1637
01:09:48,000 --> 01:09:50,880
There's tons of possible defecting kinds of things that happen there.

1638
01:09:50,880 --> 01:09:52,400
So that's why in a large company,

1639
01:09:52,400 --> 01:09:54,800
there's lose-win coalitions that happen,

1640
01:09:54,800 --> 01:09:57,040
where m people gang up on the other k people

1641
01:09:57,040 --> 01:09:58,320
and they win what the other people lose.

1642
01:09:58,320 --> 01:09:59,920
That's how politics happens.

1643
01:09:59,920 --> 01:10:02,160
When you've got a startup that's driven by equity

1644
01:10:02,160 --> 01:10:03,440
and the biggest payoff,

1645
01:10:03,440 --> 01:10:04,480
people don't have to try to think,

1646
01:10:04,480 --> 01:10:06,240
okay, well, I make more money by politics,

1647
01:10:06,240 --> 01:10:08,240
we'll make more money by the win-win-win-win-win column

1648
01:10:08,240 --> 01:10:10,320
because the exit makes everybody make the most money.

1649
01:10:10,320 --> 01:10:12,160
That's actually how the open AI people

1650
01:10:12,160 --> 01:10:13,440
were able to coordinate around,

1651
01:10:13,440 --> 01:10:15,200
we want an $80 billion company,

1652
01:10:15,200 --> 01:10:16,960
the economics help find the sell

1653
01:10:16,960 --> 01:10:18,560
that was actually the most beneficial to all of them,

1654
01:10:18,560 --> 01:10:19,840
help them coordinate, okay?

1655
01:10:19,840 --> 01:10:21,520
So you search that hyper-cube, okay.

1656
01:10:21,520 --> 01:10:24,160
That's a point of equity as lining.

1657
01:10:24,160 --> 01:10:26,320
Still, despite all of this,

1658
01:10:27,040 --> 01:10:29,440
that's one of our best mechanisms for coordinating

1659
01:10:29,440 --> 01:10:32,400
large numbers of people in the principal agent problem.

1660
01:10:32,400 --> 01:10:35,600
Despite all of this, the possibility exists

1661
01:10:36,400 --> 01:10:39,600
for any of these people to win while the others lose,

1662
01:10:39,600 --> 01:10:40,480
right, with me so far?

1663
01:10:40,480 --> 01:10:41,840
And I'll explain why this is important.

1664
01:10:41,920 --> 01:10:45,520
What that means is those 1,000 employees of the CEO

1665
01:10:46,160 --> 01:10:49,840
are their own agents with their own payoff functions

1666
01:10:49,840 --> 01:10:52,560
that are not perfectly aligned with the CEO's payoff function.

1667
01:10:53,120 --> 01:10:55,120
As such, there are scenarios

1668
01:10:55,120 --> 01:10:58,240
under which they will defect and do other things, okay?

1669
01:10:58,240 --> 01:11:02,560
The only way they become like actual limbs,

1670
01:11:02,560 --> 01:11:06,960
see, my hand is not an agent of its own.

1671
01:11:06,960 --> 01:11:08,160
It lives or dies with me.

1672
01:11:08,720 --> 01:11:11,440
Therefore, it does exactly what I'm saying at this time.

1673
01:11:11,440 --> 01:11:12,480
I tell it to go up, it goes up.

1674
01:11:12,480 --> 01:11:13,760
I tell it to go down, it goes down.

1675
01:11:13,760 --> 01:11:14,880
Sideway is sideways, right?

1676
01:11:15,520 --> 01:11:17,360
An employee is not like that.

1677
01:11:17,360 --> 01:11:19,840
They will do this and this and sideways, sideways,

1678
01:11:19,840 --> 01:11:21,360
up to a certain point.

1679
01:11:21,360 --> 01:11:23,520
And if you have them do something

1680
01:11:23,520 --> 01:11:24,880
that's extremely against their interests,

1681
01:11:24,880 --> 01:11:26,400
they will not do your action.

1682
01:11:26,400 --> 01:11:27,680
Did you understand my point?

1683
01:11:27,680 --> 01:11:31,120
Okay, that is the difference between an AI hypnotizing humans

1684
01:11:31,760 --> 01:11:33,200
versus an AI controlling drones.

1685
01:11:33,920 --> 01:11:36,160
AI controlling drones is like your hands.

1686
01:11:36,160 --> 01:11:37,520
They're actually pieces of your body.

1687
01:11:37,520 --> 01:11:38,800
There's no defecting.

1688
01:11:38,800 --> 01:11:40,160
There's no loose wind.

1689
01:11:40,160 --> 01:11:41,280
They have no mind of their own.

1690
01:11:41,280 --> 01:11:43,120
They're literally taking instructions, okay?

1691
01:11:43,120 --> 01:11:43,920
They have no payoff function.

1692
01:11:43,920 --> 01:11:46,240
They will kill themselves for the hoard, right?

1693
01:11:47,200 --> 01:11:49,440
An AI hypnotizing humans has a thousand

1694
01:11:49,440 --> 01:11:51,920
principal Asian problems for every thousand humans.

1695
01:11:52,560 --> 01:11:55,040
And it has to incentivize them to continue

1696
01:11:55,040 --> 01:11:56,320
and has to generate huge payoffs.

1697
01:11:56,320 --> 01:11:57,200
It's like an AI CEO.

1698
01:11:57,200 --> 01:11:59,600
That's really hard to do, right?

1699
01:11:59,600 --> 01:12:02,880
The history of evolution shows us how hard it is

1700
01:12:02,880 --> 01:12:04,480
to coordinate multicellular organisms.

1701
01:12:04,480 --> 01:12:06,320
You have to make them all live or die as one.

1702
01:12:06,320 --> 01:12:08,160
Then you get something along these lines.

1703
01:12:08,160 --> 01:12:10,240
Like an ant colony can coordinate like that

1704
01:12:10,240 --> 01:12:13,280
because if the queen doesn't reproduce all the ants,

1705
01:12:13,280 --> 01:12:15,280
it doesn't matter what they're having sort of genetic material.

1706
01:12:15,280 --> 01:12:15,600
Okay?

1707
01:12:16,160 --> 01:12:18,960
We are not currently set up for those humans

1708
01:12:18,960 --> 01:12:21,360
to not be able to reproduce unless the AI reproduces.

1709
01:12:22,720 --> 01:12:25,040
Do I think we eventually get to a configuration like that?

1710
01:12:25,040 --> 01:12:25,360
Maybe.

1711
01:12:26,640 --> 01:12:30,320
Where you have an AI brain is at the center of civilization

1712
01:12:30,320 --> 01:12:32,400
and it's coordinating all the people around it.

1713
01:12:32,400 --> 01:12:35,280
And every civilization that makes it

1714
01:12:35,280 --> 01:12:37,520
is capable of crowdfunding and operating its own AI.

1715
01:12:38,000 --> 01:12:40,800
That gets me to my other critique of the AI safety guys.

1716
01:12:40,800 --> 01:12:42,080
I mentioned that the first critique

1717
01:12:42,080 --> 01:12:43,920
is very theoretical rather than empirical.

1718
01:12:43,920 --> 01:12:45,600
And the second critique is they're Abrahamic

1719
01:12:45,600 --> 01:12:48,000
rather than Darmic or Sinic.

1720
01:12:48,000 --> 01:12:48,320
Okay?

1721
01:12:48,960 --> 01:12:51,600
And, you know, our background culture influences things

1722
01:12:51,600 --> 01:12:53,280
in ways we don't even think about.

1723
01:12:53,280 --> 01:12:55,520
So much of the paperclip thinking

1724
01:12:55,520 --> 01:12:58,880
is like a vengeful God will turn you into pillars of salt,

1725
01:12:58,880 --> 01:13:01,120
except it's a vengeful, you know, AI God

1726
01:13:01,120 --> 01:13:02,800
will turn you into paperclips.

1727
01:13:02,800 --> 01:13:03,040
Okay?

1728
01:13:03,680 --> 01:13:06,320
The polytheistic model of many gods,

1729
01:13:06,320 --> 01:13:07,520
as opposed to one God is,

1730
01:13:07,520 --> 01:13:09,440
we're all going to have our own AI gods

1731
01:13:09,440 --> 01:13:10,560
and there'll be war of the gods,

1732
01:13:11,600 --> 01:13:13,040
like Zeus and Hera and so on.

1733
01:13:13,040 --> 01:13:14,560
That's the closest Western version,

1734
01:13:14,560 --> 01:13:16,400
you know, the paganism that predated,

1735
01:13:16,400 --> 01:13:17,760
you know, Abrahamic religions.

1736
01:13:17,760 --> 01:13:19,120
But that's still there in India.

1737
01:13:19,120 --> 01:13:20,560
That's still how Indians think.

1738
01:13:20,560 --> 01:13:21,920
That's why India is sort of,

1739
01:13:21,920 --> 01:13:23,600
people have gotten so woke that they don't even make

1740
01:13:23,600 --> 01:13:25,520
large scale cultural generalizations anymore.

1741
01:13:25,520 --> 01:13:29,520
But it's true that India is just culturally more amenable

1742
01:13:29,520 --> 01:13:32,480
to decentralization, to, you know,

1743
01:13:32,480 --> 01:13:35,360
multiple gods rather than one God and one state.

1744
01:13:35,360 --> 01:13:35,600
Okay?

1745
01:13:36,880 --> 01:13:38,800
And then the Chinese model is yet the opposite.

1746
01:13:38,800 --> 01:13:40,240
Like they have like, I mean, of course,

1747
01:13:40,240 --> 01:13:41,920
they have their tech entrepreneurs and so on.

1748
01:13:41,920 --> 01:13:43,840
But they're, if India is more decentralized,

1749
01:13:43,840 --> 01:13:44,720
China is more centralized.

1750
01:13:44,720 --> 01:13:46,800
They have like one government and one leader

1751
01:13:46,800 --> 01:13:48,080
for the entire civilization.

1752
01:13:48,080 --> 01:13:48,320
Okay?

1753
01:13:49,200 --> 01:13:52,160
And that the biggest thing that China has done

1754
01:13:52,160 --> 01:13:55,520
over the last 20 or 30 years is they've taken various,

1755
01:13:55,520 --> 01:13:57,440
you know, U.S. things and they've made sure

1756
01:13:57,440 --> 01:13:59,040
that they have their own Chinese version

1757
01:13:59,040 --> 01:13:59,760
where they have root.

1758
01:14:00,320 --> 01:14:01,760
So they take U.S. social media

1759
01:14:01,760 --> 01:14:04,000
and they made sure they had root over Sina Weibo.

1760
01:14:04,000 --> 01:14:05,440
Okay?

1761
01:14:05,440 --> 01:14:06,960
They make sure they have their own Chinese version

1762
01:14:06,960 --> 01:14:08,400
of electric cars, the most Chinese version.

1763
01:14:08,400 --> 01:14:11,440
So the private keys, in a sense, are with G.

1764
01:14:11,440 --> 01:14:14,720
So that means that they also, at a minimum,

1765
01:14:14,720 --> 01:14:16,080
you combine these two things,

1766
01:14:16,080 --> 01:14:18,560
you're at a minimum going to get polytheistic AI

1767
01:14:18,560 --> 01:14:20,000
of the U.S. and Chinese varieties.

1768
01:14:20,960 --> 01:14:23,120
And then you add the Indian version on it

1769
01:14:23,120 --> 01:14:24,320
and you're going to get quite a few

1770
01:14:24,320 --> 01:14:26,080
of these different AIs around there.

1771
01:14:26,080 --> 01:14:27,360
And then you have War of the Gods

1772
01:14:27,360 --> 01:14:30,880
where maybe they are good at coordinating humans

1773
01:14:30,880 --> 01:14:34,000
who take instructions from them,

1774
01:14:34,000 --> 01:14:35,440
but they can't live without the humans

1775
01:14:36,080 --> 01:14:37,600
and the humans are giving input to them.

1776
01:14:38,240 --> 01:14:39,120
That's a series of things.

1777
01:14:39,120 --> 01:14:40,240
I could probably make that clearer

1778
01:14:40,240 --> 01:14:42,560
if I just laid it out in bullets in an essay,

1779
01:14:42,560 --> 01:14:45,120
but just to recap it, A, technical reasons

1780
01:14:45,120 --> 01:14:48,640
like chaos, turbulence, cryptography,

1781
01:14:48,640 --> 01:14:51,440
why AI is limited in its ability to predict time frames

1782
01:14:51,440 --> 01:14:54,560
and to solve equations, B, practical limits.

1783
01:14:54,560 --> 01:14:57,120
And AI cannot easily be a Stuxnet

1784
01:14:57,120 --> 01:14:59,920
because Microsoft and Google and Apple

1785
01:14:59,920 --> 01:15:02,560
can install software on a billion devices

1786
01:15:02,560 --> 01:15:03,920
and just kill it, right?

1787
01:15:03,920 --> 01:15:06,800
Like basically guys with torches come, all right?

1788
01:15:06,800 --> 01:15:08,960
It can't easily live off the land without humans

1789
01:15:08,960 --> 01:15:10,880
because they would need hundreds of millions

1790
01:15:10,880 --> 01:15:12,720
of autonomous robots out there to control,

1791
01:15:12,720 --> 01:15:15,040
to mine the ore and set the data centers.

1792
01:15:15,600 --> 01:15:17,520
It can't just hypnotize humans

1793
01:15:17,520 --> 01:15:19,120
like it can control drones

1794
01:15:19,120 --> 01:15:20,640
because of the principal agent problem

1795
01:15:20,640 --> 01:15:22,240
and the degree of human defection.

1796
01:15:22,240 --> 01:15:23,600
To make those humans do that,

1797
01:15:23,600 --> 01:15:26,160
you'd have to have such massive alignment

1798
01:15:26,160 --> 01:15:27,440
between the AI and humans

1799
01:15:27,440 --> 01:15:29,040
that the humans all know they'll die

1800
01:15:29,040 --> 01:15:30,400
if the AI dies and vies versa.

1801
01:15:30,400 --> 01:15:31,280
We're not there.

1802
01:15:31,280 --> 01:15:32,960
Maybe we'll be there in like, I don't know,

1803
01:15:32,960 --> 01:15:34,480
end number of years, but not for a while.

1804
01:15:34,480 --> 01:15:38,240
That's a total change in like how states are organized, okay?

1805
01:15:39,200 --> 01:15:41,440
Finally, let me just talk about the physics a little bit more.

1806
01:15:42,720 --> 01:15:44,320
There's a lot of stuff which is talked about

1807
01:15:44,320 --> 01:15:46,720
at a very sci-fi book level of,

1808
01:15:46,720 --> 01:15:49,040
it'll just invent nanomedicine and nanotech

1809
01:15:49,040 --> 01:15:51,120
and kill us all and so on and so forth.

1810
01:15:51,120 --> 01:15:52,960
Now look, I like Robert Freitas,

1811
01:15:52,960 --> 01:15:55,200
obviously Richard Feynman's a genius and so on and so forth,

1812
01:15:55,760 --> 01:15:58,080
but nanotech somehow hasn't been invented yet.

1813
01:16:00,000 --> 01:16:02,560
Meaning that there's a lot of chemists

1814
01:16:02,560 --> 01:16:03,760
that have worked in this area.

1815
01:16:05,680 --> 01:16:08,320
A lot of nanotech is like rebranded chemistry

1816
01:16:08,320 --> 01:16:10,400
because those are the molecular machines.

1817
01:16:10,400 --> 01:16:14,240
For example, DNA polymerase or ribosome,

1818
01:16:14,240 --> 01:16:15,280
those are molecular machines

1819
01:16:15,280 --> 01:16:18,000
that we can get to work at that scale, the evolved ones.

1820
01:16:18,640 --> 01:16:20,480
To my knowledge, and I may be wrong about this,

1821
01:16:20,480 --> 01:16:22,160
I haven't looked at it very, very recently,

1822
01:16:22,800 --> 01:16:26,640
we haven't actually been able to make artificial replicators

1823
01:16:26,640 --> 01:16:28,080
of the stuff that they're talking about,

1824
01:16:28,160 --> 01:16:31,280
which means it's possible that there's some practical difficulty

1825
01:16:31,280 --> 01:16:34,160
that intervened between Feynman and Freitas

1826
01:16:34,160 --> 01:16:35,440
and so on's calculations.

1827
01:16:36,000 --> 01:16:39,120
Just a sheer fact that those books have came out decades ago

1828
01:16:39,120 --> 01:16:40,720
and no progress has been made,

1829
01:16:40,720 --> 01:16:42,160
indicates that maybe there's a roadblock

1830
01:16:42,160 --> 01:16:43,680
that wasn't contemplated.

1831
01:16:43,680 --> 01:16:46,160
So you can't just click your fingers and say, boom,

1832
01:16:46,160 --> 01:16:47,840
nanomess, and it's sort of like clicking your fingers

1833
01:16:47,840 --> 01:16:49,280
and saying, boom, time travel.

1834
01:16:50,480 --> 01:16:52,880
Nanomess and exist, that was a good poke

1835
01:16:52,880 --> 01:16:54,960
that I had a while ago in a conversation like this

1836
01:16:54,960 --> 01:16:57,440
where the AI guy, AI safety guy on the other side was like,

1837
01:16:57,440 --> 01:16:59,120
well, time travel, that's too implausible.

1838
01:16:59,120 --> 01:17:02,320
I'm like, yeah, but you're waiting on the nanotech thing

1839
01:17:02,320 --> 01:17:04,000
you're thinking is like here,

1840
01:17:04,000 --> 01:17:06,240
and you're making so many assumptions there

1841
01:17:06,240 --> 01:17:08,800
that I want to actually see some more work there.

1842
01:17:08,800 --> 01:17:10,000
I want to actually see that nanotech

1843
01:17:10,000 --> 01:17:12,080
is actually more possible than you think it is.

1844
01:17:13,040 --> 01:17:15,360
As for, oh, we just need to mix things in a beaker

1845
01:17:15,360 --> 01:17:17,440
and make a virus and so on.

1846
01:17:17,440 --> 01:17:18,960
You know what is really, really good

1847
01:17:18,960 --> 01:17:22,080
at defending against novel viruses?

1848
01:17:22,080 --> 01:17:24,640
Like the human immune, that's something that's within envelope.

1849
01:17:24,720 --> 01:17:27,680
Right, like you have evolved to not die

1850
01:17:27,680 --> 01:17:29,120
and to fight off viruses.

1851
01:17:29,120 --> 01:17:31,920
Is it possible that maybe you could make some super virus?

1852
01:17:31,920 --> 01:17:36,000
I mean, maybe, but again, like humans are really good

1853
01:17:36,000 --> 01:17:38,560
and the immune system is really good at that kind of thing.

1854
01:17:38,560 --> 01:17:40,160
That is what we're set up to do, right,

1855
01:17:40,160 --> 01:17:42,560
to adapt to that billions of years of evolution

1856
01:17:42,560 --> 01:17:43,680
being set up for them.

1857
01:17:43,680 --> 01:17:47,680
Physical constraints are not really contemplated

1858
01:17:47,680 --> 01:17:49,200
when people talk about these super powerful

1859
01:17:49,200 --> 01:17:51,200
mathematical constraints, practical constraints

1860
01:17:51,200 --> 01:17:52,480
are not contemplated.

1861
01:17:52,480 --> 01:17:53,440
And I could give more,

1862
01:17:53,440 --> 01:17:54,480
but I think that was a lot right there.

1863
01:17:54,480 --> 01:17:55,680
Let me pause here.

1864
01:17:55,680 --> 01:17:58,240
Yeah, let me try to steal man a few things.

1865
01:17:58,880 --> 01:18:02,400
And then I do think it's before too long,

1866
01:18:02,400 --> 01:18:06,000
I want to kind of get back to the somewhat less,

1867
01:18:06,000 --> 01:18:07,760
you know, radically transformative scenarios

1868
01:18:07,760 --> 01:18:09,520
and ask a few follow up questions on that too.

1869
01:18:09,520 --> 01:18:12,800
But I think for starters, I would say the sort of Eliezer,

1870
01:18:12,800 --> 01:18:15,200
you know, he's updated his thinking over time as well.

1871
01:18:15,200 --> 01:18:17,360
And I would say probably doesn't get quite enough credit for it

1872
01:18:17,360 --> 01:18:19,680
because he's definitely on record, you know,

1873
01:18:19,680 --> 01:18:22,320
repeatedly saying, yeah, I was kind of expecting

1874
01:18:22,320 --> 01:18:24,960
more something from like the deep mind school to pop out

1875
01:18:24,960 --> 01:18:29,360
and be, you know, wildly overpowered very quickly.

1876
01:18:29,360 --> 01:18:32,960
And on the contrary, it seems like we're in more of a slow

1877
01:18:32,960 --> 01:18:36,160
takeoff type of scenario where, you know, we've got these,

1878
01:18:36,160 --> 01:18:39,040
again, like super high surface area kind of suck up

1879
01:18:39,040 --> 01:18:41,840
all the knowledge, gradually get better at everything.

1880
01:18:41,840 --> 01:18:43,120
Some surprises in there, you know,

1881
01:18:43,120 --> 01:18:45,200
certainly some emergent properties,

1882
01:18:45,200 --> 01:18:46,960
if you will accept that term, you know,

1883
01:18:46,960 --> 01:18:49,440
surprise surprises to the developers of nothing else,

1884
01:18:49,440 --> 01:18:51,440
right, that are definitely things

1885
01:18:51,440 --> 01:18:52,880
we don't fully understand.

1886
01:18:52,880 --> 01:18:54,880
But it does seem to be a, you know,

1887
01:18:54,880 --> 01:18:58,240
more gradual turning up of capability versus some like,

1888
01:18:58,240 --> 01:19:01,520
you know, super sudden surprise.

1889
01:19:01,520 --> 01:19:05,280
But okay, so then what is the alternative?

1890
01:19:05,280 --> 01:19:07,520
I'm going to try to kind of give you the,

1891
01:19:07,520 --> 01:19:13,920
what I think of as the most consensus strongest scenario

1892
01:19:13,920 --> 01:19:16,640
where humans lose track of the future

1893
01:19:17,360 --> 01:19:19,200
and or lose control of the future,

1894
01:19:19,200 --> 01:19:21,440
maybe starting by kind of losing track of the present

1895
01:19:21,440 --> 01:19:23,040
and then having that kind of, you know,

1896
01:19:23,040 --> 01:19:24,960
give way to losing control of the future.

1897
01:19:24,960 --> 01:19:26,320
And I think within that, by the way, the,

1898
01:19:27,200 --> 01:19:30,080
I'm not really one who cares that much about like,

1899
01:19:30,080 --> 01:19:32,400
whether AIs say something offensive today,

1900
01:19:33,040 --> 01:19:35,440
I'm not easily offended and like whatever.

1901
01:19:35,440 --> 01:19:36,960
That's not, that's not world ending.

1902
01:19:36,960 --> 01:19:37,680
I understand your point.

1903
01:19:37,680 --> 01:19:39,440
That's not like, who cares, whatever.

1904
01:19:39,440 --> 01:19:41,120
That's within scope, that's within envelope.

1905
01:19:41,120 --> 01:19:43,200
Within this bigger kind of, you know,

1906
01:19:43,200 --> 01:19:47,440
what is the real, you know, most likely path

1907
01:19:47,440 --> 01:19:50,640
to like AI disaster as understood,

1908
01:19:50,640 --> 01:19:52,160
I think by the smartest people today,

1909
01:19:52,160 --> 01:19:54,320
I think that is still a useful leading indicator

1910
01:19:54,320 --> 01:19:57,920
because it's like, okay, the developers,

1911
01:19:57,920 --> 01:19:59,440
you know, whether you agree with their politics,

1912
01:19:59,440 --> 01:20:00,320
whether you agree with their,

1913
01:20:00,320 --> 01:20:02,080
whether you think their commercial reasons are,

1914
01:20:02,080 --> 01:20:03,840
their sincere reasons are not,

1915
01:20:03,840 --> 01:20:06,800
they have made it a goal to get the AI

1916
01:20:06,800 --> 01:20:08,240
to not say certain things, right?

1917
01:20:08,240 --> 01:20:09,840
They don't want it to be offensive.

1918
01:20:09,840 --> 01:20:11,920
The most naive, you know, kind of down the fairway

1919
01:20:11,920 --> 01:20:12,880
interpretation of that is like,

1920
01:20:12,880 --> 01:20:14,720
Hey, they want to sell it to corporate customers.

1921
01:20:14,720 --> 01:20:16,560
They know that their corporate customers don't want,

1922
01:20:16,560 --> 01:20:19,120
you know, to have their AI saying offensive things.

1923
01:20:19,120 --> 01:20:21,360
So they don't want to say offensive things.

1924
01:20:21,360 --> 01:20:24,800
And yet they can't really control it.

1925
01:20:24,800 --> 01:20:27,040
It's like still pretty easy to break.

1926
01:20:27,040 --> 01:20:30,640
So I view that as just kind of a leading indicator of,

1927
01:20:30,640 --> 01:20:34,560
okay, we've seen GPT-2, 3 and 4 over the last four years.

1928
01:20:35,680 --> 01:20:39,280
And that's, you know, a big Delta in capability.

1929
01:20:39,840 --> 01:20:44,560
How much control have we seen developed in that time

1930
01:20:44,560 --> 01:20:46,160
and does it seem to be keeping pace?

1931
01:20:46,720 --> 01:20:48,800
And my answer would be on the face of it,

1932
01:20:48,800 --> 01:20:50,640
it seems like the answer is no.

1933
01:20:50,640 --> 01:20:56,000
You know, we, we don't have the ability to really dial in

1934
01:20:56,000 --> 01:20:58,960
the behavior such that we can say, okay, you're going to,

1935
01:20:58,960 --> 01:21:02,000
you know, you can expect, you can trust that these AIs

1936
01:21:02,000 --> 01:21:04,080
will like not do, you know, A, B, and C.

1937
01:21:04,720 --> 01:21:07,040
On the contrary, it's like, if you're a little clever,

1938
01:21:07,040 --> 01:21:08,320
you know, you can get them to do it.

1939
01:21:08,320 --> 01:21:09,840
You can break out of the sandbox on it.

1940
01:21:10,400 --> 01:21:12,000
Yeah. And it's, it's not even like, I mean,

1941
01:21:12,000 --> 01:21:13,920
we've talked about, you know, things where you have access

1942
01:21:13,920 --> 01:21:16,160
to the weights and you're doing like counter optimizations,

1943
01:21:16,160 --> 01:21:17,200
but you don't even need that.

1944
01:21:17,200 --> 01:21:20,080
You know, the kind of stuff I do in like my red teaming

1945
01:21:20,080 --> 01:21:24,640
in public is literally just like feed the AI a couple of words,

1946
01:21:24,640 --> 01:21:26,160
put a couple of words in its mouth, you know,

1947
01:21:26,160 --> 01:21:27,760
and it will kind of carry on from there.

1948
01:21:28,400 --> 01:21:30,320
So with that in mind is just the leading indicator.

1949
01:21:30,880 --> 01:21:32,880
You know, I don't know how powerful the most powerful

1950
01:21:32,880 --> 01:21:34,560
AI systems get over the next few years,

1951
01:21:34,560 --> 01:21:37,840
but it seems very plausible to me that it might be

1952
01:21:37,840 --> 01:21:40,880
as powerful as like an Elon Musk type figure, you know,

1953
01:21:40,880 --> 01:21:43,040
somebody who's like really good at thinking

1954
01:21:43,040 --> 01:21:45,920
from first principles, really smart, you know,

1955
01:21:45,920 --> 01:21:49,040
really dynamic across a wide range of different contexts.

1956
01:21:49,760 --> 01:21:53,200
And, you know, he's not powerful enough to like,

1957
01:21:53,200 --> 01:21:54,720
in and of himself take over the world,

1958
01:21:55,280 --> 01:21:57,760
but he is kind of becoming transformative.

1959
01:21:58,480 --> 01:22:00,400
Now imagine that you have that kind of system

1960
01:22:00,960 --> 01:22:04,080
and it's trivial to replicate it.

1961
01:22:04,080 --> 01:22:06,080
So, you know, if you have like one Elon Musk,

1962
01:22:06,080 --> 01:22:08,080
all of a sudden you can have arbitrary,

1963
01:22:08,080 --> 01:22:09,520
you know, functionally arbitrary numbers

1964
01:22:09,520 --> 01:22:13,440
of Elon Musk power things that are clones of each other.

1965
01:22:14,080 --> 01:22:15,040
Maybe I can pause you there.

1966
01:22:15,040 --> 01:22:17,440
So that's my polytheistic AI scenario,

1967
01:22:17,440 --> 01:22:20,400
but here's the thing that is, this is background,

1968
01:22:20,400 --> 01:22:21,760
but I want to push it to foreground.

1969
01:22:23,040 --> 01:22:26,000
You still have a human typing in things into that thing.

1970
01:22:26,000 --> 01:22:27,920
The human is doing the jailbreak, right?

1971
01:22:27,920 --> 01:22:31,680
What we're talking about is not artificial intelligence

1972
01:22:31,680 --> 01:22:33,520
in the sense of something separate from a human,

1973
01:22:33,520 --> 01:22:34,880
but amplified intelligence.

1974
01:22:36,320 --> 01:22:38,240
Amplified intelligence, I very much believe in.

1975
01:22:38,240 --> 01:22:40,160
The reason is amplified intelligence.

1976
01:22:40,160 --> 01:22:43,120
So here's something that people may not know about humans.

1977
01:22:43,840 --> 01:22:46,640
There's this great book, Cooking Made Us Human.

1978
01:22:48,080 --> 01:22:52,480
Tool use has shifted your biology in the following way.

1979
01:22:52,480 --> 01:22:55,840
For example, I'll map it to the present day.

1980
01:22:55,840 --> 01:22:57,840
This book by Richard Rang, I'm Cooking Made Us Human,

1981
01:22:57,840 --> 01:23:01,440
where the fact that we started cooking and using fire

1982
01:23:02,080 --> 01:23:05,360
meant that we could do metabolism outside the body,

1983
01:23:05,360 --> 01:23:10,800
which meant it freed up energy for more brain development.

1984
01:23:11,760 --> 01:23:13,680
Similarly, developing clothes

1985
01:23:13,680 --> 01:23:15,920
meant that we didn't have to evolve as much fur,

1986
01:23:16,640 --> 01:23:18,640
again, more energy for brain development.

1987
01:23:18,640 --> 01:23:21,600
Evolving tools meant we didn't have as much fangs

1988
01:23:21,600 --> 01:23:23,280
and claws and muscles,

1989
01:23:23,280 --> 01:23:25,280
again, more energy for brain development, right?

1990
01:23:25,280 --> 01:23:29,040
So encephalization quotient rose as tool use

1991
01:23:29,040 --> 01:23:31,760
meant that we didn't have to do as much natively

1992
01:23:31,760 --> 01:23:34,000
and we could push more to the machines.

1993
01:23:34,000 --> 01:23:37,760
In a very real sense, we have been a man-machine symbiosis

1994
01:23:37,760 --> 01:23:41,680
since the invention of fire and the stone axe and clothes.

1995
01:23:42,640 --> 01:23:46,080
You do not exist as a human being on your own.

1996
01:23:46,080 --> 01:23:50,480
The entire Ted Kojinski concept of living in nature by itself,

1997
01:23:50,480 --> 01:23:53,680
humans are social organisms that are adapted

1998
01:23:53,680 --> 01:23:56,480
to working with other humans and using tools.

1999
01:23:57,760 --> 01:23:59,600
And we have been for millennia.

2000
01:23:59,600 --> 01:24:02,000
This goes back, not just human history,

2001
01:24:02,000 --> 01:24:03,840
but hundreds of thousands years before,

2002
01:24:03,840 --> 01:24:05,120
100 gatherers are using tools.

2003
01:24:05,840 --> 01:24:08,720
So what that means is,

2004
01:24:08,720 --> 01:24:11,040
man-machine symbiosis is not some new thing.

2005
01:24:11,600 --> 01:24:14,640
It's actually the old thing that broke us away

2006
01:24:14,640 --> 01:24:17,600
from other primate lineages that weren't using tools.

2007
01:24:17,600 --> 01:24:18,240
Okay?

2008
01:24:18,240 --> 01:24:20,400
This is the fundamental difference between

2009
01:24:20,400 --> 01:24:22,880
what I call Uncle Ted and Uncle Fred.

2010
01:24:23,440 --> 01:24:25,120
Uncle Ted is Ted Kojinski.

2011
01:24:25,120 --> 01:24:26,720
It's a unabomber, it's a doomer,

2012
01:24:26,720 --> 01:24:29,280
it's a decelerator, the de-grocer who thinks

2013
01:24:29,280 --> 01:24:32,000
we need to go back to Gaia and Eden and become monkeys

2014
01:24:32,000 --> 01:24:35,040
and live in the jungle like Ted Kojinski, right?

2015
01:24:35,680 --> 01:24:37,360
The unabomber cell.

2016
01:24:37,360 --> 01:24:40,640
Uncle Fred is Friedrich Nietzsche, right?

2017
01:24:40,640 --> 01:24:43,440
Nietzsche and we must get to the stars

2018
01:24:43,440 --> 01:24:45,600
and become ubermen and so on and so forth.

2019
01:24:45,600 --> 01:24:47,280
This, I think, is going to become,

2020
01:24:47,280 --> 01:24:48,800
and I actually tweeted about this years ago

2021
01:24:48,800 --> 01:24:50,400
before the current AI debates,

2022
01:24:51,600 --> 01:24:55,920
between anarcho-primitivism, de-growth, deceleration,

2023
01:24:55,920 --> 01:24:57,040
okay, on the one hand,

2024
01:24:57,760 --> 01:25:01,360
and transhumanism and acceleration

2025
01:25:01,920 --> 01:25:04,240
and human 2.0 and human self-improvement

2026
01:25:04,240 --> 01:25:06,480
and make it to the stars, on the other hand.

2027
01:25:06,480 --> 01:25:09,280
This is the future political axis, the current one.

2028
01:25:10,000 --> 01:25:11,760
And roughly speaking, you can,

2029
01:25:11,760 --> 01:25:13,600
it's not really left and right

2030
01:25:13,600 --> 01:25:16,240
because you'll have both left status

2031
01:25:16,240 --> 01:25:18,640
and right conservatives go over here.

2032
01:25:18,640 --> 01:25:21,120
You know, left status will say it's against the state

2033
01:25:21,120 --> 01:25:22,240
and the right status will say,

2034
01:25:22,240 --> 01:25:24,800
the right conservatives say it's against God, okay?

2035
01:25:24,800 --> 01:25:26,560
And you'll have left libertarians

2036
01:25:26,560 --> 01:25:28,080
and right libertarians over here

2037
01:25:28,080 --> 01:25:30,080
where the left libertarians say it's my body

2038
01:25:30,080 --> 01:25:33,520
and the right libertarians say it's my money, right?

2039
01:25:34,560 --> 01:25:37,120
And so that is a re-architecting of the political axis

2040
01:25:37,120 --> 01:25:39,200
where Uncle Ted and Uncle Fred,

2041
01:25:39,200 --> 01:25:40,960
which is a kind of clever way of putting it, okay?

2042
01:25:42,240 --> 01:25:44,800
And the problem with the Uncle Ted guys, in my view,

2043
01:25:44,800 --> 01:25:49,040
is, as I said, yeah, if they go and want to live in the woods,

2044
01:25:49,040 --> 01:25:50,240
fine, go get them.

2045
01:25:50,240 --> 01:25:52,800
But once you start having even like a thousand,

2046
01:25:52,800 --> 01:25:55,120
forget a thousand, 100 people doing that,

2047
01:25:55,120 --> 01:25:58,320
your trees will very quickly get exfoliated,

2048
01:25:58,320 --> 01:26:00,400
the leaves are going to get all picked off of them.

2049
01:26:00,400 --> 01:26:02,640
Humans are not set up to just literally live

2050
01:26:02,640 --> 01:26:03,840
in the jungle right now.

2051
01:26:03,840 --> 01:26:06,160
You've had hundreds of thousands of years of evolution

2052
01:26:06,160 --> 01:26:08,560
that have driven you in the direction of tool use,

2053
01:26:08,560 --> 01:26:11,360
social organisms, farming, et cetera, et cetera.

2054
01:26:11,360 --> 01:26:13,600
The man machine symbiosis is not today,

2055
01:26:13,600 --> 01:26:15,280
it's yesterday and the day before

2056
01:26:15,280 --> 01:26:18,080
and 10,000 years ago and 100,000 years ago.

2057
01:26:18,080 --> 01:26:20,400
And how do we know we've got man machine symbiosis?

2058
01:26:20,400 --> 01:26:23,920
Can you live without, even if you're not living,

2059
01:26:23,920 --> 01:26:25,680
even if you're not using the stove,

2060
01:26:25,680 --> 01:26:28,480
somebody's using a stove to make you food, right?

2061
01:26:28,480 --> 01:26:30,160
Can you live without the tractors

2062
01:26:30,160 --> 01:26:32,320
that are digging up the grains?

2063
01:26:32,320 --> 01:26:34,880
Can you live without indoor heating?

2064
01:26:34,880 --> 01:26:36,560
Can you live without your clothes?

2065
01:26:36,560 --> 01:26:38,480
Frankly, can you do your work without your phone,

2066
01:26:38,480 --> 01:26:39,840
without your computer?

2067
01:26:39,840 --> 01:26:40,800
No, you can't.

2068
01:26:40,800 --> 01:26:42,880
You are already a man machine symbiosis.

2069
01:26:43,440 --> 01:26:45,920
Once we accept that, then the question is,

2070
01:26:45,920 --> 01:26:46,800
what's the next step?

2071
01:26:47,520 --> 01:26:50,240
And right now, we're in the middle of that next step,

2072
01:26:50,240 --> 01:26:52,480
which is AI is amplified intelligence.

2073
01:26:53,120 --> 01:26:56,720
So what you're talking about is not that the AI is Elon Musk,

2074
01:26:56,720 --> 01:27:00,320
it is that the AI human fusion means

2075
01:27:00,320 --> 01:27:01,840
there's another 20 Elon Musk's.

2076
01:27:02,320 --> 01:27:03,920
Or whatever the number is, okay?

2077
01:27:04,560 --> 01:27:06,880
And that's good.

2078
01:27:06,880 --> 01:27:07,600
That's fine.

2079
01:27:07,600 --> 01:27:09,120
That's within envelope.

2080
01:27:09,120 --> 01:27:11,520
That's just a bunch of smarter humans on the planet.

2081
01:27:11,520 --> 01:27:13,120
That is amplified intelligence.

2082
01:27:13,120 --> 01:27:17,200
That is more like, I mentioned the tool thing, okay?

2083
01:27:17,200 --> 01:27:18,960
The other analogy would be like a dog.

2084
01:27:18,960 --> 01:27:21,920
You know, a dog is man's best friend, right?

2085
01:27:21,920 --> 01:27:24,560
So that AI does not live without you.

2086
01:27:25,200 --> 01:27:26,960
Humans can turn it off.

2087
01:27:26,960 --> 01:27:28,320
They have to power it.

2088
01:27:28,320 --> 01:27:30,240
They have to give it subsidence, right?

2089
01:27:30,240 --> 01:27:32,160
Eventually, that might become like a ceremonial thing,

2090
01:27:32,160 --> 01:27:35,280
like this is our God that we pray to, right?

2091
01:27:35,280 --> 01:27:37,120
Because it's wiser and smarter than us,

2092
01:27:37,120 --> 01:27:39,120
and it appears in an image.

2093
01:27:39,120 --> 01:27:40,800
But the priests maintain it.

2094
01:27:40,800 --> 01:27:42,880
You know, just like you go to a Hindu temple

2095
01:27:42,880 --> 01:27:43,840
or something like that,

2096
01:27:43,840 --> 01:27:45,600
and the priests will pour out the ghee,

2097
01:27:45,600 --> 01:27:47,360
you know, for the fires and so on and so forth.

2098
01:27:47,360 --> 01:27:49,360
And then everybody comes in and prays, okay?

2099
01:27:49,360 --> 01:27:51,680
The priests believe in the whole thing,

2100
01:27:51,680 --> 01:27:53,600
but they also maintain the back of the house.

2101
01:27:53,600 --> 01:27:56,160
They do the system administration for the temple.

2102
01:27:56,160 --> 01:27:58,720
Same, you know, in a Christian church, right?

2103
01:27:59,440 --> 01:28:02,960
The, you know, it's not like it appears out of nowhere.

2104
01:28:02,960 --> 01:28:08,080
Somebody, you know, went and assembled this cathedral, right?

2105
01:28:08,080 --> 01:28:09,840
They saw the back of the house,

2106
01:28:09,840 --> 01:28:11,600
the fact that it was just woods and rocks

2107
01:28:11,600 --> 01:28:12,800
and so on that came together.

2108
01:28:12,800 --> 01:28:13,760
But then when people come there,

2109
01:28:13,760 --> 01:28:15,200
it feels like a spiritual experience.

2110
01:28:15,200 --> 01:28:16,720
You see what I'm saying, okay?

2111
01:28:16,720 --> 01:28:18,080
So the equivalent of that,

2112
01:28:18,080 --> 01:28:21,040
the priests or the people

2113
01:28:21,040 --> 01:28:23,520
maintaining temples, cathedrals, mosques, whatever,

2114
01:28:23,520 --> 01:28:27,600
is engineers who are maintaining

2115
01:28:27,600 --> 01:28:30,800
these future AIs, which appear to you as Jesus.

2116
01:28:30,800 --> 01:28:32,960
They appear to you, maybe even a hologram, okay?

2117
01:28:32,960 --> 01:28:35,200
You come there, you ask it for guidance as an oracle.

2118
01:28:35,200 --> 01:28:37,440
You've also got the personal version on your phone.

2119
01:28:37,440 --> 01:28:39,520
You ask it for guidance, but guess what?

2120
01:28:40,880 --> 01:28:44,640
You're still a human AI symbiosis until,

2121
01:28:44,640 --> 01:28:47,280
and unless, that AI actually has the terminator scenario

2122
01:28:47,280 --> 01:28:48,320
where it's got lots of robots

2123
01:28:48,320 --> 01:28:49,520
and it can live on its own.

2124
01:28:49,520 --> 01:28:51,600
I'm not saying that's physically impossible.

2125
01:28:51,600 --> 01:28:53,760
I did give some constraints on it earlier,

2126
01:28:53,760 --> 01:28:55,680
but for a while, we're not going to be there.

2127
01:28:55,680 --> 01:28:57,840
So that alone means it's not fume

2128
01:28:57,840 --> 01:28:59,520
because we don't have lots of drones running around.

2129
01:28:59,520 --> 01:29:00,800
The AI has to be with the human.

2130
01:29:00,800 --> 01:29:02,320
It's a human AI symbiosis.

2131
01:29:02,320 --> 01:29:04,000
It's not AI Elon Musk.

2132
01:29:04,000 --> 01:29:07,280
It is human AI fusion that becomes Elon Musk.

2133
01:29:07,280 --> 01:29:08,640
And frankly, that's not that different

2134
01:29:08,640 --> 01:29:09,840
from what Elon Musk himself is.

2135
01:29:09,840 --> 01:29:11,920
Elon Musk would not be Elon Musk without the internet.

2136
01:29:11,920 --> 01:29:13,040
Without the internet, you can't tweet

2137
01:29:13,040 --> 01:29:14,880
and reach 150 million people.

2138
01:29:14,880 --> 01:29:18,080
The internet itself made Elon what he is, right?

2139
01:29:18,080 --> 01:29:20,640
And so this is like the next version of that.

2140
01:29:20,640 --> 01:29:21,840
Maybe there's now 30 Elons

2141
01:29:21,840 --> 01:29:24,000
because the AI makes the next 30 Elons.

2142
01:29:24,000 --> 01:29:26,320
Yeah, I mean, again, I think I'm largely with you

2143
01:29:26,320 --> 01:29:30,720
with just this one very important nagging worry

2144
01:29:30,720 --> 01:29:33,120
that's like, what if this time is different?

2145
01:29:33,120 --> 01:29:37,440
Because what if these systems are getting so powerful,

2146
01:29:37,440 --> 01:29:39,520
so quickly that we don't really have time

2147
01:29:39,520 --> 01:29:44,960
for that techno human fusion to really work out?

2148
01:29:44,960 --> 01:29:46,880
And I'll just give you kind of a couple of data points on that.

2149
01:29:46,880 --> 01:29:49,040
Like, you know, you said like,

2150
01:29:49,040 --> 01:29:51,200
it's still somebody putting something into the AI.

2151
01:29:51,200 --> 01:29:52,240
Well, sort of, right?

2152
01:29:52,240 --> 01:29:54,960
I mean, already we have these proto agents

2153
01:29:54,960 --> 01:29:57,440
and the like super simple scaffolding of an agent

2154
01:29:57,440 --> 01:30:00,800
is just run it in a loop, give it a goal,

2155
01:30:00,800 --> 01:30:04,560
and have it kind of pursue some like plan, act,

2156
01:30:04,560 --> 01:30:08,080
get feedback and loop type of structure, right?

2157
01:30:08,080 --> 01:30:09,840
It doesn't seem to take a lot.

2158
01:30:09,840 --> 01:30:11,600
Now, they're not smart enough yet

2159
01:30:11,600 --> 01:30:13,920
to accomplish big things in the world,

2160
01:30:13,920 --> 01:30:19,040
but it seems like the language model to agent switch

2161
01:30:19,120 --> 01:30:23,680
is less one right now that is gated by the structure

2162
01:30:23,680 --> 01:30:25,680
or the architecture and more one that's just gated

2163
01:30:25,680 --> 01:30:27,440
by the fact that like the language models,

2164
01:30:27,440 --> 01:30:30,560
when framed as agents, just aren't that successful

2165
01:30:30,560 --> 01:30:32,880
at like doing practical things and getting over hump,

2166
01:30:32,880 --> 01:30:34,080
so they tend to get stuck.

2167
01:30:35,040 --> 01:30:37,040
But it doesn't seem that hard to imagine that like,

2168
01:30:37,040 --> 01:30:39,040
you know, if you had something that is sort of

2169
01:30:39,040 --> 01:30:41,680
that next level that you put it into a loop,

2170
01:30:41,680 --> 01:30:44,240
you say, okay, you're Elon Musk, LLM,

2171
01:30:44,240 --> 01:30:47,120
and your job is to like make, you know, us,

2172
01:30:47,120 --> 01:30:51,200
whatever us exactly is a, you know, multi-planetary species,

2173
01:30:51,200 --> 01:30:55,120
and then you just kind of keep updating your status,

2174
01:30:55,120 --> 01:30:57,200
keep updating your plans, keep trying stuff,

2175
01:30:57,200 --> 01:31:00,240
keep getting feedback, and you know,

2176
01:31:00,240 --> 01:31:02,880
like what really limits that?

2177
01:31:03,760 --> 01:31:05,360
There may be like a really good program,

2178
01:31:06,000 --> 01:31:09,600
but the whole AI kills everyone thing is so,

2179
01:31:09,600 --> 01:31:11,840
it's like, where's the actuator?

2180
01:31:11,840 --> 01:31:13,040
Okay, I hit enter.

2181
01:31:14,320 --> 01:31:16,480
What kills me, right?

2182
01:31:16,560 --> 01:31:19,920
Is it a hypnotized human who's being hypnotized by an AI

2183
01:31:19,920 --> 01:31:23,200
that he's typed into and he's radicalized himself

2184
01:31:23,200 --> 01:31:24,560
by typing into a computer?

2185
01:31:25,120 --> 01:31:27,200
Okay, that's not that different from a lot of other things

2186
01:31:27,200 --> 01:31:29,040
that have happened in the past, right?

2187
01:31:29,040 --> 01:31:31,280
So who is actually striking me, right?

2188
01:31:31,280 --> 01:31:32,560
Who's striking the human?

2189
01:31:32,560 --> 01:31:35,280
It's another human within acts that he's been radicalized

2190
01:31:35,280 --> 01:31:36,720
by an AI, okay?

2191
01:31:36,720 --> 01:31:38,560
He's not, actually, that's not even the right term.

2192
01:31:38,560 --> 01:31:42,080
We're giving agency to the AI when it's not really an agent.

2193
01:31:42,080 --> 01:31:44,080
It is a human who's self-radicalized

2194
01:31:44,080 --> 01:31:45,840
by typing into a computer screen.

2195
01:31:46,560 --> 01:31:48,800
And has hit another human.

2196
01:31:48,800 --> 01:31:49,760
That's one scenario.

2197
01:31:49,760 --> 01:31:50,720
The other scenario is,

2198
01:31:50,720 --> 01:31:52,880
it's literally a Skynet drone that's hitting you.

2199
01:31:53,680 --> 01:31:54,560
Those are the only two,

2200
01:31:54,560 --> 01:31:56,720
how else is it going to be physical, right?

2201
01:31:56,720 --> 01:31:59,360
How does the, the actuation step is a part

2202
01:31:59,360 --> 01:32:01,280
that is skipped over and it's a non-trivial step.

2203
01:32:02,160 --> 01:32:04,160
Well, I think it could be lots of things, right?

2204
01:32:04,160 --> 01:32:05,760
I mean, if it's not one of those two,

2205
01:32:05,760 --> 01:32:08,640
if it's not another human or a drone hitting you,

2206
01:32:08,640 --> 01:32:09,120
what is it?

2207
01:32:10,080 --> 01:32:11,680
Just habitat degradation, right?

2208
01:32:11,680 --> 01:32:13,360
I mean, how do we kill most of the other species

2209
01:32:13,360 --> 01:32:14,480
that we drive to extinction?

2210
01:32:14,480 --> 01:32:18,160
We don't go out and like hunt them down with axes one by one.

2211
01:32:18,160 --> 01:32:20,720
We just like change the environment more broadly

2212
01:32:20,720 --> 01:32:23,440
to the point where it's not suitable for them anymore

2213
01:32:23,440 --> 01:32:25,920
and they don't have enough space and they kind of die out, right?

2214
01:32:25,920 --> 01:32:28,400
Like, so we did hunt down some of the megafauna,

2215
01:32:28,400 --> 01:32:31,360
like literally one by one with, with spears and stuff.

2216
01:32:31,360 --> 01:32:34,400
But like most of the recent loss of species

2217
01:32:34,960 --> 01:32:38,080
is just like, we're out there just extracting resources

2218
01:32:38,080 --> 01:32:39,120
for our own purposes.

2219
01:32:39,680 --> 01:32:41,840
And in the course of doing that, you know,

2220
01:32:41,840 --> 01:32:45,680
whatever bird or whatever, you know, thing just kind of loses its place

2221
01:32:45,680 --> 01:32:46,720
and then it's no more.

2222
01:32:47,280 --> 01:32:49,680
And I don't think that's like totally implausible.

2223
01:32:50,240 --> 01:32:54,080
Wait, so that is though, I think, within normal world, right?

2224
01:32:54,080 --> 01:32:54,880
What does that mean?

2225
01:32:54,880 --> 01:32:59,680
That means that some people, some, some amplified intelligence,

2226
01:32:59,680 --> 01:33:02,000
and maybe we might call it HAI.

2227
01:33:02,000 --> 01:33:04,720
Okay, human plus AI combination, right?

2228
01:33:04,720 --> 01:33:08,640
Some HAIs out compete others economically

2229
01:33:08,640 --> 01:33:09,680
and they lose their jobs.

2230
01:33:09,680 --> 01:33:10,560
Is that what you're talking about?

2231
01:33:11,040 --> 01:33:14,480
I think also the humans potentially become unnecessary

2232
01:33:14,480 --> 01:33:16,000
in a lot of the configurations.

2233
01:33:16,000 --> 01:33:19,120
Like just a recent paper from DeepMind.

2234
01:33:19,120 --> 01:33:20,560
It's your marginal product workers.

2235
01:33:20,560 --> 01:33:21,280
Or negative.

2236
01:33:21,280 --> 01:33:25,680
Yeah, I mean, so the last, you know, DeepMind has been on Google,

2237
01:33:25,680 --> 01:33:31,920
Google DeepMind has been on a tear of increasingly impressive medical AI's.

2238
01:33:31,920 --> 01:33:36,800
Their most recent one takes a bunch of difficult case studies

2239
01:33:36,800 --> 01:33:37,440
from the literature.

2240
01:33:37,440 --> 01:33:38,240
I mean, case studies, you know,

2241
01:33:38,240 --> 01:33:40,720
this is like rare diseases, hard to diagnose stuff.

2242
01:33:41,520 --> 01:33:46,480
And asks an AI to do the differential diagnosis, compares that to human,

2243
01:33:46,480 --> 01:33:48,640
and compares it to human plus AI.

2244
01:33:49,280 --> 01:33:52,160
And they've phrased their results like in a very understated way.

2245
01:33:52,160 --> 01:33:56,400
But the headline is, the AI blows away the human plus AI.

2246
01:33:56,400 --> 01:33:58,000
The human makes the AI worse.

2247
01:33:58,800 --> 01:34:02,080
So here's the thing, I'll say something provocative maybe.

2248
01:34:02,080 --> 01:34:03,520
Okay, like I have in a very fine.

2249
01:34:04,080 --> 01:34:08,400
I do think that the ABC's of Economic Apocalypse for Blue America

2250
01:34:08,400 --> 01:34:10,160
are AI, Bitcoin, and China.

2251
01:34:10,160 --> 01:34:13,280
Where AI takes away their, a lot of the revenue streams,

2252
01:34:13,280 --> 01:34:18,320
the licensures that have made medical and legal costs and other things so high.

2253
01:34:18,320 --> 01:34:20,080
Bitcoin takes away their power over money,

2254
01:34:20,080 --> 01:34:22,000
and China takes away their military power.

2255
01:34:22,000 --> 01:34:25,760
So I've perceived total meltdown for Blue America

2256
01:34:26,880 --> 01:34:31,600
in the years and, you know, maybe decade to come already kind of happening.

2257
01:34:31,600 --> 01:34:33,360
But that's different than being at the end of the world.

2258
01:34:34,000 --> 01:34:34,640
Right?

2259
01:34:34,640 --> 01:34:37,760
Like, Blue America had a really great time for a long time,

2260
01:34:37,760 --> 01:34:39,520
and they've got these licensure locks.

2261
01:34:39,520 --> 01:34:43,520
But because of that, they've hyperinflated the cost of medicine.

2262
01:34:44,320 --> 01:34:47,360
It's like, how much, so what you're talking about is,

2263
01:34:48,240 --> 01:34:49,760
wow, we have infinite free medicine.

2264
01:34:50,320 --> 01:34:53,040
Man, Dr. Billing events are going to get ahead.

2265
01:34:53,040 --> 01:34:53,920
That's the point.

2266
01:34:54,640 --> 01:34:57,200
Yeah, and to be clear, I'm really with you on that too.

2267
01:34:57,200 --> 01:35:01,360
Like, I want to see, when people say like, what is good about AI?

2268
01:35:01,360 --> 01:35:03,920
You know, why should we pursue this?

2269
01:35:05,200 --> 01:35:12,880
My standard answer is high quality medical advice for everyone at pennies per visit, right?

2270
01:35:12,880 --> 01:35:14,720
It is orders of magnitude cheaper.

2271
01:35:14,720 --> 01:35:17,280
We're already starting to see that in some ways it's better.

2272
01:35:17,280 --> 01:35:19,680
People prefer it, you know, that AI is more patient.

2273
01:35:19,680 --> 01:35:21,040
It has better bedside manner.

2274
01:35:21,680 --> 01:35:26,000
I wouldn't say, you know, if I was giving my own family advice today,

2275
01:35:26,000 --> 01:35:28,320
I would say use both a human doctor and an AI,

2276
01:35:28,320 --> 01:35:30,960
but definitely use the AI as part of your mix.

2277
01:35:31,680 --> 01:35:32,880
Absolutely. That's right.

2278
01:35:32,880 --> 01:35:34,800
That's right. But you're prompting it still, right?

2279
01:35:34,800 --> 01:35:37,520
The smarter you are, the smarter the AI is.

2280
01:35:37,520 --> 01:35:40,560
You notice this immediately with your vocabulary, right?

2281
01:35:40,560 --> 01:35:42,560
The more sophisticated your vocabulary,

2282
01:35:42,560 --> 01:35:44,400
the finer the distinctions you can have,

2283
01:35:44,400 --> 01:35:46,320
the better your own ability to spot errors.

2284
01:35:47,040 --> 01:35:49,280
You can generate a basic program with it, right?

2285
01:35:49,280 --> 01:35:51,280
But really amplified intelligence is, I think,

2286
01:35:51,280 --> 01:35:52,880
a much better way of thinking about it.

2287
01:35:52,880 --> 01:35:58,160
Because whatever your IQ is, it surges it upward by a factor of three or whatever the number.

2288
01:35:58,160 --> 01:36:00,800
And maybe the amplifier increases with your intelligence.

2289
01:36:00,800 --> 01:36:04,240
But that internal intelligence difference still exists.

2290
01:36:04,240 --> 01:36:05,600
It's just like what a computer is.

2291
01:36:05,600 --> 01:36:07,760
A computer is an amplifier for intelligence.

2292
01:36:07,760 --> 01:36:11,120
If you're smart, you can hit enter and programs can go to...

2293
01:36:11,120 --> 01:36:13,520
Like thinking about the Minecraft guy, right?

2294
01:36:13,520 --> 01:36:14,320
Or Satoshi.

2295
01:36:14,880 --> 01:36:19,600
One person built a billion, or in Satoshi's case, a trillion dollar thing, you know?

2296
01:36:19,600 --> 01:36:22,560
Obviously other people continued Bitcoin and so on and so forth, right?

2297
01:36:23,440 --> 01:36:25,760
So what I feel, though, is this is what I mean

2298
01:36:25,760 --> 01:36:29,680
by going from nuclear terrorism to the TSA, okay?

2299
01:36:29,680 --> 01:36:32,160
We went from AI will kill everyone.

2300
01:36:32,160 --> 01:36:35,360
And I'm like, what's the actuator to, okay,

2301
01:36:35,360 --> 01:36:36,640
it'll gradually degrade our environment.

2302
01:36:36,640 --> 01:36:37,200
What does that mean?

2303
01:36:37,200 --> 01:36:38,320
Okay, some people will lose their jobs.

2304
01:36:38,320 --> 01:36:39,600
But then we're back in normal world.

2305
01:36:40,240 --> 01:36:40,800
Well, hold on.

2306
01:36:40,800 --> 01:36:42,480
Let me paint a little bit more complete picture,

2307
01:36:42,480 --> 01:36:44,160
because I don't think we're quite there yet.

2308
01:36:44,160 --> 01:36:48,480
So I think the differential diagnosis,

2309
01:36:48,480 --> 01:36:52,800
recent paper, that's just a data point where it's kind of like chess.

2310
01:36:52,800 --> 01:36:54,400
So this came long before, right?

2311
01:36:54,400 --> 01:36:57,120
There was a period where humans were the best chess players.

2312
01:36:57,120 --> 01:36:59,120
Then there was a period where the best were the hybrid,

2313
01:36:59,120 --> 01:37:00,880
human AI systems.

2314
01:37:00,880 --> 01:37:03,680
And now as far as I understand it, we're in a regime where

2315
01:37:03,680 --> 01:37:06,480
the human can't really help the AI anymore.

2316
01:37:06,480 --> 01:37:10,240
And so the best chess players are just pure AIs.

2317
01:37:10,240 --> 01:37:13,360
We're not there in medicine, but we're starting to see examples where,

2318
01:37:13,360 --> 01:37:16,480
hey, in a pretty defined study, differential diagnosis,

2319
01:37:16,480 --> 01:37:18,880
the AI is beating, not just beating the humans,

2320
01:37:18,880 --> 01:37:21,440
but also beating the AI human hybrid,

2321
01:37:21,440 --> 01:37:23,040
or the human with access to AI.

2322
01:37:23,760 --> 01:37:25,920
So, okay, that's not it, right?

2323
01:37:25,920 --> 01:37:28,880
There's a paper recently called Eureka.

2324
01:37:28,880 --> 01:37:35,520
Out of NVIDIA, this is Jim Fan's lab where they use GPT-4

2325
01:37:35,520 --> 01:37:39,520
to write the reward functions to train a robot.

2326
01:37:39,520 --> 01:37:42,720
So you want to train a robot to twirl a pencil in fingers.

2327
01:37:43,920 --> 01:37:44,560
Hard for me to do.

2328
01:37:45,280 --> 01:37:47,280
Robots definitely can't do it.

2329
01:37:47,280 --> 01:37:48,160
How do you train that?

2330
01:37:48,160 --> 01:37:49,520
Well, you need a reward function.

2331
01:37:49,520 --> 01:37:52,640
The reward function, basically while you're in the early process

2332
01:37:52,640 --> 01:37:54,080
of learning and failing all the time,

2333
01:37:54,640 --> 01:37:57,280
the reward function gives you encouragement

2334
01:37:57,280 --> 01:37:58,480
when you're on the right track, right?

2335
01:37:58,960 --> 01:38:01,600
There are people who have developed this skill,

2336
01:38:01,600 --> 01:38:02,720
and you might do something like,

2337
01:38:02,720 --> 01:38:04,880
well, if the pencil has angular momentum,

2338
01:38:05,520 --> 01:38:07,600
then that seems like you're on maybe the right track,

2339
01:38:07,600 --> 01:38:08,960
so give that a reward.

2340
01:38:08,960 --> 01:38:11,600
Even though, at the beginning, you're just failing all the time.

2341
01:38:11,600 --> 01:38:14,640
It turns out GPT-4 is way better than humans at this, right?

2342
01:38:14,640 --> 01:38:16,800
So it's better at training robots.

2343
01:38:17,360 --> 01:38:19,120
So all of that is awesome, and it's great.

2344
01:38:19,920 --> 01:38:24,640
But here's the thing, is there's a huge difference between AI

2345
01:38:24,640 --> 01:38:27,120
is going to kill everybody and turn everybody into paperclips.

2346
01:38:28,560 --> 01:38:33,360
Versus some humans with some AI are going to make a lot more money,

2347
01:38:33,920 --> 01:38:35,440
and some people are going to lose their jobs.

2348
01:38:36,320 --> 01:38:37,520
Yeah, I'm not scared of that.

2349
01:38:37,520 --> 01:38:38,480
I'm not scared of that, Snare.

2350
01:38:38,480 --> 01:38:39,840
I mean, it could be disruptive,

2351
01:38:39,840 --> 01:38:43,680
it could be disruptive, but it's not existential under itself.

2352
01:38:44,640 --> 01:38:45,120
Big deal.

2353
01:38:45,120 --> 01:38:46,240
Okay, so that's why I went, right.

2354
01:38:46,800 --> 01:38:49,360
There's a, the, the, to me, it comes, if I,

2355
01:38:49,360 --> 01:38:51,840
if I ask just one question is, what is the actuator?

2356
01:38:52,560 --> 01:38:52,960
Right?

2357
01:38:52,960 --> 01:38:54,640
You know, sensors and actuators, right?

2358
01:38:54,640 --> 01:38:56,240
What is the thing that's actually going to

2359
01:38:56,960 --> 01:39:00,080
plunge a knife or a bullet into you and kill you?

2360
01:39:00,800 --> 01:39:08,480
It is either a human who has hypnotized themselves by typing into a computer,

2361
01:39:08,480 --> 01:39:10,400
like basically an AI terrorist, you know,

2362
01:39:10,400 --> 01:39:14,320
which is kind of where some of the EAs are going in my view,

2363
01:39:14,320 --> 01:39:20,080
or it is like an autonomous drone that is controlled in a

2364
01:39:20,080 --> 01:39:22,240
starcraft or terminator like way.

2365
01:39:22,880 --> 01:39:27,600
We are not there yet in terms of having enough humanoid or autonomous drones

2366
01:39:27,600 --> 01:39:29,600
that are internet connected and programmable.

2367
01:39:29,600 --> 01:39:31,120
That won't be there for some time.

2368
01:39:31,120 --> 01:39:31,360
Okay.

2369
01:39:32,000 --> 01:39:33,840
So that alone means fast takeoff is,

2370
01:39:33,840 --> 01:39:35,680
and what I think by the time we get there,

2371
01:39:36,480 --> 01:39:40,080
you will have a cryptographic control over them.

2372
01:39:40,080 --> 01:39:41,440
That's a crucial thing.

2373
01:39:41,440 --> 01:39:44,720
Cryptography fragments the whole space in a very fundamental way.

2374
01:39:45,280 --> 01:39:46,720
If you don't have the private keys,

2375
01:39:46,720 --> 01:39:48,320
you do not have control over it.

2376
01:39:48,320 --> 01:39:52,720
So long as that piece of hardware, the cryptographic controller,

2377
01:39:52,720 --> 01:39:54,240
you've nailed the equations on that,

2378
01:39:54,240 --> 01:39:56,720
and frankly, you can use AI to attack that as well

2379
01:39:56,720 --> 01:39:58,960
to make sure the code is perfect, right?

2380
01:40:00,000 --> 01:40:01,840
Remember you talked about attack and defense?

2381
01:40:01,840 --> 01:40:05,120
AI is attack cryptos defense, right?

2382
01:40:05,120 --> 01:40:08,080
Because one of the things that crypto has done,

2383
01:40:08,640 --> 01:40:11,440
do you know the PKI problem is public key infrastructure?

2384
01:40:11,440 --> 01:40:14,480
I'll say no on behalf of the audience.

2385
01:40:14,480 --> 01:40:14,960
This is good.

2386
01:40:14,960 --> 01:40:16,080
We should do more of these actually.

2387
01:40:16,080 --> 01:40:19,200
I feel it's a good fusion of things or whatever, right?

2388
01:40:19,200 --> 01:40:27,360
But the public key infrastructure problem is something that was sort of,

2389
01:40:27,360 --> 01:40:30,640
lots of cryptography papers and computer science papers

2390
01:40:30,640 --> 01:40:34,560
in the 90s and 2000s assumed that this could exist

2391
01:40:34,560 --> 01:40:36,640
and essentially meant if you could assume

2392
01:40:36,640 --> 01:40:42,400
that everybody on the internet had a public key that was public

2393
01:40:42,400 --> 01:40:46,480
and a private key that was kept both secure and available at all times,

2394
01:40:46,480 --> 01:40:48,800
then there's all kinds of amazing things you can do

2395
01:40:48,800 --> 01:40:51,840
with privacy preserving, messaging, and authentication, and so on.

2396
01:40:52,480 --> 01:40:55,760
The problem is that for many years,

2397
01:40:55,760 --> 01:40:59,040
what cryptographers try to do is they try to nag people

2398
01:40:59,040 --> 01:41:01,520
into keeping their private keys secure and available.

2399
01:41:01,520 --> 01:41:05,040
And the issue is it's trivial to keep it secure and unavailable

2400
01:41:05,040 --> 01:41:07,040
where you write it down and you put it into a lockbox

2401
01:41:07,040 --> 01:41:08,080
and you lose the lockbox.

2402
01:41:08,720 --> 01:41:11,200
It's trivial to keep it available and not secure,

2403
01:41:11,200 --> 01:41:14,080
okay, where you put it on your public website

2404
01:41:14,080 --> 01:41:17,120
and it's available all the time, you never lose it,

2405
01:41:17,120 --> 01:41:20,320
but it's not secure because anybody can see it.

2406
01:41:21,120 --> 01:41:22,080
When you actually ask,

2407
01:41:22,080 --> 01:41:24,560
what does it mean to keep something secure and available?

2408
01:41:25,680 --> 01:41:27,440
That's actually a very high cost.

2409
01:41:27,440 --> 01:41:31,360
It's precious space because it's based on your wallet, right?

2410
01:41:31,360 --> 01:41:34,320
Your wallet is on your person at all times, so it's available,

2411
01:41:35,040 --> 01:41:38,960
but it's not available to everybody else, so it's secure.

2412
01:41:38,960 --> 01:41:41,760
So you actually have to touch it constantly, yes, right?

2413
01:41:41,760 --> 01:41:44,400
So it turns out that the crypto wallet,

2414
01:41:45,200 --> 01:41:50,320
by adding a literal incentive to keep your private keys secure and available,

2415
01:41:50,320 --> 01:41:52,640
because if they're not available, you've lost your money.

2416
01:41:52,640 --> 01:41:55,200
If they're not secure, you've lost your money, okay?

2417
01:41:55,200 --> 01:41:58,320
To have both of them, that was what solved the PKI problem.

2418
01:41:59,520 --> 01:42:00,960
Now we have hundreds of millions of people

2419
01:42:00,960 --> 01:42:04,480
with public private key pairs where the private keys are secure and available.

2420
01:42:04,480 --> 01:42:07,840
That means all kinds of cryptographic schemes,

2421
01:42:07,840 --> 01:42:08,720
zero-knowledge stuff.

2422
01:42:08,720 --> 01:42:12,000
There's this amazing universe of things that is happening now.

2423
01:42:12,000 --> 01:42:15,040
Zero-knowledge in particular has made cryptography much more programmable.

2424
01:42:15,040 --> 01:42:18,640
There's a whole topic which is, if you want something that's kind of,

2425
01:42:18,640 --> 01:42:21,200
you know, like AI was creeping for a while

2426
01:42:21,200 --> 01:42:23,120
and people, specialists were paying attention to it

2427
01:42:23,120 --> 01:42:24,640
and then just burst out on the scene.

2428
01:42:25,280 --> 01:42:27,440
Zero-knowledge is kind of like that for cryptography.

2429
01:42:27,440 --> 01:42:30,560
Thanks to the, you know, you've probably heard of zero-knowledge before.

2430
01:42:30,560 --> 01:42:37,440
Yeah, we did one episode with Daniel Kong on the use of zero-knowledge proofs

2431
01:42:37,840 --> 01:42:43,280
to basically prove without revealing like the weights

2432
01:42:43,280 --> 01:42:45,680
that you actually ran the model you said you were going to run

2433
01:42:45,680 --> 01:42:47,600
and things like that I think are super interesting.

2434
01:42:48,240 --> 01:42:48,880
Exactly, right?

2435
01:42:48,880 --> 01:42:52,560
So what kinds of stuff, why is that useful in the AI space?

2436
01:42:52,560 --> 01:42:55,680
Well, first is you can use it, for example,

2437
01:42:55,680 --> 01:42:58,880
for training on medical records while keeping them both private

2438
01:42:58,880 --> 01:43:01,840
but also getting the data you wanted.

2439
01:43:01,840 --> 01:43:05,760
For example, let's say you've got a collection of genomes, okay?

2440
01:43:06,400 --> 01:43:12,240
And you want to ask, okay, how many Gs were in this data set?

2441
01:43:12,240 --> 01:43:13,520
How many Cs?

2442
01:43:13,520 --> 01:43:14,080
How many A's?

2443
01:43:14,080 --> 01:43:14,880
How many T's?

2444
01:43:14,880 --> 01:43:16,960
Okay, like you just said, like that's a very simple down.

2445
01:43:16,960 --> 01:43:21,120
So what's the ACG T content of this, you know, the sequence data set?

2446
01:43:21,840 --> 01:43:24,400
You could get those numbers, you could prove they were correct

2447
01:43:24,400 --> 01:43:27,040
without giving any information about the individual sequences, right?

2448
01:43:27,040 --> 01:43:29,440
Or more specifically, you do it at one locus and you say,

2449
01:43:29,440 --> 01:43:31,920
how many Gs and how many Cs are at this particular locus

2450
01:43:31,920 --> 01:43:33,840
and you get the SNP distribution, okay?

2451
01:43:33,840 --> 01:43:37,920
So it's useful for what you just said,

2452
01:43:37,920 --> 01:43:40,240
which is like showing that you ran a particular model

2453
01:43:40,240 --> 01:43:41,760
without giving anything else away.

2454
01:43:41,760 --> 01:43:45,440
It's useful for certain kinds of data analysis.

2455
01:43:45,440 --> 01:43:47,520
There's a lot of overhead on compute on this right now.

2456
01:43:47,520 --> 01:43:49,520
So it's not something that you do trivially, okay?

2457
01:43:49,520 --> 01:43:50,800
But it'll probably come down with time.

2458
01:43:51,680 --> 01:43:55,360
But what is perhaps most interestingly useful for

2459
01:43:55,360 --> 01:44:00,880
is in the context of AI is coming up with things that AI can't fake.

2460
01:44:00,880 --> 01:44:02,880
So what we talked about earlier, right?

2461
01:44:02,880 --> 01:44:07,680
Like an AI can come up with all kinds of plausible sounding images,

2462
01:44:07,680 --> 01:44:12,480
but if it wasn't cryptographically signed by the sender,

2463
01:44:13,680 --> 01:44:18,960
then, you know, it should be signed by the sender and put on chain.

2464
01:44:18,960 --> 01:44:22,160
And then at least you know that this person or this entity

2465
01:44:22,160 --> 01:44:28,000
with this private key asserted that this object existed at this time

2466
01:44:28,000 --> 01:44:30,080
in a way that'd be extremely expensive to falsify

2467
01:44:30,080 --> 01:44:31,600
because it's either on the Bitcoin blockchain

2468
01:44:31,600 --> 01:44:33,680
or another blockchain that's very expensive to rewind, okay?

2469
01:44:34,320 --> 01:44:37,440
This starts to be a bunch of facts that an AI can't fake.

2470
01:44:38,400 --> 01:44:43,440
You know, so going back to the kind of big picture loss of control story,

2471
01:44:43,440 --> 01:44:45,520
I was just kind of trying to build up a few of these data points

2472
01:44:45,520 --> 01:44:47,840
that like, hey, look at this differential diagnosis.

2473
01:44:47,840 --> 01:44:52,000
We already see like humans are not really adding value to AIs anymore.

2474
01:44:52,000 --> 01:44:53,440
That's kind of striking.

2475
01:44:53,440 --> 01:44:55,760
And like similarly with training robot hands,

2476
01:44:55,760 --> 01:44:58,240
GPT-4 is outperforming human experts.

2477
01:44:58,800 --> 01:45:02,560
And by the way, all of the sort of latent spaces

2478
01:45:02,560 --> 01:45:04,240
are like totally bridgeable, right?

2479
01:45:04,240 --> 01:45:06,320
I mean, one of the most striking observations

2480
01:45:06,320 --> 01:45:08,320
of the last couple of years of study is that

2481
01:45:09,120 --> 01:45:11,680
AIs can talk to each other in high dimensional space,

2482
01:45:12,800 --> 01:45:16,080
which we don't really have a way of understanding natively, right?

2483
01:45:16,080 --> 01:45:18,240
It takes a lot of work for us to decode.

2484
01:45:19,040 --> 01:45:20,880
This is like the language thing?

2485
01:45:20,880 --> 01:45:23,200
We're starting to see AIs kind of develop

2486
01:45:23,200 --> 01:45:26,000
not obviously totally on their own as of now,

2487
01:45:26,080 --> 01:45:27,120
but we are...

2488
01:45:27,120 --> 01:45:32,480
There is becoming an increasingly reliable go-to set of techniques

2489
01:45:33,040 --> 01:45:36,880
if you want to bridge different modalities

2490
01:45:36,880 --> 01:45:39,680
with like a pretty small parameter adapter.

2491
01:45:39,680 --> 01:45:40,320
That's interesting.

2492
01:45:40,320 --> 01:45:41,760
Actually, what's a good paper on that?

2493
01:45:41,760 --> 01:45:42,800
I actually hadn't seen that.

2494
01:45:42,800 --> 01:45:45,680
The blip family of models out of Salesforce research

2495
01:45:45,680 --> 01:45:46,480
is really interesting.

2496
01:45:46,480 --> 01:45:48,400
And I've used that in production at...

2497
01:45:48,400 --> 01:45:49,440
Salesforce, really?

2498
01:45:49,440 --> 01:45:50,480
Yeah, Salesforce research.

2499
01:45:50,480 --> 01:45:54,080
They have a crack team that has open sourced a ton of stuff

2500
01:45:54,080 --> 01:45:59,280
in the language model, computer vision, joint space.

2501
01:46:00,160 --> 01:46:01,200
And this...

2502
01:46:01,200 --> 01:46:02,560
You see this all over the place now,

2503
01:46:02,560 --> 01:46:06,400
but basically what they did in a paper called blip2,

2504
01:46:06,400 --> 01:46:09,120
and they've had like five of these with a bunch of different techniques,

2505
01:46:09,680 --> 01:46:13,600
but in blip2, they took a pre-trained language model

2506
01:46:14,320 --> 01:46:16,080
and then a pre-trained computer vision model,

2507
01:46:16,640 --> 01:46:19,040
and they were able to train just a very small model

2508
01:46:19,040 --> 01:46:20,160
that kind of connects the two.

2509
01:46:20,640 --> 01:46:24,080
So you could take an image, put it into the image space,

2510
01:46:24,960 --> 01:46:28,960
then have their little bridge that over to language space.

2511
01:46:29,520 --> 01:46:31,680
And that everything else, the two big models are frozen.

2512
01:46:31,680 --> 01:46:33,280
So they were able to do this on just like

2513
01:46:33,280 --> 01:46:35,360
a couple days worth of GPU time,

2514
01:46:36,080 --> 01:46:37,280
which I do think goes to show

2515
01:46:37,280 --> 01:46:40,400
how it is going to be very difficult to contain proliferation.

2516
01:46:40,400 --> 01:46:41,360
It was just good.

2517
01:46:41,360 --> 01:46:42,880
In my view, that's really good.

2518
01:46:42,880 --> 01:46:44,080
As long as it doesn't get out of control,

2519
01:46:44,080 --> 01:46:45,920
I'm probably with you on that too.

2520
01:46:46,880 --> 01:46:50,080
But by bridging this vision space into the language space,

2521
01:46:50,080 --> 01:46:52,880
then the language model would be able to converse with you

2522
01:46:52,880 --> 01:46:55,520
about the image, even though the language model

2523
01:46:56,080 --> 01:46:57,760
was never trained on images,

2524
01:46:57,760 --> 01:47:01,440
but you just had this connector that kind of bridges those modalities.

2525
01:47:02,240 --> 01:47:03,680
It's like another layer of the network

2526
01:47:03,680 --> 01:47:05,200
that just bridges two networks, almost.

2527
01:47:05,920 --> 01:47:07,840
Yeah, it bridges the spaces.

2528
01:47:07,840 --> 01:47:10,240
It like it bridges the conceptual spaces

2529
01:47:10,240 --> 01:47:12,960
between something that has only understood images

2530
01:47:12,960 --> 01:47:15,040
and something that has only understood language,

2531
01:47:15,040 --> 01:47:17,040
but now you can kind of bring those together.

2532
01:47:17,040 --> 01:47:19,040
As I think about it, it's not that surprising

2533
01:47:19,120 --> 01:47:24,320
because that's what, for example, text image models are basically that.

2534
01:47:24,320 --> 01:47:27,200
They're bridging two spaces in a sense, right?

2535
01:47:27,200 --> 01:47:28,480
But I'll check this paper out.

2536
01:47:28,480 --> 01:47:31,440
So on the one hand, it's not that surprising.

2537
01:47:31,440 --> 01:47:33,280
On their hand, I should see how they implement it

2538
01:47:33,280 --> 01:47:34,480
or whatever, so blip to.

2539
01:47:34,480 --> 01:47:34,960
Okay.

2540
01:47:34,960 --> 01:47:36,960
Yeah, I think the most striking thing about that

2541
01:47:36,960 --> 01:47:38,800
is just how small it is.

2542
01:47:38,800 --> 01:47:42,640
You took these two off-the-shelf models that were trained

2543
01:47:43,760 --> 01:47:45,440
independently for other purposes,

2544
01:47:46,000 --> 01:47:50,880
and you're able to bridge them with a relatively small connector.

2545
01:47:51,600 --> 01:47:54,640
And that seems to be kind of happening all over the place.

2546
01:47:54,640 --> 01:47:57,520
I would also look at the Flamingo architecture,

2547
01:47:57,520 --> 01:48:00,640
which is like a year and a half ago now out of DeepMind.

2548
01:48:01,440 --> 01:48:04,000
That was one for me where I was like, oh my,

2549
01:48:04,000 --> 01:48:06,080
and it's also a language to vision

2550
01:48:06,080 --> 01:48:08,240
where they keep the language model frozen,

2551
01:48:08,240 --> 01:48:10,800
and then they kind of, in my mind,

2552
01:48:10,800 --> 01:48:13,040
it's like I can see the person in their garage

2553
01:48:13,040 --> 01:48:14,800
like tinkering with their soldering iron,

2554
01:48:14,800 --> 01:48:16,080
because it's just like, wow,

2555
01:48:16,080 --> 01:48:18,240
you took this whole language thing that was frozen,

2556
01:48:18,240 --> 01:48:21,280
and you kind of injected some vision stuff here,

2557
01:48:21,280 --> 01:48:22,320
and you added a couple layers,

2558
01:48:22,320 --> 01:48:24,720
and you kind of Frankensteined it, and it works.

2559
01:48:24,720 --> 01:48:26,160
And it's like, wow, that's not really,

2560
01:48:26,720 --> 01:48:29,440
it wasn't like super principled.

2561
01:48:29,440 --> 01:48:31,600
It was just kind of hack a few things together

2562
01:48:31,600 --> 01:48:32,800
and try training it.

2563
01:48:32,800 --> 01:48:34,400
And I don't want to diminish what they did,

2564
01:48:34,400 --> 01:48:36,640
because I'm sure there were more insights to it than that.

2565
01:48:37,200 --> 01:48:41,760
But it seems like we are kind of seeing a reliable pattern

2566
01:48:41,840 --> 01:48:46,640
of the key point here being model-to-model communication

2567
01:48:46,640 --> 01:48:47,920
through high-dimensional space,

2568
01:48:47,920 --> 01:48:51,120
which is not mediated by human language,

2569
01:48:52,240 --> 01:48:55,840
is I think one of the reasons that I would expect,

2570
01:48:55,840 --> 01:48:57,120
and by the way, there's lots of papers too.

2571
01:48:57,120 --> 01:49:00,080
I'm like, language models are human level

2572
01:49:00,080 --> 01:49:01,920
or even superhuman prompt engineers.

2573
01:49:01,920 --> 01:49:05,360
They're self-prompting techniques are getting pretty good.

2574
01:49:06,160 --> 01:49:08,480
So if I'm imagining the big picture of like,

2575
01:49:09,520 --> 01:49:10,800
and we can get back to like,

2576
01:49:10,800 --> 01:49:12,640
okay, well, how do we use any techniques,

2577
01:49:12,640 --> 01:49:14,320
crypto or otherwise, to keep this under control?

2578
01:49:15,440 --> 01:49:18,080
And then I would say this is kind of the newer school

2579
01:49:18,080 --> 01:49:20,880
of the big picture AI safety worry.

2580
01:49:21,520 --> 01:49:23,360
Obviously, there's a lot of flavors,

2581
01:49:23,360 --> 01:49:26,080
but if you were to go look at like a Jay Acotra,

2582
01:49:26,080 --> 01:49:28,080
for example, I think is a really good writer on this.

2583
01:49:29,360 --> 01:49:31,520
Her worldview is less that we're going to have this fume

2584
01:49:31,520 --> 01:49:33,600
and more that over a period of time,

2585
01:49:33,600 --> 01:49:34,880
and it may not be a long period of time.

2586
01:49:34,880 --> 01:49:37,040
Maybe it's like a generation, maybe it's 10 years,

2587
01:49:37,040 --> 01:49:38,400
maybe it's 100 years,

2588
01:49:38,400 --> 01:49:39,760
but obviously those are all small

2589
01:49:39,760 --> 01:49:42,000
in the sort of grand scheme of the future.

2590
01:49:43,120 --> 01:49:45,440
We have in all likelihood,

2591
01:49:46,080 --> 01:49:53,120
the development of AI centric schemes of production,

2592
01:49:53,120 --> 01:49:55,520
where you've got kind of your high level executive function

2593
01:49:55,520 --> 01:49:56,880
is like your language model.

2594
01:49:56,880 --> 01:49:58,720
You've got all these like lower level models.

2595
01:49:58,720 --> 01:49:59,840
They're all bridgeable.

2596
01:49:59,840 --> 01:50:03,200
All the spaces are bridgeable in high dimensional form,

2597
01:50:03,200 --> 01:50:05,360
where they're not really mediated by language,

2598
01:50:05,360 --> 01:50:06,560
unless we enforce that.

2599
01:50:06,560 --> 01:50:10,480
I mean, we could say it must always be mediated by language

2600
01:50:10,480 --> 01:50:12,160
so we can read the logs,

2601
01:50:12,720 --> 01:50:15,360
but there's a tax to that,

2602
01:50:15,360 --> 01:50:18,000
because going through language is like highly compressed

2603
01:50:18,800 --> 01:50:20,880
compared to the high dimensional space to space.

2604
01:50:21,600 --> 01:50:21,920
All right.

2605
01:50:21,920 --> 01:50:24,720
So let me see if I can steal man or articulate your case.

2606
01:50:24,720 --> 01:50:27,280
You're saying AIs are going to get good enough.

2607
01:50:27,280 --> 01:50:28,560
They're going to be able to communicate with each other

2608
01:50:28,560 --> 01:50:31,280
good enough, and they'll be able to do enough tasks

2609
01:50:31,280 --> 01:50:33,680
that more and more humans will be rendered economically

2610
01:50:33,680 --> 01:50:35,120
marginal and unnecessary.

2611
01:50:35,120 --> 01:50:36,800
I'm not saying I think that will happen.

2612
01:50:36,800 --> 01:50:38,480
I'm just saying I think there's a good enough chance

2613
01:50:38,480 --> 01:50:39,200
that that will happen,

2614
01:50:39,200 --> 01:50:41,200
but it's worth taking really seriously.

2615
01:50:41,200 --> 01:50:42,720
I actually think that will happen,

2616
01:50:42,720 --> 01:50:44,400
something along those lines,

2617
01:50:44,400 --> 01:50:47,040
in the sense of at least massive economic disruption.

2618
01:50:47,040 --> 01:50:47,680
Definitely.

2619
01:50:47,680 --> 01:50:48,400
Okay.

2620
01:50:48,400 --> 01:50:50,240
But I'll give an answer to that,

2621
01:50:50,240 --> 01:50:53,040
which is both maybe fun and not fun.

2622
01:50:53,040 --> 01:50:57,120
Have you seen the graph of the percentage of America

2623
01:50:57,120 --> 01:50:58,320
that was involved in farming?

2624
01:50:59,040 --> 01:50:59,280
Yeah.

2625
01:50:59,280 --> 01:51:01,120
I tweeted a version of that once.

2626
01:51:01,680 --> 01:51:02,240
Oh, you did.

2627
01:51:02,240 --> 01:51:02,480
Okay.

2628
01:51:02,480 --> 01:51:02,800
Great.

2629
01:51:02,800 --> 01:51:03,280
Good.

2630
01:51:03,280 --> 01:51:04,480
So you're familiar with this,

2631
01:51:04,480 --> 01:51:07,280
and you're familiar with what I mean by the implication of it,

2632
01:51:07,280 --> 01:51:10,480
where basically Americans used to identify themselves

2633
01:51:10,480 --> 01:51:15,280
as farmers, and manufacturing rose

2634
01:51:15,280 --> 01:51:17,040
as agriculture collapsed.

2635
01:51:17,760 --> 01:51:20,560
And here is the graph on that.

2636
01:51:20,560 --> 01:51:23,040
But from like 40% in the year 1900

2637
01:51:23,760 --> 01:51:26,720
to a total collapse of agriculture,

2638
01:51:26,720 --> 01:51:28,960
and then also more recently a collapse of manufacturing

2639
01:51:28,960 --> 01:51:32,080
into bureaucracy, paperwork, legal work,

2640
01:51:32,080 --> 01:51:34,240
what is up into the right since then

2641
01:51:34,320 --> 01:51:38,640
is the lawyers.

2642
01:51:38,640 --> 01:51:39,680
What is up into the right?

2643
01:51:39,680 --> 01:51:40,800
What is replacing that?

2644
01:51:41,440 --> 01:51:43,520
Starting in around the 1970s,

2645
01:51:44,480 --> 01:51:46,160
we used to be adding energy production

2646
01:51:46,160 --> 01:51:47,760
and energy production flatlined

2647
01:51:47,760 --> 01:51:50,720
once people got angry about nuclear power.

2648
01:51:50,720 --> 01:51:52,320
So this is a future that could have been.

2649
01:51:52,320 --> 01:51:53,600
We could be on Mars by now,

2650
01:51:53,600 --> 01:51:54,800
but we got flatlined.

2651
01:51:54,800 --> 01:51:55,120
Right?

2652
01:51:55,840 --> 01:51:57,600
What did go up into the right?

2653
01:51:57,600 --> 01:51:58,560
So construction costs,

2654
01:51:58,560 --> 01:51:59,760
this is the bad scenario,

2655
01:51:59,760 --> 01:52:02,160
where the miracle energy got destroyed

2656
01:52:02,160 --> 01:52:05,280
because regulations, the cost was flat.

2657
01:52:05,280 --> 01:52:07,840
And then when vertical, when regulations were imposed,

2658
01:52:07,840 --> 01:52:10,960
all the progress was stopped by decels and de-growthers.

2659
01:52:10,960 --> 01:52:12,720
And then Alara was implemented,

2660
01:52:12,720 --> 01:52:16,800
which said nuclear energy has to be as low risk

2661
01:52:16,800 --> 01:52:19,600
as reasonably necessary, as reasonably achievable.

2662
01:52:19,600 --> 01:52:21,840
And that meant that you just keep adding safety to it

2663
01:52:21,840 --> 01:52:23,440
until it's as same as cost as everything else,

2664
01:52:23,440 --> 01:52:25,840
which means you destroyed the value of it.

2665
01:52:27,040 --> 01:52:28,320
But you know what was up into the right?

2666
01:52:28,320 --> 01:52:30,560
What replaced those agriculture and manufacturing jobs?

2667
01:52:30,560 --> 01:52:31,600
Look at this, you see this graph?

2668
01:52:32,960 --> 01:52:34,080
We will put this on YouTube.

2669
01:52:34,080 --> 01:52:37,200
So if you want to see the graph do the YouTube version of this,

2670
01:52:37,200 --> 01:52:38,880
for the audio only group,

2671
01:52:38,880 --> 01:52:41,120
it's an exponential curve in the number of lawyers

2672
01:52:41,120 --> 01:52:42,560
in the United States from,

2673
01:52:42,560 --> 01:52:44,560
looks like maybe two thirds of a million

2674
01:52:44,560 --> 01:52:47,280
to 13 million over the last 140 years.

2675
01:52:47,280 --> 01:52:51,040
Yeah. And in 1880, it was like sub 100,000

2676
01:52:51,040 --> 01:52:52,480
or something like that, right?

2677
01:52:52,480 --> 01:52:55,440
And then it's just like, especially that 1970 point,

2678
01:52:55,440 --> 01:52:57,280
that's when it went totally vertical, okay?

2679
01:52:57,920 --> 01:52:59,760
And it's probably even more sensitive.

2680
01:52:59,760 --> 01:53:03,120
So, you know, if you add paperwork jobs, bureaucratic jobs,

2681
01:53:03,120 --> 01:53:07,040
you know, every lawyer is like, you know, sorry lawyers,

2682
01:53:07,040 --> 01:53:08,880
but you're basically negative value add, right?

2683
01:53:08,880 --> 01:53:11,200
Because it should, the fact that you have a lawyer

2684
01:53:11,200 --> 01:53:15,040
means that you couldn't just self serve a form, right?

2685
01:53:15,040 --> 01:53:16,400
Basic government is platformers

2686
01:53:16,400 --> 01:53:18,480
where you can just self serve and you fill it out.

2687
01:53:18,480 --> 01:53:20,240
And you don't have to have somebody

2688
01:53:20,240 --> 01:53:22,560
like code something for you custom, you know,

2689
01:53:22,560 --> 01:53:23,840
lawyers that's doing custom code

2690
01:53:23,840 --> 01:53:26,320
is because the legal code is so complicated.

2691
01:53:26,320 --> 01:53:28,800
So, you know, the whole Shakespeare thing,

2692
01:53:28,800 --> 01:53:31,120
like first thing we do, let's, you know, kill all the lawyers.

2693
01:53:31,120 --> 01:53:33,760
First thing we do, let's automate all the lawyers, right?

2694
01:53:33,760 --> 01:53:37,440
Only something that's the hammer blow of AI

2695
01:53:37,440 --> 01:53:40,320
can break the backbone and it will.

2696
01:53:41,200 --> 01:53:43,360
It's going to break the backbone of Blue America, right?

2697
01:53:43,360 --> 01:53:45,680
It's going to cause, that's why the political layer

2698
01:53:45,680 --> 01:53:48,960
and the sovereignty layer is not what AI people think about.

2699
01:53:48,960 --> 01:53:51,360
But it's like crucial for thinking about AI

2700
01:53:51,360 --> 01:53:53,600
because what tribes does AI benefit?

2701
01:53:54,400 --> 01:53:59,040
And again, we got away from, why does AI kill everybody?

2702
01:53:59,040 --> 01:54:00,240
Well, it's going to need actuators.

2703
01:54:00,240 --> 01:54:02,000
Who's going to stab you? Who's going to shoot you?

2704
01:54:02,000 --> 01:54:03,680
It's got to be a human hypnotized by AI

2705
01:54:03,680 --> 01:54:05,280
or a drone that AI controls.

2706
01:54:05,280 --> 01:54:08,320
A human hypnotized by AI is actually a conventional threat.

2707
01:54:08,320 --> 01:54:09,360
It looks like a terrorist cell.

2708
01:54:09,360 --> 01:54:10,880
We know how to deal with that, right?

2709
01:54:10,880 --> 01:54:13,120
It's just like radicalized humans that worship some AI

2710
01:54:13,120 --> 01:54:13,920
that stab you.

2711
01:54:13,920 --> 01:54:16,560
It's like the pause AI people are one step, I think,

2712
01:54:16,560 --> 01:54:17,760
away from that, all right?

2713
01:54:17,760 --> 01:54:19,360
But that's just like Aum Shinriko.

2714
01:54:19,360 --> 01:54:20,320
That's like allocated.

2715
01:54:20,320 --> 01:54:22,320
That's like basically terrorists

2716
01:54:22,400 --> 01:54:25,120
who think that the AI is telling them what to do, fine?

2717
01:54:25,120 --> 01:54:29,600
If it's not a human that's stabbing you, it is a drone.

2718
01:54:29,600 --> 01:54:33,520
And that's like a very different future where like five or 10

2719
01:54:33,520 --> 01:54:35,200
or 15 years up, maybe we have enough

2720
01:54:35,200 --> 01:54:36,400
internet connected drones out there,

2721
01:54:36,400 --> 01:54:38,160
but even then they'll have private keys.

2722
01:54:38,160 --> 01:54:41,360
So there's going to be fragmentation of address space.

2723
01:54:41,360 --> 01:54:44,560
Not all drones be controlled by everybody in my view, okay?

2724
01:54:44,560 --> 01:54:46,000
That's what AI safety is.

2725
01:54:46,000 --> 01:54:48,240
AI safety is can you turn it off?

2726
01:54:48,240 --> 01:54:49,120
Can you kill it?

2727
01:54:49,120 --> 01:54:51,760
Can you stop it from controlling drones?

2728
01:54:51,760 --> 01:54:53,200
That's what AI safety is.

2729
01:54:53,200 --> 01:54:54,960
Can you also open the model weights

2730
01:54:54,960 --> 01:54:56,480
so you can generate adversarial inputs?

2731
01:54:57,440 --> 01:54:59,840
Can you open the model weights and proliferate it?

2732
01:54:59,840 --> 01:55:01,040
You're saying, oh, proliferation is bad.

2733
01:55:01,040 --> 01:55:04,720
I'm saying proliferation is good because if everybody has one,

2734
01:55:04,720 --> 01:55:07,360
then nobody has an advantage on it, right?

2735
01:55:07,360 --> 01:55:09,440
Not relatively speaking, okay?

2736
01:55:09,440 --> 01:55:11,920
I have very few super confident positions.

2737
01:55:11,920 --> 01:55:17,040
So I wouldn't necessarily say I think that proliferation is bad.

2738
01:55:17,040 --> 01:55:18,640
I'd say so far it's good.

2739
01:55:19,360 --> 01:55:21,840
It has, and even most of the AI safety people,

2740
01:55:23,120 --> 01:55:26,960
I would say if I could speak on the behalf of the AI safety

2741
01:55:27,920 --> 01:55:31,040
consensus, I would say most people would say

2742
01:55:31,040 --> 01:55:37,040
even that the Llama 2 release has proven good for AI safety

2743
01:55:37,040 --> 01:55:38,080
for the reasons that you're saying.

2744
01:55:38,080 --> 01:55:39,760
But they opposed it.

2745
01:55:39,760 --> 01:55:41,040
Well, some didn't, some didn't.

2746
01:55:41,040 --> 01:55:44,880
I would say the main posture that I see AI safety people taking

2747
01:55:44,880 --> 01:55:49,280
is that we're getting really close to,

2748
01:55:49,280 --> 01:55:50,800
or we might be getting really close.

2749
01:55:51,760 --> 01:55:54,880
Certainly if we just kind of naively extrapolate out recent progress,

2750
01:55:54,880 --> 01:55:58,400
it would seem that we're getting really close to systems

2751
01:55:58,400 --> 01:56:03,360
that are sufficiently powerful that it's very hard to predict

2752
01:56:03,360 --> 01:56:05,680
what happens if they proliferate.

2753
01:56:05,680 --> 01:56:06,800
Llama 2, not there.

2754
01:56:07,600 --> 01:56:12,240
And so, yes, it has enabled a lot of interpretability work.

2755
01:56:12,240 --> 01:56:14,640
It has enabled things like representation engineering,

2756
01:56:15,280 --> 01:56:17,920
which there isn't a lot of good stuff that has come from it.

2757
01:56:17,920 --> 01:56:20,000
The big thing that I want to kind of establish is

2758
01:56:21,120 --> 01:56:23,440
you agree with me on the actuation point or not.

2759
01:56:24,080 --> 01:56:26,160
Like, the thing is this thing, like,

2760
01:56:26,960 --> 01:56:29,600
oh Llama 2 proliferates and so businesses are disrupted

2761
01:56:29,600 --> 01:56:32,400
and people, you know, maybe they paid a lot of money

2762
01:56:32,400 --> 01:56:35,200
for their MD degree and they can't make us a bunch of money.

2763
01:56:35,200 --> 01:56:38,320
That's within the realm of what I call conventional warfare.

2764
01:56:38,320 --> 01:56:39,120
You know what I mean?

2765
01:56:39,120 --> 01:56:41,600
That's like we're still in normal world as we were talking about.

2766
01:56:42,560 --> 01:56:45,200
Unconventional warfare is, you know,

2767
01:56:45,200 --> 01:56:47,040
Skynet arises and kills everybody.

2768
01:56:47,040 --> 01:56:47,360
Okay.

2769
01:56:48,000 --> 01:56:49,840
And that is what is being sold over here.

2770
01:56:50,800 --> 01:56:52,720
And when you think about the actuators,

2771
01:56:52,720 --> 01:56:54,160
we don't have the drones out there.

2772
01:56:54,160 --> 01:56:56,320
We don't have the humanoid robots at control.

2773
01:56:56,320 --> 01:56:59,200
And hypnotized humans are a very tiny subset of humans,

2774
01:56:59,200 --> 01:57:00,000
probably.

2775
01:57:00,000 --> 01:57:02,320
And if they aren't, that just looks like a religion

2776
01:57:02,320 --> 01:57:04,160
or a cult or a terrorist cell.

2777
01:57:04,160 --> 01:57:05,760
And we know how to deal with that as well.

2778
01:57:05,760 --> 01:57:08,480
The super intelligent AI with, you know,

2779
01:57:08,480 --> 01:57:11,840
lots of robots that control in a Starcraft form,

2780
01:57:11,840 --> 01:57:14,640
I would agree is something that humans haven't faced yet.

2781
01:57:14,640 --> 01:57:17,760
But by the time we get that many robots out there,

2782
01:57:17,760 --> 01:57:19,520
you won't be able to control all of them at once

2783
01:57:19,520 --> 01:57:21,120
because of the private key things I mentioned.

2784
01:57:21,920 --> 01:57:24,080
So that's why I'm like, okay,

2785
01:57:24,080 --> 01:57:25,920
everything else we're talking about is in normal world.

2786
01:57:25,920 --> 01:57:28,480
That is the single biggest thing that I wanted to get.

2787
01:57:28,480 --> 01:57:31,840
Like economic disruption, people losing jobs,

2788
01:57:31,840 --> 01:57:34,880
proliferation so that the balance of power is redistributed.

2789
01:57:34,880 --> 01:57:35,920
All that's fine.

2790
01:57:35,920 --> 01:57:38,080
The other reason I say this is people keep trying

2791
01:57:38,080 --> 01:57:40,080
to link AI to existential risk.

2792
01:57:40,080 --> 01:57:42,480
A great example is one of the things you actually had in here.

2793
01:57:42,480 --> 01:57:44,800
This is similar to the AI policy and two things.

2794
01:57:44,800 --> 01:57:45,840
It's a totally reasonable question,

2795
01:57:45,840 --> 01:57:48,160
but then I'm going to, in my view, deconstruct the question.

2796
01:57:48,800 --> 01:57:50,240
What would you think about putting the limit on the right

2797
01:57:50,240 --> 01:57:52,560
to compute or their capabilities an AI system might demonstrate

2798
01:57:52,560 --> 01:57:54,720
that you make you think open access no longer wise?

2799
01:57:54,720 --> 01:57:56,880
Most common near term answer here to be seems to be related

2800
01:57:56,880 --> 01:57:59,440
to risk of pandemic via novel pathogen engineering.

2801
01:57:59,440 --> 01:58:00,240
So guess what?

2802
01:58:00,240 --> 01:58:02,720
You know who the novel pathogen engineers are?

2803
01:58:02,720 --> 01:58:05,200
The US and Chinese governments, right?

2804
01:58:05,200 --> 01:58:07,360
They did it or probably did it,

2805
01:58:07,360 --> 01:58:09,680
credibly did it, credibly being accused of doing it.

2806
01:58:10,240 --> 01:58:11,760
They haven't been punished for COVID-19.

2807
01:58:11,760 --> 01:58:13,360
In fact, they covered up their culpability

2808
01:58:13,360 --> 01:58:15,520
and pointed everywhere other than themselves.

2809
01:58:15,520 --> 01:58:18,800
They used it to gain more power in both the US and China

2810
01:58:18,800 --> 01:58:21,440
with both lockdown in China and in the US

2811
01:58:21,440 --> 01:58:22,960
and all kinds of COVID era.

2812
01:58:23,520 --> 01:58:26,720
Trillions of dollars was printed and spent and so on and so forth.

2813
01:58:26,720 --> 01:58:29,440
They did everything other than actually solve the problem.

2814
01:58:29,440 --> 01:58:33,440
That was actually getting the vaccines in the private sector.

2815
01:58:33,440 --> 01:58:36,160
And they studied the existential risk only to generate it.

2816
01:58:36,160 --> 01:58:38,800
And they're even paid to generate pandemic prevention and failed.

2817
01:58:39,360 --> 01:58:42,160
So this would be the ultimate Fox guarding the hen house.

2818
01:58:42,800 --> 01:58:45,760
Okay, the only reason that the two organizations responsible

2819
01:58:45,760 --> 01:58:47,680
for killing millions of people novel pathogen

2820
01:58:47,680 --> 01:58:53,040
are going to prevent people from doing this by restricting compute.

2821
01:58:53,040 --> 01:58:55,760
No, you know what it is actually what's happening here is

2822
01:58:57,040 --> 01:58:59,040
one of the concepts I have in the network state

2823
01:58:59,040 --> 01:59:01,760
is this idea of God, state and network.

2824
01:59:01,760 --> 01:59:04,480
Okay, meaning what do you think is the most powerful force in the world?

2825
01:59:04,480 --> 01:59:05,520
Is it almighty God?

2826
01:59:06,080 --> 01:59:09,440
Is it the US government or is it encryption?

2827
01:59:10,240 --> 01:59:12,400
Right, or eventually maybe an AGI, right?

2828
01:59:13,040 --> 01:59:18,240
If what's happening here is a lot of people are implicitly

2829
01:59:19,120 --> 01:59:21,840
without realizing it, even if they are secular atheists,

2830
01:59:21,840 --> 01:59:24,160
they're treating GOV as GOD.

2831
01:59:24,160 --> 01:59:27,680
Okay, they treat the US government as God as the final mover.

2832
01:59:28,400 --> 01:59:31,280
No, I appreciate your little I take inspiration from you actually

2833
01:59:31,280 --> 01:59:37,680
in terms of trying to come up with these little quips that are memorable.

2834
01:59:37,680 --> 01:59:41,440
So I was just smiling at that because I think you do a great job of that.

2835
01:59:41,440 --> 01:59:46,560
And I try to encourage, I have less success coining terms than you have,

2836
01:59:46,560 --> 01:59:50,240
but certainly try to follow your example on that front.

2837
01:59:50,240 --> 01:59:53,200
It's like a helpful, if you can compress it down,

2838
01:59:53,200 --> 01:59:54,080
it's like more memorable.

2839
01:59:54,080 --> 01:59:55,280
So that's what I try to do, right?

2840
01:59:55,280 --> 01:59:58,400
So exactly a lot of these people who are secular,

2841
01:59:58,400 --> 01:59:59,840
think of themselves as atheists,

2842
01:59:59,840 --> 02:00:02,560
have just replaced GOD with GOV.

2843
02:00:02,560 --> 02:00:04,480
They worship the US government as God.

2844
02:00:04,480 --> 02:00:05,600
And there's two versions of this.

2845
02:00:05,600 --> 02:00:08,400
You know how like God has put the male and female version, right?

2846
02:00:08,400 --> 02:00:13,840
The female version is the Democrat God within the USA that has infinite money

2847
02:00:13,840 --> 02:00:16,000
and can take care of everybody and care for everybody.

2848
02:00:16,000 --> 02:00:19,280
And the Republican God is the US military that can blow up anybody,

2849
02:00:19,280 --> 02:00:22,320
and it's the biggest and strongest and most powerful America F. Yeah.

2850
02:00:22,320 --> 02:00:23,200
Okay.

2851
02:00:23,200 --> 02:00:30,400
And everybody who thinks of the US government as being able to stop something

2852
02:00:30,400 --> 02:00:32,000
is praying to a dead God.

2853
02:00:32,880 --> 02:00:33,360
Okay.

2854
02:00:33,360 --> 02:00:34,400
When you say this,

2855
02:00:34,400 --> 02:00:37,600
you actually get an interesting reaction from AI safety people

2856
02:00:37,600 --> 02:00:40,080
where you've actually hit their true solar plexus.

2857
02:00:41,680 --> 02:00:42,080
All right.

2858
02:00:42,080 --> 02:00:45,280
The true solar plexus is not that they believe in AI.

2859
02:00:45,280 --> 02:00:46,960
It's that they believe in the US government.

2860
02:00:48,800 --> 02:00:50,400
That's a true solar plexus

2861
02:00:50,400 --> 02:00:52,000
because they are appealing to,

2862
02:00:52,000 --> 02:00:53,520
they're praying to this dead God

2863
02:00:53,520 --> 02:00:57,760
that can't even clean the poop off the streets in San Francisco, right?

2864
02:00:57,760 --> 02:01:00,720
That is losing wars or fighting them to sell me.

2865
02:01:00,720 --> 02:01:03,520
It has lost all these wars around the world

2866
02:01:03,520 --> 02:01:04,800
that spent trillions of dollars

2867
02:01:04,800 --> 02:01:06,800
has been through financial crisis, coronavirus,

2868
02:01:06,800 --> 02:01:10,080
Iraq war, you know, total meltdown politically.

2869
02:01:10,080 --> 02:01:10,640
Okay.

2870
02:01:10,640 --> 02:01:14,320
That is now has interest payments more than the defense budget

2871
02:01:14,880 --> 02:01:16,160
that is, you know,

2872
02:01:16,160 --> 02:01:18,720
that spent $100 billion on the California train

2873
02:01:18,720 --> 02:01:20,080
without laying a single track.

2874
02:01:20,800 --> 02:01:23,440
It's like that, you know, that Morgan Freeman thing for,

2875
02:01:23,440 --> 02:01:24,960
you know, the clip from Batman,

2876
02:01:24,960 --> 02:01:28,720
where he's like, so this man has a billionaire,

2877
02:01:28,720 --> 02:01:29,600
blah, blah, blah, this and that,

2878
02:01:29,600 --> 02:01:32,080
and your plan is to threaten him, right?

2879
02:01:32,080 --> 02:01:34,560
And so you're going to create this super intelligence

2880
02:01:34,560 --> 02:01:36,720
and have Kamala Harris regulate it.

2881
02:01:36,720 --> 02:01:39,120
Come on, man, so to speak, right?

2882
02:01:39,120 --> 02:01:43,520
Like these people are praying to a blind, deaf and dumb God

2883
02:01:43,520 --> 02:01:47,760
that was powerful in 1945, right?

2884
02:01:47,760 --> 02:01:50,640
That's why, by the way, all the popular movies,

2885
02:01:50,640 --> 02:01:51,120
what are they?

2886
02:01:51,120 --> 02:01:53,680
It's Barbie, it's Oppenheimer, right?

2887
02:01:53,680 --> 02:01:55,680
It's, it's Top Gun.

2888
02:01:55,680 --> 02:01:58,640
They're all throwbacks the 80s or the 50s

2889
02:01:58,640 --> 02:02:01,040
when the USA was really big and strong.

2890
02:02:01,760 --> 02:02:03,520
And the future is a black mirror.

2891
02:02:03,520 --> 02:02:04,640
Yeah, I think that's tragic.

2892
02:02:05,600 --> 02:02:06,880
One of the projects that I do like,

2893
02:02:06,880 --> 02:02:07,840
and you might appreciate this,

2894
02:02:07,840 --> 02:02:08,560
I don't know if you've seen it,

2895
02:02:08,560 --> 02:02:12,560
is the, from the future of Life Institute,

2896
02:02:13,120 --> 02:02:16,240
a project called Imagine a World,

2897
02:02:16,240 --> 02:02:17,520
I think is the name of it.

2898
02:02:18,160 --> 02:02:21,200
And they basically challenged, you know,

2899
02:02:21,200 --> 02:02:25,200
their audience and the public to come up with

2900
02:02:25,920 --> 02:02:29,120
positive visions of a future,

2901
02:02:29,120 --> 02:02:31,280
you know, where technology changes a lot.

2902
02:02:31,280 --> 02:02:33,440
And obviously AI pretty central to a lot of those stories.

2903
02:02:34,400 --> 02:02:37,840
And, you know, one of the challenges that people go through

2904
02:02:37,840 --> 02:02:39,520
and how do we get there and whatever,

2905
02:02:39,520 --> 02:02:44,960
but a purposeful effort to imagine positive futures.

2906
02:02:45,840 --> 02:02:47,760
Super under provided.

2907
02:02:47,760 --> 02:02:50,640
And I really liked the,

2908
02:02:50,640 --> 02:02:51,920
the investment that they made in that.

2909
02:02:52,480 --> 02:02:54,000
You know, one of the things I've got

2910
02:02:54,000 --> 02:02:56,320
in the Never See It book is there's certain megatrends

2911
02:02:56,320 --> 02:02:57,840
that are happening, right?

2912
02:02:57,840 --> 02:03:00,240
And megatrends, I mean, it's possible for,

2913
02:03:00,960 --> 02:03:03,760
like one miraculous human maybe to reverse them, okay?

2914
02:03:04,560 --> 02:03:07,520
Because I think both the impersonal force of history theory

2915
02:03:07,520 --> 02:03:09,440
and the great man theory of history have some truth to them.

2916
02:03:10,640 --> 02:03:14,080
But the megatrends are the decline of Washington DC

2917
02:03:14,960 --> 02:03:15,920
the rise of the internet,

2918
02:03:15,920 --> 02:03:17,280
the rise of India, the rise of China.

2919
02:03:18,160 --> 02:03:19,840
That is like my worldview.

2920
02:03:19,840 --> 02:03:23,680
And I can give a thousand graphs and charts and so on for that.

2921
02:03:23,680 --> 02:03:25,280
But that's basically the last 30 years.

2922
02:03:26,000 --> 02:03:27,840
And maybe the next X, right?

2923
02:03:27,840 --> 02:03:29,360
I'm not saying there can't be trend reversal.

2924
02:03:29,360 --> 02:03:30,400
Of course it can be trend reversal,

2925
02:03:30,400 --> 02:03:32,480
as I just mentioned, some hammer blow could hit it,

2926
02:03:32,480 --> 02:03:33,360
but that's what's happening.

2927
02:03:33,920 --> 02:03:35,280
And so because of that,

2928
02:03:35,280 --> 02:03:37,520
the people who are optimistic about the future

2929
02:03:37,520 --> 02:03:39,680
are aligned with either the internet, India or China.

2930
02:03:40,320 --> 02:03:42,880
And the people who are not optimistic about the future

2931
02:03:42,880 --> 02:03:46,480
are blue Americans or left out red Americans, okay?

2932
02:03:46,480 --> 02:03:50,720
Or Westerners in general who are not tech people, okay?

2933
02:03:50,720 --> 02:03:53,120
If they're not tech people, they're not up into the right,

2934
02:03:53,680 --> 02:03:55,200
basically, because the internet's,

2935
02:03:55,200 --> 02:03:57,920
if you, I mean, one of the things is we have a misnomer,

2936
02:03:57,920 --> 02:03:59,280
as I was saying earlier,

2937
02:03:59,280 --> 02:04:00,720
of calling it the United States,

2938
02:04:00,720 --> 02:04:02,400
because it's the dis-United States.

2939
02:04:02,400 --> 02:04:03,920
It's like talking about,

2940
02:04:03,920 --> 02:04:05,680
you know, talking about America is like talking about Korea.

2941
02:04:05,680 --> 02:04:06,880
There's North Korea and South Korea,

2942
02:04:06,880 --> 02:04:08,720
and they're totally different populations.

2943
02:04:08,720 --> 02:04:10,880
And, you know, communism and capitalism

2944
02:04:10,880 --> 02:04:12,400
are totally different systems.

2945
02:04:12,400 --> 02:04:14,560
And the thing that is good for one

2946
02:04:14,560 --> 02:04:16,480
is bad for another and vice versa.

2947
02:04:16,480 --> 02:04:18,080
And so like America doesn't exist.

2948
02:04:18,080 --> 02:04:19,600
There's only, just like there's no Korea,

2949
02:04:19,600 --> 02:04:21,040
there's only North Korea and South Korea,

2950
02:04:21,040 --> 02:04:22,160
there's no America.

2951
02:04:22,160 --> 02:04:23,920
There is blue America and red America

2952
02:04:23,920 --> 02:04:25,840
and also gray America, tech America.

2953
02:04:25,840 --> 02:04:29,120
And blue America is harmed,

2954
02:04:29,120 --> 02:04:30,160
or they think they're harmed,

2955
02:04:30,160 --> 02:04:32,720
or they've gotten themselves into a spot where they're harmed,

2956
02:04:32,720 --> 02:04:34,480
by every technological development,

2957
02:04:34,480 --> 02:04:37,120
which is why they hate it so much, right?

2958
02:04:37,120 --> 02:04:38,800
AI versus journalist jobs,

2959
02:04:38,800 --> 02:04:41,440
crypto takes away banking jobs, you know,

2960
02:04:41,440 --> 02:04:42,960
everything, you know, self-driving cars,

2961
02:04:42,960 --> 02:04:45,520
they just take away regulator control, right?

2962
02:04:45,520 --> 02:04:48,240
Anything that reduces their power, they hate,

2963
02:04:48,240 --> 02:04:50,560
and they're just trying to freeze an amber with regulations.

2964
02:04:50,560 --> 02:04:52,800
Red America got crushed a long time ago

2965
02:04:52,800 --> 02:04:54,720
by offshoring to China and so on.

2966
02:04:54,720 --> 02:04:56,880
They're making, you know, inroads ally

2967
02:04:56,880 --> 02:04:58,800
with tech America or gray America.

2968
02:04:58,800 --> 02:05:01,360
Tech America is like the one piece of America

2969
02:05:01,360 --> 02:05:03,760
that's actually still functional and globally competitive.

2970
02:05:03,760 --> 02:05:06,240
And people always do this fallacy of aggregation,

2971
02:05:06,240 --> 02:05:08,000
where they talk about the USA,

2972
02:05:08,000 --> 02:05:10,720
and it's really this component that's up and to the right,

2973
02:05:10,720 --> 02:05:12,640
and the others that are down and to the right,

2974
02:05:12,640 --> 02:05:15,520
or at best flat, like red, but they're like down, right?

2975
02:05:15,520 --> 02:05:17,760
Like red is like okay functional, blue is down.

2976
02:05:18,880 --> 02:05:21,440
Point is, tech America, I think we're gonna find,

2977
02:05:21,440 --> 02:05:27,440
is not even truly, or how American is tech America,

2978
02:05:27,440 --> 02:05:30,160
because it's like 50% immigrants, right?

2979
02:05:30,160 --> 02:05:31,680
And like a lot of children immigrants,

2980
02:05:31,680 --> 02:05:33,840
and most of their customers are overseas,

2981
02:05:33,840 --> 02:05:36,720
and their users are overseas,

2982
02:05:36,720 --> 02:05:40,320
and their vantage point is global, right?

2983
02:05:40,320 --> 02:05:41,920
And they're basically not,

2984
02:05:43,040 --> 02:05:45,680
I know we're in this ultra-nationalist kick right now,

2985
02:05:45,680 --> 02:05:46,960
and I know that there's gonna be,

2986
02:05:47,840 --> 02:05:49,840
there's a degree of a fork here,

2987
02:05:49,840 --> 02:05:54,160
where you fork technology into Silicon Valley

2988
02:05:54,160 --> 02:05:56,320
and the internet, okay?

2989
02:05:56,320 --> 02:05:58,880
Where Silicon Valley is American,

2990
02:05:58,880 --> 02:06:01,040
and they'll be making like American military equipment,

2991
02:06:01,040 --> 02:06:03,040
and so on and so forth, and they're signaling USA,

2992
02:06:03,040 --> 02:06:04,640
which is fine, okay?

2993
02:06:04,640 --> 02:06:08,560
And then the internet is international global capitalism,

2994
02:06:08,560 --> 02:06:13,040
and the difference is Silicon Valley, or let's say US tech,

2995
02:06:13,040 --> 02:06:16,480
let me be less, you know, US tech says ban TikTok,

2996
02:06:16,480 --> 02:06:17,680
build military equipment, et cetera,

2997
02:06:17,680 --> 02:06:19,280
it's really identifying itself as American,

2998
02:06:19,840 --> 02:06:22,320
and it's thinking of being anti-China, okay?

2999
02:06:22,320 --> 02:06:24,640
But there's, US and China are only 20% of the world,

3000
02:06:24,640 --> 02:06:26,720
80% of the world is neither American nor Chinese.

3001
02:06:27,280 --> 02:06:30,240
So the internet is for everybody else

3002
02:06:30,240 --> 02:06:34,080
who wants actual global rule of law, right?

3003
02:06:34,080 --> 02:06:36,240
When as the US decays as a rules-based order,

3004
02:06:36,240 --> 02:06:37,920
and people don't wanna be under China,

3005
02:06:38,000 --> 02:06:40,960
people wanna be under something like blockchains,

3006
02:06:40,960 --> 02:06:43,520
where you've got like property rights contract law

3007
02:06:43,520 --> 02:06:46,960
across borders that are enforced by an impartial authority, okay?

3008
02:06:46,960 --> 02:06:49,280
That's also the kind of laws that can bind AIs,

3009
02:06:49,280 --> 02:06:50,480
like AIs across borders,

3010
02:06:50,480 --> 02:06:52,160
if you wanna make sure they're gonna do something,

3011
02:06:52,160 --> 02:06:55,280
cryptography can bind an AI in such a way that it can't fake it.

3012
02:06:55,280 --> 02:06:57,360
It can't, an AI can't mint more Bitcoin, you know?

3013
02:06:58,160 --> 02:06:59,360
Here's my last question for you.

3014
02:07:00,000 --> 02:07:04,320
AI discourse right now does seem to be polarizing into camps.

3015
02:07:04,320 --> 02:07:06,640
Obviously a big way that you think about the world

3016
02:07:06,640 --> 02:07:09,360
is by trying to figure out, you know,

3017
02:07:09,360 --> 02:07:10,320
what are the different camps?

3018
02:07:10,320 --> 02:07:11,600
How do they relate to each other?

3019
02:07:11,600 --> 02:07:12,400
So on and so forth.

3020
02:07:13,600 --> 02:07:18,000
I have the view that AI is so weird,

3021
02:07:18,560 --> 02:07:21,360
and so unlike other things that we've encountered in the past,

3022
02:07:21,360 --> 02:07:23,360
including just like, unlike humans, right?

3023
02:07:23,360 --> 02:07:25,440
I always say AI, alien intelligence,

3024
02:07:26,080 --> 02:07:30,160
that I feel like it's really important to borrow a phrase

3025
02:07:30,160 --> 02:07:32,640
from Paul Graham, keep our identities small,

3026
02:07:33,440 --> 02:07:36,160
and try to have a scout mindset

3027
02:07:36,880 --> 02:07:40,640
to really just take things on their own terms, right?

3028
02:07:40,640 --> 02:07:42,480
And not necessarily put them through a prism

3029
02:07:42,480 --> 02:07:43,760
of like, whose team am I on?

3030
02:07:43,760 --> 02:07:46,640
Or, you know, is this benefit my team

3031
02:07:46,640 --> 02:07:47,920
or hurt the other team or whatever?

3032
02:07:48,800 --> 02:07:52,880
But, you know, just try to be as kind of directly engaged

3033
02:07:52,880 --> 02:07:55,040
with the things themselves as we can

3034
02:07:55,040 --> 02:07:57,360
without mediating it through all these lenses.

3035
02:07:57,360 --> 02:07:58,800
You know, I think about, you mentioned like,

3036
02:07:59,600 --> 02:08:01,440
the gain of function, right?

3037
02:08:01,440 --> 02:08:04,240
And I don't know for sure what happened,

3038
02:08:04,240 --> 02:08:06,400
but it certainly does seem like there's a

3039
02:08:06,400 --> 02:08:08,640
very significant chance that it was a lab leak.

3040
02:08:08,640 --> 02:08:10,560
Certainly there's a long history of lab leaks,

3041
02:08:11,200 --> 02:08:13,120
but it would be like, you know,

3042
02:08:13,120 --> 02:08:15,760
it would seem to me a failure to say,

3043
02:08:15,760 --> 02:08:18,800
okay, well, what's the opposite

3044
02:08:18,800 --> 02:08:20,880
of just having like a couple of government labs?

3045
02:08:20,880 --> 02:08:23,200
Like everybody gets their own gain of function lab, right?

3046
02:08:23,200 --> 02:08:24,640
Like if we could, and this is kind of what we're doing

3047
02:08:24,640 --> 02:08:25,680
with AI, we're like,

3048
02:08:25,680 --> 02:08:28,160
let's compress this power down to as small as we can.

3049
02:08:28,160 --> 02:08:30,160
Let's make a kit that can run in everybody's home.

3050
02:08:31,040 --> 02:08:34,960
Would we want to send out these like gain of function,

3051
02:08:34,960 --> 02:08:38,800
you know, wet lab research kits to like every home in the world

3052
02:08:38,800 --> 02:08:41,360
and be like, hope you find something interesting,

3053
02:08:41,360 --> 02:08:45,280
you know, like let us know if you find any new pathogens

3054
02:08:45,280 --> 02:08:46,880
or hey, maybe you'll find life-saving drugs,

3055
02:08:46,880 --> 02:08:48,720
like whatever, we'll see what you find,

3056
02:08:48,720 --> 02:08:50,160
you know, all eight billion of you.

3057
02:08:50,800 --> 02:08:54,160
That to me seems like it would be definitely a big misstep.

3058
02:08:54,160 --> 02:08:57,280
And that's the kind of thing that I see coming out of

3059
02:08:58,640 --> 02:09:02,400
ideologically motivated reasoning or like,

3060
02:09:02,400 --> 02:09:03,680
you know, tribal reasoning.

3061
02:09:03,680 --> 02:09:06,560
And so I guess I wonder how you think about the role

3062
02:09:07,120 --> 02:09:10,960
that tribalism and ideology is playing

3063
02:09:10,960 --> 02:09:14,400
and should or shouldn't play as we try to understand AI.

3064
02:09:15,040 --> 02:09:17,760
Okay, so first is you're absolutely right

3065
02:09:17,760 --> 02:09:22,960
that just because A is bad does not mean that B is good, right?

3066
02:09:22,960 --> 02:09:26,320
So A could be a bad option, B could be a bad option,

3067
02:09:26,320 --> 02:09:27,280
C could be a bad option.

3068
02:09:27,840 --> 02:09:30,320
There might be, you have to go down to option G

3069
02:09:30,320 --> 02:09:31,520
before you find a good option.

3070
02:09:31,520 --> 02:09:33,600
Or there might be three good options and seven bad options,

3071
02:09:33,600 --> 02:09:34,800
for example, right?

3072
02:09:34,800 --> 02:09:37,600
So to map that here, in my view,

3073
02:09:37,600 --> 02:09:40,480
an extremely bad option is to ask the U.S.

3074
02:09:40,480 --> 02:09:42,560
and Chinese governments to do something.

3075
02:09:43,200 --> 02:09:46,640
Anything the U.S. government does at the federal level,

3076
02:09:46,640 --> 02:09:49,120
at the state level, in blue states, at the city level,

3077
02:09:49,680 --> 02:09:50,640
has been a failure.

3078
02:09:51,200 --> 02:09:53,440
And the way, here's a metaway of thinking about it.

3079
02:09:53,440 --> 02:09:54,880
You invest in companies, right?

3080
02:09:54,880 --> 02:09:57,600
So as an investor, here's a really important thing.

3081
02:09:58,400 --> 02:10:00,080
You might have 10 people who come to you

3082
02:10:00,080 --> 02:10:02,000
with the same words in their pitch.

3083
02:10:02,000 --> 02:10:04,080
They're all, for example, building social networks.

3084
02:10:04,800 --> 02:10:06,400
But one of them is Facebook,

3085
02:10:06,400 --> 02:10:09,600
and the others are Friendster and whatever, okay?

3086
02:10:09,600 --> 02:10:10,960
And no offense to Friendster, you know,

3087
02:10:10,960 --> 02:10:13,840
these guys were like, you know, pioneers in their own way,

3088
02:10:14,400 --> 02:10:16,640
but they just got outmatched by Facebook.

3089
02:10:16,640 --> 02:10:18,480
So the point is that the words were the same

3090
02:10:19,040 --> 02:10:20,560
on each of these packages,

3091
02:10:20,560 --> 02:10:22,240
but the execution was completely different.

3092
02:10:22,960 --> 02:10:26,880
So could I imagine a highly competent government

3093
02:10:26,880 --> 02:10:31,200
that could execute and that actually did, you know,

3094
02:10:32,000 --> 02:10:34,800
like, you know, make the right balance of things and so on?

3095
02:10:34,800 --> 02:10:36,400
I can't say it's impossible,

3096
02:10:36,400 --> 02:10:39,040
but I can say that it wouldn't be this government.

3097
02:10:40,640 --> 02:10:44,000
Okay, and so you are talking about the words,

3098
02:10:44,000 --> 02:10:45,680
and I'm talking about the substance.

3099
02:10:45,680 --> 02:10:48,640
The words are, we will protect you from AI, right?

3100
02:10:48,640 --> 02:10:49,760
In my view, the substances,

3101
02:10:49,760 --> 02:10:51,920
they aren't protecting you from anything, right?

3102
02:10:51,920 --> 02:10:53,440
You're basically giving money and power

3103
02:10:53,440 --> 02:10:56,480
to a completely incompetent and, in fact, malicious organization,

3104
02:10:56,480 --> 02:10:59,040
which is Washington DC, which is the U.S. government,

3105
02:10:59,040 --> 02:11:01,600
that has basically over the last 30 years

3106
02:11:01,600 --> 02:11:05,360
gone from a hyperpower that wins everywhere without fighting

3107
02:11:05,360 --> 02:11:07,680
to a declining power that fights everywhere without winning.

3108
02:11:08,960 --> 02:11:12,320
Okay, like just literally burn trillions of dollars doing this,

3109
02:11:12,320 --> 02:11:14,480
take maybe the greatest decline in fortunes

3110
02:11:14,480 --> 02:11:16,400
in 30 years and maybe human history.

3111
02:11:16,400 --> 02:11:18,400
Not even the Roman Empire went down this fast

3112
02:11:18,400 --> 02:11:21,440
on this many power dimensions this quickly, right?

3113
02:11:21,440 --> 02:11:25,040
So giving that guy, let's trust him.

3114
02:11:25,040 --> 02:11:27,840
That's just people running an old script in their heads

3115
02:11:27,840 --> 02:11:28,880
that they inherited.

3116
02:11:28,880 --> 02:11:31,200
They are not thinking about it from first principles that

3117
02:11:31,200 --> 02:11:34,400
this state is a failure, okay?

3118
02:11:34,400 --> 02:11:35,600
And like how much of a failure it is,

3119
02:11:35,600 --> 02:11:36,960
you have to look at the sovereign debt crisis,

3120
02:11:36,960 --> 02:11:39,360
you have to look at graphs that other people aren't looking at,

3121
02:11:39,360 --> 02:11:44,320
but like, you know, the domain of what Blue America can regulate

3122
02:11:44,320 --> 02:11:48,720
is already collapsing because it can't regulate Russia anymore.

3123
02:11:48,720 --> 02:11:50,240
It can't regulate China anymore.

3124
02:11:50,240 --> 02:11:52,160
It's less able to regulate India.

3125
02:11:52,160 --> 02:11:54,720
It's less able even to regulate Florida and Texas.

3126
02:11:54,720 --> 02:11:56,720
States are breaking away from it domestically.

3127
02:11:56,720 --> 02:11:58,000
So this gets to your other point.

3128
02:11:58,000 --> 02:12:02,640
Why is the tribal lens not something that we can put in the back,

3129
02:12:02,640 --> 02:12:04,800
we have to put in the absolute front?

3130
02:12:04,800 --> 02:12:06,880
Because the world is retribalizing.

3131
02:12:07,680 --> 02:12:11,760
Like basically your tribe determines what law you're bound by.

3132
02:12:11,760 --> 02:12:15,440
If you think you can pass some policy that binds the whole world,

3133
02:12:15,440 --> 02:12:18,480
well, there have to be guys with guns who enforce that policy.

3134
02:12:18,480 --> 02:12:20,880
And if I have guys with guns on their side that say,

3135
02:12:20,880 --> 02:12:23,280
we're not enforcing that policy, then you have no policy.

3136
02:12:23,280 --> 02:12:24,880
You've only bound your own people.

3137
02:12:24,880 --> 02:12:26,240
Does that make sense, right?

3138
02:12:26,240 --> 02:12:31,440
And so Blue America will probably succeed in choking the life out of AI

3139
02:12:31,440 --> 02:12:32,800
within Blue America.

3140
02:12:32,800 --> 02:12:35,440
But Blue America controls less and less of the world.

3141
02:12:36,080 --> 02:12:37,920
So it'll have more power over fewer people.

3142
02:12:38,880 --> 02:12:41,440
I can go into why this is, but essentially, you know,

3143
02:12:42,160 --> 02:12:44,400
a financial Berlin Wall is arising.

3144
02:12:44,400 --> 02:12:46,960
There's a lot of taxation and regulation

3145
02:12:47,520 --> 02:12:51,120
and effectively financial repression de facto confiscation

3146
02:12:51,120 --> 02:12:53,680
that will have to happen for the level of debt service

3147
02:12:53,680 --> 02:12:55,120
that the US is being taking on.

3148
02:12:55,120 --> 02:12:58,160
OK, just there's one graph just to make the point.

3149
02:12:58,160 --> 02:13:01,200
And if you want to dig into this, you can.

3150
02:13:02,000 --> 02:13:06,000
But the reason this impacts things is when you're talking about AI safety,

3151
02:13:06,000 --> 02:13:08,080
you're talking about AI regulation.

3152
02:13:08,080 --> 02:13:10,640
You're talking about the US government, right?

3153
02:13:10,640 --> 02:13:12,960
And you have to ask, what does that actually mean?

3154
02:13:13,680 --> 02:13:18,080
And it's like, in my view, it's like asking the Soviet Union in 1989

3155
02:13:18,080 --> 02:13:20,000
to regulate the internet, right?

3156
02:13:20,000 --> 02:13:22,560
That's going to outlive, you know, the country.

3157
02:13:22,560 --> 02:13:25,280
US interest payment on federal debt versus defense spending.

3158
02:13:25,280 --> 02:13:26,960
The white line is defense spending.

3159
02:13:26,960 --> 02:13:27,840
Look at the red line.

3160
02:13:27,840 --> 02:13:29,200
That's just gone absolutely vertical.

3161
02:13:29,200 --> 02:13:30,320
That's interest.

3162
02:13:30,320 --> 02:13:33,440
And it's going to go more vertical next year

3163
02:13:33,440 --> 02:13:35,840
because all of this debt is getting refinanced

3164
02:13:36,720 --> 02:13:37,840
at much higher interest rates.

3165
02:13:37,840 --> 02:13:39,600
That's why look at this.

3166
02:13:39,600 --> 02:13:41,920
You want you want AI timelines, right?

3167
02:13:41,920 --> 02:13:43,680
The question for me is DC's timeline.

3168
02:13:44,240 --> 02:13:45,920
What is DC's time left to live?

3169
02:13:46,560 --> 02:13:49,920
OK, this is the kind of thing that kills empires

3170
02:13:50,000 --> 02:13:53,440
and you either have this just go to the absolute moon

3171
02:13:54,000 --> 02:13:56,480
or they cut rates and they print a lot.

3172
02:13:56,480 --> 02:13:59,920
And either way, you know, the fundamental assumption

3173
02:13:59,920 --> 02:14:03,040
underpinning all the AI safety, all the AI regulation work

3174
02:14:03,040 --> 02:14:06,720
is that they have a functional golem in Washington DC

3175
02:14:06,720 --> 02:14:08,240
where if they convince it to do something,

3176
02:14:08,240 --> 02:14:11,360
it has enough power to control enough of the world.

3177
02:14:11,360 --> 02:14:12,720
When that assumption is broken,

3178
02:14:14,000 --> 02:14:18,000
then a lot of assumptions are broken, right?

3179
02:14:18,000 --> 02:14:21,600
And so in my view, you have to you must think

3180
02:14:21,600 --> 02:14:23,280
about a polytheistic AI world

3181
02:14:23,920 --> 02:14:26,640
because other tribes are already into this.

3182
02:14:26,640 --> 02:14:29,120
They're already funding their own, right?

3183
02:14:29,120 --> 02:14:30,800
The proliferation is already happening

3184
02:14:31,520 --> 02:14:33,120
and they're not going to bow to blue tribes.

3185
02:14:33,120 --> 02:14:36,960
So that's why I think the tribal lens is not secondary.

3186
02:14:36,960 --> 02:14:40,080
It's not some, you know, totally separate thing.

3187
02:14:40,080 --> 02:14:42,800
It is an absolutely primary way in which to look at this.

3188
02:14:42,800 --> 02:14:44,880
And in a sense, it's almost like a, you know,

3189
02:14:44,880 --> 02:14:46,800
in a well done movie.

3190
02:14:47,520 --> 02:14:50,640
All the plot lines come together at the end.

3191
02:14:51,760 --> 02:14:52,560
Okay.

3192
02:14:52,560 --> 02:14:54,560
And all the disruptions that are happening,

3193
02:14:54,560 --> 02:14:57,200
the China disruption, the rise of India,

3194
02:14:57,200 --> 02:15:00,240
the rise of the internet, the rise of crypto,

3195
02:15:00,240 --> 02:15:02,880
the rise of AI and the decline of DC

3196
02:15:02,880 --> 02:15:04,560
and the internal political conflict

3197
02:15:05,120 --> 02:15:06,640
and, you know, various other theaters

3198
02:15:06,640 --> 02:15:10,240
like what's happening in Europe and, you know, and Middle East.

3199
02:15:10,240 --> 02:15:13,360
All of those come together into a crescendo of,

3200
02:15:13,360 --> 02:15:16,240
oh, there's a lot of those graphs are all having the same time.

3201
02:15:16,960 --> 02:15:19,520
And it's not something you can analyze by just, I think,

3202
02:15:19,520 --> 02:15:21,440
looking at one of these curves on its own.

3203
02:15:21,440 --> 02:15:22,960
I think that's a great note to wrap on.

3204
02:15:22,960 --> 02:15:26,080
I am always lamenting the fact that so many people

3205
02:15:26,080 --> 02:15:29,040
are thinking about this AI moment

3206
02:15:29,040 --> 02:15:32,400
in just fundamentally too small of terms.

3207
02:15:32,960 --> 02:15:37,120
But I don't think you're one that will easily be accused of that.

3208
02:15:37,120 --> 02:15:40,560
So with an invitation to come back

3209
02:15:40,560 --> 02:15:42,640
and continue in the not too distant future,

3210
02:15:42,640 --> 02:15:45,680
for now, I will say apology, Srinivasan.

3211
02:15:45,680 --> 02:15:47,840
Thank you for being part of the Cognitive Revolution.

3212
02:15:48,960 --> 02:15:50,640
Thank you, Nathan. Good to be here.

3213
02:15:50,640 --> 02:15:52,640
It is both energizing and enlightening

3214
02:15:52,640 --> 02:15:53,920
to hear why people listen

3215
02:15:53,920 --> 02:15:56,320
and learn what they value about the show.

3216
02:15:56,320 --> 02:16:01,760
So please don't hesitate to reach out via email at TCR at turpentine.co

3217
02:16:01,760 --> 02:16:05,040
or you can DM me on the social media platform of your choice.

3218
02:16:06,080 --> 02:16:08,480
Omnike uses generative AI

3219
02:16:08,480 --> 02:16:11,360
to enable you to launch hundreds of thousands of ad iterations

3220
02:16:11,360 --> 02:16:14,880
that actually work customized across all platforms

3221
02:16:14,880 --> 02:16:16,160
with a click of a button.

3222
02:16:16,160 --> 02:16:18,720
I believe in Omnike so much that I invested in it

3223
02:16:18,720 --> 02:16:20,320
and I recommend you use it too.

3224
02:16:21,040 --> 02:16:27,280
Use COGRAV to get a 10% discount.

