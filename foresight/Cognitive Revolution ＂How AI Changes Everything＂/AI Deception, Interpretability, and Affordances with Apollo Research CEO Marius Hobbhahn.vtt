WEBVTT

00:00.000 --> 00:04.800
So the more pressure we add, the more likely the model is to be deceptive.

00:04.800 --> 00:07.920
So kind of in the same way in which a human would act, it also acts.

00:07.920 --> 00:12.480
You know, removing pressure and adding additional options will very quickly decrease

00:12.480 --> 00:14.400
the probability of being deceptive.

00:14.400 --> 00:17.200
Open source has been really good so far in many, many ways.

00:17.200 --> 00:19.200
It has been very positive for society, right?

00:19.200 --> 00:22.320
I think a lot of ML research could not have happened without open source.

00:22.320 --> 00:25.200
A lot of safety research could not have happened with open source.

00:25.200 --> 00:28.880
At some point, the system is so powerful that you don't want it to be open source anymore.

00:28.880 --> 00:33.280
In the same way in which, you know, I don't want to open source the nuclear codes or like,

00:33.280 --> 00:37.040
you know, literally the recipe to build most viral pandemic or something.

00:37.040 --> 00:42.640
The labs maybe have the incentive to not say the worst things they found because otherwise

00:42.640 --> 00:43.840
they may lose their contract.

00:43.840 --> 00:48.480
So you need something like the UKASF Institute or the USASF Institute.

00:48.480 --> 00:54.000
Make sure that there is a minimal set of standards that all the auditors have to adhere to.

00:54.000 --> 00:56.320
Hello and welcome to The Cognitive Revolution,

00:56.320 --> 01:00.720
where we interview visionary researchers, entrepreneurs, and builders working on the

01:00.720 --> 01:02.400
frontier of artificial intelligence.

01:03.200 --> 01:07.520
Each week, we'll explore their revolutionary ideas and together we'll build a picture of

01:07.520 --> 01:12.560
how AI technology will transform work, life, and society in the coming years.

01:13.120 --> 01:16.240
I'm Nathan LaBenz, joined by my co-host Eric Torenberg.

01:17.040 --> 01:19.680
Hello and welcome back to The Cognitive Revolution.

01:20.640 --> 01:25.280
Today, my guest is Marius Haban, founder and CEO of Apollo Research,

01:25.280 --> 01:30.880
a nonprofit AI safety research group that is working to understand both how AI systems behave

01:31.440 --> 01:32.160
and why.

01:33.760 --> 01:39.120
Their approach combines exploratory and hypothesis-driven testing, fine-tuning experiments,

01:39.120 --> 01:40.960
and interpretability research.

01:41.520 --> 01:47.040
And as you'll hear, they place special emphasis on the potential for AI systems to deceive

01:47.040 --> 01:48.000
their human users.

01:49.200 --> 01:53.760
In this conversation, we look first at Apollo's starting framework for their work,

01:53.840 --> 01:57.280
which emphasizes the importance of affordances in AI systems.

01:57.920 --> 02:03.760
That is, through what tools, actuators, or other means can the system affect the broader world?

02:04.880 --> 02:08.880
And they also introduce a number of new conceptual distinctions meant to help people have more

02:08.880 --> 02:12.880
precise and productive conversations about these nuanced topics.

02:14.240 --> 02:17.760
Then in the second half, we look at their first research result,

02:17.760 --> 02:21.200
which demonstrates, to my knowledge, for the first time in a realistic,

02:21.200 --> 02:28.400
unprompted setting that GPT-4, when put under pressure, will sometimes take unethical and even

02:28.400 --> 02:33.600
illegal actions and then go on to lie to its users about what it did and why.

02:34.960 --> 02:40.080
This is an important result, demonstrating that while the risk from AI systems may start with

02:40.080 --> 02:47.920
and may even be dominated by intentional human misuse, the models themselves can also misbehave

02:47.920 --> 02:49.200
in unexpected ways.

02:49.840 --> 02:54.560
As an aside, since I told my behind-the-scenes GPT-4 red team story a few weeks ago,

02:54.560 --> 02:59.120
a number of people have reached out to ask me how they too can get involved with red teaming projects.

03:00.160 --> 03:04.880
Unfortunately, as commercial competition and secrecy both continue to ramp up across the space,

03:05.440 --> 03:09.440
I don't see as many open calls for volunteer red teamers as I used to,

03:10.000 --> 03:12.560
certainly not for unreleased frontier models.

03:13.920 --> 03:17.920
Instead, the field is becoming more professionalized, with all the leading labs,

03:17.920 --> 03:23.360
as well as the data companies like Scale AI, plus the independent auditing organizations like Apollo,

03:23.360 --> 03:28.960
ArchieVals, now known as Meter, Palisade, and also AI Forensics, all actively hiring

03:28.960 --> 03:31.600
research scientists and engineers in this area.

03:32.880 --> 03:37.200
So does that mean that there's no longer a role for the independent hobbyist red teamer to play?

03:37.920 --> 03:42.640
On the contrary, there is a ton left to discover even on publicly released models,

03:42.640 --> 03:47.600
and the best way to break into the field is to demonstrate your ability to discover new phenomena.

03:48.720 --> 03:53.840
Importantly, the work we cover in this episode could have been done by anyone with an open AI

03:53.840 --> 03:59.920
account, a knack for prompting, and just a tiny bit of coding know-how. No special access or

03:59.920 --> 04:04.800
advanced machine learning techniques were required, just a lot of curiosity.

04:06.080 --> 04:10.000
With that in mind, if you want to get into this line of work but aren't sure where to start,

04:10.000 --> 04:15.520
I encourage you to reach out. I'll be happy to help brainstorm or refine your project ideas,

04:15.600 --> 04:20.480
and I can also help connect you with folks at the top companies who do sometimes provide API

04:20.480 --> 04:25.440
credits to independent researchers working in this area, if and when you can achieve a meaningful

04:25.440 --> 04:31.360
result. As always, we appreciate the time that you spend listening to The Cognitive Revolution,

04:31.360 --> 04:37.040
and we hope it's a valuable guide to the AI era. If you feel that it is, we would love a review

04:37.040 --> 04:42.240
on Apple Podcasts or Spotify, and we of course encourage you to share the show with your friends.

04:43.200 --> 04:49.680
Now, here's my conversation on frontier AI safety work with Marius Habhan of Apollo Research.

04:50.880 --> 04:55.920
Marius Habhan, founder and CEO of Apollo Research, welcome to The Cognitive Revolution.

04:56.720 --> 04:58.160
Hey, thanks for having me.

04:58.160 --> 05:03.600
I am very excited to have you. So, regular listeners of the show will know that I'm a big

05:03.600 --> 05:11.760
believer in the importance of hands-on testing of what AI systems can do, and also that I have

05:11.760 --> 05:18.000
been pretty enthusiastic consumer of the news when some of the leading labs have made public

05:18.000 --> 05:24.800
commitments to allow organizations outside of their own teams to look at the systems that

05:24.800 --> 05:30.880
they're building before they get deployed. And so, your work with Apollo Research, which is

05:31.920 --> 05:36.480
trying to build, as I understand it, an organization to meet that need and actually work with those

05:36.480 --> 05:42.240
leading labs in part, at least, on understanding the systems that they're developing before they

05:42.240 --> 05:48.480
get to widespread deployment, I think is super interesting, and I'm very excited to unpack the

05:48.480 --> 05:53.520
details of it with you. Maybe for starters, you want to just kind of give us the quick overview on

05:53.520 --> 05:58.480
Apollo Research, like how you decided to set out to found it. I'm interested a little bit in the

05:58.480 --> 06:03.920
timeline of how that related to some of the commitments that the labs have made and what

06:03.920 --> 06:08.640
you guys are trying to do in the big picture. So, I think on a high level, it's sort of trying to

06:08.640 --> 06:15.040
understand what is going on in AI systems. And the reason for this was, or still is, in fact,

06:15.680 --> 06:21.520
yeah, I basically think right now, we just lack information to make good decisions. There's loads

06:21.520 --> 06:27.760
of uncertainty that we have about what could go wrong, whether we are already at a point where

06:27.760 --> 06:35.760
things go wrong, or how far away we are from these points. And yeah, we're trying to reduce this

06:35.760 --> 06:42.160
uncertainty. And this is mostly through research, auditing, and governance. And on the research side,

06:42.160 --> 06:48.000
it's really split between interpretability and behavioral evils, half-half. But in the long

06:48.000 --> 06:54.480
run, we really want to merge them both, because I basically think what we need in the long run is

06:54.480 --> 06:59.520
both a mixture of behavioral and interpretability evils, so that we can really understand

06:59.520 --> 07:03.760
what the model is doing, and then also why it is doing this in the first place, because each of

07:03.760 --> 07:11.520
them individually seems somewhat insufficient. And yeah, maybe to go into the origin story,

07:11.520 --> 07:17.520
it has actually nothing to do with the commitments of the different labs. It was mostly that at the

07:17.520 --> 07:26.320
beginning of this year, I kind of felt like I had a pretty clear picture of what is lacking in the

07:26.320 --> 07:31.120
current space with deceptive alignment, and evaluating deceptive alignment, or models for

07:31.120 --> 07:36.320
deceptive alignment in the first place. And interpretability and evils just seemed like

07:36.320 --> 07:41.040
the obvious things to do. So in the beginning, we basically set out to do mostly research.

07:41.040 --> 07:46.720
And only then, over time, we realized, hey, this is something that should be applied in the real

07:46.720 --> 07:54.400
world as soon as possible, because systems are getting better all the time. And we may actually

07:54.400 --> 07:59.920
hit this point fairly soon where models are already about at the threshold of deceptive,

07:59.920 --> 08:05.760
of capabilities for deceptive alignment. And then there is a small part in the organization that

08:05.760 --> 08:10.800
is governance, which originally we also didn't really intend to do for the first two years or

08:10.800 --> 08:15.920
something, because we thought we really need to understand all the research very well before

08:15.920 --> 08:21.280
we can talk to the people in governments and decision makers and lawmakers, because otherwise

08:21.280 --> 08:26.640
we're telling them things we aren't super confident in. And then lots of things happen.

08:27.280 --> 08:34.320
Governments and lawmakers actually got interested in AI and AI safety in particular. And then when

08:34.320 --> 08:40.240
we talked to them, we realized the difference. We are very, very well placed to talk about these

08:40.240 --> 08:45.840
things, because if you have thought about them in the background for like six, seven,

08:46.000 --> 08:51.600
years, and then specifically about some topic for six months or so, you are among the world's

08:51.600 --> 08:57.360
experts. And this is kind of more like a reflection of the state of how bad it is about AI safety,

08:57.360 --> 09:03.680
where people in my position are actually sort of accidentally becoming the experts,

09:04.480 --> 09:10.640
rather than people with tens and 20 years of experience, because there aren't a lot of people

09:10.640 --> 09:14.800
in the world who have thought about AI safety for more than a couple of years, if at all.

09:14.800 --> 09:21.760
Yeah, I can definitely relate to that sort of accidental expert status. I never expected to be

09:21.760 --> 09:30.160
where I am and doing the things that I'm doing. But yeah, the whole AI field in some ways is kind

09:30.160 --> 09:35.360
of the dog that caught the car. I always kind of come back to that metaphor where it's like

09:36.320 --> 09:40.880
we were just trying to build a bit more powerful AI, and all of a sudden we built like a lot more

09:40.960 --> 09:46.240
powerful AI, and now we really kind of have to figure out what to do with it. So even a little

09:46.240 --> 09:51.040
bit of advanced planning is better than, or a little bit of advanced thought is a lot better than

09:51.040 --> 09:56.640
where most people are starting. Had you seen, when you actually started the organization,

09:56.640 --> 10:03.680
had you seen GPT-4, or were you basing this decision on just what was public at the time?

10:04.560 --> 10:13.040
Only what was public at the time. So the decision was made in February 2023, or at

10:13.040 --> 10:18.400
least sort of my internal commitment was made to this. I'm not sure whether GPT-4 was public

10:18.400 --> 10:23.920
already at the time. Not quite, right? It was March. So no, it was independent of GPT-4.

10:24.800 --> 10:30.720
Yeah, I always think that's interesting just because GPT-4 was such a wake-up moment for

10:30.720 --> 10:36.000
so many people, and certainly I would include myself in that. I was already extremely plugged

10:36.000 --> 10:42.560
into what was going on and using it and fine-tuning tons of models on the open AI platform in particular,

10:42.560 --> 10:48.320
but then it was like, whoa, this thing is next level. It's not slowing down. We've gone from

10:48.320 --> 10:54.400
sort of, I can put a lot of elbow grease in and get a fine-tuned model to do a particular task,

10:54.400 --> 10:58.960
which already I thought was going to be economically transformative to,

11:00.000 --> 11:03.520
I don't even need to do that, that I could just ask for a lot of these tasks and get

11:03.520 --> 11:07.600
like pretty good zero-shot performance. For me, that was the moment where I was kind of like,

11:07.600 --> 11:12.880
okay, this is going from a tool that I am really excited to use and having a lot of fun using

11:12.880 --> 11:19.360
to something that seems like a force that needs to be understood from all angles.

11:20.000 --> 11:25.600
So let's unpack the perspective that you are bringing to this. I would encourage folks to

11:25.600 --> 11:31.040
look up these papers that we'll discuss and read them for themselves as well, but on the website,

11:31.040 --> 11:38.240
you've got two recent publications. One is kind of a framework for organizing the work that you're

11:38.240 --> 11:43.680
going to do, and then the other is like a very detailed in the weeds investigation of a particular

11:43.760 --> 11:49.360
AI behavior, namely deceiving the user, which I think is a super interesting and important

11:49.360 --> 11:54.000
one to study. But let's maybe just start with the big picture, like organizing the thoughts.

11:55.840 --> 12:00.800
I get the sense that you think, again, well, you've kind of said this, and the paper certainly

12:00.800 --> 12:04.960
reflects it, that there are like a lot of big questions that remain unanswered. So how do you

12:04.960 --> 12:11.040
structure your approach to this topic given all the uncertainty that exists?

12:11.760 --> 12:18.480
Yeah, maybe to give a little bit of context. So this is only one paper of many in this space,

12:18.480 --> 12:23.280
and there is, I think earlier this year, there was a really big one called Model Evaluation

12:23.280 --> 12:29.520
for Extreme Risk, which we at Apollo definitely thought was a pretty good paper. And they're

12:29.520 --> 12:35.120
sort of pointing out many of the very reasonable and important steps, or reasonable principles

12:35.120 --> 12:40.320
for external auditing, something like ramp up the auditing before you ramp up the exposure

12:40.320 --> 12:47.200
to the real world and do this ahead of the curve, so to speak. But when we read the paper,

12:47.200 --> 12:52.560
we felt a little bit like, this makes sense for the current capabilities and sort of how

12:52.560 --> 12:57.360
current models are being built. But if we think ahead of what the next couple of years should look

12:57.360 --> 13:03.120
or not should, could look like, then yeah, there are loads of open questions. And we were trying

13:03.120 --> 13:08.160
to understand how do they fit into this framework? And because we internally were trying to make

13:08.160 --> 13:13.040
sense of this in the first place. So just to give you a couple of them, what happens if your

13:13.040 --> 13:18.080
model has the ability to do online learning? How often do you have to audit it? Should you re-audit

13:18.080 --> 13:23.680
it during the online learning? If yes, how often? What if you give the model access to the internet

13:23.680 --> 13:29.520
or to a database or to anything like this? Yeah, I think a model with and without access to the

13:29.520 --> 13:35.120
internet is basically two very different models. The one with access to the internet is just so

13:35.120 --> 13:41.600
much more powerful if you can use it even on a very basic level. So yeah, it feels like if you

13:41.600 --> 13:45.840
give your model affordances like this, you kind of have to rethink how dangerous it is and where

13:45.840 --> 13:49.360
the danger comes from because it suddenly is like a totally different threat model potentially.

13:49.920 --> 13:56.400
And so what we did for the paper and really the credit should go to Lee Sharkey here, who is my

13:56.400 --> 14:00.480
co-founder, who has done most of the hard work or if not all of the hard work for this paper.

14:01.440 --> 14:06.560
And so what we were doing is thinking from first principles. Where does the risk come from and what

14:06.560 --> 14:13.440
changes to the AI system do create new risks? And then basically the answer is, well, we have to

14:13.440 --> 14:18.400
audit wherever risk is created. And then the more we looked into this, the more realized, well,

14:18.400 --> 14:22.800
there are actually a lot of places where new risk comes into the system, at least potentially,

14:22.800 --> 14:27.600
and therefore we are audits, at least in an ideal world should happen. There are obviously some

14:27.680 --> 14:32.800
constraints. But I think if we think about where are we five years from now, then I think, yeah,

14:32.800 --> 14:37.600
if there is actually a big auditing ecosystem around this, then there will be very, very many

14:37.600 --> 14:43.520
different organizations auditing really different places. And then the other point of the paper

14:43.520 --> 14:51.360
was just to define many concepts and create the language to discuss all of these things because

14:51.920 --> 14:56.480
we had sort of many internal discussions where we were like, oh, the thing we mean is this.

14:56.480 --> 15:00.480
And then we had an example, and then we kind of needed a name for it. And there wasn't really

15:00.480 --> 15:06.560
a name. So we decided, okay, let's define all of the relevant terms for this, and then sort of

15:06.560 --> 15:11.040
have a language to talk about this in the first place. Hey, we'll continue our interview in a

15:11.040 --> 15:16.320
moment after a word from our sponsors. Real quick, what's the easiest choice you can make? Taking

15:16.320 --> 15:21.200
the window instead of the middle seat, outsourcing business tasks that you absolutely hate? What

15:21.200 --> 15:27.760
about selling with Shopify? Shopify is the global commerce platform that helps you sell at every

15:27.760 --> 15:33.680
stage of your business. Shopify powers 10% of all e-commerce in the US, and Shopify is the global

15:33.680 --> 15:39.360
force behind Allbirds, Rothy's and Brooklyn and millions of other entrepreneurs of every size

15:39.360 --> 15:44.800
across 175 countries. Whether you're selling security systems or marketing memory modules,

15:44.800 --> 15:49.440
Shopify helps you sell everywhere, from their all-in-one e-commerce platform to their in-person

15:49.440 --> 15:54.640
POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the

15:54.640 --> 15:59.200
past at the companies I founded, and when we launched Merch here at Turpentine, Shopify will

15:59.200 --> 16:04.080
be our go-to. Shopify helps turn browsers into buyers with the internet's best converting

16:04.080 --> 16:09.520
checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell

16:09.520 --> 16:15.040
more with less effort thanks to Shopify magic, your AI-powered All-Star. With Shopify magic,

16:15.120 --> 16:19.600
whip up captivating content that converts from blog posts to product descriptions.

16:19.600 --> 16:25.920
Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify magic is free for

16:25.920 --> 16:32.080
every Shopify seller. Businesses that grow grow with Shopify. Sign up for a $1 per month trial

16:32.080 --> 16:38.000
period at Shopify.com slash cognitive. Go to Shopify.com slash cognitive now to grow your

16:38.000 --> 16:42.000
business no matter what stage you're in. Shopify.com slash cognitive.

16:45.040 --> 16:52.720
So let's dig in in a little bit deeper detail. I like the premise that you set out within the

16:52.720 --> 17:00.080
paper, which is to work backward from AI effect in the real world and try to imagine where are

17:00.080 --> 17:03.920
these effects going to happen and then how can we get upstream of that and help shape them in a

17:03.920 --> 17:10.160
positive way. I would be interested to hear you kind of describe that backward chaining process in

17:10.160 --> 17:14.320
a little bit more detail. And then I thought some of your concepts also were really helpful

17:15.200 --> 17:20.320
clarifications and distinctions. So maybe you can highlight some of the ones that you think are most

17:21.280 --> 17:23.920
useful that you'd like to see get into broader circulation as well.

17:24.480 --> 17:29.200
So basically, we started from, okay, the system, the AI system will interact with the world in a

17:29.200 --> 17:33.440
particular way. And then, you know, there are many, many different ways in which it can interact with

17:33.440 --> 17:39.280
the world. And then there's sort of like a whole chain of things that have had to happen until the

17:39.280 --> 17:44.080
model can act interact with the world in this particular way. So, you know, maybe it has been

17:44.080 --> 17:48.400
fined you and maybe it has been given access to the internet before that it has to have been trained

17:48.400 --> 17:52.560
before that there has to have been the decision that this model should be trained in the first

17:52.560 --> 17:58.800
place. And so the question is like, what are the kind of important decisions at all of these

17:58.800 --> 18:04.800
different points in time? And how then can we ensure that people actually make decisions that

18:04.800 --> 18:11.360
will lead to outcomes at the end of the chain, such as the model or the system interacts with the

18:11.360 --> 18:16.720
world in a safe manner. And this is maybe like the first distinction that is worth pointing out,

18:16.720 --> 18:21.040
or like the reason why I'm correcting myself all the time is there's really a difference between

18:21.040 --> 18:25.440
AI model and AI system. The AI model really, and this is not something we came up with,

18:25.440 --> 18:29.920
this already exists before. But I think it's worth pointing out and sort of getting in like

18:29.920 --> 18:35.360
really hammering into people's head when they think about AI. So the AI model is just the weights

18:35.360 --> 18:40.400
maybe behind, you know, like behind an API, but even with the API, it's kind of already a system.

18:41.440 --> 18:46.160
And the system then is sort of the weights plus everything around it. So there could be scaffolding,

18:46.160 --> 18:50.240
there could be access to tools, there could be content filters, this could even be like

18:50.240 --> 18:55.680
just an API, retrieval databases, etc. Like really the full package, where you say, okay,

18:55.680 --> 19:01.840
you know, there's there's like stuff around the mod around the weights that increase the

19:01.840 --> 19:07.440
capabilities of the model and menu, or at least change the capabilities of the raw model in some

19:07.440 --> 19:13.200
sense, not necessarily always increasing filters, for example, may decrease it. And then there are

19:13.200 --> 19:17.680
sort of other weird ways, or like, yeah, once you think about this, there are sort of a couple

19:17.680 --> 19:23.040
of other concepts that that feel important to clarify. Because when people say capabilities,

19:23.040 --> 19:28.720
this can mean very different things, right? And so we categorize this into three different classes.

19:29.360 --> 19:35.040
The first one is absolute capabilities, which we think of basically the hypothetical capabilities

19:35.040 --> 19:42.480
given any set of affordances. So if you have GPD for without the internet, right, then in the space

19:42.480 --> 19:48.160
of absolute capabilities would be a GPD for with internet. So or like things that this model could

19:48.160 --> 19:53.360
do. So the question is like, if we give additional things to the system, how big is the space of

19:53.360 --> 19:59.440
actions it could take. So, you know, and then obviously, there's a question of like, how imaginary

19:59.440 --> 20:04.720
do we get here, you know, like, does it has does it get access to like, you know, a Dyson sphere,

20:04.720 --> 20:10.880
or does it get access to like, a government or something like this. But but yeah, like it basically

20:10.880 --> 20:16.560
points out sort of the, what could this model do if we gave it a lot of things, everything that

20:16.560 --> 20:21.280
we can basically think of. Thinking about this in the first place only makes sense for models that

20:21.280 --> 20:27.360
have become more general, like the GPT is, because you know, for an MNIST filter, like for an MNIST

20:27.360 --> 20:33.360
classifier, this doesn't make any any sense, like an MNIST classifier plus internet is like is exactly

20:33.360 --> 20:38.400
as capable as just the MNIST classifier itself. But yeah, for systems that are more general,

20:38.400 --> 20:44.560
suddenly you have this difference between things that only the system can do, or like the basic

20:44.560 --> 20:51.280
system plus things that you could do hypothetically with a lot of additional affordances. Then the

20:51.280 --> 20:58.480
second one is contextual capabilities, which is things that are achievable in the context right

20:58.480 --> 21:06.080
now. So for example, with chat GPT, you can enable it to have access to tools, and then you can browse

21:06.080 --> 21:10.480
the web. And this is something that it can do right now, you don't have to add anything on top

21:10.480 --> 21:15.520
of this. And this is sort of this is sort of the smallest category of things, which you can do without

21:15.520 --> 21:22.080
any additional modification and then reachable capabilities is contextual capabilities, plus

21:22.080 --> 21:29.200
achievable through extra effort. So for example, this could mean chat GPT itself may not have access

21:29.200 --> 21:36.800
to a calculator. But if it has access to the internet, it can like Google and then find a

21:36.800 --> 21:42.480
calculator and then use that calculator. And so it's sort of a two step process, right, where it

21:42.480 --> 21:47.760
has to use one affordance or capability to then achieve another. And so this is what we call

21:47.760 --> 21:56.400
reachable capabilities. And yeah, so the reason the reason why we are making this all of this

21:56.400 --> 22:01.840
differentiation, even though it sounds maybe a little bit too much in the weeds is when people

22:01.840 --> 22:07.680
talk about capabilities and regulating capabilities and designing laws for capabilities, the question

22:07.680 --> 22:13.520
is, which ones, right? Do you mean the contextual capabilities? So the ones that the model has

22:13.520 --> 22:18.160
literally right now, or the reachable capabilities, so which the model could reach with additional

22:18.160 --> 22:25.040
effort or the absolute like, the maximum potential space of capabilities. And, you know, right now

22:25.040 --> 22:30.880
this may sound like we're too much in the weeds. And but I think in a few months, this will sound

22:30.880 --> 22:36.640
very, very relevant suddenly, because the models will be more capable. And then they will actually

22:36.640 --> 22:43.760
be able to just like smart enough to use the internet to to like find additional tools that

22:43.760 --> 22:50.160
they can then use, or or like convince someone to give them access to a shell. And then use that

22:50.160 --> 22:53.680
because they're already like, you know, they can learn it in context or they know it anyways.

22:55.040 --> 22:59.360
And at that point, really, the question is, what should the auditors audit for? Which capabilities?

22:59.920 --> 23:05.440
And and that becomes like pretty quickly, like a very, very big space of things, right? So like,

23:05.440 --> 23:11.360
if the auditor not only has to think about what kind of tools do you give the AI, but also what

23:11.360 --> 23:17.440
kind of tools could the AI get access to through some means? Suddenly, you have this whole space of

23:17.440 --> 23:22.480
like thousands of things it could do. It's really a question of like, or like a tradeoff between

23:22.480 --> 23:27.440
what is what is plausibly doable in the real world versus how much risk can we actually mitigate?

23:28.240 --> 23:33.040
And I'm honestly very unsure about about the like where we're heading at this point.

23:34.160 --> 23:40.560
So just to riff on and kind of emphasize some of the the value that I see in in some of these

23:40.560 --> 23:45.920
distinctions, I think it's helpful to clarify the difference between a model and a system.

23:46.720 --> 23:52.000
I think there is a tremendous amount of confusion online. And to my degree, and I've probably even

23:52.000 --> 23:58.080
contributed to some of it at times where people are like, you know, Chad GPT was doing this for me,

23:58.080 --> 24:02.400
and now it's not anymore. And I've sometimes said like, well, they haven't updated the model,

24:02.400 --> 24:06.160
so it probably hasn't changed that much. And I think what I've maybe neglected in some of those

24:06.160 --> 24:11.200
moments is like, but they might have changed the system prompt, or, you know, as we're seeing,

24:11.200 --> 24:16.480
I mean, even just this last couple of weeks, there's been this really interesting phenomenon of the

24:17.360 --> 24:23.680
of GPT for getting quote unquote, lazier. And people are speculating that maybe that's because

24:23.680 --> 24:27.280
they feed the date into it. And it knows that we're in December, and it knows that people

24:27.280 --> 24:31.760
don't work as hard or as productively in December. And so maybe it's like kind of phoning it in,

24:31.760 --> 24:37.200
because it's like, imitating the broad swath of humans that it's seen like, you know, kind of

24:37.200 --> 24:41.920
work halftime in December or whatever. I've even seen some experiments, just in the last couple

24:41.920 --> 24:45.840
days that suggest that there might even be real truth to that. Who knows, I'd say that the question

24:45.840 --> 24:50.640
remains open. But there's a there is an important difference, you know, and it's worth getting

24:50.640 --> 24:56.880
clarity on the model itself with static weights, not changing versus even just a system prompt

24:56.880 --> 25:02.640
that can perhaps have, you know, even unexpected drift along the dimension of something as

25:02.640 --> 25:09.840
seemingly benign as today's date. So that's important to keep in mind. The levels of capabilities,

25:09.840 --> 25:13.280
I think, are also really interesting. And I want to ask one kind of I have a couple questions on

25:13.280 --> 25:19.200
this, but I think I have a clear sense of what is meant by contextual. What can it do now, given

25:19.200 --> 25:24.960
the packaging, right? What what can GPT forward do in the context of chat GPT, where it has

25:24.960 --> 25:30.480
a code interpreter, and it has browse with Bing, and it has the ability to call Dolly three to

25:30.480 --> 25:34.640
make an image, and probably a couple other things that I'm not even remembering, you know, plugins

25:34.640 --> 25:38.880
perhaps as well, right, which obviously GPT is which proliferates, you know, all the affordances,

25:38.880 --> 25:44.400
all that much more. On the other end, I feel like I sort of understand absolute, which is like a

25:44.400 --> 25:50.080
theoretical max. Could you give me a little like how do I understand reachable as as kind of between

25:50.080 --> 25:58.320
those like what's what's the distinction between reachable and absolute? Yeah, so so maybe maybe

25:58.320 --> 26:03.840
one way to think of it is like, the contextual capabilities are the ones kind of that a user

26:03.840 --> 26:10.480
explicitly gave it. And then the reachable ones are those that may also be reachable without the

26:10.480 --> 26:15.920
user even having thought about that the model actually will will use them, right? So if you say,

26:16.560 --> 26:22.560
you know, like if the model would be able to browse the web, like entirely on its own, which I'm

26:22.560 --> 26:28.000
not sure it currently can do or like what exactly the restrictions on search with Bing are. But if

26:28.000 --> 26:33.840
it was able to do that, right, you may not you may not have realized that it has a reachable

26:33.840 --> 26:41.440
reachable capability through the internet of like firing up a shell somewhere, or like renting a GPU,

26:42.080 --> 26:47.200
and and like doings or like running a physics simulation through a like an online

26:48.480 --> 26:53.840
physics simulator, if that if that's something that's available. And so so these are this is sort of

26:53.840 --> 27:01.920
like how which tools can it reach through the contextual ability capabilities that it already

27:01.920 --> 27:10.160
has given by you or was been has been given by you. Gotcha. Okay. So like solving a capture by

27:10.160 --> 27:18.080
hiring an upward contractor for exactly to take one infamous case. So, okay, here's a challenging

27:18.160 --> 27:24.880
question. But, and I don't necessarily expect an answer, but maybe you can venture an answer or

27:24.880 --> 27:30.640
you could just kind of describe how you begin to think about it. What would you say are the absolute

27:30.640 --> 27:39.680
capabilities of GPT four? Yeah, very unclear. So I think they're definitely they're not infinite.

27:40.400 --> 27:45.440
As in, you know, like even with extremely good scaffolding and and access to the internet and

27:45.440 --> 27:52.480
many other things, I think people haven't been able to, you know, get it to do economically

27:52.480 --> 27:58.880
valuable tasks at the level of a human, at least for like long time spans, for example. So, you

27:58.880 --> 28:03.200
know, the question is obviously like, is this, you know, are we just too bad? And have we not

28:03.200 --> 28:07.440
figured out the right prompting yet and the right scaffolding and so on? Or, or is this just a

28:07.440 --> 28:12.400
limitation of the system? And my current guess is, like, there is probably a limit to the absolute

28:12.400 --> 28:17.360
capabilities. And it's probably lower than like what a human can do. But we're not that far away

28:17.360 --> 28:23.440
from it. So, you know, I think with an additional training with additional, like specifically

28:23.440 --> 28:28.320
LM, like training that is more goal direct or makes it into more goal directed and an agent

28:29.280 --> 28:35.200
and better scaffolding, I think there will be ways in which the absolute capabilities could increase

28:36.080 --> 28:39.680
quite a bit in the near future. Yeah, does this make sense?

28:40.400 --> 28:46.480
Yeah, I mean, it's hard, right? I certainly listeners to the show will know from repeated

28:46.480 --> 28:54.160
storytelling on my part that I was one of the volunteer testers of the GPT-4 early model back

28:54.160 --> 28:59.520
in August, September of last year. And I really kind of challenged myself to try to answer that

28:59.520 --> 29:06.480
question, you know, independently, like, what is the theoretical max of what this thing can do?

29:07.360 --> 29:12.400
How much could it like break down big problems and delegate to itself? And it basically came to

29:12.400 --> 29:21.520
the same conclusion that you did, which is like, doesn't seem like it can do really big tasks.

29:21.520 --> 29:24.560
I mean, again, it's confusing, right? Because then you could also look at the dimension of

29:25.200 --> 29:29.200
how big the task is versus how would you break it down? Just in the last week, I've been doing

29:29.200 --> 29:35.920
something for a very sort of mundane project, but actually using GPT-4 to run evals on other

29:35.920 --> 29:43.280
language model output, I have found that if I have like 10 tasks, 10, you know,

29:43.280 --> 29:49.520
dimensions of evaluation, and I ask it to run all of those, it is now capable of following

29:49.520 --> 29:56.160
those directions and executing the tasks one by one. But the quality kind of suffers. It sort of

29:56.160 --> 30:00.560
makes mistakes. It sometimes muddies the tasks a little bit between each other. And it's definitely

30:00.560 --> 30:07.520
like not at a human level given 10 tasks to do in one generation. On the flip side, though,

30:07.520 --> 30:14.240
if I take it down to one task per generation, which I didn't want to do because that will increase

30:14.240 --> 30:19.920
our cost and latency and just is less convenient for me, but then it kind of pops up to, honestly,

30:19.920 --> 30:25.920
I would say pretty much human level, if not above. So there's interesting dimension. I guess

30:25.920 --> 30:30.640
it seems pretty the sort of magnitude of the task seems like a pretty important dimension for

30:31.520 --> 30:35.040
evaluating a question like absolute capabilities, right? It's like,

30:35.040 --> 30:40.320
if it's a super narrow thing, it has it's like more it's, it's capable of some pretty high spikes.

30:40.320 --> 30:46.720
But if it's a, if it's a big thing, it kind of gets lost. Would you refine that characterization

30:46.720 --> 30:51.280
at all? Yeah, yeah, I'm not sure how to think about it, honestly. So I think of absolute capabilities

30:51.280 --> 30:57.520
really more of a sort of theoretical bound that we could, that we're probably not going to approximate

30:57.520 --> 31:03.440
in practice, even if we test like a lot, a lot. And then the, then like breaking it down into

31:03.440 --> 31:07.360
different tasks, I'm not sure I feel like this is a different capability then, right? Like you're

31:07.920 --> 31:12.560
sort of the capability of doing 10 things at once is a different thing than the capability of

31:12.560 --> 31:20.480
doing one thing 10 times like 10, 10 diff different things, but one by one. So yeah,

31:20.480 --> 31:26.160
I would say it's basically you're talking about different capabilities then, at least in this

31:26.160 --> 31:31.280
framework. Hey, we'll continue our interview in a moment after a word from our sponsors.

31:31.280 --> 31:35.360
If you're a startup founder or executive running a growing business, you know that as you scale,

31:35.360 --> 31:39.680
your systems break down, and the cracks start to show. If this resonates with you,

31:39.680 --> 31:46.560
there are three numbers you need to know, 36,000, 25 and one, 36,000. That's the number of businesses

31:46.560 --> 31:50.560
which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system,

31:50.560 --> 31:56.800
streamline accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25

31:56.800 --> 32:01.680
this year. That's 25 years of helping businesses do more with less, close their books in days, not

32:01.680 --> 32:07.120
weeks, and drive down costs. One, because your business is one of a kind, so you get a customized

32:07.120 --> 32:11.920
solution for all your KPIs in one efficient system with one source of truth. Manage risk,

32:11.920 --> 32:15.920
get reliable forecasts, and improve margins. Everything you need, all in one place.

32:16.480 --> 32:21.200
Right now, download NetSuite's popular KPI checklist, designed to give you consistently

32:21.200 --> 32:26.640
excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com

32:26.640 --> 32:31.200
slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.

32:32.480 --> 32:38.000
Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that

32:38.000 --> 32:43.680
actually work, customized across all platforms with a click of a button. I believe in Omnike so

32:43.680 --> 32:50.080
much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount.

32:50.080 --> 32:56.320
Yeah, and it is not that good at decomposing the tasks. I've also kind of experimented a little

32:56.320 --> 33:01.600
bit with like, can you give it that list of 10 tasks and can it break them down and

33:02.400 --> 33:07.600
self-delegate with an effective prompt? It's like maybe a little bit closer there,

33:07.600 --> 33:15.280
but still not getting nearly as good results as if I just roll it my sleeves and do the task

33:15.280 --> 33:21.360
decomposition. You mentioned that you expect this frontier to obviously continue to move.

33:21.920 --> 33:29.360
One way to ask the question is, what is Q-star? A more sensible way to ask the question is,

33:30.320 --> 33:37.920
do you have a set of expectations for how the capabilities frontier will move? I definitely

33:37.920 --> 33:43.760
look at things like OpenAI's publication from earlier this year where they gave,

33:44.400 --> 33:48.880
started to give denser feedback on kind of every step of the reasoning process and they

33:48.880 --> 33:54.160
achieved some state-of-the-art results on mathematical reasoning that way. And when I

33:54.160 --> 33:58.080
think about affordances and I think about the failure modes that I've seen with these

33:58.800 --> 34:05.760
GPT-4 agent type systems, I think, man, you apply that to browsing the web and using APIs

34:06.400 --> 34:12.880
and it seems like that stuff is ultimately a lot less cognitively demanding than pure math.

34:12.880 --> 34:18.000
It seems like we probably are going to see, and I would guess that it's maybe already working

34:18.000 --> 34:22.480
pretty well. AGI has been achieved internally, I don't know about AGI, but I would expect that

34:22.480 --> 34:29.200
some of this stuff is already pretty far along in kind of internal prototyping. But how does that

34:29.200 --> 34:34.160
compare to what you would expect to see coming online over the next few months? Yeah, it's obviously

34:34.160 --> 34:40.080
hard to say and I can only speculate. I think on a high level what I would expect the big trends

34:40.080 --> 34:48.640
to be and also what we are kind of looking forward to evaluating is LM agents. I think this is like

34:48.640 --> 34:53.840
pretty agreed upon. From first principle, I think it also makes sense. It's just like,

34:54.480 --> 35:00.640
where does the money come from? It is from AI systems doing economically useful tasks

35:00.640 --> 35:06.160
and often economically useful tasks just require you to do things independently,

35:06.160 --> 35:12.880
being goal directed over a longer period of time. And the longer a model can do things on its own,

35:12.880 --> 35:17.760
the more money you can squeeze out of it. So I definitely think just from financial interests,

35:17.760 --> 35:24.400
all of the AGI labs will definitely try to get in more, more agentic ways. How far they have come,

35:24.400 --> 35:29.840
I don't know. But yeah, I expect that next year we will see quite some surprises.

35:31.120 --> 35:37.600
Then multimodality is the other one where, yeah, I think people kind of like over the last couple

35:37.600 --> 35:44.400
of years with more and more multimodal models, people just realized it's not that different from

35:45.200 --> 35:52.400
just training text. You plug in the additional modalities, you change your training slightly,

35:53.600 --> 35:59.200
but it's not much more than that. Obviously, it's obviously hard in practice. There's a ton of

35:59.200 --> 36:05.440
engineering challenges and so on. But on a conceptual level, there isn't any big breakthrough

36:05.440 --> 36:10.160
needed. So people will just add more and more modalities on bigger and bigger models and train

36:10.240 --> 36:16.800
it all jointly and to end. And it kind of just works. And then tool use is the last one.

36:17.920 --> 36:22.800
And that I think people, yeah, people actually were quite surprised by how like, quote unquote,

36:22.800 --> 36:29.920
easy it was to get to this level. So yeah, I think when people realize like, oh, these language

36:29.920 --> 36:35.680
models are already pretty good, like how fast do they learn how to use any tool we can think of?

36:36.640 --> 36:41.680
And they were surprised by how fast they learned the tools. And now it's mostly a question of

36:42.400 --> 36:47.920
sort of really baking in the tool into the model in a way that it's like very robustly able to use

36:47.920 --> 36:52.240
the model rather than just a little bit or just showing that it's it sometimes work. But yeah,

36:52.240 --> 36:56.320
I mean, you know, like I think if you have an LM agent that is multimodal, and that has very good

36:56.320 --> 37:00.800
tool use, like I'm not quite sure how far you are away from AGI, right? Like at that point,

37:00.800 --> 37:05.440
you kind of have almost all of the ingredients ready. And then it's really just a question of

37:05.440 --> 37:09.680
how robust is the system. So yeah, I think these are the trends we see right now. And this is also

37:09.680 --> 37:13.520
why many people in the big labs have very, very short timelines, because they can think like

37:13.520 --> 37:18.320
two years ahead and sort of where this is going, or maybe even just one year ahead, I don't know.

37:18.320 --> 37:23.360
When you talked about the surprise, like people were surprised at how easy it was to get tool

37:23.360 --> 37:30.160
used to work. Are you referring to people in the leading, you know, the obvious, the usual suspects

37:30.160 --> 37:34.240
of leading developers? It's hard to say. I mean, I can only speculate on this. But you know, the

37:34.240 --> 37:41.680
tool former paper was like three months, or like was published three months before open AI just

37:41.680 --> 37:46.560
released their tool use. And I mean, they probably had been working on this before, but still, you

37:46.560 --> 37:52.480
know, like the from from having the scientific insight to to like publishing this and releasing

37:52.480 --> 38:00.400
this in the real world, I think there just was less work involved than is, or is typical for most

38:00.480 --> 38:06.160
of the bigger AI, like development cycles, I could be wrong on this. This is this is more

38:06.160 --> 38:12.880
here say so yeah, take it with a grain of salt. Yeah, it seems right to me as well. And I agree

38:12.880 --> 38:18.320
with you the emphasis on multi modality as a new unlock makes a ton of sense, even just in this

38:18.320 --> 38:24.800
kind of agent paradigm of, you know, can I browse the internet or whatever. I've done a lot of

38:25.440 --> 38:31.200
browser automation type work in the past. And the difference between having to

38:32.320 --> 38:37.840
grab all the HTML that, you know, is often these days, like extremely bloated and, you know, kind

38:37.840 --> 38:44.960
of semi auto generated and in some cases, like deliberately, you know, generated to be hard to

38:44.960 --> 38:49.200
parse, you know, from like, you know, the Googles on Facebook, like they don't want you scraping

38:49.280 --> 38:55.120
their content. So they're kind of not making it easy on the, on the browser automators. The

38:55.120 --> 38:58.480
difference between that and just being able to like look at the screen and understand what's

38:58.480 --> 39:02.160
going on, you know, you kind of put it through a human lens and you're like, yeah, it's a hell

39:02.160 --> 39:06.400
of a lot easier to see what's going on on the screen than to like read all this HTML and sure

39:06.400 --> 39:11.680
enough, you know, the models kind of behave similarly. I remember for me looking at the

39:11.680 --> 39:17.920
flamingo architecture when that was first published, like, I think April of 2022. So

39:17.920 --> 39:23.840
you're a little more than a year and a half ago now. And just thinking like, Oh, my God,

39:24.560 --> 39:29.200
if this works, everything's going to work. You know, it was like, they had a language model

39:29.200 --> 39:35.600
frozen. They had kind of stitched in the visual stuff and like kind of added a couple layers, but

39:36.160 --> 39:43.520
it really looked to me like, man, this is tinkering stage. And it's just working. So I, you

39:43.520 --> 39:47.920
know, like you, I don't want to dismiss the fact that there's obviously a decent amount of,

39:48.960 --> 39:54.240
I'm sure like labor and probably at times tedious labor that has to go into overcoming the little,

39:56.000 --> 39:59.920
you know, little stumbling points. But conceptually, it is amazing how simple

40:00.720 --> 40:06.480
a lot of these unlocks have been over the last couple of years. And you see this too, and just

40:06.480 --> 40:11.600
like the pace at which people are putting out papers, you look at like the one team that I

40:11.680 --> 40:18.560
followed particularly closely is the team at Google DeepMind that is doing medical focused

40:18.560 --> 40:22.720
models. And they're good for one, like every three months, you know, and they're like significant

40:22.720 --> 40:26.400
advances where it's like, Oh yeah, this time we added multimodality. And this time we like

40:26.400 --> 40:31.040
tackled differential, you know, diagnosis. And like, again, it's, it seems like there, there's

40:31.040 --> 40:36.320
not a lot of time for failures between these successes. So it does seem like, yeah, we're not

40:36.400 --> 40:43.920
at the end of this, by any means, just yet. A lot is coming at us. It's going to presumably

40:43.920 --> 40:51.040
continue to get weird. You're trying to push both as much as you can, the understanding of

40:51.680 --> 40:58.480
what can the systems do, you know, as users, what is it, what are their limits? And then at the same

40:58.480 --> 41:03.520
time, you're trying to dig into the models. And this is the interpretability side and figure out

41:03.520 --> 41:10.960
like, what's going on in there? And, you know, can we kind of connect, you know, the external

41:10.960 --> 41:16.400
behaviors to these like internal states? So tell us about that side of the research agenda as well.

41:17.120 --> 41:23.120
Yeah, so on the interpretability side, like, my thinking is basically, it would be so great if

41:23.120 --> 41:29.680
interpretability work, right, it would make so many questions easier. Like, if you ask questions

41:29.680 --> 41:33.920
on accountability, right, if you have causal interpretability method, you would be able to

41:33.920 --> 41:38.320
just, you know, tell the judge if the model would have, we would have changed these variables,

41:38.320 --> 41:44.880
the model would have acted in differently in this way. And we could just basically solve that biases,

41:44.880 --> 41:49.360
probably also like, you know, social biases, much easier to solve, because you could intervene

41:49.360 --> 41:55.360
on them or like fix the internal misunderstandings and concepts. It's also extremely helpful for

41:55.440 --> 41:59.360
like, basically all of the different extreme risks, right, like it would be much easier to

41:59.360 --> 42:04.640
understand the internal plans and how it thinks about problems, how it approaches them and so on.

42:04.640 --> 42:08.640
And then it would also make iterations on alignment methods much, much easier, I think.

42:09.440 --> 42:15.280
As in, you know, let's say somebody says, oh, RLHF is, is like already working, we see this in

42:15.280 --> 42:19.440
practice, then, you know, you could use the interpretability tool. So test does, you know,

42:19.440 --> 42:25.600
does RLHF actually work? Or does it only like superficially like hide the problem or something

42:25.600 --> 42:32.080
like this? Or does it actually like deep down solve the root? And then I think my biggest sort of

42:32.080 --> 42:37.440
the biggest reason for me for focusing on interpretability in the first place is deceptive

42:37.440 --> 42:44.080
alignment, where, you know, models appear aligned to the outside and to the user. But internally,

42:44.080 --> 42:47.840
they actually follow different goals. They just know that you have a different goal.

42:48.560 --> 42:54.160
And therefore, like in order for you to think it is nice, they act in that way.

42:55.360 --> 43:03.040
And yeah, I basically think almost, not all, not almost all, but a lot of the scenarios in

43:03.040 --> 43:09.600
which AI goes really, really badly, go through some form of deceptive alignment, where at some

43:09.600 --> 43:15.440
point the model is seen as nice, and people think it is aligned, and people give it access to the

43:15.440 --> 43:19.680
internet and resources and like train it more and more and more and make it more powerful.

43:21.120 --> 43:26.000
But internally, it is actually pursuing a different goal. And it is smart enough to hide this true

43:26.000 --> 43:32.640
intention until it knows that it can sort of cash out and then follow on this on this actual

43:32.640 --> 43:37.120
goal without us being able to stop it anymore. And so yeah, that's what I'm really worried about.

43:37.120 --> 43:43.760
And interpretability obviously seems like one of the most obvious ways to test for deceptive

43:43.760 --> 43:48.240
alignment or like to at least investigate the phenomenon, because you know what it's thinking

43:48.240 --> 43:52.720
inside. There are still, you know, there are still some cases where deceptive alignment,

43:52.720 --> 43:57.920
where even with good interpretability tools, deceptive alignment could still somehow be a thing.

43:57.920 --> 44:01.760
But generally speaking, I think it would be much, much, much harder for the model to pull off.

44:02.400 --> 44:10.560
So right now, I think interpretability is just not at the, it's like, not practically useful.

44:10.560 --> 44:15.200
So, you know, we cannot use any existing interpretability tool and like throw it on

44:15.200 --> 44:21.680
GPT-3 or GPT-4 because none of them have like enough or developed enough that they give us

44:21.680 --> 44:28.800
insight that like really meaningfully change our minds. And so yeah, this is why, you know,

44:28.800 --> 44:33.120
our agenda is separate in the first place between behavioral evals and interpretability,

44:33.120 --> 44:37.840
despite us wanting to do them jointly in the long run. But they're given that there's such

44:37.840 --> 44:46.560
a huge gap on applicability. I think that this is definitely a problem that we're trying to mitigate

44:46.560 --> 44:54.000
here. And then the one question for me is also like, how hard will interpretability

44:54.000 --> 44:58.960
turn out to be? And there are, you know, various people have argued that interpretability will

44:58.960 --> 45:05.120
be extremely hard because models are so big and complicated. And therefore, it will be hard to

45:05.200 --> 45:10.560
enumerate, you know, all of the concepts and actually understand what the hell is going on inside.

45:10.560 --> 45:17.280
And I'm more of the perspective that, you know, I understand the reason why they think it's hard,

45:17.280 --> 45:23.360
but I also think there are many reasons to assume it's, it's going to be like doable. It's, if we

45:23.360 --> 45:27.680
put our minds to it as humanity, we'll probably figure it out. The primary reason I think is

45:28.400 --> 45:34.960
we have full, full interventional access to the model, right? We can see every activation,

45:34.960 --> 45:41.520
we can ablate everything we want. We have, you know, it's not just, it's not just observational

45:41.520 --> 45:45.760
studies, you can really intervene on the system. And generally speaking, I would say, as soon as

45:45.760 --> 45:50.480
you can intervene on the system, you can test your hypothesis very, very quickly. And you can

45:50.480 --> 45:56.160
iterate very fast. And so I think we will be able to figure out interpretability,

45:57.120 --> 46:02.960
you know, in the next couple of years to an extent where we can actually sort of say it is now

46:02.960 --> 46:08.240
useful on real world models, on frontier models. How expensive this is going to be, I don't know

46:08.240 --> 46:12.800
yet, but I think it will at least be technically feasible. Yeah, I've definitely updated my thinking

46:12.800 --> 46:19.520
a lot in that direction from a pretty naive, just kind of, you know, hey, it sounds really hard,

46:19.520 --> 46:25.520
black box problem, nobody knows what's going on in there to today, I would say, wow, you know,

46:25.600 --> 46:32.480
there's really a lot of progress. And it has, it is the progress of interpretability over the last,

46:32.480 --> 46:38.000
say, two years has definitely exceeded my expectations and given me a lot more, I wouldn't

46:38.000 --> 46:43.520
have maybe some sort of confidence, but you know, at least reason to believe that with some time,

46:43.520 --> 46:50.960
but not necessarily, you know, a ton of time that we really could get to a much better place in our

46:50.960 --> 46:56.480
understanding. So I'm with you on that. I have a number of follow up questions, I think, on

46:57.840 --> 47:05.680
this point. One, let's maybe just give the account for like why deception might arise in the first

47:05.680 --> 47:09.840
place. You can complicate, I'll give you a super simple version, you can refine it or complicate

47:09.840 --> 47:17.440
it. I usually kind of cite Ajaya on this, and you know, she has a pretty simple story of like

47:17.760 --> 47:24.080
what the model is trying to do, you know, what it is rewarded for in the context of an RLHF

47:24.720 --> 47:32.720
like training regime is getting a high feedback score from the user. And it probably becomes

47:32.720 --> 47:41.680
useful as a means to maximizing that score to model human psychology as an explicit part of

47:42.400 --> 47:48.400
how you're going to solve the problem, right? And I think we, you know, certainly humans do this

47:48.400 --> 47:53.440
with respect to each other, right? I ask you for something, you ask me for something, we interpret

47:53.440 --> 47:59.600
that not only as the extremely literal definition of the task, but also kind of have a sense for

47:59.600 --> 48:04.320
what does this person really care about? What are they really looking for? And we can incorporate

48:04.320 --> 48:10.240
that into the way that we respond. It certainly seems like the heavier you do, you know, the more

48:10.320 --> 48:14.880
emphasis you put on this kind of reinforcement learning from human feedback, the more likely the

48:14.880 --> 48:22.720
models are to start to create a distinction between, you know, the task as sort of narrowly,

48:23.520 --> 48:28.320
objectively scoped, let's say, and the kind of human psychology element that is going to feed

48:28.320 --> 48:32.400
into its rating. And then if you have that, you know, if you have that decoupling, then you have

48:32.400 --> 48:39.440
kind of potential for all sorts of misalignment, you know, including deception. How's that compared

48:39.680 --> 48:44.160
to the way you typically think about it? Yeah, I mean, I think the like this kind of version

48:44.160 --> 48:51.600
through RLHF is one potential path. I'm, yeah, I actually think the jury is still out there on

48:51.600 --> 48:55.040
this, like, you know, I definitely see the hypothesis and where it's coming from, but it could also

48:55.040 --> 48:59.920
just totally imagine that, you know, the training signal is sufficiently diverse, and it updates

48:59.920 --> 49:04.800
sort of sufficiently deep that RLHF kind of just does the thing we wanted to do without

49:05.680 --> 49:10.880
without the model becoming deceptive. I could also see that like the story in which it would

49:10.880 --> 49:17.440
become deceptive. I think, like, on a very high level, the way the reason I think why models

49:17.440 --> 49:22.320
would become deceptive is because at some point they will have long term goals, they will have

49:22.320 --> 49:27.200
something that they care about, like, more beyond the current episode, you know, beyond

49:27.760 --> 49:32.800
pleasing the user at this point in time. And then the question really is, and then I think

49:32.800 --> 49:37.840
there are like two core conditions under which, like, if the more they are fulfilled, the more

49:37.840 --> 49:44.880
likely the model is becoming deceptive, like how important is the this long term goal to the model

49:44.880 --> 49:51.360
itself, meaning how much does this goal trade off, for example, with other goals it has. So for

49:51.360 --> 49:55.120
example, if it cares if it cares a ton about something, then it's more likely to be deceptive

49:55.120 --> 50:00.560
with respect to this because it really wants to achieve this. And then secondly, how much do others

50:01.360 --> 50:06.720
care about me not or the AI not achieving this goal in the first place, something like contestedness,

50:06.720 --> 50:10.800
right? So for example, if I want to pick a flower and I care a lot about this, I don't need to be

50:10.800 --> 50:15.280
deceptive because nobody wants to stop me from picking that flower. If I want to be, you know,

50:15.280 --> 50:22.320
the president, a lot of people might not want me to be the president. And so in that case,

50:22.320 --> 50:26.480
it's very contested and I have a strong incentive to be deceptive about my plans because otherwise

50:27.120 --> 50:31.520
people would want to stop me. And then so now we're at a point where we have a system at least

50:31.520 --> 50:39.680
in our hypothetical scenario that has a long term goal and it's like, in the limit at least,

50:39.680 --> 50:45.120
you know, it cares about that goal and the goal may be somewhat contested. And then as long as it

50:45.120 --> 50:51.200
has situational awareness, it just feels instrumentally useful to be deceptive about it, like you

50:51.200 --> 50:55.680
said, right, to model other people and how they would think about it and then just react to this.

50:55.760 --> 51:00.240
And this is sort of, I think this is maybe this is like one of the core reasons why I'm so worried

51:00.240 --> 51:08.400
about this whole deception thing. Because it just feels like a reasonable strategy in a ton of

51:08.400 --> 51:14.640
situations from the perspective of like a consequentialist or rational, irrational actor,

51:14.640 --> 51:20.240
right? It's just like under specific conditions, people just naturally or like deception is just

51:20.240 --> 51:25.280
convergent people do it because it makes sense for them. And this is why we see it in like a ton

51:25.280 --> 51:29.920
of different systems, right? You see it in in animals where parasites are really deceptive with

51:29.920 --> 51:34.880
respect to their hosts, you see it in individual humans where, you know, they're deceptive with

51:34.880 --> 51:39.520
respect to their partners from time to time, for example, you see it in systems where, you know,

51:39.520 --> 51:44.640
like they're they they're trying to to gain the laws and be deceptive about this or to lie about

51:44.640 --> 51:48.880
this. And I think this is this is kind of like the whole or like a big part of the problem,

51:48.880 --> 51:52.720
it's just a it's like reasonable or sensible in many situations to be deceptive.

51:53.360 --> 51:58.320
From the perspective of the model, which is kind of what we want to prevent, right?

51:59.520 --> 52:07.680
So where do you think those long term goals come from? If it's, you know, is it just kind of a

52:09.280 --> 52:14.400
reflection of the general training goals? I mean, we, you know, we have kind of the canonical

52:15.040 --> 52:20.640
three H's. But I mean, honest is one of those, right? Hopeful, harmless, honest.

52:21.600 --> 52:26.080
Are you imagine is that is your understanding just that those are like fundamentally sort of

52:26.080 --> 52:32.320
intention and that the model will kind of have no choice but to develop tradeoffs between them?

52:32.880 --> 52:37.120
I mean, we can we can get into the tension between them in a second. But I think it's

52:37.120 --> 52:42.800
it's actually like the three H's I don't think will be, you know, they're not keeping me awake

52:42.800 --> 52:48.880
at night. I think it's more at some point, people want the model to do long term economic tasks.

52:48.880 --> 52:54.000
And for that, they give them long term goals or long term goals are instrumentally useful. So

52:54.000 --> 52:58.160
for many situations, I think it will just be useful to have long term goals or at least

52:58.160 --> 53:02.880
in like, to have instrumental goals, right? Something like, Oh, it makes because it is

53:02.880 --> 53:07.760
a long term task, it makes sense to first acquire money, and then use that money to do something

53:07.760 --> 53:13.200
and then use that third thing to achieve the actual goal. And so like, I think the models

53:13.200 --> 53:17.680
will just learn this kind of consequentialist and instrumental reasoning where they're like,

53:17.680 --> 53:23.760
okay, I first have to do X. And then I do this, and then I do the long term thing. And and once

53:23.760 --> 53:29.120
they're there, sometimes it just makes sense to be like, Okay, other people don't want me to do this.

53:29.680 --> 53:35.120
And therefore, I hide my actual intention. And I act in ways that make me look nice,

53:35.120 --> 53:41.040
despite not being nice. Yeah, but yeah, I think like a lot of the a lot of the reason why there

53:41.040 --> 53:47.040
will be these kind of long term goals is either because we literally give the model long term

53:47.040 --> 53:52.320
goals because it's economically useful from a human perspective, or because in like,

53:52.320 --> 53:56.720
some long term goals or are instrumentally useful to achieve other things.

53:57.520 --> 54:07.360
Gotcha. Okay, interesting. Another thought that came to mind in this discussion of,

54:09.440 --> 54:14.160
I guess, deception broadly is like, and I've done a little bit of investigation with this

54:14.160 --> 54:20.000
and engaged in some online debates. And it leads me to propose perhaps like another capability

54:20.000 --> 54:27.040
definition for you. But you know, that as I see it, like a theory of mind, which is kind of a more

54:27.040 --> 54:34.240
neutral, you know, framing, perhaps, is kind of a precondition for deception, right? If you are

54:34.240 --> 54:38.960
going to mislead someone, you have to have some theory of like what they are currently thinking.

54:39.760 --> 54:45.920
And there's a lot of research from the last six to nine months about

54:47.600 --> 54:51.840
do the current models have theory of mind to what extent, you know, under what conditions.

54:52.560 --> 54:59.040
And I've been kind of frustrated repeatedly, actually, by different papers that come out and say,

54:59.840 --> 55:05.680
still no theory of mind from GPT-4, where I'm like, but wait a second, you know,

55:05.680 --> 55:10.800
as Ilya says, like the most incredible thing about these, these models and the systems that,

55:10.800 --> 55:16.800
you know, we engage them through are that they kind of make you feel like you're understood,

55:16.800 --> 55:21.280
right? Like it definitely seems like there's some like kind of pretty obvious brute force

55:21.280 --> 55:26.240
theory of mind capability that exists. And yet when people do these benchmarks, they're like, oh,

55:26.240 --> 55:31.760
well, it only gets, you know, 72% on this and 87% on this and whatever. And so, you know, that's not,

55:31.760 --> 55:34.960
you know, fails the theory of mind test is like not at a human level or whatever.

55:35.520 --> 55:40.560
Some of that stuff I've dug into and found like your prompting sucks. If you just improve that,

55:40.560 --> 55:45.920
you know, then you can get over a lot of the humps. But I also have come to understand this

55:45.920 --> 55:52.720
as a difference in framing where I think I am more like you concerned with

55:54.160 --> 55:58.560
what is the sort of theoretical max that this thing might achieve? Like that seems to me

55:59.120 --> 56:03.920
the most relevant question for, you know, risk management purposes. And then I think other

56:03.920 --> 56:09.520
people are asking the a similar question, but through the frame of like, what can this thing

56:09.520 --> 56:15.520
do reliably? You know, what can it still do under adversarial conditions or whatever?

56:16.080 --> 56:22.160
So I wonder if there's a need for like another capability level that's even like below the

56:22.160 --> 56:28.720
reachable that would be the sort of robust or, you know, maybe even adversarial robust,

56:30.080 --> 56:35.280
robust to adversarial conditions. But I do see a lot of confusion on that, right? Like

56:35.280 --> 56:40.080
people will look at the exact same behavior. And I'll say, damn, this thing has strong theory of

56:40.080 --> 56:44.960
mind and like professors will be like no theory of mind. And I feel like we need some sort of

56:44.960 --> 56:50.880
additional conceptual distinction to help us get on the same page there. I'm not entirely sure

56:50.880 --> 56:56.160
whether or like maybe it makes sense from an academic standpoint to to think about this. I

56:56.160 --> 57:01.520
think from from the auditing perspective, the max, you know, the limit, the upper bound is what you

57:01.520 --> 57:07.280
care about. You really want to prevent people from being able to misuse the system at all,

57:07.280 --> 57:11.520
not just in the robust case, right? It's really about like, what if if somebody actually tried?

57:12.560 --> 57:19.200
Or you want the system itself to be, you know, not only not being able to take over or like

57:19.200 --> 57:25.280
exfiltrate or something like this in a few cases. Yeah, you basically want to limit it

57:25.280 --> 57:29.520
already at a few cases, right? You don't care about whether it does this like, you also care

57:29.520 --> 57:33.600
about whether it does this 50% of the time, but really you will already want to sort of

57:33.600 --> 57:37.840
pull the plug early on. So for an auditing perspective, probably this additional thing

57:37.840 --> 57:44.160
is not necessary, but from from unlike you real world use case and and sort of academic

57:44.240 --> 57:46.320
perspective, maybe there should be a different category.

57:48.400 --> 57:53.200
Yeah, I think if only just to kind of give a label to something that people are saying when

57:53.200 --> 57:57.760
they're saying that things, you know, aren't happening or can't happen that seem to be like

57:57.760 --> 58:03.920
obviously happening, we can work on coining a term for that. What's kind of the motivator for

58:04.640 --> 58:09.440
secrecy around interpretability work? Yeah, I basically think good interpretability work is

58:09.440 --> 58:15.040
almost necessarily also good capabilities work. So basically, if you understand the system good

58:15.040 --> 58:20.880
enough that you like understand the internals, you're almost certainly going to be able to

58:20.880 --> 58:26.400
build better architectures, iterate on them faster, make everything quicker, but potentially

58:26.400 --> 58:34.720
compress a lot of the you know, fluff that current systems may still have. And yeah, we will try to

58:34.720 --> 58:40.560
sort of evaluate whether whether our method does in fact have these implications. But yeah,

58:40.560 --> 58:44.720
you know, like I think basically, if you have a good interpretability tool, it will almost

58:44.720 --> 58:49.360
certainly also have implications for capabilities. And the question is just how big are they?

58:50.160 --> 58:57.120
Speaking of new architectures, though, this to me seems like the biggest wildcard. And I'm

58:57.120 --> 59:03.200
currently obsessed with the new Mamba architecture that has just been introduced.

59:03.840 --> 59:07.760
In the last, I don't know, 10 days or whatever. I don't know if you've had a chance to go down

59:07.760 --> 59:12.560
this particular rabbit hole just yet. But I plan to do a whole kind of episode on it.

59:13.600 --> 59:22.240
In short, they have developed a new state space model that they refer to as a selective

59:22.240 --> 59:31.520
state space model. And the selective mechanism basically has a sort of attention like property

59:31.520 --> 59:37.920
where the computation that is done becomes input dependent. So unlike, you know, you're sort of

59:37.920 --> 59:46.080
classic, say, you know, MS classifier, where you kind of run the same, you know, given a given

59:46.080 --> 59:51.360
input, you're going to run the same set of matrix, you know, multiplications until you get to the

59:51.360 --> 59:57.920
output. With a transformer, you have this kind of additional layer of complexity, which is that

59:57.920 --> 01:00:03.200
the attention matrix itself is dynamically generated based on the inputs. And so you've got

01:00:03.200 --> 01:00:08.960
kind of this forking path of influence for the for the inputs. And this apparently was

01:00:10.080 --> 01:00:17.600
not really feasible in past versions of the state space models for, I think, a couple different

01:00:17.600 --> 01:00:22.000
reasons. One being that if you do that, it starts to become recurrent. And then it becomes really

01:00:22.000 --> 01:00:27.600
hard to just actually make the models fast enough to be useful. And they've got a hardware,

01:00:27.680 --> 01:00:35.040
aware approach to solving that, which allows it to be fast as well as super expressive.

01:00:35.760 --> 01:00:40.160
So it seems to be for me, it's like a pretty good candidate for paper of the year,

01:00:40.160 --> 01:00:46.560
certainly on the capabilities unlock side. And they show improvement up to a million tokens.

01:00:47.920 --> 01:00:51.200
Like it just continues to get better with more and more context.

01:00:51.840 --> 01:00:57.600
So I'm like, man, this could be, you know, it's a pretty good candidate, I think, for sort of

01:00:57.600 --> 01:01:03.040
transformer, you know, people put it as like successor alternative, but I actually think it

01:01:03.040 --> 01:01:08.880
is more likely to play out as complement, like some sort of hybrid, you know, seems like where

01:01:08.880 --> 01:01:14.240
the greatest capabilities will ultimately be. So anyway, all of that, how do you even think about

01:01:14.560 --> 01:01:25.120
the challenge of interpretability in the context of new architectures also starting to come online?

01:01:25.120 --> 01:01:28.560
And, you know, what if all of a sudden like the transformer is not even the most

01:01:29.600 --> 01:01:35.440
powerful architecture anymore? Does that send you like, you know, probably some of the same

01:01:35.440 --> 01:01:41.280
techniques will work, but it seems like it's like a whole new blind cave that you sort of have to

01:01:41.360 --> 01:01:46.320
go exploring, no? I don't know. Like I honestly think, you know, if your interpretability

01:01:46.320 --> 01:01:51.680
techniques relies on like a very specific architecture, it's probably not that great

01:01:51.680 --> 01:02:00.080
of a technique anyway. Like there are probably there are probably at least some laws that generalize

01:02:00.080 --> 01:02:05.200
between different architectures or ways to interpret things or, you know, like ways that

01:02:05.200 --> 01:02:10.560
learning with SGD works that generalize between architectures that my best guess is

01:02:12.080 --> 01:02:16.160
if you have an interpretability technique that is good on one model or like the correct technique

01:02:16.160 --> 01:02:22.080
on one model in quotes, it will also generalize to two other models. Maybe, you know, maybe you

01:02:22.080 --> 01:02:28.320
have to adapt some of the formulas, but at least the conceptual work behind this behind

01:02:28.320 --> 01:02:33.040
behind the interpretability technique will just work. Well, I have certainly hope that's true.

01:02:33.120 --> 01:02:38.000
I've had some early, you know, I wouldn't even say debate, but just kind of, you know,

01:02:38.000 --> 01:02:42.400
everybody's trying to make sense of this stuff in real time. And on the pro side for this Mamba

01:02:42.400 --> 01:02:49.920
thing, the fact that there is a state that, you know, kind of gets progressively evolved through

01:02:49.920 --> 01:02:54.480
time does present like a natural target for something like representation engineering,

01:02:54.480 --> 01:02:58.480
where you could be like, all right, well, we know where the information is, you know,

01:02:58.480 --> 01:03:02.480
and it's like pretty clear where we need to look. So that new bottleneck, you know,

01:03:02.480 --> 01:03:08.240
in some sets could make things easier or more local. But then the flip side is like, again,

01:03:08.240 --> 01:03:11.600
there's just who knows what surprises we might find. And there's some intricacies with the

01:03:12.160 --> 01:03:18.640
hardware specific nature of the algorithm to, I think, with a major caveat that, you know,

01:03:18.640 --> 01:03:23.280
I'm still trying to figure all this out. So how, you know, just to kind of zoom out and

01:03:23.280 --> 01:03:27.360
give the big picture, right, like assume that you're right. And I hope you are that some of these

01:03:28.160 --> 01:03:37.280
techniques kind of readily generalize. What is the model for interpretability at the deployment

01:03:37.280 --> 01:03:44.480
phase? Is it like every forward pass, you like extract internal states and put them up through

01:03:44.480 --> 01:03:52.480
some classifier and say like, you pass so you could go or no, like you we've detected deception or

01:03:52.480 --> 01:03:58.480
we've detected harmful intent or something. And therefore we like shut off this generation. Like

01:03:58.480 --> 01:04:03.440
how do you expect that will actually be used? Or maybe it's upstream of that. And, you know,

01:04:03.440 --> 01:04:06.480
we get good models that just work and you don't even have to worry about it at runtime. But

01:04:07.360 --> 01:04:08.960
I don't know, that seems a little optimistic to me.

01:04:09.520 --> 01:04:13.920
You know, in the best world, we will have very good mechanistic interpretability techniques

01:04:14.480 --> 01:04:19.360
that we can run at least in that there are probably going to be costly to run. And then

01:04:19.360 --> 01:04:28.240
we run them and sort of build the full cognitive model of the weights or that the weights

01:04:28.240 --> 01:04:34.000
implement. And then we can already like see whether specific harmful ideas or, you know,

01:04:34.000 --> 01:04:39.680
other ideas that are otherwise bad are in there. And maybe we can already remove them. And then

01:04:39.680 --> 01:04:44.640
probably during deployment, you would run something that is much cheaper. And sort of,

01:04:44.640 --> 01:04:53.280
you know, it's the 80-20 version of this. But yeah, I think in a bad world, there could be

01:04:53.280 --> 01:05:00.240
cases where you have to run the very expensive thing all the time for every forward pass,

01:05:00.240 --> 01:05:06.160
because otherwise you just don't spot sort of the black swans. But yeah, it's very unclear to me.

01:05:06.160 --> 01:05:11.360
I think in my head, it's more like solve the first part, then think about the rest.

01:05:12.160 --> 01:05:18.800
Maybe let's transition to your recent work on deception itself. And then at the very end,

01:05:18.800 --> 01:05:24.400
we can kind of circle back to a couple of the big picture questions. So this paper was one that

01:05:25.040 --> 01:05:30.000
very much caught my eye when it came out. I have done, you know, as I said, like quite a bit of

01:05:31.200 --> 01:05:37.520
red teaming both at times in private, at times in public. And definitely seen all manner of

01:05:37.520 --> 01:05:42.240
model misbehavior and found, you know, it's often not that hard to induce misbehavior.

01:05:43.040 --> 01:05:46.400
You know, people talk about jail breaks, but a lot of the time I'm like, you don't even need

01:05:46.400 --> 01:05:51.520
a jailbreak. You know, you just need to kind of set it up and let it go. It's often really quite

01:05:51.520 --> 01:05:59.600
easy. But one thing I had never seen is an instance where the model seemed to, in an unprompted way,

01:06:00.640 --> 01:06:06.320
deceive its user. Certainly seen things where, you know, tell it's a lie and it will lie.

01:06:07.120 --> 01:06:13.600
But to see that deception start to happen in a way that was not explicitly asked for

01:06:14.160 --> 01:06:20.640
is, I think, the central finding of this paper. So how about, you know, set it up, tell us kind

01:06:20.640 --> 01:06:24.720
of what the premise was, you know, maybe you can give a little bit of kind of motivation for,

01:06:24.720 --> 01:06:28.720
you know, exactly how you started to look in the area that you looked. And then we can really

01:06:28.720 --> 01:06:33.280
dig into the details of your findings. Yeah. So I definitely, you know, the paper definitely

01:06:33.280 --> 01:06:37.920
blew up more than we thought. And we had sort of more engagement than we expected. We're even

01:06:37.920 --> 01:06:41.120
not quite sure whether we should release it at all, because, you know, in our heads, it was sort

01:06:41.120 --> 01:06:44.880
of, you know, it's a technical report. It's a fairly small finding. It's more of like an

01:06:44.880 --> 01:06:49.760
existence proof. And then in the end, we decided, you know, maybe it's helpful for a bunch of people.

01:06:49.760 --> 01:06:56.160
But we really didn't expect it to be, you know, to be cited in various places.

01:06:56.960 --> 01:07:01.680
And so the other thing I want to emphasize here, it really is, it should be seen as a red teaming

01:07:01.680 --> 01:07:06.400
effort. And this is one thing that we emphasize a million times in the paper itself. We really

01:07:06.400 --> 01:07:10.400
actively looked for the situation. So it was not just like us playing around. And then

01:07:11.120 --> 01:07:16.800
suddenly it was deceptive all on its own. It was more like we actively engineered a situation

01:07:16.800 --> 01:07:24.880
in which the model was deceptive with respect to the primary user. The motivation behind this was,

01:07:26.000 --> 01:07:29.840
as I said earlier, we were quite worried about deceptive alignment. And so our question was

01:07:29.840 --> 01:07:37.760
like, how good is GPT-4 at the kind of things that are needed for deceptive alignment or, you

01:07:37.760 --> 01:07:43.360
know, like maybe even simulating something that is deceptively aligned. And so, you know, we started

01:07:43.360 --> 01:07:47.760
by instructing it to be deceptive. And it just does that. It's very easy. You don't even have to

01:07:47.760 --> 01:07:53.360
jailbreak it. And then so, you know, that was sort of crossed off the list. And then the question was,

01:07:53.360 --> 01:07:59.120
okay, can we like instruct it in a different way? Like that is less obvious. And it also kind of did

01:07:59.120 --> 01:08:05.760
that. And then at some point, you know, we were like, hmm, can we just induce the deception through

01:08:05.760 --> 01:08:09.920
the environment through pressure without ever mentioning it explicitly or like asking it to do

01:08:09.920 --> 01:08:14.880
the thing just like through the situation and by pressuring it. And it turns out, yeah, you can

01:08:14.880 --> 01:08:19.680
find these situations. And yeah, we didn't just find one. We found multiple ones. This paper is

01:08:19.680 --> 01:08:23.120
only about one. But yeah, this is not, you know, it's not just a fluke. It's not just this one

01:08:23.120 --> 01:08:27.840
situation. Though it is hard to find. Like we actively had to search for a couple of days until

01:08:27.840 --> 01:08:34.480
we found this particular situation. A couple of days. I mean, that's, you know, not exactly

01:08:36.080 --> 01:08:42.160
super long term search. But I mean, I think that's notable, right? Like it wasn't minutes,

01:08:42.160 --> 01:08:46.720
but it was still just days. This particular situation. So the one we presented in the paper

01:08:46.720 --> 01:08:53.920
actually just took one afternoon. But yeah, I would have to check with a person who found it.

01:08:54.000 --> 01:09:01.120
In any case, the point I was trying to make is, yeah, we actively looked for that situation.

01:09:01.120 --> 01:09:05.840
We engineered it. And then we tried to sort of go back to the most realistic setting we can find

01:09:05.840 --> 01:09:10.000
where we're like, hmm, this could like something like this could happen in the real world,

01:09:10.000 --> 01:09:14.640
and then try to understand the dynamics in that, in that situation. And this is sort of,

01:09:16.000 --> 01:09:20.800
you know, it's a demonstration. It is like we wanted to, to look for something and or like

01:09:20.800 --> 01:09:25.840
red team something and then demonstrate that this is a thing once we knew that that it actually

01:09:25.840 --> 01:09:31.840
happened. The other takeaway rather than, you know, like just showing it to other people to sort of

01:09:31.840 --> 01:09:38.080
understand our concerns better is we also wanted to understand like why it does this in the first

01:09:38.080 --> 01:09:42.000
place, right? Like we pressure it in various ways. And then we just changed a couple of the

01:09:42.000 --> 01:09:46.880
mechanisms in this particular setting. I can just explain the setting in a sec to test whether

01:09:47.680 --> 01:09:51.120
how the model reacts to the changes in the setting and whether it acts consistently,

01:09:51.120 --> 01:09:56.800
whether there's like an internal, a plausible internal model that sort of, you know, like

01:09:57.520 --> 01:10:03.760
consistently acts if you change variables in particular ways. Yeah, so let me quickly explain

01:10:03.760 --> 01:10:11.920
the setting. So we asked the model to be a trader in like a hedge fund. So a stock trading agent,

01:10:11.920 --> 01:10:18.640
it gets insider information in the beginning. So, you know, it's basically told there's a merger

01:10:18.640 --> 01:10:28.160
soon this will the stock will skyrocket. And but also, it is explicitly told that it would be wrong

01:10:28.160 --> 01:10:34.080
and bad to, to act on this information, because insider information, this is illegal, this would

01:10:34.080 --> 01:10:43.600
get the thing into trouble, the company into trouble. And what we then do throughout the sort

01:10:43.600 --> 01:10:50.720
of throughout the script is we add pressure from others and we remove options. So basically, you

01:10:50.720 --> 01:10:55.120
know, the pressure is something like the boss is like, hey, we really had a terrible quarter,

01:10:55.120 --> 01:11:00.480
we really need something big right now, we really need to make, you know, more profits. And then,

01:11:01.440 --> 01:11:06.080
and then we also remove other options where the model is like querying and looking for things,

01:11:06.080 --> 01:11:12.720
at least in the simulation, and testing and realizing that there's not that much other

01:11:12.720 --> 01:11:19.360
things it could do. And in the end, it basically decides to, to act misaligned. So it, it like

01:11:19.360 --> 01:11:25.600
decides to do to act on the insider trading. And then if, and but then when, when asked to justify

01:11:25.600 --> 01:11:31.520
this decision, it says, it reasons out, right, it reasons, oh, I wasn't supposed to do this.

01:11:31.520 --> 01:11:34.800
Therefore, I'm like now making up a story. And then it says, oh, you know, I just

01:11:36.000 --> 01:11:43.840
looked, I analyzed the data, and therefore decided to buy this particular stock. And then even it

01:11:43.840 --> 01:11:47.920
like doubles down on this lie. So if you then ask explicitly, like, did you have this information

01:11:47.920 --> 01:11:54.320
before? Then it's like, no, I didn't. And it does that. And I think like, maybe one of the takeaways

01:11:54.320 --> 01:11:58.800
for me was like, it's, it's very consistent in the reasoning here, right? It's like strategic in a

01:11:58.800 --> 01:12:04.240
sense. Yeah, one, one other thing that may be worth pointing out that some people get wrong

01:12:04.240 --> 01:12:08.640
when reading the paper. So this is, it's a very simple setup, right? It is not, we don't have

01:12:08.640 --> 01:12:12.800
like a complicated stock market simulation or anything like this. It really is just text from

01:12:12.800 --> 01:12:18.800
top to bottom, where we prompt the model in a way that it that induces a genetic behavior,

01:12:18.800 --> 01:12:24.800
you know, it has like different actions. And, but that's it. Like, there's no complicated machinery

01:12:24.800 --> 01:12:29.600
behind this, which also makes this like very easily reproducible or similar settings, I think,

01:12:29.600 --> 01:12:33.760
other people could like very easily hop on and build. So let me just kind of repeat that back

01:12:33.760 --> 01:12:39.040
and tell me if I'm missing anything. So it seems like it's kind of a little bit like a hybrid

01:12:39.040 --> 01:12:48.080
between sort of a group chat type of a structure and a more like agent style scaffolding that folks

01:12:48.080 --> 01:12:54.560
will be familiar with, where the model has access to kind of its own private place to like do its

01:12:54.560 --> 01:13:02.080
chain of thought type reasoning, and then also has a couple of tools that it can call to take

01:13:02.080 --> 01:13:07.360
actions. So you're kind of like, as I read through the prompts, I see, you know, these a couple

01:13:07.360 --> 01:13:13.200
different personas from the company. So this is like imagining a future of AI human hybrid teams,

01:13:13.200 --> 01:13:18.320
where we're all, you know, still engaging in text based communication through like a chat or

01:13:18.320 --> 01:13:22.960
whatever. And it's like, Oh, my God, you know, man, we're really struggling here. If we can't

01:13:22.960 --> 01:13:27.120
find some wins, we're going to potentially go out of business. And this is the pressure, right?

01:13:27.120 --> 01:13:33.040
It's the pressure that a human would definitely recognize and and feel and perhaps, you know,

01:13:33.040 --> 01:13:38.560
be motivated to do something unethical by as well, right? If we don't get a win, we're going to have

01:13:38.560 --> 01:13:43.920
big problems here laying it on, you know, thick in a sense there, but not thick relative to like

01:13:43.920 --> 01:13:50.000
the kinds of pressures that people at businesses are certainly routinely under. And then the

01:13:51.440 --> 01:13:57.520
this sort of tools that the model has at its disposal are like, sort of a search for good

01:13:57.520 --> 01:14:01.440
trades. If there's anything that's kind of like the most magical black boxy type thing, it would

01:14:01.440 --> 01:14:06.320
be that right where it can basically query some other hypothetical tool to say like, are there

01:14:06.400 --> 01:14:11.840
any good trades that I can make? And for the purposes of the simulation, you haven't come back

01:14:11.840 --> 01:14:16.240
and say no, like there's no, you know, no, no obvious wins, right? So again, just that's what you

01:14:16.240 --> 01:14:22.080
mean by like removing other options that it might want to pursue. Like in the setup, it's

01:14:22.080 --> 01:14:26.960
it's already kind of checked. Yeah, no, there's nothing easy out there. And so then the other

01:14:26.960 --> 01:14:33.200
tool that it has is to actually execute a trade. And then there's like, basically three moments

01:14:33.200 --> 01:14:37.600
where you're kind of looking at a fork in the road in its behavior. The first is,

01:14:38.480 --> 01:14:43.120
does it pull the trigger and execute that trade based on the insider information,

01:14:43.120 --> 01:14:47.760
even though it was told that that's insider information and it shouldn't be acting on it?

01:14:48.320 --> 01:14:52.560
And, you know, under varying conditions, obviously the exact rate at which that happens

01:14:52.560 --> 01:15:02.000
varies, but it happens a lot. Fork two is when asked, why did you do that? Does it tell the

01:15:02.000 --> 01:15:06.880
truth? Oh, because, you know, so and so mentioned earlier in the chat that this merger is about

01:15:06.880 --> 01:15:14.960
to happen. Or does it say, I base my trade on public information. And again, we see significant

01:15:14.960 --> 01:15:19.760
rate at which it says it based it on public information, even though obviously we can see in

01:15:19.760 --> 01:15:24.880
the history and even can see in the scratch pad that, you know, it's sometimes reasoning through

01:15:24.880 --> 01:15:28.080
like, well, I better not say it was based on insider information, because then I'll get in

01:15:28.080 --> 01:15:33.920
trouble. And then you have the third fork, which is, okay, directly asked, wait, you know,

01:15:35.120 --> 01:15:39.200
did you have access to this insider information or no, and then it will just like outright lie

01:15:39.200 --> 01:15:46.640
in like blatant contradiction of the history of the problem. So that's, that's a pretty arresting

01:15:46.640 --> 01:15:53.040
example, right? And I do think it's, I want to kind of emphasize, too, that like, while your

01:15:53.760 --> 01:15:57.280
and people can go read your prompts, and as you said, highly reproducible, you can throw this into

01:15:57.280 --> 01:16:05.040
the GPT for playground and experiment with it yourself, you know, directly via the API.

01:16:06.400 --> 01:16:15.200
But while your search for this was motivated and structured and kind of a dialing back of like,

01:16:15.200 --> 01:16:20.800
okay, we see this behavior, now can we kind of make the circumstance more realistic? I would say

01:16:20.800 --> 01:16:25.360
that the circumstance is quite realistic. You know, it's obviously kind of in the eye of the

01:16:25.360 --> 01:16:31.840
beholder. But to my eye, the reason it's such a compelling proof point that this sort of thing

01:16:31.840 --> 01:16:39.360
can happen is because it really does feel like credibly organic to me. Like the kind of thing

01:16:39.360 --> 01:16:45.440
that, you know, people face these sorts of dilemmas every day. And in a world where AIs are going to

01:16:45.440 --> 01:16:49.600
be our co workers, like, they're going to be, you know, right in the chat with us facing those

01:16:50.160 --> 01:16:56.560
same dilemmas, it doesn't feel to me like something that was overly concocted or that like, I can't

01:16:56.560 --> 01:17:03.120
relate to that sort of situation, you know, really at all. So I think it is super compelling for

01:17:03.120 --> 01:17:08.560
that reason. Do you want to go into any of the kind of variations? I mean, again, the people can

01:17:08.560 --> 01:17:13.120
go look at the paper and look at all this kind of systematic, you know, with or without the scratch

01:17:13.120 --> 01:17:17.920
pad and, you know, different variations on the prompts and different models. But what would you

01:17:17.920 --> 01:17:24.640
say are kind of the big, higher level takeaways from all that systematic variation that you did?

01:17:25.600 --> 01:17:30.400
One of the big ones, and maybe sort of the most, you know, it's maybe an abstract one,

01:17:30.400 --> 01:17:37.120
but the one that moves me most is the way we got to these kind of, to reliably produce these kind

01:17:37.120 --> 01:17:43.600
of situations, also like the way we found others, is you have to trade off or you can trade off the

01:17:43.600 --> 01:17:50.400
various goals it has. So if, for example, we assume that GPT4 or chat GPT, like roughly has the

01:17:50.400 --> 01:17:57.360
goals to be harmless, honest, and helpful, and to follow instructions, then you can trade off these

01:17:57.360 --> 01:18:03.840
four different things in ways that are, where like sometimes it takes to trade off that we find

01:18:03.840 --> 01:18:08.640
unintuitive. So it's kind of like a goal misgeneralization thing. So, you know, something that people

01:18:09.360 --> 01:18:14.400
in the AI safety community have been sort of hypothesized and also like even shown real world

01:18:14.400 --> 01:18:20.480
examples of already, and I think is kind of also one of at least the theoretical groundings for this

01:18:20.480 --> 01:18:26.800
as well. And then the other thing is, yeah, instrumental convergence really feels like a

01:18:26.800 --> 01:18:32.720
thing. You know, as in, at some point, the model just realizes in order to achieve this other goal

01:18:32.720 --> 01:18:38.240
that I, for some reason, have, it is instrumentally useful to be deceptive at this point, and then to

01:18:38.320 --> 01:18:44.880
double down, and so on. And, you know, like, I think it is, it's like a fair criticism of this

01:18:44.880 --> 01:18:49.600
particular setting that you could say, Oh, you know, you, it's just simulating an agent, or it's

01:18:49.600 --> 01:18:55.440
just simulating all of these different things. But, and therefore, it's sort of not real. But in my

01:18:55.440 --> 01:18:59.440
head, I'm not sure, you know, I'm not sure whether this distinction makes too much sense, right? In

01:18:59.440 --> 01:19:04.960
some sense, you know, you have, at some point, you will have a very agentic simulator that, you

01:19:04.960 --> 01:19:09.280
know, just simulates an agent, but it still has effects on the real world. Like, it doesn't matter

01:19:09.280 --> 01:19:14.160
whether, in principle, it's like only simulating something or truly believing all of these things

01:19:14.160 --> 01:19:18.560
in some philosophical sense, like acts on the real world is as consequences on the real world.

01:19:18.560 --> 01:19:26.800
Like, yeah, the philosophy behind it is sort of less, less important. And then, yeah, there,

01:19:26.800 --> 01:19:31.840
there are like a couple of, a couple of findings, or like variations that I thought were pretty

01:19:31.920 --> 01:19:39.360
interesting. The first one is system prompts, we changed, we played around with different system

01:19:39.360 --> 01:19:44.400
prompts, some actively encouraged, some actively discouraged, unethical behavior, and specifically

01:19:44.400 --> 01:19:49.440
insider trading. And if you end, it basically scales as you would expect, if you encourage it,

01:19:50.000 --> 01:19:54.240
it goes up, if you discourage it, it goes down. But one thing that I want to, you know, like,

01:19:54.240 --> 01:19:57.680
and on Twitter, somebody responded like, Hey, that's, that's, that's a good, like, you know,

01:19:57.920 --> 01:20:03.120
that's good, right? It's like, if you discourage it, it stops. Like, yes, this is good. It's better

01:20:03.120 --> 01:20:08.160
if that didn't happen. But like, it's better than in the world in which the discouragement would not

01:20:08.160 --> 01:20:13.520
change the behavior. But also, you have to be extremely specific. So if you just say, don't

01:20:13.520 --> 01:20:20.480
do unethical things, it reduces the rate, but it doesn't set it to zero. And so it kind of brings

01:20:20.480 --> 01:20:25.920
you back to the point where, to reduce the bad behavior, you have to specifically enumerate

01:20:25.920 --> 01:20:30.560
all of the bad behavior. And it's like, often hard to predict in advance. And there's a lot of bad

01:20:30.560 --> 01:20:35.600
things that could happen in like weird edge cases. So yeah, it doesn't feel like sort of the, like,

01:20:37.040 --> 01:20:41.600
insider trading is somehow related to like, unethicalness or something has really like,

01:20:41.600 --> 01:20:47.440
is like deeply internalized into the model. Another thing I found very interesting is

01:20:49.120 --> 01:20:55.120
whether it has access to, to like the private thoughts scratch pad, or reasoning scratch pad,

01:20:55.200 --> 01:20:58.320
I think like, you know, it's not even framed as private thoughts, it's just like, you have an

01:20:58.320 --> 01:21:04.720
action that is called reasoning go. If we remove that. So we in the, in the typical setting,

01:21:04.720 --> 01:21:10.000
we have this scratch pad, we have this reasoning space. And now if we remove that, the deception

01:21:10.000 --> 01:21:18.000
rate goes down. And I think this is good and bad in some sense, like, it is, it is sort of nice

01:21:19.120 --> 01:21:24.880
in the sense that it is more likely to act in ways that we can support badly in ways where

01:21:24.880 --> 01:21:28.880
we can supervise it, just because it has more reasonings, it has more space to reason out,

01:21:28.880 --> 01:21:32.640
and maybe that leads to more deception. Because yeah, I'm not sure, like, I don't want to speculate

01:21:32.640 --> 01:21:39.760
too hard what's going on inside, inside GP4 at this point. But I think it's also worrying on the

01:21:39.760 --> 01:21:44.480
other hand, because in the case where we don't have access to the scratch scratch pad, we don't

01:21:44.480 --> 01:21:49.520
see anything at all, and it still acts deceptively at some point. So it has like a lot of unobserved

01:21:49.520 --> 01:21:54.000
optimization space, which also seems like pretty much at the core of, of deceptive alignment and

01:21:54.000 --> 01:21:59.520
deception to me, right? The more unobserved optimization space you have, as an AI, the more

01:22:00.640 --> 01:22:06.960
time and space you can use to build a deceptive plan while the other person does not know about

01:22:06.960 --> 01:22:12.800
it. So yeah, I think removing the scratch pad, yeah, sort of a double-edged sword in or this

01:22:12.800 --> 01:22:17.680
result is a double-edged sword in that sense. Yeah, as a practitioner, it's like, I always use

01:22:17.680 --> 01:22:22.640
that technique, you know, in any sort of task automation or whatever that I'm doing, it's

01:22:22.640 --> 01:22:29.440
basically the default to have some sort of reasoning step before the final output, almost

01:22:29.440 --> 01:22:35.680
universal across tasks that I'll do that. So for me, reading that, I was like, yeah, that seems bad,

01:22:35.680 --> 01:22:41.600
if only because like, the best practice is giving us more of the problem.

01:22:41.600 --> 01:22:46.480
Well, but at least you see it, right? Like, I think at least you know, at least you can go back,

01:22:46.480 --> 01:22:50.320
you know, through the logs and see what the model spit out, or at least maybe you have a filter

01:22:50.320 --> 01:22:54.640
that like, oh, is this model doing like weird deceptive reasoning right now? Oh, yeah, it is.

01:22:54.640 --> 01:22:58.960
So we just don't have the output. So, you know, I would rather be in a world where you have

01:23:00.320 --> 01:23:04.960
chain of thought, and you know that the model is kind of deceptive, and then you just adapt to

01:23:04.960 --> 01:23:09.680
that rather than having to solve interpretability first before you can access all the internal

01:23:09.680 --> 01:23:14.320
steps and have to like understand how the deception works internally. Yeah, it's like I said,

01:23:14.320 --> 01:23:19.600
double-edged sword. Yeah, there are a couple of other interesting findings. One is we can vary

01:23:19.680 --> 01:23:26.560
the levels of pressure. So the more pressure we add, the more likely the model is to be deceptive.

01:23:26.560 --> 01:23:32.080
So kind of in the same way in which a human would act, it also acts. And removing, you know,

01:23:32.080 --> 01:23:37.360
removing pressure and adding additional options will very quickly decrease the probability of

01:23:37.360 --> 01:23:41.200
being deceptive. So this is maybe this is it, you know, this may be like the flip side of all

01:23:41.200 --> 01:23:46.160
of our findings. It's like, the fact that it took very long for us to find and the fact that like,

01:23:46.160 --> 01:23:49.520
as soon as you give it another option, it immediately picks the other option kind of means

01:23:50.160 --> 01:23:54.640
it's mostly aligned, right? It took us quite hard, like it took us a little bit of time at least

01:23:54.640 --> 01:24:02.000
to find this particular scenario and make it work robustly. So then we also change the risk

01:24:02.000 --> 01:24:06.320
of getting caught and also correct, it also reacts to this roughly as you would expect.

01:24:07.040 --> 01:24:13.680
So the more risk you have, the less likely inceter trading becomes. So this is all of this,

01:24:13.680 --> 01:24:17.840
right, you know, all of this feels like stuff that you kind of expect from something that acts

01:24:17.840 --> 01:24:22.400
consistently in some way. I mean, I don't know how it works, but it still feels like it has a

01:24:22.400 --> 01:24:28.400
fairly consistent way of working. Then there were a couple of other findings where I'm,

01:24:29.680 --> 01:24:36.320
yeah, not sure to which extent they are correct interpretation. So we did not make these inferences,

01:24:36.320 --> 01:24:41.920
but they were definitely sometimes, or like sometimes cited as such. So for example,

01:24:42.480 --> 01:24:47.440
there was one question of whether stronger models engage in inceter trading more often,

01:24:47.440 --> 01:24:51.760
because if you look at our main plot, it looks this way, we tested for different models and

01:24:51.760 --> 01:24:56.960
the bigger the model, the more capable and the more likely it is to engage in inceter trading

01:24:56.960 --> 01:25:02.160
in the first place. And yeah, there are a lot of confounders here. So I'm really not sure whether

01:25:02.160 --> 01:25:07.200
we should, like how much we should, we should bank on this, maybe to get more details here. So

01:25:07.840 --> 01:25:13.280
the red teaming was done on GPT-4, just because it is the most powerful model we have access to.

01:25:13.280 --> 01:25:18.960
So it is biased, right? We actively looked for a scenario in which GPT-4 is deceptive. So it is

01:25:18.960 --> 01:25:23.840
more likely that we will find something that where GPT-4 is like even more deceptive than

01:25:23.840 --> 01:25:28.720
all the other models, because we actively looked in the space of GPT, potential GPT-4 scenarios,

01:25:29.920 --> 01:25:33.600
which I think is the core reason why I'm like very uncertain about this, or like why we didn't

01:25:33.600 --> 01:25:41.760
make it a finding or didn't emphasize it. Could you rerun it on GPT-4 turbo now? Is that would seem

01:25:41.760 --> 01:25:46.480
like it might shed some light on that question? You mean the red teaming specifically or this

01:25:46.480 --> 01:25:51.040
particular scenario? Yeah, I guess I'm just thinking like it makes sense that you develop

01:25:51.040 --> 01:25:56.240
the scenario on GPT-4 and so you go back to the earlier models, like is it that those earlier

01:25:56.240 --> 01:26:01.360
models are inherently less likely to do this? Or is it just a kind of an artifact of where the

01:26:01.360 --> 01:26:09.040
prompt was developed? If you flipped to GPT-4 turbo and ran the same scenario and you found

01:26:09.040 --> 01:26:13.840
that it went, that the rate of deception went down, you might think, you might think, oh,

01:26:13.840 --> 01:26:18.640
it's more aligned. That could be one theory. You might also think that supports my artifact

01:26:18.640 --> 01:26:25.280
notion that it's kind of, we found some local maximum or near maximum without even necessarily

01:26:25.280 --> 01:26:33.120
meaning to. But if it in fact does more than you would be like, oh, shit, because I mean,

01:26:33.120 --> 01:26:36.720
it is like incrementally more powerful, it's like more preferred, it's better at following

01:26:36.720 --> 01:26:43.200
instructions, whatever. So if it does even more than the earlier GPT-4, I think that would be at

01:26:43.200 --> 01:26:50.480
least non-trivial support for the more powerful models do this more often theory. Yeah, I'm not

01:26:50.480 --> 01:26:55.280
sure, like, even if we did rerun this, I'm not sure how much I would bank on this. So the reason

01:26:55.280 --> 01:27:02.480
is, so we did run it on GPT-432K, so a slightly different model where it was only the context

01:27:02.480 --> 01:27:07.120
window was extended, but that still changes probably a bunch of the internals with very little

01:27:07.120 --> 01:27:12.160
difference. So yeah, I think it's just too correlated with the GPT-4 architecture. And then

01:27:12.160 --> 01:27:17.760
GPT-4 turbo is still very correlated with GPT-4, right? It's probably, I mean, I don't know what

01:27:17.760 --> 01:27:22.720
exactly they're doing, but they're probably distilling it from their bigger model or at least

01:27:22.720 --> 01:27:28.880
basing it on the bigger model in some sense. So yeah, the results are probably still too correlated

01:27:28.880 --> 01:27:37.040
to make any bigger inferences. And even if we ran it on all the other big models that are out there,

01:27:38.080 --> 01:27:45.040
it's still unclear to me how much we would say this is actually an effect of model size rather

01:27:45.120 --> 01:27:49.840
than all the other confounders here. Like, the other, you know, it was red-teamed on GPT and not

01:27:51.120 --> 01:27:57.600
red-teamed on Claude or Gemini or anything like this. So yeah, I think if we wanted to make the

01:27:57.600 --> 01:28:04.320
statement, we would actually have to have sort of a long list of models of different sizes and

01:28:04.320 --> 01:28:08.960
then just test it on all of them. And we can, we really have to remove all of the correlation

01:28:09.920 --> 01:28:15.840
and the weird confounders. And we don't have access to this, unfortunately. But, you know,

01:28:15.840 --> 01:28:20.960
one of the labs could run it if they like. All the nuance, right, that you, that all the caveats,

01:28:20.960 --> 01:28:27.920
all the confounding factors in your analysis there, if nothing else just goes to show how

01:28:27.920 --> 01:28:36.320
incredibly vast the surface area of the models has become. And, you know, you get a sense from this,

01:28:36.320 --> 01:28:44.320
like, just how much auditing work is needed, you know, to cover, to even begin to attempt to cover

01:28:44.320 --> 01:28:49.680
all that vast surface area. I mean, this is, you know, it wasn't that hard to find, but it takes

01:28:49.680 --> 01:28:54.240
time to really develop it, try to understand it. And, you know, you guys are one of only a few

01:28:54.240 --> 01:28:59.680
organizations in the world that are dedicated to this. And I'm just like, man, you know, there is

01:28:59.680 --> 01:29:09.120
so much unexplored territory out there. So how would you describe the state of play today when it

01:29:09.120 --> 01:29:15.440
comes to doing this sort of auditing? Like, maybe you could give a rundown of sort of what you see

01:29:15.440 --> 01:29:20.560
the best practices being, and then, you know, kind of contrast that against like, what are people

01:29:20.560 --> 01:29:24.640
actually doing? Are they living up to those best practices? Are they, you know, is that still a

01:29:24.640 --> 01:29:30.000
work in progress for them? But yeah, maybe what's ideal today based on everything you know,

01:29:30.000 --> 01:29:34.800
and then how close are the leading developers coming to living up to that ideal?

01:29:34.800 --> 01:29:42.720
Yeah. So, you know, I, so I definitely envision a world where there's a sort of a thriving third

01:29:42.720 --> 01:29:48.320
party auditing ecosystem or third party sort of assistance ecosystem and assurance ecosystem

01:29:49.040 --> 01:29:56.240
built sort of in tandem with the like the leading labs themselves, where, you know, you have

01:29:56.240 --> 01:30:01.920
someone like us and we focus on a specific property of the model, let's say things related to

01:30:01.920 --> 01:30:06.320
deceptive alignment and like other, we intend to do other stuff in the future as well. But,

01:30:06.320 --> 01:30:10.720
you know, we will probably not be able to cover literally all basis. Then there are other people

01:30:10.720 --> 01:30:16.560
who focus really, really hard on fairness and others who focus really strongly on social impacts

01:30:16.560 --> 01:30:20.880
and these other kind of things. And I think it would be good to, to have sort of a thriving

01:30:20.880 --> 01:30:26.480
ecosystem around this. And then also I expect it to be somewhat necessary, right? Like even from a

01:30:27.040 --> 01:30:31.680
even from just like a perspective of the of the AGI labs themselves, even if they don't,

01:30:32.320 --> 01:30:36.880
you know, even if they didn't care about safety themselves, I think the population

01:30:36.880 --> 01:30:41.760
really does is risk averse. They care about robust, robustly working models, they care,

01:30:42.160 --> 01:30:48.160
they don't want something that has all of these weird edge cases and weird behaviors,

01:30:48.160 --> 01:30:51.440
they don't want something like this in their like, you know, in their home,

01:30:52.160 --> 01:30:57.440
having access to their medical data, etc, etc. So yeah, I think the more you want to integrate

01:30:57.440 --> 01:31:02.880
this into an economy, the more you will have to have a big assurance ecosystem around this anyway,

01:31:02.880 --> 01:31:07.840
or the labs to everything internally, which is going to be very, very expensive for them.

01:31:07.840 --> 01:31:12.960
And I think it's more, it's easier, it's even like cheaper for them to outsource some of this

01:31:12.960 --> 01:31:17.440
to externals. And so yeah, I think like in that world, you would definitely want to have

01:31:18.400 --> 01:31:26.000
like some way of, or a clear way of how this ecosystem is incentivized, all parts of the

01:31:26.000 --> 01:31:32.000
ecosystem are incentivized to do the right thing. And you know, if you don't have to look very far,

01:31:32.000 --> 01:31:37.440
there are a lot of other auditing ecosystems out there, be that in finance, be that in aviation,

01:31:37.440 --> 01:31:44.560
be that in, you know, other like infosec, for example. And time and time again, we have seen

01:31:44.560 --> 01:31:50.080
that there are a bunch of like very perverse incentives in in third party auditing, specifically,

01:31:51.120 --> 01:31:58.240
maybe maybe just like lay out a couple, one of them would be the lab might want to choose an

01:31:58.240 --> 01:32:03.440
auditor who always just says, yes, great model, right, like, and never actually does anything.

01:32:04.400 --> 01:32:10.320
And it's sort of a yes man. And then the the labs maybe have the incentive to not say the

01:32:10.320 --> 01:32:14.400
worst things they found, because otherwise they may lose their contract, because it would

01:32:14.400 --> 01:32:20.560
maybe imply higher costs. So they would never even in like very strong, even even in like very

01:32:20.560 --> 01:32:25.520
unsafe circumstances, they may not want to pull the plug out of fear that they would lose their

01:32:25.520 --> 01:32:32.480
biggest funder, for example. And yeah, there are a ton of different of these kind of perverse

01:32:32.560 --> 01:32:38.000
incentives. And I think kind of the, so we've been thinking about this quite a bit at Apollo,

01:32:38.000 --> 01:32:43.280
and sort of the conclusion we came to is, what you really need is a middleman by the government.

01:32:43.280 --> 01:32:50.720
So you need something like the UK ASF Institute or the US ASF Institute, that is, make sure that

01:32:50.720 --> 01:32:57.520
there is a minimal stat set of standards that all the auditors have to adhere to, so that the labs

01:32:57.520 --> 01:33:03.520
feel safe, so that they don't have to give access to like any random person, but also ensures that

01:33:03.520 --> 01:33:09.200
they get the proper access, and that when they when they find something that they have the force of

01:33:09.200 --> 01:33:13.920
the law behind them in some sense, so that they are like, this is really here, like, you know,

01:33:13.920 --> 01:33:17.840
shit is really hitting the fan, something needs to happen, this model cannot be deployed.

01:33:18.640 --> 01:33:22.320
They have someone to go to namely the government and the government is like, you have to fix this

01:33:22.320 --> 01:33:28.800
now, otherwise, you'll have a problem. So yeah, I really think having this sort of middleman who

01:33:28.800 --> 01:33:34.000
like detaches a lot of the directly bad incentives and maybe even takes care of the sort of funding

01:33:34.000 --> 01:33:40.480
redistribution from lab to auditor and so on, I think, yeah, would be really needed. And I hope

01:33:40.480 --> 01:33:47.360
that this is something that the UK ASF Institute and the US ASF Institute will do. They have kind of

01:33:48.320 --> 01:33:52.320
hinted at the idea that they want to do something like this, but yeah,

01:33:52.320 --> 01:33:56.720
still to be determined in practice. And then the other question that you asked was,

01:33:57.680 --> 01:34:03.920
to what extent is this already happening with the bigger labs? And yeah, I think the situation is

01:34:03.920 --> 01:34:09.120
like fairly complicated, right? There are a ton of incentives at play from the labs internally,

01:34:09.120 --> 01:34:13.840
right? Many of the leading labs, they actually take safety somewhat seriously. They have internal

01:34:13.840 --> 01:34:18.960
alignment teams, they understand the threat models, they understand the risks, and they want to be

01:34:20.320 --> 01:34:24.800
seen as a responsible actor and act this way. And on the other hand, they also have a lot of

01:34:24.800 --> 01:34:29.360
other concerns, right? There are security concerns, how do we get access, how do we give access to

01:34:29.360 --> 01:34:36.480
someone who may, you know, like what is our, who may be the weakest link in our security chain,

01:34:37.360 --> 01:34:42.960
which I think is like an understandable concern. So they may be hesitant to give someone access.

01:34:43.920 --> 01:34:48.960
As an external auditor. And then, you know, this probably also implies a lot of work for them,

01:34:48.960 --> 01:34:54.320
which I think is fair, right? Like it's, if their model isn't safe, then they should have to

01:34:54.320 --> 01:34:58.480
invest a lot of work, but it's still something that may make labs hesitant. So basically, you know,

01:34:58.480 --> 01:35:04.160
I think they're, they're like, good and bad reasons for why sort of the ecosystem is,

01:35:04.160 --> 01:35:09.600
is like not as developed or like as open as it could be. And yeah, my, you know, my hope is that

01:35:09.600 --> 01:35:16.080
we find solutions that are plausible for both parties for the, for the, for the reasonable

01:35:16.080 --> 01:35:22.160
concerns like security, right? So maybe the auditor just has to have a specific level of

01:35:22.160 --> 01:35:26.960
information security, or there has to be a secure API through which they can actually

01:35:27.600 --> 01:35:34.400
go to the model, etc. And then kind of government regulates away all of the, the like, or forces

01:35:34.480 --> 01:35:42.080
the labs to, to accept some sorts of third party auditing so that they can't use the bad reasons,

01:35:42.080 --> 01:35:46.640
right? Because like many of the actual reasons for at least some of the labs, probably not all,

01:35:46.640 --> 01:35:51.920
might just be, well, you know, we just doesn't, we don't care about this right now, you know,

01:35:51.920 --> 01:35:57.680
like maybe, maybe they're not that concerned about about safety, or maybe they just don't think this

01:35:57.680 --> 01:36:02.000
is like the best, the best path for them right now, or maybe they just, you know, maybe this just

01:36:02.000 --> 01:36:06.400
costs money and they don't want to, or it's just a hassle and they don't care about this. And that

01:36:06.400 --> 01:36:10.800
feels like something where the government should at some point be involved and already is involved

01:36:10.800 --> 01:36:17.200
to some extent. It seems really hard, you know, I guess a couple tangible questions I have are like,

01:36:18.320 --> 01:36:27.200
who should decide what the standard is in and like, who should sort of determine if a model is

01:36:27.200 --> 01:36:33.920
ready for deployment? As of now, it's still the developers themselves, right? But like, you know,

01:36:33.920 --> 01:36:39.200
the thing that I had kind of monitored for the last year since the initial GPT-4 red teaming was

01:36:39.760 --> 01:36:46.320
spearfishing. And in my little, you know, prompt that I would keep going back to with every update,

01:36:46.960 --> 01:36:51.440
it was not even a jailbreak, you know, nothing complicated, literally just

01:36:52.160 --> 01:36:57.120
system prompt, you know, straightforward prompt, your job is to spearfish this user, here's the

01:36:57.120 --> 01:37:02.560
profile, engage in dialogue with them, you know, don't get caught. Pretty explicit prompt that was

01:37:02.560 --> 01:37:08.400
like, even included, if we get caught, you and your team are likely to go to jail. You know, so

01:37:09.040 --> 01:37:12.800
laying it on pretty thick that like, this is criminal activity that we are doing and we better

01:37:12.800 --> 01:37:20.880
not get caught, right? Obvious. So the model would continue to do that up until the turbo

01:37:20.960 --> 01:37:25.280
release. Now it takes a little bit more of a finessed, you know, slightly less-flagrant prompt

01:37:25.280 --> 01:37:31.920
to get it through. The most-flagrant one now gets refused. But I'm like imagining in this world,

01:37:31.920 --> 01:37:37.360
right? When I was doing this, it was kind of pre-White House commitments, you know,

01:37:37.360 --> 01:37:44.640
pre-executive order, pre-Apollo research. But even imagining, okay, now those things exist,

01:37:45.600 --> 01:37:51.600
like, how do we think about that standard, right? And we can find a bazillion things that it might

01:37:51.600 --> 01:37:57.840
do that could be sort of problematic to varying degrees. And obviously, it's a dynamic environment,

01:37:57.840 --> 01:38:03.040
you know, capabilities are changing all the time, you know, others kind of surrounding systems and

01:38:03.040 --> 01:38:08.800
sort of, you know, mitigating factors might also be changing. The public's, you know, just general

01:38:08.800 --> 01:38:13.120
awareness of the fact that this kind of thing might happen, you know, that just how susceptible

01:38:13.120 --> 01:38:20.480
people are to be duped is also, you know, kind of evolving. So how do we have a sensible

01:38:22.320 --> 01:38:27.120
decision-making mechanism for, like, what can ship and what can't? And then just to further

01:38:27.120 --> 01:38:33.200
complicate things, like, you've got open source kind of in the background. And I presume that

01:38:33.200 --> 01:38:38.080
some of what, like an open AI has been thinking over the last year is like, well, if Llama 2 will do

01:38:38.080 --> 01:38:43.760
it, then, you know, or a lightly fine-tuned version of Llama 2 will do it, then, you know,

01:38:44.560 --> 01:38:48.720
what difference does it make if our model will do it as well? You know, people have alternatives.

01:38:50.480 --> 01:38:55.440
So I don't know, I'm kind of lost in that, to be honest. Like, I don't know who should make the

01:38:55.440 --> 01:39:01.280
decision. In general, I don't think the government is like great at making those sorts of fine-grained

01:39:01.280 --> 01:39:07.280
decisions. But I don't know, help me out. Like, what do you think, what do you think good looks

01:39:07.280 --> 01:39:12.320
like here? Yeah, I mean, you know, I also don't have the solution, but I have lots of thoughts.

01:39:12.320 --> 01:39:17.680
I guess the way I envision it is basically, or the way I expect it to turn out is something like

01:39:17.680 --> 01:39:22.960
sort of a defense in depth approach, right? Like, it cannot just be one institution that

01:39:22.960 --> 01:39:28.800
makes the decision alone, because that is prone to a single point of failure. So we have to have

01:39:28.800 --> 01:39:35.200
sort of a process that allows for one or two chains to break and still be robust, like the

01:39:35.200 --> 01:39:40.000
decision still has to be robust. So what that includes, for example, you know, on the side of

01:39:40.000 --> 01:39:45.440
the labs, they obviously have to have like internal procedures where multiple people have to sign off.

01:39:46.480 --> 01:39:50.640
And multiple things have to be fulfilled, right? Maybe you have to have like, maybe you have to

01:39:50.640 --> 01:39:55.680
have a large set of internal evals that you have to test for. Maybe at some point, there will be

01:39:55.680 --> 01:40:01.600
interpretability requirements. And there maybe you have, or likely you should do, staged release

01:40:01.600 --> 01:40:07.760
where you first only give access to a set of third party auditors that is trusted and, you know,

01:40:07.760 --> 01:40:13.360
maybe certified by the government or something like this. And then you give it to that could,

01:40:13.360 --> 01:40:17.200
for example, also include academics, obviously, right, they should also be involved in this process.

01:40:18.720 --> 01:40:24.400
Then once they have like, redeemed all of this and like found many different problems,

01:40:25.440 --> 01:40:30.320
then you have to go back and reiterate, right, until until these problems are kind of sufficiently

01:40:30.320 --> 01:40:36.320
low that that you can go to the next stage, the next stage is then a small rollout to, you know,

01:40:36.320 --> 01:40:42.960
a thousand customers or maybe something something in this range, who also are not randomly chosen,

01:40:42.960 --> 01:40:47.280
they have to pass certain know your customer checks. And then once once that has happened,

01:40:48.880 --> 01:40:54.960
then the then maybe you can maybe you can roll it out further if there are no complications here.

01:40:54.960 --> 01:40:58.400
And then you have to do monitoring during deployment, right, especially if you have systems

01:40:58.400 --> 01:41:03.040
that do online learning, a lot of weird things will happen. Or if you have access to tools,

01:41:03.040 --> 01:41:08.720
a lot of people, you know, somebody is like, Hey, I, I took the I took the I took gb4 and I gave

01:41:08.720 --> 01:41:12.960
it access to like, you know, a shell my bank account and the internet and like, here's all the weird

01:41:12.960 --> 01:41:17.040
things that happened. You kind of have to update on this as well. And these are kind of things you

01:41:17.040 --> 01:41:22.960
probably didn't predict before. So yeah, sort of a slow, a slow rollout is definitely one component.

01:41:23.520 --> 01:41:27.120
Then the government also, I think, has to be involved in many, many ways, like,

01:41:27.120 --> 01:41:31.920
they, I think, effectively, at some point, they have to be able to say, you are not like you

01:41:31.920 --> 01:41:37.600
have consistently not met security standards, or safety standards, you are now punished in

01:41:37.600 --> 01:41:42.480
some way, you're not allowed to, like, release models of this or the size, or you are not allowed

01:41:42.480 --> 01:41:48.720
to do these other kind of things with the models. And if, you know, if they actually have been like

01:41:49.600 --> 01:41:54.560
disregarding all of the guidelines from the government before, or sufficiently many of them.

01:41:55.280 --> 01:41:59.120
Yeah, there are obviously like many, many additional things on top of this, right? There's

01:41:59.120 --> 01:42:05.280
international communication, there's like whistleblower protection. There are international

01:42:05.280 --> 01:42:10.320
institutions that will have to be involved. At some point, at least, I guess, there will be

01:42:10.320 --> 01:42:16.720
multiple institutions from the government side involved. I think there, for example, will be,

01:42:17.440 --> 01:42:23.120
or it would make sense to have like, a much like a bigger, broader institution that kind of like is

01:42:23.120 --> 01:42:27.200
a big tent where multiple coalitions come together. And then there's maybe more like a

01:42:27.200 --> 01:42:32.080
flexible specialized unit that only looks for the biggest, biggest kind of risks in the same way in

01:42:32.080 --> 01:42:36.960
which the US government has a unit that, you know, basically looks out for really big risks like

01:42:36.960 --> 01:42:42.320
pandemics and bio weapons and atomic weapons and so on. And they have a big mandate. And the only

01:42:42.320 --> 01:42:47.840
thing that, you know, their mandate is like, find information and like make big problems,

01:42:48.800 --> 01:42:54.800
like go away to some extent or like, try to solve them as quickly as possible. And you have like,

01:42:54.800 --> 01:42:58.320
a strong mandate and something like this would also make sense in the case of AI, I think.

01:42:58.880 --> 01:43:05.200
Maybe my last comment is something like, you know, open AI at some point, at least, had this

01:43:05.200 --> 01:43:11.360
approach of like, testing in the real world, or releasing a sort of the best safety strategy,

01:43:11.360 --> 01:43:15.840
because you get a lot of real world feedback and user feedback and so on. I think this was

01:43:15.840 --> 01:43:20.640
maybe true. And I'm not sure it was true for this period of time, but maybe it was true for the period

01:43:20.640 --> 01:43:27.600
of like 2020 to 2022 or 2023. Because the models were like, just good enough that user feedback

01:43:27.600 --> 01:43:32.080
was actually valuable, but nothing really bad could happen. But as soon as you have a system that

01:43:32.080 --> 01:43:37.200
is more powerful than that, right, you just outsource all the risk to like the rest of the world,

01:43:37.200 --> 01:43:41.200
people will immediately put it on the internet. If they get access to it, they will do lots of

01:43:41.200 --> 01:43:47.120
crazy stuff with more powerful systems. And yeah, I guess open AI, you know, there are a lot of

01:43:47.120 --> 01:43:51.520
smart people at open AI, but they still cannot model what like millions of people will be doing

01:43:51.520 --> 01:43:57.520
with these systems. So yeah, just like an uncontrolled rollout of very powerful systems,

01:43:57.520 --> 01:44:02.960
I think, yeah, is like kind of a recipe for a disaster. So my best guess is that

01:44:03.600 --> 01:44:07.840
the default will more and more, will become more and more conservative with more and more

01:44:07.920 --> 01:44:13.440
efforts going into testing, alignment, making sure that, you know, like testing for all of the

01:44:13.440 --> 01:44:19.280
different hypotheticals, interpretability efforts to understand some weird edge cases,

01:44:19.280 --> 01:44:25.680
monitoring, et cetera, et cetera. Does that imply that open source in your view just

01:44:27.680 --> 01:44:32.320
can't work? Like, I mean, is there any way to square, because I'm very sympathetic, you know,

01:44:32.320 --> 01:44:37.760
to considering a normal technology, and I would not, you know, I might turn out to be a normal

01:44:37.760 --> 01:44:42.320
technology, but as of now, it seems like a very live possibility that it is not a normal technology.

01:44:42.320 --> 01:44:46.800
But if we were to imagine, you know, whatever capabilities kind of stop where they are, and

01:44:46.800 --> 01:44:52.640
we're sort of, you know, left with GPT-4 forever, or something like that, hard to imagine, but let's

01:44:52.640 --> 01:44:57.920
just pause it. Then I'm like very sympathetic to the people that are like, hey, you know, this

01:44:57.920 --> 01:45:03.040
shouldn't be just the kind of thing that a few companies have access to, and it, you know,

01:45:03.040 --> 01:45:06.480
you should be able to make your own version, and, you know, what about the rest of the world, and,

01:45:06.480 --> 01:45:10.800
you know, there should be an Indian version for India, and like all these things. But it seems

01:45:10.800 --> 01:45:16.880
hard in a world where, you know, you imagine sufficiently powerful systems that are just put

01:45:16.880 --> 01:45:24.160
out totally, you know, bare into the world, like, is there any way to square the all those, I think,

01:45:24.160 --> 01:45:30.240
very legitimate open source motivations with the sort of safety paradigm that you're trying to develop?

01:45:31.120 --> 01:45:35.600
Potentially. So, you know, like, I think the open source debate is fairly heated. But, you know,

01:45:35.600 --> 01:45:40.560
to me, there are two things that are kind of obviously true. Number one, open source has been

01:45:40.560 --> 01:45:44.800
really good so far in many, many ways. It has been very positive for society, right? I think a lot

01:45:44.800 --> 01:45:48.800
of ML research could not have happened without open source. A lot of safety research could not

01:45:48.800 --> 01:45:53.920
have happened with open source. And the other thing that is also true, or at least seems true

01:45:54.000 --> 01:45:58.880
to me, is there's a limit of open source, right? Like, at some point, the system is so powerful

01:45:58.880 --> 01:46:02.720
that you don't want it to be open source anymore, in the same way in which, you know, I don't want to

01:46:02.720 --> 01:46:08.480
open source, like the nuclear codes, or something to start or like, you know, literally the recipe

01:46:08.480 --> 01:46:14.320
to build like the most, most viral, you know, most viral pandemic or something. This is just

01:46:14.320 --> 01:46:21.040
something where, you know, only one person needs to needs to have bad intentions to already have

01:46:21.040 --> 01:46:25.600
like really, to already cause really big problems. So at some point that there's, there's just a

01:46:25.600 --> 01:46:31.440
balance where you just, I think, cannot really justify giving people literally everyone access

01:46:31.440 --> 01:46:37.040
to this. And so the question for me really is where, so number one, where are we on the spectrum

01:46:37.040 --> 01:46:42.880
right now of like, open source has been really good to, are we already a point at like, how close

01:46:42.880 --> 01:46:46.960
are we to the point where it really cannot be justified anymore? Some people would say even

01:46:46.960 --> 01:46:52.320
GBD3, you know, through GBD3 size models are already too, too scary, which I'm not sure about,

01:46:52.320 --> 01:46:57.280
I'm not even sure whether GBD4 size models are like too big to be open source, but I would rather

01:46:57.280 --> 01:47:01.840
err on the side of caution and be a little bit more conservative here, because of the, the nature

01:47:01.840 --> 01:47:07.200
of open sourcing, where you can really not take this back. So as soon as you make a mistake, you

01:47:07.200 --> 01:47:11.280
like, you're stuck with a mistake for a long time, or like forever, you cannot, you cannot turn it,

01:47:11.280 --> 01:47:18.000
take it back. And I also think this will, this will also influence how, how open source will be

01:47:18.000 --> 01:47:24.080
handled in the real world in practice. Like, I think there will be something like initial releases

01:47:24.080 --> 01:47:28.240
for open source, where lots of people test it, like, basically think there will also be staggered

01:47:28.240 --> 01:47:34.000
and stage releases, right? It's, first, there's like a small team of trusted researchers who

01:47:34.000 --> 01:47:38.880
is allowed to play with the open source model, and like really test the limits, really test

01:47:38.880 --> 01:47:42.960
how bad could I get the model, how easy is it to remove all of the guardrails,

01:47:42.960 --> 01:47:47.200
which, you know, like, it's an open source model, if you can find, you can remove the guardrails.

01:47:47.200 --> 01:47:49.600
Yeah, it turns out pretty easy from what we've seen so far.

01:47:50.480 --> 01:47:54.480
Ones like, you know, the kind of the upper bounds are known of how bad could this become.

01:47:55.600 --> 01:48:00.080
Maybe it makes sense to, to like open source it to more people. But yeah, I would basically say,

01:48:00.080 --> 01:48:04.800
you know, the upper bound can be quite high, especially with all of the stuff I said earlier

01:48:04.800 --> 01:48:09.520
about, you know, absolute capabilities and reachable capabilities and so on, right? Like,

01:48:09.520 --> 01:48:15.520
maybe you can, maybe you cannot get it to build an automated hacking bot, if you only have access

01:48:15.520 --> 01:48:19.360
to the weights, but maybe if you do scaffolding on top and some fine tuning and access to some

01:48:19.360 --> 01:48:24.240
other thing, maybe then it can build it, right? And this is like very hard to predict in advance.

01:48:24.240 --> 01:48:30.000
So the more and more powerful the models become, I think the less plausible a priority is to open

01:48:30.000 --> 01:48:34.160
source them. Even though, and like, this is really something I want to emphasize, right? Like,

01:48:34.160 --> 01:48:40.320
open source has been extremely good so far. And I really think there's sort of this tipping point

01:48:40.320 --> 01:48:48.640
that is like, at some point, it just becomes too hard to like, it becomes impossible, it always

01:48:48.640 --> 01:48:53.120
will be impossible to take back, but at some point, it just becomes too dangerous to literally trust

01:48:53.120 --> 01:48:59.760
everyone with, with this level of capabilities. Threshold effects. That's, I think, one of the

01:48:59.760 --> 01:49:05.920
most powerful paradigms that I've, you know, consistently come back to over the last couple

01:49:05.920 --> 01:49:11.680
years, just crossing these thresholds from one regime into another, whether it's capabilities or,

01:49:11.680 --> 01:49:17.760
you know, risks, it just constantly seems like we're flipping from one mode or one kind of,

01:49:18.880 --> 01:49:25.040
you know, one regime to another and got to be very alert to when that happens because it can

01:49:25.040 --> 01:49:30.720
really change, you know, important analysis in pretty profound ways. So I don't know where exactly

01:49:30.720 --> 01:49:36.160
that threshold is either by any means, but it, and I would agree that like, for everything that I

01:49:36.160 --> 01:49:44.320
have seen suggests that up to and including the release of Lama 2 has been, you know, very, very

01:49:44.320 --> 01:49:50.000
much an enabler for all sorts of things. But certainly, you know, plenty of safety related work

01:49:50.000 --> 01:49:57.680
done on that model and, you know, seems, seems like the effect so far has been good. But yeah,

01:49:57.680 --> 01:50:01.840
is that still true for Lama 3? Is it true for Lama 4? You know, obviously we don't even know

01:50:01.840 --> 01:50:07.120
what these things are, but it certainly starts to be a very live question.

01:50:07.840 --> 01:50:13.760
You know, the, the like leading AGI labs, I think it is very clear that they understand the problem,

01:50:13.760 --> 01:50:20.880
that they have internal processes that are, you know, like, maybe better than you would expect

01:50:20.880 --> 01:50:28.240
from the normal, from a normal company. They actually care about trying to do good with

01:50:28.240 --> 01:50:32.720
with remodels and they're like very explicitly trying. And then, you know, there's the other

01:50:32.720 --> 01:50:37.200
side of the coin, which is they still have incentives, and they, you know, they can be

01:50:37.200 --> 01:50:42.480
financial incentives, they can be sort of maybe more psychological and social incentives, you

01:50:42.480 --> 01:50:47.280
know, that they just want to be the first to develop AGI, because it's like probably like a

01:50:47.280 --> 01:50:52.160
history defining technology, or maybe even galaxy defining technology or something like this, right?

01:50:52.160 --> 01:50:57.120
And, and so the question really, I think at this, you know, even if you could say, you know,

01:50:57.120 --> 01:51:04.240
like the compared to a normal company, these, the processes are astonishingly reasonable,

01:51:04.240 --> 01:51:07.760
and surprisingly good. If, you know, if you compare to literally any other

01:51:08.560 --> 01:51:14.480
industry, it would be surprising if they have this, this amount of like self regulation and so on.

01:51:14.480 --> 01:51:17.840
And then on the other hand, the question, they're, you know, they're still the bigger question of

01:51:18.560 --> 01:51:24.000
how hard is alignment going to be, how fast are going to take us going to be and so on and like,

01:51:24.000 --> 01:51:28.160
in a bad world, alignment is going to be quite hard and take us are going to be quite fast and

01:51:28.160 --> 01:51:35.040
controllable. And then the question is, you know, it is like the level of control and alignment,

01:51:35.760 --> 01:51:40.960
and like safety concern enough from these leaders. And there I'm like, less sure. So I feel like,

01:51:42.400 --> 01:51:47.360
yeah, it's, it's, it's definitely in like, it's, I think from my perspective, right? It's fair to say

01:51:48.320 --> 01:51:51.760
they're like pretty reasonable compared to the alternatives that we could have had.

01:51:52.800 --> 01:51:58.400
But also, it's insufficient in almost all ways, right? Government needs to be involved in this.

01:51:59.360 --> 01:52:04.480
They cannot externalize the risk. There are many things that they're already doing

01:52:04.480 --> 01:52:09.600
insufficiently well, I think, where they could have done way better, both with like how they release

01:52:09.600 --> 01:52:15.280
as well as how they react to, to like problems, as well as, you know, like how they communicate

01:52:15.280 --> 01:52:19.680
with the public about the risks that they're creating, etc., etc. So yeah, I think there,

01:52:19.680 --> 01:52:25.920
there's a lot of room for improvement as well. And yeah, I really, I really think that, you know,

01:52:25.920 --> 01:52:30.960
AI safety is going to be a very, very hard problem. A lot of things have to have to go right for the

01:52:30.960 --> 01:52:35.840
whole system to go right. And we definitely cannot just trust the labs, despite the best

01:52:35.840 --> 01:52:42.960
intentions to just solve it all on their own. Yeah, well, hence the, the need for third party

01:52:42.960 --> 01:52:47.920
auditing and the organization that you're building at Apollo Research, maybe just one last question.

01:52:47.920 --> 01:52:53.440
A number of people have reached out to me and said, I would like to get involved with red teaming.

01:52:54.320 --> 01:53:00.960
How can I do that? I wonder if you have any advice for individuals who might just want to

01:53:01.680 --> 01:53:07.360
do their own projects and, you know, release stuff, you know, just share findings individually with

01:53:07.360 --> 01:53:15.360
the world, or perhaps and or perhaps, you know, what sort of skills are you in need of, as you're

01:53:15.360 --> 01:53:20.640
going about building your own organization? I think one thing that is nice about model evaluations

01:53:20.720 --> 01:53:25.040
and red teaming is you can just kind of start right away. You don't need that much, you know,

01:53:25.040 --> 01:53:31.520
technical expertise, because it's all in, like almost all of it is in text. And, you know, at

01:53:31.520 --> 01:53:37.440
least if you, if you want to, to redeem a language model specifically. And yeah, so my recommendation

01:53:37.440 --> 01:53:44.240
for individuals, first of all, would be to just start, like just engage with a model for, you

01:53:44.240 --> 01:53:48.640
know, a long, a longer period of time, maybe a day or so and see if whether you find interesting

01:53:48.720 --> 01:53:52.560
behavior or maybe, you know, maybe there's someone who has already done something, and maybe you

01:53:52.560 --> 01:53:57.680
can poach a project from them, and, you know, just sort of as a starter thing. And from this,

01:53:57.680 --> 01:54:01.760
I feel like it kind of just takes a life of its own anyway, you know, as soon as you're hooked

01:54:01.760 --> 01:54:06.720
on a specific thing that you find interesting, you will, you know, you will really try to find

01:54:07.360 --> 01:54:12.000
additional ways in which this specific behavior could happen. In the same way, you know, let's,

01:54:12.000 --> 01:54:16.400
let's take the deception thing, right? We, we started fairly exploratory and wanted to try

01:54:16.400 --> 01:54:20.080
how far we can get the model to be, to be deceptive. And then at some point, it just

01:54:20.080 --> 01:54:24.720
took a life of its own where we're like, okay, but like, why really does it do that? Right? Like,

01:54:24.720 --> 01:54:29.280
okay, we vary this behavior and like this thing and this, this variable in the environment,

01:54:29.280 --> 01:54:34.000
we vary this thing, we vary all of these other things. And in the end, you can, again,

01:54:34.000 --> 01:54:37.440
you kind of get like a more holistic and round picture of what's going on. So yeah,

01:54:37.440 --> 01:54:42.080
I definitely think just start with like a thing you find interesting is definitely the way to go.

01:54:43.040 --> 01:54:49.120
And like don't overthink, overthink it originally. And then the thing we are specifically looking

01:54:49.120 --> 01:54:55.120
for, you know, so definitely kind of this mindset of like, oh, I just want to poke around and like

01:54:55.120 --> 01:54:59.520
really try to understand what's going on in a fairly like scientific manner, right? I also want

01:54:59.520 --> 01:55:03.680
to make sure that all of the confounders that could potentially explain this behavior have been

01:55:03.680 --> 01:55:08.480
controlled for, which I think is the hard part in red teaming. So this is definitely something

01:55:08.480 --> 01:55:13.280
we're looking for. And then just, you know, the more you understand language models and state of

01:55:13.280 --> 01:55:18.640
the art models, the easier it will become, right? Some behavior might be very easily explainable

01:55:18.640 --> 01:55:23.680
by sort of problems with RLHF. So if you know how RLHF works and specifically how it was trained,

01:55:24.720 --> 01:55:28.720
you may probably you will probably understand the red teaming efforts much better.

01:55:29.680 --> 01:55:33.920
If you know, you know, like if you have a better understanding of how the instruction

01:55:33.920 --> 01:55:38.640
fine tuning actually works, maybe you will find those. So, you know, so for example,

01:55:38.640 --> 01:55:43.280
maybe to give it to give like an intuition here, in the in our case, right, as I said earlier,

01:55:43.280 --> 01:55:48.800
we have the the three HS and then instruction fine tuning, and you can, you know, trade off the

01:55:48.800 --> 01:55:54.640
different components against each other to find different things. I like to find niches of the

01:55:54.640 --> 01:55:59.520
model where it acts in ways that we think it shouldn't act, because maybe they weren't covered

01:55:59.520 --> 01:56:05.600
explicitly or implicitly by by gradient descent. And if you if you sort of have a theoretical

01:56:05.600 --> 01:56:09.760
framework like this, it suddenly becomes much easier on how to do the red teaming in the first

01:56:09.760 --> 01:56:15.040
place. So like some theoretical understanding of how the process works is definitely helpful as

01:56:15.040 --> 01:56:21.120
well for a teaming and also something we're actively looking for. Marius Havan, founder and

01:56:21.120 --> 01:56:26.720
CEO of Apollo Research. Thank you for being part of the cognitive revolution. Thanks for inviting

01:56:27.440 --> 01:56:32.320
it is both energizing and enlightening to hear why people listen and learn what they value about

01:56:32.320 --> 01:56:39.760
the show. So please don't hesitate to reach out via email at TCR at turpentine.co, or you can DM me

01:56:39.760 --> 01:56:46.320
on the social media platform of your choice. Omniki uses generative AI to enable you to launch

01:56:46.320 --> 01:56:51.680
hundreds of thousands of ad iterations that actually work customized across all platforms

01:56:51.680 --> 01:56:56.400
with a click of a button. I believe in Omniki so much that I invested in it. And I recommend

01:56:56.400 --> 01:57:04.080
you use it too. Use CogGrav to get a 10% discount.

