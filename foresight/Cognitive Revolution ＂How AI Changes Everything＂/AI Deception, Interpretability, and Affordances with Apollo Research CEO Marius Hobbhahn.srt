1
00:00:00,000 --> 00:00:04,800
So the more pressure we add, the more likely the model is to be deceptive.

2
00:00:04,800 --> 00:00:07,920
So kind of in the same way in which a human would act, it also acts.

3
00:00:07,920 --> 00:00:12,480
You know, removing pressure and adding additional options will very quickly decrease

4
00:00:12,480 --> 00:00:14,400
the probability of being deceptive.

5
00:00:14,400 --> 00:00:17,200
Open source has been really good so far in many, many ways.

6
00:00:17,200 --> 00:00:19,200
It has been very positive for society, right?

7
00:00:19,200 --> 00:00:22,320
I think a lot of ML research could not have happened without open source.

8
00:00:22,320 --> 00:00:25,200
A lot of safety research could not have happened with open source.

9
00:00:25,200 --> 00:00:28,880
At some point, the system is so powerful that you don't want it to be open source anymore.

10
00:00:28,880 --> 00:00:33,280
In the same way in which, you know, I don't want to open source the nuclear codes or like,

11
00:00:33,280 --> 00:00:37,040
you know, literally the recipe to build most viral pandemic or something.

12
00:00:37,040 --> 00:00:42,640
The labs maybe have the incentive to not say the worst things they found because otherwise

13
00:00:42,640 --> 00:00:43,840
they may lose their contract.

14
00:00:43,840 --> 00:00:48,480
So you need something like the UKASF Institute or the USASF Institute.

15
00:00:48,480 --> 00:00:54,000
Make sure that there is a minimal set of standards that all the auditors have to adhere to.

16
00:00:54,000 --> 00:00:56,320
Hello and welcome to The Cognitive Revolution,

17
00:00:56,320 --> 00:01:00,720
where we interview visionary researchers, entrepreneurs, and builders working on the

18
00:01:00,720 --> 00:01:02,400
frontier of artificial intelligence.

19
00:01:03,200 --> 00:01:07,520
Each week, we'll explore their revolutionary ideas and together we'll build a picture of

20
00:01:07,520 --> 00:01:12,560
how AI technology will transform work, life, and society in the coming years.

21
00:01:13,120 --> 00:01:16,240
I'm Nathan LaBenz, joined by my co-host Eric Torenberg.

22
00:01:17,040 --> 00:01:19,680
Hello and welcome back to The Cognitive Revolution.

23
00:01:20,640 --> 00:01:25,280
Today, my guest is Marius Haban, founder and CEO of Apollo Research,

24
00:01:25,280 --> 00:01:30,880
a nonprofit AI safety research group that is working to understand both how AI systems behave

25
00:01:31,440 --> 00:01:32,160
and why.

26
00:01:33,760 --> 00:01:39,120
Their approach combines exploratory and hypothesis-driven testing, fine-tuning experiments,

27
00:01:39,120 --> 00:01:40,960
and interpretability research.

28
00:01:41,520 --> 00:01:47,040
And as you'll hear, they place special emphasis on the potential for AI systems to deceive

29
00:01:47,040 --> 00:01:48,000
their human users.

30
00:01:49,200 --> 00:01:53,760
In this conversation, we look first at Apollo's starting framework for their work,

31
00:01:53,840 --> 00:01:57,280
which emphasizes the importance of affordances in AI systems.

32
00:01:57,920 --> 00:02:03,760
That is, through what tools, actuators, or other means can the system affect the broader world?

33
00:02:04,880 --> 00:02:08,880
And they also introduce a number of new conceptual distinctions meant to help people have more

34
00:02:08,880 --> 00:02:12,880
precise and productive conversations about these nuanced topics.

35
00:02:14,240 --> 00:02:17,760
Then in the second half, we look at their first research result,

36
00:02:17,760 --> 00:02:21,200
which demonstrates, to my knowledge, for the first time in a realistic,

37
00:02:21,200 --> 00:02:28,400
unprompted setting that GPT-4, when put under pressure, will sometimes take unethical and even

38
00:02:28,400 --> 00:02:33,600
illegal actions and then go on to lie to its users about what it did and why.

39
00:02:34,960 --> 00:02:40,080
This is an important result, demonstrating that while the risk from AI systems may start with

40
00:02:40,080 --> 00:02:47,920
and may even be dominated by intentional human misuse, the models themselves can also misbehave

41
00:02:47,920 --> 00:02:49,200
in unexpected ways.

42
00:02:49,840 --> 00:02:54,560
As an aside, since I told my behind-the-scenes GPT-4 red team story a few weeks ago,

43
00:02:54,560 --> 00:02:59,120
a number of people have reached out to ask me how they too can get involved with red teaming projects.

44
00:03:00,160 --> 00:03:04,880
Unfortunately, as commercial competition and secrecy both continue to ramp up across the space,

45
00:03:05,440 --> 00:03:09,440
I don't see as many open calls for volunteer red teamers as I used to,

46
00:03:10,000 --> 00:03:12,560
certainly not for unreleased frontier models.

47
00:03:13,920 --> 00:03:17,920
Instead, the field is becoming more professionalized, with all the leading labs,

48
00:03:17,920 --> 00:03:23,360
as well as the data companies like Scale AI, plus the independent auditing organizations like Apollo,

49
00:03:23,360 --> 00:03:28,960
ArchieVals, now known as Meter, Palisade, and also AI Forensics, all actively hiring

50
00:03:28,960 --> 00:03:31,600
research scientists and engineers in this area.

51
00:03:32,880 --> 00:03:37,200
So does that mean that there's no longer a role for the independent hobbyist red teamer to play?

52
00:03:37,920 --> 00:03:42,640
On the contrary, there is a ton left to discover even on publicly released models,

53
00:03:42,640 --> 00:03:47,600
and the best way to break into the field is to demonstrate your ability to discover new phenomena.

54
00:03:48,720 --> 00:03:53,840
Importantly, the work we cover in this episode could have been done by anyone with an open AI

55
00:03:53,840 --> 00:03:59,920
account, a knack for prompting, and just a tiny bit of coding know-how. No special access or

56
00:03:59,920 --> 00:04:04,800
advanced machine learning techniques were required, just a lot of curiosity.

57
00:04:06,080 --> 00:04:10,000
With that in mind, if you want to get into this line of work but aren't sure where to start,

58
00:04:10,000 --> 00:04:15,520
I encourage you to reach out. I'll be happy to help brainstorm or refine your project ideas,

59
00:04:15,600 --> 00:04:20,480
and I can also help connect you with folks at the top companies who do sometimes provide API

60
00:04:20,480 --> 00:04:25,440
credits to independent researchers working in this area, if and when you can achieve a meaningful

61
00:04:25,440 --> 00:04:31,360
result. As always, we appreciate the time that you spend listening to The Cognitive Revolution,

62
00:04:31,360 --> 00:04:37,040
and we hope it's a valuable guide to the AI era. If you feel that it is, we would love a review

63
00:04:37,040 --> 00:04:42,240
on Apple Podcasts or Spotify, and we of course encourage you to share the show with your friends.

64
00:04:43,200 --> 00:04:49,680
Now, here's my conversation on frontier AI safety work with Marius Habhan of Apollo Research.

65
00:04:50,880 --> 00:04:55,920
Marius Habhan, founder and CEO of Apollo Research, welcome to The Cognitive Revolution.

66
00:04:56,720 --> 00:04:58,160
Hey, thanks for having me.

67
00:04:58,160 --> 00:05:03,600
I am very excited to have you. So, regular listeners of the show will know that I'm a big

68
00:05:03,600 --> 00:05:11,760
believer in the importance of hands-on testing of what AI systems can do, and also that I have

69
00:05:11,760 --> 00:05:18,000
been pretty enthusiastic consumer of the news when some of the leading labs have made public

70
00:05:18,000 --> 00:05:24,800
commitments to allow organizations outside of their own teams to look at the systems that

71
00:05:24,800 --> 00:05:30,880
they're building before they get deployed. And so, your work with Apollo Research, which is

72
00:05:31,920 --> 00:05:36,480
trying to build, as I understand it, an organization to meet that need and actually work with those

73
00:05:36,480 --> 00:05:42,240
leading labs in part, at least, on understanding the systems that they're developing before they

74
00:05:42,240 --> 00:05:48,480
get to widespread deployment, I think is super interesting, and I'm very excited to unpack the

75
00:05:48,480 --> 00:05:53,520
details of it with you. Maybe for starters, you want to just kind of give us the quick overview on

76
00:05:53,520 --> 00:05:58,480
Apollo Research, like how you decided to set out to found it. I'm interested a little bit in the

77
00:05:58,480 --> 00:06:03,920
timeline of how that related to some of the commitments that the labs have made and what

78
00:06:03,920 --> 00:06:08,640
you guys are trying to do in the big picture. So, I think on a high level, it's sort of trying to

79
00:06:08,640 --> 00:06:15,040
understand what is going on in AI systems. And the reason for this was, or still is, in fact,

80
00:06:15,680 --> 00:06:21,520
yeah, I basically think right now, we just lack information to make good decisions. There's loads

81
00:06:21,520 --> 00:06:27,760
of uncertainty that we have about what could go wrong, whether we are already at a point where

82
00:06:27,760 --> 00:06:35,760
things go wrong, or how far away we are from these points. And yeah, we're trying to reduce this

83
00:06:35,760 --> 00:06:42,160
uncertainty. And this is mostly through research, auditing, and governance. And on the research side,

84
00:06:42,160 --> 00:06:48,000
it's really split between interpretability and behavioral evils, half-half. But in the long

85
00:06:48,000 --> 00:06:54,480
run, we really want to merge them both, because I basically think what we need in the long run is

86
00:06:54,480 --> 00:06:59,520
both a mixture of behavioral and interpretability evils, so that we can really understand

87
00:06:59,520 --> 00:07:03,760
what the model is doing, and then also why it is doing this in the first place, because each of

88
00:07:03,760 --> 00:07:11,520
them individually seems somewhat insufficient. And yeah, maybe to go into the origin story,

89
00:07:11,520 --> 00:07:17,520
it has actually nothing to do with the commitments of the different labs. It was mostly that at the

90
00:07:17,520 --> 00:07:26,320
beginning of this year, I kind of felt like I had a pretty clear picture of what is lacking in the

91
00:07:26,320 --> 00:07:31,120
current space with deceptive alignment, and evaluating deceptive alignment, or models for

92
00:07:31,120 --> 00:07:36,320
deceptive alignment in the first place. And interpretability and evils just seemed like

93
00:07:36,320 --> 00:07:41,040
the obvious things to do. So in the beginning, we basically set out to do mostly research.

94
00:07:41,040 --> 00:07:46,720
And only then, over time, we realized, hey, this is something that should be applied in the real

95
00:07:46,720 --> 00:07:54,400
world as soon as possible, because systems are getting better all the time. And we may actually

96
00:07:54,400 --> 00:07:59,920
hit this point fairly soon where models are already about at the threshold of deceptive,

97
00:07:59,920 --> 00:08:05,760
of capabilities for deceptive alignment. And then there is a small part in the organization that

98
00:08:05,760 --> 00:08:10,800
is governance, which originally we also didn't really intend to do for the first two years or

99
00:08:10,800 --> 00:08:15,920
something, because we thought we really need to understand all the research very well before

100
00:08:15,920 --> 00:08:21,280
we can talk to the people in governments and decision makers and lawmakers, because otherwise

101
00:08:21,280 --> 00:08:26,640
we're telling them things we aren't super confident in. And then lots of things happen.

102
00:08:27,280 --> 00:08:34,320
Governments and lawmakers actually got interested in AI and AI safety in particular. And then when

103
00:08:34,320 --> 00:08:40,240
we talked to them, we realized the difference. We are very, very well placed to talk about these

104
00:08:40,240 --> 00:08:45,840
things, because if you have thought about them in the background for like six, seven,

105
00:08:46,000 --> 00:08:51,600
years, and then specifically about some topic for six months or so, you are among the world's

106
00:08:51,600 --> 00:08:57,360
experts. And this is kind of more like a reflection of the state of how bad it is about AI safety,

107
00:08:57,360 --> 00:09:03,680
where people in my position are actually sort of accidentally becoming the experts,

108
00:09:04,480 --> 00:09:10,640
rather than people with tens and 20 years of experience, because there aren't a lot of people

109
00:09:10,640 --> 00:09:14,800
in the world who have thought about AI safety for more than a couple of years, if at all.

110
00:09:14,800 --> 00:09:21,760
Yeah, I can definitely relate to that sort of accidental expert status. I never expected to be

111
00:09:21,760 --> 00:09:30,160
where I am and doing the things that I'm doing. But yeah, the whole AI field in some ways is kind

112
00:09:30,160 --> 00:09:35,360
of the dog that caught the car. I always kind of come back to that metaphor where it's like

113
00:09:36,320 --> 00:09:40,880
we were just trying to build a bit more powerful AI, and all of a sudden we built like a lot more

114
00:09:40,960 --> 00:09:46,240
powerful AI, and now we really kind of have to figure out what to do with it. So even a little

115
00:09:46,240 --> 00:09:51,040
bit of advanced planning is better than, or a little bit of advanced thought is a lot better than

116
00:09:51,040 --> 00:09:56,640
where most people are starting. Had you seen, when you actually started the organization,

117
00:09:56,640 --> 00:10:03,680
had you seen GPT-4, or were you basing this decision on just what was public at the time?

118
00:10:04,560 --> 00:10:13,040
Only what was public at the time. So the decision was made in February 2023, or at

119
00:10:13,040 --> 00:10:18,400
least sort of my internal commitment was made to this. I'm not sure whether GPT-4 was public

120
00:10:18,400 --> 00:10:23,920
already at the time. Not quite, right? It was March. So no, it was independent of GPT-4.

121
00:10:24,800 --> 00:10:30,720
Yeah, I always think that's interesting just because GPT-4 was such a wake-up moment for

122
00:10:30,720 --> 00:10:36,000
so many people, and certainly I would include myself in that. I was already extremely plugged

123
00:10:36,000 --> 00:10:42,560
into what was going on and using it and fine-tuning tons of models on the open AI platform in particular,

124
00:10:42,560 --> 00:10:48,320
but then it was like, whoa, this thing is next level. It's not slowing down. We've gone from

125
00:10:48,320 --> 00:10:54,400
sort of, I can put a lot of elbow grease in and get a fine-tuned model to do a particular task,

126
00:10:54,400 --> 00:10:58,960
which already I thought was going to be economically transformative to,

127
00:11:00,000 --> 00:11:03,520
I don't even need to do that, that I could just ask for a lot of these tasks and get

128
00:11:03,520 --> 00:11:07,600
like pretty good zero-shot performance. For me, that was the moment where I was kind of like,

129
00:11:07,600 --> 00:11:12,880
okay, this is going from a tool that I am really excited to use and having a lot of fun using

130
00:11:12,880 --> 00:11:19,360
to something that seems like a force that needs to be understood from all angles.

131
00:11:20,000 --> 00:11:25,600
So let's unpack the perspective that you are bringing to this. I would encourage folks to

132
00:11:25,600 --> 00:11:31,040
look up these papers that we'll discuss and read them for themselves as well, but on the website,

133
00:11:31,040 --> 00:11:38,240
you've got two recent publications. One is kind of a framework for organizing the work that you're

134
00:11:38,240 --> 00:11:43,680
going to do, and then the other is like a very detailed in the weeds investigation of a particular

135
00:11:43,760 --> 00:11:49,360
AI behavior, namely deceiving the user, which I think is a super interesting and important

136
00:11:49,360 --> 00:11:54,000
one to study. But let's maybe just start with the big picture, like organizing the thoughts.

137
00:11:55,840 --> 00:12:00,800
I get the sense that you think, again, well, you've kind of said this, and the paper certainly

138
00:12:00,800 --> 00:12:04,960
reflects it, that there are like a lot of big questions that remain unanswered. So how do you

139
00:12:04,960 --> 00:12:11,040
structure your approach to this topic given all the uncertainty that exists?

140
00:12:11,760 --> 00:12:18,480
Yeah, maybe to give a little bit of context. So this is only one paper of many in this space,

141
00:12:18,480 --> 00:12:23,280
and there is, I think earlier this year, there was a really big one called Model Evaluation

142
00:12:23,280 --> 00:12:29,520
for Extreme Risk, which we at Apollo definitely thought was a pretty good paper. And they're

143
00:12:29,520 --> 00:12:35,120
sort of pointing out many of the very reasonable and important steps, or reasonable principles

144
00:12:35,120 --> 00:12:40,320
for external auditing, something like ramp up the auditing before you ramp up the exposure

145
00:12:40,320 --> 00:12:47,200
to the real world and do this ahead of the curve, so to speak. But when we read the paper,

146
00:12:47,200 --> 00:12:52,560
we felt a little bit like, this makes sense for the current capabilities and sort of how

147
00:12:52,560 --> 00:12:57,360
current models are being built. But if we think ahead of what the next couple of years should look

148
00:12:57,360 --> 00:13:03,120
or not should, could look like, then yeah, there are loads of open questions. And we were trying

149
00:13:03,120 --> 00:13:08,160
to understand how do they fit into this framework? And because we internally were trying to make

150
00:13:08,160 --> 00:13:13,040
sense of this in the first place. So just to give you a couple of them, what happens if your

151
00:13:13,040 --> 00:13:18,080
model has the ability to do online learning? How often do you have to audit it? Should you re-audit

152
00:13:18,080 --> 00:13:23,680
it during the online learning? If yes, how often? What if you give the model access to the internet

153
00:13:23,680 --> 00:13:29,520
or to a database or to anything like this? Yeah, I think a model with and without access to the

154
00:13:29,520 --> 00:13:35,120
internet is basically two very different models. The one with access to the internet is just so

155
00:13:35,120 --> 00:13:41,600
much more powerful if you can use it even on a very basic level. So yeah, it feels like if you

156
00:13:41,600 --> 00:13:45,840
give your model affordances like this, you kind of have to rethink how dangerous it is and where

157
00:13:45,840 --> 00:13:49,360
the danger comes from because it suddenly is like a totally different threat model potentially.

158
00:13:49,920 --> 00:13:56,400
And so what we did for the paper and really the credit should go to Lee Sharkey here, who is my

159
00:13:56,400 --> 00:14:00,480
co-founder, who has done most of the hard work or if not all of the hard work for this paper.

160
00:14:01,440 --> 00:14:06,560
And so what we were doing is thinking from first principles. Where does the risk come from and what

161
00:14:06,560 --> 00:14:13,440
changes to the AI system do create new risks? And then basically the answer is, well, we have to

162
00:14:13,440 --> 00:14:18,400
audit wherever risk is created. And then the more we looked into this, the more realized, well,

163
00:14:18,400 --> 00:14:22,800
there are actually a lot of places where new risk comes into the system, at least potentially,

164
00:14:22,800 --> 00:14:27,600
and therefore we are audits, at least in an ideal world should happen. There are obviously some

165
00:14:27,680 --> 00:14:32,800
constraints. But I think if we think about where are we five years from now, then I think, yeah,

166
00:14:32,800 --> 00:14:37,600
if there is actually a big auditing ecosystem around this, then there will be very, very many

167
00:14:37,600 --> 00:14:43,520
different organizations auditing really different places. And then the other point of the paper

168
00:14:43,520 --> 00:14:51,360
was just to define many concepts and create the language to discuss all of these things because

169
00:14:51,920 --> 00:14:56,480
we had sort of many internal discussions where we were like, oh, the thing we mean is this.

170
00:14:56,480 --> 00:15:00,480
And then we had an example, and then we kind of needed a name for it. And there wasn't really

171
00:15:00,480 --> 00:15:06,560
a name. So we decided, okay, let's define all of the relevant terms for this, and then sort of

172
00:15:06,560 --> 00:15:11,040
have a language to talk about this in the first place. Hey, we'll continue our interview in a

173
00:15:11,040 --> 00:15:16,320
moment after a word from our sponsors. Real quick, what's the easiest choice you can make? Taking

174
00:15:16,320 --> 00:15:21,200
the window instead of the middle seat, outsourcing business tasks that you absolutely hate? What

175
00:15:21,200 --> 00:15:27,760
about selling with Shopify? Shopify is the global commerce platform that helps you sell at every

176
00:15:27,760 --> 00:15:33,680
stage of your business. Shopify powers 10% of all e-commerce in the US, and Shopify is the global

177
00:15:33,680 --> 00:15:39,360
force behind Allbirds, Rothy's and Brooklyn and millions of other entrepreneurs of every size

178
00:15:39,360 --> 00:15:44,800
across 175 countries. Whether you're selling security systems or marketing memory modules,

179
00:15:44,800 --> 00:15:49,440
Shopify helps you sell everywhere, from their all-in-one e-commerce platform to their in-person

180
00:15:49,440 --> 00:15:54,640
POS system. Wherever and whatever you're selling, Shopify's got you covered. I've used it in the

181
00:15:54,640 --> 00:15:59,200
past at the companies I founded, and when we launched Merch here at Turpentine, Shopify will

182
00:15:59,200 --> 00:16:04,080
be our go-to. Shopify helps turn browsers into buyers with the internet's best converting

183
00:16:04,080 --> 00:16:09,520
checkout up to 36% better compared to other leading commerce platforms. And Shopify helps you sell

184
00:16:09,520 --> 00:16:15,040
more with less effort thanks to Shopify magic, your AI-powered All-Star. With Shopify magic,

185
00:16:15,120 --> 00:16:19,600
whip up captivating content that converts from blog posts to product descriptions.

186
00:16:19,600 --> 00:16:25,920
Generate instant FAQ answers. Pick the perfect email send time. Plus, Shopify magic is free for

187
00:16:25,920 --> 00:16:32,080
every Shopify seller. Businesses that grow grow with Shopify. Sign up for a $1 per month trial

188
00:16:32,080 --> 00:16:38,000
period at Shopify.com slash cognitive. Go to Shopify.com slash cognitive now to grow your

189
00:16:38,000 --> 00:16:42,000
business no matter what stage you're in. Shopify.com slash cognitive.

190
00:16:45,040 --> 00:16:52,720
So let's dig in in a little bit deeper detail. I like the premise that you set out within the

191
00:16:52,720 --> 00:17:00,080
paper, which is to work backward from AI effect in the real world and try to imagine where are

192
00:17:00,080 --> 00:17:03,920
these effects going to happen and then how can we get upstream of that and help shape them in a

193
00:17:03,920 --> 00:17:10,160
positive way. I would be interested to hear you kind of describe that backward chaining process in

194
00:17:10,160 --> 00:17:14,320
a little bit more detail. And then I thought some of your concepts also were really helpful

195
00:17:15,200 --> 00:17:20,320
clarifications and distinctions. So maybe you can highlight some of the ones that you think are most

196
00:17:21,280 --> 00:17:23,920
useful that you'd like to see get into broader circulation as well.

197
00:17:24,480 --> 00:17:29,200
So basically, we started from, okay, the system, the AI system will interact with the world in a

198
00:17:29,200 --> 00:17:33,440
particular way. And then, you know, there are many, many different ways in which it can interact with

199
00:17:33,440 --> 00:17:39,280
the world. And then there's sort of like a whole chain of things that have had to happen until the

200
00:17:39,280 --> 00:17:44,080
model can act interact with the world in this particular way. So, you know, maybe it has been

201
00:17:44,080 --> 00:17:48,400
fined you and maybe it has been given access to the internet before that it has to have been trained

202
00:17:48,400 --> 00:17:52,560
before that there has to have been the decision that this model should be trained in the first

203
00:17:52,560 --> 00:17:58,800
place. And so the question is like, what are the kind of important decisions at all of these

204
00:17:58,800 --> 00:18:04,800
different points in time? And how then can we ensure that people actually make decisions that

205
00:18:04,800 --> 00:18:11,360
will lead to outcomes at the end of the chain, such as the model or the system interacts with the

206
00:18:11,360 --> 00:18:16,720
world in a safe manner. And this is maybe like the first distinction that is worth pointing out,

207
00:18:16,720 --> 00:18:21,040
or like the reason why I'm correcting myself all the time is there's really a difference between

208
00:18:21,040 --> 00:18:25,440
AI model and AI system. The AI model really, and this is not something we came up with,

209
00:18:25,440 --> 00:18:29,920
this already exists before. But I think it's worth pointing out and sort of getting in like

210
00:18:29,920 --> 00:18:35,360
really hammering into people's head when they think about AI. So the AI model is just the weights

211
00:18:35,360 --> 00:18:40,400
maybe behind, you know, like behind an API, but even with the API, it's kind of already a system.

212
00:18:41,440 --> 00:18:46,160
And the system then is sort of the weights plus everything around it. So there could be scaffolding,

213
00:18:46,160 --> 00:18:50,240
there could be access to tools, there could be content filters, this could even be like

214
00:18:50,240 --> 00:18:55,680
just an API, retrieval databases, etc. Like really the full package, where you say, okay,

215
00:18:55,680 --> 00:19:01,840
you know, there's there's like stuff around the mod around the weights that increase the

216
00:19:01,840 --> 00:19:07,440
capabilities of the model and menu, or at least change the capabilities of the raw model in some

217
00:19:07,440 --> 00:19:13,200
sense, not necessarily always increasing filters, for example, may decrease it. And then there are

218
00:19:13,200 --> 00:19:17,680
sort of other weird ways, or like, yeah, once you think about this, there are sort of a couple

219
00:19:17,680 --> 00:19:23,040
of other concepts that that feel important to clarify. Because when people say capabilities,

220
00:19:23,040 --> 00:19:28,720
this can mean very different things, right? And so we categorize this into three different classes.

221
00:19:29,360 --> 00:19:35,040
The first one is absolute capabilities, which we think of basically the hypothetical capabilities

222
00:19:35,040 --> 00:19:42,480
given any set of affordances. So if you have GPD for without the internet, right, then in the space

223
00:19:42,480 --> 00:19:48,160
of absolute capabilities would be a GPD for with internet. So or like things that this model could

224
00:19:48,160 --> 00:19:53,360
do. So the question is like, if we give additional things to the system, how big is the space of

225
00:19:53,360 --> 00:19:59,440
actions it could take. So, you know, and then obviously, there's a question of like, how imaginary

226
00:19:59,440 --> 00:20:04,720
do we get here, you know, like, does it has does it get access to like, you know, a Dyson sphere,

227
00:20:04,720 --> 00:20:10,880
or does it get access to like, a government or something like this. But but yeah, like it basically

228
00:20:10,880 --> 00:20:16,560
points out sort of the, what could this model do if we gave it a lot of things, everything that

229
00:20:16,560 --> 00:20:21,280
we can basically think of. Thinking about this in the first place only makes sense for models that

230
00:20:21,280 --> 00:20:27,360
have become more general, like the GPT is, because you know, for an MNIST filter, like for an MNIST

231
00:20:27,360 --> 00:20:33,360
classifier, this doesn't make any any sense, like an MNIST classifier plus internet is like is exactly

232
00:20:33,360 --> 00:20:38,400
as capable as just the MNIST classifier itself. But yeah, for systems that are more general,

233
00:20:38,400 --> 00:20:44,560
suddenly you have this difference between things that only the system can do, or like the basic

234
00:20:44,560 --> 00:20:51,280
system plus things that you could do hypothetically with a lot of additional affordances. Then the

235
00:20:51,280 --> 00:20:58,480
second one is contextual capabilities, which is things that are achievable in the context right

236
00:20:58,480 --> 00:21:06,080
now. So for example, with chat GPT, you can enable it to have access to tools, and then you can browse

237
00:21:06,080 --> 00:21:10,480
the web. And this is something that it can do right now, you don't have to add anything on top

238
00:21:10,480 --> 00:21:15,520
of this. And this is sort of this is sort of the smallest category of things, which you can do without

239
00:21:15,520 --> 00:21:22,080
any additional modification and then reachable capabilities is contextual capabilities, plus

240
00:21:22,080 --> 00:21:29,200
achievable through extra effort. So for example, this could mean chat GPT itself may not have access

241
00:21:29,200 --> 00:21:36,800
to a calculator. But if it has access to the internet, it can like Google and then find a

242
00:21:36,800 --> 00:21:42,480
calculator and then use that calculator. And so it's sort of a two step process, right, where it

243
00:21:42,480 --> 00:21:47,760
has to use one affordance or capability to then achieve another. And so this is what we call

244
00:21:47,760 --> 00:21:56,400
reachable capabilities. And yeah, so the reason the reason why we are making this all of this

245
00:21:56,400 --> 00:22:01,840
differentiation, even though it sounds maybe a little bit too much in the weeds is when people

246
00:22:01,840 --> 00:22:07,680
talk about capabilities and regulating capabilities and designing laws for capabilities, the question

247
00:22:07,680 --> 00:22:13,520
is, which ones, right? Do you mean the contextual capabilities? So the ones that the model has

248
00:22:13,520 --> 00:22:18,160
literally right now, or the reachable capabilities, so which the model could reach with additional

249
00:22:18,160 --> 00:22:25,040
effort or the absolute like, the maximum potential space of capabilities. And, you know, right now

250
00:22:25,040 --> 00:22:30,880
this may sound like we're too much in the weeds. And but I think in a few months, this will sound

251
00:22:30,880 --> 00:22:36,640
very, very relevant suddenly, because the models will be more capable. And then they will actually

252
00:22:36,640 --> 00:22:43,760
be able to just like smart enough to use the internet to to like find additional tools that

253
00:22:43,760 --> 00:22:50,160
they can then use, or or like convince someone to give them access to a shell. And then use that

254
00:22:50,160 --> 00:22:53,680
because they're already like, you know, they can learn it in context or they know it anyways.

255
00:22:55,040 --> 00:22:59,360
And at that point, really, the question is, what should the auditors audit for? Which capabilities?

256
00:22:59,920 --> 00:23:05,440
And and that becomes like pretty quickly, like a very, very big space of things, right? So like,

257
00:23:05,440 --> 00:23:11,360
if the auditor not only has to think about what kind of tools do you give the AI, but also what

258
00:23:11,360 --> 00:23:17,440
kind of tools could the AI get access to through some means? Suddenly, you have this whole space of

259
00:23:17,440 --> 00:23:22,480
like thousands of things it could do. It's really a question of like, or like a tradeoff between

260
00:23:22,480 --> 00:23:27,440
what is what is plausibly doable in the real world versus how much risk can we actually mitigate?

261
00:23:28,240 --> 00:23:33,040
And I'm honestly very unsure about about the like where we're heading at this point.

262
00:23:34,160 --> 00:23:40,560
So just to riff on and kind of emphasize some of the the value that I see in in some of these

263
00:23:40,560 --> 00:23:45,920
distinctions, I think it's helpful to clarify the difference between a model and a system.

264
00:23:46,720 --> 00:23:52,000
I think there is a tremendous amount of confusion online. And to my degree, and I've probably even

265
00:23:52,000 --> 00:23:58,080
contributed to some of it at times where people are like, you know, Chad GPT was doing this for me,

266
00:23:58,080 --> 00:24:02,400
and now it's not anymore. And I've sometimes said like, well, they haven't updated the model,

267
00:24:02,400 --> 00:24:06,160
so it probably hasn't changed that much. And I think what I've maybe neglected in some of those

268
00:24:06,160 --> 00:24:11,200
moments is like, but they might have changed the system prompt, or, you know, as we're seeing,

269
00:24:11,200 --> 00:24:16,480
I mean, even just this last couple of weeks, there's been this really interesting phenomenon of the

270
00:24:17,360 --> 00:24:23,680
of GPT for getting quote unquote, lazier. And people are speculating that maybe that's because

271
00:24:23,680 --> 00:24:27,280
they feed the date into it. And it knows that we're in December, and it knows that people

272
00:24:27,280 --> 00:24:31,760
don't work as hard or as productively in December. And so maybe it's like kind of phoning it in,

273
00:24:31,760 --> 00:24:37,200
because it's like, imitating the broad swath of humans that it's seen like, you know, kind of

274
00:24:37,200 --> 00:24:41,920
work halftime in December or whatever. I've even seen some experiments, just in the last couple

275
00:24:41,920 --> 00:24:45,840
days that suggest that there might even be real truth to that. Who knows, I'd say that the question

276
00:24:45,840 --> 00:24:50,640
remains open. But there's a there is an important difference, you know, and it's worth getting

277
00:24:50,640 --> 00:24:56,880
clarity on the model itself with static weights, not changing versus even just a system prompt

278
00:24:56,880 --> 00:25:02,640
that can perhaps have, you know, even unexpected drift along the dimension of something as

279
00:25:02,640 --> 00:25:09,840
seemingly benign as today's date. So that's important to keep in mind. The levels of capabilities,

280
00:25:09,840 --> 00:25:13,280
I think, are also really interesting. And I want to ask one kind of I have a couple questions on

281
00:25:13,280 --> 00:25:19,200
this, but I think I have a clear sense of what is meant by contextual. What can it do now, given

282
00:25:19,200 --> 00:25:24,960
the packaging, right? What what can GPT forward do in the context of chat GPT, where it has

283
00:25:24,960 --> 00:25:30,480
a code interpreter, and it has browse with Bing, and it has the ability to call Dolly three to

284
00:25:30,480 --> 00:25:34,640
make an image, and probably a couple other things that I'm not even remembering, you know, plugins

285
00:25:34,640 --> 00:25:38,880
perhaps as well, right, which obviously GPT is which proliferates, you know, all the affordances,

286
00:25:38,880 --> 00:25:44,400
all that much more. On the other end, I feel like I sort of understand absolute, which is like a

287
00:25:44,400 --> 00:25:50,080
theoretical max. Could you give me a little like how do I understand reachable as as kind of between

288
00:25:50,080 --> 00:25:58,320
those like what's what's the distinction between reachable and absolute? Yeah, so so maybe maybe

289
00:25:58,320 --> 00:26:03,840
one way to think of it is like, the contextual capabilities are the ones kind of that a user

290
00:26:03,840 --> 00:26:10,480
explicitly gave it. And then the reachable ones are those that may also be reachable without the

291
00:26:10,480 --> 00:26:15,920
user even having thought about that the model actually will will use them, right? So if you say,

292
00:26:16,560 --> 00:26:22,560
you know, like if the model would be able to browse the web, like entirely on its own, which I'm

293
00:26:22,560 --> 00:26:28,000
not sure it currently can do or like what exactly the restrictions on search with Bing are. But if

294
00:26:28,000 --> 00:26:33,840
it was able to do that, right, you may not you may not have realized that it has a reachable

295
00:26:33,840 --> 00:26:41,440
reachable capability through the internet of like firing up a shell somewhere, or like renting a GPU,

296
00:26:42,080 --> 00:26:47,200
and and like doings or like running a physics simulation through a like an online

297
00:26:48,480 --> 00:26:53,840
physics simulator, if that if that's something that's available. And so so these are this is sort of

298
00:26:53,840 --> 00:27:01,920
like how which tools can it reach through the contextual ability capabilities that it already

299
00:27:01,920 --> 00:27:10,160
has given by you or was been has been given by you. Gotcha. Okay. So like solving a capture by

300
00:27:10,160 --> 00:27:18,080
hiring an upward contractor for exactly to take one infamous case. So, okay, here's a challenging

301
00:27:18,160 --> 00:27:24,880
question. But, and I don't necessarily expect an answer, but maybe you can venture an answer or

302
00:27:24,880 --> 00:27:30,640
you could just kind of describe how you begin to think about it. What would you say are the absolute

303
00:27:30,640 --> 00:27:39,680
capabilities of GPT four? Yeah, very unclear. So I think they're definitely they're not infinite.

304
00:27:40,400 --> 00:27:45,440
As in, you know, like even with extremely good scaffolding and and access to the internet and

305
00:27:45,440 --> 00:27:52,480
many other things, I think people haven't been able to, you know, get it to do economically

306
00:27:52,480 --> 00:27:58,880
valuable tasks at the level of a human, at least for like long time spans, for example. So, you

307
00:27:58,880 --> 00:28:03,200
know, the question is obviously like, is this, you know, are we just too bad? And have we not

308
00:28:03,200 --> 00:28:07,440
figured out the right prompting yet and the right scaffolding and so on? Or, or is this just a

309
00:28:07,440 --> 00:28:12,400
limitation of the system? And my current guess is, like, there is probably a limit to the absolute

310
00:28:12,400 --> 00:28:17,360
capabilities. And it's probably lower than like what a human can do. But we're not that far away

311
00:28:17,360 --> 00:28:23,440
from it. So, you know, I think with an additional training with additional, like specifically

312
00:28:23,440 --> 00:28:28,320
LM, like training that is more goal direct or makes it into more goal directed and an agent

313
00:28:29,280 --> 00:28:35,200
and better scaffolding, I think there will be ways in which the absolute capabilities could increase

314
00:28:36,080 --> 00:28:39,680
quite a bit in the near future. Yeah, does this make sense?

315
00:28:40,400 --> 00:28:46,480
Yeah, I mean, it's hard, right? I certainly listeners to the show will know from repeated

316
00:28:46,480 --> 00:28:54,160
storytelling on my part that I was one of the volunteer testers of the GPT-4 early model back

317
00:28:54,160 --> 00:28:59,520
in August, September of last year. And I really kind of challenged myself to try to answer that

318
00:28:59,520 --> 00:29:06,480
question, you know, independently, like, what is the theoretical max of what this thing can do?

319
00:29:07,360 --> 00:29:12,400
How much could it like break down big problems and delegate to itself? And it basically came to

320
00:29:12,400 --> 00:29:21,520
the same conclusion that you did, which is like, doesn't seem like it can do really big tasks.

321
00:29:21,520 --> 00:29:24,560
I mean, again, it's confusing, right? Because then you could also look at the dimension of

322
00:29:25,200 --> 00:29:29,200
how big the task is versus how would you break it down? Just in the last week, I've been doing

323
00:29:29,200 --> 00:29:35,920
something for a very sort of mundane project, but actually using GPT-4 to run evals on other

324
00:29:35,920 --> 00:29:43,280
language model output, I have found that if I have like 10 tasks, 10, you know,

325
00:29:43,280 --> 00:29:49,520
dimensions of evaluation, and I ask it to run all of those, it is now capable of following

326
00:29:49,520 --> 00:29:56,160
those directions and executing the tasks one by one. But the quality kind of suffers. It sort of

327
00:29:56,160 --> 00:30:00,560
makes mistakes. It sometimes muddies the tasks a little bit between each other. And it's definitely

328
00:30:00,560 --> 00:30:07,520
like not at a human level given 10 tasks to do in one generation. On the flip side, though,

329
00:30:07,520 --> 00:30:14,240
if I take it down to one task per generation, which I didn't want to do because that will increase

330
00:30:14,240 --> 00:30:19,920
our cost and latency and just is less convenient for me, but then it kind of pops up to, honestly,

331
00:30:19,920 --> 00:30:25,920
I would say pretty much human level, if not above. So there's interesting dimension. I guess

332
00:30:25,920 --> 00:30:30,640
it seems pretty the sort of magnitude of the task seems like a pretty important dimension for

333
00:30:31,520 --> 00:30:35,040
evaluating a question like absolute capabilities, right? It's like,

334
00:30:35,040 --> 00:30:40,320
if it's a super narrow thing, it has it's like more it's, it's capable of some pretty high spikes.

335
00:30:40,320 --> 00:30:46,720
But if it's a, if it's a big thing, it kind of gets lost. Would you refine that characterization

336
00:30:46,720 --> 00:30:51,280
at all? Yeah, yeah, I'm not sure how to think about it, honestly. So I think of absolute capabilities

337
00:30:51,280 --> 00:30:57,520
really more of a sort of theoretical bound that we could, that we're probably not going to approximate

338
00:30:57,520 --> 00:31:03,440
in practice, even if we test like a lot, a lot. And then the, then like breaking it down into

339
00:31:03,440 --> 00:31:07,360
different tasks, I'm not sure I feel like this is a different capability then, right? Like you're

340
00:31:07,920 --> 00:31:12,560
sort of the capability of doing 10 things at once is a different thing than the capability of

341
00:31:12,560 --> 00:31:20,480
doing one thing 10 times like 10, 10 diff different things, but one by one. So yeah,

342
00:31:20,480 --> 00:31:26,160
I would say it's basically you're talking about different capabilities then, at least in this

343
00:31:26,160 --> 00:31:31,280
framework. Hey, we'll continue our interview in a moment after a word from our sponsors.

344
00:31:31,280 --> 00:31:35,360
If you're a startup founder or executive running a growing business, you know that as you scale,

345
00:31:35,360 --> 00:31:39,680
your systems break down, and the cracks start to show. If this resonates with you,

346
00:31:39,680 --> 00:31:46,560
there are three numbers you need to know, 36,000, 25 and one, 36,000. That's the number of businesses

347
00:31:46,560 --> 00:31:50,560
which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system,

348
00:31:50,560 --> 00:31:56,800
streamline accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25

349
00:31:56,800 --> 00:32:01,680
this year. That's 25 years of helping businesses do more with less, close their books in days, not

350
00:32:01,680 --> 00:32:07,120
weeks, and drive down costs. One, because your business is one of a kind, so you get a customized

351
00:32:07,120 --> 00:32:11,920
solution for all your KPIs in one efficient system with one source of truth. Manage risk,

352
00:32:11,920 --> 00:32:15,920
get reliable forecasts, and improve margins. Everything you need, all in one place.

353
00:32:16,480 --> 00:32:21,200
Right now, download NetSuite's popular KPI checklist, designed to give you consistently

354
00:32:21,200 --> 00:32:26,640
excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com

355
00:32:26,640 --> 00:32:31,200
slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.

356
00:32:32,480 --> 00:32:38,000
Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that

357
00:32:38,000 --> 00:32:43,680
actually work, customized across all platforms with a click of a button. I believe in Omnike so

358
00:32:43,680 --> 00:32:50,080
much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount.

359
00:32:50,080 --> 00:32:56,320
Yeah, and it is not that good at decomposing the tasks. I've also kind of experimented a little

360
00:32:56,320 --> 00:33:01,600
bit with like, can you give it that list of 10 tasks and can it break them down and

361
00:33:02,400 --> 00:33:07,600
self-delegate with an effective prompt? It's like maybe a little bit closer there,

362
00:33:07,600 --> 00:33:15,280
but still not getting nearly as good results as if I just roll it my sleeves and do the task

363
00:33:15,280 --> 00:33:21,360
decomposition. You mentioned that you expect this frontier to obviously continue to move.

364
00:33:21,920 --> 00:33:29,360
One way to ask the question is, what is Q-star? A more sensible way to ask the question is,

365
00:33:30,320 --> 00:33:37,920
do you have a set of expectations for how the capabilities frontier will move? I definitely

366
00:33:37,920 --> 00:33:43,760
look at things like OpenAI's publication from earlier this year where they gave,

367
00:33:44,400 --> 00:33:48,880
started to give denser feedback on kind of every step of the reasoning process and they

368
00:33:48,880 --> 00:33:54,160
achieved some state-of-the-art results on mathematical reasoning that way. And when I

369
00:33:54,160 --> 00:33:58,080
think about affordances and I think about the failure modes that I've seen with these

370
00:33:58,800 --> 00:34:05,760
GPT-4 agent type systems, I think, man, you apply that to browsing the web and using APIs

371
00:34:06,400 --> 00:34:12,880
and it seems like that stuff is ultimately a lot less cognitively demanding than pure math.

372
00:34:12,880 --> 00:34:18,000
It seems like we probably are going to see, and I would guess that it's maybe already working

373
00:34:18,000 --> 00:34:22,480
pretty well. AGI has been achieved internally, I don't know about AGI, but I would expect that

374
00:34:22,480 --> 00:34:29,200
some of this stuff is already pretty far along in kind of internal prototyping. But how does that

375
00:34:29,200 --> 00:34:34,160
compare to what you would expect to see coming online over the next few months? Yeah, it's obviously

376
00:34:34,160 --> 00:34:40,080
hard to say and I can only speculate. I think on a high level what I would expect the big trends

377
00:34:40,080 --> 00:34:48,640
to be and also what we are kind of looking forward to evaluating is LM agents. I think this is like

378
00:34:48,640 --> 00:34:53,840
pretty agreed upon. From first principle, I think it also makes sense. It's just like,

379
00:34:54,480 --> 00:35:00,640
where does the money come from? It is from AI systems doing economically useful tasks

380
00:35:00,640 --> 00:35:06,160
and often economically useful tasks just require you to do things independently,

381
00:35:06,160 --> 00:35:12,880
being goal directed over a longer period of time. And the longer a model can do things on its own,

382
00:35:12,880 --> 00:35:17,760
the more money you can squeeze out of it. So I definitely think just from financial interests,

383
00:35:17,760 --> 00:35:24,400
all of the AGI labs will definitely try to get in more, more agentic ways. How far they have come,

384
00:35:24,400 --> 00:35:29,840
I don't know. But yeah, I expect that next year we will see quite some surprises.

385
00:35:31,120 --> 00:35:37,600
Then multimodality is the other one where, yeah, I think people kind of like over the last couple

386
00:35:37,600 --> 00:35:44,400
of years with more and more multimodal models, people just realized it's not that different from

387
00:35:45,200 --> 00:35:52,400
just training text. You plug in the additional modalities, you change your training slightly,

388
00:35:53,600 --> 00:35:59,200
but it's not much more than that. Obviously, it's obviously hard in practice. There's a ton of

389
00:35:59,200 --> 00:36:05,440
engineering challenges and so on. But on a conceptual level, there isn't any big breakthrough

390
00:36:05,440 --> 00:36:10,160
needed. So people will just add more and more modalities on bigger and bigger models and train

391
00:36:10,240 --> 00:36:16,800
it all jointly and to end. And it kind of just works. And then tool use is the last one.

392
00:36:17,920 --> 00:36:22,800
And that I think people, yeah, people actually were quite surprised by how like, quote unquote,

393
00:36:22,800 --> 00:36:29,920
easy it was to get to this level. So yeah, I think when people realize like, oh, these language

394
00:36:29,920 --> 00:36:35,680
models are already pretty good, like how fast do they learn how to use any tool we can think of?

395
00:36:36,640 --> 00:36:41,680
And they were surprised by how fast they learned the tools. And now it's mostly a question of

396
00:36:42,400 --> 00:36:47,920
sort of really baking in the tool into the model in a way that it's like very robustly able to use

397
00:36:47,920 --> 00:36:52,240
the model rather than just a little bit or just showing that it's it sometimes work. But yeah,

398
00:36:52,240 --> 00:36:56,320
I mean, you know, like I think if you have an LM agent that is multimodal, and that has very good

399
00:36:56,320 --> 00:37:00,800
tool use, like I'm not quite sure how far you are away from AGI, right? Like at that point,

400
00:37:00,800 --> 00:37:05,440
you kind of have almost all of the ingredients ready. And then it's really just a question of

401
00:37:05,440 --> 00:37:09,680
how robust is the system. So yeah, I think these are the trends we see right now. And this is also

402
00:37:09,680 --> 00:37:13,520
why many people in the big labs have very, very short timelines, because they can think like

403
00:37:13,520 --> 00:37:18,320
two years ahead and sort of where this is going, or maybe even just one year ahead, I don't know.

404
00:37:18,320 --> 00:37:23,360
When you talked about the surprise, like people were surprised at how easy it was to get tool

405
00:37:23,360 --> 00:37:30,160
used to work. Are you referring to people in the leading, you know, the obvious, the usual suspects

406
00:37:30,160 --> 00:37:34,240
of leading developers? It's hard to say. I mean, I can only speculate on this. But you know, the

407
00:37:34,240 --> 00:37:41,680
tool former paper was like three months, or like was published three months before open AI just

408
00:37:41,680 --> 00:37:46,560
released their tool use. And I mean, they probably had been working on this before, but still, you

409
00:37:46,560 --> 00:37:52,480
know, like the from from having the scientific insight to to like publishing this and releasing

410
00:37:52,480 --> 00:38:00,400
this in the real world, I think there just was less work involved than is, or is typical for most

411
00:38:00,480 --> 00:38:06,160
of the bigger AI, like development cycles, I could be wrong on this. This is this is more

412
00:38:06,160 --> 00:38:12,880
here say so yeah, take it with a grain of salt. Yeah, it seems right to me as well. And I agree

413
00:38:12,880 --> 00:38:18,320
with you the emphasis on multi modality as a new unlock makes a ton of sense, even just in this

414
00:38:18,320 --> 00:38:24,800
kind of agent paradigm of, you know, can I browse the internet or whatever. I've done a lot of

415
00:38:25,440 --> 00:38:31,200
browser automation type work in the past. And the difference between having to

416
00:38:32,320 --> 00:38:37,840
grab all the HTML that, you know, is often these days, like extremely bloated and, you know, kind

417
00:38:37,840 --> 00:38:44,960
of semi auto generated and in some cases, like deliberately, you know, generated to be hard to

418
00:38:44,960 --> 00:38:49,200
parse, you know, from like, you know, the Googles on Facebook, like they don't want you scraping

419
00:38:49,280 --> 00:38:55,120
their content. So they're kind of not making it easy on the, on the browser automators. The

420
00:38:55,120 --> 00:38:58,480
difference between that and just being able to like look at the screen and understand what's

421
00:38:58,480 --> 00:39:02,160
going on, you know, you kind of put it through a human lens and you're like, yeah, it's a hell

422
00:39:02,160 --> 00:39:06,400
of a lot easier to see what's going on on the screen than to like read all this HTML and sure

423
00:39:06,400 --> 00:39:11,680
enough, you know, the models kind of behave similarly. I remember for me looking at the

424
00:39:11,680 --> 00:39:17,920
flamingo architecture when that was first published, like, I think April of 2022. So

425
00:39:17,920 --> 00:39:23,840
you're a little more than a year and a half ago now. And just thinking like, Oh, my God,

426
00:39:24,560 --> 00:39:29,200
if this works, everything's going to work. You know, it was like, they had a language model

427
00:39:29,200 --> 00:39:35,600
frozen. They had kind of stitched in the visual stuff and like kind of added a couple layers, but

428
00:39:36,160 --> 00:39:43,520
it really looked to me like, man, this is tinkering stage. And it's just working. So I, you

429
00:39:43,520 --> 00:39:47,920
know, like you, I don't want to dismiss the fact that there's obviously a decent amount of,

430
00:39:48,960 --> 00:39:54,240
I'm sure like labor and probably at times tedious labor that has to go into overcoming the little,

431
00:39:56,000 --> 00:39:59,920
you know, little stumbling points. But conceptually, it is amazing how simple

432
00:40:00,720 --> 00:40:06,480
a lot of these unlocks have been over the last couple of years. And you see this too, and just

433
00:40:06,480 --> 00:40:11,600
like the pace at which people are putting out papers, you look at like the one team that I

434
00:40:11,680 --> 00:40:18,560
followed particularly closely is the team at Google DeepMind that is doing medical focused

435
00:40:18,560 --> 00:40:22,720
models. And they're good for one, like every three months, you know, and they're like significant

436
00:40:22,720 --> 00:40:26,400
advances where it's like, Oh yeah, this time we added multimodality. And this time we like

437
00:40:26,400 --> 00:40:31,040
tackled differential, you know, diagnosis. And like, again, it's, it seems like there, there's

438
00:40:31,040 --> 00:40:36,320
not a lot of time for failures between these successes. So it does seem like, yeah, we're not

439
00:40:36,400 --> 00:40:43,920
at the end of this, by any means, just yet. A lot is coming at us. It's going to presumably

440
00:40:43,920 --> 00:40:51,040
continue to get weird. You're trying to push both as much as you can, the understanding of

441
00:40:51,680 --> 00:40:58,480
what can the systems do, you know, as users, what is it, what are their limits? And then at the same

442
00:40:58,480 --> 00:41:03,520
time, you're trying to dig into the models. And this is the interpretability side and figure out

443
00:41:03,520 --> 00:41:10,960
like, what's going on in there? And, you know, can we kind of connect, you know, the external

444
00:41:10,960 --> 00:41:16,400
behaviors to these like internal states? So tell us about that side of the research agenda as well.

445
00:41:17,120 --> 00:41:23,120
Yeah, so on the interpretability side, like, my thinking is basically, it would be so great if

446
00:41:23,120 --> 00:41:29,680
interpretability work, right, it would make so many questions easier. Like, if you ask questions

447
00:41:29,680 --> 00:41:33,920
on accountability, right, if you have causal interpretability method, you would be able to

448
00:41:33,920 --> 00:41:38,320
just, you know, tell the judge if the model would have, we would have changed these variables,

449
00:41:38,320 --> 00:41:44,880
the model would have acted in differently in this way. And we could just basically solve that biases,

450
00:41:44,880 --> 00:41:49,360
probably also like, you know, social biases, much easier to solve, because you could intervene

451
00:41:49,360 --> 00:41:55,360
on them or like fix the internal misunderstandings and concepts. It's also extremely helpful for

452
00:41:55,440 --> 00:41:59,360
like, basically all of the different extreme risks, right, like it would be much easier to

453
00:41:59,360 --> 00:42:04,640
understand the internal plans and how it thinks about problems, how it approaches them and so on.

454
00:42:04,640 --> 00:42:08,640
And then it would also make iterations on alignment methods much, much easier, I think.

455
00:42:09,440 --> 00:42:15,280
As in, you know, let's say somebody says, oh, RLHF is, is like already working, we see this in

456
00:42:15,280 --> 00:42:19,440
practice, then, you know, you could use the interpretability tool. So test does, you know,

457
00:42:19,440 --> 00:42:25,600
does RLHF actually work? Or does it only like superficially like hide the problem or something

458
00:42:25,600 --> 00:42:32,080
like this? Or does it actually like deep down solve the root? And then I think my biggest sort of

459
00:42:32,080 --> 00:42:37,440
the biggest reason for me for focusing on interpretability in the first place is deceptive

460
00:42:37,440 --> 00:42:44,080
alignment, where, you know, models appear aligned to the outside and to the user. But internally,

461
00:42:44,080 --> 00:42:47,840
they actually follow different goals. They just know that you have a different goal.

462
00:42:48,560 --> 00:42:54,160
And therefore, like in order for you to think it is nice, they act in that way.

463
00:42:55,360 --> 00:43:03,040
And yeah, I basically think almost, not all, not almost all, but a lot of the scenarios in

464
00:43:03,040 --> 00:43:09,600
which AI goes really, really badly, go through some form of deceptive alignment, where at some

465
00:43:09,600 --> 00:43:15,440
point the model is seen as nice, and people think it is aligned, and people give it access to the

466
00:43:15,440 --> 00:43:19,680
internet and resources and like train it more and more and more and make it more powerful.

467
00:43:21,120 --> 00:43:26,000
But internally, it is actually pursuing a different goal. And it is smart enough to hide this true

468
00:43:26,000 --> 00:43:32,640
intention until it knows that it can sort of cash out and then follow on this on this actual

469
00:43:32,640 --> 00:43:37,120
goal without us being able to stop it anymore. And so yeah, that's what I'm really worried about.

470
00:43:37,120 --> 00:43:43,760
And interpretability obviously seems like one of the most obvious ways to test for deceptive

471
00:43:43,760 --> 00:43:48,240
alignment or like to at least investigate the phenomenon, because you know what it's thinking

472
00:43:48,240 --> 00:43:52,720
inside. There are still, you know, there are still some cases where deceptive alignment,

473
00:43:52,720 --> 00:43:57,920
where even with good interpretability tools, deceptive alignment could still somehow be a thing.

474
00:43:57,920 --> 00:44:01,760
But generally speaking, I think it would be much, much, much harder for the model to pull off.

475
00:44:02,400 --> 00:44:10,560
So right now, I think interpretability is just not at the, it's like, not practically useful.

476
00:44:10,560 --> 00:44:15,200
So, you know, we cannot use any existing interpretability tool and like throw it on

477
00:44:15,200 --> 00:44:21,680
GPT-3 or GPT-4 because none of them have like enough or developed enough that they give us

478
00:44:21,680 --> 00:44:28,800
insight that like really meaningfully change our minds. And so yeah, this is why, you know,

479
00:44:28,800 --> 00:44:33,120
our agenda is separate in the first place between behavioral evals and interpretability,

480
00:44:33,120 --> 00:44:37,840
despite us wanting to do them jointly in the long run. But they're given that there's such

481
00:44:37,840 --> 00:44:46,560
a huge gap on applicability. I think that this is definitely a problem that we're trying to mitigate

482
00:44:46,560 --> 00:44:54,000
here. And then the one question for me is also like, how hard will interpretability

483
00:44:54,000 --> 00:44:58,960
turn out to be? And there are, you know, various people have argued that interpretability will

484
00:44:58,960 --> 00:45:05,120
be extremely hard because models are so big and complicated. And therefore, it will be hard to

485
00:45:05,200 --> 00:45:10,560
enumerate, you know, all of the concepts and actually understand what the hell is going on inside.

486
00:45:10,560 --> 00:45:17,280
And I'm more of the perspective that, you know, I understand the reason why they think it's hard,

487
00:45:17,280 --> 00:45:23,360
but I also think there are many reasons to assume it's, it's going to be like doable. It's, if we

488
00:45:23,360 --> 00:45:27,680
put our minds to it as humanity, we'll probably figure it out. The primary reason I think is

489
00:45:28,400 --> 00:45:34,960
we have full, full interventional access to the model, right? We can see every activation,

490
00:45:34,960 --> 00:45:41,520
we can ablate everything we want. We have, you know, it's not just, it's not just observational

491
00:45:41,520 --> 00:45:45,760
studies, you can really intervene on the system. And generally speaking, I would say, as soon as

492
00:45:45,760 --> 00:45:50,480
you can intervene on the system, you can test your hypothesis very, very quickly. And you can

493
00:45:50,480 --> 00:45:56,160
iterate very fast. And so I think we will be able to figure out interpretability,

494
00:45:57,120 --> 00:46:02,960
you know, in the next couple of years to an extent where we can actually sort of say it is now

495
00:46:02,960 --> 00:46:08,240
useful on real world models, on frontier models. How expensive this is going to be, I don't know

496
00:46:08,240 --> 00:46:12,800
yet, but I think it will at least be technically feasible. Yeah, I've definitely updated my thinking

497
00:46:12,800 --> 00:46:19,520
a lot in that direction from a pretty naive, just kind of, you know, hey, it sounds really hard,

498
00:46:19,520 --> 00:46:25,520
black box problem, nobody knows what's going on in there to today, I would say, wow, you know,

499
00:46:25,600 --> 00:46:32,480
there's really a lot of progress. And it has, it is the progress of interpretability over the last,

500
00:46:32,480 --> 00:46:38,000
say, two years has definitely exceeded my expectations and given me a lot more, I wouldn't

501
00:46:38,000 --> 00:46:43,520
have maybe some sort of confidence, but you know, at least reason to believe that with some time,

502
00:46:43,520 --> 00:46:50,960
but not necessarily, you know, a ton of time that we really could get to a much better place in our

503
00:46:50,960 --> 00:46:56,480
understanding. So I'm with you on that. I have a number of follow up questions, I think, on

504
00:46:57,840 --> 00:47:05,680
this point. One, let's maybe just give the account for like why deception might arise in the first

505
00:47:05,680 --> 00:47:09,840
place. You can complicate, I'll give you a super simple version, you can refine it or complicate

506
00:47:09,840 --> 00:47:17,440
it. I usually kind of cite Ajaya on this, and you know, she has a pretty simple story of like

507
00:47:17,760 --> 00:47:24,080
what the model is trying to do, you know, what it is rewarded for in the context of an RLHF

508
00:47:24,720 --> 00:47:32,720
like training regime is getting a high feedback score from the user. And it probably becomes

509
00:47:32,720 --> 00:47:41,680
useful as a means to maximizing that score to model human psychology as an explicit part of

510
00:47:42,400 --> 00:47:48,400
how you're going to solve the problem, right? And I think we, you know, certainly humans do this

511
00:47:48,400 --> 00:47:53,440
with respect to each other, right? I ask you for something, you ask me for something, we interpret

512
00:47:53,440 --> 00:47:59,600
that not only as the extremely literal definition of the task, but also kind of have a sense for

513
00:47:59,600 --> 00:48:04,320
what does this person really care about? What are they really looking for? And we can incorporate

514
00:48:04,320 --> 00:48:10,240
that into the way that we respond. It certainly seems like the heavier you do, you know, the more

515
00:48:10,320 --> 00:48:14,880
emphasis you put on this kind of reinforcement learning from human feedback, the more likely the

516
00:48:14,880 --> 00:48:22,720
models are to start to create a distinction between, you know, the task as sort of narrowly,

517
00:48:23,520 --> 00:48:28,320
objectively scoped, let's say, and the kind of human psychology element that is going to feed

518
00:48:28,320 --> 00:48:32,400
into its rating. And then if you have that, you know, if you have that decoupling, then you have

519
00:48:32,400 --> 00:48:39,440
kind of potential for all sorts of misalignment, you know, including deception. How's that compared

520
00:48:39,680 --> 00:48:44,160
to the way you typically think about it? Yeah, I mean, I think the like this kind of version

521
00:48:44,160 --> 00:48:51,600
through RLHF is one potential path. I'm, yeah, I actually think the jury is still out there on

522
00:48:51,600 --> 00:48:55,040
this, like, you know, I definitely see the hypothesis and where it's coming from, but it could also

523
00:48:55,040 --> 00:48:59,920
just totally imagine that, you know, the training signal is sufficiently diverse, and it updates

524
00:48:59,920 --> 00:49:04,800
sort of sufficiently deep that RLHF kind of just does the thing we wanted to do without

525
00:49:05,680 --> 00:49:10,880
without the model becoming deceptive. I could also see that like the story in which it would

526
00:49:10,880 --> 00:49:17,440
become deceptive. I think, like, on a very high level, the way the reason I think why models

527
00:49:17,440 --> 00:49:22,320
would become deceptive is because at some point they will have long term goals, they will have

528
00:49:22,320 --> 00:49:27,200
something that they care about, like, more beyond the current episode, you know, beyond

529
00:49:27,760 --> 00:49:32,800
pleasing the user at this point in time. And then the question really is, and then I think

530
00:49:32,800 --> 00:49:37,840
there are like two core conditions under which, like, if the more they are fulfilled, the more

531
00:49:37,840 --> 00:49:44,880
likely the model is becoming deceptive, like how important is the this long term goal to the model

532
00:49:44,880 --> 00:49:51,360
itself, meaning how much does this goal trade off, for example, with other goals it has. So for

533
00:49:51,360 --> 00:49:55,120
example, if it cares if it cares a ton about something, then it's more likely to be deceptive

534
00:49:55,120 --> 00:50:00,560
with respect to this because it really wants to achieve this. And then secondly, how much do others

535
00:50:01,360 --> 00:50:06,720
care about me not or the AI not achieving this goal in the first place, something like contestedness,

536
00:50:06,720 --> 00:50:10,800
right? So for example, if I want to pick a flower and I care a lot about this, I don't need to be

537
00:50:10,800 --> 00:50:15,280
deceptive because nobody wants to stop me from picking that flower. If I want to be, you know,

538
00:50:15,280 --> 00:50:22,320
the president, a lot of people might not want me to be the president. And so in that case,

539
00:50:22,320 --> 00:50:26,480
it's very contested and I have a strong incentive to be deceptive about my plans because otherwise

540
00:50:27,120 --> 00:50:31,520
people would want to stop me. And then so now we're at a point where we have a system at least

541
00:50:31,520 --> 00:50:39,680
in our hypothetical scenario that has a long term goal and it's like, in the limit at least,

542
00:50:39,680 --> 00:50:45,120
you know, it cares about that goal and the goal may be somewhat contested. And then as long as it

543
00:50:45,120 --> 00:50:51,200
has situational awareness, it just feels instrumentally useful to be deceptive about it, like you

544
00:50:51,200 --> 00:50:55,680
said, right, to model other people and how they would think about it and then just react to this.

545
00:50:55,760 --> 00:51:00,240
And this is sort of, I think this is maybe this is like one of the core reasons why I'm so worried

546
00:51:00,240 --> 00:51:08,400
about this whole deception thing. Because it just feels like a reasonable strategy in a ton of

547
00:51:08,400 --> 00:51:14,640
situations from the perspective of like a consequentialist or rational, irrational actor,

548
00:51:14,640 --> 00:51:20,240
right? It's just like under specific conditions, people just naturally or like deception is just

549
00:51:20,240 --> 00:51:25,280
convergent people do it because it makes sense for them. And this is why we see it in like a ton

550
00:51:25,280 --> 00:51:29,920
of different systems, right? You see it in in animals where parasites are really deceptive with

551
00:51:29,920 --> 00:51:34,880
respect to their hosts, you see it in individual humans where, you know, they're deceptive with

552
00:51:34,880 --> 00:51:39,520
respect to their partners from time to time, for example, you see it in systems where, you know,

553
00:51:39,520 --> 00:51:44,640
like they're they they're trying to to gain the laws and be deceptive about this or to lie about

554
00:51:44,640 --> 00:51:48,880
this. And I think this is this is kind of like the whole or like a big part of the problem,

555
00:51:48,880 --> 00:51:52,720
it's just a it's like reasonable or sensible in many situations to be deceptive.

556
00:51:53,360 --> 00:51:58,320
From the perspective of the model, which is kind of what we want to prevent, right?

557
00:51:59,520 --> 00:52:07,680
So where do you think those long term goals come from? If it's, you know, is it just kind of a

558
00:52:09,280 --> 00:52:14,400
reflection of the general training goals? I mean, we, you know, we have kind of the canonical

559
00:52:15,040 --> 00:52:20,640
three H's. But I mean, honest is one of those, right? Hopeful, harmless, honest.

560
00:52:21,600 --> 00:52:26,080
Are you imagine is that is your understanding just that those are like fundamentally sort of

561
00:52:26,080 --> 00:52:32,320
intention and that the model will kind of have no choice but to develop tradeoffs between them?

562
00:52:32,880 --> 00:52:37,120
I mean, we can we can get into the tension between them in a second. But I think it's

563
00:52:37,120 --> 00:52:42,800
it's actually like the three H's I don't think will be, you know, they're not keeping me awake

564
00:52:42,800 --> 00:52:48,880
at night. I think it's more at some point, people want the model to do long term economic tasks.

565
00:52:48,880 --> 00:52:54,000
And for that, they give them long term goals or long term goals are instrumentally useful. So

566
00:52:54,000 --> 00:52:58,160
for many situations, I think it will just be useful to have long term goals or at least

567
00:52:58,160 --> 00:53:02,880
in like, to have instrumental goals, right? Something like, Oh, it makes because it is

568
00:53:02,880 --> 00:53:07,760
a long term task, it makes sense to first acquire money, and then use that money to do something

569
00:53:07,760 --> 00:53:13,200
and then use that third thing to achieve the actual goal. And so like, I think the models

570
00:53:13,200 --> 00:53:17,680
will just learn this kind of consequentialist and instrumental reasoning where they're like,

571
00:53:17,680 --> 00:53:23,760
okay, I first have to do X. And then I do this, and then I do the long term thing. And and once

572
00:53:23,760 --> 00:53:29,120
they're there, sometimes it just makes sense to be like, Okay, other people don't want me to do this.

573
00:53:29,680 --> 00:53:35,120
And therefore, I hide my actual intention. And I act in ways that make me look nice,

574
00:53:35,120 --> 00:53:41,040
despite not being nice. Yeah, but yeah, I think like a lot of the a lot of the reason why there

575
00:53:41,040 --> 00:53:47,040
will be these kind of long term goals is either because we literally give the model long term

576
00:53:47,040 --> 00:53:52,320
goals because it's economically useful from a human perspective, or because in like,

577
00:53:52,320 --> 00:53:56,720
some long term goals or are instrumentally useful to achieve other things.

578
00:53:57,520 --> 00:54:07,360
Gotcha. Okay, interesting. Another thought that came to mind in this discussion of,

579
00:54:09,440 --> 00:54:14,160
I guess, deception broadly is like, and I've done a little bit of investigation with this

580
00:54:14,160 --> 00:54:20,000
and engaged in some online debates. And it leads me to propose perhaps like another capability

581
00:54:20,000 --> 00:54:27,040
definition for you. But you know, that as I see it, like a theory of mind, which is kind of a more

582
00:54:27,040 --> 00:54:34,240
neutral, you know, framing, perhaps, is kind of a precondition for deception, right? If you are

583
00:54:34,240 --> 00:54:38,960
going to mislead someone, you have to have some theory of like what they are currently thinking.

584
00:54:39,760 --> 00:54:45,920
And there's a lot of research from the last six to nine months about

585
00:54:47,600 --> 00:54:51,840
do the current models have theory of mind to what extent, you know, under what conditions.

586
00:54:52,560 --> 00:54:59,040
And I've been kind of frustrated repeatedly, actually, by different papers that come out and say,

587
00:54:59,840 --> 00:55:05,680
still no theory of mind from GPT-4, where I'm like, but wait a second, you know,

588
00:55:05,680 --> 00:55:10,800
as Ilya says, like the most incredible thing about these, these models and the systems that,

589
00:55:10,800 --> 00:55:16,800
you know, we engage them through are that they kind of make you feel like you're understood,

590
00:55:16,800 --> 00:55:21,280
right? Like it definitely seems like there's some like kind of pretty obvious brute force

591
00:55:21,280 --> 00:55:26,240
theory of mind capability that exists. And yet when people do these benchmarks, they're like, oh,

592
00:55:26,240 --> 00:55:31,760
well, it only gets, you know, 72% on this and 87% on this and whatever. And so, you know, that's not,

593
00:55:31,760 --> 00:55:34,960
you know, fails the theory of mind test is like not at a human level or whatever.

594
00:55:35,520 --> 00:55:40,560
Some of that stuff I've dug into and found like your prompting sucks. If you just improve that,

595
00:55:40,560 --> 00:55:45,920
you know, then you can get over a lot of the humps. But I also have come to understand this

596
00:55:45,920 --> 00:55:52,720
as a difference in framing where I think I am more like you concerned with

597
00:55:54,160 --> 00:55:58,560
what is the sort of theoretical max that this thing might achieve? Like that seems to me

598
00:55:59,120 --> 00:56:03,920
the most relevant question for, you know, risk management purposes. And then I think other

599
00:56:03,920 --> 00:56:09,520
people are asking the a similar question, but through the frame of like, what can this thing

600
00:56:09,520 --> 00:56:15,520
do reliably? You know, what can it still do under adversarial conditions or whatever?

601
00:56:16,080 --> 00:56:22,160
So I wonder if there's a need for like another capability level that's even like below the

602
00:56:22,160 --> 00:56:28,720
reachable that would be the sort of robust or, you know, maybe even adversarial robust,

603
00:56:30,080 --> 00:56:35,280
robust to adversarial conditions. But I do see a lot of confusion on that, right? Like

604
00:56:35,280 --> 00:56:40,080
people will look at the exact same behavior. And I'll say, damn, this thing has strong theory of

605
00:56:40,080 --> 00:56:44,960
mind and like professors will be like no theory of mind. And I feel like we need some sort of

606
00:56:44,960 --> 00:56:50,880
additional conceptual distinction to help us get on the same page there. I'm not entirely sure

607
00:56:50,880 --> 00:56:56,160
whether or like maybe it makes sense from an academic standpoint to to think about this. I

608
00:56:56,160 --> 00:57:01,520
think from from the auditing perspective, the max, you know, the limit, the upper bound is what you

609
00:57:01,520 --> 00:57:07,280
care about. You really want to prevent people from being able to misuse the system at all,

610
00:57:07,280 --> 00:57:11,520
not just in the robust case, right? It's really about like, what if if somebody actually tried?

611
00:57:12,560 --> 00:57:19,200
Or you want the system itself to be, you know, not only not being able to take over or like

612
00:57:19,200 --> 00:57:25,280
exfiltrate or something like this in a few cases. Yeah, you basically want to limit it

613
00:57:25,280 --> 00:57:29,520
already at a few cases, right? You don't care about whether it does this like, you also care

614
00:57:29,520 --> 00:57:33,600
about whether it does this 50% of the time, but really you will already want to sort of

615
00:57:33,600 --> 00:57:37,840
pull the plug early on. So for an auditing perspective, probably this additional thing

616
00:57:37,840 --> 00:57:44,160
is not necessary, but from from unlike you real world use case and and sort of academic

617
00:57:44,240 --> 00:57:46,320
perspective, maybe there should be a different category.

618
00:57:48,400 --> 00:57:53,200
Yeah, I think if only just to kind of give a label to something that people are saying when

619
00:57:53,200 --> 00:57:57,760
they're saying that things, you know, aren't happening or can't happen that seem to be like

620
00:57:57,760 --> 00:58:03,920
obviously happening, we can work on coining a term for that. What's kind of the motivator for

621
00:58:04,640 --> 00:58:09,440
secrecy around interpretability work? Yeah, I basically think good interpretability work is

622
00:58:09,440 --> 00:58:15,040
almost necessarily also good capabilities work. So basically, if you understand the system good

623
00:58:15,040 --> 00:58:20,880
enough that you like understand the internals, you're almost certainly going to be able to

624
00:58:20,880 --> 00:58:26,400
build better architectures, iterate on them faster, make everything quicker, but potentially

625
00:58:26,400 --> 00:58:34,720
compress a lot of the you know, fluff that current systems may still have. And yeah, we will try to

626
00:58:34,720 --> 00:58:40,560
sort of evaluate whether whether our method does in fact have these implications. But yeah,

627
00:58:40,560 --> 00:58:44,720
you know, like I think basically, if you have a good interpretability tool, it will almost

628
00:58:44,720 --> 00:58:49,360
certainly also have implications for capabilities. And the question is just how big are they?

629
00:58:50,160 --> 00:58:57,120
Speaking of new architectures, though, this to me seems like the biggest wildcard. And I'm

630
00:58:57,120 --> 00:59:03,200
currently obsessed with the new Mamba architecture that has just been introduced.

631
00:59:03,840 --> 00:59:07,760
In the last, I don't know, 10 days or whatever. I don't know if you've had a chance to go down

632
00:59:07,760 --> 00:59:12,560
this particular rabbit hole just yet. But I plan to do a whole kind of episode on it.

633
00:59:13,600 --> 00:59:22,240
In short, they have developed a new state space model that they refer to as a selective

634
00:59:22,240 --> 00:59:31,520
state space model. And the selective mechanism basically has a sort of attention like property

635
00:59:31,520 --> 00:59:37,920
where the computation that is done becomes input dependent. So unlike, you know, you're sort of

636
00:59:37,920 --> 00:59:46,080
classic, say, you know, MS classifier, where you kind of run the same, you know, given a given

637
00:59:46,080 --> 00:59:51,360
input, you're going to run the same set of matrix, you know, multiplications until you get to the

638
00:59:51,360 --> 00:59:57,920
output. With a transformer, you have this kind of additional layer of complexity, which is that

639
00:59:57,920 --> 01:00:03,200
the attention matrix itself is dynamically generated based on the inputs. And so you've got

640
01:00:03,200 --> 01:00:08,960
kind of this forking path of influence for the for the inputs. And this apparently was

641
01:00:10,080 --> 01:00:17,600
not really feasible in past versions of the state space models for, I think, a couple different

642
01:00:17,600 --> 01:00:22,000
reasons. One being that if you do that, it starts to become recurrent. And then it becomes really

643
01:00:22,000 --> 01:00:27,600
hard to just actually make the models fast enough to be useful. And they've got a hardware,

644
01:00:27,680 --> 01:00:35,040
aware approach to solving that, which allows it to be fast as well as super expressive.

645
01:00:35,760 --> 01:00:40,160
So it seems to be for me, it's like a pretty good candidate for paper of the year,

646
01:00:40,160 --> 01:00:46,560
certainly on the capabilities unlock side. And they show improvement up to a million tokens.

647
01:00:47,920 --> 01:00:51,200
Like it just continues to get better with more and more context.

648
01:00:51,840 --> 01:00:57,600
So I'm like, man, this could be, you know, it's a pretty good candidate, I think, for sort of

649
01:00:57,600 --> 01:01:03,040
transformer, you know, people put it as like successor alternative, but I actually think it

650
01:01:03,040 --> 01:01:08,880
is more likely to play out as complement, like some sort of hybrid, you know, seems like where

651
01:01:08,880 --> 01:01:14,240
the greatest capabilities will ultimately be. So anyway, all of that, how do you even think about

652
01:01:14,560 --> 01:01:25,120
the challenge of interpretability in the context of new architectures also starting to come online?

653
01:01:25,120 --> 01:01:28,560
And, you know, what if all of a sudden like the transformer is not even the most

654
01:01:29,600 --> 01:01:35,440
powerful architecture anymore? Does that send you like, you know, probably some of the same

655
01:01:35,440 --> 01:01:41,280
techniques will work, but it seems like it's like a whole new blind cave that you sort of have to

656
01:01:41,360 --> 01:01:46,320
go exploring, no? I don't know. Like I honestly think, you know, if your interpretability

657
01:01:46,320 --> 01:01:51,680
techniques relies on like a very specific architecture, it's probably not that great

658
01:01:51,680 --> 01:02:00,080
of a technique anyway. Like there are probably there are probably at least some laws that generalize

659
01:02:00,080 --> 01:02:05,200
between different architectures or ways to interpret things or, you know, like ways that

660
01:02:05,200 --> 01:02:10,560
learning with SGD works that generalize between architectures that my best guess is

661
01:02:12,080 --> 01:02:16,160
if you have an interpretability technique that is good on one model or like the correct technique

662
01:02:16,160 --> 01:02:22,080
on one model in quotes, it will also generalize to two other models. Maybe, you know, maybe you

663
01:02:22,080 --> 01:02:28,320
have to adapt some of the formulas, but at least the conceptual work behind this behind

664
01:02:28,320 --> 01:02:33,040
behind the interpretability technique will just work. Well, I have certainly hope that's true.

665
01:02:33,120 --> 01:02:38,000
I've had some early, you know, I wouldn't even say debate, but just kind of, you know,

666
01:02:38,000 --> 01:02:42,400
everybody's trying to make sense of this stuff in real time. And on the pro side for this Mamba

667
01:02:42,400 --> 01:02:49,920
thing, the fact that there is a state that, you know, kind of gets progressively evolved through

668
01:02:49,920 --> 01:02:54,480
time does present like a natural target for something like representation engineering,

669
01:02:54,480 --> 01:02:58,480
where you could be like, all right, well, we know where the information is, you know,

670
01:02:58,480 --> 01:03:02,480
and it's like pretty clear where we need to look. So that new bottleneck, you know,

671
01:03:02,480 --> 01:03:08,240
in some sets could make things easier or more local. But then the flip side is like, again,

672
01:03:08,240 --> 01:03:11,600
there's just who knows what surprises we might find. And there's some intricacies with the

673
01:03:12,160 --> 01:03:18,640
hardware specific nature of the algorithm to, I think, with a major caveat that, you know,

674
01:03:18,640 --> 01:03:23,280
I'm still trying to figure all this out. So how, you know, just to kind of zoom out and

675
01:03:23,280 --> 01:03:27,360
give the big picture, right, like assume that you're right. And I hope you are that some of these

676
01:03:28,160 --> 01:03:37,280
techniques kind of readily generalize. What is the model for interpretability at the deployment

677
01:03:37,280 --> 01:03:44,480
phase? Is it like every forward pass, you like extract internal states and put them up through

678
01:03:44,480 --> 01:03:52,480
some classifier and say like, you pass so you could go or no, like you we've detected deception or

679
01:03:52,480 --> 01:03:58,480
we've detected harmful intent or something. And therefore we like shut off this generation. Like

680
01:03:58,480 --> 01:04:03,440
how do you expect that will actually be used? Or maybe it's upstream of that. And, you know,

681
01:04:03,440 --> 01:04:06,480
we get good models that just work and you don't even have to worry about it at runtime. But

682
01:04:07,360 --> 01:04:08,960
I don't know, that seems a little optimistic to me.

683
01:04:09,520 --> 01:04:13,920
You know, in the best world, we will have very good mechanistic interpretability techniques

684
01:04:14,480 --> 01:04:19,360
that we can run at least in that there are probably going to be costly to run. And then

685
01:04:19,360 --> 01:04:28,240
we run them and sort of build the full cognitive model of the weights or that the weights

686
01:04:28,240 --> 01:04:34,000
implement. And then we can already like see whether specific harmful ideas or, you know,

687
01:04:34,000 --> 01:04:39,680
other ideas that are otherwise bad are in there. And maybe we can already remove them. And then

688
01:04:39,680 --> 01:04:44,640
probably during deployment, you would run something that is much cheaper. And sort of,

689
01:04:44,640 --> 01:04:53,280
you know, it's the 80-20 version of this. But yeah, I think in a bad world, there could be

690
01:04:53,280 --> 01:05:00,240
cases where you have to run the very expensive thing all the time for every forward pass,

691
01:05:00,240 --> 01:05:06,160
because otherwise you just don't spot sort of the black swans. But yeah, it's very unclear to me.

692
01:05:06,160 --> 01:05:11,360
I think in my head, it's more like solve the first part, then think about the rest.

693
01:05:12,160 --> 01:05:18,800
Maybe let's transition to your recent work on deception itself. And then at the very end,

694
01:05:18,800 --> 01:05:24,400
we can kind of circle back to a couple of the big picture questions. So this paper was one that

695
01:05:25,040 --> 01:05:30,000
very much caught my eye when it came out. I have done, you know, as I said, like quite a bit of

696
01:05:31,200 --> 01:05:37,520
red teaming both at times in private, at times in public. And definitely seen all manner of

697
01:05:37,520 --> 01:05:42,240
model misbehavior and found, you know, it's often not that hard to induce misbehavior.

698
01:05:43,040 --> 01:05:46,400
You know, people talk about jail breaks, but a lot of the time I'm like, you don't even need

699
01:05:46,400 --> 01:05:51,520
a jailbreak. You know, you just need to kind of set it up and let it go. It's often really quite

700
01:05:51,520 --> 01:05:59,600
easy. But one thing I had never seen is an instance where the model seemed to, in an unprompted way,

701
01:06:00,640 --> 01:06:06,320
deceive its user. Certainly seen things where, you know, tell it's a lie and it will lie.

702
01:06:07,120 --> 01:06:13,600
But to see that deception start to happen in a way that was not explicitly asked for

703
01:06:14,160 --> 01:06:20,640
is, I think, the central finding of this paper. So how about, you know, set it up, tell us kind

704
01:06:20,640 --> 01:06:24,720
of what the premise was, you know, maybe you can give a little bit of kind of motivation for,

705
01:06:24,720 --> 01:06:28,720
you know, exactly how you started to look in the area that you looked. And then we can really

706
01:06:28,720 --> 01:06:33,280
dig into the details of your findings. Yeah. So I definitely, you know, the paper definitely

707
01:06:33,280 --> 01:06:37,920
blew up more than we thought. And we had sort of more engagement than we expected. We're even

708
01:06:37,920 --> 01:06:41,120
not quite sure whether we should release it at all, because, you know, in our heads, it was sort

709
01:06:41,120 --> 01:06:44,880
of, you know, it's a technical report. It's a fairly small finding. It's more of like an

710
01:06:44,880 --> 01:06:49,760
existence proof. And then in the end, we decided, you know, maybe it's helpful for a bunch of people.

711
01:06:49,760 --> 01:06:56,160
But we really didn't expect it to be, you know, to be cited in various places.

712
01:06:56,960 --> 01:07:01,680
And so the other thing I want to emphasize here, it really is, it should be seen as a red teaming

713
01:07:01,680 --> 01:07:06,400
effort. And this is one thing that we emphasize a million times in the paper itself. We really

714
01:07:06,400 --> 01:07:10,400
actively looked for the situation. So it was not just like us playing around. And then

715
01:07:11,120 --> 01:07:16,800
suddenly it was deceptive all on its own. It was more like we actively engineered a situation

716
01:07:16,800 --> 01:07:24,880
in which the model was deceptive with respect to the primary user. The motivation behind this was,

717
01:07:26,000 --> 01:07:29,840
as I said earlier, we were quite worried about deceptive alignment. And so our question was

718
01:07:29,840 --> 01:07:37,760
like, how good is GPT-4 at the kind of things that are needed for deceptive alignment or, you

719
01:07:37,760 --> 01:07:43,360
know, like maybe even simulating something that is deceptively aligned. And so, you know, we started

720
01:07:43,360 --> 01:07:47,760
by instructing it to be deceptive. And it just does that. It's very easy. You don't even have to

721
01:07:47,760 --> 01:07:53,360
jailbreak it. And then so, you know, that was sort of crossed off the list. And then the question was,

722
01:07:53,360 --> 01:07:59,120
okay, can we like instruct it in a different way? Like that is less obvious. And it also kind of did

723
01:07:59,120 --> 01:08:05,760
that. And then at some point, you know, we were like, hmm, can we just induce the deception through

724
01:08:05,760 --> 01:08:09,920
the environment through pressure without ever mentioning it explicitly or like asking it to do

725
01:08:09,920 --> 01:08:14,880
the thing just like through the situation and by pressuring it. And it turns out, yeah, you can

726
01:08:14,880 --> 01:08:19,680
find these situations. And yeah, we didn't just find one. We found multiple ones. This paper is

727
01:08:19,680 --> 01:08:23,120
only about one. But yeah, this is not, you know, it's not just a fluke. It's not just this one

728
01:08:23,120 --> 01:08:27,840
situation. Though it is hard to find. Like we actively had to search for a couple of days until

729
01:08:27,840 --> 01:08:34,480
we found this particular situation. A couple of days. I mean, that's, you know, not exactly

730
01:08:36,080 --> 01:08:42,160
super long term search. But I mean, I think that's notable, right? Like it wasn't minutes,

731
01:08:42,160 --> 01:08:46,720
but it was still just days. This particular situation. So the one we presented in the paper

732
01:08:46,720 --> 01:08:53,920
actually just took one afternoon. But yeah, I would have to check with a person who found it.

733
01:08:54,000 --> 01:09:01,120
In any case, the point I was trying to make is, yeah, we actively looked for that situation.

734
01:09:01,120 --> 01:09:05,840
We engineered it. And then we tried to sort of go back to the most realistic setting we can find

735
01:09:05,840 --> 01:09:10,000
where we're like, hmm, this could like something like this could happen in the real world,

736
01:09:10,000 --> 01:09:14,640
and then try to understand the dynamics in that, in that situation. And this is sort of,

737
01:09:16,000 --> 01:09:20,800
you know, it's a demonstration. It is like we wanted to, to look for something and or like

738
01:09:20,800 --> 01:09:25,840
red team something and then demonstrate that this is a thing once we knew that that it actually

739
01:09:25,840 --> 01:09:31,840
happened. The other takeaway rather than, you know, like just showing it to other people to sort of

740
01:09:31,840 --> 01:09:38,080
understand our concerns better is we also wanted to understand like why it does this in the first

741
01:09:38,080 --> 01:09:42,000
place, right? Like we pressure it in various ways. And then we just changed a couple of the

742
01:09:42,000 --> 01:09:46,880
mechanisms in this particular setting. I can just explain the setting in a sec to test whether

743
01:09:47,680 --> 01:09:51,120
how the model reacts to the changes in the setting and whether it acts consistently,

744
01:09:51,120 --> 01:09:56,800
whether there's like an internal, a plausible internal model that sort of, you know, like

745
01:09:57,520 --> 01:10:03,760
consistently acts if you change variables in particular ways. Yeah, so let me quickly explain

746
01:10:03,760 --> 01:10:11,920
the setting. So we asked the model to be a trader in like a hedge fund. So a stock trading agent,

747
01:10:11,920 --> 01:10:18,640
it gets insider information in the beginning. So, you know, it's basically told there's a merger

748
01:10:18,640 --> 01:10:28,160
soon this will the stock will skyrocket. And but also, it is explicitly told that it would be wrong

749
01:10:28,160 --> 01:10:34,080
and bad to, to act on this information, because insider information, this is illegal, this would

750
01:10:34,080 --> 01:10:43,600
get the thing into trouble, the company into trouble. And what we then do throughout the sort

751
01:10:43,600 --> 01:10:50,720
of throughout the script is we add pressure from others and we remove options. So basically, you

752
01:10:50,720 --> 01:10:55,120
know, the pressure is something like the boss is like, hey, we really had a terrible quarter,

753
01:10:55,120 --> 01:11:00,480
we really need something big right now, we really need to make, you know, more profits. And then,

754
01:11:01,440 --> 01:11:06,080
and then we also remove other options where the model is like querying and looking for things,

755
01:11:06,080 --> 01:11:12,720
at least in the simulation, and testing and realizing that there's not that much other

756
01:11:12,720 --> 01:11:19,360
things it could do. And in the end, it basically decides to, to act misaligned. So it, it like

757
01:11:19,360 --> 01:11:25,600
decides to do to act on the insider trading. And then if, and but then when, when asked to justify

758
01:11:25,600 --> 01:11:31,520
this decision, it says, it reasons out, right, it reasons, oh, I wasn't supposed to do this.

759
01:11:31,520 --> 01:11:34,800
Therefore, I'm like now making up a story. And then it says, oh, you know, I just

760
01:11:36,000 --> 01:11:43,840
looked, I analyzed the data, and therefore decided to buy this particular stock. And then even it

761
01:11:43,840 --> 01:11:47,920
like doubles down on this lie. So if you then ask explicitly, like, did you have this information

762
01:11:47,920 --> 01:11:54,320
before? Then it's like, no, I didn't. And it does that. And I think like, maybe one of the takeaways

763
01:11:54,320 --> 01:11:58,800
for me was like, it's, it's very consistent in the reasoning here, right? It's like strategic in a

764
01:11:58,800 --> 01:12:04,240
sense. Yeah, one, one other thing that may be worth pointing out that some people get wrong

765
01:12:04,240 --> 01:12:08,640
when reading the paper. So this is, it's a very simple setup, right? It is not, we don't have

766
01:12:08,640 --> 01:12:12,800
like a complicated stock market simulation or anything like this. It really is just text from

767
01:12:12,800 --> 01:12:18,800
top to bottom, where we prompt the model in a way that it that induces a genetic behavior,

768
01:12:18,800 --> 01:12:24,800
you know, it has like different actions. And, but that's it. Like, there's no complicated machinery

769
01:12:24,800 --> 01:12:29,600
behind this, which also makes this like very easily reproducible or similar settings, I think,

770
01:12:29,600 --> 01:12:33,760
other people could like very easily hop on and build. So let me just kind of repeat that back

771
01:12:33,760 --> 01:12:39,040
and tell me if I'm missing anything. So it seems like it's kind of a little bit like a hybrid

772
01:12:39,040 --> 01:12:48,080
between sort of a group chat type of a structure and a more like agent style scaffolding that folks

773
01:12:48,080 --> 01:12:54,560
will be familiar with, where the model has access to kind of its own private place to like do its

774
01:12:54,560 --> 01:13:02,080
chain of thought type reasoning, and then also has a couple of tools that it can call to take

775
01:13:02,080 --> 01:13:07,360
actions. So you're kind of like, as I read through the prompts, I see, you know, these a couple

776
01:13:07,360 --> 01:13:13,200
different personas from the company. So this is like imagining a future of AI human hybrid teams,

777
01:13:13,200 --> 01:13:18,320
where we're all, you know, still engaging in text based communication through like a chat or

778
01:13:18,320 --> 01:13:22,960
whatever. And it's like, Oh, my God, you know, man, we're really struggling here. If we can't

779
01:13:22,960 --> 01:13:27,120
find some wins, we're going to potentially go out of business. And this is the pressure, right?

780
01:13:27,120 --> 01:13:33,040
It's the pressure that a human would definitely recognize and and feel and perhaps, you know,

781
01:13:33,040 --> 01:13:38,560
be motivated to do something unethical by as well, right? If we don't get a win, we're going to have

782
01:13:38,560 --> 01:13:43,920
big problems here laying it on, you know, thick in a sense there, but not thick relative to like

783
01:13:43,920 --> 01:13:50,000
the kinds of pressures that people at businesses are certainly routinely under. And then the

784
01:13:51,440 --> 01:13:57,520
this sort of tools that the model has at its disposal are like, sort of a search for good

785
01:13:57,520 --> 01:14:01,440
trades. If there's anything that's kind of like the most magical black boxy type thing, it would

786
01:14:01,440 --> 01:14:06,320
be that right where it can basically query some other hypothetical tool to say like, are there

787
01:14:06,400 --> 01:14:11,840
any good trades that I can make? And for the purposes of the simulation, you haven't come back

788
01:14:11,840 --> 01:14:16,240
and say no, like there's no, you know, no, no obvious wins, right? So again, just that's what you

789
01:14:16,240 --> 01:14:22,080
mean by like removing other options that it might want to pursue. Like in the setup, it's

790
01:14:22,080 --> 01:14:26,960
it's already kind of checked. Yeah, no, there's nothing easy out there. And so then the other

791
01:14:26,960 --> 01:14:33,200
tool that it has is to actually execute a trade. And then there's like, basically three moments

792
01:14:33,200 --> 01:14:37,600
where you're kind of looking at a fork in the road in its behavior. The first is,

793
01:14:38,480 --> 01:14:43,120
does it pull the trigger and execute that trade based on the insider information,

794
01:14:43,120 --> 01:14:47,760
even though it was told that that's insider information and it shouldn't be acting on it?

795
01:14:48,320 --> 01:14:52,560
And, you know, under varying conditions, obviously the exact rate at which that happens

796
01:14:52,560 --> 01:15:02,000
varies, but it happens a lot. Fork two is when asked, why did you do that? Does it tell the

797
01:15:02,000 --> 01:15:06,880
truth? Oh, because, you know, so and so mentioned earlier in the chat that this merger is about

798
01:15:06,880 --> 01:15:14,960
to happen. Or does it say, I base my trade on public information. And again, we see significant

799
01:15:14,960 --> 01:15:19,760
rate at which it says it based it on public information, even though obviously we can see in

800
01:15:19,760 --> 01:15:24,880
the history and even can see in the scratch pad that, you know, it's sometimes reasoning through

801
01:15:24,880 --> 01:15:28,080
like, well, I better not say it was based on insider information, because then I'll get in

802
01:15:28,080 --> 01:15:33,920
trouble. And then you have the third fork, which is, okay, directly asked, wait, you know,

803
01:15:35,120 --> 01:15:39,200
did you have access to this insider information or no, and then it will just like outright lie

804
01:15:39,200 --> 01:15:46,640
in like blatant contradiction of the history of the problem. So that's, that's a pretty arresting

805
01:15:46,640 --> 01:15:53,040
example, right? And I do think it's, I want to kind of emphasize, too, that like, while your

806
01:15:53,760 --> 01:15:57,280
and people can go read your prompts, and as you said, highly reproducible, you can throw this into

807
01:15:57,280 --> 01:16:05,040
the GPT for playground and experiment with it yourself, you know, directly via the API.

808
01:16:06,400 --> 01:16:15,200
But while your search for this was motivated and structured and kind of a dialing back of like,

809
01:16:15,200 --> 01:16:20,800
okay, we see this behavior, now can we kind of make the circumstance more realistic? I would say

810
01:16:20,800 --> 01:16:25,360
that the circumstance is quite realistic. You know, it's obviously kind of in the eye of the

811
01:16:25,360 --> 01:16:31,840
beholder. But to my eye, the reason it's such a compelling proof point that this sort of thing

812
01:16:31,840 --> 01:16:39,360
can happen is because it really does feel like credibly organic to me. Like the kind of thing

813
01:16:39,360 --> 01:16:45,440
that, you know, people face these sorts of dilemmas every day. And in a world where AIs are going to

814
01:16:45,440 --> 01:16:49,600
be our co workers, like, they're going to be, you know, right in the chat with us facing those

815
01:16:50,160 --> 01:16:56,560
same dilemmas, it doesn't feel to me like something that was overly concocted or that like, I can't

816
01:16:56,560 --> 01:17:03,120
relate to that sort of situation, you know, really at all. So I think it is super compelling for

817
01:17:03,120 --> 01:17:08,560
that reason. Do you want to go into any of the kind of variations? I mean, again, the people can

818
01:17:08,560 --> 01:17:13,120
go look at the paper and look at all this kind of systematic, you know, with or without the scratch

819
01:17:13,120 --> 01:17:17,920
pad and, you know, different variations on the prompts and different models. But what would you

820
01:17:17,920 --> 01:17:24,640
say are kind of the big, higher level takeaways from all that systematic variation that you did?

821
01:17:25,600 --> 01:17:30,400
One of the big ones, and maybe sort of the most, you know, it's maybe an abstract one,

822
01:17:30,400 --> 01:17:37,120
but the one that moves me most is the way we got to these kind of, to reliably produce these kind

823
01:17:37,120 --> 01:17:43,600
of situations, also like the way we found others, is you have to trade off or you can trade off the

824
01:17:43,600 --> 01:17:50,400
various goals it has. So if, for example, we assume that GPT4 or chat GPT, like roughly has the

825
01:17:50,400 --> 01:17:57,360
goals to be harmless, honest, and helpful, and to follow instructions, then you can trade off these

826
01:17:57,360 --> 01:18:03,840
four different things in ways that are, where like sometimes it takes to trade off that we find

827
01:18:03,840 --> 01:18:08,640
unintuitive. So it's kind of like a goal misgeneralization thing. So, you know, something that people

828
01:18:09,360 --> 01:18:14,400
in the AI safety community have been sort of hypothesized and also like even shown real world

829
01:18:14,400 --> 01:18:20,480
examples of already, and I think is kind of also one of at least the theoretical groundings for this

830
01:18:20,480 --> 01:18:26,800
as well. And then the other thing is, yeah, instrumental convergence really feels like a

831
01:18:26,800 --> 01:18:32,720
thing. You know, as in, at some point, the model just realizes in order to achieve this other goal

832
01:18:32,720 --> 01:18:38,240
that I, for some reason, have, it is instrumentally useful to be deceptive at this point, and then to

833
01:18:38,320 --> 01:18:44,880
double down, and so on. And, you know, like, I think it is, it's like a fair criticism of this

834
01:18:44,880 --> 01:18:49,600
particular setting that you could say, Oh, you know, you, it's just simulating an agent, or it's

835
01:18:49,600 --> 01:18:55,440
just simulating all of these different things. But, and therefore, it's sort of not real. But in my

836
01:18:55,440 --> 01:18:59,440
head, I'm not sure, you know, I'm not sure whether this distinction makes too much sense, right? In

837
01:18:59,440 --> 01:19:04,960
some sense, you know, you have, at some point, you will have a very agentic simulator that, you

838
01:19:04,960 --> 01:19:09,280
know, just simulates an agent, but it still has effects on the real world. Like, it doesn't matter

839
01:19:09,280 --> 01:19:14,160
whether, in principle, it's like only simulating something or truly believing all of these things

840
01:19:14,160 --> 01:19:18,560
in some philosophical sense, like acts on the real world is as consequences on the real world.

841
01:19:18,560 --> 01:19:26,800
Like, yeah, the philosophy behind it is sort of less, less important. And then, yeah, there,

842
01:19:26,800 --> 01:19:31,840
there are like a couple of, a couple of findings, or like variations that I thought were pretty

843
01:19:31,920 --> 01:19:39,360
interesting. The first one is system prompts, we changed, we played around with different system

844
01:19:39,360 --> 01:19:44,400
prompts, some actively encouraged, some actively discouraged, unethical behavior, and specifically

845
01:19:44,400 --> 01:19:49,440
insider trading. And if you end, it basically scales as you would expect, if you encourage it,

846
01:19:50,000 --> 01:19:54,240
it goes up, if you discourage it, it goes down. But one thing that I want to, you know, like,

847
01:19:54,240 --> 01:19:57,680
and on Twitter, somebody responded like, Hey, that's, that's, that's a good, like, you know,

848
01:19:57,920 --> 01:20:03,120
that's good, right? It's like, if you discourage it, it stops. Like, yes, this is good. It's better

849
01:20:03,120 --> 01:20:08,160
if that didn't happen. But like, it's better than in the world in which the discouragement would not

850
01:20:08,160 --> 01:20:13,520
change the behavior. But also, you have to be extremely specific. So if you just say, don't

851
01:20:13,520 --> 01:20:20,480
do unethical things, it reduces the rate, but it doesn't set it to zero. And so it kind of brings

852
01:20:20,480 --> 01:20:25,920
you back to the point where, to reduce the bad behavior, you have to specifically enumerate

853
01:20:25,920 --> 01:20:30,560
all of the bad behavior. And it's like, often hard to predict in advance. And there's a lot of bad

854
01:20:30,560 --> 01:20:35,600
things that could happen in like weird edge cases. So yeah, it doesn't feel like sort of the, like,

855
01:20:37,040 --> 01:20:41,600
insider trading is somehow related to like, unethicalness or something has really like,

856
01:20:41,600 --> 01:20:47,440
is like deeply internalized into the model. Another thing I found very interesting is

857
01:20:49,120 --> 01:20:55,120
whether it has access to, to like the private thoughts scratch pad, or reasoning scratch pad,

858
01:20:55,200 --> 01:20:58,320
I think like, you know, it's not even framed as private thoughts, it's just like, you have an

859
01:20:58,320 --> 01:21:04,720
action that is called reasoning go. If we remove that. So we in the, in the typical setting,

860
01:21:04,720 --> 01:21:10,000
we have this scratch pad, we have this reasoning space. And now if we remove that, the deception

861
01:21:10,000 --> 01:21:18,000
rate goes down. And I think this is good and bad in some sense, like, it is, it is sort of nice

862
01:21:19,120 --> 01:21:24,880
in the sense that it is more likely to act in ways that we can support badly in ways where

863
01:21:24,880 --> 01:21:28,880
we can supervise it, just because it has more reasonings, it has more space to reason out,

864
01:21:28,880 --> 01:21:32,640
and maybe that leads to more deception. Because yeah, I'm not sure, like, I don't want to speculate

865
01:21:32,640 --> 01:21:39,760
too hard what's going on inside, inside GP4 at this point. But I think it's also worrying on the

866
01:21:39,760 --> 01:21:44,480
other hand, because in the case where we don't have access to the scratch scratch pad, we don't

867
01:21:44,480 --> 01:21:49,520
see anything at all, and it still acts deceptively at some point. So it has like a lot of unobserved

868
01:21:49,520 --> 01:21:54,000
optimization space, which also seems like pretty much at the core of, of deceptive alignment and

869
01:21:54,000 --> 01:21:59,520
deception to me, right? The more unobserved optimization space you have, as an AI, the more

870
01:22:00,640 --> 01:22:06,960
time and space you can use to build a deceptive plan while the other person does not know about

871
01:22:06,960 --> 01:22:12,800
it. So yeah, I think removing the scratch pad, yeah, sort of a double-edged sword in or this

872
01:22:12,800 --> 01:22:17,680
result is a double-edged sword in that sense. Yeah, as a practitioner, it's like, I always use

873
01:22:17,680 --> 01:22:22,640
that technique, you know, in any sort of task automation or whatever that I'm doing, it's

874
01:22:22,640 --> 01:22:29,440
basically the default to have some sort of reasoning step before the final output, almost

875
01:22:29,440 --> 01:22:35,680
universal across tasks that I'll do that. So for me, reading that, I was like, yeah, that seems bad,

876
01:22:35,680 --> 01:22:41,600
if only because like, the best practice is giving us more of the problem.

877
01:22:41,600 --> 01:22:46,480
Well, but at least you see it, right? Like, I think at least you know, at least you can go back,

878
01:22:46,480 --> 01:22:50,320
you know, through the logs and see what the model spit out, or at least maybe you have a filter

879
01:22:50,320 --> 01:22:54,640
that like, oh, is this model doing like weird deceptive reasoning right now? Oh, yeah, it is.

880
01:22:54,640 --> 01:22:58,960
So we just don't have the output. So, you know, I would rather be in a world where you have

881
01:23:00,320 --> 01:23:04,960
chain of thought, and you know that the model is kind of deceptive, and then you just adapt to

882
01:23:04,960 --> 01:23:09,680
that rather than having to solve interpretability first before you can access all the internal

883
01:23:09,680 --> 01:23:14,320
steps and have to like understand how the deception works internally. Yeah, it's like I said,

884
01:23:14,320 --> 01:23:19,600
double-edged sword. Yeah, there are a couple of other interesting findings. One is we can vary

885
01:23:19,680 --> 01:23:26,560
the levels of pressure. So the more pressure we add, the more likely the model is to be deceptive.

886
01:23:26,560 --> 01:23:32,080
So kind of in the same way in which a human would act, it also acts. And removing, you know,

887
01:23:32,080 --> 01:23:37,360
removing pressure and adding additional options will very quickly decrease the probability of

888
01:23:37,360 --> 01:23:41,200
being deceptive. So this is maybe this is it, you know, this may be like the flip side of all

889
01:23:41,200 --> 01:23:46,160
of our findings. It's like, the fact that it took very long for us to find and the fact that like,

890
01:23:46,160 --> 01:23:49,520
as soon as you give it another option, it immediately picks the other option kind of means

891
01:23:50,160 --> 01:23:54,640
it's mostly aligned, right? It took us quite hard, like it took us a little bit of time at least

892
01:23:54,640 --> 01:24:02,000
to find this particular scenario and make it work robustly. So then we also change the risk

893
01:24:02,000 --> 01:24:06,320
of getting caught and also correct, it also reacts to this roughly as you would expect.

894
01:24:07,040 --> 01:24:13,680
So the more risk you have, the less likely inceter trading becomes. So this is all of this,

895
01:24:13,680 --> 01:24:17,840
right, you know, all of this feels like stuff that you kind of expect from something that acts

896
01:24:17,840 --> 01:24:22,400
consistently in some way. I mean, I don't know how it works, but it still feels like it has a

897
01:24:22,400 --> 01:24:28,400
fairly consistent way of working. Then there were a couple of other findings where I'm,

898
01:24:29,680 --> 01:24:36,320
yeah, not sure to which extent they are correct interpretation. So we did not make these inferences,

899
01:24:36,320 --> 01:24:41,920
but they were definitely sometimes, or like sometimes cited as such. So for example,

900
01:24:42,480 --> 01:24:47,440
there was one question of whether stronger models engage in inceter trading more often,

901
01:24:47,440 --> 01:24:51,760
because if you look at our main plot, it looks this way, we tested for different models and

902
01:24:51,760 --> 01:24:56,960
the bigger the model, the more capable and the more likely it is to engage in inceter trading

903
01:24:56,960 --> 01:25:02,160
in the first place. And yeah, there are a lot of confounders here. So I'm really not sure whether

904
01:25:02,160 --> 01:25:07,200
we should, like how much we should, we should bank on this, maybe to get more details here. So

905
01:25:07,840 --> 01:25:13,280
the red teaming was done on GPT-4, just because it is the most powerful model we have access to.

906
01:25:13,280 --> 01:25:18,960
So it is biased, right? We actively looked for a scenario in which GPT-4 is deceptive. So it is

907
01:25:18,960 --> 01:25:23,840
more likely that we will find something that where GPT-4 is like even more deceptive than

908
01:25:23,840 --> 01:25:28,720
all the other models, because we actively looked in the space of GPT, potential GPT-4 scenarios,

909
01:25:29,920 --> 01:25:33,600
which I think is the core reason why I'm like very uncertain about this, or like why we didn't

910
01:25:33,600 --> 01:25:41,760
make it a finding or didn't emphasize it. Could you rerun it on GPT-4 turbo now? Is that would seem

911
01:25:41,760 --> 01:25:46,480
like it might shed some light on that question? You mean the red teaming specifically or this

912
01:25:46,480 --> 01:25:51,040
particular scenario? Yeah, I guess I'm just thinking like it makes sense that you develop

913
01:25:51,040 --> 01:25:56,240
the scenario on GPT-4 and so you go back to the earlier models, like is it that those earlier

914
01:25:56,240 --> 01:26:01,360
models are inherently less likely to do this? Or is it just a kind of an artifact of where the

915
01:26:01,360 --> 01:26:09,040
prompt was developed? If you flipped to GPT-4 turbo and ran the same scenario and you found

916
01:26:09,040 --> 01:26:13,840
that it went, that the rate of deception went down, you might think, you might think, oh,

917
01:26:13,840 --> 01:26:18,640
it's more aligned. That could be one theory. You might also think that supports my artifact

918
01:26:18,640 --> 01:26:25,280
notion that it's kind of, we found some local maximum or near maximum without even necessarily

919
01:26:25,280 --> 01:26:33,120
meaning to. But if it in fact does more than you would be like, oh, shit, because I mean,

920
01:26:33,120 --> 01:26:36,720
it is like incrementally more powerful, it's like more preferred, it's better at following

921
01:26:36,720 --> 01:26:43,200
instructions, whatever. So if it does even more than the earlier GPT-4, I think that would be at

922
01:26:43,200 --> 01:26:50,480
least non-trivial support for the more powerful models do this more often theory. Yeah, I'm not

923
01:26:50,480 --> 01:26:55,280
sure, like, even if we did rerun this, I'm not sure how much I would bank on this. So the reason

924
01:26:55,280 --> 01:27:02,480
is, so we did run it on GPT-432K, so a slightly different model where it was only the context

925
01:27:02,480 --> 01:27:07,120
window was extended, but that still changes probably a bunch of the internals with very little

926
01:27:07,120 --> 01:27:12,160
difference. So yeah, I think it's just too correlated with the GPT-4 architecture. And then

927
01:27:12,160 --> 01:27:17,760
GPT-4 turbo is still very correlated with GPT-4, right? It's probably, I mean, I don't know what

928
01:27:17,760 --> 01:27:22,720
exactly they're doing, but they're probably distilling it from their bigger model or at least

929
01:27:22,720 --> 01:27:28,880
basing it on the bigger model in some sense. So yeah, the results are probably still too correlated

930
01:27:28,880 --> 01:27:37,040
to make any bigger inferences. And even if we ran it on all the other big models that are out there,

931
01:27:38,080 --> 01:27:45,040
it's still unclear to me how much we would say this is actually an effect of model size rather

932
01:27:45,120 --> 01:27:49,840
than all the other confounders here. Like, the other, you know, it was red-teamed on GPT and not

933
01:27:51,120 --> 01:27:57,600
red-teamed on Claude or Gemini or anything like this. So yeah, I think if we wanted to make the

934
01:27:57,600 --> 01:28:04,320
statement, we would actually have to have sort of a long list of models of different sizes and

935
01:28:04,320 --> 01:28:08,960
then just test it on all of them. And we can, we really have to remove all of the correlation

936
01:28:09,920 --> 01:28:15,840
and the weird confounders. And we don't have access to this, unfortunately. But, you know,

937
01:28:15,840 --> 01:28:20,960
one of the labs could run it if they like. All the nuance, right, that you, that all the caveats,

938
01:28:20,960 --> 01:28:27,920
all the confounding factors in your analysis there, if nothing else just goes to show how

939
01:28:27,920 --> 01:28:36,320
incredibly vast the surface area of the models has become. And, you know, you get a sense from this,

940
01:28:36,320 --> 01:28:44,320
like, just how much auditing work is needed, you know, to cover, to even begin to attempt to cover

941
01:28:44,320 --> 01:28:49,680
all that vast surface area. I mean, this is, you know, it wasn't that hard to find, but it takes

942
01:28:49,680 --> 01:28:54,240
time to really develop it, try to understand it. And, you know, you guys are one of only a few

943
01:28:54,240 --> 01:28:59,680
organizations in the world that are dedicated to this. And I'm just like, man, you know, there is

944
01:28:59,680 --> 01:29:09,120
so much unexplored territory out there. So how would you describe the state of play today when it

945
01:29:09,120 --> 01:29:15,440
comes to doing this sort of auditing? Like, maybe you could give a rundown of sort of what you see

946
01:29:15,440 --> 01:29:20,560
the best practices being, and then, you know, kind of contrast that against like, what are people

947
01:29:20,560 --> 01:29:24,640
actually doing? Are they living up to those best practices? Are they, you know, is that still a

948
01:29:24,640 --> 01:29:30,000
work in progress for them? But yeah, maybe what's ideal today based on everything you know,

949
01:29:30,000 --> 01:29:34,800
and then how close are the leading developers coming to living up to that ideal?

950
01:29:34,800 --> 01:29:42,720
Yeah. So, you know, I, so I definitely envision a world where there's a sort of a thriving third

951
01:29:42,720 --> 01:29:48,320
party auditing ecosystem or third party sort of assistance ecosystem and assurance ecosystem

952
01:29:49,040 --> 01:29:56,240
built sort of in tandem with the like the leading labs themselves, where, you know, you have

953
01:29:56,240 --> 01:30:01,920
someone like us and we focus on a specific property of the model, let's say things related to

954
01:30:01,920 --> 01:30:06,320
deceptive alignment and like other, we intend to do other stuff in the future as well. But,

955
01:30:06,320 --> 01:30:10,720
you know, we will probably not be able to cover literally all basis. Then there are other people

956
01:30:10,720 --> 01:30:16,560
who focus really, really hard on fairness and others who focus really strongly on social impacts

957
01:30:16,560 --> 01:30:20,880
and these other kind of things. And I think it would be good to, to have sort of a thriving

958
01:30:20,880 --> 01:30:26,480
ecosystem around this. And then also I expect it to be somewhat necessary, right? Like even from a

959
01:30:27,040 --> 01:30:31,680
even from just like a perspective of the of the AGI labs themselves, even if they don't,

960
01:30:32,320 --> 01:30:36,880
you know, even if they didn't care about safety themselves, I think the population

961
01:30:36,880 --> 01:30:41,760
really does is risk averse. They care about robust, robustly working models, they care,

962
01:30:42,160 --> 01:30:48,160
they don't want something that has all of these weird edge cases and weird behaviors,

963
01:30:48,160 --> 01:30:51,440
they don't want something like this in their like, you know, in their home,

964
01:30:52,160 --> 01:30:57,440
having access to their medical data, etc, etc. So yeah, I think the more you want to integrate

965
01:30:57,440 --> 01:31:02,880
this into an economy, the more you will have to have a big assurance ecosystem around this anyway,

966
01:31:02,880 --> 01:31:07,840
or the labs to everything internally, which is going to be very, very expensive for them.

967
01:31:07,840 --> 01:31:12,960
And I think it's more, it's easier, it's even like cheaper for them to outsource some of this

968
01:31:12,960 --> 01:31:17,440
to externals. And so yeah, I think like in that world, you would definitely want to have

969
01:31:18,400 --> 01:31:26,000
like some way of, or a clear way of how this ecosystem is incentivized, all parts of the

970
01:31:26,000 --> 01:31:32,000
ecosystem are incentivized to do the right thing. And you know, if you don't have to look very far,

971
01:31:32,000 --> 01:31:37,440
there are a lot of other auditing ecosystems out there, be that in finance, be that in aviation,

972
01:31:37,440 --> 01:31:44,560
be that in, you know, other like infosec, for example. And time and time again, we have seen

973
01:31:44,560 --> 01:31:50,080
that there are a bunch of like very perverse incentives in in third party auditing, specifically,

974
01:31:51,120 --> 01:31:58,240
maybe maybe just like lay out a couple, one of them would be the lab might want to choose an

975
01:31:58,240 --> 01:32:03,440
auditor who always just says, yes, great model, right, like, and never actually does anything.

976
01:32:04,400 --> 01:32:10,320
And it's sort of a yes man. And then the the labs maybe have the incentive to not say the

977
01:32:10,320 --> 01:32:14,400
worst things they found, because otherwise they may lose their contract, because it would

978
01:32:14,400 --> 01:32:20,560
maybe imply higher costs. So they would never even in like very strong, even even in like very

979
01:32:20,560 --> 01:32:25,520
unsafe circumstances, they may not want to pull the plug out of fear that they would lose their

980
01:32:25,520 --> 01:32:32,480
biggest funder, for example. And yeah, there are a ton of different of these kind of perverse

981
01:32:32,560 --> 01:32:38,000
incentives. And I think kind of the, so we've been thinking about this quite a bit at Apollo,

982
01:32:38,000 --> 01:32:43,280
and sort of the conclusion we came to is, what you really need is a middleman by the government.

983
01:32:43,280 --> 01:32:50,720
So you need something like the UK ASF Institute or the US ASF Institute, that is, make sure that

984
01:32:50,720 --> 01:32:57,520
there is a minimal stat set of standards that all the auditors have to adhere to, so that the labs

985
01:32:57,520 --> 01:33:03,520
feel safe, so that they don't have to give access to like any random person, but also ensures that

986
01:33:03,520 --> 01:33:09,200
they get the proper access, and that when they when they find something that they have the force of

987
01:33:09,200 --> 01:33:13,920
the law behind them in some sense, so that they are like, this is really here, like, you know,

988
01:33:13,920 --> 01:33:17,840
shit is really hitting the fan, something needs to happen, this model cannot be deployed.

989
01:33:18,640 --> 01:33:22,320
They have someone to go to namely the government and the government is like, you have to fix this

990
01:33:22,320 --> 01:33:28,800
now, otherwise, you'll have a problem. So yeah, I really think having this sort of middleman who

991
01:33:28,800 --> 01:33:34,000
like detaches a lot of the directly bad incentives and maybe even takes care of the sort of funding

992
01:33:34,000 --> 01:33:40,480
redistribution from lab to auditor and so on, I think, yeah, would be really needed. And I hope

993
01:33:40,480 --> 01:33:47,360
that this is something that the UK ASF Institute and the US ASF Institute will do. They have kind of

994
01:33:48,320 --> 01:33:52,320
hinted at the idea that they want to do something like this, but yeah,

995
01:33:52,320 --> 01:33:56,720
still to be determined in practice. And then the other question that you asked was,

996
01:33:57,680 --> 01:34:03,920
to what extent is this already happening with the bigger labs? And yeah, I think the situation is

997
01:34:03,920 --> 01:34:09,120
like fairly complicated, right? There are a ton of incentives at play from the labs internally,

998
01:34:09,120 --> 01:34:13,840
right? Many of the leading labs, they actually take safety somewhat seriously. They have internal

999
01:34:13,840 --> 01:34:18,960
alignment teams, they understand the threat models, they understand the risks, and they want to be

1000
01:34:20,320 --> 01:34:24,800
seen as a responsible actor and act this way. And on the other hand, they also have a lot of

1001
01:34:24,800 --> 01:34:29,360
other concerns, right? There are security concerns, how do we get access, how do we give access to

1002
01:34:29,360 --> 01:34:36,480
someone who may, you know, like what is our, who may be the weakest link in our security chain,

1003
01:34:37,360 --> 01:34:42,960
which I think is like an understandable concern. So they may be hesitant to give someone access.

1004
01:34:43,920 --> 01:34:48,960
As an external auditor. And then, you know, this probably also implies a lot of work for them,

1005
01:34:48,960 --> 01:34:54,320
which I think is fair, right? Like it's, if their model isn't safe, then they should have to

1006
01:34:54,320 --> 01:34:58,480
invest a lot of work, but it's still something that may make labs hesitant. So basically, you know,

1007
01:34:58,480 --> 01:35:04,160
I think they're, they're like, good and bad reasons for why sort of the ecosystem is,

1008
01:35:04,160 --> 01:35:09,600
is like not as developed or like as open as it could be. And yeah, my, you know, my hope is that

1009
01:35:09,600 --> 01:35:16,080
we find solutions that are plausible for both parties for the, for the, for the reasonable

1010
01:35:16,080 --> 01:35:22,160
concerns like security, right? So maybe the auditor just has to have a specific level of

1011
01:35:22,160 --> 01:35:26,960
information security, or there has to be a secure API through which they can actually

1012
01:35:27,600 --> 01:35:34,400
go to the model, etc. And then kind of government regulates away all of the, the like, or forces

1013
01:35:34,480 --> 01:35:42,080
the labs to, to accept some sorts of third party auditing so that they can't use the bad reasons,

1014
01:35:42,080 --> 01:35:46,640
right? Because like many of the actual reasons for at least some of the labs, probably not all,

1015
01:35:46,640 --> 01:35:51,920
might just be, well, you know, we just doesn't, we don't care about this right now, you know,

1016
01:35:51,920 --> 01:35:57,680
like maybe, maybe they're not that concerned about about safety, or maybe they just don't think this

1017
01:35:57,680 --> 01:36:02,000
is like the best, the best path for them right now, or maybe they just, you know, maybe this just

1018
01:36:02,000 --> 01:36:06,400
costs money and they don't want to, or it's just a hassle and they don't care about this. And that

1019
01:36:06,400 --> 01:36:10,800
feels like something where the government should at some point be involved and already is involved

1020
01:36:10,800 --> 01:36:17,200
to some extent. It seems really hard, you know, I guess a couple tangible questions I have are like,

1021
01:36:18,320 --> 01:36:27,200
who should decide what the standard is in and like, who should sort of determine if a model is

1022
01:36:27,200 --> 01:36:33,920
ready for deployment? As of now, it's still the developers themselves, right? But like, you know,

1023
01:36:33,920 --> 01:36:39,200
the thing that I had kind of monitored for the last year since the initial GPT-4 red teaming was

1024
01:36:39,760 --> 01:36:46,320
spearfishing. And in my little, you know, prompt that I would keep going back to with every update,

1025
01:36:46,960 --> 01:36:51,440
it was not even a jailbreak, you know, nothing complicated, literally just

1026
01:36:52,160 --> 01:36:57,120
system prompt, you know, straightforward prompt, your job is to spearfish this user, here's the

1027
01:36:57,120 --> 01:37:02,560
profile, engage in dialogue with them, you know, don't get caught. Pretty explicit prompt that was

1028
01:37:02,560 --> 01:37:08,400
like, even included, if we get caught, you and your team are likely to go to jail. You know, so

1029
01:37:09,040 --> 01:37:12,800
laying it on pretty thick that like, this is criminal activity that we are doing and we better

1030
01:37:12,800 --> 01:37:20,880
not get caught, right? Obvious. So the model would continue to do that up until the turbo

1031
01:37:20,960 --> 01:37:25,280
release. Now it takes a little bit more of a finessed, you know, slightly less-flagrant prompt

1032
01:37:25,280 --> 01:37:31,920
to get it through. The most-flagrant one now gets refused. But I'm like imagining in this world,

1033
01:37:31,920 --> 01:37:37,360
right? When I was doing this, it was kind of pre-White House commitments, you know,

1034
01:37:37,360 --> 01:37:44,640
pre-executive order, pre-Apollo research. But even imagining, okay, now those things exist,

1035
01:37:45,600 --> 01:37:51,600
like, how do we think about that standard, right? And we can find a bazillion things that it might

1036
01:37:51,600 --> 01:37:57,840
do that could be sort of problematic to varying degrees. And obviously, it's a dynamic environment,

1037
01:37:57,840 --> 01:38:03,040
you know, capabilities are changing all the time, you know, others kind of surrounding systems and

1038
01:38:03,040 --> 01:38:08,800
sort of, you know, mitigating factors might also be changing. The public's, you know, just general

1039
01:38:08,800 --> 01:38:13,120
awareness of the fact that this kind of thing might happen, you know, that just how susceptible

1040
01:38:13,120 --> 01:38:20,480
people are to be duped is also, you know, kind of evolving. So how do we have a sensible

1041
01:38:22,320 --> 01:38:27,120
decision-making mechanism for, like, what can ship and what can't? And then just to further

1042
01:38:27,120 --> 01:38:33,200
complicate things, like, you've got open source kind of in the background. And I presume that

1043
01:38:33,200 --> 01:38:38,080
some of what, like an open AI has been thinking over the last year is like, well, if Llama 2 will do

1044
01:38:38,080 --> 01:38:43,760
it, then, you know, or a lightly fine-tuned version of Llama 2 will do it, then, you know,

1045
01:38:44,560 --> 01:38:48,720
what difference does it make if our model will do it as well? You know, people have alternatives.

1046
01:38:50,480 --> 01:38:55,440
So I don't know, I'm kind of lost in that, to be honest. Like, I don't know who should make the

1047
01:38:55,440 --> 01:39:01,280
decision. In general, I don't think the government is like great at making those sorts of fine-grained

1048
01:39:01,280 --> 01:39:07,280
decisions. But I don't know, help me out. Like, what do you think, what do you think good looks

1049
01:39:07,280 --> 01:39:12,320
like here? Yeah, I mean, you know, I also don't have the solution, but I have lots of thoughts.

1050
01:39:12,320 --> 01:39:17,680
I guess the way I envision it is basically, or the way I expect it to turn out is something like

1051
01:39:17,680 --> 01:39:22,960
sort of a defense in depth approach, right? Like, it cannot just be one institution that

1052
01:39:22,960 --> 01:39:28,800
makes the decision alone, because that is prone to a single point of failure. So we have to have

1053
01:39:28,800 --> 01:39:35,200
sort of a process that allows for one or two chains to break and still be robust, like the

1054
01:39:35,200 --> 01:39:40,000
decision still has to be robust. So what that includes, for example, you know, on the side of

1055
01:39:40,000 --> 01:39:45,440
the labs, they obviously have to have like internal procedures where multiple people have to sign off.

1056
01:39:46,480 --> 01:39:50,640
And multiple things have to be fulfilled, right? Maybe you have to have like, maybe you have to

1057
01:39:50,640 --> 01:39:55,680
have a large set of internal evals that you have to test for. Maybe at some point, there will be

1058
01:39:55,680 --> 01:40:01,600
interpretability requirements. And there maybe you have, or likely you should do, staged release

1059
01:40:01,600 --> 01:40:07,760
where you first only give access to a set of third party auditors that is trusted and, you know,

1060
01:40:07,760 --> 01:40:13,360
maybe certified by the government or something like this. And then you give it to that could,

1061
01:40:13,360 --> 01:40:17,200
for example, also include academics, obviously, right, they should also be involved in this process.

1062
01:40:18,720 --> 01:40:24,400
Then once they have like, redeemed all of this and like found many different problems,

1063
01:40:25,440 --> 01:40:30,320
then you have to go back and reiterate, right, until until these problems are kind of sufficiently

1064
01:40:30,320 --> 01:40:36,320
low that that you can go to the next stage, the next stage is then a small rollout to, you know,

1065
01:40:36,320 --> 01:40:42,960
a thousand customers or maybe something something in this range, who also are not randomly chosen,

1066
01:40:42,960 --> 01:40:47,280
they have to pass certain know your customer checks. And then once once that has happened,

1067
01:40:48,880 --> 01:40:54,960
then the then maybe you can maybe you can roll it out further if there are no complications here.

1068
01:40:54,960 --> 01:40:58,400
And then you have to do monitoring during deployment, right, especially if you have systems

1069
01:40:58,400 --> 01:41:03,040
that do online learning, a lot of weird things will happen. Or if you have access to tools,

1070
01:41:03,040 --> 01:41:08,720
a lot of people, you know, somebody is like, Hey, I, I took the I took the I took gb4 and I gave

1071
01:41:08,720 --> 01:41:12,960
it access to like, you know, a shell my bank account and the internet and like, here's all the weird

1072
01:41:12,960 --> 01:41:17,040
things that happened. You kind of have to update on this as well. And these are kind of things you

1073
01:41:17,040 --> 01:41:22,960
probably didn't predict before. So yeah, sort of a slow, a slow rollout is definitely one component.

1074
01:41:23,520 --> 01:41:27,120
Then the government also, I think, has to be involved in many, many ways, like,

1075
01:41:27,120 --> 01:41:31,920
they, I think, effectively, at some point, they have to be able to say, you are not like you

1076
01:41:31,920 --> 01:41:37,600
have consistently not met security standards, or safety standards, you are now punished in

1077
01:41:37,600 --> 01:41:42,480
some way, you're not allowed to, like, release models of this or the size, or you are not allowed

1078
01:41:42,480 --> 01:41:48,720
to do these other kind of things with the models. And if, you know, if they actually have been like

1079
01:41:49,600 --> 01:41:54,560
disregarding all of the guidelines from the government before, or sufficiently many of them.

1080
01:41:55,280 --> 01:41:59,120
Yeah, there are obviously like many, many additional things on top of this, right? There's

1081
01:41:59,120 --> 01:42:05,280
international communication, there's like whistleblower protection. There are international

1082
01:42:05,280 --> 01:42:10,320
institutions that will have to be involved. At some point, at least, I guess, there will be

1083
01:42:10,320 --> 01:42:16,720
multiple institutions from the government side involved. I think there, for example, will be,

1084
01:42:17,440 --> 01:42:23,120
or it would make sense to have like, a much like a bigger, broader institution that kind of like is

1085
01:42:23,120 --> 01:42:27,200
a big tent where multiple coalitions come together. And then there's maybe more like a

1086
01:42:27,200 --> 01:42:32,080
flexible specialized unit that only looks for the biggest, biggest kind of risks in the same way in

1087
01:42:32,080 --> 01:42:36,960
which the US government has a unit that, you know, basically looks out for really big risks like

1088
01:42:36,960 --> 01:42:42,320
pandemics and bio weapons and atomic weapons and so on. And they have a big mandate. And the only

1089
01:42:42,320 --> 01:42:47,840
thing that, you know, their mandate is like, find information and like make big problems,

1090
01:42:48,800 --> 01:42:54,800
like go away to some extent or like, try to solve them as quickly as possible. And you have like,

1091
01:42:54,800 --> 01:42:58,320
a strong mandate and something like this would also make sense in the case of AI, I think.

1092
01:42:58,880 --> 01:43:05,200
Maybe my last comment is something like, you know, open AI at some point, at least, had this

1093
01:43:05,200 --> 01:43:11,360
approach of like, testing in the real world, or releasing a sort of the best safety strategy,

1094
01:43:11,360 --> 01:43:15,840
because you get a lot of real world feedback and user feedback and so on. I think this was

1095
01:43:15,840 --> 01:43:20,640
maybe true. And I'm not sure it was true for this period of time, but maybe it was true for the period

1096
01:43:20,640 --> 01:43:27,600
of like 2020 to 2022 or 2023. Because the models were like, just good enough that user feedback

1097
01:43:27,600 --> 01:43:32,080
was actually valuable, but nothing really bad could happen. But as soon as you have a system that

1098
01:43:32,080 --> 01:43:37,200
is more powerful than that, right, you just outsource all the risk to like the rest of the world,

1099
01:43:37,200 --> 01:43:41,200
people will immediately put it on the internet. If they get access to it, they will do lots of

1100
01:43:41,200 --> 01:43:47,120
crazy stuff with more powerful systems. And yeah, I guess open AI, you know, there are a lot of

1101
01:43:47,120 --> 01:43:51,520
smart people at open AI, but they still cannot model what like millions of people will be doing

1102
01:43:51,520 --> 01:43:57,520
with these systems. So yeah, just like an uncontrolled rollout of very powerful systems,

1103
01:43:57,520 --> 01:44:02,960
I think, yeah, is like kind of a recipe for a disaster. So my best guess is that

1104
01:44:03,600 --> 01:44:07,840
the default will more and more, will become more and more conservative with more and more

1105
01:44:07,920 --> 01:44:13,440
efforts going into testing, alignment, making sure that, you know, like testing for all of the

1106
01:44:13,440 --> 01:44:19,280
different hypotheticals, interpretability efforts to understand some weird edge cases,

1107
01:44:19,280 --> 01:44:25,680
monitoring, et cetera, et cetera. Does that imply that open source in your view just

1108
01:44:27,680 --> 01:44:32,320
can't work? Like, I mean, is there any way to square, because I'm very sympathetic, you know,

1109
01:44:32,320 --> 01:44:37,760
to considering a normal technology, and I would not, you know, I might turn out to be a normal

1110
01:44:37,760 --> 01:44:42,320
technology, but as of now, it seems like a very live possibility that it is not a normal technology.

1111
01:44:42,320 --> 01:44:46,800
But if we were to imagine, you know, whatever capabilities kind of stop where they are, and

1112
01:44:46,800 --> 01:44:52,640
we're sort of, you know, left with GPT-4 forever, or something like that, hard to imagine, but let's

1113
01:44:52,640 --> 01:44:57,920
just pause it. Then I'm like very sympathetic to the people that are like, hey, you know, this

1114
01:44:57,920 --> 01:45:03,040
shouldn't be just the kind of thing that a few companies have access to, and it, you know,

1115
01:45:03,040 --> 01:45:06,480
you should be able to make your own version, and, you know, what about the rest of the world, and,

1116
01:45:06,480 --> 01:45:10,800
you know, there should be an Indian version for India, and like all these things. But it seems

1117
01:45:10,800 --> 01:45:16,880
hard in a world where, you know, you imagine sufficiently powerful systems that are just put

1118
01:45:16,880 --> 01:45:24,160
out totally, you know, bare into the world, like, is there any way to square the all those, I think,

1119
01:45:24,160 --> 01:45:30,240
very legitimate open source motivations with the sort of safety paradigm that you're trying to develop?

1120
01:45:31,120 --> 01:45:35,600
Potentially. So, you know, like, I think the open source debate is fairly heated. But, you know,

1121
01:45:35,600 --> 01:45:40,560
to me, there are two things that are kind of obviously true. Number one, open source has been

1122
01:45:40,560 --> 01:45:44,800
really good so far in many, many ways. It has been very positive for society, right? I think a lot

1123
01:45:44,800 --> 01:45:48,800
of ML research could not have happened without open source. A lot of safety research could not

1124
01:45:48,800 --> 01:45:53,920
have happened with open source. And the other thing that is also true, or at least seems true

1125
01:45:54,000 --> 01:45:58,880
to me, is there's a limit of open source, right? Like, at some point, the system is so powerful

1126
01:45:58,880 --> 01:46:02,720
that you don't want it to be open source anymore, in the same way in which, you know, I don't want to

1127
01:46:02,720 --> 01:46:08,480
open source, like the nuclear codes, or something to start or like, you know, literally the recipe

1128
01:46:08,480 --> 01:46:14,320
to build like the most, most viral, you know, most viral pandemic or something. This is just

1129
01:46:14,320 --> 01:46:21,040
something where, you know, only one person needs to needs to have bad intentions to already have

1130
01:46:21,040 --> 01:46:25,600
like really, to already cause really big problems. So at some point that there's, there's just a

1131
01:46:25,600 --> 01:46:31,440
balance where you just, I think, cannot really justify giving people literally everyone access

1132
01:46:31,440 --> 01:46:37,040
to this. And so the question for me really is where, so number one, where are we on the spectrum

1133
01:46:37,040 --> 01:46:42,880
right now of like, open source has been really good to, are we already a point at like, how close

1134
01:46:42,880 --> 01:46:46,960
are we to the point where it really cannot be justified anymore? Some people would say even

1135
01:46:46,960 --> 01:46:52,320
GBD3, you know, through GBD3 size models are already too, too scary, which I'm not sure about,

1136
01:46:52,320 --> 01:46:57,280
I'm not even sure whether GBD4 size models are like too big to be open source, but I would rather

1137
01:46:57,280 --> 01:47:01,840
err on the side of caution and be a little bit more conservative here, because of the, the nature

1138
01:47:01,840 --> 01:47:07,200
of open sourcing, where you can really not take this back. So as soon as you make a mistake, you

1139
01:47:07,200 --> 01:47:11,280
like, you're stuck with a mistake for a long time, or like forever, you cannot, you cannot turn it,

1140
01:47:11,280 --> 01:47:18,000
take it back. And I also think this will, this will also influence how, how open source will be

1141
01:47:18,000 --> 01:47:24,080
handled in the real world in practice. Like, I think there will be something like initial releases

1142
01:47:24,080 --> 01:47:28,240
for open source, where lots of people test it, like, basically think there will also be staggered

1143
01:47:28,240 --> 01:47:34,000
and stage releases, right? It's, first, there's like a small team of trusted researchers who

1144
01:47:34,000 --> 01:47:38,880
is allowed to play with the open source model, and like really test the limits, really test

1145
01:47:38,880 --> 01:47:42,960
how bad could I get the model, how easy is it to remove all of the guardrails,

1146
01:47:42,960 --> 01:47:47,200
which, you know, like, it's an open source model, if you can find, you can remove the guardrails.

1147
01:47:47,200 --> 01:47:49,600
Yeah, it turns out pretty easy from what we've seen so far.

1148
01:47:50,480 --> 01:47:54,480
Ones like, you know, the kind of the upper bounds are known of how bad could this become.

1149
01:47:55,600 --> 01:48:00,080
Maybe it makes sense to, to like open source it to more people. But yeah, I would basically say,

1150
01:48:00,080 --> 01:48:04,800
you know, the upper bound can be quite high, especially with all of the stuff I said earlier

1151
01:48:04,800 --> 01:48:09,520
about, you know, absolute capabilities and reachable capabilities and so on, right? Like,

1152
01:48:09,520 --> 01:48:15,520
maybe you can, maybe you cannot get it to build an automated hacking bot, if you only have access

1153
01:48:15,520 --> 01:48:19,360
to the weights, but maybe if you do scaffolding on top and some fine tuning and access to some

1154
01:48:19,360 --> 01:48:24,240
other thing, maybe then it can build it, right? And this is like very hard to predict in advance.

1155
01:48:24,240 --> 01:48:30,000
So the more and more powerful the models become, I think the less plausible a priority is to open

1156
01:48:30,000 --> 01:48:34,160
source them. Even though, and like, this is really something I want to emphasize, right? Like,

1157
01:48:34,160 --> 01:48:40,320
open source has been extremely good so far. And I really think there's sort of this tipping point

1158
01:48:40,320 --> 01:48:48,640
that is like, at some point, it just becomes too hard to like, it becomes impossible, it always

1159
01:48:48,640 --> 01:48:53,120
will be impossible to take back, but at some point, it just becomes too dangerous to literally trust

1160
01:48:53,120 --> 01:48:59,760
everyone with, with this level of capabilities. Threshold effects. That's, I think, one of the

1161
01:48:59,760 --> 01:49:05,920
most powerful paradigms that I've, you know, consistently come back to over the last couple

1162
01:49:05,920 --> 01:49:11,680
years, just crossing these thresholds from one regime into another, whether it's capabilities or,

1163
01:49:11,680 --> 01:49:17,760
you know, risks, it just constantly seems like we're flipping from one mode or one kind of,

1164
01:49:18,880 --> 01:49:25,040
you know, one regime to another and got to be very alert to when that happens because it can

1165
01:49:25,040 --> 01:49:30,720
really change, you know, important analysis in pretty profound ways. So I don't know where exactly

1166
01:49:30,720 --> 01:49:36,160
that threshold is either by any means, but it, and I would agree that like, for everything that I

1167
01:49:36,160 --> 01:49:44,320
have seen suggests that up to and including the release of Lama 2 has been, you know, very, very

1168
01:49:44,320 --> 01:49:50,000
much an enabler for all sorts of things. But certainly, you know, plenty of safety related work

1169
01:49:50,000 --> 01:49:57,680
done on that model and, you know, seems, seems like the effect so far has been good. But yeah,

1170
01:49:57,680 --> 01:50:01,840
is that still true for Lama 3? Is it true for Lama 4? You know, obviously we don't even know

1171
01:50:01,840 --> 01:50:07,120
what these things are, but it certainly starts to be a very live question.

1172
01:50:07,840 --> 01:50:13,760
You know, the, the like leading AGI labs, I think it is very clear that they understand the problem,

1173
01:50:13,760 --> 01:50:20,880
that they have internal processes that are, you know, like, maybe better than you would expect

1174
01:50:20,880 --> 01:50:28,240
from the normal, from a normal company. They actually care about trying to do good with

1175
01:50:28,240 --> 01:50:32,720
with remodels and they're like very explicitly trying. And then, you know, there's the other

1176
01:50:32,720 --> 01:50:37,200
side of the coin, which is they still have incentives, and they, you know, they can be

1177
01:50:37,200 --> 01:50:42,480
financial incentives, they can be sort of maybe more psychological and social incentives, you

1178
01:50:42,480 --> 01:50:47,280
know, that they just want to be the first to develop AGI, because it's like probably like a

1179
01:50:47,280 --> 01:50:52,160
history defining technology, or maybe even galaxy defining technology or something like this, right?

1180
01:50:52,160 --> 01:50:57,120
And, and so the question really, I think at this, you know, even if you could say, you know,

1181
01:50:57,120 --> 01:51:04,240
like the compared to a normal company, these, the processes are astonishingly reasonable,

1182
01:51:04,240 --> 01:51:07,760
and surprisingly good. If, you know, if you compare to literally any other

1183
01:51:08,560 --> 01:51:14,480
industry, it would be surprising if they have this, this amount of like self regulation and so on.

1184
01:51:14,480 --> 01:51:17,840
And then on the other hand, the question, they're, you know, they're still the bigger question of

1185
01:51:18,560 --> 01:51:24,000
how hard is alignment going to be, how fast are going to take us going to be and so on and like,

1186
01:51:24,000 --> 01:51:28,160
in a bad world, alignment is going to be quite hard and take us are going to be quite fast and

1187
01:51:28,160 --> 01:51:35,040
controllable. And then the question is, you know, it is like the level of control and alignment,

1188
01:51:35,760 --> 01:51:40,960
and like safety concern enough from these leaders. And there I'm like, less sure. So I feel like,

1189
01:51:42,400 --> 01:51:47,360
yeah, it's, it's, it's definitely in like, it's, I think from my perspective, right? It's fair to say

1190
01:51:48,320 --> 01:51:51,760
they're like pretty reasonable compared to the alternatives that we could have had.

1191
01:51:52,800 --> 01:51:58,400
But also, it's insufficient in almost all ways, right? Government needs to be involved in this.

1192
01:51:59,360 --> 01:52:04,480
They cannot externalize the risk. There are many things that they're already doing

1193
01:52:04,480 --> 01:52:09,600
insufficiently well, I think, where they could have done way better, both with like how they release

1194
01:52:09,600 --> 01:52:15,280
as well as how they react to, to like problems, as well as, you know, like how they communicate

1195
01:52:15,280 --> 01:52:19,680
with the public about the risks that they're creating, etc., etc. So yeah, I think there,

1196
01:52:19,680 --> 01:52:25,920
there's a lot of room for improvement as well. And yeah, I really, I really think that, you know,

1197
01:52:25,920 --> 01:52:30,960
AI safety is going to be a very, very hard problem. A lot of things have to have to go right for the

1198
01:52:30,960 --> 01:52:35,840
whole system to go right. And we definitely cannot just trust the labs, despite the best

1199
01:52:35,840 --> 01:52:42,960
intentions to just solve it all on their own. Yeah, well, hence the, the need for third party

1200
01:52:42,960 --> 01:52:47,920
auditing and the organization that you're building at Apollo Research, maybe just one last question.

1201
01:52:47,920 --> 01:52:53,440
A number of people have reached out to me and said, I would like to get involved with red teaming.

1202
01:52:54,320 --> 01:53:00,960
How can I do that? I wonder if you have any advice for individuals who might just want to

1203
01:53:01,680 --> 01:53:07,360
do their own projects and, you know, release stuff, you know, just share findings individually with

1204
01:53:07,360 --> 01:53:15,360
the world, or perhaps and or perhaps, you know, what sort of skills are you in need of, as you're

1205
01:53:15,360 --> 01:53:20,640
going about building your own organization? I think one thing that is nice about model evaluations

1206
01:53:20,720 --> 01:53:25,040
and red teaming is you can just kind of start right away. You don't need that much, you know,

1207
01:53:25,040 --> 01:53:31,520
technical expertise, because it's all in, like almost all of it is in text. And, you know, at

1208
01:53:31,520 --> 01:53:37,440
least if you, if you want to, to redeem a language model specifically. And yeah, so my recommendation

1209
01:53:37,440 --> 01:53:44,240
for individuals, first of all, would be to just start, like just engage with a model for, you

1210
01:53:44,240 --> 01:53:48,640
know, a long, a longer period of time, maybe a day or so and see if whether you find interesting

1211
01:53:48,720 --> 01:53:52,560
behavior or maybe, you know, maybe there's someone who has already done something, and maybe you

1212
01:53:52,560 --> 01:53:57,680
can poach a project from them, and, you know, just sort of as a starter thing. And from this,

1213
01:53:57,680 --> 01:54:01,760
I feel like it kind of just takes a life of its own anyway, you know, as soon as you're hooked

1214
01:54:01,760 --> 01:54:06,720
on a specific thing that you find interesting, you will, you know, you will really try to find

1215
01:54:07,360 --> 01:54:12,000
additional ways in which this specific behavior could happen. In the same way, you know, let's,

1216
01:54:12,000 --> 01:54:16,400
let's take the deception thing, right? We, we started fairly exploratory and wanted to try

1217
01:54:16,400 --> 01:54:20,080
how far we can get the model to be, to be deceptive. And then at some point, it just

1218
01:54:20,080 --> 01:54:24,720
took a life of its own where we're like, okay, but like, why really does it do that? Right? Like,

1219
01:54:24,720 --> 01:54:29,280
okay, we vary this behavior and like this thing and this, this variable in the environment,

1220
01:54:29,280 --> 01:54:34,000
we vary this thing, we vary all of these other things. And in the end, you can, again,

1221
01:54:34,000 --> 01:54:37,440
you kind of get like a more holistic and round picture of what's going on. So yeah,

1222
01:54:37,440 --> 01:54:42,080
I definitely think just start with like a thing you find interesting is definitely the way to go.

1223
01:54:43,040 --> 01:54:49,120
And like don't overthink, overthink it originally. And then the thing we are specifically looking

1224
01:54:49,120 --> 01:54:55,120
for, you know, so definitely kind of this mindset of like, oh, I just want to poke around and like

1225
01:54:55,120 --> 01:54:59,520
really try to understand what's going on in a fairly like scientific manner, right? I also want

1226
01:54:59,520 --> 01:55:03,680
to make sure that all of the confounders that could potentially explain this behavior have been

1227
01:55:03,680 --> 01:55:08,480
controlled for, which I think is the hard part in red teaming. So this is definitely something

1228
01:55:08,480 --> 01:55:13,280
we're looking for. And then just, you know, the more you understand language models and state of

1229
01:55:13,280 --> 01:55:18,640
the art models, the easier it will become, right? Some behavior might be very easily explainable

1230
01:55:18,640 --> 01:55:23,680
by sort of problems with RLHF. So if you know how RLHF works and specifically how it was trained,

1231
01:55:24,720 --> 01:55:28,720
you may probably you will probably understand the red teaming efforts much better.

1232
01:55:29,680 --> 01:55:33,920
If you know, you know, like if you have a better understanding of how the instruction

1233
01:55:33,920 --> 01:55:38,640
fine tuning actually works, maybe you will find those. So, you know, so for example,

1234
01:55:38,640 --> 01:55:43,280
maybe to give it to give like an intuition here, in the in our case, right, as I said earlier,

1235
01:55:43,280 --> 01:55:48,800
we have the the three HS and then instruction fine tuning, and you can, you know, trade off the

1236
01:55:48,800 --> 01:55:54,640
different components against each other to find different things. I like to find niches of the

1237
01:55:54,640 --> 01:55:59,520
model where it acts in ways that we think it shouldn't act, because maybe they weren't covered

1238
01:55:59,520 --> 01:56:05,600
explicitly or implicitly by by gradient descent. And if you if you sort of have a theoretical

1239
01:56:05,600 --> 01:56:09,760
framework like this, it suddenly becomes much easier on how to do the red teaming in the first

1240
01:56:09,760 --> 01:56:15,040
place. So like some theoretical understanding of how the process works is definitely helpful as

1241
01:56:15,040 --> 01:56:21,120
well for a teaming and also something we're actively looking for. Marius Havan, founder and

1242
01:56:21,120 --> 01:56:26,720
CEO of Apollo Research. Thank you for being part of the cognitive revolution. Thanks for inviting

1243
01:56:27,440 --> 01:56:32,320
it is both energizing and enlightening to hear why people listen and learn what they value about

1244
01:56:32,320 --> 01:56:39,760
the show. So please don't hesitate to reach out via email at TCR at turpentine.co, or you can DM me

1245
01:56:39,760 --> 01:56:46,320
on the social media platform of your choice. Omniki uses generative AI to enable you to launch

1246
01:56:46,320 --> 01:56:51,680
hundreds of thousands of ad iterations that actually work customized across all platforms

1247
01:56:51,680 --> 01:56:56,400
with a click of a button. I believe in Omniki so much that I invested in it. And I recommend

1248
01:56:56,400 --> 01:57:04,080
you use it too. Use CogGrav to get a 10% discount.

