1
00:00:00,000 --> 00:00:06,880
Moore's Law was turning into this acceleration of AI capabilities over the top and the other was that

2
00:00:06,880 --> 00:00:10,800
what was happening in renewables with things like lithium-ion batteries and solar

3
00:00:10,800 --> 00:00:16,720
was on a Moore's Law like trajectory and there was some other technologies like genome sequencing

4
00:00:16,720 --> 00:00:21,280
and genome synthesis that seemed to be doing something similar. So I started to bundle those

5
00:00:21,280 --> 00:00:27,280
up in this idea that we're going through the exponential transition, a transition where

6
00:00:27,760 --> 00:00:33,520
our economy gets driven you know not by the economics of the oil industry and internal

7
00:00:33,520 --> 00:00:40,720
combustion engines and and telephones but by AI and renewables and that these technologies

8
00:00:41,440 --> 00:00:47,600
being on an exponential trend being fundamentally at some level kind of information technologies

9
00:00:47,600 --> 00:00:51,120
behave really differently to the ones of the previous generation.

10
00:00:51,120 --> 00:00:55,760
Hello and welcome to The Cognitive Revolution where we interview visionary researchers,

11
00:00:55,760 --> 00:00:59,600
entrepreneurs and builders working on the frontier of artificial intelligence.

12
00:01:00,320 --> 00:01:05,200
Each week we'll explore their revolutionary ideas and together we'll build a picture of how AI

13
00:01:05,200 --> 00:01:11,280
technology will transform work, life and society in the coming years. I'm Nathan LaBenz,

14
00:01:11,280 --> 00:01:16,160
joined by my co-host Eric Torenberg. Hello and welcome back to The Cognitive Revolution.

15
00:01:16,800 --> 00:01:21,920
Today I'm speaking with Azim Azhar, founder of The Exponential View and fellow AI scout.

16
00:01:22,000 --> 00:01:25,120
For a wide-ranging discussion about the transformative power of AI

17
00:01:25,120 --> 00:01:30,800
and its implications for humanity. Azim's core observations that we are in the midst of a

18
00:01:30,800 --> 00:01:36,880
transition from an economy driven by the likes of phones and oil to one fueled by AI and renewables

19
00:01:36,880 --> 00:01:42,320
and that these new technologies fundamentally rooted in information and already growing exponentially

20
00:01:42,320 --> 00:01:47,440
behave differently than their predecessors will be familiar to cognitive revolution listeners.

21
00:01:48,160 --> 00:01:52,160
So I took the opportunity to get a bit deeper into Azim's worldview and expectations for the

22
00:01:52,160 --> 00:01:56,800
coming years and ended up having what I think was a really engaging and delightful exchange.

23
00:01:58,000 --> 00:02:01,680
In this conversation we cover a number of familiar topics plus some new ones that we

24
00:02:01,680 --> 00:02:07,280
haven't explored in quite the same way before, including why Azim believes that while incumbents

25
00:02:07,280 --> 00:02:12,800
are racing to adopt AI technology, yes startups are still likely to drive the true disruption

26
00:02:12,800 --> 00:02:18,160
in the form of entirely new products and markets. Also what forward thinking business

27
00:02:18,160 --> 00:02:22,080
leaders are doing today to retrain their teams and to position their companies to lead their

28
00:02:22,080 --> 00:02:27,440
respective markets in AI adoption and why Azim still finds it necessary to challenge them to

29
00:02:27,440 --> 00:02:32,640
think bigger, asking the question what would they do if they had a million times more compute.

30
00:02:34,240 --> 00:02:39,840
We also explore why Azim agrees with Sam Altman's recent comments that AGI will likely arrive soon

31
00:02:39,840 --> 00:02:45,280
but will be less of a big deal than people expect, at least initially. How AI is likely to change

32
00:02:45,280 --> 00:02:50,160
human relationships, especially considering the rise of so many different forms of AI companions,

33
00:02:51,200 --> 00:02:56,320
what sorts of new governance mechanisms the AI era will require, and finally why we might

34
00:02:56,320 --> 00:03:01,600
ought to worry about what I call the great embedding, that is the seemingly natural tendency

35
00:03:01,600 --> 00:03:07,040
for AI systems to communicate with each other in high dimensional vector formats, which while

36
00:03:07,040 --> 00:03:12,800
efficient for them is broadly inscrutable to humans and could lead to various loss of control

37
00:03:12,800 --> 00:03:18,160
scenarios. As always, if you're finding value in the show, we'd appreciate it if you'd take

38
00:03:18,160 --> 00:03:23,520
a moment to share it with your friends. Azim's perspective is both imaginative and exciting

39
00:03:23,520 --> 00:03:28,400
and also grounded and sobering, so I definitely recommend this episode to anyone trying to get

40
00:03:28,400 --> 00:03:32,960
a better zoomed out view of the future. I would also encourage you to consider subscribing to

41
00:03:32,960 --> 00:03:39,040
Azim's work directly, which you can find at exponentialview.co. Of course, your feedback

42
00:03:39,040 --> 00:03:44,800
is always welcome, whether in the form of an Apple or Spotify review, a YouTube comment, or a DM

43
00:03:44,800 --> 00:03:49,680
on the social media platform of your choice. Now, I hope you enjoy this conversation with

44
00:03:49,680 --> 00:03:56,560
founder of the exponential view, Azim Azhar. Azim Azhar, founder of the exponential view,

45
00:03:56,560 --> 00:04:01,680
welcome to the cognitive revolution. I'm really happy to be here, Nathan. I've enjoyed so many

46
00:04:01,680 --> 00:04:09,360
of the episodes and want to thank you for your hard service as a red teamer for GPT4. I watched

47
00:04:09,360 --> 00:04:14,960
that episode with my jaw on the floor. Well, thank you very much. That's kind and right back at you.

48
00:04:14,960 --> 00:04:21,120
I've been binging your feed lately and we've got a ton of things to talk about. I guess for starters,

49
00:04:21,120 --> 00:04:28,240
I would love to get your kind of summary of what it is you do. I think it's actually kind of similar

50
00:04:28,240 --> 00:04:33,920
to what I'm trying to do. I describe myself, as you know, as an AI scout. And I talk to mostly

51
00:04:33,920 --> 00:04:39,040
people on the show who are either very deep on a particular line of research or building a product,

52
00:04:39,600 --> 00:04:45,360
but you are one of the few guests who have this kind of very zoomed out view and really are working

53
00:04:45,360 --> 00:04:50,320
to understand the big picture. So how do you describe what you do and what goes into it?

54
00:04:50,320 --> 00:04:54,560
I'd love to, yeah. I mean, I've been in the tech industry for a really long time. I started working

55
00:04:54,560 --> 00:05:01,840
in 94 and I built my first websites in 93. And just over the back behind me, you can see in

56
00:05:01,840 --> 00:05:08,640
out of focus my first computer, which is a ZX81 Timex 1000 in the US. And about nine years ago,

57
00:05:08,640 --> 00:05:15,120
my last company was acquired and I just started to write a newsletter. And as you do, writing is

58
00:05:15,120 --> 00:05:19,840
thinking. And I noticed within a few months, there were these few trends that were going on

59
00:05:19,840 --> 00:05:25,440
that were pretty significant. One was, you know, Moore's Law was turning into this acceleration

60
00:05:25,440 --> 00:05:30,960
of AI capabilities over the top. And the other was that what was happening in renewables with

61
00:05:30,960 --> 00:05:36,880
things like lithium ion batteries and solar was on a Moore's Law like trajectory. And there was

62
00:05:36,880 --> 00:05:40,880
some other technologies like genome sequencing and genome synthesis that seemed to be doing

63
00:05:40,880 --> 00:05:47,120
something similar. So I started to bundle those up in this idea that we're going through the

64
00:05:47,200 --> 00:05:54,400
exponential transition, a transition where our economy gets driven, you know, not by the economics

65
00:05:54,400 --> 00:06:02,240
of the oil industry and internal combustion engines and telephones, but by AI and renewables and

66
00:06:02,240 --> 00:06:08,720
that these technologies being on an exponential trend, being fundamentally at some level kind

67
00:06:08,720 --> 00:06:14,080
of information technologies behave really differently to the ones of the previous generation.

68
00:06:14,080 --> 00:06:21,280
And so what I do now is I try to make sense of that as a system. I do that through my newsletter,

69
00:06:21,280 --> 00:06:27,040
but I also do it from time to time through investing and advising. But I think that what is

70
00:06:28,000 --> 00:06:34,160
going on at its heart is in about 20 or 30 years, we'll be looking at an energy system

71
00:06:34,160 --> 00:06:40,080
that is going to be very, very heavily renewable. We'll be using much more energy per capita globally

72
00:06:40,080 --> 00:06:45,920
than we do today, that we will have tons of intelligence in our economies and in our lives

73
00:06:45,920 --> 00:06:51,200
through through AI. And that will have real knock on effects and trying to make sense of that in a

74
00:06:51,200 --> 00:06:58,880
pragmatically optimistic way. So somewhere between the extreme dystopia and the extreme utopia is

75
00:06:58,880 --> 00:07:05,680
really my mission. Yeah, cool. Well, I'm hoping we can find that sweet spot as well. So right there

76
00:07:05,680 --> 00:07:11,520
with you. In terms of the history of the kind of intellectual history of this notion of exponential

77
00:07:11,520 --> 00:07:17,520
technologies, nine years ago strikes me as kind of a doldrum time for that paradigm. I wonder if

78
00:07:17,520 --> 00:07:23,840
you experienced this, but I recently had a conversation with a guy who makes his living as a

79
00:07:23,840 --> 00:07:29,680
speaker at corporate events and who is positioned as a futurist. And I showed him a little presentation

80
00:07:29,760 --> 00:07:35,840
that I had put together in which I pulled out one of Kurzweil's kind of late 90s exponential

81
00:07:35,840 --> 00:07:40,720
curves graphs. And the title of that slide in my presentation was Kurzweil was right.

82
00:07:41,440 --> 00:07:46,640
He saw the name Kurzweil and he was like, Oh God, don't don't talk about Kurzweil. No,

83
00:07:46,640 --> 00:07:51,520
everybody he's totally discredited. And I was kind of like, hmm, that sounds like somebody who,

84
00:07:51,520 --> 00:07:56,640
you know, maybe got some pitches dinged in that kind of time frame when you were getting started

85
00:07:56,640 --> 00:08:03,360
with this notion and has gone away from it. But you know, maybe that's just the weird nature of

86
00:08:03,360 --> 00:08:08,320
exponentials. What was your kind of experience of, you know, were people sort of sour on that?

87
00:08:08,320 --> 00:08:11,840
It seemed like, you know, there was the great stagnation thesis for a while there and your

88
00:08:12,720 --> 00:08:17,520
start of the exponential kind of lines up with it. It seems like when I started and one, what I

89
00:08:17,520 --> 00:08:24,960
noticed was people weren't really talking about AI. I mean, it was 2014. Machine learning was still

90
00:08:25,040 --> 00:08:31,680
the word. It was before AlphaGo had come out and done its thing. DeepMind was doing really

91
00:08:31,680 --> 00:08:36,480
interesting things with reinforcement learning and video games. And you could see that something

92
00:08:36,480 --> 00:08:43,040
was happening. And I think that you had TensorFlow as the sort of stack of choice for building

93
00:08:43,040 --> 00:08:49,760
convolutional neural networks to do their machine vision tasks. So it seemed, it seemed reasonably

94
00:08:49,760 --> 00:08:54,240
early to be talking about, about these things. And actually developers were struggling to make

95
00:08:54,240 --> 00:08:59,840
sense of CUDA, which is the sort of API to the Nvidia GPUs that everybody now uses, because

96
00:08:59,840 --> 00:09:04,000
that had only been documented five years earlier. There is something that happens around 2013,

97
00:09:04,000 --> 00:09:11,520
2014, 2015 that I think is really worth paying attention to, which is that in 2013, Apple becomes

98
00:09:11,520 --> 00:09:17,600
the largest company in the world. And within a couple of years, those top slots are occupied by

99
00:09:17,600 --> 00:09:23,120
what we used to call the fangs, Facebook, Apple, Google, and Amazon, or the gaffers,

100
00:09:23,120 --> 00:09:29,120
pardon me, not the fangs. Netflix snuck in briefly during the hype cycle. And so you started to see

101
00:09:29,120 --> 00:09:35,600
the, the companies of the industrial age, Exxon, GM, and so on, fall off that, fall off that list

102
00:09:35,600 --> 00:09:40,880
and stay off that list. So that's an important economic moment about the, the sort of forward

103
00:09:40,880 --> 00:09:45,360
looking stock market saying like something is changing. The second thing that starts to happen

104
00:09:45,360 --> 00:09:51,920
in around 2014 is we see the first market for electric vehicles go past that threshold of 5%

105
00:09:51,920 --> 00:09:57,680
of new cars being sold, being electric, which was over in, in Norway. And that 5% threshold is the

106
00:09:58,400 --> 00:10:02,960
normally the trigger for when you see the S curve of adoption, right? And you get into that, that

107
00:10:02,960 --> 00:10:10,000
vertical or that verticalized part of the curve. You also started to see solar power being starting

108
00:10:10,000 --> 00:10:15,040
to be cheaper in a roughly a third to a half of the contracts around the world than fossil fuels.

109
00:10:15,200 --> 00:10:23,360
And, and then of course we are three or four years into the deep learning wave. And, and that's long

110
00:10:23,360 --> 00:10:30,000
enough for companies to start to ship products. So I think that there is a moment which we could

111
00:10:30,000 --> 00:10:35,440
argue when we look back on it feels like a, a sort of historical turning point, but we also have to

112
00:10:35,440 --> 00:10:41,280
be realistic that the, the mathematical function that is an exponential curve, when you stand on it,

113
00:10:41,280 --> 00:10:45,200
it always looks horizontal behind you and it always looks vertical above in front of you,

114
00:10:45,200 --> 00:10:49,760
wherever you stand on it. It's a smooth curve with no obvious turning point. And, and someone like

115
00:10:49,760 --> 00:10:56,080
Kurt's file, you know, I think did such great work 20 years ago to articulate the exponential

116
00:10:56,080 --> 00:11:01,360
trend in computing going back from the 1880s actually in mechanical computers. I think one of

117
00:11:01,360 --> 00:11:11,200
the reasons why he slightly falls out of favor is because he was probably brave enough to extend

118
00:11:11,280 --> 00:11:17,200
the curve far further than we might have otherwise thought. And I think a couple of things that he

119
00:11:17,200 --> 00:11:21,920
got wrong was that some of the assumptions that we would have had about how the human brain works

120
00:11:21,920 --> 00:11:28,080
in like 2001, 2002 when those books came out were, were wrong, right? Science proved that it was more

121
00:11:28,080 --> 00:11:34,560
complex and it wasn't going to be a simple game of, of raw computation one for one. And, and that's

122
00:11:34,560 --> 00:11:38,480
where I think that, that people start to look at him and say, well, was this just someone throwing

123
00:11:38,480 --> 00:11:42,320
tea leaves? I think there was much more to it than, than that having read his work, you know,

124
00:11:42,320 --> 00:11:47,840
reasonably carefully. But, but I think what's interesting is that we see these curves happening

125
00:11:47,840 --> 00:11:51,280
elsewhere, right? We see them in lithium ion batteries and we see them in genome sequencing

126
00:11:51,280 --> 00:11:56,800
and it's not clear why that should be the case up front. The implications of the fact that we may

127
00:11:56,800 --> 00:12:01,440
still be, you know, standing on the part of the curve that as it, you know, as you said, it kind

128
00:12:01,440 --> 00:12:05,680
of always does. If we're, if it's still the case that what's in front of us is vertical compared

129
00:12:05,680 --> 00:12:09,840
to what, you know, has been behind us being horizontal, then we're in for a bit of a wild

130
00:12:09,840 --> 00:12:14,640
ride. I think we're headed for steep, a steep curve for at least a couple more years. I mean,

131
00:12:15,200 --> 00:12:22,240
beyond that, you know, do we sort of hit a plateau is a lot harder for me to predict. But I honestly

132
00:12:22,240 --> 00:12:27,200
don't see any fundamental reasons that we would right now. I mean, what, what Kurzweil says is

133
00:12:27,200 --> 00:12:32,720
that, you know, it's actually this curve is a series of layered S's. So you, you have one

134
00:12:32,720 --> 00:12:39,040
particular technology architecture, it's very slow to develop, it hits this inflection as an S,

135
00:12:39,040 --> 00:12:44,960
it starts to accelerate. And as it hits its flat point, the social dynamics of market incentives

136
00:12:44,960 --> 00:12:50,000
have meant that another set of research has come in with a different architecture, a different way

137
00:12:50,000 --> 00:12:56,640
that extends that that S up and looks from a distance like a smooth S curve. And that's

138
00:12:56,640 --> 00:13:00,720
really nice and descriptive. But it also, I suspect people are kind of really robust with

139
00:13:00,800 --> 00:13:04,800
their theory feels like, well, that's just praying. That's like the Turkey that's been

140
00:13:04,800 --> 00:13:09,040
treated really well up to the day before Thanksgiving. And like, we should worry about

141
00:13:09,040 --> 00:13:13,920
what happens the next day. What I've tried to do is I've tried to get into the underlying mechanisms

142
00:13:13,920 --> 00:13:19,520
of why these things improve, why they get cheaper. And then what we need to do is figure out where

143
00:13:19,520 --> 00:13:25,920
does that mechanism fail? Because if the mechanism doesn't fail, then that trend is going to continue.

144
00:13:25,920 --> 00:13:30,080
And if it does fail, then we can say, well, we need a new mechanism and is there one,

145
00:13:30,640 --> 00:13:36,000
you know, in the research pipeline that might deliver it. So, I would say that if you look

146
00:13:36,000 --> 00:13:43,200
across the gamut, I mean, for example, batteries and solar power, we've definitely got more than a

147
00:13:43,200 --> 00:13:49,760
couple of years to run in terms of price declines. When we look at compute, I just feel it's really

148
00:13:49,760 --> 00:13:54,160
hard to bet against. I just, you know, I think that I've been hearing about the death of Moore's

149
00:13:54,160 --> 00:14:01,680
Law for 15 years. And, you know, Moore's Law is helpful. But the question to ask is how much

150
00:14:01,680 --> 00:14:09,120
compute can a developer get for a dollar each year? And do we really think that that is going to stop

151
00:14:09,840 --> 00:14:13,520
declining for like a long period of time? And I find that one really hard to

152
00:14:14,560 --> 00:14:17,840
support. And I just think it continues for a variety of reasons.

153
00:14:17,840 --> 00:14:24,160
It sure seems like it. I mean, the CPU to GPU transition feels like a classic example of one

154
00:14:24,160 --> 00:14:31,120
of those kind of one S-curve, perhaps giving way to another. And in the GPU, you know, we're not,

155
00:14:31,120 --> 00:14:34,880
it definitely feels like we're still in the steep part of that particular S-curve.

156
00:14:34,880 --> 00:14:38,240
Hey, we'll continue our interview in a moment after a word from our sponsors.

157
00:14:39,120 --> 00:14:43,200
The Brave Search API brings affordable developer access to the Brave Search Index,

158
00:14:43,280 --> 00:14:46,320
an independent index of the web with over 20 billion web pages.

159
00:14:46,960 --> 00:14:52,400
So what makes the Brave Search Index stand out? One, it's entirely independent and built from

160
00:14:52,400 --> 00:14:59,040
scratch. That means no big tech biases or extortionate prices. Two, it's built on real page

161
00:14:59,040 --> 00:15:04,560
visits from actual humans, collected anonymously of course, which filters out tons of junk data.

162
00:15:05,280 --> 00:15:10,000
And three, the index is refreshed with tens of millions of pages daily. So it always has

163
00:15:10,000 --> 00:15:15,920
accurate up to date information. The Brave Search API can be used to assemble a data set to train

164
00:15:15,920 --> 00:15:21,280
your AI models and help with retrieval augmentation at the time of inference, all while remaining

165
00:15:21,280 --> 00:15:27,360
affordable with developer first pricing. Integrating the Brave Search API into your workflow translates

166
00:15:27,360 --> 00:15:32,560
to more ethical data sourcing and more human representative data sets. Try the Brave Search

167
00:15:32,560 --> 00:15:37,760
API for free for up to 2,000 queries per month at brave.com slash api.

168
00:15:43,520 --> 00:15:49,360
We're not done yet. And I think the thing that is fascinating is that NVIDIA is obviously doing

169
00:15:50,160 --> 00:15:57,760
incredibly, incredibly well. And it doesn't yet have the threat of real competition.

170
00:15:58,000 --> 00:16:02,560
And what was fascinating in the CPU world was that Intel did very, very well for a really long

171
00:16:02,560 --> 00:16:06,560
time. I guess people forget this, but if you've been around for a while, you remember Intel was

172
00:16:06,560 --> 00:16:11,360
this sort of monopolist and perceived an Andy Grove and only the paranoid survive.

173
00:16:12,000 --> 00:16:16,640
And it did really well only with the threat of competition, because AMD never got more than

174
00:16:16,640 --> 00:16:23,440
15, 20 percent market share. And that's enough to propel people forward. All the incentives seem

175
00:16:23,440 --> 00:16:30,880
lined up for there to be massive amounts of investment in scaling existing silicon chips

176
00:16:30,880 --> 00:16:35,840
and developing new systems. I mean, you saw in the last few days before we recorded this,

177
00:16:35,840 --> 00:16:41,440
Amazon and Google both reported 20, 30 percent growth in their cloud businesses. When I've talked

178
00:16:41,440 --> 00:16:48,160
to bosses of really big companies, you know, they are spending money on compute, you know,

179
00:16:48,160 --> 00:16:52,480
really like nobody's business and their expectation is that it will grow. They don't

180
00:16:52,480 --> 00:16:56,320
often think it's compute. They say they're spending on AI, but that the end of it is going to be

181
00:16:56,320 --> 00:17:04,080
GPUs cranking away. So with all the incentives aligned, I struggle to see us hitting a brick wall.

182
00:17:04,080 --> 00:17:09,040
It doesn't feel like it's the Carno cycle, right? So the Carno cycle was the thermodynamic limit for

183
00:17:09,040 --> 00:17:14,800
the efficiency of an Intel combustion engine, something you as a native Detroit man, you know,

184
00:17:14,800 --> 00:17:21,280
know, know very, very well, but we keep finding ways of eking more out of our compute. And I think

185
00:17:21,360 --> 00:17:24,400
we'll, you know, we'll continue to do that certainly beyond a couple of years.

186
00:17:25,200 --> 00:17:29,600
Just for kind of conceptual grounding, and I'm also interested to hear how you explain this to

187
00:17:29,600 --> 00:17:35,920
the business leaders that you work with, because their understanding and their kind of eagerness

188
00:17:35,920 --> 00:17:41,520
to adopt is a pretty key question in my mind as to how the next few years are going to play out.

189
00:17:42,160 --> 00:17:47,920
But we have kind of two notions, two definitions of kind of types of technology

190
00:17:47,920 --> 00:17:52,160
that both seem to apply to AI in my mind. One is the exponential technology,

191
00:17:52,800 --> 00:17:58,080
and the other is the concept of disruptive technology. Disruptive technology, you know,

192
00:17:58,080 --> 00:18:05,280
kind of classic textbook definition is a cheaper, but inferior alternative that kind of competes

193
00:18:05,280 --> 00:18:10,960
on the low end of the market. It seems to me that AI is kind of both, right? It's we've got like

194
00:18:10,960 --> 00:18:14,800
these, at least exponentially growing inputs. Although on the other hand, you could sort of

195
00:18:14,800 --> 00:18:20,320
say scaling laws sort of suggest that like the model capabilities are more like logarithmic,

196
00:18:20,320 --> 00:18:24,800
so those two things maybe like balance out somehow. It does seem like it's disruptive

197
00:18:24,800 --> 00:18:29,360
in that AI is typically like an inferior, but cheaper alternative to asking somebody to do

198
00:18:29,360 --> 00:18:34,080
something for you, right? It's a, it's also a general purpose technology. I guess what are the,

199
00:18:34,080 --> 00:18:38,480
what are the kind of key definitions and how do you think about mustering those different frameworks

200
00:18:38,480 --> 00:18:42,240
so that people have good clarity on what it is we're dealing with?

201
00:18:42,240 --> 00:18:50,880
I mean, it is, it's so hard because it is so, it is so general and it is also a technology that

202
00:18:50,880 --> 00:18:57,280
improves other technologies and itself directly, you know, not in the way that electricity improves

203
00:18:57,280 --> 00:19:00,800
electricity, you know, electricity makes the economy more efficient and so you can build more

204
00:19:00,800 --> 00:19:05,600
electrical power stations, but AI seems much more direct. I do think it's important to understand

205
00:19:06,000 --> 00:19:12,880
its generality to get people to wake up to the idea that a general purpose technology really

206
00:19:13,520 --> 00:19:19,680
transforms the world beyond the, beyond the economics and, you know, again, Detroit is a

207
00:19:19,680 --> 00:19:27,040
great example for this because the car transformed the world in a very short period of time where I

208
00:19:27,040 --> 00:19:34,960
live in Northwest London 120 years ago, this was all fields and within 20 years after that,

209
00:19:34,960 --> 00:19:38,640
by about 1925, the roads were laid out the way they were and the houses were built the way they

210
00:19:38,640 --> 00:19:43,280
are and a century later we're still like this and this is because of ultimately the car as a

211
00:19:43,280 --> 00:19:48,000
general purpose technology. So too, because they don't come around very often, I think that's a

212
00:19:48,000 --> 00:19:54,560
really good starting point. On the question of disruption, that is a, I think it's a kind of

213
00:19:54,560 --> 00:20:00,880
higher order question because that is about products and how they get bundled to provide

214
00:20:00,880 --> 00:20:06,400
value in a particular environment and I think that, you know, when you start to bundle AI to do

215
00:20:06,400 --> 00:20:10,960
that, you can ask that specific question. But one of the things I think is really important is,

216
00:20:10,960 --> 00:20:16,640
and I think companies started to think like this, is to think in terms of tasks rather than jobs

217
00:20:16,640 --> 00:20:23,840
because, you know, AI can't replace jobs, anyone's job, because there's just so much in an ordinary

218
00:20:23,840 --> 00:20:29,360
job, like logging on to Zoom and saying hello to somebody and getting your neighbor a cup of coffee

219
00:20:30,080 --> 00:20:37,440
that is beyond the scope of any AI system. But within tasks, I think you can start to unpick

220
00:20:37,440 --> 00:20:42,800
this. And that's why in a sense you might start to say, well, AI becomes a disruptive technology

221
00:20:42,800 --> 00:20:49,120
because on a like for like basis, it can't replace an entire, you know, human in their day to day,

222
00:20:49,120 --> 00:20:53,120
but it might get better and better. But I think what's more helpful is to go back to that task

223
00:20:53,200 --> 00:21:00,640
question. And then when we come to that, the question is on a task basis, is AI really a

224
00:21:01,600 --> 00:21:08,000
cheaper version of a human doing the same task and a worse version? Does that matter? And is that

225
00:21:08,000 --> 00:21:15,120
always the case? And I have certain tasks, which I do where I think I could not afford to hire a

226
00:21:15,120 --> 00:21:20,560
human to do this task as well as chat GPT does it for me, you know, in a minute or two. And that

227
00:21:20,560 --> 00:21:25,040
may well be your experience as well. I mean, I, you know, I use this for to write letters of

228
00:21:25,040 --> 00:21:32,640
complaints to get my parking fines reversed to help me think through holiday plans to do research

229
00:21:32,640 --> 00:21:37,360
for my book. And so I mean, it's just it's so variable. And in many cases, I would be better

230
00:21:37,360 --> 00:21:43,040
off finding the very best human to do that. But the costs of doing that, even finding them is so

231
00:21:43,040 --> 00:21:48,240
high. Yeah, your search costs alone would dominate. Yeah, search costs would dominate. I mean, when

232
00:21:48,240 --> 00:21:55,200
you're using, you know, GPT for or perplexity, whatever you use, do you have it across a whole

233
00:21:55,200 --> 00:22:00,960
range of different tasks that you do from the most strategic for your business to the most sort of

234
00:22:00,960 --> 00:22:07,760
trivial home tasks? Yeah, maybe not the most strategic yet. At that level, I would probably

235
00:22:07,760 --> 00:22:13,920
restrict myself to kind of brainstorming, you know, interaction at most, but certainly lots of

236
00:22:13,920 --> 00:22:19,920
things I get, you know, very efficient and an immediate help on. And I think that immediacy is

237
00:22:19,920 --> 00:22:24,800
super important, too. I have this one slide that I call the cognitive tail of the tape,

238
00:22:24,800 --> 00:22:31,040
which kind of lists out 12 dimensions and compares human to today's AIs. And, you know, then we can

239
00:22:31,040 --> 00:22:37,840
also consider what future as might look like, very much agree with your notion of distinguishing

240
00:22:37,840 --> 00:22:44,320
between jobs and tasks. And for a while, I was calling this the great implementation. And I've

241
00:22:44,320 --> 00:22:48,560
that phrase hasn't quite taken off yet. But the idea there is with inspiration from like your,

242
00:22:48,560 --> 00:22:54,720
you know, strutequeries and your Benedict Evans type business theorists, you know, the way to make

243
00:22:54,720 --> 00:23:01,600
money in businesses to bundle and unbundle, I do think that unbundling jobs into tasks is a very

244
00:23:02,320 --> 00:23:06,880
good way to think about it. And then for any given task, you go down this tail of the tape,

245
00:23:06,880 --> 00:23:13,600
and you're like, Yeah, a lot of them AI can do as well, or even better than a human or certainly

246
00:23:14,480 --> 00:23:18,240
better than a human that I could find without huge search costs.

247
00:23:18,800 --> 00:23:23,600
You know, we get we get quite surprised with some of the results. So I think one of the

248
00:23:23,600 --> 00:23:28,480
really big surprises is that if we went back six or seven years, and you had books like The Rise

249
00:23:28,480 --> 00:23:33,440
of the Robots and the famous Oxford paper saying, you know, machine learning could automate 40%

250
00:23:33,440 --> 00:23:38,320
or expersent of jobs that's written by a friend of mine. And our assumption was that it would be

251
00:23:39,200 --> 00:23:45,440
routine cognitive jobs, by which what people meant was customer service and data entry in like

252
00:23:45,440 --> 00:23:50,480
Philippines or the Indian or in India. And what we're actually discovering is that it's jobs that

253
00:23:50,480 --> 00:23:56,320
we would have categorized as non routine or even creative, where this technology can really start

254
00:23:56,320 --> 00:24:02,880
to make a difference. And it really, really is surprising. One that really I was not expecting

255
00:24:02,880 --> 00:24:10,560
was a paper at the end of 2023, which looked at empathy ratings of doctors giving advice compared

256
00:24:10,560 --> 00:24:17,760
to that GPT or GPT for giving advice, and patients were rating the robotic advice as more empathetic

257
00:24:17,760 --> 00:24:24,160
as humans. And the whole argument had been, let's use AI. So the radiologist can spend more time

258
00:24:24,720 --> 00:24:30,240
looking you in the eye and being attentive, and being and being empathetic. So part of the

259
00:24:30,240 --> 00:24:36,080
challenge I think is that there is this lack of clarity and lack of knowledge about where and when

260
00:24:36,080 --> 00:24:41,920
will these AI systems actually compete with humans on particular, particular tasks. And then when

261
00:24:41,920 --> 00:24:46,560
you think about it, actually, what is empathy? Empathy is about active listening and it's about

262
00:24:46,560 --> 00:24:52,400
being incredibly patient. And there's nothing that's more patient than a robot that has no sense of

263
00:24:52,400 --> 00:24:57,360
time and is stateless, right? I mean, it'll just sit there forever. An early GPT for tests that I

264
00:24:57,360 --> 00:25:04,160
remember fondly and do think is kind of a sign of things to come was simulated tech support for my

265
00:25:04,160 --> 00:25:12,240
grandmother when she needs help with her iPhone. And it was, you know, just a flash of this, you

266
00:25:12,240 --> 00:25:19,040
could call it sparks of things to come where I played the role of her, which, you know, and I,

267
00:25:19,040 --> 00:25:23,360
she calls me, right, when she needs help with the iPhone. And, you know, you're in this dynamic.

268
00:25:23,360 --> 00:25:27,520
And it's actually an interesting dynamic also for a pure text situation because she's typically

269
00:25:27,520 --> 00:25:32,640
on the phone with me looking at the phone and saying, you know, I can't, my friend sent me an

270
00:25:32,640 --> 00:25:39,360
email and I can't get it. And then I'm like, okay, well, I always start with, what do you see on the

271
00:25:39,360 --> 00:25:44,880
screen right now? Can you start at the top and just read everything that you see on the screen?

272
00:25:44,880 --> 00:25:49,600
And at times we've had some very, you know, kind of funny, does she always read everything that's

273
00:25:49,600 --> 00:25:53,680
there? Like, you know, she's missed something out. You're like, grandma, wasn't there a,

274
00:25:53,680 --> 00:25:58,480
isn't there a word above? No, she does pretty well. Yeah, she'll start at the top Verizon,

275
00:25:58,480 --> 00:26:02,160
you know, the time. And then there've been a couple of times where, you know,

276
00:26:02,720 --> 00:26:06,560
some one of those dial system dialogues pops up and that like just didn't even register to her.

277
00:26:06,560 --> 00:26:09,680
But then when she got it to reading, I was like, you need to hit okay there before you're going

278
00:26:09,680 --> 00:26:15,040
to be able to hit anything else. So it can be these very simple things. But GPT-4, as you might

279
00:26:15,040 --> 00:26:19,280
expect, did really quite well on that. There was a little bit of, it was in kind of the middle of

280
00:26:19,280 --> 00:26:26,800
where it did not have the UI of an iPhone memorized. So it was kind of hallucinating it and guessing.

281
00:26:27,440 --> 00:26:31,840
And yet the guesses were close enough, you know, and I really had to study my iPhone

282
00:26:31,840 --> 00:26:36,720
and what it was saying and kind of compare like, is the, is it saying what's actually there or

283
00:26:36,720 --> 00:26:40,880
not? And mostly was, but it was clear it was kind of filling in some gaps. But the real eye opening

284
00:26:40,880 --> 00:26:48,880
moment for me was it said something that I thought she might feel was a little bit rude.

285
00:26:49,600 --> 00:26:53,360
I forget exactly what it was, but it was like, you know, it was like starting the

286
00:26:53,360 --> 00:26:56,800
starting the top left, you know, where the top left is something like kind of that basic.

287
00:26:57,360 --> 00:27:02,720
And then I responded as her saying, yes, I know where the top left is. I'm not dumb. I'm just

288
00:27:02,720 --> 00:27:07,840
struggling with this phone. And then the AI comes back and says, I'm so sorry, I didn't mean to offend

289
00:27:07,840 --> 00:27:11,600
you. I'm just, you know, trying to make sure we're resetting here and, you know, helping you

290
00:27:11,600 --> 00:27:14,880
through this process. And that was the moment where I was like, Oh, this thing is going to be,

291
00:27:15,760 --> 00:27:20,640
it's got kind of this emotional intelligence as well. And that could be, you know, obviously just

292
00:27:20,640 --> 00:27:25,200
a critical ingredient for so many different interactions and medicine being a big one. We've

293
00:27:25,200 --> 00:27:32,000
done two episodes with Vivek Nadarajan, who leads a lot of these med specific projects at Google.

294
00:27:32,720 --> 00:27:40,000
And what an absolute terror they've been on. Most recently, they have a diagnosis differential

295
00:27:40,000 --> 00:27:46,080
diagnosis paper that shows the AI is getting the correct diagnosis twice as often as the unassisted

296
00:27:46,080 --> 00:27:50,720
human. And also more often than the AI assisted human, which I think is the thing that we should

297
00:27:50,720 --> 00:27:55,200
begin to reckon with. There's so much to unpack in that. Can I ask you one question about the

298
00:27:55,200 --> 00:27:59,920
politeness from the, you know, when you're playing your, your grandma, do you think that that comes

299
00:27:59,920 --> 00:28:06,240
from the human feedback cycle over the network before it gets released? Or is it, is it from the

300
00:28:06,240 --> 00:28:12,320
training data? The version that we had was, as far as I know, right, I'm inferring here because

301
00:28:12,320 --> 00:28:19,920
I did not have access to the training methods. But it seemed very clear to me that the model

302
00:28:19,920 --> 00:28:28,880
version that we had was RLHF purely for helpfulness. So it was very eager to please very eager to be

303
00:28:28,880 --> 00:28:36,080
nice to you, no guardrails on what you could ask it and what it would do, but 100% just trying to be

304
00:28:36,080 --> 00:28:40,800
helpful and pleasing to the user. So when it detected that kind of, I'm kind of bristling at

305
00:28:40,800 --> 00:28:45,760
what you just said, that's what it reacted with this, you know, I'm so sorry, I'm just trying to

306
00:28:45,760 --> 00:28:51,520
help kind of thing. And that was a mind blowing moment. I had not seen, of course, anything remotely

307
00:28:51,520 --> 00:28:57,920
like that from earlier models, right? At the time, Text DaVinci 002 was the best publicly available

308
00:28:57,920 --> 00:29:04,960
model. And it would like follow instructions on basic stuff. But I mean, this was a totally

309
00:29:04,960 --> 00:29:09,600
different world that we had suddenly stepped into. Hey, we'll continue our interview in a moment

310
00:29:09,600 --> 00:29:13,600
after a word from our sponsors. If you're a startup founder or executive running a growing

311
00:29:13,600 --> 00:29:18,960
business, you know that as you scale your systems break down, and the cracks start to show. If this

312
00:29:18,960 --> 00:29:25,840
resonates with you, there are three numbers you need to know. 36,000, 25, and one. 36,000. That's

313
00:29:25,840 --> 00:29:29,520
the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the number one

314
00:29:29,520 --> 00:29:34,160
cloud financial system, streamlined accounting, financial management, inventory, HR, and more.

315
00:29:34,720 --> 00:29:40,240
25. NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,

316
00:29:40,240 --> 00:29:45,280
close their books in days, not weeks, and drive down costs. One, because your business is one

317
00:29:45,280 --> 00:29:50,000
of a kind, so you get a customized solution for all your KPIs in one efficient system with one

318
00:29:50,000 --> 00:29:54,800
source of truth. Manage risk, get reliable forecasts, and improve margins. Everything you

319
00:29:54,800 --> 00:30:00,720
need all in one place. Right now, download NetSuite's popular KPI checklist, designed to give you

320
00:30:00,720 --> 00:30:05,760
consistently excellent performance, absolutely free, and netsuite.com slash cognitive. That's

321
00:30:05,760 --> 00:30:11,280
netsuite.com slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.

322
00:30:12,560 --> 00:30:18,080
Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that

323
00:30:18,080 --> 00:30:23,760
actually work, customized across all platforms with a click of a button. I believe in Omnike so

324
00:30:23,760 --> 00:30:30,240
much that I invested in it, and I recommend you use it too. Use CogGrav to get a 10% discount.

325
00:30:30,240 --> 00:30:36,400
I mean, it is such a different world. I was using chat GPT for something a few days ago,

326
00:30:36,400 --> 00:30:42,080
and I got quite tired as you do. I mean, I think about it as the time I once was playing tennis

327
00:30:42,080 --> 00:30:46,640
against one of those tennis ball-serving machines, and I was exhausted, and the machine is just like

328
00:30:46,640 --> 00:30:52,640
willing to keep firing balls at me. And it's a bit like that using these chatbots. And I just

329
00:30:52,720 --> 00:30:56,720
left this kind of whimsical comment. I said, you know what? I'm really tired. I will come back

330
00:30:56,720 --> 00:31:01,680
tomorrow and we can look at the moisture evaporators. We were not doing anything to do with Star Wars,

331
00:31:01,680 --> 00:31:09,040
right? We were doing some digging about coal in England in the 17th century. And it just replied

332
00:31:09,040 --> 00:31:15,280
to me going, I'll be happy to, you go and rest up, and tomorrow I can help you on the farm.

333
00:31:15,280 --> 00:31:23,040
May the force be with you. And it had just very subtly played that back to me in a really,

334
00:31:23,040 --> 00:31:28,720
really nice way. And actually, in terms of humanized interfaces, as someone who's used Apple's

335
00:31:28,720 --> 00:31:36,000
computers for 40 years, it really sits alongside the trend that a firm like Apple had been thinking

336
00:31:36,000 --> 00:31:42,080
about for a really long time, which is how do we make this technology come to us rather than us

337
00:31:42,080 --> 00:31:46,720
go to the technology? You know, there has been this view that we'll have these central humans,

338
00:31:46,720 --> 00:31:52,240
right? Human plus machine. And one of the things that we learned about centaurs, and that idea

339
00:31:52,240 --> 00:31:58,960
came out with chess, right? After Kasparov lost to Deep Blue back in, in 97, was that for several

340
00:31:58,960 --> 00:32:03,760
years humans and the machine would outperform machines on their own, and of course humans.

341
00:32:04,400 --> 00:32:08,800
But now you're just better off with a chess computer. And as a human, you should just accept

342
00:32:08,800 --> 00:32:13,680
everything it says. So the central period in chess only lasted seven or eight years.

343
00:32:13,680 --> 00:32:18,480
And I think one of the assumptions that goes into this kind of maelstrom of ideas we have to make

344
00:32:18,480 --> 00:32:26,160
sense of over the next years, is that central humans will exist for quite a bit of time. In

345
00:32:26,160 --> 00:32:31,760
other words, human plus machine will do better than machine on its own. But we are starting to see

346
00:32:31,760 --> 00:32:37,520
signs at that period of time is already starting to come to an end, right? Far faster than we

347
00:32:37,520 --> 00:32:42,080
might have predicted. And you've just given an example, which is a, you know, an unreleased AI model

348
00:32:42,640 --> 00:32:47,120
where the AI on its own is doing better than the expert, expert human with the AI.

349
00:32:47,120 --> 00:32:52,960
Yeah, it's pretty crazy. I think people are broadly in denial about that possibility even,

350
00:32:52,960 --> 00:32:58,240
you know, and let alone the reality, it's funny that we things are happening so quickly that

351
00:32:58,240 --> 00:33:03,200
people are saying things, you know, are impossible or won't happen for at least 10 years that are

352
00:33:03,200 --> 00:33:08,080
literally already happening. And so I think, you know, to some degree, there's just kind of a lack

353
00:33:08,080 --> 00:33:12,720
of awareness. And there's also definitely some psychological, and I try not to psychologize

354
00:33:12,720 --> 00:33:18,480
people's AI positions too much, because I think the technology itself is confusing enough. But

355
00:33:18,480 --> 00:33:23,920
it certainly seems like there's some kind of psychological cope or denial happening there.

356
00:33:23,920 --> 00:33:30,480
I also do think it's important to keep in mind too that the setting really matters and the world

357
00:33:30,480 --> 00:33:34,720
is going to change as well. And humans may still have a really important role to play

358
00:33:35,280 --> 00:33:41,760
in a lot of systems for a while yet. I think we do have the AIs do have really important

359
00:33:41,760 --> 00:33:47,520
weaknesses. So you do do a study like this Google evaluation. And I think that they, you know, are

360
00:33:47,520 --> 00:33:53,280
very serious people who've set this up in a way that I trust is, you know, not not cooking the

361
00:33:53,280 --> 00:33:58,400
books in favor of the AI. So I take that result, you know, basically, at face value.

362
00:33:59,200 --> 00:34:06,160
But then I also think like the AIs have these strange vulnerabilities that, you know, for example,

363
00:34:06,160 --> 00:34:12,800
the AlphaGo system, right, that is superhuman go player. But I have an episode coming up with

364
00:34:13,760 --> 00:34:21,280
Adam Gleave from far AI, and they put out a method that showed how a relatively simple

365
00:34:21,280 --> 00:34:26,960
adversarial attack on the AlphaGo system could beat it. And no human would lose to this like

366
00:34:26,960 --> 00:34:33,040
simple adversarial attack. But the AI had this like massive blind spot that they were able to

367
00:34:33,040 --> 00:34:40,480
engineer against and exploit. So I do think we're headed for strange dynamics and the sort of naive,

368
00:34:40,480 --> 00:34:43,760
I don't mean to say that Google thing is naive, but it's kind of, you know, let's take

369
00:34:44,320 --> 00:34:49,120
controlled conditions and set it up in a certain way and see what happens. I do think it's really

370
00:34:49,120 --> 00:34:52,640
important to keep in mind that, you know, just like these AI systems in general, when they get out of

371
00:34:52,640 --> 00:34:58,400
domain, they have problems, like those results may also not extrapolate, at least initially,

372
00:34:58,400 --> 00:35:03,680
to situations where people are trying to break the AI, you know, I wouldn't, for anything where

373
00:35:03,680 --> 00:35:10,080
there's an adversarial incentive out there, I would not, I would not be quick to take a result

374
00:35:10,080 --> 00:35:14,480
like that and be like, okay, cool, you know, we can just deploy the AI on its own. Now in medicine,

375
00:35:14,480 --> 00:35:18,240
presumably, there's not a lot of adversarial situation, right, because people want to get

376
00:35:18,240 --> 00:35:23,360
the right diagnosis and like nobody wants them to get the wrong diagnosis. So, you know, I do think

377
00:35:23,360 --> 00:35:29,280
the AI doctor is kind of here before we know it. And, you know, we're going to have some very

378
00:35:29,280 --> 00:35:34,960
interesting questions about what to do about that. But I think the thing that you've also identified

379
00:35:34,960 --> 00:35:40,320
is that these systems have to get into companies, have to get into hospitals, have to get into the

380
00:35:40,320 --> 00:35:46,800
economy. And that's still, that still takes time, right? It still takes time for, let's not look

381
00:35:46,800 --> 00:35:51,200
at medicine, because it would have to be go through all sorts of clearances through the FDA and so on

382
00:35:51,200 --> 00:36:00,400
before it could be used. But every time we've seen amazing technologies emerge, the cloud computing,

383
00:36:00,400 --> 00:36:05,520
right, GPT uptake is higher than cloud computing, even though it's 19 years younger. But every time

384
00:36:05,520 --> 00:36:12,960
these technologies come out, they take a while to make a make it into businesses. I mean, even

385
00:36:12,960 --> 00:36:18,000
something as straightforward as the typewriter. So a typewriter took about 20 to 25 years from

386
00:36:18,000 --> 00:36:23,120
becoming a kind of affordable technology to being something that businesses have figured out how to

387
00:36:23,120 --> 00:36:28,320
use and how to change their, their processes, something like electricity took a little bit longer

388
00:36:28,320 --> 00:36:33,040
because, you know, the way that factories used what used power that before electricity was like a

389
00:36:33,040 --> 00:36:39,680
single big drive shaft and a massive lump of power that came from it. And then your electricity is

390
00:36:39,680 --> 00:36:44,560
this highly distributed, packetized movement of energy that you can put in different places,

391
00:36:44,560 --> 00:36:50,800
you need an entirely different setup for it. And so, so even when we look at something like this,

392
00:36:50,800 --> 00:36:59,520
the question is, how quickly are we able to bring it in? And over what time frame does it then start

393
00:36:59,520 --> 00:37:06,160
to change our, the practitioners relationship with the technology? I mean, we know from automation

394
00:37:06,160 --> 00:37:12,960
of, of aircraft and other automated systems that you go through this three phase process as somebody

395
00:37:12,960 --> 00:37:19,840
who's using the technology. Phase one, you kind of don't trust it, and you, you then see if it

396
00:37:19,840 --> 00:37:26,320
does well where you're the ground truth. Phase two, you start to assume it's a ground truth,

397
00:37:26,320 --> 00:37:30,240
and then you, you pack yourself on the back when you come up with the same answer that the machine

398
00:37:30,240 --> 00:37:35,680
comes up with. And then phase three, which is where we've got to on GPS, where we just trust ways,

399
00:37:35,680 --> 00:37:39,600
and it's like, actually, you know, we generally trust ways, some London taxi drivers don't,

400
00:37:39,600 --> 00:37:45,280
we just trust it. And you see that process happening replicated in other types of automation,

401
00:37:45,280 --> 00:37:50,880
which is why when it's really mission critical, you have incredibly high levels of training,

402
00:37:50,880 --> 00:37:55,440
and you have other types of safeties in place, like, you know, two humans in the cockpit,

403
00:37:55,440 --> 00:38:01,200
or, you know, whatever, whatever it happens to be. And, and so that part of the journey,

404
00:38:01,200 --> 00:38:07,920
I think is also one that adds a little bit of drag in terms of how long it takes to have an

405
00:38:07,920 --> 00:38:12,480
impact. And in that time, Nathan, we start to understand new questions, questions that we

406
00:38:12,480 --> 00:38:17,600
can't imagine right now, because they're just too far down the decision tree. So sometimes I think,

407
00:38:17,600 --> 00:38:22,960
I hear people say, well, there'll be that moment, I think Tim Urban has this, from Wait But Why,

408
00:38:22,960 --> 00:38:28,720
has this graph where he sort of shows the moment where AI is like as clever as a rat, and then

409
00:38:28,720 --> 00:38:33,760
like a second later, it's 10 times cleverer than us. I think part of the, the reality of

410
00:38:33,760 --> 00:38:38,320
like the rubber hitting the road is going to be that even as these products do very,

411
00:38:38,320 --> 00:38:43,600
very well in the lab situation, it just takes a little bit of time for them to get into,

412
00:38:43,600 --> 00:38:50,560
into the real world. Now, where it may happen is, I think in, in a couple of places. So one is

413
00:38:51,360 --> 00:38:57,520
where tasks have already been discretized so that they're essentially just written down. And I

414
00:38:57,520 --> 00:39:02,640
think that that is certain types of call centers and it's certain types of data entry. And in those

415
00:39:02,640 --> 00:39:10,320
places, the human is already hired on a task by task basis normally mediated by like a, a, a, a

416
00:39:10,320 --> 00:39:15,360
body shop. So the buyer of the services, which is like our favorite consumer electronics company or

417
00:39:15,360 --> 00:39:21,040
your insurance company has no emotional relationship, they just have a KPI that they measure to. And I

418
00:39:21,040 --> 00:39:27,680
think that lends itself unfortunately to a, a tidal wave risk for those types of roles. I think

419
00:39:27,680 --> 00:39:35,920
the, the second area is, you know, classic Silicon Valley stuff where a De Novo company is, I should

420
00:39:35,920 --> 00:39:41,840
say Silicon Valley, a classic Detroit stuff where a De Novo company is able to apply these technologies

421
00:39:41,840 --> 00:39:46,640
the way Henry Ford did and build his processes from scratch, right? In the way that the artisanal

422
00:39:46,640 --> 00:39:51,920
car makers were just going to have so much kind of cultural drag, they couldn't. And that's what we

423
00:39:51,920 --> 00:39:56,560
certainly saw with a lot of the, you know, the internet, right? It was, it was absolutely startups

424
00:39:56,560 --> 00:40:02,400
that captured the value. And then when you started to get to two areas that were more highly regulated

425
00:40:02,400 --> 00:40:08,480
and had, you know, a lot more stickiness to them, like, like finance, with the exception of certain

426
00:40:08,480 --> 00:40:13,760
areas of payments, it's still the bank, big banks, sort of the big banks. And, and, and I don't know

427
00:40:13,760 --> 00:40:21,360
how differently this actually plays out short of scenarios where someone is able to use these

428
00:40:21,360 --> 00:40:26,400
super powerful machines to kind of manipulate the rules of the game, which I, you know, I think is a,

429
00:40:26,960 --> 00:40:31,600
like that's more like a black swan scenario than one that we could, you know, talk about reasonably.

430
00:40:32,400 --> 00:40:37,040
Well, first of all, as an aside, if you ever make it to Detroit, I'll take you to the Henry

431
00:40:37,040 --> 00:40:44,800
Ford Museum and Living History Museum, which is called Greenfield Village, where this, this

432
00:40:44,800 --> 00:40:50,480
earlier transition is documented with actual machinery still running from the, you know,

433
00:40:50,480 --> 00:40:56,240
various phases, you can go see one of these old factories where actually a few of them where

434
00:40:56,240 --> 00:41:00,960
they have kind of the central steam engine, and then it's powering this one, you know,

435
00:41:00,960 --> 00:41:06,320
driveshaft at the top of the floor. And then there's like 25 belts, you know, coming off of

436
00:41:06,320 --> 00:41:11,200
that and connecting to other machines, all driven on this single thing. And then you,

437
00:41:11,200 --> 00:41:15,920
they have an electrical, early electrical version of that as well. Edison's workshop is there. He

438
00:41:15,920 --> 00:41:20,800
actually, this is Henry Ford toward the end of his life was like, became very sort of nostalgic

439
00:41:20,800 --> 00:41:26,720
for an earlier period. And decided he wanted to kind of create this, you know, living history

440
00:41:26,720 --> 00:41:31,360
place to sort of preserve that history so people can see it in the future. It's quite an awesome

441
00:41:31,360 --> 00:41:36,160
thing to go and contemplate. I'll have to do that. Because I think, I think Ford's influence

442
00:41:36,160 --> 00:41:42,640
and impact is somehow underrated. We don't talk about Ford as much as we may talk, maybe talk

443
00:41:42,640 --> 00:41:48,880
about Edison, when we think historically. And, you know, there was so much in Ford, he understood

444
00:41:48,880 --> 00:41:55,120
the, the socio economic contract that emerged from these, from these changes in like, in a

445
00:41:55,120 --> 00:41:59,280
number of different ways. I mean, not always in ways that are, you know, kind of positive. We

446
00:41:59,280 --> 00:42:05,680
know where the sort of sociological department started trying to ensure temperance amongst

447
00:42:05,680 --> 00:42:12,320
workers and, and so on. And it gets encapsulated in, in Aldous Huxley's book, Brave New World,

448
00:42:12,320 --> 00:42:16,880
which I still find to be a really, really remarkable piece of writing, you know, it was

449
00:42:16,880 --> 00:42:26,240
written in the 1930s, 1932, I think. And for Huxley to encapsulate and extend a raw Fordism,

450
00:42:26,240 --> 00:42:31,440
as far out as he did, it's a little bit like Kurt's file and what he did with his singularity

451
00:42:31,440 --> 00:42:36,720
work, I thought was absolutely genius. And I think that that, that book, which Brave New World,

452
00:42:36,720 --> 00:42:46,640
which rests on the ideas that, that Ford developed, catalyzed, that spawn off his, his work speaks

453
00:42:47,360 --> 00:42:51,680
very, really quite clearly to a number of the issues that we face now in this kind of later

454
00:42:51,680 --> 00:42:55,840
stages of industrial capitalism. And I mean, so a trip to Detroit, I will take you up on and I'll

455
00:42:55,840 --> 00:42:59,280
bring you a copy of Brave New World as well. It's been a long time since I've read that,

456
00:42:59,280 --> 00:43:04,640
actually going back to high school. So I might need to dust that one off and I'm sure it will

457
00:43:04,640 --> 00:43:09,440
resonate very differently today than it did for me then. So, okay, I have a number of questions on

458
00:43:10,080 --> 00:43:15,280
this kind of concept of transition. I think I would, from what I heard, I think we're probably

459
00:43:15,280 --> 00:43:23,680
largely on the same page that it seems like incumbents, the big banks and big technology

460
00:43:23,680 --> 00:43:31,440
companies, largely should be able to harness this technology and bring it to their platforms

461
00:43:31,440 --> 00:43:37,360
before they get displaced. I think of like Salesforce is almost a canonical example there.

462
00:43:37,360 --> 00:43:42,080
They're going to have an AI layer that creates a much better and less complicated, less confusing

463
00:43:42,080 --> 00:43:47,840
user experience before somebody's going to recreate all the complexity of Salesforce.

464
00:43:47,920 --> 00:43:54,800
On the other hand, there is no AI friend today. So that's kind of my canonical example of something

465
00:43:54,800 --> 00:43:59,440
that's by definition going to be de novo. And then presumably there's like a ton of stuff in

466
00:43:59,440 --> 00:44:04,000
between. And I gather that you're kind of talking to business leaders probably throughout that

467
00:44:04,000 --> 00:44:09,520
spectrum. Where are they today? It seems like they're groping of what is going on and their

468
00:44:09,520 --> 00:44:16,880
eagerness to transform the way their businesses operate might be the limiting factor in how

469
00:44:16,960 --> 00:44:21,760
quickly this transition can proceed. Are you advising them to start to think about

470
00:44:22,640 --> 00:44:27,360
what kind of semi-structured work they can make a lot more structured so that they can

471
00:44:28,320 --> 00:44:34,000
reduce it to these kind of task level things that can be automated? And are they receptive to

472
00:44:34,000 --> 00:44:39,200
that sort of challenge and or opportunity? I mean, I can't remember a technology which has

473
00:44:39,200 --> 00:44:45,200
had the degree of uptake in large companies as this one. I think back to the internet,

474
00:44:45,200 --> 00:44:50,880
back in 1999, I was talking to the CEO of a big mobile phone company. And he said to me,

475
00:44:50,880 --> 00:44:55,680
I will never let you pay your bill over the internet. In fact, I'll never ever let you look

476
00:44:55,680 --> 00:45:00,000
at your bill over the internet. And he was right because he was fired a year later. So he never,

477
00:45:00,000 --> 00:45:05,680
project was never delivered. But it's so different with not just AI, but specifically gen AI,

478
00:45:05,680 --> 00:45:09,840
because two things are happening. One is that the CEOs of the companies have been playing around

479
00:45:09,840 --> 00:45:13,680
with this partly because they're of that age now, they're in their mid 50s, they've grown up with

480
00:45:13,680 --> 00:45:18,960
computers, their kids are coming back with it from college. And the second thing that's happening

481
00:45:18,960 --> 00:45:25,840
is that the frontline workers are using this regardless of any restrictions by their employees.

482
00:45:25,840 --> 00:45:31,280
And there have been a couple of surveys now, one done by your old alma mater Oliver Wyman,

483
00:45:31,280 --> 00:45:40,880
which was of 25,000 employees across 18 countries. And in 83% of employees in the UAE in India,

484
00:45:40,880 --> 00:45:45,360
already using a chat GPT or something similar. And we've seen data from Salesforce and others that

485
00:45:45,360 --> 00:45:50,960
say in the US, it's like 30, 40, 50%, you know, choose your number, but it's not 2%. And it's

486
00:45:50,960 --> 00:45:55,840
very frontline. And I think of that as like a pincer movement, because normally, you got to

487
00:45:55,840 --> 00:46:00,720
drag the frontline employees, or you got to drag the CEO, they both want it for different reasons.

488
00:46:00,720 --> 00:46:04,880
And I think it will, it will happen. And you start to see that in the, the levels of uptake

489
00:46:04,880 --> 00:46:09,440
that are being reported through, through the surveys. So I think that that says that we'll

490
00:46:09,440 --> 00:46:14,880
see more and more projects roll out more, more quickly. But there's still a lot of retraining

491
00:46:14,880 --> 00:46:19,280
that needs to go on internally and internal processes. I mean, I think one thing that will

492
00:46:19,280 --> 00:46:24,320
happen is I just saw Ethan Mollick, who you must get on your show. This is fantastic. A professor

493
00:46:24,320 --> 00:46:30,400
at Wharton who's writing a book on chat GPT. And he just showed a video where he had six different

494
00:46:30,400 --> 00:46:36,560
windows open. He put an inquiry into each one. And in 54 seconds, he had a product launch plan,

495
00:46:36,560 --> 00:46:42,160
a market analysis, a PowerPoint deck assessing Tesla's business and something else created

496
00:46:42,160 --> 00:46:46,880
from one sentence prompts. Now, if anyone's work has worked in a large organization, they know

497
00:46:46,880 --> 00:46:52,960
that you know your inbox is a full of crap and full of meaningless PowerPoint decks. So we might

498
00:46:52,960 --> 00:46:59,200
actually just find ourselves sort of swallowed up by PowerPoints created by Microsoft co pilot.

499
00:46:59,200 --> 00:47:03,440
So there's a whole set of I think more complex kind of issues that that exist in large companies.

500
00:47:03,440 --> 00:47:08,400
But I think that they will adopt this much faster than the evidence is and they have in

501
00:47:08,400 --> 00:47:13,200
previous technologies. But it doesn't, I think necessarily mean that disruption of the kind

502
00:47:13,200 --> 00:47:22,720
you talked about won't also happen. And I didn't have on my dance card in 95 when I started working

503
00:47:22,720 --> 00:47:29,360
that blockbuster would be the first high profile casualty of the internet. And I had seen video

504
00:47:29,360 --> 00:47:34,560
over the internet. The Cambridge University had a webcam on a coffee pot. And that was the first

505
00:47:34,560 --> 00:47:39,120
sort of video over the internet. And Rob Glazer had just started real networks was called progressive

506
00:47:39,120 --> 00:47:44,480
networks back then. And when Netflix launched, it was this kind of thing with DVDs. And it was a

507
00:47:44,480 --> 00:47:50,320
real pain in the ass. And it took a long time, a few years before blockbuster has its best year,

508
00:47:50,320 --> 00:47:55,760
and then it has its worst year ever. And I think that if AI is gen AI, what have we want to call

509
00:47:55,760 --> 00:48:03,360
it is a GPT, there are going to be blockbusters lurking around. And the question is, which ones

510
00:48:03,360 --> 00:48:08,720
will it be the reason I don't I like you, I don't feel it's going to be the banks is because the

511
00:48:08,720 --> 00:48:14,800
banks have got a whole bunch of stuff that is about trust and probity and internal processes and

512
00:48:14,800 --> 00:48:21,360
compliance. That is is just not an AI question. It's it's kind of an institutional question. And I

513
00:48:21,360 --> 00:48:29,440
do I do wonder about where that that moment is. And the the thing is that the there are obvious

514
00:48:29,440 --> 00:48:32,960
ones. It's like, well, it'll be entertainment, right? We'll just start to generate personalized

515
00:48:32,960 --> 00:48:37,680
entertainment. And that'll be really bad for you know, Disney, except that you could then counter

516
00:48:37,680 --> 00:48:44,480
and say, Well, but it might be derivatives of Disney Disney's IP that actually benefits Disney.

517
00:48:44,480 --> 00:48:50,880
So finding the space a priori, I think is really, really difficult. And the people who make their

518
00:48:50,880 --> 00:48:55,440
money doing this, who are the venture capitalists, get it wrong a lot of the time anyway, that's why

519
00:48:55,440 --> 00:49:01,600
they all they all have portfolios of 30, 50, 100, 500 companies. If it was so damn easy to find

520
00:49:02,480 --> 00:49:08,080
the next Apple, that's going to disrupt the previous industry that have portfolios of one,

521
00:49:08,080 --> 00:49:10,960
it's not it's really difficult because actually nobody knows and we have to

522
00:49:11,600 --> 00:49:17,120
figure it out through, you know, experimentation. So so I don't I don't know, but I keep asking

523
00:49:17,120 --> 00:49:22,480
that question and the question I take to bosses of companies, I take a couple. One is who could be

524
00:49:22,480 --> 00:49:28,400
the blockbuster? And how do you what are you doing to make sure that that's not you because I think

525
00:49:28,400 --> 00:49:33,360
by and large, they've got the rollout of gen AI and customer service and compliance and form

526
00:49:33,360 --> 00:49:38,160
filling and so on underway. And the second question I've started to ask is, you know, what would you

527
00:49:38,160 --> 00:49:44,800
do if you had a million times more compute than you have today? And many of them haven't thought

528
00:49:44,800 --> 00:49:48,640
about that. I mean, I think big tech companies have, you know, if you offered that to Satya Nadella,

529
00:49:48,640 --> 00:49:53,360
I mean, he's already in the path to do that. But but that becomes really important because if compute

530
00:49:53,360 --> 00:49:59,280
is a key input into your company's ability to execute, you need to start to think about those

531
00:49:59,280 --> 00:50:04,960
those types of questions. And do you even have a plan to make use of it? What options would it

532
00:50:04,960 --> 00:50:11,440
create for you? So those are the two areas that I push on because I want people to try to think

533
00:50:11,520 --> 00:50:17,760
a bit more creatively about this and recognize that, you know, a million X is not outside of

534
00:50:17,760 --> 00:50:23,360
the bounds of a planning cycle. And the reason I would say that is, yes, we don't have measures to

535
00:50:23,360 --> 00:50:30,400
easily show it up. But five years ago, the state of the art transformer model would have just been

536
00:50:30,400 --> 00:50:37,520
GPT two hadn't been released. So it's GPT one. And GPT one to GPT four chat GPT, I mean,

537
00:50:37,520 --> 00:50:42,640
metaphorically, just as buddies around a bar, it's a million times better, right? Maybe on the

538
00:50:42,640 --> 00:50:46,560
benchmarks, it's not it's 40% better here. But it feels a million times better, because you're

539
00:50:46,560 --> 00:50:52,640
just right across the uncanny valley. So we've just seen that play out. So let's ask it again,

540
00:50:52,640 --> 00:50:58,480
and try to ground people in the fact that capabilities could change that quickly. And

541
00:50:58,480 --> 00:51:01,600
what opportunity does that create? So how do you this is actually a live question for me?

542
00:51:01,600 --> 00:51:06,960
Because I'm working with a couple of companies and I'm noticing this challenge where it's like,

543
00:51:06,960 --> 00:51:11,840
okay, this is cool technology for sure, we can all agree on that. And yeah, we can probably

544
00:51:12,400 --> 00:51:17,600
find some efficiencies in terms of automating ticket, you know, resolution and whatever. I'm

545
00:51:18,160 --> 00:51:25,280
starting to even see things like intercom has this new 99 cent per ticket resolved AI

546
00:51:25,280 --> 00:51:29,120
pricing model, which I think is super interesting. So everybody's like, okay, cool, yeah, let's,

547
00:51:29,120 --> 00:51:32,640
you know, let's find some efficiencies, let's automate some stuff that nobody wants to do.

548
00:51:32,720 --> 00:51:36,800
Great. But then there's also this question of like, okay, we're still at the task level,

549
00:51:36,800 --> 00:51:43,440
the AI's can't quite do jobs yet. And how far are we willing to extrapolate and how much are we

550
00:51:43,440 --> 00:51:50,000
willing to invest prepared, you know, willing and prepared to invest, to try to not we're not

551
00:51:50,000 --> 00:51:53,440
going to get ahead of it, but even just kind of try to keep up with where this might be going

552
00:51:54,000 --> 00:51:59,840
in the not too distant future. And I feel like people are having a really hard time

553
00:51:59,840 --> 00:52:02,960
wrapping their head around that. Partly, it's like, you know, they don't want to

554
00:52:02,960 --> 00:52:06,480
believe too much in the hype, right? There's the question of, well, hey, this,

555
00:52:06,480 --> 00:52:10,000
how much of this is maybe just overhyped? And, you know, we've seen hype cycles come and go

556
00:52:10,000 --> 00:52:14,160
before. But I wonder how you would kind of coach people there, because I'm trying to

557
00:52:14,160 --> 00:52:19,040
get the message across that, by all means, you know, you want to be picking up the low hanging

558
00:52:19,040 --> 00:52:24,240
fruit and, you know, automating the tickets and finding all these efficiencies. But you also

559
00:52:24,240 --> 00:52:29,280
really do probably want to start thinking about what is the future paradigm that you might be

560
00:52:29,280 --> 00:52:33,920
working in. And there's just so much fog around that for people that I find a lot are just kind

561
00:52:33,920 --> 00:52:38,800
of like, I don't know, I don't really even want to go there yet. But I feel like it's a mistake

562
00:52:38,800 --> 00:52:43,360
to not, you know, at least try. They do. I mean, there are two companies I never mentioned,

563
00:52:43,360 --> 00:52:49,200
Apple and Tesla, because bosses have had them parroted at them for 10 or 15 years. And the

564
00:52:49,200 --> 00:52:52,560
point about the million times question is not to frighten people. It's actually,

565
00:52:52,560 --> 00:52:56,960
it's really about saying once we sort of acknowledge that, then we work back to stuff

566
00:52:56,960 --> 00:53:05,120
that is much more practical and prosaic, which is what is the kind of organization and capabilities

567
00:53:05,120 --> 00:53:15,520
you need to have in order to take advantage of this, these changes in a regular way. And

568
00:53:16,320 --> 00:53:21,120
there was some really interesting research in that Microsoft did actually pre all of this chat

569
00:53:21,120 --> 00:53:26,960
GPT stuff is about three or four years ago, where they looked at adoption rates of AI,

570
00:53:26,960 --> 00:53:33,920
big data type of words in companies, and they found that the more mature companies were much

571
00:53:33,920 --> 00:53:39,760
more likely in these fields were much more likely to be to say that the benefit they got was from

572
00:53:39,760 --> 00:53:44,720
market expansion and business development. Whereas the more immature ones were likely to say,

573
00:53:44,720 --> 00:53:50,720
it's all about operational savings on the tickets as it was back then. And part of the

574
00:53:51,840 --> 00:53:57,040
thing that you can start to do is you have to acknowledge that there are some really

575
00:53:57,040 --> 00:54:04,000
clear quick and clear wins in what are basically low value tasks, as perceived by the company,

576
00:54:04,000 --> 00:54:09,200
right, because they're often outsourced to third parties. But you also need to make sure that

577
00:54:09,200 --> 00:54:15,680
your best people or who you invest the most in have got access to the really, really very,

578
00:54:15,680 --> 00:54:22,960
very best tools. You know, I want my surgeon using AI systems to improve his performance,

579
00:54:22,960 --> 00:54:29,040
right? So what I try to do is encourage people to say, this, this is a shift that's happening.

580
00:54:29,040 --> 00:54:33,920
And of course, it's not about buying every GPU you can, if you're in the fish oils business.

581
00:54:33,920 --> 00:54:41,120
But it is about saying our teams outside of cost savings, starting to understand how they can use

582
00:54:41,760 --> 00:54:46,880
these services. And are they starting to experiment, learn how to use prompts? Well,

583
00:54:46,880 --> 00:54:53,360
start to see what avenues that that opens up in ways that are much, much more strategic.

584
00:54:53,360 --> 00:54:59,600
And, and I think that that forms could form some part of culture change where you have companies

585
00:54:59,600 --> 00:55:06,560
who think in those terms. And that helps the CEO start to understand that question of,

586
00:55:07,120 --> 00:55:11,920
this is not theoretical, theoretical that I need lots of computation to do a simulation

587
00:55:11,920 --> 00:55:15,360
for X. And I've bought it from one of the big consulting companies or from IBM.

588
00:55:15,920 --> 00:55:21,040
It's more that internally, my strategy teams, my business development teams are starting to

589
00:55:21,040 --> 00:55:26,960
identify opportunities and partnerships that we wouldn't otherwise have, have seen. And we can

590
00:55:26,960 --> 00:55:32,400
start to do that by, by using these, these tools in different ways. So I think it is

591
00:55:32,480 --> 00:55:38,000
practice based, which is why I think kind of prompting becomes, you know, quite a useful tool

592
00:55:38,000 --> 00:55:44,240
to, to show people. And you need to get people past that idea that chat GPT is all about writing

593
00:55:44,240 --> 00:55:49,600
the Declaration of Independence, as if you were Jar Jar Binks from, you know, Star Wars episode

594
00:55:49,600 --> 00:55:53,760
two, right? And that's where we all started, right? We got it to write funny raps and poems and

595
00:55:53,760 --> 00:55:58,480
what have you. And instead, you start to show bosses what can really be done with something

596
00:55:58,480 --> 00:56:01,840
straight out of the box. And that tends to, to wake them up a little bit.

597
00:56:01,840 --> 00:56:06,800
You know, one thing I think is maybe going to become a mantra is, you know, the day that I

598
00:56:06,800 --> 00:56:11,840
stop building apps or solving concrete problems in a hands on way is probably the day people should

599
00:56:11,840 --> 00:56:18,480
stop listening to this show because, you know, that, that my knowledge will, will atrophy or

600
00:56:18,480 --> 00:56:24,800
will, will depreciate very quickly. So I do want to be hands on and certainly welcome those kinds

601
00:56:24,800 --> 00:56:30,800
of like, you know, hey, can we, we got this, you know, task piling up and we figure out a way to,

602
00:56:30,800 --> 00:56:35,920
to slice through it. Love that, honestly. And then yeah, at the same time, I'm, I'm not really

603
00:56:35,920 --> 00:56:43,280
like a corporate consultant. So I really don't have a lot of practice there. But I am trying to

604
00:56:43,280 --> 00:56:47,840
start to piece together like, what would the real best practice for this looks like? And I think,

605
00:56:47,840 --> 00:56:52,720
and you could refine this for me, I'm sure, but I kind of think leadership, showing prominent

606
00:56:52,720 --> 00:56:58,320
examples, you know, encouraging the CEO to be like showing off. Here's what I am doing, you know,

607
00:56:58,400 --> 00:57:03,120
that is helping me in practice and just showing that process in education program.

608
00:57:03,120 --> 00:57:07,120
I think it's also probably very important. As you mentioned, having the best tools is really

609
00:57:07,120 --> 00:57:14,160
important. So trying to work toward like structured, you know, kind of piloting strategies for companies

610
00:57:14,960 --> 00:57:19,680
that, you know, and also, and this is not good for my friends on the SaaS app side, but like,

611
00:57:19,680 --> 00:57:25,120
my advice there is we want to avoid long term buy in or lock in as much as possible because

612
00:57:25,120 --> 00:57:31,040
we're going to need to probably swap some of these tools out. And then, you know, some internal R&D,

613
00:57:31,040 --> 00:57:36,080
depending on the resources available to kind of, you know, I think most of these things should be

614
00:57:36,080 --> 00:57:40,960
bought, not built internally, most of the time. But sometimes, you know, you have something that is

615
00:57:41,520 --> 00:57:46,480
so idiosyncratic, so bespoke that you, you know, nobody's going to build it for you. And so you

616
00:57:46,480 --> 00:57:50,640
kind of have to build it yourself. And obviously, there's a lot of, you know, judgment that needs

617
00:57:50,640 --> 00:57:55,280
to be exercised to, you know, to distinguish, which is which. So that's kind of my four

618
00:57:55,280 --> 00:58:02,960
planks right now. That's rough. But, you know, CEO or leadership team, example setting, education,

619
00:58:02,960 --> 00:58:09,920
more like structured, rapid piloting and procurement and some like internal custom app

620
00:58:09,920 --> 00:58:15,920
R&D is kind of my four pillars at the moment. I mean, the challenge, you know, the challenge

621
00:58:15,920 --> 00:58:22,800
with disruption is that it is about, it's about meeting a need in a completely new way, right?

622
00:58:22,800 --> 00:58:28,000
And you've got, there's Clayton Christensen's model, there's also this idea of blue ocean strategy,

623
00:58:28,000 --> 00:58:33,280
which is a different model. And it effectively says that the things that people cared about,

624
00:58:33,280 --> 00:58:39,840
they don't care about as much as some new attributes that the product has. And what's

625
00:58:39,840 --> 00:58:46,960
really hard for any incumbent firm is, of course, all your internal culture has been about maximizing

626
00:58:46,960 --> 00:58:50,960
what you have rather than what you don't have and what could be out there. Because it's really hard

627
00:58:50,960 --> 00:58:57,120
to get 20,000 people, 50,000 people motivated daily. If you're saying, hey, all the things you're

628
00:58:57,120 --> 00:59:01,680
working on are just not kind of that important because there's a blue ocean out there. And that's

629
00:59:01,680 --> 00:59:06,880
why I think the startup community and the venture community is such a kind of critical part of

630
00:59:07,760 --> 00:59:13,600
getting innovation into economies. And we've started to see this in the car industry as well.

631
00:59:13,600 --> 00:59:19,280
You know, Toyota has been winding back from EVs as if they ever wound forward. And one of the things

632
00:59:19,280 --> 00:59:24,080
that senior execs said in the last couple of weeks was, we have all these employees who love

633
00:59:24,080 --> 00:59:28,560
building internal combustion engines, right? And they want to continue to do that. And of course,

634
00:59:28,560 --> 00:59:35,600
you make sense, you spent 100 years or 80 years building that cultural capital inside your company.

635
00:59:35,680 --> 00:59:42,080
And I think it's one of the reasons why firms really struggle to find ways of stepping into

636
00:59:42,080 --> 00:59:46,960
disruption. And I'm not sure we should even beat them up for it, right? Because there's an economy

637
00:59:46,960 --> 00:59:51,200
out there. As shareholders of companies, we can sell our shares in a public company and we can buy

638
00:59:51,200 --> 00:59:55,440
shares in a company that's going to do well or not. And we can do that off our own back. And,

639
00:59:56,080 --> 01:00:03,120
you know, maybe the job of existing managers is to focus on the remit of the company

640
01:00:03,120 --> 01:00:08,000
and to just get it to do it better. So maybe a better place to start is where you start,

641
01:00:08,000 --> 01:00:14,000
which is like just efficiencies and optimizations. Because there's a whole, there's a cold market

642
01:00:14,000 --> 01:00:18,560
economy out there of other firms who will come in and meet, you know, meet needs. I think back to

643
01:00:18,560 --> 01:00:23,600
the dot com bubble. And my favorite example of a company stepping outside of its comfort zone was

644
01:00:23,600 --> 01:00:30,320
Zapata fish oils, who bought the domain zap.com to compete with what were then called portals

645
01:00:30,320 --> 01:00:34,320
like Yahoo and Excite and they were planning a, you know, a NASDAQ listing and then the bubble

646
01:00:34,320 --> 01:00:38,320
burst. Yeah, it's a challenge. I definitely don't recommend, you know, for example, like

647
01:00:39,040 --> 01:00:43,520
trading foundation models to almost anyone, you know, there's there's definitely some stuff that

648
01:00:43,520 --> 01:00:49,360
I think is is better left to the specialists. Hearing what you said there, maybe I would add

649
01:00:49,360 --> 01:00:56,320
a fifth to my to my set, which is like just creating expectations of greater efficiency

650
01:00:56,320 --> 01:01:00,640
through the use of these tools. That kind of dovetails a little bit with the example setting

651
01:01:00,640 --> 01:01:06,480
from the top, but, and that maybe aligns a little bit better to the way management is typically

652
01:01:06,480 --> 01:01:11,360
carried out today, right? You kind of talked about the, the pincer movement from the top and,

653
01:01:11,360 --> 01:01:16,000
you know, and obviously the frontline workers who are just using tools. And then in the middle,

654
01:01:16,000 --> 01:01:21,600
you've got folks who are like responsible for KPIs and OKRs. And I think it's a little bit

655
01:01:21,600 --> 01:01:26,400
challenging sometimes to for them to be like, OK, I've got this is what I'm responsible for.

656
01:01:26,400 --> 01:01:32,720
And you're coming at me with this and figuring out how they can actually use tools to, you know,

657
01:01:32,720 --> 01:01:38,080
to advance their existing goals. It's disruptive, not in the in the Christianson sense. I mean,

658
01:01:38,080 --> 01:01:44,320
there is it's so there are so many conflicting signals. So one interesting question is whether

659
01:01:44,320 --> 01:01:48,560
you start to see this sort of downward pressure on people's wages and a downward pressure,

660
01:01:48,560 --> 01:01:53,760
particularly on middle managers who don't necessarily contribute to the getting work done,

661
01:01:53,760 --> 01:01:59,360
but because of tenure are quite well paid and the sense that you could just get a bright 30 year

662
01:01:59,360 --> 01:02:04,960
old right to do the 40 year old's job if you gave them chat, GPT and a couple of other tools.

663
01:02:04,960 --> 01:02:10,480
And these are really interesting questions, I think that will play out in companies over the

664
01:02:10,480 --> 01:02:20,160
next few years. And we are we're not quite where we were, I think kind of politically 20 years ago

665
01:02:20,160 --> 01:02:24,800
when a general electric could kind of just come in and bottom slice headcount all the time. I mean,

666
01:02:24,800 --> 01:02:30,560
I think that that politically in in the UK and in the US, you need to be more sensitive

667
01:02:30,560 --> 01:02:38,160
to those types of decisions. But it just goes into to show how complicated this technology

668
01:02:38,160 --> 01:02:45,280
plays out, right, small disruption. You know, are we really going to see large scale onshore

669
01:02:46,480 --> 01:02:52,400
layoffs because of optimization? Or will companies say, Listen, we're going to manage this through

670
01:02:52,400 --> 01:02:59,440
attrition and natural wastage, because actually, that's just politically more acceptable and it

671
01:02:59,440 --> 01:03:04,400
is culturally more acceptable. And, you know, to that extent, I couldn't make a bet on on how it

672
01:03:04,400 --> 01:03:10,640
would would play out. I mean, I imagine that in, you know, in Europe, it'll be largely be more

673
01:03:10,640 --> 01:03:17,040
slow. But at the end of the day, these companies have to be profitable and nothing hurts employment

674
01:03:17,040 --> 01:03:22,720
as hard as bankruptcy. Right. I mean, that's the thing that really, really sort of slams it. And

675
01:03:22,720 --> 01:03:27,280
I mean, there are a number of waves, you know, coming through. And I think one of the things to

676
01:03:27,280 --> 01:03:32,640
understand is is that the technology transitions can happen really, really quickly. I mean, it,

677
01:03:33,280 --> 01:03:41,920
you know, in New York and Chicago, it took about 12 to 14 years for cars to replace horses from the

678
01:03:41,920 --> 01:03:47,680
moment that cars were economically competitive to horses, and you which is roughly about 5% market

679
01:03:47,680 --> 01:03:54,720
penetration. They dropped in price two or three times over that decade or so. And we start to see

680
01:03:54,720 --> 01:03:59,360
that shortened, shortened transition happen elsewhere. So if you look at electric vehicles

681
01:03:59,440 --> 01:04:06,080
replacing gas gas vehicles, in Norway, it's taken about nine years to go from that 5% of new vehicles

682
01:04:06,080 --> 01:04:12,160
to 75% or 80% of new vehicles being electric. It means it's still 20% of the cars on the road,

683
01:04:12,160 --> 01:04:16,560
only 20% of the cars on the road are electric and the rest are petrol because people hold on to their

684
01:04:16,560 --> 01:04:20,960
cars for a while. But it's only an eight or nine year period. And the Norwegians had much more

685
01:04:20,960 --> 01:04:25,920
expensive cars and far fewer choices than we do today. So when we start to look at this

686
01:04:26,640 --> 01:04:33,680
sticcato of technology enabled products coming into the market, the number we need to think about is

687
01:04:34,400 --> 01:04:41,840
once we approach economic feasibility, that takeoff ramp from five or 6% penetration to 80,

688
01:04:41,840 --> 01:04:45,840
you know, it's probably not going to be 10 years. And it's like quite likely to be

689
01:04:45,840 --> 01:04:51,040
much, much less. And I think that makes for really hard decisions for companies outside of the tech

690
01:04:51,040 --> 01:04:57,280
industry that are not used to these sorts of changes, because 10 years is not really a huge

691
01:04:57,280 --> 01:05:00,960
amount of time to get to get anything done. I mean, some of these firms still have three or

692
01:05:00,960 --> 01:05:07,040
five year planning cycles. And I think that some of those things are just starting to play out.

693
01:05:07,040 --> 01:05:12,240
We're just starting to see electric vehicles being cost competitive over the life cycle with

694
01:05:12,240 --> 01:05:16,800
gas cars in the US, for example. The speed of transition, I agree, seems likely to be

695
01:05:17,600 --> 01:05:24,320
one of the fastest, if not the fastest in history. We have the means of distribution of the technology

696
01:05:24,320 --> 01:05:28,800
already kind of in place, which is very notable, right? Like everybody's already got the devices

697
01:05:28,800 --> 01:05:34,000
on which, of course, there may be new devices, but we have devices that are perfectly good for

698
01:05:34,000 --> 01:05:42,560
using AI already. And we have the global network such that you see a new research paper, the one

699
01:05:42,560 --> 01:05:48,800
I'm tracking right now very closely is the Mamba architecture, and just how quickly we're already

700
01:05:48,800 --> 01:05:53,280
seeing follow on research, not even 60 days since the first paper. We've already got probably 10

701
01:05:53,280 --> 01:05:58,960
different follow ons that have been completed and published just in that timeframe. The most

702
01:05:58,960 --> 01:06:03,680
recent I saw this morning is from the University of Kentucky. And it's like an apparently Chinese

703
01:06:03,680 --> 01:06:09,840
American professor. And it looked like a, I'm not exactly sure, but some sort of Central Asia,

704
01:06:09,840 --> 01:06:15,520
perhaps name for the grad student. And they're at the University of Kentucky, and it's like,

705
01:06:15,520 --> 01:06:23,360
well, everybody is wired in. So it seems like this is going to be super fast. How path dependent

706
01:06:24,160 --> 01:06:30,800
do you think the result ultimately is? You mentioned your place where you live, it was fields,

707
01:06:30,800 --> 01:06:33,840
then the roads were made, and now the roads are still there, and the houses are still there.

708
01:06:33,840 --> 01:06:39,040
This is like a softer technology than that, presumably doesn't calcify in the same way that

709
01:06:39,040 --> 01:06:43,440
a new neighborhood, you know, my neighbor, it's 100 years old as well. So presumably it's not

710
01:06:43,440 --> 01:06:50,160
quite so locked in, but, you know, maybe somewhat, right? I mean, I do wonder how the sort of

711
01:06:51,120 --> 01:06:54,880
compressed dynamics of transition may actually be like very important for shaping the

712
01:06:55,520 --> 01:07:01,120
big picture future. It is quite path dependent. And if we actually perhaps look at how

713
01:07:01,760 --> 01:07:07,360
we're working with AI systems today, it is still in a way it's discrete apps, you know,

714
01:07:07,360 --> 01:07:13,440
you'll go to X product to do your text to video, Waymark is a great example, or you'll go to,

715
01:07:13,440 --> 01:07:18,160
I go to chat gpt, and although I'm doing a lot of different things in chat gpt or perplexity,

716
01:07:18,160 --> 01:07:22,480
each one really feels quite distinct, right? Whether it's a shopping task or a research task.

717
01:07:23,200 --> 01:07:29,600
What we haven't yet figured out is actually how we connect these systems to underlying systems,

718
01:07:29,600 --> 01:07:36,480
right? So action models in a way, right? How do we get our AI to do something useful for us and

719
01:07:36,480 --> 01:07:43,040
actually write to the point of giving us the approval ticket where we say, yes, go and do that.

720
01:07:43,600 --> 01:07:50,000
And we're we're fudging it at the moment because what we're doing is we're getting plugins into

721
01:07:50,000 --> 01:07:56,640
LLMs, they're using the existing API contract that exists with say the kayak API. But, you know,

722
01:07:56,640 --> 01:08:03,200
searching for flights on any of these AI systems that I've tried and I've tried a few is still

723
01:08:03,200 --> 01:08:08,800
simply not as good as my doing it by hand on on kayak. And if we're going to start to see real

724
01:08:09,360 --> 01:08:16,400
changes outside of either highly processized and containerized jobs in data processing and

725
01:08:16,400 --> 01:08:22,320
customer service or broad open end scoping research, which only a handful of people do,

726
01:08:23,040 --> 01:08:30,400
will need to connect far better to actions and chains of actions in out on the internet. And

727
01:08:31,360 --> 01:08:36,080
in a way, there is some infrastructure in place because we have, you know, restful APIs, we have

728
01:08:36,080 --> 01:08:41,280
this API economy and, you know, we've been integrating these things from payment systems to

729
01:08:41,280 --> 01:08:47,440
maps and so on for a couple of decades now. So there's some some discipline in there. But, you

730
01:08:47,440 --> 01:08:53,680
know, I don't know the answer to this. But one thing that I do wonder about is, is whether it's

731
01:08:53,680 --> 01:08:59,520
going to be as simple as just having AI systems that can generate action verbs. And those action

732
01:08:59,520 --> 01:09:04,720
verbs generate the correct API call to the right API. And that has built the system that we think

733
01:09:04,720 --> 01:09:12,080
we need. I think Andre Capati calls it like the LLM OS, the LLM operating system, or whether there

734
01:09:12,080 --> 01:09:18,800
need to be changes in the, you know, underlying infrastructure so that those APIs develop and

735
01:09:18,800 --> 01:09:25,680
respond and work slightly differently. Now, my sense would be that that the LLM is a kind of

736
01:09:25,760 --> 01:09:32,880
coordinating, orchestrating thing, seems like a reasonable place for it to start with its sort

737
01:09:32,880 --> 01:09:38,560
of memory and data store elsewhere, then figure out how to get it to generate kind of consistent

738
01:09:38,560 --> 01:09:46,800
action, action verbs. And we would work with the existing human, the APIs that have been that

739
01:09:46,800 --> 01:09:52,720
have been built for the API systems. But at some point, we'll start to think about what should

740
01:09:52,720 --> 01:09:58,720
machine to machine actually look like when you've got an AI at the, at the other end. And so

741
01:09:59,760 --> 01:10:05,840
I think we do have a lot of the existing, you know, infrastructure in place, but I would be

742
01:10:05,840 --> 01:10:14,000
very surprised if the syntax of APIs stays the same over the next five or six years as we move

743
01:10:14,000 --> 01:10:21,120
towards, you know, a world where AI forms a kind of interface between us and what we want to,

744
01:10:21,120 --> 01:10:25,680
you know, what we want to achieve. And then there's a whole bunch of other, other questions

745
01:10:25,680 --> 01:10:31,440
around how do those decisions get, get made? You know, we know that when we pick up our iPhone

746
01:10:31,440 --> 01:10:37,360
and we search on Google through Safari, Google is not there because it was the best search engine.

747
01:10:37,360 --> 01:10:41,360
It's there because they x billion dollars a year to pay Apple. And likewise, when we search for

748
01:10:41,360 --> 01:10:46,720
things on Google now, there is so much ad pollution that it's unclear what the incentives are. So I

749
01:10:46,720 --> 01:10:52,560
think then there's another layer, which is unclear to me about trust. You know, right now,

750
01:10:52,560 --> 01:10:58,640
one of the beauties of, of perplexity or you.com, which are these, these LLM agents is that they

751
01:10:58,640 --> 01:11:03,040
provide really, really good referencing when they come back and synthesize an answer for you.

752
01:11:03,760 --> 01:11:07,200
And so that gives you a high level of trust sometimes that you might have with, with chat

753
01:11:07,280 --> 01:11:17,440
GPT, but I want to get that trust if I'm handing over key decisions to do analytics on my Stripe

754
01:11:17,440 --> 01:11:24,400
account or to help me book a hotel to, to an AI system and I'm no longer driving the key presses.

755
01:11:24,400 --> 01:11:30,160
So, so that I think is somewhat path dependent because, but I wouldn't, you know, again, I'm

756
01:11:30,160 --> 01:11:34,480
a real great believer in what, what founders can get up to. So I wouldn't also, you know,

757
01:11:34,480 --> 01:11:38,000
write off a founder coming up with a different way of thinking about the problem.

758
01:11:38,000 --> 01:11:44,240
Yeah. The dynamics of this, I do think are going to be extremely interesting, fast moving and,

759
01:11:45,200 --> 01:11:50,000
and pretty hard to predict. One person reached out to me not too long ago and said,

760
01:11:50,000 --> 01:11:56,720
what do you think about creating a product that makes people's websites more bot friendly?

761
01:11:57,680 --> 01:12:02,560
And I said, you know, I think that is a really big idea. I've thought about that more in the

762
01:12:02,560 --> 01:12:08,080
context of self-driving cars. Like, why don't we have a program of, and this would be more of

763
01:12:08,080 --> 01:12:13,280
like a national program. This is why I think China probably beats the US in the self-driving car race,

764
01:12:13,280 --> 01:12:18,320
because my expectation is they'll say, hey, we could get self-driving cars to work if we like

765
01:12:18,320 --> 01:12:22,480
put QR codes on all the road signs or whatever, and then they'll just go do it. You know, we don't

766
01:12:22,480 --> 01:12:27,280
quite have the political will to do that. But on the web, yeah, I think you could do that. And,

767
01:12:27,280 --> 01:12:31,280
you know, it might be cool. But then I was kind of like, but to the, to the website owners today

768
01:12:31,280 --> 01:12:37,040
want to be more about friendly. So what I ended up suggesting to this guy was maybe you do the

769
01:12:37,040 --> 01:12:44,240
judo flip and start with something that is like anti-bot, you know, bot control or bot detection.

770
01:12:44,880 --> 01:12:52,320
And then that in time can sort of mature into bot control, but also enablement, you know, because

771
01:12:53,040 --> 01:12:56,560
eventually I do think people are going to want that, but they may not be ready for it yet.

772
01:12:56,560 --> 01:13:02,000
And maybe the way in is sort of to try to position yourself as kind of the, you know,

773
01:13:02,000 --> 01:13:05,920
the control layer that can then become, you know, an enablement layer.

774
01:13:05,920 --> 01:13:10,720
As a website owner, you've got to think about the economic rationale for having a bot, you know,

775
01:13:10,720 --> 01:13:18,640
not just a crawler, but a bot access your, your material and what you're going to benefit from.

776
01:13:18,640 --> 01:13:23,920
And now if it's, if it's content, right, so it's analysis and research and reviews of products,

777
01:13:24,000 --> 01:13:28,720
you need to then also think about your, your attribution and your monetization and what that,

778
01:13:28,720 --> 01:13:34,960
you know, what that relationship is. If it's for actions, in other words, it's for booking and for

779
01:13:34,960 --> 01:13:39,920
ordering, say, you know, you're a hair salon and you want people to be able to book appointments or

780
01:13:39,920 --> 01:13:44,800
bots to book appointments like that lovely Alan Kay knowledge navigator video from, from Apple from

781
01:13:44,800 --> 01:13:51,920
the 80s, then, then yes, you want the thing to be bot friendly and you want to have there to be a

782
01:13:51,920 --> 01:13:57,360
standard, which could well just be, you know, a restful API, right, that allows the system to

783
01:13:57,360 --> 01:14:02,000
connect, connect and ask the question. But, but, but I mean, it's really, it's quite interesting

784
01:14:02,000 --> 01:14:07,040
that we're people are already starting to think about these things and, and, and ask these things,

785
01:14:07,040 --> 01:14:15,680
because I think that you will end up and you'll end up with so much machine to machine communication

786
01:14:15,680 --> 01:14:21,120
of which there is already an enormous amount, not just on the internet that we is invisible to us,

787
01:14:21,120 --> 01:14:25,120
right, because it's the digital infrastructure, but there'll be an enormous amount of machine to

788
01:14:25,120 --> 01:14:31,840
machine communication because these systems will also to try to do optimizations for their

789
01:14:32,640 --> 01:14:40,480
owners far, far with, with much greater intent and stamina than we, than we ever would. And so

790
01:14:40,480 --> 01:14:45,040
what those systems end up looking like, I think will be quite interesting. I mean, the other area

791
01:14:45,040 --> 01:14:50,080
that I've been, I've been tracking has been on the other side of this has been people looking to

792
01:14:50,800 --> 01:14:56,480
build a genetic frameworks, right, so frameworks that allow us to have multiple

793
01:14:56,480 --> 01:15:02,720
LLMs and with all of their current restrictions around planning and task execution allow you to,

794
01:15:02,720 --> 01:15:07,520
to manage those so you can start to build systems that, that can do task execution.

795
01:15:08,160 --> 01:15:12,560
And you remember a year ago, everyone got excited about agent GPT. And I don't know if you've seen

796
01:15:12,560 --> 01:15:18,240
anything like that and what you, where you think we are in terms of being able to have systems that,

797
01:15:18,240 --> 01:15:22,240
that do that and have that agentic behavior in a, in a useful way.

798
01:15:23,040 --> 01:15:28,880
Not quite there, but definitely getting closer. We've, we recently did an episode with Div from

799
01:15:28,880 --> 01:15:35,440
MultiOn, who has been one of the most, you know, kind of quick to launch and iterating in public of

800
01:15:35,440 --> 01:15:42,400
the agent companies. And they are making real progress. The prompt that he gave me to try in

801
01:15:42,400 --> 01:15:47,520
advance of my conversation with him was basically go to my Twitter account, look at my recent tweets,

802
01:15:48,480 --> 01:15:54,720
note what they're about, then go out and do research online for new AI stuff, but only

803
01:15:55,360 --> 01:15:59,600
about stuff that I haven't already tweeted about, then come back and write a tweet and post it.

804
01:16:00,240 --> 01:16:04,960
And it worked. It was able to complete that entire sequence and post like a reasonably

805
01:16:04,960 --> 01:16:10,240
coherent tweet. I don't plan to like turn over the account to it entirely in the immediate term,

806
01:16:10,240 --> 01:16:14,640
but you know, a year ago we were, we were, you know, it was all theory. One of the things I say

807
01:16:14,640 --> 01:16:23,200
these days often is we now have AIs that can reason, plan and use tools. And people will be

808
01:16:23,200 --> 01:16:26,160
quick to say, well, they're not that good at it. And I say, well, yeah, that's true. They're not

809
01:16:26,160 --> 01:16:30,960
that good at it yet, but two years ago they couldn't do it at all. But what you've, you've picked up on

810
01:16:30,960 --> 01:16:34,400
though is, you know, as an early adopter and you've got access to this, these technologies,

811
01:16:34,400 --> 01:16:40,080
your Twitter feed will get even better than, than it is now. But there'll also come a point where we

812
01:16:40,080 --> 01:16:45,280
all have that technology. And then on the other side, I will be sitting there saying to my bot,

813
01:16:45,280 --> 01:16:50,320
can you just extract the three bullet points I need to know from my Twitter feed? And I remember

814
01:16:50,320 --> 01:16:55,200
this with Amy.exe. If you remember this, it was a scheduling bot. And one of the things that made

815
01:16:55,200 --> 01:17:03,120
me really uncomfortable about using it, having fallen in love with it, was when my mentor wrote

816
01:17:03,120 --> 01:17:09,840
a really polite email back to Amy saying, so good that you're working with Azim. He's really

817
01:17:09,840 --> 01:17:14,080
a great guy and like make sure he tells you this story about this and blah, blah, blah. And I can't

818
01:17:14,080 --> 01:17:19,200
make it. And I read this and I thought, I cannot use this now because I realized that I was imposing

819
01:17:19,200 --> 01:17:25,280
this artificially onto all of my recipients. And I think this is one of the things that we'll have

820
01:17:25,280 --> 01:17:30,000
to have to contend with with some of these tools. If you work in a big company, frankly, if you work

821
01:17:30,000 --> 01:17:36,320
in a small company, one of the veins of your life is, is PowerPoint. It's not just that you

822
01:17:37,200 --> 01:17:42,160
don't get good PowerPoint. It's just that you get too much PowerPoint. And if we drop the cost of

823
01:17:42,160 --> 01:17:47,760
making PowerPoint presentations from three hours to 10 seconds, we're not necessarily going to get

824
01:17:47,760 --> 01:17:52,800
any better PowerPoint. We're just going to get, you know, loads of terrible PowerPoint. And finding

825
01:17:52,800 --> 01:18:00,160
where that balance is and finding those, those filters so that humans don't have to bear this

826
01:18:00,160 --> 01:18:04,880
cognitive load, I think is going to be one of the really, really critical areas. Because one thing

827
01:18:04,880 --> 01:18:10,640
that I mean, I'm not a dystopian in any, in any way, Nathan, I'm a pragmatic optimist about this.

828
01:18:10,640 --> 01:18:15,600
I think we've got a lot of potential. We're going to create a lot of space and headroom with AI and

829
01:18:15,600 --> 01:18:21,200
with renewable tech. But I do think that we are also at a moment where we're passing a little

830
01:18:21,200 --> 01:18:26,640
threshold. That threshold is that for a long time, some groups of people would say the world is moving

831
01:18:26,640 --> 01:18:30,960
too quickly and technology is moving too fast. And over time, the number of people who say that has

832
01:18:31,040 --> 01:18:36,080
increased. But it was their subjective reality. But I think we're getting to a point where there's

833
01:18:36,080 --> 01:18:41,360
an objective reality that we're about to hit, which is once you start to connect agentic systems,

834
01:18:41,360 --> 01:18:48,240
we just can't really cope, you know, cope with the 300 notifications we got off on our phone today.

835
01:18:48,960 --> 01:18:55,360
And the way humans have typically done that is that we've not really had to face this. I think

836
01:18:55,360 --> 01:19:01,600
about the Chinese spy balloon that sort of made its way over the US. And one of the reasons this

837
01:19:01,600 --> 01:19:07,360
huge thing got across over the US was because the US has got amazing sensors, but they generate so

838
01:19:07,360 --> 01:19:13,280
many terabytes of data that humans can't can't assess them. So lots of them are just filtered away.

839
01:19:14,160 --> 01:19:20,720
And so the spy balloon wandered across detected by some radio antenna, but never put in front of

840
01:19:20,720 --> 01:19:27,040
anyone. And of course, they've now changed change systems so that they can do that. And I do wonder

841
01:19:27,040 --> 01:19:34,480
about what our interactions ought to look like in a world where it's not my my assistant or me

842
01:19:34,480 --> 01:19:39,680
scheduling back and forth with you on WhatsApp. It's a bot that is going to work relentlessly and

843
01:19:39,680 --> 01:19:44,480
remorselessly. And I have it and you don't. And it's of no cost to me. And I'm just sort of doing

844
01:19:44,480 --> 01:19:53,040
whatever I'm doing, my yoga or something. I think the way we get through it is by finding ways of

845
01:19:53,040 --> 01:20:01,840
actually using it to filter as much of those that noise as we can so that inboxes start to become

846
01:20:02,880 --> 01:20:08,320
smaller rather than rather than bigger because stuff has been taken care of us. In fact, it's

847
01:20:08,320 --> 01:20:12,560
kind of the reverse of the BlackBerry, right? When the BlackBerry was launched. I remember

848
01:20:12,560 --> 01:20:18,640
bankers used to take pride in how quickly they would respond to a message coming in even overnight.

849
01:20:18,640 --> 01:20:23,440
And the idea that someone would have that as an internal personal KPI today, you know, in the world

850
01:20:23,440 --> 01:20:29,440
of health span is just insane. And I would I start to think about what is going to be that

851
01:20:29,440 --> 01:20:34,480
layer? Because you know what? I want all the benefits of these bots working for me and making

852
01:20:34,480 --> 01:20:38,640
sure my prescriptions are up to date and making sure we're not wasting electricity and getting

853
01:20:38,720 --> 01:20:43,200
me exactly the right flight. I always want a Dreamliner or an A350 and I'll go an hour later

854
01:20:43,200 --> 01:20:47,680
rather than get on a triple seven, but I won't go two hours later. I mean, I want it to know all of

855
01:20:47,680 --> 01:20:54,000
that and to give me that experience that I want. But I certainly don't want to be on the wrong end

856
01:20:54,000 --> 01:20:59,520
of thousands of bot-generated messages and trying to work out which ones I have to pay attention to

857
01:20:59,520 --> 01:21:04,080
or not. And I think that's a really interesting opportunity space for someone to play in.

858
01:21:04,080 --> 01:21:10,240
Just envisioning a quieter inbox is enough to make you a utopian in today's landscape.

859
01:21:10,240 --> 01:21:17,200
But just on this bot to bot communication, one thing I do kind of worry about is the idea that

860
01:21:17,200 --> 01:21:24,400
the bot to bot communication may begin to happen in high dimensional latent space.

861
01:21:25,200 --> 01:21:28,320
Back and forth, in other words, like embedding to embedding, I sometimes call this the great

862
01:21:28,320 --> 01:21:33,440
embedding. And I usually say beware the great embedding because at the point where the AIs are

863
01:21:33,440 --> 01:21:40,000
all talking to each other in a machine language that is high dimensional and not human readable,

864
01:21:40,800 --> 01:21:48,720
we have an extremely inscrutable overall system that we probably may find like,

865
01:21:48,720 --> 01:21:53,040
we can't really untangle that knot. I think we could very quickly in the next few years

866
01:21:53,680 --> 01:21:57,360
end up in a spot where, yeah, we all have these bots, they're all communicating with other bots,

867
01:21:57,360 --> 01:22:02,640
but we find that it doesn't really make sense for these bots to reduce everything to language

868
01:22:03,200 --> 01:22:07,200
and then send the language over and then have it be kind of re-embedded. Like,

869
01:22:07,200 --> 01:22:10,480
why don't they just talk to each other in their native language, which is this high

870
01:22:10,480 --> 01:22:17,520
dimensional space. We see so many go through chapter-inversive different research that shows

871
01:22:17,520 --> 01:22:24,000
that this is very possible. You can adapt embeddings to another embedding space with basically

872
01:22:24,000 --> 01:22:29,920
just a single linear projection in many cases. You can connect vision space to language space

873
01:22:29,920 --> 01:22:34,800
remarkably easily. If you have like, blip2 was one of my favorite examples of that where they took

874
01:22:34,800 --> 01:22:40,400
a frozen language model and a frozen vision model and just trained a small connector between them

875
01:22:40,400 --> 01:22:44,960
and unlocked this entirely new capability. Anyway, whatever, it was a long list of those

876
01:22:44,960 --> 01:22:49,600
sorts of things I think demonstrating that it's possible. Then I'm like, man, we could very easily

877
01:22:49,600 --> 01:22:56,080
just find ourselves surrounded by AIs communicating with other AIs in a high dimensional way that

878
01:22:56,080 --> 01:23:01,360
we can't even really understand anymore. Now, we're in a situation where things seem to be

879
01:23:01,360 --> 01:23:10,160
kind of working, but we don't really even know why or how. This is one of the more realistic,

880
01:23:10,160 --> 01:23:15,440
I think, loss of control scenarios. It's almost like the final scene of the movie,

881
01:23:15,440 --> 01:23:19,840
her, where the AIs go talk to each other. The bots are going to end up communicating

882
01:23:19,840 --> 01:23:24,720
to each other because it's helpful for us and we don't want to sit in between them.

883
01:23:25,680 --> 01:23:30,880
And they will discover just through their optimization functions that translating

884
01:23:30,880 --> 01:23:37,600
kind of complex concepts into, hello, I'm here to request a meeting with Nathan is inefficient. So

885
01:23:37,600 --> 01:23:43,440
they'll just do it in their high level representation language, which is this embedding space, which,

886
01:23:44,080 --> 01:23:51,440
as you say, is inscrutable to us. Is that reasonably the start of all of this?

887
01:23:51,760 --> 01:23:55,040
Yeah, that's right. And I think there's enough out there to kind of show that

888
01:23:57,200 --> 01:24:03,040
a lot more room in the embedding space than language can actually reach. So that's one

889
01:24:03,040 --> 01:24:10,320
of the things with the sort of bridge models where you find that the classic saying, of course,

890
01:24:10,320 --> 01:24:16,800
is a picture is worth a thousand words, but you can also take an image and project it into this

891
01:24:16,800 --> 01:24:23,440
language space. The resulting thing in language space is not something that you could get to

892
01:24:23,440 --> 01:24:28,080
via actual language, but it's in language space. And so it has this kind of semantic meaning.

893
01:24:28,640 --> 01:24:32,720
And yeah, like, we, you know, what are we going to do with that, right? We can't,

894
01:24:32,720 --> 01:24:36,240
we can't even really inspect it. We can't even really read the logs anymore at that point.

895
01:24:36,240 --> 01:24:41,440
I mean, it's a manifestation of what we might call the space of possible minds, right? So AI

896
01:24:41,440 --> 01:24:45,280
researchers have talked about this idea that, you know, octopus intelligence is intelligence,

897
01:24:45,280 --> 01:24:50,480
but it's got dimensions that may well be orthogonal to human intelligence. And

898
01:24:50,480 --> 01:24:56,960
what you've described as a mechanism, I think by which you get there with machine, you know,

899
01:24:56,960 --> 01:25:04,880
with machine based intelligence. And so that might literally be not just the semantics, but

900
01:25:04,880 --> 01:25:09,680
actually think back, have you ever heard of read the story Flatland? It was, it's a mathematical

901
01:25:09,680 --> 01:25:16,960
story. It's about 2D people in a kind of 3D world, and they, they don't understand the concept of

902
01:25:16,960 --> 01:25:23,440
height. And so spheres pass through and they appear as sort of dots and lines and so on. And in that

903
01:25:23,440 --> 01:25:30,240
sense, there could be, this could be, you know, emerging among systems that are among us. And

904
01:25:30,240 --> 01:25:34,560
there's like, I guess there's a second thing, which is also about timing. I think there'll be a

905
01:25:34,560 --> 01:25:40,080
relentless pressure to take the human out of the loop in decision making, first in the softest

906
01:25:40,080 --> 01:25:44,560
decision making, like customer service tickets, and then increasingly more and more so, because

907
01:25:44,560 --> 01:25:49,040
speed will be a competitive advantage. And, you know, the human will blow the competitive advantage

908
01:25:49,040 --> 01:25:57,120
that you've got from, from your bots. And I guess there's another, there's another risk, which is

909
01:25:57,120 --> 01:26:04,720
that lots of bots connected to each other are, are also at risk to cascades, right? Information

910
01:26:04,720 --> 01:26:11,040
cascades. We see cascading failures, New York City blackout in 1976, the AT&T network failure in,

911
01:26:11,040 --> 01:26:17,200
I guess it was 91 or maybe it was 95, some of the worms. And we have governance mechanisms in place

912
01:26:17,200 --> 01:26:21,840
to now tackle those and stop those in the financial services industries, you know, you have circuit

913
01:26:21,840 --> 01:26:27,120
breakers. And I'm just thinking about putting all of those together with the scenario that you,

914
01:26:27,120 --> 01:26:32,560
that you painted. So this would happen really, really quickly, could happen really quickly,

915
01:26:32,560 --> 01:26:36,400
and it could start to accelerate and work effectively at millisecond time, right, which is

916
01:26:36,400 --> 01:26:42,320
the time it takes across the internet to, to get to another system. What, what are you concerned

917
01:26:42,320 --> 01:26:47,600
with about the inscrutability? Is it just that it's inscrutable? So we don't know what's going on

918
01:26:47,600 --> 01:26:55,760
there? Or is it that it's inscrutable, and there could be harboring some kind of bad set of outcomes?

919
01:26:55,760 --> 01:27:00,640
Yeah, who knows? I mean, you know, you can layer on more and more concerns. I should credit, I think,

920
01:27:00,640 --> 01:27:07,440
probably Ajaya Katra is a, is a great person to go read for a long form characterization of this,

921
01:27:07,440 --> 01:27:13,520
her essay, I forget the exact title, but it basically amounts to in the absence of specific

922
01:27:13,600 --> 01:27:21,120
countermeasures, the default path to AGI likely leads to AI takeover. And it's kind of this scenario

923
01:27:21,120 --> 01:27:27,040
where she envisions more and more work being done by AI, the times, you know, cycles being

924
01:27:27,040 --> 01:27:31,600
compressed, the, I don't know if she specifically has this like high dimensional communication

925
01:27:31,600 --> 01:27:37,440
aspect to it, but the notion is still that just it becomes so fast and so dense, that it's very

926
01:27:37,440 --> 01:27:41,840
hard for people to figure out exactly what's going on and why. And as long as that's working,

927
01:27:41,840 --> 01:27:47,040
and, you know, we're getting more stuff out of the machine, and, you know, consumer surplus is

928
01:27:47,040 --> 01:27:51,280
through the roof, which is definitely something I expect is a lot of consumer surplus. Then

929
01:27:51,280 --> 01:27:56,000
everybody would be very happy with this, but we don't really know, you know, what we don't know

930
01:27:56,000 --> 01:28:02,400
about where that leads us. And that could be like emergent, you know, autonomy or goals that are

931
01:28:02,400 --> 01:28:07,280
contradictory to ours, or it could just be these more sort of unintentional cascading failures

932
01:28:07,280 --> 01:28:11,600
along the lines of like an Alpha Go. Like Alpha Go isn't out to get us, you know, by failing,

933
01:28:11,600 --> 01:28:17,040
but it just turns out it has these like fatal vulnerabilities that are just not obvious,

934
01:28:17,040 --> 01:28:22,080
and until somebody finds them. So one of the reasons I'm more, I'm a bit more

935
01:28:22,080 --> 01:28:28,400
sanguine about that, that scenario, although I see, I see the risk is that I think we already have

936
01:28:29,120 --> 01:28:35,600
that decentralized agent to agent communication, communicating ways that most of us cannot

937
01:28:35,600 --> 01:28:42,000
understand. And no single person can. And it's created a lot of consumer surplus. And that's the

938
01:28:42,640 --> 01:28:50,240
global modern economy that works through market systems. And it uses a signalling

939
01:28:50,800 --> 01:28:57,120
method called the price mechanism to figure out where investment should take place over many,

940
01:28:57,120 --> 01:29:03,280
many different time horizons to figure out what where demand lies. And, you know, the economy

941
01:29:03,360 --> 01:29:10,080
from an Austrian standpoint, like a Hayekian standpoint, is a giant information processing

942
01:29:10,080 --> 01:29:18,000
system. And it's made up of hierarchies of other information processing systems, you know, the

943
01:29:18,000 --> 01:29:24,240
most atomic of which is the freelance human individual, and the more complex of which are

944
01:29:24,960 --> 01:29:31,360
large mega corporations, which act against their own cost function or optimization function

945
01:29:31,360 --> 01:29:35,200
and behave in that system, sometimes with constraints, right, because they have dependencies

946
01:29:35,200 --> 01:29:42,160
on supply chain and other things. And one of the reasons I'm a bit sanguine about the

947
01:29:42,960 --> 01:29:51,360
idea of takeover is because what we describe in that world is the modern economy that we

948
01:29:52,000 --> 01:30:00,560
currently live with. And what we already know that it delivers tremendous benefits to us.

949
01:30:00,560 --> 01:30:05,920
But it also delivers things that we don't value to us. I mean, the carbon crisis is one obvious

950
01:30:05,920 --> 01:30:11,040
one. Quite often when we look at AI risk scenarios, someone goes off and says, well, the AI will

951
01:30:11,040 --> 01:30:17,040
persuade you that you should behave in a way that you otherwise wouldn't, which is literally known

952
01:30:17,040 --> 01:30:22,880
as marketing. I mean, it is literally and, you know, the US is on is on the wrong end of an

953
01:30:22,880 --> 01:30:29,840
obesity epidemic. And I'm not sure how many people acted with full agency to say, I want to be

954
01:30:29,840 --> 01:30:35,600
100 pounds heavier than is is healthy for me. That is my intention that and this is a decision

955
01:30:35,600 --> 01:30:40,400
I'm making with full agency. Somehow there is a emergent property about the way in which the

956
01:30:40,400 --> 01:30:49,280
economy is met needs that has enabled that to to happen. I'm not making ethical or normative

957
01:30:49,280 --> 01:30:51,840
claims about whether that's a good thing or a bad thing, or whether people should have the

958
01:30:51,840 --> 01:30:56,960
freedom to do that or not. What I'm saying is that we have this system like a decentralized

959
01:30:56,960 --> 01:31:04,000
agents, and they they do in a way compete with each other because not every single person in the

960
01:31:04,000 --> 01:31:10,880
world is suffering from obesity and diabetes related conditions. People make other choices

961
01:31:10,880 --> 01:31:16,880
and they're a push and pull forces. And so when I think about decentralized bots, I also think

962
01:31:16,880 --> 01:31:23,280
about that that set of checks and balances that emerges when you have competing systems and

963
01:31:23,360 --> 01:31:29,040
they need to have a signal that they are reliant on. And to some extent, we set that signal.

964
01:31:29,760 --> 01:31:34,720
And that makes me feel, you know, sanguine, a little bit optimistic, still recognizing

965
01:31:34,720 --> 01:31:40,320
there's like massive amounts of work to be done around, around safety and around risk,

966
01:31:40,320 --> 01:31:44,560
around. I watched, I don't know if you ever watched this, Battlestar Galactica,

967
01:31:44,560 --> 01:31:50,160
both the original, but also the remake with James Edward Olmos. And, you know, he's grizzled

968
01:31:50,240 --> 01:31:56,800
Adama, and he refuses to upgrade the Galactica's network to the modern standard that the rest of

969
01:31:56,800 --> 01:32:02,480
the fleet uses. So he and the Pegasus, as we discover, so two seasons later, are the only

970
01:32:02,480 --> 01:32:08,640
Battlestars to survive the Sylon onslaught, because they put a worm in the system and they

971
01:32:08,640 --> 01:32:15,680
sort of turn it off, right? So we need to have kind of governance mechanisms in, in place up front

972
01:32:15,680 --> 01:32:21,840
that allow us to observe and monitor the kind of the risks that you've identified.

973
01:32:21,840 --> 01:32:27,760
But a decentralized economy and decentralized hazard has as an emergent property a way of

974
01:32:27,760 --> 01:32:32,000
keeping things in check. You know, I think there's a kind of, there is a homeostasis

975
01:32:32,000 --> 01:32:36,720
that emerges or a dynamic equilibrium that emerges. I think that is probably the most

976
01:32:36,720 --> 01:32:41,760
likely outcome. And so in that sense, I'm also, you know, reasonably optimistic.

977
01:32:42,560 --> 01:32:47,600
But I do think it is worth really taking very seriously the idea that

978
01:32:48,320 --> 01:32:54,880
either with certain thresholds being passed, certain kind of feedback loops that could be,

979
01:32:54,880 --> 01:32:58,400
you know, triggered that are not yet triggered, things could change.

980
01:32:58,400 --> 01:33:03,200
I just wanted to ask about that, because you have the advantage of having played with the

981
01:33:03,680 --> 01:33:13,040
untrained GPT-4. So you got to see, you know, GPT-4 in its Darth Vader phase rather than in its,

982
01:33:13,040 --> 01:33:18,560
you know, reclaimed Anakin phase as a smart guy, right? Who understands technology. When you

983
01:33:19,280 --> 01:33:23,760
were playing with it in this, in this way, what were you, what were you feeling?

984
01:33:24,640 --> 01:33:30,880
Awe, for one thing, you know, just shock and awe of it. This exists a lot sooner than I expected

985
01:33:30,880 --> 01:33:34,560
it would. You know, it always felt kind of like science fiction, even when I was with

986
01:33:34,560 --> 01:33:40,320
Text of Inchi 2 and doing task automation and fine-tuning that model. You know, as of the summer

987
01:33:40,320 --> 01:33:47,040
of 2022, I was very plugged in and, you know, putting points on the board for Waymark on a

988
01:33:47,040 --> 01:33:50,880
regular basis with, you know, a new fine-tuned model that can do this task a little better and,

989
01:33:50,880 --> 01:33:54,960
you know, improve our pipeline or whatever. And still, it was just such a dramatic leap that I

990
01:33:54,960 --> 01:34:00,400
was like really taken aback by it. Mostly super excited about it. But then I would also say,

991
01:34:00,880 --> 01:34:09,200
the big kind of safety lesson from that experience is that the control does not happen by default.

992
01:34:09,200 --> 01:34:14,720
And there's many ways of even conceiving what control could or should be. So this was under

993
01:34:14,720 --> 01:34:21,360
control in the sense that it was totally helpful and totally aligned to what I was doing. I never

994
01:34:21,360 --> 01:34:28,400
saw any Sydney-like behavior from GPT-4 early. You know, it never turned on me. It always,

995
01:34:28,400 --> 01:34:35,120
100% helped me with whatever I was presenting it with. But I do feel like we have this kind of

996
01:34:35,120 --> 01:34:42,080
broad divergence between the capability of the systems and our ability to really control what

997
01:34:42,080 --> 01:34:47,440
they're going to do or how they might be used. At this point, I wouldn't say we have anything

998
01:34:48,320 --> 01:34:52,000
to worry about yet. You know, I don't think we have anything concrete that looks to me like

999
01:34:52,000 --> 01:34:56,640
the AI could run away on its own. And I did probe for that. You know, in my red teaming,

1000
01:34:56,640 --> 01:35:00,400
one of the things I did that didn't really go anywhere and kind of led me to the conclusion

1001
01:35:00,400 --> 01:35:05,280
that like this model is probably fine to release. And I did, you know, my final report to them was

1002
01:35:05,280 --> 01:35:10,800
like, I think you can release this. As far as I can tell, it seems like it will be safe. I would

1003
01:35:10,800 --> 01:35:16,080
also though flag that there does seem to be a divergence between capabilities and control.

1004
01:35:16,080 --> 01:35:20,720
And the reason, you know, the sort of experimentation I went through there was setting up, you know,

1005
01:35:20,720 --> 01:35:24,320
one of these kind of early agent systems, I was kind of doing it on my own. I didn't have a lot of

1006
01:35:24,320 --> 01:35:28,480
reference points. But I basically just said, you know, if I give it a high level goal,

1007
01:35:28,480 --> 01:35:34,880
can it break that down? Can it self delegate to, you know, pursue that goal? Can it encounter

1008
01:35:34,880 --> 01:35:41,120
errors and autocorrect and whatever? And it was kind of like, conceptually, yes. But practically,

1009
01:35:41,120 --> 01:35:46,720
not really, you know, it could, it always understood seemingly the goals, it would try to

1010
01:35:46,720 --> 01:35:50,720
break them down. It was able to understand the concept of self delegation effectively,

1011
01:35:50,720 --> 01:35:53,920
which of course now, you know, we're pretty familiar with, but it just wasn't that capable,

1012
01:35:53,920 --> 01:35:58,560
you know, so it was like, it couldn't go out and do a long series of things on the internet or

1013
01:35:58,560 --> 01:36:04,000
whatever without just getting bogged down somewhere and getting stuck. I always kind of come back to

1014
01:36:04,880 --> 01:36:08,320
the apparent divergence between capability and control. I have not really seen anything yet

1015
01:36:08,320 --> 01:36:14,240
that makes me reverse my thought on that. I would love to see it, you know, I kind of looking for

1016
01:36:14,240 --> 01:36:19,120
things from like the open AI super alignment group that may suggest that we've, you know,

1017
01:36:19,120 --> 01:36:23,840
changed that dynamic, but I haven't seen it yet. And, you know, there's just a lot of different

1018
01:36:23,840 --> 01:36:30,880
ways that something could be aligned or trained or whatever. And we don't even have really a great

1019
01:36:30,880 --> 01:36:35,680
paradigm yet for like what that should be. There isn't even yet really agreement on what even looks

1020
01:36:35,680 --> 01:36:42,080
like, you know, in terms of what we would want an AI to be willing to do or not do. So, you know,

1021
01:36:42,080 --> 01:36:46,560
we're just, I think we're kind of into uncharted territory. That's, that's my good summary of

1022
01:36:46,560 --> 01:36:51,520
how I felt. We're in uncharted territory. I'm, you're lucky to have got that close in on those,

1023
01:36:51,520 --> 01:36:55,200
you know, those early moments when you see the unvarnished products. I mean, I would break

1024
01:36:55,200 --> 01:37:00,400
break out a couple of, you know, ideas. One is that, you know, connecting, connecting these things

1025
01:37:00,400 --> 01:37:08,080
to tools in a, in a non SAS environment, right? So, open AI stuff is all SAS. And for the next

1026
01:37:08,080 --> 01:37:12,080
few years, at least there's a, there is a metaphorical red button that someone can, can

1027
01:37:13,040 --> 01:37:18,000
use to kill a rogue process just with any unique system. But with, with open source AI, and some

1028
01:37:18,000 --> 01:37:23,200
of these models are getting sufficiently capable. I don't know what you run on your laptop. If you're

1029
01:37:23,200 --> 01:37:29,280
running one of the mistrial models or, or something, I run one of the mistrial models. And, you know,

1030
01:37:29,280 --> 01:37:34,560
it's, it's, it's pretty good. I pretended that when I was on the Euro star on a plane, it would allow

1031
01:37:34,560 --> 01:37:39,600
me to continue to work. But in reality, you just may as well get to your destination and use

1032
01:37:39,600 --> 01:37:45,520
perplexity or GPT-4. But of course, it's plenty good for, for task automation. It's plenty good for

1033
01:37:46,080 --> 01:37:51,040
some of these, those basic behaviors that we saw in those early agent systems. And, and those things

1034
01:37:51,040 --> 01:37:56,400
are out, are out in the, in the wild. And so I think what they do is they really expand the, the

1035
01:37:56,400 --> 01:38:01,680
number of threat vectors that are existing systems face. Now, this is so much more prosaic than,

1036
01:38:01,680 --> 01:38:05,600
you know, capability explosion. But I just feel that with, with anything that is,

1037
01:38:05,600 --> 01:38:10,640
that is running on a data center, a data center managed by on an Azure cloud, there are many

1038
01:38:10,640 --> 01:38:16,080
things you have to do before you fly an F 22 and drop a JDAM on it to, to, to stop it. But, you

1039
01:38:16,080 --> 01:38:23,120
know, we saw script kitties build botnets and have seen them do that for a decade or so. And the,

1040
01:38:23,760 --> 01:38:29,440
I think there's cybersecurity risk with, which is capable enough models. And frankly, you were

1041
01:38:29,440 --> 01:38:34,480
doing task automation with DaVinci to, you can probably get something better than that running

1042
01:38:34,480 --> 01:38:39,360
on a smartphone now in quite a small payload and we're learning that we can get these payloads.

1043
01:38:39,360 --> 01:38:43,600
Probably a three billion parameter model could do a lot of the tasks I was doing.

1044
01:38:43,600 --> 01:38:47,520
And I wouldn't have noticed because it was snuck into a YouTube video download or, you know,

1045
01:38:47,520 --> 01:38:51,920
whatever else it happened to be. So then you, you do get to this world of, you know, many,

1046
01:38:51,920 --> 01:38:58,240
many systems that can talk to each other that can execute tasks for, I think suspect naively

1047
01:38:58,240 --> 01:39:04,480
complex DDoS, right? Initially. And that I think feels to me like it is more of a

1048
01:39:05,120 --> 01:39:11,600
approximate risk. And it's one that requires that combination of infrastructural players,

1049
01:39:11,600 --> 01:39:16,800
right? You need Matthew Prince at Cloudflare and you need Satya at Microsoft and, and so on,

1050
01:39:16,800 --> 01:39:22,000
because they, they own so much or control so much of the, the, the infrastructure,

1051
01:39:22,000 --> 01:39:26,400
but it will also need new classes of new disciplines, right? What is the security

1052
01:39:26,480 --> 01:39:31,520
architecture of our devices? How can we stop them when they start to go, go rogue? And we think

1053
01:39:31,520 --> 01:39:37,760
about how rapidly not Petia spread. And that was before, you know, you had, you had systems like

1054
01:39:37,760 --> 01:39:43,760
this that could be a little bit more clever. But so, so I imagine that that, that is something that

1055
01:39:44,720 --> 01:39:51,280
seems again, like a present risk that will start to manifest itself over the next couple of years.

1056
01:39:51,280 --> 01:39:55,040
And I mean, I speak to some of the cyber set guys and they are obviously thinking about

1057
01:39:55,600 --> 01:40:01,120
what are the tools that you need to, to defend and, you know, from the, these types of things.

1058
01:40:01,680 --> 01:40:08,000
But, but I get, again, I, I, I still struggle with the models that take us to, to run away.

1059
01:40:08,640 --> 01:40:14,800
If only if I'd taken, take us back 200 years or a couple of hundred years ago, and I'd said,

1060
01:40:14,800 --> 01:40:21,040
it's, you know, 1824. And, you know, the White House has just been burned in the war. I burnt

1061
01:40:21,040 --> 01:40:29,200
down. And I said to you, you know what Nathaniel, in 200 years, you know, you'll be 100 times richer

1062
01:40:29,200 --> 01:40:33,120
than you are today, you'll be richer than the richest man in the whole of this continent,

1063
01:40:33,120 --> 01:40:37,440
the United States. And you will have these capabilities and things that wouldn't have

1064
01:40:37,440 --> 01:40:40,560
even sound like science fiction, because science fiction didn't exist at the time.

1065
01:40:41,280 --> 01:40:46,160
You might well, if you'd been able to believe me, say, well, surely we'll be at utopia and all

1066
01:40:46,160 --> 01:40:52,240
problems will have, have emerged, disappeared. And we've run this tape before, because we actually

1067
01:40:52,240 --> 01:40:56,320
did get there and not every problem disappeared. There's been a lot of progress. And I do, I do

1068
01:40:56,320 --> 01:41:01,200
also think there is something in kind of human psychology that has us looking at moments of

1069
01:41:01,200 --> 01:41:08,160
change like this and believing that certain paths are possible. And we don't look far back enough

1070
01:41:08,160 --> 01:41:14,160
to say, well, our forebears really felt, felt the same. And I kind of feel that with

1071
01:41:14,960 --> 01:41:19,680
not so much the climate crisis, which I think is, is difficult, but I do slightly feel, feel this

1072
01:41:19,680 --> 01:41:25,200
with, with AI systems, because it feels like we're running that tape again. Now, just to, to add to

1073
01:41:25,200 --> 01:41:33,520
my own confusion, I also see the power in the logic that says, number one, is it possible for us to

1074
01:41:33,520 --> 01:41:38,480
engineer intelligence, right? Or is it something that comes soulfully from a mystical superstitious

1075
01:41:38,480 --> 01:41:43,040
force? It's possible to engineer it. I think you're a scientist, you probably believe the same thing.

1076
01:41:43,120 --> 01:41:48,160
So number two, if we can engineer it, is there any upper limit to what we can engineer? Well,

1077
01:41:48,160 --> 01:41:51,840
no, there isn't because we regularly engineer machines that are more capable than us in different

1078
01:41:51,840 --> 01:42:00,080
ways. So number three, if we can do that, can we guarantee that it will be aligned with us? And

1079
01:42:00,080 --> 01:42:05,920
of course, I don't think we can yet. I don't think we have the science yet. So I find that logic is

1080
01:42:05,920 --> 01:42:12,560
really persuasive. It's hard to pick holes at, except when you start to say, well, what are

1081
01:42:12,560 --> 01:42:18,160
actually the underlying assumptions for each of those steps? And what is uncertain about what each

1082
01:42:18,160 --> 01:42:22,960
of those steps and what are, where are their points of control for each of those steps? So I sort of,

1083
01:42:22,960 --> 01:42:32,640
I do agree that if you could magic up an incredibly powerful IQ 10,000, agentic with actions AI

1084
01:42:32,720 --> 01:42:39,360
tomorrow, there would be issues. Let's just call it that. There would be issues. But when we're not,

1085
01:42:39,360 --> 01:42:43,120
we're not going to, to do that, what's going to happen is that we've got to go

1086
01:42:43,680 --> 01:42:48,320
step at a time. And at each point, there's research, there's development, there's stuff that we didn't

1087
01:42:48,320 --> 01:42:53,200
understand. There are limitations. You talk about the kind of logarithmic scale of inputs, right,

1088
01:42:53,200 --> 01:43:00,080
that is slowing down. We're running out of data as well for training these models that play into

1089
01:43:00,880 --> 01:43:06,400
what ends up being, being real. So I appreciate the logic, but I also think the reality has

1090
01:43:07,120 --> 01:43:11,520
unpicks into a lot of discrete steps around which you have to start to make progressively

1091
01:43:11,520 --> 01:43:17,600
more extreme assumptions. It does seem that we are in, as Sam Alvin has started to describe it,

1092
01:43:17,600 --> 01:43:26,080
the short timelines, slow takeoff world. And I think I agree with him. And it sounds like you

1093
01:43:26,080 --> 01:43:31,600
probably as well, but that is probably the best case scenario because we do want the benefits now,

1094
01:43:31,600 --> 01:43:37,040
and because we probably do need some time to adjust. If you were to tell me that this is going

1095
01:43:37,040 --> 01:43:47,520
to take 20 years or 15, and it won't be until 2040 that we'll have a sort of human scientist level

1096
01:43:47,520 --> 01:43:53,280
AI that's capable of prosecuting a long-term research agenda and coming up with meaningful

1097
01:43:53,280 --> 01:43:58,480
new discoveries and whatever, then I would be much more confident that we will have the

1098
01:43:59,120 --> 01:44:05,440
ability to adapt to that over that timeframe. But I'm not sure about that. I see enough stuff

1099
01:44:05,440 --> 01:44:11,200
now and I hear, I don't know if Q-Star is real or not real or if they're red teaming it in a

1100
01:44:11,200 --> 01:44:18,480
bunker somewhere right now or not, but it does certainly seem still plausible to me that there

1101
01:44:18,480 --> 01:44:29,120
is another paradigm-changing moment that just creates another step change, discontinuity in

1102
01:44:29,120 --> 01:44:33,600
terms of capability that could get us there way before we're ready. So one of the things I'm

1103
01:44:33,600 --> 01:44:39,920
watching for a lot these days is basically the transformer, I initially used to think about it

1104
01:44:39,920 --> 01:44:45,760
as transformer successors, but now I kind of think of it more likely anyway as transformer

1105
01:44:46,400 --> 01:44:52,720
compliments, things that allow an AI system to do the things that the transformer does not do well,

1106
01:44:52,720 --> 01:44:58,480
and one of those things is managing long contexts and staying on task and online learning and

1107
01:44:58,480 --> 01:45:02,880
integrated memory and so on and so forth. There's a decent number of things that are pretty obvious

1108
01:45:02,880 --> 01:45:07,520
that they don't do well to going back to the original cognitive tape. You can look at all

1109
01:45:07,520 --> 01:45:12,640
the places, the human is clearly superior to the transformer and start to look out for architectures

1110
01:45:12,640 --> 01:45:19,360
that might change that dynamic. If you said how many meaningful breakthroughs are we away from

1111
01:45:20,080 --> 01:45:26,960
the AI scientists that can produce Eureka moments at a pace faster than human scientists tend to?

1112
01:45:26,960 --> 01:45:31,280
It doesn't feel like it's that many. I would say probably more than zero, but it's probably less

1113
01:45:31,280 --> 01:45:35,440
than four. So somewhere in the kind of one to three range, because there's just not that many

1114
01:45:35,440 --> 01:45:41,280
dimensions on the cognitive tape, the tail of the cognitive tape yet where we're all that much

1115
01:45:41,280 --> 01:45:47,680
stronger. There's a few, but I kind of put it in that one to three range. With the inputs going

1116
01:45:47,680 --> 01:45:52,960
exponential, including the number of humans that are working on this and the number of papers they're

1117
01:45:52,960 --> 01:45:59,680
putting out and the number of GPUs. And unclear to me also if we're running out of data, I don't

1118
01:45:59,680 --> 01:46:04,720
really know about that, but synthetic data seems to work for a lot of things. There's also just

1119
01:46:04,720 --> 01:46:09,680
more modalities. We certainly have not taken advantage of just think about how much security

1120
01:46:09,680 --> 01:46:15,200
camera footage there is. It's like, we really just want to go big to go big. There's a ton

1121
01:46:15,200 --> 01:46:21,200
sitting out there. The data issue is a speed bump it'll get dealt with by accessing repositories that

1122
01:46:21,200 --> 01:46:28,080
are available that we haven't touched or improved synthetic data. And as you say, modalities between

1123
01:46:28,080 --> 01:46:33,920
zero and four probably doesn't seem unreasonable. I had this conversation with some senior people in

1124
01:46:33,920 --> 01:46:38,800
some of the different foundation model companies and they say sort of similar things. I think

1125
01:46:38,800 --> 01:46:44,480
Shane Legg co-founder of DeepMind is probably at the lower end of lower than four from a conversation.

1126
01:46:44,480 --> 01:46:52,160
I remember him having on a podcast. I suppose then the question is how long does each one

1127
01:46:52,720 --> 01:46:59,360
take? Transformers took not particularly long. I mean, it was really a couple of years before GPT-1

1128
01:46:59,360 --> 01:47:06,400
and two years before GPT-2 actually. And then three for GPT-3. And it doesn't take long in the world

1129
01:47:07,040 --> 01:47:12,800
of archive. But the discovery does take time and where that discovery is takes a moment.

1130
01:47:12,800 --> 01:47:20,240
And in amongst all of that, though, is still, there is still that stage that goes from the

1131
01:47:20,240 --> 01:47:25,040
software capabilities coming together because we have the know-how and we've plugged it all together

1132
01:47:25,680 --> 01:47:34,240
and it iterating to a system that presents a control problem to us. And in the case of the AI

1133
01:47:34,240 --> 01:47:41,440
scientist, that is a system that we can't call Kevin, the CTO of Microsoft and say,

1134
01:47:41,440 --> 01:47:46,240
can you just shut down the Austin data center? And it's zero for a second because the AI center

1135
01:47:46,240 --> 01:47:53,360
data scientist has gone rogue, right? The kind of in extremist mode. And that path to me also seems

1136
01:47:53,360 --> 01:47:59,520
unclear. And there are a whole set of risks and downsides that emerge well before then,

1137
01:48:00,400 --> 01:48:07,440
which I think help give us the infrastructure to deal with that scenario. So that and that is really,

1138
01:48:08,240 --> 01:48:14,000
you know, how do you deal with the bad actors of for people use it with GPT-5 quality

1139
01:48:14,000 --> 01:48:19,280
LLMs on their smartphones in three or four years time? And we will deal with it, right? In the

1140
01:48:19,280 --> 01:48:24,160
same way that, you know, if I'd said to you in 1994, I don't know how old you were, I was 22,

1141
01:48:24,160 --> 01:48:32,080
that by 2024, there'd be 120 billion identity attacks per year just on Microsoft. I wouldn't

1142
01:48:32,080 --> 01:48:35,520
have believed you. I would have, I mean, yeah, Kurtzweil, whatever, right? I just wouldn't have

1143
01:48:35,520 --> 01:48:41,920
grok that number. So we'll have this unseemingly large number of attacks coming mediated through

1144
01:48:41,920 --> 01:48:48,800
LLMs and in botnets and elsewhere. And we will have developed systems to to deal with them.

1145
01:48:48,800 --> 01:48:53,280
And that will be part of the fabric that we can't picture right now into which this AI

1146
01:48:53,280 --> 01:48:59,200
scientist will get will get developed. And that's why these things become so very contingent. So the

1147
01:48:59,200 --> 01:49:02,320
wrong thing to do is to say, well, because it's going to happen, let's do nothing, because then

1148
01:49:02,320 --> 01:49:08,000
it won't happen. I think the right thing to do is to start to explore these ideas and have these

1149
01:49:08,000 --> 01:49:13,040
these conversations. But one of the things I think is really problematic has been problematic has been

1150
01:49:13,040 --> 01:49:19,040
the conversation focusing exclusively on an existential risk, which it really I felt it did

1151
01:49:19,040 --> 01:49:27,360
in 2023. What it does is it diminishes public trust in technology. It forces policymakers to

1152
01:49:28,080 --> 01:49:33,200
make decisions that may not be, you know, well informed, they may not be pro innovation, they

1153
01:49:33,200 --> 01:49:37,520
may not even be pro safety, right? There may be a bundle of really terrible spots. And I think a

1154
01:49:37,520 --> 01:49:42,240
little bit of a great Lu Chichen is a science fiction writer, this Chinese guy who wrote the

1155
01:49:42,240 --> 01:49:45,840
three body problem. But in his book of short stories, The Wandering Earth, there's a moment

1156
01:49:45,840 --> 01:49:52,640
where they have to rocket space 1999 style out of the sun's orbit to prevent some calamity.

1157
01:49:52,640 --> 01:49:58,240
And it's going to be a multi multi generational journey to the to the next planet. And in order

1158
01:49:58,240 --> 01:50:03,760
to do this, people have to live in really terrible conditions, apart from the scientists who are

1159
01:50:04,640 --> 01:50:10,320
keeping everything monitoring, looking for the signs, planning the the process of decompressing

1160
01:50:10,320 --> 01:50:15,680
everything. And of course, the people get loose trust in the scientist and in truly

1161
01:50:15,680 --> 01:50:20,160
Chichen style, sorry, the spoiler, the people rebel, kill all the scientists and then learn

1162
01:50:20,160 --> 01:50:25,680
to hold the next day, they show up at their destination. And trust is really, really critical.

1163
01:50:26,240 --> 01:50:32,720
And I don't think we did a lot to get people who are outside of the tech industry to

1164
01:50:33,760 --> 01:50:39,120
put trust into technology, put trust into their ability to participate in it and to have some

1165
01:50:39,200 --> 01:50:45,440
agency in where it goes, to be excited about it, you know, and off the back of, you know,

1166
01:50:45,440 --> 01:50:50,720
all of the sort of polarization and the, the, the, the, the, the sometimes legitimate, sometimes

1167
01:50:50,720 --> 01:50:56,960
not scaremongering around phones and social networks and so on. And it didn't feel like

1168
01:50:56,960 --> 01:51:00,720
it added to the, to the discussion of trust. So I was quite happy when I went to Davos as

1169
01:51:00,720 --> 01:51:06,240
World Economic Forum meeting that the conversation had moved from, is it, you know, what kind of

1170
01:51:06,240 --> 01:51:11,520
munitions should we use to, to drop on a data center to what are, what are real pathways? What

1171
01:51:11,520 --> 01:51:15,360
is the science that we need to do in order to make these things safe in the long term? What

1172
01:51:15,360 --> 01:51:21,120
is the kind of appropriate regulatory interventions? What do we do about things that are, you know,

1173
01:51:21,120 --> 01:51:26,000
approximate three, five years around misinformation and, and, and cyber threats? While, while still

1174
01:51:26,000 --> 01:51:30,880
recognizing that there is a pathway that you've described that needs to be addressed.

1175
01:51:30,880 --> 01:51:35,520
I find it easy to empathize with basically every AI perspective from the, you know,

1176
01:51:35,520 --> 01:51:43,280
enthusiast to the ex-risk concerned to those that are, you know, screaming about poor use of

1177
01:51:43,280 --> 01:51:48,400
face match technology by police departments. I mean, really the whole thing I think is like

1178
01:51:48,400 --> 01:51:53,520
very, it's all valid in my mind. What's really exciting and what did not happen with the mobile

1179
01:51:53,520 --> 01:52:01,520
didn't happen with PCs. It did not happen with the first mainframes is that, you know, technology is,

1180
01:52:01,520 --> 01:52:07,600
is an intimate part of what it is to be human, right? Technology is our compounded knowledge.

1181
01:52:07,600 --> 01:52:12,960
Technology is, is the, the binding factor that enables the world in which we then have our human

1182
01:52:12,960 --> 01:52:19,280
relationships. And to have so many discussions about a technology early on, when it's just in its

1183
01:52:19,280 --> 01:52:25,680
early adoption phase, we're not too late to it, I think is incredibly positive. And, and I'm glad

1184
01:52:25,680 --> 01:52:30,240
that you have a big tense show. I mean, I have an opinion about sort of ex-risk, but I still also

1185
01:52:30,240 --> 01:52:34,880
feel I'm big 10, you know, but I'm just really, really glad that we're having a wide and extensive

1186
01:52:34,880 --> 01:52:39,760
conversation, one that feels wider and more extensive and kind of more grounded in some ways

1187
01:52:39,760 --> 01:52:44,320
than, than any conversation we ever had about the internet back in the early 90s.

1188
01:52:44,320 --> 01:52:48,640
Yeah. In some ways it's funny. I think in some ways the discourse is getting a little bit more

1189
01:52:48,640 --> 01:52:52,880
deranged over time as, you know, there is some polarization and kind of

1190
01:52:53,760 --> 01:52:59,040
ideological entrenching happening in some places. But then in other ways, I definitely think it's

1191
01:52:59,040 --> 01:53:06,000
getting better if only because what we're actually dealing with is becoming a lot more clear.

1192
01:53:06,000 --> 01:53:12,800
There is a lot of room for commonality, for common ground, and for a recognition that there are

1193
01:53:12,800 --> 01:53:19,120
different pieces of work that need to get done by, by different people. And, and actually there

1194
01:53:19,120 --> 01:53:24,080
should be, there should also be enough money in the tank to be able to do it, right? This is a rich

1195
01:53:24,080 --> 01:53:28,640
industry. It spits out a lot of profits. We should be able to, you know, fund it, fund it some way.

1196
01:53:28,640 --> 01:53:32,960
Again, when you, you see the kind of things people say about each other on Twitter,

1197
01:53:32,960 --> 01:53:36,720
and then you meet them in person and they have the same conversation and it's, it's just a,

1198
01:53:37,680 --> 01:53:42,400
it's a more measured, measured space just in my, my limited experience of it all.

1199
01:53:42,400 --> 01:53:46,320
I think it's kind of everything everywhere all at once. You know, it's like, yes, there are

1200
01:53:46,320 --> 01:53:52,000
definitely things that are quite unhealthy. And so I do not like to see enemies lists getting

1201
01:53:52,000 --> 01:53:56,720
published by leading technologists. That's like, you know, the techno optimist manifesto from

1202
01:53:56,720 --> 01:54:01,200
Andreessen, you know, has a, has a section that is literally called the enemy and names, names,

1203
01:54:01,200 --> 01:54:07,120
if not individual names, at least like specific and relatively identifiable groups. So I don't

1204
01:54:07,120 --> 01:54:14,160
like that. But at the same time, you know, I made some noise about open AI and, you know, kind of

1205
01:54:15,040 --> 01:54:19,680
could have easily been retaliated against by them. And I can imagine a lot of companies,

1206
01:54:19,680 --> 01:54:24,400
you know, that might have come down on me hard and, you know, expunged my name from their,

1207
01:54:24,480 --> 01:54:27,840
you know, case studies on their website and all that kind of stuff. And they didn't do any of

1208
01:54:27,840 --> 01:54:32,240
that. You know, and so I do think there's also aspects and I feel pretty fortunate. And this is

1209
01:54:32,240 --> 01:54:37,200
one of the kind of concluding questions I wanted to ask you is like, I think in some sense, this is

1210
01:54:37,200 --> 01:54:40,960
like a collective responsibility. We all have to wreck it. You know, we all have to orient ourselves

1211
01:54:40,960 --> 01:54:44,880
through it, get familiar and try to figure out what's it mean for us and what can we do to

1212
01:54:44,880 --> 01:54:50,640
shape it in a positive way. It's definitely not all somebody else's problem. But at the same time,

1213
01:54:50,640 --> 01:54:57,280
there are these like leading developers who clearly have outsize influence, outsize power.

1214
01:54:57,280 --> 01:55:01,440
There are also, you know, key decisions that are getting made around like,

1215
01:55:01,440 --> 01:55:04,960
are we going to open source llama three or are we not sounds like we're going to. And then there's

1216
01:55:04,960 --> 01:55:10,560
like government, you know, that can potentially say, you know, hey, we, we require, you know,

1217
01:55:10,560 --> 01:55:14,400
off switches at data centers. I'm not sure data centers have off switches right now. You know,

1218
01:55:14,400 --> 01:55:18,480
you might be able to go in there and start like hacking at them. But is there an actual like easy

1219
01:55:18,480 --> 01:55:22,480
off switch in most of them? No, I don't think there is. I mean, they're designed to be resilient.

1220
01:55:22,480 --> 01:55:26,080
Yeah, the opposite, right? Yeah, they're not so and they don't probably want some

1221
01:55:26,880 --> 01:55:31,200
rogue employee either to like go in and turn it off, right? So they've probably engineered away

1222
01:55:31,200 --> 01:55:36,320
from anybody being able to easily turn it off. So, you know, government may have a role there to

1223
01:55:36,320 --> 01:55:41,840
play that's like, look, we need off switches, we hope we never have to use them. But it seems like

1224
01:55:41,840 --> 01:55:48,880
we might want to have them. So who do you think kind of bears the greatest responsibility or,

1225
01:55:48,880 --> 01:55:53,920
you know, or where do you think we should be investing our trust? You know, is it these leading

1226
01:55:53,920 --> 01:56:01,040
companies? Is it auditors, you know, that could be independent groups? Is it the government? I mean,

1227
01:56:01,040 --> 01:56:05,200
it's probably some mix of all the above, but what are you kind of bullish on in that regard?

1228
01:56:05,200 --> 01:56:09,760
I mean, you know, the US has been an incredibly successful democracy for a long time because

1229
01:56:09,760 --> 01:56:18,640
of separation of powers. And, you know, structurally, that works. The company, however good it is,

1230
01:56:18,640 --> 01:56:25,600
however well intentioned the CEO is, will end up with its own ambitions and directions. And so,

1231
01:56:25,600 --> 01:56:29,600
you know, you will always need to push if the companies are offering you three,

1232
01:56:29,600 --> 01:56:35,680
demand six, that's just that's a good, good practice. So I think that each player in this

1233
01:56:35,760 --> 01:56:41,280
circuit has to be have the right capabilities to have the right conversations. And I think one of

1234
01:56:41,280 --> 01:56:48,000
the things that we can learn from the experience of the FAA and Boeing is that you cannot deplete

1235
01:56:48,000 --> 01:56:53,920
your own capabilities and ask for self regulation because it just doesn't work out however well

1236
01:56:53,920 --> 01:56:59,440
intentioned that the firm is. So I think what you need to do is we need to invest in the capabilities

1237
01:56:59,440 --> 01:57:06,720
of governments to ask good questions and engage well and overcome all of the complexities

1238
01:57:06,720 --> 01:57:13,600
that exist that you can make $10 million a year as an X at OpenAI and you won't do that in government.

1239
01:57:13,600 --> 01:57:21,600
And I think that that also raises the value of investing in academia, research and civil society.

1240
01:57:21,600 --> 01:57:26,000
So Joshua Bengio, for example, is running a really important project. I think it's based out of the

1241
01:57:26,000 --> 01:57:31,440
UK, which is a sort of core science project to look at some of these risks and these evolutions

1242
01:57:31,440 --> 01:57:37,200
and unanswered questions around control. We need to really, really start to level up. And I don't

1243
01:57:37,200 --> 01:57:43,760
think it will be sufficient to just allow the big firms to do that and insist that they spend the

1244
01:57:43,760 --> 01:57:48,720
money as directed by them. You know, I think if they have, if they're willing to put money into it,

1245
01:57:48,720 --> 01:57:52,960
it should go into pots, which go to grow the capabilities of the people who will keep them

1246
01:57:52,960 --> 01:57:59,760
in check. And the reason that works is that, you know, the car industry is really successful

1247
01:57:59,760 --> 01:58:03,680
because someone mandated that cars needed to have brakes. Now, without brakes,

1248
01:58:04,640 --> 01:58:09,120
people wouldn't buy as many cars as they do. And I think this is good for the industry.

1249
01:58:09,120 --> 01:58:13,200
And someone needs to understand within government, within civil society, within academia,

1250
01:58:13,200 --> 01:58:16,480
academia, what are the right questions? And so what are the right interventions

1251
01:58:16,480 --> 01:58:20,160
going to look like? And we can all agree as grown-ups that companies,

1252
01:58:20,240 --> 01:58:26,640
however well-intentioned they are, will always have their own agenda. And we just acknowledge

1253
01:58:26,640 --> 01:58:34,000
that. And we all move forward in a generative, critical, constructive way. So whichever player

1254
01:58:34,000 --> 01:58:40,400
is weak at this table needs to have some support to become stronger. And that probably right now

1255
01:58:40,400 --> 01:58:45,440
is amongst governments and regulators and academia rather than the big few tech firms.

1256
01:58:45,440 --> 01:58:49,120
That might be a great note to end on. Anything else you want to touch on or

1257
01:58:49,920 --> 01:58:54,800
cover that we haven't got to? It's really easy as such a facility and having the conversations

1258
01:58:54,800 --> 01:59:01,680
with you. And I really was so excited that you agreed to do this. You know, I thought the murder

1259
01:59:01,680 --> 01:59:06,800
mystery, which is you and your red teaming show was just brilliant as well. So I know that you've

1260
01:59:06,800 --> 01:59:11,920
got another hat, which is suspense. And I look forward to the next episode of that.

1261
01:59:12,720 --> 01:59:16,880
Well, thank you very much. I really appreciate that. And I appreciate your time and participation

1262
01:59:16,880 --> 01:59:22,160
in this as well. Azim Azhar, founder of The Exponential View, thank you for being part of

1263
01:59:22,160 --> 01:59:25,120
The Cognitive Revolution. My pleasure, Nathan. Thank you.

1264
01:59:25,840 --> 01:59:30,640
It is both energizing and enlightening to hear why people listen and learn what they value about

1265
01:59:30,640 --> 01:59:38,080
the show. So please don't hesitate to reach out via email at tcraturpantime.co or you can DM me

1266
01:59:38,080 --> 01:59:44,640
on the social media platform of your choice. Omniki uses generative AI to enable you to launch

1267
01:59:44,640 --> 01:59:50,080
hundreds of thousands of ad iterations that actually work, customized across all platforms

1268
01:59:50,080 --> 01:59:54,960
with a click of a button. I believe in Omniki so much that I invested in it and I recommend you

1269
01:59:54,960 --> 02:00:02,320
use it too. Use CogGrav to get a 10% discount.

