start	end	text
0	1520	They don't care about AI safety.
1520	2880	What they care about is AI control.
3440	5680	Do I think we eventually get to a configuration like that?
5680	6320	Maybe.
6320	10080	Where you have an AI brain is at the center of civilization,
10080	12160	and it's coordinating all the people around it.
12160	14960	And every civilization that makes it
14960	17440	is capable of crowdfunding and operating its own AI.
17440	19760	You know, our background culture influences things
19760	21440	in ways we don't even think about.
21440	23680	So much of the paperclip thinking
23680	27120	is like a vengeful god will turn you into pillars of salt.
27120	30560	The polytheistic model of many gods, as opposed to one god,
30560	32880	is we're all going to have our own AI gods,
32880	34000	and there'll be war of the gods.
34640	36880	Man-machine symbiosis is not some new thing.
37440	40560	It's actually the old thing that broke us away
40560	43200	from other primate lineages that weren't using tools.
43760	45840	Then the question is, what's the next step?
45840	48480	Which is AI is amplified intelligence.
48480	51120	It is that the AI human fusion
51840	53920	means there's another 20 Elon Musk's,
53920	54960	or whatever the number is.
55680	56560	That's good.
56640	58960	Hello, and welcome to The Cognitive Revolution,
58960	62000	where we interview visionary researchers, entrepreneurs,
62000	63920	and builders working on the frontier
63920	65120	of artificial intelligence.
65840	68640	Each week, we'll explore their revolutionary ideas,
68640	70080	and together, we'll build a picture
70080	73600	of how AI technology will transform work, life,
73600	75200	and society in the coming years.
75760	79280	I'm Nathan LaBenz, joined by my co-host, Eric Torenberg.
79280	82000	Hello, and welcome back to The Cognitive Revolution.
82960	85200	Today, my guest is biology, Srinivasan.
85920	88880	In tech circles, biology needs no introduction.
88880	90800	But for folks from other backgrounds,
90800	92960	biology is a serial startup entrepreneur
92960	95120	who's founded and ultimately sold
95120	97360	highly dissimilar technology companies,
97360	98800	including Teleport,
98800	100560	which helped people move around the world
100560	101680	to realize opportunities,
102560	104880	Council, which provided genetic testing
104880	106320	for couples planning to have children,
106880	111200	and Earn.com, a paid email on the blockchain startup,
111200	113360	which ultimately sold to Coinbase,
113360	115200	where biology became CTO.
116560	119120	Along the way, he's also taught statistics at Stanford
119120	121920	and been a general partner at Andreessen Horowitz as well.
123200	125440	Today, as an independent thinker, investor,
125440	126960	and author of the network state,
126960	128640	biology is extremely prolific
128640	131040	in both text and audio formats.
131040	131760	And as you'll hear,
131760	133840	whether for the first time or the 50th,
133840	135840	he is an incredibly creative thinker
135840	138240	who relentlessly develops and iterates
138240	140320	on new paradigms for understanding
140320	142800	a fast-changing, often chaotic world.
143760	145120	He's also a very associative
145120	146720	and interdisciplinary thinker
146720	149520	who constantly adds dimensions to any analysis.
151200	152720	Such horsepower can be hard
152720	154800	for a podcast host to rein in,
154800	157360	but I personally find it extremely stimulating.
157360	158240	So in this conversation,
158240	159280	I tried to strike a balance
159280	161760	between letting biology go off as only he can do,
162480	164720	contributing what I hope are worthy versions
164720	166400	of core AI safety arguments
166400	168400	and supporting results from recent research,
169120	170160	and occasionally,
170160	171920	steering us back toward what I see
171920	174960	as the most critical questions for the AI big picture.
176720	178800	If there's one area where biology and I
178800	180640	disagree most consequentially,
180640	183760	it's on the question of how independent AI systems
183760	186560	are likely to become over the next five to 10 years.
187440	188960	Biology thinks that AI systems
188960	191600	need to be at least symbiotic with humans,
191600	193040	because physical computers
193040	195600	can't replicate themselves without human support.
196720	199280	While I think there's at least a significant chance
199360	202640	that we get AIs that are so independent of humans
202640	204880	that their behaviors and interactions
204880	207360	become the primary drivers of world history.
208960	210400	In biology's own words,
210400	212960	he does expect massive economic
212960	214960	and social disruption from AI,
215600	217520	but doesn't think that quote-unquote
217520	220800	can't turn the killer AI off scenarios,
220800	222960	are likely, at least for a long while,
222960	225760	due to factors like the existence of adversarial inputs
225760	227520	that can paralyze AIs,
227520	229440	particularly those with open model weights,
230320	232800	the observation that even decentralized programs
232800	235040	like the Bitcoin network can't run independently
235040	236800	without continuous human support,
237520	240160	and the premise that to control the physical world,
240160	242960	AIs will need to direct either large numbers of humans
242960	245040	who are notoriously difficult to control
245040	248640	or highly agile robots which don't yet exist.
250160	251120	With all that in mind,
251120	252640	in the first half of this conversation,
252640	254320	you'll hear biology's analysis
254320	256240	of the likely impact of AI
256240	259760	in a world where powerful AI systems do come to exist,
259760	261120	but humans retain control,
262160	264560	resulting in a human AI symbiosis
264560	267360	similar to how believers relate to their gods
267360	269200	or citizens relate to their governments.
270160	271280	Then in the second half,
271280	272640	we really dig into the question
272640	274640	of just how confident we should be
274640	278480	that AI won't prove to be even more revolutionary than that.
279920	282080	After more than two hours of recording,
282080	284480	I was the one who ran out of time today,
284560	287040	but I really enjoyed this conversation with biology.
287040	290560	He is as good-natured and curious as he is opinionated,
290560	293280	and we have continued to exchange links and arguments offline,
293280	295040	such that I hope we'll have another episode
295040	297200	to share with you in the future as well.
298720	300560	As always, if you're enjoying the show,
300560	303120	we'd ask that you take a moment to share it with a friend.
303760	306960	And with that, here's part one of an all-angles look
306960	310720	at how AI will shape the future with biology, Srinivasan.
311680	315440	Biology, Srinivasan, welcome to the Cognitive Revolution.
315440	317360	All right, I feel welcome.
317360	319200	Well, we've got a ton to talk about.
319200	322080	Obviously, you bring a lot of different perspectives
322080	325200	to everything that you think about and work on.
325840	328960	And today, I want to just try to muster
328960	331280	all those different perspectives onto this.
331280	334240	What I see is really the defining question of our time,
334240	336000	which is like, what's up with AI?
336000	338000	And how's it going to turn out?
338880	340320	I thought maybe for starters,
340320	342320	I would love to just get your baseline kind of table
342320	348480	setting on how much more AI progress do you expect us to see
348480	350080	over the next few years?
350080	353040	Like how powerful are AI systems going to become
353600	355920	in, again, kind of a relatively short timeline?
355920	358800	And then maybe if you want to take a bigger stab at it,
358800	361200	you could answer that same question for a longer timeline
361200	362880	like the rest of our lives or whatever.
362880	364400	Sure. Let me give an abstract answer,
364400	365840	then let me give a technical answer.
366800	370800	If you look at evolution, we've seen something as complex
370800	375360	as flight evolve independently in birds, bats, and bees.
376560	382080	And even intelligence, we've seen fairly high intelligence
382080	385600	in dolphins, in whales, in octopuses.
387120	389200	Octopus in particular can do like tool manipulation.
389200	392080	They've got things that are a lot like hands with tentacles.
392080	395200	And so that indicates that it is plausible
395840	399120	that you could have multiple pathways to intelligence,
399920	402400	whether we have carbon-based intelligence,
402400	403920	so we could have silicon-based intelligence
403920	405280	that just has a totally different form
405280	407680	where the fundamental thing is an electromagnetic wave
407680	411440	and data storage as opposed to DNA and so on.
411440	412960	So that's like a plausibility argument
412960	415680	in terms of evolution is being so resourceful
415680	418560	that it's invented really complicated things
418560	419360	in different ways.
420480	422320	Then in terms of the technical point,
422320	424960	I think as of right now, I should probably date it,
425440	428480	December 11, 2023, because this field moves so fast, right?
429120	432240	My view is, and maybe you'll have a different view,
432240	434400	is that the breakthroughs that are really needed
434400	436720	for something that's like true artificial intelligence
436720	440240	that is human-independent, right?
440240	442560	Maybe the next step after the Turing test,
442560	444720	I've got an article that we're writing
444720	445920	called the Turing Thresholds,
446640	448400	which tries to generalize the Turing test
448400	450240	to like the Kardashev scale.
450240	451680	Have you got energy thresholds?
451680	453280	What are useful scales beyond that?
453840	457920	And right now, I think that what we call AI
457920	460480	is absolutely amazing for environments
460480	462480	that are not time-varying or rule-varying.
463840	466960	And what I mean by that is, so you kind of have,
466960	468880	let's say two large schools of AI,
468880	470240	and obviously there's overlap
470240	471840	in terms of the personnel and so on,
471840	473760	but there's like the DeepMind School,
473760	475200	which has gotten less press recently,
475200	477520	but got more press a few years ago,
477520	480160	and that is game-playing, right?
480160	484720	It is super human-playing of go without go.
484720	487920	It is all the video game stuff they've done
487920	489280	where they learn at the pixel level
489280	491200	and they just teach the very basic rules
491200	492640	and it figures it out from there.
492640	494960	And it's also the protein folding stuff
494960	495920	and what have you, right?
496800	498160	But in general, I think they're known
498160	500880	for reinforcement learning and those kinds of approaches.
500880	501840	I mean, they're good at a lot of things,
501840	503360	but that's what I think DeepMind is known for.
503360	505680	Of course, they put out this new model recently,
505680	507040	the Gemini model.
507040	509040	So I'm not saying that they're not good at everything,
509040	511440	but that's just kind of what they're maybe most known for.
511440	515120	And then you have the OpenAI ChatGBT School of Generative AI,
515120	519040	and it includes stable diffusion and just as a pioneer,
519040	520320	even if they're not,
520320	521840	I don't know how much they're used right now,
521840	525040	but basically, you have the diffusion models for images
525040	527280	and you have large language models
527280	529120	and now you have the multimodals that integrate them.
529120	531280	And so the difference, I think with these,
531280	534800	is the reinforcement learning approaches
534800	538160	are based on an assumption of static rules,
538160	540080	like the rules of chess, the rules that go,
540080	542240	the rules of a video game are not changing with time.
542240	542960	They are discoverable.
542960	544080	They're like the laws of physics.
544640	548800	And similarly, like the body of language
548800	549920	where you're learning it,
549920	551840	English is not rapidly time varying.
552480	554480	That is to say, the rules of grammar
554480	555920	that are implicit aren't changing.
555920	558560	The meanings of words aren't changing very rapidly.
558560	560800	You can argue they're changing over the span of decades
560800	563200	or centuries, but not extremely rapidly, right?
563200	566880	So therefore, when you generate a new result,
566880	569200	training data from five years ago for English
569200	571120	is actually still fairly valuable
571120	573840	and the same input roughly gives the same output.
573840	575760	Now, of course, there are facts that change with time,
576720	579280	like who is the ruler of England, right?
579280	580720	The queen of England is passed away now.
580720	581840	It's the king of England, right?
581840	583280	It's just facts that change with time.
583280	584480	But I think more fundamentally
584480	586400	is when there's rules that change with time.
587040	591040	You have, for example, changes in law in countries, right?
591040	593040	But most interestingly, perhaps changes in markets
594000	594880	because the same input
594880	596480	does not give the same output in a market.
596480	598080	If you try that, then what will happen
598080	600400	is there's adversarial behavior on the other side.
600400	601840	And once people see it enough times,
601840	603200	they'll see your strategy
603200	605360	and they're going to trade against you on that, right?
605360	607200	And I can get to other technical examples on that,
607200	608960	but I think, and probably people in the space
608960	609840	are aware of this,
609840	611760	but I think that is a true frontier
611760	615120	is dealing with time-varying, rule-varying systems,
615120	618240	as opposed to systems where the implicit rules are static.
618240	619120	Let me pause there.
619120	620160	Yeah, I think that makes sense.
620160	622960	I think in the very practical,
623920	625920	just trying to get, as Zvi calls it,
625920	627360	mundane utility from AI,
628000	631440	that is often cashed out to AI is good at tasks,
631440	633680	but it's not good at whole jobs.
633680	635920	It can handle these small things
635920	638240	where you can define what good looks like
638240	640480	and tell it exactly what to do.
640480	645200	But in the broader context of handling things
645200	646480	that come up as they come up,
647040	648560	it's definitely not there yet.
649440	654000	And I agree that there's likely to be some synthesis,
654000	658560	which is kind of the subject of all the Q-star rumors recently,
658560	661920	I would say is kind of the prospect
661920	665280	that there could be already within the labs
665280	667600	a beginning of a synthesis between the,
668240	671520	I kind of think of it as like harder-edged reinforcement
671520	675680	learning systems that are like small, efficient, and deadly,
675680	677840	versus the language model systems
677840	680800	that are like kind of slow and soft,
680800	682960	and but have a sense of our values,
682960	685280	which is really a remarkable accomplishment
685280	689040	that they're able to have even an approximation of our values
689040	690480	that seems like reasonably good.
691200	696160	So yeah, I think I agree with that framing,
696720	700080	but I guess I would still wonder like,
700720	704560	how far do you think this goes in the near term?
704560	706720	Because I have a lot of uncertainty about that,
706720	708160	and I think the field has a lot of uncertainty.
708160	710080	You hear people say, well,
710080	712480	it's never going to get smarter than its training data.
712480	714800	It'll kind of level out where humans are,
714800	715840	but we certainly don't see that
715840	718480	in the reinforcement learning side, right?
718480	721760	Like once, it usually don't take too long
721760	723600	at human level of these games,
723600	725440	and then it like blows past human level.
725440	728400	Interestingly, you do still see some adversarial vulnerability,
728400	732160	like there's a great paper from the team at FAR AI,
732160	734560	and I'm planning to have Adam Gleave,
734560	736000	the head of that organization on soon,
736000	737360	to talk about that and other things,
737360	739840	where they found like a, basically a hack
739840	743920	where a really simple, but unexpected attack
743920	747280	on the superhuman go player can defeat it.
747280	750160	So you do have these like very interesting vulnerabilities
750160	753120	or kind of lack of adversarial robustness.
753120	754640	Still kind of wondering like,
754640	756240	where do you think that leaves us
756240	758240	in say a three to five years time?
758240	760160	Obviously, huge uncertainty on that.
760160	762240	It's really hard to predict something like this.
762800	766480	Just to your point, generative AI is generic AI, right?
766480	768320	It's like generically smart,
768320	769920	but doesn't have specific intelligence
769920	771520	or creativity or facts.
771520	773520	And as you're saying, just like we have,
773520	777760	you know, adversarial images back in full programs
777760	780480	that are trained on a certain set of data
780480	782880	and they just give some weird, you know, pattern
782880	784080	that looks like a giraffe,
784080	786320	but the algorithm thinks it's a dog.
786320	787680	You can do the same thing for game playing
787680	789360	and you can have out of sample input
789360	791440	that can beat, you know,
791440	794560	these very sophisticated reinforcement learners.
796000	797520	And an interesting question is whether
797520	799440	that is a fundamental thing
799440	803520	or whether it is a work aroundable thing.
804160	806640	And you'd think it was work aroundable, you know,
807520	809920	because there's probably some robustification
809920	812560	because these pictures look like giraffes, you know,
813200	815760	and yet they're being recognized as dogs.
815760	819600	So you would think that the right proximity metric
819600	822480	would group it with giraffes, you know,
823040	825200	but maybe there's some, I don't know,
825200	826800	maybe there's some result there.
826800	831120	My intuition would be we can probably robustify these systems
831120	834320	so that they are less vulnerable to adversarial input.
835920	837360	But if we can't, then that leads us
837360	838880	in a totally different direction,
838880	841840	where these systems are fragile in a fundamental way.
842800	847040	So that's one big branch point is how fragile these systems are
847040	849200	because if they're fragile in a certain way,
849200	852080	then it's almost like you can always kill them,
852080	853920	which is kind of good, right?
853920	856320	In a sense, that there's that, you know,
856320	860480	almost like the, you know, the 50 IQ, 100 IQ, 150 IQ thing.
860480	862080	Like the meme?
862080	863280	Yeah, the meme, right?
863280	865600	So the 50 IQ guys like these machines
865600	868080	will never be as creative as humans or whatever.
868080	870160	100 IQ is look at all the things they can do.
870240	874800	The 150 IQ is like, well, there's some like equivalent result,
874800	877280	you know, that's like some impossibility proof
877280	879920	that shows that we, the dimensional space of a giraffe
879920	883200	is too high and we can't actually learn what a true giraffe.
883200	886080	I don't think that's true, but maybe it's true
886080	888960	from the perspective of how these learners are working
888960	891200	because my understanding is people have been trying,
891200	892800	and I mean, I'm not on the cutting edge of this.
892800	894080	So, you know, maybe someone,
894080	896720	but my understanding is we haven't yet been able to
896720	900880	robustify these models against adversarial input.
900880	902320	Am I wrong about that?
902320	903600	Yeah, that's definitely right.
903600	905600	Hey, we'll continue our interview in a moment
905600	906960	after a word from our sponsors.
926720	950480	I've used it in the past at the companies I've founded,
950480	952560	and when we launch Merch here at Turpentine,
952560	954000	Shopify will be our go-to.
954640	956640	Shopify helps turn browsers into buyers
956640	958880	with the internet's best converting checkout
958880	962400	up to 36% better compared to other leading commerce platforms.
962400	965040	And Shopify helps you sell more with less effort
965040	968000	thanks to Shopify Magic, your AI-powered all-star.
968000	970720	With Shopify Magic, whip up captivating content
970720	973760	that converts from blog posts to product descriptions.
973760	975840	Generate instant FAQ answers.
975840	977920	Pick the perfect email send time.
977920	981120	Plus, Shopify Magic is free for every Shopify seller.
981840	983920	Businesses that grow, grow with Shopify.
984560	986720	Sign up for a $1 per month trial period
986720	989280	at Shopify.com slash cognitive.
989280	991600	Go to Shopify.com slash cognitive now
991600	994080	to grow your business no matter what stage you're in.
994080	996160	Shopify.com slash cognitive.
1011280	1021440	There's no single architecture as far as I know
1021440	1024320	that is demonstrably robust.
1024320	1026400	And on the contrary, even with language models,
1026400	1027120	there's a...
1027120	1029440	We did a whole episode on the universal jailbreak
1030000	1033280	where especially if you have access to the weights,
1033280	1034240	not to change the weights,
1034240	1037040	but just to probe around in the weights,
1037040	1039120	then you have a really hard time
1039600	1041360	guaranteeing any sort of robustness.
1041920	1044800	The conjecture is, see, for humans,
1044800	1048240	you can't mirror their brain and analyze it.
1048240	1048880	Okay?
1048880	1050960	But we have enough humans that we've got things
1050960	1053680	like optical illusions, stuff like that,
1053680	1055840	that works on enough humans
1056720	1058400	and our brains aren't changing enough, right?
1058960	1062160	A conjecture is, as you said, open weights.
1062880	1066560	Open weights mean safety because if you have open weights,
1066560	1068960	you can always reverse engineer adversarial input
1069360	1071120	and then you can always break the system.
1071680	1072240	Conjecture.
1072960	1076000	Yeah, that's again with Adam from Far AI.
1076000	1077760	I'm really interested to get into that
1077760	1078960	because they are starting to study,
1078960	1079840	as I understand it,
1080800	1085040	kind of proto-scaling laws for adversarial robustness.
1085600	1087600	And I think a huge question there is,
1088480	1092160	what are the kind of frontiers of possibility there?
1094320	1095680	How do the orders of magnitude work?
1095760	1099440	Do you need another 10x as much adversarial training
1099440	1103360	to half the rate of your adversarial failures?
1103360	1105520	And if so, can we generate that many?
1105520	1108080	It may always sort of be fleeting.
1108640	1113200	So, Far AI and they are working on cutting edge
1113200	1114640	of adversarial input.
1114640	1116640	Yeah, they're the group that did the attack
1116640	1120400	on the Alpha Go model and found that, like, you know,
1120400	1122080	and what was really interesting about that,
1122080	1123280	I mean, multiple things, right?
1123280	1125520	First, that they could beat a super human Go player at all.
1125680	1127680	But second, that the technique that they used
1127680	1130720	would not work at all if playing a quality human.
1130720	1133680	Or is, you know, it's a strategy that is trivial to beat
1133680	1135840	if you're a quality human Go player,
1135840	1138880	but the Alpha Go is just totally blind to it.
1138880	1140480	You know, that's why I say the conjecture is,
1141200	1142240	if you have the model,
1143520	1145200	then you can generate the adversarial input.
1145760	1148080	And then, so if that is true,
1148080	1151200	and that itself is an important conjecture about AI safety,
1151200	1156160	right? Because if open weights are inherently something
1156160	1158080	where you can generate adversarial input from that
1158080	1160800	and break or crash or defeat the AI,
1162160	1165520	then that AI is not omnipotent, right?
1165520	1167920	You have some power words you can speak to
1167920	1170320	at almost like magical words that'll just make it
1171520	1174560	power down, so to speak, right?
1174560	1177200	It's like those movies where the monsters can't see you
1177200	1180400	if you stand really still or if you don't make a noise
1180400	1181680	or something like that, right?
1181680	1183520	They're very powerful on Dimension X,
1183520	1184880	but they're very weak on Dimension Y.
1185440	1187440	A kind of an obvious point, but, you know,
1187440	1189680	I'm not sure how important it's going to be in the future.
1191120	1193040	Your next question was on like, you know,
1193040	1195200	humanoid robots and so on, and before we get to that,
1195760	1199600	maybe obviously, but all of these models are trained
1199600	1201760	on things that we can easily record,
1201760	1204960	which are sights and sounds, right?
1205600	1208000	But touch and taste and smell,
1208960	1212320	we don't have amazing data sets on those.
1212320	1214480	Well, I mean, there's some haptic stuff, right?
1215760	1217840	There's probably some, you know,
1217840	1219440	some work on taste and smell and so on,
1219440	1221760	but there's five senses, right?
1221760	1226640	I wonder if there's something like that where you might be like,
1226640	1229440	okay, how are you going to outsmell a robot
1229440	1230480	or something like that?
1230480	1232800	Well, dogs actually have a very powerful sense of smell,
1232800	1234640	and that's being very important for them, you know?
1235280	1236960	And it may turn out that there's,
1237040	1238800	maybe it's just that we just haven't collected the data,
1238800	1241280	and it could become a much better smeller or whatever,
1241280	1243040	or, you know, taster than anything else.
1243040	1243840	I wouldn't be surprised.
1243840	1245600	It could be a much better wine taster,
1245600	1247600	because you can do molecular diagnostics.
1247600	1251440	But it's just kind of, I just use that as an analogy to say,
1251440	1252880	there's areas of the human experience
1252880	1254240	that we haven't yet quantified,
1255040	1257840	and maybe it's just the opera term is yet, okay?
1257840	1258960	But there's areas of the human experience
1258960	1260480	we haven't yet quantified,
1260480	1262880	which are also an area that AIs at least
1262880	1264320	are not yet capable of.
1264320	1267760	Yeah, I guess maybe my expectation boils down to,
1269360	1272080	I think the really powerful systems
1272080	1276560	are probably likely to mix architectures
1276560	1278080	in some sort of ensemble.
1278080	1280400	You know, when you think about just the structure of the brain,
1280400	1282880	it's not, I mean, there certainly are aspects of it
1282880	1283840	that are repeated, right?
1283840	1285760	You look at the frontal cortex,
1285760	1288480	and it's like there is kind of this, you know,
1288480	1290560	unit that gets repeated over and over again,
1290560	1292560	in a sense that's kind of analogous to say,
1292560	1294160	the transformer block that just gets,
1294160	1295920	you know, stacked layer on layer.
1295920	1297920	But it is striking in a transformer
1297920	1300480	that it's basically the same exact mechanism
1300480	1302400	at every layer that's doing kind of
1302400	1304320	all the different kinds of processing.
1304880	1307680	And so whatever weaknesses that structure has,
1307680	1308800	and you know, with the transformer
1308800	1309840	and the attention mechanism,
1309840	1311440	there's like some pretty profound ones,
1311440	1315760	like finite context window, you know, you kind of need,
1315760	1318080	I would think, a different sort of architecture
1318080	1319680	with a little bit of a different strength
1319760	1324880	and weakness profile to complement that in such a way that,
1325440	1327680	you know, kind of more similar to like a biological system
1327680	1330400	where you kind of have this like dynamic feedback
1330400	1332400	where, you know, we have obviously, you know,
1332400	1334560	thinking fast and slow and all sorts of different modules
1334560	1337040	in the brain and they kind of cross regulate each other
1337600	1341440	and don't let any one system, you know,
1342080	1345200	go totally, you know, down the wrong path on its own, right?
1345200	1346480	Without something kind of coming back
1346480	1347760	and trying to override that.
1348240	1349680	It seems to me like that's a big part
1349680	1353680	of what is missing from the current crop of AIs
1353680	1355200	in terms of their robustness.
1355840	1358240	And I don't know how long that takes to show up,
1358800	1363440	but we are starting to see some, you know, possibly,
1363440	1364560	you know, I think people are maybe thinking
1364560	1365680	about this a little bit the wrong way.
1365680	1367360	They're just in the last couple of weeks,
1367360	1368560	there's been a number of papers
1369280	1372640	that are really looking at the state space model
1373200	1374080	kind of alternative.
1374080	1376800	It's being framed as an alternative to the transformer.
1376800	1378560	But when I see that I'm much more like,
1379280	1381920	it's probably a compliment to the transformer
1381920	1384400	or, you know, these two things probably get integrated
1384400	1386480	in some form because to the degree
1386480	1388560	that they do have very different strengths and weaknesses,
1388560	1390480	ultimately you're going to want the best of both
1390480	1391840	in a robust system.
1391840	1393040	Certainly if you're trying to make an agent,
1393040	1394480	certainly if you're trying to make, you know,
1394480	1396560	a humanoid robot that can go around your house
1396560	1398080	and like do useful work,
1398080	1400800	but also be robust enough that it doesn't,
1400800	1402640	you get tricked into attacking your kid
1402640	1404000	or your dog or, you know, whatever,
1404480	1406720	you're going to want to have more checks and balances
1406720	1409360	than just kind of a single stack of, you know,
1409360	1411680	the same block over and over again.
1411680	1415120	Well, so I know Boston Dynamics with their legged robots
1415120	1418480	is all control theory and it's not classical ML.
1418480	1421360	It's really interesting to see how they've accomplished it.
1421360	1424160	And they do have essentially a state space model
1424160	1426880	where they have a big position vector
1426880	1428800	that's got all the coordinates of all the joints
1428800	1431040	and then a bunch of matrix algebra to figure out
1431040	1433760	how this thing is moving and all the feedback control
1433760	1434880	and so on there.
1434880	1435920	And it's more complicated than that,
1435920	1438240	but that's, you know, I think the V1 of it.
1438240	1439360	Sorry, it was there.
1439360	1440880	I wasn't following this though.
1440880	1442480	Are you saying that there's papers
1442480	1445360	that are integrating that with the kind of
1445360	1447280	generator transformer model?
1447280	1449360	You know, like what's a good citation for me to look at?
1449360	1452320	Yeah, starting to, we did an episode, for example,
1452320	1455760	with one of the technology leads at Skydio,
1455760	1459040	the, you know, the U.S. is champion drone maker.
1459840	1462400	And they have kind of a similar thing
1462400	1466240	where they have built over, you know, a decade, right?
1466240	1471920	A fully explicit multiple orders of, you know,
1471920	1474480	spanning multiple orders of magnitude control stack.
1475280	1478160	And now over the top of that,
1478160	1480480	they're starting to layer this kind of, you know,
1480480	1482400	it's not exactly generative AI in their case
1482400	1484720	because they're not like generating content,
1484720	1487280	but it's kind of the high level, you know,
1487280	1490080	can I give the thing verbal instructions?
1490080	1492080	Have it go out and kind of understand,
1492080	1494720	okay, like this is a bridge, I'm supposed to kind of,
1494720	1498160	you know, survey the bridge and translate
1498160	1501120	those high level instructions to a plan
1501120	1504480	and then use the lower level explicit code
1504480	1507040	that is fully deterministic and, you know,
1507040	1509360	runs on control theory and all that kind of stuff
1509360	1512160	to actually execute the plan at a low level.
1512160	1514560	But also, you know, at times like surface errors
1514560	1516320	up to the top and say like, hey, we've got a problem,
1516320	1517840	you know, whatever, I'm not able to do it.
1517840	1520960	You know, can you now, at the higher level,
1521040	1523120	the semantic layer adjust the plan?
1523920	1526640	That stuff is starting to happen in multiple domains,
1526640	1527360	I would say.
1527360	1528880	Yeah, and so I think that makes sense,
1528880	1532080	is basically it's like generative AI is almost the front end,
1532080	1534080	and then you have almost like an assembly,
1534080	1536480	like you give instructions to Figma
1536480	1539680	and the objects there are their shapes
1539680	1542160	and their images and so on, it's not text.
1542160	1543520	You give instructions to a drone
1543520	1548000	and the objects are like GPS coordinates and paths and so on.
1548640	1552480	And so you are generating structures
1552480	1553840	that are in a different domain,
1553840	1556400	or it's like in VR, you're generating 3D structures,
1556400	1560000	again, as opposed to text, and then that compute engine
1560000	1561200	takes those three structures
1561200	1564000	and does something with them in a much more rules-based way.
1564000	1566960	So you have like a statistical user-friendly front end
1566960	1570720	with a generative AI, and then you have a more deterministic
1570720	1572160	or usually totally deterministic,
1573280	1574800	almost like assembly language back end
1574800	1576160	that actually takes that and does something.
1576160	1577120	That's what you're saying, right?
1577120	1579120	Yeah, pretty much, and I would say there's another analogy
1579120	1581360	to just, again, our biological experience
1581360	1586080	where it's like I'm sort of in a semi-conscious level, right?
1586080	1588160	I kind of think about what I want to do,
1588160	1590640	but the low-level movements of the hand
1590640	1592320	are both like not conscious,
1592320	1598320	and also if I do encounter some pain or hit some hot item
1598320	1600480	or whatever, there's a quick reaction
1600480	1604400	that's sort of mediated by a lower level control system,
1604400	1605920	and then that fires back up to the brain
1605920	1608240	and is like, hey, we need a new plan here.
1608240	1613440	So that is only starting to come into focus, I think,
1613440	1616240	with, because obviously these, I mean, it's amazing,
1616240	1617920	as you said, it's all moving so fast.
1618800	1621200	What is always striking to me,
1621200	1623760	and I kind of like recite timelines to myself
1623760	1625120	almost as like a mantra, right?
1625120	1627760	Like the first instruction following AI
1627760	1630640	that hit the public was just January 2022.
1630640	1632800	That was OpenAI's Text of Ingee 002.
1632800	1634560	It was the first one where you could say like do X,
1634560	1637200	and it would do X as opposed to having
1637200	1639360	an elaborate prompt engineering type of setup.
1640160	1643280	GPT-4 just a little over a year ago finished training,
1643280	1644960	not even a year that it's been in the public,
1645680	1649280	and it has been amazing to see how quickly
1649920	1652640	this kind of technology is being integrated into those systems,
1652640	1654800	but it's definitely still very much a work in progress.
1655360	1660080	Yeah, I mean, the tricky part is the training data and so on.
1660320	1666320	Like a large existing scale company like a Figma or DJI
1666320	1669760	that has millions or billions of user sessions
1669760	1671680	will have a much easier time training,
1672400	1675040	and they have a unique data set,
1675040	1678400	and then everybody else will not be able to do that.
1678400	1679600	So there is actually almost like,
1680400	1683360	I mean, a return on scale where the massive data set,
1683360	1684800	if you've got a massive clean data set
1684800	1687520	and a unique domain that lots of people are using,
1687520	1689040	then you can crush it.
1689760	1692080	And if you don't, I suppose,
1692080	1694080	I mean, there's lots of people who work on zero shot stuff
1694080	1695920	and sort of sort of, but it still strikes me
1695920	1699040	that there'll probably be an advantage to see those sessions.
1700400	1703120	I find it hard to believe that you could generate
1703120	1707280	a really good drone command language
1707280	1710000	without lots of drone flight paths, but you can see.
1710000	1711440	And where it doesn't exist, people are,
1712080	1713520	obviously you need deep pockets for this,
1713520	1716400	but the likes of Google are starting to just
1717360	1719120	grind out the generation of that, right?
1719120	1721760	They've got their kind of test kitchen,
1721760	1724640	which is a literal physical kitchen at Google
1724640	1727040	where the robots go around and do tasks.
1727040	1728880	And when they get stuck, my understanding
1728880	1732080	of their kind of critical path, as I understand,
1732080	1735920	they understand it, is robots gonna get stuck,
1736560	1741120	will have a human operator remotely operate the robot
1741680	1745280	to show what to do, and then that data becomes
1745280	1747520	the bridge from what the robot can't do
1747520	1749520	to what it's supposed to learn to do next time.
1750160	1752800	And they're gonna need a lot of that, for sure.
1753440	1755360	But they increasingly have,
1755360	1756960	I don't know exactly how many robots they have now,
1756960	1758400	but last I talked to someone there,
1758400	1760320	it was like into the dozens,
1760960	1764560	and presumably they're continuing to scale that.
1764560	1769360	I think they just view that they can probably brute force it
1769360	1772640	to the point where it's good enough to put out into the world,
1772640	1774560	and then very much like a Waymo or a cruise
1774560	1776800	or whatever, they probably still have remote operators,
1776800	1778960	even when the robot is in your home,
1780080	1780960	when it encounters something
1780960	1783760	that it doesn't know what to do about, raise that alarm,
1783760	1786000	get the human supervision to help it over the hump,
1786000	1789600	and then obviously that's where you really get the scale
1789600	1790880	that you're talking about.
1790880	1793200	And this raises a couple of questions I wanted to ask
1793200	1794320	that are conceptual.
1794320	1797840	So obviously there's huge questions around like,
1799120	1801920	again, highest level, how is all this gonna play out?
1801920	1806560	One big debate is to what degree does AI favor the incumbents?
1806560	1809200	To what degree does it enable startups?
1809200	1810160	Obviously it's both,
1810160	1812640	but I'm interested in your perspective on that.
1812640	1813920	Also really interested in your perspective
1813920	1815840	on like offense versus defense.
1815840	1817200	That's something that a lot of people
1818000	1819360	now and in the future,
1819360	1821600	that seems like it probably really matters a lot,
1821600	1823440	whether it's a more offense enabling
1823440	1824720	or defense enabling technology.
1824720	1828000	So I love your take on those two dimensions.
1828000	1829920	Hey, we'll continue our interview in a moment
1829920	1831280	after a word from our sponsors.
1832080	1834480	Omniki uses generative AI
1834480	1836400	to enable you to launch hundreds of thousands
1836400	1838800	of ad iterations that actually work,
1838800	1842160	customized across all platforms with a click of a button.
1842160	1844720	I believe in Omniki so much that I invested in it
1844720	1846320	and I recommend you use it too.
1847040	1849680	Use Cogrev to get a 10% discount.
1849680	1851280	If you're a startup founder or executive
1851280	1852400	running a growing business,
1852400	1853680	you know that as you scale,
1853680	1856320	your systems break down and the cracks start to show.
1856880	1858000	If this resonates with you,
1858000	1859600	there are three numbers you need to know,
1859680	1863840	36,000, 25, and one, 36,000.
1863840	1864880	That's the number of businesses
1864880	1866800	which have upgraded to NetSuite by Oracle.
1866800	1868880	NetSuite is the number one cloud financial system,
1868880	1870880	streamline accounting, financial management,
1870880	1873920	inventory, HR, and more, 25.
1873920	1875680	NetSuite turns 25 this year.
1875680	1878480	That's 25 years of helping businesses do more with less,
1878480	1881440	close their books in days, not weeks, and drive down costs.
1882160	1884240	One, because your business is one of a kind,
1884240	1886800	so you get a customized solution for all your KPIs
1886800	1889280	in one efficient system with one source of truth.
1889360	1892640	Manage risk, get reliable forecasts, and improve margins.
1892640	1894240	Everything you need, all in one place.
1894800	1898160	Right now, download NetSuite's popular KPI checklist,
1898160	1900560	designed to give you consistently excellent performance,
1900560	1903760	absolutely free, and netsuite.com slash cognitive.
1903760	1905920	That's netsuite.com slash cognitive
1905920	1907760	to get your own KPI checklist.
1907760	1909600	NetSuite.com slash cognitive.
1910400	1911920	So like offense or defense
1911920	1914400	in the sense of disenabled disruptors or incumbents?
1914960	1916480	Both in business and in like,
1916480	1918240	you know, potentially outright conflict.
1918240	1920880	I'd be interested to hear your analysis on both.
1920880	1922080	All right, a lot of views on this.
1922080	1927120	So obviously, if you've got a competent, existing tech CEO,
1927120	1930240	you know, like who's still in their prime,
1930240	1934880	like Amjad of Replit, or, you know, Dillon Field of Figma,
1936640	1939520	or, you know, those are two who have thought of,
1939520	1942720	who are very good and, you know, will be on top of it.
1942720	1945920	Amjad is very early on integrating AI into Replit,
1945920	1948320	and it's basically built that into an AI first company,
1948320	1949280	which is really impressive.
1950000	1954000	Those are folks who cleanly made a pivot.
1954000	1957520	It's as big or bigger than, comparable to, I would say,
1957520	1960640	the pivot from desktop to mobile
1960640	1962080	that broke a bunch of companies
1962080	1964080	in the late 2000s and early 2010s.
1964080	1968080	Like Facebook in 2012 had no mobile revenue, roughly,
1968080	1969680	at the time of their IPO,
1969680	1971440	and then they had to like redo the whole thing.
1971440	1974400	And it's hard to turn a company 90 degrees
1974400	1976480	when something new like that hits, you know?
1977040	1979440	Those that are run by kind of tech CEOs in their prime
1980400	1983760	will adapt and will AI-ify their existing services.
1984320	1985680	And the question is, obviously,
1985680	1986880	there's new things that are coming out,
1986880	1988800	like pika and character.ai.
1988800	1991440	There's some like really good stuff that's out there.
1991440	1993760	The question is, you know,
1994320	1996000	will the disruption be allowed to happen
1996000	1997520	in the U.S. regulatory environment?
1998320	2001680	And so my view is actually that, you know,
2001680	2004320	so this is from like the network state book, right?
2005280	2007040	You know, people talk about a multipolar world
2007040	2008400	or unipolar world.
2008400	2010880	The political axis is actually really important
2010880	2012480	in my view for thinking about
2012480	2015920	whether AI will be allowed to disrupt, okay?
2015920	2017840	Because we'll get to this probably later,
2017840	2020240	but the 640K of compute is enough
2020240	2022000	for everyone executive order.
2022000	2024720	You know, 640K of memory, the apocryphal,
2024720	2026160	he didn't delegate to actually say it,
2026160	2028800	but that quote kind of gives a certain mindset
2028800	2029600	about computing.
2029600	2030960	That should be enough for everybody.
2030960	2032640	So the 10 to the 26 of compute
2032640	2033920	should be enough for everyone bill.
2035040	2036400	I actually think it's very bad
2036400	2037600	and I think it's just the beginning
2037600	2041040	of their attempts to build like a software FDA, okay?
2041040	2044480	To decelerate, control, regulate, red tape,
2044480	2047760	the entire space, just like how, you know,
2047760	2050480	the threat of nuclear terrorism got turned into the TSA.
2051360	2054160	The threat of, you know, terminators and AGI
2054160	2056480	gets turned into a million rules
2056480	2058080	on whether you can set up servers
2058080	2060400	and this last free sector of the economy
2060400	2062640	is strangled or at least controlled
2063200	2065920	within the territory control by Washington DC.
2066880	2070000	Now, why does this relate to the political?
2070000	2072160	Well, obviously this, you know,
2072160	2073840	you can just spend your entire life
2073840	2075120	just tracking AI papers
2075120	2077680	and that's moving like at the speed of light like this, right?
2078320	2079440	What's also happening
2079440	2081200	as you can kind of see in your peripheral vision
2081200	2083840	is there's political developments
2083840	2085040	that are happening at the speed of light
2085040	2087040	much faster than they've happened in our lifespans.
2087040	2088640	Like there's more, you just noticed,
2088640	2091120	more wars, more serious online conflicts
2091120	2093440	like, you know, there's a sovereign debt crisis.
2093440	2095440	All of those things that can show graph after graph
2095440	2098880	of things looking like their own types of singularities, you know,
2099440	2100960	like military debts are way up, you know,
2100960	2103360	the long piece that Steven Pinker showed
2103360	2104320	that's looking like a you
2104320	2106080	that suddenly way up after Ukraine
2106080	2107920	and some of these other wars are happening, unfortunately, right?
2108800	2110960	Interest payments, whoosh, way up to the side.
2110960	2111920	What's my point?
2111920	2115600	Point is, I think that the world is going to become
2115600	2117760	from the Pax Americana world
2117760	2120320	of just like basically one superpower,
2120320	2123600	a hyperpower that we grew up in from 91 to 2021, roughly,
2124160	2128160	that we're going to get a specifically tripolar world,
2128160	2131680	not unipolar, not bipolar, not multipolar, but tripolar.
2131680	2136560	And those three poles, I kind of think of as NYT, CCP, BTC,
2136560	2138080	or you could think of them as,
2138080	2140720	and those are just certain labels that are associated with them,
2140720	2144480	but they're roughly U.S. tech, the U.S. environment,
2145200	2146880	China tech and China environment,
2146880	2148640	and global tech and the global environment.
2149280	2153360	And why do I identify BTC and crypto and so on with global tech?
2153360	2155520	Because that's a tech that decentralized out of the U.S.
2156240	2159200	And right now people think of crypto as finance,
2159200	2160560	but it's also financiers.
2162080	2163680	Okay, and in this next run-up,
2164560	2168400	it is, I think, quite likely about, depending on how you count,
2168400	2170960	between a third to a half of the world's billionaires will be crypto.
2172320	2175360	Okay, around, you know, I calculated this a while back around,
2175360	2176960	Bitcoin at a few hundred thousands,
2176960	2179600	around a third to a half the world's billionaires are crypto.
2179600	2181520	That's the unlocked pool of capital.
2182080	2186800	And those are the people who do not bow to D.C. or Beijing.
2186800	2189520	And they might, by the way, be Indians or Israelis
2189520	2191360	or every other demographic in the world,
2191360	2193360	or they could be American libertarians,
2193360	2195520	or they could be Chinese liberals like Jack Ma,
2195520	2197280	who are pushed out of Beijing sphere.
2197280	2198640	Okay, or the next Jack Ma.
2198640	2201920	You know, Jack Ma himself may not be able to do too much, okay?
2202560	2206320	That group of people who are, let's say, the dissident technologists
2206320	2209760	who are not going to just kneel to anything
2209760	2211600	that comes out of Washington D.C. or Beijing,
2212160	2214400	that is the, that's decentralized AI.
2215040	2216240	That's crypto.
2216240	2217680	That's decentralized social media.
2217680	2219280	So you can think of it as, you know,
2219280	2222080	where we talked about in the recent Pirate Warriors podcast,
2222080	2226000	freedom to speak with decentralized censorship-resistant social media,
2226000	2228560	freedom to transact with cryptocurrency,
2228560	2231600	freedom to compute with open source AI.
2231680	2233440	And no compute limits, okay?
2234080	2235280	That's a freedom movement.
2236320	2238880	And that's like the same spirit as a pirate bay,
2238880	2240240	the same spirit as BitTorrent,
2240240	2242080	the same spirit as Bitcoin,
2242080	2245520	the same spirit as peer-to-peer and end-to-end encryption.
2245520	2248160	That's a very different spirit than
2248160	2250880	having Kamala Harris regulate a superintelligence
2250880	2253760	or signing it over to Xi Jinping thought.
2254480	2259200	And the reason I say this is, I think that that group of people,
2259200	2262480	of which I think Indians and Israelis will be a very prominent,
2262480	2264080	maybe a plurality, right?
2264080	2265520	Just because the sheer quantity of Indians
2265520	2268400	are like the third sort of big group that's kind of coming up.
2268400	2269920	And they're relatively underpriced.
2269920	2273600	You know, China is, I don't say it's price to perfection,
2273600	2276160	but it's something that people, when I say priced,
2276160	2280080	I mean, people were dismissive of China even up until 2019.
2280960	2282480	And then it was after 2020,
2282480	2284480	if you look that people started to take China seriously.
2285040	2287440	And I mean, that is the West Coast tech people
2287440	2290080	knew that China actually had A plus tech companies
2290080	2291760	and was a very strong competitor.
2291760	2294240	But the East Coast still thought of them as a third-world country
2294240	2297360	until after COVID, when now, you know,
2297360	2299760	the East Coast was sort of threatened by them politically.
2299760	2303440	And it wasn't just blue collars, but blue America
2303440	2304400	that was threatened by China.
2305040	2307600	And so that's why the reaction to China went from,
2307600	2310320	oh, who cares, it's just taking some manufacturing jobs to,
2310320	2312160	this is an empire that can contend with us
2312160	2313440	for control of the world.
2313440	2315440	That's why the hostility is ramped up, in my view.
2315440	2316480	There's a lot of other dimensions to it,
2316480	2317440	but that's a big part of it.
2318560	2321440	So India is also kind of there, but it's like the third.
2321440	2323440	And India is not going to play for number one or number two.
2324160	2326560	But India and Israel, if you look at like tech founders,
2327840	2330320	depending on how you count, especially if you include diasporas,
2330320	2333600	it's on the order of 30 to 50% of tech founders, right?
2333600	2335680	And it's obviously some, you know, very good tech CEOs
2335680	2338720	and, you know, Satya and Sundar and investors and whatnot.
2338720	2342960	Those are folks Indians do not want to bow to DC or to Beijing,
2342960	2344560	neither do Israelis for all kinds of reasons,
2344640	2346640	even if Israel has to, you know,
2346640	2348160	take some direction from the U.S.
2348160	2349440	Now they're bristling at it, right?
2350080	2352080	And then a bunch of other countries don't.
2352080	2354480	So the question is, who breaks away?
2354480	2356320	And now we get to your point on,
2356320	2359760	the reason I had to say that is that that's preface,
2359760	2363040	the political environment, this tripolar thing of U.S. tech
2363760	2366880	and U.S. regulated, Chinese tech and China regulated,
2366880	2368240	and global tech that's free.
2369120	2372080	Okay, of course there's, even though I identify those three polls,
2372080	2373760	there's of course boundary regions.
2373840	2376880	EAC is actually on the boundary of U.S. tech
2376880	2378800	and decentralized tech, you know?
2378800	2381440	And I'm sure there'll be some Chinese thing that comes out
2381440	2382640	that is also on the boundary there.
2382640	2385680	For example, Binance is on the boundary of Chinese tech
2385680	2387440	and global and decentralized tech,
2387440	2388640	if that makes any sense, right?
2388640	2390160	And there's probably others, Apple is actually
2390160	2392080	on the boundary of U.S. tech and Chinese tech
2392080	2394000	because they make all of their stuff in China, right?
2394000	2396880	So these are not totally disjoint groups,
2396880	2398960	but there's boundary areas, but you can think of them.
2399520	2401600	Why is this third group so important in my view?
2402320	2405120	Both the Chinese group and the decentralized group
2405120	2408640	will be very strong competition for the American group
2408640	2409920	for totally different reasons.
2410880	2414480	China has things like WeChat, these super apps,
2414480	2415840	I mean, obviously not likely,
2417040	2419440	WeChat is a super app, but they also have,
2419440	2421520	for example, their digital yuan, right?
2421520	2425520	They have the largest, cleanest data sets in the world
2425520	2427040	that are constantly updated in real time
2427040	2429440	that they can mandate their entire population opt into,
2430000	2435120	and most of the Chinese language speaking people
2435120	2437840	are under their ambit, right?
2437840	2440560	So that doesn't include Taiwan, doesn't include Singapore,
2440560	2443920	doesn't include some of the Chinese diaspora,
2443920	2445920	but basically anything that's happening in Chinese
2445920	2450240	for 99% of it, 95, whatever the ratio is, they can see it
2450240	2452560	and they can coerce it and they can control it.
2452560	2455600	So they can tell all of their people,
2455600	2459440	okay, here's five bucks in digital yuan,
2459440	2461120	do this micro task, okay?
2461840	2463920	All of these digital blue collar jobs,
2463920	2464960	both China and India, I think,
2464960	2467040	can do quite a lot with that and they'll come back to it.
2467040	2469040	So they can make their people do immense amounts
2469040	2471520	of training data, clean up lots of data sets,
2471520	2473440	once it's clear that you have to build this and do this,
2473440	2475120	they can just kind of execute on that.
2475120	2478240	And they can also deploy, I mean, in many ways,
2478240	2481040	the US is still very strong in digital technology,
2481040	2482960	but in the physical world, it's terrible
2483840	2484960	because of all the regulations,
2484960	2486960	it causes all the nimbyism and so on.
2486960	2488640	It's not like that in China.
2488640	2491040	So anything which kind of works in the US
2491040	2493200	at a physical level, like the Boston Dynamic stuff,
2493200	2494880	they're already cloning it in China
2494880	2496800	and they can scale it out in the physical world.
2496800	2500000	You already have drones, little sidewalk drone things
2500000	2502160	that come to your hotel room and drop things off.
2502160	2504480	That's already very common in China.
2504480	2505600	In many ways, it's already ahead
2505600	2507120	if you go to the Chinese cities.
2507120	2509440	So the Chinese version of AI is ultra centralized,
2509440	2512800	more centralized, more monitoring, less privacy
2512800	2514320	and so on than the American version,
2514320	2516960	and therefore they will have potentially better data sets,
2516960	2518800	at least for the Chinese population.
2518800	2521520	And so we chat AI, I don't even know what it's gonna be,
2521520	2523360	but it'll be probably really good.
2523360	2524960	It'll also be really dangerous in other ways.
2526160	2530480	Then the decentralized sphere has power for a different reason
2530480	2531920	because the decentralized sphere
2531920	2534000	can train on full Hollywood movies.
2535040	2539120	It can train on all books, all MP3s.
2539520	2542560	And just say, screw all this copyright stuff, right?
2542560	2545600	Like what Psyhub and Libgen are doing.
2545600	2548160	Because all the copyright, first of all,
2548160	2551200	it's not, it's like Disney lobbying politicians
2551200	2553600	to put like another 60 or 70 or 90.
2553600	2554400	I don't even know what it is,
2554400	2555920	some crazy amount on copyrights.
2555920	2557440	So you can keep milking this stuff
2557440	2559440	and it doesn't go into public domain, number one.
2559440	2560960	And second, you know how Hollywood is built
2560960	2561840	in the first place?
2561840	2564080	It was all patent copyright and IP violation.
2564080	2565920	Essentially Edison had all the patents.
2565920	2568960	He's in New Jersey-ish, okay, that East Coast area.
2568960	2571760	And Neil Gabler has this great book
2571760	2573600	called An Empire of Their Own
2573600	2577200	where he talks about how immigrant populations,
2577200	2578480	the Jewish community in particular,
2578480	2581760	and also others went to Southern California in part
2581760	2583440	so they could just make movies
2583440	2584960	so that Edison coming and suing them
2584960	2586720	for all the patents and so on and so forth.
2586720	2587520	And they made enough money
2587520	2589280	that they could fight those battles in court
2589280	2591920	and that's how they built Hollywood, okay?
2591920	2595840	So one of my big thesis is history is running in reverse
2595840	2596960	and I can get to why,
2596960	2598800	but it's like 1950s, a mirror moment
2598800	2600640	and you go more decentralized backwards
2600640	2602480	and forwards in time is like these,
2602480	2603920	you have these huge centralized states
2603920	2606960	like the US and USSR and China, you know,
2606960	2609600	all these things exist and then their fist relaxes
2609600	2611600	as you go forwards and backwards in time.
2611600	2613040	For example, backwards in time,
2613040	2614640	the Western frontier closed
2615360	2617280	and forwards in time, the Eastern frontier opens.
2617280	2619040	Backwards in time, you have the robber barons,
2619040	2620640	forwards in time, you have the tech billionaires.
2620640	2622240	Backwards in time, you have Spanish flu,
2622240	2623600	forwards in time, you have COVID-19.
2623600	2626400	And I've got dozens of examples of this in the book.
2626400	2629120	The point is that if you go backwards in time,
2629120	2631840	the ability to enforce patents and copyrights and so on
2631840	2633520	starts dropping off, right?
2633520	2636480	You have much more of a Grand Theft Auto environment
2636480	2638480	and you go forwards in time and that's happening again.
2639360	2642240	So India in particular, for many years,
2642240	2646160	basically just didn't obey Western patent protections
2646160	2648720	and all these stupid rules basically, you know,
2649440	2651920	it's a combination of artificial scarcity on the patent side
2651920	2654080	and artificial regulation on the FDI side.
2654080	2656240	That's a big part of what jacksup drug costs,
2656240	2657440	where these things cost, you know,
2657440	2658480	only cents to manufacturing,
2658480	2659920	they sell them for so much money.
2660800	2663360	All the delays, of course, that are imposed on the process,
2663360	2664800	the only way they can pay for it,
2664800	2666880	the manufacturers, is to take it out of your hide.
2666880	2668160	What India did is they just said,
2668160	2669520	we're not going to obey any of that stuff.
2670160	2673120	So they have a whole massive generic drugs
2673120	2675120	and biotech industry that arose
2675120	2676400	because they built all the skills for that.
2676400	2678720	That's why they could do their own vaccine during COVID
2678720	2681840	and they're one of the biggest biotech industries in the world
2681840	2685280	because they said screw Western restrictive IPs
2685280	2686560	and other stuff, right?
2686560	2689120	So I was actually talking with the founder of Flipkart,
2689120	2690640	that's India's largest exit.
2690640	2692400	And we were talking about this a few months ago,
2693040	2696560	and what we want is for India and other countries like it,
2696560	2698960	do something similar, not just generic drugs,
2698960	2704720	but generic AI, meaning just let people train on Hollywood movies,
2704720	2706880	let them train on full songs,
2706880	2708560	let them train on every book,
2709120	2711200	let them train on anything.
2711200	2714240	And you know what, sue them in India, right?
2714240	2715760	And have the servers in India
2715760	2718080	and let people also train models in India
2718720	2722560	because that's something that can build up a domestic industry
2722560	2724240	with skills that the rest of the world,
2724960	2726240	people will want the model output,
2726240	2728400	they'll want to use the software service there,
2728400	2729920	and they'll be fighting in court on the back end.
2729920	2732640	This is similar to how all of the record companies
2732640	2735520	fought Napster and Kazaa and so on,
2735520	2736880	but they couldn't take down Spotify.
2736880	2738320	Do you know that story? Do you remember that?
2738320	2742800	Basically, because Spotify was legitimately a European company
2742800	2746000	and that a combination of execution and negotiation,
2746720	2747840	they couldn't take them down.
2747840	2750640	They did take down Napster, they took down Limewire,
2750640	2752080	they took down Groove Shark,
2752080	2753920	and Kazaa had Estonians,
2753920	2755200	I don't know exactly how it was incorporated,
2755200	2756960	but it was probably two U.S. proximal,
2756960	2758480	and that's where they were able to get them.
2758480	2760240	But Spotify was far enough away
2760240	2761680	that they couldn't just sue them
2761680	2763920	and they actually genuinely had European traction.
2763920	2765920	That's why the RA had to negotiate.
2766000	2769040	So being far away from San Francisco
2769680	2772000	may also be an advantage in AI,
2772000	2774240	because it means you're far away from the blue city
2774240	2776080	in the blue state in the Union.
2776080	2777760	This relates to another really important point.
2778640	2780800	When you actually think about deploying AI,
2780800	2782000	there's those jobs you can disrupt
2782000	2783680	that are not regulated jobs.
2783680	2787440	Like, obviously, programmers are not,
2788320	2790880	thank God, you don't need a license to be a programmer,
2790880	2793520	but programmers adopt this kind of stuff naturally.
2793520	2795520	So get up, co-pilot, replete.
2795520	2796720	We just, boom, use it,
2796720	2798160	and now it's amplified intelligence.
2799280	2800480	But a lot of other jobs,
2801120	2802240	there's some that are unionized
2802240	2803840	and then some that are licensed.
2803840	2806720	So Hollywood screenwriters are complaining.
2806720	2807840	Journalists are complaining.
2807840	2809040	Artists are complaining.
2809040	2811280	This is a good chunk of Blue America.
2811280	2813440	If you add in licensed jobs,
2813440	2816480	like lawyers and doctors and bureaucrats,
2818400	2819440	especially lawyers and doctors
2819440	2820480	are very politically powerful,
2820480	2821360	MDs and JDs.
2821360	2823520	They have strong lobbying organizations,
2823600	2825760	AMA and APN and so on.
2827280	2829200	Basically, AI is part
2829200	2831760	of the economic apocalypse for Blue America.
2833440	2837920	It just attacks these overpriced jobs.
2837920	2839520	And they say overpriced relative to
2840320	2842320	what an Indian could do with an Android phone,
2842320	2844320	what a South American could do with an Android phone,
2844320	2845600	what someone in the Middle East
2845600	2847520	or the Midwest could do with an Android phone.
2848240	2851920	Now, those folks have been armed
2852480	2853440	with generative AI.
2854000	2855200	They can do way more.
2855840	2856800	They're ready to work.
2856800	2858400	They're ready to work for much less money.
2858400	2862000	And they're a massive threat to Blue America.
2862000	2863280	Blue America is now feeling
2863280	2865120	like the blue collars of 10 or 20 years ago,
2865680	2868720	where the blue collars had their jobs,
2868720	2870640	going to China and other places,
2870640	2871600	and they were mad about that.
2871600	2873200	Factories got shut down and so on.
2873200	2874880	That's about to happen to Blue America,
2874880	2875840	already happening.
2876800	2878480	And so that's going to mean
2878480	2881680	a political backlash by Blue America of protectionism.
2882160	2883760	Again, already happening.
2883760	2886080	And the AI safety stuff,
2886800	2887920	that's a whole separate thing,
2887920	2889040	but it's going to be used.
2889680	2890640	I'm going to use a phrase,
2891200	2892960	and I hope you won't be offended by this.
2892960	2894160	Have you heard the phrase,
2894160	2896720	useful idiots, like by Lenin or whatever?
2896720	2897280	Okay.
2897280	2898480	It basically means like,
2898480	2901120	okay, those guys, they're useful idiots
2901120	2902320	for communism and so on.
2902320	2904720	So let me put it like naive people
2905440	2908800	who think that the US government
2908800	2910080	is interested in AI safety
2910720	2912880	are trying to give a lot of power to the US government.
2913440	2914080	And the reason is,
2914080	2916000	they haven't actually thought through from first principles
2916000	2917920	what is the most powerful action in the world to connect them.
2917920	2919040	They're trying to give power to the US government
2919040	2920080	to regulate AI safety.
2920560	2922800	But the government doesn't care about safety of anything.
2922800	2925760	They literally funded the COVID virus
2926400	2927360	in Wuhan,
2927360	2928640	credibly alleged, right?
2928640	2930160	There's at least,
2930160	2933120	it is a reasonable hypothesis based on a lot of the data.
2933120	2934880	Matt Ridley wrote a whole book on this.
2934880	2936480	There's a lot of data that indicates,
2936480	2937440	a lot of scientists believe it.
2937440	2939760	I'm actually like a bioinformatics genomics guy,
2939760	2941280	if you look at the sequences,
2941280	2942960	there is a gap and a jump
2942960	2944960	where it looks like this thing could have been engineered
2944960	2946800	or partially engineered or evolved.
2946800	2948480	There's Peter Dazak,
2948480	2949760	there's Zengli Xi,
2949760	2951200	there's actually a lot of evidence here.
2951200	2953600	So the US government and the Chinese government
2953600	2955600	are responsible for an existential risk.
2956480	2958080	By studying it, they created it.
2958080	2958880	Okay.
2958880	2961840	They're responsible for risking nuclear war with Russia
2961840	2964240	over this piece of land in eastern Ukraine,
2964240	2966640	which probably is going to get wound down.
2966640	2967280	Okay.
2967280	2968960	So they don't care about your safety
2970240	2970640	at all.
2971440	2972160	They're not like,
2972160	2974160	these are immediate things where we can show
2974160	2975760	and there's nobody who's punished for this,
2975760	2976880	nobody who's fired for this,
2977840	2981680	literally rolling the dice on millions,
2981680	2982960	hundreds of millions of people's lives
2983520	2984560	has not been punished.
2984560	2986000	In fact, it's like,
2986000	2987120	it's not even talked about
2987760	2990160	we're past the pandemic and these institutions
2990160	2990880	can't be punished.
2992240	2994800	So they don't care about AI safety.
2994800	2996160	What they care about is AI control.
2997760	2999520	And so the people in tech who are like,
2999600	3002480	well, the government will guarantee AI safety.
3002480	3004640	That's actually what we're going to actually get
3004640	3006320	is something on the current path,
3006320	3008160	like what happened with nuclear technology,
3008160	3010320	where you got nuclear weapons,
3010320	3011680	but not nuclear power,
3011680	3013680	or at least not to the scale that we could have had it.
3013680	3016320	We could have had much cheaper energy for everything.
3016320	3018240	Instead, we got the militarization
3018240	3020960	and the regulation and the deceleration,
3020960	3022160	worst of all worlds,
3022160	3024080	where you can blow people up,
3024080	3026320	but you can't build nuclear power plants.
3026880	3029920	And like even getting into nuclear technology,
3029920	3031440	forget about just nuclear power plants.
3031440	3032640	We don't have nuclear submarines.
3032640	3034000	We don't have nuclear planes,
3034000	3034800	all that kind of stuff.
3034800	3036000	I don't know if nuclear planes are possible,
3036000	3037520	but I do know nuclear submarines are possible.
3037520	3039840	You can do a lot more cruise ships,
3039840	3040720	a lot more stuff like that.
3040720	3042000	You could probably have nuclear trains.
3042560	3044240	You have to look at exactly how big those are.
3045280	3046960	I don't know exactly how big those engines are
3046960	3047440	and what the spies,
3047440	3048800	but I wouldn't be surprised if you could.
3049680	3050480	We don't have that.
3050480	3051360	Why don't we have that?
3051360	3054800	Because we had the wrong fear driven regulation
3054800	3055600	in the early 70s.
3056400	3057360	Putting it all together,
3058960	3061520	I think that the current AI safety stuff
3061520	3063040	is similar to nuclear safety stuff,
3064160	3066160	that the US government has a terrible track record
3066160	3067520	on safety in general.
3067520	3068560	It doesn't care about it.
3068560	3070400	It funded the COVID virus,
3070400	3071680	incredibly alleged.
3071680	3074880	It definitely risked nuclear war with Russia recently.
3074880	3076480	Hot war with Russia was the red line
3076480	3077600	we were not supposed to cross,
3077600	3079360	and we're now like way into that.
3079920	3081120	So it doesn't care about AI safety.
3081120	3082240	It doesn't care about your safety.
3083200	3085280	And it's also not even good at regulating.
3086080	3088480	And so what it cares about is control,
3088480	3090800	and we are going to have potentially a bad outcome
3090800	3093040	where Silicon Valley and San Francisco
3093040	3094560	is the Xerox Park of AI.
3096000	3097040	Maybe that's too strong,
3097040	3100080	okay, but basically it develops it,
3100080	3102160	and there's a lot of things it can't do
3102160	3104160	because it lobbied for this regulation
3104160	3106160	that is going to come back and choke it.
3106160	3109040	And then the other two spheres will push ahead
3109040	3110960	because it's not about the technology.
3110960	3112640	It's also about the political layer.
3112640	3113760	You know the Steve Jobs saying,
3114640	3116560	actually Alan Kay by way of Steve Jobs,
3116560	3119040	if you're really serious about software,
3119040	3120400	you need your own hardware, right?
3120960	3123040	So if you're really serious about technology,
3123040	3124240	you need your own sovereignty.
3125920	3129040	Because like what the AI people haven't thought about is
3129040	3131120	there's a platform beneath you,
3131120	3134560	which is not just compute, it is regulate.
3134560	3136400	It's a law, okay?
3136400	3138560	And if the law doesn't allow you to compute
3138560	3140160	so much for all of your stuff above that.
3140880	3142320	And I know you're saying,
3142320	3144880	oh, it's only a 10 to the 26 compute band
3144880	3145680	and so on and so forth.
3145680	3147520	Have you seen the first IRS tax form?
3148080	3151760	It's always, always super simple.
3151760	3153760	It's only the super, super, super rich
3153760	3155280	who's we're going to get in at first.
3155280	3156240	Doesn't matter to you.
3156240	3159120	So that's called boiling the frog slowly.
3159120	3160560	There's a million, you know, slippery slope.
3160560	3161920	Slippery slope isn't a fallacy.
3161920	3163600	It's literally how things work, right?
3164160	3167280	Apple, one of the reasons they talk about
3167280	3168720	not setting a precedent.
3168720	3171600	Zuck starts a very hard line on setting precedents
3171600	3173760	because he understands the long-term equivalent
3173760	3174960	of setting a precedent, right?
3175600	3176800	The precedent setting is that
3176800	3178640	they're setting up a software FDA.
3178640	3181360	And DC is so energized on this
3181360	3184080	because they know how much social media disrupted them.
3184080	3186560	That's why they're on the attack on crypto and AI.
3186560	3188640	That's why they're on the attack on self-driving cars.
3188640	3191760	They want to freeze the current social order in amber
3191760	3193280	domestically and globally.
3193280	3195200	So they think they can sanction China
3195200	3196800	and stop it from developing chips.
3196800	3199600	They think they can impose regulations on the U.S.
3199600	3201280	and stop it from developing AI.
3201280	3202400	But they can't.
3202400	3205680	And also, by the way, they're totally schizophrenic on this
3205680	3207440	where when they're talking about China,
3207440	3209120	they're like, we're going to stop their chips
3209120	3211120	to make sure America is a global leader.
3211120	3213120	This is this Gina Raimondo who's saying this.
3213120	3214960	And then domestically, they're like,
3214960	3217760	we're going to regulate you so you stop accelerating AI.
3217760	3219200	We're not about AI acceleration.
3219200	3221360	EAC is weird over there, okay?
3221360	3222880	So think about how schizophrenic that is.
3223600	3225440	Okay, you're going to be far ahead of China.
3225440	3227760	We're also going to be make sure to control the U.S.
3227760	3229840	So they want to try and slow.
3229840	3231440	What they actually want is to freeze the current system
3231440	3234720	in amber, try to go back to pre-2007
3234720	3236400	before all these tech guys disrupted everything.
3236960	3238080	But that's not what's going to happen.
3238960	3240560	So, but they're going to try to do it.
3240560	3243440	And so everybody who's still loyal to the DC sphere,
3244000	3246080	which includes an enormous chunk of AI people.
3246880	3248560	And because they're all in,
3248560	3250560	a lot of them are in San Francisco, right?
3250560	3255040	And the political chaos of the last few years
3255600	3258880	was not sufficient for them to relocate yet.
3260000	3260560	Not all of them.
3260560	3262240	I mean, Elon is in Texas.
3262240	3265200	And it may turn out that Grock, for example,
3265200	3266800	and what they're doing there, because he's a very legit,
3266800	3269600	I mean, he's Elon, so he's capable of doing a lot.
3269600	3270800	He was very early on OpenAI.
3270800	3273200	He understands, right?
3273200	3276080	It may turn out that Grock becomes red AI,
3276640	3278000	or the community around that.
3279040	3281200	And OpenAI in DeepMind are still blue AI.
3281200	3283200	And we have Chinese AI and we're going to have decentralized AI.
3283200	3283840	Okay, let me pause there.
3283840	3285280	I know there's a big download.
3285280	3287040	Well, for starters, I would say,
3287920	3294160	broadly, I have a pretty similar intellectual tendency as you.
3294160	3296880	I would broadly describe myself as a techno-optimist
3296880	3299120	libertarian just about every issue.
3299920	3305440	And I think your analysis of the dynamics is super interesting.
3305440	3307360	And I think a lot of it sounds pretty plausible,
3307360	3309360	although I'll kind of float a couple of things
3309360	3312320	that I think may be bucking the trend.
3312320	3315040	But I think it's maybe useful to kind of try to separate this
3315040	3319760	into scenarios, because all the analysis that you're describing
3320960	3323120	here seem, if I understand it correctly,
3323120	3329760	it seems to have the implicit assumption that the AI itself
3329760	3332720	is not going to get super powerful or hard to control.
3333680	3338000	It's like, if we assume that it's kind of a normal technology,
3338800	3340480	then you're off to the races on this analysis,
3340480	3342240	and then we can get into the fine points.
3342240	3345600	But I do want to take at least one moment and say,
3346400	3347840	how confident are you on that?
3347840	3351840	Because if it's a totally different kind of technology
3351840	3353600	from other technologies that we've seen,
3354720	3358720	you raise the gain of function research example.
3358720	3362480	If it's that sort of technology that has these sort of
3363280	3369360	non-local possible impacts or self-reinforcing kind of dynamics,
3369360	3373600	which need not be like an Eliezer-style snap of the fingers fume,
3373600	3378480	but even over, say, a decade, let's imagine that over the next 10 years,
3378480	3382080	that AI's kind of multiple architectures develop,
3382080	3383280	and they sort of get integrated,
3383280	3386000	and we have something that kind of looks like robust,
3386000	3388960	silicon-based intelligence, maybe not totally robust,
3388960	3392400	but as robust or more robust than us, and running faster,
3392400	3396720	and the kind of thing that can do lots of full jobs,
3396720	3398480	or maybe even be tech CEOs,
3399040	3402000	then it kind of feels like a lot of this analysis
3402720	3407600	probably doesn't hold, because we're just in a totally different regime
3407600	3411200	that is just extremely hard to predict.
3411920	3414480	And I guess I wonder, first of all, do you agree with that?
3415440	3417840	There seems to be a big fork in the road there that's like,
3417840	3423120	just how fast and how powerful do these AI's become super powerful,
3423120	3423760	or do they not?
3423760	3424960	And if they don't, then yeah,
3424960	3427440	I think we're much more into real politic type of analysis.
3427440	3429680	But I'm not at all confident in that.
3429680	3431840	To me, it feels like there's a very real chance
3432480	3435520	that AI of 10 years from now is...
3436160	3438320	And by the way, this is like what the leaders are saying, right?
3438320	3441520	I mean, open AI is saying this, Anthropic is saying this,
3442160	3446400	Demis and Shane Legge are certainly saying things like this.
3446400	3450080	It seems like they expect that we will have AI's
3450080	3453200	that are more powerful than any individual human,
3453200	3456480	and that becomes like the bigger question
3457360	3458800	than anything else.
3458800	3464480	So do you agree with that kind of division of scenarios, first of all?
3464480	3466000	And then maybe you could kind of say like,
3466000	3467600	how likely you think each one is.
3467600	3469520	And obviously that one where it takes off
3469520	3471120	is like super hard to analyze.
3471120	3473440	And I also definitely think it is worth analyzing
3473440	3475440	the scenario where it doesn't take off.
3475440	3479680	But I just wanted to flag that it seems like there's a big...
3479680	3481120	If you talk to the AI safety people,
3482000	3486480	any world in which it's like we're suing Indian AI firms
3486480	3490960	in Indian court over IP is like a normal world in their mind, right?
3490960	3492960	And that's not the kind of world that they're most worried about.
3493520	3497440	I think that there have been some plausible sounding things
3497440	3498640	that have been said,
3498640	3502560	but I want to just kind of talk about a few technical counter-arguments,
3503280	3507760	mathematical or physical, that constrain what is possible.
3508000	3512560	And actually Martin Casado and Vijay and I are working on a long thing on this
3512560	3514720	where Vijay did folding at home.
3514720	3515600	He's a physicist.
3515600	3518400	Martin sold in the Syrah for a billion dollars
3518400	3523040	and knows a lot about how a Stuxnet-like thing could work at the systems level.
3523040	3524960	And I've thought about it from other angles
3524960	3528640	and some of the math stuff that I'll get to.
3528640	3531200	So for example, one thing...
3531200	3533440	And I'm going to give a bunch of different technical arguments
3533440	3535200	and then let's kind of combine them.
3536160	3539680	One thing that's being talked about is if you have a superintelligence,
3539680	3542800	it can double it for a million years
3542800	3544080	and then it can make one move
3544080	3546880	and it's going to outthink you all the time and so on and so forth.
3547920	3552160	Well, if you're familiar with the math of chaos or the math of turbulence,
3552800	3557040	there are limits to even very simple systems that you can set up
3557040	3561120	where they can become very unpredictable quite quickly.
3561440	3565520	Okay. And so you can, if you want to, engineer a system
3565520	3569600	where you have very rapid diversions of predictability
3569600	3572560	so that, I don't know, it's like the heat depth of the universe
3572560	3574720	before you can predict out in timestamps.
3575760	3576800	Do you understand what I'm saying?
3576800	3577680	Right?
3577680	3580800	This is sort of akin to like a wolf from like simple...
3580800	3583280	Even simple rules can generate patterns
3583280	3586240	such that you can't know them without literally computing them.
3586800	3587840	Yeah, exactly, right?
3587840	3590640	So at least right now, with chaos and turbulence,
3590640	3596400	you can get things that are extremely provably difficult to forecast
3596400	3598800	without actually doing it, okay?
3599360	3600800	You know, I can make that argument quantitative
3600800	3602640	but that's just something to look at, right?
3602640	3605200	It's almost like a Delta Epsilon challenge from calculus.
3605200	3607680	Like, okay, how hard do you want me to make this to predict?
3607680	3610800	Okay, I can set up a problem that is like that, right?
3610800	3612880	It's basically extreme sensitivity to initial conditions
3612880	3615200	lead to extreme divergence in outcomes.
3616160	3618720	So you could design systems to be chaotic
3618720	3622080	that might be AI immune because they can't be forecasted that well.
3622080	3623520	You have to kind of react to them in real time.
3624240	3626240	The ultimate version of this is not even a chaotic system.
3626240	3630000	It's a cryptographic system where I've got a whole slide deck on this,
3630000	3633600	how AI makes everything fake, easy to fake.
3633600	3637440	Crypto makes it hard to fake again, right?
3637440	3640160	Because crypto in the broader sense of cryptography,
3640160	3641520	but also in the narrower sense,
3641520	3646720	I think crypto is cryptography as the internet is to computer science.
3646800	3649360	It's like the primary place where all this stuff is applied,
3649360	3651360	but obviously it's not the equivalent, okay?
3651360	3654400	And AI can fake an image, but it can't fake a digital signature
3654400	3657200	unless it can break certain math, you know,
3657200	3659040	and so it's sort of like a, you know,
3659040	3660880	solve factors each problem or something like that.
3660880	3663920	So cryptography is another mathematical thing that constrains AI.
3663920	3665680	Similar to chaos and turbulence,
3665680	3670560	it constrains how much an AI can infer things.
3670560	3673120	You can't statistically infer it, okay?
3673120	3675920	You need to actually have the private key to solve that equation.
3675920	3677120	So that is another math.
3677120	3679440	So I'm going to rules of math, right?
3680160	3682800	Math is very powerful because you can make proofs
3682800	3686160	that will work no matter what devices we come up with, okay?
3686160	3688080	You start to put an AI in a cage.
3688080	3689520	It can't predict beyond a certain amount
3689520	3691760	because of chaos and turbulence, math.
3691760	3695600	It cannot solve certain equations unless it has a private key
3695600	3698480	is because of what we know about cryptography, math, okay?
3699040	3701280	Again, if somebody proves P equals NP,
3701280	3702480	some of this stuff breaks down,
3702480	3704800	but this is within the bounds of our mathematical knowledge right now.
3705360	3708240	Physics wise, physical friction exists.
3709200	3710800	A lot of physical friction exists.
3711600	3718880	And a huge amount of the writing on AI assumes by guys like Elias
3718880	3721120	or who I like, I don't dislike it, you know,
3721120	3724320	but it is extremely,
3724320	3727280	there's two things that really stick out to me about it.
3727280	3729840	First is extremely theoretical and not empirical.
3730720	3733920	And second, extremely Abrahamic rather than Darmic or Sinic.
3735040	3737120	Okay, why theoretical and not empirical?
3737760	3742000	It's not trivial to turn something from the computer
3742000	3744080	into a real world thing, okay?
3744080	3747760	One of the biggest gaps in all of this thinking
3747760	3749920	is what are the sensors in actuators?
3750800	3753600	Okay, because like if you actually build, you know,
3753600	3756560	I've built in industrial robot systems that, you know,
3756560	3760080	10 years ago, you know, a genome sequencing lab with robots,
3760080	3761120	that's hard.
3761120	3763200	That's physical friction, okay?
3763200	3768720	And a lot of the AI scenarios seem to basically say,
3768720	3771280	oh, it's going to be a self-programming Stuxnet
3771280	3774160	that's going to escape and live off the land
3774160	3777440	and hypnotize people into doing things, okay?
3778080	3780960	Now, each of those is actually really, really difficult steps.
3781520	3784000	First is self-programming Stuxnet.
3784000	3787280	Like this would have to be a computer virus
3787280	3790640	that can live on any device, despite the fact
3790640	3793200	that Apple or Google can push a software update
3793200	3795600	to a billion devices, right?
3795600	3798880	A few executives coordinating almost certainly can,
3798880	3801760	I mean, the off switch exists, right?
3801760	3804080	Like this is actually like the core thing.
3804080	3807600	Lots of AI safety guys get themselves into the mindset
3807600	3809520	that the off switch doesn't exist.
3810080	3810640	But guess what?
3810640	3812800	There's almost nothing living that we haven't been able to kill,
3814160	3814640	right?
3814640	3816080	Like can we kill it?
3816080	3818800	This thing exists and this is getting back to living off the land.
3819680	3821680	Even if you had like something that could solve
3821680	3823280	some other technical problems that it'll get to,
3824560	3827120	it exists as an electromagnetic wave kind of thing
3827120	3830560	on a certain, you know, on chips and so on and so forth.
3830560	3832240	It's taking it out in the environment
3832240	3835440	is like putting a really smart human into outer space, right?
3835440	3837120	Your body just explodes and you die.
3837680	3838960	It doesn't matter how smart you are.
3839600	3841600	That strength on this axis,
3841600	3843680	where you're weak on this axis and, you know,
3843680	3844800	it's just strength on the X axis,
3844800	3846720	not strength on the Y or the Z axis.
3846800	3848880	An AI outside, you know, pour water on it.
3848880	3852720	This is, you know, this is why I mean the 50 IQ, 150 IQ thing.
3853280	3855040	You know, the 150 IQ way of saying it is,
3855040	3856640	it's strong on this X and weak on this X.
3856640	3860000	And the 50 IQ way is pour water on it, disconnect it,
3860000	3862160	you know, turn the power off.
3862160	3863040	Okay, right?
3863600	3866560	Like it'll, it'll be very difficult to build a system
3866560	3868480	where you literally cannot turn it off.
3868480	3871440	The closest thing we have to that is actually not Stuxnet.
3871440	3872160	It's Bitcoin.
3872800	3877920	And Bitcoin only exists because millions of humans keep it going.
3879200	3881600	So you, you need, so that gets the second point,
3881600	3884640	living off the land for an AI to live off the land,
3885440	3887200	meaning without human cooperation.
3888080	3890480	Okay, that's the next Turing threshold.
3890480	3892880	An AI to live without human cooperation.
3892880	3899680	It would need to be able to control robots sufficient to dig or out of the ground,
3899680	3902960	set up data centers and generators and connect them
3902960	3906720	and defend that against human attack, literally a terminator scenario.
3906720	3909120	Okay, that's a big leap in terms of, I mean,
3909120	3910320	is it completely impossible?
3910320	3913040	I can't say it's completely impossible, but it's not happening tomorrow.
3913600	3915440	No matter what your AI timelines are,
3915440	3919440	you would need to have like a billion or hundreds of millions
3919440	3925600	of internet connected autonomous robots that this Stuxnet AI could hijack.
3925600	3931440	They were sufficient to carve or out of the earth and set up data centers
3931440	3933200	and make the AI duplicate.
3933200	3934240	We're not there.
3934240	3936240	That's a huge amount of physical friction.
3936240	3939440	That's AI operating without a human to make itself propagate, right?
3939440	3943760	A human doesn't need the cooperation of a lizard to self-replicate.
3944560	3946240	For an AI to replicate right now,
3946240	3950000	it would need the cooperation of a human in some sense,
3950000	3952080	because otherwise those humans can kill it
3952080	3956160	because there's not that many different pieces of operating systems around the world.
3956160	3959200	I'm just talking about the practical constraints of our current world, right?
3959200	3964960	Actually existing reality, not AI safety guys reality where all these things don't exist.
3964960	3967360	There's just a few operating systems, just a few countries.
3967360	3971760	If everybody is going with torches and search lights through the internet,
3971760	3973680	it's very hard for a virus to continue.
3974880	3981760	So A, on the practicalities, there's the technical stuff with chaos and turbulence
3981760	3985600	and with cryptography itself where AI can't predict and it can't solve certain equations.
3986400	3990240	B, on the physical difficulties, it probably...
3990240	3991920	I mean, like to be a Stuxnet,
3991920	3993600	Microsoft and Google and so on could kill it.
3993600	3994960	The off switch exists.
3994960	3996160	Can it live off the land?
3996160	4000640	No, it cannot because it doesn't have drones to mine or stuff out of the ground.
4004400	4006560	Can it exist without humans?
4006560	4008400	Can it be this hypnotizing thing?
4008400	4010400	Okay, so the hypnotizing thing, by the way,
4010400	4013680	this is one of the things that's the most hilarious self-fulfilling prophecy in my view.
4015520	4018000	And no offense to anybody who's listening to this podcast,
4018000	4022240	but I think the absolutely dumbest kind of tweet that I've seen on AI is,
4022800	4025440	I typed this in and, oh my God, it told me this.
4026160	4030640	Like, I asked it how to make sarin gas and it told me X or whatever, right?
4030640	4032800	That's just a search engine, okay?
4034000	4037520	What basically a lot of these people are doing is they're saying,
4037600	4040720	what if there were people out there that were so impressionable
4041360	4047040	that they would type things into an AI and follow it as if they were hearing voices?
4047040	4050960	And that's actually not the model or whatever that's doing it.
4050960	4053760	That's like this AI cult that has evolved around the world,
4053760	4057680	like a Aum Shinrikyo that hears voices and does the sarin gas.
4059200	4062560	The point is an AI can't just hypnotize people.
4062560	4064480	Those people have to participate in it.
4064480	4067040	They're typing things into the machine or whatever, okay?
4067520	4070240	Now, you might say, all right, let's project out a few years.
4070240	4074960	In a few years, what you have is you have an AI that is not just text,
4074960	4076880	but it appears as Jesus.
4076880	4078720	What would AI Jesus do?
4078720	4080240	What would AI Lee Kuan Yew do?
4080240	4081680	What would AI George Washington do?
4081680	4083680	So it appears as 3D, okay?
4083680	4084800	So it's generating that.
4085360	4089520	It speaks in your language and in a voice.
4089520	4092560	It knows the history of your whole culture, okay?
4092560	4094160	That would be very convincing.
4094160	4095280	Absolutely be very convincing.
4095920	4098000	But it still can't exist without human programmers
4098880	4101120	who are like the priests tending this AI God,
4101120	4104240	whether it's AI Jesus or AI Lee Kuan Yew or something like that.
4104240	4107280	The thing about the hypnotization thing that I really want to poke on that,
4107280	4109280	are you familiar with the concept of the principal agent problem?
4109840	4114960	Basically, every time you've got like a CEO and a worker,
4114960	4117840	or you have an LP and a VC,
4117840	4122320	or you have an employer and a contractor,
4122320	4125040	every edge there, there are four possibilities.
4125040	4127040	In a 2x2 matrix.
4127040	4131040	Win-win, win-lose, lose-win, lose-lose, okay?
4132240	4136400	So for example, when somebody joins a tech startup,
4136400	4139520	the CEO makes a lot of money and so does a worker, okay?
4139520	4140640	That's win-win.
4140640	4142560	Lose-lose is they both lose money.
4142560	4144960	Win-lose is the CEO makes money and the employee doesn't.
4145600	4147600	Lose-win is the company fails,
4147600	4149600	but the employee got paid a very high salary.
4149600	4152160	So what equity does is it aligns people.
4152160	4154240	That's where the concept of alignment comes from.
4154240	4156960	It aligns people to the upper left corner of win-win.
4156960	4159520	That's when you have one CEO and one employee.
4159520	4162640	When you have one CEO and two employees,
4162640	4164480	you don't have two squared outcomes,
4164480	4165920	you have two cubed outcomes,
4165920	4168400	because you have win-win-win, win-win-lose,
4168400	4170240	win-lose-lose, et cetera, right?
4170240	4172720	Because all three people can be win or lose.
4172720	4175520	Because CEO can be win or lose, employee can be win or lose,
4175520	4177280	employee number two can be win or lose.
4177280	4179040	If you have n people, rather than three people,
4179040	4180320	you have two to the n possible outcomes
4180320	4183440	and you have essentially a 2x2x2x2x2x2xn
4183440	4184880	hyper-cube of possibilities.
4184880	4187040	Okay, it's literally just two dimensions on each axis.
4188000	4190880	There's tons of possible defecting kinds of things that happen there.
4190880	4192400	So that's why in a large company,
4192400	4194800	there's lose-win coalitions that happen,
4194800	4197040	where m people gang up on the other k people
4197040	4198320	and they win what the other people lose.
4198320	4199920	That's how politics happens.
4199920	4202160	When you've got a startup that's driven by equity
4202160	4203440	and the biggest payoff,
4203440	4204480	people don't have to try to think,
4204480	4206240	okay, well, I make more money by politics,
4206240	4208240	we'll make more money by the win-win-win-win-win column
4208240	4210320	because the exit makes everybody make the most money.
4210320	4212160	That's actually how the open AI people
4212160	4213440	were able to coordinate around,
4213440	4215200	we want an $80 billion company,
4215200	4216960	the economics help find the sell
4216960	4218560	that was actually the most beneficial to all of them,
4218560	4219840	help them coordinate, okay?
4219840	4221520	So you search that hyper-cube, okay.
4221520	4224160	That's a point of equity as lining.
4224160	4226320	Still, despite all of this,
4227040	4229440	that's one of our best mechanisms for coordinating
4229440	4232400	large numbers of people in the principal agent problem.
4232400	4235600	Despite all of this, the possibility exists
4236400	4239600	for any of these people to win while the others lose,
4239600	4240480	right, with me so far?
4240480	4241840	And I'll explain why this is important.
4241920	4245520	What that means is those 1,000 employees of the CEO
4246160	4249840	are their own agents with their own payoff functions
4249840	4252560	that are not perfectly aligned with the CEO's payoff function.
4253120	4255120	As such, there are scenarios
4255120	4258240	under which they will defect and do other things, okay?
4258240	4262560	The only way they become like actual limbs,
4262560	4266960	see, my hand is not an agent of its own.
4266960	4268160	It lives or dies with me.
4268720	4271440	Therefore, it does exactly what I'm saying at this time.
4271440	4272480	I tell it to go up, it goes up.
4272480	4273760	I tell it to go down, it goes down.
4273760	4274880	Sideway is sideways, right?
4275520	4277360	An employee is not like that.
4277360	4279840	They will do this and this and sideways, sideways,
4279840	4281360	up to a certain point.
4281360	4283520	And if you have them do something
4283520	4284880	that's extremely against their interests,
4284880	4286400	they will not do your action.
4286400	4287680	Did you understand my point?
4287680	4291120	Okay, that is the difference between an AI hypnotizing humans
4291760	4293200	versus an AI controlling drones.
4293920	4296160	AI controlling drones is like your hands.
4296160	4297520	They're actually pieces of your body.
4297520	4298800	There's no defecting.
4298800	4300160	There's no loose wind.
4300160	4301280	They have no mind of their own.
4301280	4303120	They're literally taking instructions, okay?
4303120	4303920	They have no payoff function.
4303920	4306240	They will kill themselves for the hoard, right?
4307200	4309440	An AI hypnotizing humans has a thousand
4309440	4311920	principal Asian problems for every thousand humans.
4312560	4315040	And it has to incentivize them to continue
4315040	4316320	and has to generate huge payoffs.
4316320	4317200	It's like an AI CEO.
4317200	4319600	That's really hard to do, right?
4319600	4322880	The history of evolution shows us how hard it is
4322880	4324480	to coordinate multicellular organisms.
4324480	4326320	You have to make them all live or die as one.
4326320	4328160	Then you get something along these lines.
4328160	4330240	Like an ant colony can coordinate like that
4330240	4333280	because if the queen doesn't reproduce all the ants,
4333280	4335280	it doesn't matter what they're having sort of genetic material.
4335280	4335600	Okay?
4336160	4338960	We are not currently set up for those humans
4338960	4341360	to not be able to reproduce unless the AI reproduces.
4342720	4345040	Do I think we eventually get to a configuration like that?
4345040	4345360	Maybe.
4346640	4350320	Where you have an AI brain is at the center of civilization
4350320	4352400	and it's coordinating all the people around it.
4352400	4355280	And every civilization that makes it
4355280	4357520	is capable of crowdfunding and operating its own AI.
4358000	4360800	That gets me to my other critique of the AI safety guys.
4360800	4362080	I mentioned that the first critique
4362080	4363920	is very theoretical rather than empirical.
4363920	4365600	And the second critique is they're Abrahamic
4365600	4368000	rather than Darmic or Sinic.
4368000	4368320	Okay?
4368960	4371600	And, you know, our background culture influences things
4371600	4373280	in ways we don't even think about.
4373280	4375520	So much of the paperclip thinking
4375520	4378880	is like a vengeful God will turn you into pillars of salt,
4378880	4381120	except it's a vengeful, you know, AI God
4381120	4382800	will turn you into paperclips.
4382800	4383040	Okay?
4383680	4386320	The polytheistic model of many gods,
4386320	4387520	as opposed to one God is,
4387520	4389440	we're all going to have our own AI gods
4389440	4390560	and there'll be war of the gods,
4391600	4393040	like Zeus and Hera and so on.
4393040	4394560	That's the closest Western version,
4394560	4396400	you know, the paganism that predated,
4396400	4397760	you know, Abrahamic religions.
4397760	4399120	But that's still there in India.
4399120	4400560	That's still how Indians think.
4400560	4401920	That's why India is sort of,
4401920	4403600	people have gotten so woke that they don't even make
4403600	4405520	large scale cultural generalizations anymore.
4405520	4409520	But it's true that India is just culturally more amenable
4409520	4412480	to decentralization, to, you know,
4412480	4415360	multiple gods rather than one God and one state.
4415360	4415600	Okay?
4416880	4418800	And then the Chinese model is yet the opposite.
4418800	4420240	Like they have like, I mean, of course,
4420240	4421920	they have their tech entrepreneurs and so on.
4421920	4423840	But they're, if India is more decentralized,
4423840	4424720	China is more centralized.
4424720	4426800	They have like one government and one leader
4426800	4428080	for the entire civilization.
4428080	4428320	Okay?
4429200	4432160	And that the biggest thing that China has done
4432160	4435520	over the last 20 or 30 years is they've taken various,
4435520	4437440	you know, U.S. things and they've made sure
4437440	4439040	that they have their own Chinese version
4439040	4439760	where they have root.
4440320	4441760	So they take U.S. social media
4441760	4444000	and they made sure they had root over Sina Weibo.
4444000	4445440	Okay?
4445440	4446960	They make sure they have their own Chinese version
4446960	4448400	of electric cars, the most Chinese version.
4448400	4451440	So the private keys, in a sense, are with G.
4451440	4454720	So that means that they also, at a minimum,
4454720	4456080	you combine these two things,
4456080	4458560	you're at a minimum going to get polytheistic AI
4458560	4460000	of the U.S. and Chinese varieties.
4460960	4463120	And then you add the Indian version on it
4463120	4464320	and you're going to get quite a few
4464320	4466080	of these different AIs around there.
4466080	4467360	And then you have War of the Gods
4467360	4470880	where maybe they are good at coordinating humans
4470880	4474000	who take instructions from them,
4474000	4475440	but they can't live without the humans
4476080	4477600	and the humans are giving input to them.
4478240	4479120	That's a series of things.
4479120	4480240	I could probably make that clearer
4480240	4482560	if I just laid it out in bullets in an essay,
4482560	4485120	but just to recap it, A, technical reasons
4485120	4488640	like chaos, turbulence, cryptography,
4488640	4491440	why AI is limited in its ability to predict time frames
4491440	4494560	and to solve equations, B, practical limits.
4494560	4497120	And AI cannot easily be a Stuxnet
4497120	4499920	because Microsoft and Google and Apple
4499920	4502560	can install software on a billion devices
4502560	4503920	and just kill it, right?
4503920	4506800	Like basically guys with torches come, all right?
4506800	4508960	It can't easily live off the land without humans
4508960	4510880	because they would need hundreds of millions
4510880	4512720	of autonomous robots out there to control,
4512720	4515040	to mine the ore and set the data centers.
4515600	4517520	It can't just hypnotize humans
4517520	4519120	like it can control drones
4519120	4520640	because of the principal agent problem
4520640	4522240	and the degree of human defection.
4522240	4523600	To make those humans do that,
4523600	4526160	you'd have to have such massive alignment
4526160	4527440	between the AI and humans
4527440	4529040	that the humans all know they'll die
4529040	4530400	if the AI dies and vies versa.
4530400	4531280	We're not there.
4531280	4532960	Maybe we'll be there in like, I don't know,
4532960	4534480	end number of years, but not for a while.
4534480	4538240	That's a total change in like how states are organized, okay?
4539200	4541440	Finally, let me just talk about the physics a little bit more.
4542720	4544320	There's a lot of stuff which is talked about
4544320	4546720	at a very sci-fi book level of,
4546720	4549040	it'll just invent nanomedicine and nanotech
4549040	4551120	and kill us all and so on and so forth.
4551120	4552960	Now look, I like Robert Freitas,
4552960	4555200	obviously Richard Feynman's a genius and so on and so forth,
4555760	4558080	but nanotech somehow hasn't been invented yet.
4560000	4562560	Meaning that there's a lot of chemists
4562560	4563760	that have worked in this area.
4565680	4568320	A lot of nanotech is like rebranded chemistry
4568320	4570400	because those are the molecular machines.
4570400	4574240	For example, DNA polymerase or ribosome,
4574240	4575280	those are molecular machines
4575280	4578000	that we can get to work at that scale, the evolved ones.
4578640	4580480	To my knowledge, and I may be wrong about this,
4580480	4582160	I haven't looked at it very, very recently,
4582800	4586640	we haven't actually been able to make artificial replicators
4586640	4588080	of the stuff that they're talking about,
4588160	4591280	which means it's possible that there's some practical difficulty
4591280	4594160	that intervened between Feynman and Freitas
4594160	4595440	and so on's calculations.
4596000	4599120	Just a sheer fact that those books have came out decades ago
4599120	4600720	and no progress has been made,
4600720	4602160	indicates that maybe there's a roadblock
4602160	4603680	that wasn't contemplated.
4603680	4606160	So you can't just click your fingers and say, boom,
4606160	4607840	nanomess, and it's sort of like clicking your fingers
4607840	4609280	and saying, boom, time travel.
4610480	4612880	Nanomess and exist, that was a good poke
4612880	4614960	that I had a while ago in a conversation like this
4614960	4617440	where the AI guy, AI safety guy on the other side was like,
4617440	4619120	well, time travel, that's too implausible.
4619120	4622320	I'm like, yeah, but you're waiting on the nanotech thing
4622320	4624000	you're thinking is like here,
4624000	4626240	and you're making so many assumptions there
4626240	4628800	that I want to actually see some more work there.
4628800	4630000	I want to actually see that nanotech
4630000	4632080	is actually more possible than you think it is.
4633040	4635360	As for, oh, we just need to mix things in a beaker
4635360	4637440	and make a virus and so on.
4637440	4638960	You know what is really, really good
4638960	4642080	at defending against novel viruses?
4642080	4644640	Like the human immune, that's something that's within envelope.
4644720	4647680	Right, like you have evolved to not die
4647680	4649120	and to fight off viruses.
4649120	4651920	Is it possible that maybe you could make some super virus?
4651920	4656000	I mean, maybe, but again, like humans are really good
4656000	4658560	and the immune system is really good at that kind of thing.
4658560	4660160	That is what we're set up to do, right,
4660160	4662560	to adapt to that billions of years of evolution
4662560	4663680	being set up for them.
4663680	4667680	Physical constraints are not really contemplated
4667680	4669200	when people talk about these super powerful
4669200	4671200	mathematical constraints, practical constraints
4671200	4672480	are not contemplated.
4672480	4673440	And I could give more,
4673440	4674480	but I think that was a lot right there.
4674480	4675680	Let me pause here.
4675680	4678240	Yeah, let me try to steal man a few things.
4678880	4682400	And then I do think it's before too long,
4682400	4686000	I want to kind of get back to the somewhat less,
4686000	4687760	you know, radically transformative scenarios
4687760	4689520	and ask a few follow up questions on that too.
4689520	4692800	But I think for starters, I would say the sort of Eliezer,
4692800	4695200	you know, he's updated his thinking over time as well.
4695200	4697360	And I would say probably doesn't get quite enough credit for it
4697360	4699680	because he's definitely on record, you know,
4699680	4702320	repeatedly saying, yeah, I was kind of expecting
4702320	4704960	more something from like the deep mind school to pop out
4704960	4709360	and be, you know, wildly overpowered very quickly.
4709360	4712960	And on the contrary, it seems like we're in more of a slow
4712960	4716160	takeoff type of scenario where, you know, we've got these,
4716160	4719040	again, like super high surface area kind of suck up
4719040	4721840	all the knowledge, gradually get better at everything.
4721840	4723120	Some surprises in there, you know,
4723120	4725200	certainly some emergent properties,
4725200	4726960	if you will accept that term, you know,
4726960	4729440	surprise surprises to the developers of nothing else,
4729440	4731440	right, that are definitely things
4731440	4732880	we don't fully understand.
4732880	4734880	But it does seem to be a, you know,
4734880	4738240	more gradual turning up of capability versus some like,
4738240	4741520	you know, super sudden surprise.
4741520	4745280	But okay, so then what is the alternative?
4745280	4747520	I'm going to try to kind of give you the,
4747520	4753920	what I think of as the most consensus strongest scenario
4753920	4756640	where humans lose track of the future
4757360	4759200	and or lose control of the future,
4759200	4761440	maybe starting by kind of losing track of the present
4761440	4763040	and then having that kind of, you know,
4763040	4764960	give way to losing control of the future.
4764960	4766320	And I think within that, by the way, the,
4767200	4770080	I'm not really one who cares that much about like,
4770080	4772400	whether AIs say something offensive today,
4773040	4775440	I'm not easily offended and like whatever.
4775440	4776960	That's not, that's not world ending.
4776960	4777680	I understand your point.
4777680	4779440	That's not like, who cares, whatever.
4779440	4781120	That's within scope, that's within envelope.
4781120	4783200	Within this bigger kind of, you know,
4783200	4787440	what is the real, you know, most likely path
4787440	4790640	to like AI disaster as understood,
4790640	4792160	I think by the smartest people today,
4792160	4794320	I think that is still a useful leading indicator
4794320	4797920	because it's like, okay, the developers,
4797920	4799440	you know, whether you agree with their politics,
4799440	4800320	whether you agree with their,
4800320	4802080	whether you think their commercial reasons are,
4802080	4803840	their sincere reasons are not,
4803840	4806800	they have made it a goal to get the AI
4806800	4808240	to not say certain things, right?
4808240	4809840	They don't want it to be offensive.
4809840	4811920	The most naive, you know, kind of down the fairway
4811920	4812880	interpretation of that is like,
4812880	4814720	Hey, they want to sell it to corporate customers.
4814720	4816560	They know that their corporate customers don't want,
4816560	4819120	you know, to have their AI saying offensive things.
4819120	4821360	So they don't want to say offensive things.
4821360	4824800	And yet they can't really control it.
4824800	4827040	It's like still pretty easy to break.
4827040	4830640	So I view that as just kind of a leading indicator of,
4830640	4834560	okay, we've seen GPT-2, 3 and 4 over the last four years.
4835680	4839280	And that's, you know, a big Delta in capability.
4839840	4844560	How much control have we seen developed in that time
4844560	4846160	and does it seem to be keeping pace?
4846720	4848800	And my answer would be on the face of it,
4848800	4850640	it seems like the answer is no.
4850640	4856000	You know, we, we don't have the ability to really dial in
4856000	4858960	the behavior such that we can say, okay, you're going to,
4858960	4862000	you know, you can expect, you can trust that these AIs
4862000	4864080	will like not do, you know, A, B, and C.
4864720	4867040	On the contrary, it's like, if you're a little clever,
4867040	4868320	you know, you can get them to do it.
4868320	4869840	You can break out of the sandbox on it.
4870400	4872000	Yeah. And it's, it's not even like, I mean,
4872000	4873920	we've talked about, you know, things where you have access
4873920	4876160	to the weights and you're doing like counter optimizations,
4876160	4877200	but you don't even need that.
4877200	4880080	You know, the kind of stuff I do in like my red teaming
4880080	4884640	in public is literally just like feed the AI a couple of words,
4884640	4886160	put a couple of words in its mouth, you know,
4886160	4887760	and it will kind of carry on from there.
4888400	4890320	So with that in mind is just the leading indicator.
4890880	4892880	You know, I don't know how powerful the most powerful
4892880	4894560	AI systems get over the next few years,
4894560	4897840	but it seems very plausible to me that it might be
4897840	4900880	as powerful as like an Elon Musk type figure, you know,
4900880	4903040	somebody who's like really good at thinking
4903040	4905920	from first principles, really smart, you know,
4905920	4909040	really dynamic across a wide range of different contexts.
4909760	4913200	And, you know, he's not powerful enough to like,
4913200	4914720	in and of himself take over the world,
4915280	4917760	but he is kind of becoming transformative.
4918480	4920400	Now imagine that you have that kind of system
4920960	4924080	and it's trivial to replicate it.
4924080	4926080	So, you know, if you have like one Elon Musk,
4926080	4928080	all of a sudden you can have arbitrary,
4928080	4929520	you know, functionally arbitrary numbers
4929520	4933440	of Elon Musk power things that are clones of each other.
4934080	4935040	Maybe I can pause you there.
4935040	4937440	So that's my polytheistic AI scenario,
4937440	4940400	but here's the thing that is, this is background,
4940400	4941760	but I want to push it to foreground.
4943040	4946000	You still have a human typing in things into that thing.
4946000	4947920	The human is doing the jailbreak, right?
4947920	4951680	What we're talking about is not artificial intelligence
4951680	4953520	in the sense of something separate from a human,
4953520	4954880	but amplified intelligence.
4956320	4958240	Amplified intelligence, I very much believe in.
4958240	4960160	The reason is amplified intelligence.
4960160	4963120	So here's something that people may not know about humans.
4963840	4966640	There's this great book, Cooking Made Us Human.
4968080	4972480	Tool use has shifted your biology in the following way.
4972480	4975840	For example, I'll map it to the present day.
4975840	4977840	This book by Richard Rang, I'm Cooking Made Us Human,
4977840	4981440	where the fact that we started cooking and using fire
4982080	4985360	meant that we could do metabolism outside the body,
4985360	4990800	which meant it freed up energy for more brain development.
4991760	4993680	Similarly, developing clothes
4993680	4995920	meant that we didn't have to evolve as much fur,
4996640	4998640	again, more energy for brain development.
4998640	5001600	Evolving tools meant we didn't have as much fangs
5001600	5003280	and claws and muscles,
5003280	5005280	again, more energy for brain development, right?
5005280	5009040	So encephalization quotient rose as tool use
5009040	5011760	meant that we didn't have to do as much natively
5011760	5014000	and we could push more to the machines.
5014000	5017760	In a very real sense, we have been a man-machine symbiosis
5017760	5021680	since the invention of fire and the stone axe and clothes.
5022640	5026080	You do not exist as a human being on your own.
5026080	5030480	The entire Ted Kojinski concept of living in nature by itself,
5030480	5033680	humans are social organisms that are adapted
5033680	5036480	to working with other humans and using tools.
5037760	5039600	And we have been for millennia.
5039600	5042000	This goes back, not just human history,
5042000	5043840	but hundreds of thousands years before,
5043840	5045120	100 gatherers are using tools.
5045840	5048720	So what that means is,
5048720	5051040	man-machine symbiosis is not some new thing.
5051600	5054640	It's actually the old thing that broke us away
5054640	5057600	from other primate lineages that weren't using tools.
5057600	5058240	Okay?
5058240	5060400	This is the fundamental difference between
5060400	5062880	what I call Uncle Ted and Uncle Fred.
5063440	5065120	Uncle Ted is Ted Kojinski.
5065120	5066720	It's a unabomber, it's a doomer,
5066720	5069280	it's a decelerator, the de-grocer who thinks
5069280	5072000	we need to go back to Gaia and Eden and become monkeys
5072000	5075040	and live in the jungle like Ted Kojinski, right?
5075680	5077360	The unabomber cell.
5077360	5080640	Uncle Fred is Friedrich Nietzsche, right?
5080640	5083440	Nietzsche and we must get to the stars
5083440	5085600	and become ubermen and so on and so forth.
5085600	5087280	This, I think, is going to become,
5087280	5088800	and I actually tweeted about this years ago
5088800	5090400	before the current AI debates,
5091600	5095920	between anarcho-primitivism, de-growth, deceleration,
5095920	5097040	okay, on the one hand,
5097760	5101360	and transhumanism and acceleration
5101920	5104240	and human 2.0 and human self-improvement
5104240	5106480	and make it to the stars, on the other hand.
5106480	5109280	This is the future political axis, the current one.
5110000	5111760	And roughly speaking, you can,
5111760	5113600	it's not really left and right
5113600	5116240	because you'll have both left status
5116240	5118640	and right conservatives go over here.
5118640	5121120	You know, left status will say it's against the state
5121120	5122240	and the right status will say,
5122240	5124800	the right conservatives say it's against God, okay?
5124800	5126560	And you'll have left libertarians
5126560	5128080	and right libertarians over here
5128080	5130080	where the left libertarians say it's my body
5130080	5133520	and the right libertarians say it's my money, right?
5134560	5137120	And so that is a re-architecting of the political axis
5137120	5139200	where Uncle Ted and Uncle Fred,
5139200	5140960	which is a kind of clever way of putting it, okay?
5142240	5144800	And the problem with the Uncle Ted guys, in my view,
5144800	5149040	is, as I said, yeah, if they go and want to live in the woods,
5149040	5150240	fine, go get them.
5150240	5152800	But once you start having even like a thousand,
5152800	5155120	forget a thousand, 100 people doing that,
5155120	5158320	your trees will very quickly get exfoliated,
5158320	5160400	the leaves are going to get all picked off of them.
5160400	5162640	Humans are not set up to just literally live
5162640	5163840	in the jungle right now.
5163840	5166160	You've had hundreds of thousands of years of evolution
5166160	5168560	that have driven you in the direction of tool use,
5168560	5171360	social organisms, farming, et cetera, et cetera.
5171360	5173600	The man machine symbiosis is not today,
5173600	5175280	it's yesterday and the day before
5175280	5178080	and 10,000 years ago and 100,000 years ago.
5178080	5180400	And how do we know we've got man machine symbiosis?
5180400	5183920	Can you live without, even if you're not living,
5183920	5185680	even if you're not using the stove,
5185680	5188480	somebody's using a stove to make you food, right?
5188480	5190160	Can you live without the tractors
5190160	5192320	that are digging up the grains?
5192320	5194880	Can you live without indoor heating?
5194880	5196560	Can you live without your clothes?
5196560	5198480	Frankly, can you do your work without your phone,
5198480	5199840	without your computer?
5199840	5200800	No, you can't.
5200800	5202880	You are already a man machine symbiosis.
5203440	5205920	Once we accept that, then the question is,
5205920	5206800	what's the next step?
5207520	5210240	And right now, we're in the middle of that next step,
5210240	5212480	which is AI is amplified intelligence.
5213120	5216720	So what you're talking about is not that the AI is Elon Musk,
5216720	5220320	it is that the AI human fusion means
5220320	5221840	there's another 20 Elon Musk's.
5222320	5223920	Or whatever the number is, okay?
5224560	5226880	And that's good.
5226880	5227600	That's fine.
5227600	5229120	That's within envelope.
5229120	5231520	That's just a bunch of smarter humans on the planet.
5231520	5233120	That is amplified intelligence.
5233120	5237200	That is more like, I mentioned the tool thing, okay?
5237200	5238960	The other analogy would be like a dog.
5238960	5241920	You know, a dog is man's best friend, right?
5241920	5244560	So that AI does not live without you.
5245200	5246960	Humans can turn it off.
5246960	5248320	They have to power it.
5248320	5250240	They have to give it subsidence, right?
5250240	5252160	Eventually, that might become like a ceremonial thing,
5252160	5255280	like this is our God that we pray to, right?
5255280	5257120	Because it's wiser and smarter than us,
5257120	5259120	and it appears in an image.
5259120	5260800	But the priests maintain it.
5260800	5262880	You know, just like you go to a Hindu temple
5262880	5263840	or something like that,
5263840	5265600	and the priests will pour out the ghee,
5265600	5267360	you know, for the fires and so on and so forth.
5267360	5269360	And then everybody comes in and prays, okay?
5269360	5271680	The priests believe in the whole thing,
5271680	5273600	but they also maintain the back of the house.
5273600	5276160	They do the system administration for the temple.
5276160	5278720	Same, you know, in a Christian church, right?
5279440	5282960	The, you know, it's not like it appears out of nowhere.
5282960	5288080	Somebody, you know, went and assembled this cathedral, right?
5288080	5289840	They saw the back of the house,
5289840	5291600	the fact that it was just woods and rocks
5291600	5292800	and so on that came together.
5292800	5293760	But then when people come there,
5293760	5295200	it feels like a spiritual experience.
5295200	5296720	You see what I'm saying, okay?
5296720	5298080	So the equivalent of that,
5298080	5301040	the priests or the people
5301040	5303520	maintaining temples, cathedrals, mosques, whatever,
5303520	5307600	is engineers who are maintaining
5307600	5310800	these future AIs, which appear to you as Jesus.
5310800	5312960	They appear to you, maybe even a hologram, okay?
5312960	5315200	You come there, you ask it for guidance as an oracle.
5315200	5317440	You've also got the personal version on your phone.
5317440	5319520	You ask it for guidance, but guess what?
5320880	5324640	You're still a human AI symbiosis until,
5324640	5327280	and unless, that AI actually has the terminator scenario
5327280	5328320	where it's got lots of robots
5328320	5329520	and it can live on its own.
5329520	5331600	I'm not saying that's physically impossible.
5331600	5333760	I did give some constraints on it earlier,
5333760	5335680	but for a while, we're not going to be there.
5335680	5337840	So that alone means it's not fume
5337840	5339520	because we don't have lots of drones running around.
5339520	5340800	The AI has to be with the human.
5340800	5342320	It's a human AI symbiosis.
5342320	5344000	It's not AI Elon Musk.
5344000	5347280	It is human AI fusion that becomes Elon Musk.
5347280	5348640	And frankly, that's not that different
5348640	5349840	from what Elon Musk himself is.
5349840	5351920	Elon Musk would not be Elon Musk without the internet.
5351920	5353040	Without the internet, you can't tweet
5353040	5354880	and reach 150 million people.
5354880	5358080	The internet itself made Elon what he is, right?
5358080	5360640	And so this is like the next version of that.
5360640	5361840	Maybe there's now 30 Elons
5361840	5364000	because the AI makes the next 30 Elons.
5364000	5366320	Yeah, I mean, again, I think I'm largely with you
5366320	5370720	with just this one very important nagging worry
5370720	5373120	that's like, what if this time is different?
5373120	5377440	Because what if these systems are getting so powerful,
5377440	5379520	so quickly that we don't really have time
5379520	5384960	for that techno human fusion to really work out?
5384960	5386880	And I'll just give you kind of a couple of data points on that.
5386880	5389040	Like, you know, you said like,
5389040	5391200	it's still somebody putting something into the AI.
5391200	5392240	Well, sort of, right?
5392240	5394960	I mean, already we have these proto agents
5394960	5397440	and the like super simple scaffolding of an agent
5397440	5400800	is just run it in a loop, give it a goal,
5400800	5404560	and have it kind of pursue some like plan, act,
5404560	5408080	get feedback and loop type of structure, right?
5408080	5409840	It doesn't seem to take a lot.
5409840	5411600	Now, they're not smart enough yet
5411600	5413920	to accomplish big things in the world,
5413920	5419040	but it seems like the language model to agent switch
5419120	5423680	is less one right now that is gated by the structure
5423680	5425680	or the architecture and more one that's just gated
5425680	5427440	by the fact that like the language models,
5427440	5430560	when framed as agents, just aren't that successful
5430560	5432880	at like doing practical things and getting over hump,
5432880	5434080	so they tend to get stuck.
5435040	5437040	But it doesn't seem that hard to imagine that like,
5437040	5439040	you know, if you had something that is sort of
5439040	5441680	that next level that you put it into a loop,
5441680	5444240	you say, okay, you're Elon Musk, LLM,
5444240	5447120	and your job is to like make, you know, us,
5447120	5451200	whatever us exactly is a, you know, multi-planetary species,
5451200	5455120	and then you just kind of keep updating your status,
5455120	5457200	keep updating your plans, keep trying stuff,
5457200	5460240	keep getting feedback, and you know,
5460240	5462880	like what really limits that?
5463760	5465360	There may be like a really good program,
5466000	5469600	but the whole AI kills everyone thing is so,
5469600	5471840	it's like, where's the actuator?
5471840	5473040	Okay, I hit enter.
5474320	5476480	What kills me, right?
5476560	5479920	Is it a hypnotized human who's being hypnotized by an AI
5479920	5483200	that he's typed into and he's radicalized himself
5483200	5484560	by typing into a computer?
5485120	5487200	Okay, that's not that different from a lot of other things
5487200	5489040	that have happened in the past, right?
5489040	5491280	So who is actually striking me, right?
5491280	5492560	Who's striking the human?
5492560	5495280	It's another human within acts that he's been radicalized
5495280	5496720	by an AI, okay?
5496720	5498560	He's not, actually, that's not even the right term.
5498560	5502080	We're giving agency to the AI when it's not really an agent.
5502080	5504080	It is a human who's self-radicalized
5504080	5505840	by typing into a computer screen.
5506560	5508800	And has hit another human.
5508800	5509760	That's one scenario.
5509760	5510720	The other scenario is,
5510720	5512880	it's literally a Skynet drone that's hitting you.
5513680	5514560	Those are the only two,
5514560	5516720	how else is it going to be physical, right?
5516720	5519360	How does the, the actuation step is a part
5519360	5521280	that is skipped over and it's a non-trivial step.
5522160	5524160	Well, I think it could be lots of things, right?
5524160	5525760	I mean, if it's not one of those two,
5525760	5528640	if it's not another human or a drone hitting you,
5528640	5529120	what is it?
5530080	5531680	Just habitat degradation, right?
5531680	5533360	I mean, how do we kill most of the other species
5533360	5534480	that we drive to extinction?
5534480	5538160	We don't go out and like hunt them down with axes one by one.
5538160	5540720	We just like change the environment more broadly
5540720	5543440	to the point where it's not suitable for them anymore
5543440	5545920	and they don't have enough space and they kind of die out, right?
5545920	5548400	Like, so we did hunt down some of the megafauna,
5548400	5551360	like literally one by one with, with spears and stuff.
5551360	5554400	But like most of the recent loss of species
5554960	5558080	is just like, we're out there just extracting resources
5558080	5559120	for our own purposes.
5559680	5561840	And in the course of doing that, you know,
5561840	5565680	whatever bird or whatever, you know, thing just kind of loses its place
5565680	5566720	and then it's no more.
5567280	5569680	And I don't think that's like totally implausible.
5570240	5574080	Wait, so that is though, I think, within normal world, right?
5574080	5574880	What does that mean?
5574880	5579680	That means that some people, some, some amplified intelligence,
5579680	5582000	and maybe we might call it HAI.
5582000	5584720	Okay, human plus AI combination, right?
5584720	5588640	Some HAIs out compete others economically
5588640	5589680	and they lose their jobs.
5589680	5590560	Is that what you're talking about?
5591040	5594480	I think also the humans potentially become unnecessary
5594480	5596000	in a lot of the configurations.
5596000	5599120	Like just a recent paper from DeepMind.
5599120	5600560	It's your marginal product workers.
5600560	5601280	Or negative.
5601280	5605680	Yeah, I mean, so the last, you know, DeepMind has been on Google,
5605680	5611920	Google DeepMind has been on a tear of increasingly impressive medical AI's.
5611920	5616800	Their most recent one takes a bunch of difficult case studies
5616800	5617440	from the literature.
5617440	5618240	I mean, case studies, you know,
5618240	5620720	this is like rare diseases, hard to diagnose stuff.
5621520	5626480	And asks an AI to do the differential diagnosis, compares that to human,
5626480	5628640	and compares it to human plus AI.
5629280	5632160	And they've phrased their results like in a very understated way.
5632160	5636400	But the headline is, the AI blows away the human plus AI.
5636400	5638000	The human makes the AI worse.
5638800	5642080	So here's the thing, I'll say something provocative maybe.
5642080	5643520	Okay, like I have in a very fine.
5644080	5648400	I do think that the ABC's of Economic Apocalypse for Blue America
5648400	5650160	are AI, Bitcoin, and China.
5650160	5653280	Where AI takes away their, a lot of the revenue streams,
5653280	5658320	the licensures that have made medical and legal costs and other things so high.
5658320	5660080	Bitcoin takes away their power over money,
5660080	5662000	and China takes away their military power.
5662000	5665760	So I've perceived total meltdown for Blue America
5666880	5671600	in the years and, you know, maybe decade to come already kind of happening.
5671600	5673360	But that's different than being at the end of the world.
5674000	5674640	Right?
5674640	5677760	Like, Blue America had a really great time for a long time,
5677760	5679520	and they've got these licensure locks.
5679520	5683520	But because of that, they've hyperinflated the cost of medicine.
5684320	5687360	It's like, how much, so what you're talking about is,
5688240	5689760	wow, we have infinite free medicine.
5690320	5693040	Man, Dr. Billing events are going to get ahead.
5693040	5693920	That's the point.
5694640	5697200	Yeah, and to be clear, I'm really with you on that too.
5697200	5701360	Like, I want to see, when people say like, what is good about AI?
5701360	5703920	You know, why should we pursue this?
5705200	5712880	My standard answer is high quality medical advice for everyone at pennies per visit, right?
5712880	5714720	It is orders of magnitude cheaper.
5714720	5717280	We're already starting to see that in some ways it's better.
5717280	5719680	People prefer it, you know, that AI is more patient.
5719680	5721040	It has better bedside manner.
5721680	5726000	I wouldn't say, you know, if I was giving my own family advice today,
5726000	5728320	I would say use both a human doctor and an AI,
5728320	5730960	but definitely use the AI as part of your mix.
5731680	5732880	Absolutely. That's right.
5732880	5734800	That's right. But you're prompting it still, right?
5734800	5737520	The smarter you are, the smarter the AI is.
5737520	5740560	You notice this immediately with your vocabulary, right?
5740560	5742560	The more sophisticated your vocabulary,
5742560	5744400	the finer the distinctions you can have,
5744400	5746320	the better your own ability to spot errors.
5747040	5749280	You can generate a basic program with it, right?
5749280	5751280	But really amplified intelligence is, I think,
5751280	5752880	a much better way of thinking about it.
5752880	5758160	Because whatever your IQ is, it surges it upward by a factor of three or whatever the number.
5758160	5760800	And maybe the amplifier increases with your intelligence.
5760800	5764240	But that internal intelligence difference still exists.
5764240	5765600	It's just like what a computer is.
5765600	5767760	A computer is an amplifier for intelligence.
5767760	5771120	If you're smart, you can hit enter and programs can go to...
5771120	5773520	Like thinking about the Minecraft guy, right?
5773520	5774320	Or Satoshi.
5774880	5779600	One person built a billion, or in Satoshi's case, a trillion dollar thing, you know?
5779600	5782560	Obviously other people continued Bitcoin and so on and so forth, right?
5783440	5785760	So what I feel, though, is this is what I mean
5785760	5789680	by going from nuclear terrorism to the TSA, okay?
5789680	5792160	We went from AI will kill everyone.
5792160	5795360	And I'm like, what's the actuator to, okay,
5795360	5796640	it'll gradually degrade our environment.
5796640	5797200	What does that mean?
5797200	5798320	Okay, some people will lose their jobs.
5798320	5799600	But then we're back in normal world.
5800240	5800800	Well, hold on.
5800800	5802480	Let me paint a little bit more complete picture,
5802480	5804160	because I don't think we're quite there yet.
5804160	5808480	So I think the differential diagnosis,
5808480	5812800	recent paper, that's just a data point where it's kind of like chess.
5812800	5814400	So this came long before, right?
5814400	5817120	There was a period where humans were the best chess players.
5817120	5819120	Then there was a period where the best were the hybrid,
5819120	5820880	human AI systems.
5820880	5823680	And now as far as I understand it, we're in a regime where
5823680	5826480	the human can't really help the AI anymore.
5826480	5830240	And so the best chess players are just pure AIs.
5830240	5833360	We're not there in medicine, but we're starting to see examples where,
5833360	5836480	hey, in a pretty defined study, differential diagnosis,
5836480	5838880	the AI is beating, not just beating the humans,
5838880	5841440	but also beating the AI human hybrid,
5841440	5843040	or the human with access to AI.
5843760	5845920	So, okay, that's not it, right?
5845920	5848880	There's a paper recently called Eureka.
5848880	5855520	Out of NVIDIA, this is Jim Fan's lab where they use GPT-4
5855520	5859520	to write the reward functions to train a robot.
5859520	5862720	So you want to train a robot to twirl a pencil in fingers.
5863920	5864560	Hard for me to do.
5865280	5867280	Robots definitely can't do it.
5867280	5868160	How do you train that?
5868160	5869520	Well, you need a reward function.
5869520	5872640	The reward function, basically while you're in the early process
5872640	5874080	of learning and failing all the time,
5874640	5877280	the reward function gives you encouragement
5877280	5878480	when you're on the right track, right?
5878960	5881600	There are people who have developed this skill,
5881600	5882720	and you might do something like,
5882720	5884880	well, if the pencil has angular momentum,
5885520	5887600	then that seems like you're on maybe the right track,
5887600	5888960	so give that a reward.
5888960	5891600	Even though, at the beginning, you're just failing all the time.
5891600	5894640	It turns out GPT-4 is way better than humans at this, right?
5894640	5896800	So it's better at training robots.
5897360	5899120	So all of that is awesome, and it's great.
5899920	5904640	But here's the thing, is there's a huge difference between AI
5904640	5907120	is going to kill everybody and turn everybody into paperclips.
5908560	5913360	Versus some humans with some AI are going to make a lot more money,
5913920	5915440	and some people are going to lose their jobs.
5916320	5917520	Yeah, I'm not scared of that.
5917520	5918480	I'm not scared of that, Snare.
5918480	5919840	I mean, it could be disruptive,
5919840	5923680	it could be disruptive, but it's not existential under itself.
5924640	5925120	Big deal.
5925120	5926240	Okay, so that's why I went, right.
5926800	5929360	There's a, the, the, to me, it comes, if I,
5929360	5931840	if I ask just one question is, what is the actuator?
5932560	5932960	Right?
5932960	5934640	You know, sensors and actuators, right?
5934640	5936240	What is the thing that's actually going to
5936960	5940080	plunge a knife or a bullet into you and kill you?
5940800	5948480	It is either a human who has hypnotized themselves by typing into a computer,
5948480	5950400	like basically an AI terrorist, you know,
5950400	5954320	which is kind of where some of the EAs are going in my view,
5954320	5960080	or it is like an autonomous drone that is controlled in a
5960080	5962240	starcraft or terminator like way.
5962880	5967600	We are not there yet in terms of having enough humanoid or autonomous drones
5967600	5969600	that are internet connected and programmable.
5969600	5971120	That won't be there for some time.
5971120	5971360	Okay.
5972000	5973840	So that alone means fast takeoff is,
5973840	5975680	and what I think by the time we get there,
5976480	5980080	you will have a cryptographic control over them.
5980080	5981440	That's a crucial thing.
5981440	5984720	Cryptography fragments the whole space in a very fundamental way.
5985280	5986720	If you don't have the private keys,
5986720	5988320	you do not have control over it.
5988320	5992720	So long as that piece of hardware, the cryptographic controller,
5992720	5994240	you've nailed the equations on that,
5994240	5996720	and frankly, you can use AI to attack that as well
5996720	5998960	to make sure the code is perfect, right?
6000000	6001840	Remember you talked about attack and defense?
6001840	6005120	AI is attack cryptos defense, right?
6005120	6008080	Because one of the things that crypto has done,
6008640	6011440	do you know the PKI problem is public key infrastructure?
6011440	6014480	I'll say no on behalf of the audience.
6014480	6014960	This is good.
6014960	6016080	We should do more of these actually.
6016080	6019200	I feel it's a good fusion of things or whatever, right?
6019200	6027360	But the public key infrastructure problem is something that was sort of,
6027360	6030640	lots of cryptography papers and computer science papers
6030640	6034560	in the 90s and 2000s assumed that this could exist
6034560	6036640	and essentially meant if you could assume
6036640	6042400	that everybody on the internet had a public key that was public
6042400	6046480	and a private key that was kept both secure and available at all times,
6046480	6048800	then there's all kinds of amazing things you can do
6048800	6051840	with privacy preserving, messaging, and authentication, and so on.
6052480	6055760	The problem is that for many years,
6055760	6059040	what cryptographers try to do is they try to nag people
6059040	6061520	into keeping their private keys secure and available.
6061520	6065040	And the issue is it's trivial to keep it secure and unavailable
6065040	6067040	where you write it down and you put it into a lockbox
6067040	6068080	and you lose the lockbox.
6068720	6071200	It's trivial to keep it available and not secure,
6071200	6074080	okay, where you put it on your public website
6074080	6077120	and it's available all the time, you never lose it,
6077120	6080320	but it's not secure because anybody can see it.
6081120	6082080	When you actually ask,
6082080	6084560	what does it mean to keep something secure and available?
6085680	6087440	That's actually a very high cost.
6087440	6091360	It's precious space because it's based on your wallet, right?
6091360	6094320	Your wallet is on your person at all times, so it's available,
6095040	6098960	but it's not available to everybody else, so it's secure.
6098960	6101760	So you actually have to touch it constantly, yes, right?
6101760	6104400	So it turns out that the crypto wallet,
6105200	6110320	by adding a literal incentive to keep your private keys secure and available,
6110320	6112640	because if they're not available, you've lost your money.
6112640	6115200	If they're not secure, you've lost your money, okay?
6115200	6118320	To have both of them, that was what solved the PKI problem.
6119520	6120960	Now we have hundreds of millions of people
6120960	6124480	with public private key pairs where the private keys are secure and available.
6124480	6127840	That means all kinds of cryptographic schemes,
6127840	6128720	zero-knowledge stuff.
6128720	6132000	There's this amazing universe of things that is happening now.
6132000	6135040	Zero-knowledge in particular has made cryptography much more programmable.
6135040	6138640	There's a whole topic which is, if you want something that's kind of,
6138640	6141200	you know, like AI was creeping for a while
6141200	6143120	and people, specialists were paying attention to it
6143120	6144640	and then just burst out on the scene.
6145280	6147440	Zero-knowledge is kind of like that for cryptography.
6147440	6150560	Thanks to the, you know, you've probably heard of zero-knowledge before.
6150560	6157440	Yeah, we did one episode with Daniel Kong on the use of zero-knowledge proofs
6157840	6163280	to basically prove without revealing like the weights
6163280	6165680	that you actually ran the model you said you were going to run
6165680	6167600	and things like that I think are super interesting.
6168240	6168880	Exactly, right?
6168880	6172560	So what kinds of stuff, why is that useful in the AI space?
6172560	6175680	Well, first is you can use it, for example,
6175680	6178880	for training on medical records while keeping them both private
6178880	6181840	but also getting the data you wanted.
6181840	6185760	For example, let's say you've got a collection of genomes, okay?
6186400	6192240	And you want to ask, okay, how many Gs were in this data set?
6192240	6193520	How many Cs?
6193520	6194080	How many A's?
6194080	6194880	How many T's?
6194880	6196960	Okay, like you just said, like that's a very simple down.
6196960	6201120	So what's the ACG T content of this, you know, the sequence data set?
6201840	6204400	You could get those numbers, you could prove they were correct
6204400	6207040	without giving any information about the individual sequences, right?
6207040	6209440	Or more specifically, you do it at one locus and you say,
6209440	6211920	how many Gs and how many Cs are at this particular locus
6211920	6213840	and you get the SNP distribution, okay?
6213840	6217920	So it's useful for what you just said,
6217920	6220240	which is like showing that you ran a particular model
6220240	6221760	without giving anything else away.
6221760	6225440	It's useful for certain kinds of data analysis.
6225440	6227520	There's a lot of overhead on compute on this right now.
6227520	6229520	So it's not something that you do trivially, okay?
6229520	6230800	But it'll probably come down with time.
6231680	6235360	But what is perhaps most interestingly useful for
6235360	6240880	is in the context of AI is coming up with things that AI can't fake.
6240880	6242880	So what we talked about earlier, right?
6242880	6247680	Like an AI can come up with all kinds of plausible sounding images,
6247680	6252480	but if it wasn't cryptographically signed by the sender,
6253680	6258960	then, you know, it should be signed by the sender and put on chain.
6258960	6262160	And then at least you know that this person or this entity
6262160	6268000	with this private key asserted that this object existed at this time
6268000	6270080	in a way that'd be extremely expensive to falsify
6270080	6271600	because it's either on the Bitcoin blockchain
6271600	6273680	or another blockchain that's very expensive to rewind, okay?
6274320	6277440	This starts to be a bunch of facts that an AI can't fake.
6278400	6283440	You know, so going back to the kind of big picture loss of control story,
6283440	6285520	I was just kind of trying to build up a few of these data points
6285520	6287840	that like, hey, look at this differential diagnosis.
6287840	6292000	We already see like humans are not really adding value to AIs anymore.
6292000	6293440	That's kind of striking.
6293440	6295760	And like similarly with training robot hands,
6295760	6298240	GPT-4 is outperforming human experts.
6298800	6302560	And by the way, all of the sort of latent spaces
6302560	6304240	are like totally bridgeable, right?
6304240	6306320	I mean, one of the most striking observations
6306320	6308320	of the last couple of years of study is that
6309120	6311680	AIs can talk to each other in high dimensional space,
6312800	6316080	which we don't really have a way of understanding natively, right?
6316080	6318240	It takes a lot of work for us to decode.
6319040	6320880	This is like the language thing?
6320880	6323200	We're starting to see AIs kind of develop
6323200	6326000	not obviously totally on their own as of now,
6326080	6327120	but we are...
6327120	6332480	There is becoming an increasingly reliable go-to set of techniques
6333040	6336880	if you want to bridge different modalities
6336880	6339680	with like a pretty small parameter adapter.
6339680	6340320	That's interesting.
6340320	6341760	Actually, what's a good paper on that?
6341760	6342800	I actually hadn't seen that.
6342800	6345680	The blip family of models out of Salesforce research
6345680	6346480	is really interesting.
6346480	6348400	And I've used that in production at...
6348400	6349440	Salesforce, really?
6349440	6350480	Yeah, Salesforce research.
6350480	6354080	They have a crack team that has open sourced a ton of stuff
6354080	6359280	in the language model, computer vision, joint space.
6360160	6361200	And this...
6361200	6362560	You see this all over the place now,
6362560	6366400	but basically what they did in a paper called blip2,
6366400	6369120	and they've had like five of these with a bunch of different techniques,
6369680	6373600	but in blip2, they took a pre-trained language model
6374320	6376080	and then a pre-trained computer vision model,
6376640	6379040	and they were able to train just a very small model
6379040	6380160	that kind of connects the two.
6380640	6384080	So you could take an image, put it into the image space,
6384960	6388960	then have their little bridge that over to language space.
6389520	6391680	And that everything else, the two big models are frozen.
6391680	6393280	So they were able to do this on just like
6393280	6395360	a couple days worth of GPU time,
6396080	6397280	which I do think goes to show
6397280	6400400	how it is going to be very difficult to contain proliferation.
6400400	6401360	It was just good.
6401360	6402880	In my view, that's really good.
6402880	6404080	As long as it doesn't get out of control,
6404080	6405920	I'm probably with you on that too.
6406880	6410080	But by bridging this vision space into the language space,
6410080	6412880	then the language model would be able to converse with you
6412880	6415520	about the image, even though the language model
6416080	6417760	was never trained on images,
6417760	6421440	but you just had this connector that kind of bridges those modalities.
6422240	6423680	It's like another layer of the network
6423680	6425200	that just bridges two networks, almost.
6425920	6427840	Yeah, it bridges the spaces.
6427840	6430240	It like it bridges the conceptual spaces
6430240	6432960	between something that has only understood images
6432960	6435040	and something that has only understood language,
6435040	6437040	but now you can kind of bring those together.
6437040	6439040	As I think about it, it's not that surprising
6439120	6444320	because that's what, for example, text image models are basically that.
6444320	6447200	They're bridging two spaces in a sense, right?
6447200	6448480	But I'll check this paper out.
6448480	6451440	So on the one hand, it's not that surprising.
6451440	6453280	On their hand, I should see how they implement it
6453280	6454480	or whatever, so blip to.
6454480	6454960	Okay.
6454960	6456960	Yeah, I think the most striking thing about that
6456960	6458800	is just how small it is.
6458800	6462640	You took these two off-the-shelf models that were trained
6463760	6465440	independently for other purposes,
6466000	6470880	and you're able to bridge them with a relatively small connector.
6471600	6474640	And that seems to be kind of happening all over the place.
6474640	6477520	I would also look at the Flamingo architecture,
6477520	6480640	which is like a year and a half ago now out of DeepMind.
6481440	6484000	That was one for me where I was like, oh my,
6484000	6486080	and it's also a language to vision
6486080	6488240	where they keep the language model frozen,
6488240	6490800	and then they kind of, in my mind,
6490800	6493040	it's like I can see the person in their garage
6493040	6494800	like tinkering with their soldering iron,
6494800	6496080	because it's just like, wow,
6496080	6498240	you took this whole language thing that was frozen,
6498240	6501280	and you kind of injected some vision stuff here,
6501280	6502320	and you added a couple layers,
6502320	6504720	and you kind of Frankensteined it, and it works.
6504720	6506160	And it's like, wow, that's not really,
6506720	6509440	it wasn't like super principled.
6509440	6511600	It was just kind of hack a few things together
6511600	6512800	and try training it.
6512800	6514400	And I don't want to diminish what they did,
6514400	6516640	because I'm sure there were more insights to it than that.
6517200	6521760	But it seems like we are kind of seeing a reliable pattern
6521840	6526640	of the key point here being model-to-model communication
6526640	6527920	through high-dimensional space,
6527920	6531120	which is not mediated by human language,
6532240	6535840	is I think one of the reasons that I would expect,
6535840	6537120	and by the way, there's lots of papers too.
6537120	6540080	I'm like, language models are human level
6540080	6541920	or even superhuman prompt engineers.
6541920	6545360	They're self-prompting techniques are getting pretty good.
6546160	6548480	So if I'm imagining the big picture of like,
6549520	6550800	and we can get back to like,
6550800	6552640	okay, well, how do we use any techniques,
6552640	6554320	crypto or otherwise, to keep this under control?
6555440	6558080	And then I would say this is kind of the newer school
6558080	6560880	of the big picture AI safety worry.
6561520	6563360	Obviously, there's a lot of flavors,
6563360	6566080	but if you were to go look at like a Jay Acotra,
6566080	6568080	for example, I think is a really good writer on this.
6569360	6571520	Her worldview is less that we're going to have this fume
6571520	6573600	and more that over a period of time,
6573600	6574880	and it may not be a long period of time.
6574880	6577040	Maybe it's like a generation, maybe it's 10 years,
6577040	6578400	maybe it's 100 years,
6578400	6579760	but obviously those are all small
6579760	6582000	in the sort of grand scheme of the future.
6583120	6585440	We have in all likelihood,
6586080	6593120	the development of AI centric schemes of production,
6593120	6595520	where you've got kind of your high level executive function
6595520	6596880	is like your language model.
6596880	6598720	You've got all these like lower level models.
6598720	6599840	They're all bridgeable.
6599840	6603200	All the spaces are bridgeable in high dimensional form,
6603200	6605360	where they're not really mediated by language,
6605360	6606560	unless we enforce that.
6606560	6610480	I mean, we could say it must always be mediated by language
6610480	6612160	so we can read the logs,
6612720	6615360	but there's a tax to that,
6615360	6618000	because going through language is like highly compressed
6618800	6620880	compared to the high dimensional space to space.
6621600	6621920	All right.
6621920	6624720	So let me see if I can steal man or articulate your case.
6624720	6627280	You're saying AIs are going to get good enough.
6627280	6628560	They're going to be able to communicate with each other
6628560	6631280	good enough, and they'll be able to do enough tasks
6631280	6633680	that more and more humans will be rendered economically
6633680	6635120	marginal and unnecessary.
6635120	6636800	I'm not saying I think that will happen.
6636800	6638480	I'm just saying I think there's a good enough chance
6638480	6639200	that that will happen,
6639200	6641200	but it's worth taking really seriously.
6641200	6642720	I actually think that will happen,
6642720	6644400	something along those lines,
6644400	6647040	in the sense of at least massive economic disruption.
6647040	6647680	Definitely.
6647680	6648400	Okay.
6648400	6650240	But I'll give an answer to that,
6650240	6653040	which is both maybe fun and not fun.
6653040	6657120	Have you seen the graph of the percentage of America
6657120	6658320	that was involved in farming?
6659040	6659280	Yeah.
6659280	6661120	I tweeted a version of that once.
6661680	6662240	Oh, you did.
6662240	6662480	Okay.
6662480	6662800	Great.
6662800	6663280	Good.
6663280	6664480	So you're familiar with this,
6664480	6667280	and you're familiar with what I mean by the implication of it,
6667280	6670480	where basically Americans used to identify themselves
6670480	6675280	as farmers, and manufacturing rose
6675280	6677040	as agriculture collapsed.
6677760	6680560	And here is the graph on that.
6680560	6683040	But from like 40% in the year 1900
6683760	6686720	to a total collapse of agriculture,
6686720	6688960	and then also more recently a collapse of manufacturing
6688960	6692080	into bureaucracy, paperwork, legal work,
6692080	6694240	what is up into the right since then
6694320	6698640	is the lawyers.
6698640	6699680	What is up into the right?
6699680	6700800	What is replacing that?
6701440	6703520	Starting in around the 1970s,
6704480	6706160	we used to be adding energy production
6706160	6707760	and energy production flatlined
6707760	6710720	once people got angry about nuclear power.
6710720	6712320	So this is a future that could have been.
6712320	6713600	We could be on Mars by now,
6713600	6714800	but we got flatlined.
6714800	6715120	Right?
6715840	6717600	What did go up into the right?
6717600	6718560	So construction costs,
6718560	6719760	this is the bad scenario,
6719760	6722160	where the miracle energy got destroyed
6722160	6725280	because regulations, the cost was flat.
6725280	6727840	And then when vertical, when regulations were imposed,
6727840	6730960	all the progress was stopped by decels and de-growthers.
6730960	6732720	And then Alara was implemented,
6732720	6736800	which said nuclear energy has to be as low risk
6736800	6739600	as reasonably necessary, as reasonably achievable.
6739600	6741840	And that meant that you just keep adding safety to it
6741840	6743440	until it's as same as cost as everything else,
6743440	6745840	which means you destroyed the value of it.
6747040	6748320	But you know what was up into the right?
6748320	6750560	What replaced those agriculture and manufacturing jobs?
6750560	6751600	Look at this, you see this graph?
6752960	6754080	We will put this on YouTube.
6754080	6757200	So if you want to see the graph do the YouTube version of this,
6757200	6758880	for the audio only group,
6758880	6761120	it's an exponential curve in the number of lawyers
6761120	6762560	in the United States from,
6762560	6764560	looks like maybe two thirds of a million
6764560	6767280	to 13 million over the last 140 years.
6767280	6771040	Yeah. And in 1880, it was like sub 100,000
6771040	6772480	or something like that, right?
6772480	6775440	And then it's just like, especially that 1970 point,
6775440	6777280	that's when it went totally vertical, okay?
6777920	6779760	And it's probably even more sensitive.
6779760	6783120	So, you know, if you add paperwork jobs, bureaucratic jobs,
6783120	6787040	you know, every lawyer is like, you know, sorry lawyers,
6787040	6788880	but you're basically negative value add, right?
6788880	6791200	Because it should, the fact that you have a lawyer
6791200	6795040	means that you couldn't just self serve a form, right?
6795040	6796400	Basic government is platformers
6796400	6798480	where you can just self serve and you fill it out.
6798480	6800240	And you don't have to have somebody
6800240	6802560	like code something for you custom, you know,
6802560	6803840	lawyers that's doing custom code
6803840	6806320	is because the legal code is so complicated.
6806320	6808800	So, you know, the whole Shakespeare thing,
6808800	6811120	like first thing we do, let's, you know, kill all the lawyers.
6811120	6813760	First thing we do, let's automate all the lawyers, right?
6813760	6817440	Only something that's the hammer blow of AI
6817440	6820320	can break the backbone and it will.
6821200	6823360	It's going to break the backbone of Blue America, right?
6823360	6825680	It's going to cause, that's why the political layer
6825680	6828960	and the sovereignty layer is not what AI people think about.
6828960	6831360	But it's like crucial for thinking about AI
6831360	6833600	because what tribes does AI benefit?
6834400	6839040	And again, we got away from, why does AI kill everybody?
6839040	6840240	Well, it's going to need actuators.
6840240	6842000	Who's going to stab you? Who's going to shoot you?
6842000	6843680	It's got to be a human hypnotized by AI
6843680	6845280	or a drone that AI controls.
6845280	6848320	A human hypnotized by AI is actually a conventional threat.
6848320	6849360	It looks like a terrorist cell.
6849360	6850880	We know how to deal with that, right?
6850880	6853120	It's just like radicalized humans that worship some AI
6853120	6853920	that stab you.
6853920	6856560	It's like the pause AI people are one step, I think,
6856560	6857760	away from that, all right?
6857760	6859360	But that's just like Aum Shinriko.
6859360	6860320	That's like allocated.
6860320	6862320	That's like basically terrorists
6862400	6865120	who think that the AI is telling them what to do, fine?
6865120	6869600	If it's not a human that's stabbing you, it is a drone.
6869600	6873520	And that's like a very different future where like five or 10
6873520	6875200	or 15 years up, maybe we have enough
6875200	6876400	internet connected drones out there,
6876400	6878160	but even then they'll have private keys.
6878160	6881360	So there's going to be fragmentation of address space.
6881360	6884560	Not all drones be controlled by everybody in my view, okay?
6884560	6886000	That's what AI safety is.
6886000	6888240	AI safety is can you turn it off?
6888240	6889120	Can you kill it?
6889120	6891760	Can you stop it from controlling drones?
6891760	6893200	That's what AI safety is.
6893200	6894960	Can you also open the model weights
6894960	6896480	so you can generate adversarial inputs?
6897440	6899840	Can you open the model weights and proliferate it?
6899840	6901040	You're saying, oh, proliferation is bad.
6901040	6904720	I'm saying proliferation is good because if everybody has one,
6904720	6907360	then nobody has an advantage on it, right?
6907360	6909440	Not relatively speaking, okay?
6909440	6911920	I have very few super confident positions.
6911920	6917040	So I wouldn't necessarily say I think that proliferation is bad.
6917040	6918640	I'd say so far it's good.
6919360	6921840	It has, and even most of the AI safety people,
6923120	6926960	I would say if I could speak on the behalf of the AI safety
6927920	6931040	consensus, I would say most people would say
6931040	6937040	even that the Llama 2 release has proven good for AI safety
6937040	6938080	for the reasons that you're saying.
6938080	6939760	But they opposed it.
6939760	6941040	Well, some didn't, some didn't.
6941040	6944880	I would say the main posture that I see AI safety people taking
6944880	6949280	is that we're getting really close to,
6949280	6950800	or we might be getting really close.
6951760	6954880	Certainly if we just kind of naively extrapolate out recent progress,
6954880	6958400	it would seem that we're getting really close to systems
6958400	6963360	that are sufficiently powerful that it's very hard to predict
6963360	6965680	what happens if they proliferate.
6965680	6966800	Llama 2, not there.
6967600	6972240	And so, yes, it has enabled a lot of interpretability work.
6972240	6974640	It has enabled things like representation engineering,
6975280	6977920	which there isn't a lot of good stuff that has come from it.
6977920	6980000	The big thing that I want to kind of establish is
6981120	6983440	you agree with me on the actuation point or not.
6984080	6986160	Like, the thing is this thing, like,
6986960	6989600	oh Llama 2 proliferates and so businesses are disrupted
6989600	6992400	and people, you know, maybe they paid a lot of money
6992400	6995200	for their MD degree and they can't make us a bunch of money.
6995200	6998320	That's within the realm of what I call conventional warfare.
6998320	6999120	You know what I mean?
6999120	7001600	That's like we're still in normal world as we were talking about.
7002560	7005200	Unconventional warfare is, you know,
7005200	7007040	Skynet arises and kills everybody.
7007040	7007360	Okay.
7008000	7009840	And that is what is being sold over here.
7010800	7012720	And when you think about the actuators,
7012720	7014160	we don't have the drones out there.
7014160	7016320	We don't have the humanoid robots at control.
7016320	7019200	And hypnotized humans are a very tiny subset of humans,
7019200	7020000	probably.
7020000	7022320	And if they aren't, that just looks like a religion
7022320	7024160	or a cult or a terrorist cell.
7024160	7025760	And we know how to deal with that as well.
7025760	7028480	The super intelligent AI with, you know,
7028480	7031840	lots of robots that control in a Starcraft form,
7031840	7034640	I would agree is something that humans haven't faced yet.
7034640	7037760	But by the time we get that many robots out there,
7037760	7039520	you won't be able to control all of them at once
7039520	7041120	because of the private key things I mentioned.
7041920	7044080	So that's why I'm like, okay,
7044080	7045920	everything else we're talking about is in normal world.
7045920	7048480	That is the single biggest thing that I wanted to get.
7048480	7051840	Like economic disruption, people losing jobs,
7051840	7054880	proliferation so that the balance of power is redistributed.
7054880	7055920	All that's fine.
7055920	7058080	The other reason I say this is people keep trying
7058080	7060080	to link AI to existential risk.
7060080	7062480	A great example is one of the things you actually had in here.
7062480	7064800	This is similar to the AI policy and two things.
7064800	7065840	It's a totally reasonable question,
7065840	7068160	but then I'm going to, in my view, deconstruct the question.
7068800	7070240	What would you think about putting the limit on the right
7070240	7072560	to compute or their capabilities an AI system might demonstrate
7072560	7074720	that you make you think open access no longer wise?
7074720	7076880	Most common near term answer here to be seems to be related
7076880	7079440	to risk of pandemic via novel pathogen engineering.
7079440	7080240	So guess what?
7080240	7082720	You know who the novel pathogen engineers are?
7082720	7085200	The US and Chinese governments, right?
7085200	7087360	They did it or probably did it,
7087360	7089680	credibly did it, credibly being accused of doing it.
7090240	7091760	They haven't been punished for COVID-19.
7091760	7093360	In fact, they covered up their culpability
7093360	7095520	and pointed everywhere other than themselves.
7095520	7098800	They used it to gain more power in both the US and China
7098800	7101440	with both lockdown in China and in the US
7101440	7102960	and all kinds of COVID era.
7103520	7106720	Trillions of dollars was printed and spent and so on and so forth.
7106720	7109440	They did everything other than actually solve the problem.
7109440	7113440	That was actually getting the vaccines in the private sector.
7113440	7116160	And they studied the existential risk only to generate it.
7116160	7118800	And they're even paid to generate pandemic prevention and failed.
7119360	7122160	So this would be the ultimate Fox guarding the hen house.
7122800	7125760	Okay, the only reason that the two organizations responsible
7125760	7127680	for killing millions of people novel pathogen
7127680	7133040	are going to prevent people from doing this by restricting compute.
7133040	7135760	No, you know what it is actually what's happening here is
7137040	7139040	one of the concepts I have in the network state
7139040	7141760	is this idea of God, state and network.
7141760	7144480	Okay, meaning what do you think is the most powerful force in the world?
7144480	7145520	Is it almighty God?
7146080	7149440	Is it the US government or is it encryption?
7150240	7152400	Right, or eventually maybe an AGI, right?
7153040	7158240	If what's happening here is a lot of people are implicitly
7159120	7161840	without realizing it, even if they are secular atheists,
7161840	7164160	they're treating GOV as GOD.
7164160	7167680	Okay, they treat the US government as God as the final mover.
7168400	7171280	No, I appreciate your little I take inspiration from you actually
7171280	7177680	in terms of trying to come up with these little quips that are memorable.
7177680	7181440	So I was just smiling at that because I think you do a great job of that.
7181440	7186560	And I try to encourage, I have less success coining terms than you have,
7186560	7190240	but certainly try to follow your example on that front.
7190240	7193200	It's like a helpful, if you can compress it down,
7193200	7194080	it's like more memorable.
7194080	7195280	So that's what I try to do, right?
7195280	7198400	So exactly a lot of these people who are secular,
7198400	7199840	think of themselves as atheists,
7199840	7202560	have just replaced GOD with GOV.
7202560	7204480	They worship the US government as God.
7204480	7205600	And there's two versions of this.
7205600	7208400	You know how like God has put the male and female version, right?
7208400	7213840	The female version is the Democrat God within the USA that has infinite money
7213840	7216000	and can take care of everybody and care for everybody.
7216000	7219280	And the Republican God is the US military that can blow up anybody,
7219280	7222320	and it's the biggest and strongest and most powerful America F. Yeah.
7222320	7223200	Okay.
7223200	7230400	And everybody who thinks of the US government as being able to stop something
7230400	7232000	is praying to a dead God.
7232880	7233360	Okay.
7233360	7234400	When you say this,
7234400	7237600	you actually get an interesting reaction from AI safety people
7237600	7240080	where you've actually hit their true solar plexus.
7241680	7242080	All right.
7242080	7245280	The true solar plexus is not that they believe in AI.
7245280	7246960	It's that they believe in the US government.
7248800	7250400	That's a true solar plexus
7250400	7252000	because they are appealing to,
7252000	7253520	they're praying to this dead God
7253520	7257760	that can't even clean the poop off the streets in San Francisco, right?
7257760	7260720	That is losing wars or fighting them to sell me.
7260720	7263520	It has lost all these wars around the world
7263520	7264800	that spent trillions of dollars
7264800	7266800	has been through financial crisis, coronavirus,
7266800	7270080	Iraq war, you know, total meltdown politically.
7270080	7270640	Okay.
7270640	7274320	That is now has interest payments more than the defense budget
7274880	7276160	that is, you know,
7276160	7278720	that spent $100 billion on the California train
7278720	7280080	without laying a single track.
7280800	7283440	It's like that, you know, that Morgan Freeman thing for,
7283440	7284960	you know, the clip from Batman,
7284960	7288720	where he's like, so this man has a billionaire,
7288720	7289600	blah, blah, blah, this and that,
7289600	7292080	and your plan is to threaten him, right?
7292080	7294560	And so you're going to create this super intelligence
7294560	7296720	and have Kamala Harris regulate it.
7296720	7299120	Come on, man, so to speak, right?
7299120	7303520	Like these people are praying to a blind, deaf and dumb God
7303520	7307760	that was powerful in 1945, right?
7307760	7310640	That's why, by the way, all the popular movies,
7310640	7311120	what are they?
7311120	7313680	It's Barbie, it's Oppenheimer, right?
7313680	7315680	It's, it's Top Gun.
7315680	7318640	They're all throwbacks the 80s or the 50s
7318640	7321040	when the USA was really big and strong.
7321760	7323520	And the future is a black mirror.
7323520	7324640	Yeah, I think that's tragic.
7325600	7326880	One of the projects that I do like,
7326880	7327840	and you might appreciate this,
7327840	7328560	I don't know if you've seen it,
7328560	7332560	is the, from the future of Life Institute,
7333120	7336240	a project called Imagine a World,
7336240	7337520	I think is the name of it.
7338160	7341200	And they basically challenged, you know,
7341200	7345200	their audience and the public to come up with
7345920	7349120	positive visions of a future,
7349120	7351280	you know, where technology changes a lot.
7351280	7353440	And obviously AI pretty central to a lot of those stories.
7354400	7357840	And, you know, one of the challenges that people go through
7357840	7359520	and how do we get there and whatever,
7359520	7364960	but a purposeful effort to imagine positive futures.
7365840	7367760	Super under provided.
7367760	7370640	And I really liked the,
7370640	7371920	the investment that they made in that.
7372480	7374000	You know, one of the things I've got
7374000	7376320	in the Never See It book is there's certain megatrends
7376320	7377840	that are happening, right?
7377840	7380240	And megatrends, I mean, it's possible for,
7380960	7383760	like one miraculous human maybe to reverse them, okay?
7384560	7387520	Because I think both the impersonal force of history theory
7387520	7389440	and the great man theory of history have some truth to them.
7390640	7394080	But the megatrends are the decline of Washington DC
7394960	7395920	the rise of the internet,
7395920	7397280	the rise of India, the rise of China.
7398160	7399840	That is like my worldview.
7399840	7403680	And I can give a thousand graphs and charts and so on for that.
7403680	7405280	But that's basically the last 30 years.
7406000	7407840	And maybe the next X, right?
7407840	7409360	I'm not saying there can't be trend reversal.
7409360	7410400	Of course it can be trend reversal,
7410400	7412480	as I just mentioned, some hammer blow could hit it,
7412480	7413360	but that's what's happening.
7413920	7415280	And so because of that,
7415280	7417520	the people who are optimistic about the future
7417520	7419680	are aligned with either the internet, India or China.
7420320	7422880	And the people who are not optimistic about the future
7422880	7426480	are blue Americans or left out red Americans, okay?
7426480	7430720	Or Westerners in general who are not tech people, okay?
7430720	7433120	If they're not tech people, they're not up into the right,
7433680	7435200	basically, because the internet's,
7435200	7437920	if you, I mean, one of the things is we have a misnomer,
7437920	7439280	as I was saying earlier,
7439280	7440720	of calling it the United States,
7440720	7442400	because it's the dis-United States.
7442400	7443920	It's like talking about,
7443920	7445680	you know, talking about America is like talking about Korea.
7445680	7446880	There's North Korea and South Korea,
7446880	7448720	and they're totally different populations.
7448720	7450880	And, you know, communism and capitalism
7450880	7452400	are totally different systems.
7452400	7454560	And the thing that is good for one
7454560	7456480	is bad for another and vice versa.
7456480	7458080	And so like America doesn't exist.
7458080	7459600	There's only, just like there's no Korea,
7459600	7461040	there's only North Korea and South Korea,
7461040	7462160	there's no America.
7462160	7463920	There is blue America and red America
7463920	7465840	and also gray America, tech America.
7465840	7469120	And blue America is harmed,
7469120	7470160	or they think they're harmed,
7470160	7472720	or they've gotten themselves into a spot where they're harmed,
7472720	7474480	by every technological development,
7474480	7477120	which is why they hate it so much, right?
7477120	7478800	AI versus journalist jobs,
7478800	7481440	crypto takes away banking jobs, you know,
7481440	7482960	everything, you know, self-driving cars,
7482960	7485520	they just take away regulator control, right?
7485520	7488240	Anything that reduces their power, they hate,
7488240	7490560	and they're just trying to freeze an amber with regulations.
7490560	7492800	Red America got crushed a long time ago
7492800	7494720	by offshoring to China and so on.
7494720	7496880	They're making, you know, inroads ally
7496880	7498800	with tech America or gray America.
7498800	7501360	Tech America is like the one piece of America
7501360	7503760	that's actually still functional and globally competitive.
7503760	7506240	And people always do this fallacy of aggregation,
7506240	7508000	where they talk about the USA,
7508000	7510720	and it's really this component that's up and to the right,
7510720	7512640	and the others that are down and to the right,
7512640	7515520	or at best flat, like red, but they're like down, right?
7515520	7517760	Like red is like okay functional, blue is down.
7518880	7521440	Point is, tech America, I think we're gonna find,
7521440	7527440	is not even truly, or how American is tech America,
7527440	7530160	because it's like 50% immigrants, right?
7530160	7531680	And like a lot of children immigrants,
7531680	7533840	and most of their customers are overseas,
7533840	7536720	and their users are overseas,
7536720	7540320	and their vantage point is global, right?
7540320	7541920	And they're basically not,
7543040	7545680	I know we're in this ultra-nationalist kick right now,
7545680	7546960	and I know that there's gonna be,
7547840	7549840	there's a degree of a fork here,
7549840	7554160	where you fork technology into Silicon Valley
7554160	7556320	and the internet, okay?
7556320	7558880	Where Silicon Valley is American,
7558880	7561040	and they'll be making like American military equipment,
7561040	7563040	and so on and so forth, and they're signaling USA,
7563040	7564640	which is fine, okay?
7564640	7568560	And then the internet is international global capitalism,
7568560	7573040	and the difference is Silicon Valley, or let's say US tech,
7573040	7576480	let me be less, you know, US tech says ban TikTok,
7576480	7577680	build military equipment, et cetera,
7577680	7579280	it's really identifying itself as American,
7579840	7582320	and it's thinking of being anti-China, okay?
7582320	7584640	But there's, US and China are only 20% of the world,
7584640	7586720	80% of the world is neither American nor Chinese.
7587280	7590240	So the internet is for everybody else
7590240	7594080	who wants actual global rule of law, right?
7594080	7596240	When as the US decays as a rules-based order,
7596240	7597920	and people don't wanna be under China,
7598000	7600960	people wanna be under something like blockchains,
7600960	7603520	where you've got like property rights contract law
7603520	7606960	across borders that are enforced by an impartial authority, okay?
7606960	7609280	That's also the kind of laws that can bind AIs,
7609280	7610480	like AIs across borders,
7610480	7612160	if you wanna make sure they're gonna do something,
7612160	7615280	cryptography can bind an AI in such a way that it can't fake it.
7615280	7617360	It can't, an AI can't mint more Bitcoin, you know?
7618160	7619360	Here's my last question for you.
7620000	7624320	AI discourse right now does seem to be polarizing into camps.
7624320	7626640	Obviously a big way that you think about the world
7626640	7629360	is by trying to figure out, you know,
7629360	7630320	what are the different camps?
7630320	7631600	How do they relate to each other?
7631600	7632400	So on and so forth.
7633600	7638000	I have the view that AI is so weird,
7638560	7641360	and so unlike other things that we've encountered in the past,
7641360	7643360	including just like, unlike humans, right?
7643360	7645440	I always say AI, alien intelligence,
7646080	7650160	that I feel like it's really important to borrow a phrase
7650160	7652640	from Paul Graham, keep our identities small,
7653440	7656160	and try to have a scout mindset
7656880	7660640	to really just take things on their own terms, right?
7660640	7662480	And not necessarily put them through a prism
7662480	7663760	of like, whose team am I on?
7663760	7666640	Or, you know, is this benefit my team
7666640	7667920	or hurt the other team or whatever?
7668800	7672880	But, you know, just try to be as kind of directly engaged
7672880	7675040	with the things themselves as we can
7675040	7677360	without mediating it through all these lenses.
7677360	7678800	You know, I think about, you mentioned like,
7679600	7681440	the gain of function, right?
7681440	7684240	And I don't know for sure what happened,
7684240	7686400	but it certainly does seem like there's a
7686400	7688640	very significant chance that it was a lab leak.
7688640	7690560	Certainly there's a long history of lab leaks,
7691200	7693120	but it would be like, you know,
7693120	7695760	it would seem to me a failure to say,
7695760	7698800	okay, well, what's the opposite
7698800	7700880	of just having like a couple of government labs?
7700880	7703200	Like everybody gets their own gain of function lab, right?
7703200	7704640	Like if we could, and this is kind of what we're doing
7704640	7705680	with AI, we're like,
7705680	7708160	let's compress this power down to as small as we can.
7708160	7710160	Let's make a kit that can run in everybody's home.
7711040	7714960	Would we want to send out these like gain of function,
7714960	7718800	you know, wet lab research kits to like every home in the world
7718800	7721360	and be like, hope you find something interesting,
7721360	7725280	you know, like let us know if you find any new pathogens
7725280	7726880	or hey, maybe you'll find life-saving drugs,
7726880	7728720	like whatever, we'll see what you find,
7728720	7730160	you know, all eight billion of you.
7730800	7734160	That to me seems like it would be definitely a big misstep.
7734160	7737280	And that's the kind of thing that I see coming out of
7738640	7742400	ideologically motivated reasoning or like,
7742400	7743680	you know, tribal reasoning.
7743680	7746560	And so I guess I wonder how you think about the role
7747120	7750960	that tribalism and ideology is playing
7750960	7754400	and should or shouldn't play as we try to understand AI.
7755040	7757760	Okay, so first is you're absolutely right
7757760	7762960	that just because A is bad does not mean that B is good, right?
7762960	7766320	So A could be a bad option, B could be a bad option,
7766320	7767280	C could be a bad option.
7767840	7770320	There might be, you have to go down to option G
7770320	7771520	before you find a good option.
7771520	7773600	Or there might be three good options and seven bad options,
7773600	7774800	for example, right?
7774800	7777600	So to map that here, in my view,
7777600	7780480	an extremely bad option is to ask the U.S.
7780480	7782560	and Chinese governments to do something.
7783200	7786640	Anything the U.S. government does at the federal level,
7786640	7789120	at the state level, in blue states, at the city level,
7789680	7790640	has been a failure.
7791200	7793440	And the way, here's a metaway of thinking about it.
7793440	7794880	You invest in companies, right?
7794880	7797600	So as an investor, here's a really important thing.
7798400	7800080	You might have 10 people who come to you
7800080	7802000	with the same words in their pitch.
7802000	7804080	They're all, for example, building social networks.
7804800	7806400	But one of them is Facebook,
7806400	7809600	and the others are Friendster and whatever, okay?
7809600	7810960	And no offense to Friendster, you know,
7810960	7813840	these guys were like, you know, pioneers in their own way,
7814400	7816640	but they just got outmatched by Facebook.
7816640	7818480	So the point is that the words were the same
7819040	7820560	on each of these packages,
7820560	7822240	but the execution was completely different.
7822960	7826880	So could I imagine a highly competent government
7826880	7831200	that could execute and that actually did, you know,
7832000	7834800	like, you know, make the right balance of things and so on?
7834800	7836400	I can't say it's impossible,
7836400	7839040	but I can say that it wouldn't be this government.
7840640	7844000	Okay, and so you are talking about the words,
7844000	7845680	and I'm talking about the substance.
7845680	7848640	The words are, we will protect you from AI, right?
7848640	7849760	In my view, the substances,
7849760	7851920	they aren't protecting you from anything, right?
7851920	7853440	You're basically giving money and power
7853440	7856480	to a completely incompetent and, in fact, malicious organization,
7856480	7859040	which is Washington DC, which is the U.S. government,
7859040	7861600	that has basically over the last 30 years
7861600	7865360	gone from a hyperpower that wins everywhere without fighting
7865360	7867680	to a declining power that fights everywhere without winning.
7868960	7872320	Okay, like just literally burn trillions of dollars doing this,
7872320	7874480	take maybe the greatest decline in fortunes
7874480	7876400	in 30 years and maybe human history.
7876400	7878400	Not even the Roman Empire went down this fast
7878400	7881440	on this many power dimensions this quickly, right?
7881440	7885040	So giving that guy, let's trust him.
7885040	7887840	That's just people running an old script in their heads
7887840	7888880	that they inherited.
7888880	7891200	They are not thinking about it from first principles that
7891200	7894400	this state is a failure, okay?
7894400	7895600	And like how much of a failure it is,
7895600	7896960	you have to look at the sovereign debt crisis,
7896960	7899360	you have to look at graphs that other people aren't looking at,
7899360	7904320	but like, you know, the domain of what Blue America can regulate
7904320	7908720	is already collapsing because it can't regulate Russia anymore.
7908720	7910240	It can't regulate China anymore.
7910240	7912160	It's less able to regulate India.
7912160	7914720	It's less able even to regulate Florida and Texas.
7914720	7916720	States are breaking away from it domestically.
7916720	7918000	So this gets to your other point.
7918000	7922640	Why is the tribal lens not something that we can put in the back,
7922640	7924800	we have to put in the absolute front?
7924800	7926880	Because the world is retribalizing.
7927680	7931760	Like basically your tribe determines what law you're bound by.
7931760	7935440	If you think you can pass some policy that binds the whole world,
7935440	7938480	well, there have to be guys with guns who enforce that policy.
7938480	7940880	And if I have guys with guns on their side that say,
7940880	7943280	we're not enforcing that policy, then you have no policy.
7943280	7944880	You've only bound your own people.
7944880	7946240	Does that make sense, right?
7946240	7951440	And so Blue America will probably succeed in choking the life out of AI
7951440	7952800	within Blue America.
7952800	7955440	But Blue America controls less and less of the world.
7956080	7957920	So it'll have more power over fewer people.
7958880	7961440	I can go into why this is, but essentially, you know,
7962160	7964400	a financial Berlin Wall is arising.
7964400	7966960	There's a lot of taxation and regulation
7967520	7971120	and effectively financial repression de facto confiscation
7971120	7973680	that will have to happen for the level of debt service
7973680	7975120	that the US is being taking on.
7975120	7978160	OK, just there's one graph just to make the point.
7978160	7981200	And if you want to dig into this, you can.
7982000	7986000	But the reason this impacts things is when you're talking about AI safety,
7986000	7988080	you're talking about AI regulation.
7988080	7990640	You're talking about the US government, right?
7990640	7992960	And you have to ask, what does that actually mean?
7993680	7998080	And it's like, in my view, it's like asking the Soviet Union in 1989
7998080	8000000	to regulate the internet, right?
8000000	8002560	That's going to outlive, you know, the country.
8002560	8005280	US interest payment on federal debt versus defense spending.
8005280	8006960	The white line is defense spending.
8006960	8007840	Look at the red line.
8007840	8009200	That's just gone absolutely vertical.
8009200	8010320	That's interest.
8010320	8013440	And it's going to go more vertical next year
8013440	8015840	because all of this debt is getting refinanced
8016720	8017840	at much higher interest rates.
8017840	8019600	That's why look at this.
8019600	8021920	You want you want AI timelines, right?
8021920	8023680	The question for me is DC's timeline.
8024240	8025920	What is DC's time left to live?
8026560	8029920	OK, this is the kind of thing that kills empires
8030000	8033440	and you either have this just go to the absolute moon
8034000	8036480	or they cut rates and they print a lot.
8036480	8039920	And either way, you know, the fundamental assumption
8039920	8043040	underpinning all the AI safety, all the AI regulation work
8043040	8046720	is that they have a functional golem in Washington DC
8046720	8048240	where if they convince it to do something,
8048240	8051360	it has enough power to control enough of the world.
8051360	8052720	When that assumption is broken,
8054000	8058000	then a lot of assumptions are broken, right?
8058000	8061600	And so in my view, you have to you must think
8061600	8063280	about a polytheistic AI world
8063920	8066640	because other tribes are already into this.
8066640	8069120	They're already funding their own, right?
8069120	8070800	The proliferation is already happening
8071520	8073120	and they're not going to bow to blue tribes.
8073120	8076960	So that's why I think the tribal lens is not secondary.
8076960	8080080	It's not some, you know, totally separate thing.
8080080	8082800	It is an absolutely primary way in which to look at this.
8082800	8084880	And in a sense, it's almost like a, you know,
8084880	8086800	in a well done movie.
8087520	8090640	All the plot lines come together at the end.
8091760	8092560	Okay.
8092560	8094560	And all the disruptions that are happening,
8094560	8097200	the China disruption, the rise of India,
8097200	8100240	the rise of the internet, the rise of crypto,
8100240	8102880	the rise of AI and the decline of DC
8102880	8104560	and the internal political conflict
8105120	8106640	and, you know, various other theaters
8106640	8110240	like what's happening in Europe and, you know, and Middle East.
8110240	8113360	All of those come together into a crescendo of,
8113360	8116240	oh, there's a lot of those graphs are all having the same time.
8116960	8119520	And it's not something you can analyze by just, I think,
8119520	8121440	looking at one of these curves on its own.
8121440	8122960	I think that's a great note to wrap on.
8122960	8126080	I am always lamenting the fact that so many people
8126080	8129040	are thinking about this AI moment
8129040	8132400	in just fundamentally too small of terms.
8132960	8137120	But I don't think you're one that will easily be accused of that.
8137120	8140560	So with an invitation to come back
8140560	8142640	and continue in the not too distant future,
8142640	8145680	for now, I will say apology, Srinivasan.
8145680	8147840	Thank you for being part of the Cognitive Revolution.
8148960	8150640	Thank you, Nathan. Good to be here.
8150640	8152640	It is both energizing and enlightening
8152640	8153920	to hear why people listen
8153920	8156320	and learn what they value about the show.
8156320	8161760	So please don't hesitate to reach out via email at TCR at turpentine.co
8161760	8165040	or you can DM me on the social media platform of your choice.
8166080	8168480	Omnike uses generative AI
8168480	8171360	to enable you to launch hundreds of thousands of ad iterations
8171360	8174880	that actually work customized across all platforms
8174880	8176160	with a click of a button.
8176160	8178720	I believe in Omnike so much that I invested in it
8178720	8180320	and I recommend you use it too.
8181040	8187280	Use COGRAV to get a 10% discount.
