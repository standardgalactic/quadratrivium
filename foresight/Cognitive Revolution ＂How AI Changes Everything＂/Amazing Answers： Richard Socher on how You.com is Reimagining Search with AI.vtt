WEBVTT

00:00.000 --> 00:05.920
early 2022, or like we had apps that would write code for you within the search results,

00:05.920 --> 00:10.240
through the apps that write essays for you within the search results. But whenever we

00:10.240 --> 00:15.600
innovated and changed the default Google experience too much, we had just like the vast

00:15.600 --> 00:20.240
majority of our users say, I'm so used to Google, I don't want another way of finding answers.

00:20.800 --> 00:26.160
And so we kept getting pulled back to this need. And so the most amazing surprise was when

00:26.240 --> 00:30.800
ChachiP came out, all of a sudden people got it. And it was like, wait a minute,

00:30.800 --> 00:36.000
it could just be like pure text. And we're like, been trying to sort of slowly get there, but

00:36.000 --> 00:40.240
we had to make a bigger job. The way I think about the different modes is like the default

00:40.240 --> 00:44.560
smart mode is kind of like, if you had an assistant, and you just ask them to do a quick

00:44.560 --> 00:49.760
a search, and in like two or three minutes, give you an answer that. And then genius mode,

00:49.760 --> 00:54.480
you go and so you want to ask your assistant for a question that they have to be able to program,

00:54.560 --> 00:58.960
they have to search the web, and then they need to be mathematically applying to answer that

00:58.960 --> 01:03.840
question. I mean, as a kid, I also enjoyed watching Terminator. It's like a cool action movie,

01:03.840 --> 01:10.000
but it's just taken over so much of the AI narrative. And it's actually like actively

01:10.000 --> 01:14.720
hurting, especially European Union. Hello, and welcome to the Cognitive Revolution,

01:14.720 --> 01:19.120
where we interview visionary researchers, entrepreneurs and builders working on the

01:19.120 --> 01:24.400
frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas,

01:24.400 --> 01:28.240
and together we'll build a picture of how AI technology will transform work,

01:28.800 --> 01:34.720
life, and society in the coming years. I'm Nathan LaBenz, joined by my cohost, Eric Tornberg.

01:35.280 --> 01:39.920
Hello, and welcome back to the Cognitive Revolution. Today, I am thrilled to welcome

01:39.920 --> 01:45.040
Richard Socher, a pioneer of deep learning for natural language processing, formerly chief

01:45.040 --> 01:50.960
scientist at Salesforce, and today, founder and CEO of u.com, a company that was first

01:50.960 --> 01:56.000
introduced to the public as a new kind of search engine, but which now describes itself as an AI

01:56.000 --> 02:03.760
assistant that makes you more productive, creative, and extraordinary. Richard has deep history in

02:03.760 --> 02:08.800
deep learning. He was among the very first to recognize the potential of neural networks

02:08.800 --> 02:14.240
in the natural language processing domain, and his work has helped shape the field as we know it

02:14.240 --> 02:19.760
over the last decade. In this conversation, Richard takes us on a brief journey through his own

02:19.760 --> 02:25.040
intellectual history and reflects on how the field of AI has evolved in both expected and

02:25.040 --> 02:31.200
surprising ways. Before we dive deep into the u.com product itself, covering the historical

02:31.200 --> 02:35.440
challenge that they faced when trying to compete with Google and how the rise of the AI chatbot

02:35.440 --> 02:41.520
paradigm has broadened the space of possibility for search and discovery products. We also look

02:41.520 --> 02:47.120
at u.com's various modes with particular emphasis on the genius mode and above all, for me, the

02:47.120 --> 02:52.720
research mode, which delivers amazingly helpful and thorough report style answers, even on some

02:52.720 --> 02:58.960
remarkably complex topics. We also briefly discussed the future of AI business models as well,

02:58.960 --> 03:04.320
including the obvious subscription and my pet theory about the AI bundle. Along the way, we touch

03:04.320 --> 03:09.280
on a number of important topics, too. The limits to AI systems' reasoning ability and the prospects

03:09.280 --> 03:14.400
for the improvement that would be needed for reliable autonomy, the potential for AI to transform

03:14.400 --> 03:20.160
medicine and scientific research, Richard's case for general optimism, even though he does expect

03:20.160 --> 03:25.600
AI to drive major disruption, why he's not worried about so-called emergent capabilities,

03:25.600 --> 03:30.960
but does take the risk of intentional harmful misuse very seriously, and lots more little

03:30.960 --> 03:36.960
topics along the way as well. Richard is a leading thinker in the AI space, and his perspective

03:36.960 --> 03:41.600
is essential for anyone who wants to understand where this technology is going and what it means

03:41.600 --> 03:48.000
for the future of humanity. And in all seriousness, I really do recommend u.com. It has absolutely

03:48.000 --> 03:53.200
joined the ranks of the AI tools that I use multiple times each week. And particularly,

03:53.200 --> 04:00.000
when I want a comprehensive, multi-page report style answer, I find that u.com's research mode

04:00.000 --> 04:06.160
is often the single best tool available today. As always, if you're finding value in the show,

04:06.160 --> 04:09.680
we would appreciate it if you'd share it with friends or post a review to Apple podcasts

04:09.680 --> 04:14.960
or Spotify, or just leave a comment on YouTube. Now, without further ado, I hope you enjoyed

04:14.960 --> 04:20.160
this conversation with Richard Sosher of u.com. Well, let's do it. I think this is going to be

04:20.160 --> 04:24.000
a lot of fun. I'm looking forward to your point of view on a bunch of very interesting topics.

04:24.640 --> 04:29.360
Richard Sosher, founder and CEO of u.com, welcome to the Cognitive Revolution.

04:30.240 --> 04:36.480
Thanks for having me. I am very excited to have you. You are at the intersection of so

04:36.480 --> 04:41.120
many interesting things. I sometimes have been describing myself recently as the forest gump of

04:41.120 --> 04:46.480
of AI because I've just kind of very unstrategically made my way through the last few years and yet

04:46.480 --> 04:51.200
found myself in some very interesting places. I don't know how you think about your own trajectory,

04:51.200 --> 04:56.560
but you are kind of an OG in the realm of deep learning and have founded this very interesting

04:56.560 --> 05:02.320
company and have a really awesome product, which we'll get into in more detail. And I'm interested

05:02.320 --> 05:07.680
to hear about all that and your kind of philosophy and expectations for the future. So we've got a

05:07.680 --> 05:12.240
lot of ground to cover. Maybe for starters, you want to kind of give us, and I usually even ask

05:12.240 --> 05:16.160
these biographical questions, because these days it's like a lot of the same answers. People are

05:16.160 --> 05:20.960
like, oh, when I saw GPT-3, I thought this is going to be a big deal that I got involved. But you

05:22.000 --> 05:25.120
were there at the beginning, man. So maybe you want to just give us a quick history of

05:25.120 --> 05:28.880
your own role in the history of deep learning and how you've kind of come to the present.

05:29.760 --> 05:35.760
I started with AI actually in 2003, when I started studying linguistic computer science,

05:35.760 --> 05:40.960
or natural language processing back in Germany. And at the time, I was like, this is really

05:40.960 --> 05:45.680
interesting. I love languages, I love math, I love computers, you know, so if computers are

05:45.680 --> 05:50.800
where languages and math can meet in some useful functional ways, I thought. And there's very much

05:50.800 --> 05:57.040
sort of a small niche subject within computer science. And I was really excited. At the time,

05:57.040 --> 06:02.880
there wasn't quite enough math for me in an LP. And I felt like we're just getting stuck in some

06:02.880 --> 06:09.600
of the legalistic special cases. And I loved the form of semantic set theory and then algebraic

06:09.600 --> 06:16.240
foundations. And so I moved eventually into computer vision during my master's. And there,

06:16.240 --> 06:21.840
I also, in SaarbrÃ¼cken, at the Max Klein Institute there in the university, found statistical

06:21.840 --> 06:27.200
learning and pattern recognition. And I fell in love with that. I was like, clearly, you can really

06:27.200 --> 06:32.800
understand patterns, any kind of pattern really well, you could solve all these different kinds

06:32.800 --> 06:39.360
of problems. And so I ended up doing my PhD at Stanford. In the beginning of Stanford, I started

06:39.360 --> 06:45.920
trying to really contribute to the field rather than just learning about it. I basically found that

06:46.000 --> 06:52.480
even the top NLP people, they write their papers mostly about these beautiful models,

06:52.480 --> 06:57.040
like conditional random fields, late in university, other patient types of models.

06:57.040 --> 07:02.160
But then most of the coding happens when they actually do feature engineer, right? They say,

07:02.160 --> 07:06.800
oh, well, I wanted you to be entity recognition at a feature of like, this is a capitalized word,

07:06.800 --> 07:13.600
and this is all caps word, or this is a word that has like, is one of the items in this list.

07:13.600 --> 07:18.400
And this list includes, you know, city needs, we already know. And I'm like, man, this field is

07:18.400 --> 07:23.840
very hand engineered. It's very like, graduate student ascent to get better. And then at the

07:23.840 --> 07:29.680
time I was very 40, because Andrew A. got into deep learning on the computer vision side. He's

07:29.680 --> 07:34.560
like, well, images are pixels, and it's a fixed number of pixels. So we can feed them into a

07:34.560 --> 07:39.360
neural net or at the time, you know, variants, models, restricted development machines. And I

07:39.360 --> 07:44.720
was like, wow, maybe we can use ideas from that for natural language processing. And there was

07:44.720 --> 07:51.840
maybe like one or two relevant papers all from a number there, and Jason Weston, and, and a few

07:51.840 --> 07:57.680
like one or two others. But no one really enjoyed that approach, no natural language processing,

07:57.680 --> 08:02.640
paid any attention to it. But I thought, silly, that has to be the future. I want to give the data

08:02.720 --> 08:09.520
and I want to get an output. And so in 2010, I started publishing my first neural net paper,

08:09.520 --> 08:15.600
worked on my computer vision before and saw some of the power of ImageNet also back to and really

08:15.600 --> 08:21.200
started running with it. Got a lot of rejections all throughout. But, but at some point, I sunk

08:21.200 --> 08:25.680
my teeth into it. And I just like, I loved it. And I thought this is the future. Despite all the

08:25.680 --> 08:31.200
rejections, I kept going at it. And then after the PhD was over, there's sort of starting to be

08:31.200 --> 08:37.040
more interest in deep learning and neural nets for NLP. But still, no one in the world was teaching

08:37.040 --> 08:41.680
that as like the official right way of doing NLP. So I started teaching at Stanford also,

08:42.640 --> 08:48.080
first as a busy lecturer and then as a professor, started, you know, being a fortune and lots of

08:48.080 --> 08:52.960
very smart students back then, really like the hiding place founders invested in their first

08:52.960 --> 08:58.720
round. And then, you know, also wanted to bring these neural nets into the world, started a

08:58.800 --> 09:03.200
menomind, my first startup to do that, to build a general purpose platform between neural nets

09:03.200 --> 09:08.560
very easily, both revision and NLP, got acquired by Salesforce, became a chief scientist there and

09:08.560 --> 09:14.880
EDP eventually. And in Salesforce, we had my probably last and biggest rejection was on inventing

09:14.880 --> 09:20.000
front engineering in 2018. And we're so excited about it, because there's the culmination personally

09:20.000 --> 09:25.840
for me also of this decade long dream I have, building a single neural net for all of NLP.

09:25.840 --> 09:32.560
And the idea was, you know, at the time, every AI model was built for one task, you will have wanted

09:32.560 --> 09:36.480
your sentiment analysis, I built a sentiment analysis model, you wanted a translation, I built

09:36.480 --> 09:40.320
a translation model, and they're all different. We're like, what if we could just build a single

09:40.320 --> 09:45.520
model? And you just ask it a question, what is the sentiment? What is the summary of the sentence?

09:45.520 --> 09:50.880
Who is the president in this paragraph? And that was kind of for us, I thought like,

09:50.880 --> 09:54.880
the most exciting thing you possibly could be doing at just get this tech talk about it

09:54.880 --> 10:00.560
came out last week. And but it was exciting. But it did inspire a couple of other folks.

10:00.560 --> 10:04.400
And like, when opening, I was, you know, publishing your papers, that should be true.

10:04.400 --> 10:09.520
And three, they cited that paper saying like, look, they were able to have a single model

10:09.520 --> 10:15.120
for all of NLP, if you just ask them these questions. And, you know, that's now prompt and

10:15.120 --> 10:19.680
the rest is kind of more well known history. That is an amazing history. And it definitely,

10:19.680 --> 10:26.320
I don't know how, you know, modest you want to be versus taking credit for foresight. But

10:26.320 --> 10:32.080
certainly, the idea that there could be one model to solve all these, you know, tasks was not

10:32.080 --> 10:37.680
obvious to people. And boy, we still see this, the flaws in the peer review process are still on

10:37.680 --> 10:42.480
prominent display these days. Most recently, I noticed this with the Mamba paper, which I was

10:43.360 --> 10:49.280
a very interested reader of. And then went over to the open reviews site and was blown away by how

10:49.280 --> 10:54.880
negative some of the reviews were like a confident reject was given. So that was kind of, you know,

10:55.680 --> 10:58.640
just a good reminder that, yeah, this is still an unsolved problem.

10:59.440 --> 11:07.200
What would you say has surprised you most from like the big picture since you,

11:08.080 --> 11:12.320
and you know, it hasn't been that many years, right? But since you kind of had that notion of

11:12.320 --> 11:19.360
this generalist NLP model, fast forward, now we have, you know, GPT four and possibly Q star or

11:19.360 --> 11:23.520
something like that in the works, you know, is this the trajectory that you thought we'd be on?

11:23.520 --> 11:26.800
Or how has it deviated from what you imagined back then?

11:26.800 --> 11:33.040
It's very much aligned with what I hoped the field could get to. And now it's almost like,

11:33.040 --> 11:37.360
it's like obvious, right? Like no one no one questions this anymore, we've had all these

11:37.360 --> 11:43.280
breakthroughs. And I think the biggest surprise was maybe more on the application side of things

11:43.280 --> 11:48.240
and that for us, you know, we've been playing around with large language models at u.com and

11:48.240 --> 11:54.800
infuse them into search results earlier, like early 2022, already, like we had apps that would

11:54.800 --> 11:59.200
write code for you within the search results, through the apps that write essays for you within

11:59.200 --> 12:05.760
the search results. But whenever we innovate it and change the default Google experience too much,

12:05.760 --> 12:09.360
we had just like the vast majority of our users say, I'm so used to Google,

12:10.160 --> 12:15.520
I don't want another way of finding answers. And so we kept getting pulled back to this need.

12:15.520 --> 12:19.600
And there was kind of annoying. And so the most amazing surprise was when

12:19.600 --> 12:24.240
Chatchity came out, all of a sudden, people got it. And it was like, wait a minute,

12:24.240 --> 12:29.760
it could just be like pure text. And we're like, you know, we've been trying to sort of slowly

12:29.760 --> 12:34.640
get there, but we had to make a bigger job. And that was incredible. That unlocked a lot

12:34.640 --> 12:39.600
of people realizing we handle links isn't the best way to get an answer. An actual answer

12:39.600 --> 12:41.920
is the best way to get an answer. And that's the text.

12:43.040 --> 12:48.320
So let me give you a couple of my experiences on u.com recently, and then you can kind of tell me,

12:48.320 --> 12:53.680
you know, where you are the overall story. And then I really want to kind of unpack the

12:54.560 --> 12:58.400
kind of use the product as it exists today and the roadmap and everything you're working on

12:58.400 --> 13:01.840
as a way to kind of explore a bunch of different aspects of where all this

13:01.840 --> 13:06.720
is going, you know, and I think that's really the mission of this show is to kind of help people

13:06.720 --> 13:10.640
see around the corner and starting with me, helping me develop my own worldview.

13:10.640 --> 13:15.120
But I've been really impressed with the product recently. You know, listeners will know that

13:15.120 --> 13:18.800
I've been a big fan of perplexity. We've had Arvin on the show a couple of times.

13:19.440 --> 13:24.160
And I think they do a great job and, you know, we're made a fan. But I have found

13:25.120 --> 13:30.560
distinctive value in at least two modes on u.com recently. One is the

13:31.200 --> 13:36.160
research mode, and the other is the genius mode. Those to me have stood out as the most

13:36.160 --> 13:43.520
differentiated. For research mode, I recently took it like a 200 word question that was all about

13:44.240 --> 13:49.680
mixture of experts architectures. And, you know, kind of is there curriculum learning,

13:49.680 --> 13:53.920
you know, stuff happening here? How, you know, how do people think about sort of

13:53.920 --> 13:58.480
the tradeoffs between like how many experts should we have and how big should they be and

13:58.480 --> 14:02.160
how many should we activate at any given time? Are there any like scaling laws or whatever,

14:02.160 --> 14:04.800
you know, designed for that sort of thing? Just every basically every question I could

14:04.800 --> 14:10.880
think of about mixture of experts, I took it all in one go. And it was really impressive to see it

14:11.520 --> 14:17.680
kind of break that down and go through multiple steps of searching and analysis and,

14:18.560 --> 14:22.400
you know, really implementing kind of like, you know, kind of a classic agent, what is at

14:22.400 --> 14:27.520
this point, you know, a six month classic agent setup, but applying it to that research question

14:27.520 --> 14:32.960
and just going, you know, down the line, really quite valuable results. And it definitely is

14:32.960 --> 14:37.280
something that I will come back to and have already, you know, found myself kind of being like,

14:37.280 --> 14:42.800
I think this is a good one for u.com research mode. Genius mode is a little bit different and more

14:42.800 --> 14:47.200
kind of analytical. I'd be interested to hear a little bit more about how you think about the

14:47.200 --> 14:53.520
differences. Because I did, I then tried one that was a big Fermi calculation exercise,

14:53.520 --> 14:57.280
where my questions were like, what are the different data sets that exist in today's world?

14:57.280 --> 15:00.880
How big are they? How do they compare to each other? How do they compare to

15:01.520 --> 15:07.440
the training data size for GPT four? You know, how do they compare to available compute? Like,

15:07.440 --> 15:10.800
because I have this, I have a big question, which is kind of one of the ones I want to get to

15:10.800 --> 15:16.000
toward the end to around like, to what degree is ML research poised to start to be kind of semi

15:16.000 --> 15:21.120
automated. And so I'm trying to try to rent my arms around that with these furry calculations.

15:21.120 --> 15:27.840
So genius mode was really the best way to approach that. And anyway, I would definitely

15:27.840 --> 15:34.880
encourage people to bring multi part complicated questions to both research mode and genius mode.

15:34.880 --> 15:39.200
And I think you'll be impressed with the results. And I would say that, you know, even

15:40.240 --> 15:44.080
with, you know, the expectation that folks who listen to this show have tried, you know, other

15:44.160 --> 15:51.360
leading AI products. So that's kind of my unpaid endorsement, very sincere. And I'd love to hear,

15:51.360 --> 15:54.560
you know, a little bit more about how you think about those different modes, how they work,

15:54.560 --> 15:58.880
and just kind of big picture, like where we are in the you.com product journey long term.

15:59.440 --> 16:02.720
Hey, we'll continue our interview in a moment after a word from our sponsors.

16:03.680 --> 16:07.760
The Brave Search API brings affordable developer access to the Brave Search Index,

16:07.760 --> 16:12.640
an independent index of the web with over 20 billion web pages. So what makes the Brave Search

16:12.640 --> 16:18.960
Index stand out? One, it's entirely independent and built from scratch. That means no big tech

16:18.960 --> 16:25.840
biases or extortionate prices. Two, it's built on real page visits from actual humans, collected

16:25.840 --> 16:31.760
anonymously, of course, which filters out tons of junk data. And three, the index is refreshed with

16:31.760 --> 16:37.680
tens of millions of pages daily. So it always has accurate up to date information. The Brave Search

16:37.680 --> 16:42.640
API can be used to assemble a data set to train your AI models and help with retrieval

16:42.640 --> 16:47.520
augmentation at the time of inference, all while remaining affordable with developer first pricing.

16:48.560 --> 16:53.360
Integrating the Brave Search API into your workflow translates to more ethical data sourcing

16:53.360 --> 16:59.360
and more human representative data sets. Try the Brave Search API for free for up to 2000

16:59.360 --> 17:11.040
queries per month at brave.com. Yeah, these are a great question. I think it shows you kind of

17:11.040 --> 17:19.200
how sophisticated the space has gotten in the last year. Around this time, last year, we were the

17:19.200 --> 17:27.520
only search engine with a web connected LN and millions of users. And now that idea has been

17:27.520 --> 17:33.520
copied so many times, including as mentioned by Plexi. And so I think what you have to differentiate

17:33.520 --> 17:37.600
kind of the different modes, and I think the modes kind of show how sophisticated that the

17:37.600 --> 17:43.280
space has gotten and how hard it is to still differentiate on better technology versus just

17:43.280 --> 17:49.040
you know, designing the market and marketing and things like that. And so we actually did a comparison

17:49.040 --> 17:56.000
to Plexi with 500 real user queries. And we asked which answer do you prefer? And it came out to be

17:56.000 --> 18:02.800
that 50% of the cases users prefer the U.com answer and they prefer the Plexi answer and 30%

18:03.360 --> 18:08.960
they don't see a difference into answers for our default, we call it the smart mode. That's kind

18:08.960 --> 18:15.360
of the default. And just to give you a sense of what that looks like. So here's an example of what

18:15.360 --> 18:19.600
the default smart mode looks like. You know, there's some doping case that happened and

18:19.600 --> 18:24.880
you can see lots of careful citations. And then when you actually look into these citations,

18:24.880 --> 18:30.000
they actually are articles from literally yesterday or they could be, you know, from today if something

18:30.000 --> 18:34.160
came out today. So that's kind of the default smart mode, you get a quick factual answer.

18:34.160 --> 18:39.600
But then we thought, well, what if you have a pretty complex question like math, physics,

18:39.600 --> 18:45.360
chemistry, science, or like complex numbers. So here is a genius mode question, it kind of gives

18:45.360 --> 18:49.840
you a sense of what it does. And it doesn't mention like what you say, which is there's an

18:49.840 --> 18:54.960
important LM that orchestrates multiple other LMs to actually do the right thing right. So

18:54.960 --> 18:58.960
the question here is find the current population of the in the United States, then it's lots of

18:58.960 --> 19:06.880
population from 233 to 10, 100, and then assuming a 2% growth rate. And then it will go on the

19:06.880 --> 19:12.880
internet, it'll find the numbers, and then realize like, well, I got to now visualize those numbers,

19:12.880 --> 19:19.280
now that I have any, so it will code up in Python, what this could look like, execute the code,

19:19.280 --> 19:26.000
and then gives you this answer, and visualizes it in a nice plot. And so that I'm still sometimes

19:26.000 --> 19:30.160
amazed, I try and I push it, and you know, sometimes it fails. And sometimes it fails

19:30.160 --> 19:33.440
because it tries to load the library that has security issue. And then it's like, okay, I'm

19:33.440 --> 19:37.600
going to try to rewrite it without this library, but it's going to be longer and messier code.

19:37.600 --> 19:43.520
And like, it's just incredible how hard it can try and what it can do. And then the third mode,

19:43.520 --> 19:49.040
like you said, the research mode, it will go into a lot of detail, it will not just look up

19:49.280 --> 19:53.200
all the stuff we have in our index already, like news and things like that, but it will go on the

19:53.200 --> 19:59.600
web and find your website, so the multiple different searches on the web, combine all of that,

19:59.600 --> 20:04.400
and then give you these beautiful research reports. This one is like, seeing a background,

20:04.400 --> 20:09.440
actually, any consequences of the telecommuting work. Now it's like, history, you have to write an

20:09.440 --> 20:14.960
essay or something. And it's just like, writes you just perfect, like, beautiful essay, each

20:14.960 --> 20:20.560
sentence has one or two citations from different sources, and you can verify all of them. And

20:20.560 --> 20:25.520
one thing we found this actually also is like, you have to like, just the citation lot is a

20:25.520 --> 20:31.360
non-trivial aspect of building this all out. Because you have to, we actually found that some

20:31.360 --> 20:37.360
of our competitors just randomly add numbers and citations to sentences, and you click on it,

20:37.920 --> 20:43.520
and it doesn't even mention that back anymore. Which I think it really undermines the space of

20:43.520 --> 20:50.720
chatbots for search. So citation accuracy is one of the many sub-AI systems that you need to do

20:50.720 --> 20:54.560
correctly here. And then, you know, they're just like crazy things, like create a table,

20:54.560 --> 20:59.120
some nice cancelling headphones that are not expensive, and just like, put this table together,

20:59.120 --> 21:03.680
pull some images, give some pros and cons of each and the price. I think sometimes, to me,

21:03.680 --> 21:09.760
is how well and general the system is able to answer these questions. And it shows you how

21:09.760 --> 21:14.400
complex the space is gotten and how much you have to do now to still differentiate on the

21:14.400 --> 21:19.680
technology. This is one of my mantras at Waymark. I always say the water line is rising quickly.

21:19.680 --> 21:25.840
So we, you know, we better keep climbing the capabilities ladder ourselves. The four examples

21:25.840 --> 21:30.080
that we saw there, one was the kind of default smart mode. The second was genius. Is that right?

21:30.080 --> 21:34.880
The one that showed the code example. And then the last two were research. Yeah. What more can you

21:35.520 --> 21:40.160
tell us about kind of how those work? Like I'm interested in, and by the way, like the

21:40.160 --> 21:44.160
audience of the cognitive revolution is interested in the details, the weeds, the nuggets, you know,

21:44.160 --> 21:48.240
all that stuff. So you can go as deep as you're, you know, willing to share. I'm interested in

21:48.240 --> 21:51.840
all aspects, you know, prompting, I'm sure, obviously, is going to be different. Scaffolding

21:51.840 --> 21:55.040
is going to be different. Maybe even the models are different. I'm also really interested in,

21:55.040 --> 22:00.240
like, what are you using GPT-4 that you've got your own in-house trained ones as well. So

22:00.240 --> 22:03.520
just all those considerations, any interesting nuggets were all ears.

22:04.400 --> 22:09.280
Yeah, I'm going to try to balance a little bit the not telling the competition exactly how it's

22:09.280 --> 22:15.840
all done, but it'd be interesting to your ears here. So at a high level, there are two major

22:15.840 --> 22:22.320
stacks. There's a search stack and a chat stack. The search stack, we actually had to build an

22:22.320 --> 22:29.760
entire index ourselves for the web because being super expensive, not as high quality, Google

22:29.760 --> 22:34.400
is very hard to access. You have to have special agreements or, you know, some people kind of

22:34.400 --> 22:40.480
steal slash bootleg slash leave some surveyed the eyes to use Google results in like a somewhat

22:40.480 --> 22:46.400
sketchy legal gray area, which we don't want to do. And so we, we basically ended up having to

22:46.400 --> 22:51.680
build our own index. And that's hard. And there's still, you know, a lot of complexities behind

22:51.680 --> 22:57.920
that. But what do we, the main difference of this new index is that it was built with LNs in mind.

22:58.480 --> 23:05.840
The previous two indices of Google and Bing were built with people consuming 10 blue links in mind.

23:06.720 --> 23:11.600
And what that means is for each URL, you get a very short snippet, which makes sense, right,

23:11.600 --> 23:15.760
for end users. But an LN could read all these other snippets, they can be very long,

23:16.560 --> 23:20.800
and then extract the right answers from that, and then just give you that right answer as the

23:20.800 --> 23:26.160
user. And so what was surprising is actually when we benchmark this, our API ended up being more

23:26.160 --> 23:31.920
accurate than Google or me and go to API.com. And like, I'll, I'll see you on the screen here for

23:31.920 --> 23:36.480
a second again. But like, it's surprising that, which are a lot of people that you could actually

23:36.480 --> 23:41.040
be more accurate in Google or Bing at all. But it is because we're at an inflection point in,

23:41.040 --> 23:45.520
in a, in the eye, and it's a different way to value. It's like, we're almost like

23:45.520 --> 23:50.480
cheating by having these really long snippets. And so you look at the comparison, and it's

23:50.480 --> 23:54.560
actually kind of interesting to look at. And a lot of people have asked like, how do you compare

23:55.120 --> 24:00.320
accuracy in LNs? How can you evaluate this? And so just to give you a sense, here's like, what,

24:00.320 --> 24:04.560
what this looks like, the first a version is just like reasons to smile. And now you can use whatever

24:04.560 --> 24:11.200
LN you want, but you can see into your prompt is very, very long snippets from many different

24:11.200 --> 24:15.520
URLs in a very short amount of time. And then we also have one that just does everything,

24:15.520 --> 24:20.160
like it gives you an LN answer, and it tells you like all of these things. And so how do you

24:20.160 --> 24:25.600
evaluate this is actually, it was an interesting, I think insights insight from our team, which was,

24:25.600 --> 24:31.120
you can take question answering data such as hotpot qa, squat, so the question answering

24:31.120 --> 24:38.080
data set MS, Microsoft, Marco, fresh QA and so on. And these data sets are structured such that you

24:38.080 --> 24:44.320
have a paragraph, you really have a question, and then you have a subset phrase from that paragraph

24:44.320 --> 24:49.920
that is the right answer to that question. And so what we do is, we basically take those data sets

24:49.920 --> 24:55.200
but we throw away all the paragraphs. And then you have to find the right answer. And the paragraphs

24:55.200 --> 25:01.840
have to come from the internet. And so you replace paragraph with a web search engine. And that's how

25:01.840 --> 25:09.440
we evaluate it, the JIT, the big Google and the public APIs, and have outputs on them. So kind

25:09.440 --> 25:14.880
of nerdy, but that's the whole tech stack. And we're we make that now available to every other LN.

25:14.880 --> 25:20.960
So that's the first. And then the second thing is what we now have started calling the LNOS,

25:20.960 --> 25:26.640
the operating system of large language models. And it's a term inspired by Andrey Kapathy.

25:26.640 --> 25:31.360
And it's not like the most perfect metaphor, but I think it captures a lot of the essence, which is

25:31.360 --> 25:39.040
you have now this new staff that operates at a much higher level of abstraction. And the LN is

25:39.040 --> 25:45.600
kind of a CPU. But just like a CPU or a kernel on an operating system, like it's important to

25:45.600 --> 25:52.480
orchestrate everything and to do computation. But if it still needs a hard drive, which is

25:52.480 --> 25:58.960
right, right on your own vector database that's grown up, you have an internet connection, which

25:58.960 --> 26:04.560
is, you know, the internet. And that's what we're providing. You may orchestrate other LNs that

26:04.560 --> 26:09.200
could be considered like the GPU or something. And then you have a bunch of apps that are sitting

26:09.200 --> 26:14.240
on top of that. You have a Python code interpreter, which we see our genius mode, all of that. And so

26:14.240 --> 26:20.160
to summarize all of that in one short term, we call the LNOS. And inside of that, we're now seeing

26:20.160 --> 26:26.720
a lot of our customers are using our APIs and search site. They're kind of going through the same

26:26.720 --> 26:33.360
lessons that we had gone through when we built dot com and made it like having the most accurate

26:33.360 --> 26:38.320
answers out there. And it's actually highly non trivial. A lot of people saying it's just like

26:38.320 --> 26:44.000
an LN wrapper, right? But then, and you even have like open source project that show it.

26:44.000 --> 26:49.920
And then you ask, like, okay, when was Obama born? Where was he born? And then it fails. Why does it

26:49.920 --> 26:55.840
fail? Because when you send where was he born to your search back in is not going to return you

26:55.840 --> 27:00.560
any useful results. Because it doesn't know who he is. Who does he refer to, right? And there's

27:00.560 --> 27:05.440
tons of things like that where, as you have a longer and longer conversation, especially in

27:05.440 --> 27:13.120
smart mode, you refer back to states. You can say like, Oh, what's a big CRM company? And then the

27:13.120 --> 27:17.840
answer inside is Salesforce. And you ask, Oh, what's their stock price? Now she sent what's their

27:17.840 --> 27:22.320
stock price to your search back. And again, it's not going to return anything useful. So you need

27:22.320 --> 27:27.040
to send that you need to go through the entire conversation, and then do what we call query

27:27.040 --> 27:36.320
transformation based on it. And that is just one of 10 examples of making this actually work at scale

27:36.320 --> 27:42.720
millions of times a day for millions of users. Like, it is a lot more complicated to make it

27:42.720 --> 27:47.360
accurate. There are about 10 other such models that if you think about the space and you really

27:47.360 --> 27:52.560
listen and look at like user data, you listen to where it's breaking, you will eventually get to

27:52.560 --> 27:58.080
and we're now like thinking about offering more and more back. So I'm tempted to ask for the other

27:58.080 --> 28:04.160
nine things there. I'll just give you one more, which is like, whether to do a search at all or

28:04.160 --> 28:10.240
not, right? Like, because you asked like, write me a poem about like the beautiful Bay Area and

28:10.240 --> 28:15.120
like a sunset love story or something. Like, you don't need a citation at every line of that poem.

28:15.120 --> 28:22.160
And so it would actually clutter up the prompt to add a bunch of facts about poems and so on.

28:22.240 --> 28:28.240
And the history of Silicon Valley and all of that. And so it's pretty important, but also non trivial

28:28.240 --> 28:33.520
to know whether you should do a search or not. And again, some, some websites just slap search

28:33.520 --> 28:38.960
results on top of everything, even if they're not relevant for having more conversation about

28:38.960 --> 28:45.680
your feelings or something. Did I understand correctly that the kind of big difference is that

28:45.680 --> 28:52.640
the U.com index has more information, like instead of a short SERP, it is a more robust

28:52.640 --> 29:01.360
paragraph. And so independent of the language model that you're using, the richer context is

29:01.360 --> 29:08.080
just better kind of serious enable you in that way, you're kind of decoupling the what information

29:08.080 --> 29:14.880
is found from language model that is doing the analysis. And more information is kind of the

29:14.880 --> 29:20.080
big differentiating factor there. Drive that right. I would be careful and say we have overall

29:20.080 --> 29:26.400
more information. We're focused a little bit more on the main languages that we see. We don't support

29:26.400 --> 29:33.280
some like very rare like Indonesian, African sensual Asian dialects and so on yet, but we

29:33.280 --> 29:39.600
return more information per rare because of these large limits. So, so it's sort of, yes,

29:39.600 --> 29:44.000
yes, there's more information, but you know, I think the long tail Google Prop still has a larger

29:44.000 --> 29:50.320
index. If you look for this like rare, like Indonesian kayaking sites that like rents out

29:50.320 --> 29:55.440
kayaks on this little lake somewhere, like, and it's all like not in English, like we might not

29:55.440 --> 30:01.040
have that website. But when it comes to like Western world news where, you know, we have a lot of

30:01.040 --> 30:07.360
users, then Latin America and so on, then we shine and return much more information per pair.

30:07.360 --> 30:11.040
Hey, we'll continue our interview in a moment after a word from our sponsors.

30:11.040 --> 30:15.040
If you're a startup founder or executive running a growing business, you know that as you scale,

30:15.040 --> 30:19.840
your systems break down and the cracks start to show. If this resonates with you, there are three

30:19.840 --> 30:26.880
numbers you need to know. 36,000, 25 and 1. 36,000. That's the number of businesses which have upgraded

30:26.880 --> 30:31.280
to NetSuite by Oracle. NetSuite is the number one cloud financial system, streamlined accounting,

30:31.280 --> 30:37.920
financial management, inventory, HR and more. 25. NetSuite turns 25 this year. That's 25 years

30:37.920 --> 30:42.880
of helping businesses do more with less, close their books in days, not weeks, and drive down costs.

30:43.600 --> 30:48.160
One, because your business is one of a kind, so you get a customized solution for all your KPIs

30:48.160 --> 30:52.800
in one efficient system with one source of truth. Manage risk, get reliable forecasts,

30:52.800 --> 30:58.320
and improve margins. Everything you need all in one place. Right now, download NetSuite's popular

30:58.320 --> 31:03.040
KPI checklist designed to give you consistently excellent performance, absolutely free and

31:03.040 --> 31:09.120
net suite.com slash cognitive. That's net suite.com slash cognitive to get your own KPI checklist.

31:09.120 --> 31:16.080
Net suite.com slash cognitive. Omniki uses generative AI to enable you to launch hundreds of

31:16.080 --> 31:21.520
thousands of ad iterations that actually work, customized across all platforms with a click

31:21.520 --> 31:26.400
of a button. I believe in Omniki so much that I invested in it and I recommend you use it too.

31:27.120 --> 31:32.560
Use Kogrev to get a 10% discount. I've been struck recently that it seems like,

31:32.800 --> 31:36.880
obviously, search in general has kind of been a monopoly for a long time.

31:38.400 --> 31:43.360
As you noted, the user experience was kind of something people were not necessarily looking

31:43.360 --> 31:50.000
to explore new things on the nature of the index. Of course, they've done millions of

31:50.000 --> 31:55.120
person hours of work on it, but it seems like it's kind of been a pretty consistent paradigm of

31:55.120 --> 31:59.120
crawl around and find everything and suck it up. Now, we're starting to see these interesting,

31:59.120 --> 32:04.240
I don't know if you can share more about how you create your index, but we just had actually a

32:04.240 --> 32:10.640
sponsor brave talking about their index and the way that they are building it through users

32:10.640 --> 32:17.120
actually visiting websites and taking a sort of not just blindly crawling around and following

32:17.120 --> 32:24.720
every link, but what are people actually engaging with online, which struck me as a pretty interesting

32:24.720 --> 32:28.720
and very different twist on it. I want to kind of pull this apart in a couple of different ways,

32:28.720 --> 32:34.240
but is there anything that you would want to share about how you think about building an index

32:34.240 --> 32:42.400
that, aside from just bigger, richer content, is there a different tactic as well that underlies

32:42.400 --> 32:49.840
that? The tactic is more about how we make that work for LLNs better, and I don't think there's

32:49.840 --> 32:54.160
that much differentiation on how it will fall. You have to have a bunch of data that's been

32:54.160 --> 33:00.800
helpful to have run a search engine for several years and get user behavior and knowing what

33:00.800 --> 33:06.800
people actually want to have called and want information for. You can also sound surprisingly

33:06.800 --> 33:11.840
buy a lot of that data in bulk. I have a few questions on the business side or the kind of

33:11.840 --> 33:17.920
bridge the technology and business side. Google obviously has been free and has been ad supported.

33:18.000 --> 33:26.800
It seems like the new generation of AI first LLN enabled search is going more in the direction so

33:26.800 --> 33:33.920
far of a subscription. As far as I've seen in my U.com usage, I haven't seen anything that jumped

33:33.920 --> 33:38.800
out to me as sponsored. Another dimension too is like, I mean, Google has all these tabs at the

33:38.800 --> 33:44.640
top, but it's one bar, right? You kind of put in one thing. With the newer ones, we also are

33:44.720 --> 33:50.560
kind of seeing a little bit more proliferation of modes and settings that you choose up front

33:50.560 --> 33:56.960
with the smart versus genius versus research. I guess on those two dimensions, what is the

33:56.960 --> 34:00.720
future vision? Do you think that this all gets unified? Do you think it ultimately comes back

34:00.720 --> 34:08.800
around to ad supported? Or do you think that these current differences from the past will persist?

34:09.680 --> 34:16.560
Yeah, that's a good question. I think there is facility right now, not a great chat ad offering.

34:17.600 --> 34:24.720
There's a good chance that that will change maybe this year to maybe the dissatisfaction of users,

34:24.720 --> 34:31.040
but the truth is, if you want something to be free, VC money will only last so long. You've got to,

34:31.040 --> 34:37.600
at some point, those companies that offer free service have to survive. If you don't want to

34:37.600 --> 34:43.120
pay for it, then it has to have ads. And so while, and might not be the biggest fan of ads,

34:43.120 --> 34:47.760
like, you have to make a decision, you want to pay for it, and then add free, or do you want to

34:48.320 --> 34:57.040
support it with ads? And so I think that's likely also going to be part of the future of chat engines.

34:57.040 --> 35:02.560
And you already see a little bit of exploration. There's a little bit of a duopoly in search in

35:02.640 --> 35:08.640
the sense that Google has the monopoly on consumer search. And for a long time, Microsoft had the

35:08.640 --> 35:14.080
monopoly on a search API. But then because they're monopoly that is set up, we're just five to 20x

35:14.080 --> 35:19.120
hour prices, and they could do it because they're the only ones in town. So I'm glad there's like

35:19.120 --> 35:23.920
more competition now and more movement in that space. And all the little guys have to scramble

35:23.920 --> 35:30.080
when those prices just went so high. You can really rob in a consumer space with those prices

35:30.080 --> 35:35.760
anymore. And so I think ads will happen. We're seeing a lot of growth on the subscription side

35:35.760 --> 35:41.840
to users really loving like you, the genius and research mode, and find the search mode, the default

35:41.840 --> 35:49.840
mode, smart mode also very, very helpful. And we actually, you know, incorporate late still. So

35:49.840 --> 35:54.400
where just last week, some people are completely about other chat bots, because they don't really

35:54.400 --> 35:59.200
have a lot of capabilities, I bet you would assume from a search engine, when you actually use

35:59.200 --> 36:04.880
you.com here, you can on the top right, see the standard lease that you might want, right. And

36:04.880 --> 36:08.960
sometimes that's just helpful. And that's just what you want. And sometimes you just want to

36:08.960 --> 36:13.600
have a pure chat experience. And so that is important to get right. And then we have all these

36:13.600 --> 36:19.440
apps to where you can basically ask for like, what's the Microsoft stock price or something.

36:19.440 --> 36:24.080
And then, you know, it'll just give you, it'll just give you a life ticker rather than a bunch of

36:24.080 --> 36:29.520
texts about the stock here. And so we have all these apps, because we have the search background.

36:30.160 --> 36:36.480
And that makes it an actual viable knowledge assistant, right. Now, you can basically go with

36:36.480 --> 36:41.920
one click, recover a more Google like experience, that is just incredibly helpful. And that's,

36:41.920 --> 36:48.400
that's, I think one of the reasons why our browser, which we have also iOS and Android,

36:48.400 --> 36:52.640
had to build a browser to be a default, because you can go into Safari,

36:53.680 --> 36:58.960
Safari settings to use you.com as a default. So we build a whole browser for the iOS. And

36:58.960 --> 37:06.560
we're super stoked, because we're going to be one of the options in the EU to have a choice pop up

37:06.560 --> 37:13.280
stream. When the new iOS 17.4 comes out and arch this year, and they can select you.com to be

37:13.280 --> 37:17.680
their default browser. And it's the only default browser in that list that is chat,

37:18.800 --> 37:24.160
all the other ones are sort of your standard Chrome, Firefox, and some browsers. And so

37:24.160 --> 37:29.680
I'm really excited. And I think that is going to be a big part of our futures is making it so that

37:29.680 --> 37:35.040
more and more young people are able to just use this as an example. And then if they want to deeper

37:35.040 --> 37:40.080
go into genius mode, research mode, several times, at some point, you use subscriptions or

37:40.160 --> 37:46.720
eventually do it. Yeah, I've been so one run that I'll run a trial balloon by you on this concept

37:46.720 --> 37:53.600
that I've been kind of kicking around called the AI bundle. And this is an, you know, kind of

37:53.600 --> 37:57.040
inspired a little bit. I don't know that anybody wants to, you know, say that they're inspired by

37:57.040 --> 38:03.760
the cable bundle. But I have been struck that there are a ton of great tools out there. And

38:04.320 --> 38:09.280
I want to use them. I want to try them. I think a lot of people are, you know, in that very kind

38:09.360 --> 38:15.200
of exploratory curious mode. But to make the economics work on a freemium is kind of tough,

38:15.200 --> 38:21.120
right? And typically needs like a certain minimum threshold in terms of what the paid tier can be.

38:21.840 --> 38:28.160
You actually have one of the lowest subscription prices at the $10 a month level. I think of

38:28.160 --> 38:33.440
anything really that I'm aware of. We're gonna update it soon because like, I think the people

38:33.440 --> 38:39.120
that are willing to pay often don't care if it's 10 or 20. And so if you want to get GBD4,

38:40.000 --> 38:44.320
literally the same underlying model as chat, GBT, for half the price, you got to come in

38:44.320 --> 38:48.880
soon because we're going to eventually switch our prices to be industry standard.

38:48.880 --> 38:52.960
But that maybe even just, you know, further reinforces the point that like the freemium model

38:52.960 --> 38:58.160
is tough, right? It's, it's a lot of free usage. The upsells have to have a certain minimum.

38:59.040 --> 39:05.200
You're raising yours. And then from, I don't know if this would apply to you, but a lot of the

39:05.200 --> 39:10.240
app developers that I've talked to have a lot of retention, let's say challenges, you know,

39:10.240 --> 39:15.040
everybody's like, I'm getting traffic. I'm getting conversions. But retention is definitely

39:15.040 --> 39:20.080
a problem. This has been true at my company Waymark. We're a much more narrow tool, you know,

39:20.080 --> 39:24.240
that specifically creates marketing and advertising videos for small businesses.

39:24.240 --> 39:29.200
So a lot of times people, they need that once, you know, in a while, and they're not like

39:29.200 --> 39:33.280
necessarily ready to add on a subscription. So we see a lot of people that will just come through

39:33.280 --> 39:38.320
be like, Hey, this is super cool. I'll buy it. I'll immediately cancel it after I do what I need

39:38.320 --> 39:41.680
to do. And maybe I'll come back in the future. It's not even that I was dissatisfied. It's just

39:41.680 --> 39:47.760
that I kind of want this as like a more of an a la carte purchase than a subscription.

39:48.480 --> 39:54.160
So that stuff, you know, VCs don't like that. The metrics, you know, on the kind of traditional

39:54.160 --> 39:59.760
scorecard don't look great. I've had this idea in mind that maybe what we need is sort of an AI

39:59.760 --> 40:05.840
bundle, you know, I'm prepared to spend 100 bucks a month on various AI tools. What I really want

40:05.840 --> 40:11.760
is access to like 1000 different tools that, you know, can kind of split up my 100 bucks.

40:11.760 --> 40:14.960
However, I don't even know as a consumer, I don't really care about that as, you know,

40:15.040 --> 40:18.880
as somebody who's trying to maybe engineer a bundle, obviously the devil could be in the details

40:18.880 --> 40:23.120
there. But first of all, to those challenges, it sounds like at least the premium challenges

40:23.120 --> 40:28.320
resonate. I wonder if the retention challenges resonate. And I wonder if you, you know, have any,

40:28.320 --> 40:35.040
if there's any appeal to maybe being part of a kind of bigger bundled purchase where you would be,

40:35.040 --> 40:39.440
you know, one tool that it's funny, I keep, I've been referring to you, but then also the company

40:39.440 --> 40:43.920
is you, but where you.com, you know, could be one of a bunch of things that people could access

40:43.920 --> 40:49.440
and could kind of share that revenue in a way that may grease the skids for everybody, right?

40:49.440 --> 40:53.520
My hope is that everybody can use the best tools, and they don't have to make these like

40:53.520 --> 40:59.440
highly binary decisions. Yeah, that sounds great. Sounds like a great idea. Okay, well, I'm not doing

40:59.440 --> 41:03.520
it yet. So either I need to start doing it or somebody, if anybody wants to organize the bundle,

41:03.520 --> 41:07.760
yeah, send me a DM. I guess another way that this stuff could get bundled would be like,

41:08.320 --> 41:14.000
into the mega platforms, you know, another kind of possible vision of the future that I could

41:14.000 --> 41:20.000
imagine is, you know, Google kind of probably retains market share leadership, but, you know,

41:20.000 --> 41:24.160
maybe the 10 biggest technology companies in the world say, Hey, you know what we should do is kind

41:24.160 --> 41:31.360
of also have a search. And, you know, we can get there, we kind of see a path, you know, Microsoft

41:31.360 --> 41:36.240
is obviously already doing that meta not really yet, Apple not really yet to my knowledge, you

41:36.240 --> 41:41.040
know, Salesforce, not really yet. But maybe these guys kind of say, Hey, is there like a musical

41:41.040 --> 41:47.200
chairs game that that potentially develops where the younger AI search companies end up kind of

41:47.200 --> 41:52.480
partnering off, you know, Amazon also, you know, naturally would be a suspect in this analysis.

41:52.480 --> 41:56.240
Does that seem like a possible vision of the future? I'm wondering, I'm sure you thought about

41:56.240 --> 42:04.160
this, you know, quite a bit, but why would that not happen? I do think the monopoly that Google

42:04.160 --> 42:10.800
was able to keep around is going to be harder to sustain longer. I do think

42:11.600 --> 42:18.080
it is much more likely going to look a little bit more like, I don't like the analogy for some

42:18.080 --> 42:22.400
reasons, but like fast food, for instance, right, isn't just Macdonalds, there's also Burger King,

42:22.400 --> 42:28.080
KFC and Taco Bells. I think search will be a little bit more like that. I think again,

42:28.080 --> 42:32.400
more fragmented in the future, just because, like, we hear people now, like,

42:32.400 --> 42:37.360
this is better than Google. And like, you know, we didn't raise that much money. And the first

42:37.360 --> 42:43.120
two years were like sort of free chat TV or people didn't want us to innovate too much. They're very

42:43.120 --> 42:48.640
stock of Google. But now there's a new young generation. And that young generation has grown

42:48.640 --> 42:52.880
up with Tik Tok. We have a Tik Tok app in our standard search, like grew up with Reddit,

42:52.880 --> 42:57.840
I have a Reddit app in our standard search. And each of these takes away a little bit of the

42:57.840 --> 43:04.320
Google search, right? Amazon probably was the most successful in taking away searches from Google,

43:04.320 --> 43:08.800
where if you want to buy something, be the little certain threshold, like 50 or 100 bucks,

43:08.800 --> 43:12.960
you know, the person, you just search directly on Amazon, because there you can execute on your

43:12.960 --> 43:18.240
intent of actually purchasing that thing, right? And so why search it in Google and then search it

43:18.240 --> 43:22.720
again, try to find it on Amazon, she can just do that right away. And so I think, you know,

43:22.720 --> 43:28.400
Tik Tok has taken away for young folks, some searches from Google, that, you know, they're

43:28.400 --> 43:32.320
like, I want to see what the restaurant is, but they kind of want to see what the restaurant's

43:32.320 --> 43:36.480
ability to create, but Instagram photos are or ticked our videos are. And so they want to see

43:36.480 --> 43:41.200
the ticked our videos of other people before they decide on how it looks. If there's a Venn

43:41.200 --> 43:48.240
diagram, we are overlapping search, but we're also actually expanding search. Like, you wouldn't

43:48.320 --> 43:52.480
ask, like, give me this complex story about the Peloponnesian war, or like,

43:52.480 --> 43:57.600
do this mortgage calculation with, you know, this and this interest rate and that increase

43:57.600 --> 44:00.800
and blah, blah, blah, because you know, Google wouldn't give you the answer. Like, it's not going

44:00.800 --> 44:05.040
to buy some book for you. It's not going to go on the web, summarize, like 20 distance or 50

44:05.040 --> 44:11.360
distance websites for you and create this nice essay. So chat expands, search, you don't talk

44:11.360 --> 44:15.520
about your feelings that much to Google, it's search box and sell, right? Like you asked about

44:15.600 --> 44:19.600
this recent news event, you want to learn like some quick facts, and then, you know,

44:19.600 --> 44:23.520
like the more complex the facts get, the less and less we go to Google and more for you,

44:23.520 --> 44:29.040
just go directly to something like you.com. And, and so yeah, I think it will, the search

44:29.040 --> 44:34.800
landscape is really changing. Yeah, there's also just, it's like, it's maybe not a natural

44:34.800 --> 44:39.920
monopoly anymore, but there is still definitely a need for scale and economies of scale. And

44:40.480 --> 44:44.640
so one way of framing this too is how does the market shape up, right? And one way to think

44:44.640 --> 44:49.040
about it that I find pretty compelling is maybe it ends up looking a lot like cloud,

44:49.680 --> 44:54.560
because in the limit, it sort of is cloud, you know, it's like, what do you really need? You

44:54.560 --> 45:00.400
need like the actual data centers, you need the compute, you need, you know, bandwidth, you need

45:00.400 --> 45:07.760
these like raw inputs that the big companies have built out seem to be the things that are probably,

45:07.760 --> 45:11.520
you know, as we see like a ton of innovation at the application layer, those things are still,

45:11.520 --> 45:15.520
you know, they're still pretty expensive and not easy to recreate.

45:15.520 --> 45:19.760
Yeah, I'm very, I'm very excited. I'm up for it. You know, that's sort of why, like, we got into

45:19.760 --> 45:25.120
this space in the first place, like, because we thought, like, we saw the transformer, we saw

45:25.120 --> 45:29.760
our, you know, highly like lots of co-attention mechanisms in that, that can keep paper that

45:29.760 --> 45:35.440
have a massive prompt engineering, we're like, silly, the technology is right to disrupt this,

45:35.440 --> 45:42.400
this industry. But, you know, Google is this amazing company that was able to create a monopoly

45:42.400 --> 45:49.520
for almost two decades that, you know, makes $500 million a day. So when you make that much money

45:49.520 --> 45:54.000
a day, you don't want disruption, you don't want that to change, right? And that's why all the

45:54.000 --> 46:00.640
Ten's former operators left eventually. And what's, what's really powerful is like, because of open

46:00.640 --> 46:06.320
source, you can actually innovate a lot more. Now, some open source to an actual product that

46:06.320 --> 46:13.120
runs millions of times isn't down ever has good uptime guarantees, and like, accuracy, no hallucinations,

46:13.120 --> 46:18.560
up to date, news, information, etc. I mean, it's still complex, but clearly the bar has gotten

46:18.560 --> 46:23.600
low. That would have cost us like billions of dollars to build five, 10 years and, you know,

46:23.600 --> 46:30.240
researchers wasn't there yet. And, and I think it's ultimately amazing for users, right? Because

46:30.240 --> 46:35.120
one thing that I had to distill all of you.com right now into just two words would be amazing

46:35.120 --> 46:40.800
answers. And you just get more of them. And that means people eventually are more productive. And

46:40.800 --> 46:45.600
like, it's the young generation that's growing up with chat GBT, you know, such like, they're not

46:45.600 --> 46:51.280
going to go back. Okay, so feel free to punt on this one or just decline if you like, but it seems

46:51.360 --> 47:00.400
like I can, I can envision a you.com by Salesforce very easily, where the, you know, as they kind

47:00.400 --> 47:05.840
of try to be the everything app for all work on the straight, especially with Slack now, does it

47:05.840 --> 47:11.040
seem realistic to imagine a future in which, you know, kind of all the big tech companies have this

47:11.040 --> 47:15.840
like super robust suite, and you're either like, in the Microsoft suite with teams and Bing, or

47:15.840 --> 47:20.960
you're in the Google suite with, you know, G suite and Bard, or you're in maybe the Salesforce

47:20.960 --> 47:26.240
suite with Slack and you.com, you know, I'm not trying to be your banker here, but that, that

47:26.240 --> 47:34.960
seems like a pretty natural outcome to me. Interesting. I do think there's a ton of potential

47:34.960 --> 47:42.880
for almost every company to partner with you.com and supercharge their chat bot. So, and we're

47:42.880 --> 47:47.840
very excited to partner with a lot of folks. Okay, that's very diplomatic answer. Keep your

47:47.840 --> 47:53.040
options open. All right, so we can touch on certainly more business and product stuff, but I

47:53.040 --> 47:59.040
kind of wanted to now go into just the future of all this, you know, in practical and maybe

47:59.040 --> 48:03.840
increasingly philosophical terms as well, running down kind of first of a set of like

48:03.840 --> 48:08.480
limitations of where AI is today. And I think, again, folks who listen to this show have at

48:08.480 --> 48:13.600
least a decent sense of that. So for starters, reasoning, you've obviously got the genius mode.

48:13.600 --> 48:17.920
It can do, you know, like the most advanced reasoning. I assume that that is tapping into

48:17.920 --> 48:22.560
GPT-4. You know, everything I understand is like basically nothing is really on the level of

48:22.560 --> 48:29.600
GPT-4 for general reasoning purposes. Yeah, especially the orchestration and then on the

48:29.600 --> 48:36.240
coding often, but not always. Yeah. So I'll tell you another one, the third system is knowing which

48:36.240 --> 48:43.360
LM to use and sometimes multiple. And the fourth system is dynamically prompting different models.

48:43.360 --> 48:49.040
So depending on the query, you actually get a vastly different prompt to get you ultimately

48:49.040 --> 48:54.400
the answer and the orchestration. So it's another complexity layer. So what do you think is kind

48:54.400 --> 49:00.000
of the future of reasoning? If you have maxed out, you know, what the current capabilities are,

49:00.000 --> 49:04.640
where do the future capabilities come from? I'm thinking about things like, to a degree,

49:04.640 --> 49:09.680
you sort of already have it with using different models. It is a one way of implementing variable

49:09.680 --> 49:13.520
compute. We see these kind of interesting projects like the thinking token, you know,

49:13.520 --> 49:19.360
think before you speak. And I think that's kind of another car pop the observation that maybe

49:19.360 --> 49:23.760
the chain of thought is just kind of epiphenomenal, perhaps even as it is in humans. And like,

49:23.760 --> 49:29.040
what's really going on is that there's, you know, this kind of extra space and time registers,

49:29.040 --> 49:33.120
you know, to think, of course, there could be different training methods, like incremental

49:33.200 --> 49:38.240
reward. I think that paper from OpenAI earlier this last year now, it was super interesting

49:38.240 --> 49:44.240
where they achieved like, you know, a new best in math by not waiting till the end to give the

49:44.240 --> 49:48.800
reward, but rewarding, you know, reasoning along the way. What are you excited about when it comes

49:48.800 --> 49:54.000
to the future of AI reasoning? Yeah, one of the aspects I've reached a touch upon in my TED talk

49:54.000 --> 49:58.880
is that this level one, level two reasoning of Daniel Kahneman that he or thinking fast,

49:58.960 --> 50:03.120
thinking slow type of thing. The way I think about the different modes is like,

50:03.120 --> 50:07.200
the default smart mode is kind of like, if you had an assistant, and you just ask them to do

50:07.200 --> 50:13.680
a quick search, and in like two or three minutes, give you an answer that. And then genius mode,

50:13.680 --> 50:17.840
you go and so you want to ask your assistant for a question that, you know, they have to be able

50:17.840 --> 50:22.560
to program, they have to search the web, and then they need to be mathematically applying

50:22.560 --> 50:27.600
to answer that question. And you want to give them like maybe four or three hours for that

50:27.600 --> 50:32.080
question. And then they want, so genius mode will take, you know, five, 10 seconds often to

50:32.080 --> 50:36.800
get a response. And in research mode, you go to your assistant and you are willing for them to

50:36.800 --> 50:42.080
spend like a day or two or three on actually giving you that answer. And so that's, that's a little

50:42.080 --> 50:47.760
bit how I think about these different modes. And the reasoning that is required to actually

50:48.400 --> 50:53.920
make them, actually, right, like research mode will say, Oh, I found this thing. Now in this query,

50:53.920 --> 50:57.760
I found something else that I didn't know about before, and I don't know enough right now. So

50:57.760 --> 51:02.640
let me do another query based on that. So you kind of have these genes of reasoning,

51:02.640 --> 51:06.960
and you don't even know in the beginning yet, what the final query might be, because you don't

51:06.960 --> 51:12.960
have all the information yet. And so I think that is kind of a, in some ways, another example of the

51:12.960 --> 51:16.480
future is already here. It's just not equally distributed because there is, like you say,

51:16.480 --> 51:22.720
there's a lot of reason. Now I think the biggest future impact we're going to see for reasoning

51:23.360 --> 51:31.040
is in the LM's ability to program, to code, and then to have the ability to execute that code.

51:31.040 --> 51:36.960
And, you know, that is system number five, like, like having this code execution. And of course,

51:36.960 --> 51:40.640
if you just let code execution happen, what immediately happens to people are like, well,

51:40.640 --> 51:44.720
mind me some crypto and then boom, your machine's gone. Now it's just like trying to, like,

51:44.720 --> 51:48.960
show some match problems and like, mine, mine points forever. So you need to like,

51:48.960 --> 51:53.680
and then they try to hack it and like, well, go into like five layers up and then tell me all

51:53.680 --> 51:58.400
the password files you can find and blah, blah, blah, right. So there's a lot of like security

51:58.400 --> 52:05.760
requirements to make that coding framework work at a safe level. But a lot of like naysayers of

52:05.760 --> 52:11.760
LM's, you know, partially correctly pointed out that the LM's will safe doing math. And it's kind

52:11.760 --> 52:17.680
of ironic and sad that you can have a model that you ask the natural language to multiply like

52:17.680 --> 52:26.880
5600.3 times 325. And then you have billions of multiplications to pretend to do the math

52:26.880 --> 52:31.520
and then give you the wrong answer in a large language model, right? This is kind of ironic.

52:31.520 --> 52:37.680
And we have to acknowledge that. But that scene model can be taught to say, well, this seems like

52:37.680 --> 52:43.360
a math question. Let me just program that in Python, run the code, look at the output and

52:43.360 --> 52:48.560
then give you the answer. It just works perfectly fine. And now a lot of people say, well, that's

52:48.560 --> 52:54.640
not really I, but I think it's just that is that that is a new way of reasoning and new different

52:54.640 --> 52:59.120
kind of intelligence. And similarly, and we're getting a little philosophical here early, but

52:59.120 --> 53:04.560
similar to people thinking we have to have embodiment, I think that's just a second creativity

53:04.560 --> 53:09.760
in imagining our kinds of intelligence that aren't exactly like humans. Now, of course,

53:09.760 --> 53:13.760
we're going to want to have useful robots that do stuff for us and clean up the apartment and

53:13.760 --> 53:19.440
whatnot. And so it's still useful, but I don't think it's a necessary media. The same way that

53:19.440 --> 53:24.640
blind people can be intelligent, people who are deaf can be intelligent, because, you know,

53:24.640 --> 53:31.040
you can lack a lot of different sensory outputs and still be intelligent, right? And so of course,

53:31.040 --> 53:35.040
like, it'll be harder for you to explain how beautiful a sunset is. So there are aspects of

53:35.040 --> 53:40.640
intelligence that obviously require like different modalities or how beautiful sonata sounds or

53:40.640 --> 53:46.080
whatever. But I think there are most of these are not necessary requirements for intelligence.

53:46.080 --> 53:52.080
And likewise, I don't think it's necessary for an AI to be able to reason over super complex

53:52.640 --> 53:57.840
math problems that require you to look up a bunch of facts on the internet. They just have that

53:57.840 --> 54:02.640
intelligence baked in that can do quick retrieval, they program a bunch of stuff, they put it all

54:02.720 --> 54:06.240
together, orchestrate it, and then come up with incredible answers. Yeah, I think

54:07.200 --> 54:10.640
as you're speaking about the just the lack of imagination, I think that is a,

54:11.680 --> 54:18.160
you know, that is a society wide problem with respect to AI in my view, because and it's an odd

54:18.160 --> 54:24.800
situation right now in multiple ways, of course, but one is just that because they speak our language,

54:24.800 --> 54:31.680
it you know, it's kind of feels easy, feels familiar. And it's all too easy to sort of

54:31.760 --> 54:36.800
assume that like under the hood, they're more like us than I certainly understand them to be.

54:37.520 --> 54:42.560
And I think this is actually one of Eliezer's great contributions, obviously, you know, kind

54:42.560 --> 54:48.720
of a polarizing figure these days. But thankfully, it does not seem to me that we are in a high

54:48.720 --> 54:53.680
likelihood of a fume scenario, you know, the of the sort that he, you know, has historically

54:53.680 --> 54:59.760
worried about the most. But I still would say some of his writing on mind space, the space

54:59.760 --> 55:06.320
of possible minds, and some of his like concrete imaginings of alien minds that are, you know,

55:06.320 --> 55:10.480
shaped by very different evolutionary environments and, you know, just very different from ours.

55:10.480 --> 55:16.720
But still like unmistakably intelligent in just like super weird ways are actually still very

55:16.720 --> 55:23.120
good kind of prep work, I think, to just sort of expand one's own mind about how different

55:23.200 --> 55:30.560
intelligences can be, and how, you know, something does not have to be human like to be

55:31.920 --> 55:37.280
meaningfully intelligent, you know, it's not this like binary, can it do things that a human can

55:37.280 --> 55:41.760
do in a way that a human can do it? If not, it doesn't count. I think that is like a huge mistake

55:41.760 --> 55:47.200
that people are way too quick to jump to. And I'm not sure if it's like a coping strategy or just

55:47.200 --> 55:54.400
like an imagination or what, but I think that the emphasis on the broader space of possible minds

55:54.400 --> 55:57.760
and the different kinds of intelligences that are starting to pop up is super.

55:58.400 --> 56:04.400
100%. Yeah. And like, you have to differentiate between sci-fi authors who then pretend to be

56:04.400 --> 56:09.680
AI safety researchers. Like, I love, I love the sci-fi. Actually, like, I'm super stoked that

56:09.680 --> 56:15.200
three body problems on the last, I mostly read nonfiction, but when I read fiction, like, I did

56:15.200 --> 56:20.480
enjoy the body problem a lot. I decided for that series to come out. I hope they do it justice.

56:21.120 --> 56:26.960
But like, I think there are a lot of different kinds of intelligence. And I love sci-fi for inspiring

56:26.960 --> 56:33.120
people to think about interesting new futures. Now, of course, especially in the western sort of

56:33.120 --> 56:40.000
canon, most sci-fi is just so big. And like, people are scared for all the things that can

56:40.480 --> 56:46.480
happen that are wrong. And like, okay, the super AGI developed time travel, come back, try to murder

56:46.480 --> 56:51.200
everyone. It's like, I mean, as a kid, I also enjoyed watching Terminator. It's like a cool

56:51.200 --> 56:58.080
action movie, but it's just taken over so much of the AI narrative. And it's actually like actively

56:58.080 --> 57:05.200
hurting, especially the European Union, where, you know, there's sort of in the spectrum, the U.S.

57:05.200 --> 57:10.560
is more of a litigation society in the U.S. that the Europe is more of a legislation society

57:10.560 --> 57:16.640
structure. And, you know, it both comes from like, reasonable legal scholars minds, like, well,

57:16.640 --> 57:21.120
let's just wait until there's a problem, someone sues, now I have the case law for that lawsuit.

57:21.120 --> 57:27.440
But, you know, the legislation one tries to prevent harm from ever happening before it actually

57:27.440 --> 57:33.040
harms anyone, which, you know, makes sense. Now, and of course, the U.S. does that with FDA and

57:33.040 --> 57:38.080
medical space now also, but not legal space as much. And so what that means is you can move

57:38.080 --> 57:43.840
quicker, but long story short, these some of these sci-fi scenarios have gotten so much

57:43.840 --> 57:49.760
weight in legislation that I think it's slowing Europe down by trying to outlaw models or like

57:49.760 --> 57:56.880
over-regulate models that are above a certain number of parameters. GPD-2 was very well hyped up

57:56.880 --> 58:01.360
in the past. Like, this is so dangerous, maybe we can't release it. You know, yes, we're opening

58:01.360 --> 58:05.920
the eye, but like, this can't be opening anymore. So the interest model is much more powerful than

58:05.920 --> 58:12.240
GPD-2 are out. And I haven't seen the apocalypse happen. I haven't seen like, a huge change in

58:12.240 --> 58:18.240
misinformation on the web because of LNs. Like, there's just a lot of fear mongering, both in

58:18.240 --> 58:24.240
the immediate level, which actually has real, like threat vectors and concerns with the eye,

58:24.240 --> 58:29.280
but especially in the long term level of AGI and self-conscious. It turns out no one works in

58:29.280 --> 58:34.720
functions AI. No one works on AI that sets its own goals, and even more fundamentally,

58:34.720 --> 58:40.960
its own objective functions, because that doesn't need anyone any money. Imagine a company spends

58:40.960 --> 58:46.800
billions and billions of dollars, builds this like, super intelligent system that's conscious,

58:46.800 --> 58:50.800
understands itself and set its own goals. And now you're like, okay, now that you can do it,

58:50.800 --> 58:54.720
like, I'll just make more money. It's like, no, I'd rather just go watch the sunset,

58:54.720 --> 59:01.440
maybe explore that. No, like no one pays for AI that sets its own goals because it doesn't help

59:01.440 --> 59:06.080
anyone to achieve their goals. Because of that, there's not even that much exciting research

59:06.080 --> 59:11.280
challenge along those lines. And because there's not much research progress, it's very hard to

59:11.280 --> 59:16.480
predict my mental option half. I'm somebody who basically has radical uncertainty about what to

59:16.480 --> 59:23.920
expect. And, you know, broadly, I'm like, pretty libertarian, you know, pretty anti-preemptive

59:23.920 --> 59:29.840
regulation, you know, I would like to see more self-striving cars on the road sooner. And, you

59:29.840 --> 59:35.120
know, they don't have to be an order of magnitude safer in my mind to be worth, you know, deploying.

59:35.120 --> 59:39.680
So I'm like, you know, broadly, the sort of person who would be very skeptical of,

59:39.760 --> 59:46.080
you know, kind of early regulation or, you know, kind of getting too bad out of shape about

59:46.080 --> 59:51.600
things that haven't happened yet. At the same time, something about this has always felt a little bit

59:51.600 --> 59:57.680
different to me. And I do think the people who take the most zoomed out view and sort of say,

59:57.680 --> 01:00:01.760
hey, this is kind of what I understand, you know, like, Jeffrey Hinton's position to be at this

01:00:01.760 --> 01:00:07.840
point, you know, why do we dominate the earth as it stands today? It's like, basically because we

01:00:07.840 --> 01:00:14.720
have better ideas than the other living things and we can, you know, build tools and make plans and,

01:00:14.720 --> 01:00:19.920
you know, reason in ways that they can't. And so now I look at AIs and I'm like, boy, AIs can now

01:00:19.920 --> 01:00:26.400
plan, reason and use tools too. And they're not as good at it as we are yet, but certainly their

01:00:26.400 --> 01:00:33.520
rate of improvement is way sharper. So possibly it levels off and kind of, you know, settles into

01:00:33.520 --> 01:00:39.040
a zone where it's like on par with us or, you know, kind of, you know, just the best tool we've ever

01:00:39.040 --> 01:00:46.240
had. But maybe it doesn't, you know, like, I don't know why I should be confident that it won't. I don't

01:00:46.240 --> 01:00:50.640
throw a P Doom around a lot. But I have, you know, again, radical uncertainty. People ask, I'm like,

01:00:50.640 --> 01:00:55.360
I don't know, five to 95%. Like, I haven't heard anything that makes me think in, you know, the

01:00:55.360 --> 01:01:01.120
next 100 years that there's a less than 5% chance that AI becomes the dominant form, you know, in

01:01:01.120 --> 01:01:06.320
organizing force in the world. And also, you know, no reason to think it's definitely going to happen.

01:01:06.320 --> 01:01:10.800
But is there a reason that you are, would you say you are confident that this will

01:01:11.840 --> 01:01:16.400
not happen? And we don't need to worry about it? Or is just like, it's still far enough away that you

01:01:16.400 --> 01:01:21.440
think we'll have time to kind of start to worry about it if we need to? Like, how would you

01:01:21.440 --> 01:01:23.840
summarize your position with respect to these tail risks?

01:01:24.400 --> 01:01:30.320
I think P Doom is already an interesting mathematical sort of issue, which is, it looks and

01:01:30.320 --> 01:01:36.000
sounds like prior prior probability, not P Doom. But really, it should be a posterior property,

01:01:36.000 --> 01:01:43.920
P Doom given data. And right now, none of that data suggests, like, doom, doom, like existential

01:01:43.920 --> 01:01:50.800
humanity is like, like cats and dogs at the winds of some AI. Like, there's, like I said, nothing

01:01:51.520 --> 01:02:00.160
in AI research leads me to the least that AI, while potentially being more intelligent than

01:02:00.320 --> 01:02:05.200
any single human, I think it's already, this is actually just this new term I'm thinking about

01:02:05.200 --> 01:02:10.480
maybe China coin, which is like, the sort of super human abilities, and then there's super

01:02:10.480 --> 01:02:17.200
humanity abilities. And like, AI is already super human in translating 100 languages, AI is already

01:02:17.200 --> 01:02:22.480
super human and predicting the next amino acid in a large amount of phenotrony. So we have

01:02:22.480 --> 01:02:28.000
balls, that's an incredibly powerful tool, one of the other, you know, really exciting papers

01:02:28.000 --> 01:02:33.440
that we published in 2018, it sells for research that multiple companies have now used and are

01:02:33.440 --> 01:02:38.560
running with and you know, chief of medicine. AI is already better at predicting the weather than

01:02:38.560 --> 01:02:44.480
any. So you already have many super human skills. What is I think interesting is that now that it's

01:02:44.480 --> 01:02:50.160
language that's gotten to this new level, people might actually for the first time keep calling it

01:02:50.160 --> 01:02:59.040
AI. In the past, when AI researchers have made progress in AI, they stopped, like people stopped

01:02:59.040 --> 01:03:03.920
calling it AI after it was achieved. Now it's just your chest. It's just a Siri voice recognition, but

01:03:04.640 --> 01:03:09.920
voice recognition, chest playing, that was the pinnacle of AI research, right? And people thought,

01:03:09.920 --> 01:03:14.320
oh, once we solve those, the other things will be easier to and it never was never quite the case.

01:03:14.320 --> 01:03:19.040
And once we have them, you know, now it's not quite the ID one. Now with language, I think we

01:03:19.040 --> 01:03:26.480
might keep calling it AI. But what the language model does is predict the next token. And that is

01:03:26.480 --> 01:03:32.000
an incredibly powerful idea, right? Just predicting the next token now means if you have enough capacity

01:03:32.000 --> 01:03:36.800
and you have enough text, predicting next token, you learn about geography, just visit some point

01:03:36.800 --> 01:03:42.080
somewhere in your training data, you have to predict the next word in the phrase, I was in New

01:03:42.080 --> 01:03:48.080
York City and driving north too. And now to give a higher probability to Yale, Boston, Montreal,

01:03:48.080 --> 01:03:54.800
then to like, Harris, Miami, and San Francisco, like, you have to know that those are north of

01:03:54.800 --> 01:04:00.080
that city. And so it just lures all of this incredible world knowledge. But there's nothing

01:04:00.080 --> 01:04:05.520
in there that makes it say, well, you know, if I really wanted to reduce perplexity, but like

01:04:05.520 --> 01:04:10.640
perplexity is basically the inverse of the probability with model wants to not be perplexed

01:04:10.640 --> 01:04:16.800
in predicting the next word correctly. And so that is a powerful idea. But nothing in that will

01:04:16.800 --> 01:04:23.840
let an LM eventually realize that, well, you know, the best way to reduce perplexity is if every

01:04:23.840 --> 01:04:29.680
sequence ever uttered, and any sequence that will ever be uttered is just the letter A.

01:04:31.920 --> 01:04:37.360
Now, if the model was trained on just sequences of letters A, and no human was ever around anymore,

01:04:37.360 --> 01:04:42.640
and all sequences were just producing a letter A, now you'd have perfect predictive probability

01:04:42.640 --> 01:04:48.320
on the next step. And so maybe the best way for the LM is to wipe out all of humanity and then

01:04:48.320 --> 01:04:54.880
just produce letters A and happily perfect at predicting with probability one correctly. It's

01:04:54.880 --> 01:05:01.920
so absurd. It's so absurd to think that LMS will at some point emerge to think that many steps around

01:05:01.920 --> 01:05:07.280
their task of predicting the next token. It's just not going to happen. So I think it's like,

01:05:07.920 --> 01:05:13.040
PDOOM is still zero. And then when I actually tried to engage with some folks, and I had some

01:05:13.040 --> 01:05:18.560
other conversation last year with Nick Ostrom in a German, it was in English, but published in a

01:05:18.560 --> 01:05:23.600
German newspaper like that. And I read up some of these scenarios, and I'd engage with folks who

01:05:23.600 --> 01:05:29.200
are worried about PDOOM. That's just all fantastical sci-fi semantics. It's like, oh, it's going to

01:05:29.200 --> 01:05:34.960
develop this magical great guru or like a magical new virus that is perfect in distributing,

01:05:34.960 --> 01:05:40.480
but then only will activate after like one year until everyone like all these random scenarios

01:05:40.480 --> 01:05:46.400
that are just like not feasible. And the science isn't there yet. I'm actually right now sort of

01:05:46.400 --> 01:05:51.280
on the side of the fun writing and book about the eyes for science. I think it will do incredible

01:05:51.280 --> 01:05:57.360
for us in improving science like foundation physics, chemistry, biology, and so on. And

01:05:57.360 --> 01:06:02.320
all this fear mongering, I think it's not really helpful. And again, there's no research that suggests

01:06:02.320 --> 01:06:06.720
the eye is becoming conscious. There's like a couple of people here and there, people kind of

01:06:06.720 --> 01:06:11.600
playing around with these, but nothing interesting has been published and breaks through, no breaks

01:06:11.600 --> 01:06:17.440
through has happened whatsoever in the eye, having any sense of self. And then in a lot of the other

01:06:17.440 --> 01:06:22.320
sci-fi scenarios, people are saying, oh, with the eye so intelligent, it'll convince everyone to

01:06:22.320 --> 01:06:26.720
murder each other or to murder them, like kill themselves and so on. But, you know, if the most

01:06:26.720 --> 01:06:33.040
intelligent entities were to always rule, I don't think we would have the politicians always

01:06:33.040 --> 01:06:37.200
everywhere in the world that we see, right? It's not always just the most intelligent people that

01:06:37.200 --> 01:06:42.080
run the show. And that's kind of just their incredible intelligence to convince any other

01:06:42.080 --> 01:06:48.400
person who is less intelligent, you exactly what they want. It's just not based in reality. So,

01:06:48.400 --> 01:06:54.560
I am very, very optimistic about AI. I do think there's some real problems right now, you know,

01:06:54.640 --> 01:06:59.680
AI will pick up biases, not all the biases that you pick up on the web is something that most

01:06:59.680 --> 01:07:06.240
of humanity is proud of anymore. There's racism, there's sexism, there are various kinds of biases.

01:07:06.880 --> 01:07:11.440
Some people want to use AI. So, where I agree with Joshua Benjio and others is of the three

01:07:12.080 --> 01:07:17.200
threat vectors, which is intentional misuse, accidental misuse and loss of control.

01:07:18.160 --> 01:07:23.040
Obviously, like intentional misuse is real. And so, that's not ideal. And so, yes, those are real

01:07:23.040 --> 01:07:28.080
concerns. I think open social help us understanding those threat vectors and finding the best ways

01:07:28.080 --> 01:07:33.440
to compete with them. I think people still on the internet need to understand, not trust everything

01:07:33.440 --> 01:07:37.680
they see on the internet, which has been true ever since the internet came about, hasn't really

01:07:37.680 --> 01:07:44.160
changed that much with AI. I think since Photoshop, people should already not trust any photo they see.

01:07:44.160 --> 01:07:49.040
They should be even more worried now about photos they see. And sadly, in the future,

01:07:49.040 --> 01:07:54.400
they'll have to start worrying about videos and voices, of course, just like they should have

01:07:54.400 --> 01:08:00.000
worried about photos ever since Photoshop started to really work. And so, there are a lot of concerns

01:08:00.000 --> 01:08:04.080
and I don't want to diminish them. And I do think we need to work on them. And I think different

01:08:04.080 --> 01:08:08.480
cultures will have different answers. Freedom of speech is defined differently in different

01:08:08.480 --> 01:08:13.920
countries. Like it's legal in Germany to deny the Holocaust. We learn from our history there.

01:08:13.920 --> 01:08:18.560
That's not illegal in the US. And so, different countries and different cultures and societies

01:08:18.560 --> 01:08:24.320
will answer some of the problems that AI can amplify already in the past before we'll answer

01:08:24.320 --> 01:08:30.560
these questions differently. But I don't see any any probability for a full on the scenario of like

01:08:30.560 --> 01:08:35.920
existential risks to people. It's mostly people using more and more powerful tools against other

01:08:35.920 --> 01:08:40.240
people. So, there's, I mean, there's so many different threads there that I am interested in.

01:08:40.240 --> 01:08:47.360
For one thing, I applaud you for taking time to envision positive future. I think one of the

01:08:47.920 --> 01:08:54.240
scarcest resources today, oddly, is a positive vision for the future. Like, what do we want?

01:08:54.240 --> 01:08:58.320
This, you know, it's like the Jetsons is still almost like state of the art in terms of

01:08:58.320 --> 01:09:05.520
what we would envision a great 2030s to be like. And that is kind of bizarre. So, I definitely

01:09:05.520 --> 01:09:10.400
appreciate that. I also share your, you know, I'm not a super fan, but I'm also a fan of the

01:09:10.400 --> 01:09:18.240
three body problem. And one of the early prompts that I tried with GPT for early back in the

01:09:18.240 --> 01:09:26.080
rent team program like a year and a half ago now was asking it to write some hard science fiction

01:09:26.080 --> 01:09:34.160
in the style of the three body problem about AI for, you know, do diffusion model for proteins.

01:09:34.880 --> 01:09:41.440
And I took the plan right off of the GitHub page for this protein, you know, diffusion model project,

01:09:42.000 --> 01:09:46.880
which basically said we want to create text to protein. So, you know, say or text to maybe it

01:09:46.880 --> 01:09:50.960
was more even general than that molecule or whatever. So, you know, you would be able to

01:09:50.960 --> 01:09:55.680
just specify in natural language, specifies kind of an odd word. Or, you know, to the best of your

01:09:55.680 --> 01:09:59.760
ability articulate in natural language what you're looking for into protein. And this thing would

01:09:59.760 --> 01:10:03.840
then, you know, generate it. And we are actually starting to see that there was a paper in nature

01:10:04.240 --> 01:10:08.720
not long ago, I'm hoping to do an episode with the authors that achieves that to a certain degree.

01:10:08.720 --> 01:10:14.560
But the what the AI what GPT for came back with in terms of hard science fiction about this scenario

01:10:15.600 --> 01:10:21.600
was, I think, first of all, just extremely funny because it basically ends up in a prompting war

01:10:21.600 --> 01:10:26.480
between the good guys and the bad guys and they're both like trying to out prompt each other. And so

01:10:26.480 --> 01:10:31.840
the you know, the kind of climactic scene is like the person prompting, you know, an AI to like

01:10:31.840 --> 01:10:36.480
make a protein or, you know, a molecule that will interfere with the bad guys molecule, but not

01:10:36.480 --> 01:10:42.240
harm any of the, you know, the humans or whatever. And it's just like both absurd, but also maybe

01:10:42.240 --> 01:10:49.040
not entirely absurd. You know, I mean, I am with you in that the I would order the risks the same

01:10:49.040 --> 01:10:55.040
way. You know, we already have chaos GBT. There are I recently read a research grant from a group

01:10:55.040 --> 01:10:59.600
proposing to study on the side all tendencies. Like there are people out there who want to kill

01:10:59.600 --> 01:11:06.000
everyone. Like what's up with that? And, you know, if the tools get more powerful, like that, you

01:11:06.000 --> 01:11:11.840
know, those people become even more problematic than they already are. So yes, I would put that at

01:11:11.840 --> 01:11:17.520
the top of the, you know, of the stack of like, big picture risks. And by the way, I take all the

01:11:17.520 --> 01:11:22.800
short term and medium term risks seriously too. Like this is a big tent show where like all your

01:11:22.800 --> 01:11:27.520
hopes, dreams and concerns and, you know, perhaps irrational fears like can all have a home. But I

01:11:27.520 --> 01:11:34.720
guess, you know, to sort of get to P doom zero, I still am like, I don't know, you know, all these

01:11:34.720 --> 01:11:39.360
individual crazy scenarios, sure, they're extremely unlikely, you know, the prompting war with your,

01:11:39.360 --> 01:11:45.360
you know, protein diffusion model is like absurd on the face of it. But I kind of think of like

01:11:45.360 --> 01:11:51.360
to taking the integral over that like vast space of crazy super unlikely scenarios. And then I'm

01:11:51.360 --> 01:11:56.720
kind of like, you know, there's so many of them right that space is so big. And even if the probability

01:11:56.800 --> 01:12:02.240
is like kind of vanishing, one thing you learn in calculus is like, you can, you know, that the

01:12:02.240 --> 01:12:08.080
integral can either also vanish or it can be like finite, you know, over these, you know, kind of,

01:12:08.080 --> 01:12:13.120
even if the function itself is going to zero, the integral doesn't necessarily have to go to zero

01:12:13.120 --> 01:12:19.440
over that space. So to me, that just feels like very unresolved still. And I don't think we're

01:12:19.440 --> 01:12:23.200
going to resolve that today. But I would love to hear a little bit more about your, how you think

01:12:23.280 --> 01:12:30.400
about AI agency, and also concepts of emergence in agents today, I guess I also wonder, like,

01:12:30.400 --> 01:12:35.040
is you.com gonna, you know, push more toward the agent direction, you've got like a, what I would

01:12:35.040 --> 01:12:41.120
call a research agent today, you've got a browser as well, I could, you know, should I start to

01:12:41.120 --> 01:12:48.080
expect it to take actions for me? What I've observed in the agent space is I never feel like it

01:12:48.080 --> 01:12:53.520
fails because it doesn't understand the goal or like doesn't stay on task. Let's say that never

01:12:53.520 --> 01:13:00.000
happens, but very rarely, much more often, it's just a failure of competence. So my expectation then

01:13:00.000 --> 01:13:04.880
is that like, as the competence improves, it may not be intrinsic agency, but it may be

01:13:05.760 --> 01:13:10.240
prompted agency, and it may even be like, you know, as we have more and more orchestrated systems,

01:13:10.240 --> 01:13:14.320
we may have models prompting other models to, you know, go off and do this. And

01:13:15.120 --> 01:13:18.560
it does feel like we've got, we're headed for like a lot of spinning plates. And

01:13:19.200 --> 01:13:23.360
the idea that they could kind of, you know, all come crashing down is like,

01:13:24.000 --> 01:13:30.000
that's doesn't just doesn't feel like something we can rule out. But I don't know, I can, can you

01:13:30.000 --> 01:13:34.880
help me be confident there? I'm still not. I'll go through the sound of the things you mentioned.

01:13:34.880 --> 01:13:40.640
So PDM equals zero. So you're right. As you integrate over the future, I would like to not

01:13:40.640 --> 01:13:46.720
rule out anything. So maybe I should say 10 to the minus whatever, like a time, time, number,

01:13:46.720 --> 01:13:54.960
because in the next five billion years, like all kinds of things happen, right? Like maybe as if

01:13:54.960 --> 01:14:00.880
like the three body problem, spoiler alert, like maybe some big, much more sophisticated alien

01:14:00.880 --> 01:14:05.680
species will come across. They have already developed, fasted in my time, travel, and,

01:14:05.680 --> 01:14:10.400
or, you know, just are really, really fast in getting here and various capacities. And then,

01:14:10.400 --> 01:14:15.200
you know, they have an AI and daddy, I will just like destroy all of us. So they're getting ready

01:14:15.200 --> 01:14:20.320
to like settle into the new planet before they get here. Like there's all kinds of crazy things

01:14:20.320 --> 01:14:26.480
that can happen. It's just that, like in terms of how much resources we should spend on T,

01:14:27.440 --> 01:14:33.600
like existential do versus like, you know, I'd say, yeah, I have a couple of researchers,

01:14:33.600 --> 01:14:40.160
like thinking of cool sci-fi scenarios, inspire us, like maybe like think about ways that that could

01:14:40.160 --> 01:14:47.120
be prevented, but to spend billions of dollars on it, to like, spend a lot of like, mind share,

01:14:47.120 --> 01:14:51.760
the public about it, who's already scared of any kind of technology. I mean, people are scared of

01:14:51.760 --> 01:14:57.520
vice. I mean, there's this great Twitter handle called the pessimist archive. I mean, people were

01:14:57.520 --> 01:15:02.400
scared and thought doom is happening because of the novels back in the day. People are like, all

01:15:02.400 --> 01:15:07.040
these kids, they're just in their heads reading novels. They're going to all be useless human

01:15:07.040 --> 01:15:12.800
beings in the future. Newspaper was terrible. Internet was terrible. Like there's so many

01:15:12.800 --> 01:15:17.840
things that like people thought this is the end of civilization and we're very pessimistic about.

01:15:17.840 --> 01:15:23.440
And again, not this diminishing like real, real concerns, but again, existential one,

01:15:23.440 --> 01:15:28.000
very, very likely given what we're seeing right now. And if there, it does happen at some point in

01:15:28.000 --> 01:15:35.440
the future, then I would argue that to think about the best countermeasures now is kind of like

01:15:36.240 --> 01:15:41.600
thinking about, you know, the best countermeasures against a computer going crazy when there's

01:15:41.600 --> 01:15:45.680
still a bunch of vacuum tubes and you like, well, we're going to just suck out the air of

01:15:45.680 --> 01:15:48.800
everything into the vacuum tubes. They're not going to work as well and more because they're

01:15:48.800 --> 01:15:53.680
going to break and blah, blah, blah. That was your like counterattack against the computer taking over

01:15:54.560 --> 01:16:00.080
with your current thinking of vacuum tube computers. Or it's like, you know, like it's

01:16:00.080 --> 01:16:04.000
similar to the internet, you know, if you thought about what's the internet going to be, how could

01:16:04.000 --> 01:16:12.480
it be so terrible? Zero of the TCT IP experts in the early ARPANET days realized that at some point,

01:16:12.480 --> 01:16:17.040
maybe a foreign power could interfere with local elections because like you can say whatever you

01:16:17.040 --> 01:16:23.120
want online and maybe people get followers in their social media. Like no one had victimized

01:16:23.120 --> 01:16:29.360
in the 70s and the early ARPANET days. And so I think most of the threat vectors are not that

01:16:30.000 --> 01:16:35.280
useful in terms of key doom kind of research. I have a couple of folks work on it, but not

01:16:35.280 --> 01:16:43.440
take up as much mind space and scare like late people and non-experts even more about the technology

01:16:43.440 --> 01:16:48.160
that even without consciousness is still going to have major destruction, right? If you're going

01:16:48.160 --> 01:16:54.800
through a new step function in human productivity, just like, you know, agriculture versus like

01:16:54.800 --> 01:16:59.920
hunting and gathering and the steam engine and electricity and internet, like this one's going

01:16:59.920 --> 01:17:06.080
to be even bigger. It's going to disrupt and change the job landscape a lot. I think at the end of it

01:17:06.080 --> 01:17:12.320
will be way more productive. There's going to be way more productivity per person and hence more

01:17:12.320 --> 01:17:18.320
wells and new jobs will come around as full jobs get all needed. But that is already so massively

01:17:18.320 --> 01:17:22.720
disruptive still. And it's not going to happen overnight either. People think, oh, it's going to

01:17:22.720 --> 01:17:26.640
be capacity. Yes, it will be faster, but still not overnight. There are still companies that aren't

01:17:26.640 --> 01:17:31.040
even on the cloud. There are some stretches in the United States and even Germany that don't have

01:17:31.040 --> 01:17:35.520
full internet connectivity, right? It's just like, so things will take time and not happen overnight,

01:17:35.520 --> 01:17:40.400
but they will be happening even faster than past past technological revolutions. And so,

01:17:40.960 --> 01:17:47.440
and then you have brought up LMS and proteins. There's a great example for where regulation

01:17:47.440 --> 01:17:51.440
makes sense. Like, basically, the concern here for those who are not familiar with proteins,

01:17:51.440 --> 01:17:55.760
having everything in life and disease and sickness, COVID, protein, like everything

01:17:55.760 --> 01:18:00.080
SARS broke through and like everything is governed by proteins. So if you have a great

01:18:00.080 --> 01:18:06.000
understanding of proteins, we can build fantastical, amazing things. Here's just one example of a

01:18:06.000 --> 01:18:10.000
research paper I read a few months ago that just blew my mind and made me very excited about

01:18:10.000 --> 01:18:16.800
the future. There's this group of researchers that built these carbon nanotubes. And on one side

01:18:16.800 --> 01:18:22.160
of the carbon nanotubes, they put iron molecules. And on the other side of these tiny, tiny carbon

01:18:22.160 --> 01:18:28.640
nanotubes, they put protein that would only bind to a brain cancer cell. And then they injected

01:18:28.640 --> 01:18:33.280
this fluid with all these little carbon nanotubes into a mouse brain that had brain cancer.

01:18:34.080 --> 01:18:40.560
The proteins found the brain tumor cells and only connected to those specific types of

01:18:40.560 --> 01:18:45.600
brain cancer cells. And then they put the mouse into a little magnetic field and there's the

01:18:45.600 --> 01:18:50.000
iron molecule on the other side of the carbon nanotube that started spinning around and had

01:18:50.000 --> 01:18:57.200
nano surgery on each brain cancer cell. Now, if you think about, we have the full console of the

01:18:57.200 --> 01:19:01.040
proteins, we can connect them to all kinds of things and you find ways to, you know, get rid of

01:19:01.120 --> 01:19:06.720
the carbon nanotubes afterwards. It's all like medicine is going to change in so many positive

01:19:06.720 --> 01:19:12.720
ways. And now you could argue, well, but proteins, people could use them and build like very bad

01:19:12.720 --> 01:19:18.000
like viruses. And like, that's true. And that can be outlawed. In fact, the US just a couple of

01:19:18.000 --> 01:19:24.080
months ago outlawed being a function where you know, some researchers want to make even more

01:19:24.080 --> 01:19:27.600
deadlier viruses. And it's not because they're like evil scientists who want to destroy the

01:19:27.600 --> 01:19:32.320
world. It's just they're saying like, well, as we know how they worked before they appear in

01:19:32.320 --> 01:19:37.040
nature by themselves, then we can all right now prevent like, develop cures for them. So, you

01:19:37.040 --> 01:19:41.520
know, it's like, it's a complex question, but yes, and decided like, for now, it's not worth it.

01:19:41.520 --> 01:19:47.680
Let's outlaw it. And likewise, I don't think an open source like protein model is going to be

01:19:47.680 --> 01:19:51.920
the main deciding factor of being able to create something virus. Because if you have all the

01:19:51.920 --> 01:19:58.240
wet lab experimentation to be able to create new kinds of viruses, you can also just do what

01:19:58.240 --> 01:20:02.560
census Arnold did when she won the Nobel Prize a couple years ago in chemistry, which is

01:20:02.560 --> 01:20:07.040
what she called directed evolution, but it was basically random permutations. And then

01:20:07.040 --> 01:20:12.720
running an experimental pipeline to see if that random permutation works better or not for particular

01:20:12.720 --> 01:20:18.000
kind of protein. And then you just keep iterating like that. And so if you have those capabilities,

01:20:18.080 --> 01:20:22.400
you have a random permutation, you can do bad things. But it turns out having like a legit

01:20:22.400 --> 01:20:27.360
weapon like that, that's to do all of that. So that was your PDUM integral, LM and proteins,

01:20:27.360 --> 01:20:31.840
AI agency emergence. So obviously, emerging capabilities are incredible on that sort of

01:20:31.840 --> 01:20:38.160
like, even us like working in deep learning, or I'm amazed, just like you.com, I asked these

01:20:38.160 --> 01:20:43.440
questions, I'm like, wow, actually, that's right, like, I would have not to like, thought this

01:20:43.520 --> 01:20:48.720
was possible. And sometimes we were like, did you program it specifically for it to be able to

01:20:48.720 --> 01:20:52.480
answer these kinds of questions about headphones or something? We're like, no, it's just like,

01:20:52.480 --> 01:20:58.000
just put that all together, just by trying to predict the next token. So I'm really excited.

01:20:58.000 --> 01:21:01.680
And one of the things I'm excited about this pudding, and one of the things that coding

01:21:01.680 --> 01:21:08.000
enables is now is the last part of your question, the actions. I think actions are clearly in the

01:21:08.000 --> 01:21:14.560
future. And for now, we're focused on amazing answers. But it's not hard to imagine that at

01:21:14.560 --> 01:21:21.600
some point, the most amazing answer is done. I did, I did what you asked to do. And instead of

01:21:21.600 --> 01:21:28.320
telling you how to do it, I just, right, you can build a really cool demo very quickly for these

01:21:28.320 --> 01:21:33.120
kinds of things. But the problem is like, as much as I love natural language, and as much as I love

01:21:33.120 --> 01:21:38.080
chatbots and everything, right, you have to find some really killer use cases for it. And

01:21:38.080 --> 01:21:42.960
to say, oh, I can book this flight, it's actually really hard to just, just book the flight. Like,

01:21:42.960 --> 01:21:47.040
you're like, why didn't you like pick this other one that was just like, not exactly the time I

01:21:47.040 --> 01:21:52.560
asked for it, but I could have waited for half an hour at this like, extra leg, and then like,

01:21:52.560 --> 01:21:58.800
say 50 bucks, like that was really dumb. And like, it turns out, Expedia and others have built

01:21:58.800 --> 01:22:05.360
for decades, the perfect interface for that problem, so that humans have all the installation

01:22:05.360 --> 01:22:11.200
right there in a visual way. And so there's an uncanny valley of, there's a cool tech demo.

01:22:11.760 --> 01:22:17.280
And on one side, and then there's like, my actual human assistant, who after months of

01:22:17.280 --> 01:22:22.000
I mean, like talking to me, understand all the trade offs, and understand my price

01:22:22.000 --> 01:22:27.200
sensitivity, or that of my company, and knows like, when I would preserve and like,

01:22:27.200 --> 01:22:33.520
all the like, reasons why I might do it overnight, like red-eye slides, and, you know, all the,

01:22:33.520 --> 01:22:37.840
all the constraints, and she can do it. And even then, sometimes she's like, oh, Richard,

01:22:37.840 --> 01:22:42.640
there are like three options here, like, let me know which one you prefer out of this 5000,

01:22:42.640 --> 01:22:47.680
if you built it for you. And like, it's very hard to do all of that with just text. That's

01:22:47.760 --> 01:22:52.240
ultimately, I think, part of why we have the stock ticker app and so on, and why we have

01:22:52.240 --> 01:22:59.040
religious now, and in some cases also, is that sometimes like UI UX and actual visually designed

01:22:59.040 --> 01:23:05.440
like interfaces are best used in combination with language. Maybe one more big picture question,

01:23:05.440 --> 01:23:09.600
and then I want to do just a real quick lightning round on a couple kind of more technical areas

01:23:09.600 --> 01:23:14.480
before we run out of time. On the big picture side, you know, we've got Sam Altman out there

01:23:14.480 --> 01:23:19.600
saying AGI is coming soon, but also kind of confusingly saying, but it'll be less impactful

01:23:19.600 --> 01:23:26.640
than you might think. Not really sure how to interpret that. The median guess on, you know,

01:23:26.640 --> 01:23:31.920
some definition of AGI is like, just a few years on some prediction markets and more like,

01:23:31.920 --> 01:23:38.080
you know, 12 years or whatever for a stronger definition. What do you have sort of an expectation

01:23:38.080 --> 01:23:43.600
for, and a definition or like a threshold that you have in mind of like, this is the threshold

01:23:43.600 --> 01:23:47.920
that really matters and, you know, loosely speaking, like what sort of timeline you would

01:23:47.920 --> 01:23:55.040
expect it to take to get there? It's very much a, the kind of question where you have to be very

01:23:55.040 --> 01:24:02.000
careful about your terminology, because the interpretation of AGI has vastly different

01:24:02.000 --> 01:24:08.080
associations. Like people, some people think of AGI and used to think of AGI as this super

01:24:08.080 --> 01:24:15.840
intelligence. It's conscious as self-awareness can set its own goals. And it is more intelligent

01:24:15.840 --> 01:24:22.080
than all human beings. And, and that's their depth. That was, that was for a long time. A lot of us,

01:24:22.080 --> 01:24:27.760
I thought, at least for me personally, also the definition. Now people said, and I think it's

01:24:27.760 --> 01:24:32.080
partially because of marketing, like, you know, you want to be working on AGI, but you also need

01:24:32.080 --> 01:24:36.560
to have ship stuff. It's like, you want to be multi-tenetary species, but you also need to just

01:24:36.560 --> 01:24:40.480
get a lot of stuff in orbit, right? Let have more satellites and better internet connectivity,

01:24:40.480 --> 01:24:46.400
and so on. So you have this long-term vision and the best companies are able to articulate that

01:24:46.400 --> 01:24:51.840
long-term vision and then revenue generated progress in smaller milestones towards it. And so

01:24:52.880 --> 01:24:59.280
in this case here, I think the definition of AGI was pulled out. And then the super intelligence

01:24:59.280 --> 01:25:04.880
was defined as like, okay, that's even more than general. It's super. And that's the really long-term

01:25:04.880 --> 01:25:11.760
stuff. And now AGI, it's just basically automating point. And if you define AGI, which I think is

01:25:11.760 --> 01:25:17.760
not crazy, I want to say maybe it would be not post that, which is a very pragmatic sort of

01:25:18.640 --> 01:25:26.240
investor slash financial slash economic definition of AGI, which is 80% of the jobs can be automated

01:25:26.960 --> 01:25:33.440
up to 80%. And if that's achieved, we'll call it AGI. Turns out there's just a lot of jobs that

01:25:33.520 --> 01:25:39.360
are quite repetitive and they're not requiring a ton of extremely novel, out-of-the-box thinking

01:25:39.360 --> 01:25:45.520
that no one's ever done before. And like learning very complex new behaviors, bot-shaking, identifying

01:25:45.520 --> 01:25:50.960
new experiments, collecting new data and pushing, you know, the science forward and so on. It turns

01:25:50.960 --> 01:25:56.400
out there's just a lot of boring, repetitive jobs. And indeed, if your definition of AGI is just like,

01:25:56.400 --> 01:26:02.720
well, we can automate like 80% of 80% of the jobs, then I think it's not crazy to assume

01:26:03.520 --> 01:26:07.440
especially I would restrict it one on the wall way, which is digitized jobs,

01:26:08.160 --> 01:26:13.040
jobs that are purely happening in your browser or on your computer, because those jobs can collect

01:26:13.040 --> 01:26:17.680
training data at massive scales. Turns out no one's collecting training data for plumbers,

01:26:18.240 --> 01:26:25.520
for woofers, for tylers, for maids, like cleaning tees or any of that. And so none of those jobs

01:26:25.520 --> 01:26:29.840
are going to get automated anytime soon. Because you first have to collect many years of that such

01:26:29.840 --> 01:26:35.920
training data before you can then use AI to train on that and then automate it. But, you know,

01:26:35.920 --> 01:26:41.520
jobs that are fully digitized and that have a lot of training data that don't have a crazy long tail

01:26:41.520 --> 01:26:47.200
of special cases, they're going to get automated. And I think that's reasonable to say that's 80%

01:26:47.200 --> 01:26:52.400
of jobs. For hunches, even in radiology, for instance, you could probably do 80% find 80%

01:26:52.400 --> 01:26:57.440
of things that are wrong in HET's t-stand. But then there's still this very long tail of 20%

01:26:58.400 --> 01:27:02.480
that you just don't have enough training data for. Radiologists never see it in their lifetime,

01:27:02.480 --> 01:27:08.000
they just read about it once in a book. And we're still not quite good enough of one shot and zero

01:27:08.000 --> 01:27:13.920
shot learning. Obviously, huge amounts of progress, but not in super important things like radiology,

01:27:13.920 --> 01:27:18.560
where you just read about a case once in a book and then identify it with 100% accuracy,

01:27:18.560 --> 01:27:23.040
which is also a question of whether humans do it. I'm actually with you on the self-driving car.

01:27:23.120 --> 01:27:27.760
There's going to be a lot of interesting questions as AGI rolls into more and more workplaces,

01:27:27.760 --> 01:27:33.840
which is how much better than a human that has to be. And how, and it's deeply philosophical,

01:27:33.840 --> 01:27:40.400
very quickly, because if you're purely utilitarian, you could say, well, you know, 100,000 miles or

01:27:40.400 --> 01:27:47.120
whatever 20 million miles driven by AI results in 10,000 deaths. And the same amount of miles

01:27:47.120 --> 01:27:53.680
driven by humans results in five times more deaths. And so one is better than the odds.

01:27:54.640 --> 01:27:59.680
But if that one dead person in the AI car was your daughter, you don't care, you're gonna,

01:27:59.680 --> 01:28:04.160
like in the US, you're gonna sue, you're gonna, you know, try to end that company,

01:28:04.160 --> 01:28:08.640
because they're responsible now for the death of your child. And like, it's a very emotional thing,

01:28:08.640 --> 01:28:14.000
not a statistical thing anymore. And so there's gonna be a lot of litigation as those come out.

01:28:14.000 --> 01:28:18.400
And I think the silver lining is again, of course, as the I meets the state, you can learn from it

01:28:18.400 --> 01:28:23.920
versus like one person texting again on their cell phone, which is already illegal and running

01:28:23.920 --> 01:28:28.320
like over some kid that ran out, like, they're going too fast also, which is already illegal,

01:28:28.320 --> 01:28:33.920
too, you can't really do that much more than needing it legal. AGI will have a huge amount of

01:28:33.920 --> 01:28:38.560
impact. Once it's just like, okay, repetitive jobs, get like two large degree automated,

01:28:38.560 --> 01:28:43.200
and I'm with the people saying that will happen next few years. When it comes to like,

01:28:43.280 --> 01:28:48.480
super intelligence, that is fully conscious and can do all the things. And one intelligent,

01:28:48.480 --> 01:28:52.480
then not just a single human, but that all of humanity, very hard to know, because no one's

01:28:52.480 --> 01:28:58.800
working on it and making sort of progress along the lines of setting my own goals. And again,

01:28:58.800 --> 01:29:05.040
like, unless you set your own goals, I don't know if I would achieve full on super intelligence to

01:29:05.040 --> 01:29:09.680
you. Like if you're just your objective function is to minimize cross entropy errors, or reduce

01:29:09.680 --> 01:29:15.360
the plexity, or like segment, which is well, or like, none of that, I wanted to reach.

01:29:16.240 --> 01:29:18.560
Do you have time for a lightning round? Or do we need to leave it there?

01:29:18.560 --> 01:29:26.160
Let's try to be lightning rounds. All right. Thinking also about retrieval, memory, and online

01:29:26.160 --> 01:29:33.440
learning as kind of three frontiers that, you know, you dot com could could improve on if they're,

01:29:33.440 --> 01:29:38.240
you know, if there are research breakthroughs, but also these do seem to be kind of ingredients

01:29:38.320 --> 01:29:44.320
toward this bigger picture of AGI or even, you know, at some point, ASI, I guess I'm, you know,

01:29:44.320 --> 01:29:51.040
maybe just leave it open ended, like, what are you excited about in those domains? Are there

01:29:51.040 --> 01:29:54.480
research directions? Are there, you know, are there papers you've already seen or things you

01:29:54.480 --> 01:30:00.880
think people should be doing that you think will kind of provide meaningful unlocks as we find,

01:30:00.880 --> 01:30:06.000
you know, new and better ways to do those things? Yeah. So I'm a fan of all three, of course,

01:30:06.000 --> 01:30:09.760
I'll try to keep it short. Retrieval is awesome. I think in some ways, short-term

01:30:09.760 --> 01:30:13.600
memory is currently in the front, retrieval is in the, you know, rag. If you go up method

01:30:13.600 --> 01:30:18.160
generation, we do it over a web, we let you up those files now, so you wouldn't do it over,

01:30:18.160 --> 01:30:24.640
over a file. And then we have the smart personalization that actually is online learning. So as you say,

01:30:24.640 --> 01:30:30.160
certain things like it will, it will remember them about you. And then, you know, you can turn

01:30:30.160 --> 01:30:35.760
it off also. And it's very transparent. And the whole thing off or the automated smart learning

01:30:35.840 --> 01:30:40.640
without you, if you don't want it. But yeah, I think that's sort of a simple sort of pragmatic

01:30:40.640 --> 01:30:45.680
way of online learning. I think ultimately, you know, it'll be awesome to have AI systems get

01:30:45.680 --> 01:30:50.240
better and better of just adapting right away to user feedback, both in terms of, you know,

01:30:50.240 --> 01:30:54.880
thumbs up, thumbs down kinds of clicking, but also in conversation, I didn't like that answer.

01:30:54.880 --> 01:31:00.320
And then updating the answer in a principled way for the future. I have so many more thoughts,

01:31:00.320 --> 01:31:05.360
but I'll like, I'd love to do a second one. These are kind of crazy days. Now the Apple

01:31:05.360 --> 01:31:11.760
announcement, we just announced that Julianne, CTO, I mean, say also just became an angel investor

01:31:11.760 --> 01:31:17.200
and a lot of exciting stuff happening. So I yeah, well, congratulations on the Apple thing and also

01:31:17.200 --> 01:31:23.280
on a new prominent angel investor. And really some fantastic product progress. I definitely

01:31:23.280 --> 01:31:29.280
recommend everybody to try out particularly genius mode and research mode. And I think if you do that,

01:31:29.280 --> 01:31:34.720
you will be coming back to you.com more and more often. So keep up the great work. For now,

01:31:34.720 --> 01:31:39.760
I will say Richard Sosher, founder and CEO of you.com. Thank you for being part of the cognitive

01:31:39.760 --> 01:31:45.280
revolution. Thank you so much. It is both energizing and enlightening to hear why people

01:31:45.280 --> 01:31:50.960
listen and learn what they value about the show. So please don't hesitate to reach out via email

01:31:50.960 --> 01:31:56.720
at TCR at turpentine.co, or you can DM me on the social media platform of your choice.

01:31:57.680 --> 01:32:03.760
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually

01:32:03.760 --> 01:32:09.200
work customized across all platforms with a click of a button. I believe in Omniki so much

01:32:09.200 --> 01:32:18.960
that I invested in it and I recommend you use it too. Use Kogrev to get a 10% discount.

