start	end	text
0	21480	Hello and welcome.
21480	28420	It's active inference gas stream number 51.1 on July 28th, 2023.
28420	34520	We are here with Tomaso Salvatore and we will be having a presentation and a discussion
34520	39420	on the recent work, causal inference via predictive coding.
39420	42160	So thanks so much for joining.
42160	47220	For those who are watching live, feel free to write questions in the live chat and off
47220	48220	to you.
48220	49220	Thank you.
49220	54700	Thank you very much, Daniel, for inviting me.
54700	59540	It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite
59540	64500	excited to be here and be the one speaking this time.
64500	69540	So I'm going to talk about this recent preprint that I put out, which has been the work of
69540	72620	the last couple of months.
72620	82220	And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz.
82220	87820	It's basically a joint work between Versus, which is the company I work for, the University
87820	91860	of Oxford and Theo Vien.
91860	102040	So during this talk, I will, this is basically the outline of the talk, I will start talking
102040	109340	about what predictive coding is and give an introduction of what it is, a brief historical
109340	115820	introduction, why I think it's important to study predictive coding, even for example
115820	118740	for the machine learning perspective.
118740	126100	I will then provide a small intro to what causal inference is.
126100	132300	And once we have all those informations together, I will then discuss why I wrote this paper,
132300	138820	what was basically the research question that inspired me and the other collaborators.
139820	147380	And present the main results, which are how to perform inference, so intervention and
147380	154980	counterfactual inference, and how to learn the causal structures from a given data set
154980	157060	using predictive coding.
157060	163980	And then I will of course conclude with a small summary and some discussion on why I
164020	168180	believe this work can be impactful in some future directions.
170780	173460	So what is predictive coding?
173460	178780	Predictive coding is in general famous for being a neuroscience inspired learning method,
178780	183300	so a theory of how information processing in the brain works.
184660	191380	And brain formally speaking, the theory of predictive coding can be described as basically
191380	198860	having a hierarchical structure of neurons in the brain and you have two different families
198860	200980	of neurons in the brain.
200980	207540	The first family is the one in charge of sending prediction information, so neurons in a specific
207540	216420	level of the hierarchy send information and predict the activity of the level below.
216420	220100	And the second family of neurons is that of error neurons.
220100	225140	And the error neurons, they send prediction error information up the hierarchy.
225140	229580	So one level predicts the activity of the level below.
229580	234460	This activity has some, this prediction has some mismatch, which we were actually going
234460	236540	on in the level below.
236540	241620	And the information about the prediction error gets sent up the hierarchy.
242540	250540	However, predictive coding was actually not burned as a neuroscience, as a theory from
250540	255740	neurosciences, but it was actually initially developed as a method for signal processing
255740	258500	and compression back in the 50s.
258500	266980	So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that
266980	274660	once we have a predictor, a model that works that is well in predicting data, sending messages
274660	281100	about the error in those predictions is actually much cheaper than sending the entire message
281100	283460	every time.
283460	289300	And this is how predictive coding was born, so as a signal processing and compression
289300	293900	mechanism in information theory back in the 50s.
293900	304060	He was actually in the 80s, that he became that exactly the same model was used in neuroscience.
304060	311500	And so with the work from Mumford or other works, for example, explain how the rate enough
311500	315980	processing formation, so we get prediction signals from the outside world, and we need
315980	323020	to compress these representation and have this internal representation in our neurons.
323020	329660	And the method is very similar, if not equivalent to the one that was developed by Elias and
329660	333420	Oliver in the 50s.
333420	339900	Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul
339900	347100	and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical
347100	353980	structures in the brain, where prediction information is top down and error information
353980	355860	is bottom up.
355860	362340	And something that they did that wasn't done before is that they explain and develop this
362340	368700	theory about not only inference, but also about how learning works in the brain.
368700	373660	So it's also a theory of how our synapses get updated.
373660	379380	And the last big breakthrough that I'm going to talk about in this brief historical introduction
379380	388620	is from 2003, but then it kept going in the years after, thanks to Carfriston, in which
388620	396780	basically he took the theory of Raoul and Ballard, and he extended it and generalized
396780	400740	it to the theory of generative models.
400740	407140	So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization
407140	415340	scheme of a specific kind of generative model, which I'm going to introduce later as well.
415340	424220	So to make a brief summary, the first two kinds of predictive coding that I described,
424220	428980	so signal processing and compression and the information processing in the retina and in
428980	432820	the brain in general, they are inference methods.
432820	441060	And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st
441060	445100	century, is that predictive coding was seen as a learning algorithm.
445100	451180	So we can first compress information and then update all the synapses or all the latent
451180	457540	variables that we have in our generative model to improve our generative model itself.
458540	464660	So let's give some definitions that are a little bit more formal.
464660	470820	So predictive coding can be seen as a hierarchical Gaussian generative model.
470820	475500	So here is a very simple figure in which we have this hierarchical structure, which can
475500	478860	be as deep as we want.
478860	485580	And prediction signals go from one latent variable, Xn, to the following one, and it
485580	490460	gets transformed every time via function gn, or gi.
490460	499540	And this is a generative model, as I said, and what's the marginal probability of this
499540	500540	generative model?
500540	506620	Well, it's simply the probability of the last, can you see my cursor?
506620	507620	Yes, right?
507620	508620	Yes, perfect.
508620	515140	So it's the generative model of the last vertex, is the distribution of the last vertex, times
515140	520740	the probability distribution of every other vertex, conditioned on the activity of the
520740	526060	vertex before, or the latent variable before.
526060	530980	I earlier said that it's a Gaussian generative model, which means that those probabilities
530980	542380	they are in Gaussian form, and those function g, in general, and especially since, for
542380	547060	example, in a round baller paper, and in all the papers that came afterwards, also because
547060	554580	of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with
554580	563580	activation functions, or nonlinear maps with activation function and an additive bias.
563580	569420	So we can give a formal definition of predictive coding, and we can say that predictive coding
569420	575180	is an inversion scheme for such a generative model, where its model evidence is maximized
575180	581180	by minimizing a quantity that is called the variational free energy.
581180	586420	In general, the goal of every generative model is to maximize model evidence, but this quantity
586420	595420	is always intractable, and we have some techniques that allow us to approximate the solution.
595420	602300	And the one that we use in predictive coding is minimizing a variational free energy, which
602300	605580	is a lower bound of the model evidence.
605580	612340	In this work, and actually in a lot of other ones, so is the standard way of doing it,
612340	619860	this minimization is performed via gradient descent, and there are actually other methods
619860	624780	such as expectation maximization, which is often equivalent, or you can use some other
624780	631300	message-passing algorithms such as belief propagation, for example.
631300	636380	And going a little bit back in time, so we're getting a little bit about the statistical
636380	644460	generative models, we can see predictive coding, as I said already a couple of times, as a
644460	650620	hierarchical model with neural activities, so with neurons, latent variables that represent
650620	656780	neural activities, they send their signal down the hierarchy, and with error nodes or
656780	662300	error neurons, they send their signal up the hierarchy, so they send the error information
662300	663300	back.
663300	668020	What's the variational free energy of these class-operated coding models?
668020	675820	It's simply the sum of the mean square error of all the error neurons, so it's the sum
675820	682460	of the total error squared.
682460	688180	And this representation is going to be useful in the later slides, and I'm going to explain
688180	691580	how to use predictive coding to model causal inference, for example.
691580	697100	What do you think predictive coding is important and is a nice algorithm to study?
697100	701260	Well, first of all, as I said earlier, it optimizes the correct objective, which is
701260	708300	the model evidence or marginal likelihood, and then it does so by optimizing a lower bound,
708300	712620	which is called the variational free energy, as I said, and the variational free energy
712620	718900	is interesting because it can be written as a sum of two different terms, which are
718900	726300	and each of those terms optimizing it as important impacts, for example, in machine learning
726300	729300	tasks or in general in learning tasks.
729300	732660	So one of those terms forces memorization.
732660	739860	So the second term basically tells forces the model to fit a specific data set.
739860	744860	And the first term forces the model to minimize the complexity.
744860	751060	And as we know, for example, from the Occam's razor theory, if we have two different models
751060	755980	that perform similarly on a specific training set, the one that we have to get and the one
755980	761340	that is expected to generalize the most is the less complex one.
761340	768860	So updating generative model via variational free energy allows us to basically converge
768860	775900	to the optimal Occam razor model, which both memorizes a data set, but is also able to
775900	781020	generalize very well on unseen data points.
781020	789980	A second reason why predictive coding is important is that it actually doesn't have
789980	795500	to be defined on a hierarchical structure, but it can be modeled on more complex and
795500	802100	flexible architectures such as directed graphical model with any shape or generalized even more
802100	805580	to networks with a lot of cycles that resemble brain region.
805580	811500	And the underlying reason is that you're not learning and predicting with a forward
811500	816900	pass and then back propagating the error, but you're minimizing an energy function.
816900	823780	And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and
823780	826580	allow to learn cycles.
826580	830700	And this is actually quite important because the brain is full of cycles as we have some
830700	838060	information from some recent papers that may manage to map completely the brain of some
838060	840980	animals such as fruit fly.
840980	842380	The brain is full of cycles.
842380	851020	So it makes sense to train our machine learning models or our models in general with an algorithm
851020	857500	that allows us to train using cyclic structures.
857500	862100	The third reason why predictive coding is interesting is that it has been formally proven
862100	866340	that it is more robust than standard neural networks trained with back propagation.
866340	871900	So if you have a neural network and you want to perform classification tasks, you, predictive
871900	874500	coding is more robust.
874500	880340	And this is interesting in tasks such as online learning, training on small datasets
880340	882900	or continuous learning tasks.
882900	888300	And the theory basically comes from the fact that imperative coding has been proved to
888300	893980	approximate implicit gradient descent, which is a different version of the explicit gradient
893980	900100	descent, which is the standard gradient descent used in the, in every single model basically.
900100	906020	And it's a variation that is more robust.
906020	908900	I think, okay, I did a quite a long intro to predictive coding.
908900	915020	I think I'm now moving to the second topic, which is causal inference and what's causal
915020	916020	inference?
916020	921660	Causal inference is a theory, is a very general theory that has been formalized the most by
921660	922660	Judea Perl.
922660	926740	He's definitely the most important person in the field of causal inference.
926740	928780	He wrote some very nice books.
928780	935700	For example, the book of why is highly recommended if you want to learn more about this topic.
935700	938860	And it basically tackles the following problem.
938860	942780	So let's assume we have a joint probability distribution, which is associated with a Bayesian
942780	943780	network.
943780	949940	This is going to be a little bit the running example through all the paper, especially with
949940	953940	your net with Bayesian networks of this shape.
953940	960940	Those Bayesian networks, the variables inside, they can represent different quantities.
960940	967820	So for example, a Bayesian network with this shape can represent the quantities on the
967820	968820	right.
968820	973940	So a socio-economical statue of an individual, its education level, its intelligence, and
973940	977580	its income level.
977580	984900	Something the classical statistics is very good at, and it's a while most used application,
984900	987780	is to model observations or correlations.
987780	994700	A correlation basically answered the question, what is the, if we observe another variable
994700	995820	C?
995820	1000140	So for example, in this case, what is, what's the income level, the expected income level
1000140	1004540	of an individual, if I observe his education level?
1004540	1010820	And of course, if that person has a higher degree of education, for example, a master
1010820	1016340	or a PhD, I'm expecting general that person to have a higher income level.
1016340	1017820	And this is a correlation.
1017820	1023340	However, sometimes there are things that are very hard to observe, but they play a huge
1023340	1026300	role in determining those quantities.
1026300	1032940	So for example, it could be that the income level is much, much more defined by the intelligence
1032940	1035980	of a specific person.
1035980	1041700	And maybe that the intelligence, so if a person is intelligent, he's also most likely to have
1041700	1045020	a higher education level.
1045020	1052620	But still the real reason why the income is high is because of the IQ.
1053260	1059260	This cannot be studied by simple correlations and has to be studied by a more advanced technique,
1059260	1060900	which is called an intervention.
1060900	1066900	An intervention basically answers the question, what is the, if we change C to a specific
1066900	1068500	value?
1068500	1075780	So for example, we can take an individual and check his income level, and then change
1075780	1081060	its education level, so intervene on this word, and change his education level without
1081100	1086660	touching his intelligence, and see how much his income changes.
1086660	1092620	For example, if the income changes a lot, it means that the intelligence doesn't play
1092620	1096020	a big role in this, but the education level does.
1096020	1100220	If the income level doesn't change much, it means that maybe there's a hidden variable,
1100220	1106300	in this case, the intelligence that determines the income level of a person.
1106300	1110860	The third quantity important in causal inference is that of counterfactuals.
1110860	1116780	So for example, a counterfactual answers the question, what would be, had we changed C
1116780	1119420	to a different value in the past?
1119420	1123220	So for example, we can see that the difference between interventions and counterfactuals is
1123220	1126460	that interventions act in the future.
1126460	1131300	So I'm interviewing in the world now to observe a change in the future.
1131300	1137460	Well, counterfactual allow us to go back in time and change a variable back in time and
1137540	1143180	see how the change would have influenced the world we live in now.
1143180	1148300	And those are defined by Judea Perle as the three levels of causal inference.
1148300	1152140	Correlation is the first level, intervention is the second level, and counterfactual is
1152140	1156620	the third level.
1156620	1157620	What are interventions?
1157620	1162420	I'm going to define them more formally now, how that I gave an intuitive definition.
1162420	1168220	And I'm using this notation here, which is the same actually throughout all the presentation.
1168220	1173860	So X is always going to be a latent variable, SI is always going to be a data point or an
1173860	1178180	observation, and VI is always going to be a vertex.
1178180	1185580	So every time you see VI, we're only interested in the structure of the graph, for example.
1185580	1191100	So let's assume we have a Bayesian model, which has the same structure as the Bayesian
1191100	1195140	model we saw in the previous slide.
1195140	1201780	Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the
1201780	1209620	probability or the expectation of X4, which is the latent variable related to this vertex,
1209620	1212620	given that X3 is equal to S3.
1212620	1220620	To perform an intervention, we need a new kind of notation, which is called the do operation.
1221500	1228820	So in this case, X4, we want to compute the probability of X4, given the fact that we
1228820	1233540	intervene in the world and change X3 to S3.
1233540	1234900	And how do we do this?
1234900	1241520	To perform an intervention, Judea Perl tells us that we have to have an intermediate step
1241520	1247900	before computing a correlation, is that first we have to remove all the incoming edges to
1247900	1250460	V3.
1250460	1256020	So we have to study not this Bayesian network, but this second one.
1256020	1263580	And then at this point, we are allowed to compute a correlation, as we normally do.
1263580	1267220	And this is an intervention.
1267220	1272700	A counterfactual is a generalization of this that, as I said, lived in the past, and they
1272700	1275580	are computing using structural causal models.
1275580	1282460	A structural causal model is a tuple, which is conceptually similar to a Bayesian network.
1282460	1287900	But basically, we have this new class of variables on top, which are the unobservable variables
1287900	1289420	they use.
1289420	1293500	So we have the Bayesian network that we had before, X1, X2, X3, S4.
1293500	1300500	But we also have those unobservable or variables that depend on the environment.
1300500	1307500	You cannot control them, you can infer them, but they are there.
1307500	1317500	And F is a set of functions that depends on all the, basically, F of X3 depends on X1,
1317500	1321820	because you have an arrow, on X2, because you have an arrow, and on the unobservable
1321820	1326340	variable that also influences X3.
1326340	1334100	So yes, intuitively, you can think of a structural causal model as a Bayesian network with those
1334100	1342460	unobservable variables on top, and each unobservable variable only influences its own, its own
1342460	1343460	related variable X.
1343460	1347260	So, for example, IU will never touch X1 as well.
1347260	1355260	U3 will only touch U3, U1 will only influence X1, and so forth, and so on.
1355260	1359220	So performing counterfactual inference answers the following question.
1359220	1368220	So what would X4 be at X3 being equal to another variable in a past situation, U?
1368220	1371620	And computing this counterfactual requires three different steps.
1371620	1378220	So abduction is the computation of all the background variables.
1378220	1383340	So in this step, we want to go back in time and understand how the environment, the unobservable
1383340	1388340	environment, was in that specific moment in time.
1388340	1397260	And we do this by fixing all the latent variables X to some specific data that we already have,
1397260	1400340	and performing this inference on the use.
1400340	1408780	Then we're going to use the U to keep the U that we have learned, and perform an intervention.
1408780	1415340	So a counterfactual can also be seen as an intervention back in time, in which we know
1415340	1423620	the environment variables U1, U2, and U4 in that specific moment.
1423620	1426980	And what's the missing step?
1426980	1434060	So what would X4 be at X3 being equal to another data point in that specific situation?
1434620	1437460	Now we can compute a correlation.
1437460	1443660	And the correlation, we do it on the graph in which we have already performed an intervention
1443660	1450420	using the environment variables that we have learned in the abduction step.
1450420	1455780	And this is a counterfactual inference.
1455780	1462380	This is the last slide of the causal inference introduction, and it's about structure learning.
1462380	1468740	Basically, everything I've said so far relies on the fact that we know the causal dependencies
1468740	1470540	among the data points.
1470540	1475140	So we know the structure of the graph, we know which variable influences which one,
1475140	1477580	we know the arrows in general.
1477580	1480660	But in practice, this is actually not always possible.
1480660	1486620	So we don't have access to the causal graph most of the times.
1486620	1490940	And actually learning the best causal graph from data is still an open problem.
1490940	1491940	We are improving in this.
1491940	1493140	We are getting better.
1493140	1501340	But how to perform this task exactly is still an open problem.
1501340	1505860	So as I said, basically, the goal is to infer causal relationships from observational data.
1505860	1511380	So given a data set, we want to infer the directed acyclic graph that describes the
1511380	1516140	connectivity between the system and the variables of the data set.
1516140	1522540	So for example here, we have an example that I guess we are all familiar with thanks because
1522540	1524260	of the pandemic.
1524260	1531740	So we have those four variables, age, vaccine, hospitalization, and CT.
1531740	1535900	And we want to infer the causal dependencies among those variables.
1535900	1540380	So for example, we want to learn directly from data that the probability of a person
1540380	1546780	being hospitalized depends on its age and on the fact whether it's vaccinated or not,
1546780	1551620	and so forth and so on.
1551620	1558660	So this is the end of the long introduction, but I hope it was clear enough and I hope
1558660	1565060	that I gave the basics to understand basically the results of the paper.
1565060	1567940	And now we can go to the research questions.
1567940	1570860	So the research questions are the following.
1570860	1577300	First I want to see whether creative coding can be used to perform causal inference.
1577300	1583700	So creative coding so far has only been used to perform two compute correlations in Bayesian
1583700	1585380	networks.
1585380	1590340	And the big question is, can we go beyond correlation and model intervention and counterfactual
1590340	1592940	in a biological, plausible way?
1592940	1598940	So in a way that it's, for example, simple, intuitive, and allow us to only play with
1598940	1604220	the neurons and not touch, for example, the huge structure of the graph.
1604220	1608820	And more in practice, more specifically, the question becomes, can we define a creative
1608820	1615580	coding-based structural causal model to perform interventions and counterfactuals?
1615580	1621320	The second question is, as I said, that having a structural causal model assumes that we
1621320	1624520	know the structure of the Bayesian network.
1624520	1628160	So it assumes that we have the arrows.
1628160	1632000	Can we go beyond this and use creative coding networks to learn the causal structure of
1632000	1634200	the graph?
1634200	1641560	Basically, giving positive answers to both those questions would allow us to use creative
1641560	1647400	coding as an end-to-end causal inference method, which basically takes a data set and allow
1647400	1657040	us to test interventions and counterfactual predictions directly from this data set.
1657040	1662040	So let's tackle the first problem, so causal inference via creative coding, which is also
1662040	1667040	the section that gives the title to the paper, basically.
1667040	1673040	And here I will show how to perform correlations with creative coding, which is already known,
1673040	1681440	and how to perform interventional queries, which I think is the real question of the paper.
1681440	1687560	So here is a causal graph, which is the usual graph that we had.
1687560	1690520	And here is the corresponding creative coding model.
1690520	1695960	So the axes are the latent variables and correspond to the neurons in a neural network
1695960	1698360	model.
1698360	1705880	And the black arrow passes prediction information from one neuron to the one down the hierarchy.
1705880	1711880	And every vertex also has this error neuron, which passes information up the hierarchy.
1711880	1718240	So the information of every error goes to the value node in the up the hierarchy and
1718240	1724960	basically tells it to correct itself to change the prediction.
1724960	1729120	So to perform a correlation using creative coding, what you have to do is that you take
1729120	1734000	an observation and you simply fix the value of a specific neuron.
1734000	1739840	So if you want to compute the probability of X4 given X3 equal to S3, we simply have
1739840	1746600	to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy
1746600	1748480	minimization.
1748480	1756320	And this model, by minimizing, by updating the axes via a minimization of the variational
1756320	1761080	free energy, allows the model to converge to a solution to this question.
1761080	1766320	So the probability or the expected value of X4 given X3 equals 3.
1766320	1772560	But how do I perform an intervention now without acting on the structure of the graph?
1772560	1777200	Well, this is basically the first idea of the paper.
1778160	1780520	This is still how to perform a correlation.
1780520	1785240	So fix S3 equal to X3 is the first step in the algorithm.
1785240	1791320	And the second one is to update the axes by minimizing the variational free energy.
1791320	1797040	An intervention, which in theory corresponds in removing those arrows and answers to the
1797040	1804120	question, the probability of X4 by performing an intervention, so do X3 equal S3?
1804120	1807320	This coding can be performed as follows.
1807320	1810120	So I'm going to write the algorithm here.
1810120	1819160	So first, as in a correlation, you fix X3 equal to the observation that you get.
1819160	1821560	Then this is the important step.
1821560	1827360	You have to intervene not on the graph anymore, but on the prediction error and fix it equal
1827360	1829360	to zero.
1829360	1836840	Assuming a prediction error equal to zero basically makes sense, meaning less information
1836840	1841360	up the hierarchy or actually sends no information up the hierarchy because it basically tells
1841360	1845480	you that the prediction is always correct.
1845480	1851000	And the third step is to, as we did before, to update the axes, the unconstrained axis
1851000	1855960	or X1, X2, X4 by minimizing the variational free energy.
1855960	1861680	As I will show now experimentally, by simply doing this little trick of setting a prediction
1861680	1869080	error to be equal to zero, it prevents us to actually act on the structure of the graph
1869080	1877920	as the theory of Duke-Alculus does and to infer the variables after an intervention
1877920	1885000	by simply performing a variational free energy minimization.
1885000	1886760	What about counterfactual inference?
1886760	1895040	Counterfactual inference is actually easy once we have defined how to do an intervention.
1895040	1899040	And this is because, as we saw earlier, performing a counterfactual is similar to performing an
1899040	1908320	intervention in a past situation after you have inferred the unobservable variables.
1908320	1913760	So as you can see in the plot I showed earlier about the abduction action and prediction
1913760	1919920	steps, the action and prediction steps, they did not have those two arrows.
1919920	1921680	They were removed.
1921680	1931200	Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by
1931200	1936240	simply performing an abduction step, as it was done earlier, an action step in which we
1936240	1939400	simply perform an intervention on the single node.
1939400	1946280	So we fix the value node and we set the error to zero and run the energy minimization, so
1946280	1952960	minimizing the variational free energy to compute the prediction.
1952960	1961560	So I think this is like an easy and elegant method to perform interventions and counterfactuals.
1961560	1967800	And yeah, so I think the thing we have to show now is whether it works in practice or not.
1967800	1970400	And we have a couple of experiments.
1970400	1973560	And I'm going to show you now two different experiments.
1973560	1981320	The first one is merely proof of concept experiment that shows that the predictive coding is able
1981320	1986400	to perform intervention and counterfactuals.
1986400	1992200	And the second one actually shows a simple application in how interventional queries
1992200	1997520	can be used to improve the performance of classification tasks on a specific kind of
1997520	2002720	predictive coding networks, which is that of a fully connected model.
2002720	2004840	Let's start from the first one.
2004840	2006440	So how do we do this task?
2006440	2014600	So given a structural causal model, we generate training data and we use it to learn the weights,
2014600	2020680	so to learn the functions of the structural causal models.
2020680	2026360	And then we generate test data for both interventional and counterfactual queries.
2026360	2031540	And we show whether we are able to converge to the correct test data using predictive
2031540	2033760	coding.
2033760	2040840	And for example here, those two plots represent the interventional and counterfactual queries
2040840	2046280	of this specific graph, which is the butterfly bias graph, which is a graph that is often
2046280	2052000	used in testing whether causal inference, whether interventional and counterfactual
2052000	2055440	techniques work is as simple as that.
2055440	2059520	But in the paper, you can find a lot of different graphs.
2059520	2069480	But in general, those two plots show that the method works, show that the mean absolute
2069480	2076240	error between the interventional and counterfactual quantities we compute and the interventional
2076720	2083080	and counterfactual quantities from the original graph are close to each other.
2083080	2086600	So the error is quite small.
2086600	2091920	The second experiment is basically an extension of an experiment I proposed in an earlier
2091920	2099320	paper, which is the learning on arbitrary graph topologies that I wrote last year.
2099320	2105800	In that paper, I basically proposed this kind of network as a proof of concept, which is
2105800	2114000	a fully connected network, which is in general the worst neural network you can have to perform
2114000	2125520	machine learning experiments, because given a fixed set of neurons, basically every pair
2125520	2128600	of neuron is connected by two different synapses.
2128600	2134880	So it's the model with the highest complexity possible in general.
2134880	2138320	The good thing is that since you have a lot of cycles, the model is extremely flexible
2138320	2143920	in the sense that you can train it, for example, on a minst image and on a data point and on
2143920	2144920	its label.
2144920	2150760	But then the way you can query it, thanks to the information going back, is you can query
2150760	2151760	in a lot of different ways.
2151760	2156080	So you can form classification tasks in which you provide an image and you run the energy
2156080	2158120	minimization and get the label.
2158120	2161760	But you can also, for example, perform generation tasks in which you give the label, run the
2161760	2164280	energy minimization and get the image.
2164280	2171160	You can perform, for example, image completion, which should give half the image and let the
2171160	2173800	model converge to the second half and so forth and so on.
2173800	2180160	So it's basically a model that learns the statistics of the dataset in its entirety
2180160	2185400	without being focused on classification or generation in general.
2185400	2188160	So this flexibility is great.
2188160	2194120	The problem is that because of this, every single task doesn't work well.
2194120	2199080	So you can do a lot of different things, but none of them is done well.
2199080	2206400	And here I want to show how using interventional queries instead of standard correlation queries
2206400	2212200	or conditional queries slightly improves their results of those classification tasks.
2212200	2219760	So what are the conjecture reasons of this test accuracy on those tasks not being so
2219760	2220760	high?
2220760	2227260	The first, the two reasons are that the model is distracted in correcting every single error.
2227260	2231200	So basically you present an image and you would like to get a label, but the model is
2231200	2237040	actually updating itself to also predict the error in the images.
2237040	2241960	And the second reason, which is the one I said, is that the structure is far too complex.
2241960	2250520	So again, from an Occam razor argumentation, this is the worst model you can have.
2250520	2254200	So every time you have a model that fits a dataset, that model is going to be less complex
2254200	2256640	than this one that is going to be preferred.
2256640	2263800	But in general, just to start it, the idea is can querying this model be interventions
2263800	2268160	be used to improve the performance of those fully connected models?
2268160	2271320	Well, the answer is yes.
2271320	2273680	So here is how I perform interventional queries.
2273680	2276800	So I present an image to the network.
2276800	2280160	I fix the error of the pixels to be equal to zero.
2280160	2283480	So this error doesn't get propagated in the network.
2283480	2286000	And then I compute the label.
2286000	2291400	And as you can see, the accuracy improves, for example, from 89 using the standard query
2291400	2297680	method of pretty difficult in networks to 92, which is the accuracy after the intervention
2297680	2302000	and the same happens for fashion means.
2302000	2307480	And I think that a very legit critic that probably everyone would think when seeing
2307480	2314400	those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically.
2314400	2316720	And yeah, it's true.
2316720	2320920	And I'm actually in the later slides, I'm going to show how to act on the structure
2320920	2327040	of this fully connected model will improve the results even more until the point they
2327720	2333000	reach a performance that is not even close to state of the art performance, of course.
2333000	2342240	But it's still up to a level that becomes basically acceptable and worth investigating.
2342240	2348680	So yes, so this is the part about causal inference using predictive coding.
2348680	2356600	And I guess to summarize, I can say that the interesting part of the results I just showed
2357240	2362520	is that I showed that predictive coding is able to perform interventions in a very easy
2362520	2366760	and intuitive way because you don't have to act on the structure of the old graph anymore.
2366760	2371120	Sometimes those functions are not available, so forth and so on.
2371120	2381880	But you simply have to intervene on a single neuron, set its prediction error to zero and
2381880	2384680	perform an energy minimization process.
2386760	2393040	And these extended allowed us to define predictive coding based structural causal models.
2393040	2399680	Now we move to the second part of the work, which is about structure learning.
2402480	2408000	So structure learning, as I said, deals with the problem of learning the causal structure
2408000	2412080	of the model from observational data.
2412080	2421000	This is actually no problem that has been around for decades and has always been, until
2421000	2425800	a couple of years ago, tackled using combinatorial search methods.
2425800	2430800	The problem with those combinatorial search methods is that their complexity grows double
2430800	2433040	exponentially.
2433040	2440200	So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn
2440200	2447000	grows in size, learning it, it's incredibly slow.
2447000	2454640	The new solution that came out actually a couple of years ago in a newspaper from 2018
2454640	2458240	showed that it's possible to actually learn this structure, not using a combinatorial
2458240	2461920	search method, but by using a gradient-based method.
2461920	2469320	And this was basically this killed the problem in general because now you can simply apply
2469320	2473440	your on the parameters, which is the prior proposed that I'm going to define a little
2473440	2477640	bit better in this slide, around gradient descent.
2477640	2483400	And even if you have a model that is double, triple the size, the algorithm is still incredibly
2483400	2485800	fast.
2485800	2491800	And for this reason, this paper is, yeah, I think it's kind of new and I think already
2491800	2495800	has around 600 citations or things like that.
2495800	2499560	And every paper that I'm seeing now about causal inference and learning causal structure
2499560	2502320	of the graph uses their method.
2502320	2508560	It just changes a little bit, they find faster or slightly better inference methods, but
2508560	2516760	still they all use the prior, this paper defined, and I do as well, and we do as well.
2516760	2521080	So here we define a new quantity, which is the agency matrix.
2521080	2525800	The agency matrix is simply a matrix that encodes the connections of the model.
2525800	2530560	So it's a binary matrix, and in general, it's a binary matrix.
2530560	2535440	Then of course, when you do gradient-based optimization, you make it continuous and then
2535440	2541960	you have some threshold at some point that basically kills an edge or set it to one.
2542280	2554360	The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j
2554360	2555760	or zero otherwise.
2555760	2560400	So for example, this agency matrix here represents the connectivity structure of this Bayesian
2560400	2563160	network.
2563160	2571520	And basically this method tackles two problems that we want about learning the structure
2571560	2573360	of the Bayesian network.
2573360	2580200	The idea is that we start from a fully connected model, which conceptually is similar, actually
2580200	2584680	is equivalent to the predictive coding network I defined earlier, which is fully connected.
2584680	2590480	So you have a lot of vertices and every pair of vertices is connected by two different
2590480	2596080	edges, and you simply want to prune the ones that are not needed.
2596080	2600320	So it can be seen as a method that performs model reduction.
2600320	2603080	You start from a big model and you want to make it small.
2603080	2606640	So what's the first ingredient to reduce models?
2606640	2609720	Well, it's of course sparse city.
2609720	2615400	And what's the prior that everyone uses to make a model more sparse is the Laplace prior,
2615400	2621240	which in machine learning is simply known as the L1 norm, which is defined here.
2621240	2627120	The solution that this paper that I mentioned earlier proposed is to add a second prior
2627120	2636440	on top, which enforces what's probably the biggest characteristic of Bayesian networks
2636440	2640440	on which you want to perform causal inference, is that you want them to be acyclic.
2640440	2648440	And basically they show that acyclicity can be imposed on an agency matrix as a prior,
2648440	2650720	and it has this shape here.
2650720	2659200	So it's the trace of the matrix that is the exponential of A times A, where A is the
2659200	2661280	agency matrix again.
2661280	2668840	And basically this quantity here is equal to zero if and only if the Bayesian network
2668840	2677960	or whatever graph you're considering is acyclic.
2677960	2686520	So I'm going to use these in some experiments, so force those two priors on different kinds
2686520	2691280	of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier
2691280	2695360	about performing causal inference via predictive coding.
2695360	2697240	So I'm going to present two different experiments.
2697240	2703560	So one is a proof of concept, which is the standard experiments showed in all the structural
2703560	2709200	learning tasks, which is the inference of the correct Bayesian network from data.
2709200	2716320	And then I'm going to build on top of the classification experiments I showed earlier,
2716320	2722200	and show how actually those priors allow us to improve the classification accuracy, the
2722200	2729720	test accuracy of fully connected predictive coding models.
2729720	2735400	So let's move to the first experiment, which is to infer the structure of the graph.
2735400	2739880	And the experiments, they all follow basically the same pipeline in all the papers in the
2739880	2740880	field.
2740880	2746320	The first step is to generate a Bayesian network from random graph.
2746320	2751560	So basically normally the two random graphs that everyone tests are Erdos-Renis graphs
2751560	2753680	and scale-free graphs.
2753680	2760560	So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some
2760560	2764920	edges that you sample randomly.
2764920	2768600	And you use this graph to generate a data set.
2768600	2773840	So you sample, for example, N, big N data points.
2773840	2778480	And what you do is that you take the graph that you have generated earlier and you throw
2778480	2779480	it away.
2779480	2781160	You only keep the data set.
2781160	2787920	And the task you want to solve now is to have a training algorithm that basically allows
2787920	2794920	you to retrieve the structure of the graph you have thrown away.
2794920	2799040	So the way we do it here is that we train a fully connected predictive coding model on
2799040	2806320	this data set D, using both the sparse and the acyclic priors we have defined earlier.
2806320	2813920	You can see whether actually the graph that we converge to, after pruning away the entries
2813920	2819640	of the agency matrix that are smaller than a certain threshold, is similar to that of
2819640	2822840	the initial graph.
2822840	2825560	And the results show that this is actually the case.
2825560	2833600	So this is an example and I show many different parametrizations and dimensions and things
2833600	2835040	like that in the paper.
2835640	2839800	But I think those two are the most representative examples with an air nosher in a graph and
2839800	2843920	a free scale graph with 20 nodes.
2843920	2851160	And here on the left, you can see the ground truth graph, which is the one sampled randomly.
2851160	2856240	And on the right, you can see the graph, the predictive coding model as learned from the
2856240	2857240	data set.
2857240	2860760	And as you can see, they are quite similar.
2860760	2862640	It's still not perfect.
2862640	2868840	So there are some errors, but in general, the structures, they work quite well.
2868840	2874240	We also have some quantitative experiments that I don't show here, because they're just
2874240	2878240	huge tables with a lot of numbers and I thought it was maybe a little bit too much for the
2878240	2879240	presentation.
2879240	2886400	But there is also that they perform similarly to contemporary methods.
2886400	2892520	Also because I have to say most of the quality comes from the acyclic priors that was introduced
2892520	2893520	in 2018.
2897120	2902760	The second class of experiments are classification experiments, which as I said, are the extensions
2902760	2905880	of the one I shared earlier.
2905880	2911000	And the idea is to use structure learning to improve the classification results on the
2911000	2916960	means and fashion means data set, starting from a fully connected graph.
2916960	2924000	So what I did is that I divided the fully connected graph in clusters of neurons.
2924000	2929440	So 1B cluster is the one related to the input.
2929440	2935680	And then we have some specific number of hidden clusters.
2935680	2942200	And then we have the label cluster, which is the cluster of neurons that are supposed
2942200	2946680	to give me the label predictions.
2946680	2950400	And I've trained them using the first time, the sparse prior only.
2950400	2959320	So the idea is, what if I prune the connections I don't need from a model and learn a sparser
2959320	2960320	model?
2960320	2961800	Does this work?
2961800	2963480	Well, the answer is no.
2963480	2964680	It doesn't work.
2964680	2971840	And the reason why is that at the end, the graph that you converge with is actually degenerate.
2971840	2977200	So basically, the model learns to predict the label based on the label itself.
2977200	2981920	So it discards all the information from the input and only keeps the label.
2981920	2984960	And as you can see here, the label y predicts itself.
2984960	2989200	Or in other experiments, when you change the parameters, you have that y predicts at zero,
2989200	2993000	that predicts x1, that predicts y again.
2993000	2996360	So what's the solution to this problem?
2996360	3003440	Well, the solution to this problem is that we have to converge to an acyclic graph.
3003440	3007000	And so we have to add something that prevents acyclicity.
3007000	3008920	And what is that?
3008920	3010840	One is, of course, the one I already proposed.
3010840	3014840	And then I show a second technique.
3014840	3019600	So the first one uses the acyclic prior defined earlier.
3019600	3024600	And the second one is a novel technique that actually makes use of negative examples.
3024600	3031640	So a negative example in this case is simply a data point in which you have an image, but
3031640	3033440	the label is wrong.
3033440	3037360	So here, for example, you have an image of a 7, but the label that I'm giving the model
3037360	3041360	is a 2.
3041360	3048040	And the idea is very simple and has been used in a lot of works already.
3048040	3054280	So every time the model sees a positive example, it has to minimize the variational free energy.
3054280	3059120	And every time it sees a negative example, it has to increase it.
3059120	3065680	So we will want this quantity to be minimized.
3065680	3070680	And actually, with a lot of experiments and a lot of experimentations, we saw that the
3070680	3077440	two techniques basically first lead to the same results and second lead to the same graph
3077440	3078440	as well.
3078840	3086480	So here are the new results on means and fashion means using the two techniques that I just
3086480	3088640	proposed.
3088640	3094480	And now we move to some which are still not great, but definitely more reasonable test
3094480	3095560	accuracies.
3095560	3102560	So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means.
3102680	3109000	Actually, those results can be much improved by learning the structure of the graph on
3109000	3115720	means and then fixing the structure of the graph and do some form of fine tuning.
3115720	3120400	So if you fine tune the model on the correct hierarchical structure, at some point you
3120400	3124720	reach the test accuracy, which is the one you would expect from a hierarchical model.
3124720	3131120	But those ones are simply the one, the fully connected model as naturally converged to.
3131120	3137400	So for example, from a test error of 18.32 of the fully connected model train on fashion
3137400	3143560	means by simply performing correlations or conditional queries, which is the standard
3143560	3150400	way of querying operative coding model, adding interventions and the acyclic prior together
3150400	3154520	makes this test error much lower.
3154520	3158120	And we can observe it for means as well.
3159120	3166360	I'm now going a little bit into details on this last experiment and on how the acyclic
3166360	3169760	prior acts on the structure of the graph.
3169760	3175920	So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset,
3175920	3181240	it may be too much, is the, I called it a two means dataset in which you have the input
3181240	3187960	point is formed of two different images and the label only depends on the second image.
3188680	3189840	On the first image story.
3190840	3197040	So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and
3197040	3202640	things like that able to recognize that the second half of the image is actually meaningless
3202800	3209720	in, in performing, in learning the in performing classification.
3211280	3212760	How does training behave in general?
3212760	3218920	Like, for example, we have this input, input node, output node, and only the nodes are
3218920	3226080	fully connected and the model converge to a hierarchical structure, which is the one
3226400	3230040	that we know performs the best on, on classification tasks.
3231080	3235960	Well, here is a, is an example of a training method of a training run.
3236480	3239640	So that's C zero, which is the beginning of training.
3240880	3241960	We have this model here.
3242040	3247920	So as zero corresponds to the, to the seven, so to the first image as one corresponds to
3247920	3252640	the second image, again, we have the label Y and all the latent variables X zero X one
3252640	3255240	X two, and the model is fully connected.
3255280	3258720	So the agency matrix is, is full of ones.
3259000	3260160	There are, there are no zeros.
3260320	3262040	We have self loops and things like that.
3263640	3270000	We train them at the model for a couple of epochs until, and what we note immediately is
3270040	3274680	that, for example, the, the model immediately understands that the four is not needed to
3274680	3275760	perform classification.
3276240	3276840	So it doesn't.
3277880	3282880	So every outgoing node from the, from the second input cluster is removed.
3284160	3288920	And something we didn't understand is that this is, this cluster is the one related to
3288920	3289480	the output.
3290560	3298280	So we have a, we have a linear map from S zero to Y directly, which is this part here.
3299240	3305720	But we know that actually a linear map is not the best map for, for performing classification
3305720	3306200	on means.
3306440	3307880	So we, we need some hierarchy.
3307880	3310520	We need some depth to, to improve the results.
3310920	3317240	And as you can see, this line here is the, is the accuracy, which up to this point, so
3317240	3323640	up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear
3323640	3324440	classification.
3325400	3330840	But once you go on with the training, the model understands that it needs some hierarchy
3330840	3332000	to better fit the data.
3332840	3339760	So you, you see that this arrow starts getting stronger and stronger over time until it, it
3339760	3344480	understands that the linear map is not actually really needed and it removes it.
3345400	3350360	And so the, so the model you converge with is a model that starts from a zero, goes to
3350680	3358040	a hidden node and then goes to the, to the label with a very weak linear map, which actually
3358040	3363480	gets removed if you, if you set that threshold of, if you set that threshold of, for example,
3363480	3366920	0.1, 0.2, at some point, the linear map gets forgotten.
3367240	3371800	And everything you end up with is with a, is with a hierarchical network.
3373400	3378120	That is, that is, so it has learned the correct structure to, to perform classification tasks,
3378120	3379080	which is a hierarchy.
3379480	3385000	And it has also learned that the second image didn't play any role in defining the,
3385560	3386840	the test accuracy.
3386840	3389480	And this is all, this is all performed.
3389480	3395320	So all those jobs are simply performed by, performed by one free energy minimization
3395320	3396120	process.
3396120	3400040	So you initialize the model, you define the free energy, you define the priors.
3400040	3406120	So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge
3406120	3410760	to hierarchical, to a hierarchical model, which is well able to perform classification on minced.
3411880	3416760	And then if you then perform some fine tuning, you reach very competitive results as you do in
3416760	3419720	feed forward networks with the, with back propagation.
3419720	3421720	But I think that's not the interesting bit.
3421720	3425400	The interesting bit is that you, like all this process, this process altogether
3426040	3431880	of intervention and the acyclicity allows you to take a fully connected network
3432680	3438040	and converge to a hierarchical one that is, that is able to perform classification with good results.
3440840	3444600	And yeah, that's basically it.
3444600	3447160	I'm now, oh yeah, wow, I've talked a lot.
3447160	3454680	And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary.
3454680	3459320	And I think the, the important takeaway if I have to give even one sentence of this paper
3459400	3465560	is that predictive coding is a belief updating method that is able to perform end to end causal
3465560	3471560	learning. So it's able to perform interventions to learn a structure from data and then perform
3471560	3479080	interventions and counterfactuals. So causal inference in natural and efficiency model
3479080	3481880	interventions by simply setting the prediction error to zero.
3481880	3485800	So it's a, it's a very easy technique to perform interventions.
3485800	3489480	And you simply only have to touch one neuron, you don't have to act on the structure of the graph.
3490920	3496440	You can, you can use it to perform, to, to create structure causal models that are biologically
3496440	3504440	plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already.
3505400	3509480	And, and a couple of sentences about future works is that
3510200	3515080	something that would be nice to do is to improve the performance of the model we,
3515080	3520520	we have defined, because I think it performs reasonably well on a lot of tasks.
3520520	3525880	So it performs reasonably well on structure learning on, for me, intervention and counterfactuals.
3527080	3531480	But actually, if you look at state of the art model, there's always like a very specific method
3531480	3537560	that performs better in a, in the single task. So it would be interesting to see if we can
3538040	3544360	reach those level of performance in, in specific tasks by, by adding some tricks on, or some,
3547080	3552600	or some new optimization methods, and to generalize it to, to dynamical systems,
3552600	3557960	which are actually much more interesting, the static systems. So such as dynamical causal models
3557960	3564600	and, or other techniques that allow you to perform causal inference in systems that move.
3564600	3571080	So an action taken in a specific time step influences another node in a later time step,
3571080	3578920	which is basically Granger causality. Yeah, that's it. And thank you very much.
3587560	3592760	Thank you. Awesome. And very comprehensive presentation. That was really
3593480	3601480	muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive
3602360	3607480	presentation. There was really a lot there. And there was also a lot of great questions
3607480	3613960	in the live chat. So maybe to warm into the questions, how did you come to study this
3614520	3620200	topic? Were you studying causality and found predictive coding to be useful or vice versa?
3620200	3625720	Or how did you come out this intersection? I actually have to say that the first person
3625720	3634280	that came out with this idea was, was better. So, so like, like, I think a year and a half ago,
3634280	3640600	even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up.
3641160	3645880	And, and last summer, I started getting curious about causality and
3646120	3652200	I read, for example, the book of why as I listen into podcasts, I don't know the
3652200	3656520	standard way in which you get interested in a topic. And, and I remember this,
3656520	3662360	this idea from Baron and proposed it to him. And I was like, why don't we expand it and,
3662360	3667880	and actually make it a paper. So I, I involve some people to work with experiments and,
3668520	3673560	and this is the final result at the end. Awesome. Cool. Yeah.
3674120	3679640	Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different
3679640	3682680	questions. And if anybody else wants to add more, I'm going to turn the light on first,
3682680	3686200	because I'm, I think I'm getting in the dark more and more. Yes.
3688360	3693400	Who said active inference can't solve the dark room issue? Oh, yes, here we are.
3694920	3697880	So would you say the light switch caused it to be lighter?
3698200	3709240	Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding,
3709240	3714040	all distributions are usually Gaussian, the bottom up messages are precision weighted
3714040	3717880	prediction errors where precision is the inverse of the Gaussian covariance.
3718520	3721320	What if non Gaussian distributions are used?
3721640	3730200	Is, um, basically the general method stays, the different, the main difference is that you,
3731000	3735720	you don't have prediction errors, which, uh, as was correctly pointed out is the,
3735720	3741320	basically the derivative of the variational free energy. If you have Gaussian assumptions,
3742920	3745880	yeah, you don't have that single quantity to set to zero.
3746520	3752120	And you probably will have to act on the structure of the graph to perform interventions.
3754040	3760280	And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian
3760280	3762680	distributions that, that looked at some of these issues, right?
3763880	3769160	Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh,
3770520	3775240	and we model transformers. That's the biggest motivation using predictive coding.
3775320	3780680	And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end,
3780680	3786520	and softmax calls to, uh, like not to Gaussian distribution, but to,
3788520	3792200	yeah, to softmax distribution, the, I don't get the name now, but yes.
3793560	3797080	And, uh, so yes, that's a generalization. It's a little bit
3797720	3801000	tricky to call it. Once you remove the Gaston assumption is a little bit
3801000	3804680	still tricky to call it predictive coding. So it's a,
3806520	3810040	so for, for example, like talking to, uh, to Carl Freestone,
3811640	3816120	like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions.
3817880	3820440	But yes, that's more a philosophical debate than, uh,
3822760	3828120	Interesting. And another, I think topic that, that's definitely of, of great interest is
3828120	3833640	similarities and differences between the attention apparatus in transformers
3834360	3839960	and the way that attention is described from a neurocognitive perspective and from a predictive
3839960	3844040	processing precision waiting angle. What do you, what do you think about that?
3846440	3853880	Well, the idea is that, um, yeah, I think about it is that in from a pretty processing
3853880	3858440	and, uh, and also operational inference perspective, attention can be seen as a,
3858440	3862280	as a kind of structure learning problem. There's a, I think there's a recent paper from,
3863080	3869000	from Chris Buckley's group that shows that there should be, there should be a reprint on archive
3869560	3872520	in which basically they show that the attention mechanism is simply
3873240	3879720	learning the, the precision on the, on the weight parameters specific to out to a data point.
3879800	3885160	So this precision is not a, is not a, is not a parameter that is in the structure of the model.
3885160	3889480	So it's not a model specific parameter. It is a fast changing parameter like the value nodes
3890040	3893080	that gets updated while minimizing the version of free energy.
3893720	3897000	And once they, once you've minimized it and compute it, then you throw it away.
3897000	3899960	And from the next data point, you have to really compute it from scratch.
3900760	3907160	So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as
3907160	3913080	a kind of structure learning, but a structure learning that is data point specific and not
3913080	3917960	model specific. And I think if you want to generalize a little bit and go from,
3918920	3923000	from the attention mechanism in transformers to the attention mechanism cognitive science,
3924200	3929720	I feel they're probably too different to, like to draw similarities and, uh,
3931240	3936840	I think the structure learning analogy and the, how important one connection in is
3936840	3939800	with respect to another one probably does job much better.
3942040	3949560	Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference
3949560	3953640	between hidden variables X and unobserved variables U?
3955560	3962440	The difference is that you can, uh, I think the main one is that you cannot observe the,
3962520	3967480	the use. You can use them because you can, you can compute them and fix them,
3968200	3972440	but you cannot, the idea is that you have no control over them. So the use,
3972440	3977720	the use should be seen as a environment specific variables that they are there. They,
3977720	3982840	they influence your process. Okay. Because the, for example, when you go back in time,
3982840	3985720	the environment is different. So the idea is for example, if you,
3986520	3991720	like going back to the, to the example before of the, of the expected income of a person with
3991720	3998760	a specific intelligence of education, uh, uh, education degree, the idea is that if I want to,
3999320	4005320	to see how much I will learn today with a, with a, with a, I don't know, with a master degree,
4005320	4011000	is different with respect to how much I would earn 20 years ago with a master degree is different.
4011000	4016440	For example, here in Italy with respect to other countries and all those variables that are not
4016440	4021400	under your control, you can not model them using your vision network, but they are there.
4021400	4026040	Okay. So you, you cannot ignore them when you, when you want to draw conclusions.
4026760	4031720	So it's, yeah, it's basically everything that you cannot control. You can infer them. So you
4031720	4036840	can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years
4036840	4043400	ago, I would have earned this much if I, if I was disintelligent at this degree on average,
4043400	4049000	of course. And, but it's not that I can change the government policies towards jobs or the,
4049640	4057240	or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use.
4058440	4064200	Awesome. All right. Have you implemented generalized coordinates in predictive coding?
4065960	4072600	No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never
4072680	4079880	implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable.
4079880	4086200	I think that's the, that's the takeaway that I got from talking to people that have implemented them.
4088440	4093960	But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that,
4093960	4098840	that tested on some threshold encoder style. Actually, I think still from Baron,
4099480	4105400	there's a, there's a paper out there that came out last summer, but no, I've never played them with
4105400	4113960	them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction
4113960	4122920	problem of predicting input? Adding more level in, uh, in which sense, because the
4122920	4128600	destruction problem is given by cycles. So basically you provide an image and the fact that you have
4128840	4136040	a, so edges going out of the image, going in the, in the neurons, and then other edges going back,
4137640	4143400	the, this basically creates the fact that you have a, that the error of, that those basically,
4143960	4149000	these ingoing edges to the pixels of the image, they create some prediction errors. So you have
4149000	4154200	some prediction errors that get spread inside the model. And that's, yeah, and this problem,
4154200	4159400	I think is general of cycles. And it's probably not related to hierarchy in general.
4161000	4166360	So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges,
4166360	4173800	you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic
4173800	4183240	network through the trace operator, that's a very interesting technique. And when was that
4183320	4184280	brought into play?
4186840	4193320	As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know,
4193320	4199000	at least in the causal inference literature, I'm, I'm not aware of any previous methods.
4199000	4204120	I would say no, because that, I mean, that's the highly cited paper. So I would say they came out
4204120	4209240	with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn
4209240	4215000	the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like
4215000	4221000	when you look at when different features of Bayesian inference and causal inference became
4221000	4228680	available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal
4228680	4235800	modeling framework? It's like, because there's only been like five to 25 years of this happening.
4236680	4242360	And so that's very, very short. And also it's relatively technical. So there's relatively
4242360	4248280	few research groups engaging in it. And it's just really cool what it's enabling.
4249800	4254200	No, yes, yes, exactly. I mean, that's also, I think the exciting part of this field a little bit
4254200	4259640	that is, I mean, there are definitely break breakthroughs out there that, that still have
4259640	4265080	to be discovered and probably like, for example, like, or as much as a breakthrough that paper was
4265880	4272920	they found like, they simply found out the right prior for acyclic structures. Okay, it's a
4274120	4279640	yeah, I mean, I, I don't know exactly, but it may be an idea that you have in one afternoon.
4279640	4284520	I don't know about the story of the, how the authors came up with that, but could potentially be
4284520	4289080	that if they, they are there at the whiteboard, you're like, Oh, that actually works. That's a
4289080	4296280	huge breakthrough. And I simply defined the prior. And also a lot of these breakthroughs,
4297160	4306120	they, they don't just stack. It's not like a, a tower of blocks, they layer and they compose.
4306920	4311000	So then something will be generalized to generalized coordinates or generalized
4311000	4317800	synchrony or arbitrarily large graphs or sensor fusion with multimodal inputs. And it's like those
4317800	4324920	all blend in really satisfying and effective ways. So, so even little things that again,
4324920	4333160	someone can just come up with in a moment can really have impact. Okay, ML Don says,
4333160	4338200	thanks a lot for asking my questions and thanks a million to Tomaso for the inspiring presentation.
4338200	4342120	So nice. Thank you very much. And then Bert asks,
4343080	4348120	how would language models using predictive coding differ from those using transformers?
4352520	4357320	Okay, I think that actually, if I would have to build today a language model using predictive
4357320	4361720	coding, I would still use transformers. So the idea is that, for example, if you have a,
4362760	4369480	let's say this hierarchical graphical model, or this hierarchical Bayesian network,
4370440	4376040	I've defined in the, in the very first slides, one arrow to encode a function, which is the linear
4376040	4381720	map. Okay, so one arrow was simply the multiplication of a, of the vector encoded in the latent
4381720	4388200	variables times the, this weight matrix that you can then make non-linear and things like that.
4388200	4392200	But that can be actually something much more complex. The, the function encoded in the arrow
4392280	4399560	can be a convolution, can be an attention mechanism. So, so actually how I would do it,
4399560	4406680	I will still use the, I mean, which is actually the way we did it in, in, in the Oxford group last
4406680	4412840	year is that we, we had exactly the structure. Every arrow is a transformer now. So one is
4412840	4417240	the attention mechanism and the, the next one is the feed forward network as transformers.
4418280	4422040	And basically the only difference that you have is that those variables you want to compute the
4422040	4427240	posterior and you make those posterior's independence, independent via, via mean field
4427240	4432360	approximation. So basically you follow all the steps that allow you to, to converge to the
4432360	4437400	very, to the variational free energy of creative coding. But the, the way, the way you compute
4437400	4446120	predictions and the way you, you send signals back is a, is done via transformer. So I will
4446120	4451720	still use transformers in general. I mean, they work so well that I, I don't think that we can be
4452120	4456440	arrogant and say, oh no, I'm going to do it better via a purely predictive coding way.
4457640	4461720	Structure learning is a way to do it, but we'll still approximate transformers anyway.
4462520	4466680	So you said structure learning would approximate the transformer approach?
4467560	4473240	Yes. Destruction learning I mentioned earlier in, when, when someone asked the similarities
4473240	4476040	between predictive coding and the attention mechanism.
4478360	4486040	Very, yeah, very interesting. One thing I am wondering from MLBong, I could not see the
4486040	4490600	concept of depth in the predictive coding networks you mentioned. Most likely I missed it. The
4490600	4497800	definition provided for predictive coding involved the concept of depth. What did you mean by depth?
4498360	4505560	No, yes, it's true. It's a, because the standard definition, as I said, multiple times is a,
4505560	4509240	is hierarchical. You have predictions going one directions and prediction error going the
4509240	4515720	opposite direction. Basically, what, what we did in, in this paper and also in the last one in
4516520	4520360	which is called the learning on arbitrary graph topologies via predictive coding
4521080	4530840	is that we can consider depth like as a, as independent, basically pair of latent variable,
4530840	4535640	latent variable, and arrow. And you have predictions going that direction and prediction
4535640	4541960	error going the other. But then you can compose these in how many, a lot of ways. So you can,
4542920	4547960	you can, so basically this composition doesn't have to be hierarchical in the end.
4548680	4551720	Can have cycles. So then you can, for example, plug in another,
4553000	4557480	another latent variable to the first one, and then connect the other two. And you can have a
4557480	4563320	structure that is as entangled as you want. So for example, in the, in the other paper, we train
4563320	4569000	the, a network that has the shape of a brain structure. So we have a lot of brain regions
4569000	4575000	that are sparsely connected inside and sparsely connected among each other. And, and there's,
4575000	4578760	there's nothing hierarchical there at the end, but you can still train it by minimizing
4578760	4583400	a ratio of free energy and by minimizing the, the total prediction error of the network.
4585800	4595640	So you could have for a given motif in a entangled graph, you might see three successive layers
4595640	4599240	that when you looked at them alone, you'd say, Oh, that's a three story building.
4599240	4604520	That's a three layer model that has a depth of three. But then when you take a bigger picture
4605160	4610840	there isn't like an explicit top or an explicit bottom to that network.
4612200	4615880	Yes, exactly. And this is basically given by the, by the fact that every operation in
4615880	4621640	creative coding networks is strictly local. So, so basically every message passing every
4621640	4626760	prediction and every prediction error that you send, you only send it to the very nearby neurons.
4627320	4631160	Okay. And whether the global structure is actually hierarchical or not,
4631880	4634840	the, the single message passing doesn't even see that.
4637480	4647000	I guess that's sort of the hope for learning new model architectures is the space of what is
4647880	4654840	designed top down is very small and a lot of models in use today,
4655480	4662600	albeit super effective models. Although you could ask effective per unit of compute or not,
4662600	4667320	that's a second level question. But a lot of effective models today do not have some of these
4667320	4675240	properties of predictive coding networks, like their capacity to use only local computations,
4675880	4684760	which gives biological realism or just spatio temporal realism, but also may provide a lot
4684760	4688920	of advantages in like federated compute or distributed computing settings.
4690600	4695960	No, yes, exactly. I completely agree. I think the idea in general is that, and I don't know if
4695960	4700120	that's going to be an advantage. I think it's very promising exactly for the reasons you said.
4700920	4704920	And the reason is that today's models string with back propagation,
4704920	4712840	you can basically summarize them as a model string back propagation is a function,
4712840	4717640	because basically you have a map from input to output, and back propagation basically
4718280	4725160	spreads information back from its computational graph. So every neural network model used today
4726040	4731960	is a function. While predictive coding and not only predictive coding, like the whole class of
4731960	4738360	functions, the class of methods that train in using local computations and actually work by
4738360	4745000	minimizing a global energy function, they're not limited to model functions from input to output.
4745000	4749960	They actually model something that kind of resembles physical systems. So you have a physical
4749960	4756200	system, you fix some values to whatever input you have, and you let the system converge,
4756200	4761320	and then you read some other value of neurons or variables that are supposed to be output.
4761320	4766040	But this physical system doesn't have to be a feedforward map. It doesn't have to be a function
4766040	4772200	that has an input space and an output space, and that's it. So the class of models that you can
4772200	4778760	learn is also basically you can see like feedforward models and functions, and then a much bigger
4778760	4783640	class, which is that of physical systems. Whether there's something interesting out here, I don't
4783640	4788280	know yet, because the functions are working extremely well. We are seeing those days with
4788280	4794440	back propagation, they work crazy well. So yeah, I don't know if there's anything interesting in
4794440	4800520	the big part, but the big part is quite big. There are a lot of models that you cannot
4801160	4803800	train with back propagation, and you can train with predictive coding,
4804680	4811240	or a background propagation or other methods. That is super interesting. Certainly biological
4811240	4818200	systems, physical systems solve all kinds of interesting problems. But there's still no free
4818200	4823640	lunch, and ant species does really well in this environment might not do very well in another
4823640	4831000	environment. And so out there in the in the hinterlands, there might be some really unique
4831080	4837080	special algorithms that are not well described by being a function,
4838520	4848040	yet still provide like a procedural way to to implement heuristics, which might be extremely,
4848040	4854920	extremely effective. No, yes, yes, exactly. And yeah, and I think this has been most of my
4855720	4861640	focus of research during my PhD, for example, like finding this application that is like out
4861640	4872040	here and not inside the the functions. Cool. Well, where does this work go from here? Like,
4872040	4878600	what directions are you excited about? And how do you see people in the active inference ecosystem
4878600	4885160	getting involved in this type of work? I think every probably the most promising
4885720	4892760	direction, which is something maybe I would like to explore a little bit is to, as I said,
4892760	4899640	there is to go behind statistical models. So everything I've shown so far is about static
4899640	4905560	data. So the data don't change over time, there's no time inside the definition of
4905560	4910200	predictive coding as it is as I presented it here. However, you can, for example,
4910200	4915400	generalize predictive coding to to work with temporal data using generalized coordinates,
4915400	4923000	as you mentioned earlier, by by presenting it as a as a Kalman Kalman filter generative model.
4924280	4930440	And and that's where, for example, the causal inference direction could be very useful,
4930440	4936600	because at that model, in at that point, maybe you can be able to model Granger causality and
4937400	4940040	and more complex and and useful
4941960	4947480	dynamical causal models, basically. Because in general, the the due calculus and the
4947480	4955000	interventional and counterfactual branch of science is mostly developed on on small models.
4955960	4963400	So it's like you don't do interventions on gigantic models in general. So if you if you
4963400	4970280	look at medical data, they use relatively small vision networks. And but of course,
4970280	4976200	if you want to have a dynamical causal model, that models a specific environment or a specific
4976200	4981080	reality, you have a lot of neurons inside, you have a lot of latent variables, they change over
4981080	4987000	time and an intervention at some more at some moment creates an effect in a different time step.
4987000	4991560	So maybe the next time step in 10 different time steps later. And I think that would be
4991560	4996840	very interesting to develop like a biologically plausible way of passing information
4997960	5001480	that is also able to model Granger causality, basically.
5004680	5007400	Where do you see action in these models?
5007400	5019080	Where do I see action? I didn't think of that. I think I see actions in those models,
5019080	5021960	maybe in the same way as I as you see in other models, because
5023080	5028680	creative coding is basically a model of perception. So so an action is you can see
5028680	5034680	that's a consequence of what you're experiencing. So by changing the way you're you're
5035320	5040840	experiencing something, then you can compute maybe you can simply perform a smarter action
5040840	5048280	now that you have more information. But but yeah, I don't think action is very easy. Like,
5048280	5053080	yeah, I don't see any explicit consequence of actions, besides the fact that this can allow
5053080	5060200	you to basically maybe to simply draw better conclusions to then perform actions in the future.
5060760	5066760	I'll add on to that a few ways that people have talked about predictive coding and action.
5067720	5075560	First off, internal action or covert action is attention. So we can think about perception
5075560	5080920	as an internal action that that's one approach. Another approach pretty micro is the outputs
5080920	5088440	of a given node. We can understand that node as a particular thing with its own sensory cognitive
5088440	5095720	and action states. And so in that sense, the output of a node. And then lastly, which we
5095720	5100520	explored a little bit in live stream 43, on the theoretical review on predictive coding,
5101080	5105240	we're reading all the way through. And it was all about perception all about perception. And then
5105240	5115400	it was like section 5.3. If you have expectations about action, then action is just another variable
5115480	5120680	in this architecture. And that's really aligned with inactive inference, where instead of having
5120680	5126440	like a reward or utility function that we maximize, we select action based upon it being the
5126440	5131240	likeliest course of action, the path of least action, that's Bayesian mechanics. And so it's
5131240	5139400	actually very natural to bring in an action variable and utilize it essentially as it as if it were
5140120	5146840	a prediction about something else. Exteroceptively in the world, because we're also expecting action.
5148520	5154760	No, yes, yes, exactly. No, I like the way of defining actions a lot, actually. And I still
5154760	5161080	think if it's been like, for example, there are not so many papers that apply this method. I think
5161080	5167080	there are a couple from from Alexander Orobrie does something similar. But in practice, like
5167880	5172360	outside of the pure active inference, like applying predictive coding and actions to
5173160	5177080	solve practical problems hasn't been explored a lot.
5179720	5185640	Well, thank you for this excellent presentation and discussion. Is there anything else that you
5185640	5193640	want to say or point people towards? No, just a big thank you for inviting me. And
5194600	5198760	it was really fun. And I hope to come back at some point for for some future works.
5200280	5210600	Cool. Anytime, anytime. Thank you, Thomas. So thank you, Daniel. See you. Bye. Bye.
5223640	5225100	You
