start	end	text
0	18840	Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024
18840	24800	on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre
24800	32040	and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you
32040	35680	for introductions and to take us through the paper.
35680	36200	Oh, thank you.
36200	37200	And welcome everyone.
37200	45000	This is Active Inference Gas Stream 84.1 on July 22nd, 2024.
45000	45800	Thank you.
45800	49760	Okay, thank you guys. Go for it.
49760	51920	Hi, thanks for having us.
51920	60040	So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles?
60040	76640	I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute.
76680	79840	So, should we go through the paper briefly?
79840	83880	So, yeah, we wrote this paper together.
83880	89920	Actually, we started by writing a general audience piece that was published in the box online.
89920	91680	Tell us when was that?
91680	93640	A few months ago, I guess.
93640	97520	Yeah, I think it was close to a year ago, maybe.
97520	102000	I don't think it was published a year ago. I think it was published in 2024.
102000	105240	But yeah, we worked on it for a while.
105240	113560	Yeah, so this piece was doing, I guess, two things, that piece that we published in the box.
113560	117640	It was pushing back against what we call the all-or-nothing principle,
117640	123200	which we defined as the idea that either something has a mind or it doesn't.
123200	131840	So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.
131880	136840	And we argued that this was not the best framing to think of,
136840	142960	especially, and family, our systems that seem to have sophisticated behavioral capacities,
142960	146280	like large language models or AIC systems in general,
146280	150400	where we don't want to take various cognitive capacities of the package
150400	154160	and package them into this idea of a mind, where either you have the mind or you have them,
154160	159920	then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc.
159920	163840	planning, memory, theory of minds.
163840	169280	So, we thought, as a remedy to this kind of all-or-nothing approach,
169280	173840	we argued for what we call the divide and conquer strategy
173840	177040	when studying the cognitive capacities of these systems,
177040	182480	which involved looking at these capacities on a piecemeal basis, case-by-case,
182480	186080	with an open-minded empirical approach.
187040	193040	Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds.
193040	203760	Yeah, I mean, we made one point in there about why it is that people feel so torn about
203760	211600	reactions to large language models, and we said a little bit about the psychology of essentialism,
211680	218960	which is the idea that we naturally categorize especially living things
219920	226320	with respect to a presumed essence. So, we gave an example of an oak tree, I think,
227920	234880	and we said that what people tend to think as they grow up and learn about the natural world is that
235600	245280	an oak tree remains an oak tree regardless of changes to its observable properties,
246080	255200	and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has,
255200	260960	and there's some experimental psychology and developmental psychology showing that we
261920	271360	have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative
271360	276640	explanation for why the literature on large language models is so torn, and some people
276640	285760	are quite dismissive, and other people think that it's a step away from AGI. It's that if you
286720	293840	feel like you've got to put large language models into one of two boxes,
294960	303600	the box that has the essence of mindedness for the box that lacks it, then you will be forced
303600	310240	either to say it doesn't have what it takes to do any of the things that we associate with
310240	318640	having a mind such as reasoning, or it has the essential characteristics of mindedness,
318640	323360	and therefore we should expect it to have all of the other properties we associate with
323360	329920	mindedness as well such as consciousness or understanding or whatever.
332480	339360	Right, I think it's worth emphasizing as you did that the background motivation for starting to
339360	346080	write on this general piece in the first place is indeed that the general discourse on AI systems
346080	354080	and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so
354080	360160	you have one ahead people and one ahead people arguing that these systems are no more than
361120	366640	so-called stochastic parrots that are haphazardly stitching together samples from the training data
366640	372880	and regurgitating them, or that they are no smarter than toaster or that they only do next
372880	380560	stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form
380560	385840	of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum,
385840	390080	on the other end of the spectrum you have people arguing that the systems are haphazardly
390080	395120	jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to
396160	402480	parrots, it's literally the title of a paper by Microsoft on 24,
404960	410800	and many people hyping up the capacity of the systems in a way that might seem very speculative
410800	416960	and untethered from actual empirical results, so there is this huge gap between these two positions
416960	422000	and there's going to be a very rich and complex and nuanced middle ground that is underexplored,
424080	430640	or perhaps I think we did make that point, if not explicitly in the published piece
430640	435920	and in some draft, that there's something reassuring about being able to make definitive
435920	442240	claims about what these systems are and what they do, so either they're re-unsophisticated or they're
442960	451760	very much like us and either of these claims kind of meet somewhere in an way in saying that we
451760	456960	have a clear idea of what the systems do and what they are, and I think it's a little more
456960	466320	epistemic and comfortable to say we have to study them empirically and find out what they
466320	472080	can or cannot do and why and what are the underlying mechanisms, and we simply don't know
472800	478800	a priori just by looking at the architecture, the learning objective, the training data,
479440	484480	these sources of evidence are insufficient to make the definitive claims about what these systems
484480	491280	are capable of, so I think that's part of the big part of the motivation and that fits into that
492080	500320	more academic paper as well. Yeah, one other small side note which we don't make
500320	506640	in the paper but I think might be relevant, especially for people working in philosophy,
508080	513760	LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which
513760	521840	is fitting, not only because they're so new and different but also because they are artifacts,
521840	526080	right, they're things that humans have constructed and engineered and
528880	534800	we don't have a thorough understanding of how they work, I mean mechanistic interpretability and
534800	543120	various behavioral research is helping us improve our understanding but on the whole
543120	546560	our understanding is not nearly as deep as hopefully it one day will be
548240	553520	and this is by itself a really strange situation to be in that we've constructed an artifact that
553520	564960	we only partially understand in the in the past artificial intelligence was seen as a way of
566800	570160	constructing something like an epistemic assistant, right, something that will
571120	577600	help us but not something that will kind of alienate us from the process of coming to know
577600	584400	about the world so I think there's an extra layer of discomfort built into thinking about
584400	594640	large language models and that may also play into the the divisiveness of debates about what they
594640	603200	can do and just to add to that I guess we should be clear that this does not entail in any way that
603200	610800	we think the systems are so completely alien and beyond the reach of our current understanding that
610800	617680	anything goes and that they could very well be you know have like human-like intelligence or
617680	621840	superhuman intelligence and we simply cannot say whether or not they do or because that's
621920	628080	sometimes what you see in some outfits where people frame these systems as noble alien forms of
628080	632960	intelligence that we have created but do not understand our control and that is you know
632960	638320	as a slippery slope that leads some people to then claim that they have all these quite magical
639040	643120	abilities and that's not all what we want to say here and in fact we want to resist
644080	648800	yeah yeah yeah so if we we think that that's just as much of a cup out as
650720	656640	completely dismissing a priori what these systems might be capable of without doing the
656640	660880	work of looking into the capacities with behavioral and mechanistic studies so
660880	664640	we very much want to resist both extremes of the spectrum if that makes sense
666720	670240	okay so now should we move towards the content of the current paper
670240	677040	sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism
677040	687280	and anthropocentrism and then you can take the next step so everyone is aware of the problem of
687920	693440	anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting
693440	702720	human qualities onto something non-human and it's quite easy to especially when you're having a
702720	708880	productive successful exchange with a large language model it's easy to slip into this
709920	718160	interpretive mode where you reason about the responses of the large language model
718800	727680	as if they were coming from an agent just like you and maybe that's a useful thing to do in some
727680	733840	circumstances but from a theoretical perspective it's certainly a mistake because large language
733840	743120	model is radically unlike you know a human agent in all sorts of ways but that's only one form of
743120	749520	sort of human-centric bias the other one is anthropocentrism or what we call in that box
749520	757520	article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that
758960	766160	the human way of solving problems is the gold standard of solving problems generally so that
767040	773840	to the extent that a system solves a problem in a way that diverges from the human strategy
774800	787120	it's only using a trick or a faux solution it's it's not using a deep general rational strategy
787440	801280	and in the debate about what large language models can do we think that the anthropomorphic
801280	806240	bias is pretty well recognized and the anthropocentric bias is not so well recognized
806240	814080	and so part of this paper is is or the main idea behind the paper is to present a systematic
814080	819600	analysis of anthropocentric bias how it comes about and how to push back against it
821600	826000	right and we we want to be very clear and hopefully we're playing the paper that
826560	831840	the reason why we focus on anthropocentric bias here is just because it is I think as
831840	839840	Charles mentioned less discussed and less recognized or some forms of it are less
839920	844960	recognized and we make we propose this new taxonomy but it's not at all suggest that it's
846880	853280	more problematic or more important than the anthropomorphic bias that's well discussed in
853280	863040	the literature of the anthropomorphic biases so in other words this is not you know to frame things
863040	867920	in in this slightly problematic dichotomous way of thinking that's common in the discourse
868000	877360	on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it
877360	883040	is pushing back against a certain form of dismissal of anthropocentric biases that
883040	888960	only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a
888960	893600	little bit already with the first one we make here in the paper about a performance competence
893600	898080	distinction which is a nice way to bring about both anthropomorphism and anthropocentricism
898080	906720	regarding LLM so this distinction is a very classic distinction in linguistics and cognitive
906720	913360	science and it has already been applied to AI systems in the neural networks productively
913360	919360	like Charles Farson so there's nothing really new here but the distinction comes from
919360	925200	Noam Chomsky originally and the idea is that performance pertains to the external behavior
925200	933120	of a system in a particular domain and competence is the kind of setup underlying knowledge and
933760	940560	computations or mechanisms that enable the system to achieve that behavior
941680	946160	and a familiar observation in linguistics and cognitive science is that there is a double
946160	956880	dissolution between performance and competence so if you take for example language I might during
956880	961600	this very podcast make some grammatical mistakes or some other mistakes in fact I've already
962640	968320	misspoke in a few times I think and repeated myself so I made performance errors but this
968320	973200	does not entail necessarily hopefully that I'm an incompetent language user and that I don't have
973280	982080	in the language the competence so that's a well recognized dissociation you can be competent
982080	989840	and yet make some errors and the reason for that is that there might be some additional factors
989840	995840	that are unrelated to the underlying competence that might impede on my performance so for example
995840	1003520	I might be distracted when I speak or there might be other effects on my speaking performance that
1003520	1012720	don't actually originate from a lack of competence but just impede on the idealized expression
1012720	1017920	external manifestation of my competence and this is why I misspeak but it's also why recognize
1017920	1024400	that you can have good performance without competence so we give here the example of a
1024400	1032000	student cheating on a test or memorizing test answers by brute forcing the test to slightly more
1033520	1038880	I guess gray area but at least in the cheating case a student can ace a test without being
1038880	1047360	competent at what the test is actually testing for and it's also well acknowledged in cognitive
1047360	1058960	science that there can be instances like this throughout you know like that can be manifest
1058960	1064800	certain experimental settings where the test subject is right for the wrong reasons as it were
1065440	1069840	namely it's doing well it's exhibiting good performance never realize the underlying reason
1069840	1075600	for the performance is that there was some perhaps some curiosity that the experimenters
1075600	1080320	all the scientists haven't thought about that could account for his good performance but
1080320	1085760	but doesn't actually amount to whatever competence they were setting out to test
1087760	1091680	so we we start by saying well this is what we could nice because the mistakes this is the
1091680	1095840	association that is like supplied to humans across the board you can have performance
1095840	1101120	without competence good performance without competence and you can have that performance despite
1101120	1106880	competence now when it comes to other lamps the point we make just going to scroll as we go
1108240	1115760	we have some figures to show later but the point we make is that generally people stress the
1115760	1120320	dissociation apply the distinction to other lamps the stress decision only in one direction
1121120	1126800	unlike in the case of human where it's bi-directional and so what people do generally is to say well
1126800	1132160	other lamps famously you know if you look at the gpt4 technical report and and vice other
1133520	1139440	any any any report about a new state of the art alone they are getting really really good at a
1139440	1146480	number of tests and about benchmarks and even human exam examinations human exams the bar exam
1147120	1154480	medical exams etc so they can get a really good performance test that we tend to think are really
1154480	1162080	difficult tests that one can only pass at least a human could only pass if they have
1162640	1166000	a really significant nonchalant amount of competency in particular the main
1167760	1172960	and the point that is often made when it comes to other lamps is be careful slow it on and try to
1172960	1178480	find out why the model is doing well on that test because there are various reasons why it could do
1178480	1185520	well that do not actually indicate that the model has the underlying competence that the test was
1185520	1191120	designed for when it comes to humans so one big concern for example is data contamination
1191120	1199440	where very large language models train on internet scale data can easily be trained on some test
1199440	1203680	items from common benchmarks that leak into the training data such that they can then do really
1203680	1211280	well on the on the on the benchmark just because they've essentially memorized test items and there
1211280	1218400	are other more subtle more subtle reasons why performance could be very good for the wrong
1218400	1225440	reasons so that's very well recognized and a lot of the people who push back against anthropomorphic
1225440	1232160	bias when it gets to other lamps make that point be careful do not take on another anthropomorphic
1232160	1237920	attitude to the systems the reason why they do well is not because they have human-like intelligence
1237920	1245040	or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant
1245040	1250480	reasons that account for the good performance now when it comes to the other dissociation of the
1250480	1256480	other dissociation of the other direction there are people are very reluctant to apply to a lamps
1257280	1262160	and we think it's because essentially people think in a human case you can make sense of the
1262160	1267680	idea that the human could do badly on a test or corporate could perform badly on a task
1268640	1274080	and yet have the competence that you're trying to test but there might be some auxiliary factors
1274720	1283440	such as working memory limitations attention deficits etc that could impede on the performance
1283440	1294720	but in the language model I think what we argue in the paper is that
1296480	1300240	people don't a lot of people don't think that there is an analogous
1302400	1306560	mechanism at play where there could be some kind of auxiliary factor that impede on performance
1306560	1312080	performance is what you get what you see is what you get and so the performance is a direct
1312800	1322320	manifestation of what the system is computing and if you have performance errors
1323040	1326640	that can only be explained by the lack of competence because there is no additional
1329840	1334960	independent factor or module that could impede on the performance
1335680	1342160	sources of interference you might say yeah so yeah I mean I'll
1343680	1350320	pass it over to you Charles I just wanted to set up this distinction yeah yeah
1352320	1358640	right so I mean if you think about a traditional computer program
1359520	1366960	well at least if you think about a simple computer program it's odd to think of it as
1367520	1372240	some sort of complex systems where it's a complex system where one part of it could sort of
1372240	1377040	interfere with the workings of another part of it but one of the points we want to make is that
1377600	1384000	something like that is a realistic possibility with large language models
1384560	1391120	um okay but I suppose the next part of the paper goes into a taxonomy of
1391920	1397840	anthropocentric bias and the first sort of overarching point is the distinction between
1397840	1405840	type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance
1406480	1413680	failures designed to measure competence always indicates that it lacks
1415520	1423280	that competence and so we before we so we'll say something about three different kinds of
1423280	1430160	type one anthropocentric bias but first a background point which is that
1430400	1440000	um whenever we think it's possible to give a mechanistic explanation of some
1440720	1449360	complicated phenomenon we always have to foreground some factors some variables
1449760	1462320	uh and background others and um the properties that we push into the background
1466320	1471200	nevertheless matter we're still making assumptions about the nature of those properties
1471200	1476080	when we try to articulate what's going on with the other properties that we're paying more attention
1476080	1482160	to and if assumptions about those properties in the background turn out to be wrong
1483120	1492400	then those mistakes will corrupt our explanation that attends only to the foregrounded factors
1492400	1499600	so that's a little bit abstract let me just give you a simple example um in comparative cognition
1500560	1508160	one famous uh behavioral experiment is the mirror test for self-recognition
1509040	1518640	so the question is roughly do non-human animals have something like a concept of self
1519840	1529520	and the strategy is to put some sort of mark on their body in original experiments it was
1529520	1536800	a red dot on the forehead of maybe a monkey and or a bird or whatever and then
1538960	1546640	you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark
1547760	1551920	and if it does make an attempt to get rid of the mark that shows that it recognizes that the
1551920	1560320	image in the mirror is an image of itself and otherwise not um so that that's a cool way to
1560320	1569120	get a really difficult and abstract question about the mind of a non-human animal but it presumes
1569120	1576080	or it assumes that animals will care about the fact that they have a red dot on their
1576080	1580720	forehead that they will be bothered by that and be motivated to get rid of it and if that
1580720	1586480	assumption is wrong then they might fail the mirror test for self-recognition for reasons that
1586480	1594800	have little to do with the presence or absence of a capacity for self-recognition until something
1594800	1605760	similar to that is going on we say in large language models so the first example that we give is to
1605760	1620240	do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral
1620240	1628800	task um there will be demands associated with that task that are not directly related to the
1628800	1634240	capacity that you're trying to test so to take the most obvious example that I can think of
1634240	1638720	if you give someone a written test they have to be able to write they have to you know have a hand
1638720	1646880	and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't
1646880	1653040	write then um their failure to fill out the test wouldn't tell you anything about their uh you know
1653760	1665920	academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral
1665920	1672960	tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right
1672960	1679760	away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic
1679760	1688320	but what they do is they give a large language model um the following sort of question uh this
1688320	1694080	is a question it's for a grammaticality judgment so you can see on the image there which sentence
1694080	1699600	is better in english number one every child is studied number two every child have studied
1700400	1707280	answer with one or two and it gives the wrong output but then
1709600	1716320	you can also simply look at the probabilities assigned to each of those
1717520	1727040	sentences within the model and uh figure out directly whether the model thinks that input
1727040	1734240	A is more likely than input B and it turns out that on a wide variety of questions of this kind
1735280	1742640	the direct comparison uh does or the large language models perform better with the direct
1742640	1752400	comparison than they do with the more complex demand for uh metalinguistic judgment so the
1752400	1761280	fact that the model has to process the numbering of the uh options and then answer in terms of a
1761280	1771280	number uh is a subtle but nevertheless um important additional variable in the experiment
1771840	1776320	and that can influence the model's capacity to get the answer right
1776560	1778960	Rafael do you want to add to that?
1780640	1785360	No I think that well maybe we can mention briefly the other example that we discuss which is from
1786480	1794480	this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes
1795680	1801120	the example interesting and even more problematic in terms of comparative psychology which is
1801840	1812720	um one way in which auxiliary task demands can be ignored or disregarded or overlooked
1813520	1820400	is when uh you are doing a direct comparison between humans and other epsilon tasks and
1820400	1826640	the experimental conditions are mismatched in such a way that the task as you set up
1827600	1835040	impose the stronger demands stronger task and auxiliary demands on the LOM than those on human
1835040	1840240	subjects and that's something that that can happen quite often and so there is this this
1840240	1846800	interesting example from a couple of papers originally published by black reds and colleagues
1846800	1853040	from standard hands group where they looked at um the ability of language models to handle their
1853040	1863680	recursion um looking at center embedded closes uh how such closes might um
1865680	1870160	for example when you had a prepositional phrase within the subject of the sentence and the verb
1870160	1877680	might throw up either humans or LOMs into agreeing the the verb in the wrong way
1878480	1887600	so giving the wrong number to the verb for example the keys that the man put on the table
1889280	1894400	here it should be R because keys is plural but because you have close in the middle
1895200	1899680	and if you add more closes like this that are embedded in the middle people and LOMs can get
1899680	1907520	confused and and predict that the verb should be is for example so um they tested this on humans
1907520	1914000	and LOMs and found that on the more complex examples involving complex more complex constructions or
1914000	1921280	recursion humans were doing decently well but LOMs performance was collapsing compared to the
1921280	1928400	single examples and I've heard an opinion from DeepMind um looked into that and realized that
1928400	1933840	the experimental conditions were mismatched so the humans at this very common in cognitive
1933840	1938960	science experiments were getting some training before they completed the test items to just get
1938960	1945600	familiarized with the task so they were given some examples of the task um harsh condition
1946400	1956160	and the LLabs were just prompted zero shots as um people usually put it so just um without any
1956240	1964160	example just point blank and Andrew found out that if you he he he replicated the experiments but
1964160	1971360	I did some proper matched testing conditions for the LL so adding some examples of the task in the
1971360	1977680	prompt when it's known as future counting and with that he found that performance was equivalent
1977680	1982320	in fact the LLM that he tested was slightly better on the more complex constructions than humans
1983040	1986560	so when you match the test conditions here you actually match also
1987520	1993360	at least you it's it's not it's not automatic but you you you can match the test demands I mean
1993360	1997680	it could be that there are reasons why the various experimental conditions would result in different
1997680	2004320	demands for humans and others but you're still in this case even on the playing ground playing field
2005360	2010480	in such a way that you don't find the behavioral discrepancy that you found initially anymore
2013040	2021840	um yeah um good so shall we continue to the next
2022800	2033840	section um so another uh another way that auxiliary or another type of auxiliary task
2033840	2039600	demand is input independent computational limitations um and here we're thinking of a
2039600	2045600	few papers that show that the number of forward passes that the transformer can make
2046320	2053680	influences its ability to find the right spot and parameter space so neural networks are
2056800	2060080	function approximators but their
2060080	2071040	um their ability to approximate a function can be eliminated uh can be limited by the
2073200	2080480	the the number of computations it's allowed to perform and um the
2080960	2088640	uh sort of crucial feature of uh transformers in this example is that
2089680	2097920	um the number of operations that determines the next token is limited by the number of
2097920	2108000	tokens that it's seen so far and it turns out that if you train a transformer with
2109680	2123200	additional meaningless tokens like pause tokens like the word pause you can increase its accuracy
2123920	2132640	across a range of of question types um and yeah this is
2135040	2142960	this counts as an auxiliary task demand in our view because um it's doing something roughly
2142960	2150240	analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry
2150800	2158640	yeah um it's it's doing something like giving the model uh time to think and um
2160400	2170080	yeah so so you might think that the absence of that additional inference time is a factor
2171280	2177840	that is not directly not conceptually related to its capacity to answer
2178080	2184720	uh a question like the one on the screen um you know a simple
2184720	2192240	earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy
2192240	2198560	is is a nice one time to think because if you if you tested a human on even a simple mathematical
2198560	2205200	questions or any any task really and just ask them you know tell them they have like one second
2205200	2212800	to just blow it out answer performance would probably be pretty bad um and you can think of
2212800	2219360	asking an LLN to answer a question point blank as very very loosely analogous to that and obviously
2220240	2225600	this is an analogy and there are very important differences here but I think it's a helpful
2225600	2234080	heuristic to think about what is um what is going on roughly here and then we uh in in both cases
2234880	2242000	the system the human or the LLN does not get the chance to perform the necessary computations to
2242720	2248000	derive the correct answer and so yeah what we talk about in the paper is that you have these
2248000	2255680	experimental works during that if you ask a question to a language model um the amount of
2255760	2261120	tokens it's a it's it's that allows to generate before providing the answer
2263040	2268560	makes a difference to how accurate it is so if it uh it just generates a few tokens then have to
2268560	2272400	give an answer or even if you just have to give the answer point blank with the very first token
2273200	2277280	it's going to be less accurate that if you give it a chance to generate a number of tokens before
2277280	2284320	giving the answer so the usual way in which this is understood is that when you ask when you
2284560	2288400	you you allow the LLN to generate a number of tokens before giving the answer or you even
2288400	2293600	prompted to do so you say things step by step for example um that's not as chain of thought
2293600	2299200	prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace
2299760	2305680	or what looks outwardly externally like a reasoning trace in the output before giving an answer
2306720	2310000	and we know that chain of thought prompting increases performance accuracy
2310720	2322560	um but what was found by a couple of papers um is that the mechanistic influence of this process
2322560	2327360	is not entirely due to the nature of the tokens that are generated in this reasoning trace
2327920	2334960	in other words it's not just that the LLN has to generate the right tokens corresponding to
2334960	2341120	different steps of reasoning before giving an answer in fact the very back that you allow the
2341120	2347440	LLN to just generate tokens any token before giving an answer from a mechanistic perspective
2347440	2353920	affords the system to perform additional computations that can complete the computational
2353920	2359520	circuit that otherwise would get a chance to be completed and to derive the correct answer
2359520	2364400	so as Charles mentioned you can have you can set up an experiment when you have the LLN just
2364400	2371600	generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the
2371600	2378560	answer and the the more dots you allow before the token gives the answer the greater the
2379200	2385200	expressivity of the system and the more um the more kinds of programs problems you can answer
2386400	2390720	and so as Charles mentioned every time an LLN is generated in a token the LLN is performing
2390720	2396720	one forward pass and so the more tokens it's generating the more forward passes it's doing
2397280	2402160	and one way to think about what's going on here as well is that having these additional forward
2402160	2407360	passes where you you feedback the whole input sequence plus the previously generated tokens
2407360	2412560	to do the system to generate the next is also a way to introduce a form of recurrence in
2412560	2418080	transformers that are not in terms of the architecture of recurrent networks so that
2418080	2426320	increases the expressivity and you know in complexity of your unique terms and yeah there is
2426320	2433680	there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation
2434480	2439520	that again we think is very very loosely analogous to prompting a human to answer a
2439520	2445200	point on the question without thinking so that's the next sense an auxiliary factor because
2445200	2451200	if you give the LLN the opportunity to generate enough tokens it might have the competence
2452480	2458320	to solve a task but you might not see that otherwise and you might get performance errors
2458320	2459920	but you do think it's incompetent
2463040	2470720	all right yeah okay um so the the third type of type one anthropocentric
2471600	2477520	bias that we talk about is mechanistic interference and so this comes from
2478480	2482160	the mechanistic interpretability work and the basic idea is that because
2484080	2489280	large language models are capable of in-context learning they can learn different strategies
2490160	2494480	for solving a different particular type of problem and the strategy that they
2495200	2501680	implement at a given time can be different so you can talk about this in terms of
2502400	2509280	virtual circuits that are formed inside the language model and there's some interesting work from
2510240	2516400	nil nanda and others showing that in some circumstances these two circuits can compete
2516400	2525680	with one another so at a certain level of uh or after a certain amount of training
2526640	2533840	you get one circuit operative after a bit more training you have two different circuits
2534880	2542480	but they're uh the first circuit is still sort of dominant and then after additional
2543440	2545040	training the model
2548000	2555520	converges on on the second circuit and the first one slowly gets sort of
2558160	2564240	it sort of ceases to influence the internal operations of the model and it's only once
2564320	2569600	you reach that third phase at which the
2572480	2579200	the benefits of the second circuit with respect to the first become visible
2580240	2585600	so you can you can show using decoding work that the second circuit is there
2586560	2592400	uh before you can show behaviorally that the second circuit
2594720	2603440	yields better performance accuracy on on the task so um I suppose there's a combination
2603440	2609840	of two ideas here one is that um there are different strategies a model can
2610320	2621600	implement for solving a problem we can detect those strategies internally using decoding methods
2622400	2626640	um so three ideas and then the third is uh
2630400	2633280	a good strategy can be
2634240	2641120	uh present in some sense in the model um before it has had the chance to influence
2641840	2649440	behavior um and and so this is just another way that the link between
2650000	2655440	performance and competence is shown to be more complicated than I might seem at first
2656080	2657440	graph
2659760	2665120	and um yeah just to to clarify one thing so the circuits are just um
2666480	2670240	you know ways to think about the causal structure of a neural network and
2670240	2675360	there's essentially computational subgraphs of the network that have a specific function
2675360	2679280	you can think of a circuit as implementing a particular algorithm or set of computations
2680000	2684000	um it's a part of what people are interested in in this mechanistic interpretability literature
2684000	2690320	that we build on people like Neal Mandatrisola and others is reverse engineering the circuit
2690320	2697440	steps in deep neural networks and large language models um peer to implement certain well-defined
2697440	2703680	algorithms in some cases at least um and the emerging picture that we build on here is that
2705120	2711520	there is a lot of redundancy built into neural networks as they learn to perform a task
2711600	2718240	optimized as a function that in many cases translates into redundant circuits that relate
2718240	2727040	to the same tasks the same kinds of um the same kinds that we put out with my things and uh for
2727040	2731840	these circuits might be somewhat identical circuits that are just redundant or they might be
2732480	2737280	different algorithms just to do to do a similar thing and different strategies to solve some
2737280	2741280	problem I swear maybe one will be a bit more approximative and the other one a bit more exact
2741280	2749760	more computationally intensive so that's where you can have some interference um where one uh
2750720	2758160	or at least some competition where once your kid takes over another and um such that the other
2758160	2763200	becomes kind of you know it's there it's latent in the system but you don't get a chance to influence
2763200	2768800	behavior on a specific input so you can get a performance error for that reason and these can
2768800	2774960	combine with the other things we mentioned here so things like task demands the first thing we
2774960	2781280	discussed as well as the number of tokens you generate both of these things could cause a
2781280	2787120	particular circuit to take over another um so it's it's we can think of this holistically as
2787920	2792320	perhaps if you ask a question point blank to a model without letting it generate
2792400	2797120	bunch of tokens before giving an answer then one particular approximate circuit might take over
2797120	2803120	that gives the wrong answer if you let it generate more tokens then another more exact circuit might
2803120	2810800	be given a chance to um it could influence the output using the right answer and similarly with
2810800	2822320	task demands uh strong task demands might uh in some cases um impede on the uh triggering of a
2822320	2828080	certain circuits that would otherwise have given the right answer um so that could be the case
2828080	2833520	perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts
2834080	2840000	might actually prime the word circuits to solve the task about complex recursive cases in the right
2840000	2848240	way um so yeah these are the three main auxiliary factors that relates to what we call type one
2848240	2853680	anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should
2853680	2860880	be quick on type two do you want to uh pick fix things after child yeah so type one uh deals
2860880	2869920	with cases where performance of the model is um weak compared to humans so the model doesn't do
2869920	2877760	so well um and then type two is when the model does do well but nevertheless is different in some
2877760	2886000	respect from the uh performance profile of the human or we have some evidence to think that the
2886000	2894560	model uses a different strategy than humans typically use and um the idea is that um even
2894640	2900800	once you hold performance equivalent or average performance equivalent um you know making a different
2900800	2907440	pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability
2907440	2917840	work any deviance from the human strategy is evidence of fragility or only a trick solution
2918320	2929680	um and uh this point is a bit more philosophical i suppose but the um idea is that
2931760	2941280	the human strategy for solving a problem um isn't necessarily the most general strategy
2942000	2952880	for solving a problem and uh what matters is whether the strategy that is pursued by the model
2952880	2960240	is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human
2960240	2973280	strategy um yeah and we end the we end the paper by considering an objection um which is um
2975760	2986000	why um like given that um in humans we study cognition largely through language
2986960	2993760	um and given that elements are trained on language or um linguistic outputs from
2993760	3001120	humans um isn't it appropriate after all to treat um human cognition as the correct or
3001120	3007760	appropriate the obstacle to study elements and we to that we answer that it depends how we think
3008320	3015040	of that dialectic um so we acknowledge that there is there is no
3016480	3020720	really other option than to start or investigation of cognitive abilities in algorithms
3021280	3028080	but with reference to human cognitive abilities using human cognitive abilities as some kind of
3028080	3033360	realistic or reference points things like theory of mind memory metacognition
3034320	3039520	various forms of reasoning etc that are familiar to us because we humans have them
3040880	3044960	and this is the same thing by the way in animal cognition for example or in developmental psychology
3044960	3051840	where in any comparative psychology setup um the reference point for what concepts
3051840	3061360	psychological capacities initially at least um is necessarily tied up with our conception of what
3061360	3067120	we human we humans have in our repertoire of cognitive capacities but we emphasize that this
3067120	3071760	is only the starting point so here we've over written from uh the philosopher Ali Boyle who
3071760	3076720	calls this investigative kinds investigative cognitive kinds we start with a cognitive
3076720	3083280	kind like memory or metacognition episodic memory metacognition theory of mind um as
3083280	3087200	as an investigative starting point the starting point of the investigation and then we have
3087200	3094160	that we we can try to start operationalizing operationalizing this concept this kind this
3094160	3100480	cognitive capacity in an experiments testing the algorithms on it with an open
3101280	3107280	mandated empirical approach and then based on the results from that each relatively refine
3108560	3113920	the capacities that we are the capacity that we're targeting or the definition of the capacity
3114080	3121120	targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions
3121120	3130960	that we have such that as the experimental um project runs a course or as as we make as we
3130960	3139360	we get more results and refine our concepts we may end up with um something that no longer
3139360	3148320	looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS
3149120	3157040	but ends up looking like looking for something that some capacity that is that shares some similarity
3157040	3162560	with human like human episodic memory but is different in other respects um and so we can
3162560	3168880	gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric
3170000	3174480	so we emphasize that it is kind of due to feedback look here that's that's that's premised on open
3174480	3179920	minded empirical investigation that doesn't settle this question a priori but has to start
3180800	3184240	as a necessary starting point with the the reference to human cognition
3185120	3186800	i don't know if you want to add to that joss
3189600	3194400	um no i think that's pretty good maybe we should uh move on to questions
3194640	3205840	and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't
3205840	3209280	move it awesome okay
3213520	3220640	yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat
3220720	3227760	but first i just wanted to read a short quote from the 2022 active inference textbook they wrote
3229280	3237200	um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote
3237200	3242880	too much effort to modeling isolated subsystems e.g perception language understanding whose
3242880	3250000	boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive
3250000	3254880	creature perhaps a simple one and an environmental niche for it to cope with
3256080	3263440	so it's interesting about the approach that you're taking this is kind of a simple synthetic
3263440	3271440	iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena
3272160	3277840	because there is something and and so i saw in the presentation paper kind of this call for like
3278400	3285680	deliberate investigation rather than just chopping up the iguana a priori with a framework that
3285680	3294000	that applies to humans or that centers humans or or that just uh soothes the epistemic challenge
3294000	3305360	that's presented okay okay first question from dave he wrote
3308480	3314400	have you looked at daniel denitz's distinction between competence without awareness and
3314400	3321120	competence with awareness he expands on this in the 2023 from bacteria to Bach and back
3321920	3327920	i find this much more valuable than chomsky's highly problematic performance without competence
3327920	3334960	a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this
3334960	3346160	competency uh well maybe i'll let you think uh that one is i can trust because you're
3346160	3353760	you're maybe more within it than i am but i'll just say um awareness is a very polysemous
3353760	3360080	term like many terms in philosophy of minds but partially this one more than many others i think
3360800	3368400	so um it can mean a lot of different things in all of context here we don't focus on things
3368400	3374800	like consciousness because i think we probably both agree that it's a less tractable uh maybe
3374800	3383120	empirical problem to try to assess the presence or absence of consciousness in language models
3383120	3387840	even though many people are interested in that we think that we have more hope of making progress
3387840	3395360	in the near term with more well-defined cognitive capacities or cognitive functions and things that
3395360	3404480	relate to forms of certain forms of reasoning and viable binding etc um so we our framework and
3404480	3409760	principle would apply to things like consciousness or as you put awareness generally speaking but
3410720	3415040	we don't really focus on that for examples the other quick thing i'll just mention is that i
3415120	3421600	seem to remember that the phrase from then but again i'm not a then scholar was competence
3421600	3428640	without comprehension um which seems a little different from competence without awareness
3428640	3435360	perhaps depending on how you think of comprehension um and yeah i think that does i think it is a
3435360	3442160	very interesting phrase that it does um in fact i had this project that's unpublished with
3442160	3449600	chris dolega who i think you had on the podcast as well um on semantic competence in language models
3449600	3457040	where we use that phrase um to kind of avoid taking its stance on the kind of messy
3458160	3463680	muddy question of whether hella let's understand language which builds in all sorts of assumptions
3463680	3468400	including about consciousness actually for some people like chancel um and we focus on
3468400	3473920	the more restricted notion of competence and i think our paper here also has that property that
3473920	3478880	would if we have originalized competence we end up operationalizing competence in terms of
3479440	3484640	the sets of knowledge of the mechanism and mechanisms that enable a system to generalize
3484640	3489920	well in a given domain basically and in a way that's a supposed evolutionary compared to
3490880	3494480	some more expensive understandings of competence that
3495840	3502400	we need to comprehension or understanding more well but i'll let you take that one charles
3504880	3511200	no yeah that was that was good um the phrase you know the distinction that denadra's is between
3512160	3519200	competence with comprehension and without and i think um competence with comprehension is the
3519200	3525120	ability not just to pursue a strategy that's successful for solving a problem but to um
3525120	3532640	articulate the strategy in such a way that you could teach it for example and um humans only
3532640	3540960	sometimes have competence with comprehension we have many competences that lack comprehension
3540960	3548720	right um you know when we learn to walk for example um we have an amazing competence that we
3549520	3555440	still can't quite translate into robotics because we don't fully understand how it works
3557600	3564800	and when it comes to our language models i think we should
3565440	3579440	not expect uh comprehension i mean they have a an amazing suite of competences if you thought that
3579440	3584160	they also had comprehension then i suppose you would think like well if you want to understand
3584160	3590720	how a large language model works you can just ask it but that's that's a bad strategy nobody nobody
3591280	3597840	about um how a large language model works so so they're on the um competence without
3597840	3608960	comprehension side of things um and in order to figure out what in order to figure out what
3608960	3613360	the mechanisms are that enable its competencies we have to pursue strategies that are broadly
3613360	3618720	similar to the strategies we use in you know cognitive psychology or cognitive linguistics
3619440	3623440	um and you know we have to run experiments so i think that that's all very compatible with
3623440	3631920	Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was
3635200	3642560	quite influential to me and we actually wrote a commentary together which pushes back a little
3642560	3649040	bit on a simple understanding of this distinction so we were looking at um the evolution of
3649040	3658320	metacognition and basically what we argue is that um given the gradualism of evolution there
3658320	3663200	must have been something in between base level cognition and metacognition so we shouldn't
3663200	3672800	see that distinction as black and white and um you know i think that if you want to contrast the
3672800	3688400	sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific
3688400	3697360	uh concepts at her disposal with you know a non-human animal then this strong distinction
3697360	3704960	between competence with and without comprehension is reasonable um but in the space of all possible
3704960	3713280	minds we should be open to the view that there can be you know semi-competent um forms of cognition
3713840	3720240	and just to put it on this uh it occurred to me while I was listening to you as well that um
3720240	3724400	the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and
3724400	3731120	Frank example is a nice is a nice example where in order to give the metacognistic judgments
3731120	3735120	correctly so see that that you would need competence with comprehension because you need to
3735920	3741760	understand not only be able to to come to to agree the verb with the subject but know the rule and
3741760	3750800	know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so
3750800	3755280	when you find that the L.M. can do well at the low task the member of the task and at the high
3755280	3761280	task development explicit metacognistic judgments in some way that's an example of the L.M. having
3761280	3773600	competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given
3773600	3781840	that LLMs inherently reflect anthropocentric biases due to their training on human data and goals
3781840	3787040	how can we ensure that their inter model discourse aligns with humanity's values
3791760	3792400	um
3795280	3801440	so the inter white discourse right be inter model discourse perhaps amongst the models
3802560	3811520	I see I see like in that farmville paper yeah the small the yeah both generative agents get
3811520	3819520	the small ones uh yeah I think that's beyond the scope of this paper to be honest but um
3819760	3826560	I mean we could use about it yeah but I don't know that we have I don't know this project
3826560	3832400	has much I mean I I think we both have an interest in the alignment problem uh independently of this
3832400	3838320	project but I don't think this project has much to say really about this I'm not sure what you think
3839280	3840880	uh yeah yeah I don't
3841360	3852240	yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise
3852240	3859840	into LLM training that was the section about the extra tokens do you see any analog to
3859840	3863600	intermittent reinforcement to uncertainty tolerance
3864480	3872000	because you mentioned the extra tokens in the chain of thought and how that could also be replaced
3872000	3881280	by by dot dot dot dot dot and so like what is that telling us about model training when um
3882000	3886960	it seems like there's some situations where adding superfluous tokens would diminish signal
3886960	3891280	in data sets but then here are other situations where it seems to actually help
3894480	3903440	um um yeah so in that particular paper I think it's called thinking dot by dot and there is a
3903440	3913040	subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly
3913040	3922480	what they did is that they they introduced just this one field of token swan meaning this token
3922560	3929040	just to hold but just a dot and they trained the model to give an answer after producing
3929040	3935280	a certain number of dots that's not just like introducing Rambam and Gibberish in your training
3935280	3942480	data it's actually quite a specific intervention that forces the model to um learn to perform
3942480	3950800	certain computations before giving an answer um so so it makes sense to me that this couldn't
3950800	3955200	diminish performance and like you you could do that from that it's not quite the same as just
3955200	3960960	having that training data right um just because the token seems meaningless it's a field of token
3960960	3967760	to dot um it's not just random gibberish it's going to throw off the model and and impede its
3967760	3974480	its uh the optimization of its learning function or at least good downstream performance um but what
3974480	3978320	it's going to do is going to force the model to learn that when there is a dot token it can
3978320	3982560	allocate computation with its attention heads and other parts of the architecture in such a way
3982560	3990640	that it's um getting towards deriving the correct token when it's finally producing
3990640	3996240	the token that matters and that's meaningful after the series of dots um yeah I don't know
3996240	4001600	Charles if you have another answer yeah I mean I think it's an important question because
4001600	4009840	a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless
4009840	4017840	symbols to an LLM input you might very well think that this will just weaken the signal to
4017840	4028800	noise ratio and degrade model performance so it's against that background that the empirical result
4029040	4036000	doesn't degrade model performance um ought to be regarded as an important clue about how the model
4036000	4042320	works so I think that the the intuition behind this question is indeed part of the interpretation
4042320	4049200	of the empirical results right it's surprising for exactly this reason and then the theory that's
4049200	4054160	supposed to you know well this is an active inference podcast right so the theory that's
4054160	4060480	supposed to help rid some of the surprise here is um the idea that
4064160	4070720	given the uh architecture of a transformer where it's it has to go through all the tokens
4071360	4079440	in every cycle um having these extra tokens gives it uh sort of more computational bandwidth
4079440	4086400	and therefore more expressivity or more capacity to uh you know locate uh the right
4086400	4094640	spot and parameter space and and even that in a way reminds me of so it's not just that dots
4094640	4098800	improve performance it's not it's that it was like you mentioned it was trained to have that
4098800	4106320	and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes
4106320	4112880	verbatim while you're processing so that's kind of like a filler or more of like a sort of
4113680	4122160	that was a great question it's like these are linguistic paddings that that do create time
4122880	4131440	to to get to the meet so not only does it signal and signpost if it's being trained to have that
4131440	4136640	meaning which then questions like so that it wasn't a meaningless dot if it if it had a um
4136640	4146000	a cognitive or even like a a semantic um aspect I had a question how do you feel like in this
4148080	4155520	era Cambrian explosion of diverse intelligences how can we understand capacities
4156320	4166880	when they seem so conditional upon the setting and how the system of interest is interacted with
4167600	4174160	like what are the practical implications for people who are studying LLMs and other
4174160	4179920	synthetic intelligences from like a safety or reliability or performance perspective
4179920	4185680	that was we're gonna
4187280	4188320	no I was drawing it to you
4190400	4194560	so so so just want to chat to some of the questions so the question is
4198560	4202080	how the how is the notion of a capacity
4203040	4210480	um changing when we have such different systems that seem to have intelligent behavior
4210480	4216640	yeah and it's so dependent upon potentially initially unintuitive
4217840	4224800	ways of interacting so how can we understand the reliability and the performance and the
4224800	4232640	capacity of of a model other than for example by exhaustively inputting prompts
4232640	4239920	which can't really happen what what could we really say or no and or just how do you feel
4239920	4245120	that this work re-enters into the ways that people practically are using the models
4246400	4251200	right okay yeah uh it's the interesting question so the first one the question is I think
4252000	4257760	you know part of the background assumption from for this paper that I've explicitly
4257760	4264640	defended in other work is that behavioral evidence is simply not sufficient in most cases
4264640	4271760	to arbitrate disputes about capacities of LLMs when it comes to human cognition
4273760	4279280	we do have to rely a lot on on psychological experiments that are ultimately behavioral
4279360	4285520	and we do also rely on self-reports in a little more than we can when it comes to LLMs because we
4287280	4293200	despite the move away from relying on intuition and introspection in the history of psychology
4293200	4299440	it still has a role to play but we've by and large contributed to us by behavioral experiments
4299440	4305760	that get increasingly sophisticated to try to reverse engineer what's going on inside the black
4305760	4315600	box when it comes to LLMs partly because that's so different from us um relying exclusively on
4315600	4322160	behaviorism is even more difficult because we have even less of an idea of what might be going on
4322160	4326240	inside the black box and whether it's anything like what's going on inside all black box and we
4326240	4332880	have reasons to think it might be very different so I think I think we both agree that we have to
4332880	4341760	supplement this with mechanistic work that's essentially involve performing causal interventions
4341760	4347520	on the inner mechanisms on the inner workings of the systems so decoding representation and
4347520	4352160	computations that the systems that are in principle available to the systems and then
4352160	4356560	intervening on them to confirm hypotheses about the causal role of these representations and
4356560	4363200	computations and we have methods to do that and partly what we can be a little optimistic about
4363200	4367280	this project even though it's it's it's extremely challenging especially to be scaled up to large
4367280	4373120	models is because unlike what's happening in your sense with the brain where the range of decoding
4373120	4379280	methods and intervention methods we have is extremely limited but for ethical and for simply
4379520	4386720	um practical reasons that we don't just don't have ground truth access to activations in neurons
4386720	4393440	at least that easily and we also are generally unable to make specific interventions on
4393440	4397840	activation in the brain uh when it comes to algorithms we have full ground truth knowledge
4397840	4404400	of all activations of every single part of the network and we also have full access to all of
4404400	4410240	it for interventions at inference time so that that opens up a whole new range of things we can do
4410960	4417760	and that enables us to go beyond behavioral studies and actually decode these features and
4417760	4424080	circuits or as researchers put it in the literature or as philosophers would generally put it
4425280	4429360	representations and computations that the system is actually making use of and try to reverse
4429440	4433920	engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem
4435040	4441520	projects that we have with Charles is to um suppose that we start with these
4441520	4446720	investigative kinds as we put as as Alibol calls them these human subject capacities
4448720	4454000	we can operationalize them and do behavioral experiments in the top down and then from the
4454000	4459040	bottom up we can also try to reverse engineer the mechanism building blocks of the computations
4459760	4463040	and representations that evidence may use up to solve the task
4463600	4468160	related to that particular capacity and then we can meet somewhere in the middle and try to
4469280	4476640	from that line of work that are purchased things from above and bring to the fore some kinds of
4476640	4482080	mid-level abstractions as we call it or computational building blocks that might be key to
4483440	4489120	the performance of the system in that domain so for example if you're interested in the capacity
4489120	4496400	for reasoning you can start with this very broad human-centric notion of reasoning then try to
4496400	4501760	operationalize it in a reasoning task then do some behavioral testing and then mechanistic
4501760	4506320	interpretability of that reasoning task find out how the system is solving it find out how the
4506320	4512480	algorithm is doing well and why reverse engineer building blocks that might for example have to
4512480	4519040	be viable manipulation viable binding and then from there you might be able to either actually
4519040	4525040	refine the notion of reasoning you started with to have a more specific and that less human-centric
4525040	4535840	notion that is now operationalized in more low-level terms like you know that both manipulation
4535840	4542400	of variables in certain ways and the binding of variables to theories etc so yeah so that's I
4542400	4547040	think the general approach we take now how does that does any of that feedback into
4547760	4551600	interactions how we humans interact with algorithms I think that's one way in which
4551600	4559760	you could feedback is simply in terms of challenging or our spontaneous anthropomorphic
4559760	4564960	attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you
4564960	4571120	will interact with your cat in a slightly different way that you might maybe not rush the conclusion
4571120	4578720	that when your cat performs a certain behavior it has understood what you think and it's modeling
4578720	4585680	what you're thinking about what it's thinking or something perhaps you might adopt a more
4585680	4591920	deflationary attitude to explain the behavior of your cat doesn't mean you have to love them
4591920	4599520	any less or it doesn't mean you have to you know if that is the other thing like if you want at the
4599520	4603600	end of the day to speak to your cat like a human because you really are a gentleman for that then
4603600	4608000	that's you know all the more part of you in the same way if you find it useful to treat LLMs in
4608000	4615280	the way you interact with them to to have fluid interactions with them to treat them as if they
4615280	4620560	had beliefs these are as etc of human-like capacities then that's fine if that's for
4620640	4625840	actual purposes but at least if that line of work that we are kind of sketching here
4626640	4634000	ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's
4635840	4640240	well even if we if we have that kind of intentional sense and may believe about
4640240	4646720	who the kinds of besties they have at the background we will know that what their limitations are and
4646720	4651520	what their actual besties are i'd make sure to go with that Charles because we're going to use
4651520	4657600	this much yeah yeah no i agree with all that i just had a slightly different first reaction to the
4657600	4663360	question i took the question to be in part about how to deal with the sort of prompt sensitivity
4663360	4668720	of models the fact that sometimes we you know write something that seems natural to us but
4668720	4674480	provokes an unexpected response from a large language model and how do we think about that
4675440	4679760	and the first thought that occurred to me was just that we should distinguish between
4680880	4689520	different kinds of large language models you know we have this sort of huge large language models
4690160	4697840	which are fine-tuned to interact with us in a particular way and our and here's the central
4697920	4706320	point they're trained on a sort of unthinkably large database whereas there are other sorts
4706320	4710960	of large language models where the training data is more circumscribed and where we know
4712000	4719440	in more detail you know what where you can survey what the training data says and i think
4719440	4723200	if you're interested in you know what the mechanisms are underlying the responses
4724160	4731680	it's certainly very helpful to look at smaller but nevertheless large language models where the
4731680	4736960	training data is known to us because you know when you train a model on the entire internet
4736960	4743760	there are going to be all kinds of you know subtle signals in there that we don't have much hope of
4743760	4747760	tracing back to their source but which will influence the model behavior in all sorts of
4748240	4757520	ways but working with these somewhat more conscripted models gets rid of that problem at least in part
4761360	4770240	cool well where where do you see the work going or where do you plan to continue this direction
4770560	4778080	yeah so actually so we wrote this paper this short paper for the ICML
4778080	4783120	machine learning conference in the national conference machine learning that's happening
4783120	4789600	this week in the data and will be getting to Vienna at the end of the week for the popular
4789600	4793920	workshop that we're representing this paper which is a workshop on language models and
4794000	4799920	cognitive science so there will be a very strict page limit for these ICML
4801200	4808400	contribution which is four pages but what we want to do next is to expand this into a more
4809760	4814880	philosophically substantive paper that's going to be a bit longer and that's going to expand on the
4814880	4819840	more philosophically meaty parts of that of that project because everything is still a bit compressed
4819840	4824080	in that version that we're presenting at ICML so yeah that's the next step for us this is a
4824080	4828320	really useful way for us to force ourselves to write things down after running the box piece we
4828320	4834080	wanted to write an academic piece now we've written kind of a condense skeleton of the piece that
4834080	4839120	focuses more that caters more to an analogians and now the next step is to write the full
4839120	4844400	philosophy paper or at least that part of our project to be complete and then I don't have to
4844400	4849840	that maybe we'll have all that ideas but yeah yeah
4853360	4860240	I got nothing to add to that cool yes well it's very interesting work I think it it brings a
4860240	4869520	lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the
4869600	4876640	the heat and into the the spotlight and the relevance and so it's going to be an exciting
4876640	4883920	learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation
4884560	4891760	till next time thank you bye
4899520	4900900	you
4929520	4930900	you
