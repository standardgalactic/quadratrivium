{"text": " Hello and welcome. It's active inference gas stream number 51.1 on July 28th, 2023. We are here with Tomaso Salvatore and we will be having a presentation and a discussion on the recent work, causal inference via predictive coding. So thanks so much for joining. For those who are watching live, feel free to write questions in the live chat and off to you. Thank you. Thank you very much, Daniel, for inviting me. It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite excited to be here and be the one speaking this time. So I'm going to talk about this recent preprint that I put out, which has been the work of the last couple of months. And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz. It's basically a joint work between Versus, which is the company I work for, the University of Oxford and Theo Vien. So during this talk, I will, this is basically the outline of the talk, I will start talking about what predictive coding is and give an introduction of what it is, a brief historical introduction, why I think it's important to study predictive coding, even for example for the machine learning perspective. I will then provide a small intro to what causal inference is. And once we have all those informations together, I will then discuss why I wrote this paper, what was basically the research question that inspired me and the other collaborators. And present the main results, which are how to perform inference, so intervention and counterfactual inference, and how to learn the causal structures from a given data set using predictive coding. And then I will of course conclude with a small summary and some discussion on why I believe this work can be impactful in some future directions. So what is predictive coding? Predictive coding is in general famous for being a neuroscience inspired learning method, so a theory of how information processing in the brain works. And brain formally speaking, the theory of predictive coding can be described as basically having a hierarchical structure of neurons in the brain and you have two different families of neurons in the brain. The first family is the one in charge of sending prediction information, so neurons in a specific level of the hierarchy send information and predict the activity of the level below. And the second family of neurons is that of error neurons. And the error neurons, they send prediction error information up the hierarchy. So one level predicts the activity of the level below. This activity has some, this prediction has some mismatch, which we were actually going on in the level below. And the information about the prediction error gets sent up the hierarchy. However, predictive coding was actually not burned as a neuroscience, as a theory from neurosciences, but it was actually initially developed as a method for signal processing and compression back in the 50s. So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that once we have a predictor, a model that works that is well in predicting data, sending messages about the error in those predictions is actually much cheaper than sending the entire message every time. And this is how predictive coding was born, so as a signal processing and compression mechanism in information theory back in the 50s. He was actually in the 80s, that he became that exactly the same model was used in neuroscience. And so with the work from Mumford or other works, for example, explain how the rate enough processing formation, so we get prediction signals from the outside world, and we need to compress these representation and have this internal representation in our neurons. And the method is very similar, if not equivalent to the one that was developed by Elias and Oliver in the 50s. Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical structures in the brain, where prediction information is top down and error information is bottom up. And something that they did that wasn't done before is that they explain and develop this theory about not only inference, but also about how learning works in the brain. So it's also a theory of how our synapses get updated. And the last big breakthrough that I'm going to talk about in this brief historical introduction is from 2003, but then it kept going in the years after, thanks to Carfriston, in which basically he took the theory of Raoul and Ballard, and he extended it and generalized it to the theory of generative models. So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization scheme of a specific kind of generative model, which I'm going to introduce later as well. So to make a brief summary, the first two kinds of predictive coding that I described, so signal processing and compression and the information processing in the retina and in the brain in general, they are inference methods. And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st century, is that predictive coding was seen as a learning algorithm. So we can first compress information and then update all the synapses or all the latent variables that we have in our generative model to improve our generative model itself. So let's give some definitions that are a little bit more formal. So predictive coding can be seen as a hierarchical Gaussian generative model. So here is a very simple figure in which we have this hierarchical structure, which can be as deep as we want. And prediction signals go from one latent variable, Xn, to the following one, and it gets transformed every time via function gn, or gi. And this is a generative model, as I said, and what's the marginal probability of this generative model? Well, it's simply the probability of the last, can you see my cursor? Yes, right? Yes, perfect. So it's the generative model of the last vertex, is the distribution of the last vertex, times the probability distribution of every other vertex, conditioned on the activity of the vertex before, or the latent variable before. I earlier said that it's a Gaussian generative model, which means that those probabilities they are in Gaussian form, and those function g, in general, and especially since, for example, in a round baller paper, and in all the papers that came afterwards, also because of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with activation functions, or nonlinear maps with activation function and an additive bias. So we can give a formal definition of predictive coding, and we can say that predictive coding is an inversion scheme for such a generative model, where its model evidence is maximized by minimizing a quantity that is called the variational free energy. In general, the goal of every generative model is to maximize model evidence, but this quantity is always intractable, and we have some techniques that allow us to approximate the solution. And the one that we use in predictive coding is minimizing a variational free energy, which is a lower bound of the model evidence. In this work, and actually in a lot of other ones, so is the standard way of doing it, this minimization is performed via gradient descent, and there are actually other methods such as expectation maximization, which is often equivalent, or you can use some other message-passing algorithms such as belief propagation, for example. And going a little bit back in time, so we're getting a little bit about the statistical generative models, we can see predictive coding, as I said already a couple of times, as a hierarchical model with neural activities, so with neurons, latent variables that represent neural activities, they send their signal down the hierarchy, and with error nodes or error neurons, they send their signal up the hierarchy, so they send the error information back. What's the variational free energy of these class-operated coding models? It's simply the sum of the mean square error of all the error neurons, so it's the sum of the total error squared. And this representation is going to be useful in the later slides, and I'm going to explain how to use predictive coding to model causal inference, for example. What do you think predictive coding is important and is a nice algorithm to study? Well, first of all, as I said earlier, it optimizes the correct objective, which is the model evidence or marginal likelihood, and then it does so by optimizing a lower bound, which is called the variational free energy, as I said, and the variational free energy is interesting because it can be written as a sum of two different terms, which are and each of those terms optimizing it as important impacts, for example, in machine learning tasks or in general in learning tasks. So one of those terms forces memorization. So the second term basically tells forces the model to fit a specific data set. And the first term forces the model to minimize the complexity. And as we know, for example, from the Occam's razor theory, if we have two different models that perform similarly on a specific training set, the one that we have to get and the one that is expected to generalize the most is the less complex one. So updating generative model via variational free energy allows us to basically converge to the optimal Occam razor model, which both memorizes a data set, but is also able to generalize very well on unseen data points. A second reason why predictive coding is important is that it actually doesn't have to be defined on a hierarchical structure, but it can be modeled on more complex and flexible architectures such as directed graphical model with any shape or generalized even more to networks with a lot of cycles that resemble brain region. And the underlying reason is that you're not learning and predicting with a forward pass and then back propagating the error, but you're minimizing an energy function. And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and allow to learn cycles. And this is actually quite important because the brain is full of cycles as we have some information from some recent papers that may manage to map completely the brain of some animals such as fruit fly. The brain is full of cycles. So it makes sense to train our machine learning models or our models in general with an algorithm that allows us to train using cyclic structures. The third reason why predictive coding is interesting is that it has been formally proven that it is more robust than standard neural networks trained with back propagation. So if you have a neural network and you want to perform classification tasks, you, predictive coding is more robust. And this is interesting in tasks such as online learning, training on small datasets or continuous learning tasks. And the theory basically comes from the fact that imperative coding has been proved to approximate implicit gradient descent, which is a different version of the explicit gradient descent, which is the standard gradient descent used in the, in every single model basically. And it's a variation that is more robust. I think, okay, I did a quite a long intro to predictive coding. I think I'm now moving to the second topic, which is causal inference and what's causal inference? Causal inference is a theory, is a very general theory that has been formalized the most by Judea Perl. He's definitely the most important person in the field of causal inference. He wrote some very nice books. For example, the book of why is highly recommended if you want to learn more about this topic. And it basically tackles the following problem. So let's assume we have a joint probability distribution, which is associated with a Bayesian network. This is going to be a little bit the running example through all the paper, especially with your net with Bayesian networks of this shape. Those Bayesian networks, the variables inside, they can represent different quantities. So for example, a Bayesian network with this shape can represent the quantities on the right. So a socio-economical statue of an individual, its education level, its intelligence, and its income level. Something the classical statistics is very good at, and it's a while most used application, is to model observations or correlations. A correlation basically answered the question, what is the, if we observe another variable C? So for example, in this case, what is, what's the income level, the expected income level of an individual, if I observe his education level? And of course, if that person has a higher degree of education, for example, a master or a PhD, I'm expecting general that person to have a higher income level. And this is a correlation. However, sometimes there are things that are very hard to observe, but they play a huge role in determining those quantities. So for example, it could be that the income level is much, much more defined by the intelligence of a specific person. And maybe that the intelligence, so if a person is intelligent, he's also most likely to have a higher education level. But still the real reason why the income is high is because of the IQ. This cannot be studied by simple correlations and has to be studied by a more advanced technique, which is called an intervention. An intervention basically answers the question, what is the, if we change C to a specific value? So for example, we can take an individual and check his income level, and then change its education level, so intervene on this word, and change his education level without touching his intelligence, and see how much his income changes. For example, if the income changes a lot, it means that the intelligence doesn't play a big role in this, but the education level does. If the income level doesn't change much, it means that maybe there's a hidden variable, in this case, the intelligence that determines the income level of a person. The third quantity important in causal inference is that of counterfactuals. So for example, a counterfactual answers the question, what would be, had we changed C to a different value in the past? So for example, we can see that the difference between interventions and counterfactuals is that interventions act in the future. So I'm interviewing in the world now to observe a change in the future. Well, counterfactual allow us to go back in time and change a variable back in time and see how the change would have influenced the world we live in now. And those are defined by Judea Perle as the three levels of causal inference. Correlation is the first level, intervention is the second level, and counterfactual is the third level. What are interventions? I'm going to define them more formally now, how that I gave an intuitive definition. And I'm using this notation here, which is the same actually throughout all the presentation. So X is always going to be a latent variable, SI is always going to be a data point or an observation, and VI is always going to be a vertex. So every time you see VI, we're only interested in the structure of the graph, for example. So let's assume we have a Bayesian model, which has the same structure as the Bayesian model we saw in the previous slide. Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the probability or the expectation of X4, which is the latent variable related to this vertex, given that X3 is equal to S3. To perform an intervention, we need a new kind of notation, which is called the do operation. So in this case, X4, we want to compute the probability of X4, given the fact that we intervene in the world and change X3 to S3. And how do we do this? To perform an intervention, Judea Perl tells us that we have to have an intermediate step before computing a correlation, is that first we have to remove all the incoming edges to V3. So we have to study not this Bayesian network, but this second one. And then at this point, we are allowed to compute a correlation, as we normally do. And this is an intervention. A counterfactual is a generalization of this that, as I said, lived in the past, and they are computing using structural causal models. A structural causal model is a tuple, which is conceptually similar to a Bayesian network. But basically, we have this new class of variables on top, which are the unobservable variables they use. So we have the Bayesian network that we had before, X1, X2, X3, S4. But we also have those unobservable or variables that depend on the environment. You cannot control them, you can infer them, but they are there. And F is a set of functions that depends on all the, basically, F of X3 depends on X1, because you have an arrow, on X2, because you have an arrow, and on the unobservable variable that also influences X3. So yes, intuitively, you can think of a structural causal model as a Bayesian network with those unobservable variables on top, and each unobservable variable only influences its own, its own related variable X. So, for example, IU will never touch X1 as well. U3 will only touch U3, U1 will only influence X1, and so forth, and so on. So performing counterfactual inference answers the following question. So what would X4 be at X3 being equal to another variable in a past situation, U? And computing this counterfactual requires three different steps. So abduction is the computation of all the background variables. So in this step, we want to go back in time and understand how the environment, the unobservable environment, was in that specific moment in time. And we do this by fixing all the latent variables X to some specific data that we already have, and performing this inference on the use. Then we're going to use the U to keep the U that we have learned, and perform an intervention. So a counterfactual can also be seen as an intervention back in time, in which we know the environment variables U1, U2, and U4 in that specific moment. And what's the missing step? So what would X4 be at X3 being equal to another data point in that specific situation? Now we can compute a correlation. And the correlation, we do it on the graph in which we have already performed an intervention using the environment variables that we have learned in the abduction step. And this is a counterfactual inference. This is the last slide of the causal inference introduction, and it's about structure learning. Basically, everything I've said so far relies on the fact that we know the causal dependencies among the data points. So we know the structure of the graph, we know which variable influences which one, we know the arrows in general. But in practice, this is actually not always possible. So we don't have access to the causal graph most of the times. And actually learning the best causal graph from data is still an open problem. We are improving in this. We are getting better. But how to perform this task exactly is still an open problem. So as I said, basically, the goal is to infer causal relationships from observational data. So given a data set, we want to infer the directed acyclic graph that describes the connectivity between the system and the variables of the data set. So for example here, we have an example that I guess we are all familiar with thanks because of the pandemic. So we have those four variables, age, vaccine, hospitalization, and CT. And we want to infer the causal dependencies among those variables. So for example, we want to learn directly from data that the probability of a person being hospitalized depends on its age and on the fact whether it's vaccinated or not, and so forth and so on. So this is the end of the long introduction, but I hope it was clear enough and I hope that I gave the basics to understand basically the results of the paper. And now we can go to the research questions. So the research questions are the following. First I want to see whether creative coding can be used to perform causal inference. So creative coding so far has only been used to perform two compute correlations in Bayesian networks. And the big question is, can we go beyond correlation and model intervention and counterfactual in a biological, plausible way? So in a way that it's, for example, simple, intuitive, and allow us to only play with the neurons and not touch, for example, the huge structure of the graph. And more in practice, more specifically, the question becomes, can we define a creative coding-based structural causal model to perform interventions and counterfactuals? The second question is, as I said, that having a structural causal model assumes that we know the structure of the Bayesian network. So it assumes that we have the arrows. Can we go beyond this and use creative coding networks to learn the causal structure of the graph? Basically, giving positive answers to both those questions would allow us to use creative coding as an end-to-end causal inference method, which basically takes a data set and allow us to test interventions and counterfactual predictions directly from this data set. So let's tackle the first problem, so causal inference via creative coding, which is also the section that gives the title to the paper, basically. And here I will show how to perform correlations with creative coding, which is already known, and how to perform interventional queries, which I think is the real question of the paper. So here is a causal graph, which is the usual graph that we had. And here is the corresponding creative coding model. So the axes are the latent variables and correspond to the neurons in a neural network model. And the black arrow passes prediction information from one neuron to the one down the hierarchy. And every vertex also has this error neuron, which passes information up the hierarchy. So the information of every error goes to the value node in the up the hierarchy and basically tells it to correct itself to change the prediction. So to perform a correlation using creative coding, what you have to do is that you take an observation and you simply fix the value of a specific neuron. So if you want to compute the probability of X4 given X3 equal to S3, we simply have to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy minimization. And this model, by minimizing, by updating the axes via a minimization of the variational free energy, allows the model to converge to a solution to this question. So the probability or the expected value of X4 given X3 equals 3. But how do I perform an intervention now without acting on the structure of the graph? Well, this is basically the first idea of the paper. This is still how to perform a correlation. So fix S3 equal to X3 is the first step in the algorithm. And the second one is to update the axes by minimizing the variational free energy. An intervention, which in theory corresponds in removing those arrows and answers to the question, the probability of X4 by performing an intervention, so do X3 equal S3? This coding can be performed as follows. So I'm going to write the algorithm here. So first, as in a correlation, you fix X3 equal to the observation that you get. Then this is the important step. You have to intervene not on the graph anymore, but on the prediction error and fix it equal to zero. Assuming a prediction error equal to zero basically makes sense, meaning less information up the hierarchy or actually sends no information up the hierarchy because it basically tells you that the prediction is always correct. And the third step is to, as we did before, to update the axes, the unconstrained axis or X1, X2, X4 by minimizing the variational free energy. As I will show now experimentally, by simply doing this little trick of setting a prediction error to be equal to zero, it prevents us to actually act on the structure of the graph as the theory of Duke-Alculus does and to infer the variables after an intervention by simply performing a variational free energy minimization. What about counterfactual inference? Counterfactual inference is actually easy once we have defined how to do an intervention. And this is because, as we saw earlier, performing a counterfactual is similar to performing an intervention in a past situation after you have inferred the unobservable variables. So as you can see in the plot I showed earlier about the abduction action and prediction steps, the action and prediction steps, they did not have those two arrows. They were removed. Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by simply performing an abduction step, as it was done earlier, an action step in which we simply perform an intervention on the single node. So we fix the value node and we set the error to zero and run the energy minimization, so minimizing the variational free energy to compute the prediction. So I think this is like an easy and elegant method to perform interventions and counterfactuals. And yeah, so I think the thing we have to show now is whether it works in practice or not. And we have a couple of experiments. And I'm going to show you now two different experiments. The first one is merely proof of concept experiment that shows that the predictive coding is able to perform intervention and counterfactuals. And the second one actually shows a simple application in how interventional queries can be used to improve the performance of classification tasks on a specific kind of predictive coding networks, which is that of a fully connected model. Let's start from the first one. So how do we do this task? So given a structural causal model, we generate training data and we use it to learn the weights, so to learn the functions of the structural causal models. And then we generate test data for both interventional and counterfactual queries. And we show whether we are able to converge to the correct test data using predictive coding. And for example here, those two plots represent the interventional and counterfactual queries of this specific graph, which is the butterfly bias graph, which is a graph that is often used in testing whether causal inference, whether interventional and counterfactual techniques work is as simple as that. But in the paper, you can find a lot of different graphs. But in general, those two plots show that the method works, show that the mean absolute error between the interventional and counterfactual quantities we compute and the interventional and counterfactual quantities from the original graph are close to each other. So the error is quite small. The second experiment is basically an extension of an experiment I proposed in an earlier paper, which is the learning on arbitrary graph topologies that I wrote last year. In that paper, I basically proposed this kind of network as a proof of concept, which is a fully connected network, which is in general the worst neural network you can have to perform machine learning experiments, because given a fixed set of neurons, basically every pair of neuron is connected by two different synapses. So it's the model with the highest complexity possible in general. The good thing is that since you have a lot of cycles, the model is extremely flexible in the sense that you can train it, for example, on a minst image and on a data point and on its label. But then the way you can query it, thanks to the information going back, is you can query in a lot of different ways. So you can form classification tasks in which you provide an image and you run the energy minimization and get the label. But you can also, for example, perform generation tasks in which you give the label, run the energy minimization and get the image. You can perform, for example, image completion, which should give half the image and let the model converge to the second half and so forth and so on. So it's basically a model that learns the statistics of the dataset in its entirety without being focused on classification or generation in general. So this flexibility is great. The problem is that because of this, every single task doesn't work well. So you can do a lot of different things, but none of them is done well. And here I want to show how using interventional queries instead of standard correlation queries or conditional queries slightly improves their results of those classification tasks. So what are the conjecture reasons of this test accuracy on those tasks not being so high? The first, the two reasons are that the model is distracted in correcting every single error. So basically you present an image and you would like to get a label, but the model is actually updating itself to also predict the error in the images. And the second reason, which is the one I said, is that the structure is far too complex. So again, from an Occam razor argumentation, this is the worst model you can have. So every time you have a model that fits a dataset, that model is going to be less complex than this one that is going to be preferred. But in general, just to start it, the idea is can querying this model be interventions be used to improve the performance of those fully connected models? Well, the answer is yes. So here is how I perform interventional queries. So I present an image to the network. I fix the error of the pixels to be equal to zero. So this error doesn't get propagated in the network. And then I compute the label. And as you can see, the accuracy improves, for example, from 89 using the standard query method of pretty difficult in networks to 92, which is the accuracy after the intervention and the same happens for fashion means. And I think that a very legit critic that probably everyone would think when seeing those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically. And yeah, it's true. And I'm actually in the later slides, I'm going to show how to act on the structure of this fully connected model will improve the results even more until the point they reach a performance that is not even close to state of the art performance, of course. But it's still up to a level that becomes basically acceptable and worth investigating. So yes, so this is the part about causal inference using predictive coding. And I guess to summarize, I can say that the interesting part of the results I just showed is that I showed that predictive coding is able to perform interventions in a very easy and intuitive way because you don't have to act on the structure of the old graph anymore. Sometimes those functions are not available, so forth and so on. But you simply have to intervene on a single neuron, set its prediction error to zero and perform an energy minimization process. And these extended allowed us to define predictive coding based structural causal models. Now we move to the second part of the work, which is about structure learning. So structure learning, as I said, deals with the problem of learning the causal structure of the model from observational data. This is actually no problem that has been around for decades and has always been, until a couple of years ago, tackled using combinatorial search methods. The problem with those combinatorial search methods is that their complexity grows double exponentially. So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn grows in size, learning it, it's incredibly slow. The new solution that came out actually a couple of years ago in a newspaper from 2018 showed that it's possible to actually learn this structure, not using a combinatorial search method, but by using a gradient-based method. And this was basically this killed the problem in general because now you can simply apply your on the parameters, which is the prior proposed that I'm going to define a little bit better in this slide, around gradient descent. And even if you have a model that is double, triple the size, the algorithm is still incredibly fast. And for this reason, this paper is, yeah, I think it's kind of new and I think already has around 600 citations or things like that. And every paper that I'm seeing now about causal inference and learning causal structure of the graph uses their method. It just changes a little bit, they find faster or slightly better inference methods, but still they all use the prior, this paper defined, and I do as well, and we do as well. So here we define a new quantity, which is the agency matrix. The agency matrix is simply a matrix that encodes the connections of the model. So it's a binary matrix, and in general, it's a binary matrix. Then of course, when you do gradient-based optimization, you make it continuous and then you have some threshold at some point that basically kills an edge or set it to one. The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j or zero otherwise. So for example, this agency matrix here represents the connectivity structure of this Bayesian network. And basically this method tackles two problems that we want about learning the structure of the Bayesian network. The idea is that we start from a fully connected model, which conceptually is similar, actually is equivalent to the predictive coding network I defined earlier, which is fully connected. So you have a lot of vertices and every pair of vertices is connected by two different edges, and you simply want to prune the ones that are not needed. So it can be seen as a method that performs model reduction. You start from a big model and you want to make it small. So what's the first ingredient to reduce models? Well, it's of course sparse city. And what's the prior that everyone uses to make a model more sparse is the Laplace prior, which in machine learning is simply known as the L1 norm, which is defined here. The solution that this paper that I mentioned earlier proposed is to add a second prior on top, which enforces what's probably the biggest characteristic of Bayesian networks on which you want to perform causal inference, is that you want them to be acyclic. And basically they show that acyclicity can be imposed on an agency matrix as a prior, and it has this shape here. So it's the trace of the matrix that is the exponential of A times A, where A is the agency matrix again. And basically this quantity here is equal to zero if and only if the Bayesian network or whatever graph you're considering is acyclic. So I'm going to use these in some experiments, so force those two priors on different kinds of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier about performing causal inference via predictive coding. So I'm going to present two different experiments. So one is a proof of concept, which is the standard experiments showed in all the structural learning tasks, which is the inference of the correct Bayesian network from data. And then I'm going to build on top of the classification experiments I showed earlier, and show how actually those priors allow us to improve the classification accuracy, the test accuracy of fully connected predictive coding models. So let's move to the first experiment, which is to infer the structure of the graph. And the experiments, they all follow basically the same pipeline in all the papers in the field. The first step is to generate a Bayesian network from random graph. So basically normally the two random graphs that everyone tests are Erdos-Renis graphs and scale-free graphs. So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some edges that you sample randomly. And you use this graph to generate a data set. So you sample, for example, N, big N data points. And what you do is that you take the graph that you have generated earlier and you throw it away. You only keep the data set. And the task you want to solve now is to have a training algorithm that basically allows you to retrieve the structure of the graph you have thrown away. So the way we do it here is that we train a fully connected predictive coding model on this data set D, using both the sparse and the acyclic priors we have defined earlier. You can see whether actually the graph that we converge to, after pruning away the entries of the agency matrix that are smaller than a certain threshold, is similar to that of the initial graph. And the results show that this is actually the case. So this is an example and I show many different parametrizations and dimensions and things like that in the paper. But I think those two are the most representative examples with an air nosher in a graph and a free scale graph with 20 nodes. And here on the left, you can see the ground truth graph, which is the one sampled randomly. And on the right, you can see the graph, the predictive coding model as learned from the data set. And as you can see, they are quite similar. It's still not perfect. So there are some errors, but in general, the structures, they work quite well. We also have some quantitative experiments that I don't show here, because they're just huge tables with a lot of numbers and I thought it was maybe a little bit too much for the presentation. But there is also that they perform similarly to contemporary methods. Also because I have to say most of the quality comes from the acyclic priors that was introduced in 2018. The second class of experiments are classification experiments, which as I said, are the extensions of the one I shared earlier. And the idea is to use structure learning to improve the classification results on the means and fashion means data set, starting from a fully connected graph. So what I did is that I divided the fully connected graph in clusters of neurons. So 1B cluster is the one related to the input. And then we have some specific number of hidden clusters. And then we have the label cluster, which is the cluster of neurons that are supposed to give me the label predictions. And I've trained them using the first time, the sparse prior only. So the idea is, what if I prune the connections I don't need from a model and learn a sparser model? Does this work? Well, the answer is no. It doesn't work. And the reason why is that at the end, the graph that you converge with is actually degenerate. So basically, the model learns to predict the label based on the label itself. So it discards all the information from the input and only keeps the label. And as you can see here, the label y predicts itself. Or in other experiments, when you change the parameters, you have that y predicts at zero, that predicts x1, that predicts y again. So what's the solution to this problem? Well, the solution to this problem is that we have to converge to an acyclic graph. And so we have to add something that prevents acyclicity. And what is that? One is, of course, the one I already proposed. And then I show a second technique. So the first one uses the acyclic prior defined earlier. And the second one is a novel technique that actually makes use of negative examples. So a negative example in this case is simply a data point in which you have an image, but the label is wrong. So here, for example, you have an image of a 7, but the label that I'm giving the model is a 2. And the idea is very simple and has been used in a lot of works already. So every time the model sees a positive example, it has to minimize the variational free energy. And every time it sees a negative example, it has to increase it. So we will want this quantity to be minimized. And actually, with a lot of experiments and a lot of experimentations, we saw that the two techniques basically first lead to the same results and second lead to the same graph as well. So here are the new results on means and fashion means using the two techniques that I just proposed. And now we move to some which are still not great, but definitely more reasonable test accuracies. So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means. Actually, those results can be much improved by learning the structure of the graph on means and then fixing the structure of the graph and do some form of fine tuning. So if you fine tune the model on the correct hierarchical structure, at some point you reach the test accuracy, which is the one you would expect from a hierarchical model. But those ones are simply the one, the fully connected model as naturally converged to. So for example, from a test error of 18.32 of the fully connected model train on fashion means by simply performing correlations or conditional queries, which is the standard way of querying operative coding model, adding interventions and the acyclic prior together makes this test error much lower. And we can observe it for means as well. I'm now going a little bit into details on this last experiment and on how the acyclic prior acts on the structure of the graph. So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset, it may be too much, is the, I called it a two means dataset in which you have the input point is formed of two different images and the label only depends on the second image. On the first image story. So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and things like that able to recognize that the second half of the image is actually meaningless in, in performing, in learning the in performing classification. How does training behave in general? Like, for example, we have this input, input node, output node, and only the nodes are fully connected and the model converge to a hierarchical structure, which is the one that we know performs the best on, on classification tasks. Well, here is a, is an example of a training method of a training run. So that's C zero, which is the beginning of training. We have this model here. So as zero corresponds to the, to the seven, so to the first image as one corresponds to the second image, again, we have the label Y and all the latent variables X zero X one X two, and the model is fully connected. So the agency matrix is, is full of ones. There are, there are no zeros. We have self loops and things like that. We train them at the model for a couple of epochs until, and what we note immediately is that, for example, the, the model immediately understands that the four is not needed to perform classification. So it doesn't. So every outgoing node from the, from the second input cluster is removed. And something we didn't understand is that this is, this cluster is the one related to the output. So we have a, we have a linear map from S zero to Y directly, which is this part here. But we know that actually a linear map is not the best map for, for performing classification on means. So we, we need some hierarchy. We need some depth to, to improve the results. And as you can see, this line here is the, is the accuracy, which up to this point, so up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear classification. But once you go on with the training, the model understands that it needs some hierarchy to better fit the data. So you, you see that this arrow starts getting stronger and stronger over time until it, it understands that the linear map is not actually really needed and it removes it. And so the, so the model you converge with is a model that starts from a zero, goes to a hidden node and then goes to the, to the label with a very weak linear map, which actually gets removed if you, if you set that threshold of, if you set that threshold of, for example, 0.1, 0.2, at some point, the linear map gets forgotten. And everything you end up with is with a, is with a hierarchical network. That is, that is, so it has learned the correct structure to, to perform classification tasks, which is a hierarchy. And it has also learned that the second image didn't play any role in defining the, the test accuracy. And this is all, this is all performed. So all those jobs are simply performed by, performed by one free energy minimization process. So you initialize the model, you define the free energy, you define the priors. So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge to hierarchical, to a hierarchical model, which is well able to perform classification on minced. And then if you then perform some fine tuning, you reach very competitive results as you do in feed forward networks with the, with back propagation. But I think that's not the interesting bit. The interesting bit is that you, like all this process, this process altogether of intervention and the acyclicity allows you to take a fully connected network and converge to a hierarchical one that is, that is able to perform classification with good results. And yeah, that's basically it. I'm now, oh yeah, wow, I've talked a lot. And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary. And I think the, the important takeaway if I have to give even one sentence of this paper is that predictive coding is a belief updating method that is able to perform end to end causal learning. So it's able to perform interventions to learn a structure from data and then perform interventions and counterfactuals. So causal inference in natural and efficiency model interventions by simply setting the prediction error to zero. So it's a, it's a very easy technique to perform interventions. And you simply only have to touch one neuron, you don't have to act on the structure of the graph. You can, you can use it to perform, to, to create structure causal models that are biologically plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already. And, and a couple of sentences about future works is that something that would be nice to do is to improve the performance of the model we, we have defined, because I think it performs reasonably well on a lot of tasks. So it performs reasonably well on structure learning on, for me, intervention and counterfactuals. But actually, if you look at state of the art model, there's always like a very specific method that performs better in a, in the single task. So it would be interesting to see if we can reach those level of performance in, in specific tasks by, by adding some tricks on, or some, or some new optimization methods, and to generalize it to, to dynamical systems, which are actually much more interesting, the static systems. So such as dynamical causal models and, or other techniques that allow you to perform causal inference in systems that move. So an action taken in a specific time step influences another node in a later time step, which is basically Granger causality. Yeah, that's it. And thank you very much. Thank you. Awesome. And very comprehensive presentation. That was really muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive presentation. There was really a lot there. And there was also a lot of great questions in the live chat. So maybe to warm into the questions, how did you come to study this topic? Were you studying causality and found predictive coding to be useful or vice versa? Or how did you come out this intersection? I actually have to say that the first person that came out with this idea was, was better. So, so like, like, I think a year and a half ago, even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up. And, and last summer, I started getting curious about causality and I read, for example, the book of why as I listen into podcasts, I don't know the standard way in which you get interested in a topic. And, and I remember this, this idea from Baron and proposed it to him. And I was like, why don't we expand it and, and actually make it a paper. So I, I involve some people to work with experiments and, and this is the final result at the end. Awesome. Cool. Yeah. Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different questions. And if anybody else wants to add more, I'm going to turn the light on first, because I'm, I think I'm getting in the dark more and more. Yes. Who said active inference can't solve the dark room issue? Oh, yes, here we are. So would you say the light switch caused it to be lighter? Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding, all distributions are usually Gaussian, the bottom up messages are precision weighted prediction errors where precision is the inverse of the Gaussian covariance. What if non Gaussian distributions are used? Is, um, basically the general method stays, the different, the main difference is that you, you don't have prediction errors, which, uh, as was correctly pointed out is the, basically the derivative of the variational free energy. If you have Gaussian assumptions, yeah, you don't have that single quantity to set to zero. And you probably will have to act on the structure of the graph to perform interventions. And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian distributions that, that looked at some of these issues, right? Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh, and we model transformers. That's the biggest motivation using predictive coding. And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end, and softmax calls to, uh, like not to Gaussian distribution, but to, yeah, to softmax distribution, the, I don't get the name now, but yes. And, uh, so yes, that's a generalization. It's a little bit tricky to call it. Once you remove the Gaston assumption is a little bit still tricky to call it predictive coding. So it's a, so for, for example, like talking to, uh, to Carl Freestone, like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions. But yes, that's more a philosophical debate than, uh, Interesting. And another, I think topic that, that's definitely of, of great interest is similarities and differences between the attention apparatus in transformers and the way that attention is described from a neurocognitive perspective and from a predictive processing precision waiting angle. What do you, what do you think about that? Well, the idea is that, um, yeah, I think about it is that in from a pretty processing and, uh, and also operational inference perspective, attention can be seen as a, as a kind of structure learning problem. There's a, I think there's a recent paper from, from Chris Buckley's group that shows that there should be, there should be a reprint on archive in which basically they show that the attention mechanism is simply learning the, the precision on the, on the weight parameters specific to out to a data point. So this precision is not a, is not a, is not a parameter that is in the structure of the model. So it's not a model specific parameter. It is a fast changing parameter like the value nodes that gets updated while minimizing the version of free energy. And once they, once you've minimized it and compute it, then you throw it away. And from the next data point, you have to really compute it from scratch. So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as a kind of structure learning, but a structure learning that is data point specific and not model specific. And I think if you want to generalize a little bit and go from, from the attention mechanism in transformers to the attention mechanism cognitive science, I feel they're probably too different to, like to draw similarities and, uh, I think the structure learning analogy and the, how important one connection in is with respect to another one probably does job much better. Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference between hidden variables X and unobserved variables U? The difference is that you can, uh, I think the main one is that you cannot observe the, the use. You can use them because you can, you can compute them and fix them, but you cannot, the idea is that you have no control over them. So the use, the use should be seen as a environment specific variables that they are there. They, they influence your process. Okay. Because the, for example, when you go back in time, the environment is different. So the idea is for example, if you, like going back to the, to the example before of the, of the expected income of a person with a specific intelligence of education, uh, uh, education degree, the idea is that if I want to, to see how much I will learn today with a, with a, with a, I don't know, with a master degree, is different with respect to how much I would earn 20 years ago with a master degree is different. For example, here in Italy with respect to other countries and all those variables that are not under your control, you can not model them using your vision network, but they are there. Okay. So you, you cannot ignore them when you, when you want to draw conclusions. So it's, yeah, it's basically everything that you cannot control. You can infer them. So you can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years ago, I would have earned this much if I, if I was disintelligent at this degree on average, of course. And, but it's not that I can change the government policies towards jobs or the, or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use. Awesome. All right. Have you implemented generalized coordinates in predictive coding? No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable. I think that's the, that's the takeaway that I got from talking to people that have implemented them. But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that, that tested on some threshold encoder style. Actually, I think still from Baron, there's a, there's a paper out there that came out last summer, but no, I've never played them with them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction problem of predicting input? Adding more level in, uh, in which sense, because the destruction problem is given by cycles. So basically you provide an image and the fact that you have a, so edges going out of the image, going in the, in the neurons, and then other edges going back, the, this basically creates the fact that you have a, that the error of, that those basically, these ingoing edges to the pixels of the image, they create some prediction errors. So you have some prediction errors that get spread inside the model. And that's, yeah, and this problem, I think is general of cycles. And it's probably not related to hierarchy in general. So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges, you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic network through the trace operator, that's a very interesting technique. And when was that brought into play? As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know, at least in the causal inference literature, I'm, I'm not aware of any previous methods. I would say no, because that, I mean, that's the highly cited paper. So I would say they came out with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like when you look at when different features of Bayesian inference and causal inference became available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal modeling framework? It's like, because there's only been like five to 25 years of this happening. And so that's very, very short. And also it's relatively technical. So there's relatively few research groups engaging in it. And it's just really cool what it's enabling. No, yes, yes, exactly. I mean, that's also, I think the exciting part of this field a little bit that is, I mean, there are definitely break breakthroughs out there that, that still have to be discovered and probably like, for example, like, or as much as a breakthrough that paper was they found like, they simply found out the right prior for acyclic structures. Okay, it's a yeah, I mean, I, I don't know exactly, but it may be an idea that you have in one afternoon. I don't know about the story of the, how the authors came up with that, but could potentially be that if they, they are there at the whiteboard, you're like, Oh, that actually works. That's a huge breakthrough. And I simply defined the prior. And also a lot of these breakthroughs, they, they don't just stack. It's not like a, a tower of blocks, they layer and they compose. So then something will be generalized to generalized coordinates or generalized synchrony or arbitrarily large graphs or sensor fusion with multimodal inputs. And it's like those all blend in really satisfying and effective ways. So, so even little things that again, someone can just come up with in a moment can really have impact. Okay, ML Don says, thanks a lot for asking my questions and thanks a million to Tomaso for the inspiring presentation. So nice. Thank you very much. And then Bert asks, how would language models using predictive coding differ from those using transformers? Okay, I think that actually, if I would have to build today a language model using predictive coding, I would still use transformers. So the idea is that, for example, if you have a, let's say this hierarchical graphical model, or this hierarchical Bayesian network, I've defined in the, in the very first slides, one arrow to encode a function, which is the linear map. Okay, so one arrow was simply the multiplication of a, of the vector encoded in the latent variables times the, this weight matrix that you can then make non-linear and things like that. But that can be actually something much more complex. The, the function encoded in the arrow can be a convolution, can be an attention mechanism. So, so actually how I would do it, I will still use the, I mean, which is actually the way we did it in, in, in the Oxford group last year is that we, we had exactly the structure. Every arrow is a transformer now. So one is the attention mechanism and the, the next one is the feed forward network as transformers. And basically the only difference that you have is that those variables you want to compute the posterior and you make those posterior's independence, independent via, via mean field approximation. So basically you follow all the steps that allow you to, to converge to the very, to the variational free energy of creative coding. But the, the way, the way you compute predictions and the way you, you send signals back is a, is done via transformer. So I will still use transformers in general. I mean, they work so well that I, I don't think that we can be arrogant and say, oh no, I'm going to do it better via a purely predictive coding way. Structure learning is a way to do it, but we'll still approximate transformers anyway. So you said structure learning would approximate the transformer approach? Yes. Destruction learning I mentioned earlier in, when, when someone asked the similarities between predictive coding and the attention mechanism. Very, yeah, very interesting. One thing I am wondering from MLBong, I could not see the concept of depth in the predictive coding networks you mentioned. Most likely I missed it. The definition provided for predictive coding involved the concept of depth. What did you mean by depth? No, yes, it's true. It's a, because the standard definition, as I said, multiple times is a, is hierarchical. You have predictions going one directions and prediction error going the opposite direction. Basically, what, what we did in, in this paper and also in the last one in which is called the learning on arbitrary graph topologies via predictive coding is that we can consider depth like as a, as independent, basically pair of latent variable, latent variable, and arrow. And you have predictions going that direction and prediction error going the other. But then you can compose these in how many, a lot of ways. So you can, you can, so basically this composition doesn't have to be hierarchical in the end. Can have cycles. So then you can, for example, plug in another, another latent variable to the first one, and then connect the other two. And you can have a structure that is as entangled as you want. So for example, in the, in the other paper, we train the, a network that has the shape of a brain structure. So we have a lot of brain regions that are sparsely connected inside and sparsely connected among each other. And, and there's, there's nothing hierarchical there at the end, but you can still train it by minimizing a ratio of free energy and by minimizing the, the total prediction error of the network. So you could have for a given motif in a entangled graph, you might see three successive layers that when you looked at them alone, you'd say, Oh, that's a three story building. That's a three layer model that has a depth of three. But then when you take a bigger picture there isn't like an explicit top or an explicit bottom to that network. Yes, exactly. And this is basically given by the, by the fact that every operation in creative coding networks is strictly local. So, so basically every message passing every prediction and every prediction error that you send, you only send it to the very nearby neurons. Okay. And whether the global structure is actually hierarchical or not, the, the single message passing doesn't even see that. I guess that's sort of the hope for learning new model architectures is the space of what is designed top down is very small and a lot of models in use today, albeit super effective models. Although you could ask effective per unit of compute or not, that's a second level question. But a lot of effective models today do not have some of these properties of predictive coding networks, like their capacity to use only local computations, which gives biological realism or just spatio temporal realism, but also may provide a lot of advantages in like federated compute or distributed computing settings. No, yes, exactly. I completely agree. I think the idea in general is that, and I don't know if that's going to be an advantage. I think it's very promising exactly for the reasons you said. And the reason is that today's models string with back propagation, you can basically summarize them as a model string back propagation is a function, because basically you have a map from input to output, and back propagation basically spreads information back from its computational graph. So every neural network model used today is a function. While predictive coding and not only predictive coding, like the whole class of functions, the class of methods that train in using local computations and actually work by minimizing a global energy function, they're not limited to model functions from input to output. They actually model something that kind of resembles physical systems. So you have a physical system, you fix some values to whatever input you have, and you let the system converge, and then you read some other value of neurons or variables that are supposed to be output. But this physical system doesn't have to be a feedforward map. It doesn't have to be a function that has an input space and an output space, and that's it. So the class of models that you can learn is also basically you can see like feedforward models and functions, and then a much bigger class, which is that of physical systems. Whether there's something interesting out here, I don't know yet, because the functions are working extremely well. We are seeing those days with back propagation, they work crazy well. So yeah, I don't know if there's anything interesting in the big part, but the big part is quite big. There are a lot of models that you cannot train with back propagation, and you can train with predictive coding, or a background propagation or other methods. That is super interesting. Certainly biological systems, physical systems solve all kinds of interesting problems. But there's still no free lunch, and ant species does really well in this environment might not do very well in another environment. And so out there in the in the hinterlands, there might be some really unique special algorithms that are not well described by being a function, yet still provide like a procedural way to to implement heuristics, which might be extremely, extremely effective. No, yes, yes, exactly. And yeah, and I think this has been most of my focus of research during my PhD, for example, like finding this application that is like out here and not inside the the functions. Cool. Well, where does this work go from here? Like, what directions are you excited about? And how do you see people in the active inference ecosystem getting involved in this type of work? I think every probably the most promising direction, which is something maybe I would like to explore a little bit is to, as I said, there is to go behind statistical models. So everything I've shown so far is about static data. So the data don't change over time, there's no time inside the definition of predictive coding as it is as I presented it here. However, you can, for example, generalize predictive coding to to work with temporal data using generalized coordinates, as you mentioned earlier, by by presenting it as a as a Kalman Kalman filter generative model. And and that's where, for example, the causal inference direction could be very useful, because at that model, in at that point, maybe you can be able to model Granger causality and and more complex and and useful dynamical causal models, basically. Because in general, the the due calculus and the interventional and counterfactual branch of science is mostly developed on on small models. So it's like you don't do interventions on gigantic models in general. So if you if you look at medical data, they use relatively small vision networks. And but of course, if you want to have a dynamical causal model, that models a specific environment or a specific reality, you have a lot of neurons inside, you have a lot of latent variables, they change over time and an intervention at some more at some moment creates an effect in a different time step. So maybe the next time step in 10 different time steps later. And I think that would be very interesting to develop like a biologically plausible way of passing information that is also able to model Granger causality, basically. Where do you see action in these models? Where do I see action? I didn't think of that. I think I see actions in those models, maybe in the same way as I as you see in other models, because creative coding is basically a model of perception. So so an action is you can see that's a consequence of what you're experiencing. So by changing the way you're you're experiencing something, then you can compute maybe you can simply perform a smarter action now that you have more information. But but yeah, I don't think action is very easy. Like, yeah, I don't see any explicit consequence of actions, besides the fact that this can allow you to basically maybe to simply draw better conclusions to then perform actions in the future. I'll add on to that a few ways that people have talked about predictive coding and action. First off, internal action or covert action is attention. So we can think about perception as an internal action that that's one approach. Another approach pretty micro is the outputs of a given node. We can understand that node as a particular thing with its own sensory cognitive and action states. And so in that sense, the output of a node. And then lastly, which we explored a little bit in live stream 43, on the theoretical review on predictive coding, we're reading all the way through. And it was all about perception all about perception. And then it was like section 5.3. If you have expectations about action, then action is just another variable in this architecture. And that's really aligned with inactive inference, where instead of having like a reward or utility function that we maximize, we select action based upon it being the likeliest course of action, the path of least action, that's Bayesian mechanics. And so it's actually very natural to bring in an action variable and utilize it essentially as it as if it were a prediction about something else. Exteroceptively in the world, because we're also expecting action. No, yes, yes, exactly. No, I like the way of defining actions a lot, actually. And I still think if it's been like, for example, there are not so many papers that apply this method. I think there are a couple from from Alexander Orobrie does something similar. But in practice, like outside of the pure active inference, like applying predictive coding and actions to solve practical problems hasn't been explored a lot. Well, thank you for this excellent presentation and discussion. Is there anything else that you want to say or point people towards? No, just a big thank you for inviting me. And it was really fun. And I hope to come back at some point for for some future works. Cool. Anytime, anytime. Thank you, Thomas. So thank you, Daniel. See you. Bye. Bye. You", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 21.48, "text": " Hello and welcome.", "tokens": [50364, 2425, 293, 2928, 13, 51438], "temperature": 0.0, "avg_logprob": -0.3673508255570023, "compression_ratio": 0.9764705882352941, "no_speech_prob": 0.20798666775226593}, {"id": 1, "seek": 0, "start": 21.48, "end": 28.42, "text": " It's active inference gas stream number 51.1 on July 28th, 2023.", "tokens": [51438, 467, 311, 4967, 38253, 4211, 4309, 1230, 18485, 13, 16, 322, 7370, 7562, 392, 11, 44377, 13, 51785], "temperature": 0.0, "avg_logprob": -0.3673508255570023, "compression_ratio": 0.9764705882352941, "no_speech_prob": 0.20798666775226593}, {"id": 2, "seek": 2842, "start": 28.42, "end": 34.52, "text": " We are here with Tomaso Salvatore and we will be having a presentation and a discussion", "tokens": [50364, 492, 366, 510, 365, 5041, 35281, 5996, 23352, 418, 293, 321, 486, 312, 1419, 257, 5860, 293, 257, 5017, 50669], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 3, "seek": 2842, "start": 34.52, "end": 39.42, "text": " on the recent work, causal inference via predictive coding.", "tokens": [50669, 322, 264, 5162, 589, 11, 38755, 38253, 5766, 35521, 17720, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 4, "seek": 2842, "start": 39.42, "end": 42.160000000000004, "text": " So thanks so much for joining.", "tokens": [50914, 407, 3231, 370, 709, 337, 5549, 13, 51051], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 5, "seek": 2842, "start": 42.160000000000004, "end": 47.22, "text": " For those who are watching live, feel free to write questions in the live chat and off", "tokens": [51051, 1171, 729, 567, 366, 1976, 1621, 11, 841, 1737, 281, 2464, 1651, 294, 264, 1621, 5081, 293, 766, 51304], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 6, "seek": 2842, "start": 47.22, "end": 48.22, "text": " to you.", "tokens": [51304, 281, 291, 13, 51354], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 7, "seek": 2842, "start": 48.22, "end": 49.22, "text": " Thank you.", "tokens": [51354, 1044, 291, 13, 51404], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 8, "seek": 2842, "start": 49.22, "end": 54.7, "text": " Thank you very much, Daniel, for inviting me.", "tokens": [51404, 1044, 291, 588, 709, 11, 8033, 11, 337, 18202, 385, 13, 51678], "temperature": 0.0, "avg_logprob": -0.20349834182045676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.6840474605560303}, {"id": 9, "seek": 5470, "start": 54.7, "end": 59.540000000000006, "text": " It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite", "tokens": [50364, 467, 311, 668, 257, 955, 3429, 295, 264, 2269, 293, 286, 600, 668, 1976, 257, 688, 295, 2145, 11, 370, 286, 478, 1596, 50606], "temperature": 0.0, "avg_logprob": -0.26694239248143564, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.6793650984764099}, {"id": 10, "seek": 5470, "start": 59.540000000000006, "end": 64.5, "text": " excited to be here and be the one speaking this time.", "tokens": [50606, 2919, 281, 312, 510, 293, 312, 264, 472, 4124, 341, 565, 13, 50854], "temperature": 0.0, "avg_logprob": -0.26694239248143564, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.6793650984764099}, {"id": 11, "seek": 5470, "start": 64.5, "end": 69.54, "text": " So I'm going to talk about this recent preprint that I put out, which has been the work of", "tokens": [50854, 407, 286, 478, 516, 281, 751, 466, 341, 5162, 659, 14030, 300, 286, 829, 484, 11, 597, 575, 668, 264, 589, 295, 51106], "temperature": 0.0, "avg_logprob": -0.26694239248143564, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.6793650984764099}, {"id": 12, "seek": 5470, "start": 69.54, "end": 72.62, "text": " the last couple of months.", "tokens": [51106, 264, 1036, 1916, 295, 2493, 13, 51260], "temperature": 0.0, "avg_logprob": -0.26694239248143564, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.6793650984764099}, {"id": 13, "seek": 5470, "start": 72.62, "end": 82.22, "text": " And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz.", "tokens": [51260, 400, 309, 311, 257, 9363, 365, 42076, 691, 4647, 12495, 11, 2012, 259, 16576, 23346, 11, 10781, 76, 3409, 293, 8500, 34992, 8483, 1093, 17946, 13, 51740], "temperature": 0.0, "avg_logprob": -0.26694239248143564, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.6793650984764099}, {"id": 14, "seek": 8222, "start": 82.22, "end": 87.82, "text": " It's basically a joint work between Versus, which is the company I work for, the University", "tokens": [50364, 467, 311, 1936, 257, 7225, 589, 1296, 12226, 301, 11, 597, 307, 264, 2237, 286, 589, 337, 11, 264, 3535, 50644], "temperature": 0.0, "avg_logprob": -0.2613464864095052, "compression_ratio": 1.5625, "no_speech_prob": 0.10224688053131104}, {"id": 15, "seek": 8222, "start": 87.82, "end": 91.86, "text": " of Oxford and Theo Vien.", "tokens": [50644, 295, 24786, 293, 42519, 691, 1053, 13, 50846], "temperature": 0.0, "avg_logprob": -0.2613464864095052, "compression_ratio": 1.5625, "no_speech_prob": 0.10224688053131104}, {"id": 16, "seek": 8222, "start": 91.86, "end": 102.03999999999999, "text": " So during this talk, I will, this is basically the outline of the talk, I will start talking", "tokens": [50846, 407, 1830, 341, 751, 11, 286, 486, 11, 341, 307, 1936, 264, 16387, 295, 264, 751, 11, 286, 486, 722, 1417, 51355], "temperature": 0.0, "avg_logprob": -0.2613464864095052, "compression_ratio": 1.5625, "no_speech_prob": 0.10224688053131104}, {"id": 17, "seek": 8222, "start": 102.03999999999999, "end": 109.34, "text": " about what predictive coding is and give an introduction of what it is, a brief historical", "tokens": [51355, 466, 437, 35521, 17720, 307, 293, 976, 364, 9339, 295, 437, 309, 307, 11, 257, 5353, 8584, 51720], "temperature": 0.0, "avg_logprob": -0.2613464864095052, "compression_ratio": 1.5625, "no_speech_prob": 0.10224688053131104}, {"id": 18, "seek": 10934, "start": 109.34, "end": 115.82000000000001, "text": " introduction, why I think it's important to study predictive coding, even for example", "tokens": [50364, 9339, 11, 983, 286, 519, 309, 311, 1021, 281, 2979, 35521, 17720, 11, 754, 337, 1365, 50688], "temperature": 0.0, "avg_logprob": -0.2383565664291382, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.14448262751102448}, {"id": 19, "seek": 10934, "start": 115.82000000000001, "end": 118.74000000000001, "text": " for the machine learning perspective.", "tokens": [50688, 337, 264, 3479, 2539, 4585, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2383565664291382, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.14448262751102448}, {"id": 20, "seek": 10934, "start": 118.74000000000001, "end": 126.10000000000001, "text": " I will then provide a small intro to what causal inference is.", "tokens": [50834, 286, 486, 550, 2893, 257, 1359, 12897, 281, 437, 38755, 38253, 307, 13, 51202], "temperature": 0.0, "avg_logprob": -0.2383565664291382, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.14448262751102448}, {"id": 21, "seek": 10934, "start": 126.10000000000001, "end": 132.3, "text": " And once we have all those informations together, I will then discuss why I wrote this paper,", "tokens": [51202, 400, 1564, 321, 362, 439, 729, 38855, 1214, 11, 286, 486, 550, 2248, 983, 286, 4114, 341, 3035, 11, 51512], "temperature": 0.0, "avg_logprob": -0.2383565664291382, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.14448262751102448}, {"id": 22, "seek": 10934, "start": 132.3, "end": 138.82, "text": " what was basically the research question that inspired me and the other collaborators.", "tokens": [51512, 437, 390, 1936, 264, 2132, 1168, 300, 7547, 385, 293, 264, 661, 39789, 13, 51838], "temperature": 0.0, "avg_logprob": -0.2383565664291382, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.14448262751102448}, {"id": 23, "seek": 13882, "start": 139.82, "end": 147.38, "text": " And present the main results, which are how to perform inference, so intervention and", "tokens": [50414, 400, 1974, 264, 2135, 3542, 11, 597, 366, 577, 281, 2042, 38253, 11, 370, 13176, 293, 50792], "temperature": 0.0, "avg_logprob": -0.2576943337917328, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.11190963536500931}, {"id": 24, "seek": 13882, "start": 147.38, "end": 154.98, "text": " counterfactual inference, and how to learn the causal structures from a given data set", "tokens": [50792, 5682, 44919, 901, 38253, 11, 293, 577, 281, 1466, 264, 38755, 9227, 490, 257, 2212, 1412, 992, 51172], "temperature": 0.0, "avg_logprob": -0.2576943337917328, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.11190963536500931}, {"id": 25, "seek": 13882, "start": 154.98, "end": 157.06, "text": " using predictive coding.", "tokens": [51172, 1228, 35521, 17720, 13, 51276], "temperature": 0.0, "avg_logprob": -0.2576943337917328, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.11190963536500931}, {"id": 26, "seek": 13882, "start": 157.06, "end": 163.98, "text": " And then I will of course conclude with a small summary and some discussion on why I", "tokens": [51276, 400, 550, 286, 486, 295, 1164, 16886, 365, 257, 1359, 12691, 293, 512, 5017, 322, 983, 286, 51622], "temperature": 0.0, "avg_logprob": -0.2576943337917328, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.11190963536500931}, {"id": 27, "seek": 16398, "start": 164.01999999999998, "end": 168.17999999999998, "text": " believe this work can be impactful in some future directions.", "tokens": [50366, 1697, 341, 589, 393, 312, 30842, 294, 512, 2027, 11095, 13, 50574], "temperature": 0.0, "avg_logprob": -0.3129605187310113, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00817126501351595}, {"id": 28, "seek": 16398, "start": 170.78, "end": 173.45999999999998, "text": " So what is predictive coding?", "tokens": [50704, 407, 437, 307, 35521, 17720, 30, 50838], "temperature": 0.0, "avg_logprob": -0.3129605187310113, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00817126501351595}, {"id": 29, "seek": 16398, "start": 173.45999999999998, "end": 178.78, "text": " Predictive coding is in general famous for being a neuroscience inspired learning method,", "tokens": [50838, 430, 24945, 488, 17720, 307, 294, 2674, 4618, 337, 885, 257, 42762, 7547, 2539, 3170, 11, 51104], "temperature": 0.0, "avg_logprob": -0.3129605187310113, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00817126501351595}, {"id": 30, "seek": 16398, "start": 178.78, "end": 183.29999999999998, "text": " so a theory of how information processing in the brain works.", "tokens": [51104, 370, 257, 5261, 295, 577, 1589, 9007, 294, 264, 3567, 1985, 13, 51330], "temperature": 0.0, "avg_logprob": -0.3129605187310113, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00817126501351595}, {"id": 31, "seek": 16398, "start": 184.66, "end": 191.38, "text": " And brain formally speaking, the theory of predictive coding can be described as basically", "tokens": [51398, 400, 3567, 25983, 4124, 11, 264, 5261, 295, 35521, 17720, 393, 312, 7619, 382, 1936, 51734], "temperature": 0.0, "avg_logprob": -0.3129605187310113, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00817126501351595}, {"id": 32, "seek": 19138, "start": 191.38, "end": 198.85999999999999, "text": " having a hierarchical structure of neurons in the brain and you have two different families", "tokens": [50364, 1419, 257, 35250, 804, 3877, 295, 22027, 294, 264, 3567, 293, 291, 362, 732, 819, 4466, 50738], "temperature": 0.0, "avg_logprob": -0.1664993310276466, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.017307069152593613}, {"id": 33, "seek": 19138, "start": 198.85999999999999, "end": 200.98, "text": " of neurons in the brain.", "tokens": [50738, 295, 22027, 294, 264, 3567, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1664993310276466, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.017307069152593613}, {"id": 34, "seek": 19138, "start": 200.98, "end": 207.54, "text": " The first family is the one in charge of sending prediction information, so neurons in a specific", "tokens": [50844, 440, 700, 1605, 307, 264, 472, 294, 4602, 295, 7750, 17630, 1589, 11, 370, 22027, 294, 257, 2685, 51172], "temperature": 0.0, "avg_logprob": -0.1664993310276466, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.017307069152593613}, {"id": 35, "seek": 19138, "start": 207.54, "end": 216.42, "text": " level of the hierarchy send information and predict the activity of the level below.", "tokens": [51172, 1496, 295, 264, 22333, 2845, 1589, 293, 6069, 264, 5191, 295, 264, 1496, 2507, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1664993310276466, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.017307069152593613}, {"id": 36, "seek": 19138, "start": 216.42, "end": 220.1, "text": " And the second family of neurons is that of error neurons.", "tokens": [51616, 400, 264, 1150, 1605, 295, 22027, 307, 300, 295, 6713, 22027, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1664993310276466, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.017307069152593613}, {"id": 37, "seek": 22010, "start": 220.1, "end": 225.14, "text": " And the error neurons, they send prediction error information up the hierarchy.", "tokens": [50364, 400, 264, 6713, 22027, 11, 436, 2845, 17630, 6713, 1589, 493, 264, 22333, 13, 50616], "temperature": 0.0, "avg_logprob": -0.24564781704464475, "compression_ratio": 1.893491124260355, "no_speech_prob": 0.0021634292788803577}, {"id": 38, "seek": 22010, "start": 225.14, "end": 229.57999999999998, "text": " So one level predicts the activity of the level below.", "tokens": [50616, 407, 472, 1496, 6069, 82, 264, 5191, 295, 264, 1496, 2507, 13, 50838], "temperature": 0.0, "avg_logprob": -0.24564781704464475, "compression_ratio": 1.893491124260355, "no_speech_prob": 0.0021634292788803577}, {"id": 39, "seek": 22010, "start": 229.57999999999998, "end": 234.45999999999998, "text": " This activity has some, this prediction has some mismatch, which we were actually going", "tokens": [50838, 639, 5191, 575, 512, 11, 341, 17630, 575, 512, 23220, 852, 11, 597, 321, 645, 767, 516, 51082], "temperature": 0.0, "avg_logprob": -0.24564781704464475, "compression_ratio": 1.893491124260355, "no_speech_prob": 0.0021634292788803577}, {"id": 40, "seek": 22010, "start": 234.45999999999998, "end": 236.54, "text": " on in the level below.", "tokens": [51082, 322, 294, 264, 1496, 2507, 13, 51186], "temperature": 0.0, "avg_logprob": -0.24564781704464475, "compression_ratio": 1.893491124260355, "no_speech_prob": 0.0021634292788803577}, {"id": 41, "seek": 22010, "start": 236.54, "end": 241.62, "text": " And the information about the prediction error gets sent up the hierarchy.", "tokens": [51186, 400, 264, 1589, 466, 264, 17630, 6713, 2170, 2279, 493, 264, 22333, 13, 51440], "temperature": 0.0, "avg_logprob": -0.24564781704464475, "compression_ratio": 1.893491124260355, "no_speech_prob": 0.0021634292788803577}, {"id": 42, "seek": 24162, "start": 242.54, "end": 250.54, "text": " However, predictive coding was actually not burned as a neuroscience, as a theory from", "tokens": [50410, 2908, 11, 35521, 17720, 390, 767, 406, 13490, 382, 257, 42762, 11, 382, 257, 5261, 490, 50810], "temperature": 0.0, "avg_logprob": -0.2123938850734545, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.003081943141296506}, {"id": 43, "seek": 24162, "start": 250.54, "end": 255.74, "text": " neurosciences, but it was actually initially developed as a method for signal processing", "tokens": [50810, 28813, 537, 2667, 11, 457, 309, 390, 767, 9105, 4743, 382, 257, 3170, 337, 6358, 9007, 51070], "temperature": 0.0, "avg_logprob": -0.2123938850734545, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.003081943141296506}, {"id": 44, "seek": 24162, "start": 255.74, "end": 258.5, "text": " and compression back in the 50s.", "tokens": [51070, 293, 19355, 646, 294, 264, 2625, 82, 13, 51208], "temperature": 0.0, "avg_logprob": -0.2123938850734545, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.003081943141296506}, {"id": 45, "seek": 24162, "start": 258.5, "end": 266.98, "text": " So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that", "tokens": [51208, 407, 264, 589, 295, 23440, 11, 16943, 296, 11, 597, 366, 767, 14878, 295, 28974, 11, 436, 5334, 300, 51632], "temperature": 0.0, "avg_logprob": -0.2123938850734545, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.003081943141296506}, {"id": 46, "seek": 26698, "start": 266.98, "end": 274.66, "text": " once we have a predictor, a model that works that is well in predicting data, sending messages", "tokens": [50364, 1564, 321, 362, 257, 6069, 284, 11, 257, 2316, 300, 1985, 300, 307, 731, 294, 32884, 1412, 11, 7750, 7897, 50748], "temperature": 0.0, "avg_logprob": -0.1666781651346307, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.0035685435868799686}, {"id": 47, "seek": 26698, "start": 274.66, "end": 281.1, "text": " about the error in those predictions is actually much cheaper than sending the entire message", "tokens": [50748, 466, 264, 6713, 294, 729, 21264, 307, 767, 709, 12284, 813, 7750, 264, 2302, 3636, 51070], "temperature": 0.0, "avg_logprob": -0.1666781651346307, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.0035685435868799686}, {"id": 48, "seek": 26698, "start": 281.1, "end": 283.46000000000004, "text": " every time.", "tokens": [51070, 633, 565, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1666781651346307, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.0035685435868799686}, {"id": 49, "seek": 26698, "start": 283.46000000000004, "end": 289.3, "text": " And this is how predictive coding was born, so as a signal processing and compression", "tokens": [51188, 400, 341, 307, 577, 35521, 17720, 390, 4232, 11, 370, 382, 257, 6358, 9007, 293, 19355, 51480], "temperature": 0.0, "avg_logprob": -0.1666781651346307, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.0035685435868799686}, {"id": 50, "seek": 26698, "start": 289.3, "end": 293.90000000000003, "text": " mechanism in information theory back in the 50s.", "tokens": [51480, 7513, 294, 1589, 5261, 646, 294, 264, 2625, 82, 13, 51710], "temperature": 0.0, "avg_logprob": -0.1666781651346307, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.0035685435868799686}, {"id": 51, "seek": 29390, "start": 293.9, "end": 304.06, "text": " He was actually in the 80s, that he became that exactly the same model was used in neuroscience.", "tokens": [50364, 634, 390, 767, 294, 264, 4688, 82, 11, 300, 415, 3062, 300, 2293, 264, 912, 2316, 390, 1143, 294, 42762, 13, 50872], "temperature": 0.0, "avg_logprob": -0.24129776954650878, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.016027845442295074}, {"id": 52, "seek": 29390, "start": 304.06, "end": 311.5, "text": " And so with the work from Mumford or other works, for example, explain how the rate enough", "tokens": [50872, 400, 370, 365, 264, 589, 490, 24279, 7404, 420, 661, 1985, 11, 337, 1365, 11, 2903, 577, 264, 3314, 1547, 51244], "temperature": 0.0, "avg_logprob": -0.24129776954650878, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.016027845442295074}, {"id": 53, "seek": 29390, "start": 311.5, "end": 315.97999999999996, "text": " processing formation, so we get prediction signals from the outside world, and we need", "tokens": [51244, 9007, 11723, 11, 370, 321, 483, 17630, 12354, 490, 264, 2380, 1002, 11, 293, 321, 643, 51468], "temperature": 0.0, "avg_logprob": -0.24129776954650878, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.016027845442295074}, {"id": 54, "seek": 29390, "start": 315.97999999999996, "end": 323.02, "text": " to compress these representation and have this internal representation in our neurons.", "tokens": [51468, 281, 14778, 613, 10290, 293, 362, 341, 6920, 10290, 294, 527, 22027, 13, 51820], "temperature": 0.0, "avg_logprob": -0.24129776954650878, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.016027845442295074}, {"id": 55, "seek": 32302, "start": 323.02, "end": 329.65999999999997, "text": " And the method is very similar, if not equivalent to the one that was developed by Elias and", "tokens": [50364, 400, 264, 3170, 307, 588, 2531, 11, 498, 406, 10344, 281, 264, 472, 300, 390, 4743, 538, 16943, 296, 293, 50696], "temperature": 0.0, "avg_logprob": -0.197424226337009, "compression_ratio": 1.4, "no_speech_prob": 0.001934635336510837}, {"id": 56, "seek": 32302, "start": 329.65999999999997, "end": 333.41999999999996, "text": " Oliver in the 50s.", "tokens": [50696, 23440, 294, 264, 2625, 82, 13, 50884], "temperature": 0.0, "avg_logprob": -0.197424226337009, "compression_ratio": 1.4, "no_speech_prob": 0.001934635336510837}, {"id": 57, "seek": 32302, "start": 333.41999999999996, "end": 339.9, "text": " Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul", "tokens": [50884, 2704, 437, 311, 264, 3880, 24709, 5513, 11, 2737, 294, 19952, 11, 3231, 281, 264, 589, 295, 7591, 3298, 51208], "temperature": 0.0, "avg_logprob": -0.197424226337009, "compression_ratio": 1.4, "no_speech_prob": 0.001934635336510837}, {"id": 58, "seek": 32302, "start": 339.9, "end": 347.09999999999997, "text": " and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical", "tokens": [51208, 293, 10744, 515, 11, 294, 597, 436, 7268, 341, 3410, 300, 286, 2835, 3071, 466, 35250, 804, 51568], "temperature": 0.0, "avg_logprob": -0.197424226337009, "compression_ratio": 1.4, "no_speech_prob": 0.001934635336510837}, {"id": 59, "seek": 34710, "start": 347.1, "end": 353.98, "text": " structures in the brain, where prediction information is top down and error information", "tokens": [50364, 9227, 294, 264, 3567, 11, 689, 17630, 1589, 307, 1192, 760, 293, 6713, 1589, 50708], "temperature": 0.0, "avg_logprob": -0.15253260196783605, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.02431340701878071}, {"id": 60, "seek": 34710, "start": 353.98, "end": 355.86, "text": " is bottom up.", "tokens": [50708, 307, 2767, 493, 13, 50802], "temperature": 0.0, "avg_logprob": -0.15253260196783605, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.02431340701878071}, {"id": 61, "seek": 34710, "start": 355.86, "end": 362.34000000000003, "text": " And something that they did that wasn't done before is that they explain and develop this", "tokens": [50802, 400, 746, 300, 436, 630, 300, 2067, 380, 1096, 949, 307, 300, 436, 2903, 293, 1499, 341, 51126], "temperature": 0.0, "avg_logprob": -0.15253260196783605, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.02431340701878071}, {"id": 62, "seek": 34710, "start": 362.34000000000003, "end": 368.70000000000005, "text": " theory about not only inference, but also about how learning works in the brain.", "tokens": [51126, 5261, 466, 406, 787, 38253, 11, 457, 611, 466, 577, 2539, 1985, 294, 264, 3567, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15253260196783605, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.02431340701878071}, {"id": 63, "seek": 34710, "start": 368.70000000000005, "end": 373.66, "text": " So it's also a theory of how our synapses get updated.", "tokens": [51444, 407, 309, 311, 611, 257, 5261, 295, 577, 527, 5451, 2382, 279, 483, 10588, 13, 51692], "temperature": 0.0, "avg_logprob": -0.15253260196783605, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.02431340701878071}, {"id": 64, "seek": 37366, "start": 373.66, "end": 379.38000000000005, "text": " And the last big breakthrough that I'm going to talk about in this brief historical introduction", "tokens": [50364, 400, 264, 1036, 955, 22397, 300, 286, 478, 516, 281, 751, 466, 294, 341, 5353, 8584, 9339, 50650], "temperature": 0.0, "avg_logprob": -0.14566030000385485, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.021751588210463524}, {"id": 65, "seek": 37366, "start": 379.38000000000005, "end": 388.62, "text": " is from 2003, but then it kept going in the years after, thanks to Carfriston, in which", "tokens": [50650, 307, 490, 16416, 11, 457, 550, 309, 4305, 516, 294, 264, 924, 934, 11, 3231, 281, 2741, 5779, 47345, 11, 294, 597, 51112], "temperature": 0.0, "avg_logprob": -0.14566030000385485, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.021751588210463524}, {"id": 66, "seek": 37366, "start": 388.62, "end": 396.78000000000003, "text": " basically he took the theory of Raoul and Ballard, and he extended it and generalized", "tokens": [51112, 1936, 415, 1890, 264, 5261, 295, 7591, 3298, 293, 10744, 515, 11, 293, 415, 10913, 309, 293, 44498, 51520], "temperature": 0.0, "avg_logprob": -0.14566030000385485, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.021751588210463524}, {"id": 67, "seek": 37366, "start": 396.78000000000003, "end": 400.74, "text": " it to the theory of generative models.", "tokens": [51520, 309, 281, 264, 5261, 295, 1337, 1166, 5245, 13, 51718], "temperature": 0.0, "avg_logprob": -0.14566030000385485, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.021751588210463524}, {"id": 68, "seek": 40074, "start": 400.74, "end": 407.14, "text": " So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization", "tokens": [50364, 407, 1936, 264, 2135, 3932, 300, 2741, 5779, 47345, 630, 307, 300, 35521, 17720, 307, 364, 4467, 12, 1696, 3081, 2144, 50684], "temperature": 0.0, "avg_logprob": -0.1476144450051444, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.004398034885525703}, {"id": 69, "seek": 40074, "start": 407.14, "end": 415.34000000000003, "text": " scheme of a specific kind of generative model, which I'm going to introduce later as well.", "tokens": [50684, 12232, 295, 257, 2685, 733, 295, 1337, 1166, 2316, 11, 597, 286, 478, 516, 281, 5366, 1780, 382, 731, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1476144450051444, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.004398034885525703}, {"id": 70, "seek": 40074, "start": 415.34000000000003, "end": 424.22, "text": " So to make a brief summary, the first two kinds of predictive coding that I described,", "tokens": [51094, 407, 281, 652, 257, 5353, 12691, 11, 264, 700, 732, 3685, 295, 35521, 17720, 300, 286, 7619, 11, 51538], "temperature": 0.0, "avg_logprob": -0.1476144450051444, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.004398034885525703}, {"id": 71, "seek": 40074, "start": 424.22, "end": 428.98, "text": " so signal processing and compression and the information processing in the retina and in", "tokens": [51538, 370, 6358, 9007, 293, 19355, 293, 264, 1589, 9007, 294, 264, 1533, 1426, 293, 294, 51776], "temperature": 0.0, "avg_logprob": -0.1476144450051444, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.004398034885525703}, {"id": 72, "seek": 42898, "start": 428.98, "end": 432.82, "text": " the brain in general, they are inference methods.", "tokens": [50364, 264, 3567, 294, 2674, 11, 436, 366, 38253, 7150, 13, 50556], "temperature": 0.0, "avg_logprob": -0.1422513878863791, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.005668713711202145}, {"id": 73, "seek": 42898, "start": 432.82, "end": 441.06, "text": " And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st", "tokens": [50556, 400, 264, 3880, 1319, 11, 264, 3880, 8894, 300, 321, 632, 294, 19952, 11, 370, 718, 311, 584, 294, 264, 5080, 372, 50968], "temperature": 0.0, "avg_logprob": -0.1422513878863791, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.005668713711202145}, {"id": 74, "seek": 42898, "start": 441.06, "end": 445.1, "text": " century, is that predictive coding was seen as a learning algorithm.", "tokens": [50968, 4901, 11, 307, 300, 35521, 17720, 390, 1612, 382, 257, 2539, 9284, 13, 51170], "temperature": 0.0, "avg_logprob": -0.1422513878863791, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.005668713711202145}, {"id": 75, "seek": 42898, "start": 445.1, "end": 451.18, "text": " So we can first compress information and then update all the synapses or all the latent", "tokens": [51170, 407, 321, 393, 700, 14778, 1589, 293, 550, 5623, 439, 264, 5451, 2382, 279, 420, 439, 264, 48994, 51474], "temperature": 0.0, "avg_logprob": -0.1422513878863791, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.005668713711202145}, {"id": 76, "seek": 42898, "start": 451.18, "end": 457.54, "text": " variables that we have in our generative model to improve our generative model itself.", "tokens": [51474, 9102, 300, 321, 362, 294, 527, 1337, 1166, 2316, 281, 3470, 527, 1337, 1166, 2316, 2564, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1422513878863791, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.005668713711202145}, {"id": 77, "seek": 45754, "start": 458.54, "end": 464.66, "text": " So let's give some definitions that are a little bit more formal.", "tokens": [50414, 407, 718, 311, 976, 512, 21988, 300, 366, 257, 707, 857, 544, 9860, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1821032916798311, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.006489588413387537}, {"id": 78, "seek": 45754, "start": 464.66, "end": 470.82000000000005, "text": " So predictive coding can be seen as a hierarchical Gaussian generative model.", "tokens": [50720, 407, 35521, 17720, 393, 312, 1612, 382, 257, 35250, 804, 39148, 1337, 1166, 2316, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1821032916798311, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.006489588413387537}, {"id": 79, "seek": 45754, "start": 470.82000000000005, "end": 475.5, "text": " So here is a very simple figure in which we have this hierarchical structure, which can", "tokens": [51028, 407, 510, 307, 257, 588, 2199, 2573, 294, 597, 321, 362, 341, 35250, 804, 3877, 11, 597, 393, 51262], "temperature": 0.0, "avg_logprob": -0.1821032916798311, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.006489588413387537}, {"id": 80, "seek": 45754, "start": 475.5, "end": 478.86, "text": " be as deep as we want.", "tokens": [51262, 312, 382, 2452, 382, 321, 528, 13, 51430], "temperature": 0.0, "avg_logprob": -0.1821032916798311, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.006489588413387537}, {"id": 81, "seek": 45754, "start": 478.86, "end": 485.58000000000004, "text": " And prediction signals go from one latent variable, Xn, to the following one, and it", "tokens": [51430, 400, 17630, 12354, 352, 490, 472, 48994, 7006, 11, 1783, 77, 11, 281, 264, 3480, 472, 11, 293, 309, 51766], "temperature": 0.0, "avg_logprob": -0.1821032916798311, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.006489588413387537}, {"id": 82, "seek": 48558, "start": 485.58, "end": 490.46, "text": " gets transformed every time via function gn, or gi.", "tokens": [50364, 2170, 16894, 633, 565, 5766, 2445, 49819, 11, 420, 1735, 13, 50608], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 83, "seek": 48558, "start": 490.46, "end": 499.53999999999996, "text": " And this is a generative model, as I said, and what's the marginal probability of this", "tokens": [50608, 400, 341, 307, 257, 1337, 1166, 2316, 11, 382, 286, 848, 11, 293, 437, 311, 264, 16885, 8482, 295, 341, 51062], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 84, "seek": 48558, "start": 499.53999999999996, "end": 500.53999999999996, "text": " generative model?", "tokens": [51062, 1337, 1166, 2316, 30, 51112], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 85, "seek": 48558, "start": 500.53999999999996, "end": 506.62, "text": " Well, it's simply the probability of the last, can you see my cursor?", "tokens": [51112, 1042, 11, 309, 311, 2935, 264, 8482, 295, 264, 1036, 11, 393, 291, 536, 452, 28169, 30, 51416], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 86, "seek": 48558, "start": 506.62, "end": 507.62, "text": " Yes, right?", "tokens": [51416, 1079, 11, 558, 30, 51466], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 87, "seek": 48558, "start": 507.62, "end": 508.62, "text": " Yes, perfect.", "tokens": [51466, 1079, 11, 2176, 13, 51516], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 88, "seek": 48558, "start": 508.62, "end": 515.14, "text": " So it's the generative model of the last vertex, is the distribution of the last vertex, times", "tokens": [51516, 407, 309, 311, 264, 1337, 1166, 2316, 295, 264, 1036, 28162, 11, 307, 264, 7316, 295, 264, 1036, 28162, 11, 1413, 51842], "temperature": 0.0, "avg_logprob": -0.25969513175413783, "compression_ratio": 1.7263681592039801, "no_speech_prob": 0.005788363050669432}, {"id": 89, "seek": 51514, "start": 515.14, "end": 520.74, "text": " the probability distribution of every other vertex, conditioned on the activity of the", "tokens": [50364, 264, 8482, 7316, 295, 633, 661, 28162, 11, 35833, 322, 264, 5191, 295, 264, 50644], "temperature": 0.0, "avg_logprob": -0.23887465656667517, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0027117535937577486}, {"id": 90, "seek": 51514, "start": 520.74, "end": 526.06, "text": " vertex before, or the latent variable before.", "tokens": [50644, 28162, 949, 11, 420, 264, 48994, 7006, 949, 13, 50910], "temperature": 0.0, "avg_logprob": -0.23887465656667517, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0027117535937577486}, {"id": 91, "seek": 51514, "start": 526.06, "end": 530.98, "text": " I earlier said that it's a Gaussian generative model, which means that those probabilities", "tokens": [50910, 286, 3071, 848, 300, 309, 311, 257, 39148, 1337, 1166, 2316, 11, 597, 1355, 300, 729, 33783, 51156], "temperature": 0.0, "avg_logprob": -0.23887465656667517, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0027117535937577486}, {"id": 92, "seek": 51514, "start": 530.98, "end": 542.38, "text": " they are in Gaussian form, and those function g, in general, and especially since, for", "tokens": [51156, 436, 366, 294, 39148, 1254, 11, 293, 729, 2445, 290, 11, 294, 2674, 11, 293, 2318, 1670, 11, 337, 51726], "temperature": 0.0, "avg_logprob": -0.23887465656667517, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0027117535937577486}, {"id": 93, "seek": 54238, "start": 542.38, "end": 547.06, "text": " example, in a round baller paper, and in all the papers that came afterwards, also because", "tokens": [50364, 1365, 11, 294, 257, 3098, 2594, 260, 3035, 11, 293, 294, 439, 264, 10577, 300, 1361, 10543, 11, 611, 570, 50598], "temperature": 0.0, "avg_logprob": -0.21214287369339555, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.06259661167860031}, {"id": 94, "seek": 54238, "start": 547.06, "end": 554.58, "text": " of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with", "tokens": [50598, 295, 264, 2452, 2539, 8894, 11, 729, 6828, 366, 2935, 8213, 11317, 11, 420, 2107, 28263, 11317, 365, 50974], "temperature": 0.0, "avg_logprob": -0.21214287369339555, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.06259661167860031}, {"id": 95, "seek": 54238, "start": 554.58, "end": 563.58, "text": " activation functions, or nonlinear maps with activation function and an additive bias.", "tokens": [50974, 24433, 6828, 11, 420, 2107, 28263, 11317, 365, 24433, 2445, 293, 364, 45558, 12577, 13, 51424], "temperature": 0.0, "avg_logprob": -0.21214287369339555, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.06259661167860031}, {"id": 96, "seek": 54238, "start": 563.58, "end": 569.42, "text": " So we can give a formal definition of predictive coding, and we can say that predictive coding", "tokens": [51424, 407, 321, 393, 976, 257, 9860, 7123, 295, 35521, 17720, 11, 293, 321, 393, 584, 300, 35521, 17720, 51716], "temperature": 0.0, "avg_logprob": -0.21214287369339555, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.06259661167860031}, {"id": 97, "seek": 56942, "start": 569.42, "end": 575.18, "text": " is an inversion scheme for such a generative model, where its model evidence is maximized", "tokens": [50364, 307, 364, 43576, 12232, 337, 1270, 257, 1337, 1166, 2316, 11, 689, 1080, 2316, 4467, 307, 5138, 1602, 50652], "temperature": 0.0, "avg_logprob": -0.13711000997808914, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.010135398246347904}, {"id": 98, "seek": 56942, "start": 575.18, "end": 581.18, "text": " by minimizing a quantity that is called the variational free energy.", "tokens": [50652, 538, 46608, 257, 11275, 300, 307, 1219, 264, 3034, 1478, 1737, 2281, 13, 50952], "temperature": 0.0, "avg_logprob": -0.13711000997808914, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.010135398246347904}, {"id": 99, "seek": 56942, "start": 581.18, "end": 586.42, "text": " In general, the goal of every generative model is to maximize model evidence, but this quantity", "tokens": [50952, 682, 2674, 11, 264, 3387, 295, 633, 1337, 1166, 2316, 307, 281, 19874, 2316, 4467, 11, 457, 341, 11275, 51214], "temperature": 0.0, "avg_logprob": -0.13711000997808914, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.010135398246347904}, {"id": 100, "seek": 56942, "start": 586.42, "end": 595.42, "text": " is always intractable, and we have some techniques that allow us to approximate the solution.", "tokens": [51214, 307, 1009, 560, 1897, 712, 11, 293, 321, 362, 512, 7512, 300, 2089, 505, 281, 30874, 264, 3827, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13711000997808914, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.010135398246347904}, {"id": 101, "seek": 59542, "start": 595.42, "end": 602.3, "text": " And the one that we use in predictive coding is minimizing a variational free energy, which", "tokens": [50364, 400, 264, 472, 300, 321, 764, 294, 35521, 17720, 307, 46608, 257, 3034, 1478, 1737, 2281, 11, 597, 50708], "temperature": 0.0, "avg_logprob": -0.15326312280470325, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.03929062560200691}, {"id": 102, "seek": 59542, "start": 602.3, "end": 605.5799999999999, "text": " is a lower bound of the model evidence.", "tokens": [50708, 307, 257, 3126, 5472, 295, 264, 2316, 4467, 13, 50872], "temperature": 0.0, "avg_logprob": -0.15326312280470325, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.03929062560200691}, {"id": 103, "seek": 59542, "start": 605.5799999999999, "end": 612.3399999999999, "text": " In this work, and actually in a lot of other ones, so is the standard way of doing it,", "tokens": [50872, 682, 341, 589, 11, 293, 767, 294, 257, 688, 295, 661, 2306, 11, 370, 307, 264, 3832, 636, 295, 884, 309, 11, 51210], "temperature": 0.0, "avg_logprob": -0.15326312280470325, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.03929062560200691}, {"id": 104, "seek": 59542, "start": 612.3399999999999, "end": 619.86, "text": " this minimization is performed via gradient descent, and there are actually other methods", "tokens": [51210, 341, 4464, 2144, 307, 10332, 5766, 16235, 23475, 11, 293, 456, 366, 767, 661, 7150, 51586], "temperature": 0.0, "avg_logprob": -0.15326312280470325, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.03929062560200691}, {"id": 105, "seek": 59542, "start": 619.86, "end": 624.78, "text": " such as expectation maximization, which is often equivalent, or you can use some other", "tokens": [51586, 1270, 382, 14334, 5138, 2144, 11, 597, 307, 2049, 10344, 11, 420, 291, 393, 764, 512, 661, 51832], "temperature": 0.0, "avg_logprob": -0.15326312280470325, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.03929062560200691}, {"id": 106, "seek": 62478, "start": 624.78, "end": 631.3, "text": " message-passing algorithms such as belief propagation, for example.", "tokens": [50364, 3636, 12, 9216, 278, 14642, 1270, 382, 7107, 38377, 11, 337, 1365, 13, 50690], "temperature": 0.0, "avg_logprob": -0.21247465182573366, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0031347854528576136}, {"id": 107, "seek": 62478, "start": 631.3, "end": 636.38, "text": " And going a little bit back in time, so we're getting a little bit about the statistical", "tokens": [50690, 400, 516, 257, 707, 857, 646, 294, 565, 11, 370, 321, 434, 1242, 257, 707, 857, 466, 264, 22820, 50944], "temperature": 0.0, "avg_logprob": -0.21247465182573366, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0031347854528576136}, {"id": 108, "seek": 62478, "start": 636.38, "end": 644.4599999999999, "text": " generative models, we can see predictive coding, as I said already a couple of times, as a", "tokens": [50944, 1337, 1166, 5245, 11, 321, 393, 536, 35521, 17720, 11, 382, 286, 848, 1217, 257, 1916, 295, 1413, 11, 382, 257, 51348], "temperature": 0.0, "avg_logprob": -0.21247465182573366, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0031347854528576136}, {"id": 109, "seek": 62478, "start": 644.4599999999999, "end": 650.62, "text": " hierarchical model with neural activities, so with neurons, latent variables that represent", "tokens": [51348, 35250, 804, 2316, 365, 18161, 5354, 11, 370, 365, 22027, 11, 48994, 9102, 300, 2906, 51656], "temperature": 0.0, "avg_logprob": -0.21247465182573366, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0031347854528576136}, {"id": 110, "seek": 65062, "start": 650.62, "end": 656.78, "text": " neural activities, they send their signal down the hierarchy, and with error nodes or", "tokens": [50364, 18161, 5354, 11, 436, 2845, 641, 6358, 760, 264, 22333, 11, 293, 365, 6713, 13891, 420, 50672], "temperature": 0.0, "avg_logprob": -0.17760796774001347, "compression_ratio": 1.7958115183246073, "no_speech_prob": 0.009112979285418987}, {"id": 111, "seek": 65062, "start": 656.78, "end": 662.3, "text": " error neurons, they send their signal up the hierarchy, so they send the error information", "tokens": [50672, 6713, 22027, 11, 436, 2845, 641, 6358, 493, 264, 22333, 11, 370, 436, 2845, 264, 6713, 1589, 50948], "temperature": 0.0, "avg_logprob": -0.17760796774001347, "compression_ratio": 1.7958115183246073, "no_speech_prob": 0.009112979285418987}, {"id": 112, "seek": 65062, "start": 662.3, "end": 663.3, "text": " back.", "tokens": [50948, 646, 13, 50998], "temperature": 0.0, "avg_logprob": -0.17760796774001347, "compression_ratio": 1.7958115183246073, "no_speech_prob": 0.009112979285418987}, {"id": 113, "seek": 65062, "start": 663.3, "end": 668.02, "text": " What's the variational free energy of these class-operated coding models?", "tokens": [50998, 708, 311, 264, 3034, 1478, 1737, 2281, 295, 613, 1508, 12, 7192, 770, 17720, 5245, 30, 51234], "temperature": 0.0, "avg_logprob": -0.17760796774001347, "compression_ratio": 1.7958115183246073, "no_speech_prob": 0.009112979285418987}, {"id": 114, "seek": 65062, "start": 668.02, "end": 675.82, "text": " It's simply the sum of the mean square error of all the error neurons, so it's the sum", "tokens": [51234, 467, 311, 2935, 264, 2408, 295, 264, 914, 3732, 6713, 295, 439, 264, 6713, 22027, 11, 370, 309, 311, 264, 2408, 51624], "temperature": 0.0, "avg_logprob": -0.17760796774001347, "compression_ratio": 1.7958115183246073, "no_speech_prob": 0.009112979285418987}, {"id": 115, "seek": 67582, "start": 675.82, "end": 682.46, "text": " of the total error squared.", "tokens": [50364, 295, 264, 3217, 6713, 8889, 13, 50696], "temperature": 0.0, "avg_logprob": -0.2126391654790834, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.010560749098658562}, {"id": 116, "seek": 67582, "start": 682.46, "end": 688.1800000000001, "text": " And this representation is going to be useful in the later slides, and I'm going to explain", "tokens": [50696, 400, 341, 10290, 307, 516, 281, 312, 4420, 294, 264, 1780, 9788, 11, 293, 286, 478, 516, 281, 2903, 50982], "temperature": 0.0, "avg_logprob": -0.2126391654790834, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.010560749098658562}, {"id": 117, "seek": 67582, "start": 688.1800000000001, "end": 691.58, "text": " how to use predictive coding to model causal inference, for example.", "tokens": [50982, 577, 281, 764, 35521, 17720, 281, 2316, 38755, 38253, 11, 337, 1365, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2126391654790834, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.010560749098658562}, {"id": 118, "seek": 67582, "start": 691.58, "end": 697.1, "text": " What do you think predictive coding is important and is a nice algorithm to study?", "tokens": [51152, 708, 360, 291, 519, 35521, 17720, 307, 1021, 293, 307, 257, 1481, 9284, 281, 2979, 30, 51428], "temperature": 0.0, "avg_logprob": -0.2126391654790834, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.010560749098658562}, {"id": 119, "seek": 67582, "start": 697.1, "end": 701.2600000000001, "text": " Well, first of all, as I said earlier, it optimizes the correct objective, which is", "tokens": [51428, 1042, 11, 700, 295, 439, 11, 382, 286, 848, 3071, 11, 309, 5028, 5660, 264, 3006, 10024, 11, 597, 307, 51636], "temperature": 0.0, "avg_logprob": -0.2126391654790834, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.010560749098658562}, {"id": 120, "seek": 70126, "start": 701.26, "end": 708.3, "text": " the model evidence or marginal likelihood, and then it does so by optimizing a lower bound,", "tokens": [50364, 264, 2316, 4467, 420, 16885, 22119, 11, 293, 550, 309, 775, 370, 538, 40425, 257, 3126, 5472, 11, 50716], "temperature": 0.0, "avg_logprob": -0.17571272168840682, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.019947068765759468}, {"id": 121, "seek": 70126, "start": 708.3, "end": 712.62, "text": " which is called the variational free energy, as I said, and the variational free energy", "tokens": [50716, 597, 307, 1219, 264, 3034, 1478, 1737, 2281, 11, 382, 286, 848, 11, 293, 264, 3034, 1478, 1737, 2281, 50932], "temperature": 0.0, "avg_logprob": -0.17571272168840682, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.019947068765759468}, {"id": 122, "seek": 70126, "start": 712.62, "end": 718.9, "text": " is interesting because it can be written as a sum of two different terms, which are", "tokens": [50932, 307, 1880, 570, 309, 393, 312, 3720, 382, 257, 2408, 295, 732, 819, 2115, 11, 597, 366, 51246], "temperature": 0.0, "avg_logprob": -0.17571272168840682, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.019947068765759468}, {"id": 123, "seek": 70126, "start": 718.9, "end": 726.3, "text": " and each of those terms optimizing it as important impacts, for example, in machine learning", "tokens": [51246, 293, 1184, 295, 729, 2115, 40425, 309, 382, 1021, 11606, 11, 337, 1365, 11, 294, 3479, 2539, 51616], "temperature": 0.0, "avg_logprob": -0.17571272168840682, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.019947068765759468}, {"id": 124, "seek": 70126, "start": 726.3, "end": 729.3, "text": " tasks or in general in learning tasks.", "tokens": [51616, 9608, 420, 294, 2674, 294, 2539, 9608, 13, 51766], "temperature": 0.0, "avg_logprob": -0.17571272168840682, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.019947068765759468}, {"id": 125, "seek": 72930, "start": 729.3, "end": 732.66, "text": " So one of those terms forces memorization.", "tokens": [50364, 407, 472, 295, 729, 2115, 5874, 10560, 2144, 13, 50532], "temperature": 0.0, "avg_logprob": -0.14807360967000324, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.01096130721271038}, {"id": 126, "seek": 72930, "start": 732.66, "end": 739.8599999999999, "text": " So the second term basically tells forces the model to fit a specific data set.", "tokens": [50532, 407, 264, 1150, 1433, 1936, 5112, 5874, 264, 2316, 281, 3318, 257, 2685, 1412, 992, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14807360967000324, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.01096130721271038}, {"id": 127, "seek": 72930, "start": 739.8599999999999, "end": 744.8599999999999, "text": " And the first term forces the model to minimize the complexity.", "tokens": [50892, 400, 264, 700, 1433, 5874, 264, 2316, 281, 17522, 264, 14024, 13, 51142], "temperature": 0.0, "avg_logprob": -0.14807360967000324, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.01096130721271038}, {"id": 128, "seek": 72930, "start": 744.8599999999999, "end": 751.06, "text": " And as we know, for example, from the Occam's razor theory, if we have two different models", "tokens": [51142, 400, 382, 321, 458, 11, 337, 1365, 11, 490, 264, 26191, 335, 311, 30478, 5261, 11, 498, 321, 362, 732, 819, 5245, 51452], "temperature": 0.0, "avg_logprob": -0.14807360967000324, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.01096130721271038}, {"id": 129, "seek": 72930, "start": 751.06, "end": 755.9799999999999, "text": " that perform similarly on a specific training set, the one that we have to get and the one", "tokens": [51452, 300, 2042, 14138, 322, 257, 2685, 3097, 992, 11, 264, 472, 300, 321, 362, 281, 483, 293, 264, 472, 51698], "temperature": 0.0, "avg_logprob": -0.14807360967000324, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.01096130721271038}, {"id": 130, "seek": 75598, "start": 755.98, "end": 761.34, "text": " that is expected to generalize the most is the less complex one.", "tokens": [50364, 300, 307, 5176, 281, 2674, 1125, 264, 881, 307, 264, 1570, 3997, 472, 13, 50632], "temperature": 0.0, "avg_logprob": -0.14473452084306357, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.0036519202403724194}, {"id": 131, "seek": 75598, "start": 761.34, "end": 768.86, "text": " So updating generative model via variational free energy allows us to basically converge", "tokens": [50632, 407, 25113, 1337, 1166, 2316, 5766, 3034, 1478, 1737, 2281, 4045, 505, 281, 1936, 41881, 51008], "temperature": 0.0, "avg_logprob": -0.14473452084306357, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.0036519202403724194}, {"id": 132, "seek": 75598, "start": 768.86, "end": 775.9, "text": " to the optimal Occam razor model, which both memorizes a data set, but is also able to", "tokens": [51008, 281, 264, 16252, 26191, 335, 30478, 2316, 11, 597, 1293, 10560, 5660, 257, 1412, 992, 11, 457, 307, 611, 1075, 281, 51360], "temperature": 0.0, "avg_logprob": -0.14473452084306357, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.0036519202403724194}, {"id": 133, "seek": 75598, "start": 775.9, "end": 781.02, "text": " generalize very well on unseen data points.", "tokens": [51360, 2674, 1125, 588, 731, 322, 40608, 1412, 2793, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14473452084306357, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.0036519202403724194}, {"id": 134, "seek": 78102, "start": 781.02, "end": 789.98, "text": " A second reason why predictive coding is important is that it actually doesn't have", "tokens": [50364, 316, 1150, 1778, 983, 35521, 17720, 307, 1021, 307, 300, 309, 767, 1177, 380, 362, 50812], "temperature": 0.0, "avg_logprob": -0.1731973103114537, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.06451783329248428}, {"id": 135, "seek": 78102, "start": 789.98, "end": 795.5, "text": " to be defined on a hierarchical structure, but it can be modeled on more complex and", "tokens": [50812, 281, 312, 7642, 322, 257, 35250, 804, 3877, 11, 457, 309, 393, 312, 37140, 322, 544, 3997, 293, 51088], "temperature": 0.0, "avg_logprob": -0.1731973103114537, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.06451783329248428}, {"id": 136, "seek": 78102, "start": 795.5, "end": 802.1, "text": " flexible architectures such as directed graphical model with any shape or generalized even more", "tokens": [51088, 11358, 6331, 1303, 1270, 382, 12898, 35942, 2316, 365, 604, 3909, 420, 44498, 754, 544, 51418], "temperature": 0.0, "avg_logprob": -0.1731973103114537, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.06451783329248428}, {"id": 137, "seek": 78102, "start": 802.1, "end": 805.5799999999999, "text": " to networks with a lot of cycles that resemble brain region.", "tokens": [51418, 281, 9590, 365, 257, 688, 295, 17796, 300, 36870, 3567, 4458, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1731973103114537, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.06451783329248428}, {"id": 138, "seek": 80558, "start": 805.58, "end": 811.5, "text": " And the underlying reason is that you're not learning and predicting with a forward", "tokens": [50364, 400, 264, 14217, 1778, 307, 300, 291, 434, 406, 2539, 293, 32884, 365, 257, 2128, 50660], "temperature": 0.0, "avg_logprob": -0.16151349885123117, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.08900522440671921}, {"id": 139, "seek": 80558, "start": 811.5, "end": 816.9000000000001, "text": " pass and then back propagating the error, but you're minimizing an energy function.", "tokens": [50660, 1320, 293, 550, 646, 12425, 990, 264, 6713, 11, 457, 291, 434, 46608, 364, 2281, 2445, 13, 50930], "temperature": 0.0, "avg_logprob": -0.16151349885123117, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.08900522440671921}, {"id": 140, "seek": 80558, "start": 816.9000000000001, "end": 823.7800000000001, "text": " And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and", "tokens": [50930, 400, 341, 4045, 1936, 633, 733, 295, 22333, 281, 312, 11, 4045, 281, 352, 2261, 35250, 530, 293, 51274], "temperature": 0.0, "avg_logprob": -0.16151349885123117, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.08900522440671921}, {"id": 141, "seek": 80558, "start": 823.7800000000001, "end": 826.58, "text": " allow to learn cycles.", "tokens": [51274, 2089, 281, 1466, 17796, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16151349885123117, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.08900522440671921}, {"id": 142, "seek": 80558, "start": 826.58, "end": 830.7, "text": " And this is actually quite important because the brain is full of cycles as we have some", "tokens": [51414, 400, 341, 307, 767, 1596, 1021, 570, 264, 3567, 307, 1577, 295, 17796, 382, 321, 362, 512, 51620], "temperature": 0.0, "avg_logprob": -0.16151349885123117, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.08900522440671921}, {"id": 143, "seek": 83070, "start": 830.7, "end": 838.0600000000001, "text": " information from some recent papers that may manage to map completely the brain of some", "tokens": [50364, 1589, 490, 512, 5162, 10577, 300, 815, 3067, 281, 4471, 2584, 264, 3567, 295, 512, 50732], "temperature": 0.0, "avg_logprob": -0.18350619428298054, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.025114107877016068}, {"id": 144, "seek": 83070, "start": 838.0600000000001, "end": 840.98, "text": " animals such as fruit fly.", "tokens": [50732, 4882, 1270, 382, 6773, 3603, 13, 50878], "temperature": 0.0, "avg_logprob": -0.18350619428298054, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.025114107877016068}, {"id": 145, "seek": 83070, "start": 840.98, "end": 842.38, "text": " The brain is full of cycles.", "tokens": [50878, 440, 3567, 307, 1577, 295, 17796, 13, 50948], "temperature": 0.0, "avg_logprob": -0.18350619428298054, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.025114107877016068}, {"id": 146, "seek": 83070, "start": 842.38, "end": 851.0200000000001, "text": " So it makes sense to train our machine learning models or our models in general with an algorithm", "tokens": [50948, 407, 309, 1669, 2020, 281, 3847, 527, 3479, 2539, 5245, 420, 527, 5245, 294, 2674, 365, 364, 9284, 51380], "temperature": 0.0, "avg_logprob": -0.18350619428298054, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.025114107877016068}, {"id": 147, "seek": 83070, "start": 851.0200000000001, "end": 857.5, "text": " that allows us to train using cyclic structures.", "tokens": [51380, 300, 4045, 505, 281, 3847, 1228, 38154, 1050, 9227, 13, 51704], "temperature": 0.0, "avg_logprob": -0.18350619428298054, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.025114107877016068}, {"id": 148, "seek": 85750, "start": 857.5, "end": 862.1, "text": " The third reason why predictive coding is interesting is that it has been formally proven", "tokens": [50364, 440, 2636, 1778, 983, 35521, 17720, 307, 1880, 307, 300, 309, 575, 668, 25983, 12785, 50594], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 149, "seek": 85750, "start": 862.1, "end": 866.34, "text": " that it is more robust than standard neural networks trained with back propagation.", "tokens": [50594, 300, 309, 307, 544, 13956, 813, 3832, 18161, 9590, 8895, 365, 646, 38377, 13, 50806], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 150, "seek": 85750, "start": 866.34, "end": 871.9, "text": " So if you have a neural network and you want to perform classification tasks, you, predictive", "tokens": [50806, 407, 498, 291, 362, 257, 18161, 3209, 293, 291, 528, 281, 2042, 21538, 9608, 11, 291, 11, 35521, 51084], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 151, "seek": 85750, "start": 871.9, "end": 874.5, "text": " coding is more robust.", "tokens": [51084, 17720, 307, 544, 13956, 13, 51214], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 152, "seek": 85750, "start": 874.5, "end": 880.34, "text": " And this is interesting in tasks such as online learning, training on small datasets", "tokens": [51214, 400, 341, 307, 1880, 294, 9608, 1270, 382, 2950, 2539, 11, 3097, 322, 1359, 42856, 51506], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 153, "seek": 85750, "start": 880.34, "end": 882.9, "text": " or continuous learning tasks.", "tokens": [51506, 420, 10957, 2539, 9608, 13, 51634], "temperature": 0.0, "avg_logprob": -0.183496308881183, "compression_ratio": 1.8, "no_speech_prob": 0.018482087180018425}, {"id": 154, "seek": 88290, "start": 882.9, "end": 888.3, "text": " And the theory basically comes from the fact that imperative coding has been proved to", "tokens": [50364, 400, 264, 5261, 1936, 1487, 490, 264, 1186, 300, 32490, 17720, 575, 668, 14617, 281, 50634], "temperature": 0.0, "avg_logprob": -0.18011085285859949, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00411851704120636}, {"id": 155, "seek": 88290, "start": 888.3, "end": 893.98, "text": " approximate implicit gradient descent, which is a different version of the explicit gradient", "tokens": [50634, 30874, 26947, 16235, 23475, 11, 597, 307, 257, 819, 3037, 295, 264, 13691, 16235, 50918], "temperature": 0.0, "avg_logprob": -0.18011085285859949, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00411851704120636}, {"id": 156, "seek": 88290, "start": 893.98, "end": 900.1, "text": " descent, which is the standard gradient descent used in the, in every single model basically.", "tokens": [50918, 23475, 11, 597, 307, 264, 3832, 16235, 23475, 1143, 294, 264, 11, 294, 633, 2167, 2316, 1936, 13, 51224], "temperature": 0.0, "avg_logprob": -0.18011085285859949, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00411851704120636}, {"id": 157, "seek": 88290, "start": 900.1, "end": 906.02, "text": " And it's a variation that is more robust.", "tokens": [51224, 400, 309, 311, 257, 12990, 300, 307, 544, 13956, 13, 51520], "temperature": 0.0, "avg_logprob": -0.18011085285859949, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00411851704120636}, {"id": 158, "seek": 88290, "start": 906.02, "end": 908.9, "text": " I think, okay, I did a quite a long intro to predictive coding.", "tokens": [51520, 286, 519, 11, 1392, 11, 286, 630, 257, 1596, 257, 938, 12897, 281, 35521, 17720, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18011085285859949, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00411851704120636}, {"id": 159, "seek": 90890, "start": 908.9, "end": 915.02, "text": " I think I'm now moving to the second topic, which is causal inference and what's causal", "tokens": [50364, 286, 519, 286, 478, 586, 2684, 281, 264, 1150, 4829, 11, 597, 307, 38755, 38253, 293, 437, 311, 38755, 50670], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 160, "seek": 90890, "start": 915.02, "end": 916.02, "text": " inference?", "tokens": [50670, 38253, 30, 50720], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 161, "seek": 90890, "start": 916.02, "end": 921.66, "text": " Causal inference is a theory, is a very general theory that has been formalized the most by", "tokens": [50720, 7544, 11765, 38253, 307, 257, 5261, 11, 307, 257, 588, 2674, 5261, 300, 575, 668, 9860, 1602, 264, 881, 538, 51002], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 162, "seek": 90890, "start": 921.66, "end": 922.66, "text": " Judea Perl.", "tokens": [51002, 36521, 64, 3026, 75, 13, 51052], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 163, "seek": 90890, "start": 922.66, "end": 926.74, "text": " He's definitely the most important person in the field of causal inference.", "tokens": [51052, 634, 311, 2138, 264, 881, 1021, 954, 294, 264, 2519, 295, 38755, 38253, 13, 51256], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 164, "seek": 90890, "start": 926.74, "end": 928.78, "text": " He wrote some very nice books.", "tokens": [51256, 634, 4114, 512, 588, 1481, 3642, 13, 51358], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 165, "seek": 90890, "start": 928.78, "end": 935.6999999999999, "text": " For example, the book of why is highly recommended if you want to learn more about this topic.", "tokens": [51358, 1171, 1365, 11, 264, 1446, 295, 983, 307, 5405, 9628, 498, 291, 528, 281, 1466, 544, 466, 341, 4829, 13, 51704], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 166, "seek": 90890, "start": 935.6999999999999, "end": 938.86, "text": " And it basically tackles the following problem.", "tokens": [51704, 400, 309, 1936, 9426, 904, 264, 3480, 1154, 13, 51862], "temperature": 0.0, "avg_logprob": -0.17298407303659538, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.20643077790737152}, {"id": 167, "seek": 93886, "start": 938.86, "end": 942.78, "text": " So let's assume we have a joint probability distribution, which is associated with a Bayesian", "tokens": [50364, 407, 718, 311, 6552, 321, 362, 257, 7225, 8482, 7316, 11, 597, 307, 6615, 365, 257, 7840, 42434, 50560], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 168, "seek": 93886, "start": 942.78, "end": 943.78, "text": " network.", "tokens": [50560, 3209, 13, 50610], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 169, "seek": 93886, "start": 943.78, "end": 949.94, "text": " This is going to be a little bit the running example through all the paper, especially with", "tokens": [50610, 639, 307, 516, 281, 312, 257, 707, 857, 264, 2614, 1365, 807, 439, 264, 3035, 11, 2318, 365, 50918], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 170, "seek": 93886, "start": 949.94, "end": 953.94, "text": " your net with Bayesian networks of this shape.", "tokens": [50918, 428, 2533, 365, 7840, 42434, 9590, 295, 341, 3909, 13, 51118], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 171, "seek": 93886, "start": 953.94, "end": 960.94, "text": " Those Bayesian networks, the variables inside, they can represent different quantities.", "tokens": [51118, 3950, 7840, 42434, 9590, 11, 264, 9102, 1854, 11, 436, 393, 2906, 819, 22927, 13, 51468], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 172, "seek": 93886, "start": 960.94, "end": 967.82, "text": " So for example, a Bayesian network with this shape can represent the quantities on the", "tokens": [51468, 407, 337, 1365, 11, 257, 7840, 42434, 3209, 365, 341, 3909, 393, 2906, 264, 22927, 322, 264, 51812], "temperature": 0.0, "avg_logprob": -0.24825195555991314, "compression_ratio": 1.800865800865801, "no_speech_prob": 0.011024411767721176}, {"id": 173, "seek": 96782, "start": 967.82, "end": 968.82, "text": " right.", "tokens": [50364, 558, 13, 50414], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 174, "seek": 96782, "start": 968.82, "end": 973.94, "text": " So a socio-economical statue of an individual, its education level, its intelligence, and", "tokens": [50414, 407, 257, 44303, 12, 68, 2291, 804, 17385, 295, 364, 2609, 11, 1080, 3309, 1496, 11, 1080, 7599, 11, 293, 50670], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 175, "seek": 96782, "start": 973.94, "end": 977.58, "text": " its income level.", "tokens": [50670, 1080, 5742, 1496, 13, 50852], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 176, "seek": 96782, "start": 977.58, "end": 984.9000000000001, "text": " Something the classical statistics is very good at, and it's a while most used application,", "tokens": [50852, 6595, 264, 13735, 12523, 307, 588, 665, 412, 11, 293, 309, 311, 257, 1339, 881, 1143, 3861, 11, 51218], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 177, "seek": 96782, "start": 984.9000000000001, "end": 987.7800000000001, "text": " is to model observations or correlations.", "tokens": [51218, 307, 281, 2316, 18163, 420, 13983, 763, 13, 51362], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 178, "seek": 96782, "start": 987.7800000000001, "end": 994.7, "text": " A correlation basically answered the question, what is the, if we observe another variable", "tokens": [51362, 316, 20009, 1936, 10103, 264, 1168, 11, 437, 307, 264, 11, 498, 321, 11441, 1071, 7006, 51708], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 179, "seek": 96782, "start": 994.7, "end": 995.82, "text": " C?", "tokens": [51708, 383, 30, 51764], "temperature": 0.0, "avg_logprob": -0.28372737973235373, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.027258502319455147}, {"id": 180, "seek": 99582, "start": 995.82, "end": 1000.1400000000001, "text": " So for example, in this case, what is, what's the income level, the expected income level", "tokens": [50364, 407, 337, 1365, 11, 294, 341, 1389, 11, 437, 307, 11, 437, 311, 264, 5742, 1496, 11, 264, 5176, 5742, 1496, 50580], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 181, "seek": 99582, "start": 1000.1400000000001, "end": 1004.5400000000001, "text": " of an individual, if I observe his education level?", "tokens": [50580, 295, 364, 2609, 11, 498, 286, 11441, 702, 3309, 1496, 30, 50800], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 182, "seek": 99582, "start": 1004.5400000000001, "end": 1010.82, "text": " And of course, if that person has a higher degree of education, for example, a master", "tokens": [50800, 400, 295, 1164, 11, 498, 300, 954, 575, 257, 2946, 4314, 295, 3309, 11, 337, 1365, 11, 257, 4505, 51114], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 183, "seek": 99582, "start": 1010.82, "end": 1016.34, "text": " or a PhD, I'm expecting general that person to have a higher income level.", "tokens": [51114, 420, 257, 14476, 11, 286, 478, 9650, 2674, 300, 954, 281, 362, 257, 2946, 5742, 1496, 13, 51390], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 184, "seek": 99582, "start": 1016.34, "end": 1017.82, "text": " And this is a correlation.", "tokens": [51390, 400, 341, 307, 257, 20009, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 185, "seek": 99582, "start": 1017.82, "end": 1023.34, "text": " However, sometimes there are things that are very hard to observe, but they play a huge", "tokens": [51464, 2908, 11, 2171, 456, 366, 721, 300, 366, 588, 1152, 281, 11441, 11, 457, 436, 862, 257, 2603, 51740], "temperature": 0.0, "avg_logprob": -0.1791958179114, "compression_ratio": 1.7375, "no_speech_prob": 0.002984530059620738}, {"id": 186, "seek": 102334, "start": 1023.34, "end": 1026.3, "text": " role in determining those quantities.", "tokens": [50364, 3090, 294, 23751, 729, 22927, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 187, "seek": 102334, "start": 1026.3, "end": 1032.94, "text": " So for example, it could be that the income level is much, much more defined by the intelligence", "tokens": [50512, 407, 337, 1365, 11, 309, 727, 312, 300, 264, 5742, 1496, 307, 709, 11, 709, 544, 7642, 538, 264, 7599, 50844], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 188, "seek": 102334, "start": 1032.94, "end": 1035.98, "text": " of a specific person.", "tokens": [50844, 295, 257, 2685, 954, 13, 50996], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 189, "seek": 102334, "start": 1035.98, "end": 1041.7, "text": " And maybe that the intelligence, so if a person is intelligent, he's also most likely to have", "tokens": [50996, 400, 1310, 300, 264, 7599, 11, 370, 498, 257, 954, 307, 13232, 11, 415, 311, 611, 881, 3700, 281, 362, 51282], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 190, "seek": 102334, "start": 1041.7, "end": 1045.02, "text": " a higher education level.", "tokens": [51282, 257, 2946, 3309, 1496, 13, 51448], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 191, "seek": 102334, "start": 1045.02, "end": 1052.6200000000001, "text": " But still the real reason why the income is high is because of the IQ.", "tokens": [51448, 583, 920, 264, 957, 1778, 983, 264, 5742, 307, 1090, 307, 570, 295, 264, 28921, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16213687630586845, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.02455950528383255}, {"id": 192, "seek": 105262, "start": 1053.26, "end": 1059.26, "text": " This cannot be studied by simple correlations and has to be studied by a more advanced technique,", "tokens": [50396, 639, 2644, 312, 9454, 538, 2199, 13983, 763, 293, 575, 281, 312, 9454, 538, 257, 544, 7339, 6532, 11, 50696], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 193, "seek": 105262, "start": 1059.26, "end": 1060.8999999999999, "text": " which is called an intervention.", "tokens": [50696, 597, 307, 1219, 364, 13176, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 194, "seek": 105262, "start": 1060.8999999999999, "end": 1066.8999999999999, "text": " An intervention basically answers the question, what is the, if we change C to a specific", "tokens": [50778, 1107, 13176, 1936, 6338, 264, 1168, 11, 437, 307, 264, 11, 498, 321, 1319, 383, 281, 257, 2685, 51078], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 195, "seek": 105262, "start": 1066.8999999999999, "end": 1068.5, "text": " value?", "tokens": [51078, 2158, 30, 51158], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 196, "seek": 105262, "start": 1068.5, "end": 1075.78, "text": " So for example, we can take an individual and check his income level, and then change", "tokens": [51158, 407, 337, 1365, 11, 321, 393, 747, 364, 2609, 293, 1520, 702, 5742, 1496, 11, 293, 550, 1319, 51522], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 197, "seek": 105262, "start": 1075.78, "end": 1081.06, "text": " its education level, so intervene on this word, and change his education level without", "tokens": [51522, 1080, 3309, 1496, 11, 370, 30407, 322, 341, 1349, 11, 293, 1319, 702, 3309, 1496, 1553, 51786], "temperature": 0.0, "avg_logprob": -0.2411573266470304, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0625888779759407}, {"id": 198, "seek": 108106, "start": 1081.1, "end": 1086.6599999999999, "text": " touching his intelligence, and see how much his income changes.", "tokens": [50366, 11175, 702, 7599, 11, 293, 536, 577, 709, 702, 5742, 2962, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 199, "seek": 108106, "start": 1086.6599999999999, "end": 1092.62, "text": " For example, if the income changes a lot, it means that the intelligence doesn't play", "tokens": [50644, 1171, 1365, 11, 498, 264, 5742, 2962, 257, 688, 11, 309, 1355, 300, 264, 7599, 1177, 380, 862, 50942], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 200, "seek": 108106, "start": 1092.62, "end": 1096.02, "text": " a big role in this, but the education level does.", "tokens": [50942, 257, 955, 3090, 294, 341, 11, 457, 264, 3309, 1496, 775, 13, 51112], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 201, "seek": 108106, "start": 1096.02, "end": 1100.22, "text": " If the income level doesn't change much, it means that maybe there's a hidden variable,", "tokens": [51112, 759, 264, 5742, 1496, 1177, 380, 1319, 709, 11, 309, 1355, 300, 1310, 456, 311, 257, 7633, 7006, 11, 51322], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 202, "seek": 108106, "start": 1100.22, "end": 1106.3, "text": " in this case, the intelligence that determines the income level of a person.", "tokens": [51322, 294, 341, 1389, 11, 264, 7599, 300, 24799, 264, 5742, 1496, 295, 257, 954, 13, 51626], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 203, "seek": 108106, "start": 1106.3, "end": 1110.86, "text": " The third quantity important in causal inference is that of counterfactuals.", "tokens": [51626, 440, 2636, 11275, 1021, 294, 38755, 38253, 307, 300, 295, 5682, 44919, 901, 82, 13, 51854], "temperature": 0.0, "avg_logprob": -0.18196403866722471, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.008364220149815083}, {"id": 204, "seek": 111086, "start": 1110.86, "end": 1116.78, "text": " So for example, a counterfactual answers the question, what would be, had we changed C", "tokens": [50364, 407, 337, 1365, 11, 257, 5682, 44919, 901, 6338, 264, 1168, 11, 437, 576, 312, 11, 632, 321, 3105, 383, 50660], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 205, "seek": 111086, "start": 1116.78, "end": 1119.4199999999998, "text": " to a different value in the past?", "tokens": [50660, 281, 257, 819, 2158, 294, 264, 1791, 30, 50792], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 206, "seek": 111086, "start": 1119.4199999999998, "end": 1123.2199999999998, "text": " So for example, we can see that the difference between interventions and counterfactuals is", "tokens": [50792, 407, 337, 1365, 11, 321, 393, 536, 300, 264, 2649, 1296, 20924, 293, 5682, 44919, 901, 82, 307, 50982], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 207, "seek": 111086, "start": 1123.2199999999998, "end": 1126.4599999999998, "text": " that interventions act in the future.", "tokens": [50982, 300, 20924, 605, 294, 264, 2027, 13, 51144], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 208, "seek": 111086, "start": 1126.4599999999998, "end": 1131.3, "text": " So I'm interviewing in the world now to observe a change in the future.", "tokens": [51144, 407, 286, 478, 26524, 294, 264, 1002, 586, 281, 11441, 257, 1319, 294, 264, 2027, 13, 51386], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 209, "seek": 111086, "start": 1131.3, "end": 1137.4599999999998, "text": " Well, counterfactual allow us to go back in time and change a variable back in time and", "tokens": [51386, 1042, 11, 5682, 44919, 901, 2089, 505, 281, 352, 646, 294, 565, 293, 1319, 257, 7006, 646, 294, 565, 293, 51694], "temperature": 0.0, "avg_logprob": -0.17046121023233654, "compression_ratio": 1.9069767441860466, "no_speech_prob": 0.0012035187100991607}, {"id": 210, "seek": 113746, "start": 1137.54, "end": 1143.18, "text": " see how the change would have influenced the world we live in now.", "tokens": [50368, 536, 577, 264, 1319, 576, 362, 15269, 264, 1002, 321, 1621, 294, 586, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 211, "seek": 113746, "start": 1143.18, "end": 1148.3, "text": " And those are defined by Judea Perle as the three levels of causal inference.", "tokens": [50650, 400, 729, 366, 7642, 538, 36521, 64, 3026, 306, 382, 264, 1045, 4358, 295, 38755, 38253, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 212, "seek": 113746, "start": 1148.3, "end": 1152.14, "text": " Correlation is the first level, intervention is the second level, and counterfactual is", "tokens": [50906, 3925, 4419, 399, 307, 264, 700, 1496, 11, 13176, 307, 264, 1150, 1496, 11, 293, 5682, 44919, 901, 307, 51098], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 213, "seek": 113746, "start": 1152.14, "end": 1156.6200000000001, "text": " the third level.", "tokens": [51098, 264, 2636, 1496, 13, 51322], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 214, "seek": 113746, "start": 1156.6200000000001, "end": 1157.6200000000001, "text": " What are interventions?", "tokens": [51322, 708, 366, 20924, 30, 51372], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 215, "seek": 113746, "start": 1157.6200000000001, "end": 1162.42, "text": " I'm going to define them more formally now, how that I gave an intuitive definition.", "tokens": [51372, 286, 478, 516, 281, 6964, 552, 544, 25983, 586, 11, 577, 300, 286, 2729, 364, 21769, 7123, 13, 51612], "temperature": 0.0, "avg_logprob": -0.2175058364868164, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.004595788195729256}, {"id": 216, "seek": 116242, "start": 1162.42, "end": 1168.22, "text": " And I'm using this notation here, which is the same actually throughout all the presentation.", "tokens": [50364, 400, 286, 478, 1228, 341, 24657, 510, 11, 597, 307, 264, 912, 767, 3710, 439, 264, 5860, 13, 50654], "temperature": 0.0, "avg_logprob": -0.15951710655575707, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0037946023512631655}, {"id": 217, "seek": 116242, "start": 1168.22, "end": 1173.8600000000001, "text": " So X is always going to be a latent variable, SI is always going to be a data point or an", "tokens": [50654, 407, 1783, 307, 1009, 516, 281, 312, 257, 48994, 7006, 11, 29083, 307, 1009, 516, 281, 312, 257, 1412, 935, 420, 364, 50936], "temperature": 0.0, "avg_logprob": -0.15951710655575707, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0037946023512631655}, {"id": 218, "seek": 116242, "start": 1173.8600000000001, "end": 1178.18, "text": " observation, and VI is always going to be a vertex.", "tokens": [50936, 14816, 11, 293, 27619, 307, 1009, 516, 281, 312, 257, 28162, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15951710655575707, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0037946023512631655}, {"id": 219, "seek": 116242, "start": 1178.18, "end": 1185.5800000000002, "text": " So every time you see VI, we're only interested in the structure of the graph, for example.", "tokens": [51152, 407, 633, 565, 291, 536, 27619, 11, 321, 434, 787, 3102, 294, 264, 3877, 295, 264, 4295, 11, 337, 1365, 13, 51522], "temperature": 0.0, "avg_logprob": -0.15951710655575707, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0037946023512631655}, {"id": 220, "seek": 116242, "start": 1185.5800000000002, "end": 1191.1000000000001, "text": " So let's assume we have a Bayesian model, which has the same structure as the Bayesian", "tokens": [51522, 407, 718, 311, 6552, 321, 362, 257, 7840, 42434, 2316, 11, 597, 575, 264, 912, 3877, 382, 264, 7840, 42434, 51798], "temperature": 0.0, "avg_logprob": -0.15951710655575707, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0037946023512631655}, {"id": 221, "seek": 119110, "start": 1191.1, "end": 1195.1399999999999, "text": " model we saw in the previous slide.", "tokens": [50364, 2316, 321, 1866, 294, 264, 3894, 4137, 13, 50566], "temperature": 0.0, "avg_logprob": -0.24415227869054773, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.002647346816956997}, {"id": 222, "seek": 119110, "start": 1195.1399999999999, "end": 1201.78, "text": " Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the", "tokens": [50566, 18600, 300, 1783, 18, 307, 2681, 281, 318, 18, 11, 341, 307, 264, 14816, 321, 652, 11, 12523, 4045, 505, 281, 14722, 264, 50898], "temperature": 0.0, "avg_logprob": -0.24415227869054773, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.002647346816956997}, {"id": 223, "seek": 119110, "start": 1201.78, "end": 1209.62, "text": " probability or the expectation of X4, which is the latent variable related to this vertex,", "tokens": [50898, 8482, 420, 264, 14334, 295, 1783, 19, 11, 597, 307, 264, 48994, 7006, 4077, 281, 341, 28162, 11, 51290], "temperature": 0.0, "avg_logprob": -0.24415227869054773, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.002647346816956997}, {"id": 224, "seek": 119110, "start": 1209.62, "end": 1212.62, "text": " given that X3 is equal to S3.", "tokens": [51290, 2212, 300, 1783, 18, 307, 2681, 281, 318, 18, 13, 51440], "temperature": 0.0, "avg_logprob": -0.24415227869054773, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.002647346816956997}, {"id": 225, "seek": 119110, "start": 1212.62, "end": 1220.62, "text": " To perform an intervention, we need a new kind of notation, which is called the do operation.", "tokens": [51440, 1407, 2042, 364, 13176, 11, 321, 643, 257, 777, 733, 295, 24657, 11, 597, 307, 1219, 264, 360, 6916, 13, 51840], "temperature": 0.0, "avg_logprob": -0.24415227869054773, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.002647346816956997}, {"id": 226, "seek": 122062, "start": 1221.5, "end": 1228.82, "text": " So in this case, X4, we want to compute the probability of X4, given the fact that we", "tokens": [50408, 407, 294, 341, 1389, 11, 1783, 19, 11, 321, 528, 281, 14722, 264, 8482, 295, 1783, 19, 11, 2212, 264, 1186, 300, 321, 50774], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 227, "seek": 122062, "start": 1228.82, "end": 1233.54, "text": " intervene in the world and change X3 to S3.", "tokens": [50774, 30407, 294, 264, 1002, 293, 1319, 1783, 18, 281, 318, 18, 13, 51010], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 228, "seek": 122062, "start": 1233.54, "end": 1234.8999999999999, "text": " And how do we do this?", "tokens": [51010, 400, 577, 360, 321, 360, 341, 30, 51078], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 229, "seek": 122062, "start": 1234.8999999999999, "end": 1241.52, "text": " To perform an intervention, Judea Perl tells us that we have to have an intermediate step", "tokens": [51078, 1407, 2042, 364, 13176, 11, 36521, 64, 3026, 75, 5112, 505, 300, 321, 362, 281, 362, 364, 19376, 1823, 51409], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 230, "seek": 122062, "start": 1241.52, "end": 1247.8999999999999, "text": " before computing a correlation, is that first we have to remove all the incoming edges to", "tokens": [51409, 949, 15866, 257, 20009, 11, 307, 300, 700, 321, 362, 281, 4159, 439, 264, 22341, 8819, 281, 51728], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 231, "seek": 122062, "start": 1247.8999999999999, "end": 1250.4599999999998, "text": " V3.", "tokens": [51728, 691, 18, 13, 51856], "temperature": 0.0, "avg_logprob": -0.194483104505037, "compression_ratio": 1.6, "no_speech_prob": 0.005316588561981916}, {"id": 232, "seek": 125046, "start": 1250.46, "end": 1256.02, "text": " So we have to study not this Bayesian network, but this second one.", "tokens": [50364, 407, 321, 362, 281, 2979, 406, 341, 7840, 42434, 3209, 11, 457, 341, 1150, 472, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1927830569715385, "compression_ratio": 1.58, "no_speech_prob": 0.0010054786689579487}, {"id": 233, "seek": 125046, "start": 1256.02, "end": 1263.58, "text": " And then at this point, we are allowed to compute a correlation, as we normally do.", "tokens": [50642, 400, 550, 412, 341, 935, 11, 321, 366, 4350, 281, 14722, 257, 20009, 11, 382, 321, 5646, 360, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1927830569715385, "compression_ratio": 1.58, "no_speech_prob": 0.0010054786689579487}, {"id": 234, "seek": 125046, "start": 1263.58, "end": 1267.22, "text": " And this is an intervention.", "tokens": [51020, 400, 341, 307, 364, 13176, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1927830569715385, "compression_ratio": 1.58, "no_speech_prob": 0.0010054786689579487}, {"id": 235, "seek": 125046, "start": 1267.22, "end": 1272.7, "text": " A counterfactual is a generalization of this that, as I said, lived in the past, and they", "tokens": [51202, 316, 5682, 44919, 901, 307, 257, 2674, 2144, 295, 341, 300, 11, 382, 286, 848, 11, 5152, 294, 264, 1791, 11, 293, 436, 51476], "temperature": 0.0, "avg_logprob": -0.1927830569715385, "compression_ratio": 1.58, "no_speech_prob": 0.0010054786689579487}, {"id": 236, "seek": 125046, "start": 1272.7, "end": 1275.58, "text": " are computing using structural causal models.", "tokens": [51476, 366, 15866, 1228, 15067, 38755, 5245, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1927830569715385, "compression_ratio": 1.58, "no_speech_prob": 0.0010054786689579487}, {"id": 237, "seek": 127558, "start": 1275.58, "end": 1282.46, "text": " A structural causal model is a tuple, which is conceptually similar to a Bayesian network.", "tokens": [50364, 316, 15067, 38755, 2316, 307, 257, 2604, 781, 11, 597, 307, 3410, 671, 2531, 281, 257, 7840, 42434, 3209, 13, 50708], "temperature": 0.0, "avg_logprob": -0.15570892255330823, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0023824337404221296}, {"id": 238, "seek": 127558, "start": 1282.46, "end": 1287.8999999999999, "text": " But basically, we have this new class of variables on top, which are the unobservable variables", "tokens": [50708, 583, 1936, 11, 321, 362, 341, 777, 1508, 295, 9102, 322, 1192, 11, 597, 366, 264, 8526, 929, 1978, 712, 9102, 50980], "temperature": 0.0, "avg_logprob": -0.15570892255330823, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0023824337404221296}, {"id": 239, "seek": 127558, "start": 1287.8999999999999, "end": 1289.4199999999998, "text": " they use.", "tokens": [50980, 436, 764, 13, 51056], "temperature": 0.0, "avg_logprob": -0.15570892255330823, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0023824337404221296}, {"id": 240, "seek": 127558, "start": 1289.4199999999998, "end": 1293.5, "text": " So we have the Bayesian network that we had before, X1, X2, X3, S4.", "tokens": [51056, 407, 321, 362, 264, 7840, 42434, 3209, 300, 321, 632, 949, 11, 1783, 16, 11, 1783, 17, 11, 1783, 18, 11, 318, 19, 13, 51260], "temperature": 0.0, "avg_logprob": -0.15570892255330823, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0023824337404221296}, {"id": 241, "seek": 127558, "start": 1293.5, "end": 1300.5, "text": " But we also have those unobservable or variables that depend on the environment.", "tokens": [51260, 583, 321, 611, 362, 729, 8526, 929, 1978, 712, 420, 9102, 300, 5672, 322, 264, 2823, 13, 51610], "temperature": 0.0, "avg_logprob": -0.15570892255330823, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0023824337404221296}, {"id": 242, "seek": 130050, "start": 1300.5, "end": 1307.5, "text": " You cannot control them, you can infer them, but they are there.", "tokens": [50364, 509, 2644, 1969, 552, 11, 291, 393, 13596, 552, 11, 457, 436, 366, 456, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20762334896039358, "compression_ratio": 1.6265060240963856, "no_speech_prob": 0.03736835718154907}, {"id": 243, "seek": 130050, "start": 1307.5, "end": 1317.5, "text": " And F is a set of functions that depends on all the, basically, F of X3 depends on X1,", "tokens": [50714, 400, 479, 307, 257, 992, 295, 6828, 300, 5946, 322, 439, 264, 11, 1936, 11, 479, 295, 1783, 18, 5946, 322, 1783, 16, 11, 51214], "temperature": 0.0, "avg_logprob": -0.20762334896039358, "compression_ratio": 1.6265060240963856, "no_speech_prob": 0.03736835718154907}, {"id": 244, "seek": 130050, "start": 1317.5, "end": 1321.82, "text": " because you have an arrow, on X2, because you have an arrow, and on the unobservable", "tokens": [51214, 570, 291, 362, 364, 11610, 11, 322, 1783, 17, 11, 570, 291, 362, 364, 11610, 11, 293, 322, 264, 8526, 929, 1978, 712, 51430], "temperature": 0.0, "avg_logprob": -0.20762334896039358, "compression_ratio": 1.6265060240963856, "no_speech_prob": 0.03736835718154907}, {"id": 245, "seek": 130050, "start": 1321.82, "end": 1326.34, "text": " variable that also influences X3.", "tokens": [51430, 7006, 300, 611, 21222, 1783, 18, 13, 51656], "temperature": 0.0, "avg_logprob": -0.20762334896039358, "compression_ratio": 1.6265060240963856, "no_speech_prob": 0.03736835718154907}, {"id": 246, "seek": 132634, "start": 1326.34, "end": 1334.1, "text": " So yes, intuitively, you can think of a structural causal model as a Bayesian network with those", "tokens": [50364, 407, 2086, 11, 46506, 11, 291, 393, 519, 295, 257, 15067, 38755, 2316, 382, 257, 7840, 42434, 3209, 365, 729, 50752], "temperature": 0.0, "avg_logprob": -0.18186394373575845, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0031765305902808905}, {"id": 247, "seek": 132634, "start": 1334.1, "end": 1342.4599999999998, "text": " unobservable variables on top, and each unobservable variable only influences its own, its own", "tokens": [50752, 8526, 929, 1978, 712, 9102, 322, 1192, 11, 293, 1184, 8526, 929, 1978, 712, 7006, 787, 21222, 1080, 1065, 11, 1080, 1065, 51170], "temperature": 0.0, "avg_logprob": -0.18186394373575845, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0031765305902808905}, {"id": 248, "seek": 132634, "start": 1342.4599999999998, "end": 1343.4599999999998, "text": " related variable X.", "tokens": [51170, 4077, 7006, 1783, 13, 51220], "temperature": 0.0, "avg_logprob": -0.18186394373575845, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0031765305902808905}, {"id": 249, "seek": 132634, "start": 1343.4599999999998, "end": 1347.26, "text": " So, for example, IU will never touch X1 as well.", "tokens": [51220, 407, 11, 337, 1365, 11, 44218, 486, 1128, 2557, 1783, 16, 382, 731, 13, 51410], "temperature": 0.0, "avg_logprob": -0.18186394373575845, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0031765305902808905}, {"id": 250, "seek": 132634, "start": 1347.26, "end": 1355.26, "text": " U3 will only touch U3, U1 will only influence X1, and so forth, and so on.", "tokens": [51410, 624, 18, 486, 787, 2557, 624, 18, 11, 624, 16, 486, 787, 6503, 1783, 16, 11, 293, 370, 5220, 11, 293, 370, 322, 13, 51810], "temperature": 0.0, "avg_logprob": -0.18186394373575845, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0031765305902808905}, {"id": 251, "seek": 135526, "start": 1355.26, "end": 1359.22, "text": " So performing counterfactual inference answers the following question.", "tokens": [50364, 407, 10205, 5682, 44919, 901, 38253, 6338, 264, 3480, 1168, 13, 50562], "temperature": 0.0, "avg_logprob": -0.12549152374267578, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.005049309693276882}, {"id": 252, "seek": 135526, "start": 1359.22, "end": 1368.22, "text": " So what would X4 be at X3 being equal to another variable in a past situation, U?", "tokens": [50562, 407, 437, 576, 1783, 19, 312, 412, 1783, 18, 885, 2681, 281, 1071, 7006, 294, 257, 1791, 2590, 11, 624, 30, 51012], "temperature": 0.0, "avg_logprob": -0.12549152374267578, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.005049309693276882}, {"id": 253, "seek": 135526, "start": 1368.22, "end": 1371.62, "text": " And computing this counterfactual requires three different steps.", "tokens": [51012, 400, 15866, 341, 5682, 44919, 901, 7029, 1045, 819, 4439, 13, 51182], "temperature": 0.0, "avg_logprob": -0.12549152374267578, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.005049309693276882}, {"id": 254, "seek": 135526, "start": 1371.62, "end": 1378.22, "text": " So abduction is the computation of all the background variables.", "tokens": [51182, 407, 410, 40335, 307, 264, 24903, 295, 439, 264, 3678, 9102, 13, 51512], "temperature": 0.0, "avg_logprob": -0.12549152374267578, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.005049309693276882}, {"id": 255, "seek": 135526, "start": 1378.22, "end": 1383.34, "text": " So in this step, we want to go back in time and understand how the environment, the unobservable", "tokens": [51512, 407, 294, 341, 1823, 11, 321, 528, 281, 352, 646, 294, 565, 293, 1223, 577, 264, 2823, 11, 264, 8526, 929, 1978, 712, 51768], "temperature": 0.0, "avg_logprob": -0.12549152374267578, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.005049309693276882}, {"id": 256, "seek": 138334, "start": 1383.34, "end": 1388.34, "text": " environment, was in that specific moment in time.", "tokens": [50364, 2823, 11, 390, 294, 300, 2685, 1623, 294, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1789379801068987, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.02300409786403179}, {"id": 257, "seek": 138334, "start": 1388.34, "end": 1397.26, "text": " And we do this by fixing all the latent variables X to some specific data that we already have,", "tokens": [50614, 400, 321, 360, 341, 538, 19442, 439, 264, 48994, 9102, 1783, 281, 512, 2685, 1412, 300, 321, 1217, 362, 11, 51060], "temperature": 0.0, "avg_logprob": -0.1789379801068987, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.02300409786403179}, {"id": 258, "seek": 138334, "start": 1397.26, "end": 1400.34, "text": " and performing this inference on the use.", "tokens": [51060, 293, 10205, 341, 38253, 322, 264, 764, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1789379801068987, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.02300409786403179}, {"id": 259, "seek": 138334, "start": 1400.34, "end": 1408.78, "text": " Then we're going to use the U to keep the U that we have learned, and perform an intervention.", "tokens": [51214, 1396, 321, 434, 516, 281, 764, 264, 624, 281, 1066, 264, 624, 300, 321, 362, 3264, 11, 293, 2042, 364, 13176, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1789379801068987, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.02300409786403179}, {"id": 260, "seek": 140878, "start": 1408.78, "end": 1415.34, "text": " So a counterfactual can also be seen as an intervention back in time, in which we know", "tokens": [50364, 407, 257, 5682, 44919, 901, 393, 611, 312, 1612, 382, 364, 13176, 646, 294, 565, 11, 294, 597, 321, 458, 50692], "temperature": 0.0, "avg_logprob": -0.158657202849517, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00416166428476572}, {"id": 261, "seek": 140878, "start": 1415.34, "end": 1423.62, "text": " the environment variables U1, U2, and U4 in that specific moment.", "tokens": [50692, 264, 2823, 9102, 624, 16, 11, 624, 17, 11, 293, 624, 19, 294, 300, 2685, 1623, 13, 51106], "temperature": 0.0, "avg_logprob": -0.158657202849517, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00416166428476572}, {"id": 262, "seek": 140878, "start": 1423.62, "end": 1426.98, "text": " And what's the missing step?", "tokens": [51106, 400, 437, 311, 264, 5361, 1823, 30, 51274], "temperature": 0.0, "avg_logprob": -0.158657202849517, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00416166428476572}, {"id": 263, "seek": 140878, "start": 1426.98, "end": 1434.06, "text": " So what would X4 be at X3 being equal to another data point in that specific situation?", "tokens": [51274, 407, 437, 576, 1783, 19, 312, 412, 1783, 18, 885, 2681, 281, 1071, 1412, 935, 294, 300, 2685, 2590, 30, 51628], "temperature": 0.0, "avg_logprob": -0.158657202849517, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00416166428476572}, {"id": 264, "seek": 143406, "start": 1434.62, "end": 1437.46, "text": " Now we can compute a correlation.", "tokens": [50392, 823, 321, 393, 14722, 257, 20009, 13, 50534], "temperature": 0.0, "avg_logprob": -0.24037451621813652, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.0129758445546031}, {"id": 265, "seek": 143406, "start": 1437.46, "end": 1443.6599999999999, "text": " And the correlation, we do it on the graph in which we have already performed an intervention", "tokens": [50534, 400, 264, 20009, 11, 321, 360, 309, 322, 264, 4295, 294, 597, 321, 362, 1217, 10332, 364, 13176, 50844], "temperature": 0.0, "avg_logprob": -0.24037451621813652, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.0129758445546031}, {"id": 266, "seek": 143406, "start": 1443.6599999999999, "end": 1450.4199999999998, "text": " using the environment variables that we have learned in the abduction step.", "tokens": [50844, 1228, 264, 2823, 9102, 300, 321, 362, 3264, 294, 264, 410, 40335, 1823, 13, 51182], "temperature": 0.0, "avg_logprob": -0.24037451621813652, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.0129758445546031}, {"id": 267, "seek": 143406, "start": 1450.4199999999998, "end": 1455.78, "text": " And this is a counterfactual inference.", "tokens": [51182, 400, 341, 307, 257, 5682, 44919, 901, 38253, 13, 51450], "temperature": 0.0, "avg_logprob": -0.24037451621813652, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.0129758445546031}, {"id": 268, "seek": 143406, "start": 1455.78, "end": 1462.3799999999999, "text": " This is the last slide of the causal inference introduction, and it's about structure learning.", "tokens": [51450, 639, 307, 264, 1036, 4137, 295, 264, 38755, 38253, 9339, 11, 293, 309, 311, 466, 3877, 2539, 13, 51780], "temperature": 0.0, "avg_logprob": -0.24037451621813652, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.0129758445546031}, {"id": 269, "seek": 146238, "start": 1462.38, "end": 1468.74, "text": " Basically, everything I've said so far relies on the fact that we know the causal dependencies", "tokens": [50364, 8537, 11, 1203, 286, 600, 848, 370, 1400, 30910, 322, 264, 1186, 300, 321, 458, 264, 38755, 36606, 50682], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 270, "seek": 146238, "start": 1468.74, "end": 1470.5400000000002, "text": " among the data points.", "tokens": [50682, 3654, 264, 1412, 2793, 13, 50772], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 271, "seek": 146238, "start": 1470.5400000000002, "end": 1475.14, "text": " So we know the structure of the graph, we know which variable influences which one,", "tokens": [50772, 407, 321, 458, 264, 3877, 295, 264, 4295, 11, 321, 458, 597, 7006, 21222, 597, 472, 11, 51002], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 272, "seek": 146238, "start": 1475.14, "end": 1477.5800000000002, "text": " we know the arrows in general.", "tokens": [51002, 321, 458, 264, 19669, 294, 2674, 13, 51124], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 273, "seek": 146238, "start": 1477.5800000000002, "end": 1480.66, "text": " But in practice, this is actually not always possible.", "tokens": [51124, 583, 294, 3124, 11, 341, 307, 767, 406, 1009, 1944, 13, 51278], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 274, "seek": 146238, "start": 1480.66, "end": 1486.6200000000001, "text": " So we don't have access to the causal graph most of the times.", "tokens": [51278, 407, 321, 500, 380, 362, 2105, 281, 264, 38755, 4295, 881, 295, 264, 1413, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 275, "seek": 146238, "start": 1486.6200000000001, "end": 1490.94, "text": " And actually learning the best causal graph from data is still an open problem.", "tokens": [51576, 400, 767, 2539, 264, 1151, 38755, 4295, 490, 1412, 307, 920, 364, 1269, 1154, 13, 51792], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 276, "seek": 146238, "start": 1490.94, "end": 1491.94, "text": " We are improving in this.", "tokens": [51792, 492, 366, 11470, 294, 341, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12878177847181047, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.013986398465931416}, {"id": 277, "seek": 149194, "start": 1491.94, "end": 1493.14, "text": " We are getting better.", "tokens": [50364, 492, 366, 1242, 1101, 13, 50424], "temperature": 0.0, "avg_logprob": -0.1983152789834105, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.0013230014592409134}, {"id": 278, "seek": 149194, "start": 1493.14, "end": 1501.3400000000001, "text": " But how to perform this task exactly is still an open problem.", "tokens": [50424, 583, 577, 281, 2042, 341, 5633, 2293, 307, 920, 364, 1269, 1154, 13, 50834], "temperature": 0.0, "avg_logprob": -0.1983152789834105, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.0013230014592409134}, {"id": 279, "seek": 149194, "start": 1501.3400000000001, "end": 1505.8600000000001, "text": " So as I said, basically, the goal is to infer causal relationships from observational data.", "tokens": [50834, 407, 382, 286, 848, 11, 1936, 11, 264, 3387, 307, 281, 13596, 38755, 6159, 490, 9951, 1478, 1412, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1983152789834105, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.0013230014592409134}, {"id": 280, "seek": 149194, "start": 1505.8600000000001, "end": 1511.38, "text": " So given a data set, we want to infer the directed acyclic graph that describes the", "tokens": [51060, 407, 2212, 257, 1412, 992, 11, 321, 528, 281, 13596, 264, 12898, 696, 88, 66, 1050, 4295, 300, 15626, 264, 51336], "temperature": 0.0, "avg_logprob": -0.1983152789834105, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.0013230014592409134}, {"id": 281, "seek": 149194, "start": 1511.38, "end": 1516.14, "text": " connectivity between the system and the variables of the data set.", "tokens": [51336, 21095, 1296, 264, 1185, 293, 264, 9102, 295, 264, 1412, 992, 13, 51574], "temperature": 0.0, "avg_logprob": -0.1983152789834105, "compression_ratio": 1.539906103286385, "no_speech_prob": 0.0013230014592409134}, {"id": 282, "seek": 151614, "start": 1516.14, "end": 1522.5400000000002, "text": " So for example here, we have an example that I guess we are all familiar with thanks because", "tokens": [50364, 407, 337, 1365, 510, 11, 321, 362, 364, 1365, 300, 286, 2041, 321, 366, 439, 4963, 365, 3231, 570, 50684], "temperature": 0.0, "avg_logprob": -0.1453863779703776, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.036692071706056595}, {"id": 283, "seek": 151614, "start": 1522.5400000000002, "end": 1524.26, "text": " of the pandemic.", "tokens": [50684, 295, 264, 5388, 13, 50770], "temperature": 0.0, "avg_logprob": -0.1453863779703776, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.036692071706056595}, {"id": 284, "seek": 151614, "start": 1524.26, "end": 1531.74, "text": " So we have those four variables, age, vaccine, hospitalization, and CT.", "tokens": [50770, 407, 321, 362, 729, 1451, 9102, 11, 3205, 11, 7007, 11, 4530, 2144, 11, 293, 19529, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1453863779703776, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.036692071706056595}, {"id": 285, "seek": 151614, "start": 1531.74, "end": 1535.9, "text": " And we want to infer the causal dependencies among those variables.", "tokens": [51144, 400, 321, 528, 281, 13596, 264, 38755, 36606, 3654, 729, 9102, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1453863779703776, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.036692071706056595}, {"id": 286, "seek": 151614, "start": 1535.9, "end": 1540.38, "text": " So for example, we want to learn directly from data that the probability of a person", "tokens": [51352, 407, 337, 1365, 11, 321, 528, 281, 1466, 3838, 490, 1412, 300, 264, 8482, 295, 257, 954, 51576], "temperature": 0.0, "avg_logprob": -0.1453863779703776, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.036692071706056595}, {"id": 287, "seek": 154038, "start": 1540.38, "end": 1546.7800000000002, "text": " being hospitalized depends on its age and on the fact whether it's vaccinated or not,", "tokens": [50364, 885, 42340, 5946, 322, 1080, 3205, 293, 322, 264, 1186, 1968, 309, 311, 14686, 420, 406, 11, 50684], "temperature": 0.0, "avg_logprob": -0.17066627740859985, "compression_ratio": 1.57, "no_speech_prob": 0.028378505259752274}, {"id": 288, "seek": 154038, "start": 1546.7800000000002, "end": 1551.6200000000001, "text": " and so forth and so on.", "tokens": [50684, 293, 370, 5220, 293, 370, 322, 13, 50926], "temperature": 0.0, "avg_logprob": -0.17066627740859985, "compression_ratio": 1.57, "no_speech_prob": 0.028378505259752274}, {"id": 289, "seek": 154038, "start": 1551.6200000000001, "end": 1558.66, "text": " So this is the end of the long introduction, but I hope it was clear enough and I hope", "tokens": [50926, 407, 341, 307, 264, 917, 295, 264, 938, 9339, 11, 457, 286, 1454, 309, 390, 1850, 1547, 293, 286, 1454, 51278], "temperature": 0.0, "avg_logprob": -0.17066627740859985, "compression_ratio": 1.57, "no_speech_prob": 0.028378505259752274}, {"id": 290, "seek": 154038, "start": 1558.66, "end": 1565.0600000000002, "text": " that I gave the basics to understand basically the results of the paper.", "tokens": [51278, 300, 286, 2729, 264, 14688, 281, 1223, 1936, 264, 3542, 295, 264, 3035, 13, 51598], "temperature": 0.0, "avg_logprob": -0.17066627740859985, "compression_ratio": 1.57, "no_speech_prob": 0.028378505259752274}, {"id": 291, "seek": 154038, "start": 1565.0600000000002, "end": 1567.94, "text": " And now we can go to the research questions.", "tokens": [51598, 400, 586, 321, 393, 352, 281, 264, 2132, 1651, 13, 51742], "temperature": 0.0, "avg_logprob": -0.17066627740859985, "compression_ratio": 1.57, "no_speech_prob": 0.028378505259752274}, {"id": 292, "seek": 156794, "start": 1567.94, "end": 1570.8600000000001, "text": " So the research questions are the following.", "tokens": [50364, 407, 264, 2132, 1651, 366, 264, 3480, 13, 50510], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 293, "seek": 156794, "start": 1570.8600000000001, "end": 1577.3, "text": " First I want to see whether creative coding can be used to perform causal inference.", "tokens": [50510, 2386, 286, 528, 281, 536, 1968, 5880, 17720, 393, 312, 1143, 281, 2042, 38755, 38253, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 294, "seek": 156794, "start": 1577.3, "end": 1583.7, "text": " So creative coding so far has only been used to perform two compute correlations in Bayesian", "tokens": [50832, 407, 5880, 17720, 370, 1400, 575, 787, 668, 1143, 281, 2042, 732, 14722, 13983, 763, 294, 7840, 42434, 51152], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 295, "seek": 156794, "start": 1583.7, "end": 1585.38, "text": " networks.", "tokens": [51152, 9590, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 296, "seek": 156794, "start": 1585.38, "end": 1590.3400000000001, "text": " And the big question is, can we go beyond correlation and model intervention and counterfactual", "tokens": [51236, 400, 264, 955, 1168, 307, 11, 393, 321, 352, 4399, 20009, 293, 2316, 13176, 293, 5682, 44919, 901, 51484], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 297, "seek": 156794, "start": 1590.3400000000001, "end": 1592.94, "text": " in a biological, plausible way?", "tokens": [51484, 294, 257, 13910, 11, 39925, 636, 30, 51614], "temperature": 0.0, "avg_logprob": -0.2049234459199101, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.020593492314219475}, {"id": 298, "seek": 159294, "start": 1592.94, "end": 1598.94, "text": " So in a way that it's, for example, simple, intuitive, and allow us to only play with", "tokens": [50364, 407, 294, 257, 636, 300, 309, 311, 11, 337, 1365, 11, 2199, 11, 21769, 11, 293, 2089, 505, 281, 787, 862, 365, 50664], "temperature": 0.0, "avg_logprob": -0.1234375762939453, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006909776944667101}, {"id": 299, "seek": 159294, "start": 1598.94, "end": 1604.22, "text": " the neurons and not touch, for example, the huge structure of the graph.", "tokens": [50664, 264, 22027, 293, 406, 2557, 11, 337, 1365, 11, 264, 2603, 3877, 295, 264, 4295, 13, 50928], "temperature": 0.0, "avg_logprob": -0.1234375762939453, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006909776944667101}, {"id": 300, "seek": 159294, "start": 1604.22, "end": 1608.8200000000002, "text": " And more in practice, more specifically, the question becomes, can we define a creative", "tokens": [50928, 400, 544, 294, 3124, 11, 544, 4682, 11, 264, 1168, 3643, 11, 393, 321, 6964, 257, 5880, 51158], "temperature": 0.0, "avg_logprob": -0.1234375762939453, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006909776944667101}, {"id": 301, "seek": 159294, "start": 1608.8200000000002, "end": 1615.5800000000002, "text": " coding-based structural causal model to perform interventions and counterfactuals?", "tokens": [51158, 17720, 12, 6032, 15067, 38755, 2316, 281, 2042, 20924, 293, 5682, 44919, 901, 82, 30, 51496], "temperature": 0.0, "avg_logprob": -0.1234375762939453, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006909776944667101}, {"id": 302, "seek": 159294, "start": 1615.5800000000002, "end": 1621.3200000000002, "text": " The second question is, as I said, that having a structural causal model assumes that we", "tokens": [51496, 440, 1150, 1168, 307, 11, 382, 286, 848, 11, 300, 1419, 257, 15067, 38755, 2316, 37808, 300, 321, 51783], "temperature": 0.0, "avg_logprob": -0.1234375762939453, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006909776944667101}, {"id": 303, "seek": 162132, "start": 1621.32, "end": 1624.52, "text": " know the structure of the Bayesian network.", "tokens": [50364, 458, 264, 3877, 295, 264, 7840, 42434, 3209, 13, 50524], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 304, "seek": 162132, "start": 1624.52, "end": 1628.1599999999999, "text": " So it assumes that we have the arrows.", "tokens": [50524, 407, 309, 37808, 300, 321, 362, 264, 19669, 13, 50706], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 305, "seek": 162132, "start": 1628.1599999999999, "end": 1632.0, "text": " Can we go beyond this and use creative coding networks to learn the causal structure of", "tokens": [50706, 1664, 321, 352, 4399, 341, 293, 764, 5880, 17720, 9590, 281, 1466, 264, 38755, 3877, 295, 50898], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 306, "seek": 162132, "start": 1632.0, "end": 1634.2, "text": " the graph?", "tokens": [50898, 264, 4295, 30, 51008], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 307, "seek": 162132, "start": 1634.2, "end": 1641.56, "text": " Basically, giving positive answers to both those questions would allow us to use creative", "tokens": [51008, 8537, 11, 2902, 3353, 6338, 281, 1293, 729, 1651, 576, 2089, 505, 281, 764, 5880, 51376], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 308, "seek": 162132, "start": 1641.56, "end": 1647.3999999999999, "text": " coding as an end-to-end causal inference method, which basically takes a data set and allow", "tokens": [51376, 17720, 382, 364, 917, 12, 1353, 12, 521, 38755, 38253, 3170, 11, 597, 1936, 2516, 257, 1412, 992, 293, 2089, 51668], "temperature": 0.0, "avg_logprob": -0.18300862644993982, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.004670282360166311}, {"id": 309, "seek": 164740, "start": 1647.4, "end": 1657.0400000000002, "text": " us to test interventions and counterfactual predictions directly from this data set.", "tokens": [50364, 505, 281, 1500, 20924, 293, 5682, 44919, 901, 21264, 3838, 490, 341, 1412, 992, 13, 50846], "temperature": 0.0, "avg_logprob": -0.14841525619094437, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0030056757386773825}, {"id": 310, "seek": 164740, "start": 1657.0400000000002, "end": 1662.0400000000002, "text": " So let's tackle the first problem, so causal inference via creative coding, which is also", "tokens": [50846, 407, 718, 311, 14896, 264, 700, 1154, 11, 370, 38755, 38253, 5766, 5880, 17720, 11, 597, 307, 611, 51096], "temperature": 0.0, "avg_logprob": -0.14841525619094437, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0030056757386773825}, {"id": 311, "seek": 164740, "start": 1662.0400000000002, "end": 1667.0400000000002, "text": " the section that gives the title to the paper, basically.", "tokens": [51096, 264, 3541, 300, 2709, 264, 4876, 281, 264, 3035, 11, 1936, 13, 51346], "temperature": 0.0, "avg_logprob": -0.14841525619094437, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0030056757386773825}, {"id": 312, "seek": 164740, "start": 1667.0400000000002, "end": 1673.0400000000002, "text": " And here I will show how to perform correlations with creative coding, which is already known,", "tokens": [51346, 400, 510, 286, 486, 855, 577, 281, 2042, 13983, 763, 365, 5880, 17720, 11, 597, 307, 1217, 2570, 11, 51646], "temperature": 0.0, "avg_logprob": -0.14841525619094437, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0030056757386773825}, {"id": 313, "seek": 167304, "start": 1673.04, "end": 1681.44, "text": " and how to perform interventional queries, which I think is the real question of the paper.", "tokens": [50364, 293, 577, 281, 2042, 13176, 304, 24109, 11, 597, 286, 519, 307, 264, 957, 1168, 295, 264, 3035, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14246481818121834, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.008250551298260689}, {"id": 314, "seek": 167304, "start": 1681.44, "end": 1687.56, "text": " So here is a causal graph, which is the usual graph that we had.", "tokens": [50784, 407, 510, 307, 257, 38755, 4295, 11, 597, 307, 264, 7713, 4295, 300, 321, 632, 13, 51090], "temperature": 0.0, "avg_logprob": -0.14246481818121834, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.008250551298260689}, {"id": 315, "seek": 167304, "start": 1687.56, "end": 1690.52, "text": " And here is the corresponding creative coding model.", "tokens": [51090, 400, 510, 307, 264, 11760, 5880, 17720, 2316, 13, 51238], "temperature": 0.0, "avg_logprob": -0.14246481818121834, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.008250551298260689}, {"id": 316, "seek": 167304, "start": 1690.52, "end": 1695.96, "text": " So the axes are the latent variables and correspond to the neurons in a neural network", "tokens": [51238, 407, 264, 35387, 366, 264, 48994, 9102, 293, 6805, 281, 264, 22027, 294, 257, 18161, 3209, 51510], "temperature": 0.0, "avg_logprob": -0.14246481818121834, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.008250551298260689}, {"id": 317, "seek": 167304, "start": 1695.96, "end": 1698.36, "text": " model.", "tokens": [51510, 2316, 13, 51630], "temperature": 0.0, "avg_logprob": -0.14246481818121834, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.008250551298260689}, {"id": 318, "seek": 169836, "start": 1698.36, "end": 1705.8799999999999, "text": " And the black arrow passes prediction information from one neuron to the one down the hierarchy.", "tokens": [50364, 400, 264, 2211, 11610, 11335, 17630, 1589, 490, 472, 34090, 281, 264, 472, 760, 264, 22333, 13, 50740], "temperature": 0.0, "avg_logprob": -0.14089224372111575, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.010633928701281548}, {"id": 319, "seek": 169836, "start": 1705.8799999999999, "end": 1711.8799999999999, "text": " And every vertex also has this error neuron, which passes information up the hierarchy.", "tokens": [50740, 400, 633, 28162, 611, 575, 341, 6713, 34090, 11, 597, 11335, 1589, 493, 264, 22333, 13, 51040], "temperature": 0.0, "avg_logprob": -0.14089224372111575, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.010633928701281548}, {"id": 320, "seek": 169836, "start": 1711.8799999999999, "end": 1718.24, "text": " So the information of every error goes to the value node in the up the hierarchy and", "tokens": [51040, 407, 264, 1589, 295, 633, 6713, 1709, 281, 264, 2158, 9984, 294, 264, 493, 264, 22333, 293, 51358], "temperature": 0.0, "avg_logprob": -0.14089224372111575, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.010633928701281548}, {"id": 321, "seek": 169836, "start": 1718.24, "end": 1724.9599999999998, "text": " basically tells it to correct itself to change the prediction.", "tokens": [51358, 1936, 5112, 309, 281, 3006, 2564, 281, 1319, 264, 17630, 13, 51694], "temperature": 0.0, "avg_logprob": -0.14089224372111575, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.010633928701281548}, {"id": 322, "seek": 172496, "start": 1724.96, "end": 1729.1200000000001, "text": " So to perform a correlation using creative coding, what you have to do is that you take", "tokens": [50364, 407, 281, 2042, 257, 20009, 1228, 5880, 17720, 11, 437, 291, 362, 281, 360, 307, 300, 291, 747, 50572], "temperature": 0.0, "avg_logprob": -0.11624124547937414, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.07100164890289307}, {"id": 323, "seek": 172496, "start": 1729.1200000000001, "end": 1734.0, "text": " an observation and you simply fix the value of a specific neuron.", "tokens": [50572, 364, 14816, 293, 291, 2935, 3191, 264, 2158, 295, 257, 2685, 34090, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11624124547937414, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.07100164890289307}, {"id": 324, "seek": 172496, "start": 1734.0, "end": 1739.8400000000001, "text": " So if you want to compute the probability of X4 given X3 equal to S3, we simply have", "tokens": [50816, 407, 498, 291, 528, 281, 14722, 264, 8482, 295, 1783, 19, 2212, 1783, 18, 2681, 281, 318, 18, 11, 321, 2935, 362, 51108], "temperature": 0.0, "avg_logprob": -0.11624124547937414, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.07100164890289307}, {"id": 325, "seek": 172496, "start": 1739.8400000000001, "end": 1746.6000000000001, "text": " to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy", "tokens": [51108, 281, 747, 1783, 18, 293, 3191, 309, 281, 318, 18, 294, 257, 636, 300, 309, 1177, 380, 1319, 3602, 293, 1190, 364, 2281, 51446], "temperature": 0.0, "avg_logprob": -0.11624124547937414, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.07100164890289307}, {"id": 326, "seek": 172496, "start": 1746.6000000000001, "end": 1748.48, "text": " minimization.", "tokens": [51446, 4464, 2144, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11624124547937414, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.07100164890289307}, {"id": 327, "seek": 174848, "start": 1748.48, "end": 1756.32, "text": " And this model, by minimizing, by updating the axes via a minimization of the variational", "tokens": [50364, 400, 341, 2316, 11, 538, 46608, 11, 538, 25113, 264, 35387, 5766, 257, 4464, 2144, 295, 264, 3034, 1478, 50756], "temperature": 0.0, "avg_logprob": -0.22099323063106327, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004473400767892599}, {"id": 328, "seek": 174848, "start": 1756.32, "end": 1761.08, "text": " free energy, allows the model to converge to a solution to this question.", "tokens": [50756, 1737, 2281, 11, 4045, 264, 2316, 281, 41881, 281, 257, 3827, 281, 341, 1168, 13, 50994], "temperature": 0.0, "avg_logprob": -0.22099323063106327, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004473400767892599}, {"id": 329, "seek": 174848, "start": 1761.08, "end": 1766.32, "text": " So the probability or the expected value of X4 given X3 equals 3.", "tokens": [50994, 407, 264, 8482, 420, 264, 5176, 2158, 295, 1783, 19, 2212, 1783, 18, 6915, 805, 13, 51256], "temperature": 0.0, "avg_logprob": -0.22099323063106327, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004473400767892599}, {"id": 330, "seek": 174848, "start": 1766.32, "end": 1772.56, "text": " But how do I perform an intervention now without acting on the structure of the graph?", "tokens": [51256, 583, 577, 360, 286, 2042, 364, 13176, 586, 1553, 6577, 322, 264, 3877, 295, 264, 4295, 30, 51568], "temperature": 0.0, "avg_logprob": -0.22099323063106327, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004473400767892599}, {"id": 331, "seek": 174848, "start": 1772.56, "end": 1777.2, "text": " Well, this is basically the first idea of the paper.", "tokens": [51568, 1042, 11, 341, 307, 1936, 264, 700, 1558, 295, 264, 3035, 13, 51800], "temperature": 0.0, "avg_logprob": -0.22099323063106327, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004473400767892599}, {"id": 332, "seek": 177720, "start": 1778.16, "end": 1780.52, "text": " This is still how to perform a correlation.", "tokens": [50412, 639, 307, 920, 577, 281, 2042, 257, 20009, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24738759673043584, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.002308216877281666}, {"id": 333, "seek": 177720, "start": 1780.52, "end": 1785.24, "text": " So fix S3 equal to X3 is the first step in the algorithm.", "tokens": [50530, 407, 3191, 318, 18, 2681, 281, 1783, 18, 307, 264, 700, 1823, 294, 264, 9284, 13, 50766], "temperature": 0.0, "avg_logprob": -0.24738759673043584, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.002308216877281666}, {"id": 334, "seek": 177720, "start": 1785.24, "end": 1791.32, "text": " And the second one is to update the axes by minimizing the variational free energy.", "tokens": [50766, 400, 264, 1150, 472, 307, 281, 5623, 264, 35387, 538, 46608, 264, 3034, 1478, 1737, 2281, 13, 51070], "temperature": 0.0, "avg_logprob": -0.24738759673043584, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.002308216877281666}, {"id": 335, "seek": 177720, "start": 1791.32, "end": 1797.04, "text": " An intervention, which in theory corresponds in removing those arrows and answers to the", "tokens": [51070, 1107, 13176, 11, 597, 294, 5261, 23249, 294, 12720, 729, 19669, 293, 6338, 281, 264, 51356], "temperature": 0.0, "avg_logprob": -0.24738759673043584, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.002308216877281666}, {"id": 336, "seek": 177720, "start": 1797.04, "end": 1804.1200000000001, "text": " question, the probability of X4 by performing an intervention, so do X3 equal S3?", "tokens": [51356, 1168, 11, 264, 8482, 295, 1783, 19, 538, 10205, 364, 13176, 11, 370, 360, 1783, 18, 2681, 318, 18, 30, 51710], "temperature": 0.0, "avg_logprob": -0.24738759673043584, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.002308216877281666}, {"id": 337, "seek": 180412, "start": 1804.12, "end": 1807.32, "text": " This coding can be performed as follows.", "tokens": [50364, 639, 17720, 393, 312, 10332, 382, 10002, 13, 50524], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 338, "seek": 180412, "start": 1807.32, "end": 1810.12, "text": " So I'm going to write the algorithm here.", "tokens": [50524, 407, 286, 478, 516, 281, 2464, 264, 9284, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 339, "seek": 180412, "start": 1810.12, "end": 1819.1599999999999, "text": " So first, as in a correlation, you fix X3 equal to the observation that you get.", "tokens": [50664, 407, 700, 11, 382, 294, 257, 20009, 11, 291, 3191, 1783, 18, 2681, 281, 264, 14816, 300, 291, 483, 13, 51116], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 340, "seek": 180412, "start": 1819.1599999999999, "end": 1821.56, "text": " Then this is the important step.", "tokens": [51116, 1396, 341, 307, 264, 1021, 1823, 13, 51236], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 341, "seek": 180412, "start": 1821.56, "end": 1827.36, "text": " You have to intervene not on the graph anymore, but on the prediction error and fix it equal", "tokens": [51236, 509, 362, 281, 30407, 406, 322, 264, 4295, 3602, 11, 457, 322, 264, 17630, 6713, 293, 3191, 309, 2681, 51526], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 342, "seek": 180412, "start": 1827.36, "end": 1829.36, "text": " to zero.", "tokens": [51526, 281, 4018, 13, 51626], "temperature": 0.0, "avg_logprob": -0.17806114385157457, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.008216730318963528}, {"id": 343, "seek": 182936, "start": 1829.36, "end": 1836.84, "text": " Assuming a prediction error equal to zero basically makes sense, meaning less information", "tokens": [50364, 6281, 24919, 257, 17630, 6713, 2681, 281, 4018, 1936, 1669, 2020, 11, 3620, 1570, 1589, 50738], "temperature": 0.0, "avg_logprob": -0.19343911931755836, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.0048018405213952065}, {"id": 344, "seek": 182936, "start": 1836.84, "end": 1841.36, "text": " up the hierarchy or actually sends no information up the hierarchy because it basically tells", "tokens": [50738, 493, 264, 22333, 420, 767, 14790, 572, 1589, 493, 264, 22333, 570, 309, 1936, 5112, 50964], "temperature": 0.0, "avg_logprob": -0.19343911931755836, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.0048018405213952065}, {"id": 345, "seek": 182936, "start": 1841.36, "end": 1845.4799999999998, "text": " you that the prediction is always correct.", "tokens": [50964, 291, 300, 264, 17630, 307, 1009, 3006, 13, 51170], "temperature": 0.0, "avg_logprob": -0.19343911931755836, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.0048018405213952065}, {"id": 346, "seek": 182936, "start": 1845.4799999999998, "end": 1851.0, "text": " And the third step is to, as we did before, to update the axes, the unconstrained axis", "tokens": [51170, 400, 264, 2636, 1823, 307, 281, 11, 382, 321, 630, 949, 11, 281, 5623, 264, 35387, 11, 264, 35847, 19639, 2001, 10298, 51446], "temperature": 0.0, "avg_logprob": -0.19343911931755836, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.0048018405213952065}, {"id": 347, "seek": 182936, "start": 1851.0, "end": 1855.9599999999998, "text": " or X1, X2, X4 by minimizing the variational free energy.", "tokens": [51446, 420, 1783, 16, 11, 1783, 17, 11, 1783, 19, 538, 46608, 264, 3034, 1478, 1737, 2281, 13, 51694], "temperature": 0.0, "avg_logprob": -0.19343911931755836, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.0048018405213952065}, {"id": 348, "seek": 185596, "start": 1855.96, "end": 1861.68, "text": " As I will show now experimentally, by simply doing this little trick of setting a prediction", "tokens": [50364, 1018, 286, 486, 855, 586, 5120, 379, 11, 538, 2935, 884, 341, 707, 4282, 295, 3287, 257, 17630, 50650], "temperature": 0.0, "avg_logprob": -0.1578396224975586, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.00261607370339334}, {"id": 349, "seek": 185596, "start": 1861.68, "end": 1869.08, "text": " error to be equal to zero, it prevents us to actually act on the structure of the graph", "tokens": [50650, 6713, 281, 312, 2681, 281, 4018, 11, 309, 22367, 505, 281, 767, 605, 322, 264, 3877, 295, 264, 4295, 51020], "temperature": 0.0, "avg_logprob": -0.1578396224975586, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.00261607370339334}, {"id": 350, "seek": 185596, "start": 1869.08, "end": 1877.92, "text": " as the theory of Duke-Alculus does and to infer the variables after an intervention", "tokens": [51020, 382, 264, 5261, 295, 17380, 12, 9171, 36002, 775, 293, 281, 13596, 264, 9102, 934, 364, 13176, 51462], "temperature": 0.0, "avg_logprob": -0.1578396224975586, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.00261607370339334}, {"id": 351, "seek": 185596, "start": 1877.92, "end": 1885.0, "text": " by simply performing a variational free energy minimization.", "tokens": [51462, 538, 2935, 10205, 257, 3034, 1478, 1737, 2281, 4464, 2144, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1578396224975586, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.00261607370339334}, {"id": 352, "seek": 188500, "start": 1885.0, "end": 1886.76, "text": " What about counterfactual inference?", "tokens": [50364, 708, 466, 5682, 44919, 901, 38253, 30, 50452], "temperature": 0.0, "avg_logprob": -0.1815782484117445, "compression_ratio": 1.8, "no_speech_prob": 0.08602303266525269}, {"id": 353, "seek": 188500, "start": 1886.76, "end": 1895.04, "text": " Counterfactual inference is actually easy once we have defined how to do an intervention.", "tokens": [50452, 35607, 44919, 901, 38253, 307, 767, 1858, 1564, 321, 362, 7642, 577, 281, 360, 364, 13176, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1815782484117445, "compression_ratio": 1.8, "no_speech_prob": 0.08602303266525269}, {"id": 354, "seek": 188500, "start": 1895.04, "end": 1899.04, "text": " And this is because, as we saw earlier, performing a counterfactual is similar to performing an", "tokens": [50866, 400, 341, 307, 570, 11, 382, 321, 1866, 3071, 11, 10205, 257, 5682, 44919, 901, 307, 2531, 281, 10205, 364, 51066], "temperature": 0.0, "avg_logprob": -0.1815782484117445, "compression_ratio": 1.8, "no_speech_prob": 0.08602303266525269}, {"id": 355, "seek": 188500, "start": 1899.04, "end": 1908.32, "text": " intervention in a past situation after you have inferred the unobservable variables.", "tokens": [51066, 13176, 294, 257, 1791, 2590, 934, 291, 362, 13596, 986, 264, 8526, 929, 1978, 712, 9102, 13, 51530], "temperature": 0.0, "avg_logprob": -0.1815782484117445, "compression_ratio": 1.8, "no_speech_prob": 0.08602303266525269}, {"id": 356, "seek": 188500, "start": 1908.32, "end": 1913.76, "text": " So as you can see in the plot I showed earlier about the abduction action and prediction", "tokens": [51530, 407, 382, 291, 393, 536, 294, 264, 7542, 286, 4712, 3071, 466, 264, 410, 40335, 3069, 293, 17630, 51802], "temperature": 0.0, "avg_logprob": -0.1815782484117445, "compression_ratio": 1.8, "no_speech_prob": 0.08602303266525269}, {"id": 357, "seek": 191376, "start": 1913.76, "end": 1919.92, "text": " steps, the action and prediction steps, they did not have those two arrows.", "tokens": [50364, 4439, 11, 264, 3069, 293, 17630, 4439, 11, 436, 630, 406, 362, 729, 732, 19669, 13, 50672], "temperature": 0.0, "avg_logprob": -0.19390569589076898, "compression_ratio": 1.693121693121693, "no_speech_prob": 0.047390881925821304}, {"id": 358, "seek": 191376, "start": 1919.92, "end": 1921.68, "text": " They were removed.", "tokens": [50672, 814, 645, 7261, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19390569589076898, "compression_ratio": 1.693121693121693, "no_speech_prob": 0.047390881925821304}, {"id": 359, "seek": 191376, "start": 1921.68, "end": 1931.2, "text": " Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by", "tokens": [50760, 10693, 17720, 4045, 505, 281, 1066, 264, 19669, 294, 264, 4295, 293, 2042, 5682, 44919, 901, 82, 538, 51236], "temperature": 0.0, "avg_logprob": -0.19390569589076898, "compression_ratio": 1.693121693121693, "no_speech_prob": 0.047390881925821304}, {"id": 360, "seek": 191376, "start": 1931.2, "end": 1936.24, "text": " simply performing an abduction step, as it was done earlier, an action step in which we", "tokens": [51236, 2935, 10205, 364, 410, 40335, 1823, 11, 382, 309, 390, 1096, 3071, 11, 364, 3069, 1823, 294, 597, 321, 51488], "temperature": 0.0, "avg_logprob": -0.19390569589076898, "compression_ratio": 1.693121693121693, "no_speech_prob": 0.047390881925821304}, {"id": 361, "seek": 191376, "start": 1936.24, "end": 1939.4, "text": " simply perform an intervention on the single node.", "tokens": [51488, 2935, 2042, 364, 13176, 322, 264, 2167, 9984, 13, 51646], "temperature": 0.0, "avg_logprob": -0.19390569589076898, "compression_ratio": 1.693121693121693, "no_speech_prob": 0.047390881925821304}, {"id": 362, "seek": 193940, "start": 1939.4, "end": 1946.2800000000002, "text": " So we fix the value node and we set the error to zero and run the energy minimization, so", "tokens": [50364, 407, 321, 3191, 264, 2158, 9984, 293, 321, 992, 264, 6713, 281, 4018, 293, 1190, 264, 2281, 4464, 2144, 11, 370, 50708], "temperature": 0.0, "avg_logprob": -0.2031720933460054, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.04076094552874565}, {"id": 363, "seek": 193940, "start": 1946.2800000000002, "end": 1952.96, "text": " minimizing the variational free energy to compute the prediction.", "tokens": [50708, 46608, 264, 3034, 1478, 1737, 2281, 281, 14722, 264, 17630, 13, 51042], "temperature": 0.0, "avg_logprob": -0.2031720933460054, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.04076094552874565}, {"id": 364, "seek": 193940, "start": 1952.96, "end": 1961.5600000000002, "text": " So I think this is like an easy and elegant method to perform interventions and counterfactuals.", "tokens": [51042, 407, 286, 519, 341, 307, 411, 364, 1858, 293, 21117, 3170, 281, 2042, 20924, 293, 5682, 44919, 901, 82, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2031720933460054, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.04076094552874565}, {"id": 365, "seek": 193940, "start": 1961.5600000000002, "end": 1967.8000000000002, "text": " And yeah, so I think the thing we have to show now is whether it works in practice or not.", "tokens": [51472, 400, 1338, 11, 370, 286, 519, 264, 551, 321, 362, 281, 855, 586, 307, 1968, 309, 1985, 294, 3124, 420, 406, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2031720933460054, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.04076094552874565}, {"id": 366, "seek": 196780, "start": 1967.8, "end": 1970.3999999999999, "text": " And we have a couple of experiments.", "tokens": [50364, 400, 321, 362, 257, 1916, 295, 12050, 13, 50494], "temperature": 0.0, "avg_logprob": -0.19801635212368435, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.06695984303951263}, {"id": 367, "seek": 196780, "start": 1970.3999999999999, "end": 1973.56, "text": " And I'm going to show you now two different experiments.", "tokens": [50494, 400, 286, 478, 516, 281, 855, 291, 586, 732, 819, 12050, 13, 50652], "temperature": 0.0, "avg_logprob": -0.19801635212368435, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.06695984303951263}, {"id": 368, "seek": 196780, "start": 1973.56, "end": 1981.32, "text": " The first one is merely proof of concept experiment that shows that the predictive coding is able", "tokens": [50652, 440, 700, 472, 307, 17003, 8177, 295, 3410, 5120, 300, 3110, 300, 264, 35521, 17720, 307, 1075, 51040], "temperature": 0.0, "avg_logprob": -0.19801635212368435, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.06695984303951263}, {"id": 369, "seek": 196780, "start": 1981.32, "end": 1986.3999999999999, "text": " to perform intervention and counterfactuals.", "tokens": [51040, 281, 2042, 13176, 293, 5682, 44919, 901, 82, 13, 51294], "temperature": 0.0, "avg_logprob": -0.19801635212368435, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.06695984303951263}, {"id": 370, "seek": 196780, "start": 1986.3999999999999, "end": 1992.2, "text": " And the second one actually shows a simple application in how interventional queries", "tokens": [51294, 400, 264, 1150, 472, 767, 3110, 257, 2199, 3861, 294, 577, 13176, 304, 24109, 51584], "temperature": 0.0, "avg_logprob": -0.19801635212368435, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.06695984303951263}, {"id": 371, "seek": 199220, "start": 1992.2, "end": 1997.52, "text": " can be used to improve the performance of classification tasks on a specific kind of", "tokens": [50364, 393, 312, 1143, 281, 3470, 264, 3389, 295, 21538, 9608, 322, 257, 2685, 733, 295, 50630], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 372, "seek": 199220, "start": 1997.52, "end": 2002.72, "text": " predictive coding networks, which is that of a fully connected model.", "tokens": [50630, 35521, 17720, 9590, 11, 597, 307, 300, 295, 257, 4498, 4582, 2316, 13, 50890], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 373, "seek": 199220, "start": 2002.72, "end": 2004.8400000000001, "text": " Let's start from the first one.", "tokens": [50890, 961, 311, 722, 490, 264, 700, 472, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 374, "seek": 199220, "start": 2004.8400000000001, "end": 2006.44, "text": " So how do we do this task?", "tokens": [50996, 407, 577, 360, 321, 360, 341, 5633, 30, 51076], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 375, "seek": 199220, "start": 2006.44, "end": 2014.6000000000001, "text": " So given a structural causal model, we generate training data and we use it to learn the weights,", "tokens": [51076, 407, 2212, 257, 15067, 38755, 2316, 11, 321, 8460, 3097, 1412, 293, 321, 764, 309, 281, 1466, 264, 17443, 11, 51484], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 376, "seek": 199220, "start": 2014.6000000000001, "end": 2020.68, "text": " so to learn the functions of the structural causal models.", "tokens": [51484, 370, 281, 1466, 264, 6828, 295, 264, 15067, 38755, 5245, 13, 51788], "temperature": 0.0, "avg_logprob": -0.12374981869472547, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.010716441087424755}, {"id": 377, "seek": 202068, "start": 2020.68, "end": 2026.3600000000001, "text": " And then we generate test data for both interventional and counterfactual queries.", "tokens": [50364, 400, 550, 321, 8460, 1500, 1412, 337, 1293, 13176, 304, 293, 5682, 44919, 901, 24109, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12560280357919087, "compression_ratio": 1.8, "no_speech_prob": 0.010104548186063766}, {"id": 378, "seek": 202068, "start": 2026.3600000000001, "end": 2031.54, "text": " And we show whether we are able to converge to the correct test data using predictive", "tokens": [50648, 400, 321, 855, 1968, 321, 366, 1075, 281, 41881, 281, 264, 3006, 1500, 1412, 1228, 35521, 50907], "temperature": 0.0, "avg_logprob": -0.12560280357919087, "compression_ratio": 1.8, "no_speech_prob": 0.010104548186063766}, {"id": 379, "seek": 202068, "start": 2031.54, "end": 2033.76, "text": " coding.", "tokens": [50907, 17720, 13, 51018], "temperature": 0.0, "avg_logprob": -0.12560280357919087, "compression_ratio": 1.8, "no_speech_prob": 0.010104548186063766}, {"id": 380, "seek": 202068, "start": 2033.76, "end": 2040.8400000000001, "text": " And for example here, those two plots represent the interventional and counterfactual queries", "tokens": [51018, 400, 337, 1365, 510, 11, 729, 732, 28609, 2906, 264, 13176, 304, 293, 5682, 44919, 901, 24109, 51372], "temperature": 0.0, "avg_logprob": -0.12560280357919087, "compression_ratio": 1.8, "no_speech_prob": 0.010104548186063766}, {"id": 381, "seek": 202068, "start": 2040.8400000000001, "end": 2046.28, "text": " of this specific graph, which is the butterfly bias graph, which is a graph that is often", "tokens": [51372, 295, 341, 2685, 4295, 11, 597, 307, 264, 22140, 12577, 4295, 11, 597, 307, 257, 4295, 300, 307, 2049, 51644], "temperature": 0.0, "avg_logprob": -0.12560280357919087, "compression_ratio": 1.8, "no_speech_prob": 0.010104548186063766}, {"id": 382, "seek": 204628, "start": 2046.28, "end": 2052.0, "text": " used in testing whether causal inference, whether interventional and counterfactual", "tokens": [50364, 1143, 294, 4997, 1968, 38755, 38253, 11, 1968, 13176, 304, 293, 5682, 44919, 901, 50650], "temperature": 0.0, "avg_logprob": -0.18431503016774248, "compression_ratio": 1.8477157360406091, "no_speech_prob": 0.017800454050302505}, {"id": 383, "seek": 204628, "start": 2052.0, "end": 2055.44, "text": " techniques work is as simple as that.", "tokens": [50650, 7512, 589, 307, 382, 2199, 382, 300, 13, 50822], "temperature": 0.0, "avg_logprob": -0.18431503016774248, "compression_ratio": 1.8477157360406091, "no_speech_prob": 0.017800454050302505}, {"id": 384, "seek": 204628, "start": 2055.44, "end": 2059.52, "text": " But in the paper, you can find a lot of different graphs.", "tokens": [50822, 583, 294, 264, 3035, 11, 291, 393, 915, 257, 688, 295, 819, 24877, 13, 51026], "temperature": 0.0, "avg_logprob": -0.18431503016774248, "compression_ratio": 1.8477157360406091, "no_speech_prob": 0.017800454050302505}, {"id": 385, "seek": 204628, "start": 2059.52, "end": 2069.48, "text": " But in general, those two plots show that the method works, show that the mean absolute", "tokens": [51026, 583, 294, 2674, 11, 729, 732, 28609, 855, 300, 264, 3170, 1985, 11, 855, 300, 264, 914, 8236, 51524], "temperature": 0.0, "avg_logprob": -0.18431503016774248, "compression_ratio": 1.8477157360406091, "no_speech_prob": 0.017800454050302505}, {"id": 386, "seek": 204628, "start": 2069.48, "end": 2076.24, "text": " error between the interventional and counterfactual quantities we compute and the interventional", "tokens": [51524, 6713, 1296, 264, 13176, 304, 293, 5682, 44919, 901, 22927, 321, 14722, 293, 264, 13176, 304, 51862], "temperature": 0.0, "avg_logprob": -0.18431503016774248, "compression_ratio": 1.8477157360406091, "no_speech_prob": 0.017800454050302505}, {"id": 387, "seek": 207624, "start": 2076.72, "end": 2083.08, "text": " and counterfactual quantities from the original graph are close to each other.", "tokens": [50388, 293, 5682, 44919, 901, 22927, 490, 264, 3380, 4295, 366, 1998, 281, 1184, 661, 13, 50706], "temperature": 0.0, "avg_logprob": -0.17557595196892234, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.007876780815422535}, {"id": 388, "seek": 207624, "start": 2083.08, "end": 2086.6, "text": " So the error is quite small.", "tokens": [50706, 407, 264, 6713, 307, 1596, 1359, 13, 50882], "temperature": 0.0, "avg_logprob": -0.17557595196892234, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.007876780815422535}, {"id": 389, "seek": 207624, "start": 2086.6, "end": 2091.9199999999996, "text": " The second experiment is basically an extension of an experiment I proposed in an earlier", "tokens": [50882, 440, 1150, 5120, 307, 1936, 364, 10320, 295, 364, 5120, 286, 10348, 294, 364, 3071, 51148], "temperature": 0.0, "avg_logprob": -0.17557595196892234, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.007876780815422535}, {"id": 390, "seek": 207624, "start": 2091.9199999999996, "end": 2099.3199999999997, "text": " paper, which is the learning on arbitrary graph topologies that I wrote last year.", "tokens": [51148, 3035, 11, 597, 307, 264, 2539, 322, 23211, 4295, 1192, 6204, 300, 286, 4114, 1036, 1064, 13, 51518], "temperature": 0.0, "avg_logprob": -0.17557595196892234, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.007876780815422535}, {"id": 391, "seek": 207624, "start": 2099.3199999999997, "end": 2105.7999999999997, "text": " In that paper, I basically proposed this kind of network as a proof of concept, which is", "tokens": [51518, 682, 300, 3035, 11, 286, 1936, 10348, 341, 733, 295, 3209, 382, 257, 8177, 295, 3410, 11, 597, 307, 51842], "temperature": 0.0, "avg_logprob": -0.17557595196892234, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.007876780815422535}, {"id": 392, "seek": 210580, "start": 2105.8, "end": 2114.0, "text": " a fully connected network, which is in general the worst neural network you can have to perform", "tokens": [50364, 257, 4498, 4582, 3209, 11, 597, 307, 294, 2674, 264, 5855, 18161, 3209, 291, 393, 362, 281, 2042, 50774], "temperature": 0.0, "avg_logprob": -0.19740177268412576, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.004805371630936861}, {"id": 393, "seek": 210580, "start": 2114.0, "end": 2125.52, "text": " machine learning experiments, because given a fixed set of neurons, basically every pair", "tokens": [50774, 3479, 2539, 12050, 11, 570, 2212, 257, 6806, 992, 295, 22027, 11, 1936, 633, 6119, 51350], "temperature": 0.0, "avg_logprob": -0.19740177268412576, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.004805371630936861}, {"id": 394, "seek": 210580, "start": 2125.52, "end": 2128.6000000000004, "text": " of neuron is connected by two different synapses.", "tokens": [51350, 295, 34090, 307, 4582, 538, 732, 819, 5451, 2382, 279, 13, 51504], "temperature": 0.0, "avg_logprob": -0.19740177268412576, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.004805371630936861}, {"id": 395, "seek": 210580, "start": 2128.6000000000004, "end": 2134.88, "text": " So it's the model with the highest complexity possible in general.", "tokens": [51504, 407, 309, 311, 264, 2316, 365, 264, 6343, 14024, 1944, 294, 2674, 13, 51818], "temperature": 0.0, "avg_logprob": -0.19740177268412576, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.004805371630936861}, {"id": 396, "seek": 213488, "start": 2134.88, "end": 2138.32, "text": " The good thing is that since you have a lot of cycles, the model is extremely flexible", "tokens": [50364, 440, 665, 551, 307, 300, 1670, 291, 362, 257, 688, 295, 17796, 11, 264, 2316, 307, 4664, 11358, 50536], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 397, "seek": 213488, "start": 2138.32, "end": 2143.92, "text": " in the sense that you can train it, for example, on a minst image and on a data point and on", "tokens": [50536, 294, 264, 2020, 300, 291, 393, 3847, 309, 11, 337, 1365, 11, 322, 257, 923, 372, 3256, 293, 322, 257, 1412, 935, 293, 322, 50816], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 398, "seek": 213488, "start": 2143.92, "end": 2144.92, "text": " its label.", "tokens": [50816, 1080, 7645, 13, 50866], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 399, "seek": 213488, "start": 2144.92, "end": 2150.76, "text": " But then the way you can query it, thanks to the information going back, is you can query", "tokens": [50866, 583, 550, 264, 636, 291, 393, 14581, 309, 11, 3231, 281, 264, 1589, 516, 646, 11, 307, 291, 393, 14581, 51158], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 400, "seek": 213488, "start": 2150.76, "end": 2151.76, "text": " in a lot of different ways.", "tokens": [51158, 294, 257, 688, 295, 819, 2098, 13, 51208], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 401, "seek": 213488, "start": 2151.76, "end": 2156.08, "text": " So you can form classification tasks in which you provide an image and you run the energy", "tokens": [51208, 407, 291, 393, 1254, 21538, 9608, 294, 597, 291, 2893, 364, 3256, 293, 291, 1190, 264, 2281, 51424], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 402, "seek": 213488, "start": 2156.08, "end": 2158.12, "text": " minimization and get the label.", "tokens": [51424, 4464, 2144, 293, 483, 264, 7645, 13, 51526], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 403, "seek": 213488, "start": 2158.12, "end": 2161.76, "text": " But you can also, for example, perform generation tasks in which you give the label, run the", "tokens": [51526, 583, 291, 393, 611, 11, 337, 1365, 11, 2042, 5125, 9608, 294, 597, 291, 976, 264, 7645, 11, 1190, 264, 51708], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 404, "seek": 213488, "start": 2161.76, "end": 2164.28, "text": " energy minimization and get the image.", "tokens": [51708, 2281, 4464, 2144, 293, 483, 264, 3256, 13, 51834], "temperature": 0.0, "avg_logprob": -0.14380045731862387, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.009484570473432541}, {"id": 405, "seek": 216428, "start": 2164.28, "end": 2171.1600000000003, "text": " You can perform, for example, image completion, which should give half the image and let the", "tokens": [50364, 509, 393, 2042, 11, 337, 1365, 11, 3256, 19372, 11, 597, 820, 976, 1922, 264, 3256, 293, 718, 264, 50708], "temperature": 0.0, "avg_logprob": -0.18949268993578458, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.00939385499805212}, {"id": 406, "seek": 216428, "start": 2171.1600000000003, "end": 2173.8, "text": " model converge to the second half and so forth and so on.", "tokens": [50708, 2316, 41881, 281, 264, 1150, 1922, 293, 370, 5220, 293, 370, 322, 13, 50840], "temperature": 0.0, "avg_logprob": -0.18949268993578458, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.00939385499805212}, {"id": 407, "seek": 216428, "start": 2173.8, "end": 2180.1600000000003, "text": " So it's basically a model that learns the statistics of the dataset in its entirety", "tokens": [50840, 407, 309, 311, 1936, 257, 2316, 300, 27152, 264, 12523, 295, 264, 28872, 294, 1080, 31557, 51158], "temperature": 0.0, "avg_logprob": -0.18949268993578458, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.00939385499805212}, {"id": 408, "seek": 216428, "start": 2180.1600000000003, "end": 2185.4, "text": " without being focused on classification or generation in general.", "tokens": [51158, 1553, 885, 5178, 322, 21538, 420, 5125, 294, 2674, 13, 51420], "temperature": 0.0, "avg_logprob": -0.18949268993578458, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.00939385499805212}, {"id": 409, "seek": 216428, "start": 2185.4, "end": 2188.1600000000003, "text": " So this flexibility is great.", "tokens": [51420, 407, 341, 12635, 307, 869, 13, 51558], "temperature": 0.0, "avg_logprob": -0.18949268993578458, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.00939385499805212}, {"id": 410, "seek": 218816, "start": 2188.16, "end": 2194.12, "text": " The problem is that because of this, every single task doesn't work well.", "tokens": [50364, 440, 1154, 307, 300, 570, 295, 341, 11, 633, 2167, 5633, 1177, 380, 589, 731, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1471135351392958, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.010591653175652027}, {"id": 411, "seek": 218816, "start": 2194.12, "end": 2199.08, "text": " So you can do a lot of different things, but none of them is done well.", "tokens": [50662, 407, 291, 393, 360, 257, 688, 295, 819, 721, 11, 457, 6022, 295, 552, 307, 1096, 731, 13, 50910], "temperature": 0.0, "avg_logprob": -0.1471135351392958, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.010591653175652027}, {"id": 412, "seek": 218816, "start": 2199.08, "end": 2206.3999999999996, "text": " And here I want to show how using interventional queries instead of standard correlation queries", "tokens": [50910, 400, 510, 286, 528, 281, 855, 577, 1228, 13176, 304, 24109, 2602, 295, 3832, 20009, 24109, 51276], "temperature": 0.0, "avg_logprob": -0.1471135351392958, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.010591653175652027}, {"id": 413, "seek": 218816, "start": 2206.3999999999996, "end": 2212.2, "text": " or conditional queries slightly improves their results of those classification tasks.", "tokens": [51276, 420, 27708, 24109, 4748, 24771, 641, 3542, 295, 729, 21538, 9608, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1471135351392958, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.010591653175652027}, {"id": 414, "seek": 221220, "start": 2212.2, "end": 2219.7599999999998, "text": " So what are the conjecture reasons of this test accuracy on those tasks not being so", "tokens": [50364, 407, 437, 366, 264, 416, 1020, 540, 4112, 295, 341, 1500, 14170, 322, 729, 9608, 406, 885, 370, 50742], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 415, "seek": 221220, "start": 2219.7599999999998, "end": 2220.7599999999998, "text": " high?", "tokens": [50742, 1090, 30, 50792], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 416, "seek": 221220, "start": 2220.7599999999998, "end": 2227.2599999999998, "text": " The first, the two reasons are that the model is distracted in correcting every single error.", "tokens": [50792, 440, 700, 11, 264, 732, 4112, 366, 300, 264, 2316, 307, 21658, 294, 47032, 633, 2167, 6713, 13, 51117], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 417, "seek": 221220, "start": 2227.2599999999998, "end": 2231.2, "text": " So basically you present an image and you would like to get a label, but the model is", "tokens": [51117, 407, 1936, 291, 1974, 364, 3256, 293, 291, 576, 411, 281, 483, 257, 7645, 11, 457, 264, 2316, 307, 51314], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 418, "seek": 221220, "start": 2231.2, "end": 2237.04, "text": " actually updating itself to also predict the error in the images.", "tokens": [51314, 767, 25113, 2564, 281, 611, 6069, 264, 6713, 294, 264, 5267, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 419, "seek": 221220, "start": 2237.04, "end": 2241.96, "text": " And the second reason, which is the one I said, is that the structure is far too complex.", "tokens": [51606, 400, 264, 1150, 1778, 11, 597, 307, 264, 472, 286, 848, 11, 307, 300, 264, 3877, 307, 1400, 886, 3997, 13, 51852], "temperature": 0.0, "avg_logprob": -0.1130996300623967, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.005910119507461786}, {"id": 420, "seek": 224196, "start": 2241.96, "end": 2250.52, "text": " So again, from an Occam razor argumentation, this is the worst model you can have.", "tokens": [50364, 407, 797, 11, 490, 364, 26191, 335, 30478, 6770, 399, 11, 341, 307, 264, 5855, 2316, 291, 393, 362, 13, 50792], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 421, "seek": 224196, "start": 2250.52, "end": 2254.2, "text": " So every time you have a model that fits a dataset, that model is going to be less complex", "tokens": [50792, 407, 633, 565, 291, 362, 257, 2316, 300, 9001, 257, 28872, 11, 300, 2316, 307, 516, 281, 312, 1570, 3997, 50976], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 422, "seek": 224196, "start": 2254.2, "end": 2256.64, "text": " than this one that is going to be preferred.", "tokens": [50976, 813, 341, 472, 300, 307, 516, 281, 312, 16494, 13, 51098], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 423, "seek": 224196, "start": 2256.64, "end": 2263.8, "text": " But in general, just to start it, the idea is can querying this model be interventions", "tokens": [51098, 583, 294, 2674, 11, 445, 281, 722, 309, 11, 264, 1558, 307, 393, 7083, 1840, 341, 2316, 312, 20924, 51456], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 424, "seek": 224196, "start": 2263.8, "end": 2268.16, "text": " be used to improve the performance of those fully connected models?", "tokens": [51456, 312, 1143, 281, 3470, 264, 3389, 295, 729, 4498, 4582, 5245, 30, 51674], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 425, "seek": 224196, "start": 2268.16, "end": 2271.32, "text": " Well, the answer is yes.", "tokens": [51674, 1042, 11, 264, 1867, 307, 2086, 13, 51832], "temperature": 0.0, "avg_logprob": -0.18150065926944509, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.0024212650023400784}, {"id": 426, "seek": 227132, "start": 2271.32, "end": 2273.6800000000003, "text": " So here is how I perform interventional queries.", "tokens": [50364, 407, 510, 307, 577, 286, 2042, 13176, 304, 24109, 13, 50482], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 427, "seek": 227132, "start": 2273.6800000000003, "end": 2276.8, "text": " So I present an image to the network.", "tokens": [50482, 407, 286, 1974, 364, 3256, 281, 264, 3209, 13, 50638], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 428, "seek": 227132, "start": 2276.8, "end": 2280.1600000000003, "text": " I fix the error of the pixels to be equal to zero.", "tokens": [50638, 286, 3191, 264, 6713, 295, 264, 18668, 281, 312, 2681, 281, 4018, 13, 50806], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 429, "seek": 227132, "start": 2280.1600000000003, "end": 2283.48, "text": " So this error doesn't get propagated in the network.", "tokens": [50806, 407, 341, 6713, 1177, 380, 483, 12425, 770, 294, 264, 3209, 13, 50972], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 430, "seek": 227132, "start": 2283.48, "end": 2286.0, "text": " And then I compute the label.", "tokens": [50972, 400, 550, 286, 14722, 264, 7645, 13, 51098], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 431, "seek": 227132, "start": 2286.0, "end": 2291.4, "text": " And as you can see, the accuracy improves, for example, from 89 using the standard query", "tokens": [51098, 400, 382, 291, 393, 536, 11, 264, 14170, 24771, 11, 337, 1365, 11, 490, 31877, 1228, 264, 3832, 14581, 51368], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 432, "seek": 227132, "start": 2291.4, "end": 2297.6800000000003, "text": " method of pretty difficult in networks to 92, which is the accuracy after the intervention", "tokens": [51368, 3170, 295, 1238, 2252, 294, 9590, 281, 28225, 11, 597, 307, 264, 14170, 934, 264, 13176, 51682], "temperature": 0.0, "avg_logprob": -0.15631423277013443, "compression_ratio": 1.646090534979424, "no_speech_prob": 0.0007421373738907278}, {"id": 433, "seek": 229768, "start": 2297.68, "end": 2302.0, "text": " and the same happens for fashion means.", "tokens": [50364, 293, 264, 912, 2314, 337, 6700, 1355, 13, 50580], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 434, "seek": 229768, "start": 2302.0, "end": 2307.48, "text": " And I think that a very legit critic that probably everyone would think when seeing", "tokens": [50580, 400, 286, 519, 300, 257, 588, 10275, 7850, 300, 1391, 1518, 576, 519, 562, 2577, 50854], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 435, "seek": 229768, "start": 2307.48, "end": 2314.3999999999996, "text": " those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically.", "tokens": [50854, 729, 28609, 307, 300, 11, 2264, 11, 291, 3470, 322, 1355, 490, 31877, 281, 28225, 11, 309, 920, 15846, 11, 1936, 13, 51200], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 436, "seek": 229768, "start": 2314.3999999999996, "end": 2316.72, "text": " And yeah, it's true.", "tokens": [51200, 400, 1338, 11, 309, 311, 2074, 13, 51316], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 437, "seek": 229768, "start": 2316.72, "end": 2320.9199999999996, "text": " And I'm actually in the later slides, I'm going to show how to act on the structure", "tokens": [51316, 400, 286, 478, 767, 294, 264, 1780, 9788, 11, 286, 478, 516, 281, 855, 577, 281, 605, 322, 264, 3877, 51526], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 438, "seek": 229768, "start": 2320.9199999999996, "end": 2327.04, "text": " of this fully connected model will improve the results even more until the point they", "tokens": [51526, 295, 341, 4498, 4582, 2316, 486, 3470, 264, 3542, 754, 544, 1826, 264, 935, 436, 51832], "temperature": 0.0, "avg_logprob": -0.20165781455464882, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.10661210864782333}, {"id": 439, "seek": 232704, "start": 2327.72, "end": 2333.0, "text": " reach a performance that is not even close to state of the art performance, of course.", "tokens": [50398, 2524, 257, 3389, 300, 307, 406, 754, 1998, 281, 1785, 295, 264, 1523, 3389, 11, 295, 1164, 13, 50662], "temperature": 0.0, "avg_logprob": -0.23388884617732122, "compression_ratio": 1.55, "no_speech_prob": 0.003564183134585619}, {"id": 440, "seek": 232704, "start": 2333.0, "end": 2342.24, "text": " But it's still up to a level that becomes basically acceptable and worth investigating.", "tokens": [50662, 583, 309, 311, 920, 493, 281, 257, 1496, 300, 3643, 1936, 15513, 293, 3163, 22858, 13, 51124], "temperature": 0.0, "avg_logprob": -0.23388884617732122, "compression_ratio": 1.55, "no_speech_prob": 0.003564183134585619}, {"id": 441, "seek": 232704, "start": 2342.24, "end": 2348.68, "text": " So yes, so this is the part about causal inference using predictive coding.", "tokens": [51124, 407, 2086, 11, 370, 341, 307, 264, 644, 466, 38755, 38253, 1228, 35521, 17720, 13, 51446], "temperature": 0.0, "avg_logprob": -0.23388884617732122, "compression_ratio": 1.55, "no_speech_prob": 0.003564183134585619}, {"id": 442, "seek": 232704, "start": 2348.68, "end": 2356.6, "text": " And I guess to summarize, I can say that the interesting part of the results I just showed", "tokens": [51446, 400, 286, 2041, 281, 20858, 11, 286, 393, 584, 300, 264, 1880, 644, 295, 264, 3542, 286, 445, 4712, 51842], "temperature": 0.0, "avg_logprob": -0.23388884617732122, "compression_ratio": 1.55, "no_speech_prob": 0.003564183134585619}, {"id": 443, "seek": 235660, "start": 2357.24, "end": 2362.52, "text": " is that I showed that predictive coding is able to perform interventions in a very easy", "tokens": [50396, 307, 300, 286, 4712, 300, 35521, 17720, 307, 1075, 281, 2042, 20924, 294, 257, 588, 1858, 50660], "temperature": 0.0, "avg_logprob": -0.2310166358947754, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0050433240830898285}, {"id": 444, "seek": 235660, "start": 2362.52, "end": 2366.7599999999998, "text": " and intuitive way because you don't have to act on the structure of the old graph anymore.", "tokens": [50660, 293, 21769, 636, 570, 291, 500, 380, 362, 281, 605, 322, 264, 3877, 295, 264, 1331, 4295, 3602, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2310166358947754, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0050433240830898285}, {"id": 445, "seek": 235660, "start": 2366.7599999999998, "end": 2371.12, "text": " Sometimes those functions are not available, so forth and so on.", "tokens": [50872, 4803, 729, 6828, 366, 406, 2435, 11, 370, 5220, 293, 370, 322, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2310166358947754, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0050433240830898285}, {"id": 446, "seek": 235660, "start": 2371.12, "end": 2381.88, "text": " But you simply have to intervene on a single neuron, set its prediction error to zero and", "tokens": [51090, 583, 291, 2935, 362, 281, 30407, 322, 257, 2167, 34090, 11, 992, 1080, 17630, 6713, 281, 4018, 293, 51628], "temperature": 0.0, "avg_logprob": -0.2310166358947754, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0050433240830898285}, {"id": 447, "seek": 235660, "start": 2381.88, "end": 2384.68, "text": " perform an energy minimization process.", "tokens": [51628, 2042, 364, 2281, 4464, 2144, 1399, 13, 51768], "temperature": 0.0, "avg_logprob": -0.2310166358947754, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0050433240830898285}, {"id": 448, "seek": 238660, "start": 2386.7599999999998, "end": 2393.04, "text": " And these extended allowed us to define predictive coding based structural causal models.", "tokens": [50372, 400, 613, 10913, 4350, 505, 281, 6964, 35521, 17720, 2361, 15067, 38755, 5245, 13, 50686], "temperature": 0.0, "avg_logprob": -0.23162012389211944, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.0029572544153779745}, {"id": 449, "seek": 238660, "start": 2393.04, "end": 2399.68, "text": " Now we move to the second part of the work, which is about structure learning.", "tokens": [50686, 823, 321, 1286, 281, 264, 1150, 644, 295, 264, 589, 11, 597, 307, 466, 3877, 2539, 13, 51018], "temperature": 0.0, "avg_logprob": -0.23162012389211944, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.0029572544153779745}, {"id": 450, "seek": 238660, "start": 2402.48, "end": 2408.0, "text": " So structure learning, as I said, deals with the problem of learning the causal structure", "tokens": [51158, 407, 3877, 2539, 11, 382, 286, 848, 11, 11215, 365, 264, 1154, 295, 2539, 264, 38755, 3877, 51434], "temperature": 0.0, "avg_logprob": -0.23162012389211944, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.0029572544153779745}, {"id": 451, "seek": 238660, "start": 2408.0, "end": 2412.08, "text": " of the model from observational data.", "tokens": [51434, 295, 264, 2316, 490, 9951, 1478, 1412, 13, 51638], "temperature": 0.0, "avg_logprob": -0.23162012389211944, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.0029572544153779745}, {"id": 452, "seek": 241208, "start": 2412.08, "end": 2421.0, "text": " This is actually no problem that has been around for decades and has always been, until", "tokens": [50364, 639, 307, 767, 572, 1154, 300, 575, 668, 926, 337, 7878, 293, 575, 1009, 668, 11, 1826, 50810], "temperature": 0.0, "avg_logprob": -0.16003884338751073, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0020545644219964743}, {"id": 453, "seek": 241208, "start": 2421.0, "end": 2425.7999999999997, "text": " a couple of years ago, tackled using combinatorial search methods.", "tokens": [50810, 257, 1916, 295, 924, 2057, 11, 9426, 1493, 1228, 2512, 31927, 831, 3164, 7150, 13, 51050], "temperature": 0.0, "avg_logprob": -0.16003884338751073, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0020545644219964743}, {"id": 454, "seek": 241208, "start": 2425.7999999999997, "end": 2430.7999999999997, "text": " The problem with those combinatorial search methods is that their complexity grows double", "tokens": [51050, 440, 1154, 365, 729, 2512, 31927, 831, 3164, 7150, 307, 300, 641, 14024, 13156, 3834, 51300], "temperature": 0.0, "avg_logprob": -0.16003884338751073, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0020545644219964743}, {"id": 455, "seek": 241208, "start": 2430.7999999999997, "end": 2433.04, "text": " exponentially.", "tokens": [51300, 37330, 13, 51412], "temperature": 0.0, "avg_logprob": -0.16003884338751073, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0020545644219964743}, {"id": 456, "seek": 241208, "start": 2433.04, "end": 2440.2, "text": " So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn", "tokens": [51412, 407, 382, 2321, 382, 264, 1412, 3643, 2120, 327, 332, 11075, 293, 264, 7840, 42434, 4295, 300, 291, 528, 281, 1466, 51770], "temperature": 0.0, "avg_logprob": -0.16003884338751073, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0020545644219964743}, {"id": 457, "seek": 244020, "start": 2440.2, "end": 2447.0, "text": " grows in size, learning it, it's incredibly slow.", "tokens": [50364, 13156, 294, 2744, 11, 2539, 309, 11, 309, 311, 6252, 2964, 13, 50704], "temperature": 0.0, "avg_logprob": -0.22870671471884085, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0010683758882805705}, {"id": 458, "seek": 244020, "start": 2447.0, "end": 2454.64, "text": " The new solution that came out actually a couple of years ago in a newspaper from 2018", "tokens": [50704, 440, 777, 3827, 300, 1361, 484, 767, 257, 1916, 295, 924, 2057, 294, 257, 13669, 490, 6096, 51086], "temperature": 0.0, "avg_logprob": -0.22870671471884085, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0010683758882805705}, {"id": 459, "seek": 244020, "start": 2454.64, "end": 2458.24, "text": " showed that it's possible to actually learn this structure, not using a combinatorial", "tokens": [51086, 4712, 300, 309, 311, 1944, 281, 767, 1466, 341, 3877, 11, 406, 1228, 257, 2512, 31927, 831, 51266], "temperature": 0.0, "avg_logprob": -0.22870671471884085, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0010683758882805705}, {"id": 460, "seek": 244020, "start": 2458.24, "end": 2461.9199999999996, "text": " search method, but by using a gradient-based method.", "tokens": [51266, 3164, 3170, 11, 457, 538, 1228, 257, 16235, 12, 6032, 3170, 13, 51450], "temperature": 0.0, "avg_logprob": -0.22870671471884085, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0010683758882805705}, {"id": 461, "seek": 244020, "start": 2461.9199999999996, "end": 2469.3199999999997, "text": " And this was basically this killed the problem in general because now you can simply apply", "tokens": [51450, 400, 341, 390, 1936, 341, 4652, 264, 1154, 294, 2674, 570, 586, 291, 393, 2935, 3079, 51820], "temperature": 0.0, "avg_logprob": -0.22870671471884085, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0010683758882805705}, {"id": 462, "seek": 246932, "start": 2469.32, "end": 2473.44, "text": " your on the parameters, which is the prior proposed that I'm going to define a little", "tokens": [50364, 428, 322, 264, 9834, 11, 597, 307, 264, 4059, 10348, 300, 286, 478, 516, 281, 6964, 257, 707, 50570], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 463, "seek": 246932, "start": 2473.44, "end": 2477.6400000000003, "text": " bit better in this slide, around gradient descent.", "tokens": [50570, 857, 1101, 294, 341, 4137, 11, 926, 16235, 23475, 13, 50780], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 464, "seek": 246932, "start": 2477.6400000000003, "end": 2483.4, "text": " And even if you have a model that is double, triple the size, the algorithm is still incredibly", "tokens": [50780, 400, 754, 498, 291, 362, 257, 2316, 300, 307, 3834, 11, 15508, 264, 2744, 11, 264, 9284, 307, 920, 6252, 51068], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 465, "seek": 246932, "start": 2483.4, "end": 2485.8, "text": " fast.", "tokens": [51068, 2370, 13, 51188], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 466, "seek": 246932, "start": 2485.8, "end": 2491.8, "text": " And for this reason, this paper is, yeah, I think it's kind of new and I think already", "tokens": [51188, 400, 337, 341, 1778, 11, 341, 3035, 307, 11, 1338, 11, 286, 519, 309, 311, 733, 295, 777, 293, 286, 519, 1217, 51488], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 467, "seek": 246932, "start": 2491.8, "end": 2495.8, "text": " has around 600 citations or things like that.", "tokens": [51488, 575, 926, 11849, 4814, 763, 420, 721, 411, 300, 13, 51688], "temperature": 0.0, "avg_logprob": -0.240568737188975, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.029531726613640785}, {"id": 468, "seek": 249580, "start": 2495.8, "end": 2499.5600000000004, "text": " And every paper that I'm seeing now about causal inference and learning causal structure", "tokens": [50364, 400, 633, 3035, 300, 286, 478, 2577, 586, 466, 38755, 38253, 293, 2539, 38755, 3877, 50552], "temperature": 0.0, "avg_logprob": -0.17711438915946268, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.013167722150683403}, {"id": 469, "seek": 249580, "start": 2499.5600000000004, "end": 2502.32, "text": " of the graph uses their method.", "tokens": [50552, 295, 264, 4295, 4960, 641, 3170, 13, 50690], "temperature": 0.0, "avg_logprob": -0.17711438915946268, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.013167722150683403}, {"id": 470, "seek": 249580, "start": 2502.32, "end": 2508.5600000000004, "text": " It just changes a little bit, they find faster or slightly better inference methods, but", "tokens": [50690, 467, 445, 2962, 257, 707, 857, 11, 436, 915, 4663, 420, 4748, 1101, 38253, 7150, 11, 457, 51002], "temperature": 0.0, "avg_logprob": -0.17711438915946268, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.013167722150683403}, {"id": 471, "seek": 249580, "start": 2508.5600000000004, "end": 2516.76, "text": " still they all use the prior, this paper defined, and I do as well, and we do as well.", "tokens": [51002, 920, 436, 439, 764, 264, 4059, 11, 341, 3035, 7642, 11, 293, 286, 360, 382, 731, 11, 293, 321, 360, 382, 731, 13, 51412], "temperature": 0.0, "avg_logprob": -0.17711438915946268, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.013167722150683403}, {"id": 472, "seek": 249580, "start": 2516.76, "end": 2521.0800000000004, "text": " So here we define a new quantity, which is the agency matrix.", "tokens": [51412, 407, 510, 321, 6964, 257, 777, 11275, 11, 597, 307, 264, 7934, 8141, 13, 51628], "temperature": 0.0, "avg_logprob": -0.17711438915946268, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.013167722150683403}, {"id": 473, "seek": 252108, "start": 2521.08, "end": 2525.7999999999997, "text": " The agency matrix is simply a matrix that encodes the connections of the model.", "tokens": [50364, 440, 7934, 8141, 307, 2935, 257, 8141, 300, 2058, 4789, 264, 9271, 295, 264, 2316, 13, 50600], "temperature": 0.0, "avg_logprob": -0.17530551137803477, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.007568646688014269}, {"id": 474, "seek": 252108, "start": 2525.7999999999997, "end": 2530.56, "text": " So it's a binary matrix, and in general, it's a binary matrix.", "tokens": [50600, 407, 309, 311, 257, 17434, 8141, 11, 293, 294, 2674, 11, 309, 311, 257, 17434, 8141, 13, 50838], "temperature": 0.0, "avg_logprob": -0.17530551137803477, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.007568646688014269}, {"id": 475, "seek": 252108, "start": 2530.56, "end": 2535.44, "text": " Then of course, when you do gradient-based optimization, you make it continuous and then", "tokens": [50838, 1396, 295, 1164, 11, 562, 291, 360, 16235, 12, 6032, 19618, 11, 291, 652, 309, 10957, 293, 550, 51082], "temperature": 0.0, "avg_logprob": -0.17530551137803477, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.007568646688014269}, {"id": 476, "seek": 252108, "start": 2535.44, "end": 2541.96, "text": " you have some threshold at some point that basically kills an edge or set it to one.", "tokens": [51082, 291, 362, 512, 14678, 412, 512, 935, 300, 1936, 14563, 364, 4691, 420, 992, 309, 281, 472, 13, 51408], "temperature": 0.0, "avg_logprob": -0.17530551137803477, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.007568646688014269}, {"id": 477, "seek": 254196, "start": 2542.28, "end": 2554.36, "text": " The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j", "tokens": [50380, 440, 8729, 741, 73, 307, 2681, 281, 472, 498, 264, 7840, 42434, 4295, 575, 364, 4691, 490, 28162, 741, 281, 28162, 361, 50984], "temperature": 0.0, "avg_logprob": -0.21066904739594797, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0967634990811348}, {"id": 478, "seek": 254196, "start": 2554.36, "end": 2555.76, "text": " or zero otherwise.", "tokens": [50984, 420, 4018, 5911, 13, 51054], "temperature": 0.0, "avg_logprob": -0.21066904739594797, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0967634990811348}, {"id": 479, "seek": 254196, "start": 2555.76, "end": 2560.4, "text": " So for example, this agency matrix here represents the connectivity structure of this Bayesian", "tokens": [51054, 407, 337, 1365, 11, 341, 7934, 8141, 510, 8855, 264, 21095, 3877, 295, 341, 7840, 42434, 51286], "temperature": 0.0, "avg_logprob": -0.21066904739594797, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0967634990811348}, {"id": 480, "seek": 254196, "start": 2560.4, "end": 2563.16, "text": " network.", "tokens": [51286, 3209, 13, 51424], "temperature": 0.0, "avg_logprob": -0.21066904739594797, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0967634990811348}, {"id": 481, "seek": 254196, "start": 2563.16, "end": 2571.52, "text": " And basically this method tackles two problems that we want about learning the structure", "tokens": [51424, 400, 1936, 341, 3170, 9426, 904, 732, 2740, 300, 321, 528, 466, 2539, 264, 3877, 51842], "temperature": 0.0, "avg_logprob": -0.21066904739594797, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0967634990811348}, {"id": 482, "seek": 257152, "start": 2571.56, "end": 2573.36, "text": " of the Bayesian network.", "tokens": [50366, 295, 264, 7840, 42434, 3209, 13, 50456], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 483, "seek": 257152, "start": 2573.36, "end": 2580.2, "text": " The idea is that we start from a fully connected model, which conceptually is similar, actually", "tokens": [50456, 440, 1558, 307, 300, 321, 722, 490, 257, 4498, 4582, 2316, 11, 597, 3410, 671, 307, 2531, 11, 767, 50798], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 484, "seek": 257152, "start": 2580.2, "end": 2584.68, "text": " is equivalent to the predictive coding network I defined earlier, which is fully connected.", "tokens": [50798, 307, 10344, 281, 264, 35521, 17720, 3209, 286, 7642, 3071, 11, 597, 307, 4498, 4582, 13, 51022], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 485, "seek": 257152, "start": 2584.68, "end": 2590.48, "text": " So you have a lot of vertices and every pair of vertices is connected by two different", "tokens": [51022, 407, 291, 362, 257, 688, 295, 32053, 293, 633, 6119, 295, 32053, 307, 4582, 538, 732, 819, 51312], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 486, "seek": 257152, "start": 2590.48, "end": 2596.08, "text": " edges, and you simply want to prune the ones that are not needed.", "tokens": [51312, 8819, 11, 293, 291, 2935, 528, 281, 582, 2613, 264, 2306, 300, 366, 406, 2978, 13, 51592], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 487, "seek": 257152, "start": 2596.08, "end": 2600.32, "text": " So it can be seen as a method that performs model reduction.", "tokens": [51592, 407, 309, 393, 312, 1612, 382, 257, 3170, 300, 26213, 2316, 11004, 13, 51804], "temperature": 0.0, "avg_logprob": -0.15165724612698697, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.014965534210205078}, {"id": 488, "seek": 260032, "start": 2600.32, "end": 2603.0800000000004, "text": " You start from a big model and you want to make it small.", "tokens": [50364, 509, 722, 490, 257, 955, 2316, 293, 291, 528, 281, 652, 309, 1359, 13, 50502], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 489, "seek": 260032, "start": 2603.0800000000004, "end": 2606.6400000000003, "text": " So what's the first ingredient to reduce models?", "tokens": [50502, 407, 437, 311, 264, 700, 14751, 281, 5407, 5245, 30, 50680], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 490, "seek": 260032, "start": 2606.6400000000003, "end": 2609.7200000000003, "text": " Well, it's of course sparse city.", "tokens": [50680, 1042, 11, 309, 311, 295, 1164, 637, 11668, 2307, 13, 50834], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 491, "seek": 260032, "start": 2609.7200000000003, "end": 2615.4, "text": " And what's the prior that everyone uses to make a model more sparse is the Laplace prior,", "tokens": [50834, 400, 437, 311, 264, 4059, 300, 1518, 4960, 281, 652, 257, 2316, 544, 637, 11668, 307, 264, 2369, 6742, 4059, 11, 51118], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 492, "seek": 260032, "start": 2615.4, "end": 2621.2400000000002, "text": " which in machine learning is simply known as the L1 norm, which is defined here.", "tokens": [51118, 597, 294, 3479, 2539, 307, 2935, 2570, 382, 264, 441, 16, 2026, 11, 597, 307, 7642, 510, 13, 51410], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 493, "seek": 260032, "start": 2621.2400000000002, "end": 2627.1200000000003, "text": " The solution that this paper that I mentioned earlier proposed is to add a second prior", "tokens": [51410, 440, 3827, 300, 341, 3035, 300, 286, 2835, 3071, 10348, 307, 281, 909, 257, 1150, 4059, 51704], "temperature": 0.0, "avg_logprob": -0.18697077556721214, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.04059332609176636}, {"id": 494, "seek": 262712, "start": 2627.12, "end": 2636.44, "text": " on top, which enforces what's probably the biggest characteristic of Bayesian networks", "tokens": [50364, 322, 1192, 11, 597, 25495, 887, 437, 311, 1391, 264, 3880, 16282, 295, 7840, 42434, 9590, 50830], "temperature": 0.0, "avg_logprob": -0.18302355448404947, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.025201603770256042}, {"id": 495, "seek": 262712, "start": 2636.44, "end": 2640.44, "text": " on which you want to perform causal inference, is that you want them to be acyclic.", "tokens": [50830, 322, 597, 291, 528, 281, 2042, 38755, 38253, 11, 307, 300, 291, 528, 552, 281, 312, 696, 88, 66, 1050, 13, 51030], "temperature": 0.0, "avg_logprob": -0.18302355448404947, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.025201603770256042}, {"id": 496, "seek": 262712, "start": 2640.44, "end": 2648.44, "text": " And basically they show that acyclicity can be imposed on an agency matrix as a prior,", "tokens": [51030, 400, 1936, 436, 855, 300, 696, 88, 66, 1050, 507, 393, 312, 26491, 322, 364, 7934, 8141, 382, 257, 4059, 11, 51430], "temperature": 0.0, "avg_logprob": -0.18302355448404947, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.025201603770256042}, {"id": 497, "seek": 262712, "start": 2648.44, "end": 2650.72, "text": " and it has this shape here.", "tokens": [51430, 293, 309, 575, 341, 3909, 510, 13, 51544], "temperature": 0.0, "avg_logprob": -0.18302355448404947, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.025201603770256042}, {"id": 498, "seek": 265072, "start": 2650.72, "end": 2659.2, "text": " So it's the trace of the matrix that is the exponential of A times A, where A is the", "tokens": [50364, 407, 309, 311, 264, 13508, 295, 264, 8141, 300, 307, 264, 21510, 295, 316, 1413, 316, 11, 689, 316, 307, 264, 50788], "temperature": 0.0, "avg_logprob": -0.15597181022167206, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.003352220170199871}, {"id": 499, "seek": 265072, "start": 2659.2, "end": 2661.2799999999997, "text": " agency matrix again.", "tokens": [50788, 7934, 8141, 797, 13, 50892], "temperature": 0.0, "avg_logprob": -0.15597181022167206, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.003352220170199871}, {"id": 500, "seek": 265072, "start": 2661.2799999999997, "end": 2668.8399999999997, "text": " And basically this quantity here is equal to zero if and only if the Bayesian network", "tokens": [50892, 400, 1936, 341, 11275, 510, 307, 2681, 281, 4018, 498, 293, 787, 498, 264, 7840, 42434, 3209, 51270], "temperature": 0.0, "avg_logprob": -0.15597181022167206, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.003352220170199871}, {"id": 501, "seek": 265072, "start": 2668.8399999999997, "end": 2677.9599999999996, "text": " or whatever graph you're considering is acyclic.", "tokens": [51270, 420, 2035, 4295, 291, 434, 8079, 307, 696, 88, 66, 1050, 13, 51726], "temperature": 0.0, "avg_logprob": -0.15597181022167206, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.003352220170199871}, {"id": 502, "seek": 267796, "start": 2677.96, "end": 2686.52, "text": " So I'm going to use these in some experiments, so force those two priors on different kinds", "tokens": [50364, 407, 286, 478, 516, 281, 764, 613, 294, 512, 12050, 11, 370, 3464, 729, 732, 1790, 830, 322, 819, 3685, 50792], "temperature": 0.0, "avg_logprob": -0.14445327049077944, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.02567630633711815}, {"id": 503, "seek": 267796, "start": 2686.52, "end": 2691.28, "text": " of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier", "tokens": [50792, 295, 7840, 42434, 9590, 11, 293, 286, 478, 1382, 281, 22183, 552, 365, 264, 7512, 321, 10348, 3071, 51030], "temperature": 0.0, "avg_logprob": -0.14445327049077944, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.02567630633711815}, {"id": 504, "seek": 267796, "start": 2691.28, "end": 2695.36, "text": " about performing causal inference via predictive coding.", "tokens": [51030, 466, 10205, 38755, 38253, 5766, 35521, 17720, 13, 51234], "temperature": 0.0, "avg_logprob": -0.14445327049077944, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.02567630633711815}, {"id": 505, "seek": 267796, "start": 2695.36, "end": 2697.2400000000002, "text": " So I'm going to present two different experiments.", "tokens": [51234, 407, 286, 478, 516, 281, 1974, 732, 819, 12050, 13, 51328], "temperature": 0.0, "avg_logprob": -0.14445327049077944, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.02567630633711815}, {"id": 506, "seek": 267796, "start": 2697.2400000000002, "end": 2703.56, "text": " So one is a proof of concept, which is the standard experiments showed in all the structural", "tokens": [51328, 407, 472, 307, 257, 8177, 295, 3410, 11, 597, 307, 264, 3832, 12050, 4712, 294, 439, 264, 15067, 51644], "temperature": 0.0, "avg_logprob": -0.14445327049077944, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.02567630633711815}, {"id": 507, "seek": 270356, "start": 2703.56, "end": 2709.2, "text": " learning tasks, which is the inference of the correct Bayesian network from data.", "tokens": [50364, 2539, 9608, 11, 597, 307, 264, 38253, 295, 264, 3006, 7840, 42434, 3209, 490, 1412, 13, 50646], "temperature": 0.0, "avg_logprob": -0.14555027905632467, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.020600643008947372}, {"id": 508, "seek": 270356, "start": 2709.2, "end": 2716.32, "text": " And then I'm going to build on top of the classification experiments I showed earlier,", "tokens": [50646, 400, 550, 286, 478, 516, 281, 1322, 322, 1192, 295, 264, 21538, 12050, 286, 4712, 3071, 11, 51002], "temperature": 0.0, "avg_logprob": -0.14555027905632467, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.020600643008947372}, {"id": 509, "seek": 270356, "start": 2716.32, "end": 2722.2, "text": " and show how actually those priors allow us to improve the classification accuracy, the", "tokens": [51002, 293, 855, 577, 767, 729, 1790, 830, 2089, 505, 281, 3470, 264, 21538, 14170, 11, 264, 51296], "temperature": 0.0, "avg_logprob": -0.14555027905632467, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.020600643008947372}, {"id": 510, "seek": 270356, "start": 2722.2, "end": 2729.72, "text": " test accuracy of fully connected predictive coding models.", "tokens": [51296, 1500, 14170, 295, 4498, 4582, 35521, 17720, 5245, 13, 51672], "temperature": 0.0, "avg_logprob": -0.14555027905632467, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.020600643008947372}, {"id": 511, "seek": 272972, "start": 2729.72, "end": 2735.3999999999996, "text": " So let's move to the first experiment, which is to infer the structure of the graph.", "tokens": [50364, 407, 718, 311, 1286, 281, 264, 700, 5120, 11, 597, 307, 281, 13596, 264, 3877, 295, 264, 4295, 13, 50648], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 512, "seek": 272972, "start": 2735.3999999999996, "end": 2739.8799999999997, "text": " And the experiments, they all follow basically the same pipeline in all the papers in the", "tokens": [50648, 400, 264, 12050, 11, 436, 439, 1524, 1936, 264, 912, 15517, 294, 439, 264, 10577, 294, 264, 50872], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 513, "seek": 272972, "start": 2739.8799999999997, "end": 2740.8799999999997, "text": " field.", "tokens": [50872, 2519, 13, 50922], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 514, "seek": 272972, "start": 2740.8799999999997, "end": 2746.3199999999997, "text": " The first step is to generate a Bayesian network from random graph.", "tokens": [50922, 440, 700, 1823, 307, 281, 8460, 257, 7840, 42434, 3209, 490, 4974, 4295, 13, 51194], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 515, "seek": 272972, "start": 2746.3199999999997, "end": 2751.56, "text": " So basically normally the two random graphs that everyone tests are Erdos-Renis graphs", "tokens": [51194, 407, 1936, 5646, 264, 732, 4974, 24877, 300, 1518, 6921, 366, 3300, 33749, 12, 49, 268, 271, 24877, 51456], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 516, "seek": 272972, "start": 2751.56, "end": 2753.68, "text": " and scale-free graphs.", "tokens": [51456, 293, 4373, 12, 10792, 24877, 13, 51562], "temperature": 0.0, "avg_logprob": -0.18303534189860027, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.0557122565805912}, {"id": 517, "seek": 275368, "start": 2753.68, "end": 2760.56, "text": " So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some", "tokens": [50364, 407, 291, 8460, 729, 955, 24877, 300, 5646, 362, 945, 11, 3356, 11, 4688, 11, 4688, 819, 13891, 293, 512, 50708], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 518, "seek": 275368, "start": 2760.56, "end": 2764.9199999999996, "text": " edges that you sample randomly.", "tokens": [50708, 8819, 300, 291, 6889, 16979, 13, 50926], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 519, "seek": 275368, "start": 2764.9199999999996, "end": 2768.6, "text": " And you use this graph to generate a data set.", "tokens": [50926, 400, 291, 764, 341, 4295, 281, 8460, 257, 1412, 992, 13, 51110], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 520, "seek": 275368, "start": 2768.6, "end": 2773.8399999999997, "text": " So you sample, for example, N, big N data points.", "tokens": [51110, 407, 291, 6889, 11, 337, 1365, 11, 426, 11, 955, 426, 1412, 2793, 13, 51372], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 521, "seek": 275368, "start": 2773.8399999999997, "end": 2778.48, "text": " And what you do is that you take the graph that you have generated earlier and you throw", "tokens": [51372, 400, 437, 291, 360, 307, 300, 291, 747, 264, 4295, 300, 291, 362, 10833, 3071, 293, 291, 3507, 51604], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 522, "seek": 275368, "start": 2778.48, "end": 2779.48, "text": " it away.", "tokens": [51604, 309, 1314, 13, 51654], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 523, "seek": 275368, "start": 2779.48, "end": 2781.16, "text": " You only keep the data set.", "tokens": [51654, 509, 787, 1066, 264, 1412, 992, 13, 51738], "temperature": 0.0, "avg_logprob": -0.15099361821224816, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.004970564041286707}, {"id": 524, "seek": 278116, "start": 2781.16, "end": 2787.92, "text": " And the task you want to solve now is to have a training algorithm that basically allows", "tokens": [50364, 400, 264, 5633, 291, 528, 281, 5039, 586, 307, 281, 362, 257, 3097, 9284, 300, 1936, 4045, 50702], "temperature": 0.0, "avg_logprob": -0.1708184465949918, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005726928822696209}, {"id": 525, "seek": 278116, "start": 2787.92, "end": 2794.92, "text": " you to retrieve the structure of the graph you have thrown away.", "tokens": [50702, 291, 281, 30254, 264, 3877, 295, 264, 4295, 291, 362, 11732, 1314, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1708184465949918, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005726928822696209}, {"id": 526, "seek": 278116, "start": 2794.92, "end": 2799.04, "text": " So the way we do it here is that we train a fully connected predictive coding model on", "tokens": [51052, 407, 264, 636, 321, 360, 309, 510, 307, 300, 321, 3847, 257, 4498, 4582, 35521, 17720, 2316, 322, 51258], "temperature": 0.0, "avg_logprob": -0.1708184465949918, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005726928822696209}, {"id": 527, "seek": 278116, "start": 2799.04, "end": 2806.3199999999997, "text": " this data set D, using both the sparse and the acyclic priors we have defined earlier.", "tokens": [51258, 341, 1412, 992, 413, 11, 1228, 1293, 264, 637, 11668, 293, 264, 696, 88, 66, 1050, 1790, 830, 321, 362, 7642, 3071, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1708184465949918, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005726928822696209}, {"id": 528, "seek": 280632, "start": 2806.32, "end": 2813.92, "text": " You can see whether actually the graph that we converge to, after pruning away the entries", "tokens": [50364, 509, 393, 536, 1968, 767, 264, 4295, 300, 321, 41881, 281, 11, 934, 582, 37726, 1314, 264, 23041, 50744], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 529, "seek": 280632, "start": 2813.92, "end": 2819.6400000000003, "text": " of the agency matrix that are smaller than a certain threshold, is similar to that of", "tokens": [50744, 295, 264, 7934, 8141, 300, 366, 4356, 813, 257, 1629, 14678, 11, 307, 2531, 281, 300, 295, 51030], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 530, "seek": 280632, "start": 2819.6400000000003, "end": 2822.84, "text": " the initial graph.", "tokens": [51030, 264, 5883, 4295, 13, 51190], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 531, "seek": 280632, "start": 2822.84, "end": 2825.56, "text": " And the results show that this is actually the case.", "tokens": [51190, 400, 264, 3542, 855, 300, 341, 307, 767, 264, 1389, 13, 51326], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 532, "seek": 280632, "start": 2825.56, "end": 2833.6000000000004, "text": " So this is an example and I show many different parametrizations and dimensions and things", "tokens": [51326, 407, 341, 307, 364, 1365, 293, 286, 855, 867, 819, 6220, 302, 24959, 763, 293, 12819, 293, 721, 51728], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 533, "seek": 280632, "start": 2833.6000000000004, "end": 2835.04, "text": " like that in the paper.", "tokens": [51728, 411, 300, 294, 264, 3035, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1950964710929177, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.1282740980386734}, {"id": 534, "seek": 283504, "start": 2835.64, "end": 2839.8, "text": " But I think those two are the most representative examples with an air nosher in a graph and", "tokens": [50394, 583, 286, 519, 729, 732, 366, 264, 881, 12424, 5110, 365, 364, 1988, 3269, 511, 294, 257, 4295, 293, 50602], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 535, "seek": 283504, "start": 2839.8, "end": 2843.92, "text": " a free scale graph with 20 nodes.", "tokens": [50602, 257, 1737, 4373, 4295, 365, 945, 13891, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 536, "seek": 283504, "start": 2843.92, "end": 2851.16, "text": " And here on the left, you can see the ground truth graph, which is the one sampled randomly.", "tokens": [50808, 400, 510, 322, 264, 1411, 11, 291, 393, 536, 264, 2727, 3494, 4295, 11, 597, 307, 264, 472, 3247, 15551, 16979, 13, 51170], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 537, "seek": 283504, "start": 2851.16, "end": 2856.24, "text": " And on the right, you can see the graph, the predictive coding model as learned from the", "tokens": [51170, 400, 322, 264, 558, 11, 291, 393, 536, 264, 4295, 11, 264, 35521, 17720, 2316, 382, 3264, 490, 264, 51424], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 538, "seek": 283504, "start": 2856.24, "end": 2857.24, "text": " data set.", "tokens": [51424, 1412, 992, 13, 51474], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 539, "seek": 283504, "start": 2857.24, "end": 2860.7599999999998, "text": " And as you can see, they are quite similar.", "tokens": [51474, 400, 382, 291, 393, 536, 11, 436, 366, 1596, 2531, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 540, "seek": 283504, "start": 2860.7599999999998, "end": 2862.64, "text": " It's still not perfect.", "tokens": [51650, 467, 311, 920, 406, 2176, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2677790018228384, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.015116458758711815}, {"id": 541, "seek": 286264, "start": 2862.64, "end": 2868.8399999999997, "text": " So there are some errors, but in general, the structures, they work quite well.", "tokens": [50364, 407, 456, 366, 512, 13603, 11, 457, 294, 2674, 11, 264, 9227, 11, 436, 589, 1596, 731, 13, 50674], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 542, "seek": 286264, "start": 2868.8399999999997, "end": 2874.24, "text": " We also have some quantitative experiments that I don't show here, because they're just", "tokens": [50674, 492, 611, 362, 512, 27778, 12050, 300, 286, 500, 380, 855, 510, 11, 570, 436, 434, 445, 50944], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 543, "seek": 286264, "start": 2874.24, "end": 2878.24, "text": " huge tables with a lot of numbers and I thought it was maybe a little bit too much for the", "tokens": [50944, 2603, 8020, 365, 257, 688, 295, 3547, 293, 286, 1194, 309, 390, 1310, 257, 707, 857, 886, 709, 337, 264, 51144], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 544, "seek": 286264, "start": 2878.24, "end": 2879.24, "text": " presentation.", "tokens": [51144, 5860, 13, 51194], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 545, "seek": 286264, "start": 2879.24, "end": 2886.4, "text": " But there is also that they perform similarly to contemporary methods.", "tokens": [51194, 583, 456, 307, 611, 300, 436, 2042, 14138, 281, 14878, 7150, 13, 51552], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 546, "seek": 286264, "start": 2886.4, "end": 2892.52, "text": " Also because I have to say most of the quality comes from the acyclic priors that was introduced", "tokens": [51552, 2743, 570, 286, 362, 281, 584, 881, 295, 264, 3125, 1487, 490, 264, 696, 88, 66, 1050, 1790, 830, 300, 390, 7268, 51858], "temperature": 0.0, "avg_logprob": -0.19314509800502233, "compression_ratio": 1.6058394160583942, "no_speech_prob": 0.0025178303476423025}, {"id": 547, "seek": 289252, "start": 2892.52, "end": 2893.52, "text": " in 2018.", "tokens": [50364, 294, 6096, 13, 50414], "temperature": 0.0, "avg_logprob": -0.21835538920234232, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.002714982256293297}, {"id": 548, "seek": 289252, "start": 2897.12, "end": 2902.7599999999998, "text": " The second class of experiments are classification experiments, which as I said, are the extensions", "tokens": [50594, 440, 1150, 1508, 295, 12050, 366, 21538, 12050, 11, 597, 382, 286, 848, 11, 366, 264, 25129, 50876], "temperature": 0.0, "avg_logprob": -0.21835538920234232, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.002714982256293297}, {"id": 549, "seek": 289252, "start": 2902.7599999999998, "end": 2905.88, "text": " of the one I shared earlier.", "tokens": [50876, 295, 264, 472, 286, 5507, 3071, 13, 51032], "temperature": 0.0, "avg_logprob": -0.21835538920234232, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.002714982256293297}, {"id": 550, "seek": 289252, "start": 2905.88, "end": 2911.0, "text": " And the idea is to use structure learning to improve the classification results on the", "tokens": [51032, 400, 264, 1558, 307, 281, 764, 3877, 2539, 281, 3470, 264, 21538, 3542, 322, 264, 51288], "temperature": 0.0, "avg_logprob": -0.21835538920234232, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.002714982256293297}, {"id": 551, "seek": 289252, "start": 2911.0, "end": 2916.96, "text": " means and fashion means data set, starting from a fully connected graph.", "tokens": [51288, 1355, 293, 6700, 1355, 1412, 992, 11, 2891, 490, 257, 4498, 4582, 4295, 13, 51586], "temperature": 0.0, "avg_logprob": -0.21835538920234232, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.002714982256293297}, {"id": 552, "seek": 291696, "start": 2916.96, "end": 2924.0, "text": " So what I did is that I divided the fully connected graph in clusters of neurons.", "tokens": [50364, 407, 437, 286, 630, 307, 300, 286, 6666, 264, 4498, 4582, 4295, 294, 23313, 295, 22027, 13, 50716], "temperature": 0.0, "avg_logprob": -0.15675133153011925, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.014307590201497078}, {"id": 553, "seek": 291696, "start": 2924.0, "end": 2929.44, "text": " So 1B cluster is the one related to the input.", "tokens": [50716, 407, 502, 33, 13630, 307, 264, 472, 4077, 281, 264, 4846, 13, 50988], "temperature": 0.0, "avg_logprob": -0.15675133153011925, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.014307590201497078}, {"id": 554, "seek": 291696, "start": 2929.44, "end": 2935.68, "text": " And then we have some specific number of hidden clusters.", "tokens": [50988, 400, 550, 321, 362, 512, 2685, 1230, 295, 7633, 23313, 13, 51300], "temperature": 0.0, "avg_logprob": -0.15675133153011925, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.014307590201497078}, {"id": 555, "seek": 291696, "start": 2935.68, "end": 2942.2, "text": " And then we have the label cluster, which is the cluster of neurons that are supposed", "tokens": [51300, 400, 550, 321, 362, 264, 7645, 13630, 11, 597, 307, 264, 13630, 295, 22027, 300, 366, 3442, 51626], "temperature": 0.0, "avg_logprob": -0.15675133153011925, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.014307590201497078}, {"id": 556, "seek": 291696, "start": 2942.2, "end": 2946.68, "text": " to give me the label predictions.", "tokens": [51626, 281, 976, 385, 264, 7645, 21264, 13, 51850], "temperature": 0.0, "avg_logprob": -0.15675133153011925, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.014307590201497078}, {"id": 557, "seek": 294668, "start": 2946.68, "end": 2950.3999999999996, "text": " And I've trained them using the first time, the sparse prior only.", "tokens": [50364, 400, 286, 600, 8895, 552, 1228, 264, 700, 565, 11, 264, 637, 11668, 4059, 787, 13, 50550], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 558, "seek": 294668, "start": 2950.3999999999996, "end": 2959.3199999999997, "text": " So the idea is, what if I prune the connections I don't need from a model and learn a sparser", "tokens": [50550, 407, 264, 1558, 307, 11, 437, 498, 286, 582, 2613, 264, 9271, 286, 500, 380, 643, 490, 257, 2316, 293, 1466, 257, 637, 685, 260, 50996], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 559, "seek": 294668, "start": 2959.3199999999997, "end": 2960.3199999999997, "text": " model?", "tokens": [50996, 2316, 30, 51046], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 560, "seek": 294668, "start": 2960.3199999999997, "end": 2961.7999999999997, "text": " Does this work?", "tokens": [51046, 4402, 341, 589, 30, 51120], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 561, "seek": 294668, "start": 2961.7999999999997, "end": 2963.48, "text": " Well, the answer is no.", "tokens": [51120, 1042, 11, 264, 1867, 307, 572, 13, 51204], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 562, "seek": 294668, "start": 2963.48, "end": 2964.68, "text": " It doesn't work.", "tokens": [51204, 467, 1177, 380, 589, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 563, "seek": 294668, "start": 2964.68, "end": 2971.8399999999997, "text": " And the reason why is that at the end, the graph that you converge with is actually degenerate.", "tokens": [51264, 400, 264, 1778, 983, 307, 300, 412, 264, 917, 11, 264, 4295, 300, 291, 41881, 365, 307, 767, 40520, 473, 13, 51622], "temperature": 0.0, "avg_logprob": -0.21351931492487589, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.011205573566257954}, {"id": 564, "seek": 297184, "start": 2971.84, "end": 2977.2000000000003, "text": " So basically, the model learns to predict the label based on the label itself.", "tokens": [50364, 407, 1936, 11, 264, 2316, 27152, 281, 6069, 264, 7645, 2361, 322, 264, 7645, 2564, 13, 50632], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 565, "seek": 297184, "start": 2977.2000000000003, "end": 2981.92, "text": " So it discards all the information from the input and only keeps the label.", "tokens": [50632, 407, 309, 2983, 2287, 439, 264, 1589, 490, 264, 4846, 293, 787, 5965, 264, 7645, 13, 50868], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 566, "seek": 297184, "start": 2981.92, "end": 2984.96, "text": " And as you can see here, the label y predicts itself.", "tokens": [50868, 400, 382, 291, 393, 536, 510, 11, 264, 7645, 288, 6069, 82, 2564, 13, 51020], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 567, "seek": 297184, "start": 2984.96, "end": 2989.2000000000003, "text": " Or in other experiments, when you change the parameters, you have that y predicts at zero,", "tokens": [51020, 1610, 294, 661, 12050, 11, 562, 291, 1319, 264, 9834, 11, 291, 362, 300, 288, 6069, 82, 412, 4018, 11, 51232], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 568, "seek": 297184, "start": 2989.2000000000003, "end": 2993.0, "text": " that predicts x1, that predicts y again.", "tokens": [51232, 300, 6069, 82, 2031, 16, 11, 300, 6069, 82, 288, 797, 13, 51422], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 569, "seek": 297184, "start": 2993.0, "end": 2996.36, "text": " So what's the solution to this problem?", "tokens": [51422, 407, 437, 311, 264, 3827, 281, 341, 1154, 30, 51590], "temperature": 0.0, "avg_logprob": -0.15549911838946956, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.03457724675536156}, {"id": 570, "seek": 299636, "start": 2996.36, "end": 3003.44, "text": " Well, the solution to this problem is that we have to converge to an acyclic graph.", "tokens": [50364, 1042, 11, 264, 3827, 281, 341, 1154, 307, 300, 321, 362, 281, 41881, 281, 364, 696, 88, 66, 1050, 4295, 13, 50718], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 571, "seek": 299636, "start": 3003.44, "end": 3007.0, "text": " And so we have to add something that prevents acyclicity.", "tokens": [50718, 400, 370, 321, 362, 281, 909, 746, 300, 22367, 696, 88, 66, 1050, 507, 13, 50896], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 572, "seek": 299636, "start": 3007.0, "end": 3008.92, "text": " And what is that?", "tokens": [50896, 400, 437, 307, 300, 30, 50992], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 573, "seek": 299636, "start": 3008.92, "end": 3010.84, "text": " One is, of course, the one I already proposed.", "tokens": [50992, 1485, 307, 11, 295, 1164, 11, 264, 472, 286, 1217, 10348, 13, 51088], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 574, "seek": 299636, "start": 3010.84, "end": 3014.84, "text": " And then I show a second technique.", "tokens": [51088, 400, 550, 286, 855, 257, 1150, 6532, 13, 51288], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 575, "seek": 299636, "start": 3014.84, "end": 3019.6, "text": " So the first one uses the acyclic prior defined earlier.", "tokens": [51288, 407, 264, 700, 472, 4960, 264, 696, 88, 66, 1050, 4059, 7642, 3071, 13, 51526], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 576, "seek": 299636, "start": 3019.6, "end": 3024.6, "text": " And the second one is a novel technique that actually makes use of negative examples.", "tokens": [51526, 400, 264, 1150, 472, 307, 257, 7613, 6532, 300, 767, 1669, 764, 295, 3671, 5110, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11055653117527471, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.03226657211780548}, {"id": 577, "seek": 302460, "start": 3024.6, "end": 3031.64, "text": " So a negative example in this case is simply a data point in which you have an image, but", "tokens": [50364, 407, 257, 3671, 1365, 294, 341, 1389, 307, 2935, 257, 1412, 935, 294, 597, 291, 362, 364, 3256, 11, 457, 50716], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 578, "seek": 302460, "start": 3031.64, "end": 3033.44, "text": " the label is wrong.", "tokens": [50716, 264, 7645, 307, 2085, 13, 50806], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 579, "seek": 302460, "start": 3033.44, "end": 3037.36, "text": " So here, for example, you have an image of a 7, but the label that I'm giving the model", "tokens": [50806, 407, 510, 11, 337, 1365, 11, 291, 362, 364, 3256, 295, 257, 1614, 11, 457, 264, 7645, 300, 286, 478, 2902, 264, 2316, 51002], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 580, "seek": 302460, "start": 3037.36, "end": 3041.36, "text": " is a 2.", "tokens": [51002, 307, 257, 568, 13, 51202], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 581, "seek": 302460, "start": 3041.36, "end": 3048.04, "text": " And the idea is very simple and has been used in a lot of works already.", "tokens": [51202, 400, 264, 1558, 307, 588, 2199, 293, 575, 668, 1143, 294, 257, 688, 295, 1985, 1217, 13, 51536], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 582, "seek": 302460, "start": 3048.04, "end": 3054.2799999999997, "text": " So every time the model sees a positive example, it has to minimize the variational free energy.", "tokens": [51536, 407, 633, 565, 264, 2316, 8194, 257, 3353, 1365, 11, 309, 575, 281, 17522, 264, 3034, 1478, 1737, 2281, 13, 51848], "temperature": 0.0, "avg_logprob": -0.15287510399679535, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.006588393822312355}, {"id": 583, "seek": 305428, "start": 3054.28, "end": 3059.1200000000003, "text": " And every time it sees a negative example, it has to increase it.", "tokens": [50364, 400, 633, 565, 309, 8194, 257, 3671, 1365, 11, 309, 575, 281, 3488, 309, 13, 50606], "temperature": 0.0, "avg_logprob": -0.16008858931692024, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.005126231350004673}, {"id": 584, "seek": 305428, "start": 3059.1200000000003, "end": 3065.6800000000003, "text": " So we will want this quantity to be minimized.", "tokens": [50606, 407, 321, 486, 528, 341, 11275, 281, 312, 4464, 1602, 13, 50934], "temperature": 0.0, "avg_logprob": -0.16008858931692024, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.005126231350004673}, {"id": 585, "seek": 305428, "start": 3065.6800000000003, "end": 3070.6800000000003, "text": " And actually, with a lot of experiments and a lot of experimentations, we saw that the", "tokens": [50934, 400, 767, 11, 365, 257, 688, 295, 12050, 293, 257, 688, 295, 5120, 763, 11, 321, 1866, 300, 264, 51184], "temperature": 0.0, "avg_logprob": -0.16008858931692024, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.005126231350004673}, {"id": 586, "seek": 305428, "start": 3070.6800000000003, "end": 3077.44, "text": " two techniques basically first lead to the same results and second lead to the same graph", "tokens": [51184, 732, 7512, 1936, 700, 1477, 281, 264, 912, 3542, 293, 1150, 1477, 281, 264, 912, 4295, 51522], "temperature": 0.0, "avg_logprob": -0.16008858931692024, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.005126231350004673}, {"id": 587, "seek": 305428, "start": 3077.44, "end": 3078.44, "text": " as well.", "tokens": [51522, 382, 731, 13, 51572], "temperature": 0.0, "avg_logprob": -0.16008858931692024, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.005126231350004673}, {"id": 588, "seek": 307844, "start": 3078.84, "end": 3086.48, "text": " So here are the new results on means and fashion means using the two techniques that I just", "tokens": [50384, 407, 510, 366, 264, 777, 3542, 322, 1355, 293, 6700, 1355, 1228, 264, 732, 7512, 300, 286, 445, 50766], "temperature": 0.0, "avg_logprob": -0.18880824299601764, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005411759950220585}, {"id": 589, "seek": 307844, "start": 3086.48, "end": 3088.64, "text": " proposed.", "tokens": [50766, 10348, 13, 50874], "temperature": 0.0, "avg_logprob": -0.18880824299601764, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005411759950220585}, {"id": 590, "seek": 307844, "start": 3088.64, "end": 3094.48, "text": " And now we move to some which are still not great, but definitely more reasonable test", "tokens": [50874, 400, 586, 321, 1286, 281, 512, 597, 366, 920, 406, 869, 11, 457, 2138, 544, 10585, 1500, 51166], "temperature": 0.0, "avg_logprob": -0.18880824299601764, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005411759950220585}, {"id": 591, "seek": 307844, "start": 3094.48, "end": 3095.56, "text": " accuracies.", "tokens": [51166, 5771, 20330, 13, 51220], "temperature": 0.0, "avg_logprob": -0.18880824299601764, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005411759950220585}, {"id": 592, "seek": 307844, "start": 3095.56, "end": 3102.56, "text": " So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means.", "tokens": [51220, 407, 510, 321, 362, 257, 1500, 6713, 295, 805, 13, 7773, 337, 1355, 293, 257, 1500, 6713, 295, 3705, 13, 22516, 337, 6700, 1355, 13, 51570], "temperature": 0.0, "avg_logprob": -0.18880824299601764, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005411759950220585}, {"id": 593, "seek": 310256, "start": 3102.68, "end": 3109.0, "text": " Actually, those results can be much improved by learning the structure of the graph on", "tokens": [50370, 5135, 11, 729, 3542, 393, 312, 709, 9689, 538, 2539, 264, 3877, 295, 264, 4295, 322, 50686], "temperature": 0.0, "avg_logprob": -0.15642859719016336, "compression_ratio": 1.8101265822784811, "no_speech_prob": 0.028663169592618942}, {"id": 594, "seek": 310256, "start": 3109.0, "end": 3115.72, "text": " means and then fixing the structure of the graph and do some form of fine tuning.", "tokens": [50686, 1355, 293, 550, 19442, 264, 3877, 295, 264, 4295, 293, 360, 512, 1254, 295, 2489, 15164, 13, 51022], "temperature": 0.0, "avg_logprob": -0.15642859719016336, "compression_ratio": 1.8101265822784811, "no_speech_prob": 0.028663169592618942}, {"id": 595, "seek": 310256, "start": 3115.72, "end": 3120.4, "text": " So if you fine tune the model on the correct hierarchical structure, at some point you", "tokens": [51022, 407, 498, 291, 2489, 10864, 264, 2316, 322, 264, 3006, 35250, 804, 3877, 11, 412, 512, 935, 291, 51256], "temperature": 0.0, "avg_logprob": -0.15642859719016336, "compression_ratio": 1.8101265822784811, "no_speech_prob": 0.028663169592618942}, {"id": 596, "seek": 310256, "start": 3120.4, "end": 3124.72, "text": " reach the test accuracy, which is the one you would expect from a hierarchical model.", "tokens": [51256, 2524, 264, 1500, 14170, 11, 597, 307, 264, 472, 291, 576, 2066, 490, 257, 35250, 804, 2316, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15642859719016336, "compression_ratio": 1.8101265822784811, "no_speech_prob": 0.028663169592618942}, {"id": 597, "seek": 310256, "start": 3124.72, "end": 3131.12, "text": " But those ones are simply the one, the fully connected model as naturally converged to.", "tokens": [51472, 583, 729, 2306, 366, 2935, 264, 472, 11, 264, 4498, 4582, 2316, 382, 8195, 9652, 3004, 281, 13, 51792], "temperature": 0.0, "avg_logprob": -0.15642859719016336, "compression_ratio": 1.8101265822784811, "no_speech_prob": 0.028663169592618942}, {"id": 598, "seek": 313112, "start": 3131.12, "end": 3137.4, "text": " So for example, from a test error of 18.32 of the fully connected model train on fashion", "tokens": [50364, 407, 337, 1365, 11, 490, 257, 1500, 6713, 295, 2443, 13, 11440, 295, 264, 4498, 4582, 2316, 3847, 322, 6700, 50678], "temperature": 0.0, "avg_logprob": -0.20673163344220416, "compression_ratio": 1.55, "no_speech_prob": 0.00187906832434237}, {"id": 599, "seek": 313112, "start": 3137.4, "end": 3143.56, "text": " means by simply performing correlations or conditional queries, which is the standard", "tokens": [50678, 1355, 538, 2935, 10205, 13983, 763, 420, 27708, 24109, 11, 597, 307, 264, 3832, 50986], "temperature": 0.0, "avg_logprob": -0.20673163344220416, "compression_ratio": 1.55, "no_speech_prob": 0.00187906832434237}, {"id": 600, "seek": 313112, "start": 3143.56, "end": 3150.4, "text": " way of querying operative coding model, adding interventions and the acyclic prior together", "tokens": [50986, 636, 295, 7083, 1840, 2208, 1166, 17720, 2316, 11, 5127, 20924, 293, 264, 696, 88, 66, 1050, 4059, 1214, 51328], "temperature": 0.0, "avg_logprob": -0.20673163344220416, "compression_ratio": 1.55, "no_speech_prob": 0.00187906832434237}, {"id": 601, "seek": 313112, "start": 3150.4, "end": 3154.52, "text": " makes this test error much lower.", "tokens": [51328, 1669, 341, 1500, 6713, 709, 3126, 13, 51534], "temperature": 0.0, "avg_logprob": -0.20673163344220416, "compression_ratio": 1.55, "no_speech_prob": 0.00187906832434237}, {"id": 602, "seek": 313112, "start": 3154.52, "end": 3158.12, "text": " And we can observe it for means as well.", "tokens": [51534, 400, 321, 393, 11441, 309, 337, 1355, 382, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20673163344220416, "compression_ratio": 1.55, "no_speech_prob": 0.00187906832434237}, {"id": 603, "seek": 315812, "start": 3159.12, "end": 3166.3599999999997, "text": " I'm now going a little bit into details on this last experiment and on how the acyclic", "tokens": [50414, 286, 478, 586, 516, 257, 707, 857, 666, 4365, 322, 341, 1036, 5120, 293, 322, 577, 264, 696, 88, 66, 1050, 50776], "temperature": 0.0, "avg_logprob": -0.27869887490874357, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.008235400542616844}, {"id": 604, "seek": 315812, "start": 3166.3599999999997, "end": 3169.7599999999998, "text": " prior acts on the structure of the graph.", "tokens": [50776, 4059, 10672, 322, 264, 3877, 295, 264, 4295, 13, 50946], "temperature": 0.0, "avg_logprob": -0.27869887490874357, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.008235400542616844}, {"id": 605, "seek": 315812, "start": 3169.7599999999998, "end": 3175.92, "text": " So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset,", "tokens": [50946, 407, 286, 2042, 364, 5120, 322, 257, 777, 28872, 11, 597, 307, 11, 286, 914, 11, 5141, 309, 257, 777, 28872, 11, 51254], "temperature": 0.0, "avg_logprob": -0.27869887490874357, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.008235400542616844}, {"id": 606, "seek": 315812, "start": 3175.92, "end": 3181.24, "text": " it may be too much, is the, I called it a two means dataset in which you have the input", "tokens": [51254, 309, 815, 312, 886, 709, 11, 307, 264, 11, 286, 1219, 309, 257, 732, 1355, 28872, 294, 597, 291, 362, 264, 4846, 51520], "temperature": 0.0, "avg_logprob": -0.27869887490874357, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.008235400542616844}, {"id": 607, "seek": 315812, "start": 3181.24, "end": 3187.96, "text": " point is formed of two different images and the label only depends on the second image.", "tokens": [51520, 935, 307, 8693, 295, 732, 819, 5267, 293, 264, 7645, 787, 5946, 322, 264, 1150, 3256, 13, 51856], "temperature": 0.0, "avg_logprob": -0.27869887490874357, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.008235400542616844}, {"id": 608, "seek": 318796, "start": 3188.68, "end": 3189.84, "text": " On the first image story.", "tokens": [50400, 1282, 264, 700, 3256, 1657, 13, 50458], "temperature": 0.0, "avg_logprob": -0.25391430854797364, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.0018758015939965844}, {"id": 609, "seek": 318796, "start": 3190.84, "end": 3197.04, "text": " So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and", "tokens": [50508, 407, 264, 1558, 510, 307, 11, 307, 264, 3877, 295, 264, 2316, 11, 264, 696, 88, 66, 1050, 11, 264, 696, 88, 66, 1050, 507, 4059, 293, 50818], "temperature": 0.0, "avg_logprob": -0.25391430854797364, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.0018758015939965844}, {"id": 610, "seek": 318796, "start": 3197.04, "end": 3202.64, "text": " things like that able to recognize that the second half of the image is actually meaningless", "tokens": [50818, 721, 411, 300, 1075, 281, 5521, 300, 264, 1150, 1922, 295, 264, 3256, 307, 767, 33232, 51098], "temperature": 0.0, "avg_logprob": -0.25391430854797364, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.0018758015939965844}, {"id": 611, "seek": 318796, "start": 3202.8, "end": 3209.7200000000003, "text": " in, in performing, in learning the in performing classification.", "tokens": [51106, 294, 11, 294, 10205, 11, 294, 2539, 264, 294, 10205, 21538, 13, 51452], "temperature": 0.0, "avg_logprob": -0.25391430854797364, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.0018758015939965844}, {"id": 612, "seek": 318796, "start": 3211.28, "end": 3212.76, "text": " How does training behave in general?", "tokens": [51530, 1012, 775, 3097, 15158, 294, 2674, 30, 51604], "temperature": 0.0, "avg_logprob": -0.25391430854797364, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.0018758015939965844}, {"id": 613, "seek": 321276, "start": 3212.76, "end": 3218.92, "text": " Like, for example, we have this input, input node, output node, and only the nodes are", "tokens": [50364, 1743, 11, 337, 1365, 11, 321, 362, 341, 4846, 11, 4846, 9984, 11, 5598, 9984, 11, 293, 787, 264, 13891, 366, 50672], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 614, "seek": 321276, "start": 3218.92, "end": 3226.0800000000004, "text": " fully connected and the model converge to a hierarchical structure, which is the one", "tokens": [50672, 4498, 4582, 293, 264, 2316, 41881, 281, 257, 35250, 804, 3877, 11, 597, 307, 264, 472, 51030], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 615, "seek": 321276, "start": 3226.4, "end": 3230.0400000000004, "text": " that we know performs the best on, on classification tasks.", "tokens": [51046, 300, 321, 458, 26213, 264, 1151, 322, 11, 322, 21538, 9608, 13, 51228], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 616, "seek": 321276, "start": 3231.0800000000004, "end": 3235.96, "text": " Well, here is a, is an example of a training method of a training run.", "tokens": [51280, 1042, 11, 510, 307, 257, 11, 307, 364, 1365, 295, 257, 3097, 3170, 295, 257, 3097, 1190, 13, 51524], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 617, "seek": 321276, "start": 3236.48, "end": 3239.6400000000003, "text": " So that's C zero, which is the beginning of training.", "tokens": [51550, 407, 300, 311, 383, 4018, 11, 597, 307, 264, 2863, 295, 3097, 13, 51708], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 618, "seek": 321276, "start": 3240.88, "end": 3241.96, "text": " We have this model here.", "tokens": [51770, 492, 362, 341, 2316, 510, 13, 51824], "temperature": 0.0, "avg_logprob": -0.21260177612304687, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.006237316410988569}, {"id": 619, "seek": 324196, "start": 3242.04, "end": 3247.92, "text": " So as zero corresponds to the, to the seven, so to the first image as one corresponds to", "tokens": [50368, 407, 382, 4018, 23249, 281, 264, 11, 281, 264, 3407, 11, 370, 281, 264, 700, 3256, 382, 472, 23249, 281, 50662], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 620, "seek": 324196, "start": 3247.92, "end": 3252.64, "text": " the second image, again, we have the label Y and all the latent variables X zero X one", "tokens": [50662, 264, 1150, 3256, 11, 797, 11, 321, 362, 264, 7645, 398, 293, 439, 264, 48994, 9102, 1783, 4018, 1783, 472, 50898], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 621, "seek": 324196, "start": 3252.64, "end": 3255.2400000000002, "text": " X two, and the model is fully connected.", "tokens": [50898, 1783, 732, 11, 293, 264, 2316, 307, 4498, 4582, 13, 51028], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 622, "seek": 324196, "start": 3255.28, "end": 3258.7200000000003, "text": " So the agency matrix is, is full of ones.", "tokens": [51030, 407, 264, 7934, 8141, 307, 11, 307, 1577, 295, 2306, 13, 51202], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 623, "seek": 324196, "start": 3259.0, "end": 3260.16, "text": " There are, there are no zeros.", "tokens": [51216, 821, 366, 11, 456, 366, 572, 35193, 13, 51274], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 624, "seek": 324196, "start": 3260.32, "end": 3262.04, "text": " We have self loops and things like that.", "tokens": [51282, 492, 362, 2698, 16121, 293, 721, 411, 300, 13, 51368], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 625, "seek": 324196, "start": 3263.64, "end": 3270.0, "text": " We train them at the model for a couple of epochs until, and what we note immediately is", "tokens": [51448, 492, 3847, 552, 412, 264, 2316, 337, 257, 1916, 295, 30992, 28346, 1826, 11, 293, 437, 321, 3637, 4258, 307, 51766], "temperature": 0.0, "avg_logprob": -0.20595610769171463, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0007507606060244143}, {"id": 626, "seek": 327000, "start": 3270.04, "end": 3274.68, "text": " that, for example, the, the model immediately understands that the four is not needed to", "tokens": [50366, 300, 11, 337, 1365, 11, 264, 11, 264, 2316, 4258, 15146, 300, 264, 1451, 307, 406, 2978, 281, 50598], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 627, "seek": 327000, "start": 3274.68, "end": 3275.76, "text": " perform classification.", "tokens": [50598, 2042, 21538, 13, 50652], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 628, "seek": 327000, "start": 3276.24, "end": 3276.84, "text": " So it doesn't.", "tokens": [50676, 407, 309, 1177, 380, 13, 50706], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 629, "seek": 327000, "start": 3277.88, "end": 3282.88, "text": " So every outgoing node from the, from the second input cluster is removed.", "tokens": [50758, 407, 633, 41565, 9984, 490, 264, 11, 490, 264, 1150, 4846, 13630, 307, 7261, 13, 51008], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 630, "seek": 327000, "start": 3284.16, "end": 3288.92, "text": " And something we didn't understand is that this is, this cluster is the one related to", "tokens": [51072, 400, 746, 321, 994, 380, 1223, 307, 300, 341, 307, 11, 341, 13630, 307, 264, 472, 4077, 281, 51310], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 631, "seek": 327000, "start": 3288.92, "end": 3289.48, "text": " the output.", "tokens": [51310, 264, 5598, 13, 51338], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 632, "seek": 327000, "start": 3290.56, "end": 3298.28, "text": " So we have a, we have a linear map from S zero to Y directly, which is this part here.", "tokens": [51392, 407, 321, 362, 257, 11, 321, 362, 257, 8213, 4471, 490, 318, 4018, 281, 398, 3838, 11, 597, 307, 341, 644, 510, 13, 51778], "temperature": 0.0, "avg_logprob": -0.16738043681229695, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0017437622882425785}, {"id": 633, "seek": 329828, "start": 3299.2400000000002, "end": 3305.7200000000003, "text": " But we know that actually a linear map is not the best map for, for performing classification", "tokens": [50412, 583, 321, 458, 300, 767, 257, 8213, 4471, 307, 406, 264, 1151, 4471, 337, 11, 337, 10205, 21538, 50736], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 634, "seek": 329828, "start": 3305.7200000000003, "end": 3306.2000000000003, "text": " on means.", "tokens": [50736, 322, 1355, 13, 50760], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 635, "seek": 329828, "start": 3306.44, "end": 3307.88, "text": " So we, we need some hierarchy.", "tokens": [50772, 407, 321, 11, 321, 643, 512, 22333, 13, 50844], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 636, "seek": 329828, "start": 3307.88, "end": 3310.52, "text": " We need some depth to, to improve the results.", "tokens": [50844, 492, 643, 512, 7161, 281, 11, 281, 3470, 264, 3542, 13, 50976], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 637, "seek": 329828, "start": 3310.92, "end": 3317.2400000000002, "text": " And as you can see, this line here is the, is the accuracy, which up to this point, so", "tokens": [50996, 400, 382, 291, 393, 536, 11, 341, 1622, 510, 307, 264, 11, 307, 264, 14170, 11, 597, 493, 281, 341, 935, 11, 370, 51312], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 638, "seek": 329828, "start": 3317.2400000000002, "end": 3323.6400000000003, "text": " up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear", "tokens": [51312, 493, 281, 383, 17, 307, 2531, 281, 257, 11, 370, 309, 311, 31064, 8923, 597, 307, 4748, 11, 4748, 1101, 813, 8213, 51632], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 639, "seek": 329828, "start": 3323.6400000000003, "end": 3324.44, "text": " classification.", "tokens": [51632, 21538, 13, 51672], "temperature": 0.0, "avg_logprob": -0.22970953265440117, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0007234051590785384}, {"id": 640, "seek": 332444, "start": 3325.4, "end": 3330.84, "text": " But once you go on with the training, the model understands that it needs some hierarchy", "tokens": [50412, 583, 1564, 291, 352, 322, 365, 264, 3097, 11, 264, 2316, 15146, 300, 309, 2203, 512, 22333, 50684], "temperature": 0.0, "avg_logprob": -0.19183523390028212, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.016553392633795738}, {"id": 641, "seek": 332444, "start": 3330.84, "end": 3332.0, "text": " to better fit the data.", "tokens": [50684, 281, 1101, 3318, 264, 1412, 13, 50742], "temperature": 0.0, "avg_logprob": -0.19183523390028212, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.016553392633795738}, {"id": 642, "seek": 332444, "start": 3332.84, "end": 3339.76, "text": " So you, you see that this arrow starts getting stronger and stronger over time until it, it", "tokens": [50784, 407, 291, 11, 291, 536, 300, 341, 11610, 3719, 1242, 7249, 293, 7249, 670, 565, 1826, 309, 11, 309, 51130], "temperature": 0.0, "avg_logprob": -0.19183523390028212, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.016553392633795738}, {"id": 643, "seek": 332444, "start": 3339.76, "end": 3344.48, "text": " understands that the linear map is not actually really needed and it removes it.", "tokens": [51130, 15146, 300, 264, 8213, 4471, 307, 406, 767, 534, 2978, 293, 309, 30445, 309, 13, 51366], "temperature": 0.0, "avg_logprob": -0.19183523390028212, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.016553392633795738}, {"id": 644, "seek": 332444, "start": 3345.4, "end": 3350.36, "text": " And so the, so the model you converge with is a model that starts from a zero, goes to", "tokens": [51412, 400, 370, 264, 11, 370, 264, 2316, 291, 41881, 365, 307, 257, 2316, 300, 3719, 490, 257, 4018, 11, 1709, 281, 51660], "temperature": 0.0, "avg_logprob": -0.19183523390028212, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.016553392633795738}, {"id": 645, "seek": 335036, "start": 3350.6800000000003, "end": 3358.04, "text": " a hidden node and then goes to the, to the label with a very weak linear map, which actually", "tokens": [50380, 257, 7633, 9984, 293, 550, 1709, 281, 264, 11, 281, 264, 7645, 365, 257, 588, 5336, 8213, 4471, 11, 597, 767, 50748], "temperature": 0.0, "avg_logprob": -0.19561403447931464, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.005971313454210758}, {"id": 646, "seek": 335036, "start": 3358.04, "end": 3363.48, "text": " gets removed if you, if you set that threshold of, if you set that threshold of, for example,", "tokens": [50748, 2170, 7261, 498, 291, 11, 498, 291, 992, 300, 14678, 295, 11, 498, 291, 992, 300, 14678, 295, 11, 337, 1365, 11, 51020], "temperature": 0.0, "avg_logprob": -0.19561403447931464, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.005971313454210758}, {"id": 647, "seek": 335036, "start": 3363.48, "end": 3366.92, "text": " 0.1, 0.2, at some point, the linear map gets forgotten.", "tokens": [51020, 1958, 13, 16, 11, 1958, 13, 17, 11, 412, 512, 935, 11, 264, 8213, 4471, 2170, 11832, 13, 51192], "temperature": 0.0, "avg_logprob": -0.19561403447931464, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.005971313454210758}, {"id": 648, "seek": 335036, "start": 3367.2400000000002, "end": 3371.8, "text": " And everything you end up with is with a, is with a hierarchical network.", "tokens": [51208, 400, 1203, 291, 917, 493, 365, 307, 365, 257, 11, 307, 365, 257, 35250, 804, 3209, 13, 51436], "temperature": 0.0, "avg_logprob": -0.19561403447931464, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.005971313454210758}, {"id": 649, "seek": 335036, "start": 3373.4, "end": 3378.1200000000003, "text": " That is, that is, so it has learned the correct structure to, to perform classification tasks,", "tokens": [51516, 663, 307, 11, 300, 307, 11, 370, 309, 575, 3264, 264, 3006, 3877, 281, 11, 281, 2042, 21538, 9608, 11, 51752], "temperature": 0.0, "avg_logprob": -0.19561403447931464, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.005971313454210758}, {"id": 650, "seek": 337812, "start": 3378.12, "end": 3379.08, "text": " which is a hierarchy.", "tokens": [50364, 597, 307, 257, 22333, 13, 50412], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 651, "seek": 337812, "start": 3379.48, "end": 3385.0, "text": " And it has also learned that the second image didn't play any role in defining the,", "tokens": [50432, 400, 309, 575, 611, 3264, 300, 264, 1150, 3256, 994, 380, 862, 604, 3090, 294, 17827, 264, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 652, "seek": 337812, "start": 3385.56, "end": 3386.8399999999997, "text": " the test accuracy.", "tokens": [50736, 264, 1500, 14170, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 653, "seek": 337812, "start": 3386.8399999999997, "end": 3389.48, "text": " And this is all, this is all performed.", "tokens": [50800, 400, 341, 307, 439, 11, 341, 307, 439, 10332, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 654, "seek": 337812, "start": 3389.48, "end": 3395.3199999999997, "text": " So all those jobs are simply performed by, performed by one free energy minimization", "tokens": [50932, 407, 439, 729, 4782, 366, 2935, 10332, 538, 11, 10332, 538, 472, 1737, 2281, 4464, 2144, 51224], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 655, "seek": 337812, "start": 3395.3199999999997, "end": 3396.12, "text": " process.", "tokens": [51224, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 656, "seek": 337812, "start": 3396.12, "end": 3400.04, "text": " So you initialize the model, you define the free energy, you define the priors.", "tokens": [51264, 407, 291, 5883, 1125, 264, 2316, 11, 291, 6964, 264, 1737, 2281, 11, 291, 6964, 264, 1790, 830, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 657, "seek": 337812, "start": 3400.04, "end": 3406.12, "text": " So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge", "tokens": [51460, 407, 264, 11, 264, 637, 11668, 293, 264, 38154, 1050, 4059, 11, 291, 1190, 264, 11, 264, 2281, 4464, 2144, 293, 291, 41881, 51764], "temperature": 0.0, "avg_logprob": -0.1258814521457838, "compression_ratio": 1.8744588744588744, "no_speech_prob": 0.0036418549716472626}, {"id": 658, "seek": 340612, "start": 3406.12, "end": 3410.7599999999998, "text": " to hierarchical, to a hierarchical model, which is well able to perform classification on minced.", "tokens": [50364, 281, 35250, 804, 11, 281, 257, 35250, 804, 2316, 11, 597, 307, 731, 1075, 281, 2042, 21538, 322, 36442, 13, 50596], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 659, "seek": 340612, "start": 3411.88, "end": 3416.7599999999998, "text": " And then if you then perform some fine tuning, you reach very competitive results as you do in", "tokens": [50652, 400, 550, 498, 291, 550, 2042, 512, 2489, 15164, 11, 291, 2524, 588, 10043, 3542, 382, 291, 360, 294, 50896], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 660, "seek": 340612, "start": 3416.7599999999998, "end": 3419.72, "text": " feed forward networks with the, with back propagation.", "tokens": [50896, 3154, 2128, 9590, 365, 264, 11, 365, 646, 38377, 13, 51044], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 661, "seek": 340612, "start": 3419.72, "end": 3421.72, "text": " But I think that's not the interesting bit.", "tokens": [51044, 583, 286, 519, 300, 311, 406, 264, 1880, 857, 13, 51144], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 662, "seek": 340612, "start": 3421.72, "end": 3425.4, "text": " The interesting bit is that you, like all this process, this process altogether", "tokens": [51144, 440, 1880, 857, 307, 300, 291, 11, 411, 439, 341, 1399, 11, 341, 1399, 19051, 51328], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 663, "seek": 340612, "start": 3426.04, "end": 3431.88, "text": " of intervention and the acyclicity allows you to take a fully connected network", "tokens": [51360, 295, 13176, 293, 264, 696, 88, 66, 1050, 507, 4045, 291, 281, 747, 257, 4498, 4582, 3209, 51652], "temperature": 0.0, "avg_logprob": -0.15006864638555617, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0026057015638798475}, {"id": 664, "seek": 343188, "start": 3432.6800000000003, "end": 3438.04, "text": " and converge to a hierarchical one that is, that is able to perform classification with good results.", "tokens": [50404, 293, 41881, 281, 257, 35250, 804, 472, 300, 307, 11, 300, 307, 1075, 281, 2042, 21538, 365, 665, 3542, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14580105260475396, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0042349896393716335}, {"id": 665, "seek": 343188, "start": 3440.84, "end": 3444.6, "text": " And yeah, that's basically it.", "tokens": [50812, 400, 1338, 11, 300, 311, 1936, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.14580105260475396, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0042349896393716335}, {"id": 666, "seek": 343188, "start": 3444.6, "end": 3447.1600000000003, "text": " I'm now, oh yeah, wow, I've talked a lot.", "tokens": [51000, 286, 478, 586, 11, 1954, 1338, 11, 6076, 11, 286, 600, 2825, 257, 688, 13, 51128], "temperature": 0.0, "avg_logprob": -0.14580105260475396, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0042349896393716335}, {"id": 667, "seek": 343188, "start": 3447.1600000000003, "end": 3454.6800000000003, "text": " And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary.", "tokens": [51128, 400, 286, 478, 11, 341, 307, 264, 10063, 295, 264, 751, 11, 597, 307, 11, 286, 478, 1936, 884, 257, 1359, 12691, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14580105260475396, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0042349896393716335}, {"id": 668, "seek": 343188, "start": 3454.6800000000003, "end": 3459.32, "text": " And I think the, the important takeaway if I have to give even one sentence of this paper", "tokens": [51504, 400, 286, 519, 264, 11, 264, 1021, 30681, 498, 286, 362, 281, 976, 754, 472, 8174, 295, 341, 3035, 51736], "temperature": 0.0, "avg_logprob": -0.14580105260475396, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0042349896393716335}, {"id": 669, "seek": 345932, "start": 3459.4, "end": 3465.56, "text": " is that predictive coding is a belief updating method that is able to perform end to end causal", "tokens": [50368, 307, 300, 35521, 17720, 307, 257, 7107, 25113, 3170, 300, 307, 1075, 281, 2042, 917, 281, 917, 38755, 50676], "temperature": 0.0, "avg_logprob": -0.16118734533136542, "compression_ratio": 1.9238095238095239, "no_speech_prob": 0.0037825964391231537}, {"id": 670, "seek": 345932, "start": 3465.56, "end": 3471.56, "text": " learning. So it's able to perform interventions to learn a structure from data and then perform", "tokens": [50676, 2539, 13, 407, 309, 311, 1075, 281, 2042, 20924, 281, 1466, 257, 3877, 490, 1412, 293, 550, 2042, 50976], "temperature": 0.0, "avg_logprob": -0.16118734533136542, "compression_ratio": 1.9238095238095239, "no_speech_prob": 0.0037825964391231537}, {"id": 671, "seek": 345932, "start": 3471.56, "end": 3479.0800000000004, "text": " interventions and counterfactuals. So causal inference in natural and efficiency model", "tokens": [50976, 20924, 293, 5682, 44919, 901, 82, 13, 407, 38755, 38253, 294, 3303, 293, 10493, 2316, 51352], "temperature": 0.0, "avg_logprob": -0.16118734533136542, "compression_ratio": 1.9238095238095239, "no_speech_prob": 0.0037825964391231537}, {"id": 672, "seek": 345932, "start": 3479.0800000000004, "end": 3481.88, "text": " interventions by simply setting the prediction error to zero.", "tokens": [51352, 20924, 538, 2935, 3287, 264, 17630, 6713, 281, 4018, 13, 51492], "temperature": 0.0, "avg_logprob": -0.16118734533136542, "compression_ratio": 1.9238095238095239, "no_speech_prob": 0.0037825964391231537}, {"id": 673, "seek": 345932, "start": 3481.88, "end": 3485.8, "text": " So it's a, it's a very easy technique to perform interventions.", "tokens": [51492, 407, 309, 311, 257, 11, 309, 311, 257, 588, 1858, 6532, 281, 2042, 20924, 13, 51688], "temperature": 0.0, "avg_logprob": -0.16118734533136542, "compression_ratio": 1.9238095238095239, "no_speech_prob": 0.0037825964391231537}, {"id": 674, "seek": 348580, "start": 3485.8, "end": 3489.48, "text": " And you simply only have to touch one neuron, you don't have to act on the structure of the graph.", "tokens": [50364, 400, 291, 2935, 787, 362, 281, 2557, 472, 34090, 11, 291, 500, 380, 362, 281, 605, 322, 264, 3877, 295, 264, 4295, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14787837437220983, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.003055211156606674}, {"id": 675, "seek": 348580, "start": 3490.92, "end": 3496.44, "text": " You can, you can use it to perform, to, to create structure causal models that are biologically", "tokens": [50620, 509, 393, 11, 291, 393, 764, 309, 281, 2042, 11, 281, 11, 281, 1884, 3877, 38755, 5245, 300, 366, 3228, 17157, 50896], "temperature": 0.0, "avg_logprob": -0.14787837437220983, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.003055211156606674}, {"id": 676, "seek": 348580, "start": 3496.44, "end": 3504.44, "text": " plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already.", "tokens": [50896, 39925, 13, 467, 307, 1075, 281, 1466, 264, 3877, 337, 11, 490, 1412, 11, 382, 286, 848, 11, 1310, 257, 688, 295, 1413, 1217, 13, 51296], "temperature": 0.0, "avg_logprob": -0.14787837437220983, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.003055211156606674}, {"id": 677, "seek": 348580, "start": 3505.4, "end": 3509.48, "text": " And, and a couple of sentences about future works is that", "tokens": [51344, 400, 11, 293, 257, 1916, 295, 16579, 466, 2027, 1985, 307, 300, 51548], "temperature": 0.0, "avg_logprob": -0.14787837437220983, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.003055211156606674}, {"id": 678, "seek": 350948, "start": 3510.2, "end": 3515.08, "text": " something that would be nice to do is to improve the performance of the model we,", "tokens": [50400, 746, 300, 576, 312, 1481, 281, 360, 307, 281, 3470, 264, 3389, 295, 264, 2316, 321, 11, 50644], "temperature": 0.0, "avg_logprob": -0.22689487315990306, "compression_ratio": 1.7808764940239044, "no_speech_prob": 0.005038489121943712}, {"id": 679, "seek": 350948, "start": 3515.08, "end": 3520.52, "text": " we have defined, because I think it performs reasonably well on a lot of tasks.", "tokens": [50644, 321, 362, 7642, 11, 570, 286, 519, 309, 26213, 23551, 731, 322, 257, 688, 295, 9608, 13, 50916], "temperature": 0.0, "avg_logprob": -0.22689487315990306, "compression_ratio": 1.7808764940239044, "no_speech_prob": 0.005038489121943712}, {"id": 680, "seek": 350948, "start": 3520.52, "end": 3525.88, "text": " So it performs reasonably well on structure learning on, for me, intervention and counterfactuals.", "tokens": [50916, 407, 309, 26213, 23551, 731, 322, 3877, 2539, 322, 11, 337, 385, 11, 13176, 293, 5682, 44919, 901, 82, 13, 51184], "temperature": 0.0, "avg_logprob": -0.22689487315990306, "compression_ratio": 1.7808764940239044, "no_speech_prob": 0.005038489121943712}, {"id": 681, "seek": 350948, "start": 3527.08, "end": 3531.48, "text": " But actually, if you look at state of the art model, there's always like a very specific method", "tokens": [51244, 583, 767, 11, 498, 291, 574, 412, 1785, 295, 264, 1523, 2316, 11, 456, 311, 1009, 411, 257, 588, 2685, 3170, 51464], "temperature": 0.0, "avg_logprob": -0.22689487315990306, "compression_ratio": 1.7808764940239044, "no_speech_prob": 0.005038489121943712}, {"id": 682, "seek": 350948, "start": 3531.48, "end": 3537.56, "text": " that performs better in a, in the single task. So it would be interesting to see if we can", "tokens": [51464, 300, 26213, 1101, 294, 257, 11, 294, 264, 2167, 5633, 13, 407, 309, 576, 312, 1880, 281, 536, 498, 321, 393, 51768], "temperature": 0.0, "avg_logprob": -0.22689487315990306, "compression_ratio": 1.7808764940239044, "no_speech_prob": 0.005038489121943712}, {"id": 683, "seek": 353756, "start": 3538.04, "end": 3544.36, "text": " reach those level of performance in, in specific tasks by, by adding some tricks on, or some,", "tokens": [50388, 2524, 729, 1496, 295, 3389, 294, 11, 294, 2685, 9608, 538, 11, 538, 5127, 512, 11733, 322, 11, 420, 512, 11, 50704], "temperature": 0.0, "avg_logprob": -0.19604573931012834, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.004777324385941029}, {"id": 684, "seek": 353756, "start": 3547.08, "end": 3552.6, "text": " or some new optimization methods, and to generalize it to, to dynamical systems,", "tokens": [50840, 420, 512, 777, 19618, 7150, 11, 293, 281, 2674, 1125, 309, 281, 11, 281, 5999, 804, 3652, 11, 51116], "temperature": 0.0, "avg_logprob": -0.19604573931012834, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.004777324385941029}, {"id": 685, "seek": 353756, "start": 3552.6, "end": 3557.96, "text": " which are actually much more interesting, the static systems. So such as dynamical causal models", "tokens": [51116, 597, 366, 767, 709, 544, 1880, 11, 264, 13437, 3652, 13, 407, 1270, 382, 5999, 804, 38755, 5245, 51384], "temperature": 0.0, "avg_logprob": -0.19604573931012834, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.004777324385941029}, {"id": 686, "seek": 353756, "start": 3557.96, "end": 3564.6, "text": " and, or other techniques that allow you to perform causal inference in systems that move.", "tokens": [51384, 293, 11, 420, 661, 7512, 300, 2089, 291, 281, 2042, 38755, 38253, 294, 3652, 300, 1286, 13, 51716], "temperature": 0.0, "avg_logprob": -0.19604573931012834, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.004777324385941029}, {"id": 687, "seek": 356460, "start": 3564.6, "end": 3571.08, "text": " So an action taken in a specific time step influences another node in a later time step,", "tokens": [50364, 407, 364, 3069, 2726, 294, 257, 2685, 565, 1823, 21222, 1071, 9984, 294, 257, 1780, 565, 1823, 11, 50688], "temperature": 0.0, "avg_logprob": -0.22366408978478383, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0023097367957234383}, {"id": 688, "seek": 356460, "start": 3571.08, "end": 3578.92, "text": " which is basically Granger causality. Yeah, that's it. And thank you very much.", "tokens": [50688, 597, 307, 1936, 2606, 3176, 3302, 1860, 13, 865, 11, 300, 311, 309, 13, 400, 1309, 291, 588, 709, 13, 51080], "temperature": 0.0, "avg_logprob": -0.22366408978478383, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0023097367957234383}, {"id": 689, "seek": 356460, "start": 3587.56, "end": 3592.7599999999998, "text": " Thank you. Awesome. And very comprehensive presentation. That was really", "tokens": [51512, 1044, 291, 13, 10391, 13, 400, 588, 13914, 5860, 13, 663, 390, 534, 51772], "temperature": 0.0, "avg_logprob": -0.22366408978478383, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0023097367957234383}, {"id": 690, "seek": 359276, "start": 3593.48, "end": 3601.48, "text": " muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive", "tokens": [50400, 32808, 13, 4919, 11, 32808, 322, 8863, 13, 583, 2086, 11, 3231, 337, 264, 3476, 293, 588, 13914, 50800], "temperature": 0.0, "avg_logprob": -0.1196559610820952, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0018096454441547394}, {"id": 691, "seek": 359276, "start": 3602.36, "end": 3607.48, "text": " presentation. There was really a lot there. And there was also a lot of great questions", "tokens": [50844, 5860, 13, 821, 390, 534, 257, 688, 456, 13, 400, 456, 390, 611, 257, 688, 295, 869, 1651, 51100], "temperature": 0.0, "avg_logprob": -0.1196559610820952, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0018096454441547394}, {"id": 692, "seek": 359276, "start": 3607.48, "end": 3613.96, "text": " in the live chat. So maybe to warm into the questions, how did you come to study this", "tokens": [51100, 294, 264, 1621, 5081, 13, 407, 1310, 281, 4561, 666, 264, 1651, 11, 577, 630, 291, 808, 281, 2979, 341, 51424], "temperature": 0.0, "avg_logprob": -0.1196559610820952, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0018096454441547394}, {"id": 693, "seek": 359276, "start": 3614.5200000000004, "end": 3620.2000000000003, "text": " topic? Were you studying causality and found predictive coding to be useful or vice versa?", "tokens": [51452, 4829, 30, 12448, 291, 7601, 3302, 1860, 293, 1352, 35521, 17720, 281, 312, 4420, 420, 11964, 25650, 30, 51736], "temperature": 0.0, "avg_logprob": -0.1196559610820952, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0018096454441547394}, {"id": 694, "seek": 362020, "start": 3620.2, "end": 3625.72, "text": " Or how did you come out this intersection? I actually have to say that the first person", "tokens": [50364, 1610, 577, 630, 291, 808, 484, 341, 15236, 30, 286, 767, 362, 281, 584, 300, 264, 700, 954, 50640], "temperature": 0.0, "avg_logprob": -0.14929562426627951, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.00419462937861681}, {"id": 695, "seek": 362020, "start": 3625.72, "end": 3634.2799999999997, "text": " that came out with this idea was, was better. So, so like, like, I think a year and a half ago,", "tokens": [50640, 300, 1361, 484, 365, 341, 1558, 390, 11, 390, 1101, 13, 407, 11, 370, 411, 11, 411, 11, 286, 519, 257, 1064, 293, 257, 1922, 2057, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14929562426627951, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.00419462937861681}, {"id": 696, "seek": 362020, "start": 3634.2799999999997, "end": 3640.6, "text": " even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up.", "tokens": [51068, 754, 544, 11, 415, 4114, 411, 257, 3028, 365, 341, 1558, 13, 400, 550, 415, 658, 11832, 11, 293, 572, 472, 6183, 309, 493, 13, 51384], "temperature": 0.0, "avg_logprob": -0.14929562426627951, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.00419462937861681}, {"id": 697, "seek": 362020, "start": 3641.16, "end": 3645.8799999999997, "text": " And, and last summer, I started getting curious about causality and", "tokens": [51412, 400, 11, 293, 1036, 4266, 11, 286, 1409, 1242, 6369, 466, 3302, 1860, 293, 51648], "temperature": 0.0, "avg_logprob": -0.14929562426627951, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.00419462937861681}, {"id": 698, "seek": 364588, "start": 3646.12, "end": 3652.2000000000003, "text": " I read, for example, the book of why as I listen into podcasts, I don't know the", "tokens": [50376, 286, 1401, 11, 337, 1365, 11, 264, 1446, 295, 983, 382, 286, 2140, 666, 24045, 11, 286, 500, 380, 458, 264, 50680], "temperature": 0.0, "avg_logprob": -0.23411847894841975, "compression_ratio": 1.5856573705179282, "no_speech_prob": 0.007759226951748133}, {"id": 699, "seek": 364588, "start": 3652.2000000000003, "end": 3656.52, "text": " standard way in which you get interested in a topic. And, and I remember this,", "tokens": [50680, 3832, 636, 294, 597, 291, 483, 3102, 294, 257, 4829, 13, 400, 11, 293, 286, 1604, 341, 11, 50896], "temperature": 0.0, "avg_logprob": -0.23411847894841975, "compression_ratio": 1.5856573705179282, "no_speech_prob": 0.007759226951748133}, {"id": 700, "seek": 364588, "start": 3656.52, "end": 3662.36, "text": " this idea from Baron and proposed it to him. And I was like, why don't we expand it and,", "tokens": [50896, 341, 1558, 490, 30978, 293, 10348, 309, 281, 796, 13, 400, 286, 390, 411, 11, 983, 500, 380, 321, 5268, 309, 293, 11, 51188], "temperature": 0.0, "avg_logprob": -0.23411847894841975, "compression_ratio": 1.5856573705179282, "no_speech_prob": 0.007759226951748133}, {"id": 701, "seek": 364588, "start": 3662.36, "end": 3667.88, "text": " and actually make it a paper. So I, I involve some people to work with experiments and,", "tokens": [51188, 293, 767, 652, 309, 257, 3035, 13, 407, 286, 11, 286, 9494, 512, 561, 281, 589, 365, 12050, 293, 11, 51464], "temperature": 0.0, "avg_logprob": -0.23411847894841975, "compression_ratio": 1.5856573705179282, "no_speech_prob": 0.007759226951748133}, {"id": 702, "seek": 364588, "start": 3668.52, "end": 3673.56, "text": " and this is the final result at the end. Awesome. Cool. Yeah.", "tokens": [51496, 293, 341, 307, 264, 2572, 1874, 412, 264, 917, 13, 10391, 13, 8561, 13, 865, 13, 51748], "temperature": 0.0, "avg_logprob": -0.23411847894841975, "compression_ratio": 1.5856573705179282, "no_speech_prob": 0.007759226951748133}, {"id": 703, "seek": 367356, "start": 3674.12, "end": 3679.64, "text": " Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different", "tokens": [50392, 3301, 11, 257, 688, 281, 584, 13, 286, 478, 445, 516, 281, 352, 281, 264, 1621, 5081, 700, 293, 2985, 257, 3840, 295, 819, 50668], "temperature": 0.0, "avg_logprob": -0.14717874703583894, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.029747869819402695}, {"id": 704, "seek": 367356, "start": 3679.64, "end": 3682.68, "text": " questions. And if anybody else wants to add more, I'm going to turn the light on first,", "tokens": [50668, 1651, 13, 400, 498, 4472, 1646, 2738, 281, 909, 544, 11, 286, 478, 516, 281, 1261, 264, 1442, 322, 700, 11, 50820], "temperature": 0.0, "avg_logprob": -0.14717874703583894, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.029747869819402695}, {"id": 705, "seek": 367356, "start": 3682.68, "end": 3686.2, "text": " because I'm, I think I'm getting in the dark more and more. Yes.", "tokens": [50820, 570, 286, 478, 11, 286, 519, 286, 478, 1242, 294, 264, 2877, 544, 293, 544, 13, 1079, 13, 50996], "temperature": 0.0, "avg_logprob": -0.14717874703583894, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.029747869819402695}, {"id": 706, "seek": 367356, "start": 3688.36, "end": 3693.4, "text": " Who said active inference can't solve the dark room issue? Oh, yes, here we are.", "tokens": [51104, 2102, 848, 4967, 38253, 393, 380, 5039, 264, 2877, 1808, 2734, 30, 876, 11, 2086, 11, 510, 321, 366, 13, 51356], "temperature": 0.0, "avg_logprob": -0.14717874703583894, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.029747869819402695}, {"id": 707, "seek": 367356, "start": 3694.92, "end": 3697.88, "text": " So would you say the light switch caused it to be lighter?", "tokens": [51432, 407, 576, 291, 584, 264, 1442, 3679, 7008, 309, 281, 312, 11546, 30, 51580], "temperature": 0.0, "avg_logprob": -0.14717874703583894, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.029747869819402695}, {"id": 708, "seek": 369788, "start": 3698.2000000000003, "end": 3709.2400000000002, "text": " Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding,", "tokens": [50380, 865, 11, 286, 519, 370, 13, 883, 2663, 510, 13, 3301, 11, 1392, 13, 21601, 1468, 4114, 1670, 294, 35521, 17720, 11, 50932], "temperature": 0.0, "avg_logprob": -0.15660271500096176, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.03065582551062107}, {"id": 709, "seek": 369788, "start": 3709.2400000000002, "end": 3714.04, "text": " all distributions are usually Gaussian, the bottom up messages are precision weighted", "tokens": [50932, 439, 37870, 366, 2673, 39148, 11, 264, 2767, 493, 7897, 366, 18356, 32807, 51172], "temperature": 0.0, "avg_logprob": -0.15660271500096176, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.03065582551062107}, {"id": 710, "seek": 369788, "start": 3714.04, "end": 3717.88, "text": " prediction errors where precision is the inverse of the Gaussian covariance.", "tokens": [51172, 17630, 13603, 689, 18356, 307, 264, 17340, 295, 264, 39148, 49851, 719, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15660271500096176, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.03065582551062107}, {"id": 711, "seek": 369788, "start": 3718.52, "end": 3721.32, "text": " What if non Gaussian distributions are used?", "tokens": [51396, 708, 498, 2107, 39148, 37870, 366, 1143, 30, 51536], "temperature": 0.0, "avg_logprob": -0.15660271500096176, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.03065582551062107}, {"id": 712, "seek": 372132, "start": 3721.6400000000003, "end": 3730.2000000000003, "text": " Is, um, basically the general method stays, the different, the main difference is that you,", "tokens": [50380, 1119, 11, 1105, 11, 1936, 264, 2674, 3170, 10834, 11, 264, 819, 11, 264, 2135, 2649, 307, 300, 291, 11, 50808], "temperature": 0.0, "avg_logprob": -0.24534722823130933, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.008078492246568203}, {"id": 713, "seek": 372132, "start": 3731.0, "end": 3735.7200000000003, "text": " you don't have prediction errors, which, uh, as was correctly pointed out is the,", "tokens": [50848, 291, 500, 380, 362, 17630, 13603, 11, 597, 11, 2232, 11, 382, 390, 8944, 10932, 484, 307, 264, 11, 51084], "temperature": 0.0, "avg_logprob": -0.24534722823130933, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.008078492246568203}, {"id": 714, "seek": 372132, "start": 3735.7200000000003, "end": 3741.32, "text": " basically the derivative of the variational free energy. If you have Gaussian assumptions,", "tokens": [51084, 1936, 264, 13760, 295, 264, 3034, 1478, 1737, 2281, 13, 759, 291, 362, 39148, 17695, 11, 51364], "temperature": 0.0, "avg_logprob": -0.24534722823130933, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.008078492246568203}, {"id": 715, "seek": 372132, "start": 3742.92, "end": 3745.88, "text": " yeah, you don't have that single quantity to set to zero.", "tokens": [51444, 1338, 11, 291, 500, 380, 362, 300, 2167, 11275, 281, 992, 281, 4018, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24534722823130933, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.008078492246568203}, {"id": 716, "seek": 374588, "start": 3746.52, "end": 3752.12, "text": " And you probably will have to act on the structure of the graph to perform interventions.", "tokens": [50396, 400, 291, 1391, 486, 362, 281, 605, 322, 264, 3877, 295, 264, 4295, 281, 2042, 20924, 13, 50676], "temperature": 0.0, "avg_logprob": -0.11405714762579534, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.0041310591623187065}, {"id": 717, "seek": 374588, "start": 3754.04, "end": 3760.28, "text": " And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian", "tokens": [50772, 400, 611, 291, 11, 2232, 11, 293, 7734, 632, 257, 3035, 294, 20229, 35521, 17720, 4399, 39148, 51084], "temperature": 0.0, "avg_logprob": -0.11405714762579534, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.0041310591623187065}, {"id": 718, "seek": 374588, "start": 3760.28, "end": 3762.6800000000003, "text": " distributions that, that looked at some of these issues, right?", "tokens": [51084, 37870, 300, 11, 300, 2956, 412, 512, 295, 613, 2663, 11, 558, 30, 51204], "temperature": 0.0, "avg_logprob": -0.11405714762579534, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.0041310591623187065}, {"id": 719, "seek": 374588, "start": 3763.88, "end": 3769.1600000000003, "text": " Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh,", "tokens": [51264, 1079, 11, 2086, 11, 2293, 13, 407, 300, 3035, 390, 257, 707, 857, 11, 264, 1558, 2261, 300, 3035, 307, 11, 2232, 11, 51528], "temperature": 0.0, "avg_logprob": -0.11405714762579534, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.0041310591623187065}, {"id": 720, "seek": 374588, "start": 3770.52, "end": 3775.2400000000002, "text": " and we model transformers. That's the biggest motivation using predictive coding.", "tokens": [51596, 293, 321, 2316, 4088, 433, 13, 663, 311, 264, 3880, 12335, 1228, 35521, 17720, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11405714762579534, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.0041310591623187065}, {"id": 721, "seek": 377524, "start": 3775.3199999999997, "end": 3780.68, "text": " And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end,", "tokens": [50368, 400, 264, 1867, 307, 11, 2232, 11, 307, 572, 11, 570, 264, 11, 264, 3202, 7513, 382, 257, 2787, 41167, 412, 264, 917, 11, 50636], "temperature": 0.0, "avg_logprob": -0.13301844017527928, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0012793700443580747}, {"id": 722, "seek": 377524, "start": 3780.68, "end": 3786.52, "text": " and softmax calls to, uh, like not to Gaussian distribution, but to,", "tokens": [50636, 293, 2787, 41167, 5498, 281, 11, 2232, 11, 411, 406, 281, 39148, 7316, 11, 457, 281, 11, 50928], "temperature": 0.0, "avg_logprob": -0.13301844017527928, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0012793700443580747}, {"id": 723, "seek": 377524, "start": 3788.52, "end": 3792.2, "text": " yeah, to softmax distribution, the, I don't get the name now, but yes.", "tokens": [51028, 1338, 11, 281, 2787, 41167, 7316, 11, 264, 11, 286, 500, 380, 483, 264, 1315, 586, 11, 457, 2086, 13, 51212], "temperature": 0.0, "avg_logprob": -0.13301844017527928, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0012793700443580747}, {"id": 724, "seek": 377524, "start": 3793.56, "end": 3797.08, "text": " And, uh, so yes, that's a generalization. It's a little bit", "tokens": [51280, 400, 11, 2232, 11, 370, 2086, 11, 300, 311, 257, 2674, 2144, 13, 467, 311, 257, 707, 857, 51456], "temperature": 0.0, "avg_logprob": -0.13301844017527928, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0012793700443580747}, {"id": 725, "seek": 377524, "start": 3797.72, "end": 3801.0, "text": " tricky to call it. Once you remove the Gaston assumption is a little bit", "tokens": [51488, 12414, 281, 818, 309, 13, 3443, 291, 4159, 264, 31988, 266, 15302, 307, 257, 707, 857, 51652], "temperature": 0.0, "avg_logprob": -0.13301844017527928, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0012793700443580747}, {"id": 726, "seek": 380100, "start": 3801.0, "end": 3804.68, "text": " still tricky to call it predictive coding. So it's a,", "tokens": [50364, 920, 12414, 281, 818, 309, 35521, 17720, 13, 407, 309, 311, 257, 11, 50548], "temperature": 0.0, "avg_logprob": -0.25147941086318465, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.0017498200759291649}, {"id": 727, "seek": 380100, "start": 3806.52, "end": 3810.04, "text": " so for, for example, like talking to, uh, to Carl Freestone,", "tokens": [50640, 370, 337, 11, 337, 1365, 11, 411, 1417, 281, 11, 2232, 11, 281, 14256, 6142, 19098, 11, 50816], "temperature": 0.0, "avg_logprob": -0.25147941086318465, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.0017498200759291649}, {"id": 728, "seek": 380100, "start": 3811.64, "end": 3816.12, "text": " like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions.", "tokens": [50896, 411, 35521, 17720, 307, 787, 498, 291, 11, 498, 291, 362, 787, 39148, 11, 39148, 17695, 13, 51120], "temperature": 0.0, "avg_logprob": -0.25147941086318465, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.0017498200759291649}, {"id": 729, "seek": 380100, "start": 3817.88, "end": 3820.44, "text": " But yes, that's more a philosophical debate than, uh,", "tokens": [51208, 583, 2086, 11, 300, 311, 544, 257, 25066, 7958, 813, 11, 2232, 11, 51336], "temperature": 0.0, "avg_logprob": -0.25147941086318465, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.0017498200759291649}, {"id": 730, "seek": 380100, "start": 3822.76, "end": 3828.12, "text": " Interesting. And another, I think topic that, that's definitely of, of great interest is", "tokens": [51452, 14711, 13, 400, 1071, 11, 286, 519, 4829, 300, 11, 300, 311, 2138, 295, 11, 295, 869, 1179, 307, 51720], "temperature": 0.0, "avg_logprob": -0.25147941086318465, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.0017498200759291649}, {"id": 731, "seek": 382812, "start": 3828.12, "end": 3833.64, "text": " similarities and differences between the attention apparatus in transformers", "tokens": [50364, 24197, 293, 7300, 1296, 264, 3202, 38573, 294, 4088, 433, 50640], "temperature": 0.0, "avg_logprob": -0.1696158208345112, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.0005969454068690538}, {"id": 732, "seek": 382812, "start": 3834.3599999999997, "end": 3839.96, "text": " and the way that attention is described from a neurocognitive perspective and from a predictive", "tokens": [50676, 293, 264, 636, 300, 3202, 307, 7619, 490, 257, 16499, 66, 2912, 2187, 4585, 293, 490, 257, 35521, 50956], "temperature": 0.0, "avg_logprob": -0.1696158208345112, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.0005969454068690538}, {"id": 733, "seek": 382812, "start": 3839.96, "end": 3844.04, "text": " processing precision waiting angle. What do you, what do you think about that?", "tokens": [50956, 9007, 18356, 3806, 5802, 13, 708, 360, 291, 11, 437, 360, 291, 519, 466, 300, 30, 51160], "temperature": 0.0, "avg_logprob": -0.1696158208345112, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.0005969454068690538}, {"id": 734, "seek": 382812, "start": 3846.44, "end": 3853.88, "text": " Well, the idea is that, um, yeah, I think about it is that in from a pretty processing", "tokens": [51280, 1042, 11, 264, 1558, 307, 300, 11, 1105, 11, 1338, 11, 286, 519, 466, 309, 307, 300, 294, 490, 257, 1238, 9007, 51652], "temperature": 0.0, "avg_logprob": -0.1696158208345112, "compression_ratio": 1.707070707070707, "no_speech_prob": 0.0005969454068690538}, {"id": 735, "seek": 385388, "start": 3853.88, "end": 3858.44, "text": " and, uh, and also operational inference perspective, attention can be seen as a,", "tokens": [50364, 293, 11, 2232, 11, 293, 611, 16607, 38253, 4585, 11, 3202, 393, 312, 1612, 382, 257, 11, 50592], "temperature": 0.0, "avg_logprob": -0.16414395500631893, "compression_ratio": 1.705179282868526, "no_speech_prob": 0.021430790424346924}, {"id": 736, "seek": 385388, "start": 3858.44, "end": 3862.28, "text": " as a kind of structure learning problem. There's a, I think there's a recent paper from,", "tokens": [50592, 382, 257, 733, 295, 3877, 2539, 1154, 13, 821, 311, 257, 11, 286, 519, 456, 311, 257, 5162, 3035, 490, 11, 50784], "temperature": 0.0, "avg_logprob": -0.16414395500631893, "compression_ratio": 1.705179282868526, "no_speech_prob": 0.021430790424346924}, {"id": 737, "seek": 385388, "start": 3863.08, "end": 3869.0, "text": " from Chris Buckley's group that shows that there should be, there should be a reprint on archive", "tokens": [50824, 490, 6688, 22006, 3420, 311, 1594, 300, 3110, 300, 456, 820, 312, 11, 456, 820, 312, 257, 1085, 19014, 322, 23507, 51120], "temperature": 0.0, "avg_logprob": -0.16414395500631893, "compression_ratio": 1.705179282868526, "no_speech_prob": 0.021430790424346924}, {"id": 738, "seek": 385388, "start": 3869.56, "end": 3872.52, "text": " in which basically they show that the attention mechanism is simply", "tokens": [51148, 294, 597, 1936, 436, 855, 300, 264, 3202, 7513, 307, 2935, 51296], "temperature": 0.0, "avg_logprob": -0.16414395500631893, "compression_ratio": 1.705179282868526, "no_speech_prob": 0.021430790424346924}, {"id": 739, "seek": 385388, "start": 3873.2400000000002, "end": 3879.7200000000003, "text": " learning the, the precision on the, on the weight parameters specific to out to a data point.", "tokens": [51332, 2539, 264, 11, 264, 18356, 322, 264, 11, 322, 264, 3364, 9834, 2685, 281, 484, 281, 257, 1412, 935, 13, 51656], "temperature": 0.0, "avg_logprob": -0.16414395500631893, "compression_ratio": 1.705179282868526, "no_speech_prob": 0.021430790424346924}, {"id": 740, "seek": 387972, "start": 3879.7999999999997, "end": 3885.16, "text": " So this precision is not a, is not a, is not a parameter that is in the structure of the model.", "tokens": [50368, 407, 341, 18356, 307, 406, 257, 11, 307, 406, 257, 11, 307, 406, 257, 13075, 300, 307, 294, 264, 3877, 295, 264, 2316, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 741, "seek": 387972, "start": 3885.16, "end": 3889.48, "text": " So it's not a model specific parameter. It is a fast changing parameter like the value nodes", "tokens": [50636, 407, 309, 311, 406, 257, 2316, 2685, 13075, 13, 467, 307, 257, 2370, 4473, 13075, 411, 264, 2158, 13891, 50852], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 742, "seek": 387972, "start": 3890.04, "end": 3893.08, "text": " that gets updated while minimizing the version of free energy.", "tokens": [50880, 300, 2170, 10588, 1339, 46608, 264, 3037, 295, 1737, 2281, 13, 51032], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 743, "seek": 387972, "start": 3893.72, "end": 3897.0, "text": " And once they, once you've minimized it and compute it, then you throw it away.", "tokens": [51064, 400, 1564, 436, 11, 1564, 291, 600, 4464, 1602, 309, 293, 14722, 309, 11, 550, 291, 3507, 309, 1314, 13, 51228], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 744, "seek": 387972, "start": 3897.0, "end": 3899.9599999999996, "text": " And from the next data point, you have to really compute it from scratch.", "tokens": [51228, 400, 490, 264, 958, 1412, 935, 11, 291, 362, 281, 534, 14722, 309, 490, 8459, 13, 51376], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 745, "seek": 387972, "start": 3900.7599999999998, "end": 3907.16, "text": " So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as", "tokens": [51416, 407, 2086, 11, 286, 519, 264, 11, 264, 21663, 24903, 10829, 307, 11, 2232, 11, 264, 3202, 7513, 393, 312, 1612, 382, 51736], "temperature": 0.0, "avg_logprob": -0.16162984333341085, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0015066406922414899}, {"id": 746, "seek": 390716, "start": 3907.16, "end": 3913.08, "text": " a kind of structure learning, but a structure learning that is data point specific and not", "tokens": [50364, 257, 733, 295, 3877, 2539, 11, 457, 257, 3877, 2539, 300, 307, 1412, 935, 2685, 293, 406, 50660], "temperature": 0.0, "avg_logprob": -0.17192753325117396, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0016971186269074678}, {"id": 747, "seek": 390716, "start": 3913.08, "end": 3917.96, "text": " model specific. And I think if you want to generalize a little bit and go from,", "tokens": [50660, 2316, 2685, 13, 400, 286, 519, 498, 291, 528, 281, 2674, 1125, 257, 707, 857, 293, 352, 490, 11, 50904], "temperature": 0.0, "avg_logprob": -0.17192753325117396, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0016971186269074678}, {"id": 748, "seek": 390716, "start": 3918.92, "end": 3923.0, "text": " from the attention mechanism in transformers to the attention mechanism cognitive science,", "tokens": [50952, 490, 264, 3202, 7513, 294, 4088, 433, 281, 264, 3202, 7513, 15605, 3497, 11, 51156], "temperature": 0.0, "avg_logprob": -0.17192753325117396, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0016971186269074678}, {"id": 749, "seek": 390716, "start": 3924.2, "end": 3929.72, "text": " I feel they're probably too different to, like to draw similarities and, uh,", "tokens": [51216, 286, 841, 436, 434, 1391, 886, 819, 281, 11, 411, 281, 2642, 24197, 293, 11, 2232, 11, 51492], "temperature": 0.0, "avg_logprob": -0.17192753325117396, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0016971186269074678}, {"id": 750, "seek": 390716, "start": 3931.24, "end": 3936.8399999999997, "text": " I think the structure learning analogy and the, how important one connection in is", "tokens": [51568, 286, 519, 264, 3877, 2539, 21663, 293, 264, 11, 577, 1021, 472, 4984, 294, 307, 51848], "temperature": 0.0, "avg_logprob": -0.17192753325117396, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0016971186269074678}, {"id": 751, "seek": 393684, "start": 3936.84, "end": 3939.8, "text": " with respect to another one probably does job much better.", "tokens": [50364, 365, 3104, 281, 1071, 472, 1391, 775, 1691, 709, 1101, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16976536931218328, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.0019564710091799498}, {"id": 752, "seek": 393684, "start": 3942.04, "end": 3949.56, "text": " Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference", "tokens": [50624, 8561, 13, 3769, 1867, 13, 1033, 13, 21601, 1468, 8962, 11, 294, 5682, 44919, 901, 82, 11, 437, 307, 264, 2649, 51000], "temperature": 0.0, "avg_logprob": -0.16976536931218328, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.0019564710091799498}, {"id": 753, "seek": 393684, "start": 3949.56, "end": 3953.6400000000003, "text": " between hidden variables X and unobserved variables U?", "tokens": [51000, 1296, 7633, 9102, 1783, 293, 8526, 929, 6913, 9102, 624, 30, 51204], "temperature": 0.0, "avg_logprob": -0.16976536931218328, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.0019564710091799498}, {"id": 754, "seek": 393684, "start": 3955.56, "end": 3962.44, "text": " The difference is that you can, uh, I think the main one is that you cannot observe the,", "tokens": [51300, 440, 2649, 307, 300, 291, 393, 11, 2232, 11, 286, 519, 264, 2135, 472, 307, 300, 291, 2644, 11441, 264, 11, 51644], "temperature": 0.0, "avg_logprob": -0.16976536931218328, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.0019564710091799498}, {"id": 755, "seek": 396244, "start": 3962.52, "end": 3967.48, "text": " the use. You can use them because you can, you can compute them and fix them,", "tokens": [50368, 264, 764, 13, 509, 393, 764, 552, 570, 291, 393, 11, 291, 393, 14722, 552, 293, 3191, 552, 11, 50616], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 756, "seek": 396244, "start": 3968.2000000000003, "end": 3972.44, "text": " but you cannot, the idea is that you have no control over them. So the use,", "tokens": [50652, 457, 291, 2644, 11, 264, 1558, 307, 300, 291, 362, 572, 1969, 670, 552, 13, 407, 264, 764, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 757, "seek": 396244, "start": 3972.44, "end": 3977.7200000000003, "text": " the use should be seen as a environment specific variables that they are there. They,", "tokens": [50864, 264, 764, 820, 312, 1612, 382, 257, 2823, 2685, 9102, 300, 436, 366, 456, 13, 814, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 758, "seek": 396244, "start": 3977.7200000000003, "end": 3982.84, "text": " they influence your process. Okay. Because the, for example, when you go back in time,", "tokens": [51128, 436, 6503, 428, 1399, 13, 1033, 13, 1436, 264, 11, 337, 1365, 11, 562, 291, 352, 646, 294, 565, 11, 51384], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 759, "seek": 396244, "start": 3982.84, "end": 3985.7200000000003, "text": " the environment is different. So the idea is for example, if you,", "tokens": [51384, 264, 2823, 307, 819, 13, 407, 264, 1558, 307, 337, 1365, 11, 498, 291, 11, 51528], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 760, "seek": 396244, "start": 3986.52, "end": 3991.7200000000003, "text": " like going back to the, to the example before of the, of the expected income of a person with", "tokens": [51568, 411, 516, 646, 281, 264, 11, 281, 264, 1365, 949, 295, 264, 11, 295, 264, 5176, 5742, 295, 257, 954, 365, 51828], "temperature": 0.0, "avg_logprob": -0.10585728454589843, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.017155904322862625}, {"id": 761, "seek": 399172, "start": 3991.72, "end": 3998.7599999999998, "text": " a specific intelligence of education, uh, uh, education degree, the idea is that if I want to,", "tokens": [50364, 257, 2685, 7599, 295, 3309, 11, 2232, 11, 2232, 11, 3309, 4314, 11, 264, 1558, 307, 300, 498, 286, 528, 281, 11, 50716], "temperature": 0.0, "avg_logprob": -0.131521887698416, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0032405611127614975}, {"id": 762, "seek": 399172, "start": 3999.3199999999997, "end": 4005.3199999999997, "text": " to see how much I will learn today with a, with a, with a, I don't know, with a master degree,", "tokens": [50744, 281, 536, 577, 709, 286, 486, 1466, 965, 365, 257, 11, 365, 257, 11, 365, 257, 11, 286, 500, 380, 458, 11, 365, 257, 4505, 4314, 11, 51044], "temperature": 0.0, "avg_logprob": -0.131521887698416, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0032405611127614975}, {"id": 763, "seek": 399172, "start": 4005.3199999999997, "end": 4011.0, "text": " is different with respect to how much I would earn 20 years ago with a master degree is different.", "tokens": [51044, 307, 819, 365, 3104, 281, 577, 709, 286, 576, 6012, 945, 924, 2057, 365, 257, 4505, 4314, 307, 819, 13, 51328], "temperature": 0.0, "avg_logprob": -0.131521887698416, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0032405611127614975}, {"id": 764, "seek": 399172, "start": 4011.0, "end": 4016.4399999999996, "text": " For example, here in Italy with respect to other countries and all those variables that are not", "tokens": [51328, 1171, 1365, 11, 510, 294, 10705, 365, 3104, 281, 661, 3517, 293, 439, 729, 9102, 300, 366, 406, 51600], "temperature": 0.0, "avg_logprob": -0.131521887698416, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0032405611127614975}, {"id": 765, "seek": 399172, "start": 4016.4399999999996, "end": 4021.3999999999996, "text": " under your control, you can not model them using your vision network, but they are there.", "tokens": [51600, 833, 428, 1969, 11, 291, 393, 406, 2316, 552, 1228, 428, 5201, 3209, 11, 457, 436, 366, 456, 13, 51848], "temperature": 0.0, "avg_logprob": -0.131521887698416, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0032405611127614975}, {"id": 766, "seek": 402140, "start": 4021.4, "end": 4026.04, "text": " Okay. So you, you cannot ignore them when you, when you want to draw conclusions.", "tokens": [50364, 1033, 13, 407, 291, 11, 291, 2644, 11200, 552, 562, 291, 11, 562, 291, 528, 281, 2642, 22865, 13, 50596], "temperature": 0.0, "avg_logprob": -0.14110870684607554, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.002376169664785266}, {"id": 767, "seek": 402140, "start": 4026.76, "end": 4031.7200000000003, "text": " So it's, yeah, it's basically everything that you cannot control. You can infer them. So you", "tokens": [50632, 407, 309, 311, 11, 1338, 11, 309, 311, 1936, 1203, 300, 291, 2644, 1969, 13, 509, 393, 13596, 552, 13, 407, 291, 50880], "temperature": 0.0, "avg_logprob": -0.14110870684607554, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.002376169664785266}, {"id": 768, "seek": 402140, "start": 4031.7200000000003, "end": 4036.84, "text": " can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years", "tokens": [50880, 393, 11, 291, 393, 2042, 257, 5682, 5682, 44919, 901, 38253, 646, 294, 565, 293, 584, 11, 876, 11, 945, 924, 51136], "temperature": 0.0, "avg_logprob": -0.14110870684607554, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.002376169664785266}, {"id": 769, "seek": 402140, "start": 4036.84, "end": 4043.4, "text": " ago, I would have earned this much if I, if I was disintelligent at this degree on average,", "tokens": [51136, 2057, 11, 286, 576, 362, 12283, 341, 709, 498, 286, 11, 498, 286, 390, 717, 20761, 25002, 412, 341, 4314, 322, 4274, 11, 51464], "temperature": 0.0, "avg_logprob": -0.14110870684607554, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.002376169664785266}, {"id": 770, "seek": 402140, "start": 4043.4, "end": 4049.0, "text": " of course. And, but it's not that I can change the government policies towards jobs or the,", "tokens": [51464, 295, 1164, 13, 400, 11, 457, 309, 311, 406, 300, 286, 393, 1319, 264, 2463, 7657, 3030, 4782, 420, 264, 11, 51744], "temperature": 0.0, "avg_logprob": -0.14110870684607554, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.002376169664785266}, {"id": 771, "seek": 404900, "start": 4049.64, "end": 4057.24, "text": " or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use.", "tokens": [50396, 420, 721, 411, 300, 13, 467, 311, 257, 7731, 5682, 44919, 901, 13, 1079, 11, 2293, 13, 407, 1338, 11, 729, 366, 264, 764, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1616938989373702, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0023961563128978014}, {"id": 772, "seek": 404900, "start": 4058.44, "end": 4064.2, "text": " Awesome. All right. Have you implemented generalized coordinates in predictive coding?", "tokens": [50836, 10391, 13, 1057, 558, 13, 3560, 291, 12270, 44498, 21056, 294, 35521, 17720, 30, 51124], "temperature": 0.0, "avg_logprob": -0.1616938989373702, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0023961563128978014}, {"id": 773, "seek": 404900, "start": 4065.96, "end": 4072.6, "text": " No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never", "tokens": [51212, 883, 11, 286, 600, 11, 572, 11, 286, 600, 1128, 1096, 309, 13, 286, 600, 11, 2232, 11, 1338, 11, 286, 600, 9454, 309, 11, 457, 286, 600, 11, 286, 600, 1128, 51544], "temperature": 0.0, "avg_logprob": -0.1616938989373702, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0023961563128978014}, {"id": 774, "seek": 407260, "start": 4072.68, "end": 4079.88, "text": " implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable.", "tokens": [50368, 12270, 309, 13, 286, 458, 436, 3928, 281, 312, 23742, 293, 11, 2232, 11, 293, 309, 311, 588, 1152, 281, 652, 552, 8351, 13, 50728], "temperature": 0.0, "avg_logprob": -0.12127873270135177, "compression_ratio": 1.7, "no_speech_prob": 0.03249473124742508}, {"id": 775, "seek": 407260, "start": 4079.88, "end": 4086.2, "text": " I think that's the, that's the takeaway that I got from talking to people that have implemented them.", "tokens": [50728, 286, 519, 300, 311, 264, 11, 300, 311, 264, 30681, 300, 286, 658, 490, 1417, 281, 561, 300, 362, 12270, 552, 13, 51044], "temperature": 0.0, "avg_logprob": -0.12127873270135177, "compression_ratio": 1.7, "no_speech_prob": 0.03249473124742508}, {"id": 776, "seek": 407260, "start": 4088.44, "end": 4093.96, "text": " But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that,", "tokens": [51156, 583, 11, 457, 1338, 11, 1338, 11, 286, 478, 3650, 295, 512, 10577, 300, 1361, 484, 767, 3938, 466, 552, 300, 11, 51432], "temperature": 0.0, "avg_logprob": -0.12127873270135177, "compression_ratio": 1.7, "no_speech_prob": 0.03249473124742508}, {"id": 777, "seek": 407260, "start": 4093.96, "end": 4098.84, "text": " that tested on some threshold encoder style. Actually, I think still from Baron,", "tokens": [51432, 300, 8246, 322, 512, 14678, 2058, 19866, 3758, 13, 5135, 11, 286, 519, 920, 490, 30978, 11, 51676], "temperature": 0.0, "avg_logprob": -0.12127873270135177, "compression_ratio": 1.7, "no_speech_prob": 0.03249473124742508}, {"id": 778, "seek": 409884, "start": 4099.4800000000005, "end": 4105.400000000001, "text": " there's a, there's a paper out there that came out last summer, but no, I've never played them with", "tokens": [50396, 456, 311, 257, 11, 456, 311, 257, 3035, 484, 456, 300, 1361, 484, 1036, 4266, 11, 457, 572, 11, 286, 600, 1128, 3737, 552, 365, 50692], "temperature": 0.0, "avg_logprob": -0.13342638015747071, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004262518137693405}, {"id": 779, "seek": 409884, "start": 4105.400000000001, "end": 4113.96, "text": " them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction", "tokens": [50692, 552, 2059, 13, 8561, 13, 3358, 29594, 11, 775, 5127, 544, 4358, 294, 264, 22333, 5407, 264, 30217, 51120], "temperature": 0.0, "avg_logprob": -0.13342638015747071, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004262518137693405}, {"id": 780, "seek": 409884, "start": 4113.96, "end": 4122.92, "text": " problem of predicting input? Adding more level in, uh, in which sense, because the", "tokens": [51120, 1154, 295, 32884, 4846, 30, 31204, 544, 1496, 294, 11, 2232, 11, 294, 597, 2020, 11, 570, 264, 51568], "temperature": 0.0, "avg_logprob": -0.13342638015747071, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004262518137693405}, {"id": 781, "seek": 409884, "start": 4122.92, "end": 4128.6, "text": " destruction problem is given by cycles. So basically you provide an image and the fact that you have", "tokens": [51568, 13563, 1154, 307, 2212, 538, 17796, 13, 407, 1936, 291, 2893, 364, 3256, 293, 264, 1186, 300, 291, 362, 51852], "temperature": 0.0, "avg_logprob": -0.13342638015747071, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004262518137693405}, {"id": 782, "seek": 412860, "start": 4128.84, "end": 4136.04, "text": " a, so edges going out of the image, going in the, in the neurons, and then other edges going back,", "tokens": [50376, 257, 11, 370, 8819, 516, 484, 295, 264, 3256, 11, 516, 294, 264, 11, 294, 264, 22027, 11, 293, 550, 661, 8819, 516, 646, 11, 50736], "temperature": 0.0, "avg_logprob": -0.199559831127678, "compression_ratio": 1.900497512437811, "no_speech_prob": 0.0007290354697033763}, {"id": 783, "seek": 412860, "start": 4137.64, "end": 4143.400000000001, "text": " the, this basically creates the fact that you have a, that the error of, that those basically,", "tokens": [50816, 264, 11, 341, 1936, 7829, 264, 1186, 300, 291, 362, 257, 11, 300, 264, 6713, 295, 11, 300, 729, 1936, 11, 51104], "temperature": 0.0, "avg_logprob": -0.199559831127678, "compression_ratio": 1.900497512437811, "no_speech_prob": 0.0007290354697033763}, {"id": 784, "seek": 412860, "start": 4143.96, "end": 4149.0, "text": " these ingoing edges to the pixels of the image, they create some prediction errors. So you have", "tokens": [51132, 613, 3957, 78, 278, 8819, 281, 264, 18668, 295, 264, 3256, 11, 436, 1884, 512, 17630, 13603, 13, 407, 291, 362, 51384], "temperature": 0.0, "avg_logprob": -0.199559831127678, "compression_ratio": 1.900497512437811, "no_speech_prob": 0.0007290354697033763}, {"id": 785, "seek": 412860, "start": 4149.0, "end": 4154.200000000001, "text": " some prediction errors that get spread inside the model. And that's, yeah, and this problem,", "tokens": [51384, 512, 17630, 13603, 300, 483, 3974, 1854, 264, 2316, 13, 400, 300, 311, 11, 1338, 11, 293, 341, 1154, 11, 51644], "temperature": 0.0, "avg_logprob": -0.199559831127678, "compression_ratio": 1.900497512437811, "no_speech_prob": 0.0007290354697033763}, {"id": 786, "seek": 415420, "start": 4154.2, "end": 4159.4, "text": " I think is general of cycles. And it's probably not related to hierarchy in general.", "tokens": [50364, 286, 519, 307, 2674, 295, 17796, 13, 400, 309, 311, 1391, 406, 4077, 281, 22333, 294, 2674, 13, 50624], "temperature": 0.0, "avg_logprob": -0.13995752943323014, "compression_ratio": 1.625, "no_speech_prob": 0.004979775287210941}, {"id": 787, "seek": 415420, "start": 4161.0, "end": 4166.36, "text": " So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges,", "tokens": [50704, 407, 309, 311, 11, 309, 311, 11, 309, 311, 264, 732, 22341, 8819, 281, 264, 18668, 13, 759, 291, 500, 380, 362, 22341, 8819, 11, 50972], "temperature": 0.0, "avg_logprob": -0.13995752943323014, "compression_ratio": 1.625, "no_speech_prob": 0.004979775287210941}, {"id": 788, "seek": 415420, "start": 4166.36, "end": 4173.8, "text": " you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic", "tokens": [50972, 291, 362, 572, 11, 2232, 11, 572, 30217, 1154, 3602, 13, 8561, 13, 400, 11, 293, 264, 31256, 295, 264, 696, 88, 66, 1050, 51344], "temperature": 0.0, "avg_logprob": -0.13995752943323014, "compression_ratio": 1.625, "no_speech_prob": 0.004979775287210941}, {"id": 789, "seek": 415420, "start": 4173.8, "end": 4183.24, "text": " network through the trace operator, that's a very interesting technique. And when was that", "tokens": [51344, 3209, 807, 264, 13508, 12973, 11, 300, 311, 257, 588, 1880, 6532, 13, 400, 562, 390, 300, 51816], "temperature": 0.0, "avg_logprob": -0.13995752943323014, "compression_ratio": 1.625, "no_speech_prob": 0.004979775287210941}, {"id": 790, "seek": 418324, "start": 4183.32, "end": 4184.28, "text": " brought into play?", "tokens": [50368, 3038, 666, 862, 30, 50416], "temperature": 0.0, "avg_logprob": -0.13779062032699585, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0015460597351193428}, {"id": 791, "seek": 418324, "start": 4186.84, "end": 4193.32, "text": " As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know,", "tokens": [50544, 1018, 1400, 382, 286, 458, 11, 286, 519, 309, 1361, 484, 365, 257, 3035, 286, 11, 286, 30134, 294, 6096, 13, 286, 11, 286, 500, 380, 458, 11, 50868], "temperature": 0.0, "avg_logprob": -0.13779062032699585, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0015460597351193428}, {"id": 792, "seek": 418324, "start": 4193.32, "end": 4199.0, "text": " at least in the causal inference literature, I'm, I'm not aware of any previous methods.", "tokens": [50868, 412, 1935, 294, 264, 38755, 38253, 10394, 11, 286, 478, 11, 286, 478, 406, 3650, 295, 604, 3894, 7150, 13, 51152], "temperature": 0.0, "avg_logprob": -0.13779062032699585, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0015460597351193428}, {"id": 793, "seek": 418324, "start": 4199.0, "end": 4204.12, "text": " I would say no, because that, I mean, that's the highly cited paper. So I would say they came out", "tokens": [51152, 286, 576, 584, 572, 11, 570, 300, 11, 286, 914, 11, 300, 311, 264, 5405, 30134, 3035, 13, 407, 286, 576, 584, 436, 1361, 484, 51408], "temperature": 0.0, "avg_logprob": -0.13779062032699585, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0015460597351193428}, {"id": 794, "seek": 418324, "start": 4204.12, "end": 4209.24, "text": " with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn", "tokens": [51408, 365, 300, 1558, 13, 3153, 13, 865, 13, 663, 311, 11, 300, 311, 1596, 1481, 300, 291, 393, 360, 16235, 23475, 293, 1466, 51664], "temperature": 0.0, "avg_logprob": -0.13779062032699585, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0015460597351193428}, {"id": 795, "seek": 420924, "start": 4209.24, "end": 4215.0, "text": " the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like", "tokens": [50364, 264, 3877, 13, 286, 519, 300, 311, 257, 11, 300, 311, 257, 588, 4005, 6532, 13, 865, 13, 4803, 309, 311, 411, 50652], "temperature": 0.0, "avg_logprob": -0.07801881727281508, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0012841816060245037}, {"id": 796, "seek": 420924, "start": 4215.0, "end": 4221.0, "text": " when you look at when different features of Bayesian inference and causal inference became", "tokens": [50652, 562, 291, 574, 412, 562, 819, 4122, 295, 7840, 42434, 38253, 293, 38755, 38253, 3062, 50952], "temperature": 0.0, "avg_logprob": -0.07801881727281508, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0012841816060245037}, {"id": 797, "seek": 420924, "start": 4221.0, "end": 4228.679999999999, "text": " available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal", "tokens": [50952, 2435, 11, 309, 311, 534, 12802, 13, 1743, 983, 11, 983, 11, 983, 6132, 380, 341, 668, 1096, 833, 257, 7840, 42434, 38755, 51336], "temperature": 0.0, "avg_logprob": -0.07801881727281508, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0012841816060245037}, {"id": 798, "seek": 420924, "start": 4228.679999999999, "end": 4235.8, "text": " modeling framework? It's like, because there's only been like five to 25 years of this happening.", "tokens": [51336, 15983, 8388, 30, 467, 311, 411, 11, 570, 456, 311, 787, 668, 411, 1732, 281, 3552, 924, 295, 341, 2737, 13, 51692], "temperature": 0.0, "avg_logprob": -0.07801881727281508, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0012841816060245037}, {"id": 799, "seek": 423580, "start": 4236.68, "end": 4242.360000000001, "text": " And so that's very, very short. And also it's relatively technical. So there's relatively", "tokens": [50408, 400, 370, 300, 311, 588, 11, 588, 2099, 13, 400, 611, 309, 311, 7226, 6191, 13, 407, 456, 311, 7226, 50692], "temperature": 0.0, "avg_logprob": -0.12354343248450238, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0023591392673552036}, {"id": 800, "seek": 423580, "start": 4242.360000000001, "end": 4248.28, "text": " few research groups engaging in it. And it's just really cool what it's enabling.", "tokens": [50692, 1326, 2132, 3935, 11268, 294, 309, 13, 400, 309, 311, 445, 534, 1627, 437, 309, 311, 23148, 13, 50988], "temperature": 0.0, "avg_logprob": -0.12354343248450238, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0023591392673552036}, {"id": 801, "seek": 423580, "start": 4249.8, "end": 4254.2, "text": " No, yes, yes, exactly. I mean, that's also, I think the exciting part of this field a little bit", "tokens": [51064, 883, 11, 2086, 11, 2086, 11, 2293, 13, 286, 914, 11, 300, 311, 611, 11, 286, 519, 264, 4670, 644, 295, 341, 2519, 257, 707, 857, 51284], "temperature": 0.0, "avg_logprob": -0.12354343248450238, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0023591392673552036}, {"id": 802, "seek": 423580, "start": 4254.2, "end": 4259.64, "text": " that is, I mean, there are definitely break breakthroughs out there that, that still have", "tokens": [51284, 300, 307, 11, 286, 914, 11, 456, 366, 2138, 1821, 22397, 82, 484, 456, 300, 11, 300, 920, 362, 51556], "temperature": 0.0, "avg_logprob": -0.12354343248450238, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0023591392673552036}, {"id": 803, "seek": 423580, "start": 4259.64, "end": 4265.08, "text": " to be discovered and probably like, for example, like, or as much as a breakthrough that paper was", "tokens": [51556, 281, 312, 6941, 293, 1391, 411, 11, 337, 1365, 11, 411, 11, 420, 382, 709, 382, 257, 22397, 300, 3035, 390, 51828], "temperature": 0.0, "avg_logprob": -0.12354343248450238, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0023591392673552036}, {"id": 804, "seek": 426580, "start": 4265.88, "end": 4272.92, "text": " they found like, they simply found out the right prior for acyclic structures. Okay, it's a", "tokens": [50368, 436, 1352, 411, 11, 436, 2935, 1352, 484, 264, 558, 4059, 337, 696, 88, 66, 1050, 9227, 13, 1033, 11, 309, 311, 257, 50720], "temperature": 0.0, "avg_logprob": -0.1740862288565006, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0067757596261799335}, {"id": 805, "seek": 426580, "start": 4274.12, "end": 4279.64, "text": " yeah, I mean, I, I don't know exactly, but it may be an idea that you have in one afternoon.", "tokens": [50780, 1338, 11, 286, 914, 11, 286, 11, 286, 500, 380, 458, 2293, 11, 457, 309, 815, 312, 364, 1558, 300, 291, 362, 294, 472, 6499, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1740862288565006, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0067757596261799335}, {"id": 806, "seek": 426580, "start": 4279.64, "end": 4284.52, "text": " I don't know about the story of the, how the authors came up with that, but could potentially be", "tokens": [51056, 286, 500, 380, 458, 466, 264, 1657, 295, 264, 11, 577, 264, 16552, 1361, 493, 365, 300, 11, 457, 727, 7263, 312, 51300], "temperature": 0.0, "avg_logprob": -0.1740862288565006, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0067757596261799335}, {"id": 807, "seek": 426580, "start": 4284.52, "end": 4289.08, "text": " that if they, they are there at the whiteboard, you're like, Oh, that actually works. That's a", "tokens": [51300, 300, 498, 436, 11, 436, 366, 456, 412, 264, 2418, 3787, 11, 291, 434, 411, 11, 876, 11, 300, 767, 1985, 13, 663, 311, 257, 51528], "temperature": 0.0, "avg_logprob": -0.1740862288565006, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0067757596261799335}, {"id": 808, "seek": 428908, "start": 4289.08, "end": 4296.28, "text": " huge breakthrough. And I simply defined the prior. And also a lot of these breakthroughs,", "tokens": [50364, 2603, 22397, 13, 400, 286, 2935, 7642, 264, 4059, 13, 400, 611, 257, 688, 295, 613, 22397, 82, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1468677742536678, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.07260826975107193}, {"id": 809, "seek": 428908, "start": 4297.16, "end": 4306.12, "text": " they, they don't just stack. It's not like a, a tower of blocks, they layer and they compose.", "tokens": [50768, 436, 11, 436, 500, 380, 445, 8630, 13, 467, 311, 406, 411, 257, 11, 257, 10567, 295, 8474, 11, 436, 4583, 293, 436, 35925, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1468677742536678, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.07260826975107193}, {"id": 810, "seek": 428908, "start": 4306.92, "end": 4311.0, "text": " So then something will be generalized to generalized coordinates or generalized", "tokens": [51256, 407, 550, 746, 486, 312, 44498, 281, 44498, 21056, 420, 44498, 51460], "temperature": 0.0, "avg_logprob": -0.1468677742536678, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.07260826975107193}, {"id": 811, "seek": 428908, "start": 4311.0, "end": 4317.8, "text": " synchrony or arbitrarily large graphs or sensor fusion with multimodal inputs. And it's like those", "tokens": [51460, 19331, 88, 420, 19071, 3289, 2416, 24877, 420, 10200, 23100, 365, 32972, 378, 304, 15743, 13, 400, 309, 311, 411, 729, 51800], "temperature": 0.0, "avg_logprob": -0.1468677742536678, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.07260826975107193}, {"id": 812, "seek": 431780, "start": 4317.8, "end": 4324.92, "text": " all blend in really satisfying and effective ways. So, so even little things that again,", "tokens": [50364, 439, 10628, 294, 534, 18348, 293, 4942, 2098, 13, 407, 11, 370, 754, 707, 721, 300, 797, 11, 50720], "temperature": 0.0, "avg_logprob": -0.13655173778533936, "compression_ratio": 1.5093457943925233, "no_speech_prob": 0.001838240772485733}, {"id": 813, "seek": 431780, "start": 4324.92, "end": 4333.16, "text": " someone can just come up with in a moment can really have impact. Okay, ML Don says,", "tokens": [50720, 1580, 393, 445, 808, 493, 365, 294, 257, 1623, 393, 534, 362, 2712, 13, 1033, 11, 21601, 1468, 1619, 11, 51132], "temperature": 0.0, "avg_logprob": -0.13655173778533936, "compression_ratio": 1.5093457943925233, "no_speech_prob": 0.001838240772485733}, {"id": 814, "seek": 431780, "start": 4333.16, "end": 4338.2, "text": " thanks a lot for asking my questions and thanks a million to Tomaso for the inspiring presentation.", "tokens": [51132, 3231, 257, 688, 337, 3365, 452, 1651, 293, 3231, 257, 2459, 281, 5041, 35281, 337, 264, 15883, 5860, 13, 51384], "temperature": 0.0, "avg_logprob": -0.13655173778533936, "compression_ratio": 1.5093457943925233, "no_speech_prob": 0.001838240772485733}, {"id": 815, "seek": 431780, "start": 4338.2, "end": 4342.12, "text": " So nice. Thank you very much. And then Bert asks,", "tokens": [51384, 407, 1481, 13, 1044, 291, 588, 709, 13, 400, 550, 29594, 8962, 11, 51580], "temperature": 0.0, "avg_logprob": -0.13655173778533936, "compression_ratio": 1.5093457943925233, "no_speech_prob": 0.001838240772485733}, {"id": 816, "seek": 434212, "start": 4343.08, "end": 4348.12, "text": " how would language models using predictive coding differ from those using transformers?", "tokens": [50412, 577, 576, 2856, 5245, 1228, 35521, 17720, 743, 490, 729, 1228, 4088, 433, 30, 50664], "temperature": 0.0, "avg_logprob": -0.2210285232727786, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.013416427187621593}, {"id": 817, "seek": 434212, "start": 4352.5199999999995, "end": 4357.32, "text": " Okay, I think that actually, if I would have to build today a language model using predictive", "tokens": [50884, 1033, 11, 286, 519, 300, 767, 11, 498, 286, 576, 362, 281, 1322, 965, 257, 2856, 2316, 1228, 35521, 51124], "temperature": 0.0, "avg_logprob": -0.2210285232727786, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.013416427187621593}, {"id": 818, "seek": 434212, "start": 4357.32, "end": 4361.72, "text": " coding, I would still use transformers. So the idea is that, for example, if you have a,", "tokens": [51124, 17720, 11, 286, 576, 920, 764, 4088, 433, 13, 407, 264, 1558, 307, 300, 11, 337, 1365, 11, 498, 291, 362, 257, 11, 51344], "temperature": 0.0, "avg_logprob": -0.2210285232727786, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.013416427187621593}, {"id": 819, "seek": 434212, "start": 4362.76, "end": 4369.48, "text": " let's say this hierarchical graphical model, or this hierarchical Bayesian network,", "tokens": [51396, 718, 311, 584, 341, 35250, 804, 35942, 2316, 11, 420, 341, 35250, 804, 7840, 42434, 3209, 11, 51732], "temperature": 0.0, "avg_logprob": -0.2210285232727786, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.013416427187621593}, {"id": 820, "seek": 436948, "start": 4370.44, "end": 4376.04, "text": " I've defined in the, in the very first slides, one arrow to encode a function, which is the linear", "tokens": [50412, 286, 600, 7642, 294, 264, 11, 294, 264, 588, 700, 9788, 11, 472, 11610, 281, 2058, 1429, 257, 2445, 11, 597, 307, 264, 8213, 50692], "temperature": 0.0, "avg_logprob": -0.12928930583753084, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0031963910441845655}, {"id": 821, "seek": 436948, "start": 4376.04, "end": 4381.719999999999, "text": " map. Okay, so one arrow was simply the multiplication of a, of the vector encoded in the latent", "tokens": [50692, 4471, 13, 1033, 11, 370, 472, 11610, 390, 2935, 264, 27290, 295, 257, 11, 295, 264, 8062, 2058, 12340, 294, 264, 48994, 50976], "temperature": 0.0, "avg_logprob": -0.12928930583753084, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0031963910441845655}, {"id": 822, "seek": 436948, "start": 4381.719999999999, "end": 4388.2, "text": " variables times the, this weight matrix that you can then make non-linear and things like that.", "tokens": [50976, 9102, 1413, 264, 11, 341, 3364, 8141, 300, 291, 393, 550, 652, 2107, 12, 28263, 293, 721, 411, 300, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12928930583753084, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0031963910441845655}, {"id": 823, "seek": 436948, "start": 4388.2, "end": 4392.2, "text": " But that can be actually something much more complex. The, the function encoded in the arrow", "tokens": [51300, 583, 300, 393, 312, 767, 746, 709, 544, 3997, 13, 440, 11, 264, 2445, 2058, 12340, 294, 264, 11610, 51500], "temperature": 0.0, "avg_logprob": -0.12928930583753084, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0031963910441845655}, {"id": 824, "seek": 439220, "start": 4392.28, "end": 4399.5599999999995, "text": " can be a convolution, can be an attention mechanism. So, so actually how I would do it,", "tokens": [50368, 393, 312, 257, 45216, 11, 393, 312, 364, 3202, 7513, 13, 407, 11, 370, 767, 577, 286, 576, 360, 309, 11, 50732], "temperature": 0.0, "avg_logprob": -0.11761724223261294, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.014607755467295647}, {"id": 825, "seek": 439220, "start": 4399.5599999999995, "end": 4406.679999999999, "text": " I will still use the, I mean, which is actually the way we did it in, in, in the Oxford group last", "tokens": [50732, 286, 486, 920, 764, 264, 11, 286, 914, 11, 597, 307, 767, 264, 636, 321, 630, 309, 294, 11, 294, 11, 294, 264, 24786, 1594, 1036, 51088], "temperature": 0.0, "avg_logprob": -0.11761724223261294, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.014607755467295647}, {"id": 826, "seek": 439220, "start": 4406.679999999999, "end": 4412.84, "text": " year is that we, we had exactly the structure. Every arrow is a transformer now. So one is", "tokens": [51088, 1064, 307, 300, 321, 11, 321, 632, 2293, 264, 3877, 13, 2048, 11610, 307, 257, 31782, 586, 13, 407, 472, 307, 51396], "temperature": 0.0, "avg_logprob": -0.11761724223261294, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.014607755467295647}, {"id": 827, "seek": 439220, "start": 4412.84, "end": 4417.24, "text": " the attention mechanism and the, the next one is the feed forward network as transformers.", "tokens": [51396, 264, 3202, 7513, 293, 264, 11, 264, 958, 472, 307, 264, 3154, 2128, 3209, 382, 4088, 433, 13, 51616], "temperature": 0.0, "avg_logprob": -0.11761724223261294, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.014607755467295647}, {"id": 828, "seek": 439220, "start": 4418.28, "end": 4422.04, "text": " And basically the only difference that you have is that those variables you want to compute the", "tokens": [51668, 400, 1936, 264, 787, 2649, 300, 291, 362, 307, 300, 729, 9102, 291, 528, 281, 14722, 264, 51856], "temperature": 0.0, "avg_logprob": -0.11761724223261294, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.014607755467295647}, {"id": 829, "seek": 442204, "start": 4422.04, "end": 4427.24, "text": " posterior and you make those posterior's independence, independent via, via mean field", "tokens": [50364, 33529, 293, 291, 652, 729, 33529, 311, 14640, 11, 6695, 5766, 11, 5766, 914, 2519, 50624], "temperature": 0.0, "avg_logprob": -0.1628215823854719, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.003509539645165205}, {"id": 830, "seek": 442204, "start": 4427.24, "end": 4432.36, "text": " approximation. So basically you follow all the steps that allow you to, to converge to the", "tokens": [50624, 28023, 13, 407, 1936, 291, 1524, 439, 264, 4439, 300, 2089, 291, 281, 11, 281, 41881, 281, 264, 50880], "temperature": 0.0, "avg_logprob": -0.1628215823854719, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.003509539645165205}, {"id": 831, "seek": 442204, "start": 4432.36, "end": 4437.4, "text": " very, to the variational free energy of creative coding. But the, the way, the way you compute", "tokens": [50880, 588, 11, 281, 264, 3034, 1478, 1737, 2281, 295, 5880, 17720, 13, 583, 264, 11, 264, 636, 11, 264, 636, 291, 14722, 51132], "temperature": 0.0, "avg_logprob": -0.1628215823854719, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.003509539645165205}, {"id": 832, "seek": 442204, "start": 4437.4, "end": 4446.12, "text": " predictions and the way you, you send signals back is a, is done via transformer. So I will", "tokens": [51132, 21264, 293, 264, 636, 291, 11, 291, 2845, 12354, 646, 307, 257, 11, 307, 1096, 5766, 31782, 13, 407, 286, 486, 51568], "temperature": 0.0, "avg_logprob": -0.1628215823854719, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.003509539645165205}, {"id": 833, "seek": 442204, "start": 4446.12, "end": 4451.72, "text": " still use transformers in general. I mean, they work so well that I, I don't think that we can be", "tokens": [51568, 920, 764, 4088, 433, 294, 2674, 13, 286, 914, 11, 436, 589, 370, 731, 300, 286, 11, 286, 500, 380, 519, 300, 321, 393, 312, 51848], "temperature": 0.0, "avg_logprob": -0.1628215823854719, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.003509539645165205}, {"id": 834, "seek": 445204, "start": 4452.12, "end": 4456.44, "text": " arrogant and say, oh no, I'm going to do it better via a purely predictive coding way.", "tokens": [50368, 30467, 293, 584, 11, 1954, 572, 11, 286, 478, 516, 281, 360, 309, 1101, 5766, 257, 17491, 35521, 17720, 636, 13, 50584], "temperature": 0.0, "avg_logprob": -0.230653200394068, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.0015555297723039985}, {"id": 835, "seek": 445204, "start": 4457.64, "end": 4461.72, "text": " Structure learning is a way to do it, but we'll still approximate transformers anyway.", "tokens": [50644, 745, 2885, 2539, 307, 257, 636, 281, 360, 309, 11, 457, 321, 603, 920, 30874, 4088, 433, 4033, 13, 50848], "temperature": 0.0, "avg_logprob": -0.230653200394068, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.0015555297723039985}, {"id": 836, "seek": 445204, "start": 4462.5199999999995, "end": 4466.68, "text": " So you said structure learning would approximate the transformer approach?", "tokens": [50888, 407, 291, 848, 3877, 2539, 576, 30874, 264, 31782, 3109, 30, 51096], "temperature": 0.0, "avg_logprob": -0.230653200394068, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.0015555297723039985}, {"id": 837, "seek": 445204, "start": 4467.56, "end": 4473.24, "text": " Yes. Destruction learning I mentioned earlier in, when, when someone asked the similarities", "tokens": [51140, 1079, 13, 16339, 3826, 2539, 286, 2835, 3071, 294, 11, 562, 11, 562, 1580, 2351, 264, 24197, 51424], "temperature": 0.0, "avg_logprob": -0.230653200394068, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.0015555297723039985}, {"id": 838, "seek": 447324, "start": 4473.24, "end": 4476.04, "text": " between predictive coding and the attention mechanism.", "tokens": [50364, 1296, 35521, 17720, 293, 264, 3202, 7513, 13, 50504], "temperature": 0.0, "avg_logprob": -0.14636578057941638, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.03249704837799072}, {"id": 839, "seek": 447324, "start": 4478.36, "end": 4486.04, "text": " Very, yeah, very interesting. One thing I am wondering from MLBong, I could not see the", "tokens": [50620, 4372, 11, 1338, 11, 588, 1880, 13, 1485, 551, 286, 669, 6359, 490, 21601, 33, 556, 11, 286, 727, 406, 536, 264, 51004], "temperature": 0.0, "avg_logprob": -0.14636578057941638, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.03249704837799072}, {"id": 840, "seek": 447324, "start": 4486.04, "end": 4490.599999999999, "text": " concept of depth in the predictive coding networks you mentioned. Most likely I missed it. The", "tokens": [51004, 3410, 295, 7161, 294, 264, 35521, 17720, 9590, 291, 2835, 13, 4534, 3700, 286, 6721, 309, 13, 440, 51232], "temperature": 0.0, "avg_logprob": -0.14636578057941638, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.03249704837799072}, {"id": 841, "seek": 447324, "start": 4490.599999999999, "end": 4497.8, "text": " definition provided for predictive coding involved the concept of depth. What did you mean by depth?", "tokens": [51232, 7123, 5649, 337, 35521, 17720, 3288, 264, 3410, 295, 7161, 13, 708, 630, 291, 914, 538, 7161, 30, 51592], "temperature": 0.0, "avg_logprob": -0.14636578057941638, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.03249704837799072}, {"id": 842, "seek": 449780, "start": 4498.360000000001, "end": 4505.56, "text": " No, yes, it's true. It's a, because the standard definition, as I said, multiple times is a,", "tokens": [50392, 883, 11, 2086, 11, 309, 311, 2074, 13, 467, 311, 257, 11, 570, 264, 3832, 7123, 11, 382, 286, 848, 11, 3866, 1413, 307, 257, 11, 50752], "temperature": 0.0, "avg_logprob": -0.2256611890571062, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0038231329526752234}, {"id": 843, "seek": 449780, "start": 4505.56, "end": 4509.24, "text": " is hierarchical. You have predictions going one directions and prediction error going the", "tokens": [50752, 307, 35250, 804, 13, 509, 362, 21264, 516, 472, 11095, 293, 17630, 6713, 516, 264, 50936], "temperature": 0.0, "avg_logprob": -0.2256611890571062, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0038231329526752234}, {"id": 844, "seek": 449780, "start": 4509.24, "end": 4515.72, "text": " opposite direction. Basically, what, what we did in, in this paper and also in the last one in", "tokens": [50936, 6182, 3513, 13, 8537, 11, 437, 11, 437, 321, 630, 294, 11, 294, 341, 3035, 293, 611, 294, 264, 1036, 472, 294, 51260], "temperature": 0.0, "avg_logprob": -0.2256611890571062, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0038231329526752234}, {"id": 845, "seek": 449780, "start": 4516.52, "end": 4520.360000000001, "text": " which is called the learning on arbitrary graph topologies via predictive coding", "tokens": [51300, 597, 307, 1219, 264, 2539, 322, 23211, 4295, 1192, 6204, 5766, 35521, 17720, 51492], "temperature": 0.0, "avg_logprob": -0.2256611890571062, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0038231329526752234}, {"id": 846, "seek": 452036, "start": 4521.08, "end": 4530.839999999999, "text": " is that we can consider depth like as a, as independent, basically pair of latent variable,", "tokens": [50400, 307, 300, 321, 393, 1949, 7161, 411, 382, 257, 11, 382, 6695, 11, 1936, 6119, 295, 48994, 7006, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1527028527370719, "compression_ratio": 1.75, "no_speech_prob": 0.011289065703749657}, {"id": 847, "seek": 452036, "start": 4530.839999999999, "end": 4535.639999999999, "text": " latent variable, and arrow. And you have predictions going that direction and prediction", "tokens": [50888, 48994, 7006, 11, 293, 11610, 13, 400, 291, 362, 21264, 516, 300, 3513, 293, 17630, 51128], "temperature": 0.0, "avg_logprob": -0.1527028527370719, "compression_ratio": 1.75, "no_speech_prob": 0.011289065703749657}, {"id": 848, "seek": 452036, "start": 4535.639999999999, "end": 4541.96, "text": " error going the other. But then you can compose these in how many, a lot of ways. So you can,", "tokens": [51128, 6713, 516, 264, 661, 13, 583, 550, 291, 393, 35925, 613, 294, 577, 867, 11, 257, 688, 295, 2098, 13, 407, 291, 393, 11, 51444], "temperature": 0.0, "avg_logprob": -0.1527028527370719, "compression_ratio": 1.75, "no_speech_prob": 0.011289065703749657}, {"id": 849, "seek": 452036, "start": 4542.92, "end": 4547.96, "text": " you can, so basically this composition doesn't have to be hierarchical in the end.", "tokens": [51492, 291, 393, 11, 370, 1936, 341, 12686, 1177, 380, 362, 281, 312, 35250, 804, 294, 264, 917, 13, 51744], "temperature": 0.0, "avg_logprob": -0.1527028527370719, "compression_ratio": 1.75, "no_speech_prob": 0.011289065703749657}, {"id": 850, "seek": 454796, "start": 4548.68, "end": 4551.72, "text": " Can have cycles. So then you can, for example, plug in another,", "tokens": [50400, 1664, 362, 17796, 13, 407, 550, 291, 393, 11, 337, 1365, 11, 5452, 294, 1071, 11, 50552], "temperature": 0.0, "avg_logprob": -0.11777515411376953, "compression_ratio": 1.8755364806866952, "no_speech_prob": 0.004061977379024029}, {"id": 851, "seek": 454796, "start": 4553.0, "end": 4557.4800000000005, "text": " another latent variable to the first one, and then connect the other two. And you can have a", "tokens": [50616, 1071, 48994, 7006, 281, 264, 700, 472, 11, 293, 550, 1745, 264, 661, 732, 13, 400, 291, 393, 362, 257, 50840], "temperature": 0.0, "avg_logprob": -0.11777515411376953, "compression_ratio": 1.8755364806866952, "no_speech_prob": 0.004061977379024029}, {"id": 852, "seek": 454796, "start": 4557.4800000000005, "end": 4563.32, "text": " structure that is as entangled as you want. So for example, in the, in the other paper, we train", "tokens": [50840, 3877, 300, 307, 382, 948, 39101, 382, 291, 528, 13, 407, 337, 1365, 11, 294, 264, 11, 294, 264, 661, 3035, 11, 321, 3847, 51132], "temperature": 0.0, "avg_logprob": -0.11777515411376953, "compression_ratio": 1.8755364806866952, "no_speech_prob": 0.004061977379024029}, {"id": 853, "seek": 454796, "start": 4563.32, "end": 4569.0, "text": " the, a network that has the shape of a brain structure. So we have a lot of brain regions", "tokens": [51132, 264, 11, 257, 3209, 300, 575, 264, 3909, 295, 257, 3567, 3877, 13, 407, 321, 362, 257, 688, 295, 3567, 10682, 51416], "temperature": 0.0, "avg_logprob": -0.11777515411376953, "compression_ratio": 1.8755364806866952, "no_speech_prob": 0.004061977379024029}, {"id": 854, "seek": 454796, "start": 4569.0, "end": 4575.0, "text": " that are sparsely connected inside and sparsely connected among each other. And, and there's,", "tokens": [51416, 300, 366, 637, 685, 736, 4582, 1854, 293, 637, 685, 736, 4582, 3654, 1184, 661, 13, 400, 11, 293, 456, 311, 11, 51716], "temperature": 0.0, "avg_logprob": -0.11777515411376953, "compression_ratio": 1.8755364806866952, "no_speech_prob": 0.004061977379024029}, {"id": 855, "seek": 457500, "start": 4575.0, "end": 4578.76, "text": " there's nothing hierarchical there at the end, but you can still train it by minimizing", "tokens": [50364, 456, 311, 1825, 35250, 804, 456, 412, 264, 917, 11, 457, 291, 393, 920, 3847, 309, 538, 46608, 50552], "temperature": 0.0, "avg_logprob": -0.12940818891612763, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0005441018147394061}, {"id": 856, "seek": 457500, "start": 4578.76, "end": 4583.4, "text": " a ratio of free energy and by minimizing the, the total prediction error of the network.", "tokens": [50552, 257, 8509, 295, 1737, 2281, 293, 538, 46608, 264, 11, 264, 3217, 17630, 6713, 295, 264, 3209, 13, 50784], "temperature": 0.0, "avg_logprob": -0.12940818891612763, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0005441018147394061}, {"id": 857, "seek": 457500, "start": 4585.8, "end": 4595.64, "text": " So you could have for a given motif in a entangled graph, you might see three successive layers", "tokens": [50904, 407, 291, 727, 362, 337, 257, 2212, 39478, 294, 257, 948, 39101, 4295, 11, 291, 1062, 536, 1045, 48043, 7914, 51396], "temperature": 0.0, "avg_logprob": -0.12940818891612763, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0005441018147394061}, {"id": 858, "seek": 457500, "start": 4595.64, "end": 4599.24, "text": " that when you looked at them alone, you'd say, Oh, that's a three story building.", "tokens": [51396, 300, 562, 291, 2956, 412, 552, 3312, 11, 291, 1116, 584, 11, 876, 11, 300, 311, 257, 1045, 1657, 2390, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12940818891612763, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0005441018147394061}, {"id": 859, "seek": 457500, "start": 4599.24, "end": 4604.52, "text": " That's a three layer model that has a depth of three. But then when you take a bigger picture", "tokens": [51576, 663, 311, 257, 1045, 4583, 2316, 300, 575, 257, 7161, 295, 1045, 13, 583, 550, 562, 291, 747, 257, 3801, 3036, 51840], "temperature": 0.0, "avg_logprob": -0.12940818891612763, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0005441018147394061}, {"id": 860, "seek": 460500, "start": 4605.16, "end": 4610.84, "text": " there isn't like an explicit top or an explicit bottom to that network.", "tokens": [50372, 456, 1943, 380, 411, 364, 13691, 1192, 420, 364, 13691, 2767, 281, 300, 3209, 13, 50656], "temperature": 0.0, "avg_logprob": -0.14116002143697537, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0021019831765443087}, {"id": 861, "seek": 460500, "start": 4612.2, "end": 4615.88, "text": " Yes, exactly. And this is basically given by the, by the fact that every operation in", "tokens": [50724, 1079, 11, 2293, 13, 400, 341, 307, 1936, 2212, 538, 264, 11, 538, 264, 1186, 300, 633, 6916, 294, 50908], "temperature": 0.0, "avg_logprob": -0.14116002143697537, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0021019831765443087}, {"id": 862, "seek": 460500, "start": 4615.88, "end": 4621.64, "text": " creative coding networks is strictly local. So, so basically every message passing every", "tokens": [50908, 5880, 17720, 9590, 307, 20792, 2654, 13, 407, 11, 370, 1936, 633, 3636, 8437, 633, 51196], "temperature": 0.0, "avg_logprob": -0.14116002143697537, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0021019831765443087}, {"id": 863, "seek": 460500, "start": 4621.64, "end": 4626.76, "text": " prediction and every prediction error that you send, you only send it to the very nearby neurons.", "tokens": [51196, 17630, 293, 633, 17630, 6713, 300, 291, 2845, 11, 291, 787, 2845, 309, 281, 264, 588, 11184, 22027, 13, 51452], "temperature": 0.0, "avg_logprob": -0.14116002143697537, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0021019831765443087}, {"id": 864, "seek": 460500, "start": 4627.32, "end": 4631.16, "text": " Okay. And whether the global structure is actually hierarchical or not,", "tokens": [51480, 1033, 13, 400, 1968, 264, 4338, 3877, 307, 767, 35250, 804, 420, 406, 11, 51672], "temperature": 0.0, "avg_logprob": -0.14116002143697537, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0021019831765443087}, {"id": 865, "seek": 463116, "start": 4631.88, "end": 4634.84, "text": " the, the single message passing doesn't even see that.", "tokens": [50400, 264, 11, 264, 2167, 3636, 8437, 1177, 380, 754, 536, 300, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11789370450106534, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.001524448744021356}, {"id": 866, "seek": 463116, "start": 4637.48, "end": 4647.0, "text": " I guess that's sort of the hope for learning new model architectures is the space of what is", "tokens": [50680, 286, 2041, 300, 311, 1333, 295, 264, 1454, 337, 2539, 777, 2316, 6331, 1303, 307, 264, 1901, 295, 437, 307, 51156], "temperature": 0.0, "avg_logprob": -0.11789370450106534, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.001524448744021356}, {"id": 867, "seek": 463116, "start": 4647.88, "end": 4654.84, "text": " designed top down is very small and a lot of models in use today,", "tokens": [51200, 4761, 1192, 760, 307, 588, 1359, 293, 257, 688, 295, 5245, 294, 764, 965, 11, 51548], "temperature": 0.0, "avg_logprob": -0.11789370450106534, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.001524448744021356}, {"id": 868, "seek": 465484, "start": 4655.4800000000005, "end": 4662.6, "text": " albeit super effective models. Although you could ask effective per unit of compute or not,", "tokens": [50396, 43654, 1687, 4942, 5245, 13, 5780, 291, 727, 1029, 4942, 680, 4985, 295, 14722, 420, 406, 11, 50752], "temperature": 0.0, "avg_logprob": -0.10150173062183818, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.00572984479367733}, {"id": 869, "seek": 465484, "start": 4662.6, "end": 4667.32, "text": " that's a second level question. But a lot of effective models today do not have some of these", "tokens": [50752, 300, 311, 257, 1150, 1496, 1168, 13, 583, 257, 688, 295, 4942, 5245, 965, 360, 406, 362, 512, 295, 613, 50988], "temperature": 0.0, "avg_logprob": -0.10150173062183818, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.00572984479367733}, {"id": 870, "seek": 465484, "start": 4667.32, "end": 4675.24, "text": " properties of predictive coding networks, like their capacity to use only local computations,", "tokens": [50988, 7221, 295, 35521, 17720, 9590, 11, 411, 641, 6042, 281, 764, 787, 2654, 2807, 763, 11, 51384], "temperature": 0.0, "avg_logprob": -0.10150173062183818, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.00572984479367733}, {"id": 871, "seek": 467524, "start": 4675.88, "end": 4684.76, "text": " which gives biological realism or just spatio temporal realism, but also may provide a lot", "tokens": [50396, 597, 2709, 13910, 38484, 420, 445, 15000, 1004, 30881, 38484, 11, 457, 611, 815, 2893, 257, 688, 50840], "temperature": 0.0, "avg_logprob": -0.12335813185747932, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.012236015871167183}, {"id": 872, "seek": 467524, "start": 4684.76, "end": 4688.92, "text": " of advantages in like federated compute or distributed computing settings.", "tokens": [50840, 295, 14906, 294, 411, 38024, 770, 14722, 420, 12631, 15866, 6257, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12335813185747932, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.012236015871167183}, {"id": 873, "seek": 467524, "start": 4690.599999999999, "end": 4695.96, "text": " No, yes, exactly. I completely agree. I think the idea in general is that, and I don't know if", "tokens": [51132, 883, 11, 2086, 11, 2293, 13, 286, 2584, 3986, 13, 286, 519, 264, 1558, 294, 2674, 307, 300, 11, 293, 286, 500, 380, 458, 498, 51400], "temperature": 0.0, "avg_logprob": -0.12335813185747932, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.012236015871167183}, {"id": 874, "seek": 467524, "start": 4695.96, "end": 4700.12, "text": " that's going to be an advantage. I think it's very promising exactly for the reasons you said.", "tokens": [51400, 300, 311, 516, 281, 312, 364, 5002, 13, 286, 519, 309, 311, 588, 20257, 2293, 337, 264, 4112, 291, 848, 13, 51608], "temperature": 0.0, "avg_logprob": -0.12335813185747932, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.012236015871167183}, {"id": 875, "seek": 470012, "start": 4700.92, "end": 4704.92, "text": " And the reason is that today's models string with back propagation,", "tokens": [50404, 400, 264, 1778, 307, 300, 965, 311, 5245, 6798, 365, 646, 38377, 11, 50604], "temperature": 0.0, "avg_logprob": -0.214889736736522, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.008144847117364407}, {"id": 876, "seek": 470012, "start": 4704.92, "end": 4712.84, "text": " you can basically summarize them as a model string back propagation is a function,", "tokens": [50604, 291, 393, 1936, 20858, 552, 382, 257, 2316, 6798, 646, 38377, 307, 257, 2445, 11, 51000], "temperature": 0.0, "avg_logprob": -0.214889736736522, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.008144847117364407}, {"id": 877, "seek": 470012, "start": 4712.84, "end": 4717.64, "text": " because basically you have a map from input to output, and back propagation basically", "tokens": [51000, 570, 1936, 291, 362, 257, 4471, 490, 4846, 281, 5598, 11, 293, 646, 38377, 1936, 51240], "temperature": 0.0, "avg_logprob": -0.214889736736522, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.008144847117364407}, {"id": 878, "seek": 470012, "start": 4718.28, "end": 4725.16, "text": " spreads information back from its computational graph. So every neural network model used today", "tokens": [51272, 25728, 1589, 646, 490, 1080, 28270, 4295, 13, 407, 633, 18161, 3209, 2316, 1143, 965, 51616], "temperature": 0.0, "avg_logprob": -0.214889736736522, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.008144847117364407}, {"id": 879, "seek": 472516, "start": 4726.04, "end": 4731.96, "text": " is a function. While predictive coding and not only predictive coding, like the whole class of", "tokens": [50408, 307, 257, 2445, 13, 3987, 35521, 17720, 293, 406, 787, 35521, 17720, 11, 411, 264, 1379, 1508, 295, 50704], "temperature": 0.0, "avg_logprob": -0.16210160074354726, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.003617579583078623}, {"id": 880, "seek": 472516, "start": 4731.96, "end": 4738.36, "text": " functions, the class of methods that train in using local computations and actually work by", "tokens": [50704, 6828, 11, 264, 1508, 295, 7150, 300, 3847, 294, 1228, 2654, 2807, 763, 293, 767, 589, 538, 51024], "temperature": 0.0, "avg_logprob": -0.16210160074354726, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.003617579583078623}, {"id": 881, "seek": 472516, "start": 4738.36, "end": 4745.0, "text": " minimizing a global energy function, they're not limited to model functions from input to output.", "tokens": [51024, 46608, 257, 4338, 2281, 2445, 11, 436, 434, 406, 5567, 281, 2316, 6828, 490, 4846, 281, 5598, 13, 51356], "temperature": 0.0, "avg_logprob": -0.16210160074354726, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.003617579583078623}, {"id": 882, "seek": 472516, "start": 4745.0, "end": 4749.96, "text": " They actually model something that kind of resembles physical systems. So you have a physical", "tokens": [51356, 814, 767, 2316, 746, 300, 733, 295, 34433, 4001, 3652, 13, 407, 291, 362, 257, 4001, 51604], "temperature": 0.0, "avg_logprob": -0.16210160074354726, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.003617579583078623}, {"id": 883, "seek": 474996, "start": 4749.96, "end": 4756.2, "text": " system, you fix some values to whatever input you have, and you let the system converge,", "tokens": [50364, 1185, 11, 291, 3191, 512, 4190, 281, 2035, 4846, 291, 362, 11, 293, 291, 718, 264, 1185, 41881, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10019617165084434, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.0018060595029965043}, {"id": 884, "seek": 474996, "start": 4756.2, "end": 4761.32, "text": " and then you read some other value of neurons or variables that are supposed to be output.", "tokens": [50676, 293, 550, 291, 1401, 512, 661, 2158, 295, 22027, 420, 9102, 300, 366, 3442, 281, 312, 5598, 13, 50932], "temperature": 0.0, "avg_logprob": -0.10019617165084434, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.0018060595029965043}, {"id": 885, "seek": 474996, "start": 4761.32, "end": 4766.04, "text": " But this physical system doesn't have to be a feedforward map. It doesn't have to be a function", "tokens": [50932, 583, 341, 4001, 1185, 1177, 380, 362, 281, 312, 257, 3154, 13305, 4471, 13, 467, 1177, 380, 362, 281, 312, 257, 2445, 51168], "temperature": 0.0, "avg_logprob": -0.10019617165084434, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.0018060595029965043}, {"id": 886, "seek": 474996, "start": 4766.04, "end": 4772.2, "text": " that has an input space and an output space, and that's it. So the class of models that you can", "tokens": [51168, 300, 575, 364, 4846, 1901, 293, 364, 5598, 1901, 11, 293, 300, 311, 309, 13, 407, 264, 1508, 295, 5245, 300, 291, 393, 51476], "temperature": 0.0, "avg_logprob": -0.10019617165084434, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.0018060595029965043}, {"id": 887, "seek": 474996, "start": 4772.2, "end": 4778.76, "text": " learn is also basically you can see like feedforward models and functions, and then a much bigger", "tokens": [51476, 1466, 307, 611, 1936, 291, 393, 536, 411, 3154, 13305, 5245, 293, 6828, 11, 293, 550, 257, 709, 3801, 51804], "temperature": 0.0, "avg_logprob": -0.10019617165084434, "compression_ratio": 1.8685258964143425, "no_speech_prob": 0.0018060595029965043}, {"id": 888, "seek": 477876, "start": 4778.76, "end": 4783.64, "text": " class, which is that of physical systems. Whether there's something interesting out here, I don't", "tokens": [50364, 1508, 11, 597, 307, 300, 295, 4001, 3652, 13, 8503, 456, 311, 746, 1880, 484, 510, 11, 286, 500, 380, 50608], "temperature": 0.0, "avg_logprob": -0.13098864371959978, "compression_ratio": 1.8189300411522633, "no_speech_prob": 0.008872423321008682}, {"id": 889, "seek": 477876, "start": 4783.64, "end": 4788.280000000001, "text": " know yet, because the functions are working extremely well. We are seeing those days with", "tokens": [50608, 458, 1939, 11, 570, 264, 6828, 366, 1364, 4664, 731, 13, 492, 366, 2577, 729, 1708, 365, 50840], "temperature": 0.0, "avg_logprob": -0.13098864371959978, "compression_ratio": 1.8189300411522633, "no_speech_prob": 0.008872423321008682}, {"id": 890, "seek": 477876, "start": 4788.280000000001, "end": 4794.4400000000005, "text": " back propagation, they work crazy well. So yeah, I don't know if there's anything interesting in", "tokens": [50840, 646, 38377, 11, 436, 589, 3219, 731, 13, 407, 1338, 11, 286, 500, 380, 458, 498, 456, 311, 1340, 1880, 294, 51148], "temperature": 0.0, "avg_logprob": -0.13098864371959978, "compression_ratio": 1.8189300411522633, "no_speech_prob": 0.008872423321008682}, {"id": 891, "seek": 477876, "start": 4794.4400000000005, "end": 4800.52, "text": " the big part, but the big part is quite big. There are a lot of models that you cannot", "tokens": [51148, 264, 955, 644, 11, 457, 264, 955, 644, 307, 1596, 955, 13, 821, 366, 257, 688, 295, 5245, 300, 291, 2644, 51452], "temperature": 0.0, "avg_logprob": -0.13098864371959978, "compression_ratio": 1.8189300411522633, "no_speech_prob": 0.008872423321008682}, {"id": 892, "seek": 477876, "start": 4801.16, "end": 4803.8, "text": " train with back propagation, and you can train with predictive coding,", "tokens": [51484, 3847, 365, 646, 38377, 11, 293, 291, 393, 3847, 365, 35521, 17720, 11, 51616], "temperature": 0.0, "avg_logprob": -0.13098864371959978, "compression_ratio": 1.8189300411522633, "no_speech_prob": 0.008872423321008682}, {"id": 893, "seek": 480380, "start": 4804.68, "end": 4811.24, "text": " or a background propagation or other methods. That is super interesting. Certainly biological", "tokens": [50408, 420, 257, 3678, 38377, 420, 661, 7150, 13, 663, 307, 1687, 1880, 13, 16628, 13910, 50736], "temperature": 0.0, "avg_logprob": -0.160079581828057, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.004534765146672726}, {"id": 894, "seek": 480380, "start": 4811.24, "end": 4818.2, "text": " systems, physical systems solve all kinds of interesting problems. But there's still no free", "tokens": [50736, 3652, 11, 4001, 3652, 5039, 439, 3685, 295, 1880, 2740, 13, 583, 456, 311, 920, 572, 1737, 51084], "temperature": 0.0, "avg_logprob": -0.160079581828057, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.004534765146672726}, {"id": 895, "seek": 480380, "start": 4818.2, "end": 4823.64, "text": " lunch, and ant species does really well in this environment might not do very well in another", "tokens": [51084, 6349, 11, 293, 2511, 6172, 775, 534, 731, 294, 341, 2823, 1062, 406, 360, 588, 731, 294, 1071, 51356], "temperature": 0.0, "avg_logprob": -0.160079581828057, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.004534765146672726}, {"id": 896, "seek": 480380, "start": 4823.64, "end": 4831.0, "text": " environment. And so out there in the in the hinterlands, there might be some really unique", "tokens": [51356, 2823, 13, 400, 370, 484, 456, 294, 264, 294, 264, 23219, 10230, 11, 456, 1062, 312, 512, 534, 3845, 51724], "temperature": 0.0, "avg_logprob": -0.160079581828057, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.004534765146672726}, {"id": 897, "seek": 483100, "start": 4831.08, "end": 4837.08, "text": " special algorithms that are not well described by being a function,", "tokens": [50368, 2121, 14642, 300, 366, 406, 731, 7619, 538, 885, 257, 2445, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11456029645858272, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0012997844023630023}, {"id": 898, "seek": 483100, "start": 4838.52, "end": 4848.04, "text": " yet still provide like a procedural way to to implement heuristics, which might be extremely,", "tokens": [50740, 1939, 920, 2893, 411, 257, 43951, 636, 281, 281, 4445, 415, 374, 6006, 11, 597, 1062, 312, 4664, 11, 51216], "temperature": 0.0, "avg_logprob": -0.11456029645858272, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0012997844023630023}, {"id": 899, "seek": 483100, "start": 4848.04, "end": 4854.92, "text": " extremely effective. No, yes, yes, exactly. And yeah, and I think this has been most of my", "tokens": [51216, 4664, 4942, 13, 883, 11, 2086, 11, 2086, 11, 2293, 13, 400, 1338, 11, 293, 286, 519, 341, 575, 668, 881, 295, 452, 51560], "temperature": 0.0, "avg_logprob": -0.11456029645858272, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0012997844023630023}, {"id": 900, "seek": 485492, "start": 4855.72, "end": 4861.64, "text": " focus of research during my PhD, for example, like finding this application that is like out", "tokens": [50404, 1879, 295, 2132, 1830, 452, 14476, 11, 337, 1365, 11, 411, 5006, 341, 3861, 300, 307, 411, 484, 50700], "temperature": 0.0, "avg_logprob": -0.10055246497645523, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.030170438811182976}, {"id": 901, "seek": 485492, "start": 4861.64, "end": 4872.04, "text": " here and not inside the the functions. Cool. Well, where does this work go from here? Like,", "tokens": [50700, 510, 293, 406, 1854, 264, 264, 6828, 13, 8561, 13, 1042, 11, 689, 775, 341, 589, 352, 490, 510, 30, 1743, 11, 51220], "temperature": 0.0, "avg_logprob": -0.10055246497645523, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.030170438811182976}, {"id": 902, "seek": 485492, "start": 4872.04, "end": 4878.6, "text": " what directions are you excited about? And how do you see people in the active inference ecosystem", "tokens": [51220, 437, 11095, 366, 291, 2919, 466, 30, 400, 577, 360, 291, 536, 561, 294, 264, 4967, 38253, 11311, 51548], "temperature": 0.0, "avg_logprob": -0.10055246497645523, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.030170438811182976}, {"id": 903, "seek": 487860, "start": 4878.6, "end": 4885.160000000001, "text": " getting involved in this type of work? I think every probably the most promising", "tokens": [50364, 1242, 3288, 294, 341, 2010, 295, 589, 30, 286, 519, 633, 1391, 264, 881, 20257, 50692], "temperature": 0.0, "avg_logprob": -0.13262602794601255, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.008296238258481026}, {"id": 904, "seek": 487860, "start": 4885.72, "end": 4892.76, "text": " direction, which is something maybe I would like to explore a little bit is to, as I said,", "tokens": [50720, 3513, 11, 597, 307, 746, 1310, 286, 576, 411, 281, 6839, 257, 707, 857, 307, 281, 11, 382, 286, 848, 11, 51072], "temperature": 0.0, "avg_logprob": -0.13262602794601255, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.008296238258481026}, {"id": 905, "seek": 487860, "start": 4892.76, "end": 4899.64, "text": " there is to go behind statistical models. So everything I've shown so far is about static", "tokens": [51072, 456, 307, 281, 352, 2261, 22820, 5245, 13, 407, 1203, 286, 600, 4898, 370, 1400, 307, 466, 13437, 51416], "temperature": 0.0, "avg_logprob": -0.13262602794601255, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.008296238258481026}, {"id": 906, "seek": 487860, "start": 4899.64, "end": 4905.56, "text": " data. So the data don't change over time, there's no time inside the definition of", "tokens": [51416, 1412, 13, 407, 264, 1412, 500, 380, 1319, 670, 565, 11, 456, 311, 572, 565, 1854, 264, 7123, 295, 51712], "temperature": 0.0, "avg_logprob": -0.13262602794601255, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.008296238258481026}, {"id": 907, "seek": 490556, "start": 4905.56, "end": 4910.200000000001, "text": " predictive coding as it is as I presented it here. However, you can, for example,", "tokens": [50364, 35521, 17720, 382, 309, 307, 382, 286, 8212, 309, 510, 13, 2908, 11, 291, 393, 11, 337, 1365, 11, 50596], "temperature": 0.0, "avg_logprob": -0.14001754393060523, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0029968766029924154}, {"id": 908, "seek": 490556, "start": 4910.200000000001, "end": 4915.400000000001, "text": " generalize predictive coding to to work with temporal data using generalized coordinates,", "tokens": [50596, 2674, 1125, 35521, 17720, 281, 281, 589, 365, 30881, 1412, 1228, 44498, 21056, 11, 50856], "temperature": 0.0, "avg_logprob": -0.14001754393060523, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0029968766029924154}, {"id": 909, "seek": 490556, "start": 4915.400000000001, "end": 4923.0, "text": " as you mentioned earlier, by by presenting it as a as a Kalman Kalman filter generative model.", "tokens": [50856, 382, 291, 2835, 3071, 11, 538, 538, 15578, 309, 382, 257, 382, 257, 12655, 1601, 12655, 1601, 6608, 1337, 1166, 2316, 13, 51236], "temperature": 0.0, "avg_logprob": -0.14001754393060523, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0029968766029924154}, {"id": 910, "seek": 490556, "start": 4924.280000000001, "end": 4930.4400000000005, "text": " And and that's where, for example, the causal inference direction could be very useful,", "tokens": [51300, 400, 293, 300, 311, 689, 11, 337, 1365, 11, 264, 38755, 38253, 3513, 727, 312, 588, 4420, 11, 51608], "temperature": 0.0, "avg_logprob": -0.14001754393060523, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0029968766029924154}, {"id": 911, "seek": 493044, "start": 4930.44, "end": 4936.599999999999, "text": " because at that model, in at that point, maybe you can be able to model Granger causality and", "tokens": [50364, 570, 412, 300, 2316, 11, 294, 412, 300, 935, 11, 1310, 291, 393, 312, 1075, 281, 2316, 2606, 3176, 3302, 1860, 293, 50672], "temperature": 0.0, "avg_logprob": -0.1996018091837565, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.0016189940506592393}, {"id": 912, "seek": 493044, "start": 4937.4, "end": 4940.04, "text": " and more complex and and useful", "tokens": [50712, 293, 544, 3997, 293, 293, 4420, 50844], "temperature": 0.0, "avg_logprob": -0.1996018091837565, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.0016189940506592393}, {"id": 913, "seek": 493044, "start": 4941.96, "end": 4947.48, "text": " dynamical causal models, basically. Because in general, the the due calculus and the", "tokens": [50940, 5999, 804, 38755, 5245, 11, 1936, 13, 1436, 294, 2674, 11, 264, 264, 3462, 33400, 293, 264, 51216], "temperature": 0.0, "avg_logprob": -0.1996018091837565, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.0016189940506592393}, {"id": 914, "seek": 493044, "start": 4947.48, "end": 4955.0, "text": " interventional and counterfactual branch of science is mostly developed on on small models.", "tokens": [51216, 13176, 304, 293, 5682, 44919, 901, 9819, 295, 3497, 307, 5240, 4743, 322, 322, 1359, 5245, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1996018091837565, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.0016189940506592393}, {"id": 915, "seek": 495500, "start": 4955.96, "end": 4963.4, "text": " So it's like you don't do interventions on gigantic models in general. So if you if you", "tokens": [50412, 407, 309, 311, 411, 291, 500, 380, 360, 20924, 322, 26800, 5245, 294, 2674, 13, 407, 498, 291, 498, 291, 50784], "temperature": 0.0, "avg_logprob": -0.1469486390037098, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0016376484418287873}, {"id": 916, "seek": 495500, "start": 4963.4, "end": 4970.28, "text": " look at medical data, they use relatively small vision networks. And but of course,", "tokens": [50784, 574, 412, 4625, 1412, 11, 436, 764, 7226, 1359, 5201, 9590, 13, 400, 457, 295, 1164, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1469486390037098, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0016376484418287873}, {"id": 917, "seek": 495500, "start": 4970.28, "end": 4976.2, "text": " if you want to have a dynamical causal model, that models a specific environment or a specific", "tokens": [51128, 498, 291, 528, 281, 362, 257, 5999, 804, 38755, 2316, 11, 300, 5245, 257, 2685, 2823, 420, 257, 2685, 51424], "temperature": 0.0, "avg_logprob": -0.1469486390037098, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0016376484418287873}, {"id": 918, "seek": 495500, "start": 4976.2, "end": 4981.08, "text": " reality, you have a lot of neurons inside, you have a lot of latent variables, they change over", "tokens": [51424, 4103, 11, 291, 362, 257, 688, 295, 22027, 1854, 11, 291, 362, 257, 688, 295, 48994, 9102, 11, 436, 1319, 670, 51668], "temperature": 0.0, "avg_logprob": -0.1469486390037098, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0016376484418287873}, {"id": 919, "seek": 498108, "start": 4981.08, "end": 4987.0, "text": " time and an intervention at some more at some moment creates an effect in a different time step.", "tokens": [50364, 565, 293, 364, 13176, 412, 512, 544, 412, 512, 1623, 7829, 364, 1802, 294, 257, 819, 565, 1823, 13, 50660], "temperature": 0.0, "avg_logprob": -0.14541420656092027, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0024566741194576025}, {"id": 920, "seek": 498108, "start": 4987.0, "end": 4991.5599999999995, "text": " So maybe the next time step in 10 different time steps later. And I think that would be", "tokens": [50660, 407, 1310, 264, 958, 565, 1823, 294, 1266, 819, 565, 4439, 1780, 13, 400, 286, 519, 300, 576, 312, 50888], "temperature": 0.0, "avg_logprob": -0.14541420656092027, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0024566741194576025}, {"id": 921, "seek": 498108, "start": 4991.5599999999995, "end": 4996.84, "text": " very interesting to develop like a biologically plausible way of passing information", "tokens": [50888, 588, 1880, 281, 1499, 411, 257, 3228, 17157, 39925, 636, 295, 8437, 1589, 51152], "temperature": 0.0, "avg_logprob": -0.14541420656092027, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0024566741194576025}, {"id": 922, "seek": 498108, "start": 4997.96, "end": 5001.48, "text": " that is also able to model Granger causality, basically.", "tokens": [51208, 300, 307, 611, 1075, 281, 2316, 2606, 3176, 3302, 1860, 11, 1936, 13, 51384], "temperature": 0.0, "avg_logprob": -0.14541420656092027, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0024566741194576025}, {"id": 923, "seek": 498108, "start": 5004.68, "end": 5007.4, "text": " Where do you see action in these models?", "tokens": [51544, 2305, 360, 291, 536, 3069, 294, 613, 5245, 30, 51680], "temperature": 0.0, "avg_logprob": -0.14541420656092027, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0024566741194576025}, {"id": 924, "seek": 500740, "start": 5007.4, "end": 5019.08, "text": " Where do I see action? I didn't think of that. I think I see actions in those models,", "tokens": [50364, 2305, 360, 286, 536, 3069, 30, 286, 994, 380, 519, 295, 300, 13, 286, 519, 286, 536, 5909, 294, 729, 5245, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1666933427374047, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0004369434027466923}, {"id": 925, "seek": 500740, "start": 5019.08, "end": 5021.96, "text": " maybe in the same way as I as you see in other models, because", "tokens": [50948, 1310, 294, 264, 912, 636, 382, 286, 382, 291, 536, 294, 661, 5245, 11, 570, 51092], "temperature": 0.0, "avg_logprob": -0.1666933427374047, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0004369434027466923}, {"id": 926, "seek": 500740, "start": 5023.08, "end": 5028.679999999999, "text": " creative coding is basically a model of perception. So so an action is you can see", "tokens": [51148, 5880, 17720, 307, 1936, 257, 2316, 295, 12860, 13, 407, 370, 364, 3069, 307, 291, 393, 536, 51428], "temperature": 0.0, "avg_logprob": -0.1666933427374047, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0004369434027466923}, {"id": 927, "seek": 500740, "start": 5028.679999999999, "end": 5034.679999999999, "text": " that's a consequence of what you're experiencing. So by changing the way you're you're", "tokens": [51428, 300, 311, 257, 18326, 295, 437, 291, 434, 11139, 13, 407, 538, 4473, 264, 636, 291, 434, 291, 434, 51728], "temperature": 0.0, "avg_logprob": -0.1666933427374047, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0004369434027466923}, {"id": 928, "seek": 503468, "start": 5035.320000000001, "end": 5040.84, "text": " experiencing something, then you can compute maybe you can simply perform a smarter action", "tokens": [50396, 11139, 746, 11, 550, 291, 393, 14722, 1310, 291, 393, 2935, 2042, 257, 20294, 3069, 50672], "temperature": 0.0, "avg_logprob": -0.14875927890639706, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.016079699620604515}, {"id": 929, "seek": 503468, "start": 5040.84, "end": 5048.280000000001, "text": " now that you have more information. But but yeah, I don't think action is very easy. Like,", "tokens": [50672, 586, 300, 291, 362, 544, 1589, 13, 583, 457, 1338, 11, 286, 500, 380, 519, 3069, 307, 588, 1858, 13, 1743, 11, 51044], "temperature": 0.0, "avg_logprob": -0.14875927890639706, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.016079699620604515}, {"id": 930, "seek": 503468, "start": 5048.280000000001, "end": 5053.08, "text": " yeah, I don't see any explicit consequence of actions, besides the fact that this can allow", "tokens": [51044, 1338, 11, 286, 500, 380, 536, 604, 13691, 18326, 295, 5909, 11, 11868, 264, 1186, 300, 341, 393, 2089, 51284], "temperature": 0.0, "avg_logprob": -0.14875927890639706, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.016079699620604515}, {"id": 931, "seek": 503468, "start": 5053.08, "end": 5060.200000000001, "text": " you to basically maybe to simply draw better conclusions to then perform actions in the future.", "tokens": [51284, 291, 281, 1936, 1310, 281, 2935, 2642, 1101, 22865, 281, 550, 2042, 5909, 294, 264, 2027, 13, 51640], "temperature": 0.0, "avg_logprob": -0.14875927890639706, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.016079699620604515}, {"id": 932, "seek": 506020, "start": 5060.76, "end": 5066.76, "text": " I'll add on to that a few ways that people have talked about predictive coding and action.", "tokens": [50392, 286, 603, 909, 322, 281, 300, 257, 1326, 2098, 300, 561, 362, 2825, 466, 35521, 17720, 293, 3069, 13, 50692], "temperature": 0.0, "avg_logprob": -0.10931272041506884, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008558572153560817}, {"id": 933, "seek": 506020, "start": 5067.72, "end": 5075.5599999999995, "text": " First off, internal action or covert action is attention. So we can think about perception", "tokens": [50740, 2386, 766, 11, 6920, 3069, 420, 45985, 3069, 307, 3202, 13, 407, 321, 393, 519, 466, 12860, 51132], "temperature": 0.0, "avg_logprob": -0.10931272041506884, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008558572153560817}, {"id": 934, "seek": 506020, "start": 5075.5599999999995, "end": 5080.92, "text": " as an internal action that that's one approach. Another approach pretty micro is the outputs", "tokens": [51132, 382, 364, 6920, 3069, 300, 300, 311, 472, 3109, 13, 3996, 3109, 1238, 4532, 307, 264, 23930, 51400], "temperature": 0.0, "avg_logprob": -0.10931272041506884, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008558572153560817}, {"id": 935, "seek": 506020, "start": 5080.92, "end": 5088.44, "text": " of a given node. We can understand that node as a particular thing with its own sensory cognitive", "tokens": [51400, 295, 257, 2212, 9984, 13, 492, 393, 1223, 300, 9984, 382, 257, 1729, 551, 365, 1080, 1065, 27233, 15605, 51776], "temperature": 0.0, "avg_logprob": -0.10931272041506884, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008558572153560817}, {"id": 936, "seek": 508844, "start": 5088.44, "end": 5095.719999999999, "text": " and action states. And so in that sense, the output of a node. And then lastly, which we", "tokens": [50364, 293, 3069, 4368, 13, 400, 370, 294, 300, 2020, 11, 264, 5598, 295, 257, 9984, 13, 400, 550, 16386, 11, 597, 321, 50728], "temperature": 0.0, "avg_logprob": -0.10149963166978625, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00043054550769738853}, {"id": 937, "seek": 508844, "start": 5095.719999999999, "end": 5100.5199999999995, "text": " explored a little bit in live stream 43, on the theoretical review on predictive coding,", "tokens": [50728, 24016, 257, 707, 857, 294, 1621, 4309, 17914, 11, 322, 264, 20864, 3131, 322, 35521, 17720, 11, 50968], "temperature": 0.0, "avg_logprob": -0.10149963166978625, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00043054550769738853}, {"id": 938, "seek": 508844, "start": 5101.08, "end": 5105.24, "text": " we're reading all the way through. And it was all about perception all about perception. And then", "tokens": [50996, 321, 434, 3760, 439, 264, 636, 807, 13, 400, 309, 390, 439, 466, 12860, 439, 466, 12860, 13, 400, 550, 51204], "temperature": 0.0, "avg_logprob": -0.10149963166978625, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00043054550769738853}, {"id": 939, "seek": 508844, "start": 5105.24, "end": 5115.4, "text": " it was like section 5.3. If you have expectations about action, then action is just another variable", "tokens": [51204, 309, 390, 411, 3541, 1025, 13, 18, 13, 759, 291, 362, 9843, 466, 3069, 11, 550, 3069, 307, 445, 1071, 7006, 51712], "temperature": 0.0, "avg_logprob": -0.10149963166978625, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00043054550769738853}, {"id": 940, "seek": 511540, "start": 5115.48, "end": 5120.679999999999, "text": " in this architecture. And that's really aligned with inactive inference, where instead of having", "tokens": [50368, 294, 341, 9482, 13, 400, 300, 311, 534, 17962, 365, 294, 12596, 38253, 11, 689, 2602, 295, 1419, 50628], "temperature": 0.0, "avg_logprob": -0.08902111272702272, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.009410912171006203}, {"id": 941, "seek": 511540, "start": 5120.679999999999, "end": 5126.44, "text": " like a reward or utility function that we maximize, we select action based upon it being the", "tokens": [50628, 411, 257, 7782, 420, 14877, 2445, 300, 321, 19874, 11, 321, 3048, 3069, 2361, 3564, 309, 885, 264, 50916], "temperature": 0.0, "avg_logprob": -0.08902111272702272, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.009410912171006203}, {"id": 942, "seek": 511540, "start": 5126.44, "end": 5131.24, "text": " likeliest course of action, the path of least action, that's Bayesian mechanics. And so it's", "tokens": [50916, 411, 16850, 1164, 295, 3069, 11, 264, 3100, 295, 1935, 3069, 11, 300, 311, 7840, 42434, 12939, 13, 400, 370, 309, 311, 51156], "temperature": 0.0, "avg_logprob": -0.08902111272702272, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.009410912171006203}, {"id": 943, "seek": 511540, "start": 5131.24, "end": 5139.4, "text": " actually very natural to bring in an action variable and utilize it essentially as it as if it were", "tokens": [51156, 767, 588, 3303, 281, 1565, 294, 364, 3069, 7006, 293, 16117, 309, 4476, 382, 309, 382, 498, 309, 645, 51564], "temperature": 0.0, "avg_logprob": -0.08902111272702272, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.009410912171006203}, {"id": 944, "seek": 513940, "start": 5140.12, "end": 5146.839999999999, "text": " a prediction about something else. Exteroceptively in the world, because we're also expecting action.", "tokens": [50400, 257, 17630, 466, 746, 1646, 13, 2111, 391, 78, 1336, 3413, 294, 264, 1002, 11, 570, 321, 434, 611, 9650, 3069, 13, 50736], "temperature": 0.0, "avg_logprob": -0.18793405532836915, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.009113640524446964}, {"id": 945, "seek": 513940, "start": 5148.5199999999995, "end": 5154.759999999999, "text": " No, yes, yes, exactly. No, I like the way of defining actions a lot, actually. And I still", "tokens": [50820, 883, 11, 2086, 11, 2086, 11, 2293, 13, 883, 11, 286, 411, 264, 636, 295, 17827, 5909, 257, 688, 11, 767, 13, 400, 286, 920, 51132], "temperature": 0.0, "avg_logprob": -0.18793405532836915, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.009113640524446964}, {"id": 946, "seek": 513940, "start": 5154.759999999999, "end": 5161.08, "text": " think if it's been like, for example, there are not so many papers that apply this method. I think", "tokens": [51132, 519, 498, 309, 311, 668, 411, 11, 337, 1365, 11, 456, 366, 406, 370, 867, 10577, 300, 3079, 341, 3170, 13, 286, 519, 51448], "temperature": 0.0, "avg_logprob": -0.18793405532836915, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.009113640524446964}, {"id": 947, "seek": 513940, "start": 5161.08, "end": 5167.08, "text": " there are a couple from from Alexander Orobrie does something similar. But in practice, like", "tokens": [51448, 456, 366, 257, 1916, 490, 490, 14845, 422, 340, 1443, 414, 775, 746, 2531, 13, 583, 294, 3124, 11, 411, 51748], "temperature": 0.0, "avg_logprob": -0.18793405532836915, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.009113640524446964}, {"id": 948, "seek": 516708, "start": 5167.88, "end": 5172.36, "text": " outside of the pure active inference, like applying predictive coding and actions to", "tokens": [50404, 2380, 295, 264, 6075, 4967, 38253, 11, 411, 9275, 35521, 17720, 293, 5909, 281, 50628], "temperature": 0.0, "avg_logprob": -0.09460806510817836, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0035810840781778097}, {"id": 949, "seek": 516708, "start": 5173.16, "end": 5177.08, "text": " solve practical problems hasn't been explored a lot.", "tokens": [50668, 5039, 8496, 2740, 6132, 380, 668, 24016, 257, 688, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09460806510817836, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0035810840781778097}, {"id": 950, "seek": 516708, "start": 5179.72, "end": 5185.64, "text": " Well, thank you for this excellent presentation and discussion. Is there anything else that you", "tokens": [50996, 1042, 11, 1309, 291, 337, 341, 7103, 5860, 293, 5017, 13, 1119, 456, 1340, 1646, 300, 291, 51292], "temperature": 0.0, "avg_logprob": -0.09460806510817836, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0035810840781778097}, {"id": 951, "seek": 516708, "start": 5185.64, "end": 5193.64, "text": " want to say or point people towards? No, just a big thank you for inviting me. And", "tokens": [51292, 528, 281, 584, 420, 935, 561, 3030, 30, 883, 11, 445, 257, 955, 1309, 291, 337, 18202, 385, 13, 400, 51692], "temperature": 0.0, "avg_logprob": -0.09460806510817836, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0035810840781778097}, {"id": 952, "seek": 519364, "start": 5194.6, "end": 5198.76, "text": " it was really fun. And I hope to come back at some point for for some future works.", "tokens": [50412, 309, 390, 534, 1019, 13, 400, 286, 1454, 281, 808, 646, 412, 512, 935, 337, 337, 512, 2027, 1985, 13, 50620], "temperature": 0.0, "avg_logprob": -0.24379555059939015, "compression_ratio": 1.336, "no_speech_prob": 0.012564186938107014}, {"id": 953, "seek": 519364, "start": 5200.280000000001, "end": 5210.6, "text": " Cool. Anytime, anytime. Thank you, Thomas. So thank you, Daniel. See you. Bye. Bye.", "tokens": [50696, 8561, 13, 39401, 11, 13038, 13, 1044, 291, 11, 8500, 13, 407, 1309, 291, 11, 8033, 13, 3008, 291, 13, 4621, 13, 4621, 13, 51212], "temperature": 0.0, "avg_logprob": -0.24379555059939015, "compression_ratio": 1.336, "no_speech_prob": 0.012564186938107014}, {"id": 954, "seek": 522364, "start": 5223.64, "end": 5225.1, "text": " You", "tokens": [50412, 509, 50437], "temperature": 0.0, "avg_logprob": -0.8853075504302979, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9026011228561401}], "language": "en"}