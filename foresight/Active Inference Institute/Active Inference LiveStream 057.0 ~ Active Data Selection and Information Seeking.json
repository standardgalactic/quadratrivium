{"text": " Alright, hello and welcome. It's May 24th, 2024. We're in ACTIMF livestream number 57.0, doing background and context video for the active data selection and information seeking paper and series. So welcome to the ACTIMF Institute. We're a participatory online institute that is communicating, learning and practicing applied active inference. This is a recorded and archived livestream. Please provide feedback so we can improve our work. All backgrounds perspectives are welcome. And we'll follow video etiquette for live streams. Head over to active inference.org to learn more about any of the projects, including the live streams. Today, we're going to do together a background first pass on a very interesting paper from Thomas Parr, Carl Friston and Peter Seidman, active data selection and information seeking from 2024. In this video, we're going to introduce ourselves, talk about big questions, go through the keywords of the paper, then most of the sections, section by section. And as always, with the dot zero, it's just like a first pass. And we'll look forward to speaking with hopefully some of the authors in the coming weeks, and also looking what people ask about. So Christopher, let us introduce ourselves and go from there. Thanks a lot also for helping in the dot zero preparation. Happily. Yeah, so I'm Christopher Bennett. I'm a bioinformatics scientist. I do a lot of data mangling data analysis and that sort of thing. This paper was of great interest to me as we kind of go into this more data driven era in making sure that with such large data sets that we have, making sure that we can actually select relevant data for any of our applications going forward, be it machine learning or whatever we're trying to do. And I'm Daniel. I'm a researcher in California and also was drawn to this on one hand on the applied side, the idea of more efficient and effective data sampling. And then on the more theory side, the connection with epistemic value information gain. So here are some of the big questions. Why don't you add some detail to this? Absolutely. So there's five major big questions that I had after reading this. For the most part, it boils down to doing our sampling. You can do sampling over time and sampling of different data sets in different ways. Is there a way that we can intelligently select the data that we're going for the time the time that we're trying to select? Is there a way that we can understand how the time aspect helps sample through time instead of just doing a dynamic or more dynamic instead of doing a static like we're going to do time zero, time seven, time 14, time 21. Can we say, hey, the differences between time one and time two are very 10.1, time point two are very interesting. It's a lot of data in there alone. So we'll sample one and two and then maybe sample 10. Is there a way that we can intelligently select the time points that we are sampling from when we get into the time series aspect? The Piper mentioned a number of different time dimension models that you can add to the four model that they're utilizing, one of which was a hidden Markov model, another was they mentioned a differential equation in the actual model itself, in the equation itself. Is there one, are there situations that one performs better over than the other? Or is what they have selected to use in the paper the optimal solution in most cases, if not all cases? When it comes to clinical trials, that was a section in this that they discussed. There's a lot of FDA regulations of the clinical trials and it's very heavy red tape right now. Is there a way that there's minimum ends that you need in many clinical trials to actually be considered passing? Is there a way that you can bound this model that they've developed into something that you can guarantee a minimum number, a minimum sampling that the FDA requires or any regulatory body? Another point is the next step of how are we going to integrate this in with other machine learning models or any downstream applications that you're going with? Is there a selection method that we can, or how do you see these, this method kind of pre, kind of before machine learning? How are we going to attach these things together so that we feed the right data into machine learning? Being an LLM. And then scalability and computation demands. That's going to be a big one if this is going to be something that is used routinely in industry. We need to make sure that this is something that is as scalable as you can get. Go from small scale, which is a lot of what they show in this paper and then all the way up to the very large scale data sets that we use to train LLMs and other models. Those are kind of the five major points that I have. Thank you. Those are very insightful. Here were some of the big questions that I was excited about. So first, from a more general information gain, epistemic foraging perspective, how do we model the implicit and explicit constraints or trade-offs or dynamics of information seeking? Which is often addressing a question that is left unaddressed in data science of where the data comes from. It's just about doing analysis with the data that's there. But even there, as this paper will kind of get into, there's still sub-sampling and all these other factors to consider. The clinical trial example brings a very serious and very real plot twist into the paper, which moves through several levels of adding theoretical generalization and incorporating like the time dimension and other features. And then the plot twist is when the preferences for certain kinds of observations is specified, then there's all this interesting behavior and decisions that come into play. So I'm sure that'll be a great discussion. And then also in section four, they mentioned the streetlight effect, which is, quote, the tendency to search where data are generated in a minimally ambiguous way, i.e. under a street lamp compared to searching elsewhere on a darkened street. And so there it's an interesting scenario and there'll be some fun art coming up and also how they distinguish the sampling method with the full information seeking from the maximum entropy sampling is a very subtle but very important point that I look forward to hearing more from the authors about. Okay, so just to summarize, the paper is Active Data Selection and Information Seeking, 2024, Thomas Parr, Carl Friston, Peter Seidman, and just a few of the aims and claims of the paper, and then Christopher will read the abstract. This paper aims to unpack the principles of active sampling of data by drawing from neurobiological research on animal exploration and from the theory of optimal experimental design. Our overall aim is to provide an intuitive overview of the principles that underwrite active data selection and to illustrate this with some simple examples. Our interest is in the selection of data, either through sampling subsets of data from a large data set or through optimizing experimental design based upon the models we have of how those data are generated. Optimizing data selection ensures we can achieve good inference with fewer data, saving on computational and experimental costs. So if you could read the abstracts. Absolutely, so the main points in the abstract are the Bayesian inference is typically focused on two major issues. The first one is that you have to estimate the parameters of the model of the data, and the second is that you need to quantify the evidence for alternative hypotheses and formulate an alternative model. But this paper is actually looking at a third issue, which is in how you're going to select the data for your models. And either through sampling subsets of large data is typically used or optimizing experiments of design. Based upon the models we have these of how these data are generated. Optimizing data selection, what's going into the models can achieve a very good inference with fewer data points. So you're saving on computational time costs, that sort of thing by actually reducing the amount of information that you're putting into a model. So what we're doing here is trying to unpack how you're going to actively select data, and I mean actively select data through a machine optimization protocol by drawing from some of these neurobiology concepts and trying to optimize the maximum information that the information can provide, maximum information gain. So they offer overview of some basic points from the field and illustrates the application in some of the toy examples that they have will go through, ranging from different approximations with basis sets to inference about how the process can evolve over time. And finally they'll go through and consider how the approach to the data selection could be applied to design of clinical trials in this case, and specifically Bayes adapted clinical trial, something that is more and more seeing headlines and it's more and more used today now that we have the technology to do it. Great. Okay. For the roadmap, the paper begins with introduction section, goes into Bayesian inference, generative models and expected information gain. They go through a simplest worked example, and then consider a few more ways to level up that model with function approximation, consideration of the time dimension with dynamic processes, and then bring in the preference for certain observations in the clinical trials. Then there's a discussion and conclusion, and they also have a paragraph explaining their kind of logic there. The keywords for the paper were experimental design, active sampling, information gain, and Bayesian inference. So the next slides are going to go into those four background topics. After the four background topics, we'll speed through the sections and just plant a few seeds for what we want to explore more. So first, experimental design. Here's two kind of classical views of experimental design in the active inference and free energy principle eras. So on the left is the statistical parametric mapping, textbook, toolbox, documentation, et cetera, has multiple chapters and kinds of analyses included in the package to specify and simulate and also to recognize data according to different experimental designs. And one very hallmark or common visualization of these kinds of patterns of experimental design are these design matrices. And it's just represented in this black to white gray scale, and it summarizes different kinds of measurements across different experimental settings. Like here might be six settings in the larger white blocks. And then there was variability within each of those trials. And those are the smaller row levels. So that's like where the data are collected. And a lot of this has to do with the linear operations that can occur on this kind of matrix in a general linear modeling framework. And then on the right is the experimental design experimenters perspective, where the experimental stimuli they output as actions are the observations going into the subjective model, like of the rat in the team is, and then the action output of the rat is the observations of the experiment of the experiment. So kind of two different perspectives, SPM with more of a matrix multiplication, f m r i optimal design, and then active inference with the more general graphical Bayesian modeling, starting to broaden the consideration of what optimal foraging and what information gain epistemic value mean. These are kind of the experimental design themes and how they connect a little bit with other experimental design factors. Want to add anything? Yeah, keep in mind that a lot of these experiments, experimental design is a very big and very important consideration when you're actually running any sort of science or analytics of any variety. And these experiments can actually get very large with huge, huge amounts of data. And all of that data is relevant for every application that you want. So you want to be able to design your experiment in a way that you can collect information in a intelligent way rather than trying to go through and just collect every data point that you can because humans in many cases are running some of these experiments and they are have limited time. I certainly do when I'm running these things. So I have to be very intelligent in how I set things up and how I actually collect data and what did I collect, because you only get in many of these cases, you only get one shot to collect the data, you miss it, it's over. You won't have that data point. So it's very critical that you actually take the experimental design seriously when you're setting these things up. Great. So connecting that kind of experimental design, experimenter on a budget perspective with a more statistical and biologically statistical based perspective, active sampling. So they wrote, when we look at the world around us, we are implicitly engaging in a form of active data sampling, also known as active sensing or active inference. So this is referencing the visual saccades. And just to kind of highlight how extreme the relative acuity difference is between the center of the eye where the gaze is focused on and the off center, among other visual changes. And vision is just being taken as one sensory example here. It could also be thought of as like looking for books within a library or any other kind of selection of what data are going to come in, even if it seems like all of it is going to all of it is coming in, that still is going to be perhaps addressed with different sensors that have different variability profiles, or like there's different RNA sequencing kits that you could buy. And so how do you balance the kind of more samples or which samples, especially as those spaces grow massive. And then just to contrast that, whereas digit recognition in a saccade based paradigm would focus on the motor patterns and the small centrally focused visual acuity and then the motor patterns that relate to circuiting around a digit. Whereas in the kind of machine learning taken all at once approach, a matrix corresponding to like the pixels in the MNIST dataset are simply taken in all at equivariance level. So that's just kind of taking in the data. There's still another higher order data selection question of like, which digits do you take? If there was a large number of digits in that library. So this is active data sampling on multiple scales, which records you pull at all, and then how the resolution and all the tradeoffs that are associated with using the data processing or making the experiment. Yeah, add more though. And, you know, keep in mind that when you're talking about something like the visual system, you know, our visual system has access to untold amounts of information, but our brain can't take advantage of all of that at once. So there's low energy usage of the brain that needs to optimize the relevant information. Think, you know, your nose is right at the end of your face. Your eyes are always seeing your nose, but your brain is filtering it out. And this is happening all the time at all points in time. There are literal blind spots in what you are actually capable of intaking and processing all at once. And then additionally, when you're moving away from something like your eye or biological systems and into the experiment design itself, you know, you oftentimes can't run a full factorial design. And there are other methods like a fractional factorial design. But those are random base. And this is trying to actually talk about actively selecting how you're going to set up that design. So it's kind of a, you can think of it multiple different ways. Awesome. The factor that's going to come into play as driving the active sampling is going to be the information gain. And there's some quotes here. And equation two is shown. They write, we have conditioned our model upon the variable pi, which represents a choice we can make in selecting our data. So data recognition, interpretation, analysis, and so on. It's often framed as kind of like an observation type or sense making type activity. Here, pi for policy, as with usual, is being framed as a control or an active data selection policy, we're applying to some data set. So it adds a action element into this sequential epistemic foraging, rather than just taking a large data set, and just munching it like all at once, it brings in this sequential question of where to sample, and potentially updating where is informative to sample through time. And that I of pi is the functional on that policy distribution or specific choice that can be decomposed, all these interesting ways that we can explore more in the coming discussions. What else would you add, though, about information gain? I think this is one of the biggest points in this whole paper is you're measuring how much information you are gaining in your model by adding these different variables in here and selecting different variables. You're effectively automatically taking out or trying to remove things that have high mutual information that don't add as much. So if you have parameter A and parameter B that are effectively just transformations of the same data, then you can easily remove one of those and still have all the information that you need. So it's a really, really powerful way of saying I'm trying to optimize and maximize the amount of information that I'm adding into the model by selecting data that actually has the information that is going to improve them. Awesome. One other interesting angle here is often in the control literature, utility, reinforcement learning, etc. The epistemic value component is added in, whereas in the structure of this paper, they start with the pure information gain perspective. And then in the clinical trial, they bring the preference in. So the pragmatic value comes in secondary to the information gain in how they build it up step by step. Bayesian inference is the last keyword. A lot of places to go. Here's what they showed for equation one. And they wrote Bayesian inference is the process of inverting a model of how data, why, are generated to obtain two things, the marginal likelihood and the posterior probability. So anything you want to say about Bayesian inference or do you want to say something about Bayesian networks and graphs? I think that you've kind of covered it here. It's, I think, barely textbook on this part. Yeah. How about graphs? On the Bayesian graphs side of it, there's multiple different ways that these Bayesian statistics is done nowadays. And the Bayesian networks and graphs is a really powerful method going forward. I know that right now in the Institute, we have an Rx and Fur group working right now, which is a Julia package for actually just building these network graphs, these Bayesian graphs and doing message passing between the different factors and the different nodes of the graph. So this is a very big up and coming area right now. It's very early in the time frame that this is going to become big. It's kind of on the upswing right now. And it's kind of, at least I would predict, going to be kind of the next big thing going forward in the next five years or so. Yeah. We've been having a great epistemic time and Livestream 55 explores some of this in more detail. Okay. That was the background now on to the paper. So first, just to get the last part of the paper out of the way, they have a GitHub, Thomas Parr's GitHub with the active data selection repo. And maybe in one of the upcoming discussions or somebody in the time between can explore and transform and play with the code. And also all these different ways that we have fun discussions around the language of the active inference model and how building it in different languages or using different styles like is there isn't plausible. These have been very fun discussions that help us get at what the core of the math really is and how that's independent of whether it's written in MATLAB or any other language. And then also as it is simulated, it's written here in MATLAB. And so that's kind of interesting. Any thoughts on that or just like coding in Rx and fur or or Yeah, I think that with Rx and fur being, I think, relatively new on the scene, you have some of these other traditional approaches with MATLAB and high MD and that sort of thing. It'll be very interesting to see how these techniques evolve over time with packages like Rx and fur really, I think, changing how we approach building these models and designing them. I think that it's going to be even more critical now in this current environment to select the data intelligently going in so that you're not muddying up your models or having to build two big of models that might have information that's not as useful to the application at hand. Yeah, great. All right, section one introduction. So we'll try to hit on some of the key points. I'll say something briefly and then feel free to add something if you want. Section one situates that inference and action cycle or loop or partition in terms of a statistician's job or process in modeling observations data as sampled and latent variables as models and the process by which there's kind of snapshot or bulk summarization or generativity or and how it's possible to have a continuous resampling of informative data or how you even evaluate how data are informative in which way. Want to add anything? I think you've captured that very well. I'm going to actually pull out my notes so that I can actually remember all the symbols. Why are going to be used for data and data for the latent variables? So distributions of data, distributions of latent variables conditioned upon data coming in. So that could be seen as just one data point sequentially or a big bulk vector coming in like all at once. Just continuing to move through this section, they wrote careful data selection is especially important when we consider the problems associated with very large data sets of the sort that are now ubiquitous in machine learning and artificial intelligence settings. So just to summarize a little bit or add a few notes that came up in the paper. So other than this topic being very fascinating and very integrative in terms of a unifying approach for information and behavior etc. Also this is definitely one of the active inference questions that has a lot of pragmatic relevance as dealing with with data sets of different kind is totally day to day. And especially the way that even the examples specify important settings is very clear, very direct. Though also the mathematics are very general about epistemics and this motivation that they lay out in the first section about how if this challenge could be addressed, then there will be all these kinds of benefits that could be realized with current systems and data sets. And then they provide the approach to at least getting there or towards it to optimize data selection. We first need to identify an appropriate optimality criterion. And so they're going to kind of go through several stages of with different generative models how that optimality criterion is defined. Anything else that And keep in mind that this is the expected information gain that they're talking about. It's effectively how much do we think we're going to gain by adding this information in there. And then you can of course train your model by looking at the actual information gain if necessary and go through kind of a learning cycle. But we're basing this all off of what do we expect to gain from this information. All right, section two, basing inference generative models and expected information gain. In this equation three, I won't read it all. It models the Markov blanket formalism in terms of upstream and downstream causal relationships in terms of messages that are passed along edges of a factor graph. They introduce in this paper the lambda operator to indicate either summation or integration. So this is across continuous variables or discrete variables. And we'll explore this more with the authors, hopefully anything you want to add on equation three. More that you know the information gain is a function of the data that you sample. So depending on how you sample that data, you're going to get different information gain out of it as you would expect. And then you start to get into the message passing, which is that base graph and factor graph, I guess, challenge going forward that that construct when you build it in a factor graph model, you have to be able to pass the messages between the nodes effectively. Yeah, and to kind of ground that in the data science situation, if you have a latent state estimate and you're generating data, generative AI, synthetic data, then the latent variable is upstream, causally, statistically from the data pseudo observation. But that might be the actual real observation if you're interested in the computer model. Whereas the data recognition case where the data are upstream of the estimate of a parameter, like a risk score or something like that, then the parents of the latent state estimate is the data. So that's the recognition direction. So this kind of covers the whole Bayesian update possibility spectrum in this essentially Markov blanket, but it could be in face space or time or a few other situations they explore. All right, three, a worked example. This section lays out the overall pipeline for how you get from the graphical notation of the Bayesian network, whether it's viewed visually graphically, like with a variable dependency structure, or whether it's just written out in terms of the plain text with the analytical, the Bayesian network is transformed into a factor graph, probably constrained factor graph, discussion for another day. On that graph, certain messages are passed at inference runtime. conditional and predictive entropies are calculated as part of the way that this system outputs or is described by different probability distributions understand in a kind of statistical mechanical way in terms of entropy. And then that is going to come together into calculation of the objective function, which is the expected information gain, which is basically conditioned upon the cognitive model of the sampler. So just because the sampling is active data sampling, doesn't mean that it's going to lead to like an adaptive behavior. It just means that where the learning rate is perceived to be highest informationally, iteratively, there is a ranking by which those can be, which this the space of experiments can be ranked by and it can connect to pragmatic value in terms of epistemic and pragmatic coming together for the full expected free energy like in the clinical trial. Anything else? And you'll notice in this story example, they are discussing here, the factors that they have in their graph in each of their nodes is actually a cosine. So that's why you get that kind of oscillation in that bottom plot there. So you'll have areas of maximal information and then areas of minimal information just based on the toy example they have. And this doesn't always have to be cosine, but in this example it is. And so it just kind of gives you a really good graphical understanding of how your information gain can be viewed over a sinusoidal sort of oscillation. Yeah, just to kind of double down on that, if you sample right here on the number line, or right here, the lines are indistinguishable. So the information gain is expected to be low under understanding the parameter family that is being generated and sampled from, which in this first example is the same, same type of equations. Whereas where the functions are maximally distinct, the information gain associated with reducing uncertainty about which one of those five the data point is coming from, those are the informative points at the zero on the number line and far out. That's where just perceiving one point uncolored would give you the most ability to even perfectly resolve which one of the five situations it was. So they're right. In effect, this model amplifies or attenuates the amplitude of the predicted data, depending upon a periodic function of our data sampling policy pi. So here the policy distribution is like that kind of around the clock direction, which is not a common setting, but the general idea of sampling amongst a finite set of alternatives, where a control variable is going to result in the most informative data point, is a theme that is going to be expanded upon, and also one interesting mathematical move. Once all terms that are constant with respect to pi are eliminated, we are left with equation six. So equation five comes down to equation six, or maybe not exactly only five to six, but six has removed all the variables that don't change as policy changes. So if the question of policy selection is taken alone, like gradients on policy updating, then everything that's constant with respect to it doesn't come into like the delta pi, delta something. So it just simplifies it down to only a function of policy, and that just kind of reflects how like the sense making and belief updating component is partitioned off from the policy selection component here. Yeah, you're looking for change in your belief based on the observations that you're gained. So if it's not changing, it's not as informative in your information model. Yeah, continuing on equation six there, which is modeling the policy dependent components of information gain as an objective function that ranks decisions about where to sample. Equation six is a special case of the third row of table one, which highlights analytical expressions for expected information gain for a few common model structures. As we might intuit, the most informative place is to sample data aligned with those in which differences in data lead to large differences in the predicted data, in which our choice of pi maximizes the sensitivity with which y depends on data. So here are the categorical, the Dirichlet, and other functions in terms of how they'd be written out in the probabilistic, like specifying a distribution way, and then how there's this relationship analytically to a related distribution, which is an objective function that ranks the information content of sampling the likelihood distribution in a certain way. And that's closed form in certain situations. And then also they explore where it's intractable formally. And so then that's where the variational approximation comes into play. Anything to add? No, I think that summarizes this slide. Okay, section four, function approximation. We next turn to a generic supervised learning problem, that of trying to approximate some function based upon known inputs and the observable outcomes they stochastically cause. Pretty general neural network or latent state observation setup. That information is composed and concatenated. So that there's a common variable with that's describing the statistical object that's going to be describing the inputs and the relationship with the observable outcomes. And then that function approximation from sequential data in figure three is simulated with random but potentially you could call all of them random. But this one is a flatter or a less informed and iterated model of data sampling, just going to show that samples of random data with even this minimal non information gain driven model has a certain baseline prediction that's associated with certain choices about sampling sequentially from this generative model. Want to add anything? Yeah, it's just I like that they highlighted in this that choice diagram there that you can actually get the inefficient sampling just by random that you start to you can randomly select two things very close together and you've effectively maybe not wasted a choice but you know not gotten the maximum gain from that choice that you could have. Yeah, they write a little bit more about figure three. Figure three illustrates a depiction of this model as a Bayesian network and a visual representation of the data generating process. Now they're going to bring in information gain. So they write this is where information gain becomes relevant into designing a more informed way to sample data than from a flat or a non updating prior data sampling distribution. It's like equivalent to having a policy prior that's fixed which might be a heuristic in certain space. They write substituting equation seven into equation three. So here's that Markov blanket parent child concept and here equation three is describing the policy dependence on the joint distribution of the observed and unobserved and this is combined into equation 10. To show what equation 10 does in terms of now that we're sampling from this distribution or like statistical distributions that this describes they'll differentiate figure three from figure four kind of like bring in this model change between those two figures. Now samples are drawn from a distribution whose log is proportional to the information gain in equation 10. So it takes the flat policy prior and in a fixed way has remapped it to be proportional to the information gain. So here's three on the right and then four on the right and the figure uses the same format as figure three but now each choice is sampled to maximize anticipated information gain and they point to some specific quantitative patterns but also like qualitative patterns. So want to add anything on figure four? Just that now if you focus on those choices because that I really did I think like that choice plot there you can see that the walks around kind of choices around your data space are a little bit more distributed evenly distributed a little bit less random but you start to get I think a more cohesive sampling of the data without entire randomness putting things too close together putting selections too close together. Yeah okay continuing on well they set up the question as this raises the question as to how many samples should we collect. So within a foraging bout where should one look and then at the kind of like pulling a layer back in the strategy when should you halt look it like if you have already sampled all three records from a data set then unless you had some other reason you could fully stop sampling there but you also might want to have a softer stopping criterion that would relate to how much information you're gaining from continuing to sample in that way before like halting and so they include that by having like an exit policy in the state space of foraging possibilities. So how do you resolve that and answer this question can be drawn from behavioral neuroscience in the so-called exploitation exploration dilemma and they introduced the notion of sampling costs to help decide that. So this method is still going to require parameterization and situation specific modeling of the relative costs versus the relative information gain however at least there's an accounting that includes costs into the sampling equation to give any possibility of exiting because if no costs are provided for sampling then the model might just converge and continue to eke out very small amounts of variance explained if it doesn't explicitly have that stop option so they take the policy vector the list of locations that can be sampled from and adds a zero element which reflects the information gained if we were to stop sampling and then there's a preference over those observations expressed in the c vector preferences and this brings in the information seeking and the cost averse of imperatives into the same objective function in 11. Anything to add on 11? Yeah eventually the idea is that it just gets to a point where you're no longer whatever you set you're kind of stopping like energy to be kind of breaking energy eventually it's just gonna get to a point where the model just says hey I've reached what I can you've set this you're not gaining any anything beyond this point we can just stop at this point which is nice since in the random selection case there's not necessarily a stopping parameter as Daniel mentioned you could continue to get eke out very very small marginal changes but you're not going to gain anything else and so you're just spinning your wheels for no reason so this is a very elegant way of saying hey I've reached kind of an inflection point of data gain I'm done. So figure five they're continuing in this genre of three four five and now they've added in to the policy decision which which has an upstream dependency on the data that's the active policy edge they add in this cost to sampling so we can explore more however it now includes not just the information gain driven choices within a trial but it includes a specific probabilistic but decisive stopping point for that trial as parameterized by how sensitive it is to information gain and preference so this is one of the most interesting parts and and discussions in the paper they they ask it out loud a reasonable question to ask at this stage is why bother with the full information seeking objective and basically how does this differ from maximum entropy sampling and um let's look forward to the authors or other guests but here's just a few notes on this because I think it'll be a great place to explore what it really means to do statistical and physical modeling on cognitive systems they directly contrast maximum entropy sampling and their whole information gain family against each other and then the rebuttal is in figure six so just to show figure six for a second the measurement noise increases in variance from the center of the function domain so the the variability profile of the function is non-uniform this means this means the amount of unresolvable uncertainty is heterogeneous through the domain of potential sampler so I in some kind of ways of thinking about what they're really getting at and just putting this out as a speculation or starting point for for this key technical point so if there were a case where the latent states were equivariants they had iid variability profiles then sampling the most variable sensory data is the most informative like if you're taking a picture of a solid black image then sampling from the noisy pixels is going to potentially provide more information gain you're reducing uncertainty more about something it might be overfitting but you can select as a heuristic wanting to sample from where variability is high at just kind of a first pass layer however as we start to think about richer or more specified statistical patterns generative models there become dependencies that are sparse but important amongst all different kinds of things so things that are variable from a sensory perspective provide high information gain potentially to one part of a generative model like a screen and static but then other events might be less variable from a sensory perspective but smaller differences even in that variable relate to some other component of uncertainty resolution from some other component of the the model like those are going to be the cases where cognitive modeling does differ from just dispersed decision-making however they're both going to result in dispersed decision-making profiles like looking at the choices in the figures but the choices to sample from the less ambiguous parts of the actual distribution that leads to a much narrower policy path in this cognitive control setting versus in a variability sampling where it would go for the areas that were just more variable but not necessarily providing more information question mark and you can add on this with the maximum entropy or anything and I would even highlight kind of on the next slide it effectively what it is doing is accepting that you're not going to gain a lot of resolution in these highly variable regions and so you don't really have to sample into those deeply because you've accepted that it is variable it is not something it is inherently variable in the data we're not going to gain a lot of information from these regions and it's highlighted in blue down there and I think that's one of the big highlight notes of this figure is this less information gain available in these highly available regions and that's something that makes this method more robust and powerful when you're dealing with some of these non-uniform variable data yeah awesome and then the the um streetlight effect is brought in there so the avoidance of sampling in ambiguous locations is sometime referred to as a streetlight effect the tendency to search where data are generated in a minimally ambiguous way i.e. under streetlamp compared to searching elsewhere on a darkened street so I made some GPT-40 images some fun streetlight and on one hand there's kind of this sense of like is it constraining to look under only the streetlight isn't that kind of absurd and then there's the joke about how what the person's looking for is elsewhere but they're still searching under the streetlight but they're looking for something they they know is elsewhere so that's the kind of tragic element of it then there's this limited element however there's also this realistic element which is like well are you supposed to search where you can't sense or outside of where you are at that moment so how could you you know say that that wasn't just and then this paper is more framing it as just a general condition of perception like you're in your tactile streetlight that is the part you can see at all you can have latent modeling of any and other things but if it's not grounded in some way to a measurement made in a streetlight under the metaphor where the light allows for observation then you're not connected to data unless you're connected to that streetlight so that's just a very interesting kind of topic and and reference that the authors use what do you think absolutely I mean it kind of boils down to you can't know what you don't know what you can't observe you know if you can only observe what's underneath the streetlight then you can't really know what else is outside of there and so your inference necessarily should be constrained to what you're able to actually observe you can't observe the unknown and so not necessarily in this case um because you don't even know if it even exists you have no data to confirm or to refute it all right section five dynamic processes so in that previous example there was a data selection challenge whether it was approached from the flat fixed prior or all these other kind of subsequent variants with the adaptivity and or with the cost now there's going to be a time element brought into the underlying generative model we'll just go quickly here because that's the big point they take the static distribution that was sampled from and now give the underlying process also variability through time so this is like a very SPM brain latent state causal modeling type set yeah anything you want to say on that before we go in oh no go ahead okay okay so they consider processes that evolve in time equation 12 can be interpreted similarly to equation eight in which the expectation of the data is treated as a function approximation which now includes a time argument so here was eight expectation of the data given latent state parameterization and policy equals so on and then here there is data also being a function of parameterization and policy and then also bringing in an element with a subscript tau for time uh then you mentioned in your big questions the different approaches that they raise here with the three ways to bring temporalities into a model so let's definitely talk about that but just to show their images seven and eight are the pair for this dynamical section so figure seven shows a graphical representation of the matrices involved in generating our data and the inferences obtained after sampling so here it is sampling from a time variance function and then figure eight goes into more detail and notes predictions based upon current data can be used to inform predictions about nearby spatial locations and to predict and post it the values of the function at different points in time so just like you could have a 2d plane grayscale and infer the location of the streetlight by pursuing like a gradient up the light and then there would be this optimal sampling like if you just got one observation you would want to sample on a line that was orthogonal to the one that you couldn't resolve lots of ways to think about this sequential prediction but now the underlying landscape also changes so there's some temporal dynamics and then that can be fit with all these different time series models and autocorrelation and so on however that's specified statistically in the generative model but this section just shows however you do make a statistical model for time it's basically going to be the same thing where information is going to be drawn from a distribution and now time is a variable in that distribution uh they write in this in the previous section we have demonstrated the way in which smarter optimal sampling may be used to select data in a manner that balances the cost of sampling or performing further experiments against the information gained from those samples or experiments each of these examples has relied upon relatively simple and analytically comfortable linear Gaussian systems next we address active sampling in a situation where analytical solutions are no longer possible so to highlight the key formalisms that they're working with in that kind of background section uh or setup section they kept one thing constant which was that the the generative model the generative process or however it's considered with the family of equations that the agent is inferring and tracking hidden states with and that being the same as the actual family of equations that's generating the function of observations and here that is relaxed so that opens it up to all empirical settings where you can just say right off the bat we do not have access to the generative model of those data so we're making a map statistical map with all the associated trade-offs and statuses of like that genetic data or that transcriptomic data all those different kinds of data sets starting from a position where it's going to have to be statistically approximated and it isn't going to be based unless explicitly otherwise on actual knowledge about the causal elements of the system any any thoughts on that well in something else that they noted in the paper by adding this time element when they're actually going through the time series the model itself will preferentially select different data beyond what it just recently selected so time point one it selects x and y data time point two it might select l and m data so it actually will go through and select different types of data and it'll take a little bit of time um what the time is variable but it'll take a little bit of time before it revisits some of that previous data um at a previous time so by this you kind of have a sliding window of data that you're selecting over different time periods yeah all right that's all going to come to play in this clinical trial which is the big final contribution section of the paper in our final example we demonstrate the potential utility of the ideas outlined above in the context of a more concrete example so they model the statistical setting here as an adaptive Bayesian clinical intervention methodology experiment for example the kind that was done during the 2014 West African Ebola outbreak the active sampling approach advocated in this paper offers two main opportunities to augment adaptive trial designs first it allows us to adapt the design to maximize the information we obtain about treatment efficacy so that's the pure sense making information gain learning sampling from where it's informative not from where like we habitually or prefer to look and then second to balance and bring together that information gain with costs and that was brought in with the cost of the sampling section which was done in this paper by adding the stop policy option which can be probabilistically selected and then as other sampling locations become less informative or if somebody was just sampled and you know that there's a slow decay through time then on that subject the stop policy cost would outweigh the information gain from an experiment and this is also I think will be a very interesting discussion this blows the line between clinical trial and public health intervention and can be seen as analogous to animal behavior that is never fully exploitive or explorative but is a balance between the two so how do we think about that in terms of biomedical and health security and all these different topics and any thoughts on this before we go into the formalism of the clinical trial just like about clinical trials or anything yeah and I think that that's going to be like that last point there is going to be a big one going forward of like how do you balance benefits to the patient benefits to your trial benefits to essentially the company like there's a lot of different benefits and costs that you have to weigh into this and so these models are going to get very complicated when you start to distill this into something especially with health related so it'll be very interesting to see how this evolves okay so here's how they do it our setup is as follows for each new cohort of participants we decide upon the randomization ratio to adopt that's the orange subscript r of policy so this is policy on a randomization ratio there's three options so this is a discrete but linearly ranked not fully categorical policy decision where one half would be the 50-50 sampling between the two groups whereas you know a priori that sampling in a skewed ratio is going to be less informative like if you sampled only from one you would obviously be maximally uninformed about the other however what's going to end up being reflected in the policy decision to shift to a one-third or a two-third which is focusing observations on one branch of the study more than the other is going to focus on the explicit quantitative preference for observations of survival so that's going to be very interesting to see how the time variable which relates to the experimental design but by way of modeling the death curves of the participants and how different preferences for complementary processes of reducing uncertainty about the treatment specific death curves and not preferring to see death observations because that would introduce the pragmatic imperative to measure low survival experiments so there's a lot of complexity in there from the public health side also in this very simple and interpretable way that like this is like a Bayesian light switch with 50-50 information seeking mode or tilt it one way or the other to bias observations whereas if no information had to be resolved then the policy selection would orient towards observing long survival whereas if that was somehow changed then it would have to be adaptively sampled on the fly and changing these ratios and all that what do you what do you think about this yeah and what we're going to kind of get into is especially with these sorts of health decisions you want people to survive like that's your that's your primary goal in a lot of these you want to see an effect you want to see a positive effect of your treatment one way or the other you know if it's the placebo that's the positive or it's the actual treatment that's the positive you want people to survive so this is kind of getting into that ethics of making sure that when you design these things that you're doing the maximum good to your participants who you know may not have you know much hope to stand on doing some of these crises or epidemics or whatever they are experiencing at the time so you want to design this in such a way that you know you keep them the patients in mind that is the whole point of this and so by having a Bayesian kind of preference and bias to keeping the patient alive and the best outcome you're maximizing how the patient's outcome in the patient's life thanks for adding that another point to make this is from figure nine that's gonna come up but it really highlights how sparse and few and interpretable the Bayesian graphical formalism is and message passing which a lot of the equations describe and the discussion about rx and fur touched upon message passing gives procedural ways to implement this in computational systems because it's sometimes hard to go from the simplicity of like this graphical model to fitting it iteratively on complex data sets but it's pretty clear to see how different variables are upstream or downstream of other variables and also how the time sampling can be shown to be which is the upstream of data sampled as these other factors are but it has a separable interpretable calculable epistemic value that doesn't have a certain kind of connection to randomization ratio for example so being able to have explicit statistical calculations and directnesses where the follow-up time doesn't influence the treatment group ratio or the randomization ratio or other processes gives a type of interpretability that the generative model gives us the equations for and then the pragmatic challenges are about actually implementing that and then even if the computational component were totally addressed and abstracted away that would basically center these broader questions which i think the health example is a great like jumping off point four yeah and uh you have probably recalled from all the other different uh figures despite how simple this figure is the other figures the the plots were very basic the models themselves like you just had the three nodes you know converging on the y so despite how simple this looks you are adding more complexity to these um to these systems and the more complexity that you add the harder it is the more computationally intensive it is and so this is that question of how big can you go you know how how many nodes can you add how many parameters can you add how much complexity can you add to the system before it starts to break down or not perform as well as you would hope yeah so other than bringing in that randomization ratio kind of expression of preference this model differs from the prior one in that it's defined that the kind of cognitive map is not the territory they're different families so that's what motivates this um approximation approach so this is a simple displacement where still it's a trackable problem as they'll unfold however the simulation family chosen for the for like the approximation basically the approximation could apply to any data set but it might be woefully inadequate like it might fit only one component of it so that's again part of the interesting question is like how similar does the statistical model have to be or what information does it really bring in and how to to model or work with an empirical side um but just on a more general statistical level equations 15 16 17 describe some of the technical details of the incremental optimization gradient scheme the newton optimization variational plus we'll talk to thomas et al figure nine is displaying the kind of before picture for the randomized control trial so here's where that graphical model is that was shown earlier and then here are these two groups and their survival through time and different sampling uh choices that are made then just to to jump to figure 10 has the same layout as figure nine but now using the expected information gain from equation 18 to guide sampling of data so this is just to show the impact of that active data sampling and it will drop back to the equation uh there are some notable differences between the choices made in figure 10 compared with nine nine choices 10 uh the most obvious of these is that the follow-up time selected have been moved later once optimal sampling is employed this makes intuitive sense as a later follow-up time is informative about the survival probabilities at all prior times whereas an earlier follow-up time is not informative about survival probabilities at later time um where it gets in the final uh simulation brings in the random sampling plus the preference element here's where the symmetry is broken to also want the measurement of survival as more likely than death which is how the preferences are specified in active inference um and then the policy switch is reflected in this like um part where the observations are shifted later because there's there's less than a threshold of information to gain by having them earlier and then they uh even if there is equal variance i'm not exactly sure we can ask between the two branches there is uh an over sampling for the group with the higher survival which in this case was the placebo group so anything to add on this preference element this is the the like the crux of the whole making sure that you optimize you know the patient's outcome on this because the treatments in this scenario were not um were not beneficial they're actually harmful and so throughout the course of the study this by utilizing this model you actually randomized more people into the placebo group which caused a greater survival of these uh individuals and so you're you can already see the effect that this sort of methodology has on clinical trials because you're optimizing the outcome and i think that is exactly what you want to do in these health decisions in these trials in these things that impact human health uh or humanity in general you want to optimize the outcome uh and you know in this way you're actually reducing the overall harm to patients yeah interesting um here's the specification for the information gain so uh bringing in the the form of the message is required for equation three now with all these extra components that have been added in with time variability demographic and the sampling they write out some of the technical details for the approximations in the variational Laplace and then some aspects about those models which we can ask about but figure 11 basically shows the big change which is that uh as you go from having a a flat sampling distribution across time and across treatment groups you can actually do better than that basically by choosing a sampling regime that makes sense given the costs of sampling so that's just very interesting because it it really does look like given the the possibility for these two lines to diverge their divergence would be largest later on whereas if you could only schedule like one check for every person if it was something that was expected to happen later in life then sampling all the young wouldn't even make sense so then one approach is like flat sampling but that is kind of sometimes erroneously called uh like unbiased or or uninformative but it is very informative and then this is pointing towards how there can be better sampling than just trying to go flat across the entire latent state estimate if there are priors relating to something they can be leveraged as part of the probabilistic sampling and adapted to the data set at the in the end however for picking prior families for the active data selection that's a big question about how much it will change how the algorithms work so I guess that kind of takes us to the discussion the paper's focus has been on illustrating how we might make use of information seeking objectives augmented with costs which gave kind of the exit criterion or preferences which gives the biased pragmatic part of data selection to choose the best data to optimize our inferences and they highlight that the maximum entropy would yield identical results in several of our examples so that'll be interesting like what were the examples where maximum entropy and the information gain are identical and then what are the real world or the statistical settings when the variance around predicted outcomes is in homogeneous how does the full cognitive epistemic model based objective do differently than the max and distribution dispersal kind of null hypothesis any thoughts on this no I think you've captured it very well okay then there are several technical points worth considering for how we might advance the concepts reviewed in this paper so let's talk about each of these one refinement of the active selection process two empirical evaluation of active versus alternative sampling methods and three identifying the appropriate cost functions so we'll talk about those coming up conclusion here's the entire conclusion the key ideas involved in appeal to foveal like sampling of small portions of the total available data to minimize computational cost that's a very cool way to put it and it highlights that kind of like sequential scanning but also opens up some very exciting directions about how efficient that could be for some but not other kinds of problems and how those problems could be identified or those patterns could be filtered for to where different kinds of succating models would be adaptive or not so like these are all fun discussions we'll have and then they kind of brought all the theoretical components together at the end with the Bayes adaptive clinical trial with the cost of sampling constraints on sampling and also the preference for survival so what are your overall thoughts or what are you excited about for the ones to come I'm excited to kind of see me I mean this is a fairly recent paper I'm excited to see kind of where they have gone since this paper was published you know they had a number of kind of next directions I would love to see what of those directions have they've taken what they've compared it to other other sampling techniques this show shows a lot of promise going forward for very complex and you know ethical situations or situations where ethics are going to be a huge component so kind of where that is what they're going into and kind of where they see you know further improvements I still kind of want to know what would happen or what of these other time metrics or what are the situations these time metrics would or alternative time models would be applicable to or if this really is like the de facto just the way it needs to be done totally totally fair I think that could very well be the case I would just love to hear exactly you know if you did a hidden Markov model how would that look you know is is there a benefit of that over the selection that they have is there does it provide more or less versatility in the models these are things that are going to be I think very interesting going forward and then you know how do we like the like you noted in the discussion how do you optimize those cost functions what's what's a cost you know you're dealing with clinical trials it's a human life that's a cost time it's a cost there's also just computer time how much you can actually get how much compute you can get and give them all the time some of these machine learning models that you might want to apply this to or you know select data going into sometimes takes a long time to train how are you going to sample data that's going into those models how are you going to continually feed those models appropriate data going forward so this is a huge broad category of um directions that you can go clinical trials was a very wonderful example of a complex situation that is you know right there applicable to human life um but then you also have just data science in general like how are you going to utilize this to you know going more broad how are you going to utilize this just going forward in any data sense data is growing it'll continue to grow it will never really stop growing so it's going to be more and more important going forward to have these methods more broadly used and random's nice random's really really good but this holds a lot of promise to being I would love to just hear their thoughts on where that's going where they see that yeah a lot of a lot of interesting directions a few things that made me think of one was about search and about relational search concepts page rank and everything syntactic semantic new kinds of search algorithms and personalization for search and learning and updating and to what extent like explicit cognitive modeling would change the way that different recommendation algorithms or different kinds of computer systems would work I'll read a question um and then any any other questions otherwise this is our our last slide so thank you Christopher um okay glia maximalist wrote interesting point about biased and unbiased sampling schemes perhaps this points out the fact that unbiased approaches are the wrong thing to strive for in research study design what do you think about that unbiased research in study design um I think there's a time and place for unbiased and I think there's a time and place for bias I think that but you when you accept a bias into your model or refute take bias out of your model you need to understand why you're doing certain bias you don't want to have you know researcher bias is something that's a huge bias that you want to not have for example but in certain cases um like in this paper where you actually do want to have a bias you want to make an intelligent decision know why you're making that decision call it out and then build it into your model that would be my problem yeah that's interesting like there are certain statistical distributions the bias or the constraints on which to find the research study whereas other ones that can have an explicitly strictly negative like data loss or something but then the trade-offs of how that distribution actually interacts with others can enter into this more complex experimental calculus that relates to like well all these different experimental factors and so the optimal experiment for for different labs or different moments for the lab could look extremely different and that's going to be the case there's going to be just first behavior out of the way but then the question is how is that actually driven in a way that is doing better than drawing from distributions however even that does interestingly well for the right variables and you can mind bias all over the place I bias in data design in experiments is can be very useful so it'll just be interesting I think it'll be case by case it's the way I see it okay well do you have any last comments I think that the main thing here is I'm just very excited to see this paper come out I would love to see how this is going to evolve over time see if this can be applied to different technologies different areas I'm really excited just to see where this is going because I think this is just right on the cusp of what's needed awesome thank you okay we will look forward to it thank you see you all right thanks guys you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.76, "text": " Alright, hello and welcome. It's May 24th, 2024. We're in ACTIMF livestream number 57.0,", "tokens": [50364, 2798, 11, 7751, 293, 2928, 13, 467, 311, 1891, 4022, 392, 11, 45237, 13, 492, 434, 294, 8157, 5422, 44, 37, 29782, 1230, 21423, 13, 15, 11, 51152], "temperature": 0.0, "avg_logprob": -0.2478242957073709, "compression_ratio": 1.3989637305699483, "no_speech_prob": 0.15173715353012085}, {"id": 1, "seek": 0, "start": 15.76, "end": 20.84, "text": " doing background and context video for the active data selection and information seeking", "tokens": [51152, 884, 3678, 293, 4319, 960, 337, 264, 4967, 1412, 9450, 293, 1589, 11670, 51406], "temperature": 0.0, "avg_logprob": -0.2478242957073709, "compression_ratio": 1.3989637305699483, "no_speech_prob": 0.15173715353012085}, {"id": 2, "seek": 0, "start": 20.84, "end": 26.72, "text": " paper and series. So welcome to the ACTIMF Institute. We're a participatory online institute", "tokens": [51406, 3035, 293, 2638, 13, 407, 2928, 281, 264, 8157, 5422, 44, 37, 9446, 13, 492, 434, 257, 3421, 4745, 2950, 26860, 51700], "temperature": 0.0, "avg_logprob": -0.2478242957073709, "compression_ratio": 1.3989637305699483, "no_speech_prob": 0.15173715353012085}, {"id": 3, "seek": 2672, "start": 26.72, "end": 31.36, "text": " that is communicating, learning and practicing applied active inference. This is a recorded", "tokens": [50364, 300, 307, 17559, 11, 2539, 293, 11350, 6456, 4967, 38253, 13, 639, 307, 257, 8287, 50596], "temperature": 0.0, "avg_logprob": -0.17596952731792742, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.029749630019068718}, {"id": 4, "seek": 2672, "start": 31.36, "end": 36.0, "text": " and archived livestream. Please provide feedback so we can improve our work. All backgrounds", "tokens": [50596, 293, 3912, 3194, 29782, 13, 2555, 2893, 5824, 370, 321, 393, 3470, 527, 589, 13, 1057, 17336, 50828], "temperature": 0.0, "avg_logprob": -0.17596952731792742, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.029749630019068718}, {"id": 5, "seek": 2672, "start": 36.0, "end": 40.72, "text": " perspectives are welcome. And we'll follow video etiquette for live streams. Head over", "tokens": [50828, 16766, 366, 2928, 13, 400, 321, 603, 1524, 960, 42177, 3007, 337, 1621, 15842, 13, 11398, 670, 51064], "temperature": 0.0, "avg_logprob": -0.17596952731792742, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.029749630019068718}, {"id": 6, "seek": 2672, "start": 40.72, "end": 47.76, "text": " to active inference.org to learn more about any of the projects, including the live streams.", "tokens": [51064, 281, 4967, 38253, 13, 4646, 281, 1466, 544, 466, 604, 295, 264, 4455, 11, 3009, 264, 1621, 15842, 13, 51416], "temperature": 0.0, "avg_logprob": -0.17596952731792742, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.029749630019068718}, {"id": 7, "seek": 4776, "start": 47.76, "end": 56.879999999999995, "text": " Today, we're going to do together a background first pass on a very interesting paper from", "tokens": [50364, 2692, 11, 321, 434, 516, 281, 360, 1214, 257, 3678, 700, 1320, 322, 257, 588, 1880, 3035, 490, 50820], "temperature": 0.0, "avg_logprob": -0.1322594346671269, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.1623304784297943}, {"id": 8, "seek": 4776, "start": 56.879999999999995, "end": 61.68, "text": " Thomas Parr, Carl Friston and Peter Seidman, active data selection and information seeking", "tokens": [50820, 8500, 47890, 11, 14256, 1526, 47345, 293, 6508, 1100, 327, 1601, 11, 4967, 1412, 9450, 293, 1589, 11670, 51060], "temperature": 0.0, "avg_logprob": -0.1322594346671269, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.1623304784297943}, {"id": 9, "seek": 4776, "start": 61.68, "end": 69.75999999999999, "text": " from 2024. In this video, we're going to introduce ourselves, talk about big questions, go through", "tokens": [51060, 490, 45237, 13, 682, 341, 960, 11, 321, 434, 516, 281, 5366, 4175, 11, 751, 466, 955, 1651, 11, 352, 807, 51464], "temperature": 0.0, "avg_logprob": -0.1322594346671269, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.1623304784297943}, {"id": 10, "seek": 4776, "start": 69.75999999999999, "end": 75.12, "text": " the keywords of the paper, then most of the sections, section by section. And as always,", "tokens": [51464, 264, 21009, 295, 264, 3035, 11, 550, 881, 295, 264, 10863, 11, 3541, 538, 3541, 13, 400, 382, 1009, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1322594346671269, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.1623304784297943}, {"id": 11, "seek": 7512, "start": 75.12, "end": 79.84, "text": " with the dot zero, it's just like a first pass. And we'll look forward to speaking with", "tokens": [50364, 365, 264, 5893, 4018, 11, 309, 311, 445, 411, 257, 700, 1320, 13, 400, 321, 603, 574, 2128, 281, 4124, 365, 50600], "temperature": 0.0, "avg_logprob": -0.15612957212660047, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.020326323807239532}, {"id": 12, "seek": 7512, "start": 79.84, "end": 86.16000000000001, "text": " hopefully some of the authors in the coming weeks, and also looking what people ask about. So", "tokens": [50600, 4696, 512, 295, 264, 16552, 294, 264, 1348, 3259, 11, 293, 611, 1237, 437, 561, 1029, 466, 13, 407, 50916], "temperature": 0.0, "avg_logprob": -0.15612957212660047, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.020326323807239532}, {"id": 13, "seek": 7512, "start": 88.0, "end": 94.08000000000001, "text": " Christopher, let us introduce ourselves and go from there. Thanks a lot also for helping in", "tokens": [51008, 20649, 11, 718, 505, 5366, 4175, 293, 352, 490, 456, 13, 2561, 257, 688, 611, 337, 4315, 294, 51312], "temperature": 0.0, "avg_logprob": -0.15612957212660047, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.020326323807239532}, {"id": 14, "seek": 7512, "start": 94.08000000000001, "end": 101.12, "text": " the dot zero preparation. Happily. Yeah, so I'm Christopher Bennett. I'm a bioinformatics", "tokens": [51312, 264, 5893, 4018, 13081, 13, 7412, 953, 13, 865, 11, 370, 286, 478, 20649, 40620, 13, 286, 478, 257, 12198, 37811, 30292, 51664], "temperature": 0.0, "avg_logprob": -0.15612957212660047, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.020326323807239532}, {"id": 15, "seek": 10112, "start": 101.12, "end": 109.76, "text": " scientist. I do a lot of data mangling data analysis and that sort of thing. This paper was", "tokens": [50364, 12662, 13, 286, 360, 257, 688, 295, 1412, 32432, 1688, 1412, 5215, 293, 300, 1333, 295, 551, 13, 639, 3035, 390, 50796], "temperature": 0.0, "avg_logprob": -0.09758135405453769, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.013218815438449383}, {"id": 16, "seek": 10112, "start": 109.76, "end": 116.0, "text": " of great interest to me as we kind of go into this more data driven era in making sure that with", "tokens": [50796, 295, 869, 1179, 281, 385, 382, 321, 733, 295, 352, 666, 341, 544, 1412, 9555, 4249, 294, 1455, 988, 300, 365, 51108], "temperature": 0.0, "avg_logprob": -0.09758135405453769, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.013218815438449383}, {"id": 17, "seek": 10112, "start": 116.0, "end": 121.12, "text": " such large data sets that we have, making sure that we can actually select relevant data for any of", "tokens": [51108, 1270, 2416, 1412, 6352, 300, 321, 362, 11, 1455, 988, 300, 321, 393, 767, 3048, 7340, 1412, 337, 604, 295, 51364], "temperature": 0.0, "avg_logprob": -0.09758135405453769, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.013218815438449383}, {"id": 18, "seek": 10112, "start": 121.12, "end": 126.80000000000001, "text": " our applications going forward, be it machine learning or whatever we're trying to do.", "tokens": [51364, 527, 5821, 516, 2128, 11, 312, 309, 3479, 2539, 420, 2035, 321, 434, 1382, 281, 360, 13, 51648], "temperature": 0.0, "avg_logprob": -0.09758135405453769, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.013218815438449383}, {"id": 19, "seek": 13112, "start": 131.44, "end": 137.28, "text": " And I'm Daniel. I'm a researcher in California and also was drawn to this on one hand on the", "tokens": [50380, 400, 286, 478, 8033, 13, 286, 478, 257, 21751, 294, 5384, 293, 611, 390, 10117, 281, 341, 322, 472, 1011, 322, 264, 50672], "temperature": 0.0, "avg_logprob": -0.08315995262890327, "compression_ratio": 1.5545023696682465, "no_speech_prob": 0.0012447632616385818}, {"id": 20, "seek": 13112, "start": 137.28, "end": 144.88, "text": " applied side, the idea of more efficient and effective data sampling. And then on the more", "tokens": [50672, 6456, 1252, 11, 264, 1558, 295, 544, 7148, 293, 4942, 1412, 21179, 13, 400, 550, 322, 264, 544, 51052], "temperature": 0.0, "avg_logprob": -0.08315995262890327, "compression_ratio": 1.5545023696682465, "no_speech_prob": 0.0012447632616385818}, {"id": 21, "seek": 13112, "start": 145.52, "end": 149.44, "text": " theory side, the connection with epistemic value information gain.", "tokens": [51084, 5261, 1252, 11, 264, 4984, 365, 2388, 468, 3438, 2158, 1589, 6052, 13, 51280], "temperature": 0.0, "avg_logprob": -0.08315995262890327, "compression_ratio": 1.5545023696682465, "no_speech_prob": 0.0012447632616385818}, {"id": 22, "seek": 13112, "start": 150.8, "end": 156.0, "text": " So here are some of the big questions. Why don't you add some detail to this?", "tokens": [51348, 407, 510, 366, 512, 295, 264, 955, 1651, 13, 1545, 500, 380, 291, 909, 512, 2607, 281, 341, 30, 51608], "temperature": 0.0, "avg_logprob": -0.08315995262890327, "compression_ratio": 1.5545023696682465, "no_speech_prob": 0.0012447632616385818}, {"id": 23, "seek": 15600, "start": 156.96, "end": 162.0, "text": " Absolutely. So there's five major big questions that I had after reading this.", "tokens": [50412, 7021, 13, 407, 456, 311, 1732, 2563, 955, 1651, 300, 286, 632, 934, 3760, 341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17476910513800545, "compression_ratio": 1.59375, "no_speech_prob": 0.001133509213104844}, {"id": 24, "seek": 15600, "start": 163.52, "end": 165.6, "text": " For the most part, it boils down to", "tokens": [50740, 1171, 264, 881, 644, 11, 309, 35049, 760, 281, 50844], "temperature": 0.0, "avg_logprob": -0.17476910513800545, "compression_ratio": 1.59375, "no_speech_prob": 0.001133509213104844}, {"id": 25, "seek": 15600, "start": 168.16, "end": 173.28, "text": " doing our sampling. You can do sampling over time and sampling of different data sets in different", "tokens": [50972, 884, 527, 21179, 13, 509, 393, 360, 21179, 670, 565, 293, 21179, 295, 819, 1412, 6352, 294, 819, 51228], "temperature": 0.0, "avg_logprob": -0.17476910513800545, "compression_ratio": 1.59375, "no_speech_prob": 0.001133509213104844}, {"id": 26, "seek": 15600, "start": 173.28, "end": 182.08, "text": " ways. Is there a way that we can intelligently select the data that we're going for the time", "tokens": [51228, 2098, 13, 1119, 456, 257, 636, 300, 321, 393, 5613, 2276, 3048, 264, 1412, 300, 321, 434, 516, 337, 264, 565, 51668], "temperature": 0.0, "avg_logprob": -0.17476910513800545, "compression_ratio": 1.59375, "no_speech_prob": 0.001133509213104844}, {"id": 27, "seek": 18208, "start": 183.04000000000002, "end": 189.12, "text": " the time that we're trying to select? Is there a way that we can understand how the", "tokens": [50412, 264, 565, 300, 321, 434, 1382, 281, 3048, 30, 1119, 456, 257, 636, 300, 321, 393, 1223, 577, 264, 50716], "temperature": 0.0, "avg_logprob": -0.20183989342222822, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.011328984051942825}, {"id": 28, "seek": 18208, "start": 189.12, "end": 197.76000000000002, "text": " time aspect helps sample through time instead of just doing a dynamic or more dynamic instead of", "tokens": [50716, 565, 4171, 3665, 6889, 807, 565, 2602, 295, 445, 884, 257, 8546, 420, 544, 8546, 2602, 295, 51148], "temperature": 0.0, "avg_logprob": -0.20183989342222822, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.011328984051942825}, {"id": 29, "seek": 18208, "start": 197.76000000000002, "end": 204.96, "text": " doing a static like we're going to do time zero, time seven, time 14, time 21. Can we say, hey,", "tokens": [51148, 884, 257, 13437, 411, 321, 434, 516, 281, 360, 565, 4018, 11, 565, 3407, 11, 565, 3499, 11, 565, 5080, 13, 1664, 321, 584, 11, 4177, 11, 51508], "temperature": 0.0, "avg_logprob": -0.20183989342222822, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.011328984051942825}, {"id": 30, "seek": 18208, "start": 204.96, "end": 210.24, "text": " the differences between time one and time two are very 10.1, time point two are very interesting.", "tokens": [51508, 264, 7300, 1296, 565, 472, 293, 565, 732, 366, 588, 1266, 13, 16, 11, 565, 935, 732, 366, 588, 1880, 13, 51772], "temperature": 0.0, "avg_logprob": -0.20183989342222822, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.011328984051942825}, {"id": 31, "seek": 21024, "start": 210.24, "end": 216.4, "text": " It's a lot of data in there alone. So we'll sample one and two and then maybe sample 10. Is", "tokens": [50364, 467, 311, 257, 688, 295, 1412, 294, 456, 3312, 13, 407, 321, 603, 6889, 472, 293, 732, 293, 550, 1310, 6889, 1266, 13, 1119, 50672], "temperature": 0.0, "avg_logprob": -0.12432779584612165, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.0006070183007977903}, {"id": 32, "seek": 21024, "start": 216.4, "end": 221.28, "text": " there a way that we can intelligently select the time points that we are sampling from", "tokens": [50672, 456, 257, 636, 300, 321, 393, 5613, 2276, 3048, 264, 565, 2793, 300, 321, 366, 21179, 490, 50916], "temperature": 0.0, "avg_logprob": -0.12432779584612165, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.0006070183007977903}, {"id": 33, "seek": 21024, "start": 222.08, "end": 228.4, "text": " when we get into the time series aspect? The Piper mentioned a number of different", "tokens": [50956, 562, 321, 483, 666, 264, 565, 2638, 4171, 30, 440, 430, 15402, 2835, 257, 1230, 295, 819, 51272], "temperature": 0.0, "avg_logprob": -0.12432779584612165, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.0006070183007977903}, {"id": 34, "seek": 21024, "start": 229.52, "end": 235.20000000000002, "text": " time dimension models that you can add to the four model that they're utilizing,", "tokens": [51328, 565, 10139, 5245, 300, 291, 393, 909, 281, 264, 1451, 2316, 300, 436, 434, 26775, 11, 51612], "temperature": 0.0, "avg_logprob": -0.12432779584612165, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.0006070183007977903}, {"id": 35, "seek": 23520, "start": 235.83999999999997, "end": 240.88, "text": " one of which was a hidden Markov model, another was they mentioned a differential equation in the", "tokens": [50396, 472, 295, 597, 390, 257, 7633, 3934, 5179, 2316, 11, 1071, 390, 436, 2835, 257, 15756, 5367, 294, 264, 50648], "temperature": 0.0, "avg_logprob": -0.15887001739151177, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012427931651473045}, {"id": 36, "seek": 23520, "start": 241.51999999999998, "end": 250.56, "text": " actual model itself, in the equation itself. Is there one, are there situations that one", "tokens": [50680, 3539, 2316, 2564, 11, 294, 264, 5367, 2564, 13, 1119, 456, 472, 11, 366, 456, 6851, 300, 472, 51132], "temperature": 0.0, "avg_logprob": -0.15887001739151177, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012427931651473045}, {"id": 37, "seek": 23520, "start": 250.56, "end": 256.71999999999997, "text": " performs better over than the other? Or is what they have selected to use in the paper the optimal", "tokens": [51132, 26213, 1101, 670, 813, 264, 661, 30, 1610, 307, 437, 436, 362, 8209, 281, 764, 294, 264, 3035, 264, 16252, 51440], "temperature": 0.0, "avg_logprob": -0.15887001739151177, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012427931651473045}, {"id": 38, "seek": 23520, "start": 256.71999999999997, "end": 264.96, "text": " solution in most cases, if not all cases? When it comes to clinical trials, that was a", "tokens": [51440, 3827, 294, 881, 3331, 11, 498, 406, 439, 3331, 30, 1133, 309, 1487, 281, 9115, 12450, 11, 300, 390, 257, 51852], "temperature": 0.0, "avg_logprob": -0.15887001739151177, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012427931651473045}, {"id": 39, "seek": 26496, "start": 265.12, "end": 273.44, "text": " section in this that they discussed. There's a lot of FDA regulations of the clinical trials and", "tokens": [50372, 3541, 294, 341, 300, 436, 7152, 13, 821, 311, 257, 688, 295, 18933, 12563, 295, 264, 9115, 12450, 293, 50788], "temperature": 0.0, "avg_logprob": -0.14367624539047924, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0008556744433008134}, {"id": 40, "seek": 26496, "start": 273.44, "end": 281.67999999999995, "text": " it's very heavy red tape right now. Is there a way that there's minimum ends that you need in", "tokens": [50788, 309, 311, 588, 4676, 2182, 7314, 558, 586, 13, 1119, 456, 257, 636, 300, 456, 311, 7285, 5314, 300, 291, 643, 294, 51200], "temperature": 0.0, "avg_logprob": -0.14367624539047924, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0008556744433008134}, {"id": 41, "seek": 26496, "start": 281.67999999999995, "end": 288.24, "text": " many clinical trials to actually be considered passing? Is there a way that you can bound this", "tokens": [51200, 867, 9115, 12450, 281, 767, 312, 4888, 8437, 30, 1119, 456, 257, 636, 300, 291, 393, 5472, 341, 51528], "temperature": 0.0, "avg_logprob": -0.14367624539047924, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0008556744433008134}, {"id": 42, "seek": 28824, "start": 289.2, "end": 296.56, "text": " model that they've developed into something that you can guarantee a minimum number,", "tokens": [50412, 2316, 300, 436, 600, 4743, 666, 746, 300, 291, 393, 10815, 257, 7285, 1230, 11, 50780], "temperature": 0.0, "avg_logprob": -0.10864876376258002, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.016911828890442848}, {"id": 43, "seek": 28824, "start": 296.56, "end": 300.48, "text": " a minimum sampling that the FDA requires or any regulatory body?", "tokens": [50780, 257, 7285, 21179, 300, 264, 18933, 7029, 420, 604, 18260, 1772, 30, 50976], "temperature": 0.0, "avg_logprob": -0.10864876376258002, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.016911828890442848}, {"id": 44, "seek": 28824, "start": 303.04, "end": 308.96000000000004, "text": " Another point is the next step of how are we going to integrate this in with other machine", "tokens": [51104, 3996, 935, 307, 264, 958, 1823, 295, 577, 366, 321, 516, 281, 13365, 341, 294, 365, 661, 3479, 51400], "temperature": 0.0, "avg_logprob": -0.10864876376258002, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.016911828890442848}, {"id": 45, "seek": 28824, "start": 308.96000000000004, "end": 315.6, "text": " learning models or any downstream applications that you're going with? Is there a selection method", "tokens": [51400, 2539, 5245, 420, 604, 30621, 5821, 300, 291, 434, 516, 365, 30, 1119, 456, 257, 9450, 3170, 51732], "temperature": 0.0, "avg_logprob": -0.10864876376258002, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.016911828890442848}, {"id": 46, "seek": 31560, "start": 315.6, "end": 324.96000000000004, "text": " that we can, or how do you see these, this method kind of pre, kind of before machine learning?", "tokens": [50364, 300, 321, 393, 11, 420, 577, 360, 291, 536, 613, 11, 341, 3170, 733, 295, 659, 11, 733, 295, 949, 3479, 2539, 30, 50832], "temperature": 0.0, "avg_logprob": -0.16775622810285115, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0028892995323985815}, {"id": 47, "seek": 31560, "start": 324.96000000000004, "end": 330.8, "text": " How are we going to attach these things together so that we feed the right data into machine learning?", "tokens": [50832, 1012, 366, 321, 516, 281, 5085, 613, 721, 1214, 370, 300, 321, 3154, 264, 558, 1412, 666, 3479, 2539, 30, 51124], "temperature": 0.0, "avg_logprob": -0.16775622810285115, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0028892995323985815}, {"id": 48, "seek": 31560, "start": 332.48, "end": 338.88, "text": " Being an LLM. And then scalability and computation demands. That's going to be a big one if this", "tokens": [51208, 8891, 364, 441, 43, 44, 13, 400, 550, 15664, 2310, 293, 24903, 15107, 13, 663, 311, 516, 281, 312, 257, 955, 472, 498, 341, 51528], "temperature": 0.0, "avg_logprob": -0.16775622810285115, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0028892995323985815}, {"id": 49, "seek": 31560, "start": 338.88, "end": 344.16, "text": " is going to be something that is used routinely in industry. We need to make sure that this is", "tokens": [51528, 307, 516, 281, 312, 746, 300, 307, 1143, 40443, 294, 3518, 13, 492, 643, 281, 652, 988, 300, 341, 307, 51792], "temperature": 0.0, "avg_logprob": -0.16775622810285115, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0028892995323985815}, {"id": 50, "seek": 34416, "start": 344.16, "end": 352.48, "text": " something that is as scalable as you can get. Go from small scale, which is a lot of what they", "tokens": [50364, 746, 300, 307, 382, 38481, 382, 291, 393, 483, 13, 1037, 490, 1359, 4373, 11, 597, 307, 257, 688, 295, 437, 436, 50780], "temperature": 0.0, "avg_logprob": -0.1050365833525962, "compression_ratio": 1.6211453744493391, "no_speech_prob": 9.314207272836939e-05}, {"id": 51, "seek": 34416, "start": 352.48, "end": 356.24, "text": " show in this paper and then all the way up to the very large scale data sets that we", "tokens": [50780, 855, 294, 341, 3035, 293, 550, 439, 264, 636, 493, 281, 264, 588, 2416, 4373, 1412, 6352, 300, 321, 50968], "temperature": 0.0, "avg_logprob": -0.1050365833525962, "compression_ratio": 1.6211453744493391, "no_speech_prob": 9.314207272836939e-05}, {"id": 52, "seek": 34416, "start": 357.28000000000003, "end": 363.12, "text": " use to train LLMs and other models. Those are kind of the five major points that I have.", "tokens": [51020, 764, 281, 3847, 441, 43, 26386, 293, 661, 5245, 13, 3950, 366, 733, 295, 264, 1732, 2563, 2793, 300, 286, 362, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1050365833525962, "compression_ratio": 1.6211453744493391, "no_speech_prob": 9.314207272836939e-05}, {"id": 53, "seek": 34416, "start": 365.12, "end": 371.68, "text": " Thank you. Those are very insightful. Here were some of the big questions that I was excited about.", "tokens": [51412, 1044, 291, 13, 3950, 366, 588, 46401, 13, 1692, 645, 512, 295, 264, 955, 1651, 300, 286, 390, 2919, 466, 13, 51740], "temperature": 0.0, "avg_logprob": -0.1050365833525962, "compression_ratio": 1.6211453744493391, "no_speech_prob": 9.314207272836939e-05}, {"id": 54, "seek": 37168, "start": 372.48, "end": 378.40000000000003, "text": " So first, from a more general information gain, epistemic foraging perspective,", "tokens": [50404, 407, 700, 11, 490, 257, 544, 2674, 1589, 6052, 11, 2388, 468, 3438, 337, 3568, 4585, 11, 50700], "temperature": 0.0, "avg_logprob": -0.09350003836289891, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.000295952515443787}, {"id": 55, "seek": 37168, "start": 378.40000000000003, "end": 383.52, "text": " how do we model the implicit and explicit constraints or trade-offs or dynamics of", "tokens": [50700, 577, 360, 321, 2316, 264, 26947, 293, 13691, 18491, 420, 4923, 12, 19231, 420, 15679, 295, 50956], "temperature": 0.0, "avg_logprob": -0.09350003836289891, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.000295952515443787}, {"id": 56, "seek": 37168, "start": 383.52, "end": 389.84000000000003, "text": " information seeking? Which is often addressing a question that is left unaddressed in data science", "tokens": [50956, 1589, 11670, 30, 3013, 307, 2049, 14329, 257, 1168, 300, 307, 1411, 517, 25224, 3805, 294, 1412, 3497, 51272], "temperature": 0.0, "avg_logprob": -0.09350003836289891, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.000295952515443787}, {"id": 57, "seek": 37168, "start": 389.84000000000003, "end": 394.08, "text": " of where the data comes from. It's just about doing analysis with the data that's there.", "tokens": [51272, 295, 689, 264, 1412, 1487, 490, 13, 467, 311, 445, 466, 884, 5215, 365, 264, 1412, 300, 311, 456, 13, 51484], "temperature": 0.0, "avg_logprob": -0.09350003836289891, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.000295952515443787}, {"id": 58, "seek": 37168, "start": 394.08, "end": 398.8, "text": " But even there, as this paper will kind of get into, there's still sub-sampling and all these", "tokens": [51484, 583, 754, 456, 11, 382, 341, 3035, 486, 733, 295, 483, 666, 11, 456, 311, 920, 1422, 12, 19988, 11970, 293, 439, 613, 51720], "temperature": 0.0, "avg_logprob": -0.09350003836289891, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.000295952515443787}, {"id": 59, "seek": 39880, "start": 398.8, "end": 406.72, "text": " other factors to consider. The clinical trial example brings a very serious and very real plot", "tokens": [50364, 661, 6771, 281, 1949, 13, 440, 9115, 7308, 1365, 5607, 257, 588, 3156, 293, 588, 957, 7542, 50760], "temperature": 0.0, "avg_logprob": -0.06670766830444336, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006262809620238841}, {"id": 60, "seek": 39880, "start": 406.72, "end": 415.44, "text": " twist into the paper, which moves through several levels of adding theoretical generalization and", "tokens": [50760, 8203, 666, 264, 3035, 11, 597, 6067, 807, 2940, 4358, 295, 5127, 20864, 2674, 2144, 293, 51196], "temperature": 0.0, "avg_logprob": -0.06670766830444336, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006262809620238841}, {"id": 61, "seek": 39880, "start": 415.44, "end": 420.40000000000003, "text": " incorporating like the time dimension and other features. And then the plot twist is when the", "tokens": [51196, 33613, 411, 264, 565, 10139, 293, 661, 4122, 13, 400, 550, 264, 7542, 8203, 307, 562, 264, 51444], "temperature": 0.0, "avg_logprob": -0.06670766830444336, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006262809620238841}, {"id": 62, "seek": 39880, "start": 420.40000000000003, "end": 427.12, "text": " preferences for certain kinds of observations is specified, then there's all this interesting", "tokens": [51444, 21910, 337, 1629, 3685, 295, 18163, 307, 22206, 11, 550, 456, 311, 439, 341, 1880, 51780], "temperature": 0.0, "avg_logprob": -0.06670766830444336, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006262809620238841}, {"id": 63, "seek": 42712, "start": 427.12, "end": 430.96, "text": " behavior and decisions that come into play. So I'm sure that'll be a great discussion.", "tokens": [50364, 5223, 293, 5327, 300, 808, 666, 862, 13, 407, 286, 478, 988, 300, 603, 312, 257, 869, 5017, 13, 50556], "temperature": 0.0, "avg_logprob": -0.09238540901328032, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.011685523204505444}, {"id": 64, "seek": 42712, "start": 431.92, "end": 437.04, "text": " And then also in section four, they mentioned the streetlight effect, which is, quote,", "tokens": [50604, 400, 550, 611, 294, 3541, 1451, 11, 436, 2835, 264, 4838, 2764, 1802, 11, 597, 307, 11, 6513, 11, 50860], "temperature": 0.0, "avg_logprob": -0.09238540901328032, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.011685523204505444}, {"id": 65, "seek": 42712, "start": 437.04, "end": 441.92, "text": " the tendency to search where data are generated in a minimally ambiguous way,", "tokens": [50860, 264, 18187, 281, 3164, 689, 1412, 366, 10833, 294, 257, 4464, 379, 39465, 636, 11, 51104], "temperature": 0.0, "avg_logprob": -0.09238540901328032, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.011685523204505444}, {"id": 66, "seek": 42712, "start": 441.92, "end": 447.04, "text": " i.e. under a street lamp compared to searching elsewhere on a darkened street. And so there it's", "tokens": [51104, 741, 13, 68, 13, 833, 257, 4838, 12684, 5347, 281, 10808, 14517, 322, 257, 2877, 5320, 4838, 13, 400, 370, 456, 309, 311, 51360], "temperature": 0.0, "avg_logprob": -0.09238540901328032, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.011685523204505444}, {"id": 67, "seek": 42712, "start": 447.68, "end": 452.56, "text": " an interesting scenario and there'll be some fun art coming up and also how they distinguish the", "tokens": [51392, 364, 1880, 9005, 293, 456, 603, 312, 512, 1019, 1523, 1348, 493, 293, 611, 577, 436, 20206, 264, 51636], "temperature": 0.0, "avg_logprob": -0.09238540901328032, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.011685523204505444}, {"id": 68, "seek": 45256, "start": 452.56, "end": 458.8, "text": " sampling method with the full information seeking from the maximum entropy sampling", "tokens": [50364, 21179, 3170, 365, 264, 1577, 1589, 11670, 490, 264, 6674, 30867, 21179, 50676], "temperature": 0.0, "avg_logprob": -0.11320241022918184, "compression_ratio": 1.5, "no_speech_prob": 0.0011335259769111872}, {"id": 69, "seek": 45256, "start": 459.44, "end": 464.8, "text": " is a very subtle but very important point that I look forward to hearing more from the authors about.", "tokens": [50708, 307, 257, 588, 13743, 457, 588, 1021, 935, 300, 286, 574, 2128, 281, 4763, 544, 490, 264, 16552, 466, 13, 50976], "temperature": 0.0, "avg_logprob": -0.11320241022918184, "compression_ratio": 1.5, "no_speech_prob": 0.0011335259769111872}, {"id": 70, "seek": 45256, "start": 469.6, "end": 479.12, "text": " Okay, so just to summarize, the paper is Active Data Selection and Information Seeking,", "tokens": [51216, 1033, 11, 370, 445, 281, 20858, 11, 264, 3035, 307, 26635, 11888, 1100, 5450, 293, 15357, 1100, 38437, 11, 51692], "temperature": 0.0, "avg_logprob": -0.11320241022918184, "compression_ratio": 1.5, "no_speech_prob": 0.0011335259769111872}, {"id": 71, "seek": 47912, "start": 479.2, "end": 485.2, "text": " 2024, Thomas Parr, Carl Friston, Peter Seidman, and just a few of the aims and claims of the paper,", "tokens": [50368, 45237, 11, 8500, 47890, 11, 14256, 1526, 47345, 11, 6508, 1100, 327, 1601, 11, 293, 445, 257, 1326, 295, 264, 24683, 293, 9441, 295, 264, 3035, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11273719750198663, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.023323766887187958}, {"id": 72, "seek": 47912, "start": 485.2, "end": 490.48, "text": " and then Christopher will read the abstract. This paper aims to unpack the principles of", "tokens": [50668, 293, 550, 20649, 486, 1401, 264, 12649, 13, 639, 3035, 24683, 281, 26699, 264, 9156, 295, 50932], "temperature": 0.0, "avg_logprob": -0.11273719750198663, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.023323766887187958}, {"id": 73, "seek": 47912, "start": 490.48, "end": 494.96, "text": " active sampling of data by drawing from neurobiological research on animal exploration", "tokens": [50932, 4967, 21179, 295, 1412, 538, 6316, 490, 16499, 5614, 4383, 2132, 322, 5496, 16197, 51156], "temperature": 0.0, "avg_logprob": -0.11273719750198663, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.023323766887187958}, {"id": 74, "seek": 47912, "start": 495.52, "end": 500.88, "text": " and from the theory of optimal experimental design. Our overall aim is to provide an intuitive", "tokens": [51184, 293, 490, 264, 5261, 295, 16252, 17069, 1715, 13, 2621, 4787, 5939, 307, 281, 2893, 364, 21769, 51452], "temperature": 0.0, "avg_logprob": -0.11273719750198663, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.023323766887187958}, {"id": 75, "seek": 47912, "start": 500.88, "end": 506.08, "text": " overview of the principles that underwrite active data selection and to illustrate this with some", "tokens": [51452, 12492, 295, 264, 9156, 300, 833, 21561, 4967, 1412, 9450, 293, 281, 23221, 341, 365, 512, 51712], "temperature": 0.0, "avg_logprob": -0.11273719750198663, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.023323766887187958}, {"id": 76, "seek": 50608, "start": 506.08, "end": 512.88, "text": " simple examples. Our interest is in the selection of data, either through sampling subsets of data", "tokens": [50364, 2199, 5110, 13, 2621, 1179, 307, 294, 264, 9450, 295, 1412, 11, 2139, 807, 21179, 2090, 1385, 295, 1412, 50704], "temperature": 0.0, "avg_logprob": -0.08036305029180986, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.002322779968380928}, {"id": 77, "seek": 50608, "start": 512.88, "end": 519.04, "text": " from a large data set or through optimizing experimental design based upon the models we have", "tokens": [50704, 490, 257, 2416, 1412, 992, 420, 807, 40425, 17069, 1715, 2361, 3564, 264, 5245, 321, 362, 51012], "temperature": 0.0, "avg_logprob": -0.08036305029180986, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.002322779968380928}, {"id": 78, "seek": 50608, "start": 519.04, "end": 524.64, "text": " of how those data are generated. Optimizing data selection ensures we can achieve good", "tokens": [51012, 295, 577, 729, 1412, 366, 10833, 13, 35013, 3319, 1412, 9450, 28111, 321, 393, 4584, 665, 51292], "temperature": 0.0, "avg_logprob": -0.08036305029180986, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.002322779968380928}, {"id": 79, "seek": 50608, "start": 524.64, "end": 531.4399999999999, "text": " inference with fewer data, saving on computational and experimental costs. So if you could read the", "tokens": [51292, 38253, 365, 13366, 1412, 11, 6816, 322, 28270, 293, 17069, 5497, 13, 407, 498, 291, 727, 1401, 264, 51632], "temperature": 0.0, "avg_logprob": -0.08036305029180986, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.002322779968380928}, {"id": 80, "seek": 53144, "start": 531.44, "end": 539.9200000000001, "text": " abstracts. Absolutely, so the main points in the abstract are the Bayesian inference is typically", "tokens": [50364, 12649, 82, 13, 7021, 11, 370, 264, 2135, 2793, 294, 264, 12649, 366, 264, 7840, 42434, 38253, 307, 5850, 50788], "temperature": 0.0, "avg_logprob": -0.08146524429321289, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0032723452895879745}, {"id": 81, "seek": 53144, "start": 539.9200000000001, "end": 545.6800000000001, "text": " focused on two major issues. The first one is that you have to estimate the parameters of the model", "tokens": [50788, 5178, 322, 732, 2563, 2663, 13, 440, 700, 472, 307, 300, 291, 362, 281, 12539, 264, 9834, 295, 264, 2316, 51076], "temperature": 0.0, "avg_logprob": -0.08146524429321289, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0032723452895879745}, {"id": 82, "seek": 53144, "start": 545.6800000000001, "end": 550.8800000000001, "text": " of the data, and the second is that you need to quantify the evidence for alternative hypotheses", "tokens": [51076, 295, 264, 1412, 11, 293, 264, 1150, 307, 300, 291, 643, 281, 40421, 264, 4467, 337, 8535, 49969, 51336], "temperature": 0.0, "avg_logprob": -0.08146524429321289, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0032723452895879745}, {"id": 83, "seek": 53144, "start": 551.6800000000001, "end": 558.32, "text": " and formulate an alternative model. But this paper is actually looking at a third issue,", "tokens": [51376, 293, 47881, 364, 8535, 2316, 13, 583, 341, 3035, 307, 767, 1237, 412, 257, 2636, 2734, 11, 51708], "temperature": 0.0, "avg_logprob": -0.08146524429321289, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0032723452895879745}, {"id": 84, "seek": 55832, "start": 558.32, "end": 565.12, "text": " which is in how you're going to select the data for your models. And either through sampling", "tokens": [50364, 597, 307, 294, 577, 291, 434, 516, 281, 3048, 264, 1412, 337, 428, 5245, 13, 400, 2139, 807, 21179, 50704], "temperature": 0.0, "avg_logprob": -0.12539421639791348, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0020504549611359835}, {"id": 85, "seek": 55832, "start": 565.12, "end": 570.1600000000001, "text": " subsets of large data is typically used or optimizing experiments of design.", "tokens": [50704, 2090, 1385, 295, 2416, 1412, 307, 5850, 1143, 420, 40425, 12050, 295, 1715, 13, 50956], "temperature": 0.0, "avg_logprob": -0.12539421639791348, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0020504549611359835}, {"id": 86, "seek": 55832, "start": 572.32, "end": 579.36, "text": " Based upon the models we have these of how these data are generated. Optimizing data selection,", "tokens": [51064, 18785, 3564, 264, 5245, 321, 362, 613, 295, 577, 613, 1412, 366, 10833, 13, 35013, 3319, 1412, 9450, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12539421639791348, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0020504549611359835}, {"id": 87, "seek": 55832, "start": 579.36, "end": 586.96, "text": " what's going into the models can achieve a very good inference with fewer data points. So you're", "tokens": [51416, 437, 311, 516, 666, 264, 5245, 393, 4584, 257, 588, 665, 38253, 365, 13366, 1412, 2793, 13, 407, 291, 434, 51796], "temperature": 0.0, "avg_logprob": -0.12539421639791348, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0020504549611359835}, {"id": 88, "seek": 58696, "start": 587.0400000000001, "end": 591.9200000000001, "text": " saving on computational time costs, that sort of thing by actually reducing the amount of", "tokens": [50368, 6816, 322, 28270, 565, 5497, 11, 300, 1333, 295, 551, 538, 767, 12245, 264, 2372, 295, 50612], "temperature": 0.0, "avg_logprob": -0.1076865739460233, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007552877650596201}, {"id": 89, "seek": 58696, "start": 591.9200000000001, "end": 598.24, "text": " information that you're putting into a model. So what we're doing here is trying to unpack", "tokens": [50612, 1589, 300, 291, 434, 3372, 666, 257, 2316, 13, 407, 437, 321, 434, 884, 510, 307, 1382, 281, 26699, 50928], "temperature": 0.0, "avg_logprob": -0.1076865739460233, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007552877650596201}, {"id": 90, "seek": 58696, "start": 598.24, "end": 604.5600000000001, "text": " how you're going to actively select data, and I mean actively select data through a machine", "tokens": [50928, 577, 291, 434, 516, 281, 13022, 3048, 1412, 11, 293, 286, 914, 13022, 3048, 1412, 807, 257, 3479, 51244], "temperature": 0.0, "avg_logprob": -0.1076865739460233, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007552877650596201}, {"id": 91, "seek": 58696, "start": 605.6800000000001, "end": 613.44, "text": " optimization protocol by drawing from some of these neurobiology concepts and trying to optimize", "tokens": [51300, 19618, 10336, 538, 6316, 490, 512, 295, 613, 16499, 5614, 1793, 10392, 293, 1382, 281, 19719, 51688], "temperature": 0.0, "avg_logprob": -0.1076865739460233, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007552877650596201}, {"id": 92, "seek": 61344, "start": 613.44, "end": 622.8800000000001, "text": " the maximum information that the information can provide, maximum information gain. So they offer", "tokens": [50364, 264, 6674, 1589, 300, 264, 1589, 393, 2893, 11, 6674, 1589, 6052, 13, 407, 436, 2626, 50836], "temperature": 0.0, "avg_logprob": -0.1229420822936219, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0013248425675556064}, {"id": 93, "seek": 61344, "start": 622.8800000000001, "end": 629.2, "text": " overview of some basic points from the field and illustrates the application in some of the toy", "tokens": [50836, 12492, 295, 512, 3875, 2793, 490, 264, 2519, 293, 41718, 264, 3861, 294, 512, 295, 264, 12058, 51152], "temperature": 0.0, "avg_logprob": -0.1229420822936219, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0013248425675556064}, {"id": 94, "seek": 61344, "start": 629.2, "end": 635.5200000000001, "text": " examples that they have will go through, ranging from different approximations with basis sets", "tokens": [51152, 5110, 300, 436, 362, 486, 352, 807, 11, 25532, 490, 819, 8542, 763, 365, 5143, 6352, 51468], "temperature": 0.0, "avg_logprob": -0.1229420822936219, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0013248425675556064}, {"id": 95, "seek": 61344, "start": 636.1600000000001, "end": 642.0, "text": " to inference about how the process can evolve over time. And finally they'll go through and", "tokens": [51500, 281, 38253, 466, 577, 264, 1399, 393, 16693, 670, 565, 13, 400, 2721, 436, 603, 352, 807, 293, 51792], "temperature": 0.0, "avg_logprob": -0.1229420822936219, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0013248425675556064}, {"id": 96, "seek": 64200, "start": 642.0, "end": 647.28, "text": " consider how the approach to the data selection could be applied to design of clinical trials in", "tokens": [50364, 1949, 577, 264, 3109, 281, 264, 1412, 9450, 727, 312, 6456, 281, 1715, 295, 9115, 12450, 294, 50628], "temperature": 0.0, "avg_logprob": -0.13144937346253213, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00044414971489459276}, {"id": 97, "seek": 64200, "start": 647.28, "end": 652.64, "text": " this case, and specifically Bayes adapted clinical trial, something that is more and more", "tokens": [50628, 341, 1389, 11, 293, 4682, 7840, 279, 20871, 9115, 7308, 11, 746, 300, 307, 544, 293, 544, 50896], "temperature": 0.0, "avg_logprob": -0.13144937346253213, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00044414971489459276}, {"id": 98, "seek": 64200, "start": 653.68, "end": 660.48, "text": " seeing headlines and it's more and more used today now that we have the technology to do it.", "tokens": [50948, 2577, 23867, 293, 309, 311, 544, 293, 544, 1143, 965, 586, 300, 321, 362, 264, 2899, 281, 360, 309, 13, 51288], "temperature": 0.0, "avg_logprob": -0.13144937346253213, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00044414971489459276}, {"id": 99, "seek": 64200, "start": 662.72, "end": 669.2, "text": " Great. Okay. For the roadmap, the paper begins with introduction section,", "tokens": [51400, 3769, 13, 1033, 13, 1171, 264, 35738, 11, 264, 3035, 7338, 365, 9339, 3541, 11, 51724], "temperature": 0.0, "avg_logprob": -0.13144937346253213, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00044414971489459276}, {"id": 100, "seek": 66920, "start": 670.08, "end": 674.48, "text": " goes into Bayesian inference, generative models and expected information gain.", "tokens": [50408, 1709, 666, 7840, 42434, 38253, 11, 1337, 1166, 5245, 293, 5176, 1589, 6052, 13, 50628], "temperature": 0.0, "avg_logprob": -0.0893142410878385, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.00446747662499547}, {"id": 101, "seek": 66920, "start": 675.2800000000001, "end": 682.24, "text": " They go through a simplest worked example, and then consider a few more ways to level up that model", "tokens": [50668, 814, 352, 807, 257, 22811, 2732, 1365, 11, 293, 550, 1949, 257, 1326, 544, 2098, 281, 1496, 493, 300, 2316, 51016], "temperature": 0.0, "avg_logprob": -0.0893142410878385, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.00446747662499547}, {"id": 102, "seek": 66920, "start": 682.24, "end": 688.4000000000001, "text": " with function approximation, consideration of the time dimension with dynamic processes,", "tokens": [51016, 365, 2445, 28023, 11, 12381, 295, 264, 565, 10139, 365, 8546, 7555, 11, 51324], "temperature": 0.0, "avg_logprob": -0.0893142410878385, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.00446747662499547}, {"id": 103, "seek": 66920, "start": 688.4000000000001, "end": 692.72, "text": " and then bring in the preference for certain observations in the clinical trials.", "tokens": [51324, 293, 550, 1565, 294, 264, 17502, 337, 1629, 18163, 294, 264, 9115, 12450, 13, 51540], "temperature": 0.0, "avg_logprob": -0.0893142410878385, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.00446747662499547}, {"id": 104, "seek": 66920, "start": 692.72, "end": 698.1600000000001, "text": " Then there's a discussion and conclusion, and they also have a paragraph explaining their", "tokens": [51540, 1396, 456, 311, 257, 5017, 293, 10063, 11, 293, 436, 611, 362, 257, 18865, 13468, 641, 51812], "temperature": 0.0, "avg_logprob": -0.0893142410878385, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.00446747662499547}, {"id": 105, "seek": 69816, "start": 698.24, "end": 704.8, "text": " kind of logic there. The keywords for the paper were experimental design, active sampling,", "tokens": [50368, 733, 295, 9952, 456, 13, 440, 21009, 337, 264, 3035, 645, 17069, 1715, 11, 4967, 21179, 11, 50696], "temperature": 0.0, "avg_logprob": -0.06696797005924178, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0006986329099163413}, {"id": 106, "seek": 69816, "start": 704.8, "end": 711.04, "text": " information gain, and Bayesian inference. So the next slides are going to go into those", "tokens": [50696, 1589, 6052, 11, 293, 7840, 42434, 38253, 13, 407, 264, 958, 9788, 366, 516, 281, 352, 666, 729, 51008], "temperature": 0.0, "avg_logprob": -0.06696797005924178, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0006986329099163413}, {"id": 107, "seek": 69816, "start": 711.04, "end": 717.68, "text": " four background topics. After the four background topics, we'll speed through the sections and", "tokens": [51008, 1451, 3678, 8378, 13, 2381, 264, 1451, 3678, 8378, 11, 321, 603, 3073, 807, 264, 10863, 293, 51340], "temperature": 0.0, "avg_logprob": -0.06696797005924178, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0006986329099163413}, {"id": 108, "seek": 69816, "start": 717.68, "end": 722.88, "text": " just plant a few seeds for what we want to explore more. So first, experimental design.", "tokens": [51340, 445, 3709, 257, 1326, 9203, 337, 437, 321, 528, 281, 6839, 544, 13, 407, 700, 11, 17069, 1715, 13, 51600], "temperature": 0.0, "avg_logprob": -0.06696797005924178, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0006986329099163413}, {"id": 109, "seek": 72288, "start": 723.6, "end": 731.6, "text": " Here's two kind of classical views of experimental design in the active inference and free energy", "tokens": [50400, 1692, 311, 732, 733, 295, 13735, 6809, 295, 17069, 1715, 294, 264, 4967, 38253, 293, 1737, 2281, 50800], "temperature": 0.0, "avg_logprob": -0.0868557499301049, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.001081641297787428}, {"id": 110, "seek": 72288, "start": 731.6, "end": 741.12, "text": " principle eras. So on the left is the statistical parametric mapping, textbook, toolbox, documentation,", "tokens": [50800, 8665, 1189, 296, 13, 407, 322, 264, 1411, 307, 264, 22820, 6220, 17475, 18350, 11, 25591, 11, 44593, 11, 14333, 11, 51276], "temperature": 0.0, "avg_logprob": -0.0868557499301049, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.001081641297787428}, {"id": 111, "seek": 72288, "start": 741.12, "end": 748.24, "text": " et cetera, has multiple chapters and kinds of analyses included in the package to specify", "tokens": [51276, 1030, 11458, 11, 575, 3866, 20013, 293, 3685, 295, 37560, 5556, 294, 264, 7372, 281, 16500, 51632], "temperature": 0.0, "avg_logprob": -0.0868557499301049, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.001081641297787428}, {"id": 112, "seek": 74824, "start": 748.24, "end": 753.36, "text": " and simulate and also to recognize data according to different experimental designs.", "tokens": [50364, 293, 27817, 293, 611, 281, 5521, 1412, 4650, 281, 819, 17069, 11347, 13, 50620], "temperature": 0.0, "avg_logprob": -0.13990660508473715, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.008845508098602295}, {"id": 113, "seek": 74824, "start": 754.08, "end": 761.52, "text": " And one very hallmark or common visualization of these kinds of patterns of experimental design", "tokens": [50656, 400, 472, 588, 6500, 5638, 420, 2689, 25801, 295, 613, 3685, 295, 8294, 295, 17069, 1715, 51028], "temperature": 0.0, "avg_logprob": -0.13990660508473715, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.008845508098602295}, {"id": 114, "seek": 74824, "start": 761.52, "end": 769.04, "text": " are these design matrices. And it's just represented in this black to white gray scale,", "tokens": [51028, 366, 613, 1715, 32284, 13, 400, 309, 311, 445, 10379, 294, 341, 2211, 281, 2418, 10855, 4373, 11, 51404], "temperature": 0.0, "avg_logprob": -0.13990660508473715, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.008845508098602295}, {"id": 115, "seek": 74824, "start": 769.92, "end": 775.2, "text": " and it summarizes different kinds of measurements across different experimental settings. Like here", "tokens": [51448, 293, 309, 14611, 5660, 819, 3685, 295, 15383, 2108, 819, 17069, 6257, 13, 1743, 510, 51712], "temperature": 0.0, "avg_logprob": -0.13990660508473715, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.008845508098602295}, {"id": 116, "seek": 77520, "start": 775.2, "end": 781.36, "text": " might be six settings in the larger white blocks. And then there was variability within each of those", "tokens": [50364, 1062, 312, 2309, 6257, 294, 264, 4833, 2418, 8474, 13, 400, 550, 456, 390, 35709, 1951, 1184, 295, 729, 50672], "temperature": 0.0, "avg_logprob": -0.07966798839002552, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.0006070348317734897}, {"id": 117, "seek": 77520, "start": 781.36, "end": 785.76, "text": " trials. And those are the smaller row levels. So that's like where the data are collected.", "tokens": [50672, 12450, 13, 400, 729, 366, 264, 4356, 5386, 4358, 13, 407, 300, 311, 411, 689, 264, 1412, 366, 11087, 13, 50892], "temperature": 0.0, "avg_logprob": -0.07966798839002552, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.0006070348317734897}, {"id": 118, "seek": 77520, "start": 785.76, "end": 790.6400000000001, "text": " And a lot of this has to do with the linear operations that can occur on this kind of", "tokens": [50892, 400, 257, 688, 295, 341, 575, 281, 360, 365, 264, 8213, 7705, 300, 393, 5160, 322, 341, 733, 295, 51136], "temperature": 0.0, "avg_logprob": -0.07966798839002552, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.0006070348317734897}, {"id": 119, "seek": 77520, "start": 790.6400000000001, "end": 797.36, "text": " matrix in a general linear modeling framework. And then on the right is the experimental design", "tokens": [51136, 8141, 294, 257, 2674, 8213, 15983, 8388, 13, 400, 550, 322, 264, 558, 307, 264, 17069, 1715, 51472], "temperature": 0.0, "avg_logprob": -0.07966798839002552, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.0006070348317734897}, {"id": 120, "seek": 77520, "start": 798.08, "end": 804.88, "text": " experimenters perspective, where the experimental stimuli they output as actions are the", "tokens": [51508, 5120, 433, 4585, 11, 689, 264, 17069, 47752, 436, 5598, 382, 5909, 366, 264, 51848], "temperature": 0.0, "avg_logprob": -0.07966798839002552, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.0006070348317734897}, {"id": 121, "seek": 80488, "start": 804.88, "end": 811.04, "text": " observations going into the subjective model, like of the rat in the team is, and then the action", "tokens": [50364, 18163, 516, 666, 264, 25972, 2316, 11, 411, 295, 264, 5937, 294, 264, 1469, 307, 11, 293, 550, 264, 3069, 50672], "temperature": 0.0, "avg_logprob": -0.169565064566476, "compression_ratio": 1.75, "no_speech_prob": 0.0003150186385028064}, {"id": 122, "seek": 80488, "start": 811.04, "end": 816.96, "text": " output of the rat is the observations of the experiment of the experiment. So kind of two", "tokens": [50672, 5598, 295, 264, 5937, 307, 264, 18163, 295, 264, 5120, 295, 264, 5120, 13, 407, 733, 295, 732, 50968], "temperature": 0.0, "avg_logprob": -0.169565064566476, "compression_ratio": 1.75, "no_speech_prob": 0.0003150186385028064}, {"id": 123, "seek": 80488, "start": 816.96, "end": 827.04, "text": " different perspectives, SPM with more of a matrix multiplication, f m r i optimal design, and then", "tokens": [50968, 819, 16766, 11, 8420, 44, 365, 544, 295, 257, 8141, 27290, 11, 283, 275, 367, 741, 16252, 1715, 11, 293, 550, 51472], "temperature": 0.0, "avg_logprob": -0.169565064566476, "compression_ratio": 1.75, "no_speech_prob": 0.0003150186385028064}, {"id": 124, "seek": 80488, "start": 827.04, "end": 832.32, "text": " active inference with the more general graphical Bayesian modeling, starting to broaden the", "tokens": [51472, 4967, 38253, 365, 264, 544, 2674, 35942, 7840, 42434, 15983, 11, 2891, 281, 47045, 264, 51736], "temperature": 0.0, "avg_logprob": -0.169565064566476, "compression_ratio": 1.75, "no_speech_prob": 0.0003150186385028064}, {"id": 125, "seek": 83232, "start": 832.32, "end": 837.84, "text": " consideration of what optimal foraging and what information gain epistemic value mean.", "tokens": [50364, 12381, 295, 437, 16252, 337, 3568, 293, 437, 1589, 6052, 2388, 468, 3438, 2158, 914, 13, 50640], "temperature": 0.0, "avg_logprob": -0.08943329254786174, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.000767240475397557}, {"id": 126, "seek": 83232, "start": 837.84, "end": 842.5600000000001, "text": " These are kind of the experimental design themes and how they connect a little bit with", "tokens": [50640, 1981, 366, 733, 295, 264, 17069, 1715, 13544, 293, 577, 436, 1745, 257, 707, 857, 365, 50876], "temperature": 0.0, "avg_logprob": -0.08943329254786174, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.000767240475397557}, {"id": 127, "seek": 83232, "start": 843.36, "end": 849.84, "text": " other experimental design factors. Want to add anything? Yeah, keep in mind that a lot of these", "tokens": [50916, 661, 17069, 1715, 6771, 13, 11773, 281, 909, 1340, 30, 865, 11, 1066, 294, 1575, 300, 257, 688, 295, 613, 51240], "temperature": 0.0, "avg_logprob": -0.08943329254786174, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.000767240475397557}, {"id": 128, "seek": 83232, "start": 849.84, "end": 856.32, "text": " experiments, experimental design is a very big and very important consideration when you're", "tokens": [51240, 12050, 11, 17069, 1715, 307, 257, 588, 955, 293, 588, 1021, 12381, 562, 291, 434, 51564], "temperature": 0.0, "avg_logprob": -0.08943329254786174, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.000767240475397557}, {"id": 129, "seek": 83232, "start": 856.32, "end": 861.9200000000001, "text": " actually running any sort of science or analytics of any variety. And these experiments can actually", "tokens": [51564, 767, 2614, 604, 1333, 295, 3497, 420, 15370, 295, 604, 5673, 13, 400, 613, 12050, 393, 767, 51844], "temperature": 0.0, "avg_logprob": -0.08943329254786174, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.000767240475397557}, {"id": 130, "seek": 86192, "start": 861.92, "end": 869.12, "text": " get very large with huge, huge amounts of data. And all of that data is relevant for every", "tokens": [50364, 483, 588, 2416, 365, 2603, 11, 2603, 11663, 295, 1412, 13, 400, 439, 295, 300, 1412, 307, 7340, 337, 633, 50724], "temperature": 0.0, "avg_logprob": -0.080231093225025, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0010320120491087437}, {"id": 131, "seek": 86192, "start": 869.12, "end": 874.0, "text": " application that you want. So you want to be able to design your experiment in a way that you can", "tokens": [50724, 3861, 300, 291, 528, 13, 407, 291, 528, 281, 312, 1075, 281, 1715, 428, 5120, 294, 257, 636, 300, 291, 393, 50968], "temperature": 0.0, "avg_logprob": -0.080231093225025, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0010320120491087437}, {"id": 132, "seek": 86192, "start": 874.0, "end": 880.64, "text": " collect information in a intelligent way rather than trying to go through and just collect every", "tokens": [50968, 2500, 1589, 294, 257, 13232, 636, 2831, 813, 1382, 281, 352, 807, 293, 445, 2500, 633, 51300], "temperature": 0.0, "avg_logprob": -0.080231093225025, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0010320120491087437}, {"id": 133, "seek": 86192, "start": 880.64, "end": 885.5999999999999, "text": " data point that you can because humans in many cases are running some of these experiments and", "tokens": [51300, 1412, 935, 300, 291, 393, 570, 6255, 294, 867, 3331, 366, 2614, 512, 295, 613, 12050, 293, 51548], "temperature": 0.0, "avg_logprob": -0.080231093225025, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0010320120491087437}, {"id": 134, "seek": 88560, "start": 885.6, "end": 891.6800000000001, "text": " they are have limited time. I certainly do when I'm running these things. So I have to be very", "tokens": [50364, 436, 366, 362, 5567, 565, 13, 286, 3297, 360, 562, 286, 478, 2614, 613, 721, 13, 407, 286, 362, 281, 312, 588, 50668], "temperature": 0.0, "avg_logprob": -0.09064544330943715, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.041447609663009644}, {"id": 135, "seek": 88560, "start": 891.6800000000001, "end": 896.96, "text": " intelligent in how I set things up and how I actually collect data and what did I collect,", "tokens": [50668, 13232, 294, 577, 286, 992, 721, 493, 293, 577, 286, 767, 2500, 1412, 293, 437, 630, 286, 2500, 11, 50932], "temperature": 0.0, "avg_logprob": -0.09064544330943715, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.041447609663009644}, {"id": 136, "seek": 88560, "start": 896.96, "end": 900.64, "text": " because you only get in many of these cases, you only get one shot to collect the data,", "tokens": [50932, 570, 291, 787, 483, 294, 867, 295, 613, 3331, 11, 291, 787, 483, 472, 3347, 281, 2500, 264, 1412, 11, 51116], "temperature": 0.0, "avg_logprob": -0.09064544330943715, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.041447609663009644}, {"id": 137, "seek": 88560, "start": 900.64, "end": 906.96, "text": " you miss it, it's over. You won't have that data point. So it's very critical that you actually", "tokens": [51116, 291, 1713, 309, 11, 309, 311, 670, 13, 509, 1582, 380, 362, 300, 1412, 935, 13, 407, 309, 311, 588, 4924, 300, 291, 767, 51432], "temperature": 0.0, "avg_logprob": -0.09064544330943715, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.041447609663009644}, {"id": 138, "seek": 88560, "start": 906.96, "end": 909.9200000000001, "text": " take the experimental design seriously when you're setting these things up.", "tokens": [51432, 747, 264, 17069, 1715, 6638, 562, 291, 434, 3287, 613, 721, 493, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09064544330943715, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.041447609663009644}, {"id": 139, "seek": 90992, "start": 910.64, "end": 918.88, "text": " Great. So connecting that kind of experimental design, experimenter on a budget perspective", "tokens": [50400, 3769, 13, 407, 11015, 300, 733, 295, 17069, 1715, 11, 5120, 260, 322, 257, 4706, 4585, 50812], "temperature": 0.0, "avg_logprob": -0.19944008588790893, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.001548627857118845}, {"id": 140, "seek": 90992, "start": 918.88, "end": 928.64, "text": " with a more statistical and biologically statistical based perspective, active sampling.", "tokens": [50812, 365, 257, 544, 22820, 293, 3228, 17157, 22820, 2361, 4585, 11, 4967, 21179, 13, 51300], "temperature": 0.0, "avg_logprob": -0.19944008588790893, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.001548627857118845}, {"id": 141, "seek": 90992, "start": 928.64, "end": 933.1999999999999, "text": " So they wrote, when we look at the world around us, we are implicitly engaging in a form of active", "tokens": [51300, 407, 436, 4114, 11, 562, 321, 574, 412, 264, 1002, 926, 505, 11, 321, 366, 26947, 356, 11268, 294, 257, 1254, 295, 4967, 51528], "temperature": 0.0, "avg_logprob": -0.19944008588790893, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.001548627857118845}, {"id": 142, "seek": 90992, "start": 933.1999999999999, "end": 938.7199999999999, "text": " data sampling, also known as active sensing or active inference. So this is referencing the", "tokens": [51528, 1412, 21179, 11, 611, 2570, 382, 4967, 30654, 420, 4967, 38253, 13, 407, 341, 307, 40582, 264, 51804], "temperature": 0.0, "avg_logprob": -0.19944008588790893, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.001548627857118845}, {"id": 143, "seek": 93872, "start": 938.8000000000001, "end": 945.2, "text": " visual saccades. And just to kind of highlight how extreme the relative acuity difference is", "tokens": [50368, 5056, 4899, 66, 2977, 13, 400, 445, 281, 733, 295, 5078, 577, 8084, 264, 4972, 696, 21757, 2649, 307, 50688], "temperature": 0.0, "avg_logprob": -0.19893526349748883, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696626220829785}, {"id": 144, "seek": 93872, "start": 945.84, "end": 951.36, "text": " between the center of the eye where the gaze is focused on and the off center,", "tokens": [50720, 1296, 264, 3056, 295, 264, 3313, 689, 264, 24294, 307, 5178, 322, 293, 264, 766, 3056, 11, 50996], "temperature": 0.0, "avg_logprob": -0.19893526349748883, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696626220829785}, {"id": 145, "seek": 93872, "start": 951.36, "end": 956.64, "text": " among other visual changes. And vision is just being taken as one sensory example here.", "tokens": [50996, 3654, 661, 5056, 2962, 13, 400, 5201, 307, 445, 885, 2726, 382, 472, 27233, 1365, 510, 13, 51260], "temperature": 0.0, "avg_logprob": -0.19893526349748883, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696626220829785}, {"id": 146, "seek": 93872, "start": 957.0400000000001, "end": 962.08, "text": " It could also be thought of as like looking for books within a library or any other kind of", "tokens": [51280, 467, 727, 611, 312, 1194, 295, 382, 411, 1237, 337, 3642, 1951, 257, 6405, 420, 604, 661, 733, 295, 51532], "temperature": 0.0, "avg_logprob": -0.19893526349748883, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696626220829785}, {"id": 147, "seek": 93872, "start": 962.08, "end": 967.84, "text": " selection of what data are going to come in, even if it seems like all of it is going to", "tokens": [51532, 9450, 295, 437, 1412, 366, 516, 281, 808, 294, 11, 754, 498, 309, 2544, 411, 439, 295, 309, 307, 516, 281, 51820], "temperature": 0.0, "avg_logprob": -0.19893526349748883, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696626220829785}, {"id": 148, "seek": 96784, "start": 967.84, "end": 974.64, "text": " all of it is coming in, that still is going to be perhaps addressed with different sensors that", "tokens": [50364, 439, 295, 309, 307, 1348, 294, 11, 300, 920, 307, 516, 281, 312, 4317, 13847, 365, 819, 14840, 300, 50704], "temperature": 0.0, "avg_logprob": -0.10315752618106795, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.000709626532625407}, {"id": 149, "seek": 96784, "start": 974.64, "end": 979.84, "text": " have different variability profiles, or like there's different RNA sequencing kits that you", "tokens": [50704, 362, 819, 35709, 23693, 11, 420, 411, 456, 311, 819, 22484, 32693, 22095, 300, 291, 50964], "temperature": 0.0, "avg_logprob": -0.10315752618106795, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.000709626532625407}, {"id": 150, "seek": 96784, "start": 979.84, "end": 986.4, "text": " could buy. And so how do you balance the kind of more samples or which samples, especially as", "tokens": [50964, 727, 2256, 13, 400, 370, 577, 360, 291, 4772, 264, 733, 295, 544, 10938, 420, 597, 10938, 11, 2318, 382, 51292], "temperature": 0.0, "avg_logprob": -0.10315752618106795, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.000709626532625407}, {"id": 151, "seek": 96784, "start": 986.4, "end": 993.0400000000001, "text": " those spaces grow massive. And then just to contrast that, whereas digit recognition in a", "tokens": [51292, 729, 7673, 1852, 5994, 13, 400, 550, 445, 281, 8712, 300, 11, 9735, 14293, 11150, 294, 257, 51624], "temperature": 0.0, "avg_logprob": -0.10315752618106795, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.000709626532625407}, {"id": 152, "seek": 99304, "start": 993.04, "end": 1000.4, "text": " saccade based paradigm would focus on the motor patterns and the small centrally focused", "tokens": [50364, 4899, 30340, 2361, 24709, 576, 1879, 322, 264, 5932, 8294, 293, 264, 1359, 32199, 379, 5178, 50732], "temperature": 0.0, "avg_logprob": -0.13671501292738802, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0013669779291376472}, {"id": 153, "seek": 99304, "start": 1000.4, "end": 1006.4, "text": " visual acuity and then the motor patterns that relate to circuiting around a digit. Whereas", "tokens": [50732, 5056, 696, 21757, 293, 550, 264, 5932, 8294, 300, 10961, 281, 3510, 84, 1748, 926, 257, 14293, 13, 13813, 51032], "temperature": 0.0, "avg_logprob": -0.13671501292738802, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0013669779291376472}, {"id": 154, "seek": 99304, "start": 1006.4, "end": 1012.9599999999999, "text": " in the kind of machine learning taken all at once approach, a matrix corresponding to like the", "tokens": [51032, 294, 264, 733, 295, 3479, 2539, 2726, 439, 412, 1564, 3109, 11, 257, 8141, 11760, 281, 411, 264, 51360], "temperature": 0.0, "avg_logprob": -0.13671501292738802, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0013669779291376472}, {"id": 155, "seek": 99304, "start": 1012.9599999999999, "end": 1020.88, "text": " pixels in the MNIST dataset are simply taken in all at equivariance level. So that's just kind", "tokens": [51360, 18668, 294, 264, 376, 45, 19756, 28872, 366, 2935, 2726, 294, 439, 412, 48726, 3504, 719, 1496, 13, 407, 300, 311, 445, 733, 51756], "temperature": 0.0, "avg_logprob": -0.13671501292738802, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0013669779291376472}, {"id": 156, "seek": 102088, "start": 1020.96, "end": 1024.8, "text": " of taking in the data. There's still another higher order data selection question of like,", "tokens": [50368, 295, 1940, 294, 264, 1412, 13, 821, 311, 920, 1071, 2946, 1668, 1412, 9450, 1168, 295, 411, 11, 50560], "temperature": 0.0, "avg_logprob": -0.12431563806096348, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.004330548457801342}, {"id": 157, "seek": 102088, "start": 1024.8, "end": 1030.4, "text": " which digits do you take? If there was a large number of digits in that library. So this is", "tokens": [50560, 597, 27011, 360, 291, 747, 30, 759, 456, 390, 257, 2416, 1230, 295, 27011, 294, 300, 6405, 13, 407, 341, 307, 50840], "temperature": 0.0, "avg_logprob": -0.12431563806096348, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.004330548457801342}, {"id": 158, "seek": 102088, "start": 1031.12, "end": 1037.28, "text": " active data sampling on multiple scales, which records you pull at all, and then how the resolution", "tokens": [50876, 4967, 1412, 21179, 322, 3866, 17408, 11, 597, 7724, 291, 2235, 412, 439, 11, 293, 550, 577, 264, 8669, 51184], "temperature": 0.0, "avg_logprob": -0.12431563806096348, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.004330548457801342}, {"id": 159, "seek": 102088, "start": 1037.28, "end": 1042.16, "text": " and all the tradeoffs that are associated with using the data processing or making the experiment.", "tokens": [51184, 293, 439, 264, 4923, 19231, 300, 366, 6615, 365, 1228, 264, 1412, 9007, 420, 1455, 264, 5120, 13, 51428], "temperature": 0.0, "avg_logprob": -0.12431563806096348, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.004330548457801342}, {"id": 160, "seek": 102088, "start": 1042.72, "end": 1050.0, "text": " Yeah, add more though. And, you know, keep in mind that when you're talking about something", "tokens": [51456, 865, 11, 909, 544, 1673, 13, 400, 11, 291, 458, 11, 1066, 294, 1575, 300, 562, 291, 434, 1417, 466, 746, 51820], "temperature": 0.0, "avg_logprob": -0.12431563806096348, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.004330548457801342}, {"id": 161, "seek": 105000, "start": 1050.0, "end": 1054.8, "text": " like the visual system, you know, our visual system has access to untold amounts of information,", "tokens": [50364, 411, 264, 5056, 1185, 11, 291, 458, 11, 527, 5056, 1185, 575, 2105, 281, 16521, 348, 11663, 295, 1589, 11, 50604], "temperature": 0.0, "avg_logprob": -0.08020870502178486, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.008844957686960697}, {"id": 162, "seek": 105000, "start": 1054.8, "end": 1062.08, "text": " but our brain can't take advantage of all of that at once. So there's low energy usage of the brain", "tokens": [50604, 457, 527, 3567, 393, 380, 747, 5002, 295, 439, 295, 300, 412, 1564, 13, 407, 456, 311, 2295, 2281, 14924, 295, 264, 3567, 50968], "temperature": 0.0, "avg_logprob": -0.08020870502178486, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.008844957686960697}, {"id": 163, "seek": 105000, "start": 1062.08, "end": 1066.96, "text": " that needs to optimize the relevant information. Think, you know, your nose is right at the end", "tokens": [50968, 300, 2203, 281, 19719, 264, 7340, 1589, 13, 6557, 11, 291, 458, 11, 428, 6690, 307, 558, 412, 264, 917, 51212], "temperature": 0.0, "avg_logprob": -0.08020870502178486, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.008844957686960697}, {"id": 164, "seek": 105000, "start": 1066.96, "end": 1072.24, "text": " of your face. Your eyes are always seeing your nose, but your brain is filtering it out. And this", "tokens": [51212, 295, 428, 1851, 13, 2260, 2575, 366, 1009, 2577, 428, 6690, 11, 457, 428, 3567, 307, 30822, 309, 484, 13, 400, 341, 51476], "temperature": 0.0, "avg_logprob": -0.08020870502178486, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.008844957686960697}, {"id": 165, "seek": 105000, "start": 1072.24, "end": 1077.04, "text": " is happening all the time at all points in time. There are literal blind spots in what you are", "tokens": [51476, 307, 2737, 439, 264, 565, 412, 439, 2793, 294, 565, 13, 821, 366, 20411, 6865, 10681, 294, 437, 291, 366, 51716], "temperature": 0.0, "avg_logprob": -0.08020870502178486, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.008844957686960697}, {"id": 166, "seek": 107704, "start": 1077.04, "end": 1085.12, "text": " actually capable of intaking and processing all at once. And then additionally, when you're", "tokens": [50364, 767, 8189, 295, 560, 2456, 293, 9007, 439, 412, 1564, 13, 400, 550, 43181, 11, 562, 291, 434, 50768], "temperature": 0.0, "avg_logprob": -0.08672448735178252, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0011694100685417652}, {"id": 167, "seek": 107704, "start": 1085.12, "end": 1090.56, "text": " moving away from something like your eye or biological systems and into the experiment", "tokens": [50768, 2684, 1314, 490, 746, 411, 428, 3313, 420, 13910, 3652, 293, 666, 264, 5120, 51040], "temperature": 0.0, "avg_logprob": -0.08672448735178252, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0011694100685417652}, {"id": 168, "seek": 107704, "start": 1090.56, "end": 1095.92, "text": " design itself, you know, you oftentimes can't run a full factorial design. And there are other", "tokens": [51040, 1715, 2564, 11, 291, 458, 11, 291, 18349, 393, 380, 1190, 257, 1577, 36916, 1715, 13, 400, 456, 366, 661, 51308], "temperature": 0.0, "avg_logprob": -0.08672448735178252, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0011694100685417652}, {"id": 169, "seek": 107704, "start": 1095.92, "end": 1102.08, "text": " methods like a fractional factorial design. But those are random base. And this is trying to", "tokens": [51308, 7150, 411, 257, 17948, 1966, 36916, 1715, 13, 583, 729, 366, 4974, 3096, 13, 400, 341, 307, 1382, 281, 51616], "temperature": 0.0, "avg_logprob": -0.08672448735178252, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0011694100685417652}, {"id": 170, "seek": 110208, "start": 1102.08, "end": 1108.8799999999999, "text": " actually talk about actively selecting how you're going to set up that design. So it's kind of a,", "tokens": [50364, 767, 751, 466, 13022, 18182, 577, 291, 434, 516, 281, 992, 493, 300, 1715, 13, 407, 309, 311, 733, 295, 257, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1199273420183846, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.008576440624892712}, {"id": 171, "seek": 110208, "start": 1108.8799999999999, "end": 1115.9199999999998, "text": " you can think of it multiple different ways. Awesome. The factor that's going to come into", "tokens": [50704, 291, 393, 519, 295, 309, 3866, 819, 2098, 13, 10391, 13, 440, 5952, 300, 311, 516, 281, 808, 666, 51056], "temperature": 0.0, "avg_logprob": -0.1199273420183846, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.008576440624892712}, {"id": 172, "seek": 110208, "start": 1115.9199999999998, "end": 1123.84, "text": " play as driving the active sampling is going to be the information gain. And there's some quotes", "tokens": [51056, 862, 382, 4840, 264, 4967, 21179, 307, 516, 281, 312, 264, 1589, 6052, 13, 400, 456, 311, 512, 19963, 51452], "temperature": 0.0, "avg_logprob": -0.1199273420183846, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.008576440624892712}, {"id": 173, "seek": 110208, "start": 1123.84, "end": 1130.32, "text": " here. And equation two is shown. They write, we have conditioned our model upon the variable", "tokens": [51452, 510, 13, 400, 5367, 732, 307, 4898, 13, 814, 2464, 11, 321, 362, 35833, 527, 2316, 3564, 264, 7006, 51776], "temperature": 0.0, "avg_logprob": -0.1199273420183846, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.008576440624892712}, {"id": 174, "seek": 113032, "start": 1130.3999999999999, "end": 1135.6, "text": " pi, which represents a choice we can make in selecting our data. So data recognition,", "tokens": [50368, 3895, 11, 597, 8855, 257, 3922, 321, 393, 652, 294, 18182, 527, 1412, 13, 407, 1412, 11150, 11, 50628], "temperature": 0.0, "avg_logprob": -0.10911984973483616, "compression_ratio": 1.64, "no_speech_prob": 0.00505994213744998}, {"id": 175, "seek": 113032, "start": 1135.6, "end": 1140.96, "text": " interpretation, analysis, and so on. It's often framed as kind of like an observation type or", "tokens": [50628, 14174, 11, 5215, 11, 293, 370, 322, 13, 467, 311, 2049, 30420, 382, 733, 295, 411, 364, 14816, 2010, 420, 50896], "temperature": 0.0, "avg_logprob": -0.10911984973483616, "compression_ratio": 1.64, "no_speech_prob": 0.00505994213744998}, {"id": 176, "seek": 113032, "start": 1140.96, "end": 1149.12, "text": " sense making type activity. Here, pi for policy, as with usual, is being framed as a control or", "tokens": [50896, 2020, 1455, 2010, 5191, 13, 1692, 11, 3895, 337, 3897, 11, 382, 365, 7713, 11, 307, 885, 30420, 382, 257, 1969, 420, 51304], "temperature": 0.0, "avg_logprob": -0.10911984973483616, "compression_ratio": 1.64, "no_speech_prob": 0.00505994213744998}, {"id": 177, "seek": 113032, "start": 1149.12, "end": 1157.36, "text": " an active data selection policy, we're applying to some data set. So it adds a action element", "tokens": [51304, 364, 4967, 1412, 9450, 3897, 11, 321, 434, 9275, 281, 512, 1412, 992, 13, 407, 309, 10860, 257, 3069, 4478, 51716], "temperature": 0.0, "avg_logprob": -0.10911984973483616, "compression_ratio": 1.64, "no_speech_prob": 0.00505994213744998}, {"id": 178, "seek": 115736, "start": 1158.0, "end": 1163.1999999999998, "text": " into this sequential epistemic foraging, rather than just taking a large data set,", "tokens": [50396, 666, 341, 42881, 2388, 468, 3438, 337, 3568, 11, 2831, 813, 445, 1940, 257, 2416, 1412, 992, 11, 50656], "temperature": 0.0, "avg_logprob": -0.098934399712946, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.002800840651616454}, {"id": 179, "seek": 115736, "start": 1163.1999999999998, "end": 1169.12, "text": " and just munching it like all at once, it brings in this sequential question of where to sample,", "tokens": [50656, 293, 445, 275, 46079, 309, 411, 439, 412, 1564, 11, 309, 5607, 294, 341, 42881, 1168, 295, 689, 281, 6889, 11, 50952], "temperature": 0.0, "avg_logprob": -0.098934399712946, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.002800840651616454}, {"id": 180, "seek": 115736, "start": 1169.12, "end": 1176.1599999999999, "text": " and potentially updating where is informative to sample through time. And that I of pi is the", "tokens": [50952, 293, 7263, 25113, 689, 307, 27759, 281, 6889, 807, 565, 13, 400, 300, 286, 295, 3895, 307, 264, 51304], "temperature": 0.0, "avg_logprob": -0.098934399712946, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.002800840651616454}, {"id": 181, "seek": 115736, "start": 1176.8, "end": 1182.4799999999998, "text": " functional on that policy distribution or specific choice that can be decomposed,", "tokens": [51336, 11745, 322, 300, 3897, 7316, 420, 2685, 3922, 300, 393, 312, 22867, 1744, 11, 51620], "temperature": 0.0, "avg_logprob": -0.098934399712946, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.002800840651616454}, {"id": 182, "seek": 115736, "start": 1182.4799999999998, "end": 1186.32, "text": " all these interesting ways that we can explore more in the coming discussions.", "tokens": [51620, 439, 613, 1880, 2098, 300, 321, 393, 6839, 544, 294, 264, 1348, 11088, 13, 51812], "temperature": 0.0, "avg_logprob": -0.098934399712946, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.002800840651616454}, {"id": 183, "seek": 118632, "start": 1186.32, "end": 1188.3999999999999, "text": " What else would you add, though, about information gain?", "tokens": [50364, 708, 1646, 576, 291, 909, 11, 1673, 11, 466, 1589, 6052, 30, 50468], "temperature": 0.0, "avg_logprob": -0.07445434947590251, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.0023963293060660362}, {"id": 184, "seek": 118632, "start": 1189.4399999999998, "end": 1194.0, "text": " I think this is one of the biggest points in this whole paper is you're measuring how much", "tokens": [50520, 286, 519, 341, 307, 472, 295, 264, 3880, 2793, 294, 341, 1379, 3035, 307, 291, 434, 13389, 577, 709, 50748], "temperature": 0.0, "avg_logprob": -0.07445434947590251, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.0023963293060660362}, {"id": 185, "seek": 118632, "start": 1194.0, "end": 1199.4399999999998, "text": " information you are gaining in your model by adding these different variables in here and", "tokens": [50748, 1589, 291, 366, 19752, 294, 428, 2316, 538, 5127, 613, 819, 9102, 294, 510, 293, 51020], "temperature": 0.0, "avg_logprob": -0.07445434947590251, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.0023963293060660362}, {"id": 186, "seek": 118632, "start": 1199.4399999999998, "end": 1205.36, "text": " selecting different variables. You're effectively automatically taking out or trying to remove", "tokens": [51020, 18182, 819, 9102, 13, 509, 434, 8659, 6772, 1940, 484, 420, 1382, 281, 4159, 51316], "temperature": 0.0, "avg_logprob": -0.07445434947590251, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.0023963293060660362}, {"id": 187, "seek": 118632, "start": 1205.36, "end": 1210.32, "text": " things that have high mutual information that don't add as much. So if you have parameter A", "tokens": [51316, 721, 300, 362, 1090, 16917, 1589, 300, 500, 380, 909, 382, 709, 13, 407, 498, 291, 362, 13075, 316, 51564], "temperature": 0.0, "avg_logprob": -0.07445434947590251, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.0023963293060660362}, {"id": 188, "seek": 121032, "start": 1210.32, "end": 1215.6799999999998, "text": " and parameter B that are effectively just transformations of the same data, then you", "tokens": [50364, 293, 13075, 363, 300, 366, 8659, 445, 34852, 295, 264, 912, 1412, 11, 550, 291, 50632], "temperature": 0.0, "avg_logprob": -0.04889395603766808, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03307396546006203}, {"id": 189, "seek": 121032, "start": 1215.6799999999998, "end": 1219.84, "text": " can easily remove one of those and still have all the information that you need.", "tokens": [50632, 393, 3612, 4159, 472, 295, 729, 293, 920, 362, 439, 264, 1589, 300, 291, 643, 13, 50840], "temperature": 0.0, "avg_logprob": -0.04889395603766808, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03307396546006203}, {"id": 190, "seek": 121032, "start": 1220.8, "end": 1226.72, "text": " So it's a really, really powerful way of saying I'm trying to optimize and maximize the amount", "tokens": [50888, 407, 309, 311, 257, 534, 11, 534, 4005, 636, 295, 1566, 286, 478, 1382, 281, 19719, 293, 19874, 264, 2372, 51184], "temperature": 0.0, "avg_logprob": -0.04889395603766808, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03307396546006203}, {"id": 191, "seek": 121032, "start": 1226.72, "end": 1233.12, "text": " of information that I'm adding into the model by selecting data that actually has the information", "tokens": [51184, 295, 1589, 300, 286, 478, 5127, 666, 264, 2316, 538, 18182, 1412, 300, 767, 575, 264, 1589, 51504], "temperature": 0.0, "avg_logprob": -0.04889395603766808, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03307396546006203}, {"id": 192, "seek": 123312, "start": 1233.12, "end": 1243.28, "text": " that is going to improve them. Awesome. One other interesting angle here is often in the control", "tokens": [50364, 300, 307, 516, 281, 3470, 552, 13, 10391, 13, 1485, 661, 1880, 5802, 510, 307, 2049, 294, 264, 1969, 50872], "temperature": 0.0, "avg_logprob": -0.12395031490023174, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.0022514869924634695}, {"id": 193, "seek": 123312, "start": 1243.28, "end": 1250.0, "text": " literature, utility, reinforcement learning, etc. The epistemic value component is added in,", "tokens": [50872, 10394, 11, 14877, 11, 29280, 2539, 11, 5183, 13, 440, 2388, 468, 3438, 2158, 6542, 307, 3869, 294, 11, 51208], "temperature": 0.0, "avg_logprob": -0.12395031490023174, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.0022514869924634695}, {"id": 194, "seek": 123312, "start": 1251.12, "end": 1258.08, "text": " whereas in the structure of this paper, they start with the pure information gain perspective.", "tokens": [51264, 9735, 294, 264, 3877, 295, 341, 3035, 11, 436, 722, 365, 264, 6075, 1589, 6052, 4585, 13, 51612], "temperature": 0.0, "avg_logprob": -0.12395031490023174, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.0022514869924634695}, {"id": 195, "seek": 125808, "start": 1258.08, "end": 1263.52, "text": " And then in the clinical trial, they bring the preference in. So the pragmatic value", "tokens": [50364, 400, 550, 294, 264, 9115, 7308, 11, 436, 1565, 264, 17502, 294, 13, 407, 264, 46904, 2158, 50636], "temperature": 0.0, "avg_logprob": -0.1043484091758728, "compression_ratio": 1.66, "no_speech_prob": 0.0003920202434528619}, {"id": 196, "seek": 125808, "start": 1264.1599999999999, "end": 1269.52, "text": " comes in secondary to the information gain in how they build it up step by step.", "tokens": [50668, 1487, 294, 11396, 281, 264, 1589, 6052, 294, 577, 436, 1322, 309, 493, 1823, 538, 1823, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1043484091758728, "compression_ratio": 1.66, "no_speech_prob": 0.0003920202434528619}, {"id": 197, "seek": 125808, "start": 1272.0, "end": 1279.36, "text": " Bayesian inference is the last keyword. A lot of places to go. Here's what they showed", "tokens": [51060, 7840, 42434, 38253, 307, 264, 1036, 20428, 13, 316, 688, 295, 3190, 281, 352, 13, 1692, 311, 437, 436, 4712, 51428], "temperature": 0.0, "avg_logprob": -0.1043484091758728, "compression_ratio": 1.66, "no_speech_prob": 0.0003920202434528619}, {"id": 198, "seek": 125808, "start": 1279.9199999999998, "end": 1285.28, "text": " for equation one. And they wrote Bayesian inference is the process of inverting", "tokens": [51456, 337, 5367, 472, 13, 400, 436, 4114, 7840, 42434, 38253, 307, 264, 1399, 295, 28653, 783, 51724], "temperature": 0.0, "avg_logprob": -0.1043484091758728, "compression_ratio": 1.66, "no_speech_prob": 0.0003920202434528619}, {"id": 199, "seek": 128528, "start": 1285.76, "end": 1292.6399999999999, "text": " a model of how data, why, are generated to obtain two things, the marginal likelihood", "tokens": [50388, 257, 2316, 295, 577, 1412, 11, 983, 11, 366, 10833, 281, 12701, 732, 721, 11, 264, 16885, 22119, 50732], "temperature": 0.0, "avg_logprob": -0.11324760525725609, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0015009698690846562}, {"id": 200, "seek": 128528, "start": 1292.6399999999999, "end": 1297.52, "text": " and the posterior probability. So anything you want to say about Bayesian inference or", "tokens": [50732, 293, 264, 33529, 8482, 13, 407, 1340, 291, 528, 281, 584, 466, 7840, 42434, 38253, 420, 50976], "temperature": 0.0, "avg_logprob": -0.11324760525725609, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0015009698690846562}, {"id": 201, "seek": 128528, "start": 1297.52, "end": 1302.32, "text": " do you want to say something about Bayesian networks and graphs? I think that you've kind of", "tokens": [50976, 360, 291, 528, 281, 584, 746, 466, 7840, 42434, 9590, 293, 24877, 30, 286, 519, 300, 291, 600, 733, 295, 51216], "temperature": 0.0, "avg_logprob": -0.11324760525725609, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0015009698690846562}, {"id": 202, "seek": 128528, "start": 1302.32, "end": 1308.96, "text": " covered it here. It's, I think, barely textbook on this part. Yeah. How about graphs?", "tokens": [51216, 5343, 309, 510, 13, 467, 311, 11, 286, 519, 11, 10268, 25591, 322, 341, 644, 13, 865, 13, 1012, 466, 24877, 30, 51548], "temperature": 0.0, "avg_logprob": -0.11324760525725609, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0015009698690846562}, {"id": 203, "seek": 130896, "start": 1309.92, "end": 1317.68, "text": " On the Bayesian graphs side of it, there's multiple different ways that these Bayesian", "tokens": [50412, 1282, 264, 7840, 42434, 24877, 1252, 295, 309, 11, 456, 311, 3866, 819, 2098, 300, 613, 7840, 42434, 50800], "temperature": 0.0, "avg_logprob": -0.13329209720387178, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.004069759044796228}, {"id": 204, "seek": 130896, "start": 1317.68, "end": 1323.3600000000001, "text": " statistics is done nowadays. And the Bayesian networks and graphs is a really powerful method", "tokens": [50800, 12523, 307, 1096, 13434, 13, 400, 264, 7840, 42434, 9590, 293, 24877, 307, 257, 534, 4005, 3170, 51084], "temperature": 0.0, "avg_logprob": -0.13329209720387178, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.004069759044796228}, {"id": 205, "seek": 130896, "start": 1323.3600000000001, "end": 1330.88, "text": " going forward. I know that right now in the Institute, we have an Rx and Fur group working", "tokens": [51084, 516, 2128, 13, 286, 458, 300, 558, 586, 294, 264, 9446, 11, 321, 362, 364, 497, 87, 293, 11705, 1594, 1364, 51460], "temperature": 0.0, "avg_logprob": -0.13329209720387178, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.004069759044796228}, {"id": 206, "seek": 130896, "start": 1330.88, "end": 1337.04, "text": " right now, which is a Julia package for actually just building these network graphs, these Bayesian", "tokens": [51460, 558, 586, 11, 597, 307, 257, 18551, 7372, 337, 767, 445, 2390, 613, 3209, 24877, 11, 613, 7840, 42434, 51768], "temperature": 0.0, "avg_logprob": -0.13329209720387178, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.004069759044796228}, {"id": 207, "seek": 133704, "start": 1337.04, "end": 1342.48, "text": " graphs and doing message passing between the different factors and the different nodes of", "tokens": [50364, 24877, 293, 884, 3636, 8437, 1296, 264, 819, 6771, 293, 264, 819, 13891, 295, 50636], "temperature": 0.0, "avg_logprob": -0.0964934673715145, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0026310693938285112}, {"id": 208, "seek": 133704, "start": 1342.48, "end": 1351.44, "text": " the graph. So this is a very big up and coming area right now. It's very early in the time frame", "tokens": [50636, 264, 4295, 13, 407, 341, 307, 257, 588, 955, 493, 293, 1348, 1859, 558, 586, 13, 467, 311, 588, 2440, 294, 264, 565, 3920, 51084], "temperature": 0.0, "avg_logprob": -0.0964934673715145, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0026310693938285112}, {"id": 209, "seek": 133704, "start": 1351.44, "end": 1356.48, "text": " that this is going to become big. It's kind of on the upswing right now. And it's kind of,", "tokens": [51084, 300, 341, 307, 516, 281, 1813, 955, 13, 467, 311, 733, 295, 322, 264, 15497, 7904, 558, 586, 13, 400, 309, 311, 733, 295, 11, 51336], "temperature": 0.0, "avg_logprob": -0.0964934673715145, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0026310693938285112}, {"id": 210, "seek": 133704, "start": 1357.44, "end": 1362.0, "text": " at least I would predict, going to be kind of the next big thing going forward in the next five", "tokens": [51384, 412, 1935, 286, 576, 6069, 11, 516, 281, 312, 733, 295, 264, 958, 955, 551, 516, 2128, 294, 264, 958, 1732, 51612], "temperature": 0.0, "avg_logprob": -0.0964934673715145, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0026310693938285112}, {"id": 211, "seek": 136200, "start": 1362.08, "end": 1370.88, "text": " years or so. Yeah. We've been having a great epistemic time and Livestream 55 explores some of", "tokens": [50368, 924, 420, 370, 13, 865, 13, 492, 600, 668, 1419, 257, 869, 2388, 468, 3438, 565, 293, 31738, 377, 1572, 12330, 45473, 512, 295, 50808], "temperature": 0.0, "avg_logprob": -0.13032550316352348, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.003123424481600523}, {"id": 212, "seek": 136200, "start": 1370.88, "end": 1379.36, "text": " this in more detail. Okay. That was the background now on to the paper. So first, just to get the", "tokens": [50808, 341, 294, 544, 2607, 13, 1033, 13, 663, 390, 264, 3678, 586, 322, 281, 264, 3035, 13, 407, 700, 11, 445, 281, 483, 264, 51232], "temperature": 0.0, "avg_logprob": -0.13032550316352348, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.003123424481600523}, {"id": 213, "seek": 136200, "start": 1380.56, "end": 1386.96, "text": " last part of the paper out of the way, they have a GitHub, Thomas Parr's GitHub with the", "tokens": [51292, 1036, 644, 295, 264, 3035, 484, 295, 264, 636, 11, 436, 362, 257, 23331, 11, 8500, 47890, 311, 23331, 365, 264, 51612], "temperature": 0.0, "avg_logprob": -0.13032550316352348, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.003123424481600523}, {"id": 214, "seek": 138696, "start": 1386.96, "end": 1392.56, "text": " active data selection repo. And maybe in one of the upcoming discussions or somebody in the time", "tokens": [50364, 4967, 1412, 9450, 49040, 13, 400, 1310, 294, 472, 295, 264, 11500, 11088, 420, 2618, 294, 264, 565, 50644], "temperature": 0.0, "avg_logprob": -0.12486694988451506, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.03209077939391136}, {"id": 215, "seek": 138696, "start": 1392.56, "end": 1400.96, "text": " between can explore and transform and play with the code. And also all these different", "tokens": [50644, 1296, 393, 6839, 293, 4088, 293, 862, 365, 264, 3089, 13, 400, 611, 439, 613, 819, 51064], "temperature": 0.0, "avg_logprob": -0.12486694988451506, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.03209077939391136}, {"id": 216, "seek": 138696, "start": 1400.96, "end": 1405.76, "text": " ways that we have fun discussions around the language of the active inference model", "tokens": [51064, 2098, 300, 321, 362, 1019, 11088, 926, 264, 2856, 295, 264, 4967, 38253, 2316, 51304], "temperature": 0.0, "avg_logprob": -0.12486694988451506, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.03209077939391136}, {"id": 217, "seek": 138696, "start": 1406.32, "end": 1412.24, "text": " and how building it in different languages or using different styles like is there isn't plausible.", "tokens": [51332, 293, 577, 2390, 309, 294, 819, 8650, 420, 1228, 819, 13273, 411, 307, 456, 1943, 380, 39925, 13, 51628], "temperature": 0.0, "avg_logprob": -0.12486694988451506, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.03209077939391136}, {"id": 218, "seek": 141224, "start": 1412.24, "end": 1418.4, "text": " These have been very fun discussions that help us get at what the core of the math really is", "tokens": [50364, 1981, 362, 668, 588, 1019, 11088, 300, 854, 505, 483, 412, 437, 264, 4965, 295, 264, 5221, 534, 307, 50672], "temperature": 0.0, "avg_logprob": -0.13071894645690918, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.002800618065521121}, {"id": 219, "seek": 141224, "start": 1419.04, "end": 1423.68, "text": " and how that's independent of whether it's written in MATLAB or any other language.", "tokens": [50704, 293, 577, 300, 311, 6695, 295, 1968, 309, 311, 3720, 294, 5904, 11435, 33, 420, 604, 661, 2856, 13, 50936], "temperature": 0.0, "avg_logprob": -0.13071894645690918, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.002800618065521121}, {"id": 220, "seek": 141224, "start": 1424.64, "end": 1429.84, "text": " And then also as it is simulated, it's written here in MATLAB. And so that's kind of interesting.", "tokens": [50984, 400, 550, 611, 382, 309, 307, 41713, 11, 309, 311, 3720, 510, 294, 5904, 11435, 33, 13, 400, 370, 300, 311, 733, 295, 1880, 13, 51244], "temperature": 0.0, "avg_logprob": -0.13071894645690918, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.002800618065521121}, {"id": 221, "seek": 141224, "start": 1430.56, "end": 1434.64, "text": " Any thoughts on that or just like coding in Rx and fur or or", "tokens": [51280, 2639, 4598, 322, 300, 420, 445, 411, 17720, 294, 497, 87, 293, 2687, 420, 420, 51484], "temperature": 0.0, "avg_logprob": -0.13071894645690918, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.002800618065521121}, {"id": 222, "seek": 143464, "start": 1435.6000000000001, "end": 1443.44, "text": " Yeah, I think that with Rx and fur being, I think, relatively new on the scene, you have some of", "tokens": [50412, 865, 11, 286, 519, 300, 365, 497, 87, 293, 2687, 885, 11, 286, 519, 11, 7226, 777, 322, 264, 4145, 11, 291, 362, 512, 295, 50804], "temperature": 0.0, "avg_logprob": -0.1735748847325643, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.00555391376838088}, {"id": 223, "seek": 143464, "start": 1443.44, "end": 1450.96, "text": " these other traditional approaches with MATLAB and high MD and that sort of thing. It'll be very", "tokens": [50804, 613, 661, 5164, 11587, 365, 5904, 11435, 33, 293, 1090, 22521, 293, 300, 1333, 295, 551, 13, 467, 603, 312, 588, 51180], "temperature": 0.0, "avg_logprob": -0.1735748847325643, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.00555391376838088}, {"id": 224, "seek": 143464, "start": 1450.96, "end": 1458.3200000000002, "text": " interesting to see how these techniques evolve over time with packages like Rx and fur really,", "tokens": [51180, 1880, 281, 536, 577, 613, 7512, 16693, 670, 565, 365, 17401, 411, 497, 87, 293, 2687, 534, 11, 51548], "temperature": 0.0, "avg_logprob": -0.1735748847325643, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.00555391376838088}, {"id": 225, "seek": 145832, "start": 1459.04, "end": 1465.28, "text": " I think, changing how we approach building these models and designing them. I think that it's going", "tokens": [50400, 286, 519, 11, 4473, 577, 321, 3109, 2390, 613, 5245, 293, 14685, 552, 13, 286, 519, 300, 309, 311, 516, 50712], "temperature": 0.0, "avg_logprob": -0.09964850779329792, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01065080426633358}, {"id": 226, "seek": 145832, "start": 1465.28, "end": 1472.24, "text": " to be even more critical now in this current environment to select the data intelligently", "tokens": [50712, 281, 312, 754, 544, 4924, 586, 294, 341, 2190, 2823, 281, 3048, 264, 1412, 5613, 2276, 51060], "temperature": 0.0, "avg_logprob": -0.09964850779329792, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01065080426633358}, {"id": 227, "seek": 145832, "start": 1472.24, "end": 1478.08, "text": " going in so that you're not muddying up your models or having to build two big of models that might", "tokens": [51060, 516, 294, 370, 300, 291, 434, 406, 8933, 67, 1840, 493, 428, 5245, 420, 1419, 281, 1322, 732, 955, 295, 5245, 300, 1062, 51352], "temperature": 0.0, "avg_logprob": -0.09964850779329792, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01065080426633358}, {"id": 228, "seek": 145832, "start": 1478.08, "end": 1486.56, "text": " have information that's not as useful to the application at hand. Yeah, great. All right,", "tokens": [51352, 362, 1589, 300, 311, 406, 382, 4420, 281, 264, 3861, 412, 1011, 13, 865, 11, 869, 13, 1057, 558, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09964850779329792, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01065080426633358}, {"id": 229, "seek": 148656, "start": 1486.6399999999999, "end": 1492.3999999999999, "text": " section one introduction. So we'll try to hit on some of the key points. I'll say something briefly", "tokens": [50368, 3541, 472, 9339, 13, 407, 321, 603, 853, 281, 2045, 322, 512, 295, 264, 2141, 2793, 13, 286, 603, 584, 746, 10515, 50656], "temperature": 0.0, "avg_logprob": -0.09174198092836322, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.012818458490073681}, {"id": 230, "seek": 148656, "start": 1492.3999999999999, "end": 1499.84, "text": " and then feel free to add something if you want. Section one situates that inference and action", "tokens": [50656, 293, 550, 841, 1737, 281, 909, 746, 498, 291, 528, 13, 21804, 472, 2054, 1024, 300, 38253, 293, 3069, 51028], "temperature": 0.0, "avg_logprob": -0.09174198092836322, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.012818458490073681}, {"id": 231, "seek": 148656, "start": 1500.8, "end": 1509.52, "text": " cycle or loop or partition in terms of a statistician's job or process in modeling", "tokens": [51076, 6586, 420, 6367, 420, 24808, 294, 2115, 295, 257, 29588, 952, 311, 1691, 420, 1399, 294, 15983, 51512], "temperature": 0.0, "avg_logprob": -0.09174198092836322, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.012818458490073681}, {"id": 232, "seek": 150952, "start": 1509.52, "end": 1517.52, "text": " observations data as sampled and latent variables as models and the process by which there's", "tokens": [50364, 18163, 1412, 382, 3247, 15551, 293, 48994, 9102, 382, 5245, 293, 264, 1399, 538, 597, 456, 311, 50764], "temperature": 0.0, "avg_logprob": -0.10562387589485414, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.03020731545984745}, {"id": 233, "seek": 150952, "start": 1518.56, "end": 1526.56, "text": " kind of snapshot or bulk summarization or generativity or and how it's possible to have a", "tokens": [50816, 733, 295, 30163, 420, 16139, 14611, 2144, 420, 1337, 30142, 420, 293, 577, 309, 311, 1944, 281, 362, 257, 51216], "temperature": 0.0, "avg_logprob": -0.10562387589485414, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.03020731545984745}, {"id": 234, "seek": 150952, "start": 1526.56, "end": 1533.36, "text": " continuous resampling of informative data or how you even evaluate how data are informative in which", "tokens": [51216, 10957, 725, 335, 11970, 295, 27759, 1412, 420, 577, 291, 754, 13059, 577, 1412, 366, 27759, 294, 597, 51556], "temperature": 0.0, "avg_logprob": -0.10562387589485414, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.03020731545984745}, {"id": 235, "seek": 153336, "start": 1533.36, "end": 1540.08, "text": " way. Want to add anything? I think you've captured that very well. I'm going to actually pull out my", "tokens": [50364, 636, 13, 11773, 281, 909, 1340, 30, 286, 519, 291, 600, 11828, 300, 588, 731, 13, 286, 478, 516, 281, 767, 2235, 484, 452, 50700], "temperature": 0.0, "avg_logprob": -0.11140648286734055, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0030750909354537725}, {"id": 236, "seek": 153336, "start": 1540.08, "end": 1548.3999999999999, "text": " notes so that I can actually remember all the symbols. Why are going to be used for data and", "tokens": [50700, 5570, 370, 300, 286, 393, 767, 1604, 439, 264, 16944, 13, 1545, 366, 516, 281, 312, 1143, 337, 1412, 293, 51116], "temperature": 0.0, "avg_logprob": -0.11140648286734055, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0030750909354537725}, {"id": 237, "seek": 153336, "start": 1548.3999999999999, "end": 1556.8, "text": " data for the latent variables? So distributions of data, distributions of latent variables", "tokens": [51116, 1412, 337, 264, 48994, 9102, 30, 407, 37870, 295, 1412, 11, 37870, 295, 48994, 9102, 51536], "temperature": 0.0, "avg_logprob": -0.11140648286734055, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0030750909354537725}, {"id": 238, "seek": 155680, "start": 1556.8, "end": 1564.08, "text": " conditioned upon data coming in. So that could be seen as just one data point sequentially or a big", "tokens": [50364, 35833, 3564, 1412, 1348, 294, 13, 407, 300, 727, 312, 1612, 382, 445, 472, 1412, 935, 5123, 3137, 420, 257, 955, 50728], "temperature": 0.0, "avg_logprob": -0.08932117223739625, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0120528694242239}, {"id": 239, "seek": 155680, "start": 1564.08, "end": 1571.36, "text": " bulk vector coming in like all at once. Just continuing to move through this section, they wrote", "tokens": [50728, 16139, 8062, 1348, 294, 411, 439, 412, 1564, 13, 1449, 9289, 281, 1286, 807, 341, 3541, 11, 436, 4114, 51092], "temperature": 0.0, "avg_logprob": -0.08932117223739625, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0120528694242239}, {"id": 240, "seek": 155680, "start": 1572.6399999999999, "end": 1577.44, "text": " careful data selection is especially important when we consider the problems associated with", "tokens": [51156, 5026, 1412, 9450, 307, 2318, 1021, 562, 321, 1949, 264, 2740, 6615, 365, 51396], "temperature": 0.0, "avg_logprob": -0.08932117223739625, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0120528694242239}, {"id": 241, "seek": 155680, "start": 1577.44, "end": 1582.24, "text": " very large data sets of the sort that are now ubiquitous in machine learning and artificial", "tokens": [51396, 588, 2416, 1412, 6352, 295, 264, 1333, 300, 366, 586, 43868, 39831, 294, 3479, 2539, 293, 11677, 51636], "temperature": 0.0, "avg_logprob": -0.08932117223739625, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0120528694242239}, {"id": 242, "seek": 158224, "start": 1582.24, "end": 1589.92, "text": " intelligence settings. So just to summarize a little bit or add a few notes that came up in", "tokens": [50364, 7599, 6257, 13, 407, 445, 281, 20858, 257, 707, 857, 420, 909, 257, 1326, 5570, 300, 1361, 493, 294, 50748], "temperature": 0.0, "avg_logprob": -0.06849232426396122, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0020827928092330694}, {"id": 243, "seek": 158224, "start": 1589.92, "end": 1596.08, "text": " the paper. So other than this topic being very fascinating and very integrative in terms of a", "tokens": [50748, 264, 3035, 13, 407, 661, 813, 341, 4829, 885, 588, 10343, 293, 588, 3572, 1166, 294, 2115, 295, 257, 51056], "temperature": 0.0, "avg_logprob": -0.06849232426396122, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0020827928092330694}, {"id": 244, "seek": 158224, "start": 1596.08, "end": 1601.52, "text": " unifying approach for information and behavior etc. Also this is definitely one of the active", "tokens": [51056, 517, 5489, 3109, 337, 1589, 293, 5223, 5183, 13, 2743, 341, 307, 2138, 472, 295, 264, 4967, 51328], "temperature": 0.0, "avg_logprob": -0.06849232426396122, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0020827928092330694}, {"id": 245, "seek": 158224, "start": 1601.52, "end": 1607.2, "text": " inference questions that has a lot of pragmatic relevance as dealing with with data sets of", "tokens": [51328, 38253, 1651, 300, 575, 257, 688, 295, 46904, 32684, 382, 6260, 365, 365, 1412, 6352, 295, 51612], "temperature": 0.0, "avg_logprob": -0.06849232426396122, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0020827928092330694}, {"id": 246, "seek": 160720, "start": 1607.2, "end": 1614.24, "text": " different kind is totally day to day. And especially the way that even the examples specify", "tokens": [50364, 819, 733, 307, 3879, 786, 281, 786, 13, 400, 2318, 264, 636, 300, 754, 264, 5110, 16500, 50716], "temperature": 0.0, "avg_logprob": -0.08945831656455994, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008314819075167179}, {"id": 247, "seek": 160720, "start": 1614.24, "end": 1620.4, "text": " important settings is very clear, very direct. Though also the mathematics are very general", "tokens": [50716, 1021, 6257, 307, 588, 1850, 11, 588, 2047, 13, 10404, 611, 264, 18666, 366, 588, 2674, 51024], "temperature": 0.0, "avg_logprob": -0.08945831656455994, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008314819075167179}, {"id": 248, "seek": 160720, "start": 1620.4, "end": 1628.4, "text": " about epistemics and this motivation that they lay out in the first section about how if this", "tokens": [51024, 466, 2388, 468, 38014, 293, 341, 12335, 300, 436, 2360, 484, 294, 264, 700, 3541, 466, 577, 498, 341, 51424], "temperature": 0.0, "avg_logprob": -0.08945831656455994, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008314819075167179}, {"id": 249, "seek": 160720, "start": 1628.4, "end": 1633.04, "text": " challenge could be addressed, then there will be all these kinds of benefits that could be realized", "tokens": [51424, 3430, 727, 312, 13847, 11, 550, 456, 486, 312, 439, 613, 3685, 295, 5311, 300, 727, 312, 5334, 51656], "temperature": 0.0, "avg_logprob": -0.08945831656455994, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008314819075167179}, {"id": 250, "seek": 163304, "start": 1633.04, "end": 1640.0, "text": " with current systems and data sets. And then they provide the approach to at least getting", "tokens": [50364, 365, 2190, 3652, 293, 1412, 6352, 13, 400, 550, 436, 2893, 264, 3109, 281, 412, 1935, 1242, 50712], "temperature": 0.0, "avg_logprob": -0.13777928603322884, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.008708273060619831}, {"id": 251, "seek": 163304, "start": 1640.0, "end": 1645.28, "text": " there or towards it to optimize data selection. We first need to identify an appropriate optimality", "tokens": [50712, 456, 420, 3030, 309, 281, 19719, 1412, 9450, 13, 492, 700, 643, 281, 5876, 364, 6854, 5028, 1860, 50976], "temperature": 0.0, "avg_logprob": -0.13777928603322884, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.008708273060619831}, {"id": 252, "seek": 163304, "start": 1645.28, "end": 1649.68, "text": " criterion. And so they're going to kind of go through several stages of with different", "tokens": [50976, 46691, 13, 400, 370, 436, 434, 516, 281, 733, 295, 352, 807, 2940, 10232, 295, 365, 819, 51196], "temperature": 0.0, "avg_logprob": -0.13777928603322884, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.008708273060619831}, {"id": 253, "seek": 163304, "start": 1650.32, "end": 1657.92, "text": " generative models how that optimality criterion is defined. Anything else that", "tokens": [51228, 1337, 1166, 5245, 577, 300, 5028, 1860, 46691, 307, 7642, 13, 11998, 1646, 300, 51608], "temperature": 0.0, "avg_logprob": -0.13777928603322884, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.008708273060619831}, {"id": 254, "seek": 165792, "start": 1658.88, "end": 1664.0, "text": " And keep in mind that this is the expected information gain that they're talking about.", "tokens": [50412, 400, 1066, 294, 1575, 300, 341, 307, 264, 5176, 1589, 6052, 300, 436, 434, 1417, 466, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1218902324808055, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.005219052545726299}, {"id": 255, "seek": 165792, "start": 1664.0, "end": 1669.76, "text": " It's effectively how much do we think we're going to gain by adding this information in there.", "tokens": [50668, 467, 311, 8659, 577, 709, 360, 321, 519, 321, 434, 516, 281, 6052, 538, 5127, 341, 1589, 294, 456, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1218902324808055, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.005219052545726299}, {"id": 256, "seek": 165792, "start": 1669.76, "end": 1676.16, "text": " And then you can of course train your model by looking at the actual information gain if necessary", "tokens": [50956, 400, 550, 291, 393, 295, 1164, 3847, 428, 2316, 538, 1237, 412, 264, 3539, 1589, 6052, 498, 4818, 51276], "temperature": 0.0, "avg_logprob": -0.1218902324808055, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.005219052545726299}, {"id": 257, "seek": 165792, "start": 1676.16, "end": 1682.0, "text": " and go through kind of a learning cycle. But we're basing this all off of what do we expect", "tokens": [51276, 293, 352, 807, 733, 295, 257, 2539, 6586, 13, 583, 321, 434, 987, 278, 341, 439, 766, 295, 437, 360, 321, 2066, 51568], "temperature": 0.0, "avg_logprob": -0.1218902324808055, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.005219052545726299}, {"id": 258, "seek": 168200, "start": 1682.08, "end": 1688.64, "text": " to gain from this information. All right, section two, basing inference generative models and", "tokens": [50368, 281, 6052, 490, 341, 1589, 13, 1057, 558, 11, 3541, 732, 11, 987, 278, 38253, 1337, 1166, 5245, 293, 50696], "temperature": 0.0, "avg_logprob": -0.12231856656361775, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.013634772971272469}, {"id": 259, "seek": 168200, "start": 1688.64, "end": 1698.4, "text": " expected information gain. In this equation three, I won't read it all. It models the Markov blanket", "tokens": [50696, 5176, 1589, 6052, 13, 682, 341, 5367, 1045, 11, 286, 1582, 380, 1401, 309, 439, 13, 467, 5245, 264, 3934, 5179, 17907, 51184], "temperature": 0.0, "avg_logprob": -0.12231856656361775, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.013634772971272469}, {"id": 260, "seek": 168200, "start": 1698.4, "end": 1704.8, "text": " formalism in terms of upstream and downstream causal relationships in terms of messages that are", "tokens": [51184, 9860, 1434, 294, 2115, 295, 33915, 293, 30621, 38755, 6159, 294, 2115, 295, 7897, 300, 366, 51504], "temperature": 0.0, "avg_logprob": -0.12231856656361775, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.013634772971272469}, {"id": 261, "seek": 168200, "start": 1704.8, "end": 1711.2, "text": " passed along edges of a factor graph. They introduce in this paper the lambda operator", "tokens": [51504, 4678, 2051, 8819, 295, 257, 5952, 4295, 13, 814, 5366, 294, 341, 3035, 264, 13607, 12973, 51824], "temperature": 0.0, "avg_logprob": -0.12231856656361775, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.013634772971272469}, {"id": 262, "seek": 171120, "start": 1711.2, "end": 1717.92, "text": " to indicate either summation or integration. So this is across continuous variables or discrete", "tokens": [50364, 281, 13330, 2139, 28811, 420, 10980, 13, 407, 341, 307, 2108, 10957, 9102, 420, 27706, 50700], "temperature": 0.0, "avg_logprob": -0.0987037291009742, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.00020342266361694783}, {"id": 263, "seek": 171120, "start": 1717.92, "end": 1724.96, "text": " variables. And we'll explore this more with the authors, hopefully anything you want to add on", "tokens": [50700, 9102, 13, 400, 321, 603, 6839, 341, 544, 365, 264, 16552, 11, 4696, 1340, 291, 528, 281, 909, 322, 51052], "temperature": 0.0, "avg_logprob": -0.0987037291009742, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.00020342266361694783}, {"id": 264, "seek": 171120, "start": 1724.96, "end": 1734.16, "text": " equation three. More that you know the information gain is a function of the data that you sample.", "tokens": [51052, 5367, 1045, 13, 5048, 300, 291, 458, 264, 1589, 6052, 307, 257, 2445, 295, 264, 1412, 300, 291, 6889, 13, 51512], "temperature": 0.0, "avg_logprob": -0.0987037291009742, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.00020342266361694783}, {"id": 265, "seek": 171120, "start": 1734.16, "end": 1738.48, "text": " So depending on how you sample that data, you're going to get different information gain out of", "tokens": [51512, 407, 5413, 322, 577, 291, 6889, 300, 1412, 11, 291, 434, 516, 281, 483, 819, 1589, 6052, 484, 295, 51728], "temperature": 0.0, "avg_logprob": -0.0987037291009742, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.00020342266361694783}, {"id": 266, "seek": 173848, "start": 1738.56, "end": 1745.3600000000001, "text": " it as you would expect. And then you start to get into the message passing, which is that base graph", "tokens": [50368, 309, 382, 291, 576, 2066, 13, 400, 550, 291, 722, 281, 483, 666, 264, 3636, 8437, 11, 597, 307, 300, 3096, 4295, 50708], "temperature": 0.0, "avg_logprob": -0.09608517993580211, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0015486414777114987}, {"id": 267, "seek": 173848, "start": 1745.3600000000001, "end": 1751.2, "text": " and factor graph, I guess, challenge going forward that that construct when you build", "tokens": [50708, 293, 5952, 4295, 11, 286, 2041, 11, 3430, 516, 2128, 300, 300, 7690, 562, 291, 1322, 51000], "temperature": 0.0, "avg_logprob": -0.09608517993580211, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0015486414777114987}, {"id": 268, "seek": 173848, "start": 1751.2, "end": 1756.72, "text": " it in a factor graph model, you have to be able to pass the messages between the nodes effectively.", "tokens": [51000, 309, 294, 257, 5952, 4295, 2316, 11, 291, 362, 281, 312, 1075, 281, 1320, 264, 7897, 1296, 264, 13891, 8659, 13, 51276], "temperature": 0.0, "avg_logprob": -0.09608517993580211, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0015486414777114987}, {"id": 269, "seek": 173848, "start": 1757.68, "end": 1764.64, "text": " Yeah, and to kind of ground that in the data science situation, if you have a latent", "tokens": [51324, 865, 11, 293, 281, 733, 295, 2727, 300, 294, 264, 1412, 3497, 2590, 11, 498, 291, 362, 257, 48994, 51672], "temperature": 0.0, "avg_logprob": -0.09608517993580211, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0015486414777114987}, {"id": 270, "seek": 176464, "start": 1764.64, "end": 1770.4, "text": " state estimate and you're generating data, generative AI, synthetic data, then the latent", "tokens": [50364, 1785, 12539, 293, 291, 434, 17746, 1412, 11, 1337, 1166, 7318, 11, 23420, 1412, 11, 550, 264, 48994, 50652], "temperature": 0.0, "avg_logprob": -0.09172468874827926, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.002396535826846957}, {"id": 271, "seek": 176464, "start": 1770.4, "end": 1776.8000000000002, "text": " variable is upstream, causally, statistically from the data pseudo observation. But that might be", "tokens": [50652, 7006, 307, 33915, 11, 3302, 379, 11, 36478, 490, 264, 1412, 35899, 14816, 13, 583, 300, 1062, 312, 50972], "temperature": 0.0, "avg_logprob": -0.09172468874827926, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.002396535826846957}, {"id": 272, "seek": 176464, "start": 1776.8000000000002, "end": 1783.0400000000002, "text": " the actual real observation if you're interested in the computer model. Whereas the data recognition", "tokens": [50972, 264, 3539, 957, 14816, 498, 291, 434, 3102, 294, 264, 3820, 2316, 13, 13813, 264, 1412, 11150, 51284], "temperature": 0.0, "avg_logprob": -0.09172468874827926, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.002396535826846957}, {"id": 273, "seek": 176464, "start": 1783.0400000000002, "end": 1789.44, "text": " case where the data are upstream of the estimate of a parameter, like a risk score or something like", "tokens": [51284, 1389, 689, 264, 1412, 366, 33915, 295, 264, 12539, 295, 257, 13075, 11, 411, 257, 3148, 6175, 420, 746, 411, 51604], "temperature": 0.0, "avg_logprob": -0.09172468874827926, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.002396535826846957}, {"id": 274, "seek": 178944, "start": 1789.44, "end": 1795.76, "text": " that, then the parents of the latent state estimate is the data. So that's the recognition", "tokens": [50364, 300, 11, 550, 264, 3152, 295, 264, 48994, 1785, 12539, 307, 264, 1412, 13, 407, 300, 311, 264, 11150, 50680], "temperature": 0.0, "avg_logprob": -0.10812545591785062, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00128421769477427}, {"id": 275, "seek": 178944, "start": 1795.76, "end": 1801.68, "text": " direction. So this kind of covers the whole Bayesian update possibility spectrum in this", "tokens": [50680, 3513, 13, 407, 341, 733, 295, 10538, 264, 1379, 7840, 42434, 5623, 7959, 11143, 294, 341, 50976], "temperature": 0.0, "avg_logprob": -0.10812545591785062, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00128421769477427}, {"id": 276, "seek": 178944, "start": 1802.64, "end": 1809.1200000000001, "text": " essentially Markov blanket, but it could be in face space or time or a few other situations", "tokens": [51024, 4476, 3934, 5179, 17907, 11, 457, 309, 727, 312, 294, 1851, 1901, 420, 565, 420, 257, 1326, 661, 6851, 51348], "temperature": 0.0, "avg_logprob": -0.10812545591785062, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00128421769477427}, {"id": 277, "seek": 180912, "start": 1809.12, "end": 1821.84, "text": " they explore. All right, three, a worked example. This section lays out the overall pipeline for", "tokens": [50364, 436, 6839, 13, 1057, 558, 11, 1045, 11, 257, 2732, 1365, 13, 639, 3541, 32714, 484, 264, 4787, 15517, 337, 51000], "temperature": 0.0, "avg_logprob": -0.0830697256421286, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.013426797464489937}, {"id": 278, "seek": 180912, "start": 1821.84, "end": 1829.04, "text": " how you get from the graphical notation of the Bayesian network, whether it's viewed visually", "tokens": [51000, 577, 291, 483, 490, 264, 35942, 24657, 295, 264, 7840, 42434, 3209, 11, 1968, 309, 311, 19174, 19622, 51360], "temperature": 0.0, "avg_logprob": -0.0830697256421286, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.013426797464489937}, {"id": 279, "seek": 180912, "start": 1829.04, "end": 1836.1599999999999, "text": " graphically, like with a variable dependency structure, or whether it's just written out", "tokens": [51360, 4295, 984, 11, 411, 365, 257, 7006, 33621, 3877, 11, 420, 1968, 309, 311, 445, 3720, 484, 51716], "temperature": 0.0, "avg_logprob": -0.0830697256421286, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.013426797464489937}, {"id": 280, "seek": 183616, "start": 1836.16, "end": 1842.5600000000002, "text": " in terms of the plain text with the analytical, the Bayesian network is transformed into a factor", "tokens": [50364, 294, 2115, 295, 264, 11121, 2487, 365, 264, 29579, 11, 264, 7840, 42434, 3209, 307, 16894, 666, 257, 5952, 50684], "temperature": 0.0, "avg_logprob": -0.12103111461057502, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.004754466935992241}, {"id": 281, "seek": 183616, "start": 1842.5600000000002, "end": 1849.44, "text": " graph, probably constrained factor graph, discussion for another day. On that graph,", "tokens": [50684, 4295, 11, 1391, 38901, 5952, 4295, 11, 5017, 337, 1071, 786, 13, 1282, 300, 4295, 11, 51028], "temperature": 0.0, "avg_logprob": -0.12103111461057502, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.004754466935992241}, {"id": 282, "seek": 183616, "start": 1849.44, "end": 1859.0400000000002, "text": " certain messages are passed at inference runtime. conditional and predictive entropies are calculated", "tokens": [51028, 1629, 7897, 366, 4678, 412, 38253, 34474, 13, 27708, 293, 35521, 948, 1513, 530, 366, 15598, 51508], "temperature": 0.0, "avg_logprob": -0.12103111461057502, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.004754466935992241}, {"id": 283, "seek": 185904, "start": 1859.04, "end": 1867.52, "text": " as part of the way that this system outputs or is described by different probability distributions", "tokens": [50364, 382, 644, 295, 264, 636, 300, 341, 1185, 23930, 420, 307, 7619, 538, 819, 8482, 37870, 50788], "temperature": 0.0, "avg_logprob": -0.054820169376421576, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014954701997339725}, {"id": 284, "seek": 185904, "start": 1867.52, "end": 1874.0, "text": " understand in a kind of statistical mechanical way in terms of entropy. And then that is going", "tokens": [50788, 1223, 294, 257, 733, 295, 22820, 12070, 636, 294, 2115, 295, 30867, 13, 400, 550, 300, 307, 516, 51112], "temperature": 0.0, "avg_logprob": -0.054820169376421576, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014954701997339725}, {"id": 285, "seek": 185904, "start": 1874.0, "end": 1880.6399999999999, "text": " to come together into calculation of the objective function, which is the expected information gain,", "tokens": [51112, 281, 808, 1214, 666, 17108, 295, 264, 10024, 2445, 11, 597, 307, 264, 5176, 1589, 6052, 11, 51444], "temperature": 0.0, "avg_logprob": -0.054820169376421576, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014954701997339725}, {"id": 286, "seek": 185904, "start": 1880.6399999999999, "end": 1886.8, "text": " which is basically conditioned upon the cognitive model of the sampler. So just because the sampling", "tokens": [51444, 597, 307, 1936, 35833, 3564, 264, 15605, 2316, 295, 264, 3247, 22732, 13, 407, 445, 570, 264, 21179, 51752], "temperature": 0.0, "avg_logprob": -0.054820169376421576, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014954701997339725}, {"id": 287, "seek": 188680, "start": 1886.8, "end": 1893.36, "text": " is active data sampling, doesn't mean that it's going to lead to like an adaptive behavior.", "tokens": [50364, 307, 4967, 1412, 21179, 11, 1177, 380, 914, 300, 309, 311, 516, 281, 1477, 281, 411, 364, 27912, 5223, 13, 50692], "temperature": 0.0, "avg_logprob": -0.09789674422320198, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003221973543986678}, {"id": 288, "seek": 188680, "start": 1894.32, "end": 1900.24, "text": " It just means that where the learning rate is perceived to be highest informationally,", "tokens": [50740, 467, 445, 1355, 300, 689, 264, 2539, 3314, 307, 19049, 281, 312, 6343, 1589, 379, 11, 51036], "temperature": 0.0, "avg_logprob": -0.09789674422320198, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003221973543986678}, {"id": 289, "seek": 188680, "start": 1900.24, "end": 1908.08, "text": " iteratively, there is a ranking by which those can be, which this the space of experiments", "tokens": [51036, 17138, 19020, 11, 456, 307, 257, 17833, 538, 597, 729, 393, 312, 11, 597, 341, 264, 1901, 295, 12050, 51428], "temperature": 0.0, "avg_logprob": -0.09789674422320198, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003221973543986678}, {"id": 290, "seek": 188680, "start": 1908.08, "end": 1916.24, "text": " can be ranked by and it can connect to pragmatic value in terms of epistemic and pragmatic coming", "tokens": [51428, 393, 312, 20197, 538, 293, 309, 393, 1745, 281, 46904, 2158, 294, 2115, 295, 2388, 468, 3438, 293, 46904, 1348, 51836], "temperature": 0.0, "avg_logprob": -0.09789674422320198, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003221973543986678}, {"id": 291, "seek": 191624, "start": 1916.24, "end": 1921.28, "text": " together for the full expected free energy like in the clinical trial. Anything else?", "tokens": [50364, 1214, 337, 264, 1577, 5176, 1737, 2281, 411, 294, 264, 9115, 7308, 13, 11998, 1646, 30, 50616], "temperature": 0.0, "avg_logprob": -0.12201886953309525, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.002713992027565837}, {"id": 292, "seek": 191624, "start": 1922.24, "end": 1929.1200000000001, "text": " And you'll notice in this story example, they are discussing here, the factors that they have in", "tokens": [50664, 400, 291, 603, 3449, 294, 341, 1657, 1365, 11, 436, 366, 10850, 510, 11, 264, 6771, 300, 436, 362, 294, 51008], "temperature": 0.0, "avg_logprob": -0.12201886953309525, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.002713992027565837}, {"id": 293, "seek": 191624, "start": 1929.1200000000001, "end": 1936.4, "text": " their graph in each of their nodes is actually a cosine. So that's why you get that kind of", "tokens": [51008, 641, 4295, 294, 1184, 295, 641, 13891, 307, 767, 257, 23565, 13, 407, 300, 311, 983, 291, 483, 300, 733, 295, 51372], "temperature": 0.0, "avg_logprob": -0.12201886953309525, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.002713992027565837}, {"id": 294, "seek": 191624, "start": 1936.4, "end": 1945.68, "text": " oscillation in that bottom plot there. So you'll have areas of maximal information and then areas", "tokens": [51372, 18225, 399, 294, 300, 2767, 7542, 456, 13, 407, 291, 603, 362, 3179, 295, 49336, 1589, 293, 550, 3179, 51836], "temperature": 0.0, "avg_logprob": -0.12201886953309525, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.002713992027565837}, {"id": 295, "seek": 194568, "start": 1945.68, "end": 1950.96, "text": " of minimal information just based on the toy example they have. And this doesn't always have", "tokens": [50364, 295, 13206, 1589, 445, 2361, 322, 264, 12058, 1365, 436, 362, 13, 400, 341, 1177, 380, 1009, 362, 50628], "temperature": 0.0, "avg_logprob": -0.08168332131354364, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0008295107982121408}, {"id": 296, "seek": 194568, "start": 1950.96, "end": 1956.24, "text": " to be cosine, but in this example it is. And so it just kind of gives you a really good graphical", "tokens": [50628, 281, 312, 23565, 11, 457, 294, 341, 1365, 309, 307, 13, 400, 370, 309, 445, 733, 295, 2709, 291, 257, 534, 665, 35942, 50892], "temperature": 0.0, "avg_logprob": -0.08168332131354364, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0008295107982121408}, {"id": 297, "seek": 194568, "start": 1956.24, "end": 1966.16, "text": " understanding of how your information gain can be viewed over a sinusoidal sort of oscillation.", "tokens": [50892, 3701, 295, 577, 428, 1589, 6052, 393, 312, 19174, 670, 257, 41503, 17079, 304, 1333, 295, 18225, 399, 13, 51388], "temperature": 0.0, "avg_logprob": -0.08168332131354364, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0008295107982121408}, {"id": 298, "seek": 194568, "start": 1967.44, "end": 1972.48, "text": " Yeah, just to kind of double down on that, if you sample right here on the number line,", "tokens": [51452, 865, 11, 445, 281, 733, 295, 3834, 760, 322, 300, 11, 498, 291, 6889, 558, 510, 322, 264, 1230, 1622, 11, 51704], "temperature": 0.0, "avg_logprob": -0.08168332131354364, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0008295107982121408}, {"id": 299, "seek": 197248, "start": 1972.48, "end": 1979.2, "text": " or right here, the lines are indistinguishable. So the information gain is expected to be low", "tokens": [50364, 420, 558, 510, 11, 264, 3876, 366, 1016, 468, 7050, 742, 712, 13, 407, 264, 1589, 6052, 307, 5176, 281, 312, 2295, 50700], "temperature": 0.0, "avg_logprob": -0.07964163650701075, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0024723445530980825}, {"id": 300, "seek": 197248, "start": 1980.08, "end": 1985.52, "text": " under understanding the parameter family that is being generated and sampled from,", "tokens": [50744, 833, 3701, 264, 13075, 1605, 300, 307, 885, 10833, 293, 3247, 15551, 490, 11, 51016], "temperature": 0.0, "avg_logprob": -0.07964163650701075, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0024723445530980825}, {"id": 301, "seek": 197248, "start": 1985.52, "end": 1992.08, "text": " which in this first example is the same, same type of equations. Whereas where the functions are", "tokens": [51016, 597, 294, 341, 700, 1365, 307, 264, 912, 11, 912, 2010, 295, 11787, 13, 13813, 689, 264, 6828, 366, 51344], "temperature": 0.0, "avg_logprob": -0.07964163650701075, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0024723445530980825}, {"id": 302, "seek": 197248, "start": 1992.08, "end": 1998.88, "text": " maximally distinct, the information gain associated with reducing uncertainty about which one of those", "tokens": [51344, 5138, 379, 10644, 11, 264, 1589, 6052, 6615, 365, 12245, 15697, 466, 597, 472, 295, 729, 51684], "temperature": 0.0, "avg_logprob": -0.07964163650701075, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0024723445530980825}, {"id": 303, "seek": 199888, "start": 1998.96, "end": 2009.68, "text": " five the data point is coming from, those are the informative points at the zero on the number line", "tokens": [50368, 1732, 264, 1412, 935, 307, 1348, 490, 11, 729, 366, 264, 27759, 2793, 412, 264, 4018, 322, 264, 1230, 1622, 50904], "temperature": 0.0, "avg_logprob": -0.10000485758627614, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0019876512233167887}, {"id": 304, "seek": 199888, "start": 2009.68, "end": 2016.88, "text": " and far out. That's where just perceiving one point uncolored would give you the most ability to", "tokens": [50904, 293, 1400, 484, 13, 663, 311, 689, 445, 9016, 2123, 472, 935, 6219, 401, 2769, 576, 976, 291, 264, 881, 3485, 281, 51264], "temperature": 0.0, "avg_logprob": -0.10000485758627614, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0019876512233167887}, {"id": 305, "seek": 199888, "start": 2016.88, "end": 2021.3600000000001, "text": " even perfectly resolve which one of the five situations it was.", "tokens": [51264, 754, 6239, 14151, 597, 472, 295, 264, 1732, 6851, 309, 390, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10000485758627614, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0019876512233167887}, {"id": 306, "seek": 202136, "start": 2021.9199999999998, "end": 2031.28, "text": " So they're right. In effect, this model amplifies or attenuates the amplitude of the predicted", "tokens": [50392, 407, 436, 434, 558, 13, 682, 1802, 11, 341, 2316, 9731, 11221, 420, 951, 268, 27710, 264, 27433, 295, 264, 19147, 50860], "temperature": 0.0, "avg_logprob": -0.14633985666128305, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0016484204679727554}, {"id": 307, "seek": 202136, "start": 2031.28, "end": 2037.84, "text": " data, depending upon a periodic function of our data sampling policy pi. So here the policy", "tokens": [50860, 1412, 11, 5413, 3564, 257, 27790, 2445, 295, 527, 1412, 21179, 3897, 3895, 13, 407, 510, 264, 3897, 51188], "temperature": 0.0, "avg_logprob": -0.14633985666128305, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0016484204679727554}, {"id": 308, "seek": 202136, "start": 2037.84, "end": 2043.52, "text": " distribution is like that kind of around the clock direction, which is not a common setting,", "tokens": [51188, 7316, 307, 411, 300, 733, 295, 926, 264, 7830, 3513, 11, 597, 307, 406, 257, 2689, 3287, 11, 51472], "temperature": 0.0, "avg_logprob": -0.14633985666128305, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0016484204679727554}, {"id": 309, "seek": 204352, "start": 2043.52, "end": 2051.68, "text": " but the general idea of sampling amongst a finite set of alternatives, where a control", "tokens": [50364, 457, 264, 2674, 1558, 295, 21179, 12918, 257, 19362, 992, 295, 20478, 11, 689, 257, 1969, 50772], "temperature": 0.0, "avg_logprob": -0.08988048991218942, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.001064920797944069}, {"id": 310, "seek": 204352, "start": 2051.68, "end": 2058.8, "text": " variable is going to result in the most informative data point, is a theme that is going to be", "tokens": [50772, 7006, 307, 516, 281, 1874, 294, 264, 881, 27759, 1412, 935, 11, 307, 257, 6314, 300, 307, 516, 281, 312, 51128], "temperature": 0.0, "avg_logprob": -0.08988048991218942, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.001064920797944069}, {"id": 311, "seek": 204352, "start": 2058.8, "end": 2066.72, "text": " expanded upon, and also one interesting mathematical move. Once all terms that are constant with", "tokens": [51128, 14342, 3564, 11, 293, 611, 472, 1880, 18894, 1286, 13, 3443, 439, 2115, 300, 366, 5754, 365, 51524], "temperature": 0.0, "avg_logprob": -0.08988048991218942, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.001064920797944069}, {"id": 312, "seek": 206672, "start": 2066.72, "end": 2073.6, "text": " respect to pi are eliminated, we are left with equation six. So equation five comes down to", "tokens": [50364, 3104, 281, 3895, 366, 20308, 11, 321, 366, 1411, 365, 5367, 2309, 13, 407, 5367, 1732, 1487, 760, 281, 50708], "temperature": 0.0, "avg_logprob": -0.06618029812732375, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.020642897114157677}, {"id": 313, "seek": 206672, "start": 2073.6, "end": 2082.9599999999996, "text": " equation six, or maybe not exactly only five to six, but six has removed all the variables", "tokens": [50708, 5367, 2309, 11, 420, 1310, 406, 2293, 787, 1732, 281, 2309, 11, 457, 2309, 575, 7261, 439, 264, 9102, 51176], "temperature": 0.0, "avg_logprob": -0.06618029812732375, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.020642897114157677}, {"id": 314, "seek": 206672, "start": 2083.68, "end": 2089.68, "text": " that don't change as policy changes. So if the question of policy selection is taken alone,", "tokens": [51212, 300, 500, 380, 1319, 382, 3897, 2962, 13, 407, 498, 264, 1168, 295, 3897, 9450, 307, 2726, 3312, 11, 51512], "temperature": 0.0, "avg_logprob": -0.06618029812732375, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.020642897114157677}, {"id": 315, "seek": 206672, "start": 2089.68, "end": 2096.16, "text": " like gradients on policy updating, then everything that's constant with respect to it", "tokens": [51512, 411, 2771, 2448, 322, 3897, 25113, 11, 550, 1203, 300, 311, 5754, 365, 3104, 281, 309, 51836], "temperature": 0.0, "avg_logprob": -0.06618029812732375, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.020642897114157677}, {"id": 316, "seek": 209616, "start": 2096.24, "end": 2102.08, "text": " doesn't come into like the delta pi, delta something. So it just simplifies it down to", "tokens": [50368, 1177, 380, 808, 666, 411, 264, 8289, 3895, 11, 8289, 746, 13, 407, 309, 445, 6883, 11221, 309, 760, 281, 50660], "temperature": 0.0, "avg_logprob": -0.10365814632839626, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00018520183220971376}, {"id": 317, "seek": 209616, "start": 2102.08, "end": 2107.12, "text": " only a function of policy, and that just kind of reflects how like the sense making and belief", "tokens": [50660, 787, 257, 2445, 295, 3897, 11, 293, 300, 445, 733, 295, 18926, 577, 411, 264, 2020, 1455, 293, 7107, 50912], "temperature": 0.0, "avg_logprob": -0.10365814632839626, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00018520183220971376}, {"id": 318, "seek": 209616, "start": 2107.12, "end": 2113.2, "text": " updating component is partitioned off from the policy selection component here.", "tokens": [50912, 25113, 6542, 307, 24808, 292, 766, 490, 264, 3897, 9450, 6542, 510, 13, 51216], "temperature": 0.0, "avg_logprob": -0.10365814632839626, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00018520183220971376}, {"id": 319, "seek": 209616, "start": 2115.52, "end": 2121.8399999999997, "text": " Yeah, you're looking for change in your belief based on the observations that you're gained.", "tokens": [51332, 865, 11, 291, 434, 1237, 337, 1319, 294, 428, 7107, 2361, 322, 264, 18163, 300, 291, 434, 12634, 13, 51648], "temperature": 0.0, "avg_logprob": -0.10365814632839626, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00018520183220971376}, {"id": 320, "seek": 212184, "start": 2121.84, "end": 2126.56, "text": " So if it's not changing, it's not as informative in your information model.", "tokens": [50364, 407, 498, 309, 311, 406, 4473, 11, 309, 311, 406, 382, 27759, 294, 428, 1589, 2316, 13, 50600], "temperature": 0.0, "avg_logprob": -0.05640162785847982, "compression_ratio": 1.638095238095238, "no_speech_prob": 6.20487189735286e-05}, {"id": 321, "seek": 212184, "start": 2128.88, "end": 2135.84, "text": " Yeah, continuing on equation six there, which is modeling the policy dependent", "tokens": [50716, 865, 11, 9289, 322, 5367, 2309, 456, 11, 597, 307, 15983, 264, 3897, 12334, 51064], "temperature": 0.0, "avg_logprob": -0.05640162785847982, "compression_ratio": 1.638095238095238, "no_speech_prob": 6.20487189735286e-05}, {"id": 322, "seek": 212184, "start": 2135.84, "end": 2142.88, "text": " components of information gain as an objective function that ranks decisions about where to", "tokens": [51064, 6677, 295, 1589, 6052, 382, 364, 10024, 2445, 300, 21406, 5327, 466, 689, 281, 51416], "temperature": 0.0, "avg_logprob": -0.05640162785847982, "compression_ratio": 1.638095238095238, "no_speech_prob": 6.20487189735286e-05}, {"id": 323, "seek": 212184, "start": 2142.88, "end": 2150.56, "text": " sample. Equation six is a special case of the third row of table one, which highlights analytical", "tokens": [51416, 6889, 13, 15624, 399, 2309, 307, 257, 2121, 1389, 295, 264, 2636, 5386, 295, 3199, 472, 11, 597, 14254, 29579, 51800], "temperature": 0.0, "avg_logprob": -0.05640162785847982, "compression_ratio": 1.638095238095238, "no_speech_prob": 6.20487189735286e-05}, {"id": 324, "seek": 215056, "start": 2150.56, "end": 2156.4, "text": " expressions for expected information gain for a few common model structures. As we might intuit,", "tokens": [50364, 15277, 337, 5176, 1589, 6052, 337, 257, 1326, 2689, 2316, 9227, 13, 1018, 321, 1062, 16224, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09140003295171828, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00025315044331364334}, {"id": 325, "seek": 215056, "start": 2156.4, "end": 2160.4, "text": " the most informative place is to sample data aligned with those in which differences in data", "tokens": [50656, 264, 881, 27759, 1081, 307, 281, 6889, 1412, 17962, 365, 729, 294, 597, 7300, 294, 1412, 50856], "temperature": 0.0, "avg_logprob": -0.09140003295171828, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00025315044331364334}, {"id": 326, "seek": 215056, "start": 2160.4, "end": 2165.36, "text": " lead to large differences in the predicted data, in which our choice of pi maximizes the sensitivity", "tokens": [50856, 1477, 281, 2416, 7300, 294, 264, 19147, 1412, 11, 294, 597, 527, 3922, 295, 3895, 5138, 5660, 264, 19392, 51104], "temperature": 0.0, "avg_logprob": -0.09140003295171828, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00025315044331364334}, {"id": 327, "seek": 215056, "start": 2165.36, "end": 2176.0, "text": " with which y depends on data. So here are the categorical, the Dirichlet, and other functions", "tokens": [51104, 365, 597, 288, 5946, 322, 1412, 13, 407, 510, 366, 264, 19250, 804, 11, 264, 34422, 480, 2631, 11, 293, 661, 6828, 51636], "temperature": 0.0, "avg_logprob": -0.09140003295171828, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.00025315044331364334}, {"id": 328, "seek": 217600, "start": 2176.0, "end": 2182.88, "text": " in terms of how they'd be written out in the probabilistic, like specifying a distribution", "tokens": [50364, 294, 2115, 295, 577, 436, 1116, 312, 3720, 484, 294, 264, 31959, 3142, 11, 411, 1608, 5489, 257, 7316, 50708], "temperature": 0.0, "avg_logprob": -0.059596431666407094, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.0033762971870601177}, {"id": 329, "seek": 217600, "start": 2182.88, "end": 2191.84, "text": " way, and then how there's this relationship analytically to a related distribution, which is", "tokens": [50708, 636, 11, 293, 550, 577, 456, 311, 341, 2480, 10783, 984, 281, 257, 4077, 7316, 11, 597, 307, 51156], "temperature": 0.0, "avg_logprob": -0.059596431666407094, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.0033762971870601177}, {"id": 330, "seek": 217600, "start": 2192.64, "end": 2200.4, "text": " an objective function that ranks the information content of sampling the likelihood distribution", "tokens": [51196, 364, 10024, 2445, 300, 21406, 264, 1589, 2701, 295, 21179, 264, 22119, 7316, 51584], "temperature": 0.0, "avg_logprob": -0.059596431666407094, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.0033762971870601177}, {"id": 331, "seek": 220040, "start": 2200.4, "end": 2207.28, "text": " in a certain way. And that's closed form in certain situations. And then also they explore", "tokens": [50364, 294, 257, 1629, 636, 13, 400, 300, 311, 5395, 1254, 294, 1629, 6851, 13, 400, 550, 611, 436, 6839, 50708], "temperature": 0.0, "avg_logprob": -0.139209465241768, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.01016800943762064}, {"id": 332, "seek": 220040, "start": 2207.28, "end": 2213.28, "text": " where it's intractable formally. And so then that's where the variational approximation comes into", "tokens": [50708, 689, 309, 311, 560, 1897, 712, 25983, 13, 400, 370, 550, 300, 311, 689, 264, 3034, 1478, 28023, 1487, 666, 51008], "temperature": 0.0, "avg_logprob": -0.139209465241768, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.01016800943762064}, {"id": 333, "seek": 220040, "start": 2213.28, "end": 2223.6, "text": " play. Anything to add? No, I think that summarizes this slide. Okay, section four, function approximation.", "tokens": [51008, 862, 13, 11998, 281, 909, 30, 883, 11, 286, 519, 300, 14611, 5660, 341, 4137, 13, 1033, 11, 3541, 1451, 11, 2445, 28023, 13, 51524], "temperature": 0.0, "avg_logprob": -0.139209465241768, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.01016800943762064}, {"id": 334, "seek": 222360, "start": 2224.56, "end": 2230.4, "text": " We next turn to a generic supervised learning problem, that of trying to approximate some", "tokens": [50412, 492, 958, 1261, 281, 257, 19577, 46533, 2539, 1154, 11, 300, 295, 1382, 281, 30874, 512, 50704], "temperature": 0.0, "avg_logprob": -0.16719146728515624, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.003075168002396822}, {"id": 335, "seek": 222360, "start": 2230.96, "end": 2234.88, "text": " function based upon known inputs and the observable outcomes they stochastically", "tokens": [50732, 2445, 2361, 3564, 2570, 15743, 293, 264, 9951, 712, 10070, 436, 342, 8997, 22808, 50928], "temperature": 0.0, "avg_logprob": -0.16719146728515624, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.003075168002396822}, {"id": 336, "seek": 222360, "start": 2234.88, "end": 2243.7599999999998, "text": " cause. Pretty general neural network or latent state observation setup.", "tokens": [50928, 3082, 13, 10693, 2674, 18161, 3209, 420, 48994, 1785, 14816, 8657, 13, 51372], "temperature": 0.0, "avg_logprob": -0.16719146728515624, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.003075168002396822}, {"id": 337, "seek": 224376, "start": 2244.0, "end": 2257.6800000000003, "text": " That information is composed and concatenated. So that there's a common variable with that's", "tokens": [50376, 663, 1589, 307, 18204, 293, 1588, 7186, 770, 13, 407, 300, 456, 311, 257, 2689, 7006, 365, 300, 311, 51060], "temperature": 0.0, "avg_logprob": -0.1839812159538269, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.0014102539280429482}, {"id": 338, "seek": 224376, "start": 2257.6800000000003, "end": 2262.5600000000004, "text": " describing the statistical object that's going to be describing the inputs and the relationship", "tokens": [51060, 16141, 264, 22820, 2657, 300, 311, 516, 281, 312, 16141, 264, 15743, 293, 264, 2480, 51304], "temperature": 0.0, "avg_logprob": -0.1839812159538269, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.0014102539280429482}, {"id": 339, "seek": 226256, "start": 2262.56, "end": 2273.92, "text": " with the observable outcomes. And then that function approximation from sequential data", "tokens": [50364, 365, 264, 9951, 712, 10070, 13, 400, 550, 300, 2445, 28023, 490, 42881, 1412, 50932], "temperature": 0.0, "avg_logprob": -0.09036945041857268, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.009858867153525352}, {"id": 340, "seek": 226256, "start": 2273.92, "end": 2280.64, "text": " in figure three is simulated with random but potentially you could call all of them random.", "tokens": [50932, 294, 2573, 1045, 307, 41713, 365, 4974, 457, 7263, 291, 727, 818, 439, 295, 552, 4974, 13, 51268], "temperature": 0.0, "avg_logprob": -0.09036945041857268, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.009858867153525352}, {"id": 341, "seek": 226256, "start": 2280.64, "end": 2290.4, "text": " But this one is a flatter or a less informed and iterated model of data sampling,", "tokens": [51268, 583, 341, 472, 307, 257, 41247, 420, 257, 1570, 11740, 293, 17138, 770, 2316, 295, 1412, 21179, 11, 51756], "temperature": 0.0, "avg_logprob": -0.09036945041857268, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.009858867153525352}, {"id": 342, "seek": 229040, "start": 2291.12, "end": 2296.8, "text": " just going to show that samples of random data with even this minimal", "tokens": [50400, 445, 516, 281, 855, 300, 10938, 295, 4974, 1412, 365, 754, 341, 13206, 50684], "temperature": 0.0, "avg_logprob": -0.11712761447854238, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0015010325005277991}, {"id": 343, "seek": 229040, "start": 2297.76, "end": 2304.7200000000003, "text": " non information gain driven model has a certain baseline prediction that's associated with", "tokens": [50732, 2107, 1589, 6052, 9555, 2316, 575, 257, 1629, 20518, 17630, 300, 311, 6615, 365, 51080], "temperature": 0.0, "avg_logprob": -0.11712761447854238, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0015010325005277991}, {"id": 344, "seek": 229040, "start": 2304.7200000000003, "end": 2312.32, "text": " certain choices about sampling sequentially from this generative model. Want to add anything?", "tokens": [51080, 1629, 7994, 466, 21179, 5123, 3137, 490, 341, 1337, 1166, 2316, 13, 11773, 281, 909, 1340, 30, 51460], "temperature": 0.0, "avg_logprob": -0.11712761447854238, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0015010325005277991}, {"id": 345, "seek": 229040, "start": 2313.36, "end": 2319.6800000000003, "text": " Yeah, it's just I like that they highlighted in this that choice diagram there that you can", "tokens": [51512, 865, 11, 309, 311, 445, 286, 411, 300, 436, 17173, 294, 341, 300, 3922, 10686, 456, 300, 291, 393, 51828], "temperature": 0.0, "avg_logprob": -0.11712761447854238, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0015010325005277991}, {"id": 346, "seek": 231968, "start": 2319.68, "end": 2325.44, "text": " actually get the inefficient sampling just by random that you start to you can randomly select", "tokens": [50364, 767, 483, 264, 43495, 21179, 445, 538, 4974, 300, 291, 722, 281, 291, 393, 16979, 3048, 50652], "temperature": 0.0, "avg_logprob": -0.12043483837230785, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0010647877352312207}, {"id": 347, "seek": 231968, "start": 2325.44, "end": 2332.7999999999997, "text": " two things very close together and you've effectively maybe not wasted a choice but", "tokens": [50652, 732, 721, 588, 1998, 1214, 293, 291, 600, 8659, 1310, 406, 19496, 257, 3922, 457, 51020], "temperature": 0.0, "avg_logprob": -0.12043483837230785, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0010647877352312207}, {"id": 348, "seek": 231968, "start": 2332.7999999999997, "end": 2337.6, "text": " you know not gotten the maximum gain from that choice that you could have.", "tokens": [51020, 291, 458, 406, 5768, 264, 6674, 6052, 490, 300, 3922, 300, 291, 727, 362, 13, 51260], "temperature": 0.0, "avg_logprob": -0.12043483837230785, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0010647877352312207}, {"id": 349, "seek": 231968, "start": 2339.44, "end": 2348.3999999999996, "text": " Yeah, they write a little bit more about figure three. Figure three illustrates a depiction of", "tokens": [51352, 865, 11, 436, 2464, 257, 707, 857, 544, 466, 2573, 1045, 13, 43225, 1045, 41718, 257, 47740, 295, 51800], "temperature": 0.0, "avg_logprob": -0.12043483837230785, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0010647877352312207}, {"id": 350, "seek": 234840, "start": 2348.4, "end": 2353.44, "text": " this model as a Bayesian network and a visual representation of the data generating process.", "tokens": [50364, 341, 2316, 382, 257, 7840, 42434, 3209, 293, 257, 5056, 10290, 295, 264, 1412, 17746, 1399, 13, 50616], "temperature": 0.0, "avg_logprob": -0.08327423334121704, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.00011591563816182315}, {"id": 351, "seek": 234840, "start": 2356.96, "end": 2362.96, "text": " Now they're going to bring in information gain. So they write this is where information gain", "tokens": [50792, 823, 436, 434, 516, 281, 1565, 294, 1589, 6052, 13, 407, 436, 2464, 341, 307, 689, 1589, 6052, 51092], "temperature": 0.0, "avg_logprob": -0.08327423334121704, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.00011591563816182315}, {"id": 352, "seek": 234840, "start": 2362.96, "end": 2369.76, "text": " becomes relevant into designing a more informed way to sample data than from a flat or a non", "tokens": [51092, 3643, 7340, 666, 14685, 257, 544, 11740, 636, 281, 6889, 1412, 813, 490, 257, 4962, 420, 257, 2107, 51432], "temperature": 0.0, "avg_logprob": -0.08327423334121704, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.00011591563816182315}, {"id": 353, "seek": 234840, "start": 2369.76, "end": 2374.8, "text": " updating prior data sampling distribution. It's like equivalent to having a policy prior that's", "tokens": [51432, 25113, 4059, 1412, 21179, 7316, 13, 467, 311, 411, 10344, 281, 1419, 257, 3897, 4059, 300, 311, 51684], "temperature": 0.0, "avg_logprob": -0.08327423334121704, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.00011591563816182315}, {"id": 354, "seek": 237480, "start": 2374.8, "end": 2382.0800000000004, "text": " fixed which might be a heuristic in certain space. They write substituting equation seven into", "tokens": [50364, 6806, 597, 1062, 312, 257, 415, 374, 3142, 294, 1629, 1901, 13, 814, 2464, 26441, 10861, 5367, 3407, 666, 50728], "temperature": 0.0, "avg_logprob": -0.08963828086853028, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0016483920626342297}, {"id": 355, "seek": 237480, "start": 2382.0800000000004, "end": 2393.76, "text": " equation three. So here's that Markov blanket parent child concept and here equation three is", "tokens": [50728, 5367, 1045, 13, 407, 510, 311, 300, 3934, 5179, 17907, 2596, 1440, 3410, 293, 510, 5367, 1045, 307, 51312], "temperature": 0.0, "avg_logprob": -0.08963828086853028, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0016483920626342297}, {"id": 356, "seek": 237480, "start": 2393.76, "end": 2400.6400000000003, "text": " describing the policy dependence on the joint distribution of the observed and unobserved", "tokens": [51312, 16141, 264, 3897, 31704, 322, 264, 7225, 7316, 295, 264, 13095, 293, 8526, 929, 6913, 51656], "temperature": 0.0, "avg_logprob": -0.08963828086853028, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0016483920626342297}, {"id": 357, "seek": 240064, "start": 2400.64, "end": 2410.64, "text": " and this is combined into equation 10. To show what equation 10 does in terms of now that we're", "tokens": [50364, 293, 341, 307, 9354, 666, 5367, 1266, 13, 1407, 855, 437, 5367, 1266, 775, 294, 2115, 295, 586, 300, 321, 434, 50864], "temperature": 0.0, "avg_logprob": -0.09980884351228413, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.001206530723720789}, {"id": 358, "seek": 240064, "start": 2410.64, "end": 2419.12, "text": " sampling from this distribution or like statistical distributions that this describes they'll", "tokens": [50864, 21179, 490, 341, 7316, 420, 411, 22820, 37870, 300, 341, 15626, 436, 603, 51288], "temperature": 0.0, "avg_logprob": -0.09980884351228413, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.001206530723720789}, {"id": 359, "seek": 240064, "start": 2419.12, "end": 2423.7599999999998, "text": " differentiate figure three from figure four kind of like bring in this model change between those", "tokens": [51288, 23203, 2573, 1045, 490, 2573, 1451, 733, 295, 411, 1565, 294, 341, 2316, 1319, 1296, 729, 51520], "temperature": 0.0, "avg_logprob": -0.09980884351228413, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.001206530723720789}, {"id": 360, "seek": 240064, "start": 2423.7599999999998, "end": 2428.64, "text": " two figures. Now samples are drawn from a distribution whose log is proportional to the", "tokens": [51520, 732, 9624, 13, 823, 10938, 366, 10117, 490, 257, 7316, 6104, 3565, 307, 24969, 281, 264, 51764], "temperature": 0.0, "avg_logprob": -0.09980884351228413, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.001206530723720789}, {"id": 361, "seek": 242864, "start": 2428.64, "end": 2436.64, "text": " information gain in equation 10. So it takes the flat policy prior and in a fixed way has remapped it", "tokens": [50364, 1589, 6052, 294, 5367, 1266, 13, 407, 309, 2516, 264, 4962, 3897, 4059, 293, 294, 257, 6806, 636, 575, 890, 20780, 309, 50764], "temperature": 0.0, "avg_logprob": -0.06191445078168597, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.00026947373407892883}, {"id": 362, "seek": 242864, "start": 2436.64, "end": 2445.7599999999998, "text": " to be proportional to the information gain. So here's three on the right and then four on the right", "tokens": [50764, 281, 312, 24969, 281, 264, 1589, 6052, 13, 407, 510, 311, 1045, 322, 264, 558, 293, 550, 1451, 322, 264, 558, 51220], "temperature": 0.0, "avg_logprob": -0.06191445078168597, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.00026947373407892883}, {"id": 363, "seek": 242864, "start": 2446.8799999999997, "end": 2453.3599999999997, "text": " and the figure uses the same format as figure three but now each choice is sampled to maximize", "tokens": [51276, 293, 264, 2573, 4960, 264, 912, 7877, 382, 2573, 1045, 457, 586, 1184, 3922, 307, 3247, 15551, 281, 19874, 51600], "temperature": 0.0, "avg_logprob": -0.06191445078168597, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.00026947373407892883}, {"id": 364, "seek": 245336, "start": 2453.36, "end": 2460.8, "text": " anticipated information gain and they point to some specific quantitative patterns but also", "tokens": [50364, 23267, 1589, 6052, 293, 436, 935, 281, 512, 2685, 27778, 8294, 457, 611, 50736], "temperature": 0.0, "avg_logprob": -0.12055028842974312, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00460923183709383}, {"id": 365, "seek": 245336, "start": 2460.8, "end": 2467.1200000000003, "text": " like qualitative patterns. So want to add anything on figure four? Just that now if you focus on those", "tokens": [50736, 411, 31312, 8294, 13, 407, 528, 281, 909, 1340, 322, 2573, 1451, 30, 1449, 300, 586, 498, 291, 1879, 322, 729, 51052], "temperature": 0.0, "avg_logprob": -0.12055028842974312, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00460923183709383}, {"id": 366, "seek": 245336, "start": 2467.1200000000003, "end": 2473.1200000000003, "text": " choices because that I really did I think like that choice plot there you can see that the walks", "tokens": [51052, 7994, 570, 300, 286, 534, 630, 286, 519, 411, 300, 3922, 7542, 456, 291, 393, 536, 300, 264, 12896, 51352], "temperature": 0.0, "avg_logprob": -0.12055028842974312, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00460923183709383}, {"id": 367, "seek": 245336, "start": 2473.1200000000003, "end": 2479.92, "text": " around kind of choices around your data space are a little bit more distributed evenly distributed", "tokens": [51352, 926, 733, 295, 7994, 926, 428, 1412, 1901, 366, 257, 707, 857, 544, 12631, 17658, 12631, 51692], "temperature": 0.0, "avg_logprob": -0.12055028842974312, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00460923183709383}, {"id": 368, "seek": 247992, "start": 2479.92, "end": 2485.6800000000003, "text": " a little bit less random but you start to get I think a more cohesive sampling of the data", "tokens": [50364, 257, 707, 857, 1570, 4974, 457, 291, 722, 281, 483, 286, 519, 257, 544, 43025, 21179, 295, 264, 1412, 50652], "temperature": 0.0, "avg_logprob": -0.08245916826179228, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.0006878200219944119}, {"id": 369, "seek": 247992, "start": 2485.6800000000003, "end": 2491.44, "text": " without entire randomness putting things too close together putting selections too close together.", "tokens": [50652, 1553, 2302, 4974, 1287, 3372, 721, 886, 1998, 1214, 3372, 47829, 886, 1998, 1214, 13, 50940], "temperature": 0.0, "avg_logprob": -0.08245916826179228, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.0006878200219944119}, {"id": 370, "seek": 247992, "start": 2494.48, "end": 2502.16, "text": " Yeah okay continuing on well they set up the question as this raises the question as to how", "tokens": [51092, 865, 1392, 9289, 322, 731, 436, 992, 493, 264, 1168, 382, 341, 19658, 264, 1168, 382, 281, 577, 51476], "temperature": 0.0, "avg_logprob": -0.08245916826179228, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.0006878200219944119}, {"id": 371, "seek": 247992, "start": 2502.16, "end": 2508.96, "text": " many samples should we collect. So within a foraging bout where should one look and then at the kind", "tokens": [51476, 867, 10938, 820, 321, 2500, 13, 407, 1951, 257, 337, 3568, 15738, 689, 820, 472, 574, 293, 550, 412, 264, 733, 51816], "temperature": 0.0, "avg_logprob": -0.08245916826179228, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.0006878200219944119}, {"id": 372, "seek": 250896, "start": 2508.96, "end": 2514.48, "text": " of like pulling a layer back in the strategy when should you halt look it like if you have already", "tokens": [50364, 295, 411, 8407, 257, 4583, 646, 294, 264, 5206, 562, 820, 291, 12479, 574, 309, 411, 498, 291, 362, 1217, 50640], "temperature": 0.0, "avg_logprob": -0.059788387875224266, "compression_ratio": 1.76, "no_speech_prob": 0.000882993103004992}, {"id": 373, "seek": 250896, "start": 2514.48, "end": 2520.7200000000003, "text": " sampled all three records from a data set then unless you had some other reason you could fully", "tokens": [50640, 3247, 15551, 439, 1045, 7724, 490, 257, 1412, 992, 550, 5969, 291, 632, 512, 661, 1778, 291, 727, 4498, 50952], "temperature": 0.0, "avg_logprob": -0.059788387875224266, "compression_ratio": 1.76, "no_speech_prob": 0.000882993103004992}, {"id": 374, "seek": 250896, "start": 2520.7200000000003, "end": 2527.36, "text": " stop sampling there but you also might want to have a softer stopping criterion that would relate to", "tokens": [50952, 1590, 21179, 456, 457, 291, 611, 1062, 528, 281, 362, 257, 23119, 12767, 46691, 300, 576, 10961, 281, 51284], "temperature": 0.0, "avg_logprob": -0.059788387875224266, "compression_ratio": 1.76, "no_speech_prob": 0.000882993103004992}, {"id": 375, "seek": 250896, "start": 2527.36, "end": 2532.8, "text": " how much information you're gaining from continuing to sample in that way before like halting and so", "tokens": [51284, 577, 709, 1589, 291, 434, 19752, 490, 9289, 281, 6889, 294, 300, 636, 949, 411, 7523, 783, 293, 370, 51556], "temperature": 0.0, "avg_logprob": -0.059788387875224266, "compression_ratio": 1.76, "no_speech_prob": 0.000882993103004992}, {"id": 376, "seek": 253280, "start": 2532.8, "end": 2541.28, "text": " they include that by having like an exit policy in the state space of foraging possibilities.", "tokens": [50364, 436, 4090, 300, 538, 1419, 411, 364, 11043, 3897, 294, 264, 1785, 1901, 295, 337, 3568, 12178, 13, 50788], "temperature": 0.0, "avg_logprob": -0.09162955448545258, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0001233922375831753}, {"id": 377, "seek": 253280, "start": 2542.4, "end": 2547.52, "text": " So how do you resolve that and answer this question can be drawn from behavioral neuroscience in the", "tokens": [50844, 407, 577, 360, 291, 14151, 300, 293, 1867, 341, 1168, 393, 312, 10117, 490, 19124, 42762, 294, 264, 51100], "temperature": 0.0, "avg_logprob": -0.09162955448545258, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0001233922375831753}, {"id": 378, "seek": 253280, "start": 2547.52, "end": 2556.5600000000004, "text": " so-called exploitation exploration dilemma and they introduced the notion of sampling costs", "tokens": [51100, 370, 12, 11880, 33122, 16197, 34312, 293, 436, 7268, 264, 10710, 295, 21179, 5497, 51552], "temperature": 0.0, "avg_logprob": -0.09162955448545258, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0001233922375831753}, {"id": 379, "seek": 255656, "start": 2557.12, "end": 2564.96, "text": " to help decide that. So this method is still going to require parameterization and situation", "tokens": [50392, 281, 854, 4536, 300, 13, 407, 341, 3170, 307, 920, 516, 281, 3651, 13075, 2144, 293, 2590, 50784], "temperature": 0.0, "avg_logprob": -0.06282151407665676, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.010168802924454212}, {"id": 380, "seek": 255656, "start": 2564.96, "end": 2570.24, "text": " specific modeling of the relative costs versus the relative information gain however at least", "tokens": [50784, 2685, 15983, 295, 264, 4972, 5497, 5717, 264, 4972, 1589, 6052, 4461, 412, 1935, 51048], "temperature": 0.0, "avg_logprob": -0.06282151407665676, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.010168802924454212}, {"id": 381, "seek": 255656, "start": 2570.24, "end": 2577.44, "text": " there's an accounting that includes costs into the sampling equation to give any possibility of", "tokens": [51048, 456, 311, 364, 19163, 300, 5974, 5497, 666, 264, 21179, 5367, 281, 976, 604, 7959, 295, 51408], "temperature": 0.0, "avg_logprob": -0.06282151407665676, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.010168802924454212}, {"id": 382, "seek": 255656, "start": 2577.44, "end": 2582.88, "text": " exiting because if no costs are provided for sampling then the model might just converge", "tokens": [51408, 48868, 570, 498, 572, 5497, 366, 5649, 337, 21179, 550, 264, 2316, 1062, 445, 41881, 51680], "temperature": 0.0, "avg_logprob": -0.06282151407665676, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.010168802924454212}, {"id": 383, "seek": 258288, "start": 2582.88, "end": 2588.2400000000002, "text": " and continue to eke out very small amounts of variance explained if it doesn't explicitly", "tokens": [50364, 293, 2354, 281, 308, 330, 484, 588, 1359, 11663, 295, 21977, 8825, 498, 309, 1177, 380, 20803, 50632], "temperature": 0.0, "avg_logprob": -0.07129987692221618, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.01115554291754961}, {"id": 384, "seek": 258288, "start": 2588.2400000000002, "end": 2596.88, "text": " have that stop option so they take the policy vector the list of locations that can be sampled", "tokens": [50632, 362, 300, 1590, 3614, 370, 436, 747, 264, 3897, 8062, 264, 1329, 295, 9253, 300, 393, 312, 3247, 15551, 51064], "temperature": 0.0, "avg_logprob": -0.07129987692221618, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.01115554291754961}, {"id": 385, "seek": 258288, "start": 2596.88, "end": 2602.48, "text": " from and adds a zero element which reflects the information gained if we were to stop sampling", "tokens": [51064, 490, 293, 10860, 257, 4018, 4478, 597, 18926, 264, 1589, 12634, 498, 321, 645, 281, 1590, 21179, 51344], "temperature": 0.0, "avg_logprob": -0.07129987692221618, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.01115554291754961}, {"id": 386, "seek": 258288, "start": 2603.44, "end": 2610.1600000000003, "text": " and then there's a preference over those observations expressed in the c vector preferences", "tokens": [51392, 293, 550, 456, 311, 257, 17502, 670, 729, 18163, 12675, 294, 264, 269, 8062, 21910, 51728], "temperature": 0.0, "avg_logprob": -0.07129987692221618, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.01115554291754961}, {"id": 387, "seek": 261016, "start": 2611.12, "end": 2616.3199999999997, "text": " and this brings in the information seeking and the cost averse of imperatives into the same", "tokens": [50412, 293, 341, 5607, 294, 264, 1589, 11670, 293, 264, 2063, 257, 4308, 295, 10100, 4884, 666, 264, 912, 50672], "temperature": 0.0, "avg_logprob": -0.10535581721815952, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0010648949537426233}, {"id": 388, "seek": 261016, "start": 2616.3199999999997, "end": 2622.96, "text": " objective function in 11. Anything to add on 11? Yeah eventually the idea is that it just gets to", "tokens": [50672, 10024, 2445, 294, 2975, 13, 11998, 281, 909, 322, 2975, 30, 865, 4728, 264, 1558, 307, 300, 309, 445, 2170, 281, 51004], "temperature": 0.0, "avg_logprob": -0.10535581721815952, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0010648949537426233}, {"id": 389, "seek": 261016, "start": 2622.96, "end": 2629.7599999999998, "text": " a point where you're no longer whatever you set you're kind of stopping like energy to be", "tokens": [51004, 257, 935, 689, 291, 434, 572, 2854, 2035, 291, 992, 291, 434, 733, 295, 12767, 411, 2281, 281, 312, 51344], "temperature": 0.0, "avg_logprob": -0.10535581721815952, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0010648949537426233}, {"id": 390, "seek": 261016, "start": 2631.12, "end": 2635.68, "text": " kind of breaking energy eventually it's just gonna get to a point where the model just says", "tokens": [51412, 733, 295, 7697, 2281, 4728, 309, 311, 445, 799, 483, 281, 257, 935, 689, 264, 2316, 445, 1619, 51640], "temperature": 0.0, "avg_logprob": -0.10535581721815952, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0010648949537426233}, {"id": 391, "seek": 263568, "start": 2635.68, "end": 2642.24, "text": " hey I've reached what I can you've set this you're not gaining any anything beyond this point", "tokens": [50364, 4177, 286, 600, 6488, 437, 286, 393, 291, 600, 992, 341, 291, 434, 406, 19752, 604, 1340, 4399, 341, 935, 50692], "temperature": 0.0, "avg_logprob": -0.07935359591529483, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0005702818161807954}, {"id": 392, "seek": 263568, "start": 2642.24, "end": 2647.52, "text": " we can just stop at this point which is nice since in the random selection case there's not", "tokens": [50692, 321, 393, 445, 1590, 412, 341, 935, 597, 307, 1481, 1670, 294, 264, 4974, 9450, 1389, 456, 311, 406, 50956], "temperature": 0.0, "avg_logprob": -0.07935359591529483, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0005702818161807954}, {"id": 393, "seek": 263568, "start": 2647.52, "end": 2653.68, "text": " necessarily a stopping parameter as Daniel mentioned you could continue to get eke out", "tokens": [50956, 4725, 257, 12767, 13075, 382, 8033, 2835, 291, 727, 2354, 281, 483, 308, 330, 484, 51264], "temperature": 0.0, "avg_logprob": -0.07935359591529483, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0005702818161807954}, {"id": 394, "seek": 263568, "start": 2653.68, "end": 2659.3599999999997, "text": " very very small marginal changes but you're not going to gain anything else and so you're just", "tokens": [51264, 588, 588, 1359, 16885, 2962, 457, 291, 434, 406, 516, 281, 6052, 1340, 1646, 293, 370, 291, 434, 445, 51548], "temperature": 0.0, "avg_logprob": -0.07935359591529483, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0005702818161807954}, {"id": 395, "seek": 263568, "start": 2659.3599999999997, "end": 2664.96, "text": " spinning your wheels for no reason so this is a very elegant way of saying hey I've reached kind", "tokens": [51548, 15640, 428, 10046, 337, 572, 1778, 370, 341, 307, 257, 588, 21117, 636, 295, 1566, 4177, 286, 600, 6488, 733, 51828], "temperature": 0.0, "avg_logprob": -0.07935359591529483, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0005702818161807954}, {"id": 396, "seek": 266496, "start": 2664.96, "end": 2674.7200000000003, "text": " of an inflection point of data gain I'm done. So figure five they're continuing in this genre of", "tokens": [50364, 295, 364, 1536, 5450, 935, 295, 1412, 6052, 286, 478, 1096, 13, 407, 2573, 1732, 436, 434, 9289, 294, 341, 11022, 295, 50852], "temperature": 0.0, "avg_logprob": -0.10215003259720341, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.0002378072967985645}, {"id": 397, "seek": 266496, "start": 2674.7200000000003, "end": 2682.16, "text": " three four five and now they've added in to the policy decision which which has an upstream", "tokens": [50852, 1045, 1451, 1732, 293, 586, 436, 600, 3869, 294, 281, 264, 3897, 3537, 597, 597, 575, 364, 33915, 51224], "temperature": 0.0, "avg_logprob": -0.10215003259720341, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.0002378072967985645}, {"id": 398, "seek": 266496, "start": 2682.16, "end": 2688.48, "text": " dependency on the data that's the active policy edge they add in this", "tokens": [51224, 33621, 322, 264, 1412, 300, 311, 264, 4967, 3897, 4691, 436, 909, 294, 341, 51540], "temperature": 0.0, "avg_logprob": -0.10215003259720341, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.0002378072967985645}, {"id": 399, "seek": 268848, "start": 2688.88, "end": 2698.4, "text": " cost to sampling so we can explore more however it now includes not just the information gain", "tokens": [50384, 2063, 281, 21179, 370, 321, 393, 6839, 544, 4461, 309, 586, 5974, 406, 445, 264, 1589, 6052, 50860], "temperature": 0.0, "avg_logprob": -0.10521928887618215, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0037652102764695883}, {"id": 400, "seek": 268848, "start": 2698.4, "end": 2706.88, "text": " driven choices within a trial but it includes a specific probabilistic but decisive stopping point", "tokens": [50860, 9555, 7994, 1951, 257, 7308, 457, 309, 5974, 257, 2685, 31959, 3142, 457, 34998, 12767, 935, 51284], "temperature": 0.0, "avg_logprob": -0.10521928887618215, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0037652102764695883}, {"id": 401, "seek": 268848, "start": 2707.44, "end": 2713.44, "text": " for that trial as parameterized by how sensitive it is to information gain and preference", "tokens": [51312, 337, 300, 7308, 382, 13075, 1602, 538, 577, 9477, 309, 307, 281, 1589, 6052, 293, 17502, 51612], "temperature": 0.0, "avg_logprob": -0.10521928887618215, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0037652102764695883}, {"id": 402, "seek": 271344, "start": 2714.0, "end": 2720.08, "text": " so this is one of the most interesting parts and and discussions in the paper", "tokens": [50392, 370, 341, 307, 472, 295, 264, 881, 1880, 3166, 293, 293, 11088, 294, 264, 3035, 50696], "temperature": 0.0, "avg_logprob": -0.11941140492757162, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0034831897355616093}, {"id": 403, "seek": 271344, "start": 2721.04, "end": 2726.8, "text": " they they ask it out loud a reasonable question to ask at this stage is why bother", "tokens": [50744, 436, 436, 1029, 309, 484, 6588, 257, 10585, 1168, 281, 1029, 412, 341, 3233, 307, 983, 8677, 51032], "temperature": 0.0, "avg_logprob": -0.11941140492757162, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0034831897355616093}, {"id": 404, "seek": 271344, "start": 2727.36, "end": 2733.04, "text": " with the full information seeking objective and basically how does this differ from maximum", "tokens": [51060, 365, 264, 1577, 1589, 11670, 10024, 293, 1936, 577, 775, 341, 743, 490, 6674, 51344], "temperature": 0.0, "avg_logprob": -0.11941140492757162, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0034831897355616093}, {"id": 405, "seek": 271344, "start": 2733.04, "end": 2743.12, "text": " entropy sampling and um let's look forward to the authors or other guests but here's just a", "tokens": [51344, 30867, 21179, 293, 1105, 718, 311, 574, 2128, 281, 264, 16552, 420, 661, 9804, 457, 510, 311, 445, 257, 51848], "temperature": 0.0, "avg_logprob": -0.11941140492757162, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0034831897355616093}, {"id": 406, "seek": 274312, "start": 2743.12, "end": 2748.4, "text": " few notes on this because I think it'll be a great place to explore what it really means to do", "tokens": [50364, 1326, 5570, 322, 341, 570, 286, 519, 309, 603, 312, 257, 869, 1081, 281, 6839, 437, 309, 534, 1355, 281, 360, 50628], "temperature": 0.0, "avg_logprob": -0.1025915749465363, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0007793229888193309}, {"id": 407, "seek": 274312, "start": 2748.4, "end": 2756.4, "text": " statistical and physical modeling on cognitive systems they directly contrast maximum entropy", "tokens": [50628, 22820, 293, 4001, 15983, 322, 15605, 3652, 436, 3838, 8712, 6674, 30867, 51028], "temperature": 0.0, "avg_logprob": -0.1025915749465363, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0007793229888193309}, {"id": 408, "seek": 274312, "start": 2756.4, "end": 2764.96, "text": " sampling and their whole information gain family against each other and then the rebuttal is in", "tokens": [51028, 21179, 293, 641, 1379, 1589, 6052, 1605, 1970, 1184, 661, 293, 550, 264, 319, 5955, 32831, 307, 294, 51456], "temperature": 0.0, "avg_logprob": -0.1025915749465363, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0007793229888193309}, {"id": 409, "seek": 274312, "start": 2765.52, "end": 2772.08, "text": " figure six so just to show figure six for a second the measurement noise increases in variance from", "tokens": [51484, 2573, 2309, 370, 445, 281, 855, 2573, 2309, 337, 257, 1150, 264, 13160, 5658, 8637, 294, 21977, 490, 51812], "temperature": 0.0, "avg_logprob": -0.1025915749465363, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0007793229888193309}, {"id": 410, "seek": 277208, "start": 2772.08, "end": 2779.44, "text": " the center of the function domain so the the variability profile of the function is non-uniform", "tokens": [50364, 264, 3056, 295, 264, 2445, 9274, 370, 264, 264, 35709, 7964, 295, 264, 2445, 307, 2107, 12, 409, 8629, 50732], "temperature": 0.0, "avg_logprob": -0.08578741550445557, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0014549002517014742}, {"id": 411, "seek": 277208, "start": 2780.08, "end": 2785.04, "text": " this means this means the amount of unresolvable uncertainty is heterogeneous through the domain", "tokens": [50764, 341, 1355, 341, 1355, 264, 2372, 295, 517, 495, 401, 17915, 15697, 307, 20789, 31112, 807, 264, 9274, 51012], "temperature": 0.0, "avg_logprob": -0.08578741550445557, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0014549002517014742}, {"id": 412, "seek": 277208, "start": 2785.04, "end": 2793.2799999999997, "text": " of potential sampler so I in some kind of ways of thinking about what they're really getting at", "tokens": [51012, 295, 3995, 3247, 22732, 370, 286, 294, 512, 733, 295, 2098, 295, 1953, 466, 437, 436, 434, 534, 1242, 412, 51424], "temperature": 0.0, "avg_logprob": -0.08578741550445557, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0014549002517014742}, {"id": 413, "seek": 277208, "start": 2793.2799999999997, "end": 2798.4, "text": " and just putting this out as a speculation or starting point for for this key technical point", "tokens": [51424, 293, 445, 3372, 341, 484, 382, 257, 27696, 420, 2891, 935, 337, 337, 341, 2141, 6191, 935, 51680], "temperature": 0.0, "avg_logprob": -0.08578741550445557, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0014549002517014742}, {"id": 414, "seek": 279840, "start": 2799.12, "end": 2806.56, "text": " so if there were a case where the latent states were equivariants they had iid variability", "tokens": [50400, 370, 498, 456, 645, 257, 1389, 689, 264, 48994, 4368, 645, 48726, 3504, 1719, 436, 632, 741, 327, 35709, 50772], "temperature": 0.0, "avg_logprob": -0.0858825735143713, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.005468668416142464}, {"id": 415, "seek": 279840, "start": 2806.56, "end": 2813.2000000000003, "text": " profiles then sampling the most variable sensory data is the most informative like if", "tokens": [50772, 23693, 550, 21179, 264, 881, 7006, 27233, 1412, 307, 264, 881, 27759, 411, 498, 51104], "temperature": 0.0, "avg_logprob": -0.0858825735143713, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.005468668416142464}, {"id": 416, "seek": 279840, "start": 2813.2000000000003, "end": 2820.7200000000003, "text": " you're taking a picture of a solid black image then sampling from the noisy pixels is going to", "tokens": [51104, 291, 434, 1940, 257, 3036, 295, 257, 5100, 2211, 3256, 550, 21179, 490, 264, 24518, 18668, 307, 516, 281, 51480], "temperature": 0.0, "avg_logprob": -0.0858825735143713, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.005468668416142464}, {"id": 417, "seek": 279840, "start": 2820.7200000000003, "end": 2825.6800000000003, "text": " potentially provide more information gain you're reducing uncertainty more about something", "tokens": [51480, 7263, 2893, 544, 1589, 6052, 291, 434, 12245, 15697, 544, 466, 746, 51728], "temperature": 0.0, "avg_logprob": -0.0858825735143713, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.005468668416142464}, {"id": 418, "seek": 282568, "start": 2825.68, "end": 2832.3999999999996, "text": " it might be overfitting but you can select as a heuristic wanting to sample from where variability", "tokens": [50364, 309, 1062, 312, 670, 69, 2414, 457, 291, 393, 3048, 382, 257, 415, 374, 3142, 7935, 281, 6889, 490, 689, 35709, 50700], "temperature": 0.0, "avg_logprob": -0.07234188818162487, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.002287024399265647}, {"id": 419, "seek": 282568, "start": 2832.3999999999996, "end": 2840.16, "text": " is high at just kind of a first pass layer however as we start to think about richer or", "tokens": [50700, 307, 1090, 412, 445, 733, 295, 257, 700, 1320, 4583, 4461, 382, 321, 722, 281, 519, 466, 29021, 420, 51088], "temperature": 0.0, "avg_logprob": -0.07234188818162487, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.002287024399265647}, {"id": 420, "seek": 282568, "start": 2840.16, "end": 2849.68, "text": " more specified statistical patterns generative models there become dependencies that are sparse", "tokens": [51088, 544, 22206, 22820, 8294, 1337, 1166, 5245, 456, 1813, 36606, 300, 366, 637, 11668, 51564], "temperature": 0.0, "avg_logprob": -0.07234188818162487, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.002287024399265647}, {"id": 421, "seek": 284968, "start": 2849.68, "end": 2856.96, "text": " but important amongst all different kinds of things so things that are variable from a sensory", "tokens": [50364, 457, 1021, 12918, 439, 819, 3685, 295, 721, 370, 721, 300, 366, 7006, 490, 257, 27233, 50728], "temperature": 0.0, "avg_logprob": -0.02763418284329501, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0033242153003811836}, {"id": 422, "seek": 284968, "start": 2856.96, "end": 2865.9199999999996, "text": " perspective provide high information gain potentially to one part of a generative model", "tokens": [50728, 4585, 2893, 1090, 1589, 6052, 7263, 281, 472, 644, 295, 257, 1337, 1166, 2316, 51176], "temperature": 0.0, "avg_logprob": -0.02763418284329501, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0033242153003811836}, {"id": 423, "seek": 284968, "start": 2866.7999999999997, "end": 2877.52, "text": " like a screen and static but then other events might be less variable from a sensory perspective", "tokens": [51220, 411, 257, 2568, 293, 13437, 457, 550, 661, 3931, 1062, 312, 1570, 7006, 490, 257, 27233, 4585, 51756], "temperature": 0.0, "avg_logprob": -0.02763418284329501, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0033242153003811836}, {"id": 424, "seek": 287752, "start": 2877.52, "end": 2883.6, "text": " but smaller differences even in that variable relate to some other component of uncertainty", "tokens": [50364, 457, 4356, 7300, 754, 294, 300, 7006, 10961, 281, 512, 661, 6542, 295, 15697, 50668], "temperature": 0.0, "avg_logprob": -0.06307798624038696, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0005883774720132351}, {"id": 425, "seek": 287752, "start": 2884.32, "end": 2890.72, "text": " resolution from some other component of the the model like those are going to be the cases where", "tokens": [50704, 8669, 490, 512, 661, 6542, 295, 264, 264, 2316, 411, 729, 366, 516, 281, 312, 264, 3331, 689, 51024], "temperature": 0.0, "avg_logprob": -0.06307798624038696, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0005883774720132351}, {"id": 426, "seek": 287752, "start": 2891.68, "end": 2898.64, "text": " cognitive modeling does differ from just dispersed decision-making however they're", "tokens": [51072, 15605, 15983, 775, 743, 490, 445, 48059, 3537, 12, 12402, 4461, 436, 434, 51420], "temperature": 0.0, "avg_logprob": -0.06307798624038696, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0005883774720132351}, {"id": 427, "seek": 287752, "start": 2898.64, "end": 2906.8, "text": " both going to result in dispersed decision-making profiles like looking at the choices in the", "tokens": [51420, 1293, 516, 281, 1874, 294, 48059, 3537, 12, 12402, 23693, 411, 1237, 412, 264, 7994, 294, 264, 51828], "temperature": 0.0, "avg_logprob": -0.06307798624038696, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0005883774720132351}, {"id": 428, "seek": 290680, "start": 2906.8, "end": 2915.52, "text": " figures but the choices to sample from the less ambiguous parts of the actual distribution", "tokens": [50364, 9624, 457, 264, 7994, 281, 6889, 490, 264, 1570, 39465, 3166, 295, 264, 3539, 7316, 50800], "temperature": 0.0, "avg_logprob": -0.04629290955407279, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.001263833837583661}, {"id": 429, "seek": 290680, "start": 2916.32, "end": 2925.6000000000004, "text": " that leads to a much narrower policy path in this cognitive control setting versus in a variability", "tokens": [50840, 300, 6689, 281, 257, 709, 46751, 3897, 3100, 294, 341, 15605, 1969, 3287, 5717, 294, 257, 35709, 51304], "temperature": 0.0, "avg_logprob": -0.04629290955407279, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.001263833837583661}, {"id": 430, "seek": 290680, "start": 2925.6000000000004, "end": 2932.5600000000004, "text": " sampling where it would go for the areas that were just more variable but not necessarily", "tokens": [51304, 21179, 689, 309, 576, 352, 337, 264, 3179, 300, 645, 445, 544, 7006, 457, 406, 4725, 51652], "temperature": 0.0, "avg_logprob": -0.04629290955407279, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.001263833837583661}, {"id": 431, "seek": 293256, "start": 2932.56, "end": 2935.7599999999998, "text": " providing more information question mark", "tokens": [50364, 6530, 544, 1589, 1168, 1491, 50524], "temperature": 0.0, "avg_logprob": -0.15480629365835616, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0095548490062356}, {"id": 432, "seek": 293256, "start": 2939.2799999999997, "end": 2941.84, "text": " and you can add on this with the maximum entropy or anything", "tokens": [50700, 293, 291, 393, 909, 322, 341, 365, 264, 6674, 30867, 420, 1340, 50828], "temperature": 0.0, "avg_logprob": -0.15480629365835616, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0095548490062356}, {"id": 433, "seek": 293256, "start": 2943.2, "end": 2950.32, "text": " and I would even highlight kind of on the next slide it effectively what it is doing is accepting", "tokens": [50896, 293, 286, 576, 754, 5078, 733, 295, 322, 264, 958, 4137, 309, 8659, 437, 309, 307, 884, 307, 17391, 51252], "temperature": 0.0, "avg_logprob": -0.15480629365835616, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0095548490062356}, {"id": 434, "seek": 293256, "start": 2950.32, "end": 2956.64, "text": " that you're not going to gain a lot of resolution in these highly variable regions and so you don't", "tokens": [51252, 300, 291, 434, 406, 516, 281, 6052, 257, 688, 295, 8669, 294, 613, 5405, 7006, 10682, 293, 370, 291, 500, 380, 51568], "temperature": 0.0, "avg_logprob": -0.15480629365835616, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0095548490062356}, {"id": 435, "seek": 295664, "start": 2956.64, "end": 2962.72, "text": " really have to sample into those deeply because you've accepted that it is variable it is not", "tokens": [50364, 534, 362, 281, 6889, 666, 729, 8760, 570, 291, 600, 9035, 300, 309, 307, 7006, 309, 307, 406, 50668], "temperature": 0.0, "avg_logprob": -0.06656702553353659, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.061850406229496}, {"id": 436, "seek": 295664, "start": 2962.72, "end": 2967.8399999999997, "text": " something it is inherently variable in the data we're not going to gain a lot of information", "tokens": [50668, 746, 309, 307, 27993, 7006, 294, 264, 1412, 321, 434, 406, 516, 281, 6052, 257, 688, 295, 1589, 50924], "temperature": 0.0, "avg_logprob": -0.06656702553353659, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.061850406229496}, {"id": 437, "seek": 295664, "start": 2967.8399999999997, "end": 2974.08, "text": " from these regions and it's highlighted in blue down there and I think that's one of the big highlight", "tokens": [50924, 490, 613, 10682, 293, 309, 311, 17173, 294, 3344, 760, 456, 293, 286, 519, 300, 311, 472, 295, 264, 955, 5078, 51236], "temperature": 0.0, "avg_logprob": -0.06656702553353659, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.061850406229496}, {"id": 438, "seek": 295664, "start": 2974.08, "end": 2981.12, "text": " notes of this figure is this less information gain available in these highly available regions", "tokens": [51236, 5570, 295, 341, 2573, 307, 341, 1570, 1589, 6052, 2435, 294, 613, 5405, 2435, 10682, 51588], "temperature": 0.0, "avg_logprob": -0.06656702553353659, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.061850406229496}, {"id": 439, "seek": 298112, "start": 2981.12, "end": 2987.7599999999998, "text": " and that's something that makes this method more robust and powerful when you're dealing with some", "tokens": [50364, 293, 300, 311, 746, 300, 1669, 341, 3170, 544, 13956, 293, 4005, 562, 291, 434, 6260, 365, 512, 50696], "temperature": 0.0, "avg_logprob": -0.06813473479692327, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0011335158487781882}, {"id": 440, "seek": 298112, "start": 2987.7599999999998, "end": 3000.16, "text": " of these non-uniform variable data yeah awesome and then the the um streetlight effect is brought in", "tokens": [50696, 295, 613, 2107, 12, 409, 8629, 7006, 1412, 1338, 3476, 293, 550, 264, 264, 1105, 4838, 2764, 1802, 307, 3038, 294, 51316], "temperature": 0.0, "avg_logprob": -0.06813473479692327, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0011335158487781882}, {"id": 441, "seek": 298112, "start": 3000.16, "end": 3005.3599999999997, "text": " there so the avoidance of sampling in ambiguous locations is sometime referred to as a streetlight", "tokens": [51316, 456, 370, 264, 5042, 719, 295, 21179, 294, 39465, 9253, 307, 15053, 10839, 281, 382, 257, 4838, 2764, 51576], "temperature": 0.0, "avg_logprob": -0.06813473479692327, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0011335158487781882}, {"id": 442, "seek": 298112, "start": 3005.3599999999997, "end": 3010.48, "text": " effect the tendency to search where data are generated in a minimally ambiguous way i.e.", "tokens": [51576, 1802, 264, 18187, 281, 3164, 689, 1412, 366, 10833, 294, 257, 4464, 379, 39465, 636, 741, 13, 68, 13, 51832], "temperature": 0.0, "avg_logprob": -0.06813473479692327, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0011335158487781882}, {"id": 443, "seek": 301048, "start": 3010.48, "end": 3018.0, "text": " under streetlamp compared to searching elsewhere on a darkened street so I made some GPT-40 images", "tokens": [50364, 833, 4838, 75, 1215, 5347, 281, 10808, 14517, 322, 257, 2877, 5320, 4838, 370, 286, 1027, 512, 26039, 51, 12, 5254, 5267, 50740], "temperature": 0.0, "avg_logprob": -0.08343595977223248, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.0023230595979839563}, {"id": 444, "seek": 301048, "start": 3018.8, "end": 3025.52, "text": " some fun streetlight and on one hand there's kind of this sense of like is it constraining to look", "tokens": [50780, 512, 1019, 4838, 2764, 293, 322, 472, 1011, 456, 311, 733, 295, 341, 2020, 295, 411, 307, 309, 11525, 1760, 281, 574, 51116], "temperature": 0.0, "avg_logprob": -0.08343595977223248, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.0023230595979839563}, {"id": 445, "seek": 301048, "start": 3025.52, "end": 3030.64, "text": " under only the streetlight isn't that kind of absurd and then there's the joke about how what", "tokens": [51116, 833, 787, 264, 4838, 2764, 1943, 380, 300, 733, 295, 19774, 293, 550, 456, 311, 264, 7647, 466, 577, 437, 51372], "temperature": 0.0, "avg_logprob": -0.08343595977223248, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.0023230595979839563}, {"id": 446, "seek": 301048, "start": 3030.64, "end": 3035.04, "text": " the person's looking for is elsewhere but they're still searching under the streetlight", "tokens": [51372, 264, 954, 311, 1237, 337, 307, 14517, 457, 436, 434, 920, 10808, 833, 264, 4838, 2764, 51592], "temperature": 0.0, "avg_logprob": -0.08343595977223248, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.0023230595979839563}, {"id": 447, "seek": 301048, "start": 3035.04, "end": 3038.56, "text": " but they're looking for something they they know is elsewhere so that's the kind of", "tokens": [51592, 457, 436, 434, 1237, 337, 746, 436, 436, 458, 307, 14517, 370, 300, 311, 264, 733, 295, 51768], "temperature": 0.0, "avg_logprob": -0.08343595977223248, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.0023230595979839563}, {"id": 448, "seek": 303856, "start": 3038.56, "end": 3044.48, "text": " tragic element of it then there's this limited element however there's also this realistic element", "tokens": [50364, 20385, 4478, 295, 309, 550, 456, 311, 341, 5567, 4478, 4461, 456, 311, 611, 341, 12465, 4478, 50660], "temperature": 0.0, "avg_logprob": -0.05069337171666762, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.007576643489301205}, {"id": 449, "seek": 303856, "start": 3044.48, "end": 3051.12, "text": " which is like well are you supposed to search where you can't sense or outside of where you", "tokens": [50660, 597, 307, 411, 731, 366, 291, 3442, 281, 3164, 689, 291, 393, 380, 2020, 420, 2380, 295, 689, 291, 50992], "temperature": 0.0, "avg_logprob": -0.05069337171666762, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.007576643489301205}, {"id": 450, "seek": 303856, "start": 3051.12, "end": 3058.88, "text": " are at that moment so how could you you know say that that wasn't just and then this paper is more", "tokens": [50992, 366, 412, 300, 1623, 370, 577, 727, 291, 291, 458, 584, 300, 300, 2067, 380, 445, 293, 550, 341, 3035, 307, 544, 51380], "temperature": 0.0, "avg_logprob": -0.05069337171666762, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.007576643489301205}, {"id": 451, "seek": 303856, "start": 3058.88, "end": 3067.2799999999997, "text": " framing it as just a general condition of perception like you're in your tactile streetlight", "tokens": [51380, 28971, 309, 382, 445, 257, 2674, 4188, 295, 12860, 411, 291, 434, 294, 428, 47319, 4838, 2764, 51800], "temperature": 0.0, "avg_logprob": -0.05069337171666762, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.007576643489301205}, {"id": 452, "seek": 306728, "start": 3067.36, "end": 3074.6400000000003, "text": " that is the part you can see at all you can have latent modeling of any and other things", "tokens": [50368, 300, 307, 264, 644, 291, 393, 536, 412, 439, 291, 393, 362, 48994, 15983, 295, 604, 293, 661, 721, 50732], "temperature": 0.0, "avg_logprob": -0.059505703335716614, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0035932078026235104}, {"id": 453, "seek": 306728, "start": 3074.6400000000003, "end": 3080.96, "text": " but if it's not grounded in some way to a measurement made in a streetlight under the", "tokens": [50732, 457, 498, 309, 311, 406, 23535, 294, 512, 636, 281, 257, 13160, 1027, 294, 257, 4838, 2764, 833, 264, 51048], "temperature": 0.0, "avg_logprob": -0.059505703335716614, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0035932078026235104}, {"id": 454, "seek": 306728, "start": 3080.96, "end": 3086.0800000000004, "text": " metaphor where the light allows for observation then you're not connected to data unless you're", "tokens": [51048, 19157, 689, 264, 1442, 4045, 337, 14816, 550, 291, 434, 406, 4582, 281, 1412, 5969, 291, 434, 51304], "temperature": 0.0, "avg_logprob": -0.059505703335716614, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0035932078026235104}, {"id": 455, "seek": 306728, "start": 3086.0800000000004, "end": 3091.2000000000003, "text": " connected to that streetlight so that's just a very interesting kind of topic and and reference", "tokens": [51304, 4582, 281, 300, 4838, 2764, 370, 300, 311, 445, 257, 588, 1880, 733, 295, 4829, 293, 293, 6408, 51560], "temperature": 0.0, "avg_logprob": -0.059505703335716614, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0035932078026235104}, {"id": 456, "seek": 306728, "start": 3091.2000000000003, "end": 3096.1600000000003, "text": " that the authors use what do you think absolutely I mean it kind of boils down to you can't", "tokens": [51560, 300, 264, 16552, 764, 437, 360, 291, 519, 3122, 286, 914, 309, 733, 295, 35049, 760, 281, 291, 393, 380, 51808], "temperature": 0.0, "avg_logprob": -0.059505703335716614, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0035932078026235104}, {"id": 457, "seek": 309616, "start": 3096.16, "end": 3101.52, "text": " know what you don't know what you can't observe you know if you can only observe what's underneath", "tokens": [50364, 458, 437, 291, 500, 380, 458, 437, 291, 393, 380, 11441, 291, 458, 498, 291, 393, 787, 11441, 437, 311, 7223, 50632], "temperature": 0.0, "avg_logprob": -0.07690102555031, "compression_ratio": 2.0105263157894737, "no_speech_prob": 0.007117819506675005}, {"id": 458, "seek": 309616, "start": 3101.52, "end": 3106.24, "text": " the streetlight then you can't really know what else is outside of there and so your inference", "tokens": [50632, 264, 4838, 2764, 550, 291, 393, 380, 534, 458, 437, 1646, 307, 2380, 295, 456, 293, 370, 428, 38253, 50868], "temperature": 0.0, "avg_logprob": -0.07690102555031, "compression_ratio": 2.0105263157894737, "no_speech_prob": 0.007117819506675005}, {"id": 459, "seek": 309616, "start": 3106.24, "end": 3111.3599999999997, "text": " necessarily should be constrained to what you're able to actually observe you can't observe the", "tokens": [50868, 4725, 820, 312, 38901, 281, 437, 291, 434, 1075, 281, 767, 11441, 291, 393, 380, 11441, 264, 51124], "temperature": 0.0, "avg_logprob": -0.07690102555031, "compression_ratio": 2.0105263157894737, "no_speech_prob": 0.007117819506675005}, {"id": 460, "seek": 309616, "start": 3111.3599999999997, "end": 3117.44, "text": " unknown and so not necessarily in this case um because you don't even know if it even exists", "tokens": [51124, 9841, 293, 370, 406, 4725, 294, 341, 1389, 1105, 570, 291, 500, 380, 754, 458, 498, 309, 754, 8198, 51428], "temperature": 0.0, "avg_logprob": -0.07690102555031, "compression_ratio": 2.0105263157894737, "no_speech_prob": 0.007117819506675005}, {"id": 461, "seek": 311744, "start": 3117.44, "end": 3125.28, "text": " you have no data to confirm or to refute it all right section five dynamic processes so", "tokens": [50364, 291, 362, 572, 1412, 281, 9064, 420, 281, 1895, 1169, 309, 439, 558, 3541, 1732, 8546, 7555, 370, 50756], "temperature": 0.0, "avg_logprob": -0.07935429105953294, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.017174839973449707}, {"id": 462, "seek": 311744, "start": 3126.08, "end": 3130.7200000000003, "text": " in that previous example there was a data selection challenge whether it was approached", "tokens": [50796, 294, 300, 3894, 1365, 456, 390, 257, 1412, 9450, 3430, 1968, 309, 390, 17247, 51028], "temperature": 0.0, "avg_logprob": -0.07935429105953294, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.017174839973449707}, {"id": 463, "seek": 311744, "start": 3130.7200000000003, "end": 3136.4, "text": " from the flat fixed prior or all these other kind of subsequent variants with the adaptivity", "tokens": [51028, 490, 264, 4962, 6806, 4059, 420, 439, 613, 661, 733, 295, 19962, 21669, 365, 264, 6231, 4253, 51312], "temperature": 0.0, "avg_logprob": -0.07935429105953294, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.017174839973449707}, {"id": 464, "seek": 311744, "start": 3136.4, "end": 3142.32, "text": " and or with the cost now there's going to be a time element brought into the underlying", "tokens": [51312, 293, 420, 365, 264, 2063, 586, 456, 311, 516, 281, 312, 257, 565, 4478, 3038, 666, 264, 14217, 51608], "temperature": 0.0, "avg_logprob": -0.07935429105953294, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.017174839973449707}, {"id": 465, "seek": 311744, "start": 3142.32, "end": 3146.96, "text": " generative model we'll just go quickly here because that's the big point they take the static", "tokens": [51608, 1337, 1166, 2316, 321, 603, 445, 352, 2661, 510, 570, 300, 311, 264, 955, 935, 436, 747, 264, 13437, 51840], "temperature": 0.0, "avg_logprob": -0.07935429105953294, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.017174839973449707}, {"id": 466, "seek": 314696, "start": 3146.96, "end": 3153.12, "text": " distribution that was sampled from and now give the underlying process also variability", "tokens": [50364, 7316, 300, 390, 3247, 15551, 490, 293, 586, 976, 264, 14217, 1399, 611, 35709, 50672], "temperature": 0.0, "avg_logprob": -0.0661910593509674, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.0008040516986511648}, {"id": 467, "seek": 314696, "start": 3153.12, "end": 3160.8, "text": " through time so this is like a very SPM brain latent state causal modeling type set yeah anything", "tokens": [50672, 807, 565, 370, 341, 307, 411, 257, 588, 8420, 44, 3567, 48994, 1785, 38755, 15983, 2010, 992, 1338, 1340, 51056], "temperature": 0.0, "avg_logprob": -0.0661910593509674, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.0008040516986511648}, {"id": 468, "seek": 314696, "start": 3160.8, "end": 3166.88, "text": " you want to say on that before we go in oh no go ahead okay okay so they consider processes that", "tokens": [51056, 291, 528, 281, 584, 322, 300, 949, 321, 352, 294, 1954, 572, 352, 2286, 1392, 1392, 370, 436, 1949, 7555, 300, 51360], "temperature": 0.0, "avg_logprob": -0.0661910593509674, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.0008040516986511648}, {"id": 469, "seek": 314696, "start": 3166.88, "end": 3172.7200000000003, "text": " evolve in time equation 12 can be interpreted similarly to equation eight in which the expectation", "tokens": [51360, 16693, 294, 565, 5367, 2272, 393, 312, 26749, 14138, 281, 5367, 3180, 294, 597, 264, 14334, 51652], "temperature": 0.0, "avg_logprob": -0.0661910593509674, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.0008040516986511648}, {"id": 470, "seek": 317272, "start": 3172.72, "end": 3177.68, "text": " of the data is treated as a function approximation which now includes a time argument so here was eight", "tokens": [50364, 295, 264, 1412, 307, 8668, 382, 257, 2445, 28023, 597, 586, 5974, 257, 565, 6770, 370, 510, 390, 3180, 50612], "temperature": 0.0, "avg_logprob": -0.0621858819738611, "compression_ratio": 1.8652849740932642, "no_speech_prob": 0.016401071101427078}, {"id": 471, "seek": 317272, "start": 3178.3199999999997, "end": 3185.04, "text": " expectation of the data given latent state parameterization and policy equals so on", "tokens": [50644, 14334, 295, 264, 1412, 2212, 48994, 1785, 13075, 2144, 293, 3897, 6915, 370, 322, 50980], "temperature": 0.0, "avg_logprob": -0.0621858819738611, "compression_ratio": 1.8652849740932642, "no_speech_prob": 0.016401071101427078}, {"id": 472, "seek": 317272, "start": 3185.8399999999997, "end": 3192.8799999999997, "text": " and then here there is data also being a function of parameterization and policy", "tokens": [51020, 293, 550, 510, 456, 307, 1412, 611, 885, 257, 2445, 295, 13075, 2144, 293, 3897, 51372], "temperature": 0.0, "avg_logprob": -0.0621858819738611, "compression_ratio": 1.8652849740932642, "no_speech_prob": 0.016401071101427078}, {"id": 473, "seek": 317272, "start": 3192.8799999999997, "end": 3200.48, "text": " and then also bringing in an element with a subscript tau for time uh then you mentioned in", "tokens": [51372, 293, 550, 611, 5062, 294, 364, 4478, 365, 257, 2325, 662, 17842, 337, 565, 2232, 550, 291, 2835, 294, 51752], "temperature": 0.0, "avg_logprob": -0.0621858819738611, "compression_ratio": 1.8652849740932642, "no_speech_prob": 0.016401071101427078}, {"id": 474, "seek": 320048, "start": 3200.48, "end": 3208.16, "text": " your big questions the different approaches that they raise here with the three ways to", "tokens": [50364, 428, 955, 1651, 264, 819, 11587, 300, 436, 5300, 510, 365, 264, 1045, 2098, 281, 50748], "temperature": 0.0, "avg_logprob": -0.03257505734761556, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.00019110146968159825}, {"id": 475, "seek": 320048, "start": 3208.96, "end": 3213.44, "text": " bring temporalities into a model so let's definitely talk about that but just to show", "tokens": [50788, 1565, 8219, 16110, 666, 257, 2316, 370, 718, 311, 2138, 751, 466, 300, 457, 445, 281, 855, 51012], "temperature": 0.0, "avg_logprob": -0.03257505734761556, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.00019110146968159825}, {"id": 476, "seek": 320048, "start": 3213.44, "end": 3221.92, "text": " their images seven and eight are the pair for this dynamical section so figure seven shows a", "tokens": [51012, 641, 5267, 3407, 293, 3180, 366, 264, 6119, 337, 341, 5999, 804, 3541, 370, 2573, 3407, 3110, 257, 51436], "temperature": 0.0, "avg_logprob": -0.03257505734761556, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.00019110146968159825}, {"id": 477, "seek": 320048, "start": 3221.92, "end": 3226.56, "text": " graphical representation of the matrices involved in generating our data and the inferences obtained", "tokens": [51436, 35942, 10290, 295, 264, 32284, 3288, 294, 17746, 527, 1412, 293, 264, 13596, 2667, 14879, 51668], "temperature": 0.0, "avg_logprob": -0.03257505734761556, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.00019110146968159825}, {"id": 478, "seek": 322656, "start": 3226.56, "end": 3234.72, "text": " after sampling so here it is sampling from a time variance function and then figure eight goes into", "tokens": [50364, 934, 21179, 370, 510, 309, 307, 21179, 490, 257, 565, 21977, 2445, 293, 550, 2573, 3180, 1709, 666, 50772], "temperature": 0.0, "avg_logprob": -0.0780518345716523, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0028894818387925625}, {"id": 479, "seek": 322656, "start": 3234.72, "end": 3241.2799999999997, "text": " more detail and notes predictions based upon current data can be used to inform predictions", "tokens": [50772, 544, 2607, 293, 5570, 21264, 2361, 3564, 2190, 1412, 393, 312, 1143, 281, 1356, 21264, 51100], "temperature": 0.0, "avg_logprob": -0.0780518345716523, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0028894818387925625}, {"id": 480, "seek": 322656, "start": 3241.2799999999997, "end": 3246.4, "text": " about nearby spatial locations and to predict and post it the values of the function at different", "tokens": [51100, 466, 11184, 23598, 9253, 293, 281, 6069, 293, 2183, 309, 264, 4190, 295, 264, 2445, 412, 819, 51356], "temperature": 0.0, "avg_logprob": -0.0780518345716523, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0028894818387925625}, {"id": 481, "seek": 322656, "start": 3246.4, "end": 3255.52, "text": " points in time so just like you could have a 2d plane grayscale and infer the location of the", "tokens": [51356, 2793, 294, 565, 370, 445, 411, 291, 727, 362, 257, 568, 67, 5720, 677, 3772, 37088, 293, 13596, 264, 4914, 295, 264, 51812], "temperature": 0.0, "avg_logprob": -0.0780518345716523, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0028894818387925625}, {"id": 482, "seek": 325552, "start": 3255.52, "end": 3260.32, "text": " streetlight by pursuing like a gradient up the light and then there would be this optimal", "tokens": [50364, 4838, 2764, 538, 20222, 411, 257, 16235, 493, 264, 1442, 293, 550, 456, 576, 312, 341, 16252, 50604], "temperature": 0.0, "avg_logprob": -0.056702328764874, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.0038241769652813673}, {"id": 483, "seek": 325552, "start": 3260.32, "end": 3265.7599999999998, "text": " sampling like if you just got one observation you would want to sample on a line that was", "tokens": [50604, 21179, 411, 498, 291, 445, 658, 472, 14816, 291, 576, 528, 281, 6889, 322, 257, 1622, 300, 390, 50876], "temperature": 0.0, "avg_logprob": -0.056702328764874, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.0038241769652813673}, {"id": 484, "seek": 325552, "start": 3265.7599999999998, "end": 3270.8, "text": " orthogonal to the one that you couldn't resolve lots of ways to think about this sequential", "tokens": [50876, 41488, 281, 264, 472, 300, 291, 2809, 380, 14151, 3195, 295, 2098, 281, 519, 466, 341, 42881, 51128], "temperature": 0.0, "avg_logprob": -0.056702328764874, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.0038241769652813673}, {"id": 485, "seek": 325552, "start": 3271.44, "end": 3275.12, "text": " prediction but now the underlying landscape also changes so there's some", "tokens": [51160, 17630, 457, 586, 264, 14217, 9661, 611, 2962, 370, 456, 311, 512, 51344], "temperature": 0.0, "avg_logprob": -0.056702328764874, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.0038241769652813673}, {"id": 486, "seek": 325552, "start": 3276.0, "end": 3280.88, "text": " temporal dynamics and then that can be fit with all these different time series models and", "tokens": [51388, 30881, 15679, 293, 550, 300, 393, 312, 3318, 365, 439, 613, 819, 565, 2638, 5245, 293, 51632], "temperature": 0.0, "avg_logprob": -0.056702328764874, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.0038241769652813673}, {"id": 487, "seek": 328088, "start": 3281.6, "end": 3287.2000000000003, "text": " autocorrelation and so on however that's specified statistically in the generative model", "tokens": [50400, 45833, 284, 4419, 399, 293, 370, 322, 4461, 300, 311, 22206, 36478, 294, 264, 1337, 1166, 2316, 50680], "temperature": 0.0, "avg_logprob": -0.07345335682233174, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.004609023220837116}, {"id": 488, "seek": 328088, "start": 3287.2000000000003, "end": 3291.84, "text": " but this section just shows however you do make a statistical model for time", "tokens": [50680, 457, 341, 3541, 445, 3110, 4461, 291, 360, 652, 257, 22820, 2316, 337, 565, 50912], "temperature": 0.0, "avg_logprob": -0.07345335682233174, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.004609023220837116}, {"id": 489, "seek": 328088, "start": 3293.36, "end": 3299.76, "text": " it's basically going to be the same thing where information is going to be drawn from a distribution", "tokens": [50988, 309, 311, 1936, 516, 281, 312, 264, 912, 551, 689, 1589, 307, 516, 281, 312, 10117, 490, 257, 7316, 51308], "temperature": 0.0, "avg_logprob": -0.07345335682233174, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.004609023220837116}, {"id": 490, "seek": 328088, "start": 3299.76, "end": 3306.1600000000003, "text": " and now time is a variable in that distribution uh they write in this in the previous section we", "tokens": [51308, 293, 586, 565, 307, 257, 7006, 294, 300, 7316, 2232, 436, 2464, 294, 341, 294, 264, 3894, 3541, 321, 51628], "temperature": 0.0, "avg_logprob": -0.07345335682233174, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.004609023220837116}, {"id": 491, "seek": 328088, "start": 3306.1600000000003, "end": 3310.4, "text": " have demonstrated the way in which smarter optimal sampling may be used to select data", "tokens": [51628, 362, 18772, 264, 636, 294, 597, 20294, 16252, 21179, 815, 312, 1143, 281, 3048, 1412, 51840], "temperature": 0.0, "avg_logprob": -0.07345335682233174, "compression_ratio": 1.859504132231405, "no_speech_prob": 0.004609023220837116}, {"id": 492, "seek": 331040, "start": 3310.4, "end": 3315.44, "text": " in a manner that balances the cost of sampling or performing further experiments against the", "tokens": [50364, 294, 257, 9060, 300, 33993, 264, 2063, 295, 21179, 420, 10205, 3052, 12050, 1970, 264, 50616], "temperature": 0.0, "avg_logprob": -0.06303914154277128, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0010648901807144284}, {"id": 493, "seek": 331040, "start": 3315.44, "end": 3320.56, "text": " information gained from those samples or experiments each of these examples has relied", "tokens": [50616, 1589, 12634, 490, 729, 10938, 420, 12050, 1184, 295, 613, 5110, 575, 35463, 50872], "temperature": 0.0, "avg_logprob": -0.06303914154277128, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0010648901807144284}, {"id": 494, "seek": 331040, "start": 3320.56, "end": 3325.6800000000003, "text": " upon relatively simple and analytically comfortable linear Gaussian systems next we address active", "tokens": [50872, 3564, 7226, 2199, 293, 10783, 984, 4619, 8213, 39148, 3652, 958, 321, 2985, 4967, 51128], "temperature": 0.0, "avg_logprob": -0.06303914154277128, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0010648901807144284}, {"id": 495, "seek": 331040, "start": 3325.6800000000003, "end": 3332.64, "text": " sampling in a situation where analytical solutions are no longer possible so to highlight the key", "tokens": [51128, 21179, 294, 257, 2590, 689, 29579, 6547, 366, 572, 2854, 1944, 370, 281, 5078, 264, 2141, 51476], "temperature": 0.0, "avg_logprob": -0.06303914154277128, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0010648901807144284}, {"id": 496, "seek": 333264, "start": 3333.6, "end": 3340.3199999999997, "text": " formalisms that they're working with in that kind of background section uh or setup section", "tokens": [50412, 9860, 13539, 300, 436, 434, 1364, 365, 294, 300, 733, 295, 3678, 3541, 2232, 420, 8657, 3541, 50748], "temperature": 0.0, "avg_logprob": -0.0979782478718818, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014061560854315758}, {"id": 497, "seek": 333264, "start": 3341.68, "end": 3347.7599999999998, "text": " they kept one thing constant which was that the the generative model the generative process or", "tokens": [50816, 436, 4305, 472, 551, 5754, 597, 390, 300, 264, 264, 1337, 1166, 2316, 264, 1337, 1166, 1399, 420, 51120], "temperature": 0.0, "avg_logprob": -0.0979782478718818, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014061560854315758}, {"id": 498, "seek": 333264, "start": 3347.7599999999998, "end": 3354.72, "text": " however it's considered with the family of equations that the agent is inferring and tracking hidden", "tokens": [51120, 4461, 309, 311, 4888, 365, 264, 1605, 295, 11787, 300, 264, 9461, 307, 13596, 2937, 293, 11603, 7633, 51468], "temperature": 0.0, "avg_logprob": -0.0979782478718818, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014061560854315758}, {"id": 499, "seek": 333264, "start": 3354.72, "end": 3360.08, "text": " states with and that being the same as the actual family of equations that's generating", "tokens": [51468, 4368, 365, 293, 300, 885, 264, 912, 382, 264, 3539, 1605, 295, 11787, 300, 311, 17746, 51736], "temperature": 0.0, "avg_logprob": -0.0979782478718818, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014061560854315758}, {"id": 500, "seek": 336008, "start": 3360.72, "end": 3369.44, "text": " the function of observations and here that is relaxed so that opens it up to all empirical", "tokens": [50396, 264, 2445, 295, 18163, 293, 510, 300, 307, 14628, 370, 300, 9870, 309, 493, 281, 439, 31886, 50832], "temperature": 0.0, "avg_logprob": -0.046450417216231187, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004399032332003117}, {"id": 501, "seek": 336008, "start": 3369.44, "end": 3375.92, "text": " settings where you can just say right off the bat we do not have access to the generative model of", "tokens": [50832, 6257, 689, 291, 393, 445, 584, 558, 766, 264, 7362, 321, 360, 406, 362, 2105, 281, 264, 1337, 1166, 2316, 295, 51156], "temperature": 0.0, "avg_logprob": -0.046450417216231187, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004399032332003117}, {"id": 502, "seek": 336008, "start": 3375.92, "end": 3380.7999999999997, "text": " those data so we're making a map statistical map with all the associated trade-offs and", "tokens": [51156, 729, 1412, 370, 321, 434, 1455, 257, 4471, 22820, 4471, 365, 439, 264, 6615, 4923, 12, 19231, 293, 51400], "temperature": 0.0, "avg_logprob": -0.046450417216231187, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004399032332003117}, {"id": 503, "seek": 336008, "start": 3380.7999999999997, "end": 3388.4, "text": " statuses of like that genetic data or that transcriptomic data all those different kinds", "tokens": [51400, 6558, 279, 295, 411, 300, 12462, 1412, 420, 300, 24444, 21401, 1412, 439, 729, 819, 3685, 51780], "temperature": 0.0, "avg_logprob": -0.046450417216231187, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004399032332003117}, {"id": 504, "seek": 338840, "start": 3388.4, "end": 3394.96, "text": " of data sets starting from a position where it's going to have to be statistically approximated", "tokens": [50364, 295, 1412, 6352, 2891, 490, 257, 2535, 689, 309, 311, 516, 281, 362, 281, 312, 36478, 8542, 770, 50692], "temperature": 0.0, "avg_logprob": -0.06538176536560059, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.0037644163239747286}, {"id": 505, "seek": 338840, "start": 3395.76, "end": 3401.6800000000003, "text": " and it isn't going to be based unless explicitly otherwise on actual knowledge about the causal", "tokens": [50732, 293, 309, 1943, 380, 516, 281, 312, 2361, 5969, 20803, 5911, 322, 3539, 3601, 466, 264, 38755, 51028], "temperature": 0.0, "avg_logprob": -0.06538176536560059, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.0037644163239747286}, {"id": 506, "seek": 338840, "start": 3401.6800000000003, "end": 3409.6, "text": " elements of the system any any thoughts on that well in something else that they noted in the paper", "tokens": [51028, 4959, 295, 264, 1185, 604, 604, 4598, 322, 300, 731, 294, 746, 1646, 300, 436, 12964, 294, 264, 3035, 51424], "temperature": 0.0, "avg_logprob": -0.06538176536560059, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.0037644163239747286}, {"id": 507, "seek": 338840, "start": 3409.6, "end": 3416.2400000000002, "text": " by adding this time element when they're actually going through the time series the model itself will", "tokens": [51424, 538, 5127, 341, 565, 4478, 562, 436, 434, 767, 516, 807, 264, 565, 2638, 264, 2316, 2564, 486, 51756], "temperature": 0.0, "avg_logprob": -0.06538176536560059, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.0037644163239747286}, {"id": 508, "seek": 341624, "start": 3417.04, "end": 3424.3999999999996, "text": " preferentially select different data beyond what it just recently selected so time point one it", "tokens": [50404, 4382, 3137, 3048, 819, 1412, 4399, 437, 309, 445, 3938, 8209, 370, 565, 935, 472, 309, 50772], "temperature": 0.0, "avg_logprob": -0.10319122738308377, "compression_ratio": 1.984375, "no_speech_prob": 0.004754782188683748}, {"id": 509, "seek": 341624, "start": 3424.3999999999996, "end": 3431.12, "text": " selects x and y data time point two it might select l and m data so it actually will go through and", "tokens": [50772, 3048, 82, 2031, 293, 288, 1412, 565, 935, 732, 309, 1062, 3048, 287, 293, 275, 1412, 370, 309, 767, 486, 352, 807, 293, 51108], "temperature": 0.0, "avg_logprob": -0.10319122738308377, "compression_ratio": 1.984375, "no_speech_prob": 0.004754782188683748}, {"id": 510, "seek": 341624, "start": 3431.12, "end": 3437.9199999999996, "text": " select different types of data and it'll take a little bit of time um what the time is variable", "tokens": [51108, 3048, 819, 3467, 295, 1412, 293, 309, 603, 747, 257, 707, 857, 295, 565, 1105, 437, 264, 565, 307, 7006, 51448], "temperature": 0.0, "avg_logprob": -0.10319122738308377, "compression_ratio": 1.984375, "no_speech_prob": 0.004754782188683748}, {"id": 511, "seek": 341624, "start": 3437.9199999999996, "end": 3443.9199999999996, "text": " but it'll take a little bit of time before it revisits some of that previous data um at a", "tokens": [51448, 457, 309, 603, 747, 257, 707, 857, 295, 565, 949, 309, 20767, 1208, 512, 295, 300, 3894, 1412, 1105, 412, 257, 51748], "temperature": 0.0, "avg_logprob": -0.10319122738308377, "compression_ratio": 1.984375, "no_speech_prob": 0.004754782188683748}, {"id": 512, "seek": 344392, "start": 3444.0, "end": 3448.56, "text": " previous time so by this you kind of have a sliding window of data that you're selecting", "tokens": [50368, 3894, 565, 370, 538, 341, 291, 733, 295, 362, 257, 21169, 4910, 295, 1412, 300, 291, 434, 18182, 50596], "temperature": 0.0, "avg_logprob": -0.048179835081100464, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0009110011160373688}, {"id": 513, "seek": 344392, "start": 3448.56, "end": 3457.6800000000003, "text": " over different time periods yeah all right that's all going to come to play in this clinical trial", "tokens": [50596, 670, 819, 565, 13804, 1338, 439, 558, 300, 311, 439, 516, 281, 808, 281, 862, 294, 341, 9115, 7308, 51052], "temperature": 0.0, "avg_logprob": -0.048179835081100464, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0009110011160373688}, {"id": 514, "seek": 344392, "start": 3458.32, "end": 3465.2000000000003, "text": " which is the big final contribution section of the paper in our final example we demonstrate the", "tokens": [51084, 597, 307, 264, 955, 2572, 13150, 3541, 295, 264, 3035, 294, 527, 2572, 1365, 321, 11698, 264, 51428], "temperature": 0.0, "avg_logprob": -0.048179835081100464, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0009110011160373688}, {"id": 515, "seek": 344392, "start": 3465.2000000000003, "end": 3471.6, "text": " potential utility of the ideas outlined above in the context of a more concrete example so", "tokens": [51428, 3995, 14877, 295, 264, 3487, 27412, 3673, 294, 264, 4319, 295, 257, 544, 9859, 1365, 370, 51748], "temperature": 0.0, "avg_logprob": -0.048179835081100464, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0009110011160373688}, {"id": 516, "seek": 347160, "start": 3471.6, "end": 3479.8399999999997, "text": " they model the statistical setting here as an adaptive Bayesian clinical intervention methodology", "tokens": [50364, 436, 2316, 264, 22820, 3287, 510, 382, 364, 27912, 7840, 42434, 9115, 13176, 24850, 50776], "temperature": 0.0, "avg_logprob": -0.06918602929988378, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0009696809574961662}, {"id": 517, "seek": 347160, "start": 3479.8399999999997, "end": 3487.2799999999997, "text": " experiment for example the kind that was done during the 2014 West African Ebola outbreak", "tokens": [50776, 5120, 337, 1365, 264, 733, 300, 390, 1096, 1830, 264, 8227, 4055, 7312, 37846, 20963, 51148], "temperature": 0.0, "avg_logprob": -0.06918602929988378, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0009696809574961662}, {"id": 518, "seek": 347160, "start": 3488.24, "end": 3493.12, "text": " the active sampling approach advocated in this paper offers two main opportunities to augment", "tokens": [51196, 264, 4967, 21179, 3109, 7915, 770, 294, 341, 3035, 7736, 732, 2135, 4786, 281, 29919, 51440], "temperature": 0.0, "avg_logprob": -0.06918602929988378, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0009696809574961662}, {"id": 519, "seek": 347160, "start": 3493.12, "end": 3500.0, "text": " adaptive trial designs first it allows us to adapt the design to maximize the information we obtain", "tokens": [51440, 27912, 7308, 11347, 700, 309, 4045, 505, 281, 6231, 264, 1715, 281, 19874, 264, 1589, 321, 12701, 51784], "temperature": 0.0, "avg_logprob": -0.06918602929988378, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0009696809574961662}, {"id": 520, "seek": 350000, "start": 3500.0, "end": 3506.24, "text": " about treatment efficacy so that's the pure sense making information gain learning sampling from", "tokens": [50364, 466, 5032, 33492, 370, 300, 311, 264, 6075, 2020, 1455, 1589, 6052, 2539, 21179, 490, 50676], "temperature": 0.0, "avg_logprob": -0.05792360181932325, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0012842592550441623}, {"id": 521, "seek": 350000, "start": 3506.24, "end": 3512.32, "text": " where it's informative not from where like we habitually or prefer to look and then second", "tokens": [50676, 689, 309, 311, 27759, 406, 490, 689, 411, 321, 7164, 671, 420, 4382, 281, 574, 293, 550, 1150, 50980], "temperature": 0.0, "avg_logprob": -0.05792360181932325, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0012842592550441623}, {"id": 522, "seek": 350000, "start": 3513.12, "end": 3521.44, "text": " to balance and bring together that information gain with costs and that was brought in with", "tokens": [51020, 281, 4772, 293, 1565, 1214, 300, 1589, 6052, 365, 5497, 293, 300, 390, 3038, 294, 365, 51436], "temperature": 0.0, "avg_logprob": -0.05792360181932325, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0012842592550441623}, {"id": 523, "seek": 350000, "start": 3521.44, "end": 3528.64, "text": " the cost of the sampling section which was done in this paper by adding the stop policy option", "tokens": [51436, 264, 2063, 295, 264, 21179, 3541, 597, 390, 1096, 294, 341, 3035, 538, 5127, 264, 1590, 3897, 3614, 51796], "temperature": 0.0, "avg_logprob": -0.05792360181932325, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0012842592550441623}, {"id": 524, "seek": 352864, "start": 3528.64, "end": 3534.96, "text": " which can be probabilistically selected and then as other sampling locations become less", "tokens": [50364, 597, 393, 312, 31959, 20458, 8209, 293, 550, 382, 661, 21179, 9253, 1813, 1570, 50680], "temperature": 0.0, "avg_logprob": -0.054566135889367214, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002672541420906782}, {"id": 525, "seek": 352864, "start": 3534.96, "end": 3539.92, "text": " informative or if somebody was just sampled and you know that there's a slow decay through time", "tokens": [50680, 27759, 420, 498, 2618, 390, 445, 3247, 15551, 293, 291, 458, 300, 456, 311, 257, 2964, 21039, 807, 565, 50928], "temperature": 0.0, "avg_logprob": -0.054566135889367214, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002672541420906782}, {"id": 526, "seek": 352864, "start": 3539.92, "end": 3549.44, "text": " then on that subject the stop policy cost would outweigh the information gain from an experiment", "tokens": [50928, 550, 322, 300, 3983, 264, 1590, 3897, 2063, 576, 484, 826, 910, 264, 1589, 6052, 490, 364, 5120, 51404], "temperature": 0.0, "avg_logprob": -0.054566135889367214, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002672541420906782}, {"id": 527, "seek": 352864, "start": 3550.64, "end": 3557.44, "text": " and this is also I think will be a very interesting discussion this blows the line between clinical", "tokens": [51464, 293, 341, 307, 611, 286, 519, 486, 312, 257, 588, 1880, 5017, 341, 18458, 264, 1622, 1296, 9115, 51804], "temperature": 0.0, "avg_logprob": -0.054566135889367214, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002672541420906782}, {"id": 528, "seek": 355744, "start": 3557.44, "end": 3562.64, "text": " trial and public health intervention and can be seen as analogous to animal behavior that is never", "tokens": [50364, 7308, 293, 1908, 1585, 13176, 293, 393, 312, 1612, 382, 16660, 563, 281, 5496, 5223, 300, 307, 1128, 50624], "temperature": 0.0, "avg_logprob": -0.05320891414780215, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.005138237960636616}, {"id": 529, "seek": 355744, "start": 3562.64, "end": 3569.12, "text": " fully exploitive or explorative but is a balance between the two so how do we think about that", "tokens": [50624, 4498, 12382, 2187, 420, 24765, 1166, 457, 307, 257, 4772, 1296, 264, 732, 370, 577, 360, 321, 519, 466, 300, 50948], "temperature": 0.0, "avg_logprob": -0.05320891414780215, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.005138237960636616}, {"id": 530, "seek": 355744, "start": 3569.12, "end": 3575.76, "text": " in terms of biomedical and health security and all these different topics and any thoughts on this", "tokens": [50948, 294, 2115, 295, 49775, 293, 1585, 3825, 293, 439, 613, 819, 8378, 293, 604, 4598, 322, 341, 51280], "temperature": 0.0, "avg_logprob": -0.05320891414780215, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.005138237960636616}, {"id": 531, "seek": 355744, "start": 3575.76, "end": 3580.8, "text": " before we go into the formalism of the clinical trial just like about clinical trials or anything", "tokens": [51280, 949, 321, 352, 666, 264, 9860, 1434, 295, 264, 9115, 7308, 445, 411, 466, 9115, 12450, 420, 1340, 51532], "temperature": 0.0, "avg_logprob": -0.05320891414780215, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.005138237960636616}, {"id": 532, "seek": 358080, "start": 3580.8, "end": 3587.2000000000003, "text": " yeah and I think that that's going to be like that last point there is going to be a big one", "tokens": [50364, 1338, 293, 286, 519, 300, 300, 311, 516, 281, 312, 411, 300, 1036, 935, 456, 307, 516, 281, 312, 257, 955, 472, 50684], "temperature": 0.0, "avg_logprob": -0.0984980863683364, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.014059136621654034}, {"id": 533, "seek": 358080, "start": 3587.2000000000003, "end": 3592.96, "text": " going forward of like how do you balance benefits to the patient benefits to your trial benefits to", "tokens": [50684, 516, 2128, 295, 411, 577, 360, 291, 4772, 5311, 281, 264, 4537, 5311, 281, 428, 7308, 5311, 281, 50972], "temperature": 0.0, "avg_logprob": -0.0984980863683364, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.014059136621654034}, {"id": 534, "seek": 358080, "start": 3594.4, "end": 3598.96, "text": " essentially the company like there's a lot of different benefits and costs that you have to", "tokens": [51044, 4476, 264, 2237, 411, 456, 311, 257, 688, 295, 819, 5311, 293, 5497, 300, 291, 362, 281, 51272], "temperature": 0.0, "avg_logprob": -0.0984980863683364, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.014059136621654034}, {"id": 535, "seek": 358080, "start": 3598.96, "end": 3605.76, "text": " weigh into this and so these models are going to get very complicated when you start to distill", "tokens": [51272, 13843, 666, 341, 293, 370, 613, 5245, 366, 516, 281, 483, 588, 6179, 562, 291, 722, 281, 42923, 51612], "temperature": 0.0, "avg_logprob": -0.0984980863683364, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.014059136621654034}, {"id": 536, "seek": 360576, "start": 3605.84, "end": 3612.48, "text": " this into something especially with health related so it'll be very interesting to see how this evolves", "tokens": [50368, 341, 666, 746, 2318, 365, 1585, 4077, 370, 309, 603, 312, 588, 1880, 281, 536, 577, 341, 43737, 50700], "temperature": 0.0, "avg_logprob": -0.07710225031926082, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.02368338778614998}, {"id": 537, "seek": 360576, "start": 3615.2000000000003, "end": 3622.2400000000002, "text": " okay so here's how they do it our setup is as follows for each new cohort of participants", "tokens": [50836, 1392, 370, 510, 311, 577, 436, 360, 309, 527, 8657, 307, 382, 10002, 337, 1184, 777, 28902, 295, 10503, 51188], "temperature": 0.0, "avg_logprob": -0.07710225031926082, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.02368338778614998}, {"id": 538, "seek": 360576, "start": 3622.2400000000002, "end": 3630.6400000000003, "text": " we decide upon the randomization ratio to adopt that's the orange subscript r of policy so this", "tokens": [51188, 321, 4536, 3564, 264, 4974, 2144, 8509, 281, 6878, 300, 311, 264, 7671, 2325, 662, 367, 295, 3897, 370, 341, 51608], "temperature": 0.0, "avg_logprob": -0.07710225031926082, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.02368338778614998}, {"id": 539, "seek": 363064, "start": 3630.64, "end": 3638.48, "text": " is policy on a randomization ratio there's three options so this is a discrete but linearly ranked", "tokens": [50364, 307, 3897, 322, 257, 4974, 2144, 8509, 456, 311, 1045, 3956, 370, 341, 307, 257, 27706, 457, 43586, 20197, 50756], "temperature": 0.0, "avg_logprob": -0.05214871059764515, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014549853513017297}, {"id": 540, "seek": 363064, "start": 3638.48, "end": 3647.6, "text": " not fully categorical policy decision where one half would be the 50-50 sampling between the two", "tokens": [50756, 406, 4498, 19250, 804, 3897, 3537, 689, 472, 1922, 576, 312, 264, 2625, 12, 2803, 21179, 1296, 264, 732, 51212], "temperature": 0.0, "avg_logprob": -0.05214871059764515, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014549853513017297}, {"id": 541, "seek": 363064, "start": 3647.6, "end": 3657.92, "text": " groups whereas you know a priori that sampling in a skewed ratio is going to be less informative", "tokens": [51212, 3935, 9735, 291, 458, 257, 4059, 72, 300, 21179, 294, 257, 8756, 26896, 8509, 307, 516, 281, 312, 1570, 27759, 51728], "temperature": 0.0, "avg_logprob": -0.05214871059764515, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014549853513017297}, {"id": 542, "seek": 365792, "start": 3657.92, "end": 3662.48, "text": " like if you sampled only from one you would obviously be maximally uninformed about the other", "tokens": [50364, 411, 498, 291, 3247, 15551, 787, 490, 472, 291, 576, 2745, 312, 5138, 379, 43456, 22892, 466, 264, 661, 50592], "temperature": 0.0, "avg_logprob": -0.0572795750182352, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.002396589145064354}, {"id": 543, "seek": 365792, "start": 3662.48, "end": 3670.32, "text": " however what's going to end up being reflected in the policy decision to shift to a one-third or", "tokens": [50592, 4461, 437, 311, 516, 281, 917, 493, 885, 15502, 294, 264, 3897, 3537, 281, 5513, 281, 257, 472, 12, 25095, 420, 50984], "temperature": 0.0, "avg_logprob": -0.0572795750182352, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.002396589145064354}, {"id": 544, "seek": 365792, "start": 3670.32, "end": 3676.88, "text": " a two-third which is focusing observations on one branch of the study more than the other", "tokens": [50984, 257, 732, 12, 25095, 597, 307, 8416, 18163, 322, 472, 9819, 295, 264, 2979, 544, 813, 264, 661, 51312], "temperature": 0.0, "avg_logprob": -0.0572795750182352, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.002396589145064354}, {"id": 545, "seek": 365792, "start": 3676.88, "end": 3682.7200000000003, "text": " is going to focus on the explicit quantitative preference for observations of survival", "tokens": [51312, 307, 516, 281, 1879, 322, 264, 13691, 27778, 17502, 337, 18163, 295, 12559, 51604], "temperature": 0.0, "avg_logprob": -0.0572795750182352, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.002396589145064354}, {"id": 546, "seek": 368272, "start": 3683.52, "end": 3689.4399999999996, "text": " so that's going to be very interesting to see how the time variable which relates to the", "tokens": [50404, 370, 300, 311, 516, 281, 312, 588, 1880, 281, 536, 577, 264, 565, 7006, 597, 16155, 281, 264, 50700], "temperature": 0.0, "avg_logprob": -0.04245305061340332, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0009848917834460735}, {"id": 547, "seek": 368272, "start": 3689.4399999999996, "end": 3698.48, "text": " experimental design but by way of modeling the death curves of the participants and how different", "tokens": [50700, 17069, 1715, 457, 538, 636, 295, 15983, 264, 2966, 19490, 295, 264, 10503, 293, 577, 819, 51152], "temperature": 0.0, "avg_logprob": -0.04245305061340332, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0009848917834460735}, {"id": 548, "seek": 368272, "start": 3698.48, "end": 3705.4399999999996, "text": " preferences for complementary processes of reducing uncertainty about the treatment specific", "tokens": [51152, 21910, 337, 40705, 7555, 295, 12245, 15697, 466, 264, 5032, 2685, 51500], "temperature": 0.0, "avg_logprob": -0.04245305061340332, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0009848917834460735}, {"id": 549, "seek": 370544, "start": 3705.44, "end": 3712.48, "text": " death curves and not preferring to see death observations because that would introduce the", "tokens": [50364, 2966, 19490, 293, 406, 4382, 2937, 281, 536, 2966, 18163, 570, 300, 576, 5366, 264, 50716], "temperature": 0.0, "avg_logprob": -0.05841436982154846, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.05260604992508888}, {"id": 550, "seek": 370544, "start": 3712.48, "end": 3719.68, "text": " pragmatic imperative to measure low survival experiments so there's a lot of complexity", "tokens": [50716, 46904, 32490, 281, 3481, 2295, 12559, 12050, 370, 456, 311, 257, 688, 295, 14024, 51076], "temperature": 0.0, "avg_logprob": -0.05841436982154846, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.05260604992508888}, {"id": 551, "seek": 370544, "start": 3719.68, "end": 3727.12, "text": " in there from the public health side also in this very simple and interpretable way that like this", "tokens": [51076, 294, 456, 490, 264, 1908, 1585, 1252, 611, 294, 341, 588, 2199, 293, 7302, 712, 636, 300, 411, 341, 51448], "temperature": 0.0, "avg_logprob": -0.05841436982154846, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.05260604992508888}, {"id": 552, "seek": 372712, "start": 3727.12, "end": 3734.24, "text": " is like a Bayesian light switch with 50-50 information seeking mode or tilt it one way or", "tokens": [50364, 307, 411, 257, 7840, 42434, 1442, 3679, 365, 2625, 12, 2803, 1589, 11670, 4391, 420, 18446, 309, 472, 636, 420, 50720], "temperature": 0.0, "avg_logprob": -0.05366342278975475, "compression_ratio": 1.7102803738317758, "no_speech_prob": 0.060069333761930466}, {"id": 553, "seek": 372712, "start": 3734.24, "end": 3741.3599999999997, "text": " the other to bias observations whereas if no information had to be resolved then the policy", "tokens": [50720, 264, 661, 281, 12577, 18163, 9735, 498, 572, 1589, 632, 281, 312, 20772, 550, 264, 3897, 51076], "temperature": 0.0, "avg_logprob": -0.05366342278975475, "compression_ratio": 1.7102803738317758, "no_speech_prob": 0.060069333761930466}, {"id": 554, "seek": 372712, "start": 3741.3599999999997, "end": 3748.48, "text": " selection would orient towards observing long survival whereas if that was somehow changed", "tokens": [51076, 9450, 576, 8579, 3030, 22107, 938, 12559, 9735, 498, 300, 390, 6063, 3105, 51432], "temperature": 0.0, "avg_logprob": -0.05366342278975475, "compression_ratio": 1.7102803738317758, "no_speech_prob": 0.060069333761930466}, {"id": 555, "seek": 372712, "start": 3748.48, "end": 3752.96, "text": " then it would have to be adaptively sampled on the fly and changing these ratios and all that", "tokens": [51432, 550, 309, 576, 362, 281, 312, 6231, 3413, 3247, 15551, 322, 264, 3603, 293, 4473, 613, 32435, 293, 439, 300, 51656], "temperature": 0.0, "avg_logprob": -0.05366342278975475, "compression_ratio": 1.7102803738317758, "no_speech_prob": 0.060069333761930466}, {"id": 556, "seek": 375296, "start": 3752.96, "end": 3757.36, "text": " what do you what do you think about this yeah and what we're going to kind of get into is", "tokens": [50364, 437, 360, 291, 437, 360, 291, 519, 466, 341, 1338, 293, 437, 321, 434, 516, 281, 733, 295, 483, 666, 307, 50584], "temperature": 0.0, "avg_logprob": -0.07646429216539538, "compression_ratio": 2.0755555555555554, "no_speech_prob": 0.03459963575005531}, {"id": 557, "seek": 375296, "start": 3759.12, "end": 3764.4, "text": " especially with these sorts of health decisions you want people to survive like that's your", "tokens": [50672, 2318, 365, 613, 7527, 295, 1585, 5327, 291, 528, 561, 281, 7867, 411, 300, 311, 428, 50936], "temperature": 0.0, "avg_logprob": -0.07646429216539538, "compression_ratio": 2.0755555555555554, "no_speech_prob": 0.03459963575005531}, {"id": 558, "seek": 375296, "start": 3764.4, "end": 3769.2, "text": " that's your primary goal in a lot of these you want to see an effect you want to see a positive", "tokens": [50936, 300, 311, 428, 6194, 3387, 294, 257, 688, 295, 613, 291, 528, 281, 536, 364, 1802, 291, 528, 281, 536, 257, 3353, 51176], "temperature": 0.0, "avg_logprob": -0.07646429216539538, "compression_ratio": 2.0755555555555554, "no_speech_prob": 0.03459963575005531}, {"id": 559, "seek": 375296, "start": 3769.2, "end": 3774.56, "text": " effect of your treatment one way or the other you know if it's the placebo that's the positive or", "tokens": [51176, 1802, 295, 428, 5032, 472, 636, 420, 264, 661, 291, 458, 498, 309, 311, 264, 42779, 300, 311, 264, 3353, 420, 51444], "temperature": 0.0, "avg_logprob": -0.07646429216539538, "compression_ratio": 2.0755555555555554, "no_speech_prob": 0.03459963575005531}, {"id": 560, "seek": 375296, "start": 3774.56, "end": 3779.6, "text": " it's the actual treatment that's the positive you want people to survive so this is kind of", "tokens": [51444, 309, 311, 264, 3539, 5032, 300, 311, 264, 3353, 291, 528, 561, 281, 7867, 370, 341, 307, 733, 295, 51696], "temperature": 0.0, "avg_logprob": -0.07646429216539538, "compression_ratio": 2.0755555555555554, "no_speech_prob": 0.03459963575005531}, {"id": 561, "seek": 377960, "start": 3779.6, "end": 3785.7599999999998, "text": " getting into that ethics of making sure that when you design these things that you're doing the", "tokens": [50364, 1242, 666, 300, 19769, 295, 1455, 988, 300, 562, 291, 1715, 613, 721, 300, 291, 434, 884, 264, 50672], "temperature": 0.0, "avg_logprob": -0.05440203736468059, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.013218991458415985}, {"id": 562, "seek": 377960, "start": 3785.7599999999998, "end": 3793.6, "text": " maximum good to your participants who you know may not have you know much hope to stand on", "tokens": [50672, 6674, 665, 281, 428, 10503, 567, 291, 458, 815, 406, 362, 291, 458, 709, 1454, 281, 1463, 322, 51064], "temperature": 0.0, "avg_logprob": -0.05440203736468059, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.013218991458415985}, {"id": 563, "seek": 377960, "start": 3794.56, "end": 3800.4, "text": " doing some of these crises or epidemics or whatever they are experiencing at the time", "tokens": [51112, 884, 512, 295, 613, 31403, 420, 13510, 38014, 420, 2035, 436, 366, 11139, 412, 264, 565, 51404], "temperature": 0.0, "avg_logprob": -0.05440203736468059, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.013218991458415985}, {"id": 564, "seek": 377960, "start": 3802.16, "end": 3807.8399999999997, "text": " so you want to design this in such a way that you know you keep them the patients in mind", "tokens": [51492, 370, 291, 528, 281, 1715, 341, 294, 1270, 257, 636, 300, 291, 458, 291, 1066, 552, 264, 4209, 294, 1575, 51776], "temperature": 0.0, "avg_logprob": -0.05440203736468059, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.013218991458415985}, {"id": 565, "seek": 380784, "start": 3807.84, "end": 3815.92, "text": " that is the whole point of this and so by having a Bayesian kind of preference and bias to keeping", "tokens": [50364, 300, 307, 264, 1379, 935, 295, 341, 293, 370, 538, 1419, 257, 7840, 42434, 733, 295, 17502, 293, 12577, 281, 5145, 50768], "temperature": 0.0, "avg_logprob": -0.06785876891192268, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0005882707773707807}, {"id": 566, "seek": 380784, "start": 3815.92, "end": 3823.1200000000003, "text": " the patient alive and the best outcome you're maximizing how the patient's outcome in the", "tokens": [50768, 264, 4537, 5465, 293, 264, 1151, 9700, 291, 434, 5138, 3319, 577, 264, 4537, 311, 9700, 294, 264, 51128], "temperature": 0.0, "avg_logprob": -0.06785876891192268, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0005882707773707807}, {"id": 567, "seek": 380784, "start": 3823.1200000000003, "end": 3830.0, "text": " patient's life thanks for adding that another point to make this is from figure nine that's", "tokens": [51128, 4537, 311, 993, 3231, 337, 5127, 300, 1071, 935, 281, 652, 341, 307, 490, 2573, 4949, 300, 311, 51472], "temperature": 0.0, "avg_logprob": -0.06785876891192268, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0005882707773707807}, {"id": 568, "seek": 380784, "start": 3830.0, "end": 3836.96, "text": " gonna come up but it really highlights how sparse and few and interpretable the Bayesian", "tokens": [51472, 799, 808, 493, 457, 309, 534, 14254, 577, 637, 11668, 293, 1326, 293, 7302, 712, 264, 7840, 42434, 51820], "temperature": 0.0, "avg_logprob": -0.06785876891192268, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0005882707773707807}, {"id": 569, "seek": 383696, "start": 3836.96, "end": 3842.2400000000002, "text": " graphical formalism is and message passing which a lot of the equations describe and", "tokens": [50364, 35942, 9860, 1434, 307, 293, 3636, 8437, 597, 257, 688, 295, 264, 11787, 6786, 293, 50628], "temperature": 0.0, "avg_logprob": -0.09471266520650763, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.003706294810399413}, {"id": 570, "seek": 383696, "start": 3842.2400000000002, "end": 3848.96, "text": " the discussion about rx and fur touched upon message passing gives procedural ways to implement this", "tokens": [50628, 264, 5017, 466, 367, 87, 293, 2687, 9828, 3564, 3636, 8437, 2709, 43951, 2098, 281, 4445, 341, 50964], "temperature": 0.0, "avg_logprob": -0.09471266520650763, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.003706294810399413}, {"id": 571, "seek": 383696, "start": 3849.92, "end": 3855.2, "text": " in computational systems because it's sometimes hard to go from the simplicity of like this", "tokens": [51012, 294, 28270, 3652, 570, 309, 311, 2171, 1152, 281, 352, 490, 264, 25632, 295, 411, 341, 51276], "temperature": 0.0, "avg_logprob": -0.09471266520650763, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.003706294810399413}, {"id": 572, "seek": 383696, "start": 3855.2, "end": 3862.96, "text": " graphical model to fitting it iteratively on complex data sets but it's pretty clear to see", "tokens": [51276, 35942, 2316, 281, 15669, 309, 17138, 19020, 322, 3997, 1412, 6352, 457, 309, 311, 1238, 1850, 281, 536, 51664], "temperature": 0.0, "avg_logprob": -0.09471266520650763, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.003706294810399413}, {"id": 573, "seek": 386296, "start": 3863.04, "end": 3869.52, "text": " how different variables are upstream or downstream of other variables and also how the time", "tokens": [50368, 577, 819, 9102, 366, 33915, 420, 30621, 295, 661, 9102, 293, 611, 577, 264, 565, 50692], "temperature": 0.0, "avg_logprob": -0.08666909687102788, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.0011694536078721285}, {"id": 574, "seek": 386296, "start": 3871.44, "end": 3879.12, "text": " sampling can be shown to be which is the upstream of data sampled as these other factors are", "tokens": [50788, 21179, 393, 312, 4898, 281, 312, 597, 307, 264, 33915, 295, 1412, 3247, 15551, 382, 613, 661, 6771, 366, 51172], "temperature": 0.0, "avg_logprob": -0.08666909687102788, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.0011694536078721285}, {"id": 575, "seek": 386296, "start": 3880.08, "end": 3889.44, "text": " but it has a separable interpretable calculable epistemic value that doesn't have a certain kind", "tokens": [51220, 457, 309, 575, 257, 3128, 712, 7302, 712, 4322, 712, 2388, 468, 3438, 2158, 300, 1177, 380, 362, 257, 1629, 733, 51688], "temperature": 0.0, "avg_logprob": -0.08666909687102788, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.0011694536078721285}, {"id": 576, "seek": 388944, "start": 3889.44, "end": 3895.44, "text": " of connection to randomization ratio for example so being able to have explicit statistical", "tokens": [50364, 295, 4984, 281, 4974, 2144, 8509, 337, 1365, 370, 885, 1075, 281, 362, 13691, 22820, 50664], "temperature": 0.0, "avg_logprob": -0.07710409164428711, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.016651593148708344}, {"id": 577, "seek": 388944, "start": 3895.44, "end": 3903.28, "text": " calculations and directnesses where the follow-up time doesn't influence the treatment group ratio", "tokens": [50664, 20448, 293, 2047, 1287, 279, 689, 264, 1524, 12, 1010, 565, 1177, 380, 6503, 264, 5032, 1594, 8509, 51056], "temperature": 0.0, "avg_logprob": -0.07710409164428711, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.016651593148708344}, {"id": 578, "seek": 388944, "start": 3903.28, "end": 3910.32, "text": " or the randomization ratio or other processes gives a type of interpretability that the", "tokens": [51056, 420, 264, 4974, 2144, 8509, 420, 661, 7555, 2709, 257, 2010, 295, 7302, 2310, 300, 264, 51408], "temperature": 0.0, "avg_logprob": -0.07710409164428711, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.016651593148708344}, {"id": 579, "seek": 388944, "start": 3910.32, "end": 3916.32, "text": " generative model gives us the equations for and then the pragmatic challenges are about actually", "tokens": [51408, 1337, 1166, 2316, 2709, 505, 264, 11787, 337, 293, 550, 264, 46904, 4759, 366, 466, 767, 51708], "temperature": 0.0, "avg_logprob": -0.07710409164428711, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.016651593148708344}, {"id": 580, "seek": 391632, "start": 3916.32, "end": 3921.44, "text": " implementing that and then even if the computational component were totally addressed and abstracted", "tokens": [50364, 18114, 300, 293, 550, 754, 498, 264, 28270, 6542, 645, 3879, 13847, 293, 12649, 292, 50620], "temperature": 0.0, "avg_logprob": -0.11056781768798828, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.006288869772106409}, {"id": 581, "seek": 391632, "start": 3921.44, "end": 3927.52, "text": " away that would basically center these broader questions which i think the health example is", "tokens": [50620, 1314, 300, 576, 1936, 3056, 613, 13227, 1651, 597, 741, 519, 264, 1585, 1365, 307, 50924], "temperature": 0.0, "avg_logprob": -0.11056781768798828, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.006288869772106409}, {"id": 582, "seek": 391632, "start": 3927.52, "end": 3934.1600000000003, "text": " a great like jumping off point four yeah and uh you have probably recalled from all the other", "tokens": [50924, 257, 869, 411, 11233, 766, 935, 1451, 1338, 293, 2232, 291, 362, 1391, 39301, 490, 439, 264, 661, 51256], "temperature": 0.0, "avg_logprob": -0.11056781768798828, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.006288869772106409}, {"id": 583, "seek": 391632, "start": 3934.1600000000003, "end": 3941.52, "text": " different uh figures despite how simple this figure is the other figures the the plots were very", "tokens": [51256, 819, 2232, 9624, 7228, 577, 2199, 341, 2573, 307, 264, 661, 9624, 264, 264, 28609, 645, 588, 51624], "temperature": 0.0, "avg_logprob": -0.11056781768798828, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.006288869772106409}, {"id": 584, "seek": 394152, "start": 3942.0, "end": 3947.92, "text": " basic the models themselves like you just had the three nodes you know converging on the y so", "tokens": [50388, 3875, 264, 5245, 2969, 411, 291, 445, 632, 264, 1045, 13891, 291, 458, 9652, 3249, 322, 264, 288, 370, 50684], "temperature": 0.0, "avg_logprob": -0.07115765823715034, "compression_ratio": 1.916256157635468, "no_speech_prob": 0.020959770306944847}, {"id": 585, "seek": 394152, "start": 3947.92, "end": 3955.04, "text": " despite how simple this looks you are adding more complexity to these um to these systems and the more", "tokens": [50684, 7228, 577, 2199, 341, 1542, 291, 366, 5127, 544, 14024, 281, 613, 1105, 281, 613, 3652, 293, 264, 544, 51040], "temperature": 0.0, "avg_logprob": -0.07115765823715034, "compression_ratio": 1.916256157635468, "no_speech_prob": 0.020959770306944847}, {"id": 586, "seek": 394152, "start": 3956.08, "end": 3962.08, "text": " complexity that you add the harder it is the more computationally intensive it is and so this is", "tokens": [51092, 14024, 300, 291, 909, 264, 6081, 309, 307, 264, 544, 24903, 379, 18957, 309, 307, 293, 370, 341, 307, 51392], "temperature": 0.0, "avg_logprob": -0.07115765823715034, "compression_ratio": 1.916256157635468, "no_speech_prob": 0.020959770306944847}, {"id": 587, "seek": 394152, "start": 3962.08, "end": 3968.56, "text": " that question of how big can you go you know how how many nodes can you add how many parameters", "tokens": [51392, 300, 1168, 295, 577, 955, 393, 291, 352, 291, 458, 577, 577, 867, 13891, 393, 291, 909, 577, 867, 9834, 51716], "temperature": 0.0, "avg_logprob": -0.07115765823715034, "compression_ratio": 1.916256157635468, "no_speech_prob": 0.020959770306944847}, {"id": 588, "seek": 396856, "start": 3968.56, "end": 3973.36, "text": " can you add how much complexity can you add to the system before it starts to break down", "tokens": [50364, 393, 291, 909, 577, 709, 14024, 393, 291, 909, 281, 264, 1185, 949, 309, 3719, 281, 1821, 760, 50604], "temperature": 0.0, "avg_logprob": -0.041905205926777406, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0015485819894820452}, {"id": 589, "seek": 396856, "start": 3973.36, "end": 3981.36, "text": " or not perform as well as you would hope yeah so other than bringing in that randomization ratio", "tokens": [50604, 420, 406, 2042, 382, 731, 382, 291, 576, 1454, 1338, 370, 661, 813, 5062, 294, 300, 4974, 2144, 8509, 51004], "temperature": 0.0, "avg_logprob": -0.041905205926777406, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0015485819894820452}, {"id": 590, "seek": 396856, "start": 3981.36, "end": 3989.2, "text": " kind of expression of preference this model differs from the prior one in that it's defined", "tokens": [51004, 733, 295, 6114, 295, 17502, 341, 2316, 37761, 490, 264, 4059, 472, 294, 300, 309, 311, 7642, 51396], "temperature": 0.0, "avg_logprob": -0.041905205926777406, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0015485819894820452}, {"id": 591, "seek": 396856, "start": 3989.2, "end": 3994.24, "text": " that the kind of cognitive map is not the territory they're different families so that's", "tokens": [51396, 300, 264, 733, 295, 15605, 4471, 307, 406, 264, 11360, 436, 434, 819, 4466, 370, 300, 311, 51648], "temperature": 0.0, "avg_logprob": -0.041905205926777406, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.0015485819894820452}, {"id": 592, "seek": 399424, "start": 3994.24, "end": 4000.8799999999997, "text": " what motivates this um approximation approach so this is a simple displacement where still it's a", "tokens": [50364, 437, 42569, 341, 1105, 28023, 3109, 370, 341, 307, 257, 2199, 21899, 689, 920, 309, 311, 257, 50696], "temperature": 0.0, "avg_logprob": -0.03097237212748467, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0011878524674102664}, {"id": 593, "seek": 399424, "start": 4000.8799999999997, "end": 4009.7599999999998, "text": " trackable problem as they'll unfold however the simulation family chosen for the for like the", "tokens": [50696, 2837, 712, 1154, 382, 436, 603, 17980, 4461, 264, 16575, 1605, 8614, 337, 264, 337, 411, 264, 51140], "temperature": 0.0, "avg_logprob": -0.03097237212748467, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0011878524674102664}, {"id": 594, "seek": 399424, "start": 4009.7599999999998, "end": 4014.3199999999997, "text": " approximation basically the approximation could apply to any data set but it might be woefully", "tokens": [51140, 28023, 1936, 264, 28023, 727, 3079, 281, 604, 1412, 992, 457, 309, 1062, 312, 6020, 68, 2277, 51368], "temperature": 0.0, "avg_logprob": -0.03097237212748467, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0011878524674102664}, {"id": 595, "seek": 399424, "start": 4014.3199999999997, "end": 4020.8799999999997, "text": " inadequate like it might fit only one component of it so that's again part of the interesting", "tokens": [51368, 42107, 411, 309, 1062, 3318, 787, 472, 6542, 295, 309, 370, 300, 311, 797, 644, 295, 264, 1880, 51696], "temperature": 0.0, "avg_logprob": -0.03097237212748467, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0011878524674102664}, {"id": 596, "seek": 402088, "start": 4020.88, "end": 4028.88, "text": " question is like how similar does the statistical model have to be or what information does it really", "tokens": [50364, 1168, 307, 411, 577, 2531, 775, 264, 22820, 2316, 362, 281, 312, 420, 437, 1589, 775, 309, 534, 50764], "temperature": 0.0, "avg_logprob": -0.0811533380727299, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.003376306500285864}, {"id": 597, "seek": 402088, "start": 4028.88, "end": 4037.2000000000003, "text": " bring in and how to to model or work with an empirical side um but just on a more general", "tokens": [50764, 1565, 294, 293, 577, 281, 281, 2316, 420, 589, 365, 364, 31886, 1252, 1105, 457, 445, 322, 257, 544, 2674, 51180], "temperature": 0.0, "avg_logprob": -0.0811533380727299, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.003376306500285864}, {"id": 598, "seek": 402088, "start": 4037.2000000000003, "end": 4045.04, "text": " statistical level equations 15 16 17 describe some of the technical details of the incremental", "tokens": [51180, 22820, 1496, 11787, 2119, 3165, 3282, 6786, 512, 295, 264, 6191, 4365, 295, 264, 35759, 51572], "temperature": 0.0, "avg_logprob": -0.0811533380727299, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.003376306500285864}, {"id": 599, "seek": 404504, "start": 4045.04, "end": 4051.44, "text": " optimization gradient scheme the newton optimization variational plus we'll talk to", "tokens": [50364, 19618, 16235, 12232, 264, 777, 1756, 19618, 3034, 1478, 1804, 321, 603, 751, 281, 50684], "temperature": 0.0, "avg_logprob": -0.08798320326086594, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.013221925124526024}, {"id": 600, "seek": 404504, "start": 4051.44, "end": 4059.92, "text": " thomas et al figure nine is displaying the kind of before picture for the randomized control", "tokens": [50684, 258, 7092, 1030, 419, 2573, 4949, 307, 36834, 264, 733, 295, 949, 3036, 337, 264, 38513, 1969, 51108], "temperature": 0.0, "avg_logprob": -0.08798320326086594, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.013221925124526024}, {"id": 601, "seek": 404504, "start": 4059.92, "end": 4067.12, "text": " trial so here's where that graphical model is that was shown earlier and then here are these two", "tokens": [51108, 7308, 370, 510, 311, 689, 300, 35942, 2316, 307, 300, 390, 4898, 3071, 293, 550, 510, 366, 613, 732, 51468], "temperature": 0.0, "avg_logprob": -0.08798320326086594, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.013221925124526024}, {"id": 602, "seek": 404504, "start": 4067.84, "end": 4074.64, "text": " groups and their survival through time and different sampling uh choices that are made", "tokens": [51504, 3935, 293, 641, 12559, 807, 565, 293, 819, 21179, 2232, 7994, 300, 366, 1027, 51844], "temperature": 0.0, "avg_logprob": -0.08798320326086594, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.013221925124526024}, {"id": 603, "seek": 407504, "start": 4076.0, "end": 4084.0, "text": " then just to to jump to figure 10 has the same layout as figure nine but now using the expected", "tokens": [50412, 550, 445, 281, 281, 3012, 281, 2573, 1266, 575, 264, 912, 13333, 382, 2573, 4949, 457, 586, 1228, 264, 5176, 50812], "temperature": 0.0, "avg_logprob": -0.1015491370695183, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00010554496111581102}, {"id": 604, "seek": 407504, "start": 4084.0, "end": 4089.52, "text": " information gain from equation 18 to guide sampling of data so this is just to show the", "tokens": [50812, 1589, 6052, 490, 5367, 2443, 281, 5934, 21179, 295, 1412, 370, 341, 307, 445, 281, 855, 264, 51088], "temperature": 0.0, "avg_logprob": -0.1015491370695183, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00010554496111581102}, {"id": 605, "seek": 407504, "start": 4089.52, "end": 4095.2, "text": " impact of that active data sampling and it will drop back to the equation uh there are some notable", "tokens": [51088, 2712, 295, 300, 4967, 1412, 21179, 293, 309, 486, 3270, 646, 281, 264, 5367, 2232, 456, 366, 512, 22556, 51372], "temperature": 0.0, "avg_logprob": -0.1015491370695183, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00010554496111581102}, {"id": 606, "seek": 407504, "start": 4095.2, "end": 4104.72, "text": " differences between the choices made in figure 10 compared with nine nine choices 10 uh the most", "tokens": [51372, 7300, 1296, 264, 7994, 1027, 294, 2573, 1266, 5347, 365, 4949, 4949, 7994, 1266, 2232, 264, 881, 51848], "temperature": 0.0, "avg_logprob": -0.1015491370695183, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00010554496111581102}, {"id": 607, "seek": 410472, "start": 4104.8, "end": 4110.4800000000005, "text": " obvious of these is that the follow-up time selected have been moved later once optimal sampling is", "tokens": [50368, 6322, 295, 613, 307, 300, 264, 1524, 12, 1010, 565, 8209, 362, 668, 4259, 1780, 1564, 16252, 21179, 307, 50652], "temperature": 0.0, "avg_logprob": -0.062152429421742754, "compression_ratio": 1.79375, "no_speech_prob": 0.0005976384854875505}, {"id": 608, "seek": 410472, "start": 4110.4800000000005, "end": 4116.0, "text": " employed this makes intuitive sense as a later follow-up time is informative about the survival", "tokens": [50652, 20115, 341, 1669, 21769, 2020, 382, 257, 1780, 1524, 12, 1010, 565, 307, 27759, 466, 264, 12559, 50928], "temperature": 0.0, "avg_logprob": -0.062152429421742754, "compression_ratio": 1.79375, "no_speech_prob": 0.0005976384854875505}, {"id": 609, "seek": 410472, "start": 4116.0, "end": 4120.72, "text": " probabilities at all prior times whereas an earlier follow-up time is not informative about", "tokens": [50928, 33783, 412, 439, 4059, 1413, 9735, 364, 3071, 1524, 12, 1010, 565, 307, 406, 27759, 466, 51164], "temperature": 0.0, "avg_logprob": -0.062152429421742754, "compression_ratio": 1.79375, "no_speech_prob": 0.0005976384854875505}, {"id": 610, "seek": 412072, "start": 4120.72, "end": 4134.240000000001, "text": " survival probabilities at later time um where it gets in the final uh simulation brings in the", "tokens": [50364, 12559, 33783, 412, 1780, 565, 1105, 689, 309, 2170, 294, 264, 2572, 2232, 16575, 5607, 294, 264, 51040], "temperature": 0.0, "avg_logprob": -0.06620260288840846, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.02161329612135887}, {"id": 611, "seek": 412072, "start": 4134.240000000001, "end": 4143.12, "text": " random sampling plus the preference element here's where the symmetry is broken to also want the", "tokens": [51040, 4974, 21179, 1804, 264, 17502, 4478, 510, 311, 689, 264, 25440, 307, 5463, 281, 611, 528, 264, 51484], "temperature": 0.0, "avg_logprob": -0.06620260288840846, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.02161329612135887}, {"id": 612, "seek": 412072, "start": 4143.12, "end": 4148.16, "text": " measurement of survival as more likely than death which is how the preferences are specified", "tokens": [51484, 13160, 295, 12559, 382, 544, 3700, 813, 2966, 597, 307, 577, 264, 21910, 366, 22206, 51736], "temperature": 0.0, "avg_logprob": -0.06620260288840846, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.02161329612135887}, {"id": 613, "seek": 414816, "start": 4148.72, "end": 4158.32, "text": " in active inference um and then the policy switch is reflected in this like um part where", "tokens": [50392, 294, 4967, 38253, 1105, 293, 550, 264, 3897, 3679, 307, 15502, 294, 341, 411, 1105, 644, 689, 50872], "temperature": 0.0, "avg_logprob": -0.08183013068305121, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.009124870412051678}, {"id": 614, "seek": 414816, "start": 4158.32, "end": 4164.96, "text": " the observations are shifted later because there's there's less than a threshold of information to", "tokens": [50872, 264, 18163, 366, 18892, 1780, 570, 456, 311, 456, 311, 1570, 813, 257, 14678, 295, 1589, 281, 51204], "temperature": 0.0, "avg_logprob": -0.08183013068305121, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.009124870412051678}, {"id": 615, "seek": 414816, "start": 4164.96, "end": 4171.84, "text": " gain by having them earlier and then they uh even if there is equal variance i'm not exactly sure we", "tokens": [51204, 6052, 538, 1419, 552, 3071, 293, 550, 436, 2232, 754, 498, 456, 307, 2681, 21977, 741, 478, 406, 2293, 988, 321, 51548], "temperature": 0.0, "avg_logprob": -0.08183013068305121, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.009124870412051678}, {"id": 616, "seek": 417184, "start": 4171.84, "end": 4178.96, "text": " can ask between the two branches there is uh an over sampling for the group with the higher", "tokens": [50364, 393, 1029, 1296, 264, 732, 14770, 456, 307, 2232, 364, 670, 21179, 337, 264, 1594, 365, 264, 2946, 50720], "temperature": 0.0, "avg_logprob": -0.08677910609417651, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.011685099452733994}, {"id": 617, "seek": 417184, "start": 4178.96, "end": 4186.16, "text": " survival which in this case was the placebo group so anything to add on this preference element", "tokens": [50720, 12559, 597, 294, 341, 1389, 390, 264, 42779, 1594, 370, 1340, 281, 909, 322, 341, 17502, 4478, 51080], "temperature": 0.0, "avg_logprob": -0.08677910609417651, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.011685099452733994}, {"id": 618, "seek": 417184, "start": 4186.16, "end": 4193.4400000000005, "text": " this is the the like the crux of the whole making sure that you optimize you know the patient's", "tokens": [51080, 341, 307, 264, 264, 411, 264, 5140, 87, 295, 264, 1379, 1455, 988, 300, 291, 19719, 291, 458, 264, 4537, 311, 51444], "temperature": 0.0, "avg_logprob": -0.08677910609417651, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.011685099452733994}, {"id": 619, "seek": 417184, "start": 4193.4400000000005, "end": 4199.04, "text": " outcome on this because the treatments in this scenario were not um were not beneficial they're", "tokens": [51444, 9700, 322, 341, 570, 264, 15795, 294, 341, 9005, 645, 406, 1105, 645, 406, 14072, 436, 434, 51724], "temperature": 0.0, "avg_logprob": -0.08677910609417651, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.011685099452733994}, {"id": 620, "seek": 419904, "start": 4199.04, "end": 4204.96, "text": " actually harmful and so throughout the course of the study this by utilizing this model you", "tokens": [50364, 767, 19727, 293, 370, 3710, 264, 1164, 295, 264, 2979, 341, 538, 26775, 341, 2316, 291, 50660], "temperature": 0.0, "avg_logprob": -0.0636348043169294, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0071190036833286285}, {"id": 621, "seek": 419904, "start": 4204.96, "end": 4211.84, "text": " actually randomized more people into the placebo group which caused a greater survival of these", "tokens": [50660, 767, 38513, 544, 561, 666, 264, 42779, 1594, 597, 7008, 257, 5044, 12559, 295, 613, 51004], "temperature": 0.0, "avg_logprob": -0.0636348043169294, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0071190036833286285}, {"id": 622, "seek": 419904, "start": 4211.84, "end": 4218.64, "text": " uh individuals and so you're you can already see the effect that this sort of methodology has on", "tokens": [51004, 2232, 5346, 293, 370, 291, 434, 291, 393, 1217, 536, 264, 1802, 300, 341, 1333, 295, 24850, 575, 322, 51344], "temperature": 0.0, "avg_logprob": -0.0636348043169294, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0071190036833286285}, {"id": 623, "seek": 419904, "start": 4218.64, "end": 4224.72, "text": " clinical trials because you're optimizing the outcome and i think that is exactly what you want", "tokens": [51344, 9115, 12450, 570, 291, 434, 40425, 264, 9700, 293, 741, 519, 300, 307, 2293, 437, 291, 528, 51648], "temperature": 0.0, "avg_logprob": -0.0636348043169294, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0071190036833286285}, {"id": 624, "seek": 422472, "start": 4224.72, "end": 4231.4400000000005, "text": " to do in these health decisions in these trials in these things that impact human health uh or", "tokens": [50364, 281, 360, 294, 613, 1585, 5327, 294, 613, 12450, 294, 613, 721, 300, 2712, 1952, 1585, 2232, 420, 50700], "temperature": 0.0, "avg_logprob": -0.05941603183746338, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0032724086195230484}, {"id": 625, "seek": 422472, "start": 4231.4400000000005, "end": 4238.4800000000005, "text": " humanity in general you want to optimize the outcome uh and you know in this way you're actually", "tokens": [50700, 10243, 294, 2674, 291, 528, 281, 19719, 264, 9700, 2232, 293, 291, 458, 294, 341, 636, 291, 434, 767, 51052], "temperature": 0.0, "avg_logprob": -0.05941603183746338, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0032724086195230484}, {"id": 626, "seek": 422472, "start": 4238.4800000000005, "end": 4248.320000000001, "text": " reducing the overall harm to patients yeah interesting um here's the specification for the", "tokens": [51052, 12245, 264, 4787, 6491, 281, 4209, 1338, 1880, 1105, 510, 311, 264, 31256, 337, 264, 51544], "temperature": 0.0, "avg_logprob": -0.05941603183746338, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0032724086195230484}, {"id": 627, "seek": 424832, "start": 4248.32, "end": 4260.4, "text": " information gain so uh bringing in the the form of the message is required for equation three now", "tokens": [50364, 1589, 6052, 370, 2232, 5062, 294, 264, 264, 1254, 295, 264, 3636, 307, 4739, 337, 5367, 1045, 586, 50968], "temperature": 0.0, "avg_logprob": -0.08594869355023918, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.011506537906825542}, {"id": 628, "seek": 424832, "start": 4260.4, "end": 4266.719999999999, "text": " with all these extra components that have been added in with time variability demographic and the", "tokens": [50968, 365, 439, 613, 2857, 6677, 300, 362, 668, 3869, 294, 365, 565, 35709, 26331, 293, 264, 51284], "temperature": 0.0, "avg_logprob": -0.08594869355023918, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.011506537906825542}, {"id": 629, "seek": 424832, "start": 4266.719999999999, "end": 4275.759999999999, "text": " sampling they write out some of the technical details for the approximations in the variational", "tokens": [51284, 21179, 436, 2464, 484, 512, 295, 264, 6191, 4365, 337, 264, 8542, 763, 294, 264, 3034, 1478, 51736], "temperature": 0.0, "avg_logprob": -0.08594869355023918, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.011506537906825542}, {"id": 630, "seek": 427576, "start": 4275.76, "end": 4283.68, "text": " Laplace and then some aspects about those models which we can ask about but figure 11 basically", "tokens": [50364, 2369, 6742, 293, 550, 512, 7270, 466, 729, 5245, 597, 321, 393, 1029, 466, 457, 2573, 2975, 1936, 50760], "temperature": 0.0, "avg_logprob": -0.08839477698008219, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.008186646737158298}, {"id": 631, "seek": 427576, "start": 4283.68, "end": 4292.64, "text": " shows the big change which is that uh as you go from having a a flat sampling distribution", "tokens": [50760, 3110, 264, 955, 1319, 597, 307, 300, 2232, 382, 291, 352, 490, 1419, 257, 257, 4962, 21179, 7316, 51208], "temperature": 0.0, "avg_logprob": -0.08839477698008219, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.008186646737158298}, {"id": 632, "seek": 427576, "start": 4293.92, "end": 4300.64, "text": " across time and across treatment groups you can actually do better than that basically by choosing", "tokens": [51272, 2108, 565, 293, 2108, 5032, 3935, 291, 393, 767, 360, 1101, 813, 300, 1936, 538, 10875, 51608], "temperature": 0.0, "avg_logprob": -0.08839477698008219, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.008186646737158298}, {"id": 633, "seek": 430064, "start": 4300.64, "end": 4309.12, "text": " a sampling regime that makes sense given the costs of sampling so that's just very interesting", "tokens": [50364, 257, 21179, 13120, 300, 1669, 2020, 2212, 264, 5497, 295, 21179, 370, 300, 311, 445, 588, 1880, 50788], "temperature": 0.0, "avg_logprob": -0.028659506847983914, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.017439894378185272}, {"id": 634, "seek": 430064, "start": 4309.12, "end": 4314.56, "text": " because it it really does look like given the the possibility for these two lines to diverge", "tokens": [50788, 570, 309, 309, 534, 775, 574, 411, 2212, 264, 264, 7959, 337, 613, 732, 3876, 281, 18558, 432, 51060], "temperature": 0.0, "avg_logprob": -0.028659506847983914, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.017439894378185272}, {"id": 635, "seek": 430064, "start": 4315.4400000000005, "end": 4321.76, "text": " their divergence would be largest later on whereas if you could only schedule like one", "tokens": [51104, 641, 47387, 576, 312, 6443, 1780, 322, 9735, 498, 291, 727, 787, 7567, 411, 472, 51420], "temperature": 0.0, "avg_logprob": -0.028659506847983914, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.017439894378185272}, {"id": 636, "seek": 430064, "start": 4322.320000000001, "end": 4327.4400000000005, "text": " check for every person if it was something that was expected to happen later in life", "tokens": [51448, 1520, 337, 633, 954, 498, 309, 390, 746, 300, 390, 5176, 281, 1051, 1780, 294, 993, 51704], "temperature": 0.0, "avg_logprob": -0.028659506847983914, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.017439894378185272}, {"id": 637, "seek": 432744, "start": 4327.44, "end": 4336.16, "text": " then sampling all the young wouldn't even make sense so then one approach is like flat", "tokens": [50364, 550, 21179, 439, 264, 2037, 2759, 380, 754, 652, 2020, 370, 550, 472, 3109, 307, 411, 4962, 50800], "temperature": 0.0, "avg_logprob": -0.05647176504135132, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0021155651193112135}, {"id": 638, "seek": 432744, "start": 4336.16, "end": 4343.28, "text": " sampling but that is kind of sometimes erroneously called uh like unbiased or or uninformative but", "tokens": [50800, 21179, 457, 300, 307, 733, 295, 2171, 1189, 26446, 5098, 1219, 2232, 411, 517, 5614, 1937, 420, 420, 517, 37811, 1166, 457, 51156], "temperature": 0.0, "avg_logprob": -0.05647176504135132, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0021155651193112135}, {"id": 639, "seek": 432744, "start": 4343.28, "end": 4349.44, "text": " it is very informative and then this is pointing towards how there can be better sampling than", "tokens": [51156, 309, 307, 588, 27759, 293, 550, 341, 307, 12166, 3030, 577, 456, 393, 312, 1101, 21179, 813, 51464], "temperature": 0.0, "avg_logprob": -0.05647176504135132, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0021155651193112135}, {"id": 640, "seek": 434944, "start": 4349.44, "end": 4357.839999999999, "text": " just trying to go flat across the entire latent state estimate if there are priors relating to", "tokens": [50364, 445, 1382, 281, 352, 4962, 2108, 264, 2302, 48994, 1785, 12539, 498, 456, 366, 1790, 830, 23968, 281, 50784], "temperature": 0.0, "avg_logprob": -0.06016182899475098, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.04813031479716301}, {"id": 641, "seek": 434944, "start": 4357.839999999999, "end": 4364.4, "text": " something they can be leveraged as part of the probabilistic sampling and adapted to the", "tokens": [50784, 746, 436, 393, 312, 12451, 2980, 382, 644, 295, 264, 31959, 3142, 21179, 293, 20871, 281, 264, 51112], "temperature": 0.0, "avg_logprob": -0.06016182899475098, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.04813031479716301}, {"id": 642, "seek": 434944, "start": 4364.4, "end": 4372.24, "text": " data set at the in the end however for picking prior families for the active data selection", "tokens": [51112, 1412, 992, 412, 264, 294, 264, 917, 4461, 337, 8867, 4059, 4466, 337, 264, 4967, 1412, 9450, 51504], "temperature": 0.0, "avg_logprob": -0.06016182899475098, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.04813031479716301}, {"id": 643, "seek": 437224, "start": 4372.48, "end": 4379.76, "text": " that's a big question about how much it will change how the algorithms work so I guess that kind of takes us to the", "tokens": [50376, 300, 311, 257, 955, 1168, 466, 577, 709, 309, 486, 1319, 577, 264, 14642, 589, 370, 286, 2041, 300, 733, 295, 2516, 505, 281, 264, 50740], "temperature": 0.0, "avg_logprob": -0.21675974057044511, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.016652463003993034}, {"id": 644, "seek": 437224, "start": 4381.12, "end": 4387.28, "text": " discussion the paper's focus has been on illustrating how we might make use of", "tokens": [50808, 5017, 264, 3035, 311, 1879, 575, 668, 322, 8490, 8754, 577, 321, 1062, 652, 764, 295, 51116], "temperature": 0.0, "avg_logprob": -0.21675974057044511, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.016652463003993034}, {"id": 645, "seek": 437224, "start": 4387.28, "end": 4392.96, "text": " information seeking objectives augmented with costs which gave kind of the exit criterion or", "tokens": [51116, 1589, 11670, 15961, 36155, 365, 5497, 597, 2729, 733, 295, 264, 11043, 46691, 420, 51400], "temperature": 0.0, "avg_logprob": -0.21675974057044511, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.016652463003993034}, {"id": 646, "seek": 437224, "start": 4392.96, "end": 4399.44, "text": " preferences which gives the biased pragmatic part of data selection to choose the best data to", "tokens": [51400, 21910, 597, 2709, 264, 28035, 46904, 644, 295, 1412, 9450, 281, 2826, 264, 1151, 1412, 281, 51724], "temperature": 0.0, "avg_logprob": -0.21675974057044511, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.016652463003993034}, {"id": 647, "seek": 439944, "start": 4400.4, "end": 4408.799999999999, "text": " optimize our inferences and they highlight that the maximum entropy would yield identical results", "tokens": [50412, 19719, 527, 13596, 2667, 293, 436, 5078, 300, 264, 6674, 30867, 576, 11257, 14800, 3542, 50832], "temperature": 0.0, "avg_logprob": -0.07082208688708319, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.00648732203990221}, {"id": 648, "seek": 439944, "start": 4409.679999999999, "end": 4414.4, "text": " in several of our examples so that'll be interesting like what were the examples where", "tokens": [50876, 294, 2940, 295, 527, 5110, 370, 300, 603, 312, 1880, 411, 437, 645, 264, 5110, 689, 51112], "temperature": 0.0, "avg_logprob": -0.07082208688708319, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.00648732203990221}, {"id": 649, "seek": 439944, "start": 4414.4, "end": 4421.36, "text": " maximum entropy and the information gain are identical and then what are the real world", "tokens": [51112, 6674, 30867, 293, 264, 1589, 6052, 366, 14800, 293, 550, 437, 366, 264, 957, 1002, 51460], "temperature": 0.0, "avg_logprob": -0.07082208688708319, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.00648732203990221}, {"id": 650, "seek": 439944, "start": 4421.36, "end": 4427.5199999999995, "text": " or the statistical settings when the variance around predicted outcomes is in homogeneous", "tokens": [51460, 420, 264, 22820, 6257, 562, 264, 21977, 926, 19147, 10070, 307, 294, 42632, 51768], "temperature": 0.0, "avg_logprob": -0.07082208688708319, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.00648732203990221}, {"id": 651, "seek": 442752, "start": 4428.4800000000005, "end": 4439.040000000001, "text": " how does the full cognitive epistemic model based objective do differently than the max and", "tokens": [50412, 577, 775, 264, 1577, 15605, 2388, 468, 3438, 2316, 2361, 10024, 360, 7614, 813, 264, 11469, 293, 50940], "temperature": 0.0, "avg_logprob": -0.143134908856086, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.0021483555901795626}, {"id": 652, "seek": 442752, "start": 4440.64, "end": 4446.4800000000005, "text": " distribution dispersal kind of null hypothesis any thoughts on this", "tokens": [51020, 7316, 24631, 304, 733, 295, 18184, 17291, 604, 4598, 322, 341, 51312], "temperature": 0.0, "avg_logprob": -0.143134908856086, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.0021483555901795626}, {"id": 653, "seek": 442752, "start": 4448.64, "end": 4455.4400000000005, "text": " no I think you've captured it very well okay then there are several technical points worth", "tokens": [51420, 572, 286, 519, 291, 600, 11828, 309, 588, 731, 1392, 550, 456, 366, 2940, 6191, 2793, 3163, 51760], "temperature": 0.0, "avg_logprob": -0.143134908856086, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.0021483555901795626}, {"id": 654, "seek": 445544, "start": 4455.44, "end": 4459.759999999999, "text": " considering for how we might advance the concepts reviewed in this paper so let's talk about each", "tokens": [50364, 8079, 337, 577, 321, 1062, 7295, 264, 10392, 18429, 294, 341, 3035, 370, 718, 311, 751, 466, 1184, 50580], "temperature": 0.0, "avg_logprob": -0.07225798271797798, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.015903828665614128}, {"id": 655, "seek": 445544, "start": 4459.759999999999, "end": 4466.799999999999, "text": " of these one refinement of the active selection process two empirical evaluation of active versus", "tokens": [50580, 295, 613, 472, 1895, 30229, 295, 264, 4967, 9450, 1399, 732, 31886, 13344, 295, 4967, 5717, 50932], "temperature": 0.0, "avg_logprob": -0.07225798271797798, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.015903828665614128}, {"id": 656, "seek": 445544, "start": 4466.799999999999, "end": 4473.5199999999995, "text": " alternative sampling methods and three identifying the appropriate cost functions so we'll talk about", "tokens": [50932, 8535, 21179, 7150, 293, 1045, 16696, 264, 6854, 2063, 6828, 370, 321, 603, 751, 466, 51268], "temperature": 0.0, "avg_logprob": -0.07225798271797798, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.015903828665614128}, {"id": 657, "seek": 445544, "start": 4473.5199999999995, "end": 4482.32, "text": " those coming up conclusion here's the entire conclusion the key ideas involved in appeal", "tokens": [51268, 729, 1348, 493, 10063, 510, 311, 264, 2302, 10063, 264, 2141, 3487, 3288, 294, 13668, 51708], "temperature": 0.0, "avg_logprob": -0.07225798271797798, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.015903828665614128}, {"id": 658, "seek": 448232, "start": 4482.32, "end": 4488.08, "text": " to foveal like sampling of small portions of the total available data to minimize computational cost", "tokens": [50364, 281, 726, 303, 304, 411, 21179, 295, 1359, 25070, 295, 264, 3217, 2435, 1412, 281, 17522, 28270, 2063, 50652], "temperature": 0.0, "avg_logprob": -0.05329952836036682, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.014954761601984501}, {"id": 659, "seek": 448232, "start": 4488.719999999999, "end": 4496.08, "text": " that's a very cool way to put it and it highlights that kind of like sequential scanning but also", "tokens": [50684, 300, 311, 257, 588, 1627, 636, 281, 829, 309, 293, 309, 14254, 300, 733, 295, 411, 42881, 27019, 457, 611, 51052], "temperature": 0.0, "avg_logprob": -0.05329952836036682, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.014954761601984501}, {"id": 660, "seek": 448232, "start": 4496.08, "end": 4503.2, "text": " opens up some very exciting directions about how efficient that could be for some but not", "tokens": [51052, 9870, 493, 512, 588, 4670, 11095, 466, 577, 7148, 300, 727, 312, 337, 512, 457, 406, 51408], "temperature": 0.0, "avg_logprob": -0.05329952836036682, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.014954761601984501}, {"id": 661, "seek": 448232, "start": 4503.2, "end": 4508.5599999999995, "text": " other kinds of problems and how those problems could be identified or those patterns could be", "tokens": [51408, 661, 3685, 295, 2740, 293, 577, 729, 2740, 727, 312, 9234, 420, 729, 8294, 727, 312, 51676], "temperature": 0.0, "avg_logprob": -0.05329952836036682, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.014954761601984501}, {"id": 662, "seek": 450856, "start": 4508.56, "end": 4515.04, "text": " filtered for to where different kinds of succating models would be adaptive or not so like these are", "tokens": [50364, 37111, 337, 281, 689, 819, 3685, 295, 21578, 990, 5245, 576, 312, 27912, 420, 406, 370, 411, 613, 366, 50688], "temperature": 0.0, "avg_logprob": -0.10334059000015258, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.010985051281750202}, {"id": 663, "seek": 450856, "start": 4515.04, "end": 4521.68, "text": " all fun discussions we'll have and then they kind of brought all the theoretical components together", "tokens": [50688, 439, 1019, 11088, 321, 603, 362, 293, 550, 436, 733, 295, 3038, 439, 264, 20864, 6677, 1214, 51020], "temperature": 0.0, "avg_logprob": -0.10334059000015258, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.010985051281750202}, {"id": 664, "seek": 450856, "start": 4521.68, "end": 4528.88, "text": " at the end with the Bayes adaptive clinical trial with the cost of sampling constraints on", "tokens": [51020, 412, 264, 917, 365, 264, 7840, 279, 27912, 9115, 7308, 365, 264, 2063, 295, 21179, 18491, 322, 51380], "temperature": 0.0, "avg_logprob": -0.10334059000015258, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.010985051281750202}, {"id": 665, "seek": 450856, "start": 4528.88, "end": 4537.6, "text": " sampling and also the preference for survival so what are your overall thoughts or what are you", "tokens": [51380, 21179, 293, 611, 264, 17502, 337, 12559, 370, 437, 366, 428, 4787, 4598, 420, 437, 366, 291, 51816], "temperature": 0.0, "avg_logprob": -0.10334059000015258, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.010985051281750202}, {"id": 666, "seek": 453760, "start": 4537.6, "end": 4543.84, "text": " excited about for the ones to come I'm excited to kind of see me I mean this is a fairly recent", "tokens": [50364, 2919, 466, 337, 264, 2306, 281, 808, 286, 478, 2919, 281, 733, 295, 536, 385, 286, 914, 341, 307, 257, 6457, 5162, 50676], "temperature": 0.0, "avg_logprob": -0.06152344809638129, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002979963319376111}, {"id": 667, "seek": 453760, "start": 4543.84, "end": 4549.280000000001, "text": " paper I'm excited to see kind of where they have gone since this paper was published you know they", "tokens": [50676, 3035, 286, 478, 2919, 281, 536, 733, 295, 689, 436, 362, 2780, 1670, 341, 3035, 390, 6572, 291, 458, 436, 50948], "temperature": 0.0, "avg_logprob": -0.06152344809638129, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002979963319376111}, {"id": 668, "seek": 453760, "start": 4549.280000000001, "end": 4556.08, "text": " had a number of kind of next directions I would love to see what of those directions have they've", "tokens": [50948, 632, 257, 1230, 295, 733, 295, 958, 11095, 286, 576, 959, 281, 536, 437, 295, 729, 11095, 362, 436, 600, 51288], "temperature": 0.0, "avg_logprob": -0.06152344809638129, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002979963319376111}, {"id": 669, "seek": 453760, "start": 4556.08, "end": 4564.88, "text": " taken what they've compared it to other other sampling techniques this show shows a lot of promise", "tokens": [51288, 2726, 437, 436, 600, 5347, 309, 281, 661, 661, 21179, 7512, 341, 855, 3110, 257, 688, 295, 6228, 51728], "temperature": 0.0, "avg_logprob": -0.06152344809638129, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002979963319376111}, {"id": 670, "seek": 456488, "start": 4564.88, "end": 4572.64, "text": " going forward for very complex and you know ethical situations or situations where ethics", "tokens": [50364, 516, 2128, 337, 588, 3997, 293, 291, 458, 18890, 6851, 420, 6851, 689, 19769, 50752], "temperature": 0.0, "avg_logprob": -0.0571636438369751, "compression_ratio": 1.899497487437186, "no_speech_prob": 0.013633226975798607}, {"id": 671, "seek": 456488, "start": 4572.64, "end": 4579.76, "text": " are going to be a huge component so kind of where that is what they're going into and kind of where", "tokens": [50752, 366, 516, 281, 312, 257, 2603, 6542, 370, 733, 295, 689, 300, 307, 437, 436, 434, 516, 666, 293, 733, 295, 689, 51108], "temperature": 0.0, "avg_logprob": -0.0571636438369751, "compression_ratio": 1.899497487437186, "no_speech_prob": 0.013633226975798607}, {"id": 672, "seek": 456488, "start": 4579.76, "end": 4586.16, "text": " they see you know further improvements I still kind of want to know what would happen or what", "tokens": [51108, 436, 536, 291, 458, 3052, 13797, 286, 920, 733, 295, 528, 281, 458, 437, 576, 1051, 420, 437, 51428], "temperature": 0.0, "avg_logprob": -0.0571636438369751, "compression_ratio": 1.899497487437186, "no_speech_prob": 0.013633226975798607}, {"id": 673, "seek": 456488, "start": 4586.16, "end": 4593.4400000000005, "text": " of these other time metrics or what are the situations these time metrics would or alternative", "tokens": [51428, 295, 613, 661, 565, 16367, 420, 437, 366, 264, 6851, 613, 565, 16367, 576, 420, 8535, 51792], "temperature": 0.0, "avg_logprob": -0.0571636438369751, "compression_ratio": 1.899497487437186, "no_speech_prob": 0.013633226975798607}, {"id": 674, "seek": 459344, "start": 4593.44, "end": 4600.799999999999, "text": " time models would be applicable to or if this really is like the de facto just the way it needs", "tokens": [50364, 565, 5245, 576, 312, 21142, 281, 420, 498, 341, 534, 307, 411, 264, 368, 42225, 445, 264, 636, 309, 2203, 50732], "temperature": 0.0, "avg_logprob": -0.07400463986140425, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0010648664319887757}, {"id": 675, "seek": 459344, "start": 4600.799999999999, "end": 4606.799999999999, "text": " to be done totally totally fair I think that could very well be the case I would just love to hear", "tokens": [50732, 281, 312, 1096, 3879, 3879, 3143, 286, 519, 300, 727, 588, 731, 312, 264, 1389, 286, 576, 445, 959, 281, 1568, 51032], "temperature": 0.0, "avg_logprob": -0.07400463986140425, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0010648664319887757}, {"id": 676, "seek": 459344, "start": 4606.799999999999, "end": 4612.48, "text": " exactly you know if you did a hidden Markov model how would that look you know is is there a benefit", "tokens": [51032, 2293, 291, 458, 498, 291, 630, 257, 7633, 3934, 5179, 2316, 577, 576, 300, 574, 291, 458, 307, 307, 456, 257, 5121, 51316], "temperature": 0.0, "avg_logprob": -0.07400463986140425, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0010648664319887757}, {"id": 677, "seek": 459344, "start": 4612.48, "end": 4618.879999999999, "text": " of that over the selection that they have is there does it provide more or less versatility in the", "tokens": [51316, 295, 300, 670, 264, 9450, 300, 436, 362, 307, 456, 775, 309, 2893, 544, 420, 1570, 1774, 20758, 294, 264, 51636], "temperature": 0.0, "avg_logprob": -0.07400463986140425, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0010648664319887757}, {"id": 678, "seek": 461888, "start": 4619.12, "end": 4624.08, "text": " models these are things that are going to be I think very interesting going forward and then", "tokens": [50376, 5245, 613, 366, 721, 300, 366, 516, 281, 312, 286, 519, 588, 1880, 516, 2128, 293, 550, 50624], "temperature": 0.0, "avg_logprob": -0.11422868706714148, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.03307425603270531}, {"id": 679, "seek": 461888, "start": 4625.28, "end": 4631.68, "text": " you know how do we like the like you noted in the discussion how do you optimize those cost", "tokens": [50684, 291, 458, 577, 360, 321, 411, 264, 411, 291, 12964, 294, 264, 5017, 577, 360, 291, 19719, 729, 2063, 51004], "temperature": 0.0, "avg_logprob": -0.11422868706714148, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.03307425603270531}, {"id": 680, "seek": 461888, "start": 4631.68, "end": 4636.88, "text": " functions what's what's a cost you know you're dealing with clinical trials it's a human life", "tokens": [51004, 6828, 437, 311, 437, 311, 257, 2063, 291, 458, 291, 434, 6260, 365, 9115, 12450, 309, 311, 257, 1952, 993, 51264], "temperature": 0.0, "avg_logprob": -0.11422868706714148, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.03307425603270531}, {"id": 681, "seek": 461888, "start": 4636.88, "end": 4642.96, "text": " that's a cost time it's a cost there's also just computer time how much you can actually get", "tokens": [51264, 300, 311, 257, 2063, 565, 309, 311, 257, 2063, 456, 311, 611, 445, 3820, 565, 577, 709, 291, 393, 767, 483, 51568], "temperature": 0.0, "avg_logprob": -0.11422868706714148, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.03307425603270531}, {"id": 682, "seek": 464296, "start": 4643.52, "end": 4649.52, "text": " how much compute you can get and give them all the time some of these machine learning models", "tokens": [50392, 577, 709, 14722, 291, 393, 483, 293, 976, 552, 439, 264, 565, 512, 295, 613, 3479, 2539, 5245, 50692], "temperature": 0.0, "avg_logprob": -0.10310715720767066, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.022974779829382896}, {"id": 683, "seek": 464296, "start": 4649.52, "end": 4655.44, "text": " that you might want to apply this to or you know select data going into sometimes takes a long", "tokens": [50692, 300, 291, 1062, 528, 281, 3079, 341, 281, 420, 291, 458, 3048, 1412, 516, 666, 2171, 2516, 257, 938, 50988], "temperature": 0.0, "avg_logprob": -0.10310715720767066, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.022974779829382896}, {"id": 684, "seek": 464296, "start": 4655.44, "end": 4662.8, "text": " time to train how are you going to sample data that's going into those models how are you going", "tokens": [50988, 565, 281, 3847, 577, 366, 291, 516, 281, 6889, 1412, 300, 311, 516, 666, 729, 5245, 577, 366, 291, 516, 51356], "temperature": 0.0, "avg_logprob": -0.10310715720767066, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.022974779829382896}, {"id": 685, "seek": 464296, "start": 4662.8, "end": 4670.96, "text": " to continually feed those models appropriate data going forward so this is a huge broad category of", "tokens": [51356, 281, 22277, 3154, 729, 5245, 6854, 1412, 516, 2128, 370, 341, 307, 257, 2603, 4152, 7719, 295, 51764], "temperature": 0.0, "avg_logprob": -0.10310715720767066, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.022974779829382896}, {"id": 686, "seek": 467096, "start": 4671.84, "end": 4679.2, "text": " um directions that you can go clinical trials was a very wonderful example of a complex situation", "tokens": [50408, 1105, 11095, 300, 291, 393, 352, 9115, 12450, 390, 257, 588, 3715, 1365, 295, 257, 3997, 2590, 50776], "temperature": 0.0, "avg_logprob": -0.08857998961494082, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0028892194386571646}, {"id": 687, "seek": 467096, "start": 4679.2, "end": 4686.24, "text": " that is you know right there applicable to human life um but then you also have just data science", "tokens": [50776, 300, 307, 291, 458, 558, 456, 21142, 281, 1952, 993, 1105, 457, 550, 291, 611, 362, 445, 1412, 3497, 51128], "temperature": 0.0, "avg_logprob": -0.08857998961494082, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0028892194386571646}, {"id": 688, "seek": 467096, "start": 4686.24, "end": 4691.84, "text": " in general like how are you going to utilize this to you know going more broad how are you", "tokens": [51128, 294, 2674, 411, 577, 366, 291, 516, 281, 16117, 341, 281, 291, 458, 516, 544, 4152, 577, 366, 291, 51408], "temperature": 0.0, "avg_logprob": -0.08857998961494082, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0028892194386571646}, {"id": 689, "seek": 467096, "start": 4691.84, "end": 4697.2, "text": " going to utilize this just going forward in any data sense data is growing it'll continue to grow", "tokens": [51408, 516, 281, 16117, 341, 445, 516, 2128, 294, 604, 1412, 2020, 1412, 307, 4194, 309, 603, 2354, 281, 1852, 51676], "temperature": 0.0, "avg_logprob": -0.08857998961494082, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0028892194386571646}, {"id": 690, "seek": 469720, "start": 4697.2, "end": 4702.5599999999995, "text": " it will never really stop growing so it's going to be more and more important going forward to", "tokens": [50364, 309, 486, 1128, 534, 1590, 4194, 370, 309, 311, 516, 281, 312, 544, 293, 544, 1021, 516, 2128, 281, 50632], "temperature": 0.0, "avg_logprob": -0.08494572272667518, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.007813566364347935}, {"id": 691, "seek": 469720, "start": 4702.5599999999995, "end": 4711.92, "text": " have these methods more broadly used and random's nice random's really really good but this holds", "tokens": [50632, 362, 613, 7150, 544, 19511, 1143, 293, 4974, 311, 1481, 4974, 311, 534, 534, 665, 457, 341, 9190, 51100], "temperature": 0.0, "avg_logprob": -0.08494572272667518, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.007813566364347935}, {"id": 692, "seek": 469720, "start": 4711.92, "end": 4717.92, "text": " a lot of promise to being I would love to just hear their thoughts on where that's going where", "tokens": [51100, 257, 688, 295, 6228, 281, 885, 286, 576, 959, 281, 445, 1568, 641, 4598, 322, 689, 300, 311, 516, 689, 51400], "temperature": 0.0, "avg_logprob": -0.08494572272667518, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.007813566364347935}, {"id": 693, "seek": 471792, "start": 4717.92, "end": 4727.52, "text": " they see that yeah a lot of a lot of interesting directions a few things that made me think of", "tokens": [50364, 436, 536, 300, 1338, 257, 688, 295, 257, 688, 295, 1880, 11095, 257, 1326, 721, 300, 1027, 385, 519, 295, 50844], "temperature": 0.0, "avg_logprob": -0.06515690398542848, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.06949537247419357}, {"id": 694, "seek": 471792, "start": 4727.52, "end": 4733.04, "text": " one was about search and about relational search concepts page rank and everything", "tokens": [50844, 472, 390, 466, 3164, 293, 466, 38444, 3164, 10392, 3028, 6181, 293, 1203, 51120], "temperature": 0.0, "avg_logprob": -0.06515690398542848, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.06949537247419357}, {"id": 695, "seek": 471792, "start": 4733.04, "end": 4740.4800000000005, "text": " syntactic semantic new kinds of search algorithms and personalization for search and learning and", "tokens": [51120, 23980, 19892, 47982, 777, 3685, 295, 3164, 14642, 293, 2973, 2144, 337, 3164, 293, 2539, 293, 51492], "temperature": 0.0, "avg_logprob": -0.06515690398542848, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.06949537247419357}, {"id": 696, "seek": 471792, "start": 4740.4800000000005, "end": 4746.32, "text": " updating and to what extent like explicit cognitive modeling would change the way that", "tokens": [51492, 25113, 293, 281, 437, 8396, 411, 13691, 15605, 15983, 576, 1319, 264, 636, 300, 51784], "temperature": 0.0, "avg_logprob": -0.06515690398542848, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.06949537247419357}, {"id": 697, "seek": 474632, "start": 4746.32, "end": 4753.2, "text": " different recommendation algorithms or different kinds of computer systems would work I'll read", "tokens": [50364, 819, 11879, 14642, 420, 819, 3685, 295, 3820, 3652, 576, 589, 286, 603, 1401, 50708], "temperature": 0.0, "avg_logprob": -0.08805102030436197, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.001987666357308626}, {"id": 698, "seek": 474632, "start": 4753.2, "end": 4759.12, "text": " a question um and then any any other questions otherwise this is our our last slide so thank", "tokens": [50708, 257, 1168, 1105, 293, 550, 604, 604, 661, 1651, 5911, 341, 307, 527, 527, 1036, 4137, 370, 1309, 51004], "temperature": 0.0, "avg_logprob": -0.08805102030436197, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.001987666357308626}, {"id": 699, "seek": 474632, "start": 4759.12, "end": 4767.44, "text": " you Christopher um okay glia maximalist wrote interesting point about biased and unbiased", "tokens": [51004, 291, 20649, 1105, 1392, 1563, 654, 49336, 468, 4114, 1880, 935, 466, 28035, 293, 517, 5614, 1937, 51420], "temperature": 0.0, "avg_logprob": -0.08805102030436197, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.001987666357308626}, {"id": 700, "seek": 474632, "start": 4767.44, "end": 4772.4, "text": " sampling schemes perhaps this points out the fact that unbiased approaches are the wrong", "tokens": [51420, 21179, 26954, 4317, 341, 2793, 484, 264, 1186, 300, 517, 5614, 1937, 11587, 366, 264, 2085, 51668], "temperature": 0.0, "avg_logprob": -0.08805102030436197, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.001987666357308626}, {"id": 701, "seek": 477240, "start": 4772.4, "end": 4779.759999999999, "text": " thing to strive for in research study design what do you think about that unbiased research", "tokens": [50364, 551, 281, 23829, 337, 294, 2132, 2979, 1715, 437, 360, 291, 519, 466, 300, 517, 5614, 1937, 2132, 50732], "temperature": 0.0, "avg_logprob": -0.07350469153860341, "compression_ratio": 1.9123711340206186, "no_speech_prob": 0.01322079822421074}, {"id": 702, "seek": 477240, "start": 4779.759999999999, "end": 4785.92, "text": " in study design um I think there's a time and place for unbiased and I think there's a time", "tokens": [50732, 294, 2979, 1715, 1105, 286, 519, 456, 311, 257, 565, 293, 1081, 337, 517, 5614, 1937, 293, 286, 519, 456, 311, 257, 565, 51040], "temperature": 0.0, "avg_logprob": -0.07350469153860341, "compression_ratio": 1.9123711340206186, "no_speech_prob": 0.01322079822421074}, {"id": 703, "seek": 477240, "start": 4785.92, "end": 4792.4, "text": " and place for bias I think that but you when you accept a bias into your model or refute", "tokens": [51040, 293, 1081, 337, 12577, 286, 519, 300, 457, 291, 562, 291, 3241, 257, 12577, 666, 428, 2316, 420, 1895, 1169, 51364], "temperature": 0.0, "avg_logprob": -0.07350469153860341, "compression_ratio": 1.9123711340206186, "no_speech_prob": 0.01322079822421074}, {"id": 704, "seek": 477240, "start": 4792.4, "end": 4798.799999999999, "text": " take bias out of your model you need to understand why you're doing certain bias you don't want to", "tokens": [51364, 747, 12577, 484, 295, 428, 2316, 291, 643, 281, 1223, 983, 291, 434, 884, 1629, 12577, 291, 500, 380, 528, 281, 51684], "temperature": 0.0, "avg_logprob": -0.07350469153860341, "compression_ratio": 1.9123711340206186, "no_speech_prob": 0.01322079822421074}, {"id": 705, "seek": 479880, "start": 4798.8, "end": 4804.56, "text": " have you know researcher bias is something that's a huge bias that you want to not have for example", "tokens": [50364, 362, 291, 458, 21751, 12577, 307, 746, 300, 311, 257, 2603, 12577, 300, 291, 528, 281, 406, 362, 337, 1365, 50652], "temperature": 0.0, "avg_logprob": -0.09345683484974474, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.003075110260397196}, {"id": 706, "seek": 479880, "start": 4806.08, "end": 4811.2, "text": " but in certain cases um like in this paper where you actually do want to have a bias", "tokens": [50728, 457, 294, 1629, 3331, 1105, 411, 294, 341, 3035, 689, 291, 767, 360, 528, 281, 362, 257, 12577, 50984], "temperature": 0.0, "avg_logprob": -0.09345683484974474, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.003075110260397196}, {"id": 707, "seek": 479880, "start": 4811.92, "end": 4816.72, "text": " you want to make an intelligent decision know why you're making that decision call it out and then", "tokens": [51020, 291, 528, 281, 652, 364, 13232, 3537, 458, 983, 291, 434, 1455, 300, 3537, 818, 309, 484, 293, 550, 51260], "temperature": 0.0, "avg_logprob": -0.09345683484974474, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.003075110260397196}, {"id": 708, "seek": 479880, "start": 4816.72, "end": 4822.8, "text": " build it into your model that would be my problem yeah that's interesting like there are certain", "tokens": [51260, 1322, 309, 666, 428, 2316, 300, 576, 312, 452, 1154, 1338, 300, 311, 1880, 411, 456, 366, 1629, 51564], "temperature": 0.0, "avg_logprob": -0.09345683484974474, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.003075110260397196}, {"id": 709, "seek": 479880, "start": 4822.8, "end": 4827.92, "text": " statistical distributions the bias or the constraints on which to find the research study", "tokens": [51564, 22820, 37870, 264, 12577, 420, 264, 18491, 322, 597, 281, 915, 264, 2132, 2979, 51820], "temperature": 0.0, "avg_logprob": -0.09345683484974474, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.003075110260397196}, {"id": 710, "seek": 482792, "start": 4828.0, "end": 4834.88, "text": " whereas other ones that can have an explicitly strictly negative like data loss or something", "tokens": [50368, 9735, 661, 2306, 300, 393, 362, 364, 20803, 20792, 3671, 411, 1412, 4470, 420, 746, 50712], "temperature": 0.0, "avg_logprob": -0.07229025223675896, "compression_ratio": 1.7475728155339805, "no_speech_prob": 0.0006878022686578333}, {"id": 711, "seek": 482792, "start": 4834.88, "end": 4840.96, "text": " but then the trade-offs of how that distribution actually interacts with others can enter into", "tokens": [50712, 457, 550, 264, 4923, 12, 19231, 295, 577, 300, 7316, 767, 43582, 365, 2357, 393, 3242, 666, 51016], "temperature": 0.0, "avg_logprob": -0.07229025223675896, "compression_ratio": 1.7475728155339805, "no_speech_prob": 0.0006878022686578333}, {"id": 712, "seek": 482792, "start": 4840.96, "end": 4847.2, "text": " this more complex experimental calculus that relates to like well all these different", "tokens": [51016, 341, 544, 3997, 17069, 33400, 300, 16155, 281, 411, 731, 439, 613, 819, 51328], "temperature": 0.0, "avg_logprob": -0.07229025223675896, "compression_ratio": 1.7475728155339805, "no_speech_prob": 0.0006878022686578333}, {"id": 713, "seek": 482792, "start": 4847.2, "end": 4855.2, "text": " experimental factors and so the optimal experiment for for different labs or different", "tokens": [51328, 17069, 6771, 293, 370, 264, 16252, 5120, 337, 337, 819, 20339, 420, 819, 51728], "temperature": 0.0, "avg_logprob": -0.07229025223675896, "compression_ratio": 1.7475728155339805, "no_speech_prob": 0.0006878022686578333}, {"id": 714, "seek": 485520, "start": 4855.28, "end": 4860.08, "text": " moments for the lab could look extremely different and that's going to be the case", "tokens": [50368, 6065, 337, 264, 2715, 727, 574, 4664, 819, 293, 300, 311, 516, 281, 312, 264, 1389, 50608], "temperature": 0.0, "avg_logprob": -0.10153742870652532, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0006461331504397094}, {"id": 715, "seek": 485520, "start": 4860.72, "end": 4863.84, "text": " there's going to be just first behavior out of the way but then the question is how is that actually", "tokens": [50640, 456, 311, 516, 281, 312, 445, 700, 5223, 484, 295, 264, 636, 457, 550, 264, 1168, 307, 577, 307, 300, 767, 50796], "temperature": 0.0, "avg_logprob": -0.10153742870652532, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0006461331504397094}, {"id": 716, "seek": 485520, "start": 4863.84, "end": 4872.88, "text": " driven in a way that is doing better than drawing from distributions however even that does interestingly", "tokens": [50796, 9555, 294, 257, 636, 300, 307, 884, 1101, 813, 6316, 490, 37870, 4461, 754, 300, 775, 25873, 51248], "temperature": 0.0, "avg_logprob": -0.10153742870652532, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0006461331504397094}, {"id": 717, "seek": 485520, "start": 4872.88, "end": 4882.4, "text": " well for the right variables and you can mind bias all over the place I bias in data design", "tokens": [51248, 731, 337, 264, 558, 9102, 293, 291, 393, 1575, 12577, 439, 670, 264, 1081, 286, 12577, 294, 1412, 1715, 51724], "temperature": 0.0, "avg_logprob": -0.10153742870652532, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0006461331504397094}, {"id": 718, "seek": 488240, "start": 4882.4, "end": 4888.16, "text": " in experiments is can be very useful so it'll just be interesting I think it'll be case by case", "tokens": [50364, 294, 12050, 307, 393, 312, 588, 4420, 370, 309, 603, 445, 312, 1880, 286, 519, 309, 603, 312, 1389, 538, 1389, 50652], "temperature": 0.0, "avg_logprob": -0.11114969009008163, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002550000324845314}, {"id": 719, "seek": 488240, "start": 4888.16, "end": 4889.04, "text": " it's the way I see it", "tokens": [50652, 309, 311, 264, 636, 286, 536, 309, 50696], "temperature": 0.0, "avg_logprob": -0.11114969009008163, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002550000324845314}, {"id": 720, "seek": 488240, "start": 4895.92, "end": 4905.12, "text": " okay well do you have any last comments I think that the main thing here is I'm just very excited", "tokens": [51040, 1392, 731, 360, 291, 362, 604, 1036, 3053, 286, 519, 300, 264, 2135, 551, 510, 307, 286, 478, 445, 588, 2919, 51500], "temperature": 0.0, "avg_logprob": -0.11114969009008163, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002550000324845314}, {"id": 721, "seek": 488240, "start": 4905.12, "end": 4911.04, "text": " to see this paper come out I would love to see how this is going to evolve over time", "tokens": [51500, 281, 536, 341, 3035, 808, 484, 286, 576, 959, 281, 536, 577, 341, 307, 516, 281, 16693, 670, 565, 51796], "temperature": 0.0, "avg_logprob": -0.11114969009008163, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002550000324845314}, {"id": 722, "seek": 491104, "start": 4911.04, "end": 4917.92, "text": " see if this can be applied to different technologies different areas I'm really excited", "tokens": [50364, 536, 498, 341, 393, 312, 6456, 281, 819, 7943, 819, 3179, 286, 478, 534, 2919, 50708], "temperature": 0.0, "avg_logprob": -0.08791611271519814, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.001673838240094483}, {"id": 723, "seek": 491104, "start": 4917.92, "end": 4924.32, "text": " just to see where this is going because I think this is just right on the cusp of what's needed", "tokens": [50708, 445, 281, 536, 689, 341, 307, 516, 570, 286, 519, 341, 307, 445, 558, 322, 264, 269, 22490, 295, 437, 311, 2978, 51028], "temperature": 0.0, "avg_logprob": -0.08791611271519814, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.001673838240094483}, {"id": 724, "seek": 491104, "start": 4926.16, "end": 4934.72, "text": " awesome thank you okay we will look forward to it thank you see you all right thanks guys", "tokens": [51120, 3476, 1309, 291, 1392, 321, 486, 574, 2128, 281, 309, 1309, 291, 536, 291, 439, 558, 3231, 1074, 51548], "temperature": 0.0, "avg_logprob": -0.08791611271519814, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.001673838240094483}, {"id": 725, "seek": 494104, "start": 4941.04, "end": 4942.32, "text": " you", "tokens": [50400, 291, 50428], "temperature": 0.0, "avg_logprob": -0.8700545430183411, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.5644159317016602}], "language": "en"}