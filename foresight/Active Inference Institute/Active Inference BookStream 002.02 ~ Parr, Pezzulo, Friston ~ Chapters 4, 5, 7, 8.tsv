start	end	text
0	16400	Hello, it's July 28th, 2023, and we're in Active Inference Textbook Group slash Bookstream
16400	25800	2.02. Thanks, Ali, for joining. So what we're going to do today is give a short overview
25800	35480	of the chapters from the PAR at all 2022 book. We're going to do chapters four, five, seven, and eight.
37560	43800	And we're just going to pause between them because then we'll clip them into the shorter videos,
43800	49480	append that to the playlist, just so there's a first video overview of each of the chapters.
49480	59800	And this is the second in that work. Alright, so we'll do chapter four. We'll just wait a few
59800	70520	seconds and then start chapter four. Okay, chapter four is called the generative models of active
70520	74680	inference. And it begins with a quotation, everything should be made as simple as possible,
74680	82760	but not simpler by Albert Einstein. Ali, what is your overview thought or warning for chapter four?
85320	94280	Okay, so after the preliminary materials in chapters two and three, which was basically
95160	103000	largely based on providing some conceptual framework for developing the further
103960	110920	theory, chapter four delves into much more detail in terms of mathematical formulation.
110920	119480	And it unpacks a lot more the way that the central equations of active inference
120280	127480	is derived and how to construct the important elements of active inference models.
128440	137320	So say matrices A, B, C, and D and also how to put together generative models in
138120	144040	different situations. So it basically lays out the foundation for
147000	155080	constructing active inference models, both for discrete time situations and continuous time
155080	165160	ones, which will be used later in chapters seven and eight. But this is probably one of the most
166360	174280	challenging and at least mathematically dense chapters in the book. So I would personally
174280	183320	suggest reading through this chapter really slowly. And even if we don't get to understand
183320	191000	every single detail of the chapter, obviously we can return per required as we go through
191720	200040	the textbook. Thank you, Ali. Yes. So let's look through the sections. Just to add on though,
200040	208360	chapter four is one of the larger and more equation dense chapters, because it is the
208360	215560	common kernel or basis that's then going to get applied in chapter five in the neurobiological
215560	221400	case. There's a recipe for making chapter four in chapter six. That's the recipe for active
221400	227560	inference modeling. Chapter seven and eight are about the discrete and the continuous time variant
227560	235720	or subtype or motif of these kinds of things called generative models. So this is the real
235720	243960	common root. And we'll just look at what the sections are. This chapter complements the
243960	248520	preceding chapter's conceptual treatment of active inference with a more formal treatment.
249720	256280	Section 4.2 from Bayesian inference to free energy. What would you say about this section, Ali?
256280	267640	Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference.
268360	273480	I mean, all the way back to Helmholtz theory about
275720	281400	unconscious inference or something to that effect. I can't remember the exact term.
282040	292360	But here, I mean, it provides in a bit more detail how we can derive free energy principle
293320	299320	formalism using the established Bayesian inference formulation.
301160	308760	And particularly, one of the key movements or at least one of the key decisions
309400	316440	in through their derivation of free energy principle formulation is using Jane's inequality
316440	330520	principle to derive an upper bound instead of just using the exact values to compute or to
330520	338760	achieve required parameters. So that's basically, in my opinion, the key premise of
339880	351640	section 4.2. And to see in a bit more detail how we can achieve those upper bounds using
351640	360360	Jane's inequality directly by using, I mean, manipulations of Bayesian inference,
361800	363480	Bayesian statistical formalism.
365640	371240	Thanks. I'll just add one point from this section. Broadly, these are the problems of
371240	375880	inferring states of the world perception and inferring a course of action planning.
375880	381480	So this is again, referring to the perception and action. And everything that happens in
381480	386760	between is the internal or the cognitive part of the inference. But this is like the blanket state
387320	395160	cybernetic input output. And then let's look at the first equation or how much equations overall
395800	398520	or what equations do you think we should highlight?
399400	415000	Okay. So maybe we can, I mean, just as a general comment about these different equations, well,
415000	423800	each of these equations provide a distinct step toward deriving the ultimate whole picture.
423800	433560	So even if we don't quite understand how we can derive from each step to the other one,
435320	445080	it's good to know that it's only required to understand how we get to that ultimate whole
445080	453560	picture. But ultimately, what we would need in order to develop active inference models
454360	459720	is the ultimate equation or ultimate whole picture. So this is just
462120	471960	a way to elucidate the steps toward developing that whole picture. But again, it's not an
471960	480840	essential requirement to understand the materials of the rest of the book. But if we go from,
481800	490440	I mean, equations 4.1 toward the 4.4, or in other words, a variation of free energy,
490440	502360	well, equation 4.1 is just a basic definition of some properties of probabilities in terms of
502360	510120	conditional probability and so on. So equation 4.2 provides the central
511000	520840	Jane's inequality principle and how it relates to, I mean, conditional probabilities and of
520840	530120	course, joint probabilities. And then by using those two properties or those two equations,
530120	537160	we ultimately get to 4.4, which is the definition of variational free energy parameter,
538440	545640	which is the parameter of interest that needs to be optimized in order to inference to happen,
547320	551000	or at least perceptual inference to happen in active inference models.
551080	561080	Thanks. The only thing I'll add is f is the letter used for variational free energy. Think of it
561080	565640	like a computer program and the arguments that it takes in or the variables that it takes in
566360	573880	are q, which is the distribution that's under the statisticians control and y, which are the data,
573880	580840	which are outside of the statisticians control. And do you want to describe more about anything
581880	583560	in this equation or carry on?
586040	595560	Just one thing that can probably be helpful is to somehow compare these steps with
596760	604600	the initial picture we had from chapter 2, because variational free energy was first introduced
604600	613560	in chapter 2. So it can be helpful to go back and forth between chapters 2 and 4 and try to
613560	627720	connect the dots between the related points there. Section 4.3 generative models. All right,
627720	633240	I'll read the first sentence, then you can give some thoughts. To calculate the free energy,
634040	640440	we need three things. Data, a family of variational distributions, and a generative model comprising
640440	650040	a prior and a likelihood. In this section, we outline two very general sorts of generative model
650040	653640	used for active inference and the form the free energy takes in relation to each.
653960	670040	Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time
670040	677160	situations. So clearly, we would need two different types of generative models for each situation.
678120	685480	And obviously, the generative models or the way to construct generative models for
685480	693320	discrete time situations would vary quite a bit from the one for continuous time situations.
695160	703000	But the general principle underlying those generative models are basically the same,
703640	712680	which is, I mean, to somehow construct a model of the environment, I mean, either be it
714760	723720	for the situation that is sequential in time or for the situations that need to be somehow,
725800	730520	each moment of the situation needs to be accommodated in terms of a continuous time
730520	742520	situation. So figure 4.2 provides some examples of both. So for, yes, let me see.
744440	755720	Yes, so we have some examples of different kinds of generative models and case studies,
756440	765720	if you like, and it provides various ways to show how the dependencies between variables
765720	774280	can be modeled using these kinds of graphical probabilistic models. So one common way to
774280	782440	represent generative models is to use these kinds of graphical probabilistic models in active inference
782440	792200	literature, which is, at least in this case, the circles would represent the random variables,
792200	798520	and the squares would represent the distributions, which would describe
799960	809640	the dependencies between those random variables. So we can see the clear relationships between
809640	817560	those parameters here, which is basically what this whole graph, what constitutes the
817560	825960	generative model that needs to be used for different situations. And then in figure 4.3,
827560	833400	we can compare the two different types of generative models based on
833880	842120	whether it's discrete time or continuous time situations. So the upper picture is a generative
842120	851560	model for the discrete time situation, and the lower picture is the parallel continuous time
851560	861880	version of it. And as we can see, the general topology of these models are the same. The only
861880	872760	things that differ is the use of parameters for policies or, I mean, discrete time policies or
872760	882200	the continuous time ones. And we can obviously compare the different elements for both priors
882200	890600	states and external states, internal states, and so on by comparing these two models here.
892600	897480	Yeah, we often return to figure 4.3. It's kind of the Rosetta Stone
898520	904280	of generative modeling for the context of this book, because it's then going to develop out into
904280	910440	chapter seven and eight. And it represents a really fundamental decision made in modeling.
911720	918040	And in the later chapters, it's also shown how it can be made into a hierarchical model
918040	923240	that combines aspects of both. But within each level of modeling, still, these are the kinds
923240	928440	of decisions that modelers are presented with when it comes to statistical modeling overall.
930520	938200	So section 4.4 goes into essentially the top half of figure 4.3, discrete time.
939800	941480	What would you say about discrete time?
942440	950920	Okay, so the discrete time situation is obviously the archetype discrete time situation,
950920	959320	which is the POMDP models. So at this point, I would very much like to recommend
960280	974040	following the material from set by step paper, because in that paper, the way to construct
974040	982360	POMDP models is described in a bit more detail. So if anyone feels like they should learn a bit
982360	991560	more about the gaps in the details, I would very much like to recommend that particular paper.
992360	1005160	So yeah, I don't know how much detail we should go into, because it's, I mean, although it's not
1005160	1013640	maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can
1014360	1022280	construct these models using the concepts we've learned in previous chapters. So
1023880	1033880	ultimately, we reach equations for point 13 and four point 14, which are basically the culmination
1033880	1041720	of POMDP formulation using the vector notations and gradients and so on. So
1045640	1050840	that's great. Then we go to continuous time. Yeah, great.
1053000	1060360	A few things intervene in the continuous time chapter was that we'll just mention here, because
1060360	1066200	they're kind of boxed or partitioned from the continuous time part, but they're following
1066200	1071160	pages versus Markov blankets. We won't go into it here, but kind of footnote that or
1071160	1077880	look at some other places where we talk about it outside of this chapter overview. Figure 4.4,
1077880	1087080	Bayesian message passing. Again, a big topic. Let's kind of just go past it now. Back to the
1087160	1095960	regularly scheduled continuous time generative model discussion. And then another box to the
1095960	1103800	generalized coordinates of motion. So taking position plus derivatives of position. And that has
1103800	1113400	some beneficial properties that are described and unpacked also elsewhere. Do you want to say
1113400	1127480	anything about 4.5.2? Well, the only thing that comes to mind is although, as I said before,
1128760	1137720	all the formulations here may look more, I mean, a bit too dense to understand at the first pass,
1137720	1152840	but some of the key maybe components here could be obviously the material from box 4.2 and 4.3.
1154600	1165080	I think are quite essential to understand the underlying principle behind deriving the continuous
1165080	1173160	time situation because without Laplace approximation, what we would have in terms of free energy
1173160	1184280	minimization would look very much like the Gibbs free energy. So I mean, the key distinction between
1186440	1192120	the free energy principle as described in active inference literature, as opposed to Gibbs free
1192120	1202200	energy, is the Laplace approximation. So this is what enables us to go from Gibbs free energy to,
1203560	1210440	I mean, the variation of free energy. So yeah, that's, I mean, quite essential to
1211560	1219400	make this, to be familiar with this essential approximation. And obviously, the concept of
1219400	1225400	generalized coordinates of motion will come time and time again throughout the whole book,
1225400	1236360	particularly in chapters eight and nine. So yeah, those two concepts, I believe, needs a bit more
1236920	1250920	attention. So yeah, sounds good. Box 4.3 Laplace approximation equations, another message passing
1251560	1259480	representation, and a summary. The key message to take away is that approximate Bayesian inference
1259480	1265640	may be framed as minimizing a quantity known as variational free energy. This depends on a
1265640	1271240	generative model that expresses our belief about how data are generated. Anything else you want to add?
1277000	1283800	Nothing comes to mind at the moment, because, as I said, this is, I mean, we're still
1284840	1292120	in the stage that we want to develop our essential tools to be used in the rest of the books. So
1293080	1300040	here, up until now, I believe, by the end of chapter four, we have acquired all the essential
1300040	1309800	necessary mathematical tools. And the next chapter, chapter five, is kind of acts like an interlude.
1311480	1320920	And I don't think it's the direct, I mean, continuation of chapters one through four. So
1321880	1331400	I believe the first section or the first part of the book, conceptually and mathematically ends here.
1331400	1339000	So yeah, that's it. Yes, it's a little bit like the pragmatic modeling part gets foreshadowed
1339560	1346840	or explored in five, now that we're all built up with four. All right, that's the end of the
1346840	1367560	overview for four. Okay, chapter five is called message passing and neurobiology.
1368520	1371560	What is your overview thought on chapter five?
1374840	1382440	Okay, I mean, it's a kind of, I don't know, I had mixed feelings about this chapter, because
1383000	1390840	on one hand, you see, as far as I understand active inference, although it originated as
1391560	1399400	quote unquote, a unified theory of the brain, I don't think it's a neurobiological theory per se.
1400600	1409080	Of course, there can be some correlations between neurobiological components or concepts
1409720	1420440	with active inference, I mean, concepts. But I mean, it's not an essential premise
1420520	1429160	of active inference theory to provide a theory, to provide a comprehensive theory about how
1429160	1441000	the neurobiology of human brain or other organisms brain behave at a detailed and neuroanatomical
1441560	1452360	level. But then again, it's nice to have these kinds of empirical correlations between the
1452360	1462600	findings of neurobiology and the active inference theory. But I don't think it's one of active inference
1463320	1467560	central assertions, at least to my understanding.
1469720	1476760	Well said, very interesting framing. Well, chapter five definitely takes a very specific
1477560	1485400	system of interest approach by highlighting one of the most studied areas, also one of the most
1485400	1496520	relevant areas, which is mammalian neuroscience. And the chapter is going to introduce a few different
1499240	1506360	motifs in the nervous system, and essentially build up towards figure 5.5,
1507080	1514840	which is at the end of the chapter, and 5.5 wires together three specific neural systems
1515400	1522520	that the chapter is going to focus on work in that area from. So Ali said it very well.
1523960	1530840	Active inference was built up to in chapter four. Here is another level or type of science
1530840	1536840	with assertions or with representations or mappings to any specific system. But this is the kind of
1537640	1545560	modeling that has been built up and done by Friston, Par, Pizzolo, and others over the decades,
1545560	1553720	with a focus coming from a human neuroimaging laboratory setting, a lot of focus and study
1554360	1558920	and attention and funding and everything on the mammalian nervous system.
1559160	1570840	But claims about the nervous system are not the basis of what active inference claims
1571400	1576840	or how it's derived. But this is like an example case study in neurobiology,
1577560	1583000	connecting back to some of the formalisms that we've just seen introduced in chapter four.
1583960	1592440	Yeah, and to add a minor point to which I just said, I think it's important to draw attention to
1593080	1598840	the last sentence of the last paragraph of the first page. It is important to draw a distinction
1598840	1604680	between a principle, i.e. the minimization of free energy and a process theory about how this
1604680	1612200	principle may be implemented in a certain kind of system. So I think this sentence here
1613000	1620120	frames this chapter in relation to all the other technical chapters of this book. So if
1621000	1627480	every other chapter is about developing, or at least up to now, was about developing
1629160	1636280	the principled formalism of active inference, now chapter five provides a kind of preliminary
1636360	1643400	sketch for the process theory of active inference, which is obviously far from an extensive
1643400	1649960	theory, it's just a single chapter. But then again, it can provide some important
1651560	1658360	signposts for anyone who wants to further investigate this area.
1659320	1668200	Awesome. Free energy principle, Bayesian mechanics, all things in that area are on this principle,
1669000	1675000	not responsive to empirical data. And then the process theory is about how the principle is
1675000	1682440	implemented. So the specific generative models that are made, and how well they map, or how well
1682440	1687960	they do in a portfolio of models that can have very different goals and assumptions and all of
1687960	1694680	this. But the process theory implementation lets us develop hypotheses that are answerable to empirical
1694680	1705080	data, like what is the kind of information or relationship between photons hitting the retina
1705720	1713560	and changes in activity in neural systems. And that's an informational question or can be
1713560	1719000	abstracted in a way to an informational question that it turns out does have empirical support
1719640	1724920	and results in unique explanations and predictions. That doesn't mean that it always
1724920	1730120	results in unique explanations and predictions, but a lot of citations are provided here.
1731640	1733400	That's what we can explore in chapter five.
1733400	1743720	The last paragraph of the first section describes that they're going to look at the
1744840	1751080	three different neural systems. Okay, section 5.2, microcircuits and messages.
1751800	1764600	What do you think, Oli? All right. So I mean, this chapter begins from how
1765320	1774760	message passing happens in neurobiological terms and compare it to the way active inference
1774760	1784040	frames this message passing mechanism. And specifically, if we look at figure 5.1 and
1784040	1793800	compare this figure to the ones we've seen before in chapters one through four, I think it was in
1793800	1801720	chapter four, we can see some clear parallels between how this kind of cortical message
1801720	1810360	passing happens in the brain versus how it is framed in active inference literature. And as we
1810360	1820440	can see, it's clearly inspired by the neurobiology of the brain. But then it's important to keep
1820440	1830840	in mind that it's not a direct one-to-one mapping between these two models. This is just
1832120	1842200	a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind
1842920	1852200	to somehow be a bit more confident about the viability of the theory we want to use for
1852200	1860360	message passing and active inference, which is to say it's not some haphazard theory that's just
1860440	1867080	been developed for practical reasons. It has some basis in neurobiology, although
1868280	1876040	it's not necessarily fully congruent with every detail of neurobiology.
1879400	1885640	Great. The specific example is going to involve this one region of mammalian cortex tissue
1886360	1890360	that has these six layers. And there's a ton of neurobiology.
1892680	1898040	The big takeaway for figure 5.1 is that it's possible to graphically lay out
1898680	1905800	nodes and variables and find some empirical correspondences. Again, some unique explanations
1905800	1913320	and predictions in certain cases. And that's one kind of modeling where it's really trying to
1913320	1920840	understand and improve the ability to do correlation and intervention and counterfactual
1920840	1928120	causal type analysis with the real system of interest. Or in a more pedagogical setting or
1928120	1933320	a research setting or an industrial setting, you might sweep across large families of structures
1933320	1942120	of models and there's no need to be grounded to any biological structure at all. So this is just
1942120	1949640	describing the specific neuroanatomical research that really arose out of the imaging work at UCL
1949640	1955880	and the SPM package. So that's where a lot of this comes from. 5.2? Yeah, good.
1956600	1964600	And sorry, just as a side note, I think watching one of Thomas Parr's lectures on neurobiology
1964600	1970440	of active inference, which is available on YouTube, would really help to understand the materials of
1970440	1983240	this chapter better. So I highly recommend watching that one. Thanks. Figure 5.2 gives a
1983240	1992520	re-rendering of a kind of classical view of a hierarchical predictive coding system works.
1993480	2004280	So here, abstracting a layer from the tissue six layer to just two layers here, computational layers
2004280	2012680	now, and then showing how there's hierarchical communication within a layer, but also others,
2012680	2020040	they're signaling within a layer and there's a hierarchy in Bayesian modeling with variables
2020120	2025480	that are higher order predictions about other variables. And that's the basis of the predictive
2025480	2035320	coding architecture. So 5.2 looks at some ways that the something that resonates with the cerebral
2035320	2043000	cortical architecture enables what might computationally look like or have some really strong and
2043720	2052280	explanatory values in actually relating to computationally a hierarchical Bayesian model,
2052280	2059080	which could do various general tasks. All right, 5.3 is motor commands,
2059080	2067720	leaving the prefrontal cortex going down to the butterfly looking cross section here. What is 5.3?
2068200	2078920	Okay, so 5.3 moves to the other half of active inference framework, which is,
2078920	2089960	I mean, how it can model the decision making and ultimately the movement of the agent in order to
2089960	2097320	minimize the expected free energy as opposed to variational free energy that we saw in perceptual
2098040	2108920	half of active inference. So it again provides a kind of correlation or analogy between the
2108920	2116680	structural neural anatomy, particularly related to, I mean, the motor commands and
2119080	2126520	how it can relate to active inference, particularly the continuous time active inference. So
2127720	2138120	we can see that for, I mean, for the external event or, I'm sorry, for the external state,
2138680	2147560	we can take, for example, the proprioceptive afferent, and then this proprioceptive afferent
2147560	2153480	acts as a kind of Y for the continuous time active inference, which needs to be,
2153720	2163640	I mean, processed in a way to optimize the expected free energy and how it relates to
2164200	2175640	both attention and precision. We'll see a bit more detail about those terms and the relation
2175720	2186200	between them in chapter eight, but I think here section 5.3 provides a good summary about the
2186200	2192840	general paths through the motor command systems of neurobiology.
2194280	2201960	Great. I'd say while the previous case study focused on how the connectivity within and between
2202840	2209560	the cortical columns could have a computational relationship with a Bayesian
2209560	2216120	hierarchical predictive coding architecture, the argument of the second case study is that a
2216120	2225080	continuous input, continuous output, kind of set point seeking reflexive motor behavior
2226040	2233320	with a moving set point with a descending moving set point enabling motion by changing ultimately
2233320	2239640	the set point and enabling a variation in the strategies to reach that set point through
2239640	2246360	different mechanisms. This is also describable in a compatible way.
2247080	2253240	That's a shorter section. Now, section 5.4, subcortical structures.
2254840	2256920	What would you say about this section?
2258920	2274120	Okay. So, subcortical structures are very important in the decision making and, I mean,
2275080	2286280	of the agents. So, obviously, here we need another kind of analogy between the way
2287400	2295960	that these plannings and decision makings happen neuroanatomically with the way that
2296600	2303080	that it's framed in active inference. But again, we can see it's clearly based on,
2304840	2310840	I mean, at least some of the important elements we've seen from the previous chapters.
2311800	2321000	So, for example, we saw how policy is described or how it relates to outcomes and preference and so
2321000	2329480	on. We can see those elements are directly inspired by neuroanatomical structures. So,
2332520	2342840	I guess that's, at least in my opinion, this section here 5.4 seems a bit more
2343160	2353400	sketchy in the meaning that it doesn't go into quite the extensive details about how
2354760	2367880	those structures can be compared. But for anyone who wants to further investigate these topics,
2367880	2378120	there are some useful references put on here on pages 93 and 94. So, yeah.
2379640	2387320	Thanks. Yeah, it's really abbreviated and over viewed. But we get an interlude from table 5.1
2388040	2395000	with putative roles of neurotransmitters. So, same perspective that we took before on neuroanatomical
2395000	2403240	functionalism here directly translates to neurotransmitter reductionism or essentialism
2403240	2406920	or something like that. So, certainly all neurotransmitters and molecules that play
2406920	2413800	variable roles in different settings. And this is the neat and scruffy
2417000	2423880	manifold all over again. One person might say, well, we need a theory for every acetylcholine
2423880	2428680	molecule in the world. They're all in a unique context. And someone else says,
2428680	2432760	all neurotransmitters are described by one parameter in this model. I'm getting value
2432760	2438920	from it. So, to me, that's an account. And somewhere in between is the work in this space,
2439640	2447400	which is making an attempt to have a principled and falsifiable approach to
2447400	2456120	model the computational aspects of specific regions and contexts and settings. And so,
2456120	2463400	acetylcholine, noradrenaline, dopamine and serotonin are given a little mini review here.
2464600	2471320	And so, it's not an exhaustive or an exclusive claim. It's kind of a provocation from computational
2471320	2477240	and molecular neuroscience. And people can look into the papers and also ones that probably
2477240	2483880	have been published since. 5.6 goes to continuous and discrete hierarchies,
2484760	2489080	which is graphically overviewed in figure 5.5. So, what would you say about this?
2491800	2497560	Yeah, one interesting thing about this section is the observation that
2497800	2506440	our lower-level engagement with the environment can be most successfully
2507000	2514520	characterized with continuous time formulations. But as we go up on the level of
2516120	2524200	cognitive concepts or at the level of cognitive hierarchies, and we come to concepts such as,
2524280	2533560	I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time
2533560	2543720	situations would probably be more efficient to characterize the behavior of the agent. So, this
2543720	2554120	multi-scale structure of active inference modeling is quite evident in the way that
2555080	2561560	our message passing happens in our brain in terms of our lower-level data processing,
2562680	2570200	often to consolidating the higher-level cognitive concepts and ontologies.
2571640	2579560	Awesome. Thank you. To me, figure 5.5 demonstrates the kind of whole-of-body approach
2579960	2585720	that you could imagine. There's so many organs and systems and phenomena for which
2585720	2589640	there aren't specific generative models, so little can be said about situations where no
2589640	2595480	generative model has been articulated. And here's one where it has, so it gives you also,
2595480	2601000	it's kind of like reading a Drosophila melanogaster review paper relatively. It's like,
2601560	2606600	this is how much work it takes to get to this state of knowledge in an insect. So then in
2606600	2613160	another insect, do we know less about that insect empirically and genetically? So consider this to
2613160	2622280	be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive
2622280	2628200	systems, at least we know. So there's that additional kind of like self-reflexive aspect
2628200	2635000	to this chapter that is not a cornerstone of active inference, but here it's just presented
2635000	2640040	in a synthetic case study. Anything else you want to say about 5?
2642920	2645640	Nothing particular comes to mind. Thank you.
2645640	2650920	All right.
2665000	2674920	Okay. Chapter seven is called active inference and discrete time.
2675880	2683080	Chapter seven is the first in a pair of chapters with chapter eight on discrete and continuous
2683080	2691320	time. So they're kind of like two forks of a river that we discussed in chapter four and before
2691880	2697000	and described the recipe in chapter six. Now seven and eight are kind of like one level deeper,
2697720	2705240	going from the kind of all of this group of animals to one level deeper into its classification scheme
2705960	2713640	on the way to the specific generative model for which it's actually given in its totality.
2713640	2720840	But everything prior to that is about the learning about its principles and this is kind of on the
2720840	2726280	trunk of the path to discrete time modeling, just like chapter eight will be about continuous
2726280	2735320	time modeling. What would you add in? Okay. So I think chapters seven and eight
2735960	2745960	really helps to understand in a more practical way how the materials from particularly chapters one
2745960	2757560	through five applies in real-time situations. So even if we somehow didn't get to understand
2758120	2764680	every details of chapters one through four, when we come to chapters seven and eight,
2765640	2773880	I think some of those uncertainties about our understandings can be clarified
2776840	2783080	at least in a practical sense. So I believe these two chapters are really helpful
2784280	2788280	in order to consolidate our understandings from the previous chapters.
2788680	2796120	Awesome. Well said. So it's going to involve specifying some discrete time models.
2798920	2804200	Seven point two goes into perceptual processing and the general structure of the chapter is going
2804200	2810920	to walk through a series of examples that build in complexity where they first start with perception
2811000	2817880	in seven point two, introduce decision making and then describe a few more types of motifs or
2817880	2826120	cognitive structure or patterns and also check out step by step and model stream one where it's
2826120	2834520	built up to in a different way. So the first example is I'll let you describe it since it's musical.
2834520	2846760	Okay. So yeah, the first example is the situation in which we try to describe the performance
2846760	2855160	of an amateur musician in terms of how we listen to the performance of an amateur musicians
2855400	2867400	in terms of the predictions we get from our anticipation of the following notes as opposed to
2867400	2877400	the actual notes that's being played. So these kinds of anticipatory reaction, listening reaction to
2877400	2884200	the musician can be successfully formalized using discrete time active inference by
2885960	2900440	putting together the matrices A for the states and matrix B for the transition between the states
2900440	2908360	or the transition probabilities which in this case describes the probability from going from
2908360	2916520	one note to the other and obviously the actual sequence that's been played which can be described
2916520	2925960	with the matrix D. So and another point I wanted to point I wanted to mention is
2927160	2933800	for anyone who has downloaded this chapter before, I don't know, I think about June or something,
2934680	2941880	I recommend re-downloading it from MIT's website because they have corrected some of the typos
2941880	2947000	that was previously present in this chapter, particularly in figure 7.2.
2950600	2958680	Cool. So this graphical model where a person is listening, this is a general perceptual
2958680	2967400	Bayesian framing, it's specified. Just like with any other equations, there's a lot to look into,
2967400	2974600	but A indicates the probability of an outcome given a state. This is saying if it were all
2975640	2981560	on the diagonal identity matrix, this is kind of a common motif, then states kind of map to
2981560	2993400	themself. So in the context of, in the context of this model, A represents the mapping between the
2993400	3002200	observed note and the underlying hidden true note. FNB describes the transition matrix of how those
3002200	3011000	change to time D is the prior. They're specified. Figure 7.2, do you want to describe it?
3015240	3022200	All right, so in figure 7.2, or at least the incomplete version of figure 7.2 we see here,
3022920	3034600	well, at the upper left part of the picture, we see, I mean, the beliefs about each note
3036040	3045480	at each step, at each time step. And at upper right, we somehow translate those beliefs into
3046280	3056040	specific numerical values. So instead of just assigning some continuous values, we
3057960	3065080	simplified the situation by assigning some discrete numerical values for each note.
3065800	3081080	And then the lower left is supposed to show the free energy gradients over time or in other terms,
3081880	3091960	the prediction errors we get from, I mean, comparing our predictions with the actual outcomes.
3092920	3101720	So lastly, the lower right picture shows, in parallel to the upper right picture,
3102680	3113800	determines the values of these errors. So we can see both the continuous, the initial,
3114360	3120040	at least initial continuous assignment and values, and then the further
3121640	3129880	discretizing of the values in order to get the discrete time situation or the more tractable
3129880	3138680	discrete time situations. Okay, so it's a general passive inference task where there's
3138680	3143240	priors about how states are going to change through time, and then there's real data coming in.
3143800	3149000	So that's the kind of classical predictive coding, video compression, Kalman filter,
3149720	3159560	Bayesian setting. 7.3 introduces a key motif, which is decision making and planning as inference.
3160200	3165640	So this is the idea of having a Bayes graph where the variables can relate to different
3165640	3172120	things. There's high composability. And here the idea is that a variable is going to be proposed
3172120	3179000	that we can do inference about that describes the process of decision making or policy selection.
3179000	3190360	So what would you say about 7.3? Okay, so 7.3 is obviously similar to what we
3191240	3200520	saw in chapter four. And if I'm not mistaken, even the topology is exactly the same with that
3200520	3213080	picture we saw previously. So this is the initial setup or which acts also as a review
3214120	3220280	about how these different components upon DP generative models
3220360	3233080	need to be described in such situations. But ultimately, the specific case study
3234200	3244760	we come across in this section is the attempt to model the behavior of the mouse in a teammate,
3244840	3256280	so the rat in a teammate. So especially teammates containing an aversive stimulus in one arm and
3256280	3263800	an attractive stimulus on the other. So this is this can act as a kind of toy example to use
3264600	3272120	this kind of probabilistic modeling to describe these situations.
3275720	3281320	Thanks. So that leads us right to figure 7.4. Here's a visualization of the situation
3282040	3293480	with the rat in this case, where there's a pleasant and aversive stimuli on each end of a
3294120	3303240	decision point. And there's also a epistemic opportunity to receive some information
3303960	3313160	about the context that the animal is in. And so that setting is described for both the case with
3313160	3319480	white on the left, black on the right, and black on the left, white on the right. And those are shown
3319480	3324840	in terms of their differences in the matrices, the explicit specification of the generative model.
3327720	3335560	Visualizations show some of the slices of the B variable, which reflect different transition
3335560	3344520	probabilities. C represents the preferences, which are expressed over the observable states.
3345400	3359400	D reflects the priors on the different states that need priors. 7.4. What would you say about this?
3359400	3369240	Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously
3369240	3382120	saw in chapters 3 and sorry, 2 and 4, which is how the exact formulation for expected free energy
3382120	3389720	can be used, sorry, variation free energy can be used to formulate the tradeoff between the
3390680	3399960	I mean, information seeking and or at least between the epistemic value and information
3399960	3411240	seeking. So here, it uses, again, that rad example in a bit more, more extended and elaborate form
3411240	3423160	to formulate the epistemic value of observing Q in a given location. And figure 7.7 is a
3423160	3436360	representation of this situation. But another situation that's been, let me see, yeah, in 7.9,
3436360	3446840	another case study discussed here is the situation of the psychotic eye movements.
3447400	3456920	And because it is something that can be quite successfully described or characterized
3456920	3464120	in terms of information seeking versus the epistemic value. And the situation here is,
3465080	3475880	let me see, yeah, shown visually in figure 7.9, which clearly shows how our visual
3477080	3486920	psychotic eye movements can be described in such a way as to kind of trace the trajectory
3487880	3498360	of our eye movement among different regions of the visual space. And how the information we gather
3498360	3510040	from a given region can affect the, I mean, the subsequent trajectories of our psychotic eye movement.
3510040	3520840	So, yeah, that's basically the main premise of this section, I guess.
3521880	3527880	Nice, great. 7.5? What would you say about it?
3528440	3541400	Okay, so 7.5, again, adds another dimension to the previous formulations. And this time,
3542120	3556040	we get to update the generative models by learning. And so the generative models for
3556040	3563240	this situation is a bit more complicated than the previous ones, because it now needs to
3563880	3572520	account for a mechanism or a way to update the matrices we had before. So in the previous
3572520	3585480	situations, we didn't account for learning, per se. But here, we directly update our general,
3585480	3595240	sorry, the word update can be confusing here. We get to somehow improve our generative models to
3596200	3610680	accommodate for these updating accounts. And yeah, so the situation here, or the case study
3612280	3624600	here, which somehow elucidates the way that the learning can be accounted for with these models.
3625240	3639800	Is again, a toy example of a creature in a simple world of black and white tiles, which kind of
3639800	3651400	tries to find a path to reach a given destination, a certain destination. So it is more complicated
3651400	3660520	than the situation we had for the rat example, because it only had, I mean, simple trajectories
3660520	3672600	that needed to traverse. But here, the creature or the agent, in this case, needs to do lots
3673160	3681000	of lots more learning and information seeking and so on. So all the previous elements
3681800	3689480	is kind of combined in this example. And it's a really good example to see how the different
3689480	3698680	components of active inference can be connected to each other. Nice. And 76 hierarchical or deep
3698680	3707560	inference burst a box 7.3 interlude on structure learning boxed off topic and a lot to say.
3708200	3713480	But structure learning broadly refers to learning the structure about a model,
3714680	3721320	using the same types of methods that you might to do inference on, for example, a more observable
3722120	3733240	sensor data reading, something like that. This section works towards the idea of nested inference
3733240	3738280	or multi scale modeling. What would you say about figure seven 12?
3738600	3752040	Okay, so again, this situation is, I think, the most complex situations of this chapter,
3752040	3760920	which builds up from the previous sections. And this time, it adds another layer to accommodate
3760920	3772120	for the inferences that happen in different time steps. So in this case, we have a multi
3772120	3781480	time or multi scale inference and learning happening, both at the levels of learning
3781480	3789400	and at the levels of information seeking. So this, this is represented in
3791800	3801720	figure seven point 12, which represents how kind of this fractal generative model
3802520	3814120	can be seen as a component in this multi scale, a bigger generative or as a kind of leaf in
3814120	3825080	this bigger, bigger generative model. So it can be seen as a lower level inference happening at the
3825160	3834280	leaf level, going up to the hierarchy and influencing, sorry, collaborating on the whole
3834280	3848200	process of learning and inference at the higher level. So yeah, I guess that's somehow summarizes
3848200	3856600	this figure. So if you have anything to add. That's, that's great. It's an example of the
3856600	3868360	composability of generative models, what we've talked about and had Toby Sinclair Smith describe as
3868360	3874920	as the compositional cognitive cartography, and just what kinds of connectors can and can't you do?
3875720	3882600	And how can that motif that the discrete time model introduces? And then the rest of these
3882600	3888760	features, including action and learning and so on get layered in on top. What can you do with that?
3890440	3897400	713 gives another example. Do you want to say anything about it or maybe continue on?
3898360	3908200	Yeah, so the case study here is the example of linguistic, I mean, language learning through
3908200	3918520	reading. So not language learning. Maybe it's just what happens in reading. Yeah, in comprehension.
3918520	3929640	So what happens when reading and in an anticipatory way, the words that that comes
3931000	3938760	each after the other. So why this kind of situation can be most successfully characterized
3938760	3946760	with this kind of modeling, because it involves different scales of learning and comprehension,
3946760	3957160	both at the level of, I mean, reading at the level of somehow observing the letters and then
3957160	3965640	going on to the words and then word groups and so on. So yeah, that's a really interesting way to,
3966600	3974760	again, combine all of those elements into a single unified model to see how those different
3974760	3988440	timescales, slow and fast timescales operate together to build this more encompassing model,
3989320	3991720	more encompassing generative model of the situation.
3993560	3995320	Great. Any closing thoughts on 7?
3995800	4001000	Nothing particular, not bad. Thanks.
4001000	4015880	All right. Next chapter is chapter eight, which is going to go into the continuous time.
4026040	4032280	All right. Chapter eight is called active inference and continuous time begins with that
4032280	4037880	timeless quote, everything flows, nothing stands still. So what would you say about chapter eight?
4039960	4047560	All right. So this chapter probably is my most favorite chapter in the book,
4047560	4055240	because of my own personal interest in, I don't know, the process materials and so on.
4055240	4065400	But yeah, so chapter seven acts as a really good starting point for anyone who wants to
4066600	4071400	develop the discrete time situations, to model discrete time situations
4072360	4079320	within active inference framework. But in chapter eight, we kind of get to
4081640	4088680	model a bit more interesting or, let's say, more involving situations.
4089640	4099000	And they're not necessarily toy examples we saw at least at the beginning of chapter seven.
4099960	4107960	So obviously, as the title suggests, this chapter deals with the continuous time situation.
4107960	4117400	So in that case, we'll need to, maybe at this point, refresh our memory about
4117400	4122280	what continuous time situation involves by reading the relevant parts,
4123160	4130760	reading or reviewing relevant parts of chapter four. So yeah, in chapter four,
4130760	4138920	we saw that the generative model for continuous time situation derives from the
4140120	4146440	it is a stochastic calculus in terms of putting the whole process
4146760	4157320	into two elements, two stochastic equations, one of which is the actual
4158120	4165160	state, the condition of actual states or the behavior of the actual states. And the other one
4165800	4173160	is the randomness that we need to account for in each real time continuous time situations.
4173240	4181720	So that's what we get here in equation 8.1. And then, building up from that equation,
4183080	4194440	we, it generalizes that equation to involve, I mean, the functionals of G and F instead of just
4195400	4205000	the single valued functions of G and F. So then we get to
4207240	4215400	put that into the situation that can be used for describing the behavior of dynamical systems,
4216040	4224040	which is a very well known situation to use these kinds of stochastic equations.
4224680	4232920	And it's widely studied how those, those kinds of dynamics can be characterized, especially
4232920	4242680	in recent Bayesian mechanics paper by Dalton, Saktiv Atevel and others. So, and then it gets to
4242680	4251320	some more specific examples such as Lothgabal-Terra dynamics and synchronicity and so on, in order
4251320	4264280	to show how these kinds of dynamics can be elaborated upon and can be generalized to,
4264280	4272440	and enables them to characterize more complex situations. So,
4275400	4281640	yeah, that's a really short and brief overview of the whole chapter. Maybe
4282840	4285560	we can talk about a bit more details as we go through it.
4285800	4294600	Great. Well said. Well, I'm sure for another day, the philosophical implications of eight,
4294600	4299320	seven and eight, and high road and low road, and all these other parts of the textbook, great topics.
4300200	4308600	I agree. I would see chapter eight as demonstrating continuity with some classical
4309560	4315880	continuous time modeling motifs from a few different areas of dynamical system science,
4316600	4321240	which is applied in like many, many, many fields, but these are some classic examples.
4321240	4327720	So, figure eight point one goes a little bit more into depth, or at least into more formalism
4327720	4335480	detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive
4335560	4340280	data coming in, and then a differential being calculated with the set point,
4340280	4347400	which reflects a descending prediction from a decision making layer. And that can be viewed as
4347400	4355320	this kind of mechanics that plays out in a phase space in continuous time, like a spring moving
4355320	4362280	around with someone making a certain path with an attractor, and a spring being dragged around
4362280	4369560	something in that area. Box eight point one goes into a very fascinating topic. Do you want to
4369560	4383960	describe it? Well, it's maybe one of the most thought provoking pages of the whole book. And
4383960	4389320	if I remember correctly, in all of the cohorts, this particular box
4391800	4398840	I mean gives always gives rise to lots of questions, because of some of the interesting
4398840	4409880	and at least initially counterintuitive claims here. But I don't want to spoil it. So
4410440	4426520	but as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief
4426520	4433800	discussion about the comparing these terms precision, attention, and sensory attenuation,
4433800	4440680	and the relation and similarities and difference between these two, these three terms,
4440680	4446760	and how each understanding each of them is essential to understanding the other ones.
4447640	4455400	But as I said, it's a really interesting topic, which gives rise to lots of discussions.
4455560	4466040	And I believe it's one of those topics that that's worth looking a bit more
4467400	4470920	looking into in some other literature as well.
4472040	4480120	Great. Well said. What a cliffhanger. Next, they go to a classic model family called Laka Volterra.
4480120	4486920	These dynamics inherit from characterizations of predator prey dynamics in ecology. So it's kind
4486920	4495000	of a classical ecology model shown in figure 8.2. On the top, it's actually the ecosystem model. Plants,
4495000	4502040	herbivores and carnivores, which follow different kinds of oscillatory trends in continuous time.
4502680	4509160	And so that also has enabled it to be applied for other so-called winnerless competitions.
4509880	4515240	And that relates to topics like neural Darwinism and also neural dynamics, where things have
4515240	4521800	kind of oscillatory relationships with each other, which are being modeled as a continuous time
4521800	4529000	underlying process with a lot of measurement noise and discretization through space and time.
4529000	4533880	Those are the kinds of algorithms that SPM explores more. And there's Laka Volterra and a
4533880	4541960	lot of other dynamical systems theory in SPM. So active inference kind of adds action and more
4542680	4550280	to what was laid out from a pure dynamical systems theory in SPM. Here, it really is just
4550280	4555560	showing the ecology example and how you can project. If you have three different species,
4555560	4562760	you can think about that motion in a cube or tetrahedron. And then you could project onto
4563400	4569800	kind of like looking at a lower dimensional manifold relating just two of the three species.
4570520	4575800	And that evinces this kind of oscillatory but also moving behavior.
4576920	4583400	That gets connected in figure 8.3 to neurobiology. What would you say about this?
4583400	4595560	Okay, so here in figure 8.3, we see some applications of Laka Volterra dynamics.
4596360	4610920	So the left column here represents what happens in, I mean, in eye blinking, eye blink conditioning.
4611720	4628360	So, of course, here we need to account for, I mean, the expected states of the sequences of events
4628360	4638440	that happens in the eye blinking. So the upper left figure shows the expectations
4639320	4649560	in terms of time. And then the parallel right hand side equation, sorry, right hand side figures,
4650280	4661560	shows the Laka Volterra system that is applied in the handwriting situation. So as we can see,
4661560	4669480	although the, I mean, mathematical technology is the same or at least the modeling technology is
4669480	4684200	the same, the outcome of each situation varies drastically in two distinct neurobiological
4684280	4693240	behavior, not neurobiological, but biological behavior. So, yeah, we can see how the same
4693240	4705560	modeling framework can give rise to different outcomes based on what parameters needs to be
4705560	4714120	optimized, what parameters are selected for modeling and so on. So I believe it's a quite
4715080	4725720	interesting example to compare handwriting and the blinking together and how those can be compared
4725720	4735160	to each other using the Laka Volterra dynamics. Great, thank you. Box 8.2 gives a variant on the
4735160	4740760	learning here presented with the formalism for continuous models, kind of a technical aside.
4741720	4749560	Section 8.4 is about generalized synchrony. So figure 8.4 is going to visualize one of the
4749560	4754520	classic dynamical systems, which is the Lorenz attractor. So what would you say about this
4756120	4765880	figure? Okay, so this section is truly interesting because when one thinks of active inference,
4765880	4775000	probably the first situations that comes to mind is the situations in which we have quite well
4775000	4784280	defined probability distributions for different parameters. But as we can see here in section
4784280	4793400	8.4, actually some of the formalism of active inference can be successfully used to characterize
4793400	4801080	even chaotic systems and in particular the way in which two chaotic systems can be synchronized
4801080	4813160	with each other. So this is a classic example of a chaotic Lorenz system, and it draws upon
4814040	4822920	from some of Professor Pristin's earlier work on birdsong synchrony. And as a side note, any
4823000	4831000	literature before 2016 is considered earlier history in active inference literature because
4831000	4843640	it evolves quite rapidly. So yeah, this kind of synchrony between two chaotic systems can be
4844600	4856280	interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of
4856280	4871080	mind in the sense that how exactly can we understand or can two agents can trace each
4871240	4883640	other's trajectories without, I mean, engaging in any direct exchange of observations between
4883640	4891320	their internal and external states. So yeah, that's a really good example and I believe one of the
4891320	4898520	most interesting examples of how active inference can even account for these kinds of behavior.
4899160	4906760	So the rest of the section goes into the details of how this kind of synchrony between
4908760	4920920	multi-scale Lorenz systems can happen and how can we formulate it mathematically in terms of
4920920	4928600	continuous time active inference. Awesome. And there's been more recent work on Mark
4928600	4936280	Alblanket since stochastic chaos, but the bird example is a classic. 8.5 goes into hybrid discrete
4936280	4943080	and continuous models. So this could be kind of like an in-between chapter of seven and eight,
4943080	4947960	but now that we've been introduced to the pure form of discrete and the pure form of continuous
4948040	4954840	models, here's shown that that composability extends to so-called hybrid models, where here
4955400	4963800	the lower level visually is using the continuous time formalism and the higher level is describing
4963800	4971080	the little line added here, the discrete time formalism. And this was the similar structure
4971720	4978200	described by the authors of the paper, active inference does not contradict folk psychology,
4979240	4984760	where they describe this lower level as motor active inference, which was closely
4984760	4993160	allied with the spinal arc reflex shown above. And then this higher level, they call decision
4993160	4996840	active inference, because in that case, it was referring to a discrete decision.
4997480	5004600	And so they used that kind of basic motif of continuous activity or continuous time modeling
5005240	5012440	at the more peripheral aspects of a cognitive entity. And like Ali said, more discretization
5013880	5019400	and hybridization as well at higher levels of the cognitive modeling.
5019560	5028200	And that type of an architecture here, instead of describing who wants the ice cream cone,
5028200	5035320	I believe, here, it's going to be a mixed or hybrid model that is going to call back the
5035320	5047240	isocade system, where there's a fixed point that is able to be moved as a set point. And then
5047240	5053800	there's a continuous time isocade that pursues the new fixed point. And so that's analogous to a new
5054520	5060600	set point or fixed point being specified from the top down muscle command about a new location
5060600	5067720	for a muscle, followed by movement towards it. This is a muscular activity that is realizing
5067720	5074680	that but but not in the elbow coming away from the hot stove. This is about the ice caking to an
5074680	5083400	epistemic foraging location specified by top down hierarchical systems. 8.3 describes little
5083400	5091800	technical aside on mixture of Gaussian Gaussian mixture models, kind of a technical modeling
5091800	5099160	note. And 8.6 closes. It says it's a huge topic and much has been left out. And so they list in
5099160	5105480	table 8.1 key advances in continuous time models. And those areas are synthetic bird song,
5106280	5112440	ocular motor delays, conditioned reflexes, smooth pursuit, eye movement, psychosis, illusions,
5113240	5121160	saccades, action observation, attention, hybrid models and self organization. And that's chapter 8.
5121880	5127880	What else would you say? And also what would you kind of lead someone to in the philosophical
5127880	5139560	implications of 8 because it sounds kind of cool? Okay, so well, the case of continuous time active
5139560	5152680	inference. I think it leads to really interesting questions, both in terms of philosophical questions
5152680	5162920	and also more practical modeling questions about what parameters needs to be accounted for and so
5162920	5174920	on. And as I said, I believe it's a more more interesting way of if not interesting, but but
5174920	5184120	at least more involved way of doing active inference modeling. But one thing that one of the
5184120	5197960	philosophical questions that Mao and I have explored in our paper is how the processes of I mean,
5198040	5210040	ontological processes can philosophically described using FPP assertions in terms of their
5210040	5220200	interaction with the environment in which they co constitute themselves. And we don't necessarily
5221160	5229160	distinguish between between the internal and the external states. So one obvious example of this
5229160	5237000	is that generalized synchrony example that we saw in this chapter, in which we don't necessarily
5237000	5247480	distinguish between which of the birds act as the agents and which one is the environment or the
5247480	5254920	vice versa. So these kinds of co constitution of the environment and the agent, which gives rise
5254920	5263000	to the partitioning of state space through a Markov blanket is one of the interesting
5265240	5272520	philosophical points that I think needs to be elaborated a bit more using
5272920	5282600	some of the recent advances in philosophy, such as the tools that's been developed in new materialism
5283640	5290200	school or some other philosophical approach approaches. But yeah, these kinds of
5291560	5298120	what exactly gives rise gives rise to emergence, what is the ontological status of emergent
5298200	5306440	properties and so on, are some of the burning questions for many philosophers today. And I
5306440	5312600	believe active inference and particularly continuous time active inference provides a clear,
5314360	5325480	precise mathematical formalism. Even if not to answer these questions, but at least
5325480	5336280	to explore it in a more rigorous and practical way, and also practical and a tractable way.
5336280	5347560	So this is the area that I believe philosophy and science are beautifully intertwined into a coherent
5347560	5354680	view of not only the phenomenon of interest, but even about the whole world.
5357480	5368760	Wow. Wow. Pretty cool. Yeah, a lot to say about that topic. After completing chapter seven and eight,
5369560	5376840	you've seen the kind of two major branches or two major motifs of just one kind of modeling.
5376840	5383240	But these kind of models have so many different forms that that's why it's such a hands on process
5383240	5389240	to specify the generative model in chapter six and fit it with data in chapter nine. Those are all
5389240	5396280	what's required. And that's kind of the last mile of where these discussions about general motifs
5396280	5402840	gets you. But also playing with these pedagogical models can be really helpful, because it will
5402840	5409160	help you understand the basic patterns and relationships and start to see see different
5409160	5416840	patterns in the graphical models and know from there what levels of technical processes can be
5416840	5431960	kind of coarse grained over. All right. Okay, well, that's it. I guess next time we will do probably
5432840	5444360	nine, 10, and maybe something else. All right, I'll end it now. Thanks, Holly. Thank you.
