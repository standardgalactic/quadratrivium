start	end	text
0	15760	Alright, hello and welcome. It's May 24th, 2024. We're in ACTIMF livestream number 57.0,
15760	20840	doing background and context video for the active data selection and information seeking
20840	26720	paper and series. So welcome to the ACTIMF Institute. We're a participatory online institute
26720	31360	that is communicating, learning and practicing applied active inference. This is a recorded
31360	36000	and archived livestream. Please provide feedback so we can improve our work. All backgrounds
36000	40720	perspectives are welcome. And we'll follow video etiquette for live streams. Head over
40720	47760	to active inference.org to learn more about any of the projects, including the live streams.
47760	56880	Today, we're going to do together a background first pass on a very interesting paper from
56880	61680	Thomas Parr, Carl Friston and Peter Seidman, active data selection and information seeking
61680	69760	from 2024. In this video, we're going to introduce ourselves, talk about big questions, go through
69760	75120	the keywords of the paper, then most of the sections, section by section. And as always,
75120	79840	with the dot zero, it's just like a first pass. And we'll look forward to speaking with
79840	86160	hopefully some of the authors in the coming weeks, and also looking what people ask about. So
88000	94080	Christopher, let us introduce ourselves and go from there. Thanks a lot also for helping in
94080	101120	the dot zero preparation. Happily. Yeah, so I'm Christopher Bennett. I'm a bioinformatics
101120	109760	scientist. I do a lot of data mangling data analysis and that sort of thing. This paper was
109760	116000	of great interest to me as we kind of go into this more data driven era in making sure that with
116000	121120	such large data sets that we have, making sure that we can actually select relevant data for any of
121120	126800	our applications going forward, be it machine learning or whatever we're trying to do.
131440	137280	And I'm Daniel. I'm a researcher in California and also was drawn to this on one hand on the
137280	144880	applied side, the idea of more efficient and effective data sampling. And then on the more
145520	149440	theory side, the connection with epistemic value information gain.
150800	156000	So here are some of the big questions. Why don't you add some detail to this?
156960	162000	Absolutely. So there's five major big questions that I had after reading this.
163520	165600	For the most part, it boils down to
168160	173280	doing our sampling. You can do sampling over time and sampling of different data sets in different
173280	182080	ways. Is there a way that we can intelligently select the data that we're going for the time
183040	189120	the time that we're trying to select? Is there a way that we can understand how the
189120	197760	time aspect helps sample through time instead of just doing a dynamic or more dynamic instead of
197760	204960	doing a static like we're going to do time zero, time seven, time 14, time 21. Can we say, hey,
204960	210240	the differences between time one and time two are very 10.1, time point two are very interesting.
210240	216400	It's a lot of data in there alone. So we'll sample one and two and then maybe sample 10. Is
216400	221280	there a way that we can intelligently select the time points that we are sampling from
222080	228400	when we get into the time series aspect? The Piper mentioned a number of different
229520	235200	time dimension models that you can add to the four model that they're utilizing,
235840	240880	one of which was a hidden Markov model, another was they mentioned a differential equation in the
241520	250560	actual model itself, in the equation itself. Is there one, are there situations that one
250560	256720	performs better over than the other? Or is what they have selected to use in the paper the optimal
256720	264960	solution in most cases, if not all cases? When it comes to clinical trials, that was a
265120	273440	section in this that they discussed. There's a lot of FDA regulations of the clinical trials and
273440	281680	it's very heavy red tape right now. Is there a way that there's minimum ends that you need in
281680	288240	many clinical trials to actually be considered passing? Is there a way that you can bound this
289200	296560	model that they've developed into something that you can guarantee a minimum number,
296560	300480	a minimum sampling that the FDA requires or any regulatory body?
303040	308960	Another point is the next step of how are we going to integrate this in with other machine
308960	315600	learning models or any downstream applications that you're going with? Is there a selection method
315600	324960	that we can, or how do you see these, this method kind of pre, kind of before machine learning?
324960	330800	How are we going to attach these things together so that we feed the right data into machine learning?
332480	338880	Being an LLM. And then scalability and computation demands. That's going to be a big one if this
338880	344160	is going to be something that is used routinely in industry. We need to make sure that this is
344160	352480	something that is as scalable as you can get. Go from small scale, which is a lot of what they
352480	356240	show in this paper and then all the way up to the very large scale data sets that we
357280	363120	use to train LLMs and other models. Those are kind of the five major points that I have.
365120	371680	Thank you. Those are very insightful. Here were some of the big questions that I was excited about.
372480	378400	So first, from a more general information gain, epistemic foraging perspective,
378400	383520	how do we model the implicit and explicit constraints or trade-offs or dynamics of
383520	389840	information seeking? Which is often addressing a question that is left unaddressed in data science
389840	394080	of where the data comes from. It's just about doing analysis with the data that's there.
394080	398800	But even there, as this paper will kind of get into, there's still sub-sampling and all these
398800	406720	other factors to consider. The clinical trial example brings a very serious and very real plot
406720	415440	twist into the paper, which moves through several levels of adding theoretical generalization and
415440	420400	incorporating like the time dimension and other features. And then the plot twist is when the
420400	427120	preferences for certain kinds of observations is specified, then there's all this interesting
427120	430960	behavior and decisions that come into play. So I'm sure that'll be a great discussion.
431920	437040	And then also in section four, they mentioned the streetlight effect, which is, quote,
437040	441920	the tendency to search where data are generated in a minimally ambiguous way,
441920	447040	i.e. under a street lamp compared to searching elsewhere on a darkened street. And so there it's
447680	452560	an interesting scenario and there'll be some fun art coming up and also how they distinguish the
452560	458800	sampling method with the full information seeking from the maximum entropy sampling
459440	464800	is a very subtle but very important point that I look forward to hearing more from the authors about.
469600	479120	Okay, so just to summarize, the paper is Active Data Selection and Information Seeking,
479200	485200	2024, Thomas Parr, Carl Friston, Peter Seidman, and just a few of the aims and claims of the paper,
485200	490480	and then Christopher will read the abstract. This paper aims to unpack the principles of
490480	494960	active sampling of data by drawing from neurobiological research on animal exploration
495520	500880	and from the theory of optimal experimental design. Our overall aim is to provide an intuitive
500880	506080	overview of the principles that underwrite active data selection and to illustrate this with some
506080	512880	simple examples. Our interest is in the selection of data, either through sampling subsets of data
512880	519040	from a large data set or through optimizing experimental design based upon the models we have
519040	524640	of how those data are generated. Optimizing data selection ensures we can achieve good
524640	531440	inference with fewer data, saving on computational and experimental costs. So if you could read the
531440	539920	abstracts. Absolutely, so the main points in the abstract are the Bayesian inference is typically
539920	545680	focused on two major issues. The first one is that you have to estimate the parameters of the model
545680	550880	of the data, and the second is that you need to quantify the evidence for alternative hypotheses
551680	558320	and formulate an alternative model. But this paper is actually looking at a third issue,
558320	565120	which is in how you're going to select the data for your models. And either through sampling
565120	570160	subsets of large data is typically used or optimizing experiments of design.
572320	579360	Based upon the models we have these of how these data are generated. Optimizing data selection,
579360	586960	what's going into the models can achieve a very good inference with fewer data points. So you're
587040	591920	saving on computational time costs, that sort of thing by actually reducing the amount of
591920	598240	information that you're putting into a model. So what we're doing here is trying to unpack
598240	604560	how you're going to actively select data, and I mean actively select data through a machine
605680	613440	optimization protocol by drawing from some of these neurobiology concepts and trying to optimize
613440	622880	the maximum information that the information can provide, maximum information gain. So they offer
622880	629200	overview of some basic points from the field and illustrates the application in some of the toy
629200	635520	examples that they have will go through, ranging from different approximations with basis sets
636160	642000	to inference about how the process can evolve over time. And finally they'll go through and
642000	647280	consider how the approach to the data selection could be applied to design of clinical trials in
647280	652640	this case, and specifically Bayes adapted clinical trial, something that is more and more
653680	660480	seeing headlines and it's more and more used today now that we have the technology to do it.
662720	669200	Great. Okay. For the roadmap, the paper begins with introduction section,
670080	674480	goes into Bayesian inference, generative models and expected information gain.
675280	682240	They go through a simplest worked example, and then consider a few more ways to level up that model
682240	688400	with function approximation, consideration of the time dimension with dynamic processes,
688400	692720	and then bring in the preference for certain observations in the clinical trials.
692720	698160	Then there's a discussion and conclusion, and they also have a paragraph explaining their
698240	704800	kind of logic there. The keywords for the paper were experimental design, active sampling,
704800	711040	information gain, and Bayesian inference. So the next slides are going to go into those
711040	717680	four background topics. After the four background topics, we'll speed through the sections and
717680	722880	just plant a few seeds for what we want to explore more. So first, experimental design.
723600	731600	Here's two kind of classical views of experimental design in the active inference and free energy
731600	741120	principle eras. So on the left is the statistical parametric mapping, textbook, toolbox, documentation,
741120	748240	et cetera, has multiple chapters and kinds of analyses included in the package to specify
748240	753360	and simulate and also to recognize data according to different experimental designs.
754080	761520	And one very hallmark or common visualization of these kinds of patterns of experimental design
761520	769040	are these design matrices. And it's just represented in this black to white gray scale,
769920	775200	and it summarizes different kinds of measurements across different experimental settings. Like here
775200	781360	might be six settings in the larger white blocks. And then there was variability within each of those
781360	785760	trials. And those are the smaller row levels. So that's like where the data are collected.
785760	790640	And a lot of this has to do with the linear operations that can occur on this kind of
790640	797360	matrix in a general linear modeling framework. And then on the right is the experimental design
798080	804880	experimenters perspective, where the experimental stimuli they output as actions are the
804880	811040	observations going into the subjective model, like of the rat in the team is, and then the action
811040	816960	output of the rat is the observations of the experiment of the experiment. So kind of two
816960	827040	different perspectives, SPM with more of a matrix multiplication, f m r i optimal design, and then
827040	832320	active inference with the more general graphical Bayesian modeling, starting to broaden the
832320	837840	consideration of what optimal foraging and what information gain epistemic value mean.
837840	842560	These are kind of the experimental design themes and how they connect a little bit with
843360	849840	other experimental design factors. Want to add anything? Yeah, keep in mind that a lot of these
849840	856320	experiments, experimental design is a very big and very important consideration when you're
856320	861920	actually running any sort of science or analytics of any variety. And these experiments can actually
861920	869120	get very large with huge, huge amounts of data. And all of that data is relevant for every
869120	874000	application that you want. So you want to be able to design your experiment in a way that you can
874000	880640	collect information in a intelligent way rather than trying to go through and just collect every
880640	885600	data point that you can because humans in many cases are running some of these experiments and
885600	891680	they are have limited time. I certainly do when I'm running these things. So I have to be very
891680	896960	intelligent in how I set things up and how I actually collect data and what did I collect,
896960	900640	because you only get in many of these cases, you only get one shot to collect the data,
900640	906960	you miss it, it's over. You won't have that data point. So it's very critical that you actually
906960	909920	take the experimental design seriously when you're setting these things up.
910640	918880	Great. So connecting that kind of experimental design, experimenter on a budget perspective
918880	928640	with a more statistical and biologically statistical based perspective, active sampling.
928640	933200	So they wrote, when we look at the world around us, we are implicitly engaging in a form of active
933200	938720	data sampling, also known as active sensing or active inference. So this is referencing the
938800	945200	visual saccades. And just to kind of highlight how extreme the relative acuity difference is
945840	951360	between the center of the eye where the gaze is focused on and the off center,
951360	956640	among other visual changes. And vision is just being taken as one sensory example here.
957040	962080	It could also be thought of as like looking for books within a library or any other kind of
962080	967840	selection of what data are going to come in, even if it seems like all of it is going to
967840	974640	all of it is coming in, that still is going to be perhaps addressed with different sensors that
974640	979840	have different variability profiles, or like there's different RNA sequencing kits that you
979840	986400	could buy. And so how do you balance the kind of more samples or which samples, especially as
986400	993040	those spaces grow massive. And then just to contrast that, whereas digit recognition in a
993040	1000400	saccade based paradigm would focus on the motor patterns and the small centrally focused
1000400	1006400	visual acuity and then the motor patterns that relate to circuiting around a digit. Whereas
1006400	1012960	in the kind of machine learning taken all at once approach, a matrix corresponding to like the
1012960	1020880	pixels in the MNIST dataset are simply taken in all at equivariance level. So that's just kind
1020960	1024800	of taking in the data. There's still another higher order data selection question of like,
1024800	1030400	which digits do you take? If there was a large number of digits in that library. So this is
1031120	1037280	active data sampling on multiple scales, which records you pull at all, and then how the resolution
1037280	1042160	and all the tradeoffs that are associated with using the data processing or making the experiment.
1042720	1050000	Yeah, add more though. And, you know, keep in mind that when you're talking about something
1050000	1054800	like the visual system, you know, our visual system has access to untold amounts of information,
1054800	1062080	but our brain can't take advantage of all of that at once. So there's low energy usage of the brain
1062080	1066960	that needs to optimize the relevant information. Think, you know, your nose is right at the end
1066960	1072240	of your face. Your eyes are always seeing your nose, but your brain is filtering it out. And this
1072240	1077040	is happening all the time at all points in time. There are literal blind spots in what you are
1077040	1085120	actually capable of intaking and processing all at once. And then additionally, when you're
1085120	1090560	moving away from something like your eye or biological systems and into the experiment
1090560	1095920	design itself, you know, you oftentimes can't run a full factorial design. And there are other
1095920	1102080	methods like a fractional factorial design. But those are random base. And this is trying to
1102080	1108880	actually talk about actively selecting how you're going to set up that design. So it's kind of a,
1108880	1115920	you can think of it multiple different ways. Awesome. The factor that's going to come into
1115920	1123840	play as driving the active sampling is going to be the information gain. And there's some quotes
1123840	1130320	here. And equation two is shown. They write, we have conditioned our model upon the variable
1130400	1135600	pi, which represents a choice we can make in selecting our data. So data recognition,
1135600	1140960	interpretation, analysis, and so on. It's often framed as kind of like an observation type or
1140960	1149120	sense making type activity. Here, pi for policy, as with usual, is being framed as a control or
1149120	1157360	an active data selection policy, we're applying to some data set. So it adds a action element
1158000	1163200	into this sequential epistemic foraging, rather than just taking a large data set,
1163200	1169120	and just munching it like all at once, it brings in this sequential question of where to sample,
1169120	1176160	and potentially updating where is informative to sample through time. And that I of pi is the
1176800	1182480	functional on that policy distribution or specific choice that can be decomposed,
1182480	1186320	all these interesting ways that we can explore more in the coming discussions.
1186320	1188400	What else would you add, though, about information gain?
1189440	1194000	I think this is one of the biggest points in this whole paper is you're measuring how much
1194000	1199440	information you are gaining in your model by adding these different variables in here and
1199440	1205360	selecting different variables. You're effectively automatically taking out or trying to remove
1205360	1210320	things that have high mutual information that don't add as much. So if you have parameter A
1210320	1215680	and parameter B that are effectively just transformations of the same data, then you
1215680	1219840	can easily remove one of those and still have all the information that you need.
1220800	1226720	So it's a really, really powerful way of saying I'm trying to optimize and maximize the amount
1226720	1233120	of information that I'm adding into the model by selecting data that actually has the information
1233120	1243280	that is going to improve them. Awesome. One other interesting angle here is often in the control
1243280	1250000	literature, utility, reinforcement learning, etc. The epistemic value component is added in,
1251120	1258080	whereas in the structure of this paper, they start with the pure information gain perspective.
1258080	1263520	And then in the clinical trial, they bring the preference in. So the pragmatic value
1264160	1269520	comes in secondary to the information gain in how they build it up step by step.
1272000	1279360	Bayesian inference is the last keyword. A lot of places to go. Here's what they showed
1279920	1285280	for equation one. And they wrote Bayesian inference is the process of inverting
1285760	1292640	a model of how data, why, are generated to obtain two things, the marginal likelihood
1292640	1297520	and the posterior probability. So anything you want to say about Bayesian inference or
1297520	1302320	do you want to say something about Bayesian networks and graphs? I think that you've kind of
1302320	1308960	covered it here. It's, I think, barely textbook on this part. Yeah. How about graphs?
1309920	1317680	On the Bayesian graphs side of it, there's multiple different ways that these Bayesian
1317680	1323360	statistics is done nowadays. And the Bayesian networks and graphs is a really powerful method
1323360	1330880	going forward. I know that right now in the Institute, we have an Rx and Fur group working
1330880	1337040	right now, which is a Julia package for actually just building these network graphs, these Bayesian
1337040	1342480	graphs and doing message passing between the different factors and the different nodes of
1342480	1351440	the graph. So this is a very big up and coming area right now. It's very early in the time frame
1351440	1356480	that this is going to become big. It's kind of on the upswing right now. And it's kind of,
1357440	1362000	at least I would predict, going to be kind of the next big thing going forward in the next five
1362080	1370880	years or so. Yeah. We've been having a great epistemic time and Livestream 55 explores some of
1370880	1379360	this in more detail. Okay. That was the background now on to the paper. So first, just to get the
1380560	1386960	last part of the paper out of the way, they have a GitHub, Thomas Parr's GitHub with the
1386960	1392560	active data selection repo. And maybe in one of the upcoming discussions or somebody in the time
1392560	1400960	between can explore and transform and play with the code. And also all these different
1400960	1405760	ways that we have fun discussions around the language of the active inference model
1406320	1412240	and how building it in different languages or using different styles like is there isn't plausible.
1412240	1418400	These have been very fun discussions that help us get at what the core of the math really is
1419040	1423680	and how that's independent of whether it's written in MATLAB or any other language.
1424640	1429840	And then also as it is simulated, it's written here in MATLAB. And so that's kind of interesting.
1430560	1434640	Any thoughts on that or just like coding in Rx and fur or or
1435600	1443440	Yeah, I think that with Rx and fur being, I think, relatively new on the scene, you have some of
1443440	1450960	these other traditional approaches with MATLAB and high MD and that sort of thing. It'll be very
1450960	1458320	interesting to see how these techniques evolve over time with packages like Rx and fur really,
1459040	1465280	I think, changing how we approach building these models and designing them. I think that it's going
1465280	1472240	to be even more critical now in this current environment to select the data intelligently
1472240	1478080	going in so that you're not muddying up your models or having to build two big of models that might
1478080	1486560	have information that's not as useful to the application at hand. Yeah, great. All right,
1486640	1492400	section one introduction. So we'll try to hit on some of the key points. I'll say something briefly
1492400	1499840	and then feel free to add something if you want. Section one situates that inference and action
1500800	1509520	cycle or loop or partition in terms of a statistician's job or process in modeling
1509520	1517520	observations data as sampled and latent variables as models and the process by which there's
1518560	1526560	kind of snapshot or bulk summarization or generativity or and how it's possible to have a
1526560	1533360	continuous resampling of informative data or how you even evaluate how data are informative in which
1533360	1540080	way. Want to add anything? I think you've captured that very well. I'm going to actually pull out my
1540080	1548400	notes so that I can actually remember all the symbols. Why are going to be used for data and
1548400	1556800	data for the latent variables? So distributions of data, distributions of latent variables
1556800	1564080	conditioned upon data coming in. So that could be seen as just one data point sequentially or a big
1564080	1571360	bulk vector coming in like all at once. Just continuing to move through this section, they wrote
1572640	1577440	careful data selection is especially important when we consider the problems associated with
1577440	1582240	very large data sets of the sort that are now ubiquitous in machine learning and artificial
1582240	1589920	intelligence settings. So just to summarize a little bit or add a few notes that came up in
1589920	1596080	the paper. So other than this topic being very fascinating and very integrative in terms of a
1596080	1601520	unifying approach for information and behavior etc. Also this is definitely one of the active
1601520	1607200	inference questions that has a lot of pragmatic relevance as dealing with with data sets of
1607200	1614240	different kind is totally day to day. And especially the way that even the examples specify
1614240	1620400	important settings is very clear, very direct. Though also the mathematics are very general
1620400	1628400	about epistemics and this motivation that they lay out in the first section about how if this
1628400	1633040	challenge could be addressed, then there will be all these kinds of benefits that could be realized
1633040	1640000	with current systems and data sets. And then they provide the approach to at least getting
1640000	1645280	there or towards it to optimize data selection. We first need to identify an appropriate optimality
1645280	1649680	criterion. And so they're going to kind of go through several stages of with different
1650320	1657920	generative models how that optimality criterion is defined. Anything else that
1658880	1664000	And keep in mind that this is the expected information gain that they're talking about.
1664000	1669760	It's effectively how much do we think we're going to gain by adding this information in there.
1669760	1676160	And then you can of course train your model by looking at the actual information gain if necessary
1676160	1682000	and go through kind of a learning cycle. But we're basing this all off of what do we expect
1682080	1688640	to gain from this information. All right, section two, basing inference generative models and
1688640	1698400	expected information gain. In this equation three, I won't read it all. It models the Markov blanket
1698400	1704800	formalism in terms of upstream and downstream causal relationships in terms of messages that are
1704800	1711200	passed along edges of a factor graph. They introduce in this paper the lambda operator
1711200	1717920	to indicate either summation or integration. So this is across continuous variables or discrete
1717920	1724960	variables. And we'll explore this more with the authors, hopefully anything you want to add on
1724960	1734160	equation three. More that you know the information gain is a function of the data that you sample.
1734160	1738480	So depending on how you sample that data, you're going to get different information gain out of
1738560	1745360	it as you would expect. And then you start to get into the message passing, which is that base graph
1745360	1751200	and factor graph, I guess, challenge going forward that that construct when you build
1751200	1756720	it in a factor graph model, you have to be able to pass the messages between the nodes effectively.
1757680	1764640	Yeah, and to kind of ground that in the data science situation, if you have a latent
1764640	1770400	state estimate and you're generating data, generative AI, synthetic data, then the latent
1770400	1776800	variable is upstream, causally, statistically from the data pseudo observation. But that might be
1776800	1783040	the actual real observation if you're interested in the computer model. Whereas the data recognition
1783040	1789440	case where the data are upstream of the estimate of a parameter, like a risk score or something like
1789440	1795760	that, then the parents of the latent state estimate is the data. So that's the recognition
1795760	1801680	direction. So this kind of covers the whole Bayesian update possibility spectrum in this
1802640	1809120	essentially Markov blanket, but it could be in face space or time or a few other situations
1809120	1821840	they explore. All right, three, a worked example. This section lays out the overall pipeline for
1821840	1829040	how you get from the graphical notation of the Bayesian network, whether it's viewed visually
1829040	1836160	graphically, like with a variable dependency structure, or whether it's just written out
1836160	1842560	in terms of the plain text with the analytical, the Bayesian network is transformed into a factor
1842560	1849440	graph, probably constrained factor graph, discussion for another day. On that graph,
1849440	1859040	certain messages are passed at inference runtime. conditional and predictive entropies are calculated
1859040	1867520	as part of the way that this system outputs or is described by different probability distributions
1867520	1874000	understand in a kind of statistical mechanical way in terms of entropy. And then that is going
1874000	1880640	to come together into calculation of the objective function, which is the expected information gain,
1880640	1886800	which is basically conditioned upon the cognitive model of the sampler. So just because the sampling
1886800	1893360	is active data sampling, doesn't mean that it's going to lead to like an adaptive behavior.
1894320	1900240	It just means that where the learning rate is perceived to be highest informationally,
1900240	1908080	iteratively, there is a ranking by which those can be, which this the space of experiments
1908080	1916240	can be ranked by and it can connect to pragmatic value in terms of epistemic and pragmatic coming
1916240	1921280	together for the full expected free energy like in the clinical trial. Anything else?
1922240	1929120	And you'll notice in this story example, they are discussing here, the factors that they have in
1929120	1936400	their graph in each of their nodes is actually a cosine. So that's why you get that kind of
1936400	1945680	oscillation in that bottom plot there. So you'll have areas of maximal information and then areas
1945680	1950960	of minimal information just based on the toy example they have. And this doesn't always have
1950960	1956240	to be cosine, but in this example it is. And so it just kind of gives you a really good graphical
1956240	1966160	understanding of how your information gain can be viewed over a sinusoidal sort of oscillation.
1967440	1972480	Yeah, just to kind of double down on that, if you sample right here on the number line,
1972480	1979200	or right here, the lines are indistinguishable. So the information gain is expected to be low
1980080	1985520	under understanding the parameter family that is being generated and sampled from,
1985520	1992080	which in this first example is the same, same type of equations. Whereas where the functions are
1992080	1998880	maximally distinct, the information gain associated with reducing uncertainty about which one of those
1998960	2009680	five the data point is coming from, those are the informative points at the zero on the number line
2009680	2016880	and far out. That's where just perceiving one point uncolored would give you the most ability to
2016880	2021360	even perfectly resolve which one of the five situations it was.
2021920	2031280	So they're right. In effect, this model amplifies or attenuates the amplitude of the predicted
2031280	2037840	data, depending upon a periodic function of our data sampling policy pi. So here the policy
2037840	2043520	distribution is like that kind of around the clock direction, which is not a common setting,
2043520	2051680	but the general idea of sampling amongst a finite set of alternatives, where a control
2051680	2058800	variable is going to result in the most informative data point, is a theme that is going to be
2058800	2066720	expanded upon, and also one interesting mathematical move. Once all terms that are constant with
2066720	2073600	respect to pi are eliminated, we are left with equation six. So equation five comes down to
2073600	2082960	equation six, or maybe not exactly only five to six, but six has removed all the variables
2083680	2089680	that don't change as policy changes. So if the question of policy selection is taken alone,
2089680	2096160	like gradients on policy updating, then everything that's constant with respect to it
2096240	2102080	doesn't come into like the delta pi, delta something. So it just simplifies it down to
2102080	2107120	only a function of policy, and that just kind of reflects how like the sense making and belief
2107120	2113200	updating component is partitioned off from the policy selection component here.
2115520	2121840	Yeah, you're looking for change in your belief based on the observations that you're gained.
2121840	2126560	So if it's not changing, it's not as informative in your information model.
2128880	2135840	Yeah, continuing on equation six there, which is modeling the policy dependent
2135840	2142880	components of information gain as an objective function that ranks decisions about where to
2142880	2150560	sample. Equation six is a special case of the third row of table one, which highlights analytical
2150560	2156400	expressions for expected information gain for a few common model structures. As we might intuit,
2156400	2160400	the most informative place is to sample data aligned with those in which differences in data
2160400	2165360	lead to large differences in the predicted data, in which our choice of pi maximizes the sensitivity
2165360	2176000	with which y depends on data. So here are the categorical, the Dirichlet, and other functions
2176000	2182880	in terms of how they'd be written out in the probabilistic, like specifying a distribution
2182880	2191840	way, and then how there's this relationship analytically to a related distribution, which is
2192640	2200400	an objective function that ranks the information content of sampling the likelihood distribution
2200400	2207280	in a certain way. And that's closed form in certain situations. And then also they explore
2207280	2213280	where it's intractable formally. And so then that's where the variational approximation comes into
2213280	2223600	play. Anything to add? No, I think that summarizes this slide. Okay, section four, function approximation.
2224560	2230400	We next turn to a generic supervised learning problem, that of trying to approximate some
2230960	2234880	function based upon known inputs and the observable outcomes they stochastically
2234880	2243760	cause. Pretty general neural network or latent state observation setup.
2244000	2257680	That information is composed and concatenated. So that there's a common variable with that's
2257680	2262560	describing the statistical object that's going to be describing the inputs and the relationship
2262560	2273920	with the observable outcomes. And then that function approximation from sequential data
2273920	2280640	in figure three is simulated with random but potentially you could call all of them random.
2280640	2290400	But this one is a flatter or a less informed and iterated model of data sampling,
2291120	2296800	just going to show that samples of random data with even this minimal
2297760	2304720	non information gain driven model has a certain baseline prediction that's associated with
2304720	2312320	certain choices about sampling sequentially from this generative model. Want to add anything?
2313360	2319680	Yeah, it's just I like that they highlighted in this that choice diagram there that you can
2319680	2325440	actually get the inefficient sampling just by random that you start to you can randomly select
2325440	2332800	two things very close together and you've effectively maybe not wasted a choice but
2332800	2337600	you know not gotten the maximum gain from that choice that you could have.
2339440	2348400	Yeah, they write a little bit more about figure three. Figure three illustrates a depiction of
2348400	2353440	this model as a Bayesian network and a visual representation of the data generating process.
2356960	2362960	Now they're going to bring in information gain. So they write this is where information gain
2362960	2369760	becomes relevant into designing a more informed way to sample data than from a flat or a non
2369760	2374800	updating prior data sampling distribution. It's like equivalent to having a policy prior that's
2374800	2382080	fixed which might be a heuristic in certain space. They write substituting equation seven into
2382080	2393760	equation three. So here's that Markov blanket parent child concept and here equation three is
2393760	2400640	describing the policy dependence on the joint distribution of the observed and unobserved
2400640	2410640	and this is combined into equation 10. To show what equation 10 does in terms of now that we're
2410640	2419120	sampling from this distribution or like statistical distributions that this describes they'll
2419120	2423760	differentiate figure three from figure four kind of like bring in this model change between those
2423760	2428640	two figures. Now samples are drawn from a distribution whose log is proportional to the
2428640	2436640	information gain in equation 10. So it takes the flat policy prior and in a fixed way has remapped it
2436640	2445760	to be proportional to the information gain. So here's three on the right and then four on the right
2446880	2453360	and the figure uses the same format as figure three but now each choice is sampled to maximize
2453360	2460800	anticipated information gain and they point to some specific quantitative patterns but also
2460800	2467120	like qualitative patterns. So want to add anything on figure four? Just that now if you focus on those
2467120	2473120	choices because that I really did I think like that choice plot there you can see that the walks
2473120	2479920	around kind of choices around your data space are a little bit more distributed evenly distributed
2479920	2485680	a little bit less random but you start to get I think a more cohesive sampling of the data
2485680	2491440	without entire randomness putting things too close together putting selections too close together.
2494480	2502160	Yeah okay continuing on well they set up the question as this raises the question as to how
2502160	2508960	many samples should we collect. So within a foraging bout where should one look and then at the kind
2508960	2514480	of like pulling a layer back in the strategy when should you halt look it like if you have already
2514480	2520720	sampled all three records from a data set then unless you had some other reason you could fully
2520720	2527360	stop sampling there but you also might want to have a softer stopping criterion that would relate to
2527360	2532800	how much information you're gaining from continuing to sample in that way before like halting and so
2532800	2541280	they include that by having like an exit policy in the state space of foraging possibilities.
2542400	2547520	So how do you resolve that and answer this question can be drawn from behavioral neuroscience in the
2547520	2556560	so-called exploitation exploration dilemma and they introduced the notion of sampling costs
2557120	2564960	to help decide that. So this method is still going to require parameterization and situation
2564960	2570240	specific modeling of the relative costs versus the relative information gain however at least
2570240	2577440	there's an accounting that includes costs into the sampling equation to give any possibility of
2577440	2582880	exiting because if no costs are provided for sampling then the model might just converge
2582880	2588240	and continue to eke out very small amounts of variance explained if it doesn't explicitly
2588240	2596880	have that stop option so they take the policy vector the list of locations that can be sampled
2596880	2602480	from and adds a zero element which reflects the information gained if we were to stop sampling
2603440	2610160	and then there's a preference over those observations expressed in the c vector preferences
2611120	2616320	and this brings in the information seeking and the cost averse of imperatives into the same
2616320	2622960	objective function in 11. Anything to add on 11? Yeah eventually the idea is that it just gets to
2622960	2629760	a point where you're no longer whatever you set you're kind of stopping like energy to be
2631120	2635680	kind of breaking energy eventually it's just gonna get to a point where the model just says
2635680	2642240	hey I've reached what I can you've set this you're not gaining any anything beyond this point
2642240	2647520	we can just stop at this point which is nice since in the random selection case there's not
2647520	2653680	necessarily a stopping parameter as Daniel mentioned you could continue to get eke out
2653680	2659360	very very small marginal changes but you're not going to gain anything else and so you're just
2659360	2664960	spinning your wheels for no reason so this is a very elegant way of saying hey I've reached kind
2664960	2674720	of an inflection point of data gain I'm done. So figure five they're continuing in this genre of
2674720	2682160	three four five and now they've added in to the policy decision which which has an upstream
2682160	2688480	dependency on the data that's the active policy edge they add in this
2688880	2698400	cost to sampling so we can explore more however it now includes not just the information gain
2698400	2706880	driven choices within a trial but it includes a specific probabilistic but decisive stopping point
2707440	2713440	for that trial as parameterized by how sensitive it is to information gain and preference
2714000	2720080	so this is one of the most interesting parts and and discussions in the paper
2721040	2726800	they they ask it out loud a reasonable question to ask at this stage is why bother
2727360	2733040	with the full information seeking objective and basically how does this differ from maximum
2733040	2743120	entropy sampling and um let's look forward to the authors or other guests but here's just a
2743120	2748400	few notes on this because I think it'll be a great place to explore what it really means to do
2748400	2756400	statistical and physical modeling on cognitive systems they directly contrast maximum entropy
2756400	2764960	sampling and their whole information gain family against each other and then the rebuttal is in
2765520	2772080	figure six so just to show figure six for a second the measurement noise increases in variance from
2772080	2779440	the center of the function domain so the the variability profile of the function is non-uniform
2780080	2785040	this means this means the amount of unresolvable uncertainty is heterogeneous through the domain
2785040	2793280	of potential sampler so I in some kind of ways of thinking about what they're really getting at
2793280	2798400	and just putting this out as a speculation or starting point for for this key technical point
2799120	2806560	so if there were a case where the latent states were equivariants they had iid variability
2806560	2813200	profiles then sampling the most variable sensory data is the most informative like if
2813200	2820720	you're taking a picture of a solid black image then sampling from the noisy pixels is going to
2820720	2825680	potentially provide more information gain you're reducing uncertainty more about something
2825680	2832400	it might be overfitting but you can select as a heuristic wanting to sample from where variability
2832400	2840160	is high at just kind of a first pass layer however as we start to think about richer or
2840160	2849680	more specified statistical patterns generative models there become dependencies that are sparse
2849680	2856960	but important amongst all different kinds of things so things that are variable from a sensory
2856960	2865920	perspective provide high information gain potentially to one part of a generative model
2866800	2877520	like a screen and static but then other events might be less variable from a sensory perspective
2877520	2883600	but smaller differences even in that variable relate to some other component of uncertainty
2884320	2890720	resolution from some other component of the the model like those are going to be the cases where
2891680	2898640	cognitive modeling does differ from just dispersed decision-making however they're
2898640	2906800	both going to result in dispersed decision-making profiles like looking at the choices in the
2906800	2915520	figures but the choices to sample from the less ambiguous parts of the actual distribution
2916320	2925600	that leads to a much narrower policy path in this cognitive control setting versus in a variability
2925600	2932560	sampling where it would go for the areas that were just more variable but not necessarily
2932560	2935760	providing more information question mark
2939280	2941840	and you can add on this with the maximum entropy or anything
2943200	2950320	and I would even highlight kind of on the next slide it effectively what it is doing is accepting
2950320	2956640	that you're not going to gain a lot of resolution in these highly variable regions and so you don't
2956640	2962720	really have to sample into those deeply because you've accepted that it is variable it is not
2962720	2967840	something it is inherently variable in the data we're not going to gain a lot of information
2967840	2974080	from these regions and it's highlighted in blue down there and I think that's one of the big highlight
2974080	2981120	notes of this figure is this less information gain available in these highly available regions
2981120	2987760	and that's something that makes this method more robust and powerful when you're dealing with some
2987760	3000160	of these non-uniform variable data yeah awesome and then the the um streetlight effect is brought in
3000160	3005360	there so the avoidance of sampling in ambiguous locations is sometime referred to as a streetlight
3005360	3010480	effect the tendency to search where data are generated in a minimally ambiguous way i.e.
3010480	3018000	under streetlamp compared to searching elsewhere on a darkened street so I made some GPT-40 images
3018800	3025520	some fun streetlight and on one hand there's kind of this sense of like is it constraining to look
3025520	3030640	under only the streetlight isn't that kind of absurd and then there's the joke about how what
3030640	3035040	the person's looking for is elsewhere but they're still searching under the streetlight
3035040	3038560	but they're looking for something they they know is elsewhere so that's the kind of
3038560	3044480	tragic element of it then there's this limited element however there's also this realistic element
3044480	3051120	which is like well are you supposed to search where you can't sense or outside of where you
3051120	3058880	are at that moment so how could you you know say that that wasn't just and then this paper is more
3058880	3067280	framing it as just a general condition of perception like you're in your tactile streetlight
3067360	3074640	that is the part you can see at all you can have latent modeling of any and other things
3074640	3080960	but if it's not grounded in some way to a measurement made in a streetlight under the
3080960	3086080	metaphor where the light allows for observation then you're not connected to data unless you're
3086080	3091200	connected to that streetlight so that's just a very interesting kind of topic and and reference
3091200	3096160	that the authors use what do you think absolutely I mean it kind of boils down to you can't
3096160	3101520	know what you don't know what you can't observe you know if you can only observe what's underneath
3101520	3106240	the streetlight then you can't really know what else is outside of there and so your inference
3106240	3111360	necessarily should be constrained to what you're able to actually observe you can't observe the
3111360	3117440	unknown and so not necessarily in this case um because you don't even know if it even exists
3117440	3125280	you have no data to confirm or to refute it all right section five dynamic processes so
3126080	3130720	in that previous example there was a data selection challenge whether it was approached
3130720	3136400	from the flat fixed prior or all these other kind of subsequent variants with the adaptivity
3136400	3142320	and or with the cost now there's going to be a time element brought into the underlying
3142320	3146960	generative model we'll just go quickly here because that's the big point they take the static
3146960	3153120	distribution that was sampled from and now give the underlying process also variability
3153120	3160800	through time so this is like a very SPM brain latent state causal modeling type set yeah anything
3160800	3166880	you want to say on that before we go in oh no go ahead okay okay so they consider processes that
3166880	3172720	evolve in time equation 12 can be interpreted similarly to equation eight in which the expectation
3172720	3177680	of the data is treated as a function approximation which now includes a time argument so here was eight
3178320	3185040	expectation of the data given latent state parameterization and policy equals so on
3185840	3192880	and then here there is data also being a function of parameterization and policy
3192880	3200480	and then also bringing in an element with a subscript tau for time uh then you mentioned in
3200480	3208160	your big questions the different approaches that they raise here with the three ways to
3208960	3213440	bring temporalities into a model so let's definitely talk about that but just to show
3213440	3221920	their images seven and eight are the pair for this dynamical section so figure seven shows a
3221920	3226560	graphical representation of the matrices involved in generating our data and the inferences obtained
3226560	3234720	after sampling so here it is sampling from a time variance function and then figure eight goes into
3234720	3241280	more detail and notes predictions based upon current data can be used to inform predictions
3241280	3246400	about nearby spatial locations and to predict and post it the values of the function at different
3246400	3255520	points in time so just like you could have a 2d plane grayscale and infer the location of the
3255520	3260320	streetlight by pursuing like a gradient up the light and then there would be this optimal
3260320	3265760	sampling like if you just got one observation you would want to sample on a line that was
3265760	3270800	orthogonal to the one that you couldn't resolve lots of ways to think about this sequential
3271440	3275120	prediction but now the underlying landscape also changes so there's some
3276000	3280880	temporal dynamics and then that can be fit with all these different time series models and
3281600	3287200	autocorrelation and so on however that's specified statistically in the generative model
3287200	3291840	but this section just shows however you do make a statistical model for time
3293360	3299760	it's basically going to be the same thing where information is going to be drawn from a distribution
3299760	3306160	and now time is a variable in that distribution uh they write in this in the previous section we
3306160	3310400	have demonstrated the way in which smarter optimal sampling may be used to select data
3310400	3315440	in a manner that balances the cost of sampling or performing further experiments against the
3315440	3320560	information gained from those samples or experiments each of these examples has relied
3320560	3325680	upon relatively simple and analytically comfortable linear Gaussian systems next we address active
3325680	3332640	sampling in a situation where analytical solutions are no longer possible so to highlight the key
3333600	3340320	formalisms that they're working with in that kind of background section uh or setup section
3341680	3347760	they kept one thing constant which was that the the generative model the generative process or
3347760	3354720	however it's considered with the family of equations that the agent is inferring and tracking hidden
3354720	3360080	states with and that being the same as the actual family of equations that's generating
3360720	3369440	the function of observations and here that is relaxed so that opens it up to all empirical
3369440	3375920	settings where you can just say right off the bat we do not have access to the generative model of
3375920	3380800	those data so we're making a map statistical map with all the associated trade-offs and
3380800	3388400	statuses of like that genetic data or that transcriptomic data all those different kinds
3388400	3394960	of data sets starting from a position where it's going to have to be statistically approximated
3395760	3401680	and it isn't going to be based unless explicitly otherwise on actual knowledge about the causal
3401680	3409600	elements of the system any any thoughts on that well in something else that they noted in the paper
3409600	3416240	by adding this time element when they're actually going through the time series the model itself will
3417040	3424400	preferentially select different data beyond what it just recently selected so time point one it
3424400	3431120	selects x and y data time point two it might select l and m data so it actually will go through and
3431120	3437920	select different types of data and it'll take a little bit of time um what the time is variable
3437920	3443920	but it'll take a little bit of time before it revisits some of that previous data um at a
3444000	3448560	previous time so by this you kind of have a sliding window of data that you're selecting
3448560	3457680	over different time periods yeah all right that's all going to come to play in this clinical trial
3458320	3465200	which is the big final contribution section of the paper in our final example we demonstrate the
3465200	3471600	potential utility of the ideas outlined above in the context of a more concrete example so
3471600	3479840	they model the statistical setting here as an adaptive Bayesian clinical intervention methodology
3479840	3487280	experiment for example the kind that was done during the 2014 West African Ebola outbreak
3488240	3493120	the active sampling approach advocated in this paper offers two main opportunities to augment
3493120	3500000	adaptive trial designs first it allows us to adapt the design to maximize the information we obtain
3500000	3506240	about treatment efficacy so that's the pure sense making information gain learning sampling from
3506240	3512320	where it's informative not from where like we habitually or prefer to look and then second
3513120	3521440	to balance and bring together that information gain with costs and that was brought in with
3521440	3528640	the cost of the sampling section which was done in this paper by adding the stop policy option
3528640	3534960	which can be probabilistically selected and then as other sampling locations become less
3534960	3539920	informative or if somebody was just sampled and you know that there's a slow decay through time
3539920	3549440	then on that subject the stop policy cost would outweigh the information gain from an experiment
3550640	3557440	and this is also I think will be a very interesting discussion this blows the line between clinical
3557440	3562640	trial and public health intervention and can be seen as analogous to animal behavior that is never
3562640	3569120	fully exploitive or explorative but is a balance between the two so how do we think about that
3569120	3575760	in terms of biomedical and health security and all these different topics and any thoughts on this
3575760	3580800	before we go into the formalism of the clinical trial just like about clinical trials or anything
3580800	3587200	yeah and I think that that's going to be like that last point there is going to be a big one
3587200	3592960	going forward of like how do you balance benefits to the patient benefits to your trial benefits to
3594400	3598960	essentially the company like there's a lot of different benefits and costs that you have to
3598960	3605760	weigh into this and so these models are going to get very complicated when you start to distill
3605840	3612480	this into something especially with health related so it'll be very interesting to see how this evolves
3615200	3622240	okay so here's how they do it our setup is as follows for each new cohort of participants
3622240	3630640	we decide upon the randomization ratio to adopt that's the orange subscript r of policy so this
3630640	3638480	is policy on a randomization ratio there's three options so this is a discrete but linearly ranked
3638480	3647600	not fully categorical policy decision where one half would be the 50-50 sampling between the two
3647600	3657920	groups whereas you know a priori that sampling in a skewed ratio is going to be less informative
3657920	3662480	like if you sampled only from one you would obviously be maximally uninformed about the other
3662480	3670320	however what's going to end up being reflected in the policy decision to shift to a one-third or
3670320	3676880	a two-third which is focusing observations on one branch of the study more than the other
3676880	3682720	is going to focus on the explicit quantitative preference for observations of survival
3683520	3689440	so that's going to be very interesting to see how the time variable which relates to the
3689440	3698480	experimental design but by way of modeling the death curves of the participants and how different
3698480	3705440	preferences for complementary processes of reducing uncertainty about the treatment specific
3705440	3712480	death curves and not preferring to see death observations because that would introduce the
3712480	3719680	pragmatic imperative to measure low survival experiments so there's a lot of complexity
3719680	3727120	in there from the public health side also in this very simple and interpretable way that like this
3727120	3734240	is like a Bayesian light switch with 50-50 information seeking mode or tilt it one way or
3734240	3741360	the other to bias observations whereas if no information had to be resolved then the policy
3741360	3748480	selection would orient towards observing long survival whereas if that was somehow changed
3748480	3752960	then it would have to be adaptively sampled on the fly and changing these ratios and all that
3752960	3757360	what do you what do you think about this yeah and what we're going to kind of get into is
3759120	3764400	especially with these sorts of health decisions you want people to survive like that's your
3764400	3769200	that's your primary goal in a lot of these you want to see an effect you want to see a positive
3769200	3774560	effect of your treatment one way or the other you know if it's the placebo that's the positive or
3774560	3779600	it's the actual treatment that's the positive you want people to survive so this is kind of
3779600	3785760	getting into that ethics of making sure that when you design these things that you're doing the
3785760	3793600	maximum good to your participants who you know may not have you know much hope to stand on
3794560	3800400	doing some of these crises or epidemics or whatever they are experiencing at the time
3802160	3807840	so you want to design this in such a way that you know you keep them the patients in mind
3807840	3815920	that is the whole point of this and so by having a Bayesian kind of preference and bias to keeping
3815920	3823120	the patient alive and the best outcome you're maximizing how the patient's outcome in the
3823120	3830000	patient's life thanks for adding that another point to make this is from figure nine that's
3830000	3836960	gonna come up but it really highlights how sparse and few and interpretable the Bayesian
3836960	3842240	graphical formalism is and message passing which a lot of the equations describe and
3842240	3848960	the discussion about rx and fur touched upon message passing gives procedural ways to implement this
3849920	3855200	in computational systems because it's sometimes hard to go from the simplicity of like this
3855200	3862960	graphical model to fitting it iteratively on complex data sets but it's pretty clear to see
3863040	3869520	how different variables are upstream or downstream of other variables and also how the time
3871440	3879120	sampling can be shown to be which is the upstream of data sampled as these other factors are
3880080	3889440	but it has a separable interpretable calculable epistemic value that doesn't have a certain kind
3889440	3895440	of connection to randomization ratio for example so being able to have explicit statistical
3895440	3903280	calculations and directnesses where the follow-up time doesn't influence the treatment group ratio
3903280	3910320	or the randomization ratio or other processes gives a type of interpretability that the
3910320	3916320	generative model gives us the equations for and then the pragmatic challenges are about actually
3916320	3921440	implementing that and then even if the computational component were totally addressed and abstracted
3921440	3927520	away that would basically center these broader questions which i think the health example is
3927520	3934160	a great like jumping off point four yeah and uh you have probably recalled from all the other
3934160	3941520	different uh figures despite how simple this figure is the other figures the the plots were very
3942000	3947920	basic the models themselves like you just had the three nodes you know converging on the y so
3947920	3955040	despite how simple this looks you are adding more complexity to these um to these systems and the more
3956080	3962080	complexity that you add the harder it is the more computationally intensive it is and so this is
3962080	3968560	that question of how big can you go you know how how many nodes can you add how many parameters
3968560	3973360	can you add how much complexity can you add to the system before it starts to break down
3973360	3981360	or not perform as well as you would hope yeah so other than bringing in that randomization ratio
3981360	3989200	kind of expression of preference this model differs from the prior one in that it's defined
3989200	3994240	that the kind of cognitive map is not the territory they're different families so that's
3994240	4000880	what motivates this um approximation approach so this is a simple displacement where still it's a
4000880	4009760	trackable problem as they'll unfold however the simulation family chosen for the for like the
4009760	4014320	approximation basically the approximation could apply to any data set but it might be woefully
4014320	4020880	inadequate like it might fit only one component of it so that's again part of the interesting
4020880	4028880	question is like how similar does the statistical model have to be or what information does it really
4028880	4037200	bring in and how to to model or work with an empirical side um but just on a more general
4037200	4045040	statistical level equations 15 16 17 describe some of the technical details of the incremental
4045040	4051440	optimization gradient scheme the newton optimization variational plus we'll talk to
4051440	4059920	thomas et al figure nine is displaying the kind of before picture for the randomized control
4059920	4067120	trial so here's where that graphical model is that was shown earlier and then here are these two
4067840	4074640	groups and their survival through time and different sampling uh choices that are made
4076000	4084000	then just to to jump to figure 10 has the same layout as figure nine but now using the expected
4084000	4089520	information gain from equation 18 to guide sampling of data so this is just to show the
4089520	4095200	impact of that active data sampling and it will drop back to the equation uh there are some notable
4095200	4104720	differences between the choices made in figure 10 compared with nine nine choices 10 uh the most
4104800	4110480	obvious of these is that the follow-up time selected have been moved later once optimal sampling is
4110480	4116000	employed this makes intuitive sense as a later follow-up time is informative about the survival
4116000	4120720	probabilities at all prior times whereas an earlier follow-up time is not informative about
4120720	4134240	survival probabilities at later time um where it gets in the final uh simulation brings in the
4134240	4143120	random sampling plus the preference element here's where the symmetry is broken to also want the
4143120	4148160	measurement of survival as more likely than death which is how the preferences are specified
4148720	4158320	in active inference um and then the policy switch is reflected in this like um part where
4158320	4164960	the observations are shifted later because there's there's less than a threshold of information to
4164960	4171840	gain by having them earlier and then they uh even if there is equal variance i'm not exactly sure we
4171840	4178960	can ask between the two branches there is uh an over sampling for the group with the higher
4178960	4186160	survival which in this case was the placebo group so anything to add on this preference element
4186160	4193440	this is the the like the crux of the whole making sure that you optimize you know the patient's
4193440	4199040	outcome on this because the treatments in this scenario were not um were not beneficial they're
4199040	4204960	actually harmful and so throughout the course of the study this by utilizing this model you
4204960	4211840	actually randomized more people into the placebo group which caused a greater survival of these
4211840	4218640	uh individuals and so you're you can already see the effect that this sort of methodology has on
4218640	4224720	clinical trials because you're optimizing the outcome and i think that is exactly what you want
4224720	4231440	to do in these health decisions in these trials in these things that impact human health uh or
4231440	4238480	humanity in general you want to optimize the outcome uh and you know in this way you're actually
4238480	4248320	reducing the overall harm to patients yeah interesting um here's the specification for the
4248320	4260400	information gain so uh bringing in the the form of the message is required for equation three now
4260400	4266720	with all these extra components that have been added in with time variability demographic and the
4266720	4275760	sampling they write out some of the technical details for the approximations in the variational
4275760	4283680	Laplace and then some aspects about those models which we can ask about but figure 11 basically
4283680	4292640	shows the big change which is that uh as you go from having a a flat sampling distribution
4293920	4300640	across time and across treatment groups you can actually do better than that basically by choosing
4300640	4309120	a sampling regime that makes sense given the costs of sampling so that's just very interesting
4309120	4314560	because it it really does look like given the the possibility for these two lines to diverge
4315440	4321760	their divergence would be largest later on whereas if you could only schedule like one
4322320	4327440	check for every person if it was something that was expected to happen later in life
4327440	4336160	then sampling all the young wouldn't even make sense so then one approach is like flat
4336160	4343280	sampling but that is kind of sometimes erroneously called uh like unbiased or or uninformative but
4343280	4349440	it is very informative and then this is pointing towards how there can be better sampling than
4349440	4357840	just trying to go flat across the entire latent state estimate if there are priors relating to
4357840	4364400	something they can be leveraged as part of the probabilistic sampling and adapted to the
4364400	4372240	data set at the in the end however for picking prior families for the active data selection
4372480	4379760	that's a big question about how much it will change how the algorithms work so I guess that kind of takes us to the
4381120	4387280	discussion the paper's focus has been on illustrating how we might make use of
4387280	4392960	information seeking objectives augmented with costs which gave kind of the exit criterion or
4392960	4399440	preferences which gives the biased pragmatic part of data selection to choose the best data to
4400400	4408800	optimize our inferences and they highlight that the maximum entropy would yield identical results
4409680	4414400	in several of our examples so that'll be interesting like what were the examples where
4414400	4421360	maximum entropy and the information gain are identical and then what are the real world
4421360	4427520	or the statistical settings when the variance around predicted outcomes is in homogeneous
4428480	4439040	how does the full cognitive epistemic model based objective do differently than the max and
4440640	4446480	distribution dispersal kind of null hypothesis any thoughts on this
4448640	4455440	no I think you've captured it very well okay then there are several technical points worth
4455440	4459760	considering for how we might advance the concepts reviewed in this paper so let's talk about each
4459760	4466800	of these one refinement of the active selection process two empirical evaluation of active versus
4466800	4473520	alternative sampling methods and three identifying the appropriate cost functions so we'll talk about
4473520	4482320	those coming up conclusion here's the entire conclusion the key ideas involved in appeal
4482320	4488080	to foveal like sampling of small portions of the total available data to minimize computational cost
4488720	4496080	that's a very cool way to put it and it highlights that kind of like sequential scanning but also
4496080	4503200	opens up some very exciting directions about how efficient that could be for some but not
4503200	4508560	other kinds of problems and how those problems could be identified or those patterns could be
4508560	4515040	filtered for to where different kinds of succating models would be adaptive or not so like these are
4515040	4521680	all fun discussions we'll have and then they kind of brought all the theoretical components together
4521680	4528880	at the end with the Bayes adaptive clinical trial with the cost of sampling constraints on
4528880	4537600	sampling and also the preference for survival so what are your overall thoughts or what are you
4537600	4543840	excited about for the ones to come I'm excited to kind of see me I mean this is a fairly recent
4543840	4549280	paper I'm excited to see kind of where they have gone since this paper was published you know they
4549280	4556080	had a number of kind of next directions I would love to see what of those directions have they've
4556080	4564880	taken what they've compared it to other other sampling techniques this show shows a lot of promise
4564880	4572640	going forward for very complex and you know ethical situations or situations where ethics
4572640	4579760	are going to be a huge component so kind of where that is what they're going into and kind of where
4579760	4586160	they see you know further improvements I still kind of want to know what would happen or what
4586160	4593440	of these other time metrics or what are the situations these time metrics would or alternative
4593440	4600800	time models would be applicable to or if this really is like the de facto just the way it needs
4600800	4606800	to be done totally totally fair I think that could very well be the case I would just love to hear
4606800	4612480	exactly you know if you did a hidden Markov model how would that look you know is is there a benefit
4612480	4618880	of that over the selection that they have is there does it provide more or less versatility in the
4619120	4624080	models these are things that are going to be I think very interesting going forward and then
4625280	4631680	you know how do we like the like you noted in the discussion how do you optimize those cost
4631680	4636880	functions what's what's a cost you know you're dealing with clinical trials it's a human life
4636880	4642960	that's a cost time it's a cost there's also just computer time how much you can actually get
4643520	4649520	how much compute you can get and give them all the time some of these machine learning models
4649520	4655440	that you might want to apply this to or you know select data going into sometimes takes a long
4655440	4662800	time to train how are you going to sample data that's going into those models how are you going
4662800	4670960	to continually feed those models appropriate data going forward so this is a huge broad category of
4671840	4679200	um directions that you can go clinical trials was a very wonderful example of a complex situation
4679200	4686240	that is you know right there applicable to human life um but then you also have just data science
4686240	4691840	in general like how are you going to utilize this to you know going more broad how are you
4691840	4697200	going to utilize this just going forward in any data sense data is growing it'll continue to grow
4697200	4702560	it will never really stop growing so it's going to be more and more important going forward to
4702560	4711920	have these methods more broadly used and random's nice random's really really good but this holds
4711920	4717920	a lot of promise to being I would love to just hear their thoughts on where that's going where
4717920	4727520	they see that yeah a lot of a lot of interesting directions a few things that made me think of
4727520	4733040	one was about search and about relational search concepts page rank and everything
4733040	4740480	syntactic semantic new kinds of search algorithms and personalization for search and learning and
4740480	4746320	updating and to what extent like explicit cognitive modeling would change the way that
4746320	4753200	different recommendation algorithms or different kinds of computer systems would work I'll read
4753200	4759120	a question um and then any any other questions otherwise this is our our last slide so thank
4759120	4767440	you Christopher um okay glia maximalist wrote interesting point about biased and unbiased
4767440	4772400	sampling schemes perhaps this points out the fact that unbiased approaches are the wrong
4772400	4779760	thing to strive for in research study design what do you think about that unbiased research
4779760	4785920	in study design um I think there's a time and place for unbiased and I think there's a time
4785920	4792400	and place for bias I think that but you when you accept a bias into your model or refute
4792400	4798800	take bias out of your model you need to understand why you're doing certain bias you don't want to
4798800	4804560	have you know researcher bias is something that's a huge bias that you want to not have for example
4806080	4811200	but in certain cases um like in this paper where you actually do want to have a bias
4811920	4816720	you want to make an intelligent decision know why you're making that decision call it out and then
4816720	4822800	build it into your model that would be my problem yeah that's interesting like there are certain
4822800	4827920	statistical distributions the bias or the constraints on which to find the research study
4828000	4834880	whereas other ones that can have an explicitly strictly negative like data loss or something
4834880	4840960	but then the trade-offs of how that distribution actually interacts with others can enter into
4840960	4847200	this more complex experimental calculus that relates to like well all these different
4847200	4855200	experimental factors and so the optimal experiment for for different labs or different
4855280	4860080	moments for the lab could look extremely different and that's going to be the case
4860720	4863840	there's going to be just first behavior out of the way but then the question is how is that actually
4863840	4872880	driven in a way that is doing better than drawing from distributions however even that does interestingly
4872880	4882400	well for the right variables and you can mind bias all over the place I bias in data design
4882400	4888160	in experiments is can be very useful so it'll just be interesting I think it'll be case by case
4888160	4889040	it's the way I see it
4895920	4905120	okay well do you have any last comments I think that the main thing here is I'm just very excited
4905120	4911040	to see this paper come out I would love to see how this is going to evolve over time
4911040	4917920	see if this can be applied to different technologies different areas I'm really excited
4917920	4924320	just to see where this is going because I think this is just right on the cusp of what's needed
4926160	4934720	awesome thank you okay we will look forward to it thank you see you all right thanks guys
4941040	4942320	you
