1
00:00:00,000 --> 00:00:18,840
Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024

2
00:00:18,840 --> 00:00:24,800
on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre

3
00:00:24,800 --> 00:00:32,040
and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you

4
00:00:32,040 --> 00:00:35,680
for introductions and to take us through the paper.

5
00:00:35,680 --> 00:00:36,200
Oh, thank you.

6
00:00:36,200 --> 00:00:37,200
And welcome everyone.

7
00:00:37,200 --> 00:00:45,000
This is Active Inference Gas Stream 84.1 on July 22nd, 2024.

8
00:00:45,000 --> 00:00:45,800
Thank you.

9
00:00:45,800 --> 00:00:49,760
Okay, thank you guys. Go for it.

10
00:00:49,760 --> 00:00:51,920
Hi, thanks for having us.

11
00:00:51,920 --> 00:01:00,040
So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles?

12
00:01:00,040 --> 00:01:16,640
I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute.

13
00:01:16,680 --> 00:01:19,840
So, should we go through the paper briefly?

14
00:01:19,840 --> 00:01:23,880
So, yeah, we wrote this paper together.

15
00:01:23,880 --> 00:01:29,920
Actually, we started by writing a general audience piece that was published in the box online.

16
00:01:29,920 --> 00:01:31,680
Tell us when was that?

17
00:01:31,680 --> 00:01:33,640
A few months ago, I guess.

18
00:01:33,640 --> 00:01:37,520
Yeah, I think it was close to a year ago, maybe.

19
00:01:37,520 --> 00:01:42,000
I don't think it was published a year ago. I think it was published in 2024.

20
00:01:42,000 --> 00:01:45,240
But yeah, we worked on it for a while.

21
00:01:45,240 --> 00:01:53,560
Yeah, so this piece was doing, I guess, two things, that piece that we published in the box.

22
00:01:53,560 --> 00:01:57,640
It was pushing back against what we call the all-or-nothing principle,

23
00:01:57,640 --> 00:02:03,200
which we defined as the idea that either something has a mind or it doesn't.

24
00:02:03,200 --> 00:02:11,840
So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.

25
00:02:11,880 --> 00:02:16,840
And we argued that this was not the best framing to think of,

26
00:02:16,840 --> 00:02:22,960
especially, and family, our systems that seem to have sophisticated behavioral capacities,

27
00:02:22,960 --> 00:02:26,280
like large language models or AIC systems in general,

28
00:02:26,280 --> 00:02:30,400
where we don't want to take various cognitive capacities of the package

29
00:02:30,400 --> 00:02:34,160
and package them into this idea of a mind, where either you have the mind or you have them,

30
00:02:34,160 --> 00:02:39,920
then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc.

31
00:02:39,920 --> 00:02:43,840
planning, memory, theory of minds.

32
00:02:43,840 --> 00:02:49,280
So, we thought, as a remedy to this kind of all-or-nothing approach,

33
00:02:49,280 --> 00:02:53,840
we argued for what we call the divide and conquer strategy

34
00:02:53,840 --> 00:02:57,040
when studying the cognitive capacities of these systems,

35
00:02:57,040 --> 00:03:02,480
which involved looking at these capacities on a piecemeal basis, case-by-case,

36
00:03:02,480 --> 00:03:06,080
with an open-minded empirical approach.

37
00:03:07,040 --> 00:03:13,040
Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds.

38
00:03:13,040 --> 00:03:23,760
Yeah, I mean, we made one point in there about why it is that people feel so torn about

39
00:03:23,760 --> 00:03:31,600
reactions to large language models, and we said a little bit about the psychology of essentialism,

40
00:03:31,680 --> 00:03:38,960
which is the idea that we naturally categorize especially living things

41
00:03:39,920 --> 00:03:46,320
with respect to a presumed essence. So, we gave an example of an oak tree, I think,

42
00:03:47,920 --> 00:03:54,880
and we said that what people tend to think as they grow up and learn about the natural world is that

43
00:03:55,600 --> 00:04:05,280
an oak tree remains an oak tree regardless of changes to its observable properties,

44
00:04:06,080 --> 00:04:15,200
and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has,

45
00:04:15,200 --> 00:04:20,960
and there's some experimental psychology and developmental psychology showing that we

46
00:04:21,920 --> 00:04:31,360
have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative

47
00:04:31,360 --> 00:04:36,640
explanation for why the literature on large language models is so torn, and some people

48
00:04:36,640 --> 00:04:45,760
are quite dismissive, and other people think that it's a step away from AGI. It's that if you

49
00:04:46,720 --> 00:04:53,840
feel like you've got to put large language models into one of two boxes,

50
00:04:54,960 --> 00:05:03,600
the box that has the essence of mindedness for the box that lacks it, then you will be forced

51
00:05:03,600 --> 00:05:10,240
either to say it doesn't have what it takes to do any of the things that we associate with

52
00:05:10,240 --> 00:05:18,640
having a mind such as reasoning, or it has the essential characteristics of mindedness,

53
00:05:18,640 --> 00:05:23,360
and therefore we should expect it to have all of the other properties we associate with

54
00:05:23,360 --> 00:05:29,920
mindedness as well such as consciousness or understanding or whatever.

55
00:05:32,480 --> 00:05:39,360
Right, I think it's worth emphasizing as you did that the background motivation for starting to

56
00:05:39,360 --> 00:05:46,080
write on this general piece in the first place is indeed that the general discourse on AI systems

57
00:05:46,080 --> 00:05:54,080
and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so

58
00:05:54,080 --> 00:06:00,160
you have one ahead people and one ahead people arguing that these systems are no more than

59
00:06:01,120 --> 00:06:06,640
so-called stochastic parrots that are haphazardly stitching together samples from the training data

60
00:06:06,640 --> 00:06:12,880
and regurgitating them, or that they are no smarter than toaster or that they only do next

61
00:06:12,880 --> 00:06:20,560
stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form

62
00:06:20,560 --> 00:06:25,840
of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum,

63
00:06:25,840 --> 00:06:30,080
on the other end of the spectrum you have people arguing that the systems are haphazardly

64
00:06:30,080 --> 00:06:35,120
jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to

65
00:06:36,160 --> 00:06:42,480
parrots, it's literally the title of a paper by Microsoft on 24,

66
00:06:44,960 --> 00:06:50,800
and many people hyping up the capacity of the systems in a way that might seem very speculative

67
00:06:50,800 --> 00:06:56,960
and untethered from actual empirical results, so there is this huge gap between these two positions

68
00:06:56,960 --> 00:07:02,000
and there's going to be a very rich and complex and nuanced middle ground that is underexplored,

69
00:07:04,080 --> 00:07:10,640
or perhaps I think we did make that point, if not explicitly in the published piece

70
00:07:10,640 --> 00:07:15,920
and in some draft, that there's something reassuring about being able to make definitive

71
00:07:15,920 --> 00:07:22,240
claims about what these systems are and what they do, so either they're re-unsophisticated or they're

72
00:07:22,960 --> 00:07:31,760
very much like us and either of these claims kind of meet somewhere in an way in saying that we

73
00:07:31,760 --> 00:07:36,960
have a clear idea of what the systems do and what they are, and I think it's a little more

74
00:07:36,960 --> 00:07:46,320
epistemic and comfortable to say we have to study them empirically and find out what they

75
00:07:46,320 --> 00:07:52,080
can or cannot do and why and what are the underlying mechanisms, and we simply don't know

76
00:07:52,800 --> 00:07:58,800
a priori just by looking at the architecture, the learning objective, the training data,

77
00:07:59,440 --> 00:08:04,480
these sources of evidence are insufficient to make the definitive claims about what these systems

78
00:08:04,480 --> 00:08:11,280
are capable of, so I think that's part of the big part of the motivation and that fits into that

79
00:08:12,080 --> 00:08:20,320
more academic paper as well. Yeah, one other small side note which we don't make

80
00:08:20,320 --> 00:08:26,640
in the paper but I think might be relevant, especially for people working in philosophy,

81
00:08:28,080 --> 00:08:33,760
LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which

82
00:08:33,760 --> 00:08:41,840
is fitting, not only because they're so new and different but also because they are artifacts,

83
00:08:41,840 --> 00:08:46,080
right, they're things that humans have constructed and engineered and

84
00:08:48,880 --> 00:08:54,800
we don't have a thorough understanding of how they work, I mean mechanistic interpretability and

85
00:08:54,800 --> 00:09:03,120
various behavioral research is helping us improve our understanding but on the whole

86
00:09:03,120 --> 00:09:06,560
our understanding is not nearly as deep as hopefully it one day will be

87
00:09:08,240 --> 00:09:13,520
and this is by itself a really strange situation to be in that we've constructed an artifact that

88
00:09:13,520 --> 00:09:24,960
we only partially understand in the in the past artificial intelligence was seen as a way of

89
00:09:26,800 --> 00:09:30,160
constructing something like an epistemic assistant, right, something that will

90
00:09:31,120 --> 00:09:37,600
help us but not something that will kind of alienate us from the process of coming to know

91
00:09:37,600 --> 00:09:44,400
about the world so I think there's an extra layer of discomfort built into thinking about

92
00:09:44,400 --> 00:09:54,640
large language models and that may also play into the the divisiveness of debates about what they

93
00:09:54,640 --> 00:10:03,200
can do and just to add to that I guess we should be clear that this does not entail in any way that

94
00:10:03,200 --> 00:10:10,800
we think the systems are so completely alien and beyond the reach of our current understanding that

95
00:10:10,800 --> 00:10:17,680
anything goes and that they could very well be you know have like human-like intelligence or

96
00:10:17,680 --> 00:10:21,840
superhuman intelligence and we simply cannot say whether or not they do or because that's

97
00:10:21,920 --> 00:10:28,080
sometimes what you see in some outfits where people frame these systems as noble alien forms of

98
00:10:28,080 --> 00:10:32,960
intelligence that we have created but do not understand our control and that is you know

99
00:10:32,960 --> 00:10:38,320
as a slippery slope that leads some people to then claim that they have all these quite magical

100
00:10:39,040 --> 00:10:43,120
abilities and that's not all what we want to say here and in fact we want to resist

101
00:10:44,080 --> 00:10:48,800
yeah yeah yeah so if we we think that that's just as much of a cup out as

102
00:10:50,720 --> 00:10:56,640
completely dismissing a priori what these systems might be capable of without doing the

103
00:10:56,640 --> 00:11:00,880
work of looking into the capacities with behavioral and mechanistic studies so

104
00:11:00,880 --> 00:11:04,640
we very much want to resist both extremes of the spectrum if that makes sense

105
00:11:06,720 --> 00:11:10,240
okay so now should we move towards the content of the current paper

106
00:11:10,240 --> 00:11:17,040
sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism

107
00:11:17,040 --> 00:11:27,280
and anthropocentrism and then you can take the next step so everyone is aware of the problem of

108
00:11:27,920 --> 00:11:33,440
anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting

109
00:11:33,440 --> 00:11:42,720
human qualities onto something non-human and it's quite easy to especially when you're having a

110
00:11:42,720 --> 00:11:48,880
productive successful exchange with a large language model it's easy to slip into this

111
00:11:49,920 --> 00:11:58,160
interpretive mode where you reason about the responses of the large language model

112
00:11:58,800 --> 00:12:07,680
as if they were coming from an agent just like you and maybe that's a useful thing to do in some

113
00:12:07,680 --> 00:12:13,840
circumstances but from a theoretical perspective it's certainly a mistake because large language

114
00:12:13,840 --> 00:12:23,120
model is radically unlike you know a human agent in all sorts of ways but that's only one form of

115
00:12:23,120 --> 00:12:29,520
sort of human-centric bias the other one is anthropocentrism or what we call in that box

116
00:12:29,520 --> 00:12:37,520
article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that

117
00:12:38,960 --> 00:12:46,160
the human way of solving problems is the gold standard of solving problems generally so that

118
00:12:47,040 --> 00:12:53,840
to the extent that a system solves a problem in a way that diverges from the human strategy

119
00:12:54,800 --> 00:13:07,120
it's only using a trick or a faux solution it's it's not using a deep general rational strategy

120
00:13:07,440 --> 00:13:21,280
and in the debate about what large language models can do we think that the anthropomorphic

121
00:13:21,280 --> 00:13:26,240
bias is pretty well recognized and the anthropocentric bias is not so well recognized

122
00:13:26,240 --> 00:13:34,080
and so part of this paper is is or the main idea behind the paper is to present a systematic

123
00:13:34,080 --> 00:13:39,600
analysis of anthropocentric bias how it comes about and how to push back against it

124
00:13:41,600 --> 00:13:46,000
right and we we want to be very clear and hopefully we're playing the paper that

125
00:13:46,560 --> 00:13:51,840
the reason why we focus on anthropocentric bias here is just because it is I think as

126
00:13:51,840 --> 00:13:59,840
Charles mentioned less discussed and less recognized or some forms of it are less

127
00:13:59,920 --> 00:14:04,960
recognized and we make we propose this new taxonomy but it's not at all suggest that it's

128
00:14:06,880 --> 00:14:13,280
more problematic or more important than the anthropomorphic bias that's well discussed in

129
00:14:13,280 --> 00:14:23,040
the literature of the anthropomorphic biases so in other words this is not you know to frame things

130
00:14:23,040 --> 00:14:27,920
in in this slightly problematic dichotomous way of thinking that's common in the discourse

131
00:14:28,000 --> 00:14:37,360
on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it

132
00:14:37,360 --> 00:14:43,040
is pushing back against a certain form of dismissal of anthropocentric biases that

133
00:14:43,040 --> 00:14:48,960
only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a

134
00:14:48,960 --> 00:14:53,600
little bit already with the first one we make here in the paper about a performance competence

135
00:14:53,600 --> 00:14:58,080
distinction which is a nice way to bring about both anthropomorphism and anthropocentricism

136
00:14:58,080 --> 00:15:06,720
regarding LLM so this distinction is a very classic distinction in linguistics and cognitive

137
00:15:06,720 --> 00:15:13,360
science and it has already been applied to AI systems in the neural networks productively

138
00:15:13,360 --> 00:15:19,360
like Charles Farson so there's nothing really new here but the distinction comes from

139
00:15:19,360 --> 00:15:25,200
Noam Chomsky originally and the idea is that performance pertains to the external behavior

140
00:15:25,200 --> 00:15:33,120
of a system in a particular domain and competence is the kind of setup underlying knowledge and

141
00:15:33,760 --> 00:15:40,560
computations or mechanisms that enable the system to achieve that behavior

142
00:15:41,680 --> 00:15:46,160
and a familiar observation in linguistics and cognitive science is that there is a double

143
00:15:46,160 --> 00:15:56,880
dissolution between performance and competence so if you take for example language I might during

144
00:15:56,880 --> 00:16:01,600
this very podcast make some grammatical mistakes or some other mistakes in fact I've already

145
00:16:02,640 --> 00:16:08,320
misspoke in a few times I think and repeated myself so I made performance errors but this

146
00:16:08,320 --> 00:16:13,200
does not entail necessarily hopefully that I'm an incompetent language user and that I don't have

147
00:16:13,280 --> 00:16:22,080
in the language the competence so that's a well recognized dissociation you can be competent

148
00:16:22,080 --> 00:16:29,840
and yet make some errors and the reason for that is that there might be some additional factors

149
00:16:29,840 --> 00:16:35,840
that are unrelated to the underlying competence that might impede on my performance so for example

150
00:16:35,840 --> 00:16:43,520
I might be distracted when I speak or there might be other effects on my speaking performance that

151
00:16:43,520 --> 00:16:52,720
don't actually originate from a lack of competence but just impede on the idealized expression

152
00:16:52,720 --> 00:16:57,920
external manifestation of my competence and this is why I misspeak but it's also why recognize

153
00:16:57,920 --> 00:17:04,400
that you can have good performance without competence so we give here the example of a

154
00:17:04,400 --> 00:17:12,000
student cheating on a test or memorizing test answers by brute forcing the test to slightly more

155
00:17:13,520 --> 00:17:18,880
I guess gray area but at least in the cheating case a student can ace a test without being

156
00:17:18,880 --> 00:17:27,360
competent at what the test is actually testing for and it's also well acknowledged in cognitive

157
00:17:27,360 --> 00:17:38,960
science that there can be instances like this throughout you know like that can be manifest

158
00:17:38,960 --> 00:17:44,800
certain experimental settings where the test subject is right for the wrong reasons as it were

159
00:17:45,440 --> 00:17:49,840
namely it's doing well it's exhibiting good performance never realize the underlying reason

160
00:17:49,840 --> 00:17:55,600
for the performance is that there was some perhaps some curiosity that the experimenters

161
00:17:55,600 --> 00:18:00,320
all the scientists haven't thought about that could account for his good performance but

162
00:18:00,320 --> 00:18:05,760
but doesn't actually amount to whatever competence they were setting out to test

163
00:18:07,760 --> 00:18:11,680
so we we start by saying well this is what we could nice because the mistakes this is the

164
00:18:11,680 --> 00:18:15,840
association that is like supplied to humans across the board you can have performance

165
00:18:15,840 --> 00:18:21,120
without competence good performance without competence and you can have that performance despite

166
00:18:21,120 --> 00:18:26,880
competence now when it comes to other lamps the point we make just going to scroll as we go

167
00:18:28,240 --> 00:18:35,760
we have some figures to show later but the point we make is that generally people stress the

168
00:18:35,760 --> 00:18:40,320
dissociation apply the distinction to other lamps the stress decision only in one direction

169
00:18:41,120 --> 00:18:46,800
unlike in the case of human where it's bi-directional and so what people do generally is to say well

170
00:18:46,800 --> 00:18:52,160
other lamps famously you know if you look at the gpt4 technical report and and vice other

171
00:18:53,520 --> 00:18:59,440
any any any report about a new state of the art alone they are getting really really good at a

172
00:18:59,440 --> 00:19:06,480
number of tests and about benchmarks and even human exam examinations human exams the bar exam

173
00:19:07,120 --> 00:19:14,480
medical exams etc so they can get a really good performance test that we tend to think are really

174
00:19:14,480 --> 00:19:22,080
difficult tests that one can only pass at least a human could only pass if they have

175
00:19:22,640 --> 00:19:26,000
a really significant nonchalant amount of competency in particular the main

176
00:19:27,760 --> 00:19:32,960
and the point that is often made when it comes to other lamps is be careful slow it on and try to

177
00:19:32,960 --> 00:19:38,480
find out why the model is doing well on that test because there are various reasons why it could do

178
00:19:38,480 --> 00:19:45,520
well that do not actually indicate that the model has the underlying competence that the test was

179
00:19:45,520 --> 00:19:51,120
designed for when it comes to humans so one big concern for example is data contamination

180
00:19:51,120 --> 00:19:59,440
where very large language models train on internet scale data can easily be trained on some test

181
00:19:59,440 --> 00:20:03,680
items from common benchmarks that leak into the training data such that they can then do really

182
00:20:03,680 --> 00:20:11,280
well on the on the on the benchmark just because they've essentially memorized test items and there

183
00:20:11,280 --> 00:20:18,400
are other more subtle more subtle reasons why performance could be very good for the wrong

184
00:20:18,400 --> 00:20:25,440
reasons so that's very well recognized and a lot of the people who push back against anthropomorphic

185
00:20:25,440 --> 00:20:32,160
bias when it gets to other lamps make that point be careful do not take on another anthropomorphic

186
00:20:32,160 --> 00:20:37,920
attitude to the systems the reason why they do well is not because they have human-like intelligence

187
00:20:37,920 --> 00:20:45,040
or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant

188
00:20:45,040 --> 00:20:50,480
reasons that account for the good performance now when it comes to the other dissociation of the

189
00:20:50,480 --> 00:20:56,480
other dissociation of the other direction there are people are very reluctant to apply to a lamps

190
00:20:57,280 --> 00:21:02,160
and we think it's because essentially people think in a human case you can make sense of the

191
00:21:02,160 --> 00:21:07,680
idea that the human could do badly on a test or corporate could perform badly on a task

192
00:21:08,640 --> 00:21:14,080
and yet have the competence that you're trying to test but there might be some auxiliary factors

193
00:21:14,720 --> 00:21:23,440
such as working memory limitations attention deficits etc that could impede on the performance

194
00:21:23,440 --> 00:21:34,720
but in the language model I think what we argue in the paper is that

195
00:21:36,480 --> 00:21:40,240
people don't a lot of people don't think that there is an analogous

196
00:21:42,400 --> 00:21:46,560
mechanism at play where there could be some kind of auxiliary factor that impede on performance

197
00:21:46,560 --> 00:21:52,080
performance is what you get what you see is what you get and so the performance is a direct

198
00:21:52,800 --> 00:22:02,320
manifestation of what the system is computing and if you have performance errors

199
00:22:03,040 --> 00:22:06,640
that can only be explained by the lack of competence because there is no additional

200
00:22:09,840 --> 00:22:14,960
independent factor or module that could impede on the performance

201
00:22:15,680 --> 00:22:22,160
sources of interference you might say yeah so yeah I mean I'll

202
00:22:23,680 --> 00:22:30,320
pass it over to you Charles I just wanted to set up this distinction yeah yeah

203
00:22:32,320 --> 00:22:38,640
right so I mean if you think about a traditional computer program

204
00:22:39,520 --> 00:22:46,960
well at least if you think about a simple computer program it's odd to think of it as

205
00:22:47,520 --> 00:22:52,240
some sort of complex systems where it's a complex system where one part of it could sort of

206
00:22:52,240 --> 00:22:57,040
interfere with the workings of another part of it but one of the points we want to make is that

207
00:22:57,600 --> 00:23:04,000
something like that is a realistic possibility with large language models

208
00:23:04,560 --> 00:23:11,120
um okay but I suppose the next part of the paper goes into a taxonomy of

209
00:23:11,920 --> 00:23:17,840
anthropocentric bias and the first sort of overarching point is the distinction between

210
00:23:17,840 --> 00:23:25,840
type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance

211
00:23:26,480 --> 00:23:33,680
failures designed to measure competence always indicates that it lacks

212
00:23:35,520 --> 00:23:43,280
that competence and so we before we so we'll say something about three different kinds of

213
00:23:43,280 --> 00:23:50,160
type one anthropocentric bias but first a background point which is that

214
00:23:50,400 --> 00:24:00,000
um whenever we think it's possible to give a mechanistic explanation of some

215
00:24:00,720 --> 00:24:09,360
complicated phenomenon we always have to foreground some factors some variables

216
00:24:09,760 --> 00:24:22,320
uh and background others and um the properties that we push into the background

217
00:24:26,320 --> 00:24:31,200
nevertheless matter we're still making assumptions about the nature of those properties

218
00:24:31,200 --> 00:24:36,080
when we try to articulate what's going on with the other properties that we're paying more attention

219
00:24:36,080 --> 00:24:42,160
to and if assumptions about those properties in the background turn out to be wrong

220
00:24:43,120 --> 00:24:52,400
then those mistakes will corrupt our explanation that attends only to the foregrounded factors

221
00:24:52,400 --> 00:24:59,600
so that's a little bit abstract let me just give you a simple example um in comparative cognition

222
00:25:00,560 --> 00:25:08,160
one famous uh behavioral experiment is the mirror test for self-recognition

223
00:25:09,040 --> 00:25:18,640
so the question is roughly do non-human animals have something like a concept of self

224
00:25:19,840 --> 00:25:29,520
and the strategy is to put some sort of mark on their body in original experiments it was

225
00:25:29,520 --> 00:25:36,800
a red dot on the forehead of maybe a monkey and or a bird or whatever and then

226
00:25:38,960 --> 00:25:46,640
you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark

227
00:25:47,760 --> 00:25:51,920
and if it does make an attempt to get rid of the mark that shows that it recognizes that the

228
00:25:51,920 --> 00:26:00,320
image in the mirror is an image of itself and otherwise not um so that that's a cool way to

229
00:26:00,320 --> 00:26:09,120
get a really difficult and abstract question about the mind of a non-human animal but it presumes

230
00:26:09,120 --> 00:26:16,080
or it assumes that animals will care about the fact that they have a red dot on their

231
00:26:16,080 --> 00:26:20,720
forehead that they will be bothered by that and be motivated to get rid of it and if that

232
00:26:20,720 --> 00:26:26,480
assumption is wrong then they might fail the mirror test for self-recognition for reasons that

233
00:26:26,480 --> 00:26:34,800
have little to do with the presence or absence of a capacity for self-recognition until something

234
00:26:34,800 --> 00:26:45,760
similar to that is going on we say in large language models so the first example that we give is to

235
00:26:45,760 --> 00:27:00,240
do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral

236
00:27:00,240 --> 00:27:08,800
task um there will be demands associated with that task that are not directly related to the

237
00:27:08,800 --> 00:27:14,240
capacity that you're trying to test so to take the most obvious example that I can think of

238
00:27:14,240 --> 00:27:18,720
if you give someone a written test they have to be able to write they have to you know have a hand

239
00:27:18,720 --> 00:27:26,880
and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't

240
00:27:26,880 --> 00:27:33,040
write then um their failure to fill out the test wouldn't tell you anything about their uh you know

241
00:27:33,760 --> 00:27:45,920
academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral

242
00:27:45,920 --> 00:27:52,960
tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right

243
00:27:52,960 --> 00:27:59,760
away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic

244
00:27:59,760 --> 00:28:08,320
but what they do is they give a large language model um the following sort of question uh this

245
00:28:08,320 --> 00:28:14,080
is a question it's for a grammaticality judgment so you can see on the image there which sentence

246
00:28:14,080 --> 00:28:19,600
is better in english number one every child is studied number two every child have studied

247
00:28:20,400 --> 00:28:27,280
answer with one or two and it gives the wrong output but then

248
00:28:29,600 --> 00:28:36,320
you can also simply look at the probabilities assigned to each of those

249
00:28:37,520 --> 00:28:47,040
sentences within the model and uh figure out directly whether the model thinks that input

250
00:28:47,040 --> 00:28:54,240
A is more likely than input B and it turns out that on a wide variety of questions of this kind

251
00:28:55,280 --> 00:29:02,640
the direct comparison uh does or the large language models perform better with the direct

252
00:29:02,640 --> 00:29:12,400
comparison than they do with the more complex demand for uh metalinguistic judgment so the

253
00:29:12,400 --> 00:29:21,280
fact that the model has to process the numbering of the uh options and then answer in terms of a

254
00:29:21,280 --> 00:29:31,280
number uh is a subtle but nevertheless um important additional variable in the experiment

255
00:29:31,840 --> 00:29:36,320
and that can influence the model's capacity to get the answer right

256
00:29:36,560 --> 00:29:38,960
Rafael do you want to add to that?

257
00:29:40,640 --> 00:29:45,360
No I think that well maybe we can mention briefly the other example that we discuss which is from

258
00:29:46,480 --> 00:29:54,480
this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes

259
00:29:55,680 --> 00:30:01,120
the example interesting and even more problematic in terms of comparative psychology which is

260
00:30:01,840 --> 00:30:12,720
um one way in which auxiliary task demands can be ignored or disregarded or overlooked

261
00:30:13,520 --> 00:30:20,400
is when uh you are doing a direct comparison between humans and other epsilon tasks and

262
00:30:20,400 --> 00:30:26,640
the experimental conditions are mismatched in such a way that the task as you set up

263
00:30:27,600 --> 00:30:35,040
impose the stronger demands stronger task and auxiliary demands on the LOM than those on human

264
00:30:35,040 --> 00:30:40,240
subjects and that's something that that can happen quite often and so there is this this

265
00:30:40,240 --> 00:30:46,800
interesting example from a couple of papers originally published by black reds and colleagues

266
00:30:46,800 --> 00:30:53,040
from standard hands group where they looked at um the ability of language models to handle their

267
00:30:53,040 --> 00:31:03,680
recursion um looking at center embedded closes uh how such closes might um

268
00:31:05,680 --> 00:31:10,160
for example when you had a prepositional phrase within the subject of the sentence and the verb

269
00:31:10,160 --> 00:31:17,680
might throw up either humans or LOMs into agreeing the the verb in the wrong way

270
00:31:18,480 --> 00:31:27,600
so giving the wrong number to the verb for example the keys that the man put on the table

271
00:31:29,280 --> 00:31:34,400
here it should be R because keys is plural but because you have close in the middle

272
00:31:35,200 --> 00:31:39,680
and if you add more closes like this that are embedded in the middle people and LOMs can get

273
00:31:39,680 --> 00:31:47,520
confused and and predict that the verb should be is for example so um they tested this on humans

274
00:31:47,520 --> 00:31:54,000
and LOMs and found that on the more complex examples involving complex more complex constructions or

275
00:31:54,000 --> 00:32:01,280
recursion humans were doing decently well but LOMs performance was collapsing compared to the

276
00:32:01,280 --> 00:32:08,400
single examples and I've heard an opinion from DeepMind um looked into that and realized that

277
00:32:08,400 --> 00:32:13,840
the experimental conditions were mismatched so the humans at this very common in cognitive

278
00:32:13,840 --> 00:32:18,960
science experiments were getting some training before they completed the test items to just get

279
00:32:18,960 --> 00:32:25,600
familiarized with the task so they were given some examples of the task um harsh condition

280
00:32:26,400 --> 00:32:36,160
and the LLabs were just prompted zero shots as um people usually put it so just um without any

281
00:32:36,240 --> 00:32:44,160
example just point blank and Andrew found out that if you he he he replicated the experiments but

282
00:32:44,160 --> 00:32:51,360
I did some proper matched testing conditions for the LL so adding some examples of the task in the

283
00:32:51,360 --> 00:32:57,680
prompt when it's known as future counting and with that he found that performance was equivalent

284
00:32:57,680 --> 00:33:02,320
in fact the LLM that he tested was slightly better on the more complex constructions than humans

285
00:33:03,040 --> 00:33:06,560
so when you match the test conditions here you actually match also

286
00:33:07,520 --> 00:33:13,360
at least you it's it's not it's not automatic but you you you can match the test demands I mean

287
00:33:13,360 --> 00:33:17,680
it could be that there are reasons why the various experimental conditions would result in different

288
00:33:17,680 --> 00:33:24,320
demands for humans and others but you're still in this case even on the playing ground playing field

289
00:33:25,360 --> 00:33:30,480
in such a way that you don't find the behavioral discrepancy that you found initially anymore

290
00:33:33,040 --> 00:33:41,840
um yeah um good so shall we continue to the next

291
00:33:42,800 --> 00:33:53,840
section um so another uh another way that auxiliary or another type of auxiliary task

292
00:33:53,840 --> 00:33:59,600
demand is input independent computational limitations um and here we're thinking of a

293
00:33:59,600 --> 00:34:05,600
few papers that show that the number of forward passes that the transformer can make

294
00:34:06,320 --> 00:34:13,680
influences its ability to find the right spot and parameter space so neural networks are

295
00:34:16,800 --> 00:34:20,080
function approximators but their

296
00:34:20,080 --> 00:34:31,040
um their ability to approximate a function can be eliminated uh can be limited by the

297
00:34:33,200 --> 00:34:40,480
the the number of computations it's allowed to perform and um the

298
00:34:40,960 --> 00:34:48,640
uh sort of crucial feature of uh transformers in this example is that

299
00:34:49,680 --> 00:34:57,920
um the number of operations that determines the next token is limited by the number of

300
00:34:57,920 --> 00:35:08,000
tokens that it's seen so far and it turns out that if you train a transformer with

301
00:35:09,680 --> 00:35:23,200
additional meaningless tokens like pause tokens like the word pause you can increase its accuracy

302
00:35:23,920 --> 00:35:32,640
across a range of of question types um and yeah this is

303
00:35:35,040 --> 00:35:42,960
this counts as an auxiliary task demand in our view because um it's doing something roughly

304
00:35:42,960 --> 00:35:50,240
analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry

305
00:35:50,800 --> 00:35:58,640
yeah um it's it's doing something like giving the model uh time to think and um

306
00:36:00,400 --> 00:36:10,080
yeah so so you might think that the absence of that additional inference time is a factor

307
00:36:11,280 --> 00:36:17,840
that is not directly not conceptually related to its capacity to answer

308
00:36:18,080 --> 00:36:24,720
uh a question like the one on the screen um you know a simple

309
00:36:24,720 --> 00:36:32,240
earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy

310
00:36:32,240 --> 00:36:38,560
is is a nice one time to think because if you if you tested a human on even a simple mathematical

311
00:36:38,560 --> 00:36:45,200
questions or any any task really and just ask them you know tell them they have like one second

312
00:36:45,200 --> 00:36:52,800
to just blow it out answer performance would probably be pretty bad um and you can think of

313
00:36:52,800 --> 00:36:59,360
asking an LLN to answer a question point blank as very very loosely analogous to that and obviously

314
00:37:00,240 --> 00:37:05,600
this is an analogy and there are very important differences here but I think it's a helpful

315
00:37:05,600 --> 00:37:14,080
heuristic to think about what is um what is going on roughly here and then we uh in in both cases

316
00:37:14,880 --> 00:37:22,000
the system the human or the LLN does not get the chance to perform the necessary computations to

317
00:37:22,720 --> 00:37:28,000
derive the correct answer and so yeah what we talk about in the paper is that you have these

318
00:37:28,000 --> 00:37:35,680
experimental works during that if you ask a question to a language model um the amount of

319
00:37:35,760 --> 00:37:41,120
tokens it's a it's it's that allows to generate before providing the answer

320
00:37:43,040 --> 00:37:48,560
makes a difference to how accurate it is so if it uh it just generates a few tokens then have to

321
00:37:48,560 --> 00:37:52,400
give an answer or even if you just have to give the answer point blank with the very first token

322
00:37:53,200 --> 00:37:57,280
it's going to be less accurate that if you give it a chance to generate a number of tokens before

323
00:37:57,280 --> 00:38:04,320
giving the answer so the usual way in which this is understood is that when you ask when you

324
00:38:04,560 --> 00:38:08,400
you you allow the LLN to generate a number of tokens before giving the answer or you even

325
00:38:08,400 --> 00:38:13,600
prompted to do so you say things step by step for example um that's not as chain of thought

326
00:38:13,600 --> 00:38:19,200
prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace

327
00:38:19,760 --> 00:38:25,680
or what looks outwardly externally like a reasoning trace in the output before giving an answer

328
00:38:26,720 --> 00:38:30,000
and we know that chain of thought prompting increases performance accuracy

329
00:38:30,720 --> 00:38:42,560
um but what was found by a couple of papers um is that the mechanistic influence of this process

330
00:38:42,560 --> 00:38:47,360
is not entirely due to the nature of the tokens that are generated in this reasoning trace

331
00:38:47,920 --> 00:38:54,960
in other words it's not just that the LLN has to generate the right tokens corresponding to

332
00:38:54,960 --> 00:39:01,120
different steps of reasoning before giving an answer in fact the very back that you allow the

333
00:39:01,120 --> 00:39:07,440
LLN to just generate tokens any token before giving an answer from a mechanistic perspective

334
00:39:07,440 --> 00:39:13,920
affords the system to perform additional computations that can complete the computational

335
00:39:13,920 --> 00:39:19,520
circuit that otherwise would get a chance to be completed and to derive the correct answer

336
00:39:19,520 --> 00:39:24,400
so as Charles mentioned you can have you can set up an experiment when you have the LLN just

337
00:39:24,400 --> 00:39:31,600
generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the

338
00:39:31,600 --> 00:39:38,560
answer and the the more dots you allow before the token gives the answer the greater the

339
00:39:39,200 --> 00:39:45,200
expressivity of the system and the more um the more kinds of programs problems you can answer

340
00:39:46,400 --> 00:39:50,720
and so as Charles mentioned every time an LLN is generated in a token the LLN is performing

341
00:39:50,720 --> 00:39:56,720
one forward pass and so the more tokens it's generating the more forward passes it's doing

342
00:39:57,280 --> 00:40:02,160
and one way to think about what's going on here as well is that having these additional forward

343
00:40:02,160 --> 00:40:07,360
passes where you you feedback the whole input sequence plus the previously generated tokens

344
00:40:07,360 --> 00:40:12,560
to do the system to generate the next is also a way to introduce a form of recurrence in

345
00:40:12,560 --> 00:40:18,080
transformers that are not in terms of the architecture of recurrent networks so that

346
00:40:18,080 --> 00:40:26,320
increases the expressivity and you know in complexity of your unique terms and yeah there is

347
00:40:26,320 --> 00:40:33,680
there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation

348
00:40:34,480 --> 00:40:39,520
that again we think is very very loosely analogous to prompting a human to answer a

349
00:40:39,520 --> 00:40:45,200
point on the question without thinking so that's the next sense an auxiliary factor because

350
00:40:45,200 --> 00:40:51,200
if you give the LLN the opportunity to generate enough tokens it might have the competence

351
00:40:52,480 --> 00:40:58,320
to solve a task but you might not see that otherwise and you might get performance errors

352
00:40:58,320 --> 00:40:59,920
but you do think it's incompetent

353
00:41:03,040 --> 00:41:10,720
all right yeah okay um so the the third type of type one anthropocentric

354
00:41:11,600 --> 00:41:17,520
bias that we talk about is mechanistic interference and so this comes from

355
00:41:18,480 --> 00:41:22,160
the mechanistic interpretability work and the basic idea is that because

356
00:41:24,080 --> 00:41:29,280
large language models are capable of in-context learning they can learn different strategies

357
00:41:30,160 --> 00:41:34,480
for solving a different particular type of problem and the strategy that they

358
00:41:35,200 --> 00:41:41,680
implement at a given time can be different so you can talk about this in terms of

359
00:41:42,400 --> 00:41:49,280
virtual circuits that are formed inside the language model and there's some interesting work from

360
00:41:50,240 --> 00:41:56,400
nil nanda and others showing that in some circumstances these two circuits can compete

361
00:41:56,400 --> 00:42:05,680
with one another so at a certain level of uh or after a certain amount of training

362
00:42:06,640 --> 00:42:13,840
you get one circuit operative after a bit more training you have two different circuits

363
00:42:14,880 --> 00:42:22,480
but they're uh the first circuit is still sort of dominant and then after additional

364
00:42:23,440 --> 00:42:25,040
training the model

365
00:42:28,000 --> 00:42:35,520
converges on on the second circuit and the first one slowly gets sort of

366
00:42:38,160 --> 00:42:44,240
it sort of ceases to influence the internal operations of the model and it's only once

367
00:42:44,320 --> 00:42:49,600
you reach that third phase at which the

368
00:42:52,480 --> 00:42:59,200
the benefits of the second circuit with respect to the first become visible

369
00:43:00,240 --> 00:43:05,600
so you can you can show using decoding work that the second circuit is there

370
00:43:06,560 --> 00:43:12,400
uh before you can show behaviorally that the second circuit

371
00:43:14,720 --> 00:43:23,440
yields better performance accuracy on on the task so um I suppose there's a combination

372
00:43:23,440 --> 00:43:29,840
of two ideas here one is that um there are different strategies a model can

373
00:43:30,320 --> 00:43:41,600
implement for solving a problem we can detect those strategies internally using decoding methods

374
00:43:42,400 --> 00:43:46,640
um so three ideas and then the third is uh

375
00:43:50,400 --> 00:43:53,280
a good strategy can be

376
00:43:54,240 --> 00:44:01,120
uh present in some sense in the model um before it has had the chance to influence

377
00:44:01,840 --> 00:44:09,440
behavior um and and so this is just another way that the link between

378
00:44:10,000 --> 00:44:15,440
performance and competence is shown to be more complicated than I might seem at first

379
00:44:16,080 --> 00:44:17,440
graph

380
00:44:19,760 --> 00:44:25,120
and um yeah just to to clarify one thing so the circuits are just um

381
00:44:26,480 --> 00:44:30,240
you know ways to think about the causal structure of a neural network and

382
00:44:30,240 --> 00:44:35,360
there's essentially computational subgraphs of the network that have a specific function

383
00:44:35,360 --> 00:44:39,280
you can think of a circuit as implementing a particular algorithm or set of computations

384
00:44:40,000 --> 00:44:44,000
um it's a part of what people are interested in in this mechanistic interpretability literature

385
00:44:44,000 --> 00:44:50,320
that we build on people like Neal Mandatrisola and others is reverse engineering the circuit

386
00:44:50,320 --> 00:44:57,440
steps in deep neural networks and large language models um peer to implement certain well-defined

387
00:44:57,440 --> 00:45:03,680
algorithms in some cases at least um and the emerging picture that we build on here is that

388
00:45:05,120 --> 00:45:11,520
there is a lot of redundancy built into neural networks as they learn to perform a task

389
00:45:11,600 --> 00:45:18,240
optimized as a function that in many cases translates into redundant circuits that relate

390
00:45:18,240 --> 00:45:27,040
to the same tasks the same kinds of um the same kinds that we put out with my things and uh for

391
00:45:27,040 --> 00:45:31,840
these circuits might be somewhat identical circuits that are just redundant or they might be

392
00:45:32,480 --> 00:45:37,280
different algorithms just to do to do a similar thing and different strategies to solve some

393
00:45:37,280 --> 00:45:41,280
problem I swear maybe one will be a bit more approximative and the other one a bit more exact

394
00:45:41,280 --> 00:45:49,760
more computationally intensive so that's where you can have some interference um where one uh

395
00:45:50,720 --> 00:45:58,160
or at least some competition where once your kid takes over another and um such that the other

396
00:45:58,160 --> 00:46:03,200
becomes kind of you know it's there it's latent in the system but you don't get a chance to influence

397
00:46:03,200 --> 00:46:08,800
behavior on a specific input so you can get a performance error for that reason and these can

398
00:46:08,800 --> 00:46:14,960
combine with the other things we mentioned here so things like task demands the first thing we

399
00:46:14,960 --> 00:46:21,280
discussed as well as the number of tokens you generate both of these things could cause a

400
00:46:21,280 --> 00:46:27,120
particular circuit to take over another um so it's it's we can think of this holistically as

401
00:46:27,920 --> 00:46:32,320
perhaps if you ask a question point blank to a model without letting it generate

402
00:46:32,400 --> 00:46:37,120
bunch of tokens before giving an answer then one particular approximate circuit might take over

403
00:46:37,120 --> 00:46:43,120
that gives the wrong answer if you let it generate more tokens then another more exact circuit might

404
00:46:43,120 --> 00:46:50,800
be given a chance to um it could influence the output using the right answer and similarly with

405
00:46:50,800 --> 00:47:02,320
task demands uh strong task demands might uh in some cases um impede on the uh triggering of a

406
00:47:02,320 --> 00:47:08,080
certain circuits that would otherwise have given the right answer um so that could be the case

407
00:47:08,080 --> 00:47:13,520
perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts

408
00:47:14,080 --> 00:47:20,000
might actually prime the word circuits to solve the task about complex recursive cases in the right

409
00:47:20,000 --> 00:47:28,240
way um so yeah these are the three main auxiliary factors that relates to what we call type one

410
00:47:28,240 --> 00:47:33,680
anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should

411
00:47:33,680 --> 00:47:40,880
be quick on type two do you want to uh pick fix things after child yeah so type one uh deals

412
00:47:40,880 --> 00:47:49,920
with cases where performance of the model is um weak compared to humans so the model doesn't do

413
00:47:49,920 --> 00:47:57,760
so well um and then type two is when the model does do well but nevertheless is different in some

414
00:47:57,760 --> 00:48:06,000
respect from the uh performance profile of the human or we have some evidence to think that the

415
00:48:06,000 --> 00:48:14,560
model uses a different strategy than humans typically use and um the idea is that um even

416
00:48:14,640 --> 00:48:20,800
once you hold performance equivalent or average performance equivalent um you know making a different

417
00:48:20,800 --> 00:48:27,440
pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability

418
00:48:27,440 --> 00:48:37,840
work any deviance from the human strategy is evidence of fragility or only a trick solution

419
00:48:38,320 --> 00:48:49,680
um and uh this point is a bit more philosophical i suppose but the um idea is that

420
00:48:51,760 --> 00:49:01,280
the human strategy for solving a problem um isn't necessarily the most general strategy

421
00:49:02,000 --> 00:49:12,880
for solving a problem and uh what matters is whether the strategy that is pursued by the model

422
00:49:12,880 --> 00:49:20,240
is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human

423
00:49:20,240 --> 00:49:33,280
strategy um yeah and we end the we end the paper by considering an objection um which is um

424
00:49:35,760 --> 00:49:46,000
why um like given that um in humans we study cognition largely through language

425
00:49:46,960 --> 00:49:53,760
um and given that elements are trained on language or um linguistic outputs from

426
00:49:53,760 --> 00:50:01,120
humans um isn't it appropriate after all to treat um human cognition as the correct or

427
00:50:01,120 --> 00:50:07,760
appropriate the obstacle to study elements and we to that we answer that it depends how we think

428
00:50:08,320 --> 00:50:15,040
of that dialectic um so we acknowledge that there is there is no

429
00:50:16,480 --> 00:50:20,720
really other option than to start or investigation of cognitive abilities in algorithms

430
00:50:21,280 --> 00:50:28,080
but with reference to human cognitive abilities using human cognitive abilities as some kind of

431
00:50:28,080 --> 00:50:33,360
realistic or reference points things like theory of mind memory metacognition

432
00:50:34,320 --> 00:50:39,520
various forms of reasoning etc that are familiar to us because we humans have them

433
00:50:40,880 --> 00:50:44,960
and this is the same thing by the way in animal cognition for example or in developmental psychology

434
00:50:44,960 --> 00:50:51,840
where in any comparative psychology setup um the reference point for what concepts

435
00:50:51,840 --> 00:51:01,360
psychological capacities initially at least um is necessarily tied up with our conception of what

436
00:51:01,360 --> 00:51:07,120
we human we humans have in our repertoire of cognitive capacities but we emphasize that this

437
00:51:07,120 --> 00:51:11,760
is only the starting point so here we've over written from uh the philosopher Ali Boyle who

438
00:51:11,760 --> 00:51:16,720
calls this investigative kinds investigative cognitive kinds we start with a cognitive

439
00:51:16,720 --> 00:51:23,280
kind like memory or metacognition episodic memory metacognition theory of mind um as

440
00:51:23,280 --> 00:51:27,200
as an investigative starting point the starting point of the investigation and then we have

441
00:51:27,200 --> 00:51:34,160
that we we can try to start operationalizing operationalizing this concept this kind this

442
00:51:34,160 --> 00:51:40,480
cognitive capacity in an experiments testing the algorithms on it with an open

443
00:51:41,280 --> 00:51:47,280
mandated empirical approach and then based on the results from that each relatively refine

444
00:51:48,560 --> 00:51:53,920
the capacities that we are the capacity that we're targeting or the definition of the capacity

445
00:51:54,080 --> 00:52:01,120
targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions

446
00:52:01,120 --> 00:52:10,960
that we have such that as the experimental um project runs a course or as as we make as we

447
00:52:10,960 --> 00:52:19,360
we get more results and refine our concepts we may end up with um something that no longer

448
00:52:19,360 --> 00:52:28,320
looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS

449
00:52:29,120 --> 00:52:37,040
but ends up looking like looking for something that some capacity that is that shares some similarity

450
00:52:37,040 --> 00:52:42,560
with human like human episodic memory but is different in other respects um and so we can

451
00:52:42,560 --> 00:52:48,880
gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric

452
00:52:50,000 --> 00:52:54,480
so we emphasize that it is kind of due to feedback look here that's that's that's premised on open

453
00:52:54,480 --> 00:52:59,920
minded empirical investigation that doesn't settle this question a priori but has to start

454
00:53:00,800 --> 00:53:04,240
as a necessary starting point with the the reference to human cognition

455
00:53:05,120 --> 00:53:06,800
i don't know if you want to add to that joss

456
00:53:09,600 --> 00:53:14,400
um no i think that's pretty good maybe we should uh move on to questions

457
00:53:14,640 --> 00:53:25,840
and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't

458
00:53:25,840 --> 00:53:29,280
move it awesome okay

459
00:53:33,520 --> 00:53:40,640
yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat

460
00:53:40,720 --> 00:53:47,760
but first i just wanted to read a short quote from the 2022 active inference textbook they wrote

461
00:53:49,280 --> 00:53:57,200
um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote

462
00:53:57,200 --> 00:54:02,880
too much effort to modeling isolated subsystems e.g perception language understanding whose

463
00:54:02,880 --> 00:54:10,000
boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive

464
00:54:10,000 --> 00:54:14,880
creature perhaps a simple one and an environmental niche for it to cope with

465
00:54:16,080 --> 00:54:23,440
so it's interesting about the approach that you're taking this is kind of a simple synthetic

466
00:54:23,440 --> 00:54:31,440
iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena

467
00:54:32,160 --> 00:54:37,840
because there is something and and so i saw in the presentation paper kind of this call for like

468
00:54:38,400 --> 00:54:45,680
deliberate investigation rather than just chopping up the iguana a priori with a framework that

469
00:54:45,680 --> 00:54:54,000
that applies to humans or that centers humans or or that just uh soothes the epistemic challenge

470
00:54:54,000 --> 00:55:05,360
that's presented okay okay first question from dave he wrote

471
00:55:08,480 --> 00:55:14,400
have you looked at daniel denitz's distinction between competence without awareness and

472
00:55:14,400 --> 00:55:21,120
competence with awareness he expands on this in the 2023 from bacteria to Bach and back

473
00:55:21,920 --> 00:55:27,920
i find this much more valuable than chomsky's highly problematic performance without competence

474
00:55:27,920 --> 00:55:34,960
a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this

475
00:55:34,960 --> 00:55:46,160
competency uh well maybe i'll let you think uh that one is i can trust because you're

476
00:55:46,160 --> 00:55:53,760
you're maybe more within it than i am but i'll just say um awareness is a very polysemous

477
00:55:53,760 --> 00:56:00,080
term like many terms in philosophy of minds but partially this one more than many others i think

478
00:56:00,800 --> 00:56:08,400
so um it can mean a lot of different things in all of context here we don't focus on things

479
00:56:08,400 --> 00:56:14,800
like consciousness because i think we probably both agree that it's a less tractable uh maybe

480
00:56:14,800 --> 00:56:23,120
empirical problem to try to assess the presence or absence of consciousness in language models

481
00:56:23,120 --> 00:56:27,840
even though many people are interested in that we think that we have more hope of making progress

482
00:56:27,840 --> 00:56:35,360
in the near term with more well-defined cognitive capacities or cognitive functions and things that

483
00:56:35,360 --> 00:56:44,480
relate to forms of certain forms of reasoning and viable binding etc um so we our framework and

484
00:56:44,480 --> 00:56:49,760
principle would apply to things like consciousness or as you put awareness generally speaking but

485
00:56:50,720 --> 00:56:55,040
we don't really focus on that for examples the other quick thing i'll just mention is that i

486
00:56:55,120 --> 00:57:01,600
seem to remember that the phrase from then but again i'm not a then scholar was competence

487
00:57:01,600 --> 00:57:08,640
without comprehension um which seems a little different from competence without awareness

488
00:57:08,640 --> 00:57:15,360
perhaps depending on how you think of comprehension um and yeah i think that does i think it is a

489
00:57:15,360 --> 00:57:22,160
very interesting phrase that it does um in fact i had this project that's unpublished with

490
00:57:22,160 --> 00:57:29,600
chris dolega who i think you had on the podcast as well um on semantic competence in language models

491
00:57:29,600 --> 00:57:37,040
where we use that phrase um to kind of avoid taking its stance on the kind of messy

492
00:57:38,160 --> 00:57:43,680
muddy question of whether hella let's understand language which builds in all sorts of assumptions

493
00:57:43,680 --> 00:57:48,400
including about consciousness actually for some people like chancel um and we focus on

494
00:57:48,400 --> 00:57:53,920
the more restricted notion of competence and i think our paper here also has that property that

495
00:57:53,920 --> 00:57:58,880
would if we have originalized competence we end up operationalizing competence in terms of

496
00:57:59,440 --> 00:58:04,640
the sets of knowledge of the mechanism and mechanisms that enable a system to generalize

497
00:58:04,640 --> 00:58:09,920
well in a given domain basically and in a way that's a supposed evolutionary compared to

498
00:58:10,880 --> 00:58:14,480
some more expensive understandings of competence that

499
00:58:15,840 --> 00:58:22,400
we need to comprehension or understanding more well but i'll let you take that one charles

500
00:58:24,880 --> 00:58:31,200
no yeah that was that was good um the phrase you know the distinction that denadra's is between

501
00:58:32,160 --> 00:58:39,200
competence with comprehension and without and i think um competence with comprehension is the

502
00:58:39,200 --> 00:58:45,120
ability not just to pursue a strategy that's successful for solving a problem but to um

503
00:58:45,120 --> 00:58:52,640
articulate the strategy in such a way that you could teach it for example and um humans only

504
00:58:52,640 --> 00:59:00,960
sometimes have competence with comprehension we have many competences that lack comprehension

505
00:59:00,960 --> 00:59:08,720
right um you know when we learn to walk for example um we have an amazing competence that we

506
00:59:09,520 --> 00:59:15,440
still can't quite translate into robotics because we don't fully understand how it works

507
00:59:17,600 --> 00:59:24,800
and when it comes to our language models i think we should

508
00:59:25,440 --> 00:59:39,440
not expect uh comprehension i mean they have a an amazing suite of competences if you thought that

509
00:59:39,440 --> 00:59:44,160
they also had comprehension then i suppose you would think like well if you want to understand

510
00:59:44,160 --> 00:59:50,720
how a large language model works you can just ask it but that's that's a bad strategy nobody nobody

511
00:59:51,280 --> 00:59:57,840
about um how a large language model works so so they're on the um competence without

512
00:59:57,840 --> 01:00:08,960
comprehension side of things um and in order to figure out what in order to figure out what

513
01:00:08,960 --> 01:00:13,360
the mechanisms are that enable its competencies we have to pursue strategies that are broadly

514
01:00:13,360 --> 01:00:18,720
similar to the strategies we use in you know cognitive psychology or cognitive linguistics

515
01:00:19,440 --> 01:00:23,440
um and you know we have to run experiments so i think that that's all very compatible with

516
01:00:23,440 --> 01:00:31,920
Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was

517
01:00:35,200 --> 01:00:42,560
quite influential to me and we actually wrote a commentary together which pushes back a little

518
01:00:42,560 --> 01:00:49,040
bit on a simple understanding of this distinction so we were looking at um the evolution of

519
01:00:49,040 --> 01:00:58,320
metacognition and basically what we argue is that um given the gradualism of evolution there

520
01:00:58,320 --> 01:01:03,200
must have been something in between base level cognition and metacognition so we shouldn't

521
01:01:03,200 --> 01:01:12,800
see that distinction as black and white and um you know i think that if you want to contrast the

522
01:01:12,800 --> 01:01:28,400
sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific

523
01:01:28,400 --> 01:01:37,360
uh concepts at her disposal with you know a non-human animal then this strong distinction

524
01:01:37,360 --> 01:01:44,960
between competence with and without comprehension is reasonable um but in the space of all possible

525
01:01:44,960 --> 01:01:53,280
minds we should be open to the view that there can be you know semi-competent um forms of cognition

526
01:01:53,840 --> 01:02:00,240
and just to put it on this uh it occurred to me while I was listening to you as well that um

527
01:02:00,240 --> 01:02:04,400
the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and

528
01:02:04,400 --> 01:02:11,120
Frank example is a nice is a nice example where in order to give the metacognistic judgments

529
01:02:11,120 --> 01:02:15,120
correctly so see that that you would need competence with comprehension because you need to

530
01:02:15,920 --> 01:02:21,760
understand not only be able to to come to to agree the verb with the subject but know the rule and

531
01:02:21,760 --> 01:02:30,800
know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so

532
01:02:30,800 --> 01:02:35,280
when you find that the L.M. can do well at the low task the member of the task and at the high

533
01:02:35,280 --> 01:02:41,280
task development explicit metacognistic judgments in some way that's an example of the L.M. having

534
01:02:41,280 --> 01:02:53,600
competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given

535
01:02:53,600 --> 01:03:01,840
that LLMs inherently reflect anthropocentric biases due to their training on human data and goals

536
01:03:01,840 --> 01:03:07,040
how can we ensure that their inter model discourse aligns with humanity's values

537
01:03:11,760 --> 01:03:12,400
um

538
01:03:15,280 --> 01:03:21,440
so the inter white discourse right be inter model discourse perhaps amongst the models

539
01:03:22,560 --> 01:03:31,520
I see I see like in that farmville paper yeah the small the yeah both generative agents get

540
01:03:31,520 --> 01:03:39,520
the small ones uh yeah I think that's beyond the scope of this paper to be honest but um

541
01:03:39,760 --> 01:03:46,560
I mean we could use about it yeah but I don't know that we have I don't know this project

542
01:03:46,560 --> 01:03:52,400
has much I mean I I think we both have an interest in the alignment problem uh independently of this

543
01:03:52,400 --> 01:03:58,320
project but I don't think this project has much to say really about this I'm not sure what you think

544
01:03:59,280 --> 01:04:00,880
uh yeah yeah I don't

545
01:04:01,360 --> 01:04:12,240
yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise

546
01:04:12,240 --> 01:04:19,840
into LLM training that was the section about the extra tokens do you see any analog to

547
01:04:19,840 --> 01:04:23,600
intermittent reinforcement to uncertainty tolerance

548
01:04:24,480 --> 01:04:32,000
because you mentioned the extra tokens in the chain of thought and how that could also be replaced

549
01:04:32,000 --> 01:04:41,280
by by dot dot dot dot dot and so like what is that telling us about model training when um

550
01:04:42,000 --> 01:04:46,960
it seems like there's some situations where adding superfluous tokens would diminish signal

551
01:04:46,960 --> 01:04:51,280
in data sets but then here are other situations where it seems to actually help

552
01:04:54,480 --> 01:05:03,440
um um yeah so in that particular paper I think it's called thinking dot by dot and there is a

553
01:05:03,440 --> 01:05:13,040
subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly

554
01:05:13,040 --> 01:05:22,480
what they did is that they they introduced just this one field of token swan meaning this token

555
01:05:22,560 --> 01:05:29,040
just to hold but just a dot and they trained the model to give an answer after producing

556
01:05:29,040 --> 01:05:35,280
a certain number of dots that's not just like introducing Rambam and Gibberish in your training

557
01:05:35,280 --> 01:05:42,480
data it's actually quite a specific intervention that forces the model to um learn to perform

558
01:05:42,480 --> 01:05:50,800
certain computations before giving an answer um so so it makes sense to me that this couldn't

559
01:05:50,800 --> 01:05:55,200
diminish performance and like you you could do that from that it's not quite the same as just

560
01:05:55,200 --> 01:06:00,960
having that training data right um just because the token seems meaningless it's a field of token

561
01:06:00,960 --> 01:06:07,760
to dot um it's not just random gibberish it's going to throw off the model and and impede its

562
01:06:07,760 --> 01:06:14,480
its uh the optimization of its learning function or at least good downstream performance um but what

563
01:06:14,480 --> 01:06:18,320
it's going to do is going to force the model to learn that when there is a dot token it can

564
01:06:18,320 --> 01:06:22,560
allocate computation with its attention heads and other parts of the architecture in such a way

565
01:06:22,560 --> 01:06:30,640
that it's um getting towards deriving the correct token when it's finally producing

566
01:06:30,640 --> 01:06:36,240
the token that matters and that's meaningful after the series of dots um yeah I don't know

567
01:06:36,240 --> 01:06:41,600
Charles if you have another answer yeah I mean I think it's an important question because

568
01:06:41,600 --> 01:06:49,840
a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless

569
01:06:49,840 --> 01:06:57,840
symbols to an LLM input you might very well think that this will just weaken the signal to

570
01:06:57,840 --> 01:07:08,800
noise ratio and degrade model performance so it's against that background that the empirical result

571
01:07:09,040 --> 01:07:16,000
doesn't degrade model performance um ought to be regarded as an important clue about how the model

572
01:07:16,000 --> 01:07:22,320
works so I think that the the intuition behind this question is indeed part of the interpretation

573
01:07:22,320 --> 01:07:29,200
of the empirical results right it's surprising for exactly this reason and then the theory that's

574
01:07:29,200 --> 01:07:34,160
supposed to you know well this is an active inference podcast right so the theory that's

575
01:07:34,160 --> 01:07:40,480
supposed to help rid some of the surprise here is um the idea that

576
01:07:44,160 --> 01:07:50,720
given the uh architecture of a transformer where it's it has to go through all the tokens

577
01:07:51,360 --> 01:07:59,440
in every cycle um having these extra tokens gives it uh sort of more computational bandwidth

578
01:07:59,440 --> 01:08:06,400
and therefore more expressivity or more capacity to uh you know locate uh the right

579
01:08:06,400 --> 01:08:14,640
spot and parameter space and and even that in a way reminds me of so it's not just that dots

580
01:08:14,640 --> 01:08:18,800
improve performance it's not it's that it was like you mentioned it was trained to have that

581
01:08:18,800 --> 01:08:26,320
and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes

582
01:08:26,320 --> 01:08:32,880
verbatim while you're processing so that's kind of like a filler or more of like a sort of

583
01:08:33,680 --> 01:08:42,160
that was a great question it's like these are linguistic paddings that that do create time

584
01:08:42,880 --> 01:08:51,440
to to get to the meet so not only does it signal and signpost if it's being trained to have that

585
01:08:51,440 --> 01:08:56,640
meaning which then questions like so that it wasn't a meaningless dot if it if it had a um

586
01:08:56,640 --> 01:09:06,000
a cognitive or even like a a semantic um aspect I had a question how do you feel like in this

587
01:09:08,080 --> 01:09:15,520
era Cambrian explosion of diverse intelligences how can we understand capacities

588
01:09:16,320 --> 01:09:26,880
when they seem so conditional upon the setting and how the system of interest is interacted with

589
01:09:27,600 --> 01:09:34,160
like what are the practical implications for people who are studying LLMs and other

590
01:09:34,160 --> 01:09:39,920
synthetic intelligences from like a safety or reliability or performance perspective

591
01:09:39,920 --> 01:09:45,680
that was we're gonna

592
01:09:47,280 --> 01:09:48,320
no I was drawing it to you

593
01:09:50,400 --> 01:09:54,560
so so so just want to chat to some of the questions so the question is

594
01:09:58,560 --> 01:10:02,080
how the how is the notion of a capacity

595
01:10:03,040 --> 01:10:10,480
um changing when we have such different systems that seem to have intelligent behavior

596
01:10:10,480 --> 01:10:16,640
yeah and it's so dependent upon potentially initially unintuitive

597
01:10:17,840 --> 01:10:24,800
ways of interacting so how can we understand the reliability and the performance and the

598
01:10:24,800 --> 01:10:32,640
capacity of of a model other than for example by exhaustively inputting prompts

599
01:10:32,640 --> 01:10:39,920
which can't really happen what what could we really say or no and or just how do you feel

600
01:10:39,920 --> 01:10:45,120
that this work re-enters into the ways that people practically are using the models

601
01:10:46,400 --> 01:10:51,200
right okay yeah uh it's the interesting question so the first one the question is I think

602
01:10:52,000 --> 01:10:57,760
you know part of the background assumption from for this paper that I've explicitly

603
01:10:57,760 --> 01:11:04,640
defended in other work is that behavioral evidence is simply not sufficient in most cases

604
01:11:04,640 --> 01:11:11,760
to arbitrate disputes about capacities of LLMs when it comes to human cognition

605
01:11:13,760 --> 01:11:19,280
we do have to rely a lot on on psychological experiments that are ultimately behavioral

606
01:11:19,360 --> 01:11:25,520
and we do also rely on self-reports in a little more than we can when it comes to LLMs because we

607
01:11:27,280 --> 01:11:33,200
despite the move away from relying on intuition and introspection in the history of psychology

608
01:11:33,200 --> 01:11:39,440
it still has a role to play but we've by and large contributed to us by behavioral experiments

609
01:11:39,440 --> 01:11:45,760
that get increasingly sophisticated to try to reverse engineer what's going on inside the black

610
01:11:45,760 --> 01:11:55,600
box when it comes to LLMs partly because that's so different from us um relying exclusively on

611
01:11:55,600 --> 01:12:02,160
behaviorism is even more difficult because we have even less of an idea of what might be going on

612
01:12:02,160 --> 01:12:06,240
inside the black box and whether it's anything like what's going on inside all black box and we

613
01:12:06,240 --> 01:12:12,880
have reasons to think it might be very different so I think I think we both agree that we have to

614
01:12:12,880 --> 01:12:21,760
supplement this with mechanistic work that's essentially involve performing causal interventions

615
01:12:21,760 --> 01:12:27,520
on the inner mechanisms on the inner workings of the systems so decoding representation and

616
01:12:27,520 --> 01:12:32,160
computations that the systems that are in principle available to the systems and then

617
01:12:32,160 --> 01:12:36,560
intervening on them to confirm hypotheses about the causal role of these representations and

618
01:12:36,560 --> 01:12:43,200
computations and we have methods to do that and partly what we can be a little optimistic about

619
01:12:43,200 --> 01:12:47,280
this project even though it's it's it's extremely challenging especially to be scaled up to large

620
01:12:47,280 --> 01:12:53,120
models is because unlike what's happening in your sense with the brain where the range of decoding

621
01:12:53,120 --> 01:12:59,280
methods and intervention methods we have is extremely limited but for ethical and for simply

622
01:12:59,520 --> 01:13:06,720
um practical reasons that we don't just don't have ground truth access to activations in neurons

623
01:13:06,720 --> 01:13:13,440
at least that easily and we also are generally unable to make specific interventions on

624
01:13:13,440 --> 01:13:17,840
activation in the brain uh when it comes to algorithms we have full ground truth knowledge

625
01:13:17,840 --> 01:13:24,400
of all activations of every single part of the network and we also have full access to all of

626
01:13:24,400 --> 01:13:30,240
it for interventions at inference time so that that opens up a whole new range of things we can do

627
01:13:30,960 --> 01:13:37,760
and that enables us to go beyond behavioral studies and actually decode these features and

628
01:13:37,760 --> 01:13:44,080
circuits or as researchers put it in the literature or as philosophers would generally put it

629
01:13:45,280 --> 01:13:49,360
representations and computations that the system is actually making use of and try to reverse

630
01:13:49,440 --> 01:13:53,920
engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem

631
01:13:55,040 --> 01:14:01,520
projects that we have with Charles is to um suppose that we start with these

632
01:14:01,520 --> 01:14:06,720
investigative kinds as we put as as Alibol calls them these human subject capacities

633
01:14:08,720 --> 01:14:14,000
we can operationalize them and do behavioral experiments in the top down and then from the

634
01:14:14,000 --> 01:14:19,040
bottom up we can also try to reverse engineer the mechanism building blocks of the computations

635
01:14:19,760 --> 01:14:23,040
and representations that evidence may use up to solve the task

636
01:14:23,600 --> 01:14:28,160
related to that particular capacity and then we can meet somewhere in the middle and try to

637
01:14:29,280 --> 01:14:36,640
from that line of work that are purchased things from above and bring to the fore some kinds of

638
01:14:36,640 --> 01:14:42,080
mid-level abstractions as we call it or computational building blocks that might be key to

639
01:14:43,440 --> 01:14:49,120
the performance of the system in that domain so for example if you're interested in the capacity

640
01:14:49,120 --> 01:14:56,400
for reasoning you can start with this very broad human-centric notion of reasoning then try to

641
01:14:56,400 --> 01:15:01,760
operationalize it in a reasoning task then do some behavioral testing and then mechanistic

642
01:15:01,760 --> 01:15:06,320
interpretability of that reasoning task find out how the system is solving it find out how the

643
01:15:06,320 --> 01:15:12,480
algorithm is doing well and why reverse engineer building blocks that might for example have to

644
01:15:12,480 --> 01:15:19,040
be viable manipulation viable binding and then from there you might be able to either actually

645
01:15:19,040 --> 01:15:25,040
refine the notion of reasoning you started with to have a more specific and that less human-centric

646
01:15:25,040 --> 01:15:35,840
notion that is now operationalized in more low-level terms like you know that both manipulation

647
01:15:35,840 --> 01:15:42,400
of variables in certain ways and the binding of variables to theories etc so yeah so that's I

648
01:15:42,400 --> 01:15:47,040
think the general approach we take now how does that does any of that feedback into

649
01:15:47,760 --> 01:15:51,600
interactions how we humans interact with algorithms I think that's one way in which

650
01:15:51,600 --> 01:15:59,760
you could feedback is simply in terms of challenging or our spontaneous anthropomorphic

651
01:15:59,760 --> 01:16:04,960
attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you

652
01:16:04,960 --> 01:16:11,120
will interact with your cat in a slightly different way that you might maybe not rush the conclusion

653
01:16:11,120 --> 01:16:18,720
that when your cat performs a certain behavior it has understood what you think and it's modeling

654
01:16:18,720 --> 01:16:25,680
what you're thinking about what it's thinking or something perhaps you might adopt a more

655
01:16:25,680 --> 01:16:31,920
deflationary attitude to explain the behavior of your cat doesn't mean you have to love them

656
01:16:31,920 --> 01:16:39,520
any less or it doesn't mean you have to you know if that is the other thing like if you want at the

657
01:16:39,520 --> 01:16:43,600
end of the day to speak to your cat like a human because you really are a gentleman for that then

658
01:16:43,600 --> 01:16:48,000
that's you know all the more part of you in the same way if you find it useful to treat LLMs in

659
01:16:48,000 --> 01:16:55,280
the way you interact with them to to have fluid interactions with them to treat them as if they

660
01:16:55,280 --> 01:17:00,560
had beliefs these are as etc of human-like capacities then that's fine if that's for

661
01:17:00,640 --> 01:17:05,840
actual purposes but at least if that line of work that we are kind of sketching here

662
01:17:06,640 --> 01:17:14,000
ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's

663
01:17:15,840 --> 01:17:20,240
well even if we if we have that kind of intentional sense and may believe about

664
01:17:20,240 --> 01:17:26,720
who the kinds of besties they have at the background we will know that what their limitations are and

665
01:17:26,720 --> 01:17:31,520
what their actual besties are i'd make sure to go with that Charles because we're going to use

666
01:17:31,520 --> 01:17:37,600
this much yeah yeah no i agree with all that i just had a slightly different first reaction to the

667
01:17:37,600 --> 01:17:43,360
question i took the question to be in part about how to deal with the sort of prompt sensitivity

668
01:17:43,360 --> 01:17:48,720
of models the fact that sometimes we you know write something that seems natural to us but

669
01:17:48,720 --> 01:17:54,480
provokes an unexpected response from a large language model and how do we think about that

670
01:17:55,440 --> 01:17:59,760
and the first thought that occurred to me was just that we should distinguish between

671
01:18:00,880 --> 01:18:09,520
different kinds of large language models you know we have this sort of huge large language models

672
01:18:10,160 --> 01:18:17,840
which are fine-tuned to interact with us in a particular way and our and here's the central

673
01:18:17,920 --> 01:18:26,320
point they're trained on a sort of unthinkably large database whereas there are other sorts

674
01:18:26,320 --> 01:18:30,960
of large language models where the training data is more circumscribed and where we know

675
01:18:32,000 --> 01:18:39,440
in more detail you know what where you can survey what the training data says and i think

676
01:18:39,440 --> 01:18:43,200
if you're interested in you know what the mechanisms are underlying the responses

677
01:18:44,160 --> 01:18:51,680
it's certainly very helpful to look at smaller but nevertheless large language models where the

678
01:18:51,680 --> 01:18:56,960
training data is known to us because you know when you train a model on the entire internet

679
01:18:56,960 --> 01:19:03,760
there are going to be all kinds of you know subtle signals in there that we don't have much hope of

680
01:19:03,760 --> 01:19:07,760
tracing back to their source but which will influence the model behavior in all sorts of

681
01:19:08,240 --> 01:19:17,520
ways but working with these somewhat more conscripted models gets rid of that problem at least in part

682
01:19:21,360 --> 01:19:30,240
cool well where where do you see the work going or where do you plan to continue this direction

683
01:19:30,560 --> 01:19:38,080
yeah so actually so we wrote this paper this short paper for the ICML

684
01:19:38,080 --> 01:19:43,120
machine learning conference in the national conference machine learning that's happening

685
01:19:43,120 --> 01:19:49,600
this week in the data and will be getting to Vienna at the end of the week for the popular

686
01:19:49,600 --> 01:19:53,920
workshop that we're representing this paper which is a workshop on language models and

687
01:19:54,000 --> 01:19:59,920
cognitive science so there will be a very strict page limit for these ICML

688
01:20:01,200 --> 01:20:08,400
contribution which is four pages but what we want to do next is to expand this into a more

689
01:20:09,760 --> 01:20:14,880
philosophically substantive paper that's going to be a bit longer and that's going to expand on the

690
01:20:14,880 --> 01:20:19,840
more philosophically meaty parts of that of that project because everything is still a bit compressed

691
01:20:19,840 --> 01:20:24,080
in that version that we're presenting at ICML so yeah that's the next step for us this is a

692
01:20:24,080 --> 01:20:28,320
really useful way for us to force ourselves to write things down after running the box piece we

693
01:20:28,320 --> 01:20:34,080
wanted to write an academic piece now we've written kind of a condense skeleton of the piece that

694
01:20:34,080 --> 01:20:39,120
focuses more that caters more to an analogians and now the next step is to write the full

695
01:20:39,120 --> 01:20:44,400
philosophy paper or at least that part of our project to be complete and then I don't have to

696
01:20:44,400 --> 01:20:49,840
that maybe we'll have all that ideas but yeah yeah

697
01:20:53,360 --> 01:21:00,240
I got nothing to add to that cool yes well it's very interesting work I think it it brings a

698
01:21:00,240 --> 01:21:09,520
lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the

699
01:21:09,600 --> 01:21:16,640
the heat and into the the spotlight and the relevance and so it's going to be an exciting

700
01:21:16,640 --> 01:21:23,920
learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation

701
01:21:24,560 --> 01:21:31,760
till next time thank you bye

702
01:21:39,520 --> 01:21:40,900
you

703
01:22:09,520 --> 01:22:10,900
you

