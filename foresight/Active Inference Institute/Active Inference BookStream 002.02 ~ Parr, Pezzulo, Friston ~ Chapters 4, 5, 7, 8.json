{"text": " Hello, it's July 28th, 2023, and we're in Active Inference Textbook Group slash Bookstream 2.02. Thanks, Ali, for joining. So what we're going to do today is give a short overview of the chapters from the PAR at all 2022 book. We're going to do chapters four, five, seven, and eight. And we're just going to pause between them because then we'll clip them into the shorter videos, append that to the playlist, just so there's a first video overview of each of the chapters. And this is the second in that work. Alright, so we'll do chapter four. We'll just wait a few seconds and then start chapter four. Okay, chapter four is called the generative models of active inference. And it begins with a quotation, everything should be made as simple as possible, but not simpler by Albert Einstein. Ali, what is your overview thought or warning for chapter four? Okay, so after the preliminary materials in chapters two and three, which was basically largely based on providing some conceptual framework for developing the further theory, chapter four delves into much more detail in terms of mathematical formulation. And it unpacks a lot more the way that the central equations of active inference is derived and how to construct the important elements of active inference models. So say matrices A, B, C, and D and also how to put together generative models in different situations. So it basically lays out the foundation for constructing active inference models, both for discrete time situations and continuous time ones, which will be used later in chapters seven and eight. But this is probably one of the most challenging and at least mathematically dense chapters in the book. So I would personally suggest reading through this chapter really slowly. And even if we don't get to understand every single detail of the chapter, obviously we can return per required as we go through the textbook. Thank you, Ali. Yes. So let's look through the sections. Just to add on though, chapter four is one of the larger and more equation dense chapters, because it is the common kernel or basis that's then going to get applied in chapter five in the neurobiological case. There's a recipe for making chapter four in chapter six. That's the recipe for active inference modeling. Chapter seven and eight are about the discrete and the continuous time variant or subtype or motif of these kinds of things called generative models. So this is the real common root. And we'll just look at what the sections are. This chapter complements the preceding chapter's conceptual treatment of active inference with a more formal treatment. Section 4.2 from Bayesian inference to free energy. What would you say about this section, Ali? Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference. I mean, all the way back to Helmholtz theory about unconscious inference or something to that effect. I can't remember the exact term. But here, I mean, it provides in a bit more detail how we can derive free energy principle formalism using the established Bayesian inference formulation. And particularly, one of the key movements or at least one of the key decisions in through their derivation of free energy principle formulation is using Jane's inequality principle to derive an upper bound instead of just using the exact values to compute or to achieve required parameters. So that's basically, in my opinion, the key premise of section 4.2. And to see in a bit more detail how we can achieve those upper bounds using Jane's inequality directly by using, I mean, manipulations of Bayesian inference, Bayesian statistical formalism. Thanks. I'll just add one point from this section. Broadly, these are the problems of inferring states of the world perception and inferring a course of action planning. So this is again, referring to the perception and action. And everything that happens in between is the internal or the cognitive part of the inference. But this is like the blanket state cybernetic input output. And then let's look at the first equation or how much equations overall or what equations do you think we should highlight? Okay. So maybe we can, I mean, just as a general comment about these different equations, well, each of these equations provide a distinct step toward deriving the ultimate whole picture. So even if we don't quite understand how we can derive from each step to the other one, it's good to know that it's only required to understand how we get to that ultimate whole picture. But ultimately, what we would need in order to develop active inference models is the ultimate equation or ultimate whole picture. So this is just a way to elucidate the steps toward developing that whole picture. But again, it's not an essential requirement to understand the materials of the rest of the book. But if we go from, I mean, equations 4.1 toward the 4.4, or in other words, a variation of free energy, well, equation 4.1 is just a basic definition of some properties of probabilities in terms of conditional probability and so on. So equation 4.2 provides the central Jane's inequality principle and how it relates to, I mean, conditional probabilities and of course, joint probabilities. And then by using those two properties or those two equations, we ultimately get to 4.4, which is the definition of variational free energy parameter, which is the parameter of interest that needs to be optimized in order to inference to happen, or at least perceptual inference to happen in active inference models. Thanks. The only thing I'll add is f is the letter used for variational free energy. Think of it like a computer program and the arguments that it takes in or the variables that it takes in are q, which is the distribution that's under the statisticians control and y, which are the data, which are outside of the statisticians control. And do you want to describe more about anything in this equation or carry on? Just one thing that can probably be helpful is to somehow compare these steps with the initial picture we had from chapter 2, because variational free energy was first introduced in chapter 2. So it can be helpful to go back and forth between chapters 2 and 4 and try to connect the dots between the related points there. Section 4.3 generative models. All right, I'll read the first sentence, then you can give some thoughts. To calculate the free energy, we need three things. Data, a family of variational distributions, and a generative model comprising a prior and a likelihood. In this section, we outline two very general sorts of generative model used for active inference and the form the free energy takes in relation to each. Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time situations. So clearly, we would need two different types of generative models for each situation. And obviously, the generative models or the way to construct generative models for discrete time situations would vary quite a bit from the one for continuous time situations. But the general principle underlying those generative models are basically the same, which is, I mean, to somehow construct a model of the environment, I mean, either be it for the situation that is sequential in time or for the situations that need to be somehow, each moment of the situation needs to be accommodated in terms of a continuous time situation. So figure 4.2 provides some examples of both. So for, yes, let me see. Yes, so we have some examples of different kinds of generative models and case studies, if you like, and it provides various ways to show how the dependencies between variables can be modeled using these kinds of graphical probabilistic models. So one common way to represent generative models is to use these kinds of graphical probabilistic models in active inference literature, which is, at least in this case, the circles would represent the random variables, and the squares would represent the distributions, which would describe the dependencies between those random variables. So we can see the clear relationships between those parameters here, which is basically what this whole graph, what constitutes the generative model that needs to be used for different situations. And then in figure 4.3, we can compare the two different types of generative models based on whether it's discrete time or continuous time situations. So the upper picture is a generative model for the discrete time situation, and the lower picture is the parallel continuous time version of it. And as we can see, the general topology of these models are the same. The only things that differ is the use of parameters for policies or, I mean, discrete time policies or the continuous time ones. And we can obviously compare the different elements for both priors states and external states, internal states, and so on by comparing these two models here. Yeah, we often return to figure 4.3. It's kind of the Rosetta Stone of generative modeling for the context of this book, because it's then going to develop out into chapter seven and eight. And it represents a really fundamental decision made in modeling. And in the later chapters, it's also shown how it can be made into a hierarchical model that combines aspects of both. But within each level of modeling, still, these are the kinds of decisions that modelers are presented with when it comes to statistical modeling overall. So section 4.4 goes into essentially the top half of figure 4.3, discrete time. What would you say about discrete time? Okay, so the discrete time situation is obviously the archetype discrete time situation, which is the POMDP models. So at this point, I would very much like to recommend following the material from set by step paper, because in that paper, the way to construct POMDP models is described in a bit more detail. So if anyone feels like they should learn a bit more about the gaps in the details, I would very much like to recommend that particular paper. So yeah, I don't know how much detail we should go into, because it's, I mean, although it's not maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can construct these models using the concepts we've learned in previous chapters. So ultimately, we reach equations for point 13 and four point 14, which are basically the culmination of POMDP formulation using the vector notations and gradients and so on. So that's great. Then we go to continuous time. Yeah, great. A few things intervene in the continuous time chapter was that we'll just mention here, because they're kind of boxed or partitioned from the continuous time part, but they're following pages versus Markov blankets. We won't go into it here, but kind of footnote that or look at some other places where we talk about it outside of this chapter overview. Figure 4.4, Bayesian message passing. Again, a big topic. Let's kind of just go past it now. Back to the regularly scheduled continuous time generative model discussion. And then another box to the generalized coordinates of motion. So taking position plus derivatives of position. And that has some beneficial properties that are described and unpacked also elsewhere. Do you want to say anything about 4.5.2? Well, the only thing that comes to mind is although, as I said before, all the formulations here may look more, I mean, a bit too dense to understand at the first pass, but some of the key maybe components here could be obviously the material from box 4.2 and 4.3. I think are quite essential to understand the underlying principle behind deriving the continuous time situation because without Laplace approximation, what we would have in terms of free energy minimization would look very much like the Gibbs free energy. So I mean, the key distinction between the free energy principle as described in active inference literature, as opposed to Gibbs free energy, is the Laplace approximation. So this is what enables us to go from Gibbs free energy to, I mean, the variation of free energy. So yeah, that's, I mean, quite essential to make this, to be familiar with this essential approximation. And obviously, the concept of generalized coordinates of motion will come time and time again throughout the whole book, particularly in chapters eight and nine. So yeah, those two concepts, I believe, needs a bit more attention. So yeah, sounds good. Box 4.3 Laplace approximation equations, another message passing representation, and a summary. The key message to take away is that approximate Bayesian inference may be framed as minimizing a quantity known as variational free energy. This depends on a generative model that expresses our belief about how data are generated. Anything else you want to add? Nothing comes to mind at the moment, because, as I said, this is, I mean, we're still in the stage that we want to develop our essential tools to be used in the rest of the books. So here, up until now, I believe, by the end of chapter four, we have acquired all the essential necessary mathematical tools. And the next chapter, chapter five, is kind of acts like an interlude. And I don't think it's the direct, I mean, continuation of chapters one through four. So I believe the first section or the first part of the book, conceptually and mathematically ends here. So yeah, that's it. Yes, it's a little bit like the pragmatic modeling part gets foreshadowed or explored in five, now that we're all built up with four. All right, that's the end of the overview for four. Okay, chapter five is called message passing and neurobiology. What is your overview thought on chapter five? Okay, I mean, it's a kind of, I don't know, I had mixed feelings about this chapter, because on one hand, you see, as far as I understand active inference, although it originated as quote unquote, a unified theory of the brain, I don't think it's a neurobiological theory per se. Of course, there can be some correlations between neurobiological components or concepts with active inference, I mean, concepts. But I mean, it's not an essential premise of active inference theory to provide a theory, to provide a comprehensive theory about how the neurobiology of human brain or other organisms brain behave at a detailed and neuroanatomical level. But then again, it's nice to have these kinds of empirical correlations between the findings of neurobiology and the active inference theory. But I don't think it's one of active inference central assertions, at least to my understanding. Well said, very interesting framing. Well, chapter five definitely takes a very specific system of interest approach by highlighting one of the most studied areas, also one of the most relevant areas, which is mammalian neuroscience. And the chapter is going to introduce a few different motifs in the nervous system, and essentially build up towards figure 5.5, which is at the end of the chapter, and 5.5 wires together three specific neural systems that the chapter is going to focus on work in that area from. So Ali said it very well. Active inference was built up to in chapter four. Here is another level or type of science with assertions or with representations or mappings to any specific system. But this is the kind of modeling that has been built up and done by Friston, Par, Pizzolo, and others over the decades, with a focus coming from a human neuroimaging laboratory setting, a lot of focus and study and attention and funding and everything on the mammalian nervous system. But claims about the nervous system are not the basis of what active inference claims or how it's derived. But this is like an example case study in neurobiology, connecting back to some of the formalisms that we've just seen introduced in chapter four. Yeah, and to add a minor point to which I just said, I think it's important to draw attention to the last sentence of the last paragraph of the first page. It is important to draw a distinction between a principle, i.e. the minimization of free energy and a process theory about how this principle may be implemented in a certain kind of system. So I think this sentence here frames this chapter in relation to all the other technical chapters of this book. So if every other chapter is about developing, or at least up to now, was about developing the principled formalism of active inference, now chapter five provides a kind of preliminary sketch for the process theory of active inference, which is obviously far from an extensive theory, it's just a single chapter. But then again, it can provide some important signposts for anyone who wants to further investigate this area. Awesome. Free energy principle, Bayesian mechanics, all things in that area are on this principle, not responsive to empirical data. And then the process theory is about how the principle is implemented. So the specific generative models that are made, and how well they map, or how well they do in a portfolio of models that can have very different goals and assumptions and all of this. But the process theory implementation lets us develop hypotheses that are answerable to empirical data, like what is the kind of information or relationship between photons hitting the retina and changes in activity in neural systems. And that's an informational question or can be abstracted in a way to an informational question that it turns out does have empirical support and results in unique explanations and predictions. That doesn't mean that it always results in unique explanations and predictions, but a lot of citations are provided here. That's what we can explore in chapter five. The last paragraph of the first section describes that they're going to look at the three different neural systems. Okay, section 5.2, microcircuits and messages. What do you think, Oli? All right. So I mean, this chapter begins from how message passing happens in neurobiological terms and compare it to the way active inference frames this message passing mechanism. And specifically, if we look at figure 5.1 and compare this figure to the ones we've seen before in chapters one through four, I think it was in chapter four, we can see some clear parallels between how this kind of cortical message passing happens in the brain versus how it is framed in active inference literature. And as we can see, it's clearly inspired by the neurobiology of the brain. But then it's important to keep in mind that it's not a direct one-to-one mapping between these two models. This is just a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind to somehow be a bit more confident about the viability of the theory we want to use for message passing and active inference, which is to say it's not some haphazard theory that's just been developed for practical reasons. It has some basis in neurobiology, although it's not necessarily fully congruent with every detail of neurobiology. Great. The specific example is going to involve this one region of mammalian cortex tissue that has these six layers. And there's a ton of neurobiology. The big takeaway for figure 5.1 is that it's possible to graphically lay out nodes and variables and find some empirical correspondences. Again, some unique explanations and predictions in certain cases. And that's one kind of modeling where it's really trying to understand and improve the ability to do correlation and intervention and counterfactual causal type analysis with the real system of interest. Or in a more pedagogical setting or a research setting or an industrial setting, you might sweep across large families of structures of models and there's no need to be grounded to any biological structure at all. So this is just describing the specific neuroanatomical research that really arose out of the imaging work at UCL and the SPM package. So that's where a lot of this comes from. 5.2? Yeah, good. And sorry, just as a side note, I think watching one of Thomas Parr's lectures on neurobiology of active inference, which is available on YouTube, would really help to understand the materials of this chapter better. So I highly recommend watching that one. Thanks. Figure 5.2 gives a re-rendering of a kind of classical view of a hierarchical predictive coding system works. So here, abstracting a layer from the tissue six layer to just two layers here, computational layers now, and then showing how there's hierarchical communication within a layer, but also others, they're signaling within a layer and there's a hierarchy in Bayesian modeling with variables that are higher order predictions about other variables. And that's the basis of the predictive coding architecture. So 5.2 looks at some ways that the something that resonates with the cerebral cortical architecture enables what might computationally look like or have some really strong and explanatory values in actually relating to computationally a hierarchical Bayesian model, which could do various general tasks. All right, 5.3 is motor commands, leaving the prefrontal cortex going down to the butterfly looking cross section here. What is 5.3? Okay, so 5.3 moves to the other half of active inference framework, which is, I mean, how it can model the decision making and ultimately the movement of the agent in order to minimize the expected free energy as opposed to variational free energy that we saw in perceptual half of active inference. So it again provides a kind of correlation or analogy between the structural neural anatomy, particularly related to, I mean, the motor commands and how it can relate to active inference, particularly the continuous time active inference. So we can see that for, I mean, for the external event or, I'm sorry, for the external state, we can take, for example, the proprioceptive afferent, and then this proprioceptive afferent acts as a kind of Y for the continuous time active inference, which needs to be, I mean, processed in a way to optimize the expected free energy and how it relates to both attention and precision. We'll see a bit more detail about those terms and the relation between them in chapter eight, but I think here section 5.3 provides a good summary about the general paths through the motor command systems of neurobiology. Great. I'd say while the previous case study focused on how the connectivity within and between the cortical columns could have a computational relationship with a Bayesian hierarchical predictive coding architecture, the argument of the second case study is that a continuous input, continuous output, kind of set point seeking reflexive motor behavior with a moving set point with a descending moving set point enabling motion by changing ultimately the set point and enabling a variation in the strategies to reach that set point through different mechanisms. This is also describable in a compatible way. That's a shorter section. Now, section 5.4, subcortical structures. What would you say about this section? Okay. So, subcortical structures are very important in the decision making and, I mean, of the agents. So, obviously, here we need another kind of analogy between the way that these plannings and decision makings happen neuroanatomically with the way that that it's framed in active inference. But again, we can see it's clearly based on, I mean, at least some of the important elements we've seen from the previous chapters. So, for example, we saw how policy is described or how it relates to outcomes and preference and so on. We can see those elements are directly inspired by neuroanatomical structures. So, I guess that's, at least in my opinion, this section here 5.4 seems a bit more sketchy in the meaning that it doesn't go into quite the extensive details about how those structures can be compared. But for anyone who wants to further investigate these topics, there are some useful references put on here on pages 93 and 94. So, yeah. Thanks. Yeah, it's really abbreviated and over viewed. But we get an interlude from table 5.1 with putative roles of neurotransmitters. So, same perspective that we took before on neuroanatomical functionalism here directly translates to neurotransmitter reductionism or essentialism or something like that. So, certainly all neurotransmitters and molecules that play variable roles in different settings. And this is the neat and scruffy manifold all over again. One person might say, well, we need a theory for every acetylcholine molecule in the world. They're all in a unique context. And someone else says, all neurotransmitters are described by one parameter in this model. I'm getting value from it. So, to me, that's an account. And somewhere in between is the work in this space, which is making an attempt to have a principled and falsifiable approach to model the computational aspects of specific regions and contexts and settings. And so, acetylcholine, noradrenaline, dopamine and serotonin are given a little mini review here. And so, it's not an exhaustive or an exclusive claim. It's kind of a provocation from computational and molecular neuroscience. And people can look into the papers and also ones that probably have been published since. 5.6 goes to continuous and discrete hierarchies, which is graphically overviewed in figure 5.5. So, what would you say about this? Yeah, one interesting thing about this section is the observation that our lower-level engagement with the environment can be most successfully characterized with continuous time formulations. But as we go up on the level of cognitive concepts or at the level of cognitive hierarchies, and we come to concepts such as, I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time situations would probably be more efficient to characterize the behavior of the agent. So, this multi-scale structure of active inference modeling is quite evident in the way that our message passing happens in our brain in terms of our lower-level data processing, often to consolidating the higher-level cognitive concepts and ontologies. Awesome. Thank you. To me, figure 5.5 demonstrates the kind of whole-of-body approach that you could imagine. There's so many organs and systems and phenomena for which there aren't specific generative models, so little can be said about situations where no generative model has been articulated. And here's one where it has, so it gives you also, it's kind of like reading a Drosophila melanogaster review paper relatively. It's like, this is how much work it takes to get to this state of knowledge in an insect. So then in another insect, do we know less about that insect empirically and genetically? So consider this to be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive systems, at least we know. So there's that additional kind of like self-reflexive aspect to this chapter that is not a cornerstone of active inference, but here it's just presented in a synthetic case study. Anything else you want to say about 5? Nothing particular comes to mind. Thank you. All right. Okay. Chapter seven is called active inference and discrete time. Chapter seven is the first in a pair of chapters with chapter eight on discrete and continuous time. So they're kind of like two forks of a river that we discussed in chapter four and before and described the recipe in chapter six. Now seven and eight are kind of like one level deeper, going from the kind of all of this group of animals to one level deeper into its classification scheme on the way to the specific generative model for which it's actually given in its totality. But everything prior to that is about the learning about its principles and this is kind of on the trunk of the path to discrete time modeling, just like chapter eight will be about continuous time modeling. What would you add in? Okay. So I think chapters seven and eight really helps to understand in a more practical way how the materials from particularly chapters one through five applies in real-time situations. So even if we somehow didn't get to understand every details of chapters one through four, when we come to chapters seven and eight, I think some of those uncertainties about our understandings can be clarified at least in a practical sense. So I believe these two chapters are really helpful in order to consolidate our understandings from the previous chapters. Awesome. Well said. So it's going to involve specifying some discrete time models. Seven point two goes into perceptual processing and the general structure of the chapter is going to walk through a series of examples that build in complexity where they first start with perception in seven point two, introduce decision making and then describe a few more types of motifs or cognitive structure or patterns and also check out step by step and model stream one where it's built up to in a different way. So the first example is I'll let you describe it since it's musical. Okay. So yeah, the first example is the situation in which we try to describe the performance of an amateur musician in terms of how we listen to the performance of an amateur musicians in terms of the predictions we get from our anticipation of the following notes as opposed to the actual notes that's being played. So these kinds of anticipatory reaction, listening reaction to the musician can be successfully formalized using discrete time active inference by putting together the matrices A for the states and matrix B for the transition between the states or the transition probabilities which in this case describes the probability from going from one note to the other and obviously the actual sequence that's been played which can be described with the matrix D. So and another point I wanted to point I wanted to mention is for anyone who has downloaded this chapter before, I don't know, I think about June or something, I recommend re-downloading it from MIT's website because they have corrected some of the typos that was previously present in this chapter, particularly in figure 7.2. Cool. So this graphical model where a person is listening, this is a general perceptual Bayesian framing, it's specified. Just like with any other equations, there's a lot to look into, but A indicates the probability of an outcome given a state. This is saying if it were all on the diagonal identity matrix, this is kind of a common motif, then states kind of map to themself. So in the context of, in the context of this model, A represents the mapping between the observed note and the underlying hidden true note. FNB describes the transition matrix of how those change to time D is the prior. They're specified. Figure 7.2, do you want to describe it? All right, so in figure 7.2, or at least the incomplete version of figure 7.2 we see here, well, at the upper left part of the picture, we see, I mean, the beliefs about each note at each step, at each time step. And at upper right, we somehow translate those beliefs into specific numerical values. So instead of just assigning some continuous values, we simplified the situation by assigning some discrete numerical values for each note. And then the lower left is supposed to show the free energy gradients over time or in other terms, the prediction errors we get from, I mean, comparing our predictions with the actual outcomes. So lastly, the lower right picture shows, in parallel to the upper right picture, determines the values of these errors. So we can see both the continuous, the initial, at least initial continuous assignment and values, and then the further discretizing of the values in order to get the discrete time situation or the more tractable discrete time situations. Okay, so it's a general passive inference task where there's priors about how states are going to change through time, and then there's real data coming in. So that's the kind of classical predictive coding, video compression, Kalman filter, Bayesian setting. 7.3 introduces a key motif, which is decision making and planning as inference. So this is the idea of having a Bayes graph where the variables can relate to different things. There's high composability. And here the idea is that a variable is going to be proposed that we can do inference about that describes the process of decision making or policy selection. So what would you say about 7.3? Okay, so 7.3 is obviously similar to what we saw in chapter four. And if I'm not mistaken, even the topology is exactly the same with that picture we saw previously. So this is the initial setup or which acts also as a review about how these different components upon DP generative models need to be described in such situations. But ultimately, the specific case study we come across in this section is the attempt to model the behavior of the mouse in a teammate, so the rat in a teammate. So especially teammates containing an aversive stimulus in one arm and an attractive stimulus on the other. So this is this can act as a kind of toy example to use this kind of probabilistic modeling to describe these situations. Thanks. So that leads us right to figure 7.4. Here's a visualization of the situation with the rat in this case, where there's a pleasant and aversive stimuli on each end of a decision point. And there's also a epistemic opportunity to receive some information about the context that the animal is in. And so that setting is described for both the case with white on the left, black on the right, and black on the left, white on the right. And those are shown in terms of their differences in the matrices, the explicit specification of the generative model. Visualizations show some of the slices of the B variable, which reflect different transition probabilities. C represents the preferences, which are expressed over the observable states. D reflects the priors on the different states that need priors. 7.4. What would you say about this? Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously saw in chapters 3 and sorry, 2 and 4, which is how the exact formulation for expected free energy can be used, sorry, variation free energy can be used to formulate the tradeoff between the I mean, information seeking and or at least between the epistemic value and information seeking. So here, it uses, again, that rad example in a bit more, more extended and elaborate form to formulate the epistemic value of observing Q in a given location. And figure 7.7 is a representation of this situation. But another situation that's been, let me see, yeah, in 7.9, another case study discussed here is the situation of the psychotic eye movements. And because it is something that can be quite successfully described or characterized in terms of information seeking versus the epistemic value. And the situation here is, let me see, yeah, shown visually in figure 7.9, which clearly shows how our visual psychotic eye movements can be described in such a way as to kind of trace the trajectory of our eye movement among different regions of the visual space. And how the information we gather from a given region can affect the, I mean, the subsequent trajectories of our psychotic eye movement. So, yeah, that's basically the main premise of this section, I guess. Nice, great. 7.5? What would you say about it? Okay, so 7.5, again, adds another dimension to the previous formulations. And this time, we get to update the generative models by learning. And so the generative models for this situation is a bit more complicated than the previous ones, because it now needs to account for a mechanism or a way to update the matrices we had before. So in the previous situations, we didn't account for learning, per se. But here, we directly update our general, sorry, the word update can be confusing here. We get to somehow improve our generative models to accommodate for these updating accounts. And yeah, so the situation here, or the case study here, which somehow elucidates the way that the learning can be accounted for with these models. Is again, a toy example of a creature in a simple world of black and white tiles, which kind of tries to find a path to reach a given destination, a certain destination. So it is more complicated than the situation we had for the rat example, because it only had, I mean, simple trajectories that needed to traverse. But here, the creature or the agent, in this case, needs to do lots of lots more learning and information seeking and so on. So all the previous elements is kind of combined in this example. And it's a really good example to see how the different components of active inference can be connected to each other. Nice. And 76 hierarchical or deep inference burst a box 7.3 interlude on structure learning boxed off topic and a lot to say. But structure learning broadly refers to learning the structure about a model, using the same types of methods that you might to do inference on, for example, a more observable sensor data reading, something like that. This section works towards the idea of nested inference or multi scale modeling. What would you say about figure seven 12? Okay, so again, this situation is, I think, the most complex situations of this chapter, which builds up from the previous sections. And this time, it adds another layer to accommodate for the inferences that happen in different time steps. So in this case, we have a multi time or multi scale inference and learning happening, both at the levels of learning and at the levels of information seeking. So this, this is represented in figure seven point 12, which represents how kind of this fractal generative model can be seen as a component in this multi scale, a bigger generative or as a kind of leaf in this bigger, bigger generative model. So it can be seen as a lower level inference happening at the leaf level, going up to the hierarchy and influencing, sorry, collaborating on the whole process of learning and inference at the higher level. So yeah, I guess that's somehow summarizes this figure. So if you have anything to add. That's, that's great. It's an example of the composability of generative models, what we've talked about and had Toby Sinclair Smith describe as as the compositional cognitive cartography, and just what kinds of connectors can and can't you do? And how can that motif that the discrete time model introduces? And then the rest of these features, including action and learning and so on get layered in on top. What can you do with that? 713 gives another example. Do you want to say anything about it or maybe continue on? Yeah, so the case study here is the example of linguistic, I mean, language learning through reading. So not language learning. Maybe it's just what happens in reading. Yeah, in comprehension. So what happens when reading and in an anticipatory way, the words that that comes each after the other. So why this kind of situation can be most successfully characterized with this kind of modeling, because it involves different scales of learning and comprehension, both at the level of, I mean, reading at the level of somehow observing the letters and then going on to the words and then word groups and so on. So yeah, that's a really interesting way to, again, combine all of those elements into a single unified model to see how those different timescales, slow and fast timescales operate together to build this more encompassing model, more encompassing generative model of the situation. Great. Any closing thoughts on 7? Nothing particular, not bad. Thanks. All right. Next chapter is chapter eight, which is going to go into the continuous time. All right. Chapter eight is called active inference and continuous time begins with that timeless quote, everything flows, nothing stands still. So what would you say about chapter eight? All right. So this chapter probably is my most favorite chapter in the book, because of my own personal interest in, I don't know, the process materials and so on. But yeah, so chapter seven acts as a really good starting point for anyone who wants to develop the discrete time situations, to model discrete time situations within active inference framework. But in chapter eight, we kind of get to model a bit more interesting or, let's say, more involving situations. And they're not necessarily toy examples we saw at least at the beginning of chapter seven. So obviously, as the title suggests, this chapter deals with the continuous time situation. So in that case, we'll need to, maybe at this point, refresh our memory about what continuous time situation involves by reading the relevant parts, reading or reviewing relevant parts of chapter four. So yeah, in chapter four, we saw that the generative model for continuous time situation derives from the it is a stochastic calculus in terms of putting the whole process into two elements, two stochastic equations, one of which is the actual state, the condition of actual states or the behavior of the actual states. And the other one is the randomness that we need to account for in each real time continuous time situations. So that's what we get here in equation 8.1. And then, building up from that equation, we, it generalizes that equation to involve, I mean, the functionals of G and F instead of just the single valued functions of G and F. So then we get to put that into the situation that can be used for describing the behavior of dynamical systems, which is a very well known situation to use these kinds of stochastic equations. And it's widely studied how those, those kinds of dynamics can be characterized, especially in recent Bayesian mechanics paper by Dalton, Saktiv Atevel and others. So, and then it gets to some more specific examples such as Lothgabal-Terra dynamics and synchronicity and so on, in order to show how these kinds of dynamics can be elaborated upon and can be generalized to, and enables them to characterize more complex situations. So, yeah, that's a really short and brief overview of the whole chapter. Maybe we can talk about a bit more details as we go through it. Great. Well said. Well, I'm sure for another day, the philosophical implications of eight, seven and eight, and high road and low road, and all these other parts of the textbook, great topics. I agree. I would see chapter eight as demonstrating continuity with some classical continuous time modeling motifs from a few different areas of dynamical system science, which is applied in like many, many, many fields, but these are some classic examples. So, figure eight point one goes a little bit more into depth, or at least into more formalism detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive data coming in, and then a differential being calculated with the set point, which reflects a descending prediction from a decision making layer. And that can be viewed as this kind of mechanics that plays out in a phase space in continuous time, like a spring moving around with someone making a certain path with an attractor, and a spring being dragged around something in that area. Box eight point one goes into a very fascinating topic. Do you want to describe it? Well, it's maybe one of the most thought provoking pages of the whole book. And if I remember correctly, in all of the cohorts, this particular box I mean gives always gives rise to lots of questions, because of some of the interesting and at least initially counterintuitive claims here. But I don't want to spoil it. So but as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief discussion about the comparing these terms precision, attention, and sensory attenuation, and the relation and similarities and difference between these two, these three terms, and how each understanding each of them is essential to understanding the other ones. But as I said, it's a really interesting topic, which gives rise to lots of discussions. And I believe it's one of those topics that that's worth looking a bit more looking into in some other literature as well. Great. Well said. What a cliffhanger. Next, they go to a classic model family called Laka Volterra. These dynamics inherit from characterizations of predator prey dynamics in ecology. So it's kind of a classical ecology model shown in figure 8.2. On the top, it's actually the ecosystem model. Plants, herbivores and carnivores, which follow different kinds of oscillatory trends in continuous time. And so that also has enabled it to be applied for other so-called winnerless competitions. And that relates to topics like neural Darwinism and also neural dynamics, where things have kind of oscillatory relationships with each other, which are being modeled as a continuous time underlying process with a lot of measurement noise and discretization through space and time. Those are the kinds of algorithms that SPM explores more. And there's Laka Volterra and a lot of other dynamical systems theory in SPM. So active inference kind of adds action and more to what was laid out from a pure dynamical systems theory in SPM. Here, it really is just showing the ecology example and how you can project. If you have three different species, you can think about that motion in a cube or tetrahedron. And then you could project onto kind of like looking at a lower dimensional manifold relating just two of the three species. And that evinces this kind of oscillatory but also moving behavior. That gets connected in figure 8.3 to neurobiology. What would you say about this? Okay, so here in figure 8.3, we see some applications of Laka Volterra dynamics. So the left column here represents what happens in, I mean, in eye blinking, eye blink conditioning. So, of course, here we need to account for, I mean, the expected states of the sequences of events that happens in the eye blinking. So the upper left figure shows the expectations in terms of time. And then the parallel right hand side equation, sorry, right hand side figures, shows the Laka Volterra system that is applied in the handwriting situation. So as we can see, although the, I mean, mathematical technology is the same or at least the modeling technology is the same, the outcome of each situation varies drastically in two distinct neurobiological behavior, not neurobiological, but biological behavior. So, yeah, we can see how the same modeling framework can give rise to different outcomes based on what parameters needs to be optimized, what parameters are selected for modeling and so on. So I believe it's a quite interesting example to compare handwriting and the blinking together and how those can be compared to each other using the Laka Volterra dynamics. Great, thank you. Box 8.2 gives a variant on the learning here presented with the formalism for continuous models, kind of a technical aside. Section 8.4 is about generalized synchrony. So figure 8.4 is going to visualize one of the classic dynamical systems, which is the Lorenz attractor. So what would you say about this figure? Okay, so this section is truly interesting because when one thinks of active inference, probably the first situations that comes to mind is the situations in which we have quite well defined probability distributions for different parameters. But as we can see here in section 8.4, actually some of the formalism of active inference can be successfully used to characterize even chaotic systems and in particular the way in which two chaotic systems can be synchronized with each other. So this is a classic example of a chaotic Lorenz system, and it draws upon from some of Professor Pristin's earlier work on birdsong synchrony. And as a side note, any literature before 2016 is considered earlier history in active inference literature because it evolves quite rapidly. So yeah, this kind of synchrony between two chaotic systems can be interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of mind in the sense that how exactly can we understand or can two agents can trace each other's trajectories without, I mean, engaging in any direct exchange of observations between their internal and external states. So yeah, that's a really good example and I believe one of the most interesting examples of how active inference can even account for these kinds of behavior. So the rest of the section goes into the details of how this kind of synchrony between multi-scale Lorenz systems can happen and how can we formulate it mathematically in terms of continuous time active inference. Awesome. And there's been more recent work on Mark Alblanket since stochastic chaos, but the bird example is a classic. 8.5 goes into hybrid discrete and continuous models. So this could be kind of like an in-between chapter of seven and eight, but now that we've been introduced to the pure form of discrete and the pure form of continuous models, here's shown that that composability extends to so-called hybrid models, where here the lower level visually is using the continuous time formalism and the higher level is describing the little line added here, the discrete time formalism. And this was the similar structure described by the authors of the paper, active inference does not contradict folk psychology, where they describe this lower level as motor active inference, which was closely allied with the spinal arc reflex shown above. And then this higher level, they call decision active inference, because in that case, it was referring to a discrete decision. And so they used that kind of basic motif of continuous activity or continuous time modeling at the more peripheral aspects of a cognitive entity. And like Ali said, more discretization and hybridization as well at higher levels of the cognitive modeling. And that type of an architecture here, instead of describing who wants the ice cream cone, I believe, here, it's going to be a mixed or hybrid model that is going to call back the isocade system, where there's a fixed point that is able to be moved as a set point. And then there's a continuous time isocade that pursues the new fixed point. And so that's analogous to a new set point or fixed point being specified from the top down muscle command about a new location for a muscle, followed by movement towards it. This is a muscular activity that is realizing that but but not in the elbow coming away from the hot stove. This is about the ice caking to an epistemic foraging location specified by top down hierarchical systems. 8.3 describes little technical aside on mixture of Gaussian Gaussian mixture models, kind of a technical modeling note. And 8.6 closes. It says it's a huge topic and much has been left out. And so they list in table 8.1 key advances in continuous time models. And those areas are synthetic bird song, ocular motor delays, conditioned reflexes, smooth pursuit, eye movement, psychosis, illusions, saccades, action observation, attention, hybrid models and self organization. And that's chapter 8. What else would you say? And also what would you kind of lead someone to in the philosophical implications of 8 because it sounds kind of cool? Okay, so well, the case of continuous time active inference. I think it leads to really interesting questions, both in terms of philosophical questions and also more practical modeling questions about what parameters needs to be accounted for and so on. And as I said, I believe it's a more more interesting way of if not interesting, but but at least more involved way of doing active inference modeling. But one thing that one of the philosophical questions that Mao and I have explored in our paper is how the processes of I mean, ontological processes can philosophically described using FPP assertions in terms of their interaction with the environment in which they co constitute themselves. And we don't necessarily distinguish between between the internal and the external states. So one obvious example of this is that generalized synchrony example that we saw in this chapter, in which we don't necessarily distinguish between which of the birds act as the agents and which one is the environment or the vice versa. So these kinds of co constitution of the environment and the agent, which gives rise to the partitioning of state space through a Markov blanket is one of the interesting philosophical points that I think needs to be elaborated a bit more using some of the recent advances in philosophy, such as the tools that's been developed in new materialism school or some other philosophical approach approaches. But yeah, these kinds of what exactly gives rise gives rise to emergence, what is the ontological status of emergent properties and so on, are some of the burning questions for many philosophers today. And I believe active inference and particularly continuous time active inference provides a clear, precise mathematical formalism. Even if not to answer these questions, but at least to explore it in a more rigorous and practical way, and also practical and a tractable way. So this is the area that I believe philosophy and science are beautifully intertwined into a coherent view of not only the phenomenon of interest, but even about the whole world. Wow. Wow. Pretty cool. Yeah, a lot to say about that topic. After completing chapter seven and eight, you've seen the kind of two major branches or two major motifs of just one kind of modeling. But these kind of models have so many different forms that that's why it's such a hands on process to specify the generative model in chapter six and fit it with data in chapter nine. Those are all what's required. And that's kind of the last mile of where these discussions about general motifs gets you. But also playing with these pedagogical models can be really helpful, because it will help you understand the basic patterns and relationships and start to see see different patterns in the graphical models and know from there what levels of technical processes can be kind of coarse grained over. All right. Okay, well, that's it. I guess next time we will do probably nine, 10, and maybe something else. All right, I'll end it now. Thanks, Holly. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.4, "text": " Hello, it's July 28th, 2023, and we're in Active Inference Textbook Group slash Bookstream", "tokens": [50364, 2425, 11, 309, 311, 7370, 7562, 392, 11, 44377, 11, 293, 321, 434, 294, 26635, 682, 5158, 18643, 2939, 10500, 17330, 9476, 9291, 51184], "temperature": 0.0, "avg_logprob": -0.25370248758567954, "compression_ratio": 1.2094594594594594, "no_speech_prob": 0.13796654343605042}, {"id": 1, "seek": 0, "start": 16.4, "end": 25.8, "text": " 2.02. Thanks, Ali, for joining. So what we're going to do today is give a short overview", "tokens": [51184, 568, 13, 12756, 13, 2561, 11, 12020, 11, 337, 5549, 13, 407, 437, 321, 434, 516, 281, 360, 965, 307, 976, 257, 2099, 12492, 51654], "temperature": 0.0, "avg_logprob": -0.25370248758567954, "compression_ratio": 1.2094594594594594, "no_speech_prob": 0.13796654343605042}, {"id": 2, "seek": 2580, "start": 25.8, "end": 35.480000000000004, "text": " of the chapters from the PAR at all 2022 book. We're going to do chapters four, five, seven, and eight.", "tokens": [50364, 295, 264, 20013, 490, 264, 21720, 412, 439, 20229, 1446, 13, 492, 434, 516, 281, 360, 20013, 1451, 11, 1732, 11, 3407, 11, 293, 3180, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13806723259590767, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.07153856009244919}, {"id": 3, "seek": 2580, "start": 37.56, "end": 43.8, "text": " And we're just going to pause between them because then we'll clip them into the shorter videos,", "tokens": [50952, 400, 321, 434, 445, 516, 281, 10465, 1296, 552, 570, 550, 321, 603, 7353, 552, 666, 264, 11639, 2145, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13806723259590767, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.07153856009244919}, {"id": 4, "seek": 2580, "start": 43.8, "end": 49.480000000000004, "text": " append that to the playlist, just so there's a first video overview of each of the chapters.", "tokens": [51264, 34116, 300, 281, 264, 16788, 11, 445, 370, 456, 311, 257, 700, 960, 12492, 295, 1184, 295, 264, 20013, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13806723259590767, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.07153856009244919}, {"id": 5, "seek": 4948, "start": 49.48, "end": 59.8, "text": " And this is the second in that work. Alright, so we'll do chapter four. We'll just wait a few", "tokens": [50364, 400, 341, 307, 264, 1150, 294, 300, 589, 13, 2798, 11, 370, 321, 603, 360, 7187, 1451, 13, 492, 603, 445, 1699, 257, 1326, 50880], "temperature": 0.0, "avg_logprob": -0.11619887144669243, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.010167205706238747}, {"id": 6, "seek": 4948, "start": 59.8, "end": 70.52, "text": " seconds and then start chapter four. Okay, chapter four is called the generative models of active", "tokens": [50880, 3949, 293, 550, 722, 7187, 1451, 13, 1033, 11, 7187, 1451, 307, 1219, 264, 1337, 1166, 5245, 295, 4967, 51416], "temperature": 0.0, "avg_logprob": -0.11619887144669243, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.010167205706238747}, {"id": 7, "seek": 4948, "start": 70.52, "end": 74.67999999999999, "text": " inference. And it begins with a quotation, everything should be made as simple as possible,", "tokens": [51416, 38253, 13, 400, 309, 7338, 365, 257, 47312, 11, 1203, 820, 312, 1027, 382, 2199, 382, 1944, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11619887144669243, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.010167205706238747}, {"id": 8, "seek": 7468, "start": 74.68, "end": 82.76, "text": " but not simpler by Albert Einstein. Ali, what is your overview thought or warning for chapter four?", "tokens": [50364, 457, 406, 18587, 538, 20812, 23486, 13, 12020, 11, 437, 307, 428, 12492, 1194, 420, 9164, 337, 7187, 1451, 30, 50768], "temperature": 0.0, "avg_logprob": -0.10999580730091442, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.002756968606263399}, {"id": 9, "seek": 7468, "start": 85.32000000000001, "end": 94.28, "text": " Okay, so after the preliminary materials in chapters two and three, which was basically", "tokens": [50896, 1033, 11, 370, 934, 264, 28817, 5319, 294, 20013, 732, 293, 1045, 11, 597, 390, 1936, 51344], "temperature": 0.0, "avg_logprob": -0.10999580730091442, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.002756968606263399}, {"id": 10, "seek": 7468, "start": 95.16000000000001, "end": 103.0, "text": " largely based on providing some conceptual framework for developing the further", "tokens": [51388, 11611, 2361, 322, 6530, 512, 24106, 8388, 337, 6416, 264, 3052, 51780], "temperature": 0.0, "avg_logprob": -0.10999580730091442, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.002756968606263399}, {"id": 11, "seek": 10300, "start": 103.96, "end": 110.92, "text": " theory, chapter four delves into much more detail in terms of mathematical formulation.", "tokens": [50412, 5261, 11, 7187, 1451, 1103, 977, 666, 709, 544, 2607, 294, 2115, 295, 18894, 37642, 13, 50760], "temperature": 0.0, "avg_logprob": -0.07362058869114628, "compression_ratio": 1.5886075949367089, "no_speech_prob": 0.00307416426949203}, {"id": 12, "seek": 10300, "start": 110.92, "end": 119.48, "text": " And it unpacks a lot more the way that the central equations of active inference", "tokens": [50760, 400, 309, 20994, 7424, 257, 688, 544, 264, 636, 300, 264, 5777, 11787, 295, 4967, 38253, 51188], "temperature": 0.0, "avg_logprob": -0.07362058869114628, "compression_ratio": 1.5886075949367089, "no_speech_prob": 0.00307416426949203}, {"id": 13, "seek": 10300, "start": 120.28, "end": 127.48, "text": " is derived and how to construct the important elements of active inference models.", "tokens": [51228, 307, 18949, 293, 577, 281, 7690, 264, 1021, 4959, 295, 4967, 38253, 5245, 13, 51588], "temperature": 0.0, "avg_logprob": -0.07362058869114628, "compression_ratio": 1.5886075949367089, "no_speech_prob": 0.00307416426949203}, {"id": 14, "seek": 12748, "start": 128.44, "end": 137.32, "text": " So say matrices A, B, C, and D and also how to put together generative models in", "tokens": [50412, 407, 584, 32284, 316, 11, 363, 11, 383, 11, 293, 413, 293, 611, 577, 281, 829, 1214, 1337, 1166, 5245, 294, 50856], "temperature": 0.0, "avg_logprob": -0.17545338396756155, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.009118789806962013}, {"id": 15, "seek": 12748, "start": 138.12, "end": 144.04, "text": " different situations. So it basically lays out the foundation for", "tokens": [50896, 819, 6851, 13, 407, 309, 1936, 32714, 484, 264, 7030, 337, 51192], "temperature": 0.0, "avg_logprob": -0.17545338396756155, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.009118789806962013}, {"id": 16, "seek": 12748, "start": 147.0, "end": 155.08, "text": " constructing active inference models, both for discrete time situations and continuous time", "tokens": [51340, 39969, 4967, 38253, 5245, 11, 1293, 337, 27706, 565, 6851, 293, 10957, 565, 51744], "temperature": 0.0, "avg_logprob": -0.17545338396756155, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.009118789806962013}, {"id": 17, "seek": 15508, "start": 155.08, "end": 165.16000000000003, "text": " ones, which will be used later in chapters seven and eight. But this is probably one of the most", "tokens": [50364, 2306, 11, 597, 486, 312, 1143, 1780, 294, 20013, 3407, 293, 3180, 13, 583, 341, 307, 1391, 472, 295, 264, 881, 50868], "temperature": 0.0, "avg_logprob": -0.06164591429663486, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.012809298001229763}, {"id": 18, "seek": 15508, "start": 166.36, "end": 174.28, "text": " challenging and at least mathematically dense chapters in the book. So I would personally", "tokens": [50928, 7595, 293, 412, 1935, 44003, 18011, 20013, 294, 264, 1446, 13, 407, 286, 576, 5665, 51324], "temperature": 0.0, "avg_logprob": -0.06164591429663486, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.012809298001229763}, {"id": 19, "seek": 15508, "start": 174.28, "end": 183.32000000000002, "text": " suggest reading through this chapter really slowly. And even if we don't get to understand", "tokens": [51324, 3402, 3760, 807, 341, 7187, 534, 5692, 13, 400, 754, 498, 321, 500, 380, 483, 281, 1223, 51776], "temperature": 0.0, "avg_logprob": -0.06164591429663486, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.012809298001229763}, {"id": 20, "seek": 18332, "start": 183.32, "end": 191.0, "text": " every single detail of the chapter, obviously we can return per required as we go through", "tokens": [50364, 633, 2167, 2607, 295, 264, 7187, 11, 2745, 321, 393, 2736, 680, 4739, 382, 321, 352, 807, 50748], "temperature": 0.0, "avg_logprob": -0.12025934277158795, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0013247126480564475}, {"id": 21, "seek": 18332, "start": 191.72, "end": 200.04, "text": " the textbook. Thank you, Ali. Yes. So let's look through the sections. Just to add on though,", "tokens": [50784, 264, 25591, 13, 1044, 291, 11, 12020, 13, 1079, 13, 407, 718, 311, 574, 807, 264, 10863, 13, 1449, 281, 909, 322, 1673, 11, 51200], "temperature": 0.0, "avg_logprob": -0.12025934277158795, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0013247126480564475}, {"id": 22, "seek": 18332, "start": 200.04, "end": 208.35999999999999, "text": " chapter four is one of the larger and more equation dense chapters, because it is the", "tokens": [51200, 7187, 1451, 307, 472, 295, 264, 4833, 293, 544, 5367, 18011, 20013, 11, 570, 309, 307, 264, 51616], "temperature": 0.0, "avg_logprob": -0.12025934277158795, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0013247126480564475}, {"id": 23, "seek": 20836, "start": 208.36, "end": 215.56, "text": " common kernel or basis that's then going to get applied in chapter five in the neurobiological", "tokens": [50364, 2689, 28256, 420, 5143, 300, 311, 550, 516, 281, 483, 6456, 294, 7187, 1732, 294, 264, 16499, 5614, 4383, 50724], "temperature": 0.0, "avg_logprob": -0.06890465792487649, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.012238046154379845}, {"id": 24, "seek": 20836, "start": 215.56, "end": 221.4, "text": " case. There's a recipe for making chapter four in chapter six. That's the recipe for active", "tokens": [50724, 1389, 13, 821, 311, 257, 6782, 337, 1455, 7187, 1451, 294, 7187, 2309, 13, 663, 311, 264, 6782, 337, 4967, 51016], "temperature": 0.0, "avg_logprob": -0.06890465792487649, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.012238046154379845}, {"id": 25, "seek": 20836, "start": 221.4, "end": 227.56, "text": " inference modeling. Chapter seven and eight are about the discrete and the continuous time variant", "tokens": [51016, 38253, 15983, 13, 18874, 3407, 293, 3180, 366, 466, 264, 27706, 293, 264, 10957, 565, 17501, 51324], "temperature": 0.0, "avg_logprob": -0.06890465792487649, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.012238046154379845}, {"id": 26, "seek": 20836, "start": 227.56, "end": 235.72000000000003, "text": " or subtype or motif of these kinds of things called generative models. So this is the real", "tokens": [51324, 420, 1422, 20467, 420, 39478, 295, 613, 3685, 295, 721, 1219, 1337, 1166, 5245, 13, 407, 341, 307, 264, 957, 51732], "temperature": 0.0, "avg_logprob": -0.06890465792487649, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.012238046154379845}, {"id": 27, "seek": 23572, "start": 235.72, "end": 243.96, "text": " common root. And we'll just look at what the sections are. This chapter complements the", "tokens": [50364, 2689, 5593, 13, 400, 321, 603, 445, 574, 412, 437, 264, 10863, 366, 13, 639, 7187, 715, 17988, 264, 50776], "temperature": 0.0, "avg_logprob": -0.1128823459148407, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.0012840767158195376}, {"id": 28, "seek": 23572, "start": 243.96, "end": 248.52, "text": " preceding chapter's conceptual treatment of active inference with a more formal treatment.", "tokens": [50776, 16969, 278, 7187, 311, 24106, 5032, 295, 4967, 38253, 365, 257, 544, 9860, 5032, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1128823459148407, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.0012840767158195376}, {"id": 29, "seek": 23572, "start": 249.72, "end": 256.28, "text": " Section 4.2 from Bayesian inference to free energy. What would you say about this section, Ali?", "tokens": [51064, 21804, 1017, 13, 17, 490, 7840, 42434, 38253, 281, 1737, 2281, 13, 708, 576, 291, 584, 466, 341, 3541, 11, 12020, 30, 51392], "temperature": 0.0, "avg_logprob": -0.1128823459148407, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.0012840767158195376}, {"id": 30, "seek": 25628, "start": 256.28, "end": 267.64, "text": " Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference.", "tokens": [50364, 1033, 11, 370, 382, 321, 458, 11, 264, 1737, 2281, 8665, 307, 7547, 538, 3894, 589, 322, 7840, 42434, 38253, 13, 50932], "temperature": 0.0, "avg_logprob": -0.16774198214213054, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.00453233765438199}, {"id": 31, "seek": 25628, "start": 268.35999999999996, "end": 273.47999999999996, "text": " I mean, all the way back to Helmholtz theory about", "tokens": [50968, 286, 914, 11, 439, 264, 636, 646, 281, 6128, 76, 71, 4837, 89, 5261, 466, 51224], "temperature": 0.0, "avg_logprob": -0.16774198214213054, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.00453233765438199}, {"id": 32, "seek": 25628, "start": 275.71999999999997, "end": 281.4, "text": " unconscious inference or something to that effect. I can't remember the exact term.", "tokens": [51336, 18900, 38253, 420, 746, 281, 300, 1802, 13, 286, 393, 380, 1604, 264, 1900, 1433, 13, 51620], "temperature": 0.0, "avg_logprob": -0.16774198214213054, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.00453233765438199}, {"id": 33, "seek": 28140, "start": 282.03999999999996, "end": 292.35999999999996, "text": " But here, I mean, it provides in a bit more detail how we can derive free energy principle", "tokens": [50396, 583, 510, 11, 286, 914, 11, 309, 6417, 294, 257, 857, 544, 2607, 577, 321, 393, 28446, 1737, 2281, 8665, 50912], "temperature": 0.0, "avg_logprob": -0.12525653839111328, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.006476996000856161}, {"id": 34, "seek": 28140, "start": 293.32, "end": 299.32, "text": " formalism using the established Bayesian inference formulation.", "tokens": [50960, 9860, 1434, 1228, 264, 7545, 7840, 42434, 38253, 37642, 13, 51260], "temperature": 0.0, "avg_logprob": -0.12525653839111328, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.006476996000856161}, {"id": 35, "seek": 28140, "start": 301.15999999999997, "end": 308.76, "text": " And particularly, one of the key movements or at least one of the key decisions", "tokens": [51352, 400, 4098, 11, 472, 295, 264, 2141, 9981, 420, 412, 1935, 472, 295, 264, 2141, 5327, 51732], "temperature": 0.0, "avg_logprob": -0.12525653839111328, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.006476996000856161}, {"id": 36, "seek": 30876, "start": 309.4, "end": 316.44, "text": " in through their derivation of free energy principle formulation is using Jane's inequality", "tokens": [50396, 294, 807, 641, 10151, 399, 295, 1737, 2281, 8665, 37642, 307, 1228, 13048, 311, 16970, 50748], "temperature": 0.0, "avg_logprob": -0.14945902322468005, "compression_ratio": 1.4330708661417322, "no_speech_prob": 0.005905598867684603}, {"id": 37, "seek": 30876, "start": 316.44, "end": 330.52, "text": " principle to derive an upper bound instead of just using the exact values to compute or to", "tokens": [50748, 8665, 281, 28446, 364, 6597, 5472, 2602, 295, 445, 1228, 264, 1900, 4190, 281, 14722, 420, 281, 51452], "temperature": 0.0, "avg_logprob": -0.14945902322468005, "compression_ratio": 1.4330708661417322, "no_speech_prob": 0.005905598867684603}, {"id": 38, "seek": 33052, "start": 330.52, "end": 338.76, "text": " achieve required parameters. So that's basically, in my opinion, the key premise of", "tokens": [50364, 4584, 4739, 9834, 13, 407, 300, 311, 1936, 11, 294, 452, 4800, 11, 264, 2141, 22045, 295, 50776], "temperature": 0.0, "avg_logprob": -0.127705303105441, "compression_ratio": 1.2740740740740741, "no_speech_prob": 0.04017625376582146}, {"id": 39, "seek": 33052, "start": 339.88, "end": 351.64, "text": " section 4.2. And to see in a bit more detail how we can achieve those upper bounds using", "tokens": [50832, 3541, 1017, 13, 17, 13, 400, 281, 536, 294, 257, 857, 544, 2607, 577, 321, 393, 4584, 729, 6597, 29905, 1228, 51420], "temperature": 0.0, "avg_logprob": -0.127705303105441, "compression_ratio": 1.2740740740740741, "no_speech_prob": 0.04017625376582146}, {"id": 40, "seek": 35164, "start": 351.64, "end": 360.36, "text": " Jane's inequality directly by using, I mean, manipulations of Bayesian inference,", "tokens": [50364, 13048, 311, 16970, 3838, 538, 1228, 11, 286, 914, 11, 9258, 4136, 295, 7840, 42434, 38253, 11, 50800], "temperature": 0.0, "avg_logprob": -0.11122003500012384, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.004132422152906656}, {"id": 41, "seek": 35164, "start": 361.8, "end": 363.47999999999996, "text": " Bayesian statistical formalism.", "tokens": [50872, 7840, 42434, 22820, 9860, 1434, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11122003500012384, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.004132422152906656}, {"id": 42, "seek": 35164, "start": 365.64, "end": 371.24, "text": " Thanks. I'll just add one point from this section. Broadly, these are the problems of", "tokens": [51064, 2561, 13, 286, 603, 445, 909, 472, 935, 490, 341, 3541, 13, 14074, 356, 11, 613, 366, 264, 2740, 295, 51344], "temperature": 0.0, "avg_logprob": -0.11122003500012384, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.004132422152906656}, {"id": 43, "seek": 35164, "start": 371.24, "end": 375.88, "text": " inferring states of the world perception and inferring a course of action planning.", "tokens": [51344, 13596, 2937, 4368, 295, 264, 1002, 12860, 293, 13596, 2937, 257, 1164, 295, 3069, 5038, 13, 51576], "temperature": 0.0, "avg_logprob": -0.11122003500012384, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.004132422152906656}, {"id": 44, "seek": 37588, "start": 375.88, "end": 381.48, "text": " So this is again, referring to the perception and action. And everything that happens in", "tokens": [50364, 407, 341, 307, 797, 11, 13761, 281, 264, 12860, 293, 3069, 13, 400, 1203, 300, 2314, 294, 50644], "temperature": 0.0, "avg_logprob": -0.09676529859241687, "compression_ratio": 1.6, "no_speech_prob": 0.00225124042481184}, {"id": 45, "seek": 37588, "start": 381.48, "end": 386.76, "text": " between is the internal or the cognitive part of the inference. But this is like the blanket state", "tokens": [50644, 1296, 307, 264, 6920, 420, 264, 15605, 644, 295, 264, 38253, 13, 583, 341, 307, 411, 264, 17907, 1785, 50908], "temperature": 0.0, "avg_logprob": -0.09676529859241687, "compression_ratio": 1.6, "no_speech_prob": 0.00225124042481184}, {"id": 46, "seek": 37588, "start": 387.32, "end": 395.15999999999997, "text": " cybernetic input output. And then let's look at the first equation or how much equations overall", "tokens": [50936, 13411, 77, 3532, 4846, 5598, 13, 400, 550, 718, 311, 574, 412, 264, 700, 5367, 420, 577, 709, 11787, 4787, 51328], "temperature": 0.0, "avg_logprob": -0.09676529859241687, "compression_ratio": 1.6, "no_speech_prob": 0.00225124042481184}, {"id": 47, "seek": 37588, "start": 395.8, "end": 398.52, "text": " or what equations do you think we should highlight?", "tokens": [51360, 420, 437, 11787, 360, 291, 519, 321, 820, 5078, 30, 51496], "temperature": 0.0, "avg_logprob": -0.09676529859241687, "compression_ratio": 1.6, "no_speech_prob": 0.00225124042481184}, {"id": 48, "seek": 39852, "start": 399.4, "end": 415.0, "text": " Okay. So maybe we can, I mean, just as a general comment about these different equations, well,", "tokens": [50408, 1033, 13, 407, 1310, 321, 393, 11, 286, 914, 11, 445, 382, 257, 2674, 2871, 466, 613, 819, 11787, 11, 731, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1595540696924383, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.005134494975209236}, {"id": 49, "seek": 39852, "start": 415.0, "end": 423.79999999999995, "text": " each of these equations provide a distinct step toward deriving the ultimate whole picture.", "tokens": [51188, 1184, 295, 613, 11787, 2893, 257, 10644, 1823, 7361, 1163, 2123, 264, 9705, 1379, 3036, 13, 51628], "temperature": 0.0, "avg_logprob": -0.1595540696924383, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.005134494975209236}, {"id": 50, "seek": 42380, "start": 423.8, "end": 433.56, "text": " So even if we don't quite understand how we can derive from each step to the other one,", "tokens": [50364, 407, 754, 498, 321, 500, 380, 1596, 1223, 577, 321, 393, 28446, 490, 1184, 1823, 281, 264, 661, 472, 11, 50852], "temperature": 0.0, "avg_logprob": -0.07425763530115928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0002413737092865631}, {"id": 51, "seek": 42380, "start": 435.32, "end": 445.08000000000004, "text": " it's good to know that it's only required to understand how we get to that ultimate whole", "tokens": [50940, 309, 311, 665, 281, 458, 300, 309, 311, 787, 4739, 281, 1223, 577, 321, 483, 281, 300, 9705, 1379, 51428], "temperature": 0.0, "avg_logprob": -0.07425763530115928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0002413737092865631}, {"id": 52, "seek": 42380, "start": 445.08000000000004, "end": 453.56, "text": " picture. But ultimately, what we would need in order to develop active inference models", "tokens": [51428, 3036, 13, 583, 6284, 11, 437, 321, 576, 643, 294, 1668, 281, 1499, 4967, 38253, 5245, 51852], "temperature": 0.0, "avg_logprob": -0.07425763530115928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0002413737092865631}, {"id": 53, "seek": 45380, "start": 454.36, "end": 459.72, "text": " is the ultimate equation or ultimate whole picture. So this is just", "tokens": [50392, 307, 264, 9705, 5367, 420, 9705, 1379, 3036, 13, 407, 341, 307, 445, 50660], "temperature": 0.0, "avg_logprob": -0.07787705236865629, "compression_ratio": 1.5493827160493827, "no_speech_prob": 0.0031210491433739662}, {"id": 54, "seek": 45380, "start": 462.12, "end": 471.96000000000004, "text": " a way to elucidate the steps toward developing that whole picture. But again, it's not an", "tokens": [50780, 257, 636, 281, 806, 1311, 327, 473, 264, 4439, 7361, 6416, 300, 1379, 3036, 13, 583, 797, 11, 309, 311, 406, 364, 51272], "temperature": 0.0, "avg_logprob": -0.07787705236865629, "compression_ratio": 1.5493827160493827, "no_speech_prob": 0.0031210491433739662}, {"id": 55, "seek": 45380, "start": 471.96000000000004, "end": 480.84000000000003, "text": " essential requirement to understand the materials of the rest of the book. But if we go from,", "tokens": [51272, 7115, 11695, 281, 1223, 264, 5319, 295, 264, 1472, 295, 264, 1446, 13, 583, 498, 321, 352, 490, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07787705236865629, "compression_ratio": 1.5493827160493827, "no_speech_prob": 0.0031210491433739662}, {"id": 56, "seek": 48084, "start": 481.79999999999995, "end": 490.44, "text": " I mean, equations 4.1 toward the 4.4, or in other words, a variation of free energy,", "tokens": [50412, 286, 914, 11, 11787, 1017, 13, 16, 7361, 264, 1017, 13, 19, 11, 420, 294, 661, 2283, 11, 257, 12990, 295, 1737, 2281, 11, 50844], "temperature": 0.0, "avg_logprob": -0.19284318417918925, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.005817464552819729}, {"id": 57, "seek": 48084, "start": 490.44, "end": 502.35999999999996, "text": " well, equation 4.1 is just a basic definition of some properties of probabilities in terms of", "tokens": [50844, 731, 11, 5367, 1017, 13, 16, 307, 445, 257, 3875, 7123, 295, 512, 7221, 295, 33783, 294, 2115, 295, 51440], "temperature": 0.0, "avg_logprob": -0.19284318417918925, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.005817464552819729}, {"id": 58, "seek": 50236, "start": 502.36, "end": 510.12, "text": " conditional probability and so on. So equation 4.2 provides the central", "tokens": [50364, 27708, 8482, 293, 370, 322, 13, 407, 5367, 1017, 13, 17, 6417, 264, 5777, 50752], "temperature": 0.0, "avg_logprob": -0.13236362593514578, "compression_ratio": 1.59375, "no_speech_prob": 0.04600415751338005}, {"id": 59, "seek": 50236, "start": 511.0, "end": 520.84, "text": " Jane's inequality principle and how it relates to, I mean, conditional probabilities and of", "tokens": [50796, 13048, 311, 16970, 8665, 293, 577, 309, 16155, 281, 11, 286, 914, 11, 27708, 33783, 293, 295, 51288], "temperature": 0.0, "avg_logprob": -0.13236362593514578, "compression_ratio": 1.59375, "no_speech_prob": 0.04600415751338005}, {"id": 60, "seek": 50236, "start": 520.84, "end": 530.12, "text": " course, joint probabilities. And then by using those two properties or those two equations,", "tokens": [51288, 1164, 11, 7225, 33783, 13, 400, 550, 538, 1228, 729, 732, 7221, 420, 729, 732, 11787, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13236362593514578, "compression_ratio": 1.59375, "no_speech_prob": 0.04600415751338005}, {"id": 61, "seek": 53012, "start": 530.12, "end": 537.16, "text": " we ultimately get to 4.4, which is the definition of variational free energy parameter,", "tokens": [50364, 321, 6284, 483, 281, 1017, 13, 19, 11, 597, 307, 264, 7123, 295, 3034, 1478, 1737, 2281, 13075, 11, 50716], "temperature": 0.0, "avg_logprob": -0.06842914531970846, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.001048167934641242}, {"id": 62, "seek": 53012, "start": 538.44, "end": 545.64, "text": " which is the parameter of interest that needs to be optimized in order to inference to happen,", "tokens": [50780, 597, 307, 264, 13075, 295, 1179, 300, 2203, 281, 312, 26941, 294, 1668, 281, 38253, 281, 1051, 11, 51140], "temperature": 0.0, "avg_logprob": -0.06842914531970846, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.001048167934641242}, {"id": 63, "seek": 53012, "start": 547.32, "end": 551.0, "text": " or at least perceptual inference to happen in active inference models.", "tokens": [51224, 420, 412, 1935, 43276, 901, 38253, 281, 1051, 294, 4967, 38253, 5245, 13, 51408], "temperature": 0.0, "avg_logprob": -0.06842914531970846, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.001048167934641242}, {"id": 64, "seek": 55100, "start": 551.08, "end": 561.08, "text": " Thanks. The only thing I'll add is f is the letter used for variational free energy. Think of it", "tokens": [50368, 2561, 13, 440, 787, 551, 286, 603, 909, 307, 283, 307, 264, 5063, 1143, 337, 3034, 1478, 1737, 2281, 13, 6557, 295, 309, 50868], "temperature": 0.0, "avg_logprob": -0.1542056401570638, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.013426524586975574}, {"id": 65, "seek": 55100, "start": 561.08, "end": 565.64, "text": " like a computer program and the arguments that it takes in or the variables that it takes in", "tokens": [50868, 411, 257, 3820, 1461, 293, 264, 12869, 300, 309, 2516, 294, 420, 264, 9102, 300, 309, 2516, 294, 51096], "temperature": 0.0, "avg_logprob": -0.1542056401570638, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.013426524586975574}, {"id": 66, "seek": 55100, "start": 566.36, "end": 573.88, "text": " are q, which is the distribution that's under the statisticians control and y, which are the data,", "tokens": [51132, 366, 9505, 11, 597, 307, 264, 7316, 300, 311, 833, 264, 29588, 2567, 1969, 293, 288, 11, 597, 366, 264, 1412, 11, 51508], "temperature": 0.0, "avg_logprob": -0.1542056401570638, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.013426524586975574}, {"id": 67, "seek": 55100, "start": 573.88, "end": 580.84, "text": " which are outside of the statisticians control. And do you want to describe more about anything", "tokens": [51508, 597, 366, 2380, 295, 264, 29588, 2567, 1969, 13, 400, 360, 291, 528, 281, 6786, 544, 466, 1340, 51856], "temperature": 0.0, "avg_logprob": -0.1542056401570638, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.013426524586975574}, {"id": 68, "seek": 58100, "start": 581.88, "end": 583.56, "text": " in this equation or carry on?", "tokens": [50408, 294, 341, 5367, 420, 3985, 322, 30, 50492], "temperature": 0.0, "avg_logprob": -0.12962689298264524, "compression_ratio": 1.3774834437086092, "no_speech_prob": 0.0013666029553860426}, {"id": 69, "seek": 58100, "start": 586.04, "end": 595.56, "text": " Just one thing that can probably be helpful is to somehow compare these steps with", "tokens": [50616, 1449, 472, 551, 300, 393, 1391, 312, 4961, 307, 281, 6063, 6794, 613, 4439, 365, 51092], "temperature": 0.0, "avg_logprob": -0.12962689298264524, "compression_ratio": 1.3774834437086092, "no_speech_prob": 0.0013666029553860426}, {"id": 70, "seek": 58100, "start": 596.76, "end": 604.6, "text": " the initial picture we had from chapter 2, because variational free energy was first introduced", "tokens": [51152, 264, 5883, 3036, 321, 632, 490, 7187, 568, 11, 570, 3034, 1478, 1737, 2281, 390, 700, 7268, 51544], "temperature": 0.0, "avg_logprob": -0.12962689298264524, "compression_ratio": 1.3774834437086092, "no_speech_prob": 0.0013666029553860426}, {"id": 71, "seek": 60460, "start": 604.6, "end": 613.5600000000001, "text": " in chapter 2. So it can be helpful to go back and forth between chapters 2 and 4 and try to", "tokens": [50364, 294, 7187, 568, 13, 407, 309, 393, 312, 4961, 281, 352, 646, 293, 5220, 1296, 20013, 568, 293, 1017, 293, 853, 281, 50812], "temperature": 0.0, "avg_logprob": -0.11864089965820312, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.0019876735750585794}, {"id": 72, "seek": 60460, "start": 613.5600000000001, "end": 627.72, "text": " connect the dots between the related points there. Section 4.3 generative models. All right,", "tokens": [50812, 1745, 264, 15026, 1296, 264, 4077, 2793, 456, 13, 21804, 1017, 13, 18, 1337, 1166, 5245, 13, 1057, 558, 11, 51520], "temperature": 0.0, "avg_logprob": -0.11864089965820312, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.0019876735750585794}, {"id": 73, "seek": 60460, "start": 627.72, "end": 633.24, "text": " I'll read the first sentence, then you can give some thoughts. To calculate the free energy,", "tokens": [51520, 286, 603, 1401, 264, 700, 8174, 11, 550, 291, 393, 976, 512, 4598, 13, 1407, 8873, 264, 1737, 2281, 11, 51796], "temperature": 0.0, "avg_logprob": -0.11864089965820312, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.0019876735750585794}, {"id": 74, "seek": 63324, "start": 634.04, "end": 640.44, "text": " we need three things. Data, a family of variational distributions, and a generative model comprising", "tokens": [50404, 321, 643, 1045, 721, 13, 11888, 11, 257, 1605, 295, 3034, 1478, 37870, 11, 293, 257, 1337, 1166, 2316, 16802, 3436, 50724], "temperature": 0.0, "avg_logprob": -0.09129602725689227, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.018531471490859985}, {"id": 75, "seek": 63324, "start": 640.44, "end": 650.04, "text": " a prior and a likelihood. In this section, we outline two very general sorts of generative model", "tokens": [50724, 257, 4059, 293, 257, 22119, 13, 682, 341, 3541, 11, 321, 16387, 732, 588, 2674, 7527, 295, 1337, 1166, 2316, 51204], "temperature": 0.0, "avg_logprob": -0.09129602725689227, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.018531471490859985}, {"id": 76, "seek": 63324, "start": 650.04, "end": 653.64, "text": " used for active inference and the form the free energy takes in relation to each.", "tokens": [51204, 1143, 337, 4967, 38253, 293, 264, 1254, 264, 1737, 2281, 2516, 294, 9721, 281, 1184, 13, 51384], "temperature": 0.0, "avg_logprob": -0.09129602725689227, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.018531471490859985}, {"id": 77, "seek": 65364, "start": 653.96, "end": 670.04, "text": " Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time", "tokens": [50380, 1033, 11, 370, 382, 2835, 3071, 11, 341, 7187, 11215, 1293, 365, 27706, 565, 293, 10957, 565, 51184], "temperature": 0.0, "avg_logprob": -0.15086001441592262, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0037054549902677536}, {"id": 78, "seek": 65364, "start": 670.04, "end": 677.16, "text": " situations. So clearly, we would need two different types of generative models for each situation.", "tokens": [51184, 6851, 13, 407, 4448, 11, 321, 576, 643, 732, 819, 3467, 295, 1337, 1166, 5245, 337, 1184, 2590, 13, 51540], "temperature": 0.0, "avg_logprob": -0.15086001441592262, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0037054549902677536}, {"id": 79, "seek": 67716, "start": 678.12, "end": 685.48, "text": " And obviously, the generative models or the way to construct generative models for", "tokens": [50412, 400, 2745, 11, 264, 1337, 1166, 5245, 420, 264, 636, 281, 7690, 1337, 1166, 5245, 337, 50780], "temperature": 0.0, "avg_logprob": -0.06840819341165048, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.007931903935968876}, {"id": 80, "seek": 67716, "start": 685.48, "end": 693.3199999999999, "text": " discrete time situations would vary quite a bit from the one for continuous time situations.", "tokens": [50780, 27706, 565, 6851, 576, 10559, 1596, 257, 857, 490, 264, 472, 337, 10957, 565, 6851, 13, 51172], "temperature": 0.0, "avg_logprob": -0.06840819341165048, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.007931903935968876}, {"id": 81, "seek": 67716, "start": 695.16, "end": 703.0, "text": " But the general principle underlying those generative models are basically the same,", "tokens": [51264, 583, 264, 2674, 8665, 14217, 729, 1337, 1166, 5245, 366, 1936, 264, 912, 11, 51656], "temperature": 0.0, "avg_logprob": -0.06840819341165048, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.007931903935968876}, {"id": 82, "seek": 70300, "start": 703.64, "end": 712.68, "text": " which is, I mean, to somehow construct a model of the environment, I mean, either be it", "tokens": [50396, 597, 307, 11, 286, 914, 11, 281, 6063, 7690, 257, 2316, 295, 264, 2823, 11, 286, 914, 11, 2139, 312, 309, 50848], "temperature": 0.0, "avg_logprob": -0.12126161181737506, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.004459901247173548}, {"id": 83, "seek": 70300, "start": 714.76, "end": 723.72, "text": " for the situation that is sequential in time or for the situations that need to be somehow,", "tokens": [50952, 337, 264, 2590, 300, 307, 42881, 294, 565, 420, 337, 264, 6851, 300, 643, 281, 312, 6063, 11, 51400], "temperature": 0.0, "avg_logprob": -0.12126161181737506, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.004459901247173548}, {"id": 84, "seek": 70300, "start": 725.8, "end": 730.52, "text": " each moment of the situation needs to be accommodated in terms of a continuous time", "tokens": [51504, 1184, 1623, 295, 264, 2590, 2203, 281, 312, 11713, 770, 294, 2115, 295, 257, 10957, 565, 51740], "temperature": 0.0, "avg_logprob": -0.12126161181737506, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.004459901247173548}, {"id": 85, "seek": 73052, "start": 730.52, "end": 742.52, "text": " situation. So figure 4.2 provides some examples of both. So for, yes, let me see.", "tokens": [50364, 2590, 13, 407, 2573, 1017, 13, 17, 6417, 512, 5110, 295, 1293, 13, 407, 337, 11, 2086, 11, 718, 385, 536, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1368756812551747, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.0014985086163505912}, {"id": 86, "seek": 73052, "start": 744.4399999999999, "end": 755.72, "text": " Yes, so we have some examples of different kinds of generative models and case studies,", "tokens": [51060, 1079, 11, 370, 321, 362, 512, 5110, 295, 819, 3685, 295, 1337, 1166, 5245, 293, 1389, 5313, 11, 51624], "temperature": 0.0, "avg_logprob": -0.1368756812551747, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.0014985086163505912}, {"id": 87, "seek": 75572, "start": 756.44, "end": 765.72, "text": " if you like, and it provides various ways to show how the dependencies between variables", "tokens": [50400, 498, 291, 411, 11, 293, 309, 6417, 3683, 2098, 281, 855, 577, 264, 36606, 1296, 9102, 50864], "temperature": 0.0, "avg_logprob": -0.08242153299265895, "compression_ratio": 1.7030303030303031, "no_speech_prob": 0.0034776765387505293}, {"id": 88, "seek": 75572, "start": 765.72, "end": 774.28, "text": " can be modeled using these kinds of graphical probabilistic models. So one common way to", "tokens": [50864, 393, 312, 37140, 1228, 613, 3685, 295, 35942, 31959, 3142, 5245, 13, 407, 472, 2689, 636, 281, 51292], "temperature": 0.0, "avg_logprob": -0.08242153299265895, "compression_ratio": 1.7030303030303031, "no_speech_prob": 0.0034776765387505293}, {"id": 89, "seek": 75572, "start": 774.28, "end": 782.44, "text": " represent generative models is to use these kinds of graphical probabilistic models in active inference", "tokens": [51292, 2906, 1337, 1166, 5245, 307, 281, 764, 613, 3685, 295, 35942, 31959, 3142, 5245, 294, 4967, 38253, 51700], "temperature": 0.0, "avg_logprob": -0.08242153299265895, "compression_ratio": 1.7030303030303031, "no_speech_prob": 0.0034776765387505293}, {"id": 90, "seek": 78244, "start": 782.44, "end": 792.2, "text": " literature, which is, at least in this case, the circles would represent the random variables,", "tokens": [50364, 10394, 11, 597, 307, 11, 412, 1935, 294, 341, 1389, 11, 264, 13040, 576, 2906, 264, 4974, 9102, 11, 50852], "temperature": 0.0, "avg_logprob": -0.11813040499417286, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.015164312906563282}, {"id": 91, "seek": 78244, "start": 792.2, "end": 798.5200000000001, "text": " and the squares would represent the distributions, which would describe", "tokens": [50852, 293, 264, 19368, 576, 2906, 264, 37870, 11, 597, 576, 6786, 51168], "temperature": 0.0, "avg_logprob": -0.11813040499417286, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.015164312906563282}, {"id": 92, "seek": 78244, "start": 799.96, "end": 809.6400000000001, "text": " the dependencies between those random variables. So we can see the clear relationships between", "tokens": [51240, 264, 36606, 1296, 729, 4974, 9102, 13, 407, 321, 393, 536, 264, 1850, 6159, 1296, 51724], "temperature": 0.0, "avg_logprob": -0.11813040499417286, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.015164312906563282}, {"id": 93, "seek": 80964, "start": 809.64, "end": 817.56, "text": " those parameters here, which is basically what this whole graph, what constitutes the", "tokens": [50364, 729, 9834, 510, 11, 597, 307, 1936, 437, 341, 1379, 4295, 11, 437, 44204, 264, 50760], "temperature": 0.0, "avg_logprob": -0.07860096011843, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.004005552269518375}, {"id": 94, "seek": 80964, "start": 817.56, "end": 825.96, "text": " generative model that needs to be used for different situations. And then in figure 4.3,", "tokens": [50760, 1337, 1166, 2316, 300, 2203, 281, 312, 1143, 337, 819, 6851, 13, 400, 550, 294, 2573, 1017, 13, 18, 11, 51180], "temperature": 0.0, "avg_logprob": -0.07860096011843, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.004005552269518375}, {"id": 95, "seek": 80964, "start": 827.56, "end": 833.4, "text": " we can compare the two different types of generative models based on", "tokens": [51260, 321, 393, 6794, 264, 732, 819, 3467, 295, 1337, 1166, 5245, 2361, 322, 51552], "temperature": 0.0, "avg_logprob": -0.07860096011843, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.004005552269518375}, {"id": 96, "seek": 83340, "start": 833.88, "end": 842.12, "text": " whether it's discrete time or continuous time situations. So the upper picture is a generative", "tokens": [50388, 1968, 309, 311, 27706, 565, 420, 10957, 565, 6851, 13, 407, 264, 6597, 3036, 307, 257, 1337, 1166, 50800], "temperature": 0.0, "avg_logprob": -0.09271432436429537, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.004005325958132744}, {"id": 97, "seek": 83340, "start": 842.12, "end": 851.56, "text": " model for the discrete time situation, and the lower picture is the parallel continuous time", "tokens": [50800, 2316, 337, 264, 27706, 565, 2590, 11, 293, 264, 3126, 3036, 307, 264, 8952, 10957, 565, 51272], "temperature": 0.0, "avg_logprob": -0.09271432436429537, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.004005325958132744}, {"id": 98, "seek": 83340, "start": 851.56, "end": 861.88, "text": " version of it. And as we can see, the general topology of these models are the same. The only", "tokens": [51272, 3037, 295, 309, 13, 400, 382, 321, 393, 536, 11, 264, 2674, 1192, 1793, 295, 613, 5245, 366, 264, 912, 13, 440, 787, 51788], "temperature": 0.0, "avg_logprob": -0.09271432436429537, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.004005325958132744}, {"id": 99, "seek": 86188, "start": 861.88, "end": 872.76, "text": " things that differ is the use of parameters for policies or, I mean, discrete time policies or", "tokens": [50364, 721, 300, 743, 307, 264, 764, 295, 9834, 337, 7657, 420, 11, 286, 914, 11, 27706, 565, 7657, 420, 50908], "temperature": 0.0, "avg_logprob": -0.12942715160182264, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0007429698016494513}, {"id": 100, "seek": 86188, "start": 872.76, "end": 882.2, "text": " the continuous time ones. And we can obviously compare the different elements for both priors", "tokens": [50908, 264, 10957, 565, 2306, 13, 400, 321, 393, 2745, 6794, 264, 819, 4959, 337, 1293, 1790, 830, 51380], "temperature": 0.0, "avg_logprob": -0.12942715160182264, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0007429698016494513}, {"id": 101, "seek": 86188, "start": 882.2, "end": 890.6, "text": " states and external states, internal states, and so on by comparing these two models here.", "tokens": [51380, 4368, 293, 8320, 4368, 11, 6920, 4368, 11, 293, 370, 322, 538, 15763, 613, 732, 5245, 510, 13, 51800], "temperature": 0.0, "avg_logprob": -0.12942715160182264, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0007429698016494513}, {"id": 102, "seek": 89188, "start": 892.6, "end": 897.48, "text": " Yeah, we often return to figure 4.3. It's kind of the Rosetta Stone", "tokens": [50400, 865, 11, 321, 2049, 2736, 281, 2573, 1017, 13, 18, 13, 467, 311, 733, 295, 264, 11144, 16593, 15012, 50644], "temperature": 0.0, "avg_logprob": -0.08073287290685317, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003799747792072594}, {"id": 103, "seek": 89188, "start": 898.52, "end": 904.28, "text": " of generative modeling for the context of this book, because it's then going to develop out into", "tokens": [50696, 295, 1337, 1166, 15983, 337, 264, 4319, 295, 341, 1446, 11, 570, 309, 311, 550, 516, 281, 1499, 484, 666, 50984], "temperature": 0.0, "avg_logprob": -0.08073287290685317, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003799747792072594}, {"id": 104, "seek": 89188, "start": 904.28, "end": 910.4399999999999, "text": " chapter seven and eight. And it represents a really fundamental decision made in modeling.", "tokens": [50984, 7187, 3407, 293, 3180, 13, 400, 309, 8855, 257, 534, 8088, 3537, 1027, 294, 15983, 13, 51292], "temperature": 0.0, "avg_logprob": -0.08073287290685317, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003799747792072594}, {"id": 105, "seek": 89188, "start": 911.72, "end": 918.04, "text": " And in the later chapters, it's also shown how it can be made into a hierarchical model", "tokens": [51356, 400, 294, 264, 1780, 20013, 11, 309, 311, 611, 4898, 577, 309, 393, 312, 1027, 666, 257, 35250, 804, 2316, 51672], "temperature": 0.0, "avg_logprob": -0.08073287290685317, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003799747792072594}, {"id": 106, "seek": 91804, "start": 918.04, "end": 923.24, "text": " that combines aspects of both. But within each level of modeling, still, these are the kinds", "tokens": [50364, 300, 29520, 7270, 295, 1293, 13, 583, 1951, 1184, 1496, 295, 15983, 11, 920, 11, 613, 366, 264, 3685, 50624], "temperature": 0.0, "avg_logprob": -0.08378364615244409, "compression_ratio": 1.525, "no_speech_prob": 0.0010160694364458323}, {"id": 107, "seek": 91804, "start": 923.24, "end": 928.4399999999999, "text": " of decisions that modelers are presented with when it comes to statistical modeling overall.", "tokens": [50624, 295, 5327, 300, 2316, 433, 366, 8212, 365, 562, 309, 1487, 281, 22820, 15983, 4787, 13, 50884], "temperature": 0.0, "avg_logprob": -0.08378364615244409, "compression_ratio": 1.525, "no_speech_prob": 0.0010160694364458323}, {"id": 108, "seek": 91804, "start": 930.52, "end": 938.1999999999999, "text": " So section 4.4 goes into essentially the top half of figure 4.3, discrete time.", "tokens": [50988, 407, 3541, 1017, 13, 19, 1709, 666, 4476, 264, 1192, 1922, 295, 2573, 1017, 13, 18, 11, 27706, 565, 13, 51372], "temperature": 0.0, "avg_logprob": -0.08378364615244409, "compression_ratio": 1.525, "no_speech_prob": 0.0010160694364458323}, {"id": 109, "seek": 91804, "start": 939.8, "end": 941.48, "text": " What would you say about discrete time?", "tokens": [51452, 708, 576, 291, 584, 466, 27706, 565, 30, 51536], "temperature": 0.0, "avg_logprob": -0.08378364615244409, "compression_ratio": 1.525, "no_speech_prob": 0.0010160694364458323}, {"id": 110, "seek": 94148, "start": 942.44, "end": 950.9200000000001, "text": " Okay, so the discrete time situation is obviously the archetype discrete time situation,", "tokens": [50412, 1033, 11, 370, 264, 27706, 565, 2590, 307, 2745, 264, 41852, 494, 27706, 565, 2590, 11, 50836], "temperature": 0.0, "avg_logprob": -0.2455465453011649, "compression_ratio": 1.3739837398373984, "no_speech_prob": 0.006897561717778444}, {"id": 111, "seek": 94148, "start": 950.9200000000001, "end": 959.32, "text": " which is the POMDP models. So at this point, I would very much like to recommend", "tokens": [50836, 597, 307, 264, 430, 5251, 11373, 5245, 13, 407, 412, 341, 935, 11, 286, 576, 588, 709, 411, 281, 2748, 51256], "temperature": 0.0, "avg_logprob": -0.2455465453011649, "compression_ratio": 1.3739837398373984, "no_speech_prob": 0.006897561717778444}, {"id": 112, "seek": 95932, "start": 960.2800000000001, "end": 974.0400000000001, "text": " following the material from set by step paper, because in that paper, the way to construct", "tokens": [50412, 3480, 264, 2527, 490, 992, 538, 1823, 3035, 11, 570, 294, 300, 3035, 11, 264, 636, 281, 7690, 51100], "temperature": 0.0, "avg_logprob": -0.17959994855134384, "compression_ratio": 1.3576642335766422, "no_speech_prob": 0.07469706982374191}, {"id": 113, "seek": 95932, "start": 974.0400000000001, "end": 982.36, "text": " POMDP models is described in a bit more detail. So if anyone feels like they should learn a bit", "tokens": [51100, 430, 5251, 11373, 5245, 307, 7619, 294, 257, 857, 544, 2607, 13, 407, 498, 2878, 3417, 411, 436, 820, 1466, 257, 857, 51516], "temperature": 0.0, "avg_logprob": -0.17959994855134384, "compression_ratio": 1.3576642335766422, "no_speech_prob": 0.07469706982374191}, {"id": 114, "seek": 98236, "start": 982.36, "end": 991.5600000000001, "text": " more about the gaps in the details, I would very much like to recommend that particular paper.", "tokens": [50364, 544, 466, 264, 15031, 294, 264, 4365, 11, 286, 576, 588, 709, 411, 281, 2748, 300, 1729, 3035, 13, 50824], "temperature": 0.0, "avg_logprob": -0.12534059262743183, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.051792170852422714}, {"id": 115, "seek": 98236, "start": 992.36, "end": 1005.16, "text": " So yeah, I don't know how much detail we should go into, because it's, I mean, although it's not", "tokens": [50864, 407, 1338, 11, 286, 500, 380, 458, 577, 709, 2607, 321, 820, 352, 666, 11, 570, 309, 311, 11, 286, 914, 11, 4878, 309, 311, 406, 51504], "temperature": 0.0, "avg_logprob": -0.12534059262743183, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.051792170852422714}, {"id": 116, "seek": 100516, "start": 1005.16, "end": 1013.64, "text": " maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can", "tokens": [50364, 1310, 9942, 1547, 337, 512, 8666, 11, 457, 309, 1709, 294, 257, 1596, 13246, 2607, 466, 577, 321, 393, 50788], "temperature": 0.0, "avg_logprob": -0.1432732582092285, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.00940023548901081}, {"id": 117, "seek": 100516, "start": 1014.36, "end": 1022.28, "text": " construct these models using the concepts we've learned in previous chapters. So", "tokens": [50824, 7690, 613, 5245, 1228, 264, 10392, 321, 600, 3264, 294, 3894, 20013, 13, 407, 51220], "temperature": 0.0, "avg_logprob": -0.1432732582092285, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.00940023548901081}, {"id": 118, "seek": 100516, "start": 1023.88, "end": 1033.8799999999999, "text": " ultimately, we reach equations for point 13 and four point 14, which are basically the culmination", "tokens": [51300, 6284, 11, 321, 2524, 11787, 337, 935, 3705, 293, 1451, 935, 3499, 11, 597, 366, 1936, 264, 28583, 399, 51800], "temperature": 0.0, "avg_logprob": -0.1432732582092285, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.00940023548901081}, {"id": 119, "seek": 103388, "start": 1033.88, "end": 1041.72, "text": " of POMDP formulation using the vector notations and gradients and so on. So", "tokens": [50364, 295, 430, 5251, 11373, 37642, 1228, 264, 8062, 406, 763, 293, 2771, 2448, 293, 370, 322, 13, 407, 50756], "temperature": 0.0, "avg_logprob": -0.18545422311556542, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.0015003582229837775}, {"id": 120, "seek": 103388, "start": 1045.64, "end": 1050.8400000000001, "text": " that's great. Then we go to continuous time. Yeah, great.", "tokens": [50952, 300, 311, 869, 13, 1396, 321, 352, 281, 10957, 565, 13, 865, 11, 869, 13, 51212], "temperature": 0.0, "avg_logprob": -0.18545422311556542, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.0015003582229837775}, {"id": 121, "seek": 103388, "start": 1053.0, "end": 1060.3600000000001, "text": " A few things intervene in the continuous time chapter was that we'll just mention here, because", "tokens": [51320, 316, 1326, 721, 30407, 294, 264, 10957, 565, 7187, 390, 300, 321, 603, 445, 2152, 510, 11, 570, 51688], "temperature": 0.0, "avg_logprob": -0.18545422311556542, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.0015003582229837775}, {"id": 122, "seek": 106036, "start": 1060.36, "end": 1066.1999999999998, "text": " they're kind of boxed or partitioned from the continuous time part, but they're following", "tokens": [50364, 436, 434, 733, 295, 2424, 292, 420, 24808, 292, 490, 264, 10957, 565, 644, 11, 457, 436, 434, 3480, 50656], "temperature": 0.0, "avg_logprob": -0.11258413415206106, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.007576148025691509}, {"id": 123, "seek": 106036, "start": 1066.1999999999998, "end": 1071.1599999999999, "text": " pages versus Markov blankets. We won't go into it here, but kind of footnote that or", "tokens": [50656, 7183, 5717, 3934, 5179, 38710, 13, 492, 1582, 380, 352, 666, 309, 510, 11, 457, 733, 295, 2671, 22178, 300, 420, 50904], "temperature": 0.0, "avg_logprob": -0.11258413415206106, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.007576148025691509}, {"id": 124, "seek": 106036, "start": 1071.1599999999999, "end": 1077.8799999999999, "text": " look at some other places where we talk about it outside of this chapter overview. Figure 4.4,", "tokens": [50904, 574, 412, 512, 661, 3190, 689, 321, 751, 466, 309, 2380, 295, 341, 7187, 12492, 13, 43225, 1017, 13, 19, 11, 51240], "temperature": 0.0, "avg_logprob": -0.11258413415206106, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.007576148025691509}, {"id": 125, "seek": 106036, "start": 1077.8799999999999, "end": 1087.08, "text": " Bayesian message passing. Again, a big topic. Let's kind of just go past it now. Back to the", "tokens": [51240, 7840, 42434, 3636, 8437, 13, 3764, 11, 257, 955, 4829, 13, 961, 311, 733, 295, 445, 352, 1791, 309, 586, 13, 5833, 281, 264, 51700], "temperature": 0.0, "avg_logprob": -0.11258413415206106, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.007576148025691509}, {"id": 126, "seek": 108708, "start": 1087.1599999999999, "end": 1095.96, "text": " regularly scheduled continuous time generative model discussion. And then another box to the", "tokens": [50368, 11672, 15678, 10957, 565, 1337, 1166, 2316, 5017, 13, 400, 550, 1071, 2424, 281, 264, 50808], "temperature": 0.0, "avg_logprob": -0.0949734023639134, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00040446623461320996}, {"id": 127, "seek": 108708, "start": 1095.96, "end": 1103.8, "text": " generalized coordinates of motion. So taking position plus derivatives of position. And that has", "tokens": [50808, 44498, 21056, 295, 5394, 13, 407, 1940, 2535, 1804, 33733, 295, 2535, 13, 400, 300, 575, 51200], "temperature": 0.0, "avg_logprob": -0.0949734023639134, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00040446623461320996}, {"id": 128, "seek": 108708, "start": 1103.8, "end": 1113.3999999999999, "text": " some beneficial properties that are described and unpacked also elsewhere. Do you want to say", "tokens": [51200, 512, 14072, 7221, 300, 366, 7619, 293, 26699, 292, 611, 14517, 13, 1144, 291, 528, 281, 584, 51680], "temperature": 0.0, "avg_logprob": -0.0949734023639134, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00040446623461320996}, {"id": 129, "seek": 111340, "start": 1113.4, "end": 1127.48, "text": " anything about 4.5.2? Well, the only thing that comes to mind is although, as I said before,", "tokens": [50364, 1340, 466, 1017, 13, 20, 13, 17, 30, 1042, 11, 264, 787, 551, 300, 1487, 281, 1575, 307, 4878, 11, 382, 286, 848, 949, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14539789270471642, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.00100032240152359}, {"id": 130, "seek": 111340, "start": 1128.76, "end": 1137.72, "text": " all the formulations here may look more, I mean, a bit too dense to understand at the first pass,", "tokens": [51132, 439, 264, 1254, 4136, 510, 815, 574, 544, 11, 286, 914, 11, 257, 857, 886, 18011, 281, 1223, 412, 264, 700, 1320, 11, 51580], "temperature": 0.0, "avg_logprob": -0.14539789270471642, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.00100032240152359}, {"id": 131, "seek": 113772, "start": 1137.72, "end": 1152.84, "text": " but some of the key maybe components here could be obviously the material from box 4.2 and 4.3.", "tokens": [50364, 457, 512, 295, 264, 2141, 1310, 6677, 510, 727, 312, 2745, 264, 2527, 490, 2424, 1017, 13, 17, 293, 1017, 13, 18, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1484605832533403, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.0012634734157472849}, {"id": 132, "seek": 113772, "start": 1154.6000000000001, "end": 1165.08, "text": " I think are quite essential to understand the underlying principle behind deriving the continuous", "tokens": [51208, 286, 519, 366, 1596, 7115, 281, 1223, 264, 14217, 8665, 2261, 1163, 2123, 264, 10957, 51732], "temperature": 0.0, "avg_logprob": -0.1484605832533403, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.0012634734157472849}, {"id": 133, "seek": 116508, "start": 1165.08, "end": 1173.1599999999999, "text": " time situation because without Laplace approximation, what we would have in terms of free energy", "tokens": [50364, 565, 2590, 570, 1553, 2369, 6742, 28023, 11, 437, 321, 576, 362, 294, 2115, 295, 1737, 2281, 50768], "temperature": 0.0, "avg_logprob": -0.14500924407458696, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0038216125685721636}, {"id": 134, "seek": 116508, "start": 1173.1599999999999, "end": 1184.28, "text": " minimization would look very much like the Gibbs free energy. So I mean, the key distinction between", "tokens": [50768, 4464, 2144, 576, 574, 588, 709, 411, 264, 30199, 1737, 2281, 13, 407, 286, 914, 11, 264, 2141, 16844, 1296, 51324], "temperature": 0.0, "avg_logprob": -0.14500924407458696, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0038216125685721636}, {"id": 135, "seek": 116508, "start": 1186.4399999999998, "end": 1192.12, "text": " the free energy principle as described in active inference literature, as opposed to Gibbs free", "tokens": [51432, 264, 1737, 2281, 8665, 382, 7619, 294, 4967, 38253, 10394, 11, 382, 8851, 281, 30199, 1737, 51716], "temperature": 0.0, "avg_logprob": -0.14500924407458696, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0038216125685721636}, {"id": 136, "seek": 119212, "start": 1192.12, "end": 1202.1999999999998, "text": " energy, is the Laplace approximation. So this is what enables us to go from Gibbs free energy to,", "tokens": [50364, 2281, 11, 307, 264, 2369, 6742, 28023, 13, 407, 341, 307, 437, 17077, 505, 281, 352, 490, 30199, 1737, 2281, 281, 11, 50868], "temperature": 0.0, "avg_logprob": -0.12462973594665527, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004130727145820856}, {"id": 137, "seek": 119212, "start": 1203.56, "end": 1210.4399999999998, "text": " I mean, the variation of free energy. So yeah, that's, I mean, quite essential to", "tokens": [50936, 286, 914, 11, 264, 12990, 295, 1737, 2281, 13, 407, 1338, 11, 300, 311, 11, 286, 914, 11, 1596, 7115, 281, 51280], "temperature": 0.0, "avg_logprob": -0.12462973594665527, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004130727145820856}, {"id": 138, "seek": 119212, "start": 1211.56, "end": 1219.3999999999999, "text": " make this, to be familiar with this essential approximation. And obviously, the concept of", "tokens": [51336, 652, 341, 11, 281, 312, 4963, 365, 341, 7115, 28023, 13, 400, 2745, 11, 264, 3410, 295, 51728], "temperature": 0.0, "avg_logprob": -0.12462973594665527, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004130727145820856}, {"id": 139, "seek": 121940, "start": 1219.4, "end": 1225.4, "text": " generalized coordinates of motion will come time and time again throughout the whole book,", "tokens": [50364, 44498, 21056, 295, 5394, 486, 808, 565, 293, 565, 797, 3710, 264, 1379, 1446, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13273309525989352, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.001324712997302413}, {"id": 140, "seek": 121940, "start": 1225.4, "end": 1236.3600000000001, "text": " particularly in chapters eight and nine. So yeah, those two concepts, I believe, needs a bit more", "tokens": [50664, 4098, 294, 20013, 3180, 293, 4949, 13, 407, 1338, 11, 729, 732, 10392, 11, 286, 1697, 11, 2203, 257, 857, 544, 51212], "temperature": 0.0, "avg_logprob": -0.13273309525989352, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.001324712997302413}, {"id": 141, "seek": 123636, "start": 1236.9199999999998, "end": 1250.9199999999998, "text": " attention. So yeah, sounds good. Box 4.3 Laplace approximation equations, another message passing", "tokens": [50392, 3202, 13, 407, 1338, 11, 3263, 665, 13, 15112, 1017, 13, 18, 2369, 6742, 28023, 11787, 11, 1071, 3636, 8437, 51092], "temperature": 0.0, "avg_logprob": -0.11001014709472656, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.044664423912763596}, {"id": 142, "seek": 123636, "start": 1251.56, "end": 1259.4799999999998, "text": " representation, and a summary. The key message to take away is that approximate Bayesian inference", "tokens": [51124, 10290, 11, 293, 257, 12691, 13, 440, 2141, 3636, 281, 747, 1314, 307, 300, 30874, 7840, 42434, 38253, 51520], "temperature": 0.0, "avg_logprob": -0.11001014709472656, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.044664423912763596}, {"id": 143, "seek": 123636, "start": 1259.4799999999998, "end": 1265.6399999999999, "text": " may be framed as minimizing a quantity known as variational free energy. This depends on a", "tokens": [51520, 815, 312, 30420, 382, 46608, 257, 11275, 2570, 382, 3034, 1478, 1737, 2281, 13, 639, 5946, 322, 257, 51828], "temperature": 0.0, "avg_logprob": -0.11001014709472656, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.044664423912763596}, {"id": 144, "seek": 126564, "start": 1265.64, "end": 1271.24, "text": " generative model that expresses our belief about how data are generated. Anything else you want to add?", "tokens": [50364, 1337, 1166, 2316, 300, 39204, 527, 7107, 466, 577, 1412, 366, 10833, 13, 11998, 1646, 291, 528, 281, 909, 30, 50644], "temperature": 0.0, "avg_logprob": -0.0974839158254127, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0006876910338178277}, {"id": 145, "seek": 126564, "start": 1277.0, "end": 1283.8000000000002, "text": " Nothing comes to mind at the moment, because, as I said, this is, I mean, we're still", "tokens": [50932, 6693, 1487, 281, 1575, 412, 264, 1623, 11, 570, 11, 382, 286, 848, 11, 341, 307, 11, 286, 914, 11, 321, 434, 920, 51272], "temperature": 0.0, "avg_logprob": -0.0974839158254127, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0006876910338178277}, {"id": 146, "seek": 126564, "start": 1284.8400000000001, "end": 1292.1200000000001, "text": " in the stage that we want to develop our essential tools to be used in the rest of the books. So", "tokens": [51324, 294, 264, 3233, 300, 321, 528, 281, 1499, 527, 7115, 3873, 281, 312, 1143, 294, 264, 1472, 295, 264, 3642, 13, 407, 51688], "temperature": 0.0, "avg_logprob": -0.0974839158254127, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0006876910338178277}, {"id": 147, "seek": 129212, "start": 1293.08, "end": 1300.04, "text": " here, up until now, I believe, by the end of chapter four, we have acquired all the essential", "tokens": [50412, 510, 11, 493, 1826, 586, 11, 286, 1697, 11, 538, 264, 917, 295, 7187, 1451, 11, 321, 362, 17554, 439, 264, 7115, 50760], "temperature": 0.0, "avg_logprob": -0.1847466362847222, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0005356669425964355}, {"id": 148, "seek": 129212, "start": 1300.04, "end": 1309.8, "text": " necessary mathematical tools. And the next chapter, chapter five, is kind of acts like an interlude.", "tokens": [50760, 4818, 18894, 3873, 13, 400, 264, 958, 7187, 11, 7187, 1732, 11, 307, 733, 295, 10672, 411, 364, 728, 32334, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1847466362847222, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0005356669425964355}, {"id": 149, "seek": 129212, "start": 1311.4799999999998, "end": 1320.9199999999998, "text": " And I don't think it's the direct, I mean, continuation of chapters one through four. So", "tokens": [51332, 400, 286, 500, 380, 519, 309, 311, 264, 2047, 11, 286, 914, 11, 29357, 295, 20013, 472, 807, 1451, 13, 407, 51804], "temperature": 0.0, "avg_logprob": -0.1847466362847222, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0005356669425964355}, {"id": 150, "seek": 132092, "start": 1321.88, "end": 1331.4, "text": " I believe the first section or the first part of the book, conceptually and mathematically ends here.", "tokens": [50412, 286, 1697, 264, 700, 3541, 420, 264, 700, 644, 295, 264, 1446, 11, 3410, 671, 293, 44003, 5314, 510, 13, 50888], "temperature": 0.0, "avg_logprob": -0.09732886364585475, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0003250183362979442}, {"id": 151, "seek": 132092, "start": 1331.4, "end": 1339.0, "text": " So yeah, that's it. Yes, it's a little bit like the pragmatic modeling part gets foreshadowed", "tokens": [50888, 407, 1338, 11, 300, 311, 309, 13, 1079, 11, 309, 311, 257, 707, 857, 411, 264, 46904, 15983, 644, 2170, 2091, 2716, 11528, 292, 51268], "temperature": 0.0, "avg_logprob": -0.09732886364585475, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0003250183362979442}, {"id": 152, "seek": 132092, "start": 1339.5600000000002, "end": 1346.8400000000001, "text": " or explored in five, now that we're all built up with four. All right, that's the end of the", "tokens": [51296, 420, 24016, 294, 1732, 11, 586, 300, 321, 434, 439, 3094, 493, 365, 1451, 13, 1057, 558, 11, 300, 311, 264, 917, 295, 264, 51660], "temperature": 0.0, "avg_logprob": -0.09732886364585475, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0003250183362979442}, {"id": 153, "seek": 134684, "start": 1346.84, "end": 1367.56, "text": " overview for four. Okay, chapter five is called message passing and neurobiology.", "tokens": [50364, 12492, 337, 1451, 13, 1033, 11, 7187, 1732, 307, 1219, 3636, 8437, 293, 16499, 5614, 1793, 13, 51400], "temperature": 0.0, "avg_logprob": -0.16774687312898182, "compression_ratio": 1.0384615384615385, "no_speech_prob": 0.006096697878092527}, {"id": 154, "seek": 136756, "start": 1368.52, "end": 1371.56, "text": " What is your overview thought on chapter five?", "tokens": [50412, 708, 307, 428, 12492, 1194, 322, 7187, 1732, 30, 50564], "temperature": 0.0, "avg_logprob": -0.2031554081400887, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.03304251655936241}, {"id": 155, "seek": 136756, "start": 1374.84, "end": 1382.44, "text": " Okay, I mean, it's a kind of, I don't know, I had mixed feelings about this chapter, because", "tokens": [50728, 1033, 11, 286, 914, 11, 309, 311, 257, 733, 295, 11, 286, 500, 380, 458, 11, 286, 632, 7467, 6640, 466, 341, 7187, 11, 570, 51108], "temperature": 0.0, "avg_logprob": -0.2031554081400887, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.03304251655936241}, {"id": 156, "seek": 136756, "start": 1383.0, "end": 1390.84, "text": " on one hand, you see, as far as I understand active inference, although it originated as", "tokens": [51136, 322, 472, 1011, 11, 291, 536, 11, 382, 1400, 382, 286, 1223, 4967, 38253, 11, 4878, 309, 31129, 382, 51528], "temperature": 0.0, "avg_logprob": -0.2031554081400887, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.03304251655936241}, {"id": 157, "seek": 139084, "start": 1391.56, "end": 1399.3999999999999, "text": " quote unquote, a unified theory of the brain, I don't think it's a neurobiological theory per se.", "tokens": [50400, 6513, 37557, 11, 257, 26787, 5261, 295, 264, 3567, 11, 286, 500, 380, 519, 309, 311, 257, 16499, 5614, 4383, 5261, 680, 369, 13, 50792], "temperature": 0.0, "avg_logprob": -0.11314888854524982, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.008704965002834797}, {"id": 158, "seek": 139084, "start": 1400.6, "end": 1409.08, "text": " Of course, there can be some correlations between neurobiological components or concepts", "tokens": [50852, 2720, 1164, 11, 456, 393, 312, 512, 13983, 763, 1296, 16499, 5614, 4383, 6677, 420, 10392, 51276], "temperature": 0.0, "avg_logprob": -0.11314888854524982, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.008704965002834797}, {"id": 159, "seek": 139084, "start": 1409.72, "end": 1420.4399999999998, "text": " with active inference, I mean, concepts. But I mean, it's not an essential premise", "tokens": [51308, 365, 4967, 38253, 11, 286, 914, 11, 10392, 13, 583, 286, 914, 11, 309, 311, 406, 364, 7115, 22045, 51844], "temperature": 0.0, "avg_logprob": -0.11314888854524982, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.008704965002834797}, {"id": 160, "seek": 142044, "start": 1420.52, "end": 1429.16, "text": " of active inference theory to provide a theory, to provide a comprehensive theory about how", "tokens": [50368, 295, 4967, 38253, 5261, 281, 2893, 257, 5261, 11, 281, 2893, 257, 13914, 5261, 466, 577, 50800], "temperature": 0.0, "avg_logprob": -0.14125995857771054, "compression_ratio": 1.512, "no_speech_prob": 0.002548669697716832}, {"id": 161, "seek": 142044, "start": 1429.16, "end": 1441.0, "text": " the neurobiology of human brain or other organisms brain behave at a detailed and neuroanatomical", "tokens": [50800, 264, 16499, 5614, 1793, 295, 1952, 3567, 420, 661, 22110, 3567, 15158, 412, 257, 9942, 293, 16499, 282, 267, 298, 804, 51392], "temperature": 0.0, "avg_logprob": -0.14125995857771054, "compression_ratio": 1.512, "no_speech_prob": 0.002548669697716832}, {"id": 162, "seek": 144100, "start": 1441.56, "end": 1452.36, "text": " level. But then again, it's nice to have these kinds of empirical correlations between the", "tokens": [50392, 1496, 13, 583, 550, 797, 11, 309, 311, 1481, 281, 362, 613, 3685, 295, 31886, 13983, 763, 1296, 264, 50932], "temperature": 0.0, "avg_logprob": -0.13907283417722013, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.015182258561253548}, {"id": 163, "seek": 144100, "start": 1452.36, "end": 1462.6, "text": " findings of neurobiology and the active inference theory. But I don't think it's one of active inference", "tokens": [50932, 16483, 295, 16499, 5614, 1793, 293, 264, 4967, 38253, 5261, 13, 583, 286, 500, 380, 519, 309, 311, 472, 295, 4967, 38253, 51444], "temperature": 0.0, "avg_logprob": -0.13907283417722013, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.015182258561253548}, {"id": 164, "seek": 146260, "start": 1463.32, "end": 1467.56, "text": " central assertions, at least to my understanding.", "tokens": [50400, 5777, 19810, 626, 11, 412, 1935, 281, 452, 3701, 13, 50612], "temperature": 0.0, "avg_logprob": -0.0918502532518827, "compression_ratio": 1.5496688741721854, "no_speech_prob": 0.00831264816224575}, {"id": 165, "seek": 146260, "start": 1469.7199999999998, "end": 1476.76, "text": " Well said, very interesting framing. Well, chapter five definitely takes a very specific", "tokens": [50720, 1042, 848, 11, 588, 1880, 28971, 13, 1042, 11, 7187, 1732, 2138, 2516, 257, 588, 2685, 51072], "temperature": 0.0, "avg_logprob": -0.0918502532518827, "compression_ratio": 1.5496688741721854, "no_speech_prob": 0.00831264816224575}, {"id": 166, "seek": 146260, "start": 1477.56, "end": 1485.3999999999999, "text": " system of interest approach by highlighting one of the most studied areas, also one of the most", "tokens": [51112, 1185, 295, 1179, 3109, 538, 26551, 472, 295, 264, 881, 9454, 3179, 11, 611, 472, 295, 264, 881, 51504], "temperature": 0.0, "avg_logprob": -0.0918502532518827, "compression_ratio": 1.5496688741721854, "no_speech_prob": 0.00831264816224575}, {"id": 167, "seek": 148540, "start": 1485.4, "end": 1496.52, "text": " relevant areas, which is mammalian neuroscience. And the chapter is going to introduce a few different", "tokens": [50364, 7340, 3179, 11, 597, 307, 49312, 952, 42762, 13, 400, 264, 7187, 307, 516, 281, 5366, 257, 1326, 819, 50920], "temperature": 0.0, "avg_logprob": -0.09832223769157164, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.035135842859745026}, {"id": 168, "seek": 148540, "start": 1499.24, "end": 1506.3600000000001, "text": " motifs in the nervous system, and essentially build up towards figure 5.5,", "tokens": [51056, 2184, 18290, 294, 264, 6296, 1185, 11, 293, 4476, 1322, 493, 3030, 2573, 1025, 13, 20, 11, 51412], "temperature": 0.0, "avg_logprob": -0.09832223769157164, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.035135842859745026}, {"id": 169, "seek": 148540, "start": 1507.0800000000002, "end": 1514.8400000000001, "text": " which is at the end of the chapter, and 5.5 wires together three specific neural systems", "tokens": [51448, 597, 307, 412, 264, 917, 295, 264, 7187, 11, 293, 1025, 13, 20, 15537, 1214, 1045, 2685, 18161, 3652, 51836], "temperature": 0.0, "avg_logprob": -0.09832223769157164, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.035135842859745026}, {"id": 170, "seek": 151540, "start": 1515.4, "end": 1522.52, "text": " that the chapter is going to focus on work in that area from. So Ali said it very well.", "tokens": [50364, 300, 264, 7187, 307, 516, 281, 1879, 322, 589, 294, 300, 1859, 490, 13, 407, 12020, 848, 309, 588, 731, 13, 50720], "temperature": 0.0, "avg_logprob": -0.09732529654431699, "compression_ratio": 1.5027027027027027, "no_speech_prob": 0.0013455020962283015}, {"id": 171, "seek": 151540, "start": 1523.96, "end": 1530.8400000000001, "text": " Active inference was built up to in chapter four. Here is another level or type of science", "tokens": [50792, 26635, 38253, 390, 3094, 493, 281, 294, 7187, 1451, 13, 1692, 307, 1071, 1496, 420, 2010, 295, 3497, 51136], "temperature": 0.0, "avg_logprob": -0.09732529654431699, "compression_ratio": 1.5027027027027027, "no_speech_prob": 0.0013455020962283015}, {"id": 172, "seek": 151540, "start": 1530.8400000000001, "end": 1536.8400000000001, "text": " with assertions or with representations or mappings to any specific system. But this is the kind of", "tokens": [51136, 365, 19810, 626, 420, 365, 33358, 420, 463, 28968, 281, 604, 2685, 1185, 13, 583, 341, 307, 264, 733, 295, 51436], "temperature": 0.0, "avg_logprob": -0.09732529654431699, "compression_ratio": 1.5027027027027027, "no_speech_prob": 0.0013455020962283015}, {"id": 173, "seek": 153684, "start": 1537.6399999999999, "end": 1545.56, "text": " modeling that has been built up and done by Friston, Par, Pizzolo, and others over the decades,", "tokens": [50404, 15983, 300, 575, 668, 3094, 493, 293, 1096, 538, 1526, 47345, 11, 3457, 11, 430, 8072, 7902, 11, 293, 2357, 670, 264, 7878, 11, 50800], "temperature": 0.0, "avg_logprob": -0.15015047788619995, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005910491570830345}, {"id": 174, "seek": 153684, "start": 1545.56, "end": 1553.72, "text": " with a focus coming from a human neuroimaging laboratory setting, a lot of focus and study", "tokens": [50800, 365, 257, 1879, 1348, 490, 257, 1952, 16499, 332, 3568, 16523, 3287, 11, 257, 688, 295, 1879, 293, 2979, 51208], "temperature": 0.0, "avg_logprob": -0.15015047788619995, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005910491570830345}, {"id": 175, "seek": 153684, "start": 1554.36, "end": 1558.9199999999998, "text": " and attention and funding and everything on the mammalian nervous system.", "tokens": [51240, 293, 3202, 293, 6137, 293, 1203, 322, 264, 49312, 952, 6296, 1185, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15015047788619995, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005910491570830345}, {"id": 176, "seek": 155892, "start": 1559.16, "end": 1570.8400000000001, "text": " But claims about the nervous system are not the basis of what active inference claims", "tokens": [50376, 583, 9441, 466, 264, 6296, 1185, 366, 406, 264, 5143, 295, 437, 4967, 38253, 9441, 50960], "temperature": 0.0, "avg_logprob": -0.15296850204467774, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0023958683013916016}, {"id": 177, "seek": 155892, "start": 1571.4, "end": 1576.8400000000001, "text": " or how it's derived. But this is like an example case study in neurobiology,", "tokens": [50988, 420, 577, 309, 311, 18949, 13, 583, 341, 307, 411, 364, 1365, 1389, 2979, 294, 16499, 5614, 1793, 11, 51260], "temperature": 0.0, "avg_logprob": -0.15296850204467774, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0023958683013916016}, {"id": 178, "seek": 155892, "start": 1577.5600000000002, "end": 1583.0, "text": " connecting back to some of the formalisms that we've just seen introduced in chapter four.", "tokens": [51296, 11015, 646, 281, 512, 295, 264, 9860, 13539, 300, 321, 600, 445, 1612, 7268, 294, 7187, 1451, 13, 51568], "temperature": 0.0, "avg_logprob": -0.15296850204467774, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0023958683013916016}, {"id": 179, "seek": 158300, "start": 1583.96, "end": 1592.44, "text": " Yeah, and to add a minor point to which I just said, I think it's important to draw attention to", "tokens": [50412, 865, 11, 293, 281, 909, 257, 6696, 935, 281, 597, 286, 445, 848, 11, 286, 519, 309, 311, 1021, 281, 2642, 3202, 281, 50836], "temperature": 0.0, "avg_logprob": -0.1365065786573622, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.004393536597490311}, {"id": 180, "seek": 158300, "start": 1593.08, "end": 1598.84, "text": " the last sentence of the last paragraph of the first page. It is important to draw a distinction", "tokens": [50868, 264, 1036, 8174, 295, 264, 1036, 18865, 295, 264, 700, 3028, 13, 467, 307, 1021, 281, 2642, 257, 16844, 51156], "temperature": 0.0, "avg_logprob": -0.1365065786573622, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.004393536597490311}, {"id": 181, "seek": 158300, "start": 1598.84, "end": 1604.68, "text": " between a principle, i.e. the minimization of free energy and a process theory about how this", "tokens": [51156, 1296, 257, 8665, 11, 741, 13, 68, 13, 264, 4464, 2144, 295, 1737, 2281, 293, 257, 1399, 5261, 466, 577, 341, 51448], "temperature": 0.0, "avg_logprob": -0.1365065786573622, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.004393536597490311}, {"id": 182, "seek": 158300, "start": 1604.68, "end": 1612.2, "text": " principle may be implemented in a certain kind of system. So I think this sentence here", "tokens": [51448, 8665, 815, 312, 12270, 294, 257, 1629, 733, 295, 1185, 13, 407, 286, 519, 341, 8174, 510, 51824], "temperature": 0.0, "avg_logprob": -0.1365065786573622, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.004393536597490311}, {"id": 183, "seek": 161220, "start": 1613.0, "end": 1620.1200000000001, "text": " frames this chapter in relation to all the other technical chapters of this book. So if", "tokens": [50404, 12083, 341, 7187, 294, 9721, 281, 439, 264, 661, 6191, 20013, 295, 341, 1446, 13, 407, 498, 50760], "temperature": 0.0, "avg_logprob": -0.11427327333870581, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004678059834986925}, {"id": 184, "seek": 161220, "start": 1621.0, "end": 1627.48, "text": " every other chapter is about developing, or at least up to now, was about developing", "tokens": [50804, 633, 661, 7187, 307, 466, 6416, 11, 420, 412, 1935, 493, 281, 586, 11, 390, 466, 6416, 51128], "temperature": 0.0, "avg_logprob": -0.11427327333870581, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004678059834986925}, {"id": 185, "seek": 161220, "start": 1629.16, "end": 1636.28, "text": " the principled formalism of active inference, now chapter five provides a kind of preliminary", "tokens": [51212, 264, 3681, 15551, 9860, 1434, 295, 4967, 38253, 11, 586, 7187, 1732, 6417, 257, 733, 295, 28817, 51568], "temperature": 0.0, "avg_logprob": -0.11427327333870581, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004678059834986925}, {"id": 186, "seek": 163628, "start": 1636.36, "end": 1643.3999999999999, "text": " sketch for the process theory of active inference, which is obviously far from an extensive", "tokens": [50368, 12325, 337, 264, 1399, 5261, 295, 4967, 38253, 11, 597, 307, 2745, 1400, 490, 364, 13246, 50720], "temperature": 0.0, "avg_logprob": -0.14006559198552912, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.023320529609918594}, {"id": 187, "seek": 163628, "start": 1643.3999999999999, "end": 1649.96, "text": " theory, it's just a single chapter. But then again, it can provide some important", "tokens": [50720, 5261, 11, 309, 311, 445, 257, 2167, 7187, 13, 583, 550, 797, 11, 309, 393, 2893, 512, 1021, 51048], "temperature": 0.0, "avg_logprob": -0.14006559198552912, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.023320529609918594}, {"id": 188, "seek": 163628, "start": 1651.56, "end": 1658.36, "text": " signposts for anyone who wants to further investigate this area.", "tokens": [51128, 1465, 23744, 82, 337, 2878, 567, 2738, 281, 3052, 15013, 341, 1859, 13, 51468], "temperature": 0.0, "avg_logprob": -0.14006559198552912, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.023320529609918594}, {"id": 189, "seek": 165836, "start": 1659.32, "end": 1668.1999999999998, "text": " Awesome. Free energy principle, Bayesian mechanics, all things in that area are on this principle,", "tokens": [50412, 10391, 13, 11551, 2281, 8665, 11, 7840, 42434, 12939, 11, 439, 721, 294, 300, 1859, 366, 322, 341, 8665, 11, 50856], "temperature": 0.0, "avg_logprob": -0.13515163022418356, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.016150228679180145}, {"id": 190, "seek": 165836, "start": 1669.0, "end": 1675.0, "text": " not responsive to empirical data. And then the process theory is about how the principle is", "tokens": [50896, 406, 21826, 281, 31886, 1412, 13, 400, 550, 264, 1399, 5261, 307, 466, 577, 264, 8665, 307, 51196], "temperature": 0.0, "avg_logprob": -0.13515163022418356, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.016150228679180145}, {"id": 191, "seek": 165836, "start": 1675.0, "end": 1682.4399999999998, "text": " implemented. So the specific generative models that are made, and how well they map, or how well", "tokens": [51196, 12270, 13, 407, 264, 2685, 1337, 1166, 5245, 300, 366, 1027, 11, 293, 577, 731, 436, 4471, 11, 420, 577, 731, 51568], "temperature": 0.0, "avg_logprob": -0.13515163022418356, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.016150228679180145}, {"id": 192, "seek": 165836, "start": 1682.4399999999998, "end": 1687.9599999999998, "text": " they do in a portfolio of models that can have very different goals and assumptions and all of", "tokens": [51568, 436, 360, 294, 257, 12583, 295, 5245, 300, 393, 362, 588, 819, 5493, 293, 17695, 293, 439, 295, 51844], "temperature": 0.0, "avg_logprob": -0.13515163022418356, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.016150228679180145}, {"id": 193, "seek": 168796, "start": 1687.96, "end": 1694.68, "text": " this. But the process theory implementation lets us develop hypotheses that are answerable to empirical", "tokens": [50364, 341, 13, 583, 264, 1399, 5261, 11420, 6653, 505, 1499, 49969, 300, 366, 1867, 712, 281, 31886, 50700], "temperature": 0.0, "avg_logprob": -0.09062405764046362, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0004878296167589724}, {"id": 194, "seek": 168796, "start": 1694.68, "end": 1705.08, "text": " data, like what is the kind of information or relationship between photons hitting the retina", "tokens": [50700, 1412, 11, 411, 437, 307, 264, 733, 295, 1589, 420, 2480, 1296, 40209, 8850, 264, 1533, 1426, 51220], "temperature": 0.0, "avg_logprob": -0.09062405764046362, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0004878296167589724}, {"id": 195, "seek": 168796, "start": 1705.72, "end": 1713.56, "text": " and changes in activity in neural systems. And that's an informational question or can be", "tokens": [51252, 293, 2962, 294, 5191, 294, 18161, 3652, 13, 400, 300, 311, 364, 49391, 1168, 420, 393, 312, 51644], "temperature": 0.0, "avg_logprob": -0.09062405764046362, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0004878296167589724}, {"id": 196, "seek": 171356, "start": 1713.56, "end": 1719.0, "text": " abstracted in a way to an informational question that it turns out does have empirical support", "tokens": [50364, 12649, 292, 294, 257, 636, 281, 364, 49391, 1168, 300, 309, 4523, 484, 775, 362, 31886, 1406, 50636], "temperature": 0.0, "avg_logprob": -0.08401648894600246, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.004198279697448015}, {"id": 197, "seek": 171356, "start": 1719.6399999999999, "end": 1724.9199999999998, "text": " and results in unique explanations and predictions. That doesn't mean that it always", "tokens": [50668, 293, 3542, 294, 3845, 28708, 293, 21264, 13, 663, 1177, 380, 914, 300, 309, 1009, 50932], "temperature": 0.0, "avg_logprob": -0.08401648894600246, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.004198279697448015}, {"id": 198, "seek": 171356, "start": 1724.9199999999998, "end": 1730.12, "text": " results in unique explanations and predictions, but a lot of citations are provided here.", "tokens": [50932, 3542, 294, 3845, 28708, 293, 21264, 11, 457, 257, 688, 295, 4814, 763, 366, 5649, 510, 13, 51192], "temperature": 0.0, "avg_logprob": -0.08401648894600246, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.004198279697448015}, {"id": 199, "seek": 171356, "start": 1731.6399999999999, "end": 1733.3999999999999, "text": " That's what we can explore in chapter five.", "tokens": [51268, 663, 311, 437, 321, 393, 6839, 294, 7187, 1732, 13, 51356], "temperature": 0.0, "avg_logprob": -0.08401648894600246, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.004198279697448015}, {"id": 200, "seek": 173340, "start": 1733.4, "end": 1743.72, "text": " The last paragraph of the first section describes that they're going to look at the", "tokens": [50364, 440, 1036, 18865, 295, 264, 700, 3541, 15626, 300, 436, 434, 516, 281, 574, 412, 264, 50880], "temperature": 0.0, "avg_logprob": -0.21966171264648438, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.015419578179717064}, {"id": 201, "seek": 173340, "start": 1744.8400000000001, "end": 1751.0800000000002, "text": " three different neural systems. Okay, section 5.2, microcircuits and messages.", "tokens": [50936, 1045, 819, 18161, 3652, 13, 1033, 11, 3541, 1025, 13, 17, 11, 4532, 23568, 66, 7688, 293, 7897, 13, 51248], "temperature": 0.0, "avg_logprob": -0.21966171264648438, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.015419578179717064}, {"id": 202, "seek": 175108, "start": 1751.8, "end": 1764.6, "text": " What do you think, Oli? All right. So I mean, this chapter begins from how", "tokens": [50400, 708, 360, 291, 519, 11, 422, 2081, 30, 1057, 558, 13, 407, 286, 914, 11, 341, 7187, 7338, 490, 577, 51040], "temperature": 0.0, "avg_logprob": -0.2261549404689244, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.015394169837236404}, {"id": 203, "seek": 175108, "start": 1765.32, "end": 1774.76, "text": " message passing happens in neurobiological terms and compare it to the way active inference", "tokens": [51076, 3636, 8437, 2314, 294, 16499, 5614, 4383, 2115, 293, 6794, 309, 281, 264, 636, 4967, 38253, 51548], "temperature": 0.0, "avg_logprob": -0.2261549404689244, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.015394169837236404}, {"id": 204, "seek": 177476, "start": 1774.76, "end": 1784.04, "text": " frames this message passing mechanism. And specifically, if we look at figure 5.1 and", "tokens": [50364, 12083, 341, 3636, 8437, 7513, 13, 400, 4682, 11, 498, 321, 574, 412, 2573, 1025, 13, 16, 293, 50828], "temperature": 0.0, "avg_logprob": -0.11165095120668411, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.07247859239578247}, {"id": 205, "seek": 177476, "start": 1784.04, "end": 1793.8, "text": " compare this figure to the ones we've seen before in chapters one through four, I think it was in", "tokens": [50828, 6794, 341, 2573, 281, 264, 2306, 321, 600, 1612, 949, 294, 20013, 472, 807, 1451, 11, 286, 519, 309, 390, 294, 51316], "temperature": 0.0, "avg_logprob": -0.11165095120668411, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.07247859239578247}, {"id": 206, "seek": 177476, "start": 1793.8, "end": 1801.72, "text": " chapter four, we can see some clear parallels between how this kind of cortical message", "tokens": [51316, 7187, 1451, 11, 321, 393, 536, 512, 1850, 44223, 1296, 577, 341, 733, 295, 11278, 804, 3636, 51712], "temperature": 0.0, "avg_logprob": -0.11165095120668411, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.07247859239578247}, {"id": 207, "seek": 180172, "start": 1801.72, "end": 1810.3600000000001, "text": " passing happens in the brain versus how it is framed in active inference literature. And as we", "tokens": [50364, 8437, 2314, 294, 264, 3567, 5717, 577, 309, 307, 30420, 294, 4967, 38253, 10394, 13, 400, 382, 321, 50796], "temperature": 0.0, "avg_logprob": -0.07758674621582032, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0022153796162456274}, {"id": 208, "seek": 180172, "start": 1810.3600000000001, "end": 1820.44, "text": " can see, it's clearly inspired by the neurobiology of the brain. But then it's important to keep", "tokens": [50796, 393, 536, 11, 309, 311, 4448, 7547, 538, 264, 16499, 5614, 1793, 295, 264, 3567, 13, 583, 550, 309, 311, 1021, 281, 1066, 51300], "temperature": 0.0, "avg_logprob": -0.07758674621582032, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0022153796162456274}, {"id": 209, "seek": 180172, "start": 1820.44, "end": 1830.84, "text": " in mind that it's not a direct one-to-one mapping between these two models. This is just", "tokens": [51300, 294, 1575, 300, 309, 311, 406, 257, 2047, 472, 12, 1353, 12, 546, 18350, 1296, 613, 732, 5245, 13, 639, 307, 445, 51820], "temperature": 0.0, "avg_logprob": -0.07758674621582032, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0022153796162456274}, {"id": 210, "seek": 183172, "start": 1832.1200000000001, "end": 1842.2, "text": " a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind", "tokens": [50384, 257, 733, 295, 11, 286, 500, 380, 458, 11, 364, 1880, 420, 28593, 990, 11, 498, 291, 411, 11, 8952, 281, 1066, 294, 1575, 50888], "temperature": 0.0, "avg_logprob": -0.0963965703363288, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.0011507397284731269}, {"id": 211, "seek": 183172, "start": 1842.92, "end": 1852.2, "text": " to somehow be a bit more confident about the viability of the theory we want to use for", "tokens": [50924, 281, 6063, 312, 257, 857, 544, 6679, 466, 264, 1932, 2310, 295, 264, 5261, 321, 528, 281, 764, 337, 51388], "temperature": 0.0, "avg_logprob": -0.0963965703363288, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.0011507397284731269}, {"id": 212, "seek": 183172, "start": 1852.2, "end": 1860.3600000000001, "text": " message passing and active inference, which is to say it's not some haphazard theory that's just", "tokens": [51388, 3636, 8437, 293, 4967, 38253, 11, 597, 307, 281, 584, 309, 311, 406, 512, 324, 950, 921, 515, 5261, 300, 311, 445, 51796], "temperature": 0.0, "avg_logprob": -0.0963965703363288, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.0011507397284731269}, {"id": 213, "seek": 186036, "start": 1860.4399999999998, "end": 1867.08, "text": " been developed for practical reasons. It has some basis in neurobiology, although", "tokens": [50368, 668, 4743, 337, 8496, 4112, 13, 467, 575, 512, 5143, 294, 16499, 5614, 1793, 11, 4878, 50700], "temperature": 0.0, "avg_logprob": -0.0918087192944118, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0012641617795452476}, {"id": 214, "seek": 186036, "start": 1868.28, "end": 1876.04, "text": " it's not necessarily fully congruent with every detail of neurobiology.", "tokens": [50760, 309, 311, 406, 4725, 4498, 8882, 19226, 365, 633, 2607, 295, 16499, 5614, 1793, 13, 51148], "temperature": 0.0, "avg_logprob": -0.0918087192944118, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0012641617795452476}, {"id": 215, "seek": 186036, "start": 1879.3999999999999, "end": 1885.6399999999999, "text": " Great. The specific example is going to involve this one region of mammalian cortex tissue", "tokens": [51316, 3769, 13, 440, 2685, 1365, 307, 516, 281, 9494, 341, 472, 4458, 295, 49312, 952, 33312, 12404, 51628], "temperature": 0.0, "avg_logprob": -0.0918087192944118, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0012641617795452476}, {"id": 216, "seek": 188564, "start": 1886.3600000000001, "end": 1890.3600000000001, "text": " that has these six layers. And there's a ton of neurobiology.", "tokens": [50400, 300, 575, 613, 2309, 7914, 13, 400, 456, 311, 257, 2952, 295, 16499, 5614, 1793, 13, 50600], "temperature": 0.0, "avg_logprob": -0.08429615314190204, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.004609060939401388}, {"id": 217, "seek": 188564, "start": 1892.68, "end": 1898.0400000000002, "text": " The big takeaway for figure 5.1 is that it's possible to graphically lay out", "tokens": [50716, 440, 955, 30681, 337, 2573, 1025, 13, 16, 307, 300, 309, 311, 1944, 281, 4295, 984, 2360, 484, 50984], "temperature": 0.0, "avg_logprob": -0.08429615314190204, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.004609060939401388}, {"id": 218, "seek": 188564, "start": 1898.68, "end": 1905.8000000000002, "text": " nodes and variables and find some empirical correspondences. Again, some unique explanations", "tokens": [51016, 13891, 293, 9102, 293, 915, 512, 31886, 6805, 2667, 13, 3764, 11, 512, 3845, 28708, 51372], "temperature": 0.0, "avg_logprob": -0.08429615314190204, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.004609060939401388}, {"id": 219, "seek": 188564, "start": 1905.8000000000002, "end": 1913.3200000000002, "text": " and predictions in certain cases. And that's one kind of modeling where it's really trying to", "tokens": [51372, 293, 21264, 294, 1629, 3331, 13, 400, 300, 311, 472, 733, 295, 15983, 689, 309, 311, 534, 1382, 281, 51748], "temperature": 0.0, "avg_logprob": -0.08429615314190204, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.004609060939401388}, {"id": 220, "seek": 191332, "start": 1913.32, "end": 1920.84, "text": " understand and improve the ability to do correlation and intervention and counterfactual", "tokens": [50364, 1223, 293, 3470, 264, 3485, 281, 360, 20009, 293, 13176, 293, 5682, 44919, 901, 50740], "temperature": 0.0, "avg_logprob": -0.09288674592971802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0027148041408509016}, {"id": 221, "seek": 191332, "start": 1920.84, "end": 1928.12, "text": " causal type analysis with the real system of interest. Or in a more pedagogical setting or", "tokens": [50740, 38755, 2010, 5215, 365, 264, 957, 1185, 295, 1179, 13, 1610, 294, 257, 544, 5670, 31599, 804, 3287, 420, 51104], "temperature": 0.0, "avg_logprob": -0.09288674592971802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0027148041408509016}, {"id": 222, "seek": 191332, "start": 1928.12, "end": 1933.32, "text": " a research setting or an industrial setting, you might sweep across large families of structures", "tokens": [51104, 257, 2132, 3287, 420, 364, 9987, 3287, 11, 291, 1062, 22169, 2108, 2416, 4466, 295, 9227, 51364], "temperature": 0.0, "avg_logprob": -0.09288674592971802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0027148041408509016}, {"id": 223, "seek": 191332, "start": 1933.32, "end": 1942.12, "text": " of models and there's no need to be grounded to any biological structure at all. So this is just", "tokens": [51364, 295, 5245, 293, 456, 311, 572, 643, 281, 312, 23535, 281, 604, 13910, 3877, 412, 439, 13, 407, 341, 307, 445, 51804], "temperature": 0.0, "avg_logprob": -0.09288674592971802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0027148041408509016}, {"id": 224, "seek": 194212, "start": 1942.12, "end": 1949.6399999999999, "text": " describing the specific neuroanatomical research that really arose out of the imaging work at UCL", "tokens": [50364, 16141, 264, 2685, 16499, 282, 267, 298, 804, 2132, 300, 534, 37192, 484, 295, 264, 25036, 589, 412, 14079, 43, 50740], "temperature": 0.0, "avg_logprob": -0.1200036601016396, "compression_ratio": 1.468503937007874, "no_speech_prob": 0.0034811669029295444}, {"id": 225, "seek": 194212, "start": 1949.6399999999999, "end": 1955.8799999999999, "text": " and the SPM package. So that's where a lot of this comes from. 5.2? Yeah, good.", "tokens": [50740, 293, 264, 8420, 44, 7372, 13, 407, 300, 311, 689, 257, 688, 295, 341, 1487, 490, 13, 1025, 13, 17, 30, 865, 11, 665, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1200036601016396, "compression_ratio": 1.468503937007874, "no_speech_prob": 0.0034811669029295444}, {"id": 226, "seek": 194212, "start": 1956.6, "end": 1964.6, "text": " And sorry, just as a side note, I think watching one of Thomas Parr's lectures on neurobiology", "tokens": [51088, 400, 2597, 11, 445, 382, 257, 1252, 3637, 11, 286, 519, 1976, 472, 295, 8500, 47890, 311, 16564, 322, 16499, 5614, 1793, 51488], "temperature": 0.0, "avg_logprob": -0.1200036601016396, "compression_ratio": 1.468503937007874, "no_speech_prob": 0.0034811669029295444}, {"id": 227, "seek": 194212, "start": 1964.6, "end": 1970.4399999999998, "text": " of active inference, which is available on YouTube, would really help to understand the materials of", "tokens": [51488, 295, 4967, 38253, 11, 597, 307, 2435, 322, 3088, 11, 576, 534, 854, 281, 1223, 264, 5319, 295, 51780], "temperature": 0.0, "avg_logprob": -0.1200036601016396, "compression_ratio": 1.468503937007874, "no_speech_prob": 0.0034811669029295444}, {"id": 228, "seek": 197044, "start": 1970.44, "end": 1983.24, "text": " this chapter better. So I highly recommend watching that one. Thanks. Figure 5.2 gives a", "tokens": [50364, 341, 7187, 1101, 13, 407, 286, 5405, 2748, 1976, 300, 472, 13, 2561, 13, 43225, 1025, 13, 17, 2709, 257, 51004], "temperature": 0.0, "avg_logprob": -0.05362272792392307, "compression_ratio": 1.2877697841726619, "no_speech_prob": 0.0010815395507961512}, {"id": 229, "seek": 197044, "start": 1983.24, "end": 1992.52, "text": " re-rendering of a kind of classical view of a hierarchical predictive coding system works.", "tokens": [51004, 319, 12, 4542, 1794, 295, 257, 733, 295, 13735, 1910, 295, 257, 35250, 804, 35521, 17720, 1185, 1985, 13, 51468], "temperature": 0.0, "avg_logprob": -0.05362272792392307, "compression_ratio": 1.2877697841726619, "no_speech_prob": 0.0010815395507961512}, {"id": 230, "seek": 199252, "start": 1993.48, "end": 2004.28, "text": " So here, abstracting a layer from the tissue six layer to just two layers here, computational layers", "tokens": [50412, 407, 510, 11, 12649, 278, 257, 4583, 490, 264, 12404, 2309, 4583, 281, 445, 732, 7914, 510, 11, 28270, 7914, 50952], "temperature": 0.0, "avg_logprob": -0.17041335999965668, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.05832424387335777}, {"id": 231, "seek": 199252, "start": 2004.28, "end": 2012.68, "text": " now, and then showing how there's hierarchical communication within a layer, but also others,", "tokens": [50952, 586, 11, 293, 550, 4099, 577, 456, 311, 35250, 804, 6101, 1951, 257, 4583, 11, 457, 611, 2357, 11, 51372], "temperature": 0.0, "avg_logprob": -0.17041335999965668, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.05832424387335777}, {"id": 232, "seek": 199252, "start": 2012.68, "end": 2020.04, "text": " they're signaling within a layer and there's a hierarchy in Bayesian modeling with variables", "tokens": [51372, 436, 434, 38639, 1951, 257, 4583, 293, 456, 311, 257, 22333, 294, 7840, 42434, 15983, 365, 9102, 51740], "temperature": 0.0, "avg_logprob": -0.17041335999965668, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.05832424387335777}, {"id": 233, "seek": 202004, "start": 2020.12, "end": 2025.48, "text": " that are higher order predictions about other variables. And that's the basis of the predictive", "tokens": [50368, 300, 366, 2946, 1668, 21264, 466, 661, 9102, 13, 400, 300, 311, 264, 5143, 295, 264, 35521, 50636], "temperature": 0.0, "avg_logprob": -0.07712457974751791, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027147468645125628}, {"id": 234, "seek": 202004, "start": 2025.48, "end": 2035.32, "text": " coding architecture. So 5.2 looks at some ways that the something that resonates with the cerebral", "tokens": [50636, 17720, 9482, 13, 407, 1025, 13, 17, 1542, 412, 512, 2098, 300, 264, 746, 300, 41051, 365, 264, 43561, 51128], "temperature": 0.0, "avg_logprob": -0.07712457974751791, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027147468645125628}, {"id": 235, "seek": 202004, "start": 2035.32, "end": 2043.0, "text": " cortical architecture enables what might computationally look like or have some really strong and", "tokens": [51128, 11278, 804, 9482, 17077, 437, 1062, 24903, 379, 574, 411, 420, 362, 512, 534, 2068, 293, 51512], "temperature": 0.0, "avg_logprob": -0.07712457974751791, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027147468645125628}, {"id": 236, "seek": 204300, "start": 2043.72, "end": 2052.28, "text": " explanatory values in actually relating to computationally a hierarchical Bayesian model,", "tokens": [50400, 9045, 4745, 4190, 294, 767, 23968, 281, 24903, 379, 257, 35250, 804, 7840, 42434, 2316, 11, 50828], "temperature": 0.0, "avg_logprob": -0.09995244798206147, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.003593053901568055}, {"id": 237, "seek": 204300, "start": 2052.28, "end": 2059.08, "text": " which could do various general tasks. All right, 5.3 is motor commands,", "tokens": [50828, 597, 727, 360, 3683, 2674, 9608, 13, 1057, 558, 11, 1025, 13, 18, 307, 5932, 16901, 11, 51168], "temperature": 0.0, "avg_logprob": -0.09995244798206147, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.003593053901568055}, {"id": 238, "seek": 204300, "start": 2059.08, "end": 2067.72, "text": " leaving the prefrontal cortex going down to the butterfly looking cross section here. What is 5.3?", "tokens": [51168, 5012, 264, 659, 11496, 304, 33312, 516, 760, 281, 264, 22140, 1237, 3278, 3541, 510, 13, 708, 307, 1025, 13, 18, 30, 51600], "temperature": 0.0, "avg_logprob": -0.09995244798206147, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.003593053901568055}, {"id": 239, "seek": 206772, "start": 2068.2, "end": 2078.9199999999996, "text": " Okay, so 5.3 moves to the other half of active inference framework, which is,", "tokens": [50388, 1033, 11, 370, 1025, 13, 18, 6067, 281, 264, 661, 1922, 295, 4967, 38253, 8388, 11, 597, 307, 11, 50924], "temperature": 0.0, "avg_logprob": -0.1761161983013153, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0009392765932716429}, {"id": 240, "seek": 206772, "start": 2078.9199999999996, "end": 2089.9599999999996, "text": " I mean, how it can model the decision making and ultimately the movement of the agent in order to", "tokens": [50924, 286, 914, 11, 577, 309, 393, 2316, 264, 3537, 1455, 293, 6284, 264, 3963, 295, 264, 9461, 294, 1668, 281, 51476], "temperature": 0.0, "avg_logprob": -0.1761161983013153, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0009392765932716429}, {"id": 241, "seek": 206772, "start": 2089.9599999999996, "end": 2097.3199999999997, "text": " minimize the expected free energy as opposed to variational free energy that we saw in perceptual", "tokens": [51476, 17522, 264, 5176, 1737, 2281, 382, 8851, 281, 3034, 1478, 1737, 2281, 300, 321, 1866, 294, 43276, 901, 51844], "temperature": 0.0, "avg_logprob": -0.1761161983013153, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0009392765932716429}, {"id": 242, "seek": 209772, "start": 2098.04, "end": 2108.9199999999996, "text": " half of active inference. So it again provides a kind of correlation or analogy between the", "tokens": [50380, 1922, 295, 4967, 38253, 13, 407, 309, 797, 6417, 257, 733, 295, 20009, 420, 21663, 1296, 264, 50924], "temperature": 0.0, "avg_logprob": -0.14155894192782315, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00154799222946167}, {"id": 243, "seek": 209772, "start": 2108.9199999999996, "end": 2116.68, "text": " structural neural anatomy, particularly related to, I mean, the motor commands and", "tokens": [50924, 15067, 18161, 31566, 11, 4098, 4077, 281, 11, 286, 914, 11, 264, 5932, 16901, 293, 51312], "temperature": 0.0, "avg_logprob": -0.14155894192782315, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00154799222946167}, {"id": 244, "seek": 209772, "start": 2119.08, "end": 2126.52, "text": " how it can relate to active inference, particularly the continuous time active inference. So", "tokens": [51432, 577, 309, 393, 10961, 281, 4967, 38253, 11, 4098, 264, 10957, 565, 4967, 38253, 13, 407, 51804], "temperature": 0.0, "avg_logprob": -0.14155894192782315, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00154799222946167}, {"id": 245, "seek": 212772, "start": 2127.72, "end": 2138.12, "text": " we can see that for, I mean, for the external event or, I'm sorry, for the external state,", "tokens": [50364, 321, 393, 536, 300, 337, 11, 286, 914, 11, 337, 264, 8320, 2280, 420, 11, 286, 478, 2597, 11, 337, 264, 8320, 1785, 11, 50884], "temperature": 0.0, "avg_logprob": -0.1304633939588392, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007203795830719173}, {"id": 246, "seek": 212772, "start": 2138.68, "end": 2147.56, "text": " we can take, for example, the proprioceptive afferent, and then this proprioceptive afferent", "tokens": [50912, 321, 393, 747, 11, 337, 1365, 11, 264, 28203, 1336, 488, 2096, 260, 317, 11, 293, 550, 341, 28203, 1336, 488, 2096, 260, 317, 51356], "temperature": 0.0, "avg_logprob": -0.1304633939588392, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007203795830719173}, {"id": 247, "seek": 212772, "start": 2147.56, "end": 2153.48, "text": " acts as a kind of Y for the continuous time active inference, which needs to be,", "tokens": [51356, 10672, 382, 257, 733, 295, 398, 337, 264, 10957, 565, 4967, 38253, 11, 597, 2203, 281, 312, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1304633939588392, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007203795830719173}, {"id": 248, "seek": 215348, "start": 2153.72, "end": 2163.64, "text": " I mean, processed in a way to optimize the expected free energy and how it relates to", "tokens": [50376, 286, 914, 11, 18846, 294, 257, 636, 281, 19719, 264, 5176, 1737, 2281, 293, 577, 309, 16155, 281, 50872], "temperature": 0.0, "avg_logprob": -0.15241894267854236, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.0013871442060917616}, {"id": 249, "seek": 215348, "start": 2164.2, "end": 2175.64, "text": " both attention and precision. We'll see a bit more detail about those terms and the relation", "tokens": [50900, 1293, 3202, 293, 18356, 13, 492, 603, 536, 257, 857, 544, 2607, 466, 729, 2115, 293, 264, 9721, 51472], "temperature": 0.0, "avg_logprob": -0.15241894267854236, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.0013871442060917616}, {"id": 250, "seek": 217564, "start": 2175.72, "end": 2186.2, "text": " between them in chapter eight, but I think here section 5.3 provides a good summary about the", "tokens": [50368, 1296, 552, 294, 7187, 3180, 11, 457, 286, 519, 510, 3541, 1025, 13, 18, 6417, 257, 665, 12691, 466, 264, 50892], "temperature": 0.0, "avg_logprob": -0.12404087494159567, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.027570480480790138}, {"id": 251, "seek": 217564, "start": 2186.2, "end": 2192.8399999999997, "text": " general paths through the motor command systems of neurobiology.", "tokens": [50892, 2674, 14518, 807, 264, 5932, 5622, 3652, 295, 16499, 5614, 1793, 13, 51224], "temperature": 0.0, "avg_logprob": -0.12404087494159567, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.027570480480790138}, {"id": 252, "seek": 217564, "start": 2194.2799999999997, "end": 2201.96, "text": " Great. I'd say while the previous case study focused on how the connectivity within and between", "tokens": [51296, 3769, 13, 286, 1116, 584, 1339, 264, 3894, 1389, 2979, 5178, 322, 577, 264, 21095, 1951, 293, 1296, 51680], "temperature": 0.0, "avg_logprob": -0.12404087494159567, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.027570480480790138}, {"id": 253, "seek": 220196, "start": 2202.84, "end": 2209.56, "text": " the cortical columns could have a computational relationship with a Bayesian", "tokens": [50408, 264, 11278, 804, 13766, 727, 362, 257, 28270, 2480, 365, 257, 7840, 42434, 50744], "temperature": 0.0, "avg_logprob": -0.12644219398498535, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.007344451732933521}, {"id": 254, "seek": 220196, "start": 2209.56, "end": 2216.12, "text": " hierarchical predictive coding architecture, the argument of the second case study is that a", "tokens": [50744, 35250, 804, 35521, 17720, 9482, 11, 264, 6770, 295, 264, 1150, 1389, 2979, 307, 300, 257, 51072], "temperature": 0.0, "avg_logprob": -0.12644219398498535, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.007344451732933521}, {"id": 255, "seek": 220196, "start": 2216.12, "end": 2225.08, "text": " continuous input, continuous output, kind of set point seeking reflexive motor behavior", "tokens": [51072, 10957, 4846, 11, 10957, 5598, 11, 733, 295, 992, 935, 11670, 23802, 488, 5932, 5223, 51520], "temperature": 0.0, "avg_logprob": -0.12644219398498535, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.007344451732933521}, {"id": 256, "seek": 222508, "start": 2226.04, "end": 2233.3199999999997, "text": " with a moving set point with a descending moving set point enabling motion by changing ultimately", "tokens": [50412, 365, 257, 2684, 992, 935, 365, 257, 40182, 2684, 992, 935, 23148, 5394, 538, 4473, 6284, 50776], "temperature": 0.0, "avg_logprob": -0.1370887932954011, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.007345354650169611}, {"id": 257, "seek": 222508, "start": 2233.3199999999997, "end": 2239.64, "text": " the set point and enabling a variation in the strategies to reach that set point through", "tokens": [50776, 264, 992, 935, 293, 23148, 257, 12990, 294, 264, 9029, 281, 2524, 300, 992, 935, 807, 51092], "temperature": 0.0, "avg_logprob": -0.1370887932954011, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.007345354650169611}, {"id": 258, "seek": 222508, "start": 2239.64, "end": 2246.36, "text": " different mechanisms. This is also describable in a compatible way.", "tokens": [51092, 819, 15902, 13, 639, 307, 611, 2189, 65, 712, 294, 257, 18218, 636, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1370887932954011, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.007345354650169611}, {"id": 259, "seek": 224636, "start": 2247.08, "end": 2253.2400000000002, "text": " That's a shorter section. Now, section 5.4, subcortical structures.", "tokens": [50400, 663, 311, 257, 11639, 3541, 13, 823, 11, 3541, 1025, 13, 19, 11, 1422, 66, 477, 804, 9227, 13, 50708], "temperature": 0.0, "avg_logprob": -0.37067365646362305, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.009706954471766949}, {"id": 260, "seek": 224636, "start": 2254.84, "end": 2256.92, "text": " What would you say about this section?", "tokens": [50788, 708, 576, 291, 584, 466, 341, 3541, 30, 50892], "temperature": 0.0, "avg_logprob": -0.37067365646362305, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.009706954471766949}, {"id": 261, "seek": 224636, "start": 2258.92, "end": 2274.1200000000003, "text": " Okay. So, subcortical structures are very important in the decision making and, I mean,", "tokens": [50992, 1033, 13, 407, 11, 1422, 66, 477, 804, 9227, 366, 588, 1021, 294, 264, 3537, 1455, 293, 11, 286, 914, 11, 51752], "temperature": 0.0, "avg_logprob": -0.37067365646362305, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.009706954471766949}, {"id": 262, "seek": 227412, "start": 2275.08, "end": 2286.2799999999997, "text": " of the agents. So, obviously, here we need another kind of analogy between the way", "tokens": [50412, 295, 264, 12554, 13, 407, 11, 2745, 11, 510, 321, 643, 1071, 733, 295, 21663, 1296, 264, 636, 50972], "temperature": 0.0, "avg_logprob": -0.2572327795482817, "compression_ratio": 1.3688524590163935, "no_speech_prob": 0.0019854146521538496}, {"id": 263, "seek": 227412, "start": 2287.4, "end": 2295.96, "text": " that these plannings and decision makings happen neuroanatomically with the way that", "tokens": [51028, 300, 613, 5038, 82, 293, 3537, 963, 1109, 1051, 16499, 282, 267, 298, 984, 365, 264, 636, 300, 51456], "temperature": 0.0, "avg_logprob": -0.2572327795482817, "compression_ratio": 1.3688524590163935, "no_speech_prob": 0.0019854146521538496}, {"id": 264, "seek": 229596, "start": 2296.6, "end": 2303.08, "text": " that it's framed in active inference. But again, we can see it's clearly based on,", "tokens": [50396, 300, 309, 311, 30420, 294, 4967, 38253, 13, 583, 797, 11, 321, 393, 536, 309, 311, 4448, 2361, 322, 11, 50720], "temperature": 0.0, "avg_logprob": -0.12832698537342585, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.05573820695281029}, {"id": 265, "seek": 229596, "start": 2304.84, "end": 2310.84, "text": " I mean, at least some of the important elements we've seen from the previous chapters.", "tokens": [50808, 286, 914, 11, 412, 1935, 512, 295, 264, 1021, 4959, 321, 600, 1612, 490, 264, 3894, 20013, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12832698537342585, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.05573820695281029}, {"id": 266, "seek": 229596, "start": 2311.8, "end": 2321.0, "text": " So, for example, we saw how policy is described or how it relates to outcomes and preference and so", "tokens": [51156, 407, 11, 337, 1365, 11, 321, 1866, 577, 3897, 307, 7619, 420, 577, 309, 16155, 281, 10070, 293, 17502, 293, 370, 51616], "temperature": 0.0, "avg_logprob": -0.12832698537342585, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.05573820695281029}, {"id": 267, "seek": 232100, "start": 2321.0, "end": 2329.48, "text": " on. We can see those elements are directly inspired by neuroanatomical structures. So,", "tokens": [50364, 322, 13, 492, 393, 536, 729, 4959, 366, 3838, 7547, 538, 16499, 282, 267, 298, 804, 9227, 13, 407, 11, 50788], "temperature": 0.0, "avg_logprob": -0.12605406375641517, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.010475934483110905}, {"id": 268, "seek": 232100, "start": 2332.52, "end": 2342.84, "text": " I guess that's, at least in my opinion, this section here 5.4 seems a bit more", "tokens": [50940, 286, 2041, 300, 311, 11, 412, 1935, 294, 452, 4800, 11, 341, 3541, 510, 1025, 13, 19, 2544, 257, 857, 544, 51456], "temperature": 0.0, "avg_logprob": -0.12605406375641517, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.010475934483110905}, {"id": 269, "seek": 234284, "start": 2343.1600000000003, "end": 2353.4, "text": " sketchy in the meaning that it doesn't go into quite the extensive details about how", "tokens": [50380, 12325, 88, 294, 264, 3620, 300, 309, 1177, 380, 352, 666, 1596, 264, 13246, 4365, 466, 577, 50892], "temperature": 0.0, "avg_logprob": -0.17432435750961303, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.008308366872370243}, {"id": 270, "seek": 234284, "start": 2354.76, "end": 2367.88, "text": " those structures can be compared. But for anyone who wants to further investigate these topics,", "tokens": [50960, 729, 9227, 393, 312, 5347, 13, 583, 337, 2878, 567, 2738, 281, 3052, 15013, 613, 8378, 11, 51616], "temperature": 0.0, "avg_logprob": -0.17432435750961303, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.008308366872370243}, {"id": 271, "seek": 236788, "start": 2367.88, "end": 2378.12, "text": " there are some useful references put on here on pages 93 and 94. So, yeah.", "tokens": [50364, 456, 366, 512, 4420, 15400, 829, 322, 510, 322, 7183, 28876, 293, 30849, 13, 407, 11, 1338, 13, 50876], "temperature": 0.0, "avg_logprob": -0.11702524649130332, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.0010481851641088724}, {"id": 272, "seek": 236788, "start": 2379.6400000000003, "end": 2387.32, "text": " Thanks. Yeah, it's really abbreviated and over viewed. But we get an interlude from table 5.1", "tokens": [50952, 2561, 13, 865, 11, 309, 311, 534, 35839, 770, 293, 670, 19174, 13, 583, 321, 483, 364, 728, 32334, 490, 3199, 1025, 13, 16, 51336], "temperature": 0.0, "avg_logprob": -0.11702524649130332, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.0010481851641088724}, {"id": 273, "seek": 236788, "start": 2388.04, "end": 2395.0, "text": " with putative roles of neurotransmitters. So, same perspective that we took before on neuroanatomical", "tokens": [51372, 365, 829, 1166, 9604, 295, 43286, 25392, 3508, 1559, 13, 407, 11, 912, 4585, 300, 321, 1890, 949, 322, 16499, 282, 267, 298, 804, 51720], "temperature": 0.0, "avg_logprob": -0.11702524649130332, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.0010481851641088724}, {"id": 274, "seek": 239500, "start": 2395.0, "end": 2403.24, "text": " functionalism here directly translates to neurotransmitter reductionism or essentialism", "tokens": [50364, 11745, 1434, 510, 3838, 28468, 281, 43286, 25392, 3508, 391, 11004, 1434, 420, 7115, 1434, 50776], "temperature": 0.0, "avg_logprob": -0.09768636226654052, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004331044852733612}, {"id": 275, "seek": 239500, "start": 2403.24, "end": 2406.92, "text": " or something like that. So, certainly all neurotransmitters and molecules that play", "tokens": [50776, 420, 746, 411, 300, 13, 407, 11, 3297, 439, 43286, 25392, 3508, 1559, 293, 13093, 300, 862, 50960], "temperature": 0.0, "avg_logprob": -0.09768636226654052, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004331044852733612}, {"id": 276, "seek": 239500, "start": 2406.92, "end": 2413.8, "text": " variable roles in different settings. And this is the neat and scruffy", "tokens": [50960, 7006, 9604, 294, 819, 6257, 13, 400, 341, 307, 264, 10654, 293, 795, 50154, 88, 51304], "temperature": 0.0, "avg_logprob": -0.09768636226654052, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004331044852733612}, {"id": 277, "seek": 239500, "start": 2417.0, "end": 2423.88, "text": " manifold all over again. One person might say, well, we need a theory for every acetylcholine", "tokens": [51464, 47138, 439, 670, 797, 13, 1485, 954, 1062, 584, 11, 731, 11, 321, 643, 257, 5261, 337, 633, 696, 2210, 75, 339, 18773, 51808], "temperature": 0.0, "avg_logprob": -0.09768636226654052, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004331044852733612}, {"id": 278, "seek": 242388, "start": 2423.88, "end": 2428.6800000000003, "text": " molecule in the world. They're all in a unique context. And someone else says,", "tokens": [50364, 15582, 294, 264, 1002, 13, 814, 434, 439, 294, 257, 3845, 4319, 13, 400, 1580, 1646, 1619, 11, 50604], "temperature": 0.0, "avg_logprob": -0.062156273336971504, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.00426391139626503}, {"id": 279, "seek": 242388, "start": 2428.6800000000003, "end": 2432.76, "text": " all neurotransmitters are described by one parameter in this model. I'm getting value", "tokens": [50604, 439, 43286, 25392, 3508, 1559, 366, 7619, 538, 472, 13075, 294, 341, 2316, 13, 286, 478, 1242, 2158, 50808], "temperature": 0.0, "avg_logprob": -0.062156273336971504, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.00426391139626503}, {"id": 280, "seek": 242388, "start": 2432.76, "end": 2438.92, "text": " from it. So, to me, that's an account. And somewhere in between is the work in this space,", "tokens": [50808, 490, 309, 13, 407, 11, 281, 385, 11, 300, 311, 364, 2696, 13, 400, 4079, 294, 1296, 307, 264, 589, 294, 341, 1901, 11, 51116], "temperature": 0.0, "avg_logprob": -0.062156273336971504, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.00426391139626503}, {"id": 281, "seek": 242388, "start": 2439.6400000000003, "end": 2447.4, "text": " which is making an attempt to have a principled and falsifiable approach to", "tokens": [51152, 597, 307, 1455, 364, 5217, 281, 362, 257, 3681, 15551, 293, 16720, 30876, 3109, 281, 51540], "temperature": 0.0, "avg_logprob": -0.062156273336971504, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.00426391139626503}, {"id": 282, "seek": 244740, "start": 2447.4, "end": 2456.12, "text": " model the computational aspects of specific regions and contexts and settings. And so,", "tokens": [50364, 2316, 264, 28270, 7270, 295, 2685, 10682, 293, 30628, 293, 6257, 13, 400, 370, 11, 50800], "temperature": 0.0, "avg_logprob": -0.14131769452776227, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.003707056399434805}, {"id": 283, "seek": 244740, "start": 2456.12, "end": 2463.4, "text": " acetylcholine, noradrenaline, dopamine and serotonin are given a little mini review here.", "tokens": [50800, 696, 2210, 75, 339, 18773, 11, 6051, 345, 23658, 533, 11, 37219, 293, 816, 27794, 259, 366, 2212, 257, 707, 8382, 3131, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14131769452776227, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.003707056399434805}, {"id": 284, "seek": 244740, "start": 2464.6, "end": 2471.32, "text": " And so, it's not an exhaustive or an exclusive claim. It's kind of a provocation from computational", "tokens": [51224, 400, 370, 11, 309, 311, 406, 364, 14687, 488, 420, 364, 13005, 3932, 13, 467, 311, 733, 295, 257, 24568, 399, 490, 28270, 51560], "temperature": 0.0, "avg_logprob": -0.14131769452776227, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.003707056399434805}, {"id": 285, "seek": 247132, "start": 2471.32, "end": 2477.2400000000002, "text": " and molecular neuroscience. And people can look into the papers and also ones that probably", "tokens": [50364, 293, 19046, 42762, 13, 400, 561, 393, 574, 666, 264, 10577, 293, 611, 2306, 300, 1391, 50660], "temperature": 0.0, "avg_logprob": -0.09424318313598633, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.010486219078302383}, {"id": 286, "seek": 247132, "start": 2477.2400000000002, "end": 2483.88, "text": " have been published since. 5.6 goes to continuous and discrete hierarchies,", "tokens": [50660, 362, 668, 6572, 1670, 13, 1025, 13, 21, 1709, 281, 10957, 293, 27706, 35250, 530, 11, 50992], "temperature": 0.0, "avg_logprob": -0.09424318313598633, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.010486219078302383}, {"id": 287, "seek": 247132, "start": 2484.76, "end": 2489.0800000000004, "text": " which is graphically overviewed in figure 5.5. So, what would you say about this?", "tokens": [51036, 597, 307, 4295, 984, 12492, 292, 294, 2573, 1025, 13, 20, 13, 407, 11, 437, 576, 291, 584, 466, 341, 30, 51252], "temperature": 0.0, "avg_logprob": -0.09424318313598633, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.010486219078302383}, {"id": 288, "seek": 247132, "start": 2491.8, "end": 2497.56, "text": " Yeah, one interesting thing about this section is the observation that", "tokens": [51388, 865, 11, 472, 1880, 551, 466, 341, 3541, 307, 264, 14816, 300, 51676], "temperature": 0.0, "avg_logprob": -0.09424318313598633, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.010486219078302383}, {"id": 289, "seek": 249756, "start": 2497.7999999999997, "end": 2506.44, "text": " our lower-level engagement with the environment can be most successfully", "tokens": [50376, 527, 3126, 12, 12418, 8742, 365, 264, 2823, 393, 312, 881, 10727, 50808], "temperature": 0.0, "avg_logprob": -0.17690178264271128, "compression_ratio": 1.5732484076433122, "no_speech_prob": 0.007330046966671944}, {"id": 290, "seek": 249756, "start": 2507.0, "end": 2514.52, "text": " characterized with continuous time formulations. But as we go up on the level of", "tokens": [50836, 29361, 365, 10957, 565, 1254, 4136, 13, 583, 382, 321, 352, 493, 322, 264, 1496, 295, 51212], "temperature": 0.0, "avg_logprob": -0.17690178264271128, "compression_ratio": 1.5732484076433122, "no_speech_prob": 0.007330046966671944}, {"id": 291, "seek": 249756, "start": 2516.12, "end": 2524.2, "text": " cognitive concepts or at the level of cognitive hierarchies, and we come to concepts such as,", "tokens": [51292, 15605, 10392, 420, 412, 264, 1496, 295, 15605, 35250, 530, 11, 293, 321, 808, 281, 10392, 1270, 382, 11, 51696], "temperature": 0.0, "avg_logprob": -0.17690178264271128, "compression_ratio": 1.5732484076433122, "no_speech_prob": 0.007330046966671944}, {"id": 292, "seek": 252420, "start": 2524.2799999999997, "end": 2533.56, "text": " I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time", "tokens": [50368, 286, 500, 380, 458, 11, 5327, 420, 754, 13585, 293, 370, 322, 11, 321, 393, 2524, 264, 1859, 300, 264, 27706, 565, 50832], "temperature": 0.0, "avg_logprob": -0.09241163535196273, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.004065418150275946}, {"id": 293, "seek": 252420, "start": 2533.56, "end": 2543.72, "text": " situations would probably be more efficient to characterize the behavior of the agent. So, this", "tokens": [50832, 6851, 576, 1391, 312, 544, 7148, 281, 38463, 264, 5223, 295, 264, 9461, 13, 407, 11, 341, 51340], "temperature": 0.0, "avg_logprob": -0.09241163535196273, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.004065418150275946}, {"id": 294, "seek": 252420, "start": 2543.72, "end": 2554.12, "text": " multi-scale structure of active inference modeling is quite evident in the way that", "tokens": [51340, 4825, 12, 20033, 3877, 295, 4967, 38253, 15983, 307, 1596, 16371, 294, 264, 636, 300, 51860], "temperature": 0.0, "avg_logprob": -0.09241163535196273, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.004065418150275946}, {"id": 295, "seek": 255420, "start": 2555.08, "end": 2561.56, "text": " our message passing happens in our brain in terms of our lower-level data processing,", "tokens": [50408, 527, 3636, 8437, 2314, 294, 527, 3567, 294, 2115, 295, 527, 3126, 12, 12418, 1412, 9007, 11, 50732], "temperature": 0.0, "avg_logprob": -0.09702757538342086, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.00023779782350175083}, {"id": 296, "seek": 255420, "start": 2562.68, "end": 2570.2, "text": " often to consolidating the higher-level cognitive concepts and ontologies.", "tokens": [50788, 2049, 281, 19045, 990, 264, 2946, 12, 12418, 15605, 10392, 293, 6592, 6204, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09702757538342086, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.00023779782350175083}, {"id": 297, "seek": 255420, "start": 2571.64, "end": 2579.56, "text": " Awesome. Thank you. To me, figure 5.5 demonstrates the kind of whole-of-body approach", "tokens": [51236, 10391, 13, 1044, 291, 13, 1407, 385, 11, 2573, 1025, 13, 20, 31034, 264, 733, 295, 1379, 12, 2670, 12, 1067, 3109, 51632], "temperature": 0.0, "avg_logprob": -0.09702757538342086, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.00023779782350175083}, {"id": 298, "seek": 257956, "start": 2579.96, "end": 2585.72, "text": " that you could imagine. There's so many organs and systems and phenomena for which", "tokens": [50384, 300, 291, 727, 3811, 13, 821, 311, 370, 867, 20659, 293, 3652, 293, 22004, 337, 597, 50672], "temperature": 0.0, "avg_logprob": -0.12121268806107548, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.037883952260017395}, {"id": 299, "seek": 257956, "start": 2585.72, "end": 2589.64, "text": " there aren't specific generative models, so little can be said about situations where no", "tokens": [50672, 456, 3212, 380, 2685, 1337, 1166, 5245, 11, 370, 707, 393, 312, 848, 466, 6851, 689, 572, 50868], "temperature": 0.0, "avg_logprob": -0.12121268806107548, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.037883952260017395}, {"id": 300, "seek": 257956, "start": 2589.64, "end": 2595.48, "text": " generative model has been articulated. And here's one where it has, so it gives you also,", "tokens": [50868, 1337, 1166, 2316, 575, 668, 43322, 13, 400, 510, 311, 472, 689, 309, 575, 11, 370, 309, 2709, 291, 611, 11, 51160], "temperature": 0.0, "avg_logprob": -0.12121268806107548, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.037883952260017395}, {"id": 301, "seek": 257956, "start": 2595.48, "end": 2601.0, "text": " it's kind of like reading a Drosophila melanogaster review paper relatively. It's like,", "tokens": [51160, 309, 311, 733, 295, 411, 3760, 257, 2491, 329, 5317, 7371, 47969, 664, 1727, 3131, 3035, 7226, 13, 467, 311, 411, 11, 51436], "temperature": 0.0, "avg_logprob": -0.12121268806107548, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.037883952260017395}, {"id": 302, "seek": 257956, "start": 2601.56, "end": 2606.6, "text": " this is how much work it takes to get to this state of knowledge in an insect. So then in", "tokens": [51464, 341, 307, 577, 709, 589, 309, 2516, 281, 483, 281, 341, 1785, 295, 3601, 294, 364, 13261, 13, 407, 550, 294, 51716], "temperature": 0.0, "avg_logprob": -0.12121268806107548, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.037883952260017395}, {"id": 303, "seek": 260660, "start": 2606.6, "end": 2613.16, "text": " another insect, do we know less about that insect empirically and genetically? So consider this to", "tokens": [50364, 1071, 13261, 11, 360, 321, 458, 1570, 466, 300, 13261, 25790, 984, 293, 37582, 30, 407, 1949, 341, 281, 50692], "temperature": 0.0, "avg_logprob": -0.07901697158813477, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00109866913408041}, {"id": 304, "seek": 260660, "start": 2613.16, "end": 2622.2799999999997, "text": " be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive", "tokens": [50692, 312, 437, 311, 2570, 281, 312, 257, 688, 11, 4461, 11, 611, 466, 472, 295, 264, 881, 16950, 420, 2685, 15605, 51148], "temperature": 0.0, "avg_logprob": -0.07901697158813477, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00109866913408041}, {"id": 305, "seek": 260660, "start": 2622.2799999999997, "end": 2628.2, "text": " systems, at least we know. So there's that additional kind of like self-reflexive aspect", "tokens": [51148, 3652, 11, 412, 1935, 321, 458, 13, 407, 456, 311, 300, 4497, 733, 295, 411, 2698, 12, 33115, 2021, 488, 4171, 51444], "temperature": 0.0, "avg_logprob": -0.07901697158813477, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00109866913408041}, {"id": 306, "seek": 260660, "start": 2628.2, "end": 2635.0, "text": " to this chapter that is not a cornerstone of active inference, but here it's just presented", "tokens": [51444, 281, 341, 7187, 300, 307, 406, 257, 4538, 11243, 295, 4967, 38253, 11, 457, 510, 309, 311, 445, 8212, 51784], "temperature": 0.0, "avg_logprob": -0.07901697158813477, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00109866913408041}, {"id": 307, "seek": 263500, "start": 2635.0, "end": 2640.04, "text": " in a synthetic case study. Anything else you want to say about 5?", "tokens": [50364, 294, 257, 23420, 1389, 2979, 13, 11998, 1646, 291, 528, 281, 584, 466, 1025, 30, 50616], "temperature": 0.0, "avg_logprob": -0.3211560529821059, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0016691829077899456}, {"id": 308, "seek": 263500, "start": 2642.92, "end": 2645.64, "text": " Nothing particular comes to mind. Thank you.", "tokens": [50760, 6693, 1729, 1487, 281, 1575, 13, 1044, 291, 13, 50896], "temperature": 0.0, "avg_logprob": -0.3211560529821059, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0016691829077899456}, {"id": 309, "seek": 263500, "start": 2645.64, "end": 2650.92, "text": " All right.", "tokens": [50896, 1057, 558, 13, 51160], "temperature": 0.0, "avg_logprob": -0.3211560529821059, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0016691829077899456}, {"id": 310, "seek": 266500, "start": 2665.0, "end": 2674.92, "text": " Okay. Chapter seven is called active inference and discrete time.", "tokens": [50364, 1033, 13, 18874, 3407, 307, 1219, 4967, 38253, 293, 27706, 565, 13, 50860], "temperature": 0.0, "avg_logprob": -0.17295548067254535, "compression_ratio": 1.5900621118012421, "no_speech_prob": 0.0056406850926578045}, {"id": 311, "seek": 266500, "start": 2675.88, "end": 2683.08, "text": " Chapter seven is the first in a pair of chapters with chapter eight on discrete and continuous", "tokens": [50908, 18874, 3407, 307, 264, 700, 294, 257, 6119, 295, 20013, 365, 7187, 3180, 322, 27706, 293, 10957, 51268], "temperature": 0.0, "avg_logprob": -0.17295548067254535, "compression_ratio": 1.5900621118012421, "no_speech_prob": 0.0056406850926578045}, {"id": 312, "seek": 266500, "start": 2683.08, "end": 2691.32, "text": " time. So they're kind of like two forks of a river that we discussed in chapter four and before", "tokens": [51268, 565, 13, 407, 436, 434, 733, 295, 411, 732, 337, 1694, 295, 257, 6810, 300, 321, 7152, 294, 7187, 1451, 293, 949, 51680], "temperature": 0.0, "avg_logprob": -0.17295548067254535, "compression_ratio": 1.5900621118012421, "no_speech_prob": 0.0056406850926578045}, {"id": 313, "seek": 269132, "start": 2691.88, "end": 2697.0, "text": " and described the recipe in chapter six. Now seven and eight are kind of like one level deeper,", "tokens": [50392, 293, 7619, 264, 6782, 294, 7187, 2309, 13, 823, 3407, 293, 3180, 366, 733, 295, 411, 472, 1496, 7731, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09215392849662087, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0028892976697534323}, {"id": 314, "seek": 269132, "start": 2697.7200000000003, "end": 2705.2400000000002, "text": " going from the kind of all of this group of animals to one level deeper into its classification scheme", "tokens": [50684, 516, 490, 264, 733, 295, 439, 295, 341, 1594, 295, 4882, 281, 472, 1496, 7731, 666, 1080, 21538, 12232, 51060], "temperature": 0.0, "avg_logprob": -0.09215392849662087, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0028892976697534323}, {"id": 315, "seek": 269132, "start": 2705.96, "end": 2713.6400000000003, "text": " on the way to the specific generative model for which it's actually given in its totality.", "tokens": [51096, 322, 264, 636, 281, 264, 2685, 1337, 1166, 2316, 337, 597, 309, 311, 767, 2212, 294, 1080, 1993, 1860, 13, 51480], "temperature": 0.0, "avg_logprob": -0.09215392849662087, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0028892976697534323}, {"id": 316, "seek": 269132, "start": 2713.6400000000003, "end": 2720.84, "text": " But everything prior to that is about the learning about its principles and this is kind of on the", "tokens": [51480, 583, 1203, 4059, 281, 300, 307, 466, 264, 2539, 466, 1080, 9156, 293, 341, 307, 733, 295, 322, 264, 51840], "temperature": 0.0, "avg_logprob": -0.09215392849662087, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0028892976697534323}, {"id": 317, "seek": 272084, "start": 2720.84, "end": 2726.28, "text": " trunk of the path to discrete time modeling, just like chapter eight will be about continuous", "tokens": [50364, 19849, 295, 264, 3100, 281, 27706, 565, 15983, 11, 445, 411, 7187, 3180, 486, 312, 466, 10957, 50636], "temperature": 0.0, "avg_logprob": -0.09942620487536415, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.0001333903637714684}, {"id": 318, "seek": 272084, "start": 2726.28, "end": 2735.32, "text": " time modeling. What would you add in? Okay. So I think chapters seven and eight", "tokens": [50636, 565, 15983, 13, 708, 576, 291, 909, 294, 30, 1033, 13, 407, 286, 519, 20013, 3407, 293, 3180, 51088], "temperature": 0.0, "avg_logprob": -0.09942620487536415, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.0001333903637714684}, {"id": 319, "seek": 272084, "start": 2735.96, "end": 2745.96, "text": " really helps to understand in a more practical way how the materials from particularly chapters one", "tokens": [51120, 534, 3665, 281, 1223, 294, 257, 544, 8496, 636, 577, 264, 5319, 490, 4098, 20013, 472, 51620], "temperature": 0.0, "avg_logprob": -0.09942620487536415, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.0001333903637714684}, {"id": 320, "seek": 274596, "start": 2745.96, "end": 2757.56, "text": " through five applies in real-time situations. So even if we somehow didn't get to understand", "tokens": [50364, 807, 1732, 13165, 294, 957, 12, 3766, 6851, 13, 407, 754, 498, 321, 6063, 994, 380, 483, 281, 1223, 50944], "temperature": 0.0, "avg_logprob": -0.13882933344159806, "compression_ratio": 1.390625, "no_speech_prob": 0.014939380809664726}, {"id": 321, "seek": 274596, "start": 2758.12, "end": 2764.68, "text": " every details of chapters one through four, when we come to chapters seven and eight,", "tokens": [50972, 633, 4365, 295, 20013, 472, 807, 1451, 11, 562, 321, 808, 281, 20013, 3407, 293, 3180, 11, 51300], "temperature": 0.0, "avg_logprob": -0.13882933344159806, "compression_ratio": 1.390625, "no_speech_prob": 0.014939380809664726}, {"id": 322, "seek": 276468, "start": 2765.64, "end": 2773.8799999999997, "text": " I think some of those uncertainties about our understandings can be clarified", "tokens": [50412, 286, 519, 512, 295, 729, 11308, 6097, 466, 527, 1223, 1109, 393, 312, 47605, 50824], "temperature": 0.0, "avg_logprob": -0.13274018287658693, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.02031099423766136}, {"id": 323, "seek": 276468, "start": 2776.8399999999997, "end": 2783.08, "text": " at least in a practical sense. So I believe these two chapters are really helpful", "tokens": [50972, 412, 1935, 294, 257, 8496, 2020, 13, 407, 286, 1697, 613, 732, 20013, 366, 534, 4961, 51284], "temperature": 0.0, "avg_logprob": -0.13274018287658693, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.02031099423766136}, {"id": 324, "seek": 276468, "start": 2784.2799999999997, "end": 2788.2799999999997, "text": " in order to consolidate our understandings from the previous chapters.", "tokens": [51344, 294, 1668, 281, 49521, 527, 1223, 1109, 490, 264, 3894, 20013, 13, 51544], "temperature": 0.0, "avg_logprob": -0.13274018287658693, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.02031099423766136}, {"id": 325, "seek": 278828, "start": 2788.6800000000003, "end": 2796.1200000000003, "text": " Awesome. Well said. So it's going to involve specifying some discrete time models.", "tokens": [50384, 10391, 13, 1042, 848, 13, 407, 309, 311, 516, 281, 9494, 1608, 5489, 512, 27706, 565, 5245, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14739135106404622, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0015729839215055108}, {"id": 326, "seek": 278828, "start": 2798.92, "end": 2804.2000000000003, "text": " Seven point two goes into perceptual processing and the general structure of the chapter is going", "tokens": [50896, 14868, 935, 732, 1709, 666, 43276, 901, 9007, 293, 264, 2674, 3877, 295, 264, 7187, 307, 516, 51160], "temperature": 0.0, "avg_logprob": -0.14739135106404622, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0015729839215055108}, {"id": 327, "seek": 278828, "start": 2804.2000000000003, "end": 2810.92, "text": " to walk through a series of examples that build in complexity where they first start with perception", "tokens": [51160, 281, 1792, 807, 257, 2638, 295, 5110, 300, 1322, 294, 14024, 689, 436, 700, 722, 365, 12860, 51496], "temperature": 0.0, "avg_logprob": -0.14739135106404622, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0015729839215055108}, {"id": 328, "seek": 281092, "start": 2811.0, "end": 2817.88, "text": " in seven point two, introduce decision making and then describe a few more types of motifs or", "tokens": [50368, 294, 3407, 935, 732, 11, 5366, 3537, 1455, 293, 550, 6786, 257, 1326, 544, 3467, 295, 2184, 18290, 420, 50712], "temperature": 0.0, "avg_logprob": -0.12019527822301007, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.04334607720375061}, {"id": 329, "seek": 281092, "start": 2817.88, "end": 2826.12, "text": " cognitive structure or patterns and also check out step by step and model stream one where it's", "tokens": [50712, 15605, 3877, 420, 8294, 293, 611, 1520, 484, 1823, 538, 1823, 293, 2316, 4309, 472, 689, 309, 311, 51124], "temperature": 0.0, "avg_logprob": -0.12019527822301007, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.04334607720375061}, {"id": 330, "seek": 281092, "start": 2826.12, "end": 2834.52, "text": " built up to in a different way. So the first example is I'll let you describe it since it's musical.", "tokens": [51124, 3094, 493, 281, 294, 257, 819, 636, 13, 407, 264, 700, 1365, 307, 286, 603, 718, 291, 6786, 309, 1670, 309, 311, 9165, 13, 51544], "temperature": 0.0, "avg_logprob": -0.12019527822301007, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.04334607720375061}, {"id": 331, "seek": 283452, "start": 2834.52, "end": 2846.7599999999998, "text": " Okay. So yeah, the first example is the situation in which we try to describe the performance", "tokens": [50364, 1033, 13, 407, 1338, 11, 264, 700, 1365, 307, 264, 2590, 294, 597, 321, 853, 281, 6786, 264, 3389, 50976], "temperature": 0.0, "avg_logprob": -0.18451171829586938, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.004594726487994194}, {"id": 332, "seek": 283452, "start": 2846.7599999999998, "end": 2855.16, "text": " of an amateur musician in terms of how we listen to the performance of an amateur musicians", "tokens": [50976, 295, 364, 29339, 19570, 294, 2115, 295, 577, 321, 2140, 281, 264, 3389, 295, 364, 29339, 16916, 51396], "temperature": 0.0, "avg_logprob": -0.18451171829586938, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.004594726487994194}, {"id": 333, "seek": 285516, "start": 2855.3999999999996, "end": 2867.3999999999996, "text": " in terms of the predictions we get from our anticipation of the following notes as opposed to", "tokens": [50376, 294, 2115, 295, 264, 21264, 321, 483, 490, 527, 35979, 295, 264, 3480, 5570, 382, 8851, 281, 50976], "temperature": 0.0, "avg_logprob": -0.14177496092660086, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.09389583766460419}, {"id": 334, "seek": 285516, "start": 2867.3999999999996, "end": 2877.3999999999996, "text": " the actual notes that's being played. So these kinds of anticipatory reaction, listening reaction to", "tokens": [50976, 264, 3539, 5570, 300, 311, 885, 3737, 13, 407, 613, 3685, 295, 10416, 4745, 5480, 11, 4764, 5480, 281, 51476], "temperature": 0.0, "avg_logprob": -0.14177496092660086, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.09389583766460419}, {"id": 335, "seek": 285516, "start": 2877.3999999999996, "end": 2884.2, "text": " the musician can be successfully formalized using discrete time active inference by", "tokens": [51476, 264, 19570, 393, 312, 10727, 9860, 1602, 1228, 27706, 565, 4967, 38253, 538, 51816], "temperature": 0.0, "avg_logprob": -0.14177496092660086, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.09389583766460419}, {"id": 336, "seek": 288516, "start": 2885.96, "end": 2900.44, "text": " putting together the matrices A for the states and matrix B for the transition between the states", "tokens": [50404, 3372, 1214, 264, 32284, 316, 337, 264, 4368, 293, 8141, 363, 337, 264, 6034, 1296, 264, 4368, 51128], "temperature": 0.0, "avg_logprob": -0.11672071508459143, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00312027451582253}, {"id": 337, "seek": 288516, "start": 2900.44, "end": 2908.3599999999997, "text": " or the transition probabilities which in this case describes the probability from going from", "tokens": [51128, 420, 264, 6034, 33783, 597, 294, 341, 1389, 15626, 264, 8482, 490, 516, 490, 51524], "temperature": 0.0, "avg_logprob": -0.11672071508459143, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.00312027451582253}, {"id": 338, "seek": 290836, "start": 2908.36, "end": 2916.52, "text": " one note to the other and obviously the actual sequence that's been played which can be described", "tokens": [50364, 472, 3637, 281, 264, 661, 293, 2745, 264, 3539, 8310, 300, 311, 668, 3737, 597, 393, 312, 7619, 50772], "temperature": 0.0, "avg_logprob": -0.13381786346435548, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.03017125092446804}, {"id": 339, "seek": 290836, "start": 2916.52, "end": 2925.96, "text": " with the matrix D. So and another point I wanted to point I wanted to mention is", "tokens": [50772, 365, 264, 8141, 413, 13, 407, 293, 1071, 935, 286, 1415, 281, 935, 286, 1415, 281, 2152, 307, 51244], "temperature": 0.0, "avg_logprob": -0.13381786346435548, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.03017125092446804}, {"id": 340, "seek": 290836, "start": 2927.1600000000003, "end": 2933.8, "text": " for anyone who has downloaded this chapter before, I don't know, I think about June or something,", "tokens": [51304, 337, 2878, 567, 575, 21748, 341, 7187, 949, 11, 286, 500, 380, 458, 11, 286, 519, 466, 6928, 420, 746, 11, 51636], "temperature": 0.0, "avg_logprob": -0.13381786346435548, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.03017125092446804}, {"id": 341, "seek": 293380, "start": 2934.6800000000003, "end": 2941.88, "text": " I recommend re-downloading it from MIT's website because they have corrected some of the typos", "tokens": [50408, 286, 2748, 319, 12, 5093, 2907, 278, 309, 490, 13100, 311, 3144, 570, 436, 362, 31687, 512, 295, 264, 2125, 329, 50768], "temperature": 0.0, "avg_logprob": -0.10898707758995795, "compression_ratio": 1.3709677419354838, "no_speech_prob": 0.005213411059230566}, {"id": 342, "seek": 293380, "start": 2941.88, "end": 2947.0, "text": " that was previously present in this chapter, particularly in figure 7.2.", "tokens": [50768, 300, 390, 8046, 1974, 294, 341, 7187, 11, 4098, 294, 2573, 1614, 13, 17, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10898707758995795, "compression_ratio": 1.3709677419354838, "no_speech_prob": 0.005213411059230566}, {"id": 343, "seek": 293380, "start": 2950.6000000000004, "end": 2958.6800000000003, "text": " Cool. So this graphical model where a person is listening, this is a general perceptual", "tokens": [51204, 8561, 13, 407, 341, 35942, 2316, 689, 257, 954, 307, 4764, 11, 341, 307, 257, 2674, 43276, 901, 51608], "temperature": 0.0, "avg_logprob": -0.10898707758995795, "compression_ratio": 1.3709677419354838, "no_speech_prob": 0.005213411059230566}, {"id": 344, "seek": 295868, "start": 2958.68, "end": 2967.3999999999996, "text": " Bayesian framing, it's specified. Just like with any other equations, there's a lot to look into,", "tokens": [50364, 7840, 42434, 28971, 11, 309, 311, 22206, 13, 1449, 411, 365, 604, 661, 11787, 11, 456, 311, 257, 688, 281, 574, 666, 11, 50800], "temperature": 0.0, "avg_logprob": -0.12685207639421736, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0035932501778006554}, {"id": 345, "seek": 295868, "start": 2967.3999999999996, "end": 2974.6, "text": " but A indicates the probability of an outcome given a state. This is saying if it were all", "tokens": [50800, 457, 316, 16203, 264, 8482, 295, 364, 9700, 2212, 257, 1785, 13, 639, 307, 1566, 498, 309, 645, 439, 51160], "temperature": 0.0, "avg_logprob": -0.12685207639421736, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0035932501778006554}, {"id": 346, "seek": 295868, "start": 2975.64, "end": 2981.56, "text": " on the diagonal identity matrix, this is kind of a common motif, then states kind of map to", "tokens": [51212, 322, 264, 21539, 6575, 8141, 11, 341, 307, 733, 295, 257, 2689, 39478, 11, 550, 4368, 733, 295, 4471, 281, 51508], "temperature": 0.0, "avg_logprob": -0.12685207639421736, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0035932501778006554}, {"id": 347, "seek": 298156, "start": 2981.56, "end": 2993.4, "text": " themself. So in the context of, in the context of this model, A represents the mapping between the", "tokens": [50364, 552, 927, 13, 407, 294, 264, 4319, 295, 11, 294, 264, 4319, 295, 341, 2316, 11, 316, 8855, 264, 18350, 1296, 264, 50956], "temperature": 0.0, "avg_logprob": -0.14494868542285674, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.00353789865039289}, {"id": 348, "seek": 298156, "start": 2993.4, "end": 3002.2, "text": " observed note and the underlying hidden true note. FNB describes the transition matrix of how those", "tokens": [50956, 13095, 3637, 293, 264, 14217, 7633, 2074, 3637, 13, 479, 45, 33, 15626, 264, 6034, 8141, 295, 577, 729, 51396], "temperature": 0.0, "avg_logprob": -0.14494868542285674, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.00353789865039289}, {"id": 349, "seek": 300220, "start": 3002.2, "end": 3011.0, "text": " change to time D is the prior. They're specified. Figure 7.2, do you want to describe it?", "tokens": [50364, 1319, 281, 565, 413, 307, 264, 4059, 13, 814, 434, 22206, 13, 43225, 1614, 13, 17, 11, 360, 291, 528, 281, 6786, 309, 30, 50804], "temperature": 0.0, "avg_logprob": -0.16659252860329368, "compression_ratio": 1.3235294117647058, "no_speech_prob": 0.008708669804036617}, {"id": 350, "seek": 300220, "start": 3015.24, "end": 3022.2, "text": " All right, so in figure 7.2, or at least the incomplete version of figure 7.2 we see here,", "tokens": [51016, 1057, 558, 11, 370, 294, 2573, 1614, 13, 17, 11, 420, 412, 1935, 264, 31709, 3037, 295, 2573, 1614, 13, 17, 321, 536, 510, 11, 51364], "temperature": 0.0, "avg_logprob": -0.16659252860329368, "compression_ratio": 1.3235294117647058, "no_speech_prob": 0.008708669804036617}, {"id": 351, "seek": 302220, "start": 3022.9199999999996, "end": 3034.6, "text": " well, at the upper left part of the picture, we see, I mean, the beliefs about each note", "tokens": [50400, 731, 11, 412, 264, 6597, 1411, 644, 295, 264, 3036, 11, 321, 536, 11, 286, 914, 11, 264, 13585, 466, 1184, 3637, 50984], "temperature": 0.0, "avg_logprob": -0.15852664907773337, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.02668306604027748}, {"id": 352, "seek": 302220, "start": 3036.04, "end": 3045.48, "text": " at each step, at each time step. And at upper right, we somehow translate those beliefs into", "tokens": [51056, 412, 1184, 1823, 11, 412, 1184, 565, 1823, 13, 400, 412, 6597, 558, 11, 321, 6063, 13799, 729, 13585, 666, 51528], "temperature": 0.0, "avg_logprob": -0.15852664907773337, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.02668306604027748}, {"id": 353, "seek": 304548, "start": 3046.28, "end": 3056.04, "text": " specific numerical values. So instead of just assigning some continuous values, we", "tokens": [50404, 2685, 29054, 4190, 13, 407, 2602, 295, 445, 49602, 512, 10957, 4190, 11, 321, 50892], "temperature": 0.0, "avg_logprob": -0.1475783550378048, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.003761484520509839}, {"id": 354, "seek": 304548, "start": 3057.96, "end": 3065.08, "text": " simplified the situation by assigning some discrete numerical values for each note.", "tokens": [50988, 26335, 264, 2590, 538, 49602, 512, 27706, 29054, 4190, 337, 1184, 3637, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1475783550378048, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.003761484520509839}, {"id": 355, "seek": 306508, "start": 3065.7999999999997, "end": 3081.08, "text": " And then the lower left is supposed to show the free energy gradients over time or in other terms,", "tokens": [50400, 400, 550, 264, 3126, 1411, 307, 3442, 281, 855, 264, 1737, 2281, 2771, 2448, 670, 565, 420, 294, 661, 2115, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12308602862887913, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.02192111685872078}, {"id": 356, "seek": 306508, "start": 3081.88, "end": 3091.96, "text": " the prediction errors we get from, I mean, comparing our predictions with the actual outcomes.", "tokens": [51204, 264, 17630, 13603, 321, 483, 490, 11, 286, 914, 11, 15763, 527, 21264, 365, 264, 3539, 10070, 13, 51708], "temperature": 0.0, "avg_logprob": -0.12308602862887913, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.02192111685872078}, {"id": 357, "seek": 309196, "start": 3092.92, "end": 3101.7200000000003, "text": " So lastly, the lower right picture shows, in parallel to the upper right picture,", "tokens": [50412, 407, 16386, 11, 264, 3126, 558, 3036, 3110, 11, 294, 8952, 281, 264, 6597, 558, 3036, 11, 50852], "temperature": 0.0, "avg_logprob": -0.11904489703294707, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.001956561114639044}, {"id": 358, "seek": 309196, "start": 3102.68, "end": 3113.8, "text": " determines the values of these errors. So we can see both the continuous, the initial,", "tokens": [50900, 24799, 264, 4190, 295, 613, 13603, 13, 407, 321, 393, 536, 1293, 264, 10957, 11, 264, 5883, 11, 51456], "temperature": 0.0, "avg_logprob": -0.11904489703294707, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.001956561114639044}, {"id": 359, "seek": 311380, "start": 3114.36, "end": 3120.04, "text": " at least initial continuous assignment and values, and then the further", "tokens": [50392, 412, 1935, 5883, 10957, 15187, 293, 4190, 11, 293, 550, 264, 3052, 50676], "temperature": 0.0, "avg_logprob": -0.12192945356492872, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.002018786035478115}, {"id": 360, "seek": 311380, "start": 3121.6400000000003, "end": 3129.88, "text": " discretizing of the values in order to get the discrete time situation or the more tractable", "tokens": [50756, 25656, 3319, 295, 264, 4190, 294, 1668, 281, 483, 264, 27706, 565, 2590, 420, 264, 544, 24207, 712, 51168], "temperature": 0.0, "avg_logprob": -0.12192945356492872, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.002018786035478115}, {"id": 361, "seek": 311380, "start": 3129.88, "end": 3138.6800000000003, "text": " discrete time situations. Okay, so it's a general passive inference task where there's", "tokens": [51168, 27706, 565, 6851, 13, 1033, 11, 370, 309, 311, 257, 2674, 14975, 38253, 5633, 689, 456, 311, 51608], "temperature": 0.0, "avg_logprob": -0.12192945356492872, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.002018786035478115}, {"id": 362, "seek": 311380, "start": 3138.6800000000003, "end": 3143.2400000000002, "text": " priors about how states are going to change through time, and then there's real data coming in.", "tokens": [51608, 1790, 830, 466, 577, 4368, 366, 516, 281, 1319, 807, 565, 11, 293, 550, 456, 311, 957, 1412, 1348, 294, 13, 51836], "temperature": 0.0, "avg_logprob": -0.12192945356492872, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.002018786035478115}, {"id": 363, "seek": 314380, "start": 3143.8, "end": 3149.0, "text": " So that's the kind of classical predictive coding, video compression, Kalman filter,", "tokens": [50364, 407, 300, 311, 264, 733, 295, 13735, 35521, 17720, 11, 960, 19355, 11, 12655, 1601, 6608, 11, 50624], "temperature": 0.0, "avg_logprob": -0.09484733932319729, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00028684132848866284}, {"id": 364, "seek": 314380, "start": 3149.7200000000003, "end": 3159.5600000000004, "text": " Bayesian setting. 7.3 introduces a key motif, which is decision making and planning as inference.", "tokens": [50660, 7840, 42434, 3287, 13, 1614, 13, 18, 31472, 257, 2141, 39478, 11, 597, 307, 3537, 1455, 293, 5038, 382, 38253, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09484733932319729, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00028684132848866284}, {"id": 365, "seek": 314380, "start": 3160.2000000000003, "end": 3165.6400000000003, "text": " So this is the idea of having a Bayes graph where the variables can relate to different", "tokens": [51184, 407, 341, 307, 264, 1558, 295, 1419, 257, 7840, 279, 4295, 689, 264, 9102, 393, 10961, 281, 819, 51456], "temperature": 0.0, "avg_logprob": -0.09484733932319729, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00028684132848866284}, {"id": 366, "seek": 314380, "start": 3165.6400000000003, "end": 3172.1200000000003, "text": " things. There's high composability. And here the idea is that a variable is going to be proposed", "tokens": [51456, 721, 13, 821, 311, 1090, 10199, 2310, 13, 400, 510, 264, 1558, 307, 300, 257, 7006, 307, 516, 281, 312, 10348, 51780], "temperature": 0.0, "avg_logprob": -0.09484733932319729, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.00028684132848866284}, {"id": 367, "seek": 317212, "start": 3172.12, "end": 3179.0, "text": " that we can do inference about that describes the process of decision making or policy selection.", "tokens": [50364, 300, 321, 393, 360, 38253, 466, 300, 15626, 264, 1399, 295, 3537, 1455, 420, 3897, 9450, 13, 50708], "temperature": 0.0, "avg_logprob": -0.08186136466869409, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.00015354444622062147}, {"id": 368, "seek": 317212, "start": 3179.0, "end": 3190.3599999999997, "text": " So what would you say about 7.3? Okay, so 7.3 is obviously similar to what we", "tokens": [50708, 407, 437, 576, 291, 584, 466, 1614, 13, 18, 30, 1033, 11, 370, 1614, 13, 18, 307, 2745, 2531, 281, 437, 321, 51276], "temperature": 0.0, "avg_logprob": -0.08186136466869409, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.00015354444622062147}, {"id": 369, "seek": 317212, "start": 3191.24, "end": 3200.52, "text": " saw in chapter four. And if I'm not mistaken, even the topology is exactly the same with that", "tokens": [51320, 1866, 294, 7187, 1451, 13, 400, 498, 286, 478, 406, 21333, 11, 754, 264, 1192, 1793, 307, 2293, 264, 912, 365, 300, 51784], "temperature": 0.0, "avg_logprob": -0.08186136466869409, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.00015354444622062147}, {"id": 370, "seek": 320052, "start": 3200.52, "end": 3213.08, "text": " picture we saw previously. So this is the initial setup or which acts also as a review", "tokens": [50364, 3036, 321, 1866, 8046, 13, 407, 341, 307, 264, 5883, 8657, 420, 597, 10672, 611, 382, 257, 3131, 50992], "temperature": 0.0, "avg_logprob": -0.1750352663152358, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.002548351651057601}, {"id": 371, "seek": 320052, "start": 3214.12, "end": 3220.28, "text": " about how these different components upon DP generative models", "tokens": [51044, 466, 577, 613, 819, 6677, 3564, 42796, 1337, 1166, 5245, 51352], "temperature": 0.0, "avg_logprob": -0.1750352663152358, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.002548351651057601}, {"id": 372, "seek": 322028, "start": 3220.36, "end": 3233.0800000000004, "text": " need to be described in such situations. But ultimately, the specific case study", "tokens": [50368, 643, 281, 312, 7619, 294, 1270, 6851, 13, 583, 6284, 11, 264, 2685, 1389, 2979, 51004], "temperature": 0.0, "avg_logprob": -0.2229944787374357, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.11741383373737335}, {"id": 373, "seek": 322028, "start": 3234.2000000000003, "end": 3244.76, "text": " we come across in this section is the attempt to model the behavior of the mouse in a teammate,", "tokens": [51060, 321, 808, 2108, 294, 341, 3541, 307, 264, 5217, 281, 2316, 264, 5223, 295, 264, 9719, 294, 257, 25467, 11, 51588], "temperature": 0.0, "avg_logprob": -0.2229944787374357, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.11741383373737335}, {"id": 374, "seek": 324476, "start": 3244.84, "end": 3256.28, "text": " so the rat in a teammate. So especially teammates containing an aversive stimulus in one arm and", "tokens": [50368, 370, 264, 5937, 294, 257, 25467, 13, 407, 2318, 20461, 19273, 364, 257, 840, 488, 21366, 294, 472, 3726, 293, 50940], "temperature": 0.0, "avg_logprob": -0.15444091214972028, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.005297440104186535}, {"id": 375, "seek": 324476, "start": 3256.28, "end": 3263.8, "text": " an attractive stimulus on the other. So this is this can act as a kind of toy example to use", "tokens": [50940, 364, 12609, 21366, 322, 264, 661, 13, 407, 341, 307, 341, 393, 605, 382, 257, 733, 295, 12058, 1365, 281, 764, 51316], "temperature": 0.0, "avg_logprob": -0.15444091214972028, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.005297440104186535}, {"id": 376, "seek": 324476, "start": 3264.6000000000004, "end": 3272.1200000000003, "text": " this kind of probabilistic modeling to describe these situations.", "tokens": [51356, 341, 733, 295, 31959, 3142, 15983, 281, 6786, 613, 6851, 13, 51732], "temperature": 0.0, "avg_logprob": -0.15444091214972028, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.005297440104186535}, {"id": 377, "seek": 327476, "start": 3275.7200000000003, "end": 3281.32, "text": " Thanks. So that leads us right to figure 7.4. Here's a visualization of the situation", "tokens": [50412, 2561, 13, 407, 300, 6689, 505, 558, 281, 2573, 1614, 13, 19, 13, 1692, 311, 257, 25801, 295, 264, 2590, 50692], "temperature": 0.0, "avg_logprob": -0.17099573214848837, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.0002453580964356661}, {"id": 378, "seek": 327476, "start": 3282.0400000000004, "end": 3293.48, "text": " with the rat in this case, where there's a pleasant and aversive stimuli on each end of a", "tokens": [50728, 365, 264, 5937, 294, 341, 1389, 11, 689, 456, 311, 257, 16232, 293, 257, 840, 488, 47752, 322, 1184, 917, 295, 257, 51300], "temperature": 0.0, "avg_logprob": -0.17099573214848837, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.0002453580964356661}, {"id": 379, "seek": 329348, "start": 3294.12, "end": 3303.2400000000002, "text": " decision point. And there's also a epistemic opportunity to receive some information", "tokens": [50396, 3537, 935, 13, 400, 456, 311, 611, 257, 2388, 468, 3438, 2650, 281, 4774, 512, 1589, 50852], "temperature": 0.0, "avg_logprob": -0.06953184155450351, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.05581197515130043}, {"id": 380, "seek": 329348, "start": 3303.96, "end": 3313.16, "text": " about the context that the animal is in. And so that setting is described for both the case with", "tokens": [50888, 466, 264, 4319, 300, 264, 5496, 307, 294, 13, 400, 370, 300, 3287, 307, 7619, 337, 1293, 264, 1389, 365, 51348], "temperature": 0.0, "avg_logprob": -0.06953184155450351, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.05581197515130043}, {"id": 381, "seek": 329348, "start": 3313.16, "end": 3319.48, "text": " white on the left, black on the right, and black on the left, white on the right. And those are shown", "tokens": [51348, 2418, 322, 264, 1411, 11, 2211, 322, 264, 558, 11, 293, 2211, 322, 264, 1411, 11, 2418, 322, 264, 558, 13, 400, 729, 366, 4898, 51664], "temperature": 0.0, "avg_logprob": -0.06953184155450351, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.05581197515130043}, {"id": 382, "seek": 331948, "start": 3319.48, "end": 3324.84, "text": " in terms of their differences in the matrices, the explicit specification of the generative model.", "tokens": [50364, 294, 2115, 295, 641, 7300, 294, 264, 32284, 11, 264, 13691, 31256, 295, 264, 1337, 1166, 2316, 13, 50632], "temperature": 0.0, "avg_logprob": -0.06474459993428197, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.002251672325655818}, {"id": 383, "seek": 331948, "start": 3327.72, "end": 3335.56, "text": " Visualizations show some of the slices of the B variable, which reflect different transition", "tokens": [50776, 23187, 14455, 855, 512, 295, 264, 19793, 295, 264, 363, 7006, 11, 597, 5031, 819, 6034, 51168], "temperature": 0.0, "avg_logprob": -0.06474459993428197, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.002251672325655818}, {"id": 384, "seek": 331948, "start": 3335.56, "end": 3344.52, "text": " probabilities. C represents the preferences, which are expressed over the observable states.", "tokens": [51168, 33783, 13, 383, 8855, 264, 21910, 11, 597, 366, 12675, 670, 264, 9951, 712, 4368, 13, 51616], "temperature": 0.0, "avg_logprob": -0.06474459993428197, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.002251672325655818}, {"id": 385, "seek": 334452, "start": 3345.4, "end": 3359.4, "text": " D reflects the priors on the different states that need priors. 7.4. What would you say about this?", "tokens": [50408, 413, 18926, 264, 1790, 830, 322, 264, 819, 4368, 300, 643, 1790, 830, 13, 1614, 13, 19, 13, 708, 576, 291, 584, 466, 341, 30, 51108], "temperature": 0.0, "avg_logprob": -0.19645815075568432, "compression_ratio": 1.4, "no_speech_prob": 0.002115141600370407}, {"id": 386, "seek": 334452, "start": 3359.4, "end": 3369.24, "text": " Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously", "tokens": [51108, 1033, 11, 370, 294, 1614, 13, 19, 11, 309, 15182, 493, 322, 264, 3894, 3541, 293, 10860, 661, 4959, 300, 321, 8046, 51600], "temperature": 0.0, "avg_logprob": -0.19645815075568432, "compression_ratio": 1.4, "no_speech_prob": 0.002115141600370407}, {"id": 387, "seek": 336924, "start": 3369.24, "end": 3382.12, "text": " saw in chapters 3 and sorry, 2 and 4, which is how the exact formulation for expected free energy", "tokens": [50364, 1866, 294, 20013, 805, 293, 2597, 11, 568, 293, 1017, 11, 597, 307, 577, 264, 1900, 37642, 337, 5176, 1737, 2281, 51008], "temperature": 0.0, "avg_logprob": -0.16645895916482675, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.006685001775622368}, {"id": 388, "seek": 336924, "start": 3382.12, "end": 3389.72, "text": " can be used, sorry, variation free energy can be used to formulate the tradeoff between the", "tokens": [51008, 393, 312, 1143, 11, 2597, 11, 12990, 1737, 2281, 393, 312, 1143, 281, 47881, 264, 4923, 4506, 1296, 264, 51388], "temperature": 0.0, "avg_logprob": -0.16645895916482675, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.006685001775622368}, {"id": 389, "seek": 338972, "start": 3390.68, "end": 3399.9599999999996, "text": " I mean, information seeking and or at least between the epistemic value and information", "tokens": [50412, 286, 914, 11, 1589, 11670, 293, 420, 412, 1935, 1296, 264, 2388, 468, 3438, 2158, 293, 1589, 50876], "temperature": 0.0, "avg_logprob": -0.20647372370180878, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.023668838664889336}, {"id": 390, "seek": 338972, "start": 3399.9599999999996, "end": 3411.24, "text": " seeking. So here, it uses, again, that rad example in a bit more, more extended and elaborate form", "tokens": [50876, 11670, 13, 407, 510, 11, 309, 4960, 11, 797, 11, 300, 2843, 1365, 294, 257, 857, 544, 11, 544, 10913, 293, 20945, 1254, 51440], "temperature": 0.0, "avg_logprob": -0.20647372370180878, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.023668838664889336}, {"id": 391, "seek": 341124, "start": 3411.24, "end": 3423.16, "text": " to formulate the epistemic value of observing Q in a given location. And figure 7.7 is a", "tokens": [50364, 281, 47881, 264, 2388, 468, 3438, 2158, 295, 22107, 1249, 294, 257, 2212, 4914, 13, 400, 2573, 1614, 13, 22, 307, 257, 50960], "temperature": 0.0, "avg_logprob": -0.15181554532518574, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.02030709572136402}, {"id": 392, "seek": 341124, "start": 3423.16, "end": 3436.3599999999997, "text": " representation of this situation. But another situation that's been, let me see, yeah, in 7.9,", "tokens": [50960, 10290, 295, 341, 2590, 13, 583, 1071, 2590, 300, 311, 668, 11, 718, 385, 536, 11, 1338, 11, 294, 1614, 13, 24, 11, 51620], "temperature": 0.0, "avg_logprob": -0.15181554532518574, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.02030709572136402}, {"id": 393, "seek": 343636, "start": 3436.36, "end": 3446.84, "text": " another case study discussed here is the situation of the psychotic eye movements.", "tokens": [50364, 1071, 1389, 2979, 7152, 510, 307, 264, 2590, 295, 264, 4681, 9411, 3313, 9981, 13, 50888], "temperature": 0.0, "avg_logprob": -0.15578953425089517, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.004257885739207268}, {"id": 394, "seek": 343636, "start": 3447.4, "end": 3456.92, "text": " And because it is something that can be quite successfully described or characterized", "tokens": [50916, 400, 570, 309, 307, 746, 300, 393, 312, 1596, 10727, 7619, 420, 29361, 51392], "temperature": 0.0, "avg_logprob": -0.15578953425089517, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.004257885739207268}, {"id": 395, "seek": 343636, "start": 3456.92, "end": 3464.1200000000003, "text": " in terms of information seeking versus the epistemic value. And the situation here is,", "tokens": [51392, 294, 2115, 295, 1589, 11670, 5717, 264, 2388, 468, 3438, 2158, 13, 400, 264, 2590, 510, 307, 11, 51752], "temperature": 0.0, "avg_logprob": -0.15578953425089517, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.004257885739207268}, {"id": 396, "seek": 346412, "start": 3465.08, "end": 3475.88, "text": " let me see, yeah, shown visually in figure 7.9, which clearly shows how our visual", "tokens": [50412, 718, 385, 536, 11, 1338, 11, 4898, 19622, 294, 2573, 1614, 13, 24, 11, 597, 4448, 3110, 577, 527, 5056, 50952], "temperature": 0.0, "avg_logprob": -0.17229279604825107, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0025052218697965145}, {"id": 397, "seek": 346412, "start": 3477.08, "end": 3486.92, "text": " psychotic eye movements can be described in such a way as to kind of trace the trajectory", "tokens": [51012, 4681, 9411, 3313, 9981, 393, 312, 7619, 294, 1270, 257, 636, 382, 281, 733, 295, 13508, 264, 21512, 51504], "temperature": 0.0, "avg_logprob": -0.17229279604825107, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0025052218697965145}, {"id": 398, "seek": 348692, "start": 3487.88, "end": 3498.36, "text": " of our eye movement among different regions of the visual space. And how the information we gather", "tokens": [50412, 295, 527, 3313, 3963, 3654, 819, 10682, 295, 264, 5056, 1901, 13, 400, 577, 264, 1589, 321, 5448, 50936], "temperature": 0.0, "avg_logprob": -0.12142161701036536, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.011316119693219662}, {"id": 399, "seek": 348692, "start": 3498.36, "end": 3510.04, "text": " from a given region can affect the, I mean, the subsequent trajectories of our psychotic eye movement.", "tokens": [50936, 490, 257, 2212, 4458, 393, 3345, 264, 11, 286, 914, 11, 264, 19962, 18257, 2083, 295, 527, 4681, 9411, 3313, 3963, 13, 51520], "temperature": 0.0, "avg_logprob": -0.12142161701036536, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.011316119693219662}, {"id": 400, "seek": 351004, "start": 3510.04, "end": 3520.84, "text": " So, yeah, that's basically the main premise of this section, I guess.", "tokens": [50364, 407, 11, 1338, 11, 300, 311, 1936, 264, 2135, 22045, 295, 341, 3541, 11, 286, 2041, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1319519595095986, "compression_ratio": 1.0943396226415094, "no_speech_prob": 0.0018380171386525035}, {"id": 401, "seek": 351004, "start": 3521.88, "end": 3527.88, "text": " Nice, great. 7.5? What would you say about it?", "tokens": [50956, 5490, 11, 869, 13, 1614, 13, 20, 30, 708, 576, 291, 584, 466, 309, 30, 51256], "temperature": 0.0, "avg_logprob": -0.1319519595095986, "compression_ratio": 1.0943396226415094, "no_speech_prob": 0.0018380171386525035}, {"id": 402, "seek": 352788, "start": 3528.44, "end": 3541.4, "text": " Okay, so 7.5, again, adds another dimension to the previous formulations. And this time,", "tokens": [50392, 1033, 11, 370, 1614, 13, 20, 11, 797, 11, 10860, 1071, 10139, 281, 264, 3894, 1254, 4136, 13, 400, 341, 565, 11, 51040], "temperature": 0.0, "avg_logprob": -0.18499662565148395, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.006091757211834192}, {"id": 403, "seek": 352788, "start": 3542.12, "end": 3556.04, "text": " we get to update the generative models by learning. And so the generative models for", "tokens": [51076, 321, 483, 281, 5623, 264, 1337, 1166, 5245, 538, 2539, 13, 400, 370, 264, 1337, 1166, 5245, 337, 51772], "temperature": 0.0, "avg_logprob": -0.18499662565148395, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.006091757211834192}, {"id": 404, "seek": 355604, "start": 3556.04, "end": 3563.24, "text": " this situation is a bit more complicated than the previous ones, because it now needs to", "tokens": [50364, 341, 2590, 307, 257, 857, 544, 6179, 813, 264, 3894, 2306, 11, 570, 309, 586, 2203, 281, 50724], "temperature": 0.0, "avg_logprob": -0.08540059271312896, "compression_ratio": 1.424, "no_speech_prob": 0.007568666245788336}, {"id": 405, "seek": 355604, "start": 3563.88, "end": 3572.52, "text": " account for a mechanism or a way to update the matrices we had before. So in the previous", "tokens": [50756, 2696, 337, 257, 7513, 420, 257, 636, 281, 5623, 264, 32284, 321, 632, 949, 13, 407, 294, 264, 3894, 51188], "temperature": 0.0, "avg_logprob": -0.08540059271312896, "compression_ratio": 1.424, "no_speech_prob": 0.007568666245788336}, {"id": 406, "seek": 357252, "start": 3572.52, "end": 3585.48, "text": " situations, we didn't account for learning, per se. But here, we directly update our general,", "tokens": [50364, 6851, 11, 321, 994, 380, 2696, 337, 2539, 11, 680, 369, 13, 583, 510, 11, 321, 3838, 5623, 527, 2674, 11, 51012], "temperature": 0.0, "avg_logprob": -0.16152567559100212, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.035089023411273956}, {"id": 407, "seek": 357252, "start": 3585.48, "end": 3595.24, "text": " sorry, the word update can be confusing here. We get to somehow improve our generative models to", "tokens": [51012, 2597, 11, 264, 1349, 5623, 393, 312, 13181, 510, 13, 492, 483, 281, 6063, 3470, 527, 1337, 1166, 5245, 281, 51500], "temperature": 0.0, "avg_logprob": -0.16152567559100212, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.035089023411273956}, {"id": 408, "seek": 359524, "start": 3596.2, "end": 3610.68, "text": " accommodate for these updating accounts. And yeah, so the situation here, or the case study", "tokens": [50412, 21410, 337, 613, 25113, 9402, 13, 400, 1338, 11, 370, 264, 2590, 510, 11, 420, 264, 1389, 2979, 51136], "temperature": 0.0, "avg_logprob": -0.20086097717285156, "compression_ratio": 1.46875, "no_speech_prob": 0.0030701912473887205}, {"id": 409, "seek": 359524, "start": 3612.2799999999997, "end": 3624.6, "text": " here, which somehow elucidates the way that the learning can be accounted for with these models.", "tokens": [51216, 510, 11, 597, 6063, 806, 1311, 327, 1024, 264, 636, 300, 264, 2539, 393, 312, 43138, 337, 365, 613, 5245, 13, 51832], "temperature": 0.0, "avg_logprob": -0.20086097717285156, "compression_ratio": 1.46875, "no_speech_prob": 0.0030701912473887205}, {"id": 410, "seek": 362524, "start": 3625.24, "end": 3639.7999999999997, "text": " Is again, a toy example of a creature in a simple world of black and white tiles, which kind of", "tokens": [50364, 1119, 797, 11, 257, 12058, 1365, 295, 257, 12797, 294, 257, 2199, 1002, 295, 2211, 293, 2418, 21982, 11, 597, 733, 295, 51092], "temperature": 0.0, "avg_logprob": -0.11066055297851562, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0005353916203603148}, {"id": 411, "seek": 362524, "start": 3639.7999999999997, "end": 3651.3999999999996, "text": " tries to find a path to reach a given destination, a certain destination. So it is more complicated", "tokens": [51092, 9898, 281, 915, 257, 3100, 281, 2524, 257, 2212, 12236, 11, 257, 1629, 12236, 13, 407, 309, 307, 544, 6179, 51672], "temperature": 0.0, "avg_logprob": -0.11066055297851562, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0005353916203603148}, {"id": 412, "seek": 365140, "start": 3651.4, "end": 3660.52, "text": " than the situation we had for the rat example, because it only had, I mean, simple trajectories", "tokens": [50364, 813, 264, 2590, 321, 632, 337, 264, 5937, 1365, 11, 570, 309, 787, 632, 11, 286, 914, 11, 2199, 18257, 2083, 50820], "temperature": 0.0, "avg_logprob": -0.08037674670316736, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00419497350230813}, {"id": 413, "seek": 365140, "start": 3660.52, "end": 3672.6, "text": " that needed to traverse. But here, the creature or the agent, in this case, needs to do lots", "tokens": [50820, 300, 2978, 281, 45674, 13, 583, 510, 11, 264, 12797, 420, 264, 9461, 11, 294, 341, 1389, 11, 2203, 281, 360, 3195, 51424], "temperature": 0.0, "avg_logprob": -0.08037674670316736, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00419497350230813}, {"id": 414, "seek": 367260, "start": 3673.16, "end": 3681.0, "text": " of lots more learning and information seeking and so on. So all the previous elements", "tokens": [50392, 295, 3195, 544, 2539, 293, 1589, 11670, 293, 370, 322, 13, 407, 439, 264, 3894, 4959, 50784], "temperature": 0.0, "avg_logprob": -0.11224720213148329, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.07681818306446075}, {"id": 415, "seek": 367260, "start": 3681.7999999999997, "end": 3689.48, "text": " is kind of combined in this example. And it's a really good example to see how the different", "tokens": [50824, 307, 733, 295, 9354, 294, 341, 1365, 13, 400, 309, 311, 257, 534, 665, 1365, 281, 536, 577, 264, 819, 51208], "temperature": 0.0, "avg_logprob": -0.11224720213148329, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.07681818306446075}, {"id": 416, "seek": 367260, "start": 3689.48, "end": 3698.68, "text": " components of active inference can be connected to each other. Nice. And 76 hierarchical or deep", "tokens": [51208, 6677, 295, 4967, 38253, 393, 312, 4582, 281, 1184, 661, 13, 5490, 13, 400, 24733, 35250, 804, 420, 2452, 51668], "temperature": 0.0, "avg_logprob": -0.11224720213148329, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.07681818306446075}, {"id": 417, "seek": 369868, "start": 3698.68, "end": 3707.56, "text": " inference burst a box 7.3 interlude on structure learning boxed off topic and a lot to say.", "tokens": [50364, 38253, 12712, 257, 2424, 1614, 13, 18, 728, 32334, 322, 3877, 2539, 2424, 292, 766, 4829, 293, 257, 688, 281, 584, 13, 50808], "temperature": 0.0, "avg_logprob": -0.11095087230205536, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.008984386920928955}, {"id": 418, "seek": 369868, "start": 3708.2, "end": 3713.48, "text": " But structure learning broadly refers to learning the structure about a model,", "tokens": [50840, 583, 3877, 2539, 19511, 14942, 281, 2539, 264, 3877, 466, 257, 2316, 11, 51104], "temperature": 0.0, "avg_logprob": -0.11095087230205536, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.008984386920928955}, {"id": 419, "seek": 369868, "start": 3714.68, "end": 3721.3199999999997, "text": " using the same types of methods that you might to do inference on, for example, a more observable", "tokens": [51164, 1228, 264, 912, 3467, 295, 7150, 300, 291, 1062, 281, 360, 38253, 322, 11, 337, 1365, 11, 257, 544, 9951, 712, 51496], "temperature": 0.0, "avg_logprob": -0.11095087230205536, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.008984386920928955}, {"id": 420, "seek": 372132, "start": 3722.1200000000003, "end": 3733.2400000000002, "text": " sensor data reading, something like that. This section works towards the idea of nested inference", "tokens": [50404, 10200, 1412, 3760, 11, 746, 411, 300, 13, 639, 3541, 1985, 3030, 264, 1558, 295, 15646, 292, 38253, 50960], "temperature": 0.0, "avg_logprob": -0.17383394743266858, "compression_ratio": 1.2713178294573644, "no_speech_prob": 0.03257494419813156}, {"id": 421, "seek": 372132, "start": 3733.2400000000002, "end": 3738.28, "text": " or multi scale modeling. What would you say about figure seven 12?", "tokens": [50960, 420, 4825, 4373, 15983, 13, 708, 576, 291, 584, 466, 2573, 3407, 2272, 30, 51212], "temperature": 0.0, "avg_logprob": -0.17383394743266858, "compression_ratio": 1.2713178294573644, "no_speech_prob": 0.03257494419813156}, {"id": 422, "seek": 373828, "start": 3738.6000000000004, "end": 3752.0400000000004, "text": " Okay, so again, this situation is, I think, the most complex situations of this chapter,", "tokens": [50380, 1033, 11, 370, 797, 11, 341, 2590, 307, 11, 286, 519, 11, 264, 881, 3997, 6851, 295, 341, 7187, 11, 51052], "temperature": 0.0, "avg_logprob": -0.1945796229622581, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.01031968742609024}, {"id": 423, "seek": 373828, "start": 3752.0400000000004, "end": 3760.92, "text": " which builds up from the previous sections. And this time, it adds another layer to accommodate", "tokens": [51052, 597, 15182, 493, 490, 264, 3894, 10863, 13, 400, 341, 565, 11, 309, 10860, 1071, 4583, 281, 21410, 51496], "temperature": 0.0, "avg_logprob": -0.1945796229622581, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.01031968742609024}, {"id": 424, "seek": 376092, "start": 3760.92, "end": 3772.12, "text": " for the inferences that happen in different time steps. So in this case, we have a multi", "tokens": [50364, 337, 264, 13596, 2667, 300, 1051, 294, 819, 565, 4439, 13, 407, 294, 341, 1389, 11, 321, 362, 257, 4825, 50924], "temperature": 0.0, "avg_logprob": -0.16687630444038204, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.008181450888514519}, {"id": 425, "seek": 376092, "start": 3772.12, "end": 3781.48, "text": " time or multi scale inference and learning happening, both at the levels of learning", "tokens": [50924, 565, 420, 4825, 4373, 38253, 293, 2539, 2737, 11, 1293, 412, 264, 4358, 295, 2539, 51392], "temperature": 0.0, "avg_logprob": -0.16687630444038204, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.008181450888514519}, {"id": 426, "seek": 378148, "start": 3781.48, "end": 3789.4, "text": " and at the levels of information seeking. So this, this is represented in", "tokens": [50364, 293, 412, 264, 4358, 295, 1589, 11670, 13, 407, 341, 11, 341, 307, 10379, 294, 50760], "temperature": 0.0, "avg_logprob": -0.12295882766311234, "compression_ratio": 1.3247863247863247, "no_speech_prob": 0.010645580478012562}, {"id": 427, "seek": 378148, "start": 3791.8, "end": 3801.72, "text": " figure seven point 12, which represents how kind of this fractal generative model", "tokens": [50880, 2573, 3407, 935, 2272, 11, 597, 8855, 577, 733, 295, 341, 17948, 304, 1337, 1166, 2316, 51376], "temperature": 0.0, "avg_logprob": -0.12295882766311234, "compression_ratio": 1.3247863247863247, "no_speech_prob": 0.010645580478012562}, {"id": 428, "seek": 380172, "start": 3802.52, "end": 3814.12, "text": " can be seen as a component in this multi scale, a bigger generative or as a kind of leaf in", "tokens": [50404, 393, 312, 1612, 382, 257, 6542, 294, 341, 4825, 4373, 11, 257, 3801, 1337, 1166, 420, 382, 257, 733, 295, 10871, 294, 50984], "temperature": 0.0, "avg_logprob": -0.12579702844425122, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.057377297431230545}, {"id": 429, "seek": 380172, "start": 3814.12, "end": 3825.08, "text": " this bigger, bigger generative model. So it can be seen as a lower level inference happening at the", "tokens": [50984, 341, 3801, 11, 3801, 1337, 1166, 2316, 13, 407, 309, 393, 312, 1612, 382, 257, 3126, 1496, 38253, 2737, 412, 264, 51532], "temperature": 0.0, "avg_logprob": -0.12579702844425122, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.057377297431230545}, {"id": 430, "seek": 382508, "start": 3825.16, "end": 3834.2799999999997, "text": " leaf level, going up to the hierarchy and influencing, sorry, collaborating on the whole", "tokens": [50368, 10871, 1496, 11, 516, 493, 281, 264, 22333, 293, 40396, 11, 2597, 11, 30188, 322, 264, 1379, 50824], "temperature": 0.0, "avg_logprob": -0.12035529558048692, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.00844111293554306}, {"id": 431, "seek": 382508, "start": 3834.2799999999997, "end": 3848.2, "text": " process of learning and inference at the higher level. So yeah, I guess that's somehow summarizes", "tokens": [50824, 1399, 295, 2539, 293, 38253, 412, 264, 2946, 1496, 13, 407, 1338, 11, 286, 2041, 300, 311, 6063, 14611, 5660, 51520], "temperature": 0.0, "avg_logprob": -0.12035529558048692, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.00844111293554306}, {"id": 432, "seek": 384820, "start": 3848.2, "end": 3856.6, "text": " this figure. So if you have anything to add. That's, that's great. It's an example of the", "tokens": [50364, 341, 2573, 13, 407, 498, 291, 362, 1340, 281, 909, 13, 663, 311, 11, 300, 311, 869, 13, 467, 311, 364, 1365, 295, 264, 50784], "temperature": 0.0, "avg_logprob": -0.13330548518412821, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.00940984021872282}, {"id": 433, "seek": 384820, "start": 3856.6, "end": 3868.3599999999997, "text": " composability of generative models, what we've talked about and had Toby Sinclair Smith describe as", "tokens": [50784, 10199, 2310, 295, 1337, 1166, 5245, 11, 437, 321, 600, 2825, 466, 293, 632, 40223, 318, 4647, 24319, 8538, 6786, 382, 51372], "temperature": 0.0, "avg_logprob": -0.13330548518412821, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.00940984021872282}, {"id": 434, "seek": 384820, "start": 3868.3599999999997, "end": 3874.9199999999996, "text": " as the compositional cognitive cartography, and just what kinds of connectors can and can't you do?", "tokens": [51372, 382, 264, 10199, 2628, 15605, 5467, 5820, 11, 293, 445, 437, 3685, 295, 31865, 393, 293, 393, 380, 291, 360, 30, 51700], "temperature": 0.0, "avg_logprob": -0.13330548518412821, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.00940984021872282}, {"id": 435, "seek": 387492, "start": 3875.7200000000003, "end": 3882.6, "text": " And how can that motif that the discrete time model introduces? And then the rest of these", "tokens": [50404, 400, 577, 393, 300, 39478, 300, 264, 27706, 565, 2316, 31472, 30, 400, 550, 264, 1472, 295, 613, 50748], "temperature": 0.0, "avg_logprob": -0.111781483265891, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.0043302271515131}, {"id": 436, "seek": 387492, "start": 3882.6, "end": 3888.76, "text": " features, including action and learning and so on get layered in on top. What can you do with that?", "tokens": [50748, 4122, 11, 3009, 3069, 293, 2539, 293, 370, 322, 483, 34666, 294, 322, 1192, 13, 708, 393, 291, 360, 365, 300, 30, 51056], "temperature": 0.0, "avg_logprob": -0.111781483265891, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.0043302271515131}, {"id": 437, "seek": 387492, "start": 3890.44, "end": 3897.4, "text": " 713 gives another example. Do you want to say anything about it or maybe continue on?", "tokens": [51140, 1614, 7668, 2709, 1071, 1365, 13, 1144, 291, 528, 281, 584, 1340, 466, 309, 420, 1310, 2354, 322, 30, 51488], "temperature": 0.0, "avg_logprob": -0.111781483265891, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.0043302271515131}, {"id": 438, "seek": 389740, "start": 3898.36, "end": 3908.2000000000003, "text": " Yeah, so the case study here is the example of linguistic, I mean, language learning through", "tokens": [50412, 865, 11, 370, 264, 1389, 2979, 510, 307, 264, 1365, 295, 43002, 11, 286, 914, 11, 2856, 2539, 807, 50904], "temperature": 0.0, "avg_logprob": -0.25265152558036474, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.012811744585633278}, {"id": 439, "seek": 389740, "start": 3908.2000000000003, "end": 3918.52, "text": " reading. So not language learning. Maybe it's just what happens in reading. Yeah, in comprehension.", "tokens": [50904, 3760, 13, 407, 406, 2856, 2539, 13, 2704, 309, 311, 445, 437, 2314, 294, 3760, 13, 865, 11, 294, 44991, 13, 51420], "temperature": 0.0, "avg_logprob": -0.25265152558036474, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.012811744585633278}, {"id": 440, "seek": 391852, "start": 3918.52, "end": 3929.64, "text": " So what happens when reading and in an anticipatory way, the words that that comes", "tokens": [50364, 407, 437, 2314, 562, 3760, 293, 294, 364, 10416, 4745, 636, 11, 264, 2283, 300, 300, 1487, 50920], "temperature": 0.0, "avg_logprob": -0.14547783031798245, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.0439748540520668}, {"id": 441, "seek": 391852, "start": 3931.0, "end": 3938.7599999999998, "text": " each after the other. So why this kind of situation can be most successfully characterized", "tokens": [50988, 1184, 934, 264, 661, 13, 407, 983, 341, 733, 295, 2590, 393, 312, 881, 10727, 29361, 51376], "temperature": 0.0, "avg_logprob": -0.14547783031798245, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.0439748540520668}, {"id": 442, "seek": 391852, "start": 3938.7599999999998, "end": 3946.7599999999998, "text": " with this kind of modeling, because it involves different scales of learning and comprehension,", "tokens": [51376, 365, 341, 733, 295, 15983, 11, 570, 309, 11626, 819, 17408, 295, 2539, 293, 44991, 11, 51776], "temperature": 0.0, "avg_logprob": -0.14547783031798245, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.0439748540520668}, {"id": 443, "seek": 394676, "start": 3946.76, "end": 3957.1600000000003, "text": " both at the level of, I mean, reading at the level of somehow observing the letters and then", "tokens": [50364, 1293, 412, 264, 1496, 295, 11, 286, 914, 11, 3760, 412, 264, 1496, 295, 6063, 22107, 264, 7825, 293, 550, 50884], "temperature": 0.0, "avg_logprob": -0.09112530169279678, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.000983923557214439}, {"id": 444, "seek": 394676, "start": 3957.1600000000003, "end": 3965.6400000000003, "text": " going on to the words and then word groups and so on. So yeah, that's a really interesting way to,", "tokens": [50884, 516, 322, 281, 264, 2283, 293, 550, 1349, 3935, 293, 370, 322, 13, 407, 1338, 11, 300, 311, 257, 534, 1880, 636, 281, 11, 51308], "temperature": 0.0, "avg_logprob": -0.09112530169279678, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.000983923557214439}, {"id": 445, "seek": 394676, "start": 3966.6000000000004, "end": 3974.76, "text": " again, combine all of those elements into a single unified model to see how those different", "tokens": [51356, 797, 11, 10432, 439, 295, 729, 4959, 666, 257, 2167, 26787, 2316, 281, 536, 577, 729, 819, 51764], "temperature": 0.0, "avg_logprob": -0.09112530169279678, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.000983923557214439}, {"id": 446, "seek": 397476, "start": 3974.76, "end": 3988.44, "text": " timescales, slow and fast timescales operate together to build this more encompassing model,", "tokens": [50364, 1413, 66, 4229, 11, 2964, 293, 2370, 1413, 66, 4229, 9651, 1214, 281, 1322, 341, 544, 28268, 278, 2316, 11, 51048], "temperature": 0.0, "avg_logprob": -0.2011974168860394, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.001700068823993206}, {"id": 447, "seek": 397476, "start": 3989.32, "end": 3991.7200000000003, "text": " more encompassing generative model of the situation.", "tokens": [51092, 544, 28268, 278, 1337, 1166, 2316, 295, 264, 2590, 13, 51212], "temperature": 0.0, "avg_logprob": -0.2011974168860394, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.001700068823993206}, {"id": 448, "seek": 397476, "start": 3993.5600000000004, "end": 3995.32, "text": " Great. Any closing thoughts on 7?", "tokens": [51304, 3769, 13, 2639, 10377, 4598, 322, 1614, 30, 51392], "temperature": 0.0, "avg_logprob": -0.2011974168860394, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.001700068823993206}, {"id": 449, "seek": 399532, "start": 3995.8, "end": 4001.0, "text": " Nothing particular, not bad. Thanks.", "tokens": [50388, 6693, 1729, 11, 406, 1578, 13, 2561, 13, 50648], "temperature": 0.0, "avg_logprob": -0.49755406379699707, "compression_ratio": 1.2019230769230769, "no_speech_prob": 0.04333676025271416}, {"id": 450, "seek": 399532, "start": 4001.0, "end": 4015.88, "text": " All right. Next chapter is chapter eight, which is going to go into the continuous time.", "tokens": [50648, 1057, 558, 13, 3087, 7187, 307, 7187, 3180, 11, 597, 307, 516, 281, 352, 666, 264, 10957, 565, 13, 51392], "temperature": 0.0, "avg_logprob": -0.49755406379699707, "compression_ratio": 1.2019230769230769, "no_speech_prob": 0.04333676025271416}, {"id": 451, "seek": 402532, "start": 4026.04, "end": 4032.28, "text": " All right. Chapter eight is called active inference and continuous time begins with that", "tokens": [50400, 1057, 558, 13, 18874, 3180, 307, 1219, 4967, 38253, 293, 10957, 565, 7338, 365, 300, 50712], "temperature": 0.0, "avg_logprob": -0.16675069414336105, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.005383218638598919}, {"id": 452, "seek": 402532, "start": 4032.28, "end": 4037.88, "text": " timeless quote, everything flows, nothing stands still. So what would you say about chapter eight?", "tokens": [50712, 41200, 6513, 11, 1203, 12867, 11, 1825, 7382, 920, 13, 407, 437, 576, 291, 584, 466, 7187, 3180, 30, 50992], "temperature": 0.0, "avg_logprob": -0.16675069414336105, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.005383218638598919}, {"id": 453, "seek": 402532, "start": 4039.96, "end": 4047.56, "text": " All right. So this chapter probably is my most favorite chapter in the book,", "tokens": [51096, 1057, 558, 13, 407, 341, 7187, 1391, 307, 452, 881, 2954, 7187, 294, 264, 1446, 11, 51476], "temperature": 0.0, "avg_logprob": -0.16675069414336105, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.005383218638598919}, {"id": 454, "seek": 404756, "start": 4047.56, "end": 4055.24, "text": " because of my own personal interest in, I don't know, the process materials and so on.", "tokens": [50364, 570, 295, 452, 1065, 2973, 1179, 294, 11, 286, 500, 380, 458, 11, 264, 1399, 5319, 293, 370, 322, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15916253809343306, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.008308895863592625}, {"id": 455, "seek": 404756, "start": 4055.24, "end": 4065.4, "text": " But yeah, so chapter seven acts as a really good starting point for anyone who wants to", "tokens": [50748, 583, 1338, 11, 370, 7187, 3407, 10672, 382, 257, 534, 665, 2891, 935, 337, 2878, 567, 2738, 281, 51256], "temperature": 0.0, "avg_logprob": -0.15916253809343306, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.008308895863592625}, {"id": 456, "seek": 404756, "start": 4066.6, "end": 4071.4, "text": " develop the discrete time situations, to model discrete time situations", "tokens": [51316, 1499, 264, 27706, 565, 6851, 11, 281, 2316, 27706, 565, 6851, 51556], "temperature": 0.0, "avg_logprob": -0.15916253809343306, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.008308895863592625}, {"id": 457, "seek": 407140, "start": 4072.36, "end": 4079.32, "text": " within active inference framework. But in chapter eight, we kind of get to", "tokens": [50412, 1951, 4967, 38253, 8388, 13, 583, 294, 7187, 3180, 11, 321, 733, 295, 483, 281, 50760], "temperature": 0.0, "avg_logprob": -0.16741859912872314, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.008981159888207912}, {"id": 458, "seek": 407140, "start": 4081.64, "end": 4088.6800000000003, "text": " model a bit more interesting or, let's say, more involving situations.", "tokens": [50876, 2316, 257, 857, 544, 1880, 420, 11, 718, 311, 584, 11, 544, 17030, 6851, 13, 51228], "temperature": 0.0, "avg_logprob": -0.16741859912872314, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.008981159888207912}, {"id": 459, "seek": 407140, "start": 4089.64, "end": 4099.0, "text": " And they're not necessarily toy examples we saw at least at the beginning of chapter seven.", "tokens": [51276, 400, 436, 434, 406, 4725, 12058, 5110, 321, 1866, 412, 1935, 412, 264, 2863, 295, 7187, 3407, 13, 51744], "temperature": 0.0, "avg_logprob": -0.16741859912872314, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.008981159888207912}, {"id": 460, "seek": 409900, "start": 4099.96, "end": 4107.96, "text": " So obviously, as the title suggests, this chapter deals with the continuous time situation.", "tokens": [50412, 407, 2745, 11, 382, 264, 4876, 13409, 11, 341, 7187, 11215, 365, 264, 10957, 565, 2590, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13562437404285776, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0008826336124911904}, {"id": 461, "seek": 409900, "start": 4107.96, "end": 4117.4, "text": " So in that case, we'll need to, maybe at this point, refresh our memory about", "tokens": [50812, 407, 294, 300, 1389, 11, 321, 603, 643, 281, 11, 1310, 412, 341, 935, 11, 15134, 527, 4675, 466, 51284], "temperature": 0.0, "avg_logprob": -0.13562437404285776, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0008826336124911904}, {"id": 462, "seek": 409900, "start": 4117.4, "end": 4122.28, "text": " what continuous time situation involves by reading the relevant parts,", "tokens": [51284, 437, 10957, 565, 2590, 11626, 538, 3760, 264, 7340, 3166, 11, 51528], "temperature": 0.0, "avg_logprob": -0.13562437404285776, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0008826336124911904}, {"id": 463, "seek": 412228, "start": 4123.16, "end": 4130.759999999999, "text": " reading or reviewing relevant parts of chapter four. So yeah, in chapter four,", "tokens": [50408, 3760, 420, 19576, 7340, 3166, 295, 7187, 1451, 13, 407, 1338, 11, 294, 7187, 1451, 11, 50788], "temperature": 0.0, "avg_logprob": -0.20873012182847508, "compression_ratio": 1.4834437086092715, "no_speech_prob": 0.006486499682068825}, {"id": 464, "seek": 412228, "start": 4130.759999999999, "end": 4138.92, "text": " we saw that the generative model for continuous time situation derives from the", "tokens": [50788, 321, 1866, 300, 264, 1337, 1166, 2316, 337, 10957, 565, 2590, 1163, 1539, 490, 264, 51196], "temperature": 0.0, "avg_logprob": -0.20873012182847508, "compression_ratio": 1.4834437086092715, "no_speech_prob": 0.006486499682068825}, {"id": 465, "seek": 412228, "start": 4140.12, "end": 4146.44, "text": " it is a stochastic calculus in terms of putting the whole process", "tokens": [51256, 309, 307, 257, 342, 8997, 2750, 33400, 294, 2115, 295, 3372, 264, 1379, 1399, 51572], "temperature": 0.0, "avg_logprob": -0.20873012182847508, "compression_ratio": 1.4834437086092715, "no_speech_prob": 0.006486499682068825}, {"id": 466, "seek": 414644, "start": 4146.759999999999, "end": 4157.32, "text": " into two elements, two stochastic equations, one of which is the actual", "tokens": [50380, 666, 732, 4959, 11, 732, 342, 8997, 2750, 11787, 11, 472, 295, 597, 307, 264, 3539, 50908], "temperature": 0.0, "avg_logprob": -0.150978150914927, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.01910957507789135}, {"id": 467, "seek": 414644, "start": 4158.12, "end": 4165.16, "text": " state, the condition of actual states or the behavior of the actual states. And the other one", "tokens": [50948, 1785, 11, 264, 4188, 295, 3539, 4368, 420, 264, 5223, 295, 264, 3539, 4368, 13, 400, 264, 661, 472, 51300], "temperature": 0.0, "avg_logprob": -0.150978150914927, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.01910957507789135}, {"id": 468, "seek": 414644, "start": 4165.799999999999, "end": 4173.16, "text": " is the randomness that we need to account for in each real time continuous time situations.", "tokens": [51332, 307, 264, 4974, 1287, 300, 321, 643, 281, 2696, 337, 294, 1184, 957, 565, 10957, 565, 6851, 13, 51700], "temperature": 0.0, "avg_logprob": -0.150978150914927, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.01910957507789135}, {"id": 469, "seek": 417316, "start": 4173.24, "end": 4181.72, "text": " So that's what we get here in equation 8.1. And then, building up from that equation,", "tokens": [50368, 407, 300, 311, 437, 321, 483, 510, 294, 5367, 1649, 13, 16, 13, 400, 550, 11, 2390, 493, 490, 300, 5367, 11, 50792], "temperature": 0.0, "avg_logprob": -0.18144596324247472, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0030719698406755924}, {"id": 470, "seek": 417316, "start": 4183.08, "end": 4194.44, "text": " we, it generalizes that equation to involve, I mean, the functionals of G and F instead of just", "tokens": [50860, 321, 11, 309, 2674, 5660, 300, 5367, 281, 9494, 11, 286, 914, 11, 264, 2445, 1124, 295, 460, 293, 479, 2602, 295, 445, 51428], "temperature": 0.0, "avg_logprob": -0.18144596324247472, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0030719698406755924}, {"id": 471, "seek": 419444, "start": 4195.4, "end": 4205.0, "text": " the single valued functions of G and F. So then we get to", "tokens": [50412, 264, 2167, 22608, 6828, 295, 460, 293, 479, 13, 407, 550, 321, 483, 281, 50892], "temperature": 0.0, "avg_logprob": -0.13496927211159154, "compression_ratio": 1.2881355932203389, "no_speech_prob": 0.00757051445543766}, {"id": 472, "seek": 419444, "start": 4207.24, "end": 4215.4, "text": " put that into the situation that can be used for describing the behavior of dynamical systems,", "tokens": [51004, 829, 300, 666, 264, 2590, 300, 393, 312, 1143, 337, 16141, 264, 5223, 295, 5999, 804, 3652, 11, 51412], "temperature": 0.0, "avg_logprob": -0.13496927211159154, "compression_ratio": 1.2881355932203389, "no_speech_prob": 0.00757051445543766}, {"id": 473, "seek": 421540, "start": 4216.04, "end": 4224.04, "text": " which is a very well known situation to use these kinds of stochastic equations.", "tokens": [50396, 597, 307, 257, 588, 731, 2570, 2590, 281, 764, 613, 3685, 295, 342, 8997, 2750, 11787, 13, 50796], "temperature": 0.0, "avg_logprob": -0.19961674073163202, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.026731597259640694}, {"id": 474, "seek": 421540, "start": 4224.679999999999, "end": 4232.92, "text": " And it's widely studied how those, those kinds of dynamics can be characterized, especially", "tokens": [50828, 400, 309, 311, 13371, 9454, 577, 729, 11, 729, 3685, 295, 15679, 393, 312, 29361, 11, 2318, 51240], "temperature": 0.0, "avg_logprob": -0.19961674073163202, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.026731597259640694}, {"id": 475, "seek": 421540, "start": 4232.92, "end": 4242.679999999999, "text": " in recent Bayesian mechanics paper by Dalton, Saktiv Atevel and others. So, and then it gets to", "tokens": [51240, 294, 5162, 7840, 42434, 12939, 3035, 538, 17357, 1756, 11, 318, 5886, 592, 316, 975, 779, 293, 2357, 13, 407, 11, 293, 550, 309, 2170, 281, 51728], "temperature": 0.0, "avg_logprob": -0.19961674073163202, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.026731597259640694}, {"id": 476, "seek": 424268, "start": 4242.68, "end": 4251.320000000001, "text": " some more specific examples such as Lothgabal-Terra dynamics and synchronicity and so on, in order", "tokens": [50364, 512, 544, 2685, 5110, 1270, 382, 441, 900, 70, 455, 304, 12, 43352, 424, 15679, 293, 5451, 339, 10011, 507, 293, 370, 322, 11, 294, 1668, 50796], "temperature": 0.0, "avg_logprob": -0.2758630180358887, "compression_ratio": 1.393939393939394, "no_speech_prob": 0.006789997220039368}, {"id": 477, "seek": 424268, "start": 4251.320000000001, "end": 4264.280000000001, "text": " to show how these kinds of dynamics can be elaborated upon and can be generalized to,", "tokens": [50796, 281, 855, 577, 613, 3685, 295, 15679, 393, 312, 16298, 770, 3564, 293, 393, 312, 44498, 281, 11, 51444], "temperature": 0.0, "avg_logprob": -0.2758630180358887, "compression_ratio": 1.393939393939394, "no_speech_prob": 0.006789997220039368}, {"id": 478, "seek": 426428, "start": 4264.28, "end": 4272.44, "text": " and enables them to characterize more complex situations. So,", "tokens": [50364, 293, 17077, 552, 281, 38463, 544, 3997, 6851, 13, 407, 11, 50772], "temperature": 0.0, "avg_logprob": -0.15550189115563218, "compression_ratio": 1.3472222222222223, "no_speech_prob": 0.0022508143447339535}, {"id": 479, "seek": 426428, "start": 4275.4, "end": 4281.639999999999, "text": " yeah, that's a really short and brief overview of the whole chapter. Maybe", "tokens": [50920, 1338, 11, 300, 311, 257, 534, 2099, 293, 5353, 12492, 295, 264, 1379, 7187, 13, 2704, 51232], "temperature": 0.0, "avg_logprob": -0.15550189115563218, "compression_ratio": 1.3472222222222223, "no_speech_prob": 0.0022508143447339535}, {"id": 480, "seek": 426428, "start": 4282.84, "end": 4285.5599999999995, "text": " we can talk about a bit more details as we go through it.", "tokens": [51292, 321, 393, 751, 466, 257, 857, 544, 4365, 382, 321, 352, 807, 309, 13, 51428], "temperature": 0.0, "avg_logprob": -0.15550189115563218, "compression_ratio": 1.3472222222222223, "no_speech_prob": 0.0022508143447339535}, {"id": 481, "seek": 428556, "start": 4285.8, "end": 4294.6, "text": " Great. Well said. Well, I'm sure for another day, the philosophical implications of eight,", "tokens": [50376, 3769, 13, 1042, 848, 13, 1042, 11, 286, 478, 988, 337, 1071, 786, 11, 264, 25066, 16602, 295, 3180, 11, 50816], "temperature": 0.0, "avg_logprob": -0.18966947702261117, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.0050598871894180775}, {"id": 482, "seek": 428556, "start": 4294.6, "end": 4299.320000000001, "text": " seven and eight, and high road and low road, and all these other parts of the textbook, great topics.", "tokens": [50816, 3407, 293, 3180, 11, 293, 1090, 3060, 293, 2295, 3060, 11, 293, 439, 613, 661, 3166, 295, 264, 25591, 11, 869, 8378, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18966947702261117, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.0050598871894180775}, {"id": 483, "seek": 428556, "start": 4300.200000000001, "end": 4308.6, "text": " I agree. I would see chapter eight as demonstrating continuity with some classical", "tokens": [51096, 286, 3986, 13, 286, 576, 536, 7187, 3180, 382, 29889, 23807, 365, 512, 13735, 51516], "temperature": 0.0, "avg_logprob": -0.18966947702261117, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.0050598871894180775}, {"id": 484, "seek": 430860, "start": 4309.56, "end": 4315.88, "text": " continuous time modeling motifs from a few different areas of dynamical system science,", "tokens": [50412, 10957, 565, 15983, 2184, 18290, 490, 257, 1326, 819, 3179, 295, 5999, 804, 1185, 3497, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1520668254179113, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0027148197405040264}, {"id": 485, "seek": 430860, "start": 4316.6, "end": 4321.240000000001, "text": " which is applied in like many, many, many fields, but these are some classic examples.", "tokens": [50764, 597, 307, 6456, 294, 411, 867, 11, 867, 11, 867, 7909, 11, 457, 613, 366, 512, 7230, 5110, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1520668254179113, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0027148197405040264}, {"id": 486, "seek": 430860, "start": 4321.240000000001, "end": 4327.72, "text": " So, figure eight point one goes a little bit more into depth, or at least into more formalism", "tokens": [50996, 407, 11, 2573, 3180, 935, 472, 1709, 257, 707, 857, 544, 666, 7161, 11, 420, 412, 1935, 666, 544, 9860, 1434, 51320], "temperature": 0.0, "avg_logprob": -0.1520668254179113, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0027148197405040264}, {"id": 487, "seek": 430860, "start": 4327.72, "end": 4335.4800000000005, "text": " detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive", "tokens": [51320, 2607, 466, 2293, 437, 321, 1866, 294, 7187, 1732, 365, 264, 28022, 23802, 10346, 365, 264, 28203, 1336, 488, 51708], "temperature": 0.0, "avg_logprob": -0.1520668254179113, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0027148197405040264}, {"id": 488, "seek": 433548, "start": 4335.5599999999995, "end": 4340.28, "text": " data coming in, and then a differential being calculated with the set point,", "tokens": [50368, 1412, 1348, 294, 11, 293, 550, 257, 15756, 885, 15598, 365, 264, 992, 935, 11, 50604], "temperature": 0.0, "avg_logprob": -0.09221265889421294, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.008576270192861557}, {"id": 489, "seek": 433548, "start": 4340.28, "end": 4347.4, "text": " which reflects a descending prediction from a decision making layer. And that can be viewed as", "tokens": [50604, 597, 18926, 257, 40182, 17630, 490, 257, 3537, 1455, 4583, 13, 400, 300, 393, 312, 19174, 382, 50960], "temperature": 0.0, "avg_logprob": -0.09221265889421294, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.008576270192861557}, {"id": 490, "seek": 433548, "start": 4347.4, "end": 4355.32, "text": " this kind of mechanics that plays out in a phase space in continuous time, like a spring moving", "tokens": [50960, 341, 733, 295, 12939, 300, 5749, 484, 294, 257, 5574, 1901, 294, 10957, 565, 11, 411, 257, 5587, 2684, 51356], "temperature": 0.0, "avg_logprob": -0.09221265889421294, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.008576270192861557}, {"id": 491, "seek": 433548, "start": 4355.32, "end": 4362.28, "text": " around with someone making a certain path with an attractor, and a spring being dragged around", "tokens": [51356, 926, 365, 1580, 1455, 257, 1629, 3100, 365, 364, 5049, 284, 11, 293, 257, 5587, 885, 25717, 926, 51704], "temperature": 0.0, "avg_logprob": -0.09221265889421294, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.008576270192861557}, {"id": 492, "seek": 436228, "start": 4362.28, "end": 4369.5599999999995, "text": " something in that area. Box eight point one goes into a very fascinating topic. Do you want to", "tokens": [50364, 746, 294, 300, 1859, 13, 15112, 3180, 935, 472, 1709, 666, 257, 588, 10343, 4829, 13, 1144, 291, 528, 281, 50728], "temperature": 0.0, "avg_logprob": -0.12006404995918274, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0010003572097048163}, {"id": 493, "seek": 436228, "start": 4369.5599999999995, "end": 4383.96, "text": " describe it? Well, it's maybe one of the most thought provoking pages of the whole book. And", "tokens": [50728, 6786, 309, 30, 1042, 11, 309, 311, 1310, 472, 295, 264, 881, 1194, 1439, 5953, 7183, 295, 264, 1379, 1446, 13, 400, 51448], "temperature": 0.0, "avg_logprob": -0.12006404995918274, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0010003572097048163}, {"id": 494, "seek": 438396, "start": 4383.96, "end": 4389.32, "text": " if I remember correctly, in all of the cohorts, this particular box", "tokens": [50364, 498, 286, 1604, 8944, 11, 294, 439, 295, 264, 21683, 3299, 11, 341, 1729, 2424, 50632], "temperature": 0.0, "avg_logprob": -0.14114810625712076, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.06084456667304039}, {"id": 495, "seek": 438396, "start": 4391.8, "end": 4398.84, "text": " I mean gives always gives rise to lots of questions, because of some of the interesting", "tokens": [50756, 286, 914, 2709, 1009, 2709, 6272, 281, 3195, 295, 1651, 11, 570, 295, 512, 295, 264, 1880, 51108], "temperature": 0.0, "avg_logprob": -0.14114810625712076, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.06084456667304039}, {"id": 496, "seek": 438396, "start": 4398.84, "end": 4409.88, "text": " and at least initially counterintuitive claims here. But I don't want to spoil it. So", "tokens": [51108, 293, 412, 1935, 9105, 5682, 686, 48314, 9441, 510, 13, 583, 286, 500, 380, 528, 281, 18630, 309, 13, 407, 51660], "temperature": 0.0, "avg_logprob": -0.14114810625712076, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.06084456667304039}, {"id": 497, "seek": 440988, "start": 4410.4400000000005, "end": 4426.52, "text": " but as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief", "tokens": [50392, 457, 382, 257, 733, 295, 26927, 9615, 11, 309, 733, 295, 2170, 281, 534, 1880, 11, 457, 419, 296, 11, 588, 5353, 51196], "temperature": 0.0, "avg_logprob": -0.1863369724967263, "compression_ratio": 1.4596774193548387, "no_speech_prob": 0.001806717598810792}, {"id": 498, "seek": 440988, "start": 4426.52, "end": 4433.8, "text": " discussion about the comparing these terms precision, attention, and sensory attenuation,", "tokens": [51196, 5017, 466, 264, 15763, 613, 2115, 18356, 11, 3202, 11, 293, 27233, 951, 268, 16073, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1863369724967263, "compression_ratio": 1.4596774193548387, "no_speech_prob": 0.001806717598810792}, {"id": 499, "seek": 443380, "start": 4433.8, "end": 4440.68, "text": " and the relation and similarities and difference between these two, these three terms,", "tokens": [50364, 293, 264, 9721, 293, 24197, 293, 2649, 1296, 613, 732, 11, 613, 1045, 2115, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1061596130502635, "compression_ratio": 1.535294117647059, "no_speech_prob": 0.003479946171864867}, {"id": 500, "seek": 443380, "start": 4440.68, "end": 4446.76, "text": " and how each understanding each of them is essential to understanding the other ones.", "tokens": [50708, 293, 577, 1184, 3701, 1184, 295, 552, 307, 7115, 281, 3701, 264, 661, 2306, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1061596130502635, "compression_ratio": 1.535294117647059, "no_speech_prob": 0.003479946171864867}, {"id": 501, "seek": 443380, "start": 4447.64, "end": 4455.400000000001, "text": " But as I said, it's a really interesting topic, which gives rise to lots of discussions.", "tokens": [51056, 583, 382, 286, 848, 11, 309, 311, 257, 534, 1880, 4829, 11, 597, 2709, 6272, 281, 3195, 295, 11088, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1061596130502635, "compression_ratio": 1.535294117647059, "no_speech_prob": 0.003479946171864867}, {"id": 502, "seek": 445540, "start": 4455.5599999999995, "end": 4466.04, "text": " And I believe it's one of those topics that that's worth looking a bit more", "tokens": [50372, 400, 286, 1697, 309, 311, 472, 295, 729, 8378, 300, 300, 311, 3163, 1237, 257, 857, 544, 50896], "temperature": 0.0, "avg_logprob": -0.18234943952716764, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00793557520955801}, {"id": 503, "seek": 445540, "start": 4467.4, "end": 4470.92, "text": " looking into in some other literature as well.", "tokens": [50964, 1237, 666, 294, 512, 661, 10394, 382, 731, 13, 51140], "temperature": 0.0, "avg_logprob": -0.18234943952716764, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00793557520955801}, {"id": 504, "seek": 445540, "start": 4472.04, "end": 4480.12, "text": " Great. Well said. What a cliffhanger. Next, they go to a classic model family called Laka Volterra.", "tokens": [51196, 3769, 13, 1042, 848, 13, 708, 257, 22316, 71, 3176, 13, 3087, 11, 436, 352, 281, 257, 7230, 2316, 1605, 1219, 441, 7849, 8911, 391, 424, 13, 51600], "temperature": 0.0, "avg_logprob": -0.18234943952716764, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00793557520955801}, {"id": 505, "seek": 448012, "start": 4480.12, "end": 4486.92, "text": " These dynamics inherit from characterizations of predator prey dynamics in ecology. So it's kind", "tokens": [50364, 1981, 15679, 21389, 490, 2517, 14455, 295, 35377, 21107, 15679, 294, 39683, 13, 407, 309, 311, 733, 50704], "temperature": 0.0, "avg_logprob": -0.13647238067958667, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.006002902053296566}, {"id": 506, "seek": 448012, "start": 4486.92, "end": 4495.0, "text": " of a classical ecology model shown in figure 8.2. On the top, it's actually the ecosystem model. Plants,", "tokens": [50704, 295, 257, 13735, 39683, 2316, 4898, 294, 2573, 1649, 13, 17, 13, 1282, 264, 1192, 11, 309, 311, 767, 264, 11311, 2316, 13, 2149, 1719, 11, 51108], "temperature": 0.0, "avg_logprob": -0.13647238067958667, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.006002902053296566}, {"id": 507, "seek": 448012, "start": 4495.0, "end": 4502.04, "text": " herbivores and carnivores, which follow different kinds of oscillatory trends in continuous time.", "tokens": [51108, 22662, 592, 2706, 293, 23796, 592, 2706, 11, 597, 1524, 819, 3685, 295, 18225, 4745, 13892, 294, 10957, 565, 13, 51460], "temperature": 0.0, "avg_logprob": -0.13647238067958667, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.006002902053296566}, {"id": 508, "seek": 448012, "start": 4502.68, "end": 4509.16, "text": " And so that also has enabled it to be applied for other so-called winnerless competitions.", "tokens": [51492, 400, 370, 300, 611, 575, 15172, 309, 281, 312, 6456, 337, 661, 370, 12, 11880, 8507, 1832, 26185, 13, 51816], "temperature": 0.0, "avg_logprob": -0.13647238067958667, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.006002902053296566}, {"id": 509, "seek": 450916, "start": 4509.88, "end": 4515.24, "text": " And that relates to topics like neural Darwinism and also neural dynamics, where things have", "tokens": [50400, 400, 300, 16155, 281, 8378, 411, 18161, 30233, 1434, 293, 611, 18161, 15679, 11, 689, 721, 362, 50668], "temperature": 0.0, "avg_logprob": -0.08801589942559963, "compression_ratio": 1.55, "no_speech_prob": 0.0012447878252714872}, {"id": 510, "seek": 450916, "start": 4515.24, "end": 4521.8, "text": " kind of oscillatory relationships with each other, which are being modeled as a continuous time", "tokens": [50668, 733, 295, 18225, 4745, 6159, 365, 1184, 661, 11, 597, 366, 885, 37140, 382, 257, 10957, 565, 50996], "temperature": 0.0, "avg_logprob": -0.08801589942559963, "compression_ratio": 1.55, "no_speech_prob": 0.0012447878252714872}, {"id": 511, "seek": 450916, "start": 4521.8, "end": 4529.0, "text": " underlying process with a lot of measurement noise and discretization through space and time.", "tokens": [50996, 14217, 1399, 365, 257, 688, 295, 13160, 5658, 293, 25656, 2144, 807, 1901, 293, 565, 13, 51356], "temperature": 0.0, "avg_logprob": -0.08801589942559963, "compression_ratio": 1.55, "no_speech_prob": 0.0012447878252714872}, {"id": 512, "seek": 450916, "start": 4529.0, "end": 4533.88, "text": " Those are the kinds of algorithms that SPM explores more. And there's Laka Volterra and a", "tokens": [51356, 3950, 366, 264, 3685, 295, 14642, 300, 8420, 44, 45473, 544, 13, 400, 456, 311, 441, 7849, 8911, 391, 424, 293, 257, 51600], "temperature": 0.0, "avg_logprob": -0.08801589942559963, "compression_ratio": 1.55, "no_speech_prob": 0.0012447878252714872}, {"id": 513, "seek": 453388, "start": 4533.88, "end": 4541.96, "text": " lot of other dynamical systems theory in SPM. So active inference kind of adds action and more", "tokens": [50364, 688, 295, 661, 5999, 804, 3652, 5261, 294, 8420, 44, 13, 407, 4967, 38253, 733, 295, 10860, 3069, 293, 544, 50768], "temperature": 0.0, "avg_logprob": -0.06362643241882324, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.000588390976190567}, {"id": 514, "seek": 453388, "start": 4542.68, "end": 4550.28, "text": " to what was laid out from a pure dynamical systems theory in SPM. Here, it really is just", "tokens": [50804, 281, 437, 390, 9897, 484, 490, 257, 6075, 5999, 804, 3652, 5261, 294, 8420, 44, 13, 1692, 11, 309, 534, 307, 445, 51184], "temperature": 0.0, "avg_logprob": -0.06362643241882324, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.000588390976190567}, {"id": 515, "seek": 453388, "start": 4550.28, "end": 4555.56, "text": " showing the ecology example and how you can project. If you have three different species,", "tokens": [51184, 4099, 264, 39683, 1365, 293, 577, 291, 393, 1716, 13, 759, 291, 362, 1045, 819, 6172, 11, 51448], "temperature": 0.0, "avg_logprob": -0.06362643241882324, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.000588390976190567}, {"id": 516, "seek": 453388, "start": 4555.56, "end": 4562.76, "text": " you can think about that motion in a cube or tetrahedron. And then you could project onto", "tokens": [51448, 291, 393, 519, 466, 300, 5394, 294, 257, 13728, 420, 23319, 15688, 292, 2044, 13, 400, 550, 291, 727, 1716, 3911, 51808], "temperature": 0.0, "avg_logprob": -0.06362643241882324, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.000588390976190567}, {"id": 517, "seek": 456276, "start": 4563.400000000001, "end": 4569.8, "text": " kind of like looking at a lower dimensional manifold relating just two of the three species.", "tokens": [50396, 733, 295, 411, 1237, 412, 257, 3126, 18795, 47138, 23968, 445, 732, 295, 264, 1045, 6172, 13, 50716], "temperature": 0.0, "avg_logprob": -0.102656356493632, "compression_ratio": 1.3908045977011494, "no_speech_prob": 0.0006666465778835118}, {"id": 518, "seek": 456276, "start": 4570.52, "end": 4575.8, "text": " And that evinces this kind of oscillatory but also moving behavior.", "tokens": [50752, 400, 300, 1073, 259, 887, 341, 733, 295, 18225, 4745, 457, 611, 2684, 5223, 13, 51016], "temperature": 0.0, "avg_logprob": -0.102656356493632, "compression_ratio": 1.3908045977011494, "no_speech_prob": 0.0006666465778835118}, {"id": 519, "seek": 456276, "start": 4576.92, "end": 4583.400000000001, "text": " That gets connected in figure 8.3 to neurobiology. What would you say about this?", "tokens": [51072, 663, 2170, 4582, 294, 2573, 1649, 13, 18, 281, 16499, 5614, 1793, 13, 708, 576, 291, 584, 466, 341, 30, 51396], "temperature": 0.0, "avg_logprob": -0.102656356493632, "compression_ratio": 1.3908045977011494, "no_speech_prob": 0.0006666465778835118}, {"id": 520, "seek": 458340, "start": 4583.4, "end": 4595.5599999999995, "text": " Okay, so here in figure 8.3, we see some applications of Laka Volterra dynamics.", "tokens": [50364, 1033, 11, 370, 510, 294, 2573, 1649, 13, 18, 11, 321, 536, 512, 5821, 295, 441, 7849, 8911, 391, 424, 15679, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2231370770201391, "compression_ratio": 1.292857142857143, "no_speech_prob": 0.0033740049693733454}, {"id": 521, "seek": 458340, "start": 4596.36, "end": 4610.92, "text": " So the left column here represents what happens in, I mean, in eye blinking, eye blink conditioning.", "tokens": [51012, 407, 264, 1411, 7738, 510, 8855, 437, 2314, 294, 11, 286, 914, 11, 294, 3313, 45879, 11, 3313, 24667, 21901, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2231370770201391, "compression_ratio": 1.292857142857143, "no_speech_prob": 0.0033740049693733454}, {"id": 522, "seek": 461092, "start": 4611.72, "end": 4628.36, "text": " So, of course, here we need to account for, I mean, the expected states of the sequences of events", "tokens": [50404, 407, 11, 295, 1164, 11, 510, 321, 643, 281, 2696, 337, 11, 286, 914, 11, 264, 5176, 4368, 295, 264, 22978, 295, 3931, 51236], "temperature": 0.0, "avg_logprob": -0.12678314339030872, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.003320345189422369}, {"id": 523, "seek": 461092, "start": 4628.36, "end": 4638.4400000000005, "text": " that happens in the eye blinking. So the upper left figure shows the expectations", "tokens": [51236, 300, 2314, 294, 264, 3313, 45879, 13, 407, 264, 6597, 1411, 2573, 3110, 264, 9843, 51740], "temperature": 0.0, "avg_logprob": -0.12678314339030872, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.003320345189422369}, {"id": 524, "seek": 463844, "start": 4639.32, "end": 4649.5599999999995, "text": " in terms of time. And then the parallel right hand side equation, sorry, right hand side figures,", "tokens": [50408, 294, 2115, 295, 565, 13, 400, 550, 264, 8952, 558, 1011, 1252, 5367, 11, 2597, 11, 558, 1011, 1252, 9624, 11, 50920], "temperature": 0.0, "avg_logprob": -0.10366603306361608, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.005635939072817564}, {"id": 525, "seek": 463844, "start": 4650.28, "end": 4661.5599999999995, "text": " shows the Laka Volterra system that is applied in the handwriting situation. So as we can see,", "tokens": [50956, 3110, 264, 441, 7849, 8911, 391, 424, 1185, 300, 307, 6456, 294, 264, 39179, 2590, 13, 407, 382, 321, 393, 536, 11, 51520], "temperature": 0.0, "avg_logprob": -0.10366603306361608, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.005635939072817564}, {"id": 526, "seek": 466156, "start": 4661.56, "end": 4669.4800000000005, "text": " although the, I mean, mathematical technology is the same or at least the modeling technology is", "tokens": [50364, 4878, 264, 11, 286, 914, 11, 18894, 2899, 307, 264, 912, 420, 412, 1935, 264, 15983, 2899, 307, 50760], "temperature": 0.0, "avg_logprob": -0.1256160855293274, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0019250381737947464}, {"id": 527, "seek": 466156, "start": 4669.4800000000005, "end": 4684.200000000001, "text": " the same, the outcome of each situation varies drastically in two distinct neurobiological", "tokens": [50760, 264, 912, 11, 264, 9700, 295, 1184, 2590, 21716, 29673, 294, 732, 10644, 16499, 5614, 4383, 51496], "temperature": 0.0, "avg_logprob": -0.1256160855293274, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0019250381737947464}, {"id": 528, "seek": 468420, "start": 4684.28, "end": 4693.24, "text": " behavior, not neurobiological, but biological behavior. So, yeah, we can see how the same", "tokens": [50368, 5223, 11, 406, 16499, 5614, 4383, 11, 457, 13910, 5223, 13, 407, 11, 1338, 11, 321, 393, 536, 577, 264, 912, 50816], "temperature": 0.0, "avg_logprob": -0.15129743303571427, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.045264605432748795}, {"id": 529, "seek": 468420, "start": 4693.24, "end": 4705.5599999999995, "text": " modeling framework can give rise to different outcomes based on what parameters needs to be", "tokens": [50816, 15983, 8388, 393, 976, 6272, 281, 819, 10070, 2361, 322, 437, 9834, 2203, 281, 312, 51432], "temperature": 0.0, "avg_logprob": -0.15129743303571427, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.045264605432748795}, {"id": 530, "seek": 468420, "start": 4705.5599999999995, "end": 4714.12, "text": " optimized, what parameters are selected for modeling and so on. So I believe it's a quite", "tokens": [51432, 26941, 11, 437, 9834, 366, 8209, 337, 15983, 293, 370, 322, 13, 407, 286, 1697, 309, 311, 257, 1596, 51860], "temperature": 0.0, "avg_logprob": -0.15129743303571427, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.045264605432748795}, {"id": 531, "seek": 471412, "start": 4715.08, "end": 4725.72, "text": " interesting example to compare handwriting and the blinking together and how those can be compared", "tokens": [50412, 1880, 1365, 281, 6794, 39179, 293, 264, 45879, 1214, 293, 577, 729, 393, 312, 5347, 50944], "temperature": 0.0, "avg_logprob": -0.10377469929781827, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0005975498934276402}, {"id": 532, "seek": 471412, "start": 4725.72, "end": 4735.16, "text": " to each other using the Laka Volterra dynamics. Great, thank you. Box 8.2 gives a variant on the", "tokens": [50944, 281, 1184, 661, 1228, 264, 441, 7849, 8911, 391, 424, 15679, 13, 3769, 11, 1309, 291, 13, 15112, 1649, 13, 17, 2709, 257, 17501, 322, 264, 51416], "temperature": 0.0, "avg_logprob": -0.10377469929781827, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0005975498934276402}, {"id": 533, "seek": 471412, "start": 4735.16, "end": 4740.76, "text": " learning here presented with the formalism for continuous models, kind of a technical aside.", "tokens": [51416, 2539, 510, 8212, 365, 264, 9860, 1434, 337, 10957, 5245, 11, 733, 295, 257, 6191, 7359, 13, 51696], "temperature": 0.0, "avg_logprob": -0.10377469929781827, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0005975498934276402}, {"id": 534, "seek": 474076, "start": 4741.72, "end": 4749.56, "text": " Section 8.4 is about generalized synchrony. So figure 8.4 is going to visualize one of the", "tokens": [50412, 21804, 1649, 13, 19, 307, 466, 44498, 19331, 88, 13, 407, 2573, 1649, 13, 19, 307, 516, 281, 23273, 472, 295, 264, 50804], "temperature": 0.0, "avg_logprob": -0.14567401829887838, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0043306248262524605}, {"id": 535, "seek": 474076, "start": 4749.56, "end": 4754.52, "text": " classic dynamical systems, which is the Lorenz attractor. So what would you say about this", "tokens": [50804, 7230, 5999, 804, 3652, 11, 597, 307, 264, 37162, 89, 5049, 284, 13, 407, 437, 576, 291, 584, 466, 341, 51052], "temperature": 0.0, "avg_logprob": -0.14567401829887838, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0043306248262524605}, {"id": 536, "seek": 474076, "start": 4756.12, "end": 4765.88, "text": " figure? Okay, so this section is truly interesting because when one thinks of active inference,", "tokens": [51132, 2573, 30, 1033, 11, 370, 341, 3541, 307, 4908, 1880, 570, 562, 472, 7309, 295, 4967, 38253, 11, 51620], "temperature": 0.0, "avg_logprob": -0.14567401829887838, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0043306248262524605}, {"id": 537, "seek": 476588, "start": 4765.88, "end": 4775.0, "text": " probably the first situations that comes to mind is the situations in which we have quite well", "tokens": [50364, 1391, 264, 700, 6851, 300, 1487, 281, 1575, 307, 264, 6851, 294, 597, 321, 362, 1596, 731, 50820], "temperature": 0.0, "avg_logprob": -0.11613804607068078, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.002672189148142934}, {"id": 538, "seek": 476588, "start": 4775.0, "end": 4784.28, "text": " defined probability distributions for different parameters. But as we can see here in section", "tokens": [50820, 7642, 8482, 37870, 337, 819, 9834, 13, 583, 382, 321, 393, 536, 510, 294, 3541, 51284], "temperature": 0.0, "avg_logprob": -0.11613804607068078, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.002672189148142934}, {"id": 539, "seek": 476588, "start": 4784.28, "end": 4793.400000000001, "text": " 8.4, actually some of the formalism of active inference can be successfully used to characterize", "tokens": [51284, 1649, 13, 19, 11, 767, 512, 295, 264, 9860, 1434, 295, 4967, 38253, 393, 312, 10727, 1143, 281, 38463, 51740], "temperature": 0.0, "avg_logprob": -0.11613804607068078, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.002672189148142934}, {"id": 540, "seek": 479340, "start": 4793.4, "end": 4801.08, "text": " even chaotic systems and in particular the way in which two chaotic systems can be synchronized", "tokens": [50364, 754, 27013, 3652, 293, 294, 1729, 264, 636, 294, 597, 732, 27013, 3652, 393, 312, 19331, 1602, 50748], "temperature": 0.0, "avg_logprob": -0.17001833431962607, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0027973658870905638}, {"id": 541, "seek": 479340, "start": 4801.08, "end": 4813.16, "text": " with each other. So this is a classic example of a chaotic Lorenz system, and it draws upon", "tokens": [50748, 365, 1184, 661, 13, 407, 341, 307, 257, 7230, 1365, 295, 257, 27013, 37162, 89, 1185, 11, 293, 309, 20045, 3564, 51352], "temperature": 0.0, "avg_logprob": -0.17001833431962607, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0027973658870905638}, {"id": 542, "seek": 479340, "start": 4814.04, "end": 4822.92, "text": " from some of Professor Pristin's earlier work on birdsong synchrony. And as a side note, any", "tokens": [51396, 490, 512, 295, 8419, 2114, 468, 259, 311, 3071, 589, 322, 9009, 556, 19331, 88, 13, 400, 382, 257, 1252, 3637, 11, 604, 51840], "temperature": 0.0, "avg_logprob": -0.17001833431962607, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0027973658870905638}, {"id": 543, "seek": 482292, "start": 4823.0, "end": 4831.0, "text": " literature before 2016 is considered earlier history in active inference literature because", "tokens": [50368, 10394, 949, 6549, 307, 4888, 3071, 2503, 294, 4967, 38253, 10394, 570, 50768], "temperature": 0.0, "avg_logprob": -0.10066669696086161, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.004260677844285965}, {"id": 544, "seek": 482292, "start": 4831.0, "end": 4843.64, "text": " it evolves quite rapidly. So yeah, this kind of synchrony between two chaotic systems can be", "tokens": [50768, 309, 43737, 1596, 12910, 13, 407, 1338, 11, 341, 733, 295, 19331, 88, 1296, 732, 27013, 3652, 393, 312, 51400], "temperature": 0.0, "avg_logprob": -0.10066669696086161, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.004260677844285965}, {"id": 545, "seek": 484364, "start": 4844.6, "end": 4856.280000000001, "text": " interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of", "tokens": [50412, 26749, 382, 6530, 4467, 420, 754, 11, 718, 311, 584, 11, 257, 636, 281, 2316, 257, 733, 295, 28540, 5261, 295, 50996], "temperature": 0.0, "avg_logprob": -0.18810629844665527, "compression_ratio": 1.426356589147287, "no_speech_prob": 0.01880609057843685}, {"id": 546, "seek": 484364, "start": 4856.280000000001, "end": 4871.08, "text": " mind in the sense that how exactly can we understand or can two agents can trace each", "tokens": [50996, 1575, 294, 264, 2020, 300, 577, 2293, 393, 321, 1223, 420, 393, 732, 12554, 393, 13508, 1184, 51736], "temperature": 0.0, "avg_logprob": -0.18810629844665527, "compression_ratio": 1.426356589147287, "no_speech_prob": 0.01880609057843685}, {"id": 547, "seek": 487108, "start": 4871.24, "end": 4883.64, "text": " other's trajectories without, I mean, engaging in any direct exchange of observations between", "tokens": [50372, 661, 311, 18257, 2083, 1553, 11, 286, 914, 11, 11268, 294, 604, 2047, 7742, 295, 18163, 1296, 50992], "temperature": 0.0, "avg_logprob": -0.10049299270875993, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0027925935573875904}, {"id": 548, "seek": 487108, "start": 4883.64, "end": 4891.32, "text": " their internal and external states. So yeah, that's a really good example and I believe one of the", "tokens": [50992, 641, 6920, 293, 8320, 4368, 13, 407, 1338, 11, 300, 311, 257, 534, 665, 1365, 293, 286, 1697, 472, 295, 264, 51376], "temperature": 0.0, "avg_logprob": -0.10049299270875993, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0027925935573875904}, {"id": 549, "seek": 487108, "start": 4891.32, "end": 4898.5199999999995, "text": " most interesting examples of how active inference can even account for these kinds of behavior.", "tokens": [51376, 881, 1880, 5110, 295, 577, 4967, 38253, 393, 754, 2696, 337, 613, 3685, 295, 5223, 13, 51736], "temperature": 0.0, "avg_logprob": -0.10049299270875993, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0027925935573875904}, {"id": 550, "seek": 489852, "start": 4899.160000000001, "end": 4906.76, "text": " So the rest of the section goes into the details of how this kind of synchrony between", "tokens": [50396, 407, 264, 1472, 295, 264, 3541, 1709, 666, 264, 4365, 295, 577, 341, 733, 295, 19331, 88, 1296, 50776], "temperature": 0.0, "avg_logprob": -0.16208115078154064, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.005128654185682535}, {"id": 551, "seek": 489852, "start": 4908.76, "end": 4920.92, "text": " multi-scale Lorenz systems can happen and how can we formulate it mathematically in terms of", "tokens": [50876, 4825, 12, 20033, 37162, 89, 3652, 393, 1051, 293, 577, 393, 321, 47881, 309, 44003, 294, 2115, 295, 51484], "temperature": 0.0, "avg_logprob": -0.16208115078154064, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.005128654185682535}, {"id": 552, "seek": 492092, "start": 4920.92, "end": 4928.6, "text": " continuous time active inference. Awesome. And there's been more recent work on Mark", "tokens": [50364, 10957, 565, 4967, 38253, 13, 10391, 13, 400, 456, 311, 668, 544, 5162, 589, 322, 3934, 50748], "temperature": 0.0, "avg_logprob": -0.16476088303786057, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0197127778083086}, {"id": 553, "seek": 492092, "start": 4928.6, "end": 4936.28, "text": " Alblanket since stochastic chaos, but the bird example is a classic. 8.5 goes into hybrid discrete", "tokens": [50748, 967, 5199, 657, 302, 1670, 342, 8997, 2750, 14158, 11, 457, 264, 5255, 1365, 307, 257, 7230, 13, 1649, 13, 20, 1709, 666, 13051, 27706, 51132], "temperature": 0.0, "avg_logprob": -0.16476088303786057, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0197127778083086}, {"id": 554, "seek": 492092, "start": 4936.28, "end": 4943.08, "text": " and continuous models. So this could be kind of like an in-between chapter of seven and eight,", "tokens": [51132, 293, 10957, 5245, 13, 407, 341, 727, 312, 733, 295, 411, 364, 294, 12, 32387, 7187, 295, 3407, 293, 3180, 11, 51472], "temperature": 0.0, "avg_logprob": -0.16476088303786057, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0197127778083086}, {"id": 555, "seek": 492092, "start": 4943.08, "end": 4947.96, "text": " but now that we've been introduced to the pure form of discrete and the pure form of continuous", "tokens": [51472, 457, 586, 300, 321, 600, 668, 7268, 281, 264, 6075, 1254, 295, 27706, 293, 264, 6075, 1254, 295, 10957, 51716], "temperature": 0.0, "avg_logprob": -0.16476088303786057, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0197127778083086}, {"id": 556, "seek": 494796, "start": 4948.04, "end": 4954.84, "text": " models, here's shown that that composability extends to so-called hybrid models, where here", "tokens": [50368, 5245, 11, 510, 311, 4898, 300, 300, 10199, 2310, 26448, 281, 370, 12, 11880, 13051, 5245, 11, 689, 510, 50708], "temperature": 0.0, "avg_logprob": -0.13490316944737588, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.006589207798242569}, {"id": 557, "seek": 494796, "start": 4955.4, "end": 4963.8, "text": " the lower level visually is using the continuous time formalism and the higher level is describing", "tokens": [50736, 264, 3126, 1496, 19622, 307, 1228, 264, 10957, 565, 9860, 1434, 293, 264, 2946, 1496, 307, 16141, 51156], "temperature": 0.0, "avg_logprob": -0.13490316944737588, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.006589207798242569}, {"id": 558, "seek": 494796, "start": 4963.8, "end": 4971.08, "text": " the little line added here, the discrete time formalism. And this was the similar structure", "tokens": [51156, 264, 707, 1622, 3869, 510, 11, 264, 27706, 565, 9860, 1434, 13, 400, 341, 390, 264, 2531, 3877, 51520], "temperature": 0.0, "avg_logprob": -0.13490316944737588, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.006589207798242569}, {"id": 559, "seek": 497108, "start": 4971.72, "end": 4978.2, "text": " described by the authors of the paper, active inference does not contradict folk psychology,", "tokens": [50396, 7619, 538, 264, 16552, 295, 264, 3035, 11, 4967, 38253, 775, 406, 28900, 15748, 15105, 11, 50720], "temperature": 0.0, "avg_logprob": -0.12475705791164089, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.023686030879616737}, {"id": 560, "seek": 497108, "start": 4979.24, "end": 4984.76, "text": " where they describe this lower level as motor active inference, which was closely", "tokens": [50772, 689, 436, 6786, 341, 3126, 1496, 382, 5932, 4967, 38253, 11, 597, 390, 8185, 51048], "temperature": 0.0, "avg_logprob": -0.12475705791164089, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.023686030879616737}, {"id": 561, "seek": 497108, "start": 4984.76, "end": 4993.16, "text": " allied with the spinal arc reflex shown above. And then this higher level, they call decision", "tokens": [51048, 41969, 365, 264, 28022, 10346, 23802, 4898, 3673, 13, 400, 550, 341, 2946, 1496, 11, 436, 818, 3537, 51468], "temperature": 0.0, "avg_logprob": -0.12475705791164089, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.023686030879616737}, {"id": 562, "seek": 497108, "start": 4993.16, "end": 4996.84, "text": " active inference, because in that case, it was referring to a discrete decision.", "tokens": [51468, 4967, 38253, 11, 570, 294, 300, 1389, 11, 309, 390, 13761, 281, 257, 27706, 3537, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12475705791164089, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.023686030879616737}, {"id": 563, "seek": 499684, "start": 4997.4800000000005, "end": 5004.6, "text": " And so they used that kind of basic motif of continuous activity or continuous time modeling", "tokens": [50396, 400, 370, 436, 1143, 300, 733, 295, 3875, 39478, 295, 10957, 5191, 420, 10957, 565, 15983, 50752], "temperature": 0.0, "avg_logprob": -0.07556130669333717, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.000687819963786751}, {"id": 564, "seek": 499684, "start": 5005.24, "end": 5012.4400000000005, "text": " at the more peripheral aspects of a cognitive entity. And like Ali said, more discretization", "tokens": [50784, 412, 264, 544, 40235, 7270, 295, 257, 15605, 13977, 13, 400, 411, 12020, 848, 11, 544, 25656, 2144, 51144], "temperature": 0.0, "avg_logprob": -0.07556130669333717, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.000687819963786751}, {"id": 565, "seek": 499684, "start": 5013.88, "end": 5019.400000000001, "text": " and hybridization as well at higher levels of the cognitive modeling.", "tokens": [51216, 293, 13051, 2144, 382, 731, 412, 2946, 4358, 295, 264, 15605, 15983, 13, 51492], "temperature": 0.0, "avg_logprob": -0.07556130669333717, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.000687819963786751}, {"id": 566, "seek": 501940, "start": 5019.5599999999995, "end": 5028.2, "text": " And that type of an architecture here, instead of describing who wants the ice cream cone,", "tokens": [50372, 400, 300, 2010, 295, 364, 9482, 510, 11, 2602, 295, 16141, 567, 2738, 264, 4435, 4689, 19749, 11, 50804], "temperature": 0.0, "avg_logprob": -0.13142193688286674, "compression_ratio": 1.56, "no_speech_prob": 0.0028008082881569862}, {"id": 567, "seek": 501940, "start": 5028.2, "end": 5035.32, "text": " I believe, here, it's going to be a mixed or hybrid model that is going to call back the", "tokens": [50804, 286, 1697, 11, 510, 11, 309, 311, 516, 281, 312, 257, 7467, 420, 13051, 2316, 300, 307, 516, 281, 818, 646, 264, 51160], "temperature": 0.0, "avg_logprob": -0.13142193688286674, "compression_ratio": 1.56, "no_speech_prob": 0.0028008082881569862}, {"id": 568, "seek": 501940, "start": 5035.32, "end": 5047.24, "text": " isocade system, where there's a fixed point that is able to be moved as a set point. And then", "tokens": [51160, 307, 905, 762, 1185, 11, 689, 456, 311, 257, 6806, 935, 300, 307, 1075, 281, 312, 4259, 382, 257, 992, 935, 13, 400, 550, 51756], "temperature": 0.0, "avg_logprob": -0.13142193688286674, "compression_ratio": 1.56, "no_speech_prob": 0.0028008082881569862}, {"id": 569, "seek": 504724, "start": 5047.24, "end": 5053.8, "text": " there's a continuous time isocade that pursues the new fixed point. And so that's analogous to a new", "tokens": [50364, 456, 311, 257, 10957, 565, 307, 905, 762, 300, 7088, 1247, 264, 777, 6806, 935, 13, 400, 370, 300, 311, 16660, 563, 281, 257, 777, 50692], "temperature": 0.0, "avg_logprob": -0.1039494135046518, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0026315010618418455}, {"id": 570, "seek": 504724, "start": 5054.5199999999995, "end": 5060.599999999999, "text": " set point or fixed point being specified from the top down muscle command about a new location", "tokens": [50728, 992, 935, 420, 6806, 935, 885, 22206, 490, 264, 1192, 760, 8679, 5622, 466, 257, 777, 4914, 51032], "temperature": 0.0, "avg_logprob": -0.1039494135046518, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0026315010618418455}, {"id": 571, "seek": 504724, "start": 5060.599999999999, "end": 5067.719999999999, "text": " for a muscle, followed by movement towards it. This is a muscular activity that is realizing", "tokens": [51032, 337, 257, 8679, 11, 6263, 538, 3963, 3030, 309, 13, 639, 307, 257, 31641, 5191, 300, 307, 16734, 51388], "temperature": 0.0, "avg_logprob": -0.1039494135046518, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0026315010618418455}, {"id": 572, "seek": 504724, "start": 5067.719999999999, "end": 5074.679999999999, "text": " that but but not in the elbow coming away from the hot stove. This is about the ice caking to an", "tokens": [51388, 300, 457, 457, 406, 294, 264, 18507, 1348, 1314, 490, 264, 2368, 19263, 13, 639, 307, 466, 264, 4435, 269, 2456, 281, 364, 51736], "temperature": 0.0, "avg_logprob": -0.1039494135046518, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0026315010618418455}, {"id": 573, "seek": 507468, "start": 5074.68, "end": 5083.400000000001, "text": " epistemic foraging location specified by top down hierarchical systems. 8.3 describes little", "tokens": [50364, 2388, 468, 3438, 337, 3568, 4914, 22206, 538, 1192, 760, 35250, 804, 3652, 13, 1649, 13, 18, 15626, 707, 50800], "temperature": 0.0, "avg_logprob": -0.09129500043564949, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0006461879820562899}, {"id": 574, "seek": 507468, "start": 5083.400000000001, "end": 5091.8, "text": " technical aside on mixture of Gaussian Gaussian mixture models, kind of a technical modeling", "tokens": [50800, 6191, 7359, 322, 9925, 295, 39148, 39148, 9925, 5245, 11, 733, 295, 257, 6191, 15983, 51220], "temperature": 0.0, "avg_logprob": -0.09129500043564949, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0006461879820562899}, {"id": 575, "seek": 507468, "start": 5091.8, "end": 5099.16, "text": " note. And 8.6 closes. It says it's a huge topic and much has been left out. And so they list in", "tokens": [51220, 3637, 13, 400, 1649, 13, 21, 24157, 13, 467, 1619, 309, 311, 257, 2603, 4829, 293, 709, 575, 668, 1411, 484, 13, 400, 370, 436, 1329, 294, 51588], "temperature": 0.0, "avg_logprob": -0.09129500043564949, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0006461879820562899}, {"id": 576, "seek": 509916, "start": 5099.16, "end": 5105.48, "text": " table 8.1 key advances in continuous time models. And those areas are synthetic bird song,", "tokens": [50364, 3199, 1649, 13, 16, 2141, 25297, 294, 10957, 565, 5245, 13, 400, 729, 3179, 366, 23420, 5255, 2153, 11, 50680], "temperature": 0.0, "avg_logprob": -0.15482339966163206, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.1754578948020935}, {"id": 577, "seek": 509916, "start": 5106.28, "end": 5112.44, "text": " ocular motor delays, conditioned reflexes, smooth pursuit, eye movement, psychosis, illusions,", "tokens": [50720, 10409, 1040, 5932, 28610, 11, 35833, 23802, 279, 11, 5508, 23365, 11, 3313, 3963, 11, 4681, 8211, 11, 49836, 11, 51028], "temperature": 0.0, "avg_logprob": -0.15482339966163206, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.1754578948020935}, {"id": 578, "seek": 509916, "start": 5113.24, "end": 5121.16, "text": " saccades, action observation, attention, hybrid models and self organization. And that's chapter 8.", "tokens": [51068, 4899, 66, 2977, 11, 3069, 14816, 11, 3202, 11, 13051, 5245, 293, 2698, 4475, 13, 400, 300, 311, 7187, 1649, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15482339966163206, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.1754578948020935}, {"id": 579, "seek": 509916, "start": 5121.88, "end": 5127.88, "text": " What else would you say? And also what would you kind of lead someone to in the philosophical", "tokens": [51500, 708, 1646, 576, 291, 584, 30, 400, 611, 437, 576, 291, 733, 295, 1477, 1580, 281, 294, 264, 25066, 51800], "temperature": 0.0, "avg_logprob": -0.15482339966163206, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.1754578948020935}, {"id": 580, "seek": 512788, "start": 5127.88, "end": 5139.56, "text": " implications of 8 because it sounds kind of cool? Okay, so well, the case of continuous time active", "tokens": [50364, 16602, 295, 1649, 570, 309, 3263, 733, 295, 1627, 30, 1033, 11, 370, 731, 11, 264, 1389, 295, 10957, 565, 4967, 50948], "temperature": 0.0, "avg_logprob": -0.14080521193417636, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.0026651695370674133}, {"id": 581, "seek": 512788, "start": 5139.56, "end": 5152.68, "text": " inference. I think it leads to really interesting questions, both in terms of philosophical questions", "tokens": [50948, 38253, 13, 286, 519, 309, 6689, 281, 534, 1880, 1651, 11, 1293, 294, 2115, 295, 25066, 1651, 51604], "temperature": 0.0, "avg_logprob": -0.14080521193417636, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.0026651695370674133}, {"id": 582, "seek": 515268, "start": 5152.68, "end": 5162.92, "text": " and also more practical modeling questions about what parameters needs to be accounted for and so", "tokens": [50364, 293, 611, 544, 8496, 15983, 1651, 466, 437, 9834, 2203, 281, 312, 43138, 337, 293, 370, 50876], "temperature": 0.0, "avg_logprob": -0.10662079917060005, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.0027126965578645468}, {"id": 583, "seek": 515268, "start": 5162.92, "end": 5174.92, "text": " on. And as I said, I believe it's a more more interesting way of if not interesting, but but", "tokens": [50876, 322, 13, 400, 382, 286, 848, 11, 286, 1697, 309, 311, 257, 544, 544, 1880, 636, 295, 498, 406, 1880, 11, 457, 457, 51476], "temperature": 0.0, "avg_logprob": -0.10662079917060005, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.0027126965578645468}, {"id": 584, "seek": 517492, "start": 5174.92, "end": 5184.12, "text": " at least more involved way of doing active inference modeling. But one thing that one of the", "tokens": [50364, 412, 1935, 544, 3288, 636, 295, 884, 4967, 38253, 15983, 13, 583, 472, 551, 300, 472, 295, 264, 50824], "temperature": 0.0, "avg_logprob": -0.16077307767646257, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.004606415517628193}, {"id": 585, "seek": 517492, "start": 5184.12, "end": 5197.96, "text": " philosophical questions that Mao and I have explored in our paper is how the processes of I mean,", "tokens": [50824, 25066, 1651, 300, 38030, 293, 286, 362, 24016, 294, 527, 3035, 307, 577, 264, 7555, 295, 286, 914, 11, 51516], "temperature": 0.0, "avg_logprob": -0.16077307767646257, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.004606415517628193}, {"id": 586, "seek": 519796, "start": 5198.04, "end": 5210.04, "text": " ontological processes can philosophically described using FPP assertions in terms of their", "tokens": [50368, 6592, 4383, 7555, 393, 14529, 984, 7619, 1228, 479, 17755, 19810, 626, 294, 2115, 295, 641, 50968], "temperature": 0.0, "avg_logprob": -0.15066606120059364, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.11721781641244888}, {"id": 587, "seek": 519796, "start": 5210.04, "end": 5220.2, "text": " interaction with the environment in which they co constitute themselves. And we don't necessarily", "tokens": [50968, 9285, 365, 264, 2823, 294, 597, 436, 598, 41658, 2969, 13, 400, 321, 500, 380, 4725, 51476], "temperature": 0.0, "avg_logprob": -0.15066606120059364, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.11721781641244888}, {"id": 588, "seek": 522020, "start": 5221.16, "end": 5229.16, "text": " distinguish between between the internal and the external states. So one obvious example of this", "tokens": [50412, 20206, 1296, 1296, 264, 6920, 293, 264, 8320, 4368, 13, 407, 472, 6322, 1365, 295, 341, 50812], "temperature": 0.0, "avg_logprob": -0.1500846206164751, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.08619742095470428}, {"id": 589, "seek": 522020, "start": 5229.16, "end": 5237.0, "text": " is that generalized synchrony example that we saw in this chapter, in which we don't necessarily", "tokens": [50812, 307, 300, 44498, 19331, 88, 1365, 300, 321, 1866, 294, 341, 7187, 11, 294, 597, 321, 500, 380, 4725, 51204], "temperature": 0.0, "avg_logprob": -0.1500846206164751, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.08619742095470428}, {"id": 590, "seek": 522020, "start": 5237.0, "end": 5247.48, "text": " distinguish between which of the birds act as the agents and which one is the environment or the", "tokens": [51204, 20206, 1296, 597, 295, 264, 9009, 605, 382, 264, 12554, 293, 597, 472, 307, 264, 2823, 420, 264, 51728], "temperature": 0.0, "avg_logprob": -0.1500846206164751, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.08619742095470428}, {"id": 591, "seek": 524748, "start": 5247.48, "end": 5254.919999999999, "text": " vice versa. So these kinds of co constitution of the environment and the agent, which gives rise", "tokens": [50364, 11964, 25650, 13, 407, 613, 3685, 295, 598, 11937, 295, 264, 2823, 293, 264, 9461, 11, 597, 2709, 6272, 50736], "temperature": 0.0, "avg_logprob": -0.14492335812798862, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.004195752553641796}, {"id": 592, "seek": 524748, "start": 5254.919999999999, "end": 5263.0, "text": " to the partitioning of state space through a Markov blanket is one of the interesting", "tokens": [50736, 281, 264, 24808, 278, 295, 1785, 1901, 807, 257, 3934, 5179, 17907, 307, 472, 295, 264, 1880, 51140], "temperature": 0.0, "avg_logprob": -0.14492335812798862, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.004195752553641796}, {"id": 593, "seek": 524748, "start": 5265.24, "end": 5272.5199999999995, "text": " philosophical points that I think needs to be elaborated a bit more using", "tokens": [51252, 25066, 2793, 300, 286, 519, 2203, 281, 312, 16298, 770, 257, 857, 544, 1228, 51616], "temperature": 0.0, "avg_logprob": -0.14492335812798862, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.004195752553641796}, {"id": 594, "seek": 527252, "start": 5272.92, "end": 5282.6, "text": " some of the recent advances in philosophy, such as the tools that's been developed in new materialism", "tokens": [50384, 512, 295, 264, 5162, 25297, 294, 10675, 11, 1270, 382, 264, 3873, 300, 311, 668, 4743, 294, 777, 2527, 1434, 50868], "temperature": 0.0, "avg_logprob": -0.17999126116434733, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.012412439100444317}, {"id": 595, "seek": 527252, "start": 5283.64, "end": 5290.200000000001, "text": " school or some other philosophical approach approaches. But yeah, these kinds of", "tokens": [50920, 1395, 420, 512, 661, 25066, 3109, 11587, 13, 583, 1338, 11, 613, 3685, 295, 51248], "temperature": 0.0, "avg_logprob": -0.17999126116434733, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.012412439100444317}, {"id": 596, "seek": 527252, "start": 5291.56, "end": 5298.120000000001, "text": " what exactly gives rise gives rise to emergence, what is the ontological status of emergent", "tokens": [51316, 437, 2293, 2709, 6272, 2709, 6272, 281, 36211, 11, 437, 307, 264, 6592, 4383, 6558, 295, 4345, 6930, 51644], "temperature": 0.0, "avg_logprob": -0.17999126116434733, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.012412439100444317}, {"id": 597, "seek": 529812, "start": 5298.2, "end": 5306.44, "text": " properties and so on, are some of the burning questions for many philosophers today. And I", "tokens": [50368, 7221, 293, 370, 322, 11, 366, 512, 295, 264, 9488, 1651, 337, 867, 36839, 965, 13, 400, 286, 50780], "temperature": 0.0, "avg_logprob": -0.12639190500432795, "compression_ratio": 1.5, "no_speech_prob": 0.00818064995110035}, {"id": 598, "seek": 529812, "start": 5306.44, "end": 5312.599999999999, "text": " believe active inference and particularly continuous time active inference provides a clear,", "tokens": [50780, 1697, 4967, 38253, 293, 4098, 10957, 565, 4967, 38253, 6417, 257, 1850, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12639190500432795, "compression_ratio": 1.5, "no_speech_prob": 0.00818064995110035}, {"id": 599, "seek": 529812, "start": 5314.36, "end": 5325.48, "text": " precise mathematical formalism. Even if not to answer these questions, but at least", "tokens": [51176, 13600, 18894, 9860, 1434, 13, 2754, 498, 406, 281, 1867, 613, 1651, 11, 457, 412, 1935, 51732], "temperature": 0.0, "avg_logprob": -0.12639190500432795, "compression_ratio": 1.5, "no_speech_prob": 0.00818064995110035}, {"id": 600, "seek": 532548, "start": 5325.48, "end": 5336.28, "text": " to explore it in a more rigorous and practical way, and also practical and a tractable way.", "tokens": [50364, 281, 6839, 309, 294, 257, 544, 29882, 293, 8496, 636, 11, 293, 611, 8496, 293, 257, 24207, 712, 636, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16825529662045566, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.004252882674336433}, {"id": 601, "seek": 532548, "start": 5336.28, "end": 5347.5599999999995, "text": " So this is the area that I believe philosophy and science are beautifully intertwined into a coherent", "tokens": [50904, 407, 341, 307, 264, 1859, 300, 286, 1697, 10675, 293, 3497, 366, 16525, 44400, 2001, 666, 257, 36239, 51468], "temperature": 0.0, "avg_logprob": -0.16825529662045566, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.004252882674336433}, {"id": 602, "seek": 534756, "start": 5347.56, "end": 5354.68, "text": " view of not only the phenomenon of interest, but even about the whole world.", "tokens": [50364, 1910, 295, 406, 787, 264, 14029, 295, 1179, 11, 457, 754, 466, 264, 1379, 1002, 13, 50720], "temperature": 0.0, "avg_logprob": -0.11996620288793591, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.014947540126740932}, {"id": 603, "seek": 534756, "start": 5357.4800000000005, "end": 5368.76, "text": " Wow. Wow. Pretty cool. Yeah, a lot to say about that topic. After completing chapter seven and eight,", "tokens": [50860, 3153, 13, 3153, 13, 10693, 1627, 13, 865, 11, 257, 688, 281, 584, 466, 300, 4829, 13, 2381, 19472, 7187, 3407, 293, 3180, 11, 51424], "temperature": 0.0, "avg_logprob": -0.11996620288793591, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.014947540126740932}, {"id": 604, "seek": 534756, "start": 5369.56, "end": 5376.84, "text": " you've seen the kind of two major branches or two major motifs of just one kind of modeling.", "tokens": [51464, 291, 600, 1612, 264, 733, 295, 732, 2563, 14770, 420, 732, 2563, 2184, 18290, 295, 445, 472, 733, 295, 15983, 13, 51828], "temperature": 0.0, "avg_logprob": -0.11996620288793591, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.014947540126740932}, {"id": 605, "seek": 537684, "start": 5376.84, "end": 5383.24, "text": " But these kind of models have so many different forms that that's why it's such a hands on process", "tokens": [50364, 583, 613, 733, 295, 5245, 362, 370, 867, 819, 6422, 300, 300, 311, 983, 309, 311, 1270, 257, 2377, 322, 1399, 50684], "temperature": 0.0, "avg_logprob": -0.07241952419281006, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.003375687636435032}, {"id": 606, "seek": 537684, "start": 5383.24, "end": 5389.24, "text": " to specify the generative model in chapter six and fit it with data in chapter nine. Those are all", "tokens": [50684, 281, 16500, 264, 1337, 1166, 2316, 294, 7187, 2309, 293, 3318, 309, 365, 1412, 294, 7187, 4949, 13, 3950, 366, 439, 50984], "temperature": 0.0, "avg_logprob": -0.07241952419281006, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.003375687636435032}, {"id": 607, "seek": 537684, "start": 5389.24, "end": 5396.28, "text": " what's required. And that's kind of the last mile of where these discussions about general motifs", "tokens": [50984, 437, 311, 4739, 13, 400, 300, 311, 733, 295, 264, 1036, 12620, 295, 689, 613, 11088, 466, 2674, 2184, 18290, 51336], "temperature": 0.0, "avg_logprob": -0.07241952419281006, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.003375687636435032}, {"id": 608, "seek": 537684, "start": 5396.28, "end": 5402.84, "text": " gets you. But also playing with these pedagogical models can be really helpful, because it will", "tokens": [51336, 2170, 291, 13, 583, 611, 2433, 365, 613, 5670, 31599, 804, 5245, 393, 312, 534, 4961, 11, 570, 309, 486, 51664], "temperature": 0.0, "avg_logprob": -0.07241952419281006, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.003375687636435032}, {"id": 609, "seek": 540284, "start": 5402.84, "end": 5409.16, "text": " help you understand the basic patterns and relationships and start to see see different", "tokens": [50364, 854, 291, 1223, 264, 3875, 8294, 293, 6159, 293, 722, 281, 536, 536, 819, 50680], "temperature": 0.0, "avg_logprob": -0.09735298156738281, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0010004602372646332}, {"id": 610, "seek": 540284, "start": 5409.16, "end": 5416.84, "text": " patterns in the graphical models and know from there what levels of technical processes can be", "tokens": [50680, 8294, 294, 264, 35942, 5245, 293, 458, 490, 456, 437, 4358, 295, 6191, 7555, 393, 312, 51064], "temperature": 0.0, "avg_logprob": -0.09735298156738281, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0010004602372646332}, {"id": 611, "seek": 540284, "start": 5416.84, "end": 5431.96, "text": " kind of coarse grained over. All right. Okay, well, that's it. I guess next time we will do probably", "tokens": [51064, 733, 295, 39312, 1295, 2001, 670, 13, 1057, 558, 13, 1033, 11, 731, 11, 300, 311, 309, 13, 286, 2041, 958, 565, 321, 486, 360, 1391, 51820], "temperature": 0.0, "avg_logprob": -0.09735298156738281, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0010004602372646332}, {"id": 612, "seek": 543284, "start": 5432.84, "end": 5444.360000000001, "text": " nine, 10, and maybe something else. All right, I'll end it now. Thanks, Holly. Thank you.", "tokens": [50412, 4949, 11, 1266, 11, 293, 1310, 746, 1646, 13, 1057, 558, 11, 286, 603, 917, 309, 586, 13, 2561, 11, 10055, 13, 1044, 291, 13, 50940], "temperature": 0.0, "avg_logprob": -0.24317152159554617, "compression_ratio": 1.0229885057471264, "no_speech_prob": 0.0015716886846348643}], "language": "en"}