Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024
on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre
and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you
for introductions and to take us through the paper.
Oh, thank you.
And welcome everyone.
This is Active Inference Gas Stream 84.1 on July 22nd, 2024.
Thank you.
Okay, thank you guys. Go for it.
Hi, thanks for having us.
So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles?
I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute.
So, should we go through the paper briefly?
So, yeah, we wrote this paper together.
Actually, we started by writing a general audience piece that was published in the box online.
Tell us when was that?
A few months ago, I guess.
Yeah, I think it was close to a year ago, maybe.
I don't think it was published a year ago. I think it was published in 2024.
But yeah, we worked on it for a while.
Yeah, so this piece was doing, I guess, two things, that piece that we published in the box.
It was pushing back against what we call the all-or-nothing principle,
which we defined as the idea that either something has a mind or it doesn't.
So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.
And we argued that this was not the best framing to think of,
especially, and family, our systems that seem to have sophisticated behavioral capacities,
like large language models or AIC systems in general,
where we don't want to take various cognitive capacities of the package
and package them into this idea of a mind, where either you have the mind or you have them,
then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc.
planning, memory, theory of minds.
So, we thought, as a remedy to this kind of all-or-nothing approach,
we argued for what we call the divide and conquer strategy
when studying the cognitive capacities of these systems,
which involved looking at these capacities on a piecemeal basis, case-by-case,
with an open-minded empirical approach.
Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds.
Yeah, I mean, we made one point in there about why it is that people feel so torn about
reactions to large language models, and we said a little bit about the psychology of essentialism,
which is the idea that we naturally categorize especially living things
with respect to a presumed essence. So, we gave an example of an oak tree, I think,
and we said that what people tend to think as they grow up and learn about the natural world is that
an oak tree remains an oak tree regardless of changes to its observable properties,
and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has,
and there's some experimental psychology and developmental psychology showing that we
have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative
explanation for why the literature on large language models is so torn, and some people
are quite dismissive, and other people think that it's a step away from AGI. It's that if you
feel like you've got to put large language models into one of two boxes,
the box that has the essence of mindedness for the box that lacks it, then you will be forced
either to say it doesn't have what it takes to do any of the things that we associate with
having a mind such as reasoning, or it has the essential characteristics of mindedness,
and therefore we should expect it to have all of the other properties we associate with
mindedness as well such as consciousness or understanding or whatever.
Right, I think it's worth emphasizing as you did that the background motivation for starting to
write on this general piece in the first place is indeed that the general discourse on AI systems
and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so
you have one ahead people and one ahead people arguing that these systems are no more than
so-called stochastic parrots that are haphazardly stitching together samples from the training data
and regurgitating them, or that they are no smarter than toaster or that they only do next
stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form
of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum,
on the other end of the spectrum you have people arguing that the systems are haphazardly
jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to
parrots, it's literally the title of a paper by Microsoft on 24,
and many people hyping up the capacity of the systems in a way that might seem very speculative
and untethered from actual empirical results, so there is this huge gap between these two positions
and there's going to be a very rich and complex and nuanced middle ground that is underexplored,
or perhaps I think we did make that point, if not explicitly in the published piece
and in some draft, that there's something reassuring about being able to make definitive
claims about what these systems are and what they do, so either they're re-unsophisticated or they're
very much like us and either of these claims kind of meet somewhere in an way in saying that we
have a clear idea of what the systems do and what they are, and I think it's a little more
epistemic and comfortable to say we have to study them empirically and find out what they
can or cannot do and why and what are the underlying mechanisms, and we simply don't know
a priori just by looking at the architecture, the learning objective, the training data,
these sources of evidence are insufficient to make the definitive claims about what these systems
are capable of, so I think that's part of the big part of the motivation and that fits into that
more academic paper as well. Yeah, one other small side note which we don't make
in the paper but I think might be relevant, especially for people working in philosophy,
LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which
is fitting, not only because they're so new and different but also because they are artifacts,
right, they're things that humans have constructed and engineered and
we don't have a thorough understanding of how they work, I mean mechanistic interpretability and
various behavioral research is helping us improve our understanding but on the whole
our understanding is not nearly as deep as hopefully it one day will be
and this is by itself a really strange situation to be in that we've constructed an artifact that
we only partially understand in the in the past artificial intelligence was seen as a way of
constructing something like an epistemic assistant, right, something that will
help us but not something that will kind of alienate us from the process of coming to know
about the world so I think there's an extra layer of discomfort built into thinking about
large language models and that may also play into the the divisiveness of debates about what they
can do and just to add to that I guess we should be clear that this does not entail in any way that
we think the systems are so completely alien and beyond the reach of our current understanding that
anything goes and that they could very well be you know have like human-like intelligence or
superhuman intelligence and we simply cannot say whether or not they do or because that's
sometimes what you see in some outfits where people frame these systems as noble alien forms of
intelligence that we have created but do not understand our control and that is you know
as a slippery slope that leads some people to then claim that they have all these quite magical
abilities and that's not all what we want to say here and in fact we want to resist
yeah yeah yeah so if we we think that that's just as much of a cup out as
completely dismissing a priori what these systems might be capable of without doing the
work of looking into the capacities with behavioral and mechanistic studies so
we very much want to resist both extremes of the spectrum if that makes sense
okay so now should we move towards the content of the current paper
sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism
and anthropocentrism and then you can take the next step so everyone is aware of the problem of
anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting
human qualities onto something non-human and it's quite easy to especially when you're having a
productive successful exchange with a large language model it's easy to slip into this
interpretive mode where you reason about the responses of the large language model
as if they were coming from an agent just like you and maybe that's a useful thing to do in some
circumstances but from a theoretical perspective it's certainly a mistake because large language
model is radically unlike you know a human agent in all sorts of ways but that's only one form of
sort of human-centric bias the other one is anthropocentrism or what we call in that box
article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that
the human way of solving problems is the gold standard of solving problems generally so that
to the extent that a system solves a problem in a way that diverges from the human strategy
it's only using a trick or a faux solution it's it's not using a deep general rational strategy
and in the debate about what large language models can do we think that the anthropomorphic
bias is pretty well recognized and the anthropocentric bias is not so well recognized
and so part of this paper is is or the main idea behind the paper is to present a systematic
analysis of anthropocentric bias how it comes about and how to push back against it
right and we we want to be very clear and hopefully we're playing the paper that
the reason why we focus on anthropocentric bias here is just because it is I think as
Charles mentioned less discussed and less recognized or some forms of it are less
recognized and we make we propose this new taxonomy but it's not at all suggest that it's
more problematic or more important than the anthropomorphic bias that's well discussed in
the literature of the anthropomorphic biases so in other words this is not you know to frame things
in in this slightly problematic dichotomous way of thinking that's common in the discourse
on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it
is pushing back against a certain form of dismissal of anthropocentric biases that
only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a
little bit already with the first one we make here in the paper about a performance competence
distinction which is a nice way to bring about both anthropomorphism and anthropocentricism
regarding LLM so this distinction is a very classic distinction in linguistics and cognitive
science and it has already been applied to AI systems in the neural networks productively
like Charles Farson so there's nothing really new here but the distinction comes from
Noam Chomsky originally and the idea is that performance pertains to the external behavior
of a system in a particular domain and competence is the kind of setup underlying knowledge and
computations or mechanisms that enable the system to achieve that behavior
and a familiar observation in linguistics and cognitive science is that there is a double
dissolution between performance and competence so if you take for example language I might during
this very podcast make some grammatical mistakes or some other mistakes in fact I've already
misspoke in a few times I think and repeated myself so I made performance errors but this
does not entail necessarily hopefully that I'm an incompetent language user and that I don't have
in the language the competence so that's a well recognized dissociation you can be competent
and yet make some errors and the reason for that is that there might be some additional factors
that are unrelated to the underlying competence that might impede on my performance so for example
I might be distracted when I speak or there might be other effects on my speaking performance that
don't actually originate from a lack of competence but just impede on the idealized expression
external manifestation of my competence and this is why I misspeak but it's also why recognize
that you can have good performance without competence so we give here the example of a
student cheating on a test or memorizing test answers by brute forcing the test to slightly more
I guess gray area but at least in the cheating case a student can ace a test without being
competent at what the test is actually testing for and it's also well acknowledged in cognitive
science that there can be instances like this throughout you know like that can be manifest
certain experimental settings where the test subject is right for the wrong reasons as it were
namely it's doing well it's exhibiting good performance never realize the underlying reason
for the performance is that there was some perhaps some curiosity that the experimenters
all the scientists haven't thought about that could account for his good performance but
but doesn't actually amount to whatever competence they were setting out to test
so we we start by saying well this is what we could nice because the mistakes this is the
association that is like supplied to humans across the board you can have performance
without competence good performance without competence and you can have that performance despite
competence now when it comes to other lamps the point we make just going to scroll as we go
we have some figures to show later but the point we make is that generally people stress the
dissociation apply the distinction to other lamps the stress decision only in one direction
unlike in the case of human where it's bi-directional and so what people do generally is to say well
other lamps famously you know if you look at the gpt4 technical report and and vice other
any any any report about a new state of the art alone they are getting really really good at a
number of tests and about benchmarks and even human exam examinations human exams the bar exam
medical exams etc so they can get a really good performance test that we tend to think are really
difficult tests that one can only pass at least a human could only pass if they have
a really significant nonchalant amount of competency in particular the main
and the point that is often made when it comes to other lamps is be careful slow it on and try to
find out why the model is doing well on that test because there are various reasons why it could do
well that do not actually indicate that the model has the underlying competence that the test was
designed for when it comes to humans so one big concern for example is data contamination
where very large language models train on internet scale data can easily be trained on some test
items from common benchmarks that leak into the training data such that they can then do really
well on the on the on the benchmark just because they've essentially memorized test items and there
are other more subtle more subtle reasons why performance could be very good for the wrong
reasons so that's very well recognized and a lot of the people who push back against anthropomorphic
bias when it gets to other lamps make that point be careful do not take on another anthropomorphic
attitude to the systems the reason why they do well is not because they have human-like intelligence
or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant
reasons that account for the good performance now when it comes to the other dissociation of the
other dissociation of the other direction there are people are very reluctant to apply to a lamps
and we think it's because essentially people think in a human case you can make sense of the
idea that the human could do badly on a test or corporate could perform badly on a task
and yet have the competence that you're trying to test but there might be some auxiliary factors
such as working memory limitations attention deficits etc that could impede on the performance
but in the language model I think what we argue in the paper is that
people don't a lot of people don't think that there is an analogous
mechanism at play where there could be some kind of auxiliary factor that impede on performance
performance is what you get what you see is what you get and so the performance is a direct
manifestation of what the system is computing and if you have performance errors
that can only be explained by the lack of competence because there is no additional
independent factor or module that could impede on the performance
sources of interference you might say yeah so yeah I mean I'll
pass it over to you Charles I just wanted to set up this distinction yeah yeah
right so I mean if you think about a traditional computer program
well at least if you think about a simple computer program it's odd to think of it as
some sort of complex systems where it's a complex system where one part of it could sort of
interfere with the workings of another part of it but one of the points we want to make is that
something like that is a realistic possibility with large language models
um okay but I suppose the next part of the paper goes into a taxonomy of
anthropocentric bias and the first sort of overarching point is the distinction between
type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance
failures designed to measure competence always indicates that it lacks
that competence and so we before we so we'll say something about three different kinds of
type one anthropocentric bias but first a background point which is that
um whenever we think it's possible to give a mechanistic explanation of some
complicated phenomenon we always have to foreground some factors some variables
uh and background others and um the properties that we push into the background
nevertheless matter we're still making assumptions about the nature of those properties
when we try to articulate what's going on with the other properties that we're paying more attention
to and if assumptions about those properties in the background turn out to be wrong
then those mistakes will corrupt our explanation that attends only to the foregrounded factors
so that's a little bit abstract let me just give you a simple example um in comparative cognition
one famous uh behavioral experiment is the mirror test for self-recognition
so the question is roughly do non-human animals have something like a concept of self
and the strategy is to put some sort of mark on their body in original experiments it was
a red dot on the forehead of maybe a monkey and or a bird or whatever and then
you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark
and if it does make an attempt to get rid of the mark that shows that it recognizes that the
image in the mirror is an image of itself and otherwise not um so that that's a cool way to
get a really difficult and abstract question about the mind of a non-human animal but it presumes
or it assumes that animals will care about the fact that they have a red dot on their
forehead that they will be bothered by that and be motivated to get rid of it and if that
assumption is wrong then they might fail the mirror test for self-recognition for reasons that
have little to do with the presence or absence of a capacity for self-recognition until something
similar to that is going on we say in large language models so the first example that we give is to
do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral
task um there will be demands associated with that task that are not directly related to the
capacity that you're trying to test so to take the most obvious example that I can think of
if you give someone a written test they have to be able to write they have to you know have a hand
and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't
write then um their failure to fill out the test wouldn't tell you anything about their uh you know
academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral
tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right
away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic
but what they do is they give a large language model um the following sort of question uh this
is a question it's for a grammaticality judgment so you can see on the image there which sentence
is better in english number one every child is studied number two every child have studied
answer with one or two and it gives the wrong output but then
you can also simply look at the probabilities assigned to each of those
sentences within the model and uh figure out directly whether the model thinks that input
A is more likely than input B and it turns out that on a wide variety of questions of this kind
the direct comparison uh does or the large language models perform better with the direct
comparison than they do with the more complex demand for uh metalinguistic judgment so the
fact that the model has to process the numbering of the uh options and then answer in terms of a
number uh is a subtle but nevertheless um important additional variable in the experiment
and that can influence the model's capacity to get the answer right
Rafael do you want to add to that?
No I think that well maybe we can mention briefly the other example that we discuss which is from
this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes
the example interesting and even more problematic in terms of comparative psychology which is
um one way in which auxiliary task demands can be ignored or disregarded or overlooked
is when uh you are doing a direct comparison between humans and other epsilon tasks and
the experimental conditions are mismatched in such a way that the task as you set up
impose the stronger demands stronger task and auxiliary demands on the LOM than those on human
subjects and that's something that that can happen quite often and so there is this this
interesting example from a couple of papers originally published by black reds and colleagues
from standard hands group where they looked at um the ability of language models to handle their
recursion um looking at center embedded closes uh how such closes might um
for example when you had a prepositional phrase within the subject of the sentence and the verb
might throw up either humans or LOMs into agreeing the the verb in the wrong way
so giving the wrong number to the verb for example the keys that the man put on the table
here it should be R because keys is plural but because you have close in the middle
and if you add more closes like this that are embedded in the middle people and LOMs can get
confused and and predict that the verb should be is for example so um they tested this on humans
and LOMs and found that on the more complex examples involving complex more complex constructions or
recursion humans were doing decently well but LOMs performance was collapsing compared to the
single examples and I've heard an opinion from DeepMind um looked into that and realized that
the experimental conditions were mismatched so the humans at this very common in cognitive
science experiments were getting some training before they completed the test items to just get
familiarized with the task so they were given some examples of the task um harsh condition
and the LLabs were just prompted zero shots as um people usually put it so just um without any
example just point blank and Andrew found out that if you he he he replicated the experiments but
I did some proper matched testing conditions for the LL so adding some examples of the task in the
prompt when it's known as future counting and with that he found that performance was equivalent
in fact the LLM that he tested was slightly better on the more complex constructions than humans
so when you match the test conditions here you actually match also
at least you it's it's not it's not automatic but you you you can match the test demands I mean
it could be that there are reasons why the various experimental conditions would result in different
demands for humans and others but you're still in this case even on the playing ground playing field
in such a way that you don't find the behavioral discrepancy that you found initially anymore
um yeah um good so shall we continue to the next
section um so another uh another way that auxiliary or another type of auxiliary task
demand is input independent computational limitations um and here we're thinking of a
few papers that show that the number of forward passes that the transformer can make
influences its ability to find the right spot and parameter space so neural networks are
function approximators but their
um their ability to approximate a function can be eliminated uh can be limited by the
the the number of computations it's allowed to perform and um the
uh sort of crucial feature of uh transformers in this example is that
um the number of operations that determines the next token is limited by the number of
tokens that it's seen so far and it turns out that if you train a transformer with
additional meaningless tokens like pause tokens like the word pause you can increase its accuracy
across a range of of question types um and yeah this is
this counts as an auxiliary task demand in our view because um it's doing something roughly
analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry
yeah um it's it's doing something like giving the model uh time to think and um
yeah so so you might think that the absence of that additional inference time is a factor
that is not directly not conceptually related to its capacity to answer
uh a question like the one on the screen um you know a simple
earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy
is is a nice one time to think because if you if you tested a human on even a simple mathematical
questions or any any task really and just ask them you know tell them they have like one second
to just blow it out answer performance would probably be pretty bad um and you can think of
asking an LLN to answer a question point blank as very very loosely analogous to that and obviously
this is an analogy and there are very important differences here but I think it's a helpful
heuristic to think about what is um what is going on roughly here and then we uh in in both cases
the system the human or the LLN does not get the chance to perform the necessary computations to
derive the correct answer and so yeah what we talk about in the paper is that you have these
experimental works during that if you ask a question to a language model um the amount of
tokens it's a it's it's that allows to generate before providing the answer
makes a difference to how accurate it is so if it uh it just generates a few tokens then have to
give an answer or even if you just have to give the answer point blank with the very first token
it's going to be less accurate that if you give it a chance to generate a number of tokens before
giving the answer so the usual way in which this is understood is that when you ask when you
you you allow the LLN to generate a number of tokens before giving the answer or you even
prompted to do so you say things step by step for example um that's not as chain of thought
prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace
or what looks outwardly externally like a reasoning trace in the output before giving an answer
and we know that chain of thought prompting increases performance accuracy
um but what was found by a couple of papers um is that the mechanistic influence of this process
is not entirely due to the nature of the tokens that are generated in this reasoning trace
in other words it's not just that the LLN has to generate the right tokens corresponding to
different steps of reasoning before giving an answer in fact the very back that you allow the
LLN to just generate tokens any token before giving an answer from a mechanistic perspective
affords the system to perform additional computations that can complete the computational
circuit that otherwise would get a chance to be completed and to derive the correct answer
so as Charles mentioned you can have you can set up an experiment when you have the LLN just
generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the
answer and the the more dots you allow before the token gives the answer the greater the
expressivity of the system and the more um the more kinds of programs problems you can answer
and so as Charles mentioned every time an LLN is generated in a token the LLN is performing
one forward pass and so the more tokens it's generating the more forward passes it's doing
and one way to think about what's going on here as well is that having these additional forward
passes where you you feedback the whole input sequence plus the previously generated tokens
to do the system to generate the next is also a way to introduce a form of recurrence in
transformers that are not in terms of the architecture of recurrent networks so that
increases the expressivity and you know in complexity of your unique terms and yeah there is
there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation
that again we think is very very loosely analogous to prompting a human to answer a
point on the question without thinking so that's the next sense an auxiliary factor because
if you give the LLN the opportunity to generate enough tokens it might have the competence
to solve a task but you might not see that otherwise and you might get performance errors
but you do think it's incompetent
all right yeah okay um so the the third type of type one anthropocentric
bias that we talk about is mechanistic interference and so this comes from
the mechanistic interpretability work and the basic idea is that because
large language models are capable of in-context learning they can learn different strategies
for solving a different particular type of problem and the strategy that they
implement at a given time can be different so you can talk about this in terms of
virtual circuits that are formed inside the language model and there's some interesting work from
nil nanda and others showing that in some circumstances these two circuits can compete
with one another so at a certain level of uh or after a certain amount of training
you get one circuit operative after a bit more training you have two different circuits
but they're uh the first circuit is still sort of dominant and then after additional
training the model
converges on on the second circuit and the first one slowly gets sort of
it sort of ceases to influence the internal operations of the model and it's only once
you reach that third phase at which the
the benefits of the second circuit with respect to the first become visible
so you can you can show using decoding work that the second circuit is there
uh before you can show behaviorally that the second circuit
yields better performance accuracy on on the task so um I suppose there's a combination
of two ideas here one is that um there are different strategies a model can
implement for solving a problem we can detect those strategies internally using decoding methods
um so three ideas and then the third is uh
a good strategy can be
uh present in some sense in the model um before it has had the chance to influence
behavior um and and so this is just another way that the link between
performance and competence is shown to be more complicated than I might seem at first
graph
and um yeah just to to clarify one thing so the circuits are just um
you know ways to think about the causal structure of a neural network and
there's essentially computational subgraphs of the network that have a specific function
you can think of a circuit as implementing a particular algorithm or set of computations
um it's a part of what people are interested in in this mechanistic interpretability literature
that we build on people like Neal Mandatrisola and others is reverse engineering the circuit
steps in deep neural networks and large language models um peer to implement certain well-defined
algorithms in some cases at least um and the emerging picture that we build on here is that
there is a lot of redundancy built into neural networks as they learn to perform a task
optimized as a function that in many cases translates into redundant circuits that relate
to the same tasks the same kinds of um the same kinds that we put out with my things and uh for
these circuits might be somewhat identical circuits that are just redundant or they might be
different algorithms just to do to do a similar thing and different strategies to solve some
problem I swear maybe one will be a bit more approximative and the other one a bit more exact
more computationally intensive so that's where you can have some interference um where one uh
or at least some competition where once your kid takes over another and um such that the other
becomes kind of you know it's there it's latent in the system but you don't get a chance to influence
behavior on a specific input so you can get a performance error for that reason and these can
combine with the other things we mentioned here so things like task demands the first thing we
discussed as well as the number of tokens you generate both of these things could cause a
particular circuit to take over another um so it's it's we can think of this holistically as
perhaps if you ask a question point blank to a model without letting it generate
bunch of tokens before giving an answer then one particular approximate circuit might take over
that gives the wrong answer if you let it generate more tokens then another more exact circuit might
be given a chance to um it could influence the output using the right answer and similarly with
task demands uh strong task demands might uh in some cases um impede on the uh triggering of a
certain circuits that would otherwise have given the right answer um so that could be the case
perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts
might actually prime the word circuits to solve the task about complex recursive cases in the right
way um so yeah these are the three main auxiliary factors that relates to what we call type one
anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should
be quick on type two do you want to uh pick fix things after child yeah so type one uh deals
with cases where performance of the model is um weak compared to humans so the model doesn't do
so well um and then type two is when the model does do well but nevertheless is different in some
respect from the uh performance profile of the human or we have some evidence to think that the
model uses a different strategy than humans typically use and um the idea is that um even
once you hold performance equivalent or average performance equivalent um you know making a different
pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability
work any deviance from the human strategy is evidence of fragility or only a trick solution
um and uh this point is a bit more philosophical i suppose but the um idea is that
the human strategy for solving a problem um isn't necessarily the most general strategy
for solving a problem and uh what matters is whether the strategy that is pursued by the model
is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human
strategy um yeah and we end the we end the paper by considering an objection um which is um
why um like given that um in humans we study cognition largely through language
um and given that elements are trained on language or um linguistic outputs from
humans um isn't it appropriate after all to treat um human cognition as the correct or
appropriate the obstacle to study elements and we to that we answer that it depends how we think
of that dialectic um so we acknowledge that there is there is no
really other option than to start or investigation of cognitive abilities in algorithms
but with reference to human cognitive abilities using human cognitive abilities as some kind of
realistic or reference points things like theory of mind memory metacognition
various forms of reasoning etc that are familiar to us because we humans have them
and this is the same thing by the way in animal cognition for example or in developmental psychology
where in any comparative psychology setup um the reference point for what concepts
psychological capacities initially at least um is necessarily tied up with our conception of what
we human we humans have in our repertoire of cognitive capacities but we emphasize that this
is only the starting point so here we've over written from uh the philosopher Ali Boyle who
calls this investigative kinds investigative cognitive kinds we start with a cognitive
kind like memory or metacognition episodic memory metacognition theory of mind um as
as an investigative starting point the starting point of the investigation and then we have
that we we can try to start operationalizing operationalizing this concept this kind this
cognitive capacity in an experiments testing the algorithms on it with an open
mandated empirical approach and then based on the results from that each relatively refine
the capacities that we are the capacity that we're targeting or the definition of the capacity
targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions
that we have such that as the experimental um project runs a course or as as we make as we
we get more results and refine our concepts we may end up with um something that no longer
looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS
but ends up looking like looking for something that some capacity that is that shares some similarity
with human like human episodic memory but is different in other respects um and so we can
gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric
so we emphasize that it is kind of due to feedback look here that's that's that's premised on open
minded empirical investigation that doesn't settle this question a priori but has to start
as a necessary starting point with the the reference to human cognition
i don't know if you want to add to that joss
um no i think that's pretty good maybe we should uh move on to questions
and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't
move it awesome okay
yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat
but first i just wanted to read a short quote from the 2022 active inference textbook they wrote
um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote
too much effort to modeling isolated subsystems e.g perception language understanding whose
boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive
creature perhaps a simple one and an environmental niche for it to cope with
so it's interesting about the approach that you're taking this is kind of a simple synthetic
iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena
because there is something and and so i saw in the presentation paper kind of this call for like
deliberate investigation rather than just chopping up the iguana a priori with a framework that
that applies to humans or that centers humans or or that just uh soothes the epistemic challenge
that's presented okay okay first question from dave he wrote
have you looked at daniel denitz's distinction between competence without awareness and
competence with awareness he expands on this in the 2023 from bacteria to Bach and back
i find this much more valuable than chomsky's highly problematic performance without competence
a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this
competency uh well maybe i'll let you think uh that one is i can trust because you're
you're maybe more within it than i am but i'll just say um awareness is a very polysemous
term like many terms in philosophy of minds but partially this one more than many others i think
so um it can mean a lot of different things in all of context here we don't focus on things
like consciousness because i think we probably both agree that it's a less tractable uh maybe
empirical problem to try to assess the presence or absence of consciousness in language models
even though many people are interested in that we think that we have more hope of making progress
in the near term with more well-defined cognitive capacities or cognitive functions and things that
relate to forms of certain forms of reasoning and viable binding etc um so we our framework and
principle would apply to things like consciousness or as you put awareness generally speaking but
we don't really focus on that for examples the other quick thing i'll just mention is that i
seem to remember that the phrase from then but again i'm not a then scholar was competence
without comprehension um which seems a little different from competence without awareness
perhaps depending on how you think of comprehension um and yeah i think that does i think it is a
very interesting phrase that it does um in fact i had this project that's unpublished with
chris dolega who i think you had on the podcast as well um on semantic competence in language models
where we use that phrase um to kind of avoid taking its stance on the kind of messy
muddy question of whether hella let's understand language which builds in all sorts of assumptions
including about consciousness actually for some people like chancel um and we focus on
the more restricted notion of competence and i think our paper here also has that property that
would if we have originalized competence we end up operationalizing competence in terms of
the sets of knowledge of the mechanism and mechanisms that enable a system to generalize
well in a given domain basically and in a way that's a supposed evolutionary compared to
some more expensive understandings of competence that
we need to comprehension or understanding more well but i'll let you take that one charles
no yeah that was that was good um the phrase you know the distinction that denadra's is between
competence with comprehension and without and i think um competence with comprehension is the
ability not just to pursue a strategy that's successful for solving a problem but to um
articulate the strategy in such a way that you could teach it for example and um humans only
sometimes have competence with comprehension we have many competences that lack comprehension
right um you know when we learn to walk for example um we have an amazing competence that we
still can't quite translate into robotics because we don't fully understand how it works
and when it comes to our language models i think we should
not expect uh comprehension i mean they have a an amazing suite of competences if you thought that
they also had comprehension then i suppose you would think like well if you want to understand
how a large language model works you can just ask it but that's that's a bad strategy nobody nobody
about um how a large language model works so so they're on the um competence without
comprehension side of things um and in order to figure out what in order to figure out what
the mechanisms are that enable its competencies we have to pursue strategies that are broadly
similar to the strategies we use in you know cognitive psychology or cognitive linguistics
um and you know we have to run experiments so i think that that's all very compatible with
Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was
quite influential to me and we actually wrote a commentary together which pushes back a little
bit on a simple understanding of this distinction so we were looking at um the evolution of
metacognition and basically what we argue is that um given the gradualism of evolution there
must have been something in between base level cognition and metacognition so we shouldn't
see that distinction as black and white and um you know i think that if you want to contrast the
sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific
uh concepts at her disposal with you know a non-human animal then this strong distinction
between competence with and without comprehension is reasonable um but in the space of all possible
minds we should be open to the view that there can be you know semi-competent um forms of cognition
and just to put it on this uh it occurred to me while I was listening to you as well that um
the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and
Frank example is a nice is a nice example where in order to give the metacognistic judgments
correctly so see that that you would need competence with comprehension because you need to
understand not only be able to to come to to agree the verb with the subject but know the rule and
know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so
when you find that the L.M. can do well at the low task the member of the task and at the high
task development explicit metacognistic judgments in some way that's an example of the L.M. having
competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given
that LLMs inherently reflect anthropocentric biases due to their training on human data and goals
how can we ensure that their inter model discourse aligns with humanity's values
um
so the inter white discourse right be inter model discourse perhaps amongst the models
I see I see like in that farmville paper yeah the small the yeah both generative agents get
the small ones uh yeah I think that's beyond the scope of this paper to be honest but um
I mean we could use about it yeah but I don't know that we have I don't know this project
has much I mean I I think we both have an interest in the alignment problem uh independently of this
project but I don't think this project has much to say really about this I'm not sure what you think
uh yeah yeah I don't
yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise
into LLM training that was the section about the extra tokens do you see any analog to
intermittent reinforcement to uncertainty tolerance
because you mentioned the extra tokens in the chain of thought and how that could also be replaced
by by dot dot dot dot dot and so like what is that telling us about model training when um
it seems like there's some situations where adding superfluous tokens would diminish signal
in data sets but then here are other situations where it seems to actually help
um um yeah so in that particular paper I think it's called thinking dot by dot and there is a
subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly
what they did is that they they introduced just this one field of token swan meaning this token
just to hold but just a dot and they trained the model to give an answer after producing
a certain number of dots that's not just like introducing Rambam and Gibberish in your training
data it's actually quite a specific intervention that forces the model to um learn to perform
certain computations before giving an answer um so so it makes sense to me that this couldn't
diminish performance and like you you could do that from that it's not quite the same as just
having that training data right um just because the token seems meaningless it's a field of token
to dot um it's not just random gibberish it's going to throw off the model and and impede its
its uh the optimization of its learning function or at least good downstream performance um but what
it's going to do is going to force the model to learn that when there is a dot token it can
allocate computation with its attention heads and other parts of the architecture in such a way
that it's um getting towards deriving the correct token when it's finally producing
the token that matters and that's meaningful after the series of dots um yeah I don't know
Charles if you have another answer yeah I mean I think it's an important question because
a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless
symbols to an LLM input you might very well think that this will just weaken the signal to
noise ratio and degrade model performance so it's against that background that the empirical result
doesn't degrade model performance um ought to be regarded as an important clue about how the model
works so I think that the the intuition behind this question is indeed part of the interpretation
of the empirical results right it's surprising for exactly this reason and then the theory that's
supposed to you know well this is an active inference podcast right so the theory that's
supposed to help rid some of the surprise here is um the idea that
given the uh architecture of a transformer where it's it has to go through all the tokens
in every cycle um having these extra tokens gives it uh sort of more computational bandwidth
and therefore more expressivity or more capacity to uh you know locate uh the right
spot and parameter space and and even that in a way reminds me of so it's not just that dots
improve performance it's not it's that it was like you mentioned it was trained to have that
and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes
verbatim while you're processing so that's kind of like a filler or more of like a sort of
that was a great question it's like these are linguistic paddings that that do create time
to to get to the meet so not only does it signal and signpost if it's being trained to have that
meaning which then questions like so that it wasn't a meaningless dot if it if it had a um
a cognitive or even like a a semantic um aspect I had a question how do you feel like in this
era Cambrian explosion of diverse intelligences how can we understand capacities
when they seem so conditional upon the setting and how the system of interest is interacted with
like what are the practical implications for people who are studying LLMs and other
synthetic intelligences from like a safety or reliability or performance perspective
that was we're gonna
no I was drawing it to you
so so so just want to chat to some of the questions so the question is
how the how is the notion of a capacity
um changing when we have such different systems that seem to have intelligent behavior
yeah and it's so dependent upon potentially initially unintuitive
ways of interacting so how can we understand the reliability and the performance and the
capacity of of a model other than for example by exhaustively inputting prompts
which can't really happen what what could we really say or no and or just how do you feel
that this work re-enters into the ways that people practically are using the models
right okay yeah uh it's the interesting question so the first one the question is I think
you know part of the background assumption from for this paper that I've explicitly
defended in other work is that behavioral evidence is simply not sufficient in most cases
to arbitrate disputes about capacities of LLMs when it comes to human cognition
we do have to rely a lot on on psychological experiments that are ultimately behavioral
and we do also rely on self-reports in a little more than we can when it comes to LLMs because we
despite the move away from relying on intuition and introspection in the history of psychology
it still has a role to play but we've by and large contributed to us by behavioral experiments
that get increasingly sophisticated to try to reverse engineer what's going on inside the black
box when it comes to LLMs partly because that's so different from us um relying exclusively on
behaviorism is even more difficult because we have even less of an idea of what might be going on
inside the black box and whether it's anything like what's going on inside all black box and we
have reasons to think it might be very different so I think I think we both agree that we have to
supplement this with mechanistic work that's essentially involve performing causal interventions
on the inner mechanisms on the inner workings of the systems so decoding representation and
computations that the systems that are in principle available to the systems and then
intervening on them to confirm hypotheses about the causal role of these representations and
computations and we have methods to do that and partly what we can be a little optimistic about
this project even though it's it's it's extremely challenging especially to be scaled up to large
models is because unlike what's happening in your sense with the brain where the range of decoding
methods and intervention methods we have is extremely limited but for ethical and for simply
um practical reasons that we don't just don't have ground truth access to activations in neurons
at least that easily and we also are generally unable to make specific interventions on
activation in the brain uh when it comes to algorithms we have full ground truth knowledge
of all activations of every single part of the network and we also have full access to all of
it for interventions at inference time so that that opens up a whole new range of things we can do
and that enables us to go beyond behavioral studies and actually decode these features and
circuits or as researchers put it in the literature or as philosophers would generally put it
representations and computations that the system is actually making use of and try to reverse
engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem
projects that we have with Charles is to um suppose that we start with these
investigative kinds as we put as as Alibol calls them these human subject capacities
we can operationalize them and do behavioral experiments in the top down and then from the
bottom up we can also try to reverse engineer the mechanism building blocks of the computations
and representations that evidence may use up to solve the task
related to that particular capacity and then we can meet somewhere in the middle and try to
from that line of work that are purchased things from above and bring to the fore some kinds of
mid-level abstractions as we call it or computational building blocks that might be key to
the performance of the system in that domain so for example if you're interested in the capacity
for reasoning you can start with this very broad human-centric notion of reasoning then try to
operationalize it in a reasoning task then do some behavioral testing and then mechanistic
interpretability of that reasoning task find out how the system is solving it find out how the
algorithm is doing well and why reverse engineer building blocks that might for example have to
be viable manipulation viable binding and then from there you might be able to either actually
refine the notion of reasoning you started with to have a more specific and that less human-centric
notion that is now operationalized in more low-level terms like you know that both manipulation
of variables in certain ways and the binding of variables to theories etc so yeah so that's I
think the general approach we take now how does that does any of that feedback into
interactions how we humans interact with algorithms I think that's one way in which
you could feedback is simply in terms of challenging or our spontaneous anthropomorphic
attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you
will interact with your cat in a slightly different way that you might maybe not rush the conclusion
that when your cat performs a certain behavior it has understood what you think and it's modeling
what you're thinking about what it's thinking or something perhaps you might adopt a more
deflationary attitude to explain the behavior of your cat doesn't mean you have to love them
any less or it doesn't mean you have to you know if that is the other thing like if you want at the
end of the day to speak to your cat like a human because you really are a gentleman for that then
that's you know all the more part of you in the same way if you find it useful to treat LLMs in
the way you interact with them to to have fluid interactions with them to treat them as if they
had beliefs these are as etc of human-like capacities then that's fine if that's for
actual purposes but at least if that line of work that we are kind of sketching here
ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's
well even if we if we have that kind of intentional sense and may believe about
who the kinds of besties they have at the background we will know that what their limitations are and
what their actual besties are i'd make sure to go with that Charles because we're going to use
this much yeah yeah no i agree with all that i just had a slightly different first reaction to the
question i took the question to be in part about how to deal with the sort of prompt sensitivity
of models the fact that sometimes we you know write something that seems natural to us but
provokes an unexpected response from a large language model and how do we think about that
and the first thought that occurred to me was just that we should distinguish between
different kinds of large language models you know we have this sort of huge large language models
which are fine-tuned to interact with us in a particular way and our and here's the central
point they're trained on a sort of unthinkably large database whereas there are other sorts
of large language models where the training data is more circumscribed and where we know
in more detail you know what where you can survey what the training data says and i think
if you're interested in you know what the mechanisms are underlying the responses
it's certainly very helpful to look at smaller but nevertheless large language models where the
training data is known to us because you know when you train a model on the entire internet
there are going to be all kinds of you know subtle signals in there that we don't have much hope of
tracing back to their source but which will influence the model behavior in all sorts of
ways but working with these somewhat more conscripted models gets rid of that problem at least in part
cool well where where do you see the work going or where do you plan to continue this direction
yeah so actually so we wrote this paper this short paper for the ICML
machine learning conference in the national conference machine learning that's happening
this week in the data and will be getting to Vienna at the end of the week for the popular
workshop that we're representing this paper which is a workshop on language models and
cognitive science so there will be a very strict page limit for these ICML
contribution which is four pages but what we want to do next is to expand this into a more
philosophically substantive paper that's going to be a bit longer and that's going to expand on the
more philosophically meaty parts of that of that project because everything is still a bit compressed
in that version that we're presenting at ICML so yeah that's the next step for us this is a
really useful way for us to force ourselves to write things down after running the box piece we
wanted to write an academic piece now we've written kind of a condense skeleton of the piece that
focuses more that caters more to an analogians and now the next step is to write the full
philosophy paper or at least that part of our project to be complete and then I don't have to
that maybe we'll have all that ideas but yeah yeah
I got nothing to add to that cool yes well it's very interesting work I think it it brings a
lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the
the heat and into the the spotlight and the relevance and so it's going to be an exciting
learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation
till next time thank you bye
you
you
