WEBVTT

00:00.000 --> 00:14.520
Hello and welcome, everyone. It's January 26th, 2024. We're here in active inference,

00:14.520 --> 00:22.480
MathStream 8.1 with Richard Sarajevan. And we're going to have an interesting presentation

00:22.480 --> 00:28.080
and discussion today on introduction to Bayesian mechanics, free energy principle and the state

00:28.080 --> 00:34.880
based formalism. This is part one. So Richard, thank you for joining. Looking forward to this

00:34.880 --> 00:42.480
presentation and discussion. So to you. Hi, everybody. So yeah, my name is Richard Sarajevan.

00:42.480 --> 00:51.520
I'm French working in Switzerland. I'm a PhD student at EPFL in Lausanne. And just to bring a bit

00:51.520 --> 01:00.080
of context, I'm not working on Bayesian mechanics. We are, we do have a physics background, but we

01:00.080 --> 01:07.680
are interested in modeling bacterial evolution and ecology. And what happened is that something like,

01:08.560 --> 01:14.400
I mean, the free energy principle was always in the corner of my head. And one year and a half ago,

01:14.400 --> 01:21.040
I decided to really read about the free energy principle, especially if I wanted to transition

01:21.120 --> 01:27.200
to the field. And I do want to transition to the field after my PhD. And so I started to ask

01:27.200 --> 01:32.720
many questions to the people from the FEP community. And I'm so grateful. Thanks for them.

01:33.440 --> 01:40.240
And also on the discord of the of the active inference Institute. And at some point, I said that

01:40.240 --> 01:48.320
I was preparing a lab meeting about the free energy principle. And Daniel proposed to have this

01:48.880 --> 01:55.440
discussed on the live stream, because there isn't such material to specifically learn about

01:56.240 --> 02:00.320
Bayesian mechanics and the actual physics underlying the free energy principle.

02:01.520 --> 02:08.160
And so here I am. So once again, I'm not an expert on the matter. So always refer to the

02:08.160 --> 02:16.080
original papers. But hopefully I gonna, I gonna do a decent job. So without further ado, let's,

02:16.080 --> 02:23.120
let's start. I'm not going to tell you what we where we are heading, what questions we would like to

02:24.080 --> 02:28.960
to address or whatever. I'm rather rather going to start building the framework right away.

02:29.680 --> 02:34.000
And at some point, what we're doing, doing will become clear. So

02:35.440 --> 02:40.000
as you may know, there are two formulations or formalisms

02:40.960 --> 02:47.280
of the free energy principles, the so called state based formulation, and the so called

02:47.280 --> 02:55.280
path based formulation. So today, we will focus on the state based formalism. It's not like the old

02:55.280 --> 03:01.840
versus the new formulation. In fact, thinking in terms of path, or so called generalized

03:01.840 --> 03:08.400
coordinates of motion, I've been around forever, but in the literature, but it kind of came back

03:08.400 --> 03:14.320
to the front scene of the Bayesian mechanics literature, I think. Anyway, today, we will

03:14.320 --> 03:23.120
focus on the state based form, formalism. So the very starting point is to write down

03:23.120 --> 03:29.840
Langevin equation, a generic Langevin equation. So it's literally like saying, let's consider

03:29.840 --> 03:37.120
a random dynamical system. Very briefly for the people not acquainted with such an equation,

03:37.920 --> 03:44.480
x here is the state of your system. So it could be a simple scalar if you are

03:44.480 --> 03:51.600
considering a one dimensional process. But in general, x would be a vector. For instance,

03:51.600 --> 03:59.200
if I don't know, you want to, to model the 3d diffusion of a Brownian particle immersed in

03:59.200 --> 04:06.640
a liquid, x would be a 3d vector was components are the coordinates of your Brownian particle.

04:07.680 --> 04:14.080
And you can see on the left hand side that we have dx over dt, the time derivative of the state

04:14.080 --> 04:22.080
vector. So that's such an equation really describes or specifies the dynamics of the system.

04:23.120 --> 04:28.160
So many things can influence indeed the dynamics of the system. If I stick to my

04:28.160 --> 04:34.960
Brownian particle example, maybe it is subject to an external force. So whatever is relevant here,

04:34.960 --> 04:42.000
you put it in F, the so-called deterministic term or flow, we will refer to it as the flow

04:42.000 --> 04:49.040
for the presentation. However, in some cases, there is stuff you don't want to explicitly model.

04:49.760 --> 04:57.040
For instance, if I stick with my Brownian particle example, it is constantly hit by

04:58.240 --> 05:02.720
the molecules of the medium surrounding it. Hence it's a Brownian motion, right?

05:03.680 --> 05:08.720
And it would be so if you want to take into account this thermal fluctuations,

05:08.720 --> 05:15.440
it would be mission impossible to explicitly model every single molecule of the millions,

05:15.440 --> 05:21.840
if not billions of the molecules surrounding it. So a convenient way to still take into account

05:21.840 --> 05:28.000
these fluctuations, which are literally thermal fluctuations in my example, a convenient way

05:28.080 --> 05:36.960
to proceed is just to add a noisy term to the equation. So omega here is a random variable

05:37.520 --> 05:46.240
was value changes with time with the appropriate statistics. Okay, so two brief remarks before

05:46.240 --> 05:53.920
moving on. If you assume that the state of your system changes slowly compared to the time relaxation

05:54.880 --> 06:00.560
of your fluctuations, you can write the autocorrelation function of the

06:01.360 --> 06:08.480
noise like that, where gamma is the diffusion matrix and delta is the delta direct function.

06:08.480 --> 06:16.240
So what it means, it's just that in that case, your noise is super rough and it's not correlated

06:16.240 --> 06:24.160
in time basically. Also, second remark, you can, I mean, you can use the central limit theorem

06:24.160 --> 06:32.560
to argue that it makes sense to assume that omega is normally distributed. So that in the end,

06:33.760 --> 06:39.280
the noise is a Gaussian white noise, but not that in the next live stream where we will

06:40.000 --> 06:46.000
discuss the path based formulation of the FEP, we will relax the white noise assumption.

06:46.720 --> 06:55.920
Anyway, so we have this random dynamical system. And we can do something cool with the flow. So

06:55.920 --> 07:02.560
the flow F is a vector, it has the same dimension than the state vector, because each component

07:02.560 --> 07:09.200
of the state vector has its own longitudinal equation, if you will. And you can decompose it

07:09.200 --> 07:19.280
into a solenoidal and a gradient terms. So before telling you what this decomposition is all about,

07:20.000 --> 07:29.520
on a technical note, just notice that first Q here is the so-called solenoidal matrix. Gamma

07:29.520 --> 07:38.720
is the diffusion matrix just as before. And the I here with the nabla I, this I of X here,

07:38.720 --> 07:45.280
is a negative log of a density. So it's a self-information or surprise we will refer to

07:45.280 --> 07:54.080
it as the surprise throughout the presentation. And the density at play here in this negative

07:54.080 --> 08:02.880
log density is the steady state or nest for non-equilibrium steady state density of the system.

08:02.880 --> 08:11.280
So we assume that there is such nest density that exists, so that if you from a given initial

08:13.200 --> 08:20.080
initial state you let your system evolve, it will reach at some point a unique well-defined

08:21.040 --> 08:28.160
nest density. And second remark before telling you what this decomposition is all about,

08:28.160 --> 08:37.920
note that usually in the papers the divergent terms here and here are put together in a third term

08:39.600 --> 08:47.280
which is sometimes called housekeeping or correction term. But actually if Q and gamma

08:47.280 --> 08:57.040
are not state dependent, these divergent terms vanish anyway and we end up with these the two

08:57.040 --> 09:04.720
remaining terms which can be nicely factorized like that. Also a last thing I want to say is that

09:05.440 --> 09:12.880
if you you consider the solenoidal term, the first term, it is indeed a solenoidal term,

09:12.880 --> 09:19.520
you can indeed write it as the rotational of some potential. I'm saying that because sometimes

09:19.520 --> 09:29.840
people get confused when they see a gradient in both terms. Anyway, what this decomposition is all

09:29.840 --> 09:44.320
about is quite in fact simple. Let's consider this nice 2D single-moded nest density. Okay, so the flow

09:44.320 --> 09:52.880
and more specifically the gradient component of the flow which is here the vertical flow will drive

09:52.880 --> 10:00.880
the system towards its mode while fluctuations kind of push it away. But it's not the only flow,

10:00.880 --> 10:08.320
there is also the solenoidal flow which is here called the horizontal flow which

10:10.000 --> 10:21.440
will make the system kind of converge to its mode with ever-decreasing cycles. And so if you want to

10:21.440 --> 10:27.360
get some more intuitions on this solenoidal flow what we can do is to remove the fluctuations.

10:28.240 --> 10:35.360
So all the entries of gamma, the diffusion matrix go to zero and this means that we would not have

10:35.360 --> 10:41.280
any gradient flow anymore. We end up with only the solenoidal flow and if we do that

10:43.360 --> 10:49.760
the system will just follow an isocontour circulation on the nest density that's the

10:50.320 --> 11:00.000
bottom right panel here where the solenoidal flow kind of drives the system on this circulation here.

11:00.560 --> 11:10.560
So a small remark about this solenoidal flow because it kind of drives the system in this simple

11:10.560 --> 11:18.080
example in either clockwise or anti-clockwise direction in an irreversible fashion, irreversible

11:18.080 --> 11:26.560
in the statistical physics sense. So it breaks detail balance and so on. People sometimes view

11:26.560 --> 11:33.920
this solenoidal flow as underwriting the symmetry breaking ubiquitous in living systems. Anyway,

11:34.880 --> 11:43.120
okay so before using this decomposition of the flow to do some cool stuff I need to introduce

11:43.120 --> 11:50.800
some stuff. So I will have to go through a couple of things of notions one after the other

11:51.440 --> 11:58.160
and afterwards we will put everything together and actually derive the free energy principle.

11:59.040 --> 12:07.200
So the first thing I want to introduce is the notion of sparse coupling. So let's say that

12:07.280 --> 12:15.600
in my state vector x here I have a subset of variables this mu here we refer to as the

12:15.600 --> 12:25.600
internal states and they specify the state of some subsystem called mu. So I mean you get the idea

12:27.360 --> 12:34.400
that we have like an organism, an agent, the bacteria in my schematic and these variables here

12:34.400 --> 12:43.120
literally specifies the internal states of my bacteria and this bacteria is in a given

12:43.840 --> 12:52.400
environment, niche, whatever. So there is this other subsets of variable we refer to as the

12:52.400 --> 12:59.440
external states and which corresponds to the external world, the external states of the

12:59.680 --> 13:08.240
bacteria. And the idea here is that these two subsystems are not connected to each other. So

13:08.240 --> 13:13.840
when I'm saying that two variables are not connected to each other I just mean that

13:15.760 --> 13:22.800
their respective flows do not take the other one state as arguments so they do not influence

13:22.880 --> 13:30.400
each other basically. In fact they are indirectly connected to each other thanks

13:30.400 --> 13:39.280
or through a third subsystem we refer to as the marcov blanket so that these guys here

13:39.280 --> 13:44.880
are called the blanket states and we will see in a minute that it really corresponds to a

13:44.880 --> 13:53.200
marcov blanket in a statistical sense. Okay so we have this architecture, this

13:53.200 --> 13:59.280
path coping architecture here and in fact we can even go a bit further and assume that

13:59.920 --> 14:07.040
within the blanket there are two more systems, the so-called sensory states and the so-called

14:07.040 --> 14:16.800
active states A. So basically the idea here is that the external states eta they influence the

14:16.800 --> 14:24.480
sensory states S and these sensory states S influence the internal states mu but not the

14:24.480 --> 14:33.120
other way around and the internal states mu they influence the active states A which influence

14:33.120 --> 14:40.480
the external states eta but not the other way around. So it's really a path coping architecture

14:40.480 --> 14:47.760
inspired by the so-called action perception loop however you could ask questions like why do the

14:47.760 --> 14:55.360
sensory states influence the external states or why do the active states influence the internal

14:55.360 --> 15:02.320
states etc. So we don't have really time to discuss this I guess you can think of some

15:03.360 --> 15:11.120
qualitative example in biology but I just want to point out that even though this architecture is

15:11.120 --> 15:18.000
quite canonical it's not a definitive feature of the free-nury principle and in fact in the next

15:18.000 --> 15:24.720
time when we will discuss the other formalism we will do a bit of zoology and we will look at

15:24.720 --> 15:33.920
other path coping architecture. Okay so on a technical note just notice that such

15:34.720 --> 15:40.640
path couplings are encoded by zero entries in the Jacobian matrix of the flow.

15:41.520 --> 15:48.000
Anyway so thanks to this path coupling architecture we have this system of four

15:48.560 --> 15:54.880
coupled long-run equations which respectively describes or specifies

15:56.240 --> 16:03.760
the dynamics of the external states eta the sensory and active states s and a and

16:04.480 --> 16:16.880
of the internal states mu. Okay so I want to say here about the Markov-Blanket thing

16:16.880 --> 16:23.520
that under some conditions I'm not going to discuss here so for the people acquainted it

16:23.520 --> 16:30.400
involves having no solenoidal couplings between autonomous and not autonomous states but anyway

16:30.400 --> 16:38.560
I'm not going to go into this let's say under some conditions the external states eta and the

16:38.560 --> 16:45.280
internal states mu are conditional independent so they are independent when conditioned upon b

16:45.280 --> 16:53.280
which makes sense because all the informations kind of transit through b however not that when

16:53.280 --> 16:59.440
I'm talking about conditional independences here I'm talking about conditional independences

16:59.440 --> 17:07.120
in the stationary density so basically if you fix p and you have this joint

17:08.080 --> 17:16.640
uh conditional stationary density here for x i and x j if these two guys are conditional

17:16.640 --> 17:24.000
independent it just means that you can write this joint density like that and so that such

17:24.000 --> 17:31.440
conditional uh independences are encoded by zero entries in the ACN matrix of surprizo.

17:31.840 --> 17:42.240
Okay so now just a bit more of vocabulary before moving on um note that if we

17:43.440 --> 17:49.200
put together a and mu so we consider the couple active states and internal states

17:50.160 --> 17:57.120
we refer to to these guys are as the autonomous states alpha and the cool thing about the

17:57.120 --> 18:05.440
autonomous states a and mu or alpha is that they are conditionally independent of the external

18:05.440 --> 18:13.760
states so autonomous and external states are independent when conditioned upon uh sensory

18:13.760 --> 18:23.360
states and if you add the sensory state s to the autonomous states so you consider the whole thing

18:23.360 --> 18:30.160
the whole marco blanket and the act and the internal states we refer to to these guys as the

18:30.160 --> 18:37.600
particular states pi and pi constitutes a particle a particle in a generic sense of course

18:37.600 --> 18:46.320
so an organism an agent whatever a bacteria in my schematic okay so here I just want to make a

18:46.320 --> 18:51.760
point to make a bit more clear what we are doing what what this this approach is all about

18:52.000 --> 19:01.040
so basically here we kind of define what it means for something a bacteria any whatever to exist in

19:01.040 --> 19:09.760
the sense that it has its own internal dynamics statistically separated from the the external

19:09.760 --> 19:17.360
it does have a marco blanket it does have its own physical integrity so we have no clue of how it

19:18.000 --> 19:26.080
maintains indeed it's it's uh it's integrity in the sense that if you're considering uh real systems

19:26.080 --> 19:34.560
like like an like an actual bacteria or a human being or whatever it it does survive at a given

19:34.560 --> 19:41.440
in a given time scale um right for instance this playing I don't know like active processes

19:42.000 --> 19:48.320
contouring dissipation for instance here we don't say anything about how it does survive it just

19:48.320 --> 19:55.440
does we do have this path coupling architecture and from there from the starting point we are going

19:55.440 --> 20:05.120
to derive the the necessary uh the consequences of such sparse coupling so basically we kind of ask

20:05.120 --> 20:13.600
or answer to or try to answer the questions if things exist what must they do and so if you're

20:13.600 --> 20:19.760
a bit confused don't worry we're going to go back to this idea later but I just want first to show

20:19.760 --> 20:27.280
you this quote here which tells you many theories in the biological sciences are answers to the

20:27.280 --> 20:34.720
question what must things do in order to exist the FEP turned this question on its head and

20:34.720 --> 20:42.160
asks if things exist what must they do but once again we are going to go back to this idea later

20:42.160 --> 20:51.760
but that's kind of the idea of this approach in a nutshell so as I told you I still have a couple

20:51.760 --> 20:59.120
of things to present so I will have to go through each of them one after the other and finally we

20:59.120 --> 21:06.720
will put everything together and finally derive the free energy principle so the next thing I need

21:06.720 --> 21:18.320
to introduce is the notion of synchronization map so very uh very generally speaking I'm not

21:18.320 --> 21:25.280
specifically here talking about our random dynamical system if you have a linear map

21:26.000 --> 21:36.640
uh g mu here which gives you mu from b and g eta here which gives you eta from b then

21:37.920 --> 21:44.160
if g mu here is injected so that basically you can go back to the pre-image from the image

21:44.720 --> 21:53.360
you can use the pseudo inverse of g mu so that from mu you go back to b and from b you can go back

21:53.360 --> 22:08.640
to you can go to eta so the successive application of the pseudo inverse of g mu and then of g eta

22:09.680 --> 22:17.040
is called the synchronization map and it basically allows you to directly go to eta from mu

22:18.000 --> 22:29.920
okay so now let's try to uh to use this idea in the context of our system so b here corresponds to

22:29.920 --> 22:37.920
the blanket states so if I fix the blanket states I have a corresponding conditional

22:37.920 --> 22:46.160
densities for mu and eta I have p of mu given b and p of eta given b and I call and what I

22:46.240 --> 22:55.200
I am and their their modes are bold mu and bold eta so in virtue of this synchronization map

22:55.920 --> 23:03.520
I can go back to the external mode from the internal mode thanks to once again this

23:03.520 --> 23:11.040
synchronization map here and I'm going to give an example in a sec which is going to to make

23:11.040 --> 23:16.560
to clarify a bit more what we are doing here but first I just want to say that

23:17.440 --> 23:24.080
in this nice paper by Lenz Dacosta about this this this synchronization map

23:24.800 --> 23:34.000
basically everything was Gaussian but sometimes I mean if it is not the case a Laplace approximation

23:34.000 --> 23:41.200
which which is literally a Gaussian approximation might be necessary to derive a synchronization

23:41.200 --> 23:47.760
map of closed form but don't worry we'll go back to this idea of Laplace approximation later just

23:47.760 --> 23:55.440
remember that we have this synchronization map here which allows you to go to the external map

23:56.160 --> 24:05.360
external mode sorry from the internal mode so for instance if given b given the blanket states

24:06.480 --> 24:16.320
the corresponding p of eta given b follows this nice normal distribution where bold

24:16.640 --> 24:27.440
bold eta here corresponds to the mode then in virtue of the synchronization map I can view

24:27.440 --> 24:36.800
the internal mode mu as parameterizing a density I write it that way q mu which is equal to this

24:36.800 --> 24:44.720
nice normal distribution where the mode is just the synchronization map applied to the internal

24:44.720 --> 24:53.120
mode to itself and by construction of the synchronization map it is equal to the true

24:53.120 --> 25:02.240
external density so you can view the internal mode as parameterizing a distribution over

25:02.240 --> 25:07.760
external states basically thanks to the synchronization map that's why what the synchronization map

25:07.760 --> 25:14.960
is all about so just a small point because maybe some of you are a bit confused here because we're

25:14.960 --> 25:25.360
talking about modes as opposed to actual states so we will talk about that later but indeed I mean

25:25.360 --> 25:32.160
if I take the actual internal states at a given time t they are not necessarily equal to their modes

25:32.240 --> 25:38.720
just because of fluctuations or whatever so that if I apply the synchronization map on the actual

25:39.680 --> 25:47.520
internal states it might not give you the true external mode but anyway we will discuss this

25:49.040 --> 25:54.560
a bit more later so that was the notion of synchronization map in a nutshell basically

25:55.360 --> 26:03.200
so last thing I want to to introduce before finally putting everything together and actually

26:03.200 --> 26:11.520
derive the free energy principle is the notion of variational inference so very simply let's say

26:11.520 --> 26:20.000
that you have some latent variables or hidden variable or some latent generative process

26:20.960 --> 26:31.520
causing some data s so you have a prior p of eta over the state of these hidden causes of data

26:32.320 --> 26:39.600
and you are also equipped with a generative model which just designates this joint distribution

26:39.600 --> 26:48.320
here p of eta and s so you can view it as a model of how the latent variables cause the data

26:49.200 --> 26:57.600
so the idea here is the following you sample some data s and you want to compute the posterior

26:57.600 --> 27:04.560
distribution p of eta given s so in a way you want to refine your belief about the

27:06.000 --> 27:14.000
the hidden cause of data thanks to a new sample data so it's very simple you in principle because

27:14.000 --> 27:23.920
you just have to apply base theorem right however in practical settings the denominator here p of s

27:24.560 --> 27:35.040
so the marginal density over sensory data usually requires a monstrous marginalization so it's just

27:35.040 --> 27:46.480
not tractable so we can't just apply a base theorem so we need we need a method uh which

27:46.480 --> 27:56.080
given some variational sorry which given some some variational distribution q also called recognition

27:56.080 --> 28:05.920
density uh gives us uh i mean we want a method that makes it as close as possible if not equal

28:06.480 --> 28:13.760
to the true distribution we want ultimately to compute namely p of eta given s and these two

28:13.760 --> 28:22.480
density so q our variational distribution and the true distribution p of eta given s are equal or

28:22.880 --> 28:32.960
more or less equal if their divergence cal divergence here is zero because this quantity here the cal

28:32.960 --> 28:42.000
divergence basically measure the difference between two distributions so that's what i wrote here on

28:42.000 --> 28:49.840
the top of the slides finding an accurate distribution q in the sense of finding a q as close as possible

28:49.840 --> 28:59.200
if not equal to the true target density is equal to minimize to minimizing these divergence

28:59.840 --> 29:06.960
however these divergence i mean there is the target density appearing here we can't

29:07.520 --> 29:13.440
do anything directly with it we can't compute it or whatever we need a proxy for this target

29:13.440 --> 29:21.200
divergence and there in the proxy called variational free energy f in green in my in my uh slide here

29:21.760 --> 29:31.840
so f is is equal to this divergence here between q and the generative model and the idea here is

29:31.840 --> 29:39.120
that you can decompose these divergence into the true the target sorry into the target divergence

29:39.840 --> 29:46.000
in red here plus something so it is indeed a proxy for the target divergence

29:47.520 --> 29:56.240
and note that interestingly enough the second term here is the surprise over sensory data or

29:56.240 --> 30:04.480
negative log p of s so that f is it can be viewed as an upper bound or lower bound depending on how

30:04.480 --> 30:13.440
you define it uh unsurprise okay so what i just said here is that minimizing the target

30:14.160 --> 30:23.520
divergence just means minimizing f so that's basically what variational inference uh is all

30:23.520 --> 30:32.720
about and note that usually algorithms require q to be Gaussian or require a mean field approximation

30:32.720 --> 30:41.440
or whatever and if q is required to be Gaussian even though the target density is not Gaussian

30:41.440 --> 30:48.320
we would end up with the best Gaussian approximation of the target density basically

30:48.320 --> 30:55.840
and in practice it would mean working with a so-called Laplace encoded free energy

30:55.920 --> 31:06.000
okay so before moving on i just want to say that uh this quantity the variational free energy

31:06.800 --> 31:13.920
is in itself a quite rich and interesting quantity so you can decompose it in many ways

31:13.920 --> 31:21.200
and each decompose uh provide interesting uh interpretations for instance if you look at

31:21.200 --> 31:27.760
the second line here you can see that minimizing free free energy means

31:30.480 --> 31:36.880
maximizing this accuracy term here you basically want to explain the data i would say but at the

31:36.880 --> 31:44.960
same time you you want q to differ the least possible from a prior distribution so that's

31:44.960 --> 31:53.760
that's um an interesting quantity anyway now let's finally go back to our sparsely coupled

31:53.760 --> 32:01.760
random dynamica system and use everything we we talked about and finally let's uh derive the

32:01.760 --> 32:10.640
free energy principle so here is our system and we have this four langen equations and the first

32:10.640 --> 32:16.560
thing to do is just to apply the decomposition we talked about in the beginning so basically

32:17.920 --> 32:25.360
the flows of each of them can be written like that so i just directly applied the Helmholtz

32:25.360 --> 32:36.320
decomposition we talked about in the beginning okay so now um let's try to understand uh how

32:36.320 --> 32:45.040
it works let's talk about the dynamics of the system let's say that uh so there is a momentary

32:45.040 --> 32:52.720
instantiated uh uh sensory state and let's fix let's say that the sensory states are fixed

32:52.720 --> 33:00.080
and there is a corresponding autonomous density uh autonomous mode toward which the autonomous

33:00.080 --> 33:08.240
states are going to converge and stay in the vicinity of the of their mode in the closed

33:08.240 --> 33:18.080
vicinity if uh fluctuations are not too large okay but in fact sensory states with time changes

33:19.600 --> 33:28.960
so that the the mode of the autonomous state move as well and in fact it it moves on its

33:28.960 --> 33:33.920
corresponding autonomous manifold so i'm not going to go into the details but just

33:33.920 --> 33:40.960
have in mind that the autonomous mode moves on a so-called autonomous manifold which can

33:40.960 --> 33:46.160
be viewed as a statistical manifold and which can also be viewed as a so-called center or

33:46.160 --> 33:57.280
center manifold so if i kind of rephrase what i am saying here is that the flow of the of the

33:57.280 --> 34:07.440
autonomous states can be decomposed into uh off manifold flow and uh on manifold flow which

34:07.440 --> 34:14.960
corresponds to the path of the mode itself on the manifold okay so just to be a bit more clear

34:15.520 --> 34:24.480
let's say in my bottom right uh illustration diagram here the autonomous states are here

34:25.040 --> 34:36.160
and i'm i'm interested in the the off manifold flow so basically i have the this component here

34:36.160 --> 34:41.920
which corresponds to the gradient flow towards the manifold towards the mode basically here it's

34:41.920 --> 34:49.120
pretty much like what we discussed in the beginning and at the same time there is here this orthogonal

34:49.760 --> 34:58.880
component which corresponds to the solenoidal flow so that's basically the way the autonomous state

34:58.880 --> 35:07.440
are going to reach their mode here it can be viewed as this ever decreasing cycle towards

35:07.440 --> 35:16.560
the the manifold on which the autonomous mode move okay so that's a bit dense i guess so

35:17.200 --> 35:23.120
i recommend to check the paper the free energy principle made simpler but not too simple which

35:23.120 --> 35:34.800
kind of discuss all this this idea about center manifolds and stuff so here the interesting point

35:34.800 --> 35:44.560
is that if you assume a separation of timescale between the fast flow of the manifold as opposed

35:44.560 --> 35:53.040
to the slow flow on the manifold basically the autonomous state always are always in the vicinity

35:53.040 --> 35:58.480
of their modes and if you want to characterize the overall dynamics of the autonomous states

35:58.480 --> 36:07.040
you can focus on the autonomous mode on the path of the mode and in the next slides we will indeed

36:07.040 --> 36:17.200
focus on the autonomous mode and and by definition as we already discussed the autonomous mode

36:18.000 --> 36:23.520
the autonomous mode is or corresponds to the autonomous states which minimize

36:25.760 --> 36:33.520
surprise here in the last two launch variations because the autonomous mode corresponds to the

36:33.520 --> 36:40.320
least surprise of autonomous states before moving on i just want to say something

36:41.360 --> 36:46.000
we can maybe discuss afterwards because i'm not sure to fully understand but basically

36:47.280 --> 36:54.480
if i'm here in my bottom right schematic and so i have this gradient flow towards the manifold and

36:54.560 --> 37:04.720
this solenoidal flow parallel to the manifold and if i remove fluctuations so all the corresponding

37:04.720 --> 37:11.600
entries in the diffusion matrix go to zero as we saw in the beginning it means that there is no

37:13.120 --> 37:17.920
gradient component anymore and what the system will be doing is kind of

37:18.560 --> 37:28.240
um orbiting or oscillating around a point which is which moves on the manifold so that's

37:28.240 --> 37:36.400
interesting and i guess that if we do the exact same reasoning but starting already on the mode

37:36.400 --> 37:45.920
then the world flow reduces to the unmanifold flow and i guess that in that case the autonomous

37:45.920 --> 37:52.320
states follow and in fact coincide with their mode but anyway maybe we we can discuss about that

37:52.320 --> 38:02.240
afterwards so okay so let's use the various things we talked about and especially the notion of

38:02.240 --> 38:10.480
synchronization map we as we said the internal mode parametrize indeed um a distribution over

38:10.480 --> 38:18.880
the external state so mu here parametrize a distribution which by construction coincides

38:18.880 --> 38:26.240
with the true distribution p of eta given b and in fact thanks to the conditional independence

38:26.240 --> 38:34.800
between external states and autonomous states you can just drop the condition upon a and you

38:34.800 --> 38:45.600
just have q mu equal p of eta given f and equivalently you can you can write it p of eta given pi

38:46.400 --> 38:55.840
and the idea here is that you can view q mu as a variational distribution if you want you can

38:56.400 --> 39:05.680
write its associated variational free energy so you have this this formula here the free energy

39:05.680 --> 39:13.760
and because q mu is already already coincide with the true posterior distribution if you will

39:14.400 --> 39:24.080
the first term here goes to zero and so that f here reduces if you will to

39:25.440 --> 39:31.760
the surprise over particular states and surprise over particular states they appear here in the

39:31.760 --> 39:38.240
equations of the autonomous states so we can do this identification and we realize that the

39:38.240 --> 39:47.600
autonomous uh mode not only minimize um not only minimize surprise though but free energy in general

39:48.400 --> 39:56.640
and the way uh mu the internal states will be updated when the sensory states will change

39:56.640 --> 40:05.280
will always be so that this divergence here is zero so that mu is always always keeps track or

40:05.280 --> 40:11.840
synchronized with or in fact interfere the external states so that you can interpret that

40:12.640 --> 40:19.440
under a generative model which is here p the next entity the internal states can be viewed as

40:19.440 --> 40:29.520
performing inference over external states and so in fact it's not only this divergence which is

40:29.520 --> 40:38.640
minimized but it's also surprised and it's not only um only the internal states which uh minimize

40:38.640 --> 40:46.320
free energy but also the active states so let me give an example let's say that the actual

40:47.360 --> 40:54.400
instantiated sensory states are likely sensory states or unsurprising sensory states and by

40:54.400 --> 41:02.240
definition in general the instantiated sensory states will be likely sensory states so mu will

41:02.240 --> 41:07.680
will the corresponding mu will be so that this divergence will be zero as we just discussed

41:08.400 --> 41:16.320
and at the same times the corresponding active mode will be so you can see in composition with the

41:16.320 --> 41:25.920
third term here i of a given s and mu a this active mode will just be the one the most consistent

41:25.920 --> 41:32.480
with this in intensiated sensory states and in fact you can view it the other way around and say that

41:33.440 --> 41:42.320
the active mode is the mode which yield unsurprising sensory states so that the particle can be viewed

41:43.040 --> 41:52.240
viewed as uh actively sampling unsurprising or likely sensory states or equivalently you can

41:52.240 --> 41:59.360
say that the particle kind of um accumulate evidence for its own generative model and i'm going to

41:59.360 --> 42:06.320
say something about the generative model in a sec but i just want first to so yeah this sentence

42:06.320 --> 42:14.400
here just sum up what we said mu is updated so that q mu is always the the best distribution of

42:14.400 --> 42:22.000
our external states and we refer to this as perceptual inference and the idea to in addition

42:22.800 --> 42:29.200
trying to minimize surprise for action is called active inference so a brief note

42:29.840 --> 42:37.440
uh we said earlier that in order to have a synchronization map of closed form it could

42:37.440 --> 42:46.640
be necessary to work under a Laplace approximation so that in that case q mu is always is is just

42:46.640 --> 42:54.320
the best Gaussian for instance of the target density so that the divergence here would not

42:54.320 --> 43:03.520
be zero but it still would be minimized so that the identification here between the two gradients

43:04.240 --> 43:11.840
still hold and nothing change um nothing changes with respect to our discussion so here i just want

43:11.840 --> 43:21.200
to say something about this what we are doing here basically we assume that we have our agents

43:21.200 --> 43:27.680
or organisms that survives indeed exist or persist in a given environment let's say at a

43:27.680 --> 43:36.640
given time scale and we end up with the fact that our particle must be equipped with or must be

43:36.640 --> 43:45.440
must embody a generative model which may or may not exactly coincide with the true generative process

43:46.160 --> 43:53.840
and which encodes the causal structure of the world under which it tries to perform inference

43:53.840 --> 44:01.840
and to minimize surprise to perform perceptual and active inference but the interesting thing

44:01.840 --> 44:08.400
as well is that and i think that's something fundamental that people tend to misunderstood

44:08.400 --> 44:18.160
i guess maybe i'm not sure is that the generative model also encodes the preferences of the system

44:18.160 --> 44:26.080
and let me explain why if i tell you that an organism manages to survive to exist to persist

44:26.080 --> 44:35.840
etc and so it means that such an organism manages to stay in its homeostatic life compatible

44:36.800 --> 44:42.080
states you would be of course it almost sounds like a tautology survive equal

44:44.240 --> 44:50.080
staying in it in in its homeostatic states that obvious right and that's exactly what we are doing

44:50.080 --> 44:58.880
here we assume existence survival so that the likely state in which the particle will will persist

44:59.840 --> 45:08.080
are preferred states per se so that for instance if i'm considering the prior of my generative model

45:08.080 --> 45:19.520
over sensory inputs p of s sensory outcomes s associated with high p of s so likely or

45:19.520 --> 45:27.760
unsurprising sensory states are preferred sensory states states hence when i'm saying that the active

45:27.760 --> 45:35.520
states try to sample unsurprising sensory states it means trying to sample preferred sensory states

45:36.400 --> 45:46.240
and so basically the particle appears to kind of actively accumulate evidence

45:49.040 --> 45:56.240
for its own existence in a way it kind of sample life to sample it kind of sample life compatible

45:56.240 --> 46:02.800
data if you will and that's exactly the definition of self-evident thing so i think

46:02.800 --> 46:09.200
we touch here something fundamental about agency is that agents are self-evident thing

46:09.920 --> 46:20.080
creatures in that sense okay anyway so basically i think that's the most interesting things of

46:20.080 --> 46:26.480
the free energy principle we start from existence and we end up that such a particle which is

46:26.480 --> 46:34.640
coupled to the world in that way must embody a generative model which encodes the causal structure

46:34.640 --> 46:42.000
of the world and which encodes the its preferences in terms of what is life compatible if you will

46:42.240 --> 46:50.400
okay so just to sum up what we did here this idea that free energy is minimized

46:51.680 --> 46:59.360
you can write it that way and this is in a way a variational principle for self organization

46:59.360 --> 47:06.880
that's a free energy principle so here i just wrote what we just discussed the agent keeps

47:06.880 --> 47:11.680
tracks and acts on its external milieu through perceptual and active inference

47:14.080 --> 47:21.280
and note that interestingly enough you can write such a principle as a principle of

47:21.280 --> 47:28.240
least or stationary action where the Lagrangian which is constantly minimized along the path

47:28.800 --> 47:39.040
is variational free energy so here are some concluding remarks i'm not going to throw all

47:39.040 --> 47:45.120
of them but the first one is basically what we just discussed this idea that the generative model

47:46.720 --> 47:54.400
encodes preferences if an agent maintains existence its likely states are its preferred ones per se

47:55.360 --> 48:03.680
hence the notion of stealthily dancing and i just also want to point out that this new approach

48:05.520 --> 48:14.960
or chapter of physics let's say consisting in describing physical systems as encoding probabilistic

48:14.960 --> 48:24.160
beliefs is called Bayesian mechanics okay so having said that thank you very much and especially

48:24.400 --> 48:32.880
thanks to all these guys who who helped me so much especially Len and yeah thank you for

48:33.840 --> 48:35.360
for your your attention

48:45.360 --> 48:48.240
I'm back thank you Richard

48:48.880 --> 48:59.680
okay well while we're settling back in and anyone is asking questions in a live stream

49:00.880 --> 49:09.360
what is your phd research and if this is your side project what is your main project that this

49:09.360 --> 49:17.040
kind of relates to yes so well in fact um i kind of read about the free energy principle

49:17.760 --> 49:28.480
in my free time whenever i i had some time and what i'm doing in my phd is so we have

49:29.680 --> 49:36.880
a couple of projects the first project we did was really modeling bacterial evolution through

49:36.880 --> 49:43.440
so basically we model bacterial evolution as a bias random work on genotype space with

49:44.320 --> 49:50.960
successive mutations and and and successful fixations so that's what we are doing it's

49:50.960 --> 50:02.240
not related to the fp at all and the second thing we have been doing is modeling so basically we

50:02.240 --> 50:10.160
had a system where you have bacteria which can kill each other thanks to a system which is called

50:10.160 --> 50:17.600
the t6 secretion system they kind of have needles with which they can go through the membrane of

50:17.600 --> 50:24.480
other bacteria and and liberate toxins and they can also bind to each other so there is like a

50:24.480 --> 50:30.960
prepredator kind of dynamics and we did like a lattice gas modeling of such systems so basically

50:30.960 --> 50:38.240
that's what i'm doing in my phd which is not related to to Bayesian mechanics but i would like to

50:38.960 --> 50:43.440
to transition to to the field afterwards so yeah i will see how it it goes

50:45.360 --> 50:48.560
i remember when i thought my phd wasn't related to active inference

50:51.440 --> 51:01.600
okay cool well the work built to an amazing crescendo that in its simplicity

51:02.560 --> 51:10.880
even though you highlighted it's easy to fly by which is the coincidence of the preferences

51:10.880 --> 51:17.680
and the expectations so could you maybe give a little context how else has that nexus

51:18.480 --> 51:27.600
of preference and expectation been approached and is the fvp only and simply and always that

51:27.680 --> 51:33.440
coincidence is that coincidence upstream or downstream of some other commitment that we make

51:33.440 --> 51:41.600
like what are the commitments that we really make and is that um alignment the commitment or a

51:41.600 --> 51:52.320
resulting commitment yeah so um so first of all i think the notion of um self evidencing

51:52.400 --> 51:58.400
may be a bit refined with the next formulation but anyway it's i think that's a crucial point

51:59.120 --> 52:06.240
about the fvp and usually it's kind of confusing because when you're reading the papers and

52:06.800 --> 52:14.960
people are starting to write that the system um sample evidence for its own existence you're like

52:14.960 --> 52:21.600
what i mean i'm not sure to understand what's going on here um but in fact yeah it's i i think

52:22.880 --> 52:33.680
the way i i um introduced it this idea that by definition um a living thing is a thing which

52:34.960 --> 52:45.360
which managed to sample live compatible uh sensory data is really what allows this

52:45.680 --> 52:55.680
um align alignment story between the that the idea that between surprise and preferences

52:55.680 --> 53:04.640
basically and this idea that actively sampling um unsurprising data is in fact and it's not

53:04.640 --> 53:12.240
like a tricky wording it's in in a way that's really what's happening it is sampling uh live

53:12.240 --> 53:21.840
compatible or preferred in that sense data hence the notion of self evidencing um but um

53:22.800 --> 53:31.200
yeah i think the whole idea here is that we start from existence we start from the uh

53:31.920 --> 53:37.920
from this past coupling architecture where the particle uh managed to maintain its physical

53:37.920 --> 53:45.920
integrity managed to display a mark of blanket which uh allows the the agent to have its nice

53:45.920 --> 53:51.120
it's um its own internal dynamics separated from the external so somehow it managed to

53:51.120 --> 54:00.960
counter dissipation or whatever and so from there likely states are states consistent with the fact

54:00.960 --> 54:08.480
that it is existing existing indeed so i think that's basically the the the idea but yeah in the

54:08.480 --> 54:14.640
beginning this kind of um line of reasoning can be a bit uh confusing but in fact i think that's

54:15.680 --> 54:24.160
very much what the FEP is all about and actually last remark um in a machine learning street talk

54:24.160 --> 54:33.200
interview of Maxwell Ramstead he it was titled the FEP as um a physics of survival if i remember

54:33.200 --> 54:39.600
well and i think that's that's very very much what what it is all about in a way

54:42.160 --> 54:49.200
awesome how would you relate what you just described to reward or to reinforcement type

54:49.200 --> 54:58.480
learning schemes yeah so i i mean i'm not an expert at all i could not uh make the bridge here

54:59.200 --> 55:07.440
but i know that um Lance Dacosta made several uh works and interviews about the the subject

55:07.440 --> 55:14.160
and actually i think there is a very new paper called active inference as a model of agency

55:14.160 --> 55:21.120
you just shared actually today um so i yeah i recommend the viewers to to check them out

55:21.120 --> 55:27.680
and as far as i know but here i'm just i'm just uh seeing what i heard is that um any

55:27.680 --> 55:35.440
reinforcement learning algorithms can be um can be framed in terms of active inference

55:36.160 --> 55:46.160
so i think active inference is a very um uh fundamental scheme but yeah yeah it's all good

55:46.160 --> 55:52.800
like the reason i ask just with how you presented it is what kind of observations do we want to

55:52.800 --> 55:58.160
sample that could be the sensory embodied interface between the agent and the environment

55:58.160 --> 56:04.640
or you can take a more cognitivist approach and sample internal observations but those are just

56:04.720 --> 56:10.560
external some other internal so what do we want to really sample well if you're even in a position

56:10.560 --> 56:16.240
where you're talking about sampling from like a utility or a reward distribution you've already

56:16.240 --> 56:27.280
specified a distribution why not just specify the existence distribution the actual attractors

56:27.280 --> 56:40.320
and stationaries of the measurements and then um it's simpler because there's no proposal of a

56:40.320 --> 56:47.360
secondary intermediate between the temperature and how good different temperatures are by going

56:47.360 --> 56:52.560
and just saying it's not rewarding to be at 37 homeostatic temperature it's just expected and

56:52.560 --> 57:00.800
likely and the ball rims downhill it's actually a lot simpler and more general yeah and i i think that

57:01.920 --> 57:12.080
um it's way more simpler to i mean the idea here is that the agent has a kind of world model which

57:12.080 --> 57:19.920
as you said uh specified what are the the expectation uh with regards to just existing

57:20.000 --> 57:25.280
in a way and as opposed to designing explicitly

57:29.760 --> 57:30.880
objective functions

57:34.400 --> 57:41.200
with the which incorporates the notions of utility and so on so yeah i'm very much agree

57:41.520 --> 57:52.880
um earlier when we were looking at the flows and we had the breakdown of a flow um could you maybe just

57:55.360 --> 58:05.200
um what animal are you thinking about or what scenario can help us understand like what's the

58:05.200 --> 58:12.160
solid black line what's the small red line what's the spiral what's like a physiological setting

58:12.160 --> 58:16.800
that we could associate here to help us understand that kind of complex movement

58:18.640 --> 58:27.520
yeah so uh generally speaking the first thing i could say is that this notion of solenoidal flow

58:27.520 --> 58:34.320
so it's like in the schematic schematic in the first slide where you have you had this

58:34.320 --> 58:41.200
either contour circulation on the next entity or here the the the component of the flow which

58:41.200 --> 58:53.200
creates this sort of spiral here it can so that um it's this sort of um oscillations are i think

58:54.160 --> 59:01.600
the sort of oscillations or cycles that are ubiquitous in living systems um i mean i i'm not

59:02.240 --> 59:08.880
a biologist but you can uh or not really a biologist but you can think of the circadian

59:08.880 --> 59:16.960
cycle or or anything in any sort of systems there is this sort of of of um attractor where

59:17.120 --> 59:26.480
you're circulating along and so here specifically to this to this um a schematic here i think the

59:26.480 --> 59:37.440
idea is that um um you have so you have you basically let's say that uh for a given sensory

59:37.440 --> 59:45.120
states you have a corresponding autonomous mode and the when the sensory states change the

59:45.200 --> 59:53.120
autonomous mode mode changes as well and in fact move on its so-called manifold so basically i guess

59:53.120 --> 01:00:03.440
here you have the mode moving on its manifold and now if we take the perspective of this

01:00:04.160 --> 01:00:11.040
autonomous states here we converge to the the manifold to the mode

01:00:11.360 --> 01:00:21.280
um and because of the solenoidal uh component of this flow the way we will um reach it is

01:00:21.920 --> 01:00:32.240
with this kind of uh ever-decreasing cycles um so here's the idea and i and um i really recommend

01:00:32.240 --> 01:00:40.880
here the free energy principle simple paper you have the the flow on the manifold it's just the

01:00:40.880 --> 01:00:51.280
path of the mode itself let's say and you have the flow of the manifold was gradient component

01:00:51.280 --> 01:01:02.240
is the flow towards the manifold in fact um so basically so that's basically how autonomous states

01:01:02.240 --> 01:01:11.120
kind of um reacts to to to sensory data which change the autonomous mode and i think the whole

01:01:11.920 --> 01:01:20.240
an important idea here is to assume that the flow of the manifold is fast as opposed to the flow

01:01:20.960 --> 01:01:27.040
on the manifold so that's basically the sensory states are always uh in the vicinity of their

01:01:27.040 --> 01:01:38.240
mode and move with their mode and um sorry and um and um and yeah i think that's pretty much the the

01:01:38.240 --> 01:01:48.880
idea here okay so let's just say that the black line is um our homeostatic body existence life

01:01:48.960 --> 01:01:58.320
compatible ph oxygen blood sugar and yeah we are that light blue dot that's off that manifold

01:01:59.680 --> 01:02:06.880
of course if we were far enough off to be dead it would be a moot question but we're off but

01:02:06.880 --> 01:02:15.600
within a life um scaffolding a compatible zone and now as time pushes us down into the right

01:02:16.240 --> 01:02:26.800
um there are different slices that we can trace um we could take the shortest path the gradient

01:02:26.800 --> 01:02:33.040
flow directly towards the manifold so as that plays out through time it would look like a linear line

01:02:33.920 --> 01:02:41.840
converging to the thick black line or pure solenoidal flow would just stay equally far away

01:02:41.840 --> 01:02:47.040
from the thick black line and continue to spiral so that would look like a cork screw uh through

01:02:47.040 --> 01:02:55.040
time and then here when you have the combined character of the linearized convergence towards

01:02:55.040 --> 01:03:04.080
the manifold and the cork screw out through time we get this kind of winding spiral so

01:03:04.400 --> 01:03:17.200
it reflects on me that the gradient flow is pragmatic value in that it aligns future observations

01:03:17.200 --> 01:03:26.880
with preferences and the solenoidal flow has an almost epistemic character in that it circulates

01:03:26.880 --> 01:03:36.640
amongst a set of equally valid outcomes yet here we're not looking at the pragmatic plus

01:03:36.640 --> 01:03:45.840
epistemic decomposition of the expected free energy policy selection strategy like equation 2.6

01:03:45.840 --> 01:03:54.080
in the 2022 textbook so is that just a concordance or where do you see some of those topics connecting

01:03:57.600 --> 01:04:06.480
um I am not sure maybe uh but having said that on this on the the meaning of the solenoidal part

01:04:06.480 --> 01:04:14.720
here I know that on the on the uh I don't remember if it's in the free energy principle

01:04:14.720 --> 01:04:23.680
simpler paper or or someone else but there is an analogy I mean they discuss the the meaning

01:04:23.680 --> 01:04:33.920
and the role of the solenoidal flow where they say that it it it kind of help um it kind of helps

01:04:33.920 --> 01:04:40.800
mixing system the systems and you can view and they discuss the metaphor with where you want to

01:04:40.800 --> 01:04:50.080
dilute your um your um your coffee for instance and you're going to have this sort of uh motion

01:04:50.080 --> 01:04:59.040
in order to reach the the as fast as possible the the steady state where everything is diluted but I

01:05:00.080 --> 01:05:06.320
I am I'm not sure I didn't think enough myself to provide any sort of interesting insight

01:05:07.440 --> 01:05:15.200
oh good just to have composed it it's very insightful um well you made choices assembling

01:05:15.200 --> 01:05:24.080
things like what do you feel like would have been background maybe a course or a skill what

01:05:24.080 --> 01:05:28.960
background do you feel like you kind of conditioned upon that somebody might want to check out

01:05:29.520 --> 01:05:35.760
and then what do you feel like you would have wanted to include in the state-based formalism

01:05:36.720 --> 01:05:49.760
um because to to bring it into a under one hour timing is very concise so where do you feel like

01:05:50.400 --> 01:05:57.600
somebody could fill in some background to pick up with you at the beginning and then what else

01:05:57.600 --> 01:06:07.120
do you think would make a fuller presentation I think I mean there are a few aspects and details

01:06:07.120 --> 01:06:18.000
I didn't really uh like fully discuss um well first of all all these um things which

01:06:18.560 --> 01:06:23.520
here which involves like center theory a center manifold theory and stuff like that

01:06:24.240 --> 01:06:31.520
uh we we kind of played uh qualitatively with it we didn't really go into that

01:06:32.400 --> 01:06:44.080
and also if we want to be like full really full formally speaking um let's see maybe um

01:06:44.960 --> 01:06:54.800
um uh well there are a couple of things where we that we kind of accept without really checking

01:06:54.800 --> 01:07:00.240
all the assumptions and all the derivation derivation and I'm especially thinking of the

01:07:01.040 --> 01:07:07.200
of the Helmholtz Hau decomposition of F because of course you need a steady state

01:07:07.200 --> 01:07:15.520
net density to exist to in order to have such a decomposition so here I think it's it's there is

01:07:16.960 --> 01:07:25.280
a lot of stuff to to check and I mean there is a nice I think it's in the appendix B of the

01:07:25.280 --> 01:07:33.840
Bayesian mechanics of stationary process paper by Lenz where it derives the Helmholtz decomposition

01:07:34.080 --> 01:07:44.560
uh so yeah there are quite a few things we kind of state we vote um derive so it can

01:07:45.200 --> 01:07:51.840
if people are interested in in going further I think that's kind of interesting formal uh directions

01:07:52.320 --> 01:08:04.720
um and um um yeah cool I think it'll be a really fun collaborative project to

01:08:05.760 --> 01:08:14.560
axiomatize and formalize and modularize using the actin fontology and understand a lot of these um

01:08:15.360 --> 01:08:19.920
relationships and then the other piece that that made me think about is like

01:08:20.720 --> 01:08:27.520
what work is any of this math doing at all just kind of like the the ultimate existential question

01:08:27.520 --> 01:08:37.120
here um and when we condition upon existence we've kind of like off sourced a lot of cognition

01:08:37.680 --> 01:08:45.040
we don't need to make the jump or the walk or the miracle from axiom to embodied existence or to

01:08:45.040 --> 01:08:53.680
even measured hypothetical existence so that is left unaddressed the margin was not big enough

01:08:53.680 --> 01:09:00.880
but it wasn't even addressed and maybe there are even advantages to leaving the um

01:09:01.440 --> 01:09:05.040
um what happens before the conditioning

01:09:07.680 --> 01:09:11.680
you don't want to take it with you after you condition upon it that's the whole mark off

01:09:11.680 --> 01:09:17.040
like a concept like if you're like well I'm conditioning on five years ago in the present

01:09:17.040 --> 01:09:21.600
but also I'm carrying five years with me today well then it's like well then it wasn't conditioned

01:09:21.600 --> 01:09:33.040
upon so to really condition upon measurements is an extremely radically simplifying maneuver

01:09:34.160 --> 01:09:40.160
that may change the scope or the applicability of the framework

01:09:42.000 --> 01:09:47.440
relative to a conception in which what the free energy principle does is describe how things come

01:09:47.440 --> 01:09:58.000
to be however yeah this rather conditioning upon it opens up that discussion and more circumscribes

01:09:58.720 --> 01:10:06.960
this very analytically tractable setting of the agent and the environment across a conditional

01:10:06.960 --> 01:10:17.040
interface yeah by the way about about the conditional thing there is now the notion of

01:10:17.120 --> 01:10:24.240
you know weak mark of blankets that Dalton introduced which kind of lose the approach let's say

01:10:24.960 --> 01:10:36.400
and because indeed there is a question on I mean does it apart from the formal setting we have here

01:10:36.400 --> 01:10:45.520
can we really apply it to real systems and stuff like that and also I think it's the physics of

01:10:45.520 --> 01:10:55.760
survival in itself at a given survive at a given time scale there is at if I have at a given time

01:10:55.760 --> 01:11:01.840
scale we survive indeed in the sense that there is indeed this partition or conditional independence

01:11:01.840 --> 01:11:08.240
between the internal and external here is the physics you have to comply with but we didn't

01:11:08.240 --> 01:11:15.120
tell you tells you how was the mark of blanket rise or whatever it's it's it's it's just not what it

01:11:15.600 --> 01:11:27.520
is designed to to explain but I think generally speaking it's it's really informative because

01:11:27.520 --> 01:11:34.960
for instance if you are considering the I mean the sort of approach in general I mean for instance

01:11:34.960 --> 01:11:42.720
if you consider the pendulum effect where you put pendulum oscillating on the table and they are

01:11:42.720 --> 01:11:49.040
going to synchronize synchronize with each other and I think that Kuiha Isomura did a paper about

01:11:49.040 --> 01:11:58.160
that recently in order to understand what is going on and why the pendulum synchronized at some point

01:11:58.160 --> 01:12:05.600
you just have to recycle all this line of reasoning with the synchronization map that's very what is

01:12:05.600 --> 01:12:11.600
at play and what explains why the pendulum synchronized when they are both on the same table

01:12:12.160 --> 01:12:21.040
so I think it really it is really informative to in order to understand what is going on when we

01:12:21.040 --> 01:12:29.760
are talking about synchronization phenomena across sparsely coupled systems and also it gives you

01:12:29.840 --> 01:12:40.400
I guess the sort of recipe to understand what it what it what it takes to be an agent if you want

01:12:40.960 --> 01:12:52.960
to design your an intelligence system and but yeah the question of how much useful it is

01:12:53.920 --> 01:13:01.680
beyond the fact that it's just some nice formal framework it's it's an interesting

01:13:02.400 --> 01:13:11.200
interesting discussion yeah and I just two things first I would like to go back to your

01:13:11.200 --> 01:13:19.520
previous question about what sort of things could be could be discussed further I think

01:13:19.520 --> 01:13:28.160
an interesting point we didn't really discuss fully is the notion of synchronization map

01:13:28.720 --> 01:13:39.280
because we didn't necessarily discuss the the hypothesis and stuff about the synchronization

01:13:39.280 --> 01:13:48.080
map and I in fact I think there is much things that can be said for instance because we assume

01:13:48.080 --> 01:13:57.440
injectivity thanks to the rank nullity theorem it's kind of constrain the dimension of the

01:13:57.440 --> 01:14:05.200
internal manifold here with respect to the blanket manifold here and it kinds of constrain

01:14:05.200 --> 01:14:10.800
in order to have injectivity thanks to the rank nullity theorem and so it kind of constrain the

01:14:11.680 --> 01:14:17.840
the in order to say it in a qualitative fashion it kind of constrain the the complexity or

01:14:17.840 --> 01:14:26.640
richness of the internal states which speaks nicely to other frameworks like

01:14:30.240 --> 01:14:37.360
like HB's laws of requisite variability where you want the regulator system to be as

01:14:38.320 --> 01:14:45.360
as sophisticated or as rich to the regulated systems and here you need the internal states to

01:14:46.160 --> 01:14:54.080
be enough complex to or to constitute the sufficient statistics let's say to parametrize

01:14:54.080 --> 01:15:03.440
to be able to parametrize the density indeed and this and this richness let's say is constrained by

01:15:04.080 --> 01:15:11.760
the the cardinality of your sensory channels if you will because basically you need the internal

01:15:12.640 --> 01:15:20.480
manifold to be to have the same dimension than the blanket manifold or the sensory manifold to be

01:15:20.480 --> 01:15:29.200
to have the same dimensions than the autonomous manifold so I I mean I think there is many things

01:15:29.200 --> 01:15:38.560
to discuss about this this aspect here and the last thing I would like to say about your

01:15:39.840 --> 01:15:46.160
about your last question about the applicability of the framework and how much it's useful

01:15:46.160 --> 01:15:54.400
as opposed to be a simple elegant formal framework I think so you know there is this

01:15:54.960 --> 01:16:04.000
these papers about about like the Markov-Blanquet trick and stuff like that

01:16:05.520 --> 01:16:11.920
about how much difficulties like to identify what states corresponds to the Markov-Blanquet or whatever

01:16:12.880 --> 01:16:24.480
and I'm personally I'm not really convinced by this these critiques because to me it's like

01:16:24.480 --> 01:16:31.280
to me it's like saying to Newton yeah I mean it's I'm not sure that I can do anything with your

01:16:31.280 --> 01:16:40.160
framework it's it's complicated if not impossible to model systems with clearly identified and

01:16:40.160 --> 01:16:47.840
separated rows and masses let's say okay fine but we are talking about Newton mechanics here so

01:16:47.840 --> 01:16:53.280
I mean I think it's the same here it's if you have a sparsic coupled random domain because

01:16:53.280 --> 01:16:58.720
systems that's the sort of behavior it will display it tells you fundamental things about

01:16:58.720 --> 01:17:05.840
the nature of living systems and the idea that when it comes to a specific system it can be

01:17:05.840 --> 01:17:09.600
quite tricky to to model it that's another question

01:17:12.720 --> 01:17:18.720
and indeed when it comes to the art of modeling complex systems it's it's it's interesting and

01:17:18.720 --> 01:17:26.400
and we can discuss about how much complicated it can be to apply the framework yeah awesome I love

01:17:26.400 --> 01:17:34.960
that it's like the art of the science and the art of the modeling and the and the craft especially

01:17:34.960 --> 01:17:43.520
in the kind of early hand-built largely custom stage like one thing I even wondered looking

01:17:43.520 --> 01:17:51.680
through these slides what fraction of these representations and formalisms exist only

01:17:52.240 --> 01:17:58.800
analytically and do or is there a code representation of this exact scenario

01:17:58.800 --> 01:18:07.200
or you know are some of these areas equations that don't have

01:18:09.760 --> 01:18:13.840
code realizations they're just pure existing equations

01:18:16.000 --> 01:18:25.920
so I mean I think more or less everything here can be can be simulated even this synchronization

01:18:25.920 --> 01:18:35.280
thing here you can perform simulations where you can really literally see within the simulations

01:18:35.280 --> 01:18:45.760
the synchronization and I mean the the the whole thing here can be you can simulate such

01:18:45.760 --> 01:18:53.520
sparsic coupled random dynamic systems and and kind of interprets the dynamics indeed as

01:18:54.240 --> 01:18:56.880
the way we we frame we frame it

01:18:59.760 --> 01:19:08.480
but but yeah that's also an interesting aspect it could be cool like in the github repo

01:19:09.200 --> 01:19:15.520
in the journal for this transcript or something like that to to curate together

01:19:15.520 --> 01:19:22.160
the simulations that demonstrate or a minimal specification for it

01:19:24.240 --> 01:19:30.560
you know because it's it's actually there is a great yeah and actually there is a

01:19:31.760 --> 01:19:38.240
I mean I think it's in the in in length paper about synchronization map the Bayesian mechanics

01:19:38.240 --> 01:19:47.280
of stationary process processes paper there there are some simulations where he shows that

01:19:49.120 --> 01:19:53.840
I mean he shows the the synchronization map at play and it shows that

01:19:54.640 --> 01:20:02.240
basically you can't go back to I mean if the the the map between the blanket states to the

01:20:02.240 --> 01:20:10.720
internet states is not injective uh and you apply the synchronization map to the actual

01:20:10.720 --> 01:20:17.520
sensory uh to the actual internet states it gives you like some not relevant things and

01:20:17.520 --> 01:20:22.640
there are some nice plots from simulations so that's definitely a paper to to check out

01:20:22.640 --> 01:20:35.040
so where do we land and then how do we leap exercise relax to prepare for part two

01:20:36.640 --> 01:20:44.960
yeah so I think here's the the world point was I mean the this world formulation is in a way

01:20:44.960 --> 01:20:54.400
about the momentary the short day the short term and the momentary response to autonomous states to

01:20:54.400 --> 01:21:06.080
sensory stimuli let's say if there is this uh this um I mean the the kind of instantiated active

01:21:06.160 --> 01:21:15.200
states are so that it comes whatever but in the next uh video where we will look at the path

01:21:15.200 --> 01:21:22.720
based formulation of the framework the world idea would be to ask what about path and what about

01:21:23.360 --> 01:21:34.480
future path and what about the long term behavior uh and what about planning what about higher order

01:21:34.480 --> 01:21:44.960
cognitive abilities um and we we will kind of extend the scope of what we are doing in in in

01:21:44.960 --> 01:21:55.520
that sense um but um yeah I mean I think from a formal point of view uh here I kind of introduced

01:21:55.520 --> 01:22:01.200
many things variational variational inference synchronization map etc one after the other

01:22:01.200 --> 01:22:06.560
before actually deriving the free energy principle next time I think we will it will

01:22:06.560 --> 01:22:15.520
be more straightforward but the the main concepts to to which will be at the core of the of the

01:22:15.520 --> 01:22:21.840
framework and which can be confusing if it's the first time you you look at it is the notion of

01:22:23.280 --> 01:22:30.480
of generalized coordinates of motion when you relax the white noise assumption and that's

01:22:30.480 --> 01:22:34.960
something that can be confusing especially for the physicists because when you're starting

01:22:34.960 --> 01:22:40.960
saying yeah the generalized lagrangian he plays a role of an action or whatever they are like

01:22:40.960 --> 01:22:46.800
no but lagrangian is not an action what are you talking about etc but when you get acquainted with

01:22:47.520 --> 01:22:54.160
the the world construction is very elegant but that definitely something people can

01:22:54.160 --> 01:23:02.800
started look at before prior to the the live stream yeah awesome yeah well it was excellent

01:23:02.800 --> 01:23:11.120
you you brought a lot together and a lot of trails leading off this trail and the citations and

01:23:11.120 --> 01:23:18.800
previous um papers that that also brought things together lance's work and others

01:23:19.440 --> 01:23:26.000
and it's going to be awesome to see part two so thank you Richard yeah thank you very much

01:23:26.000 --> 01:23:32.000
Daniel thank you all right see ya bye bye

01:23:48.800 --> 01:23:50.080
you

