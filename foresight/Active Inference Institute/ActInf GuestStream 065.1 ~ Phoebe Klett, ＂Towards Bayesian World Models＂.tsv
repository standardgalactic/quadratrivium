start	end	text
0	11120	Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll
11120	17520	be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent
17520	24320	reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe,
24320	30800	very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah,
30800	35440	I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art
35440	43280	language models into more probabilistic machinery and what that might bias. All right. Let's get
43280	49120	right into it. All right. So some of the things that I'm hoping to discuss today include why we
49120	54240	might use a language model for something that isn't long-form text generation and how we might do
54240	60640	that, and then motivate a little bit kind of why we might need a world model, what simple
61280	66960	self-organizing world models might look like, and even in the simplest cases how we might start to
66960	72480	use those as effective recommendation engines in the wild today, and then a little bit of discussion
72480	82560	about kind of where this research is going. All right. So what are language models good at?
83520	88080	Today's especially large language models are trained on next token prediction.
88800	94640	So this means given a sequence of tokens, we're going to estimate which token is most likely to
94640	101040	come next. Maybe rephrasing that a little bit. We might also say that language models are trained
101040	109040	to estimate which sequences of tokens or words are likely. The caveat being likely to appear in
109040	113280	the training set, but today our training sets are quite extensive.
116880	122160	So given this simple training objective, it's arguably surprising that we've seen language
122160	128480	models do as well as they have in really impressive tasks. So they start to demonstrate
128480	133760	like really great language understanding, meaning like the semantics of language itself,
134400	140800	not just the syntax of which sentences make sense. They've also started to demonstrate some
140800	145440	world knowledge, so implicitly learning kind of how the world works just through our own human
145440	153760	abstraction and how we articulate that. We start to see language models falter at tasks,
153760	159280	which look more like symbolic problem solving. So math in particular, we see this in programming,
159280	164400	although we're getting better at this. And in general, any kind of long-term planning tasks,
164400	169760	which require abstract reasoning. So we see this also in problems that look like this,
169760	177200	which are word problems, but which are really about understanding abstractly how these abstract
177200	181840	objects relate to each other. And again, this shouldn't be surprising that language models
181840	187200	struggle to do things like this, since it's so far off from their original training objective.
187600	193200	And in particular, even when we arrive at the right answer in some of these cases,
193200	197520	it's really hard to know after the fact kind of the reasoning for how we ended up
197520	203520	at the right answer in this particular case. And maybe more fundamentally, if we don't know
203520	208000	where the abstraction is happening or where the reasoning is happening, it's very hard to guide
208000	213840	that process. And so this starts to motivate the need for something that looks more like
213840	219680	an explicit world model. And now, to start to borrow some words from Yasha Benjio,
219680	223680	this is one of the kind of biggest issues with today's language models,
223680	227760	is arguably that we're asking the language model to be both the inference machine
227760	234800	and the world model implicitly, when this doesn't quite make sense. And so some things that we
234800	240400	might hope for in a world model are to model causal relationships, to be really adapt at
240400	248320	modeling, using uncertainty, and to be modular. Yann LeCun puts this in a similar way, so we
248320	253200	might kind of ask a world model to be able to distinguish between which details are important
253200	258000	versus irrelevant, and to be able to make predictions that can be performed in sort
258000	264480	of this abstract space of representations. And so hopefully in the next following slides,
264480	270000	we're going to motivate what it might look like to use language models as inference machines
270000	277360	in maybe like the simplest case of self-organizing world models. And even in those most simple
277360	283280	cases, how we start to get something much closer to the kind of explainable reasoning,
283280	285600	which I think lots of us are grasping for right now.
290240	296320	All right, so in a very simple case, we might think of a world model as simply a collection
296320	302880	of hypotheses, where we model confidence over each of those hypotheses. And in particular,
302880	307680	we care about predictive world models. So given some evidence or data that we've observed in the
307680	316080	world, we'd like to propose the world models, which best explain the evidence that we've observed.
320080	326080	And if that language sounded leading, it was indeed. We are proposing in this case to use
326080	331520	Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given
331520	340080	some new evidence. And so when our hypotheses are kind of these Bernoulli random variables,
340080	347440	this bottom term can simply be expanded into this equation on the slide. And so the tricky part
347440	355280	in this computation is the likelihood piece. So given this evidence, given that our hypothesis
355280	360720	is true, what's the likelihood that we would have observed the evidence? And the claim that we're
360720	367200	making is that language models are actually, this is like a natural task to ask of a language model.
368160	372640	When our evidence and our hypothesis are both semantic objects, as we discussed, language
372640	380560	models are kind of trained exactly to understand which sequences of text are likely. So asking it to
381440	387360	prompting it in a clever way such that we can extract this particular likelihood
387360	394800	is actually really natural. So how do we do this? So we come up with this kind of clever
394800	402240	prompting scheme that allows us to extract again exactly how likely is this should have occurred.
402880	409440	So given that our hypothesis is H in this setting and our evidence is this curly E,
410640	414880	we only need to change our evidence into the conditional form and then phrase the question
414880	421920	like this to the language model. So the input sequence of text goes like, given that potentially
421920	427280	if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic,
427280	432160	would we have observed the evidence that Walmart laid off 10% of the material staff?
434000	439920	And so either using something like few shot prompting or guidance or some kind of control
439920	445600	generation technique, we can ensure that our language model outputs either yes or no,
445600	450320	and then use the logits from its answer to estimate that probability.
450400	458560	And again, the claim here is that this is actually a really natural task to ask of a language model
458560	464720	that leverages its innate reasoning engine better than just kind of allowing it to ramble
466320	476160	using text sometimes. And we can even make this updating scheme a bit more sophisticated
476160	482400	using things like precision waiting. So this will bias our posterior over hypothesis
482400	488080	either towards our prior or our evidence that we've observed based on how confident we are
488080	500240	in our prior and in our evidence. All right. So the world models that we've seen so far
500800	506000	are fairly simple. In particular, we're modeling all our hypotheses as independent from each other.
506880	512480	And this seems like a large simplifying assumption. So how might we support more complex world
512480	521120	models where we condition our hypotheses on each other? All right. So another thing that
521120	525520	hopefully you'll be familiar to this crowd, Bayes and Nets have been around for forever and
525520	533040	are often used to model mechanistic failures and enlarge systems and can be understood simply as
533120	536880	distribution where each variable depends on some small number of ancestor variables.
538080	541680	Perhaps more intuitively, we can also think of these as directed graphs
543200	547280	where we have edges between variables which are directed in the sense that
547280	553680	the parent is conditioned on the child. So let's see an example.
555600	562720	All right. So now we have two different kinds of hypotheses. The ones which are kind of parents
562720	569360	in our simple setup are kind of qualitatively more abstract than the children. So our two
569360	575680	children nodes are similar to the hypothesis we saw before. Mark a sentiment for Best Buy is Poor
575680	581360	or Walmart will grow its physical footprint this year. And then the more abstract hypothesis
581360	589200	being retailers were negatively impacted by COVID-19. And so then we specify this conditional
589200	594400	structure either kind of in a classical sense by specifying all of the joint probabilities
594400	600560	upfront or alternatively learning it given some healthy amount of training data. And again,
600560	605840	we're proposing that instead of doing that intensive process, we can use language models
605840	612960	to extract those probabilities in a natural way. And so one natural complaint at this point might
612960	618560	be like, well, this space is going to start to get very large. If we're trying to update these
618560	624720	beliefs in real time given some large data stream is going to become intractable given our current
624720	631840	framework. So how might we start to augment that? Luckily, there's been a lot of work historically
631840	637280	done on this problem. And we can start to use things like message passing to update our beliefs
637280	643200	in an approximate way. So as long as our Bayes net has a tree dependency structure, we can use
643200	648000	things like the sum product algorithm to update our beliefs. And I don't want you to worry too
648000	652960	much about the equations on this slide. If you've seen this before, this kind of recursive structure
652960	658640	will look familiar. But if not, the general kind of intuitive idea that we're going to compute these
658640	664160	messages from all of the children or the neighboring nodes and use those to propagate through the graph
664160	672400	to update each independent belief. Let's just see an example. All right, so suppose we observed
672400	678080	some new evidence and we'd like to know how should we update probability over hypothesis A,
678080	684960	which as you remember is the parent node in our graph. All right, so B here on the left hand side
684960	690560	is the belief. XA is essentially like just the variable representing the hypothesis A.
691680	698320	And then the fee and the side terms are specified by our language model or the
698960	704400	the Bayes net in the classical sense. And so we're summing over all of the variables,
704400	708640	all of the values that the variables XB and XC can take, which in our case, of course, is just zero
708640	715520	and one. So if you, you might recognize at this point that this is indeed the exact marginal
715520	721600	probability for the hypothesis A, and that's because our graph is simply connected in this example.
722160	726480	So in general, this isn't true, but it turns out to be the case in our example.
726720	733280	And again, we can compute these fee and side terms using the language model itself.
738080	742720	All right, so because we're all lovers of free energy here, I'm going to walk through kind of
742720	750640	how this is maybe the first example of a self evidencing or minimizing free energy kind of model.
751200	756320	So as we noted, belief propagation isn't exact for more complicated graphs. And so
757040	762640	it makes sense and might be useful to ask the question, you know, how far apart or when are
762640	769920	our beliefs close to the exact marginals. And so we often use things like KL divergence to
769920	775680	compare the difference between two probability distributions. And that is explicated on the
775680	784720	right hand side. And then those of us who are like have a background in physics might recognize
784720	789920	Boltzmann's law as well. So this is just the idea that we might represent the probability of a given
789920	795040	state using an energy function. And we're not going to accept this as truth, maybe some of us
795040	799360	have done in the past, but we are going to just use it as a definition for this energy function,
800080	807200	such that when we plug in that term here, and expand out, we start to see these first two terms
807200	811840	look a lot like the kind of energy and entropy functions which we are used to. And indeed,
811840	819920	we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy
819920	826080	to see this all coming together. And in particular, it might make sense just to note at this point that
826800	832320	using world models which are self organizing in the sense seems to be very compelling,
832320	838240	since the kind of world model which we want is one which promotes the evidence that we've seen so far.
843920	848560	Why is it useful to formulate this in terms of free energy, besides the fact that we all
849360	853760	find it compelling here? Well, you can make a lot of progress by constructing analytically
853760	859680	tractable approximations of Gibbs free energy often. I'm not going to go into the details here,
860800	863760	but here are two examples where that's been fruitful.
868480	873440	All right. So now I'm going to chat briefly about kind of how we might use recommendation
873440	878400	systems like, or how we might use these systems as recommendation systems, and indeed in the world
879280	887440	today. All right. So one prime example for a system like this might be useful is a situation
887440	893760	where we have lots of data incoming at very high frequencies, and we always want to have
893760	899520	some set of naturally discrete hypotheses that we're modeling beliefs over which are
899520	904960	being kept up to date at a very regular cadence. And so actually, a lot of the
905600	912560	the muscle here is just reformatting documents or however our data comes in as evidences. This is
912560	918400	not always obvious or easy to do. But once you've kind of figured out that part, and in particular,
919200	926000	we've been using things like RAG, retrieval augmented generation, or embedding based systems to
926000	932560	kind of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind
932640	939520	of built up that machinery, the actual updating computations, as we've shown already, is actually
939520	944960	pretty simple. So we do these likelihood computations and we update our beliefs. And then at any given
944960	951600	time, we can query that model for our marginal distribution over any given hypothesis.
953120	957760	And it turns out that this kind of setup has many practical applications.
957760	966160	It's also noteworthy that even with very simple systems like these, these are like
966160	972480	out of the box, controllable and explainable. So just by storing the magnitude and the direction
972480	978160	of the update to the posterior for each piece of evidence that we observe, we have a very natural
978160	986400	built in explanation for our belief at any given time. And that makes kind of like these applications
986400	991520	where folks might really like to use a language model, but really require like a robust,
992320	999200	like causal relationship between the outputs and the explanation, which you don't get from
999200	1003760	a language model on its own. A system like this can be very appealing in those situations.
1008800	1010720	All right, so now on to further work.
1011440	1021600	So everything that we've discussed today is early work towards integrating language models
1021600	1028960	into more probabilistic frameworks. And there's been a lot of exciting work done in this vein
1028960	1034080	right now. Some important questions which are especially interesting to me are which parts
1034080	1040400	of the world model should be learned versus encoded? And how do we want intelligence to scale?
1041360	1046560	Both in the sense of composing systems naturally, there should be some very like natural way that
1046560	1051920	we can compose to intelligent systems and also such that we can scale them with compute.
1053040	1057840	And I don't mean to restrict myself either to the kinds of compute that we have today.
1057840	1062720	We're also working at some exciting new computing paradigms at normal, which might be more
1062720	1069520	compatible with software of this nature. Also the two folks that we referenced at the beginning
1069520	1073680	of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area,
1075280	1080640	which is very inspiring. And so in particular, G flow nets are also probabilistic graphical
1080640	1088240	models, which I think folks will find a natural next step in reading if you so desire.
1091360	1093760	And that's it for me.
1094400	1103360	Awesome. Thank you. Wow, very cool.
1106560	1111040	Dan, do you want to give a first reflection or thought? And then meanwhile, anyone who's
1111040	1114720	watching live, please feel free to write questions. I'll relay them in.
1114720	1125040	Absolutely. So hi, I'm Dan. I work with Phoebe on this project. And yes, the
1126480	1133120	I think the thing that's most exciting about this for me personally is sort of twofold.
1133840	1144320	One of them is that it's a way of avoiding sort of having to trust a language model to
1145040	1154800	understand and reason about text. Because they're not it's not that bad. The thing is that the
1154800	1160080	extremely strange thing about language models is they're quite good at being almost good enough.
1161040	1167360	But they're never quite what you could use. You could never use a language model to, I don't know,
1170000	1175520	sort of triage, like an important sort of situation where a bunch of different things
1175520	1179840	are coming in, you have to make a decision about which is important. The reason you can't do that
1179840	1187200	is you simply cannot understand the encoded biases. You cannot get it to reliably generate
1187200	1192080	reasoning. You can ask it for reasoning. But the thing that it prints out is not the reasoning
1192080	1199280	that it used internally because it doesn't reason. Fundamentally, while these have input and output
1199280	1205360	that are natural language, they are not artificially intelligent. They are just prediction machines.
1206560	1211600	And so we have to be very careful about not anthropomorphizing them. So this is a way of
1211600	1218720	using those incredibly powerful prediction machines in a framework where we can
1220320	1229840	make sure that we essentially keep a record of what we're doing so that a human can look at it.
1229840	1234400	Because, I mean, there's a lot of talk in this world about sort of post-human AI
1235040	1239680	and those sorts of things. The idea that the machines will become intelligent enough,
1239680	1243760	or the machines will rise up in a slightly more alarming type of way.
1245760	1249840	And that's all great and wonderful, but that's not particularly interesting to us at normal.
1249840	1256400	We're much more interested in sort of having mimicking explicit decision processes so that
1256400	1263440	a human can audit them and can make these things work. That's kind of the area that we're coming from.
1264400	1271280	Awesome. All right. I'll go to a question from the chat. So Josh asks,
1271840	1278080	great talk. Where does hypothesis relevance enter the calculus? Is it folded into confidence?
1278080	1281840	Not sure if it ought to be. Just saw hypothesis relevance mentioned.
1285360	1291440	Hypothesis relevance. Does that mean like which hypotheses are conditioned on each other?
1292400	1296560	Is it possible to ask a clarifying question there? Maybe Dan, you have a better idea.
1296560	1302800	They can follow up. But yeah, I also wondered about this. You might know what was relevance.
1303360	1311440	Maybe the temperature and the rainfall were relevant, but then how does this approach help us
1312080	1317440	understand when one of those relevant factors no longer is relevant or when a new relevant
1317440	1325120	factor comes into play? Yeah, these are great questions. So I think in terms of understanding
1325120	1330480	in an automatic sense, when two hypotheses are relevant to each other, we can leverage
1331280	1338480	embedding type language models for this kind of thing also. If we don't have a more kind of like
1338480	1346240	structured human intuitive sense for when two hypotheses are related, in terms of like how
1346240	1350720	those relationships evolve over time, this is something that's really interesting to me.
1350720	1356160	And I think looking at the theory behind structure learning or when we propose to add new nodes to
1356160	1361840	the network or propose to add a new edge to the network or things like this is a really exciting
1361840	1367600	research direction. Although I don't have like a silver bullet answer to how we should do that.
1368240	1375760	Just to like add a little bit more to that, it is like it is a really interesting research
1375760	1382560	direction. Like one of the things that Phoebe mentioned in the talk is that there is a difficult
1382560	1386720	step that we're not talking about, which is actually translating this natural language
1386720	1394000	into reasonable hypotheses. So there is a step in there where you take essentially a chunk of text
1394000	1401520	and you have to decide if this is a hypothesis, if this is a hypothesis we've seen before,
1402160	1409280	if this is a sub hypothesis or a clarification of a hypothesis that we've seen before,
1409280	1416160	and so on and so forth. So that in some sense part of the data processing and it is an important
1416720	1423440	step and one that we are sort of continuing to work on and refine. The other thing like a different
1424160	1429520	sort of interpretation of the question around relevance is around sort of
1430720	1436160	is the hypothesis relevant to the thing that you're looking at? I mean we could have a hypothesis
1436160	1443920	the sky is blue, but if we are deciding to deciding you know whether or not we need to
1443920	1451600	check that part's oil, like the truth or not of the color of the sky is very irrelevant.
1451600	1457840	And that then comes into the nice thing about having your world described as a
1459680	1465440	collection of statements with truth values associated with them in that you can directly
1465440	1473360	reason over them. So you can put a classical decision framework over that to take into account
1473920	1478480	both the sort of the knowledge you have of the world and also which parts of these worlds are
1479120	1485040	sort of unknown. So in that sort of situation the person using the world model to construct a
1489040	1498480	sort of decision or an output will be responsible in some sense for assigning a weight or a cost
1498480	1503920	to each hypothesis being true. And for some of those hypotheses obviously it will be zero
1504880	1508720	because again we do not care about the color of the sky if all I want to know
1508720	1516080	is if I need to change the oil in my car. So that's the sort of the other end of the answer.
1516080	1519920	So there's a version of the answer at the start of the information flow and there's a version of
1519920	1525120	the answer at the end of the information flow. But it is a tricky point and one that we are sort
1525120	1530400	of continuing to iterate on to try and find sort of good ways on both ends of that.
1530560	1541680	Yeah well a lot there. It's very interesting how in that presentation in response I heard
1541680	1551120	both about probability distributions on rules and rules on probability distributions and like
1552080	1557280	which one whether it's the tail wagging the dog or the horse in the cart how to design
1557280	1561600	these synthetic intelligence systems that appropriately bring together
1562720	1569920	aspects that are more symbolic more rule like and then more probabilistic more embedding like.
1569920	1577280	So where does that end with you or how do you see the design of these systems with mixed symbolic
1577280	1582960	and probabilistic components? Yeah yeah that's a great point and I think this really gets it like
1583040	1587760	which parts of the world model should be learned or should be represented in some like more
1587760	1594320	discrete space versus like encoded based on our own human intuition for rules and structure.
1595920	1600800	And I think like maybe this would be fairly represented as a cop-out answer but I think
1600800	1605680	it depends a lot on the application. I think like when we're developing systems like this and
1605760	1613840	just trying to iterate through as many different hypotheses as you can quickly like choosing an
1613840	1620960	application and benchmarking and testing and seeing like what actually works is a go-to strategy for
1620960	1629200	us in terms of like well which parts should be fixed and are actually helpful to increase
1629200	1634640	reliability such that like we can use our human intuition for how this particular you know system
1634640	1641120	is built versus like well this is something that we we want uncertainty over that's like a really
1641120	1647680	important part of the learning process for us in terms of yeah that kind of iteration so I think
1647680	1655040	it probably depends on the application. Yeah it's um it it definitely depends on the application
1655040	1663680	it's also like it depends on where the actual challenge points are so we've got like outside of
1663680	1670000	this we've got sort of a few other things that we've released publicly that kind of look at this idea
1670000	1681200	of there being like external rules to the system and whether or not we can add those in.
1681200	1686560	So one of them is something called constrained generation where we sort of force the model
1686560	1693280	to only produce something valid and that's sort of quite a useful way of removing
1693280	1699680	one particular aspect of stress from the model which is that it may make sort of syntactically
1700320	1709360	or sort of somehow incoherent outputs that don't follow the rules and then we can then
1709360	1714800	focus with the rest of our energy we can then take that as given and focus with the rest of our
1714800	1722880	energy on improving the bit that we don't have rules for. So those sorts of things and sort of a
1722880	1728720	different version is trying to improve something by saying no you broke a rule we need to like go
1728720	1736880	back and make this sort of true so this kind of sort of chain of thought prompting type of idea.
1739040	1742960	So so yeah the the symbolic and the probabilistic
1745600	1748800	I think in our minds live very closely together as
1749520	1757200	two tools that don't completely solve the same problem and I think there's sort of in
1757200	1762160	in the world of I'm not sure how familiar anyone in the audience is with language modeling but like
1762160	1767120	in the world of language modeling before this sort of explosion of neural networks and artificial
1767120	1774080	intelligence type methods there was a lot of work on symbolics of language and grammars and
1774080	1780000	all of that sort of stuff and that work pushed quite a long way forward and this work is pushing
1780000	1784240	quite a long way forward and I suspect the next thing is going to involve them joining up again
1784880	1791840	because they each have good points they each have bad points and you know two wrongs don't
1791840	1798480	necessarily make a right but they can make the less wrong. Nice yeah recently we heard from
1798480	1803920	Elliot Murphy and talking about the neuro linguistics and about how the statistics of language
1803920	1809520	are not the rules of language you can always come up with a new expression that's never been
1809520	1812800	uttered that's not going to be in the training distribution or any distribution.
1813840	1818000	Okay I'll ask a question in chat from Upcycle Club they wrote
1819040	1824880	what are some of the key challenges associated with developing such Bayesian world models?
1824880	1832560	Hmm I think we've touched on a bunch of them the ones that are most top of mind for me right now
1832560	1838560	are the structure learning thing that came up so how do we understand like when to propose new
1838560	1845360	hypotheses and how to integrate those into the models and then yeah just figuring out like
1846480	1851840	yeah I guess this like proposal and evolution process of the nodes themselves since everything
1851840	1858000	else like the framework like works pretty automatically and in a reasonable way thank you
1858000	1865120	Bayes thank you to the development of language models but kind of moving from this like more
1866320	1872240	discrete case into a continuous case which like more fully represents the space that
1872240	1880560	we're learning over can be challenging. Yeah I would also say that like it's a sort of a
1880560	1888480	maxism that max maxism what on earth did I just say there's a there's a common saying let's go in
1888480	1894080	that direction there's a common saying in this world that um that sort of no model ever survives
1894080	1902080	its first encounter with data um and that that becomes true here as well so there's lots of like
1902080	1907440	as we've been building these things and using them we found lots of little spiky edge corners
1907440	1911920	with sort of making sure that the language world is actually doing what we want it to do
1911920	1916400	so there are a lot of questions in building these things around how do you actually test that the
1917360	1923840	components of it are actually working the way you want and then on like a broader level how do you
1924720	1933920	compare something that is fundamentally trying to solve a different problem to other methods so
1934720	1939840	we are solving a problem under the constraints that we want a fully auditable system
1942240	1948320	we could also solve all of these problems by a thing called in-context learning which is basically
1948320	1954720	putting the context into the prompt of a large language model and asking it the answer and that
1954720	1961440	also works especially when you've got things like GPT-4 which are just wonders and glories
1961600	1967920	um it works really well so then we come to the question of how do we actually make the case
1967920	1976320	from this from a like a bigger picture perspective can it be more than just a like can we find
1976320	1985680	benchmarks that reflect um the structural advantages of this approach over something
1985680	1994720	like in-context learning that don't come across as false so that's kind of like a a stranger answer
1994720	2000640	because it's not really about like actually developing the world model it's about sort of
2000640	2008320	convincing other people that it's a good idea um and that's you know that that is a thing that is
2008320	2015200	true of essentially all of the things on this slide as well they are all quite complex and odd
2015200	2021920	little methods um that you know there's a there's a degree to which well we definitely can solve this
2021920	2031440	an easier way um so what is the thing that what is the the the application or the benchmark where
2031440	2037040	we can say no if you do it the easier way you will fail at this measurable thing
2037680	2045360	very interesting um so you mentioned the self-evidencing
2047360	2055920	advantages of using world models that are self-evidencing rather than reward maximizing
2055920	2062240	for example so how do you see that playing out and I can connect it back to active inference of
2062240	2068320	course but how do you see this self-evidencing centrality play out in the kinds of models described
2068320	2073840	here yeah I think there are a couple reasons why it's so compelling to me uh and the first just has
2073840	2079040	to do with explainability right like it's really convincing to people to say like well why why did
2079040	2083840	we predict this why do we believe this well this is the actual real world data that we've observed
2083840	2090720	such that you know this this is the impact that that's had uh and then I think like uh I don't know
2090720	2095200	you hear a lot about like designing these really complicated reward functions which are often very
2095200	2101200	clever but which um often I feel like are close to being a pitfall because they very easily become
2101200	2107200	like disconnected from like the complex world that we're trying to model and so you end up in like
2107200	2114000	weird local maximums or minimums and um yeah you start like just kind of uh solving the problem
2114000	2119440	that you've designed versus like the problem which actually exists and so um I just have always
2119440	2124720	loved the idea that what we should be doing is um kind of self-evidencing and from an intuitive
2124720	2131200	sense that feels like what it feels like what an intelligence system should do uh yeah
2133840	2137760	yeah I actually don't have anything interesting to add to that I just agree with Fabie
2139760	2145760	that explainability and the capacity to explicitly reference previous data including
2145760	2151360	like leave one out so techniques from non-parametric statistics about the effect of adding in another
2151360	2159840	piece of data or removing a piece of data um and then just like to bring it to like a homeostatic
2159840	2166000	setting which is commonly considered an active inference like we're trying to be within a homeostatic
2166000	2175440	temperature range of 37 yes we could propose reward functions but as those start to include
2176160	2181280	open-endedness and exploration structure learning just like you described it Fabie like
2181280	2187440	we're solving the problem as designed rather than the actual question of the homeostatic
2187440	2195360	temperature and the sort of path of least action first principles physics grounded intelligence
2195360	2200560	perspective from active inference is like make it the kind of thing that measures itself at 37
2201280	2208080	and then as long as it is it is and when it isn't it's dead and that's the kind of mortal computing
2208880	2215440	crossover which is like outside of its zone of surprise it it's not just that it's getting
2215440	2223440	a bad grade in the class that is like a deeper failure signal than that and to understand okay
2223440	2230640	when is it a yellow flag when is it a red flag in terms of the new scientific literature coming in
2231680	2244080	those have plain straightforward ways to interpret that developing larger higher-order
2244160	2249120	apparatuses will never return to that kind of basal simplicity
2251920	2253120	yeah couldn't agree more
2255680	2263120	yeah I mean absolutely it also like the other thing that it can do quite well is deal with
2263920	2274880	essentially outlier studies so situations where you have a new piece of information that is
2275520	2282320	strongly conflicting with all the previous pieces of information and trying to sort of
2282320	2288880	work through what that really means and there's a like there's a degree to which
2289360	2297440	we can even sort of extend this process to multiple agents that have these belief systems
2297440	2304240	and then look at sort of consensus of experts or weighted consensus of experts so for instance
2304240	2311040	you could have like a weather vane type of situation where somebody really over indexes to every new
2311040	2316480	piece of information and you would do that with you know technically you do that with maybe a power
2317040	2323040	posterior type thing or you can have somebody who's built in strong priors in a particular
2323040	2332400	direction and you can then like take your consensus of artificial sort of decision making all of which
2332400	2340400	has within their universe well-reasoned updates to the data and then you can look and try and
2340400	2350560	work out what that swarm of experts can tell you and sort of do very empirical things like
2351520	2357200	try and you know work out which of these experts is doing well at a particular moment in time
2358640	2364480	because you know there could be there could be times when the world's very or the problem
2364480	2370800	you're solving is very chaotic in which case the over indexing expert would probably be a pretty
2370800	2378400	pretty solid bet while there are other times where sort of things are pretty stable and you
2378400	2383520	probably it would be possible that the sort of the more conservative expert is more
2385600	2392320	sort of empirically making good decisions and good recommendations so there's like a lot of
2392320	2401600	ways that we can not just like incorporate these sort of homeostasis ideas but we can also change
2401600	2408240	what that means for different agents and artificially like do that artificially and then combine them
2408240	2420720	together to try and get a almost like a like a blanking on the word but you know
2422160	2428080	a forecast under a sort of a hypothetical set of situations and we can actually sort of bring
2428080	2433360	those ideas of the world forward and see what happens when they sort of meet with actual information.
2433680	2441760	Yeah this angle of mixture of experts as it's sometimes called more in the language model space
2441760	2447760	or ecosystems of shared intelligence or diverse intelligences in the active inference area like
2447760	2456960	that's very interesting obviously has connections back to human teams and teams of beyond humans and
2456960	2468000	so on a lot of this is still text based so maybe you did or didn't mention what representation the
2468000	2476320	base graphs are but they're plain text like and there was a lot of discussion about bringing
2476320	2482720	from natural language scientific papers or however it is into a structured form and then the
2483280	2490160	explain method that you showed kind of taking the structured form and just giving a little
2490800	2500240	syntactic fluency so it looks human readable so how do you see that essence coming into play with
2500240	2508720	multimodal models and then with action in the world that isn't just developing the next text
2508720	2515120	token but a robotic actuator or modifying some other control element of the world.
2518000	2522720	Yeah that's a really good question. I honestly haven't thought much about multimodal stuff in
2522720	2529760	this particular context but I think the framework is general enough at this point such that it
2529760	2536720	it's definitely could support lots of different modalities. I'd be really curious to see how
2536720	2545360	this did with something like audio in particular. Yeah and then to your point about like yeah this
2545360	2552320	maybe like discrete versus continuous relationship I think I think that's like part of what we're
2552320	2557600	learning is how to go from like these long natural text documents to a system which is
2557600	2563840	appropriately discretizing our hypotheses such that we have these like meaningful explanations
2563840	2574000	like you mentioned so I think yeah I think like continuing to develop like robust ways of
2574560	2580880	surfacing those explanations is a big part of this as well like over time we're going to observe
2580880	2586720	lots and lots and lots of evidence how do we make sure like hypotheses don't get stale and how do we
2586720	2592480	use evidence to know when they are and things like this are part of that also I don't know if
2592480	2595440	that directly answered your question but that's some of the stuff that I've been thinking about
2595440	2608160	related to that. So in the I mean in examples like sort of moving towards robotics and sort of tech
2608160	2612720	video generation and image generation other sort of audio other sort of multimodality is
2612960	2624720	to be honest I think of these processes in general as enabling us like building a world
2624720	2632640	model to enable a sort of sequential decision process so if that decision process happens to
2632640	2637520	be should the robot turn left and that's what the sort of the decision process is it's it's
2638480	2644080	multimodal in like a very classical sense that you can put any type of decision framework over
2644080	2649920	the top but it's not sort of generatively multimodal I'm not saying write me a song that
2649920	2656720	sounds like Beyonce and a song that sounds like Beyonce comes out I think this this sort of this
2656720	2662080	sort of Bayesian world model layer is blocking towards that sort of thing but that's that's
2662080	2668640	really not sort of the aim of what we're trying to do it's also like within normal like our
2670240	2676240	almost I don't want to I don't want to say mantra or manifesto because that sounds culty
2676240	2684480	and no one wants to sound culty but like our basic aim is to always center like humans within our
2684480	2696480	process and so some of this multimodal stuff it's less clear where the human lives so for
2696480	2702240	instance like a video generation type thing where does the human live so keeping it at this abstraction
2702240	2707040	of sequential decision making then it's a decision that a human could do you know human with their
2707040	2714640	thumbs could be moving a like a robot around and doing that sort of stuff but yeah it's it's
2714640	2719280	really all about sort of controllability and auditability for us in sort of a sequential
2719280	2726160	decision process so to the extent that that sort of leads in its multimodal world that's
2727680	2732480	that that's sort of part of what we're doing and like to some some versions of multimodality
2732560	2742080	um is we're just not swearing in that particular space um yeah not a great answer but a long one
2745200	2754800	let them distill it down later um in the um auditability area it almost falls out to me
2754800	2759520	to be like a syntax of auditability in a semantics at the syntactic level just
2760480	2766800	tagging or versioning when a file comes in or when a given computation is executed that is
2766800	2774400	basically transfer across all settings and then where I see you honing in on with with this work
2774400	2783360	is kind of the semantic auditability which is actually how we compose our accounts
2784080	2791280	I would have driven but I decided to walk because this happens and so bringing that
2792480	2802800	different kind of trace to systems is gonna make it um what will it open up in science or
2802800	2812560	education or how do you see this sitting at a console somebody is at now and making this
2812560	2820560	different like over what timeline yeah I mean it's really quite nice for storytelling because
2820560	2828400	as you said you can say things precisely like well you know because we observed this thing
2828400	2833440	or because if we had observed something else you know like maybe you can even make statements which
2833440	2840960	are um yeah conditional in that sense uh I think it does like empower whoever is sitting in front
2841040	2846400	of this data to feel like really sure about again like that the reasoning engine that like
2846960	2851840	that went on uh which to me is is pretty different from what it feels like to sit in
2851840	2857040	front of chat gbt even though it's quite useful often um you know you you try the code and it
2857040	2862720	works or doesn't work or you like you know ask your friend is this really true um and that feels
2862720	2868480	pretty different to me from being able to to look at the evidences themselves and say like oh well
2868480	2873440	actually if this is the reason you think that I know that that evidence is is not true or you
2873440	2879840	know like you can bring your own human intuition or world model uh in terms of validating or um
2880400	2886000	yeah super imposing what you believe on top of what this system believes and so that makes it
2886000	2894160	really easy to make decisions um quickly I think there's also sort of a converse of this which is
2894240	2901120	that it also makes it clear which evidence was not used to make a decision uh and that can be quite
2901120	2907840	telling in these situations where you could be worried that a particular type of evidence isn't
2907840	2915360	being weighted correctly or isn't being um sort of formatted correctly so again like if this is a
2916000	2926080	sort of a like a system that builds an assistant um that sort of does surfaces all this information
2926080	2932080	and sort of makes a recommendation with reasoning for a person that person can then look and be like
2932080	2937440	and they know what the data is they can look at the deck and say you know why didn't you consider
2937440	2943680	the make of the car or why didn't you consider this or why didn't you consider that and they can
2943680	2951440	then use their understanding of what's not being prominently used by the model to
2952720	2959600	sort of sense test like it's it's sort of I mean in some sense that usage of it is a reformulation
2959600	2964080	of what Phoebe just said where you use your internal world model but it's like I think it's
2964240	2974560	important to know when evidence is being used and this is like I think you simply cannot get
2974560	2981440	um from from like a GPT type thing or any sort of like prompting type method we know for instance
2981440	2988720	that like um the order of the order that you submit your evidence in is probably going to matter
2988720	2995200	for a prompting based method okay that's obviously not true for a Bayesian update where we have this
2995200	3000720	sort of this this coherence principle where if you shuffle your data and enter it in a different
3000720	3008240	way you will get the same posterior up to computational artifacts um so so all of that
3008240	3013920	is like in my mind is just as important to order ability as the ability to write a report that says
3013920	3023600	I made this decision for these reasons yeah well that makes me think about this kind of view from
3023600	3028560	the inside interpretability where the rules help and also knowing what evidence is not used is
3028560	3034320	importance for compliance and knowing what information like in a healthcare setting was or wasn't
3034320	3047280	used um what about thermodynamics we heard about free energy boltzmann came up how do you see the
3048000	3055840	info thermo nexus what have we learned from the last hundred years of thermodynamics and
3055840	3066080	information theory and all of this and on the software or hardware side how is that kind of a
3066080	3077200	free energy nexus being used yeah i mean i i'm really excited about how all of this seems to be coming
3077200	3083520	together um i the free energy just keeps showing up in all of these exciting areas to me we have
3083520	3087840	like a book club for singular learning theory and like they talk all about free energy too and i
3087840	3093680	think some of those ideas are really exciting um i mean at normal i think like the thing that i would
3093680	3099280	highlight is like this idea of software hardware co-design um which is really special uh and so
3099280	3104320	we're trying to do this hard and fun dance towards each other where we're like thinking about these
3104320	3111200	new kinds of systems and how they might support each other and um empower each other and and yeah
3111200	3119120	how to build full stack systems um which is really challenging and and also really exciting um yeah
3119120	3124240	and i think like from like the first principles of thermodynamics perspective like like we're all
3124240	3131200	just uh yeah we're all kind of like mathematics and physics people at heart so like going taking
3131200	3136720	like uh you know all of what people have learned in language modeling and all of that like um very
3136720	3141440	much to heart as well like i think approaching whatever problem that we're facing from a first
3141440	3146080	principles how do physical systems work in the world what do we really what are the assumptions
3146080	3152160	we're really comfortable with uh and building up from there um is definitely our our natural mode
3153040	3156080	uh so i think that makes it easier to to start working together also
3158080	3165200	um it's also probably worth saying that we have a sort of a secondary not secondary a very different
3165200	3170800	stream of interest in thermodynamics as well which is the ways that we can use actual physical
3170800	3178240	thermo dynamical principles to build hardware that is specifically has sort of noise in it as a
3178240	3185040	first class citizen and because of that it is particularly well suited to probabilistic tasks
3185840	3192720	um and so we've we've built if you if anyone wants to look we have a blog i believe the URLs
3192720	3200240	blog dot normal computing dot ar um and amongst other things that are on it uh there is the very
3200240	3209200	first demonstration of using physical thermodynamic hardware to actually do computations um is the
3209200	3213680	computation the most vital computation that we will ever do it's inverting an eight by eight
3213680	3221360	matrix so no we can do that otherwise um but it it is sort of building up towards this idea that
3221360	3227040	we can use thermodynamics not just in our modeling and our understanding of the world but also in our
3227040	3237840	sort of low energy compute stack to actually realize these things um so i think i it's i think
3237840	3246560	it would be challenging to find a group of people on this earth who have more investment in thermodynamics
3246560	3255680	and don't work in a physics department uh because we have investment all the way through from
3255680	3261840	sort of active inference type things all the way down to this like this this hot thing goes there
3263920	3269120	which is kind of cool i'm not a physicist so i have but but that's my that's my understanding
3269120	3275520	of thermodynamics this hot thing goes there informative thing goes here hot thing over there
3276240	3289280	call it a day um yep that it's a really cool fusion with the kind of parsimony and elegance
3289280	3298240	and the aesthetic of math and physics and first principles and the different parsimony of pragmatism
3298800	3306560	with the actual material basis like of a synapse the size of the synapse and the kind
3306560	3317200	of stochasticity that that size alone um entails with like membranes and all of this those stochastic
3317200	3326320	aspects are leveraged for the compute the synapse is not simply a variance reducing machine and so
3329120	3339600	it's like both the platonic slash mathematical ish spirit it finds a common home in these real
3340480	3349600	simple physical demonstrations and um today it feels like there's a big gap between the um
3350320	3358320	mesoscale computational architectures that you described today that are very much running on
3358880	3366880	the kind of von Neumann architecture turing completeness paradigm and yet very tantalizingly
3366880	3377200	close like to a physical object that has a constrained rule de facto like only one thing
3377200	3386320	can come out of this at a time as long as the funnel is this wide and so bringing the rules
3386320	3395760	and the regularities of what we call physical things to bear with the fundamental and the
3395760	3408640	imposed constraints on informational spaces it's very cool directions um one other note about
3408640	3414800	just where active inference um an action plays a role is um and also you mentioned like the
3414800	3422480	hypothesis going stale or like sort of data being over relied on um in the proactive stance
3422480	3429760	where we're using expected free energy or something like it to to calculate future courses of action
3429760	3436000	over observations that we haven't seen yet moves that haven't been made yet there's an explicit
3436000	3445280	epistemic value and so that can be diagnosed and observed as a measure of where a given
3446160	3452400	computation is on the continuum between purely pragmatic value just constraint
3452400	3457840	satisfying and and realizing preferences and expectations and then the pure epistemic value
3457840	3465280	where all outcomes are good and the more information gain the better and then being able to take
3465280	3472080	control of that balance and know amidst changing situations again taking probabilistic or rule
3472080	3479200	based approach there to when epistemic and pragmatic like gas and break kind of come into play
3479200	3489520	these are very basal um control knobs or features in active inference that it's just
3490480	3497600	not going to show up at the 50th layer of scaling is all you need
3497600	3512640	yeah cool well do you have any other like thoughts or things you want to add or questions or where
3512640	3521200	things are heading for your works nothing to add at this moment but certainly uh excited to keep
3521200	3531440	in touch with this community and yeah collaborating yeah absolutely um and we sort of share I mean
3531440	3539120	we write papers and stuff but we mostly like we share most of what we do be at academic in the
3539120	3545680	sort of machine learning space or be it in the um sort of the physical hardware space uh on our
3545680	3553520	blog which is blog.normalcomputing.ai um and yeah thank you so much for inviting us it's been very fun
3554400	3563120	awesome thank you hope to speak again so peace bye thanks hi
3575680	3576180	you
