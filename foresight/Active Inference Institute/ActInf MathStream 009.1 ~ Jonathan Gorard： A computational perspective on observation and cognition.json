{"text": " Hello and welcome, everyone. This is Active Inference Mathstream 9.1 on March 5, 2024. We're here with Jonathan Gord and we'll be discussing a variety of topics yet to be determined or are they? So thank you for joining and to you for any introduction and we'll really look forward to everyone's comments and questions. So thanks again for joining to you. Okay, well, yeah, thanks very much, Daniel, for the introduction and for inviting me to be here on Active Inference. I'm looking forward to a very, very fun discussion. So I don't have anything especially prepared to talk about, which is probably a good thing because it means we'll be able to extend the kind of the unstructured part of this for as long as possible. But I think just to give a little bit of context, I want to talk about an area where I think some things that I've been working on, some collaborators that might have been working on, that might have some kind of intersection of interest with things that, you know, Active Inference type people might care about, right? So, and in particular, that concerns the relationship between kind of computation, observation, and cognition, and specifically using methods that come from category theory and topos theory and some other kind of branches of mathematics and theoretical computer science to understand the relationship between system, specifically the computational and algorithmic complexity of systems versus the computational algorithmic complexity of observers of those systems and how those things trade off between each other. So, so just to give a little bit of context to that, I want to show these are just some visuals from a paper that I put out about a year ago now, and that this kind of really defines this research program that I've been working on for the last year and a half in some form or another, which is looking at exactly this trade-off using category theoretic machinery. So, here's a specification of a Turing machine. This is just a simple deterministic computation. It's saying, you know, you have a Turing machine that has this head state and this tape state, and on the next step, you're going to replace the tape state with something that looks like this, the head state with something that looks like that, and you're going to scroll the Turing machine head left, or in this case, scroll it right, etc. So, this is just a, you know, specification of a very simple computation. I think this is a two-state, two-color Turing machine on a simple, you know, one-dimensional tape. It's about as simple a computation as you could define. So, if you run that thing for some initial condition, you'll get an evolution that looks like this. And so, right now, this is just a purely deterministic, you know, single-path evolution. But from this, we can construct, we can build a mathematical structure. Namely, we can build a category. So, and the rules for how we build that category are very simple. So, you know, each arrow here is some simple computation, some application of the Turing machine transition function. And then what we can do is we can say, well, any time we have two arrows that are laid end-to-end like this, we can compose them together to create a third arrow that goes like that. I may even have a picture, yes, like this. So, you know, we have a computation f that takes us from x to y, a computation g that takes us from y to z, and we then we obtain a composite computation g compose f that takes us directly from x to z. And we also add some additional edges, some additional arrows on each state itself, a sort of identity, an identity operation that maps the computational state directly to itself. And so, this combined with some axioms of associativity and identity forms a category of elementary computations. So, this is a very, very simple example. But what I want to try and build up towards and kind of pump your intuition for is a category which I call comp, which is a category whose objects are all essentially the class of all data structures and whose arrows or morphisms are the class of all elementary computations. So, you start by just applying, you know, all possible computations or, you know, in this case, for the case of Turing machines, all possible, you know, Turing machine transition functions. And then you do this closure operation where you, you know, where you essentially do what I'm doing here, but you, you know, you allow those elementary computations to be composed together in arbitrary ways. And so, that gives you effectively a class of all possible programs. So, this category contains not only all possible data structures as objects, but all possible programs as morphisms. And this is a very rich category with some very interesting algebraic structure that we'll kind of, again, I'm sure we'll allude to in our subsequent discussion. But in a sense, when we do this, when we do this operation of taking what mathematically we call a transitive closure, right, where we allow two elementary computations to be composed together to produce a third, we are essentially kind of neglecting considerations of computational complexity, right, because, you know, this arrow here might correspond to one application of the Turing machine transition function, this arrow might correspond to another application of the transition function, but this composite arrow correspond might correspond to two applications of the transition function. And so, somehow, when we allow arrows or morphisms to be composed in this way, we're neglecting considerations of the complexity of operations. So, the question then is, you know, could we imagine constructing a generalization of category theory, which takes into account computational complexity. So, here's an example of how that would look, right. So, here you can see every edge, every morphism has been tagged with certain computational complexity information, in particular, it's been tagged with a number specifying what is the minimum number of applications of my transition function, what's the minimum number of elementary computations that I need in order to evolve from this data structure to this data structure. So, here, to go from here to here, it's just one, to go from here to here, it's one, to go from here to here, it's one, etc. But to go from here to here directly, it would be three, to go from here to here directly, it would be two. And, you know, just for convention, we say that the identity, the identity computation, the trivial computation always has complexity zero. And then this, so this is, again, a fairly simple mathematical structure, and you can construct this, again, using purely category theoretic technology by building a particular functor from the category of computations, and from the category of data structures and computations to a what's called a discrete co-borderism category. And again, we might discuss that later on if people are interested, but let me not get too bogged down into the technical details of how we do that. But once you've got this, it gives us immediately a very nice way of characterizing phenomena like computational irreducibility. So, there is this idea that has existed in some form or another since the very early days of theoretical computer science, since the days of, you know, girdle and turing and post and church and so on, but was given this term computational irreducibility by Stephen Wolfram, where the idea is essentially that you just, you know, intuitively, you describe a computation as being irreducible, or, you know, the result of the computation as being irreducibly complex, if it's not possible to shortcut it in any way, right? So, where, you know, it takes that computation takes a certain number of steps, and there does not exist a shorter computation that would give you the same answer in less time. And one of the nice features of thinking about computations and their complexity algebraically like this is that it gives you a purely algebraic characterization of irreducibility. In particular, what it says is that irreducible computations are ones for which the computational complexity acts additive, purely additively under composition. So, if it's the case that if we compose, say, two computations of complexity one together, if the resulting composite takes, you know, has complexity two, then it's an irreducible computation. If it has complexity less than two, like one, then that means that we could have jumped directly from the input to the output without having to pass through the two elementary computations that made it up. So, that would be an example of a reducible computation. So, reducible computations are ones whose complexities compose sub-additively in this category theoretic sense. And, okay, so here's an illustration of showing what intermediate computational states you had to go through in order to get from one data structure to another data structure. So, to go from here to here, you had to go through steps one to two. To go from here to here, you had to go through steps one, two, and one, two, three, and four, et cetera. So, you can build up a kind of complete algebra of complexity this way, which has some nice properties, which, again, I can talk about, but let me not get too bogged down in mathematical details right now. But here's the thing I really want to talk about, which is what happens when you go to multi-way systems. What happens when you go to non-deterministic computations? So, now, imagine having, instead of just a Turing machine with a single rule, a single transition function that just evolves deterministically with a single thread of time, now imagine having a Turing machine that has, say, two transition functions, like this one and this one. And so, at any given point, it can apply one of the two. And so, now, evolution, instead of just being a single path, becomes this kind of branching structure, which, if we didn't have any merging, would be a tree, but because we are merging equivalent states, it's actually just a kind of more general directed graph. And so, it looks like this, and this we call a multi-way system. And so, we can build a category out of these multi-way systems as well. We can build a category using exactly the same rules. So, again, we do this transitive closure operation. So, we add an edge for every possible composition of these elementary computations and an identity edge that maps every data structure to itself. But it turns out this category has even more structure than the single-way system that we showed previously, because now, it's possible to compose computations not just sequentially in time using ordinary composition, but it's possible to compose them in parallel across what is sometimes referred to as branchial space. So, essentially, you're saying instead of, you know, the ordinary morphism composition that I showed previously is essentially saying, you know, I apply this elementary computation, then this elementary computation sequentially. Whereas this parallel composition is saying, I apply this computation and this computation in parallel to the same data structure. And so, that parallel operation is what causes these branches, right? Effectively, when you have two threads of time that are branching from the same state, like here, that's arising because we have chosen to apply this elementary computation and this elementary computation together in parallel rather than sequentialized in time. And so, here you can see this parallelization indicated using what's referred to as a branchial decomposition, which is just a kind of a visual way of decomposing what's going on between these different threads of time. And again, there's a purely algebraic characterization of what's going on here, which is that what we've done is we've taken our simple category that we started with, and we've equipped it with a tensor product structure. And so, it's become what we fancily call a monoidal category or actually a symmetric monoidal category. So, the tensor, so we now have these two operations. We have sequential composition in time, and we have this tensor product operation, which is a parallel composition in branchial space. And just like we can have, just like before, where we equipped our edges, our morphisms with certain computational complexity information, we can do the same thing, and we described how those complexities composed sequentially in time. We can do the same thing and describe how the complexities compose in parallel as one composes morphisms in branchial space. And so, this allows one by exactly the same token to quantify multi-computational irreducibility rather than just computational irreducibility. So, now, multi-computational irreducibility becomes a measure of how additive or sub-additive your time complexities are when you compose them in parallel through the tensor products rather than just in sequence through standard morphism composition. Okay, but I promise I am cut, and so here's an analogous diagram to the one I showed before showing all the kind of intermediate steps that are being applied when one constructs computations or indeed multi-computations by composing elementary computations both sequentially in time and in parallel in branchial space. Now, but I promise I am the point that I'm trying to get to is that it turns out that in addition to just being a useful way to think about computational complexity theory and to formulate complexity classes like, you know, polynomial time on non-deterministic polynomial time, etc., it turns out this is also an interesting way to think about the role of observation in sort of computational models of reality. Because so here's where I'm going to get a little bit philosophical, and I don't immediately have a slide or a graphic that I can show to illustrate this point. But so when we think about modeling a system computationally, one has to bear in mind that there are really two computations going on, right? There's the computation that the system is itself performing, and then there's the computation that the observer, the person who is measuring that system and concluding things from it, there's the computation that they are performing. And somehow, you know, so when we construct models of reality or when we construct models of systems, you know, and we want to describe kind of at a meta level what we're doing in computational terms, there's our own computation that, you know, that's going on inside our own internal representation of the world. And then there's presumably some external computation that's going on outside. And then when we make observations and when we make measurements, when we construct theoretical models, what we're doing is we're somehow constructing some kind of encoding function that allows us to take a concrete physical state of the system we're observing and encode it as some abstract state of the internal model that we have of what's going on. And that's all very well. But then one, but then now we don't just have one computation to care about, we have three, right? We've got the computation of the system, computation of the observer, and the computation of this encoding function computation that's responsible for their, for their, you know, the interface between their internal model of the world and the external reality. And the computational complexities of these computations into play in an extremely interesting way. And so the, you know, part of the reason for trying to develop this algebraic semantics for thinking about computational complexity and multi computational complexity was to try to give one a systematic way to reason about exactly this three-way interplay between systems, observers, and encoding functions. And so in particular, when we make when an observer makes a model of the world, one thing that they're doing is that they are, you know, for any, for any model, isn't just, you know, a complete description of reality, there's a certain amount of coarse-graining, right? There's a certain amount of taking a bunch of states that in the system itself are distinguished, but in the internal model are treated as the same, they're kind of, you know, they're cast in the same bucket. So in some sense, you know, how coarse a model is, is a measure of how much the encoding function fails to be subjective, right? And so again, there's a kind of algebraic or category theoretic characterization of what's going on, that, you know, the fewer of your morphisms are epimorphisms, the more coarse your model is, the more abstract or idealized your model of reality is. And so then the interesting thing is that this characterization of multi computational irreducibility, this measure of how additive or sub-additive your complexities are, as you compose them together in parallel, gives you a measure of the relative complexity of the evolution function, that is the function that evolves your computation forwards in time, versus the equivalence function, that is the function that declares that two computational states, two data structures, are to be treated as equivalent. And that interplay, I claim, is a kind of abstract meta way of thinking about the interplay between the computation of systems versus the computation of observers, because, you know, so in a sense, the role of the system is to evolve forwards in time, whereas the role of the observer is to take states in the system that are distinguished in reality and say, you know, subject to my idealized model, I'm going to treat these as the same. So the system is defining the evolution function, but the observer is defining this equivalence function. And so then the tradeoff in their complexities becomes exactly a tradeoff between what are the algebraic rules that describe the complexities as they compose sequentially, versus the algebraic rules that describe the complexities as they compose under this tensor product operation. And so I've shown this in particular for Turing machine systems, but this is a very general kind of algebraic semantics, you can apply it to hypergraphs, you can apply it to combinators, lambda calculus, doesn't matter. In a sense, there is just one category up to isomorphism of data structures and computations, and there are simply many different ways of parameterizing what that category is doing through things like Turing machines or hypergraphs or whatever. The algebraic formalism transcends the particular details of the computations that one's dealing with. And so yeah, as I say, what one ends up with is, I think, a fairly general formalism for thinking about the interplay between observers and the systems that they observe. And that gives one a, I promise I'll stop monologuing in a moment and we'll try and pick apart what I'm really talking about here. But so I'll just conclude with, you know, once one has that algebraic semantics, a whole bunch of things which I think previously would have been, at least to me, previously seemed like kind of fundamental confusions about, you know, how scientific observation works and how it interplays with computational models, those confusions kind of become much easier to clarify once you think about it in this kind of more compositional way. So to give a very simple example, or kind of very degenerate example, you can, you know, within this algebraic semantics, you can effectively trade off the computational complexity of the system for the computational complexity of the observer, right? So you can have, you can have kind of, in effect, two degenerate cases. You can have the case where the system itself has a completely trivial evolution function. The system itself has, you know, is doing something completely elementary in its, in how it evolves. But then the observer has some incredibly complicated equivalence function that makes the system look like it's doing something really complicated, even though what it's actually doing is something very simple. And so then you, so you have the phenomenon where actually kind of all of the complexity is in the eye of the, is in the eye of the observer. You can also have the other degenerate case where the observer is doing something absolutely trivial, where the, you know, the encoding function or the observer's own internal representation is just an identity function or something. So there's no complexity there, but the system is doing something incredibly complex. It's doing some, some totally, some really sophisticated universal computation. And so that will also appear very complex to that observer. And so, and you can also have any kind of, any intermediate, you know, there's this vast interstitial space between these two extremes. And so one thing that's kind of always, one sort of philosophical problem that I've always kind of been interested in ever since I was a kid, which is this, this sort of, this tension between empiricism versus rationalism, right? You know, the question of, you know, on the white, if you look back at the history of, where, you know, early European philosophy, or, you know, it's certainly Western, you know, Western post enlightenment philosophy, you had people like, you know, Descartes and Leibniz and, and so on, who were, you know, in a more sophisticated way, Bishop Barkley with subjective immaterialism, who were trying to push for this idea that, oh, you know, all the, all the sophistication is what's going on inside the observer's head. And, you know, what goes on in reality is somehow secondary. And then you had people like, you know, Locke and Hume and the empiricists, who were saying, no, no, we should try and get the observer as much out of the picture as possible. And we should say all the sophistication is going on kind of in the external world. And this in one, you know, one nice consequence of this is it gives one, one nice consequence of this formalism is it gives one actually an algebraic way of kind of parameterizing this spectrum from rationalism to empiricism, right, that, that you can, you can choose the rationalist extreme where, you know, you know, you just have some, some space of all possible computations and, and the observer is doing all of the work to try to narrow down to a particular one, or you can have the kind of empiricist extreme where, you know, the observer is a completely elementary system. And, you know, and everything and everything they observe is just being built up from a kind of bottom up construction, or you can have anything in between. And in a sense, we now have, I think, the beginnings of a mathematical theory that explain, that's able to explain how those complexities trade off in a very direct way. So I think there's potentially places of mutual interest there in kind of the, in thinking about, yeah, as I say, cognition, observation, measurement, scientific modeling, and so on, in fundamentally computational terms. So I think that hopefully that will provide some, some context for, for a discussion. Thank you. Great opening. There's so many places to spin in and jump through. I guess I'll start with the two things I wrote down were unity is plural and at minimum two and beauty is in the eye of the beholder and the way that these kinds of pieces of timeless wisdom that describe that fundamentally relational component to observers in systems, which are not all kinds of systems per se, but those kinds of systems, those things are true for, and then the way in which along the formalism that you described with the system observer encoding freeway partition, and then the way that in free energy principle and the particular physics, that interface gets broken out from the agent's perspective into the incoming sensory and the outgoing action. So then that results in the fourfold particular partition. So maybe just to kind of, well, there, how do we partition the, from a category theory perspective or however, the action perception loop or the engagement loop? Like, how do we make a topology or compare a contrast different topologies and flows over this kind of seemingly pervasive or universal interface like concept? That's a fascinating question. So I don't have the answer and this is maybe a place where, where both you Daniel and perhaps David may have, you know, useful perspectives on this because, you know, I'm, I'm, you know, I read Carl's work a few years back and so I have some familiarity with the terms, but I'm by no means a kind of, you know, an expert on free energy principle or, you know, active inference and those kinds of things. But I think it's a very good point that you raise. And so, you know, I should begin by just being honest and say that, you know, everything I'm doing, you know, all that I just described is of course an idealization and that, you know, in reality, you know, in particular, it's an idealization, which I think you were very right, Daniel, to kind of pick up on. It's an idealization in which we say the observer is completely kind of non-interacting with the world somehow, right? That, you know, in a sense that there's just input coming in and nothing, nothing coming out. But of course, we know that's not really how observation works. Observation is necessarily a kind of two-way process. And so what's needed is not just this kind of very clean algebraic semantics that I've described here, which assumes that there's a essentially a one-way function from the world to the observer, but actually something more like a kind of second-order cybernetics description of, you know, what's really going on where you have, you know, first-order and second-order interactions, whereas exactly as you say, you can get these sort of feedback loops from observation to action and back again, which are probably, which is, I mean, still an idealization, but probably a more realistic idealization for how real observers and real measurement apparatus work. So I just want to begin by saying that I don't know, right? And, you know, the question of how this formalism into plays with things like second-order cybernetics and other areas where I know these kinds of questions have been explored, that's something I'm very interested to find out about, you know, going forward. But I think, hmm, okay, so, yeah, so at some point, maybe you could help me understand, you know, potentially where things might fit in with, you know, with the kind of broader active inference framework. I'm not sure I necessarily have that much more to comment on than that. Yeah, other than to say that, you know, in a sense, okay, so maybe, you know, one further comment is that, because, I mean, you asked specifically about how the kind of compositional category theoretic perspective might be useful. So I don't think category theory in itself is going to be the complete answer. I think it will be category theory augmented with some other things, computational complexity, probably second-order cybernetics, and some other things that I may not be aware of. But one place where I think that viewpoint is useful, at least on a philosophical level, is the idea that comes about, that you really obtain by studying mathematical structures in a category theoretic way, which is that the identity of something, you can define it both in terms of its intrinsic properties, or you can define it in terms of, you know, the stuff that you can do to it, right? So, you know, this was really the transition that happened in the foundations of mathematics as a result of people like Samuel Alenberg and Saunders-McLean. So, you know, category theory has its origins in this sort of slightly obstruous branch of algebraic topology. It was, you know, initially developed by people like Alexander Gordon-Deek and John P. S. Sear for doing homological algebra for, you know, for reasoning about sort of the algebraic structure of topological spaces. But then later, in the, I think, 1960s, 1970s, these two American mathematicians, Alenberg and McLean, realized that it was useful not just for thinking about topology, but for thinking about kind of mathematical structure in general. And then later on applied category theorists started saying, well, maybe it's useful for just thinking about structure in general. But, you know, the key kind of conceptual or philosophical shift that it imposes is, you know, historically, thanks to the work of people like Kantor and Frege and Russell and so on, people have thought about mathematical structures in the foundations of mathematics as, you know, in terms of set theory. And the idea in set theory is you have this, you know, things like the axiom of extension that effectively say set is defined by what's inside it, right? So in other words, you know, you, a mathematical structure obtains its identity by, you know, you break it apart and you look at what's inside. In category theory, it's a completely different view. The view instead is you say, well, no, you can't look inside, you know, it's a fundamental rule of category theory that you can't look inside an object. You know, it's internal structure, if it has any sort of outer bounds to you. And instead, you give that object identity in terms of how it relates to other objects of the same type, right? So in other words, you know, you can ask, you know, what can I do to this? What functions can I apply to it? What functions can I apply to something else that map into this? So, you know, if I want to define, I don't know, the real numbers or the integers or something in the set theory, you know, from a set theory perspective, you would say that the, you know, the essence of the real numbers are all the numbers that are inside that set or all the numbers that are inside are. Whereas the category theory perspective is no, the essence of the real numbers are all the functions that you can define that take real numbers to some other number system or real numbers to themselves or that take some other number system into the real numbers or et cetera. And, you know, some of the deepest results in category theory, like the Oneida Lemma and other things, are telling one in some very precise sense that these two perspectives are really the same at some fundamental level, but that, you know, identifying an object based on its internal structure, based on breaking it apart and asking what's inside and identifying an object by asking, what can I do to it? And what, you know, what can this object be transformed into and what things can be transformed into this object? Those give you exactly the same information. It's far from obvious that that's true, you know, the Oneida Lemma is a very kind of, one of those results where you can never quite work out of its obvious or if it's incredibly, you know, mysterious. But I tend to fall on the side that it's incredibly mysterious. It's far from evidence that those two perspectives would really be the same. And yet, the point you're making Daniel, I think, is that in a sense, historical ways of thinking about scientific observation have tended towards the set theoretic viewpoint, tended towards the viewpoint that we understand systems based on kind of breaking them apart into constituent components. But perhaps a more realistic view is something more like the category theory perspective, where we say, you know, I understand a system by interacting with it, right? By asking, what can I do to it? And how does it behave as I, when I perform certain operations to it? And that's a fundamentally, you know, that's a fundamentally two-way process that involves not just passive observation, but also kind of active participation. And somehow we need to develop a formalism that kind of incorporates those two elements. And maybe, you know, maybe it already exists. And it exists in this large literature tree of which I'm largely unaware. That's partly why I want to be here to try and find out, you know, what things I missed, so to speak. Well, all right, a few points. Self-evident is far from evident. Also, I tend to the mysterious, which is to say, saying more with less, especially for these frameworks, because they're less opinionated, so that their space of internal semantics can be larger. And then that description that you provided with the relationship between the set and the category theory. So I kind of summarized it as set is to essential inclusion as category is to relational function. Now, if our concept of organismality or of action in the niche is constructive compositional material, then we are looking for, like, what is in or out? Is the microbiome in? Is the pheromone in the ant colony in or out of that thing? Because it's looking for, like, a static material answer. And then in contrast, the other side of that coin highlights the dynamic, like, whatever it is that self-organizing of the tornado is the tornado, whatever it is that self-organizing for the ant is the ant. And then also this, like, hint slash mobius strip or something that those two in the moment are indistinguishable. And yet systems that we choose to define one way or another, or keeping both open, those design decisions do make all the difference, even if for real systems, as they're observed, there's indistinguishability. Right, right. I think that's a very important point. And one which, I mean, this is a, it's always a kind of concern I have whenever I start thinking about, you know, embodied cognition or, you know, extended phenotype type ideas, right? But in a sense, you know, if what one is trying to do here is construct a kind of formalistic model of observation or of cognition or something, then as a kind of first order approximation, one has to start by somehow decomposing the world into observers and systems. But of course, that, you know, we know that that decomposition is somehow arbitrarily imposed, right? And that, you know, if you take these things to their extremes, and you say, you know, you allow essentially everything that the agent is interacting with to be considered, you know, like, not just the microbiome, as you say, but also, you know, tools that they construct or environments in which they exist and so on. If you, as you start to consider all of that to be a component of that organism, you know, of that agent's phenotype, which is a completely reasonable thing to do, then, and you start to, you know, you start to say, okay, well, their cognitive processes are not just localized to their brain or their spinal column, but are kind of somehow extended to, you know, the computers they use, the paper they write on, the books they read, et cetera. Again, perfectly reasonable thing to do and sort of somehow more descriptive of what's really going on. But my fear is always, if you take that too far, then, you know, you end up destroying the whole assumption that the idealization was based on, which is that you can neatly decompose, you know, the world into observers and systems. And so, I always get a bit nervous when thinking about that, that it's like, yes, you know, in a sense, you know, we know this is an approximation and we know that that approximation is not really true, but how, you know, how much can you afford to sort of loosen your grip on that approximation before the whole thing just kind of falls apart. I don't really know the answer to that question. But I think it's an interesting one. Yeah. How about ask some questions from chat and then give your first thoughts and then we'll see maybe where that kind of lands with further questions or how it connects to active inference. So that sounds good. Oh, by the way, should I keep my screen share on or should I? Yeah, we might want to go to a figure. So it's fine. Okay. Yeah. All right. Quantum Bell wrote, how does this help us reason about causality? That's a fascinating question. Okay. So that's that's another major aspect of, you know, why I think this research program is exciting, because so again, this is something where I'm interested to get the kind of active inference perspective, because I know this again, it's a topic in which much has been written and I'm largely ignorant. But yes, so one question you could ask is, yeah, if you have a description of a computation like this, like, let's go back up to the Turing machine case, the single way Turing machine case that's relatively easy to analyze, although still far from obvious what's going on. So suppose you have a computation of this kind, and you want to ask, what is its causal structure? In other words, you know, for each edge, each time I'm applying this Turing machine transition function, can I construct some kind of graph, you know, some some directed graph representation that tells me how these events are linked together. So in the, within the original research program, the so-called Wolfram model research program that kind of started a lot of these investigations, we were looking at this all the time, right? We were looking at kind of, you know, taking computations and looking at that causal structure and trying to, you know, infer things about what was going on about the, you know, the semantics of the computation based on causal relationships. And at a certain point, I started to realize, and I think other people had realized this before I did, but I'm often slow to pick these things up. I, I, I and other people started to realize that the notion of causality we were using was kind of nonsense. I mean, it was not completely hopeless, but it wasn't really causality, or it couldn't really be called causality in any, in any definite sense. So what do I mean by that? So first of all, it was a very technical problem. So if you're looking at something like a Turing machine evolution or a hypergraphic writing system as we were, then there's a very tempting and apparently obvious natural definition of causality that you can use, which is to ask, you know, when you split the world up into events that take, you know, some part of your data structure as input and output, you know, some other part of a data structure as output, then you can very easily ask, well, does the output of one event intersect with the input of another event? So if I show the hypergraph example, it's perhaps easier to see. So you have a hypergraphy writing rule that looks like this, right? So you know, you have, you say, if I have a piece of hypergraph that looks like that, I replace it with another piece of hypergraph that looks like this. So at each, each time you apply an event, you can think of that event as, you know, ingesting certain hyper edges and kind of, you know, replacing them with others. So you can divide it up into a sort of the, the, the input hyper edges that are being ingested versus the output hyper edges that are being produced. And so then you can ask, well, do the output, did I use, did I subsequently ingest in some future event hyper edges that were output in some previous event? Well, if the answer is yes, then pretty obviously that future event couldn't have occurred unless the previous event had already occurred. So then you could say, well, then one of those events causes the other. So in general, you could say that two, you know, an event A causes event B, if it's the case that the output, that the collection of tokens that was produced in the output of event A has a non-zero intersection with the collection tokens that were ingested as part of the input of event B. And that's a very tempting, very natural definition of kind of causality in these systems. Turns out it doesn't really work. I mean, it works pretty well, but there are cases in which it fails and it fails pretty spectacularly. And so the kind of canonical case where it fails spectacularly is that you can have events that don't actually do anything, right? You can have events that just kind of touch an edge, touch a token and output it again unchanged, but maybe, you know, it modifies the name, it modifies the identifier, but it doesn't actually change anything about the structure of the hypergraph or the Turing machine state or whatever. So pretty obviously that event doesn't matter. It shouldn't be causally related to anything in the future. But because it ingested the edge and then didn't do anything, you know, did some identity operation, but then, you know, produced it in the output again, it will kind of register as being causally related to any future event that used that edge, even though it didn't make any difference. That's just one very obvious example. There are other cases where it became clear that whatever this thing was, whatever this algorithm was detecting, it wasn't really causality. So I tried to think about, you know, what's a more sensible definition of causality. And I started working on things to do with, you know, a slightly kind of blockchain inspired ideas where you say, okay, well, rather than just arbitrarily assigning, you know, identifiers to these tokens every time they're created, what if I recursively construct the identifier of the token based on its causal history. So in other words, each token, like each hyper edge or each state in my each, each tape square on my Turing machine tape, I, the identifier is not just some random number that gets generated by my algorithm, but instead its identifier is a directed graph representation of its complete causal history. Well, then kind of recursively, it's identified can only change if the causal history was updated. And so you don't end up with these kind of spurious causal relations that I described before. So that seemed like one tempting way of resolving this problem. But then I realized, actually, there's a much more fundamental problem. There's a problem, there's a philosophical problem with the way that we're thinking about causality, which is that it's not really, you know, so, okay, this is a long tangent, which I'll talk about a little bit, but I won't get into the complete details unless people are interested. But I ended up T I ended up talking to a bunch of philosophers who, you know, who worked on causality and people who worked on parallel programming and quantum information theory and other places where causality was, was, was studied and asked them kind of basically what, what is, what, what do you mean by causality? What is causality? What is this thing we're trying to define? And in some form or another, all of the definitions boiled down to, you know, event A causes event B if had event A not occurred, then event B would not have occurred. So in other words, you need a counterfactual, you need some possible history, some possible world in which event A didn't happen. But if you're reasoning about a purely deterministic event system like a Turing machine, that doesn't make any sense. Because if you're, you know, if you, if you have a single Turing machine transition function, there is no possible world in which that transition function didn't fire in that particular way. Because if it didn't fire in that particular way, you would not be reasoning about that Turing machine anymore, you'd be reasoning about different Turing machine. So suddenly this, you know, to make sense of these notions of causality, you need a kind of Leibnizian, you know, modality is view of reality that just doesn't exist for these deterministic computational systems. So either you need to define computation, you need to define causality only at the multiway level, only at the level where you have many computations or possibly all computations happening in parallel, and then you can define causality relative to all of the, you know, relative between them, or you were kind of posed, right? There wasn't really, you know, that seemed like the only kind of the only get out, or you'd need some fundamentally new philosophical theory of causality that I was not qualified to produce. And so that's, again, part of the reason, part of what motivated this general research program, which is trying to think about this category of not just a single computation, and with a single sequence of data structures, because it's clear that you can't, you know, philosophically meaningful way assign causality in that case, but rather, you know, looking at the algebraic structure of the category of all possible computations and all possible data structures. And in that situation, there is a notion of causality you can equip that with, and there's a nice, again, a nice mathematical description in terms of, in terms of weak two categories and so on, which again, I can talk about if people are interested. But yeah, so it's clear that these things are very deeply related that this sort of theory of the category of computations and data structures, and the theory of how you assign causality in a meaningful way, are very deeply related. And I'll just mention one other thing on that topic, which is again, just the area which I find quite exciting, because it's an unexpected spin out of this program, which is that, so once you have a way of consistently applying causality at a per token level in these systems, it gives you a way of vastly generalizing what computation is. And you can, in particular, you can get, you can derive something which I call, well, which I'm provisionally calling covariant computation, although it should probably have a better name than that, which is, so in our traditional kind of Turing Church type models of computation, computation is a purely forwards in time operation. So at every point, you know, you have a complete data structure, and computation is about deriving what is the next state of that data structure. So in a sense, it's only a forwards in time thing. You might be able to kind of reconstruct the initial conditions based on some subsequent data structures, you might be able to go backwards in time, but that's essentially what you're doing. But then you could imagine, okay, suppose I don't know the complete state of my data structure, I know instead, I know one part of my data structure, but I know it, I know its history throughout all of time. So you could imagine, say an elementary cellular automaton or a Turing machine tape, where you'd know nothing about the tape, but you know the state of one cell, and you know it, you know, throughout all of time. And then the question is, what can you infer about the rest of the computation? And it turns out that for those kind of structured array type systems, you can infer a lot, you can actually evolve the system, not forwards in time, but sideways in space, and obtain a kind of causal diamond that so okay, the top left, top right, bottom left, bottom right corners are undetermined. But everything inside that diamond can be determined just from that one row, or that one column that you know, you know, sort of extended throughout time. And so, you know, and that's a fundamentally different notion of computation. So it's a version of computation, which is not forwards in time, but sideways in space. But you can also have version of computation that is sideways and branch your space where you know, you know, one complete state, you know, you know, one branch of the multiway system extended throughout time. And then the question is, what else can you infer about the, you know, but the rest of the multiway system just from that one branch? And again, the answer turns out to be, you can infer a lot, but not everything. And so, just like in the reason I call this covariant computation is because it's very analogous to what happens in relativity. So in relativity, once you buy into this notion of general covariance and the notion that space and time are kind of fundamentally the same thing, then you have to somehow relax your traditional view of what dynamical systems do, which is, you know, we typically think of systems as evolving, you know, you have a snapshot of your initial of your data at, you know, localized on a, you know, for a particular state of space, you know, on a particular space like hypersurface. And then your laws of physics tell you how that space like hypersurface evolves forwards or backwards in time. But in a covariant picture of physics, then you must also allow for, you know, your initial data to be defined on a time like hypersurface, and you for you to be able to evolve that time like hypersurface sideways in space or mixtures of the two and so on. And so it's clear that there's a very general, a vast generalization of ordinary computation theory that you can, that you can construct that's kind of physics inspired in that sense, in which you can have mixing of space time and kind of multiway directions in a completely consistent way. But to make those things consistent, you need to have a definite way of assigning causality. You need to because, you know, any computation that you do, even if it permutes the directions of space and time and branching space and so on, must always somehow preserve the causal structure, has to respect the causal structure of what's going on, or else it's inconsistent. And so this question of how you construct a covariant theory of computation is, it turns out, intimately related to the question of how you take this category of computations and data structures and equip it with a consistent notion of causality. So very interesting question, we could talk about that at great length. Okay, to follow with a few pieces, it's very related to Professor Mike Levin's notion of poly computing, and about the necessity for a causality concept to be created or deployed when the question arises, was that me? Was that action or change due to me? Also, connectivities, even just in the neuroimaging setting, which is kind of the cradle from which active inference and free energy principle arise from, it's really important to distinguish the functional, effective, and anatomical connectivities. And that was one of the points that Toby St. Clair Smith made in his dissertation, which is that a lot of times the Bayesian graphs don't convey all of the necessary and sufficient information to make the reproducible computation, which is one of those kind of what's missing from the graph is what motivated a lot of the category theory developments in active inference, as well as some of the formal ontological works with Sumo and Dave here and Adam Pease, because implementing modal and higher order logics is really important if it's a possible situation where a mind can have a perspective on a mind and all these things like that. Then the ant-turing tape, the tape is the pheromone, and then the decision space is the nest mate's scrolling. So when you had a deterministic turing tape, that was like a movie because the nest mate couldn't make any choices, except for internal action, which is kind of side topic, but it couldn't make any choices on the tape. Whereas when there's a multi-way, which is basically in active inference, what we talk about in terms of affordances and the policy space and the temporal depth of planning and counterfactuals on action and action-conditioned world transition states like the B matrix, all those kinds of topics come into play, because if you want to have a causal buffer or grasp on what is it that something that could do otherwise does, what does it cause to do when it does or doesn't do otherwise, you need something like a deterministic handle around what could be a probabilistic or deterministic, but at least multi-way map of some kind of cognitive territory. That's a very, very interesting perspective. Again, I'm betraying my ignorance of active inference theory here, but it sounds almost like, when you have this kind of interplay between sort of epistemic versus pragmatic, there's two aspects of how this kind of speculative part of cognition works. This is something which I thought about in a completely different context in relation to things like quantum information theory, but I wonder if there's a potential overlap there. There are certain situations when thinking about these kinds of systems, purely abstractly, where you kind of need two different notions of causality. You need a kind of speculative notion that's dynamic, that can be rewritten, and then you need a kind of definite notion that's immutable. A classic example of this is for something like quantum information theory. You can have superpositions of causal orders, you can have quantum switches, you can have causal structure that exists in superpositions of different kind of directed graph states, but then once you apply Hermitian operator, once you apply a measurement, the causal structure is definite because then everything is relativistic and you have covariance. You have similar things, as I understand, with distributed computing, with parallel computing, where you potentially allow for speculative execution for a certain number of steps where you're kind of treeing out this multi-way system and you have a superposition or at least a collection of possible causal histories, but then eventually you have to choose an actual operation to do and then the causal history, you have this big block that gets laid down and then the causal history is somehow definite. I wonder if there's a way of thinking about speculative execution of agents and the interrelation between that speculative execution and agent actions in terms of, again, this interplay between two different causal structures, between a dynamic one versus an immutable one. Yeah. Well, one funny way to think about that is a single agent that has this counterfactual contemplative ability could be in the center place foraging arena and then imagining with discrete branching paths like a chess algorithm or like a probability distribution could be like imagining where it could go, but not all cognitive things or the kind of things that make plans of their own actions, whereas like an ant colony has nest mates on the ground. So they're actually realizing in these finite trajectories, the real exoskeleton on the ground that plays out, ending up with those simulated trajectories could have been simulated or could have been probabilistically blurred, but that's kind of the difference between like the embodiment and like the body moving there for a mammal or for an animal and then like the mind simulating it and then just to the epistemic and pragmatic tradeoff in decision making. So let's just say that we're in that multi-way moment. We'll just have two options, two different slices of the B variable and the policy selection question is about which way are you going to go? Which affordance in the moment? Policy is basically the affordances for the time horizon of planning, but if it's only one time step or just the next one, then the affordance space is just the actions that can be taken. One way to make it so that what happens is the likeliest thing, path of least action, which is kind of what opens up the whole physics of cognitive systems angle in contrast to like a reinforcement or reward learning perspective, what makes it the likeliest thing is starting with habit. So it could just be drawn from a fixed habitual distribution. However, for adaptive action, habit gets up-weighted with expected free energy, which is a functional that takes in the policy space, which is summing up to one because there's a probability over actions, and then up-weighting policies according to their score on expected free energy, which is consisting of epistemic plus pragmatic value. So how much is it going to align the observations to be what I like to see? That's pragmatic value with a preference. What is my expected information gain? That's the epistemic value. So how those are parameterized make the agent that always seeks out new information or always goes with habit or there's so much policy space because the knobs are not just simple sliders or there are multiple knobs, even though they are seemingly quite conciliant and minimal, like it's hard to imagine less, yet especially when there's richness in the environment, even simple systems can have like enormously complex or adaptive behaviors. I'll just leave it there. No, I never really thought of the, so okay, yeah, two things, right? So first of all, the perspective of, you know, thinking of an ant colony or a termite colony or something as being akin to a mind, that's, you know, I was familiar with that perspective from people like Dan Dennett and so on. But the idea that the individual ants in that colony are in a sense enact, they're kind of the hardware enacting the speculative execution, that's a very interesting idea. It's not speculative for them. Well, yeah, no, exactly. But it's sort of from the mind's perspective, I guess it's almost like speculative execution, but it's speculative execution that's being actuated in the physical world, which is very interesting. I not really thought about that before. But then, yeah, okay, so then the point you're making about connection to free energy and sort of habit formation and so on, okay, so I wonder if, you know, if we're thinking about a model of cognition in which there are these two distinct causality notions, the immutable versus the dynamic one, I wonder if the, so you gave a very, very nice account of how habit formation sort of works in these kinds of formalisms based on, you know, prior experience of expected free energy. So I wonder if there's a way of describing that abstractly in terms of something like, you know, you perform the speculative execution step where you're, you know, you're treeing out several multiway possibilities. And initially, you kind of, you know, you know nothing or you have no habits, you're just kind of, you're treeing everything out with kind of equal waiting. But then, you know, for each possible path, you're calculating either an actual or an expected free energy. And then somehow, you know, in future speculative executions, you wait those paths which you previously had found to have higher free energy as higher. And so, you know, you're more likely to explore those and less likely explore ones which are, you know, which have that lower expected value. It feels like something, something like that should fit very, very nicely into an algebraic semantics like this, which would be interesting. Oh, how about more questions from the chat? Okay, upcycle club writes, acknowledging the limitations of traditional entropy in multi computations motivates us to develop context specific entropy metrics. Can you share some insights towards such efforts? Yeah, I can certainly try. So, so yes, I mean, the first point is that, you know, it's, I think it was, there's that famous conversation between John von Neumann and Claude Shannon, where I think von Neumann famously said that like, Shannon should call his measure entropy because no one knows what it means, right? And I submit that the reason that no one knows what entropy means is because it's dependent. I mean, okay, one of the reasons no one knows what entropy means is because it's dependent on exactly what we've been talking about is dependent on the equivalence function of the observer. So it's one of these things like, I don't know, maybe this is a stupid analogy to use, but it's, sorry, I'm going to go off on a tangent, but I promise it's sort of relevant. But so one thing that, okay, one thing that always breaks my brain is when I try and think about like actuaries and life insurance policies, because it's one of those areas where those models only make sense if they're not perfect in a sense, right? Like, so if you had an actuary who knew exactly how long everyone was going to live, and somehow that information was kind of openly available, there would be no, like life insurance policies would be pointless. It's, but you know, whereas also if you had a model that was completely hopeless of predicting how long people would live, they would also be life insurance policies would also be pointless. The very existence of actuarial science relies on your model neither being perfect nor being awful. It somehow has to exist somewhere in between. And as I said, that's something which I, it's one of those topics where if I think about it for too long, it all just stops making sense. And entropy has very much that same character, right? Because if you were Laplace's demon, if you had perfect information about the system that you were observing, there's no notion of entropy, right? It's just every, you know, you know every micro-estate. So, you know, the Boltzmann formula gives you an entropy value of zero, where, you know, the notion of entropy only exists once you take that perfect knowledge of a system and you coarse-grain it, you define, as I was describing earlier, you introduce an encoding function that is not 100% subjective, so that now you are mapping certain distinct micro-estates onto the same coarse-grained macro-estates. And then now you can ask, okay, what's the number of micro-estates that, you know, certainly how coarse is my coarse-graining? What's the number of micro-estates consistent with this macro-estate? What's the number of different values of my domain that gets mapped to a single point in my co-domain of my encoding function? And that's what entropy is. And so it's, it's very closely, I mean, it is effectively a measure of how good is my coarse-graining. So if you had perfect knowledge, there's no entropy. If you have no knowledge, there's no entropy. It relies upon you having a not completely trivial, but also not 100%, you know, a slightly subjective, but not 100% subjective encoding function, just like with life insurance policies. But so the reason, the reason I'm stating that is because, so now it kind of, I think from that perspective, becomes a little bit clearer why there are all these different notions of entropy and why, as the questioner was alluding to, why entropy seems to be so, as a concept seems to be so domain and system specific, because every different system, every different observer will, in principle, have a different set of encoding functions, a different set of equivalence functions, and each one will give rise to a different calculation of entropy. And so one way that you can think about this program, this program to try to understand the algebraic interplay between time complexity versus kind of equivalency complexity or computational irreducibility versus multi-computational irreducibility. In some sense, that is a program to try to understand how different definitions of entropy relate to each other in these kinds of systems. How, you know, if I take one idealized observer that has this equivalence function, and I ask, okay, suppose now they communicate with this different observer with a different equivalency function, they come to different understandings of what the entropy of the system is, but what is the relationship between their measured entropy values? They clearly is one that depends algebraically on some details of the distinction between their respective equivalence functions, but there doesn't seem to be yet any general theory for how those things are related, and that's part of the kind of raison d'\u00eatre of this of this research program. But yeah, I mean, okay, so one thing that I will comment on, although this is a little bit more speculative, it can sort of quasi philosophical comment, but so one place where these notions of entropy become one place where the fact that you have all these different notions of entropy becomes kind of interesting is in fundamental physics. So when you start to think about, if you try to model physics and the universe in these fundamentally computational terms, then one fairly generic sort of conclusion that you can reach is that gravitation, general relativity is essentially an entropic phenomenon. I mean, Valinde and people have kind of talked about this in non computational contexts too, but it's very, very natural if you start to think about space like hypersurfaces being a sort of hypergraphs, then, you know, in order to obtain a continuum geometry that's compatible with the Einstein equations, you need to have certain ergodicity, you know, you need to be able to make certain ergodicity assumptions on the rewriting, which in turn, sort of implies certain lower bounds on the entropy of the system. So somehow gravitation, general relativity, is a coarse grained theory that you obtain in the limit as the entropy goes to infinity. But something that's interesting is that quantum mechanics, on the other hand, is an idealization that you obtain in the limit as entropy goes to zero, because in kind of one of these purely computational models of physics, the quantum mechanical state of a system is described in terms of its multi-way structure. It's described in terms of, you know, when I have a kind of a branching program like this, let me find one of these like this, then, you know, I can divide it up into these sort of, into these simultaneity surfaces. And if I associate each state of the program as being like the analog of a quantum eigen state, and the kind of path weightings as being an analogous to the amplitudes associated with the eigen states, I can quickly build up a description of this multi-way system in terms of the evolution of some discrete analog of the Schrodinger equation. And it turns out you get a theory that is kind of equivalent of the mathematically isomorphic to standard quantum mechanics out of it. So quantum mechanics is sort of, you know, inextricably bound up with the phenomenon of the multi-way system. But if you take the entropy to infinity here, then you're effectively, then the sophistication of your equivalence function becomes arbitrarily large, which means that you can describe any, essentially any pair of states as being equivalent. And so it turns out that the, that actually the quantum mechanical case corresponds to the zero entropy limit, whereas the kind of general relativistic case corresponds to the infinite entropy limit. But they're kind of two different, two fundamentally different notions of entropy, one of which exists at the single-way level, one of which exists at the multi-way level. And again, the question of how these things into play is partly why we're, you know, why we're investigating this. And it's clear that that question has links to these quite foundational questions in fundamental physics. The ideal point mass and the ideal distribution with its center of gravity and all this, Dave, question for you, from you. Yes. Okay, I was, can you hear? Yeah, go for it. Okay, good. Yeah, I'd like to hear down to the low road during this discussion, maybe show business. What people are trying to explain, what is computational reducibility or irreducibility? Often you'll see a graph that says, well, now here's the, the computation, our target running along, the fox is running along, and behind it, there's a team of algorithms that would like to catch it. And yeah, it either does or it doesn't outrun all of them, but some can come, can seem to come pretty close to catching it. Now, there's something else people have been looking at for a few years, the Mandelbrot set. The Mandelbrot set really is a determined, not only a deterministic computation, it's a crisp computation. Every set, every point, either is inside the safe zone, it's going to sit there, and it's happy and quiet, and you color it black, or it flies off to infinity. So you could, it could just be a bunch of yes or no, but that's not the way people trying to calm down after a day of work want to look at it. They want to say, hey, I want to see the 32 million deep Mandelbrot set. And I want to see it in colors. So when you get the colors, you ask, how hard did I have to work? How long did I have to grind along before I made that decision? Oh, this is the point that's going to settle down and be quiet and uninteresting, or it's going to fly off to infinity. So you put colors in. Now, I just wonder, would it be interesting to anyone to fuzzify these gorgeous causal and multi-causal graphs and just show this is the portion where we had to work really hard, rule 35 or whatever. We had to go 15,000 generations before we gave up and said, we're not going to follow this anymore. The other one, oh, after 30 steps, I see. That's a really interesting question. And I mean, yes. So a couple of comments on that. I mean, so one is, I mean, you're right in the sense that, yes, the Mandelbrot set is a very clean thing to describe. I would say it's not, I mean, I would argue it's actually not a crisp computation in that sense, precisely for reasons of computational irreducibility, because as you go arbitrarily close to the boundary of the set, you can have complex numbers that stay, that have a kind of indefinite period of transient, right? There's no upper bound on how long the ZN squared plus C orbit can last before it either diverges or converges to zero. And that statement that there will be points that can remain, that can get tied up in these orbits indefinitely is really a computational irreducibility statement. But yeah, I mean, your question about, could you construct, I mean, that way of coloring points that are on the boundary of the Mandelbrot set in that way, the so-called escape time algorithm, right? Where you kind of, where you color them based on how many steps that I need to do before it either converged or escaped off above some, or the complex numbers modulus exceeded some value. Yeah, it's an interesting idea that you could try and kind of construct geometrical representations of the space of possible computations based on a kind of escape time algorithm for computational irreducibility. Yeah, so I mean, there are the possible ways that you could do that, right? That you said, we know, we've known since the days of Turing, that the halting problem for Turing machines is undecidable, right? That there's no finite computation that you can do that will determine over an arbitrary computer program will terminate in finite time. So you could do it, you could, if you once you have a way of kind of geometrizing the space of possible computations, which we, which we have now, as you say. Yeah, you could easily construct a kind of escape time algorithm where you know, you have your analog of the Mandelbrot set, which is, you know, here's all the halting computations, here are all the definitely non halting computations. And then there's some boundary of very fuzzy stuff where we kind of don't really know, we have to do a lot of work. And even then it's only heuristic, which is, which is so, yeah, I mean, a very directly analogous. And then, yeah, and then the question becomes, you know, so in the case of the M set, you know, there is some underlying theory mostly, I think mostly due to like Dwadi and Hubbard and people that allows you to kind of predict, like, you know, if you have a finite filament, you can, like, of the Mandelbrot set, you can kind of predict where that filament is going to be based on some complicated complex analysis argument. And, you know, I mean, the question is once you, if you once you have a geometrization of the space of possible computations, can you construct some general theory like the theory, you know, developed by Dwadi and Hubbard that tells you things about, you know, the topology of the computations, whether certain regions of the space are connected, whether they're compact, whether you can make, you know, whether you can do a similar thing, or you can make predictions based on the geometry of one part of the set, you know, where you can extrapolate to things about the geometry of another part of the set. That's a really interesting question. And yeah, again, as with so many of these things, it's one of these, like, I don't know the answer, but it's a good reason for investigating, you know, for pursuing the program, right? Yeah, from the Wolfram side, as well as from the active inference side, there's both the information topology and the information geometry side, and not just in the kind of topological deep learning, but rather looking at the topology of information flows, which has been heavily developed in the category related to quantum information sciences. And then the information geometries that allow us to do, like, machine learning type, accelerated optimization ingredients, all these kinds of concepts that come into play with geometry, and we just don't get them from topology. So topology kind of sketches the skeleton, and then with the computers that we have, the information geometry is at least on the data sets and the ways that we have computation today, that's kind of like the quantitative numerical versus the formal. One question is, are these all discrete state space, discrete time formalisms? Because in active inference, we often deal with hybrid models that have discrete and continuous state spaces, and the same generative model or the same system of interest could be modeled with like a discrete time chapter seven, or a continuous time chapter eight model. So how does this deal with that? That's a really good question. I mean, yeah, so most of what I've been looking at so far has consisted of discrete time, discrete space models, for no particularly principled reason other than they're easier to analyze, right? For the most part, because you can do explicit computations because they're kind of more amenable to constructive analysis. It's easier to do, but the beauty of a lot of this, that's one of the beauties of using kind of general mathematical formalism is that once you develop it, it's often quite easy to extend even to cases that, or to extrapolate to the cases that you didn't explicitly analyze. So in principle, this formalism works for continuum space and continuum time systems as well, just with some slight modifications. So rather than having say, branching and merging, you instead have, if you think about this thing as now being a dynamical system, described on the on some symplectic manifold, then these kind of branching, merging operations of the multi-way system become effectively divergence and convergence, differential operators are defined on the symplectic manifold. And so one place where we can start to analyze that explicitly, and which I've done a little bit of work on, but it's one of these things which I want to go back to very soon, is looking at PetriNets. So PetriNets are interesting because they are a discrete time, discrete space system, but they admit a continuum space, continuum time, description in terms of ordinary differential equations and so on. So they're a nice example of a hybrid kind of discrete event versus continuum event system, where it's clear that this formalism can be used and is somehow agnostic as to whether the underlying system is discrete or continuous. Again, there's a broader philosophical point to make here, which is that in a way, one of the reasons I don't feel embarrassed to be working primarily with discrete systems is because, again, once you start to think about things in terms of, okay, you have to not just care about nature, you also have to care about the computations the observer can perform and what it's able to infer, then you quickly realize that in a sense, just like, whatever, as you were saying earlier, Danny, beauty is in the eye of the beholder. I think discreteness and continuity are also in the eye of the beholder, right? So if you have a universe that is fundamentally continuous, that's described by a continuum, the Renzi and Manifold or something, but your constraints are that the only experiments you perform have computable outcomes, have discrete outcomes, where the possible number of observables is always countable, then in a sense, it doesn't matter, right? It's irrelevant to you as an observer whether the system is discrete or continuous, because the only parts of it that you can interface with and interact with are discrete, and so you could have replaced the underlying substrate with a purely discrete mathematical structure and you wouldn't be able to tell. So in some sense, I don't feel too embarrassed dealing with discrete event systems because even if I don't necessarily believe that nature is discrete, because I don't think that's, I'm not even sure how we would be able to answer that, I'm reasonably convinced that the experiments that we can perform and the observations that we're able to perform are ultimately computable, and therefore, the underlying substrate might as well be discrete, even if it's not in reality, so to speak. Yeah, that's a great comment and definitely calls back to your earlier points about discretization being in the eye of the beholder, like in the active inference models, observations, raw data may already be discretized depending on the situation, but even if it weren't, like it were a continuous sensory perception or modeled as such analytically, still commonly models discretizing categorize as they move up cognitive hierarchies, and that was like initially explored to get more of this discrete either or decision making, planning, all those kinds of properties. Well, there's many interesting angles like I'm sure also it could be a multiplexed language model prompt, but like what are you working on or excited about for 2024? That's a good question. So I've kind of already given some hints about, you know, like this general research program of trying to understand computational complexity and algorithmic complexity and interplays between observers and systems through this category theoretic lens. That's a major thing which I started on, say, about maybe a couple of years ago. I mean, in some form, I've been working on it for a long time, but this more recent perspective on it is maybe a couple of years old. But for the various reasons over the last year or so, I've kind of put that to rest and I've been focused on these much more physics oriented questions about discrete space time and understanding things like, you know, how do black holes work and how does accretion work in discrete space time, which is also very important and very exciting. But I've sort of slightly been missing these more abstract directions. And so I have maybe one or two major physics related things that I need to finish off and then I really want to go back to this to the greatest extent possible. And yeah, I mean, so one thing is that's quite clear is that there's great interplay between this formalism and existing theories of computational and algorithmic complexity. So in particular, you know, so one very basic example is I mentioned before that, you know, you have this kind of coherence between these two different algebraic structures between your the operation of time like composition versus the operation of kind of parallel composition. And these two algebraic structures are in general related, although the precise conditions that relate them are not clear. And that's partly what we're trying to what we're trying to understand. But it turns out that degenerate cases of that of that question corresponds to unsolved problems in computational complexity theory. So, for instance, the P bus NP problem can essentially be recast in these terms that you can recast the P bus NP problem is the question about is the coherence between the time like composition of computational complexity and the parallel composition of computational complexity, which are what P and NP respectively are really about, it are those coherence conditions, the strictest they can be, which would be the case that the P equals NP, or are they somehow more lax, which would be the case that P does not equal NP. And so there's, you know, that's that's one thing that we kind of were already investigated, but it's clear that a whole bunch of questions about, you know, how the time complexity and space complexity trade off or how to come over of complexity and time complexity trade off, these, these are questions which can be recast in this kind of more algebraic category, theoretic lens, and, and, and will hopefully give insight into this general program of trying to understand observers and their relationship to the world. And those are kind of major, well, with any luck, those are major theorems that I hope we'll be able to prove at some point in 2024. That's that's that's that's awesome. And it makes me think about parallel, more nest mates, more CPU threads, deeper in times, more sequential, more planning, and more cognitive single monolithic agent. And then the kind of question is like, can anything that a single agent, mega matrix could do, cannot be decomposable at space advantage, or even at space disadvantage in decomposed it into a single time step operation? Yeah, that's a super important question. And one that, you know, with the possible exception of this community, not many people have asked, right? I mean, so that's something which comes up in quantum computing, right? So a lot of the hype around quantum computation comes from these theoretical speedups that derive from the fact that you're able to, you know, you're able to support these super positions of different, you know, where, you know, each, that each state of your data structure corresponds to a different eigenstate, and you're able to evolve some super superposition of those eigenstates. But then at the end, you have to actually come to a definite conclusion about what the answer is, you have to perform some measurement operation. And that measurement operation is lossy, it's often non deterministic, you, you know, you often have to repeat it multiple times. And, you know, it's becoming increasingly clear that for a large class of operations that were previously thought to have quantum advantage, the additional complexity of the measurement step really kills any quantum advantage that you may have had that you know, you get some advantage by doing unitary evolution. But then you lose all of it by having to do the submission projection at the end. And that's really a story of, again, this interplay between the time complexity saving of doing a multi doing a computation of doing a multi computation in parallel, versus the loss that comes from the complexity of the equivalence function that you need to apply in order to get to some definite conclusion about what the, you know, about what happened because, you know, ultimately, you need to somehow collapse that that that directed graph into a single thread of time in order to be able to have some coherent representation of what happened. And so again, understand that, you know, that's a place where understanding these, you know, the these tradeoffs will become very important. And as I say, in some limiting case that that gives some perspective on your question, Daniel, which is, which I agree is a very interesting question about, you know, in principle, we know that anything that a deterministic Turing machine can do a non deterministic Turing machine can do and vice versa with some speed up or slow down. But that statement, which is a classic result in, you know, in computability theory, neglects all consideration of the equivalence function. So there may be cases where the equivalence function is so complex, that essentially, you know, that to do state equivalence becomes undecidable. And so in that case, you have a scenario where actually, you know, you've got a multi computational system, but to collapse it to one that's equivalent to a single way system requires unbounded amounts of computational effort. And so that's so actually they become inequivalent, even though, you know, computability theory says they should be the same. So it's clear that there's a there's a more rich, more subtle theory that's underlying here that we're just beginning to kind of glimpse, and that I hope will, you know, we'll be able to kind of to prove some new limited results about soon, once we understand it a bit better. Awesome. And I think definitely a special shout out to all of our colleagues on either like the Wolfram and or active inference side, because we've seen few if any active inference models phrased analytically or computationally with the Wolfram technology from studying complexity in other areas. It's really clear to see how productive and powerful the software and the tools can be and changing and growing every day. So it's really interesting, maybe someone can if they're listening this far in, like, go from one side to the other and back or make a Wolfram active inference model, or do some other kind of combination, because it's very fruitful territory. And we know that our elders have already spoken, they've okayed it. No, but really, it's so rich with connections here between the areas that we're all studying and feeling like converging on many common places to scaffold and jump off from together. Yeah, I agree. And I mean, at least the kinds of things that we were discussing here about, you know, speculative execution and behavior formation through free energy principle and so on, those things should be relatively easy to implement in the framework that's already been developed here. I mean, so that's just a question of just implementing some kind of computation of expected free energy and using that to weight multiway paths in the speculative execution model. So at least the beginnings of that implementation, I think the path is pretty clear. And we will probably end up doing at some point in the future anyway, as part of other research. So yeah, so I agree. It's a very exciting kind of point of interface. Yeah, well, I hope that we can stay in touch if you ever want to come back for a 009.2, or if we want to even facilitate some kind of working group or some connections to really strengthen and like include the participation of more people in this super exciting area, that would be amazing. Yeah, that sounds fun. Let's let's try and set something up. Well, Dave, first penultimate comments, then Jonathan, you can have the kind of last comments. Yes, I hope you do get to continue on the Wolfram physics side to think about a more general notion of what these ultimate things are. Are they observers or does that already prejudice the case of what you might find if you call them workers or actors or, you know, go back and ask Stuart Kaufman, what must a mind do to earn its way in the world? Let's make this keep happening. Thank you. Thank you, Dave, for suggesting it also. It was a great suggestion. Jonathan? No, I think that's a fantastic note to end on. I mean, in a sense, this idea that we should start to move, I mean, so it's okay, big picture for a moment, like where, you know, this formalism is being developed, the formalism I've described in this in this discussion is being developed, you know, assuming a kind of purely passive observer idealization. And that's already been incredibly difficult, right? This is clear, there's a lot we don't understand at that. But of course, David is right that in a sense, you know, what, you know, ultimately, we want to start transitioning to a participatory observer model, where you allow for two-way interactions or, you know, higher order interactions between observers and systems and things that don't just go in one direction. And yeah, you know, in a sense, I view a lot of what we're trying to do with, you know, trying to nail down these notions of causality, trying to understand these interplays between different complexity and entropy measures as, you know, the necessary groundwork for developing that subsequent theory, right? That does, you know, it's clear that if we want to have a version of the, you know, of this kind of compositional multi-way formalism that is also compatible with things like second-order cybernetics, then, you know, at the very least, we need to have a very coherent notion for what causality is and, you know, and a robust algebraic description of that that's not going to break or change. And so I think the nonparticipatory observer model is a useful starting point because it's one that is just within our grasp of, you know, of being kind of mathematically tractable. And then the hope is that the technology and the ideas and the conceptual structure that we develop for understanding that will then, as I say, lay the groundwork for developing something that's more like what real, you know, active participatory observers do. And, and yeah, I mean, I think I don't, on the, on the scientific side, I'm not really sure I have any final comments apart from just, you know, as obvious, this is a, you know, this is still a story that's being, that's being developed. And, and yeah, as Daniel alluded to, I hope that we can continue to interact and collaborate where, where that makes sense. And, and yeah, at the very least, I think, in cooperation of kind of these, you know, these active inference models within these discrete time, in the first instance, discrete time, you know, computational frameworks, and allowing things like speculative execution and multi-way path waiting based on free energy estimates. I think that's, you know, that's a, that's a project that's of obvious mutual interest and, and something that I, that I hope will happen in the coming months. Thank you. We just speculatively executed active Wolfram inference. Basically. Sounds very good. All right. Thank you, Jonathan. Thank you, Dave. Thank you, everyone. See y'all next time. you you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.200000000000001, "text": " Hello and welcome, everyone. This is Active Inference Mathstream 9.1 on March 5, 2024.", "tokens": [50364, 2425, 293, 2928, 11, 1518, 13, 639, 307, 26635, 682, 5158, 15776, 9291, 1722, 13, 16, 322, 6129, 1025, 11, 45237, 13, 51074], "temperature": 0.0, "avg_logprob": -0.24884535252362833, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.22196152806282043}, {"id": 1, "seek": 0, "start": 14.200000000000001, "end": 20.64, "text": " We're here with Jonathan Gord and we'll be discussing a variety of topics yet to be determined", "tokens": [51074, 492, 434, 510, 365, 15471, 460, 765, 293, 321, 603, 312, 10850, 257, 5673, 295, 8378, 1939, 281, 312, 9540, 51396], "temperature": 0.0, "avg_logprob": -0.24884535252362833, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.22196152806282043}, {"id": 2, "seek": 0, "start": 20.64, "end": 25.96, "text": " or are they? So thank you for joining and to you for any introduction and we'll really", "tokens": [51396, 420, 366, 436, 30, 407, 1309, 291, 337, 5549, 293, 281, 291, 337, 604, 9339, 293, 321, 603, 534, 51662], "temperature": 0.0, "avg_logprob": -0.24884535252362833, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.22196152806282043}, {"id": 3, "seek": 0, "start": 25.96, "end": 29.68, "text": " look forward to everyone's comments and questions. So thanks again for joining to you.", "tokens": [51662, 574, 2128, 281, 1518, 311, 3053, 293, 1651, 13, 407, 3231, 797, 337, 5549, 281, 291, 13, 51848], "temperature": 0.0, "avg_logprob": -0.24884535252362833, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.22196152806282043}, {"id": 4, "seek": 3000, "start": 30.32, "end": 35.04, "text": " Okay, well, yeah, thanks very much, Daniel, for the introduction and for inviting me to be here", "tokens": [50380, 1033, 11, 731, 11, 1338, 11, 3231, 588, 709, 11, 8033, 11, 337, 264, 9339, 293, 337, 18202, 385, 281, 312, 510, 50616], "temperature": 0.0, "avg_logprob": -0.15727148056030274, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.009384206496179104}, {"id": 5, "seek": 3000, "start": 35.04, "end": 44.08, "text": " on Active Inference. I'm looking forward to a very, very fun discussion. So I don't have anything", "tokens": [50616, 322, 26635, 682, 5158, 13, 286, 478, 1237, 2128, 281, 257, 588, 11, 588, 1019, 5017, 13, 407, 286, 500, 380, 362, 1340, 51068], "temperature": 0.0, "avg_logprob": -0.15727148056030274, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.009384206496179104}, {"id": 6, "seek": 3000, "start": 44.08, "end": 47.28, "text": " especially prepared to talk about, which is probably a good thing because it means we'll be", "tokens": [51068, 2318, 4927, 281, 751, 466, 11, 597, 307, 1391, 257, 665, 551, 570, 309, 1355, 321, 603, 312, 51228], "temperature": 0.0, "avg_logprob": -0.15727148056030274, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.009384206496179104}, {"id": 7, "seek": 3000, "start": 47.28, "end": 51.84, "text": " able to extend the kind of the unstructured part of this for as long as possible. But I think just", "tokens": [51228, 1075, 281, 10101, 264, 733, 295, 264, 18799, 46847, 644, 295, 341, 337, 382, 938, 382, 1944, 13, 583, 286, 519, 445, 51456], "temperature": 0.0, "avg_logprob": -0.15727148056030274, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.009384206496179104}, {"id": 8, "seek": 3000, "start": 51.84, "end": 57.120000000000005, "text": " to give a little bit of context, I want to talk about an area where I think some things that I've", "tokens": [51456, 281, 976, 257, 707, 857, 295, 4319, 11, 286, 528, 281, 751, 466, 364, 1859, 689, 286, 519, 512, 721, 300, 286, 600, 51720], "temperature": 0.0, "avg_logprob": -0.15727148056030274, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.009384206496179104}, {"id": 9, "seek": 5712, "start": 57.199999999999996, "end": 62.4, "text": " been working on, some collaborators that might have been working on, that might have some kind of", "tokens": [50368, 668, 1364, 322, 11, 512, 39789, 300, 1062, 362, 668, 1364, 322, 11, 300, 1062, 362, 512, 733, 295, 50628], "temperature": 0.0, "avg_logprob": -0.12453760560025874, "compression_ratio": 1.8515625, "no_speech_prob": 0.05644116550683975}, {"id": 10, "seek": 5712, "start": 62.4, "end": 67.36, "text": " intersection of interest with things that, you know, Active Inference type people might care about,", "tokens": [50628, 15236, 295, 1179, 365, 721, 300, 11, 291, 458, 11, 26635, 682, 5158, 2010, 561, 1062, 1127, 466, 11, 50876], "temperature": 0.0, "avg_logprob": -0.12453760560025874, "compression_ratio": 1.8515625, "no_speech_prob": 0.05644116550683975}, {"id": 11, "seek": 5712, "start": 67.36, "end": 73.28, "text": " right? So, and in particular, that concerns the relationship between kind of computation,", "tokens": [50876, 558, 30, 407, 11, 293, 294, 1729, 11, 300, 7389, 264, 2480, 1296, 733, 295, 24903, 11, 51172], "temperature": 0.0, "avg_logprob": -0.12453760560025874, "compression_ratio": 1.8515625, "no_speech_prob": 0.05644116550683975}, {"id": 12, "seek": 5712, "start": 73.28, "end": 78.64, "text": " observation, and cognition, and specifically using methods that come from category theory and", "tokens": [51172, 14816, 11, 293, 46905, 11, 293, 4682, 1228, 7150, 300, 808, 490, 7719, 5261, 293, 51440], "temperature": 0.0, "avg_logprob": -0.12453760560025874, "compression_ratio": 1.8515625, "no_speech_prob": 0.05644116550683975}, {"id": 13, "seek": 5712, "start": 78.64, "end": 82.96, "text": " topos theory and some other kind of branches of mathematics and theoretical computer science", "tokens": [51440, 1192, 329, 5261, 293, 512, 661, 733, 295, 14770, 295, 18666, 293, 20864, 3820, 3497, 51656], "temperature": 0.0, "avg_logprob": -0.12453760560025874, "compression_ratio": 1.8515625, "no_speech_prob": 0.05644116550683975}, {"id": 14, "seek": 8296, "start": 83.03999999999999, "end": 88.32, "text": " to understand the relationship between system, specifically the computational and algorithmic", "tokens": [50368, 281, 1223, 264, 2480, 1296, 1185, 11, 4682, 264, 28270, 293, 9284, 299, 50632], "temperature": 0.0, "avg_logprob": -0.08280459488972579, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.002548057120293379}, {"id": 15, "seek": 8296, "start": 88.32, "end": 93.44, "text": " complexity of systems versus the computational algorithmic complexity of observers of those", "tokens": [50632, 14024, 295, 3652, 5717, 264, 28270, 9284, 299, 14024, 295, 48090, 295, 729, 50888], "temperature": 0.0, "avg_logprob": -0.08280459488972579, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.002548057120293379}, {"id": 16, "seek": 8296, "start": 93.44, "end": 98.47999999999999, "text": " systems and how those things trade off between each other. So, so just to give a little bit of", "tokens": [50888, 3652, 293, 577, 729, 721, 4923, 766, 1296, 1184, 661, 13, 407, 11, 370, 445, 281, 976, 257, 707, 857, 295, 51140], "temperature": 0.0, "avg_logprob": -0.08280459488972579, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.002548057120293379}, {"id": 17, "seek": 8296, "start": 98.47999999999999, "end": 103.91999999999999, "text": " context to that, I want to show these are just some visuals from a paper that I put out about a", "tokens": [51140, 4319, 281, 300, 11, 286, 528, 281, 855, 613, 366, 445, 512, 26035, 490, 257, 3035, 300, 286, 829, 484, 466, 257, 51412], "temperature": 0.0, "avg_logprob": -0.08280459488972579, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.002548057120293379}, {"id": 18, "seek": 8296, "start": 103.91999999999999, "end": 108.8, "text": " year ago now, and that this kind of really defines this research program that I've been working on", "tokens": [51412, 1064, 2057, 586, 11, 293, 300, 341, 733, 295, 534, 23122, 341, 2132, 1461, 300, 286, 600, 668, 1364, 322, 51656], "temperature": 0.0, "avg_logprob": -0.08280459488972579, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.002548057120293379}, {"id": 19, "seek": 10880, "start": 108.8, "end": 113.36, "text": " for the last year and a half in some form or another, which is looking at exactly this trade-off", "tokens": [50364, 337, 264, 1036, 1064, 293, 257, 1922, 294, 512, 1254, 420, 1071, 11, 597, 307, 1237, 412, 2293, 341, 4923, 12, 4506, 50592], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 20, "seek": 10880, "start": 113.92, "end": 118.56, "text": " using category theoretic machinery. So, here's a specification of a Turing machine. This is", "tokens": [50620, 1228, 7719, 14308, 299, 27302, 13, 407, 11, 510, 311, 257, 31256, 295, 257, 314, 1345, 3479, 13, 639, 307, 50852], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 21, "seek": 10880, "start": 118.56, "end": 123.28, "text": " just a simple deterministic computation. It's saying, you know, you have a Turing machine that", "tokens": [50852, 445, 257, 2199, 15957, 3142, 24903, 13, 467, 311, 1566, 11, 291, 458, 11, 291, 362, 257, 314, 1345, 3479, 300, 51088], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 22, "seek": 10880, "start": 123.28, "end": 127.67999999999999, "text": " has this head state and this tape state, and on the next step, you're going to replace the tape", "tokens": [51088, 575, 341, 1378, 1785, 293, 341, 7314, 1785, 11, 293, 322, 264, 958, 1823, 11, 291, 434, 516, 281, 7406, 264, 7314, 51308], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 23, "seek": 10880, "start": 127.67999999999999, "end": 130.72, "text": " state with something that looks like this, the head state with something that looks like that,", "tokens": [51308, 1785, 365, 746, 300, 1542, 411, 341, 11, 264, 1378, 1785, 365, 746, 300, 1542, 411, 300, 11, 51460], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 24, "seek": 10880, "start": 130.72, "end": 134.56, "text": " and you're going to scroll the Turing machine head left, or in this case, scroll it right,", "tokens": [51460, 293, 291, 434, 516, 281, 11369, 264, 314, 1345, 3479, 1378, 1411, 11, 420, 294, 341, 1389, 11, 11369, 309, 558, 11, 51652], "temperature": 0.0, "avg_logprob": -0.08093984178501926, "compression_ratio": 1.9283276450511946, "no_speech_prob": 0.03671948239207268}, {"id": 25, "seek": 13456, "start": 134.56, "end": 139.6, "text": " etc. So, this is just a, you know, specification of a very simple computation. I think this is a", "tokens": [50364, 5183, 13, 407, 11, 341, 307, 445, 257, 11, 291, 458, 11, 31256, 295, 257, 588, 2199, 24903, 13, 286, 519, 341, 307, 257, 50616], "temperature": 0.0, "avg_logprob": -0.0841594530841497, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003941678907722235}, {"id": 26, "seek": 13456, "start": 139.6, "end": 143.92000000000002, "text": " two-state, two-color Turing machine on a simple, you know, one-dimensional tape. It's about as", "tokens": [50616, 732, 12, 15406, 11, 732, 12, 23851, 314, 1345, 3479, 322, 257, 2199, 11, 291, 458, 11, 472, 12, 18759, 7314, 13, 467, 311, 466, 382, 50832], "temperature": 0.0, "avg_logprob": -0.0841594530841497, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003941678907722235}, {"id": 27, "seek": 13456, "start": 143.92000000000002, "end": 148.32, "text": " simple a computation as you could define. So, if you run that thing for some initial condition,", "tokens": [50832, 2199, 257, 24903, 382, 291, 727, 6964, 13, 407, 11, 498, 291, 1190, 300, 551, 337, 512, 5883, 4188, 11, 51052], "temperature": 0.0, "avg_logprob": -0.0841594530841497, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003941678907722235}, {"id": 28, "seek": 13456, "start": 148.32, "end": 154.4, "text": " you'll get an evolution that looks like this. And so, right now, this is just a purely deterministic,", "tokens": [51052, 291, 603, 483, 364, 9303, 300, 1542, 411, 341, 13, 400, 370, 11, 558, 586, 11, 341, 307, 445, 257, 17491, 15957, 3142, 11, 51356], "temperature": 0.0, "avg_logprob": -0.0841594530841497, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003941678907722235}, {"id": 29, "seek": 13456, "start": 154.4, "end": 159.76, "text": " you know, single-path evolution. But from this, we can construct, we can build a mathematical", "tokens": [51356, 291, 458, 11, 2167, 12, 31852, 9303, 13, 583, 490, 341, 11, 321, 393, 7690, 11, 321, 393, 1322, 257, 18894, 51624], "temperature": 0.0, "avg_logprob": -0.0841594530841497, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003941678907722235}, {"id": 30, "seek": 15976, "start": 159.76, "end": 165.51999999999998, "text": " structure. Namely, we can build a category. So, and the rules for how we build that category are", "tokens": [50364, 3877, 13, 10684, 736, 11, 321, 393, 1322, 257, 7719, 13, 407, 11, 293, 264, 4474, 337, 577, 321, 1322, 300, 7719, 366, 50652], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 31, "seek": 15976, "start": 165.51999999999998, "end": 170.39999999999998, "text": " very simple. So, you know, each arrow here is some simple computation, some application of", "tokens": [50652, 588, 2199, 13, 407, 11, 291, 458, 11, 1184, 11610, 510, 307, 512, 2199, 24903, 11, 512, 3861, 295, 50896], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 32, "seek": 15976, "start": 170.39999999999998, "end": 174.64, "text": " the Turing machine transition function. And then what we can do is we can say, well, any time we", "tokens": [50896, 264, 314, 1345, 3479, 6034, 2445, 13, 400, 550, 437, 321, 393, 360, 307, 321, 393, 584, 11, 731, 11, 604, 565, 321, 51108], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 33, "seek": 15976, "start": 174.64, "end": 178.79999999999998, "text": " have two arrows that are laid end-to-end like this, we can compose them together to create a", "tokens": [51108, 362, 732, 19669, 300, 366, 9897, 917, 12, 1353, 12, 521, 411, 341, 11, 321, 393, 35925, 552, 1214, 281, 1884, 257, 51316], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 34, "seek": 15976, "start": 178.79999999999998, "end": 183.51999999999998, "text": " third arrow that goes like that. I may even have a picture, yes, like this. So, you know, we have", "tokens": [51316, 2636, 11610, 300, 1709, 411, 300, 13, 286, 815, 754, 362, 257, 3036, 11, 2086, 11, 411, 341, 13, 407, 11, 291, 458, 11, 321, 362, 51552], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 35, "seek": 15976, "start": 183.51999999999998, "end": 188.79999999999998, "text": " a computation f that takes us from x to y, a computation g that takes us from y to z, and we", "tokens": [51552, 257, 24903, 283, 300, 2516, 505, 490, 2031, 281, 288, 11, 257, 24903, 290, 300, 2516, 505, 490, 288, 281, 710, 11, 293, 321, 51816], "temperature": 0.0, "avg_logprob": -0.10004476837764513, "compression_ratio": 1.9060402684563758, "no_speech_prob": 0.013632547110319138}, {"id": 36, "seek": 18880, "start": 189.04000000000002, "end": 193.92000000000002, "text": " then we obtain a composite computation g compose f that takes us directly from x to z.", "tokens": [50376, 550, 321, 12701, 257, 25557, 24903, 290, 35925, 283, 300, 2516, 505, 3838, 490, 2031, 281, 710, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09576025375953087, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.0007093867170624435}, {"id": 37, "seek": 18880, "start": 193.92000000000002, "end": 199.68, "text": " And we also add some additional edges, some additional arrows on each state itself,", "tokens": [50620, 400, 321, 611, 909, 512, 4497, 8819, 11, 512, 4497, 19669, 322, 1184, 1785, 2564, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09576025375953087, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.0007093867170624435}, {"id": 38, "seek": 18880, "start": 199.68, "end": 204.64000000000001, "text": " a sort of identity, an identity operation that maps the computational state directly to itself.", "tokens": [50908, 257, 1333, 295, 6575, 11, 364, 6575, 6916, 300, 11317, 264, 28270, 1785, 3838, 281, 2564, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09576025375953087, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.0007093867170624435}, {"id": 39, "seek": 18880, "start": 205.36, "end": 211.52, "text": " And so, this combined with some axioms of associativity and identity forms a category", "tokens": [51192, 400, 370, 11, 341, 9354, 365, 512, 6360, 72, 4785, 295, 4180, 30142, 293, 6575, 6422, 257, 7719, 51500], "temperature": 0.0, "avg_logprob": -0.09576025375953087, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.0007093867170624435}, {"id": 40, "seek": 18880, "start": 211.52, "end": 215.84, "text": " of elementary computations. So, this is a very, very simple example. But what I want to try and", "tokens": [51500, 295, 16429, 2807, 763, 13, 407, 11, 341, 307, 257, 588, 11, 588, 2199, 1365, 13, 583, 437, 286, 528, 281, 853, 293, 51716], "temperature": 0.0, "avg_logprob": -0.09576025375953087, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.0007093867170624435}, {"id": 41, "seek": 21584, "start": 215.84, "end": 222.24, "text": " build up towards and kind of pump your intuition for is a category which I call comp, which is a", "tokens": [50364, 1322, 493, 3030, 293, 733, 295, 5889, 428, 24002, 337, 307, 257, 7719, 597, 286, 818, 715, 11, 597, 307, 257, 50684], "temperature": 0.0, "avg_logprob": -0.09428681364846886, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.025155913084745407}, {"id": 42, "seek": 21584, "start": 222.24, "end": 228.4, "text": " category whose objects are all essentially the class of all data structures and whose arrows or", "tokens": [50684, 7719, 6104, 6565, 366, 439, 4476, 264, 1508, 295, 439, 1412, 9227, 293, 6104, 19669, 420, 50992], "temperature": 0.0, "avg_logprob": -0.09428681364846886, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.025155913084745407}, {"id": 43, "seek": 21584, "start": 228.4, "end": 233.12, "text": " morphisms are the class of all elementary computations. So, you start by just applying, you", "tokens": [50992, 25778, 13539, 366, 264, 1508, 295, 439, 16429, 2807, 763, 13, 407, 11, 291, 722, 538, 445, 9275, 11, 291, 51228], "temperature": 0.0, "avg_logprob": -0.09428681364846886, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.025155913084745407}, {"id": 44, "seek": 21584, "start": 233.12, "end": 236.72, "text": " know, all possible computations or, you know, in this case, for the case of Turing machines,", "tokens": [51228, 458, 11, 439, 1944, 2807, 763, 420, 11, 291, 458, 11, 294, 341, 1389, 11, 337, 264, 1389, 295, 314, 1345, 8379, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09428681364846886, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.025155913084745407}, {"id": 45, "seek": 21584, "start": 236.72, "end": 240.64000000000001, "text": " all possible, you know, Turing machine transition functions. And then you do this", "tokens": [51408, 439, 1944, 11, 291, 458, 11, 314, 1345, 3479, 6034, 6828, 13, 400, 550, 291, 360, 341, 51604], "temperature": 0.0, "avg_logprob": -0.09428681364846886, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.025155913084745407}, {"id": 46, "seek": 24064, "start": 240.64, "end": 245.27999999999997, "text": " closure operation where you, you know, where you essentially do what I'm doing here, but you,", "tokens": [50364, 24653, 6916, 689, 291, 11, 291, 458, 11, 689, 291, 4476, 360, 437, 286, 478, 884, 510, 11, 457, 291, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10131642553541395, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.0017805865500122309}, {"id": 47, "seek": 24064, "start": 245.27999999999997, "end": 250.07999999999998, "text": " you know, you allow those elementary computations to be composed together in arbitrary ways.", "tokens": [50596, 291, 458, 11, 291, 2089, 729, 16429, 2807, 763, 281, 312, 18204, 1214, 294, 23211, 2098, 13, 50836], "temperature": 0.0, "avg_logprob": -0.10131642553541395, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.0017805865500122309}, {"id": 48, "seek": 24064, "start": 250.07999999999998, "end": 255.6, "text": " And so, that gives you effectively a class of all possible programs. So, this category contains", "tokens": [50836, 400, 370, 11, 300, 2709, 291, 8659, 257, 1508, 295, 439, 1944, 4268, 13, 407, 11, 341, 7719, 8306, 51112], "temperature": 0.0, "avg_logprob": -0.10131642553541395, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.0017805865500122309}, {"id": 49, "seek": 24064, "start": 255.6, "end": 260.24, "text": " not only all possible data structures as objects, but all possible programs as morphisms. And this", "tokens": [51112, 406, 787, 439, 1944, 1412, 9227, 382, 6565, 11, 457, 439, 1944, 4268, 382, 25778, 13539, 13, 400, 341, 51344], "temperature": 0.0, "avg_logprob": -0.10131642553541395, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.0017805865500122309}, {"id": 50, "seek": 24064, "start": 260.24, "end": 265.84, "text": " is a very rich category with some very interesting algebraic structure that we'll kind of, again,", "tokens": [51344, 307, 257, 588, 4593, 7719, 365, 512, 588, 1880, 21989, 299, 3877, 300, 321, 603, 733, 295, 11, 797, 11, 51624], "temperature": 0.0, "avg_logprob": -0.10131642553541395, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.0017805865500122309}, {"id": 51, "seek": 26584, "start": 265.91999999999996, "end": 272.15999999999997, "text": " I'm sure we'll allude to in our subsequent discussion. But in a sense, when we do this,", "tokens": [50368, 286, 478, 988, 321, 603, 439, 2303, 281, 294, 527, 19962, 5017, 13, 583, 294, 257, 2020, 11, 562, 321, 360, 341, 11, 50680], "temperature": 0.0, "avg_logprob": -0.07819579379393322, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.002017867285758257}, {"id": 52, "seek": 26584, "start": 272.15999999999997, "end": 276.08, "text": " when we do this operation of taking what mathematically we call a transitive closure,", "tokens": [50680, 562, 321, 360, 341, 6916, 295, 1940, 437, 44003, 321, 818, 257, 1145, 2187, 24653, 11, 50876], "temperature": 0.0, "avg_logprob": -0.07819579379393322, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.002017867285758257}, {"id": 53, "seek": 26584, "start": 276.08, "end": 281.28, "text": " right, where we allow two elementary computations to be composed together to produce a third,", "tokens": [50876, 558, 11, 689, 321, 2089, 732, 16429, 2807, 763, 281, 312, 18204, 1214, 281, 5258, 257, 2636, 11, 51136], "temperature": 0.0, "avg_logprob": -0.07819579379393322, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.002017867285758257}, {"id": 54, "seek": 26584, "start": 282.32, "end": 286.15999999999997, "text": " we are essentially kind of neglecting considerations of computational complexity,", "tokens": [51188, 321, 366, 4476, 733, 295, 17745, 278, 24070, 295, 28270, 14024, 11, 51380], "temperature": 0.0, "avg_logprob": -0.07819579379393322, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.002017867285758257}, {"id": 55, "seek": 26584, "start": 286.15999999999997, "end": 292.15999999999997, "text": " right, because, you know, this arrow here might correspond to one application of the Turing machine", "tokens": [51380, 558, 11, 570, 11, 291, 458, 11, 341, 11610, 510, 1062, 6805, 281, 472, 3861, 295, 264, 314, 1345, 3479, 51680], "temperature": 0.0, "avg_logprob": -0.07819579379393322, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.002017867285758257}, {"id": 56, "seek": 29216, "start": 292.16, "end": 296.32000000000005, "text": " transition function, this arrow might correspond to another application of the transition function,", "tokens": [50364, 6034, 2445, 11, 341, 11610, 1062, 6805, 281, 1071, 3861, 295, 264, 6034, 2445, 11, 50572], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 57, "seek": 29216, "start": 296.32000000000005, "end": 300.48, "text": " but this composite arrow correspond might correspond to two applications of the transition", "tokens": [50572, 457, 341, 25557, 11610, 6805, 1062, 6805, 281, 732, 5821, 295, 264, 6034, 50780], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 58, "seek": 29216, "start": 300.48, "end": 306.48, "text": " function. And so, somehow, when we allow arrows or morphisms to be composed in this way,", "tokens": [50780, 2445, 13, 400, 370, 11, 6063, 11, 562, 321, 2089, 19669, 420, 25778, 13539, 281, 312, 18204, 294, 341, 636, 11, 51080], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 59, "seek": 29216, "start": 306.48, "end": 311.28000000000003, "text": " we're neglecting considerations of the complexity of operations. So, the question then is, you know,", "tokens": [51080, 321, 434, 17745, 278, 24070, 295, 264, 14024, 295, 7705, 13, 407, 11, 264, 1168, 550, 307, 11, 291, 458, 11, 51320], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 60, "seek": 29216, "start": 311.28000000000003, "end": 316.56, "text": " could we imagine constructing a generalization of category theory, which takes into account", "tokens": [51320, 727, 321, 3811, 39969, 257, 2674, 2144, 295, 7719, 5261, 11, 597, 2516, 666, 2696, 51584], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 61, "seek": 29216, "start": 316.56, "end": 321.20000000000005, "text": " computational complexity. So, here's an example of how that would look, right. So, here you can see", "tokens": [51584, 28270, 14024, 13, 407, 11, 510, 311, 364, 1365, 295, 577, 300, 576, 574, 11, 558, 13, 407, 11, 510, 291, 393, 536, 51816], "temperature": 0.0, "avg_logprob": -0.0997003336421779, "compression_ratio": 1.9930313588850175, "no_speech_prob": 0.001064793555997312}, {"id": 62, "seek": 32120, "start": 321.2, "end": 325.44, "text": " every edge, every morphism has been tagged with certain computational complexity information,", "tokens": [50364, 633, 4691, 11, 633, 25778, 1434, 575, 668, 40239, 365, 1629, 28270, 14024, 1589, 11, 50576], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 63, "seek": 32120, "start": 325.44, "end": 330.47999999999996, "text": " in particular, it's been tagged with a number specifying what is the minimum number of applications", "tokens": [50576, 294, 1729, 11, 309, 311, 668, 40239, 365, 257, 1230, 1608, 5489, 437, 307, 264, 7285, 1230, 295, 5821, 50828], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 64, "seek": 32120, "start": 330.47999999999996, "end": 334.64, "text": " of my transition function, what's the minimum number of elementary computations that I need in", "tokens": [50828, 295, 452, 6034, 2445, 11, 437, 311, 264, 7285, 1230, 295, 16429, 2807, 763, 300, 286, 643, 294, 51036], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 65, "seek": 32120, "start": 334.64, "end": 338.4, "text": " order to evolve from this data structure to this data structure. So, here, to go from here to here,", "tokens": [51036, 1668, 281, 16693, 490, 341, 1412, 3877, 281, 341, 1412, 3877, 13, 407, 11, 510, 11, 281, 352, 490, 510, 281, 510, 11, 51224], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 66, "seek": 32120, "start": 338.4, "end": 342.8, "text": " it's just one, to go from here to here, it's one, to go from here to here, it's one, etc.", "tokens": [51224, 309, 311, 445, 472, 11, 281, 352, 490, 510, 281, 510, 11, 309, 311, 472, 11, 281, 352, 490, 510, 281, 510, 11, 309, 311, 472, 11, 5183, 13, 51444], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 67, "seek": 32120, "start": 343.59999999999997, "end": 348.48, "text": " But to go from here to here directly, it would be three, to go from here to here directly,", "tokens": [51484, 583, 281, 352, 490, 510, 281, 510, 3838, 11, 309, 576, 312, 1045, 11, 281, 352, 490, 510, 281, 510, 3838, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08157663345336914, "compression_ratio": 2.22265625, "no_speech_prob": 0.0038211613427847624}, {"id": 68, "seek": 34848, "start": 348.56, "end": 352.88, "text": " it would be two. And, you know, just for convention, we say that the identity,", "tokens": [50368, 309, 576, 312, 732, 13, 400, 11, 291, 458, 11, 445, 337, 10286, 11, 321, 584, 300, 264, 6575, 11, 50584], "temperature": 0.0, "avg_logprob": -0.15349714534798847, "compression_ratio": 1.8869565217391304, "no_speech_prob": 0.009250749833881855}, {"id": 69, "seek": 34848, "start": 353.6, "end": 357.44, "text": " the identity computation, the trivial computation always has complexity zero.", "tokens": [50620, 264, 6575, 24903, 11, 264, 26703, 24903, 1009, 575, 14024, 4018, 13, 50812], "temperature": 0.0, "avg_logprob": -0.15349714534798847, "compression_ratio": 1.8869565217391304, "no_speech_prob": 0.009250749833881855}, {"id": 70, "seek": 34848, "start": 358.64000000000004, "end": 363.20000000000005, "text": " And then this, so this is, again, a fairly simple mathematical structure, and you can", "tokens": [50872, 400, 550, 341, 11, 370, 341, 307, 11, 797, 11, 257, 6457, 2199, 18894, 3877, 11, 293, 291, 393, 51100], "temperature": 0.0, "avg_logprob": -0.15349714534798847, "compression_ratio": 1.8869565217391304, "no_speech_prob": 0.009250749833881855}, {"id": 71, "seek": 34848, "start": 364.40000000000003, "end": 370.08000000000004, "text": " construct this, again, using purely category theoretic technology by building a particular", "tokens": [51160, 7690, 341, 11, 797, 11, 1228, 17491, 7719, 14308, 299, 2899, 538, 2390, 257, 1729, 51444], "temperature": 0.0, "avg_logprob": -0.15349714534798847, "compression_ratio": 1.8869565217391304, "no_speech_prob": 0.009250749833881855}, {"id": 72, "seek": 34848, "start": 370.08000000000004, "end": 375.52000000000004, "text": " functor from the category of computations, and from the category of data structures and computations", "tokens": [51444, 1019, 1672, 490, 264, 7719, 295, 2807, 763, 11, 293, 490, 264, 7719, 295, 1412, 9227, 293, 2807, 763, 51716], "temperature": 0.0, "avg_logprob": -0.15349714534798847, "compression_ratio": 1.8869565217391304, "no_speech_prob": 0.009250749833881855}, {"id": 73, "seek": 37552, "start": 375.59999999999997, "end": 380.15999999999997, "text": " to a what's called a discrete co-borderism category. And again, we might discuss that later on if", "tokens": [50368, 281, 257, 437, 311, 1219, 257, 27706, 598, 12, 65, 4687, 1434, 7719, 13, 400, 797, 11, 321, 1062, 2248, 300, 1780, 322, 498, 50596], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 74, "seek": 37552, "start": 380.15999999999997, "end": 383.68, "text": " people are interested, but let me not get too bogged down into the technical details of how we", "tokens": [50596, 561, 366, 3102, 11, 457, 718, 385, 406, 483, 886, 26132, 3004, 760, 666, 264, 6191, 4365, 295, 577, 321, 50772], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 75, "seek": 37552, "start": 383.68, "end": 389.03999999999996, "text": " do that. But once you've got this, it gives us immediately a very nice way of characterizing", "tokens": [50772, 360, 300, 13, 583, 1564, 291, 600, 658, 341, 11, 309, 2709, 505, 4258, 257, 588, 1481, 636, 295, 2517, 3319, 51040], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 76, "seek": 37552, "start": 389.03999999999996, "end": 395.76, "text": " phenomena like computational irreducibility. So, there is this idea that has existed in some form", "tokens": [51040, 22004, 411, 28270, 16014, 769, 537, 39802, 13, 407, 11, 456, 307, 341, 1558, 300, 575, 13135, 294, 512, 1254, 51376], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 77, "seek": 37552, "start": 395.76, "end": 399.12, "text": " or another since the very early days of theoretical computer science, since the days of, you know,", "tokens": [51376, 420, 1071, 1670, 264, 588, 2440, 1708, 295, 20864, 3820, 3497, 11, 1670, 264, 1708, 295, 11, 291, 458, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 78, "seek": 37552, "start": 399.12, "end": 404.71999999999997, "text": " girdle and turing and post and church and so on, but was given this term computational irreducibility", "tokens": [51544, 48219, 306, 293, 256, 1345, 293, 2183, 293, 4128, 293, 370, 322, 11, 457, 390, 2212, 341, 1433, 28270, 16014, 769, 537, 39802, 51824], "temperature": 0.0, "avg_logprob": -0.09138361836822939, "compression_ratio": 1.7432835820895523, "no_speech_prob": 0.01031390018761158}, {"id": 79, "seek": 40472, "start": 404.72, "end": 412.16, "text": " by Stephen Wolfram, where the idea is essentially that you just, you know, intuitively, you describe", "tokens": [50364, 538, 13391, 16634, 2356, 11, 689, 264, 1558, 307, 4476, 300, 291, 445, 11, 291, 458, 11, 46506, 11, 291, 6786, 50736], "temperature": 0.0, "avg_logprob": -0.11537308070970619, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0016989103751257062}, {"id": 80, "seek": 40472, "start": 412.16, "end": 416.24, "text": " a computation as being irreducible, or, you know, the result of the computation as being", "tokens": [50736, 257, 24903, 382, 885, 16014, 769, 32128, 11, 420, 11, 291, 458, 11, 264, 1874, 295, 264, 24903, 382, 885, 50940], "temperature": 0.0, "avg_logprob": -0.11537308070970619, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0016989103751257062}, {"id": 81, "seek": 40472, "start": 416.24, "end": 422.40000000000003, "text": " irreducibly complex, if it's not possible to shortcut it in any way, right? So, where, you", "tokens": [50940, 16014, 769, 537, 25021, 3997, 11, 498, 309, 311, 406, 1944, 281, 24822, 309, 294, 604, 636, 11, 558, 30, 407, 11, 689, 11, 291, 51248], "temperature": 0.0, "avg_logprob": -0.11537308070970619, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0016989103751257062}, {"id": 82, "seek": 40472, "start": 422.40000000000003, "end": 426.40000000000003, "text": " know, it takes that computation takes a certain number of steps, and there does not exist a", "tokens": [51248, 458, 11, 309, 2516, 300, 24903, 2516, 257, 1629, 1230, 295, 4439, 11, 293, 456, 775, 406, 2514, 257, 51448], "temperature": 0.0, "avg_logprob": -0.11537308070970619, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0016989103751257062}, {"id": 83, "seek": 40472, "start": 426.40000000000003, "end": 430.72, "text": " shorter computation that would give you the same answer in less time. And one of the nice", "tokens": [51448, 11639, 24903, 300, 576, 976, 291, 264, 912, 1867, 294, 1570, 565, 13, 400, 472, 295, 264, 1481, 51664], "temperature": 0.0, "avg_logprob": -0.11537308070970619, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0016989103751257062}, {"id": 84, "seek": 43072, "start": 430.72, "end": 434.8, "text": " features of thinking about computations and their complexity algebraically like this is that it", "tokens": [50364, 4122, 295, 1953, 466, 2807, 763, 293, 641, 14024, 21989, 984, 411, 341, 307, 300, 309, 50568], "temperature": 0.0, "avg_logprob": -0.10749428943522925, "compression_ratio": 1.9068825910931173, "no_speech_prob": 0.025934891775250435}, {"id": 85, "seek": 43072, "start": 434.8, "end": 439.04, "text": " gives you a purely algebraic characterization of irreducibility. In particular, what it says", "tokens": [50568, 2709, 291, 257, 17491, 21989, 299, 49246, 295, 16014, 769, 537, 39802, 13, 682, 1729, 11, 437, 309, 1619, 50780], "temperature": 0.0, "avg_logprob": -0.10749428943522925, "compression_ratio": 1.9068825910931173, "no_speech_prob": 0.025934891775250435}, {"id": 86, "seek": 43072, "start": 439.04, "end": 444.72, "text": " is that irreducible computations are ones for which the computational complexity acts", "tokens": [50780, 307, 300, 16014, 769, 32128, 2807, 763, 366, 2306, 337, 597, 264, 28270, 14024, 10672, 51064], "temperature": 0.0, "avg_logprob": -0.10749428943522925, "compression_ratio": 1.9068825910931173, "no_speech_prob": 0.025934891775250435}, {"id": 87, "seek": 43072, "start": 444.72, "end": 451.84000000000003, "text": " additive, purely additively under composition. So, if it's the case that if we compose, say, two", "tokens": [51064, 45558, 11, 17491, 45558, 356, 833, 12686, 13, 407, 11, 498, 309, 311, 264, 1389, 300, 498, 321, 35925, 11, 584, 11, 732, 51420], "temperature": 0.0, "avg_logprob": -0.10749428943522925, "compression_ratio": 1.9068825910931173, "no_speech_prob": 0.025934891775250435}, {"id": 88, "seek": 43072, "start": 453.68, "end": 460.40000000000003, "text": " computations of complexity one together, if the resulting composite takes, you know, has complexity", "tokens": [51512, 2807, 763, 295, 14024, 472, 1214, 11, 498, 264, 16505, 25557, 2516, 11, 291, 458, 11, 575, 14024, 51848], "temperature": 0.0, "avg_logprob": -0.10749428943522925, "compression_ratio": 1.9068825910931173, "no_speech_prob": 0.025934891775250435}, {"id": 89, "seek": 46040, "start": 460.96, "end": 464.96, "text": " two, then it's an irreducible computation. If it has complexity less than two, like one,", "tokens": [50392, 732, 11, 550, 309, 311, 364, 16014, 769, 32128, 24903, 13, 759, 309, 575, 14024, 1570, 813, 732, 11, 411, 472, 11, 50592], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 90, "seek": 46040, "start": 464.96, "end": 468.71999999999997, "text": " then that means that we could have jumped directly from the input to the output", "tokens": [50592, 550, 300, 1355, 300, 321, 727, 362, 13864, 3838, 490, 264, 4846, 281, 264, 5598, 50780], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 91, "seek": 46040, "start": 468.71999999999997, "end": 471.84, "text": " without having to pass through the two elementary computations that made it up. So, that would be", "tokens": [50780, 1553, 1419, 281, 1320, 807, 264, 732, 16429, 2807, 763, 300, 1027, 309, 493, 13, 407, 11, 300, 576, 312, 50936], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 92, "seek": 46040, "start": 471.84, "end": 476.23999999999995, "text": " an example of a reducible computation. So, reducible computations are ones whose complexities", "tokens": [50936, 364, 1365, 295, 257, 2783, 32128, 24903, 13, 407, 11, 2783, 32128, 2807, 763, 366, 2306, 6104, 48705, 51156], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 93, "seek": 46040, "start": 476.23999999999995, "end": 482.56, "text": " compose sub-additively in this category theoretic sense. And, okay, so here's an", "tokens": [51156, 35925, 1422, 12, 25224, 2187, 356, 294, 341, 7719, 14308, 299, 2020, 13, 400, 11, 1392, 11, 370, 510, 311, 364, 51472], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 94, "seek": 46040, "start": 484.0, "end": 489.59999999999997, "text": " illustration of showing what intermediate computational states you had to go through", "tokens": [51544, 22645, 295, 4099, 437, 19376, 28270, 4368, 291, 632, 281, 352, 807, 51824], "temperature": 0.0, "avg_logprob": -0.10937525014408299, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.0023951707407832146}, {"id": 95, "seek": 48960, "start": 489.6, "end": 492.8, "text": " in order to get from one data structure to another data structure. So, to go from here", "tokens": [50364, 294, 1668, 281, 483, 490, 472, 1412, 3877, 281, 1071, 1412, 3877, 13, 407, 11, 281, 352, 490, 510, 50524], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 96, "seek": 48960, "start": 492.8, "end": 496.72, "text": " to here, you had to go through steps one to two. To go from here to here, you had to go through", "tokens": [50524, 281, 510, 11, 291, 632, 281, 352, 807, 4439, 472, 281, 732, 13, 1407, 352, 490, 510, 281, 510, 11, 291, 632, 281, 352, 807, 50720], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 97, "seek": 48960, "start": 496.72, "end": 502.72, "text": " steps one, two, and one, two, three, and four, et cetera. So, you can build up a kind of complete", "tokens": [50720, 4439, 472, 11, 732, 11, 293, 472, 11, 732, 11, 1045, 11, 293, 1451, 11, 1030, 11458, 13, 407, 11, 291, 393, 1322, 493, 257, 733, 295, 3566, 51020], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 98, "seek": 48960, "start": 502.72, "end": 507.20000000000005, "text": " algebra of complexity this way, which has some nice properties, which, again, I can talk about,", "tokens": [51020, 21989, 295, 14024, 341, 636, 11, 597, 575, 512, 1481, 7221, 11, 597, 11, 797, 11, 286, 393, 751, 466, 11, 51244], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 99, "seek": 48960, "start": 507.20000000000005, "end": 512.48, "text": " but let me not get too bogged down in mathematical details right now. But here's the thing I really", "tokens": [51244, 457, 718, 385, 406, 483, 886, 26132, 3004, 760, 294, 18894, 4365, 558, 586, 13, 583, 510, 311, 264, 551, 286, 534, 51508], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 100, "seek": 48960, "start": 512.48, "end": 517.44, "text": " want to talk about, which is what happens when you go to multi-way systems. What happens when", "tokens": [51508, 528, 281, 751, 466, 11, 597, 307, 437, 2314, 562, 291, 352, 281, 4825, 12, 676, 3652, 13, 708, 2314, 562, 51756], "temperature": 0.0, "avg_logprob": -0.09028477350870769, "compression_ratio": 1.9453924914675769, "no_speech_prob": 0.00572572136297822}, {"id": 101, "seek": 51744, "start": 517.5200000000001, "end": 522.4000000000001, "text": " you go to non-deterministic computations? So, now, imagine having, instead of just a", "tokens": [50368, 291, 352, 281, 2107, 12, 49136, 259, 3142, 2807, 763, 30, 407, 11, 586, 11, 3811, 1419, 11, 2602, 295, 445, 257, 50612], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 102, "seek": 51744, "start": 522.4000000000001, "end": 527.44, "text": " Turing machine with a single rule, a single transition function that just evolves deterministically", "tokens": [50612, 314, 1345, 3479, 365, 257, 2167, 4978, 11, 257, 2167, 6034, 2445, 300, 445, 43737, 15957, 20458, 50864], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 103, "seek": 51744, "start": 527.44, "end": 530.8000000000001, "text": " with a single thread of time, now imagine having a Turing machine that has, say, two", "tokens": [50864, 365, 257, 2167, 7207, 295, 565, 11, 586, 3811, 1419, 257, 314, 1345, 3479, 300, 575, 11, 584, 11, 732, 51032], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 104, "seek": 51744, "start": 530.8000000000001, "end": 534.8800000000001, "text": " transition functions, like this one and this one. And so, at any given point, it can apply one of", "tokens": [51032, 6034, 6828, 11, 411, 341, 472, 293, 341, 472, 13, 400, 370, 11, 412, 604, 2212, 935, 11, 309, 393, 3079, 472, 295, 51236], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 105, "seek": 51744, "start": 534.8800000000001, "end": 540.96, "text": " the two. And so, now, evolution, instead of just being a single path, becomes this kind of branching", "tokens": [51236, 264, 732, 13, 400, 370, 11, 586, 11, 9303, 11, 2602, 295, 445, 885, 257, 2167, 3100, 11, 3643, 341, 733, 295, 9819, 278, 51540], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 106, "seek": 51744, "start": 540.96, "end": 545.5200000000001, "text": " structure, which, if we didn't have any merging, would be a tree, but because we are merging", "tokens": [51540, 3877, 11, 597, 11, 498, 321, 994, 380, 362, 604, 44559, 11, 576, 312, 257, 4230, 11, 457, 570, 321, 366, 44559, 51768], "temperature": 0.0, "avg_logprob": -0.09472088075019944, "compression_ratio": 1.9278350515463918, "no_speech_prob": 0.003882208839058876}, {"id": 107, "seek": 54552, "start": 545.6, "end": 551.12, "text": " equivalent states, it's actually just a kind of more general directed graph. And so, it looks", "tokens": [50368, 10344, 4368, 11, 309, 311, 767, 445, 257, 733, 295, 544, 2674, 12898, 4295, 13, 400, 370, 11, 309, 1542, 50644], "temperature": 0.0, "avg_logprob": -0.08088290280309217, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.009701777249574661}, {"id": 108, "seek": 54552, "start": 551.12, "end": 556.24, "text": " like this, and this we call a multi-way system. And so, we can build a category out of these", "tokens": [50644, 411, 341, 11, 293, 341, 321, 818, 257, 4825, 12, 676, 1185, 13, 400, 370, 11, 321, 393, 1322, 257, 7719, 484, 295, 613, 50900], "temperature": 0.0, "avg_logprob": -0.08088290280309217, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.009701777249574661}, {"id": 109, "seek": 54552, "start": 556.24, "end": 562.24, "text": " multi-way systems as well. We can build a category using exactly the same rules. So, again, we do", "tokens": [50900, 4825, 12, 676, 3652, 382, 731, 13, 492, 393, 1322, 257, 7719, 1228, 2293, 264, 912, 4474, 13, 407, 11, 797, 11, 321, 360, 51200], "temperature": 0.0, "avg_logprob": -0.08088290280309217, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.009701777249574661}, {"id": 110, "seek": 54552, "start": 562.24, "end": 568.24, "text": " this transitive closure operation. So, we add an edge for every possible composition of these", "tokens": [51200, 341, 1145, 2187, 24653, 6916, 13, 407, 11, 321, 909, 364, 4691, 337, 633, 1944, 12686, 295, 613, 51500], "temperature": 0.0, "avg_logprob": -0.08088290280309217, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.009701777249574661}, {"id": 111, "seek": 54552, "start": 568.24, "end": 573.76, "text": " elementary computations and an identity edge that maps every data structure to itself. But it turns", "tokens": [51500, 16429, 2807, 763, 293, 364, 6575, 4691, 300, 11317, 633, 1412, 3877, 281, 2564, 13, 583, 309, 4523, 51776], "temperature": 0.0, "avg_logprob": -0.08088290280309217, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.009701777249574661}, {"id": 112, "seek": 57376, "start": 573.76, "end": 579.4399999999999, "text": " out this category has even more structure than the single-way system that we showed previously,", "tokens": [50364, 484, 341, 7719, 575, 754, 544, 3877, 813, 264, 2167, 12, 676, 1185, 300, 321, 4712, 8046, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10743324429381128, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0050561800599098206}, {"id": 113, "seek": 57376, "start": 579.4399999999999, "end": 585.36, "text": " because now, it's possible to compose computations not just sequentially in time using ordinary", "tokens": [50648, 570, 586, 11, 309, 311, 1944, 281, 35925, 2807, 763, 406, 445, 5123, 3137, 294, 565, 1228, 10547, 50944], "temperature": 0.0, "avg_logprob": -0.10743324429381128, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0050561800599098206}, {"id": 114, "seek": 57376, "start": 585.36, "end": 590.24, "text": " composition, but it's possible to compose them in parallel across what is sometimes referred", "tokens": [50944, 12686, 11, 457, 309, 311, 1944, 281, 35925, 552, 294, 8952, 2108, 437, 307, 2171, 10839, 51188], "temperature": 0.0, "avg_logprob": -0.10743324429381128, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0050561800599098206}, {"id": 115, "seek": 57376, "start": 590.24, "end": 597.04, "text": " to as branchial space. So, essentially, you're saying instead of, you know, the ordinary morphism", "tokens": [51188, 281, 382, 9819, 831, 1901, 13, 407, 11, 4476, 11, 291, 434, 1566, 2602, 295, 11, 291, 458, 11, 264, 10547, 25778, 1434, 51528], "temperature": 0.0, "avg_logprob": -0.10743324429381128, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0050561800599098206}, {"id": 116, "seek": 57376, "start": 597.04, "end": 601.52, "text": " composition that I showed previously is essentially saying, you know, I apply this", "tokens": [51528, 12686, 300, 286, 4712, 8046, 307, 4476, 1566, 11, 291, 458, 11, 286, 3079, 341, 51752], "temperature": 0.0, "avg_logprob": -0.10743324429381128, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0050561800599098206}, {"id": 117, "seek": 60152, "start": 601.6, "end": 606.88, "text": " elementary computation, then this elementary computation sequentially. Whereas this parallel", "tokens": [50368, 16429, 24903, 11, 550, 341, 16429, 24903, 5123, 3137, 13, 13813, 341, 8952, 50632], "temperature": 0.0, "avg_logprob": -0.1096426712839227, "compression_ratio": 1.949579831932773, "no_speech_prob": 0.0015005838358774781}, {"id": 118, "seek": 60152, "start": 606.88, "end": 611.1999999999999, "text": " composition is saying, I apply this computation and this computation in parallel to the same", "tokens": [50632, 12686, 307, 1566, 11, 286, 3079, 341, 24903, 293, 341, 24903, 294, 8952, 281, 264, 912, 50848], "temperature": 0.0, "avg_logprob": -0.1096426712839227, "compression_ratio": 1.949579831932773, "no_speech_prob": 0.0015005838358774781}, {"id": 119, "seek": 60152, "start": 611.1999999999999, "end": 616.24, "text": " data structure. And so, that parallel operation is what causes these branches, right? Effectively,", "tokens": [50848, 1412, 3877, 13, 400, 370, 11, 300, 8952, 6916, 307, 437, 7700, 613, 14770, 11, 558, 30, 17764, 3413, 11, 51100], "temperature": 0.0, "avg_logprob": -0.1096426712839227, "compression_ratio": 1.949579831932773, "no_speech_prob": 0.0015005838358774781}, {"id": 120, "seek": 60152, "start": 616.24, "end": 621.04, "text": " when you have two threads of time that are branching from the same state, like here,", "tokens": [51100, 562, 291, 362, 732, 19314, 295, 565, 300, 366, 9819, 278, 490, 264, 912, 1785, 11, 411, 510, 11, 51340], "temperature": 0.0, "avg_logprob": -0.1096426712839227, "compression_ratio": 1.949579831932773, "no_speech_prob": 0.0015005838358774781}, {"id": 121, "seek": 60152, "start": 621.84, "end": 627.12, "text": " that's arising because we have chosen to apply this elementary computation and this elementary", "tokens": [51380, 300, 311, 44900, 570, 321, 362, 8614, 281, 3079, 341, 16429, 24903, 293, 341, 16429, 51644], "temperature": 0.0, "avg_logprob": -0.1096426712839227, "compression_ratio": 1.949579831932773, "no_speech_prob": 0.0015005838358774781}, {"id": 122, "seek": 62712, "start": 627.2, "end": 634.32, "text": " computation together in parallel rather than sequentialized in time. And so, here you can see", "tokens": [50368, 24903, 1214, 294, 8952, 2831, 813, 5123, 2549, 1602, 294, 565, 13, 400, 370, 11, 510, 291, 393, 536, 50724], "temperature": 0.0, "avg_logprob": -0.09934396289643788, "compression_ratio": 1.77734375, "no_speech_prob": 0.0027532470412552357}, {"id": 123, "seek": 62712, "start": 635.04, "end": 640.16, "text": " this parallelization indicated using what's referred to as a branchial decomposition,", "tokens": [50760, 341, 8952, 2144, 16176, 1228, 437, 311, 10839, 281, 382, 257, 9819, 831, 48356, 11, 51016], "temperature": 0.0, "avg_logprob": -0.09934396289643788, "compression_ratio": 1.77734375, "no_speech_prob": 0.0027532470412552357}, {"id": 124, "seek": 62712, "start": 640.16, "end": 644.4, "text": " which is just a kind of a visual way of decomposing what's going on between these different threads", "tokens": [51016, 597, 307, 445, 257, 733, 295, 257, 5056, 636, 295, 22867, 6110, 437, 311, 516, 322, 1296, 613, 819, 19314, 51228], "temperature": 0.0, "avg_logprob": -0.09934396289643788, "compression_ratio": 1.77734375, "no_speech_prob": 0.0027532470412552357}, {"id": 125, "seek": 62712, "start": 644.4, "end": 649.2, "text": " of time. And again, there's a purely algebraic characterization of what's going on here,", "tokens": [51228, 295, 565, 13, 400, 797, 11, 456, 311, 257, 17491, 21989, 299, 49246, 295, 437, 311, 516, 322, 510, 11, 51468], "temperature": 0.0, "avg_logprob": -0.09934396289643788, "compression_ratio": 1.77734375, "no_speech_prob": 0.0027532470412552357}, {"id": 126, "seek": 62712, "start": 649.2, "end": 654.0, "text": " which is that what we've done is we've taken our simple category that we started with,", "tokens": [51468, 597, 307, 300, 437, 321, 600, 1096, 307, 321, 600, 2726, 527, 2199, 7719, 300, 321, 1409, 365, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09934396289643788, "compression_ratio": 1.77734375, "no_speech_prob": 0.0027532470412552357}, {"id": 127, "seek": 65400, "start": 654.0, "end": 657.84, "text": " and we've equipped it with a tensor product structure. And so, it's become what we fancily", "tokens": [50364, 293, 321, 600, 15218, 309, 365, 257, 40863, 1674, 3877, 13, 400, 370, 11, 309, 311, 1813, 437, 321, 3429, 537, 356, 50556], "temperature": 0.0, "avg_logprob": -0.14874331734397195, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0009688737918622792}, {"id": 128, "seek": 65400, "start": 657.84, "end": 662.4, "text": " call a monoidal category or actually a symmetric monoidal category. So, the tensor, so we now", "tokens": [50556, 818, 257, 1108, 17079, 304, 7719, 420, 767, 257, 32330, 1108, 17079, 304, 7719, 13, 407, 11, 264, 40863, 11, 370, 321, 586, 50784], "temperature": 0.0, "avg_logprob": -0.14874331734397195, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0009688737918622792}, {"id": 129, "seek": 65400, "start": 662.4, "end": 667.2, "text": " have these two operations. We have sequential composition in time, and we have this tensor", "tokens": [50784, 362, 613, 732, 7705, 13, 492, 362, 42881, 12686, 294, 565, 11, 293, 321, 362, 341, 40863, 51024], "temperature": 0.0, "avg_logprob": -0.14874331734397195, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0009688737918622792}, {"id": 130, "seek": 65400, "start": 667.2, "end": 674.08, "text": " product operation, which is a parallel composition in branchial space. And just like we can have,", "tokens": [51024, 1674, 6916, 11, 597, 307, 257, 8952, 12686, 294, 9819, 831, 1901, 13, 400, 445, 411, 321, 393, 362, 11, 51368], "temperature": 0.0, "avg_logprob": -0.14874331734397195, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0009688737918622792}, {"id": 131, "seek": 65400, "start": 674.08, "end": 679.12, "text": " just like before, where we equipped our edges, our morphisms with certain computational", "tokens": [51368, 445, 411, 949, 11, 689, 321, 15218, 527, 8819, 11, 527, 25778, 13539, 365, 1629, 28270, 51620], "temperature": 0.0, "avg_logprob": -0.14874331734397195, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0009688737918622792}, {"id": 132, "seek": 67912, "start": 679.44, "end": 684.64, "text": " complexity information, we can do the same thing, and we described how those complexities composed", "tokens": [50380, 14024, 1589, 11, 321, 393, 360, 264, 912, 551, 11, 293, 321, 7619, 577, 729, 48705, 18204, 50640], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 133, "seek": 67912, "start": 684.64, "end": 688.88, "text": " sequentially in time. We can do the same thing and describe how the complexities compose in", "tokens": [50640, 5123, 3137, 294, 565, 13, 492, 393, 360, 264, 912, 551, 293, 6786, 577, 264, 48705, 35925, 294, 50852], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 134, "seek": 67912, "start": 688.88, "end": 694.5600000000001, "text": " parallel as one composes morphisms in branchial space. And so, this allows one by exactly the", "tokens": [50852, 8952, 382, 472, 715, 4201, 25778, 13539, 294, 9819, 831, 1901, 13, 400, 370, 11, 341, 4045, 472, 538, 2293, 264, 51136], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 135, "seek": 67912, "start": 694.5600000000001, "end": 699.12, "text": " same token to quantify multi-computational irreducibility rather than just computational", "tokens": [51136, 912, 14862, 281, 40421, 4825, 12, 1112, 2582, 1478, 16014, 769, 537, 39802, 2831, 813, 445, 28270, 51364], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 136, "seek": 67912, "start": 699.12, "end": 704.0, "text": " irreducibility. So, now, multi-computational irreducibility becomes a measure of how additive", "tokens": [51364, 16014, 769, 537, 39802, 13, 407, 11, 586, 11, 4825, 12, 1112, 2582, 1478, 16014, 769, 537, 39802, 3643, 257, 3481, 295, 577, 45558, 51608], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 137, "seek": 67912, "start": 704.0, "end": 708.64, "text": " or sub-additive your time complexities are when you compose them in parallel through the tensor", "tokens": [51608, 420, 1422, 12, 25224, 2187, 428, 565, 48705, 366, 562, 291, 35925, 552, 294, 8952, 807, 264, 40863, 51840], "temperature": 0.0, "avg_logprob": -0.08198193986286488, "compression_ratio": 2.062271062271062, "no_speech_prob": 0.03729570657014847}, {"id": 138, "seek": 70864, "start": 708.64, "end": 714.64, "text": " products rather than just in sequence through standard morphism composition. Okay, but I promise", "tokens": [50364, 3383, 2831, 813, 445, 294, 8310, 807, 3832, 25778, 1434, 12686, 13, 1033, 11, 457, 286, 6228, 50664], "temperature": 0.0, "avg_logprob": -0.1211395263671875, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0017806354444473982}, {"id": 139, "seek": 70864, "start": 714.64, "end": 718.56, "text": " I am cut, and so here's an analogous diagram to the one I showed before showing all the kind of", "tokens": [50664, 286, 669, 1723, 11, 293, 370, 510, 311, 364, 16660, 563, 10686, 281, 264, 472, 286, 4712, 949, 4099, 439, 264, 733, 295, 50860], "temperature": 0.0, "avg_logprob": -0.1211395263671875, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0017806354444473982}, {"id": 140, "seek": 70864, "start": 718.56, "end": 728.96, "text": " intermediate steps that are being applied when one constructs computations or indeed multi-computations", "tokens": [50860, 19376, 4439, 300, 366, 885, 6456, 562, 472, 7690, 82, 2807, 763, 420, 6451, 4825, 12, 1112, 2582, 763, 51380], "temperature": 0.0, "avg_logprob": -0.1211395263671875, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0017806354444473982}, {"id": 141, "seek": 70864, "start": 728.96, "end": 733.92, "text": " by composing elementary computations both sequentially in time and in parallel in branchial", "tokens": [51380, 538, 715, 6110, 16429, 2807, 763, 1293, 5123, 3137, 294, 565, 293, 294, 8952, 294, 9819, 831, 51628], "temperature": 0.0, "avg_logprob": -0.1211395263671875, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0017806354444473982}, {"id": 142, "seek": 73392, "start": 733.92, "end": 739.8399999999999, "text": " space. Now, but I promise I am the point that I'm trying to get to is that it turns out that in", "tokens": [50364, 1901, 13, 823, 11, 457, 286, 6228, 286, 669, 264, 935, 300, 286, 478, 1382, 281, 483, 281, 307, 300, 309, 4523, 484, 300, 294, 50660], "temperature": 0.0, "avg_logprob": -0.1072379571420175, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.016897086054086685}, {"id": 143, "seek": 73392, "start": 739.8399999999999, "end": 743.8399999999999, "text": " addition to just being a useful way to think about computational complexity theory and to formulate", "tokens": [50660, 4500, 281, 445, 885, 257, 4420, 636, 281, 519, 466, 28270, 14024, 5261, 293, 281, 47881, 50860], "temperature": 0.0, "avg_logprob": -0.1072379571420175, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.016897086054086685}, {"id": 144, "seek": 73392, "start": 743.8399999999999, "end": 748.24, "text": " complexity classes like, you know, polynomial time on non-deterministic polynomial time, etc.,", "tokens": [50860, 14024, 5359, 411, 11, 291, 458, 11, 26110, 565, 322, 2107, 12, 49136, 259, 3142, 26110, 565, 11, 5183, 7933, 51080], "temperature": 0.0, "avg_logprob": -0.1072379571420175, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.016897086054086685}, {"id": 145, "seek": 73392, "start": 748.24, "end": 751.68, "text": " it turns out this is also an interesting way to think about the role of observation", "tokens": [51080, 309, 4523, 484, 341, 307, 611, 364, 1880, 636, 281, 519, 466, 264, 3090, 295, 14816, 51252], "temperature": 0.0, "avg_logprob": -0.1072379571420175, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.016897086054086685}, {"id": 146, "seek": 73392, "start": 752.56, "end": 759.76, "text": " in sort of computational models of reality. Because so here's where I'm going to get a", "tokens": [51296, 294, 1333, 295, 28270, 5245, 295, 4103, 13, 1436, 370, 510, 311, 689, 286, 478, 516, 281, 483, 257, 51656], "temperature": 0.0, "avg_logprob": -0.1072379571420175, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.016897086054086685}, {"id": 147, "seek": 75976, "start": 759.76, "end": 764.4, "text": " little bit philosophical, and I don't immediately have a slide or a graphic that I can show to", "tokens": [50364, 707, 857, 25066, 11, 293, 286, 500, 380, 4258, 362, 257, 4137, 420, 257, 14089, 300, 286, 393, 855, 281, 50596], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 148, "seek": 75976, "start": 764.4, "end": 769.04, "text": " illustrate this point. But so when we think about modeling a system computationally,", "tokens": [50596, 23221, 341, 935, 13, 583, 370, 562, 321, 519, 466, 15983, 257, 1185, 24903, 379, 11, 50828], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 149, "seek": 75976, "start": 770.3199999999999, "end": 773.6, "text": " one has to bear in mind that there are really two computations going on, right? There's the", "tokens": [50892, 472, 575, 281, 6155, 294, 1575, 300, 456, 366, 534, 732, 2807, 763, 516, 322, 11, 558, 30, 821, 311, 264, 51056], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 150, "seek": 75976, "start": 773.6, "end": 778.08, "text": " computation that the system is itself performing, and then there's the computation that the observer,", "tokens": [51056, 24903, 300, 264, 1185, 307, 2564, 10205, 11, 293, 550, 456, 311, 264, 24903, 300, 264, 27878, 11, 51280], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 151, "seek": 75976, "start": 778.08, "end": 782.72, "text": " the person who is measuring that system and concluding things from it, there's the computation", "tokens": [51280, 264, 954, 567, 307, 13389, 300, 1185, 293, 9312, 278, 721, 490, 309, 11, 456, 311, 264, 24903, 51512], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 152, "seek": 75976, "start": 782.72, "end": 788.72, "text": " that they are performing. And somehow, you know, so when we construct models of reality or when we", "tokens": [51512, 300, 436, 366, 10205, 13, 400, 6063, 11, 291, 458, 11, 370, 562, 321, 7690, 5245, 295, 4103, 420, 562, 321, 51812], "temperature": 0.0, "avg_logprob": -0.11759625375270844, "compression_ratio": 1.9417808219178083, "no_speech_prob": 0.01742609404027462}, {"id": 153, "seek": 78872, "start": 788.72, "end": 794.0, "text": " construct models of systems, you know, and we want to describe kind of at a meta level what we're", "tokens": [50364, 7690, 5245, 295, 3652, 11, 291, 458, 11, 293, 321, 528, 281, 6786, 733, 295, 412, 257, 19616, 1496, 437, 321, 434, 50628], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 154, "seek": 78872, "start": 794.0, "end": 798.96, "text": " doing in computational terms, there's our own computation that, you know, that's going on", "tokens": [50628, 884, 294, 28270, 2115, 11, 456, 311, 527, 1065, 24903, 300, 11, 291, 458, 11, 300, 311, 516, 322, 50876], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 155, "seek": 78872, "start": 798.96, "end": 802.72, "text": " inside our own internal representation of the world. And then there's presumably some external", "tokens": [50876, 1854, 527, 1065, 6920, 10290, 295, 264, 1002, 13, 400, 550, 456, 311, 26742, 512, 8320, 51064], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 156, "seek": 78872, "start": 802.72, "end": 807.12, "text": " computation that's going on outside. And then when we make observations and when we make measurements,", "tokens": [51064, 24903, 300, 311, 516, 322, 2380, 13, 400, 550, 562, 321, 652, 18163, 293, 562, 321, 652, 15383, 11, 51284], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 157, "seek": 78872, "start": 807.12, "end": 811.44, "text": " when we construct theoretical models, what we're doing is we're somehow constructing some kind", "tokens": [51284, 562, 321, 7690, 20864, 5245, 11, 437, 321, 434, 884, 307, 321, 434, 6063, 39969, 512, 733, 51500], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 158, "seek": 78872, "start": 811.44, "end": 816.64, "text": " of encoding function that allows us to take a concrete physical state of the system we're", "tokens": [51500, 295, 43430, 2445, 300, 4045, 505, 281, 747, 257, 9859, 4001, 1785, 295, 264, 1185, 321, 434, 51760], "temperature": 0.0, "avg_logprob": -0.05938874137017035, "compression_ratio": 2.0357142857142856, "no_speech_prob": 0.0010981801897287369}, {"id": 159, "seek": 81664, "start": 816.64, "end": 821.76, "text": " observing and encode it as some abstract state of the internal model that we have of what's going on.", "tokens": [50364, 22107, 293, 2058, 1429, 309, 382, 512, 12649, 1785, 295, 264, 6920, 2316, 300, 321, 362, 295, 437, 311, 516, 322, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10577706173733548, "compression_ratio": 1.959016393442623, "no_speech_prob": 0.0028880538884550333}, {"id": 160, "seek": 81664, "start": 822.64, "end": 828.72, "text": " And that's all very well. But then one, but then now we don't just have one computation to care", "tokens": [50664, 400, 300, 311, 439, 588, 731, 13, 583, 550, 472, 11, 457, 550, 586, 321, 500, 380, 445, 362, 472, 24903, 281, 1127, 50968], "temperature": 0.0, "avg_logprob": -0.10577706173733548, "compression_ratio": 1.959016393442623, "no_speech_prob": 0.0028880538884550333}, {"id": 161, "seek": 81664, "start": 828.72, "end": 832.48, "text": " about, we have three, right? We've got the computation of the system, computation of the", "tokens": [50968, 466, 11, 321, 362, 1045, 11, 558, 30, 492, 600, 658, 264, 24903, 295, 264, 1185, 11, 24903, 295, 264, 51156], "temperature": 0.0, "avg_logprob": -0.10577706173733548, "compression_ratio": 1.959016393442623, "no_speech_prob": 0.0028880538884550333}, {"id": 162, "seek": 81664, "start": 832.48, "end": 836.96, "text": " observer, and the computation of this encoding function computation that's responsible for their,", "tokens": [51156, 27878, 11, 293, 264, 24903, 295, 341, 43430, 2445, 24903, 300, 311, 6250, 337, 641, 11, 51380], "temperature": 0.0, "avg_logprob": -0.10577706173733548, "compression_ratio": 1.959016393442623, "no_speech_prob": 0.0028880538884550333}, {"id": 163, "seek": 81664, "start": 836.96, "end": 842.4, "text": " for their, you know, the interface between their internal model of the world and the external", "tokens": [51380, 337, 641, 11, 291, 458, 11, 264, 9226, 1296, 641, 6920, 2316, 295, 264, 1002, 293, 264, 8320, 51652], "temperature": 0.0, "avg_logprob": -0.10577706173733548, "compression_ratio": 1.959016393442623, "no_speech_prob": 0.0028880538884550333}, {"id": 164, "seek": 84240, "start": 842.4, "end": 847.1999999999999, "text": " reality. And the computational complexities of these computations into play in an extremely", "tokens": [50364, 4103, 13, 400, 264, 28270, 48705, 295, 613, 2807, 763, 666, 862, 294, 364, 4664, 50604], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 165, "seek": 84240, "start": 847.1999999999999, "end": 851.52, "text": " interesting way. And so the, you know, part of the reason for trying to develop this algebraic", "tokens": [50604, 1880, 636, 13, 400, 370, 264, 11, 291, 458, 11, 644, 295, 264, 1778, 337, 1382, 281, 1499, 341, 21989, 299, 50820], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 166, "seek": 84240, "start": 851.52, "end": 855.36, "text": " semantics for thinking about computational complexity and multi computational complexity", "tokens": [50820, 4361, 45298, 337, 1953, 466, 28270, 14024, 293, 4825, 28270, 14024, 51012], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 167, "seek": 84240, "start": 855.36, "end": 861.12, "text": " was to try to give one a systematic way to reason about exactly this three-way interplay between", "tokens": [51012, 390, 281, 853, 281, 976, 472, 257, 27249, 636, 281, 1778, 466, 2293, 341, 1045, 12, 676, 728, 2858, 1296, 51300], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 168, "seek": 84240, "start": 861.12, "end": 867.84, "text": " systems, observers, and encoding functions. And so in particular, when we make when an", "tokens": [51300, 3652, 11, 48090, 11, 293, 43430, 6828, 13, 400, 370, 294, 1729, 11, 562, 321, 652, 562, 364, 51636], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 169, "seek": 84240, "start": 867.84, "end": 871.92, "text": " observer makes a model of the world, one thing that they're doing is that they are, you know,", "tokens": [51636, 27878, 1669, 257, 2316, 295, 264, 1002, 11, 472, 551, 300, 436, 434, 884, 307, 300, 436, 366, 11, 291, 458, 11, 51840], "temperature": 0.0, "avg_logprob": -0.10799030430060773, "compression_ratio": 1.9335664335664335, "no_speech_prob": 0.015869559720158577}, {"id": 170, "seek": 87192, "start": 872.16, "end": 876.16, "text": " for any, for any model, isn't just, you know, a complete description of reality,", "tokens": [50376, 337, 604, 11, 337, 604, 2316, 11, 1943, 380, 445, 11, 291, 458, 11, 257, 3566, 3855, 295, 4103, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 171, "seek": 87192, "start": 876.16, "end": 878.9599999999999, "text": " there's a certain amount of coarse-graining, right? There's a certain amount of", "tokens": [50576, 456, 311, 257, 1629, 2372, 295, 39312, 12, 20735, 1760, 11, 558, 30, 821, 311, 257, 1629, 2372, 295, 50716], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 172, "seek": 87192, "start": 880.9599999999999, "end": 884.64, "text": " taking a bunch of states that in the system itself are distinguished,", "tokens": [50816, 1940, 257, 3840, 295, 4368, 300, 294, 264, 1185, 2564, 366, 21702, 11, 51000], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 173, "seek": 87192, "start": 884.64, "end": 889.12, "text": " but in the internal model are treated as the same, they're kind of, you know, they're cast in the", "tokens": [51000, 457, 294, 264, 6920, 2316, 366, 8668, 382, 264, 912, 11, 436, 434, 733, 295, 11, 291, 458, 11, 436, 434, 4193, 294, 264, 51224], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 174, "seek": 87192, "start": 889.12, "end": 893.8399999999999, "text": " same bucket. So in some sense, you know, how coarse a model is, is a measure of how much the", "tokens": [51224, 912, 13058, 13, 407, 294, 512, 2020, 11, 291, 458, 11, 577, 39312, 257, 2316, 307, 11, 307, 257, 3481, 295, 577, 709, 264, 51460], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 175, "seek": 87192, "start": 893.8399999999999, "end": 898.4, "text": " encoding function fails to be subjective, right? And so again, there's a kind of algebraic or", "tokens": [51460, 43430, 2445, 18199, 281, 312, 25972, 11, 558, 30, 400, 370, 797, 11, 456, 311, 257, 733, 295, 21989, 299, 420, 51688], "temperature": 0.0, "avg_logprob": -0.12813969364872685, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.0006872627418488264}, {"id": 176, "seek": 89840, "start": 898.4, "end": 904.72, "text": " category theoretic characterization of what's going on, that, you know, the fewer of your", "tokens": [50364, 7719, 14308, 299, 49246, 295, 437, 311, 516, 322, 11, 300, 11, 291, 458, 11, 264, 13366, 295, 428, 50680], "temperature": 0.0, "avg_logprob": -0.12436045692080543, "compression_ratio": 1.8306451612903225, "no_speech_prob": 0.0015449816128239036}, {"id": 177, "seek": 89840, "start": 904.72, "end": 910.3199999999999, "text": " morphisms are epimorphisms, the more coarse your model is, the more abstract or idealized", "tokens": [50680, 25778, 13539, 366, 2388, 332, 18191, 13539, 11, 264, 544, 39312, 428, 2316, 307, 11, 264, 544, 12649, 420, 7157, 1602, 50960], "temperature": 0.0, "avg_logprob": -0.12436045692080543, "compression_ratio": 1.8306451612903225, "no_speech_prob": 0.0015449816128239036}, {"id": 178, "seek": 89840, "start": 910.3199999999999, "end": 916.48, "text": " your model of reality is. And so then the interesting thing is that this characterization", "tokens": [50960, 428, 2316, 295, 4103, 307, 13, 400, 370, 550, 264, 1880, 551, 307, 300, 341, 49246, 51268], "temperature": 0.0, "avg_logprob": -0.12436045692080543, "compression_ratio": 1.8306451612903225, "no_speech_prob": 0.0015449816128239036}, {"id": 179, "seek": 89840, "start": 916.48, "end": 921.52, "text": " of multi computational irreducibility, this measure of how additive or sub-additive your", "tokens": [51268, 295, 4825, 28270, 16014, 769, 537, 39802, 11, 341, 3481, 295, 577, 45558, 420, 1422, 12, 25224, 2187, 428, 51520], "temperature": 0.0, "avg_logprob": -0.12436045692080543, "compression_ratio": 1.8306451612903225, "no_speech_prob": 0.0015449816128239036}, {"id": 180, "seek": 89840, "start": 921.52, "end": 926.3199999999999, "text": " complexities are, as you compose them together in parallel, gives you a measure of the relative", "tokens": [51520, 48705, 366, 11, 382, 291, 35925, 552, 1214, 294, 8952, 11, 2709, 291, 257, 3481, 295, 264, 4972, 51760], "temperature": 0.0, "avg_logprob": -0.12436045692080543, "compression_ratio": 1.8306451612903225, "no_speech_prob": 0.0015449816128239036}, {"id": 181, "seek": 92632, "start": 926.32, "end": 930.72, "text": " complexity of the evolution function, that is the function that evolves your computation", "tokens": [50364, 14024, 295, 264, 9303, 2445, 11, 300, 307, 264, 2445, 300, 43737, 428, 24903, 50584], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 182, "seek": 92632, "start": 930.72, "end": 935.44, "text": " forwards in time, versus the equivalence function, that is the function that declares that two", "tokens": [50584, 30126, 294, 565, 11, 5717, 264, 9052, 655, 2445, 11, 300, 307, 264, 2445, 300, 979, 19415, 300, 732, 50820], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 183, "seek": 92632, "start": 935.44, "end": 940.1600000000001, "text": " computational states, two data structures, are to be treated as equivalent. And that interplay,", "tokens": [50820, 28270, 4368, 11, 732, 1412, 9227, 11, 366, 281, 312, 8668, 382, 10344, 13, 400, 300, 728, 2858, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 184, "seek": 92632, "start": 940.72, "end": 945.6800000000001, "text": " I claim, is a kind of abstract meta way of thinking about the interplay between the", "tokens": [51084, 286, 3932, 11, 307, 257, 733, 295, 12649, 19616, 636, 295, 1953, 466, 264, 728, 2858, 1296, 264, 51332], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 185, "seek": 92632, "start": 945.6800000000001, "end": 949.7600000000001, "text": " computation of systems versus the computation of observers, because, you know, so in a sense,", "tokens": [51332, 24903, 295, 3652, 5717, 264, 24903, 295, 48090, 11, 570, 11, 291, 458, 11, 370, 294, 257, 2020, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 186, "seek": 92632, "start": 949.7600000000001, "end": 954.96, "text": " the role of the system is to evolve forwards in time, whereas the role of the observer is to take", "tokens": [51536, 264, 3090, 295, 264, 1185, 307, 281, 16693, 30126, 294, 565, 11, 9735, 264, 3090, 295, 264, 27878, 307, 281, 747, 51796], "temperature": 0.0, "avg_logprob": -0.10818691407480548, "compression_ratio": 2.1102661596958177, "no_speech_prob": 0.007796487305313349}, {"id": 187, "seek": 95496, "start": 955.76, "end": 962.48, "text": " states in the system that are distinguished in reality and say, you know, subject to my", "tokens": [50404, 4368, 294, 264, 1185, 300, 366, 21702, 294, 4103, 293, 584, 11, 291, 458, 11, 3983, 281, 452, 50740], "temperature": 0.0, "avg_logprob": -0.0980371680914187, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0025852899998426437}, {"id": 188, "seek": 95496, "start": 962.48, "end": 967.2, "text": " idealized model, I'm going to treat these as the same. So the system is defining the evolution", "tokens": [50740, 7157, 1602, 2316, 11, 286, 478, 516, 281, 2387, 613, 382, 264, 912, 13, 407, 264, 1185, 307, 17827, 264, 9303, 50976], "temperature": 0.0, "avg_logprob": -0.0980371680914187, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0025852899998426437}, {"id": 189, "seek": 95496, "start": 967.2, "end": 972.24, "text": " function, but the observer is defining this equivalence function. And so then the tradeoff", "tokens": [50976, 2445, 11, 457, 264, 27878, 307, 17827, 341, 9052, 655, 2445, 13, 400, 370, 550, 264, 4923, 4506, 51228], "temperature": 0.0, "avg_logprob": -0.0980371680914187, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0025852899998426437}, {"id": 190, "seek": 95496, "start": 972.24, "end": 980.32, "text": " in their complexities becomes exactly a tradeoff between what are the algebraic rules that describe", "tokens": [51228, 294, 641, 48705, 3643, 2293, 257, 4923, 4506, 1296, 437, 366, 264, 21989, 299, 4474, 300, 6786, 51632], "temperature": 0.0, "avg_logprob": -0.0980371680914187, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0025852899998426437}, {"id": 191, "seek": 95496, "start": 980.32, "end": 984.08, "text": " the complexities as they compose sequentially, versus the algebraic rules that describe the", "tokens": [51632, 264, 48705, 382, 436, 35925, 5123, 3137, 11, 5717, 264, 21989, 299, 4474, 300, 6786, 264, 51820], "temperature": 0.0, "avg_logprob": -0.0980371680914187, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0025852899998426437}, {"id": 192, "seek": 98408, "start": 984.08, "end": 989.84, "text": " complexities as they compose under this tensor product operation. And so I've shown this in", "tokens": [50364, 48705, 382, 436, 35925, 833, 341, 40863, 1674, 6916, 13, 400, 370, 286, 600, 4898, 341, 294, 50652], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 193, "seek": 98408, "start": 989.84, "end": 995.0400000000001, "text": " particular for Turing machine systems, but this is a very general kind of algebraic semantics,", "tokens": [50652, 1729, 337, 314, 1345, 3479, 3652, 11, 457, 341, 307, 257, 588, 2674, 733, 295, 21989, 299, 4361, 45298, 11, 50912], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 194, "seek": 98408, "start": 995.0400000000001, "end": 998.32, "text": " you can apply it to hypergraphs, you can apply it to combinators, lambda calculus,", "tokens": [50912, 291, 393, 3079, 309, 281, 9848, 34091, 82, 11, 291, 393, 3079, 309, 281, 38514, 3391, 11, 13607, 33400, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 195, "seek": 98408, "start": 998.32, "end": 1005.0400000000001, "text": " doesn't matter. In a sense, there is just one category up to isomorphism of data structures", "tokens": [51076, 1177, 380, 1871, 13, 682, 257, 2020, 11, 456, 307, 445, 472, 7719, 493, 281, 307, 32702, 1434, 295, 1412, 9227, 51412], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 196, "seek": 98408, "start": 1005.0400000000001, "end": 1008.88, "text": " and computations, and there are simply many different ways of parameterizing what that", "tokens": [51412, 293, 2807, 763, 11, 293, 456, 366, 2935, 867, 819, 2098, 295, 13075, 3319, 437, 300, 51604], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 197, "seek": 98408, "start": 1008.88, "end": 1013.2800000000001, "text": " category is doing through things like Turing machines or hypergraphs or whatever. The", "tokens": [51604, 7719, 307, 884, 807, 721, 411, 314, 1345, 8379, 420, 9848, 34091, 82, 420, 2035, 13, 440, 51824], "temperature": 0.0, "avg_logprob": -0.11069472503662109, "compression_ratio": 1.706070287539936, "no_speech_prob": 0.0010641084518283606}, {"id": 198, "seek": 101328, "start": 1013.28, "end": 1017.76, "text": " algebraic formalism transcends the particular details of the computations that one's dealing with.", "tokens": [50364, 21989, 299, 9860, 1434, 43800, 2581, 264, 1729, 4365, 295, 264, 2807, 763, 300, 472, 311, 6260, 365, 13, 50588], "temperature": 0.0, "avg_logprob": -0.10525215083155139, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0020802756771445274}, {"id": 199, "seek": 101328, "start": 1018.56, "end": 1023.36, "text": " And so yeah, as I say, what one ends up with is, I think, a fairly general formalism for", "tokens": [50628, 400, 370, 1338, 11, 382, 286, 584, 11, 437, 472, 5314, 493, 365, 307, 11, 286, 519, 11, 257, 6457, 2674, 9860, 1434, 337, 50868], "temperature": 0.0, "avg_logprob": -0.10525215083155139, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0020802756771445274}, {"id": 200, "seek": 101328, "start": 1023.36, "end": 1029.44, "text": " thinking about the interplay between observers and the systems that they observe. And that gives", "tokens": [50868, 1953, 466, 264, 728, 2858, 1296, 48090, 293, 264, 3652, 300, 436, 11441, 13, 400, 300, 2709, 51172], "temperature": 0.0, "avg_logprob": -0.10525215083155139, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0020802756771445274}, {"id": 201, "seek": 101328, "start": 1029.44, "end": 1034.72, "text": " one a, I promise I'll stop monologuing in a moment and we'll try and pick apart what I'm", "tokens": [51172, 472, 257, 11, 286, 6228, 286, 603, 1590, 1108, 1132, 9635, 294, 257, 1623, 293, 321, 603, 853, 293, 1888, 4936, 437, 286, 478, 51436], "temperature": 0.0, "avg_logprob": -0.10525215083155139, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0020802756771445274}, {"id": 202, "seek": 101328, "start": 1034.72, "end": 1040.24, "text": " really talking about here. But so I'll just conclude with, you know, once one has that", "tokens": [51436, 534, 1417, 466, 510, 13, 583, 370, 286, 603, 445, 16886, 365, 11, 291, 458, 11, 1564, 472, 575, 300, 51712], "temperature": 0.0, "avg_logprob": -0.10525215083155139, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0020802756771445274}, {"id": 203, "seek": 104024, "start": 1040.24, "end": 1045.76, "text": " algebraic semantics, a whole bunch of things which I think previously would have been, at least to", "tokens": [50364, 21989, 299, 4361, 45298, 11, 257, 1379, 3840, 295, 721, 597, 286, 519, 8046, 576, 362, 668, 11, 412, 1935, 281, 50640], "temperature": 0.0, "avg_logprob": -0.08711984842130453, "compression_ratio": 1.734375, "no_speech_prob": 0.0012815409572795033}, {"id": 204, "seek": 104024, "start": 1045.76, "end": 1050.64, "text": " me, previously seemed like kind of fundamental confusions about, you know, how scientific", "tokens": [50640, 385, 11, 8046, 6576, 411, 733, 295, 8088, 1497, 27255, 466, 11, 291, 458, 11, 577, 8134, 50884], "temperature": 0.0, "avg_logprob": -0.08711984842130453, "compression_ratio": 1.734375, "no_speech_prob": 0.0012815409572795033}, {"id": 205, "seek": 104024, "start": 1050.64, "end": 1054.72, "text": " observation works and how it interplays with computational models, those confusions kind of", "tokens": [50884, 14816, 1985, 293, 577, 309, 728, 45755, 365, 28270, 5245, 11, 729, 1497, 27255, 733, 295, 51088], "temperature": 0.0, "avg_logprob": -0.08711984842130453, "compression_ratio": 1.734375, "no_speech_prob": 0.0012815409572795033}, {"id": 206, "seek": 104024, "start": 1054.72, "end": 1058.96, "text": " become much easier to clarify once you think about it in this kind of more compositional way. So", "tokens": [51088, 1813, 709, 3571, 281, 17594, 1564, 291, 519, 466, 309, 294, 341, 733, 295, 544, 10199, 2628, 636, 13, 407, 51300], "temperature": 0.0, "avg_logprob": -0.08711984842130453, "compression_ratio": 1.734375, "no_speech_prob": 0.0012815409572795033}, {"id": 207, "seek": 104024, "start": 1060.08, "end": 1065.52, "text": " to give a very simple example, or kind of very degenerate example,", "tokens": [51356, 281, 976, 257, 588, 2199, 1365, 11, 420, 733, 295, 588, 40520, 473, 1365, 11, 51628], "temperature": 0.0, "avg_logprob": -0.08711984842130453, "compression_ratio": 1.734375, "no_speech_prob": 0.0012815409572795033}, {"id": 208, "seek": 106552, "start": 1065.92, "end": 1074.0, "text": " you can, you know, within this algebraic semantics, you can effectively trade off the", "tokens": [50384, 291, 393, 11, 291, 458, 11, 1951, 341, 21989, 299, 4361, 45298, 11, 291, 393, 8659, 4923, 766, 264, 50788], "temperature": 0.0, "avg_logprob": -0.13822741788976334, "compression_ratio": 1.855, "no_speech_prob": 0.0036976649425923824}, {"id": 209, "seek": 106552, "start": 1074.8, "end": 1078.6399999999999, "text": " computational complexity of the system for the computational complexity of the observer, right?", "tokens": [50828, 28270, 14024, 295, 264, 1185, 337, 264, 28270, 14024, 295, 264, 27878, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.13822741788976334, "compression_ratio": 1.855, "no_speech_prob": 0.0036976649425923824}, {"id": 210, "seek": 106552, "start": 1078.6399999999999, "end": 1086.16, "text": " So you can have, you can have kind of, in effect, two degenerate cases. You can have the case where", "tokens": [51020, 407, 291, 393, 362, 11, 291, 393, 362, 733, 295, 11, 294, 1802, 11, 732, 40520, 473, 3331, 13, 509, 393, 362, 264, 1389, 689, 51396], "temperature": 0.0, "avg_logprob": -0.13822741788976334, "compression_ratio": 1.855, "no_speech_prob": 0.0036976649425923824}, {"id": 211, "seek": 106552, "start": 1086.16, "end": 1091.28, "text": " the system itself has a completely trivial evolution function. The system itself has, you", "tokens": [51396, 264, 1185, 2564, 575, 257, 2584, 26703, 9303, 2445, 13, 440, 1185, 2564, 575, 11, 291, 51652], "temperature": 0.0, "avg_logprob": -0.13822741788976334, "compression_ratio": 1.855, "no_speech_prob": 0.0036976649425923824}, {"id": 212, "seek": 109128, "start": 1091.28, "end": 1096.6399999999999, "text": " know, is doing something completely elementary in its, in how it evolves. But then the observer", "tokens": [50364, 458, 11, 307, 884, 746, 2584, 16429, 294, 1080, 11, 294, 577, 309, 43737, 13, 583, 550, 264, 27878, 50632], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 213, "seek": 109128, "start": 1097.28, "end": 1100.8799999999999, "text": " has some incredibly complicated equivalence function that makes the system look like it's", "tokens": [50664, 575, 512, 6252, 6179, 9052, 655, 2445, 300, 1669, 264, 1185, 574, 411, 309, 311, 50844], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 214, "seek": 109128, "start": 1100.8799999999999, "end": 1104.08, "text": " doing something really complicated, even though what it's actually doing is something very simple.", "tokens": [50844, 884, 746, 534, 6179, 11, 754, 1673, 437, 309, 311, 767, 884, 307, 746, 588, 2199, 13, 51004], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 215, "seek": 109128, "start": 1105.36, "end": 1109.44, "text": " And so then you, so you have the phenomenon where actually kind of all of the complexity is in the", "tokens": [51068, 400, 370, 550, 291, 11, 370, 291, 362, 264, 14029, 689, 767, 733, 295, 439, 295, 264, 14024, 307, 294, 264, 51272], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 216, "seek": 109128, "start": 1109.44, "end": 1113.68, "text": " eye of the, is in the eye of the observer. You can also have the other degenerate case where the", "tokens": [51272, 3313, 295, 264, 11, 307, 294, 264, 3313, 295, 264, 27878, 13, 509, 393, 611, 362, 264, 661, 40520, 473, 1389, 689, 264, 51484], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 217, "seek": 109128, "start": 1113.68, "end": 1117.84, "text": " observer is doing something absolutely trivial, where the, you know, the encoding function or the", "tokens": [51484, 27878, 307, 884, 746, 3122, 26703, 11, 689, 264, 11, 291, 458, 11, 264, 43430, 2445, 420, 264, 51692], "temperature": 0.0, "avg_logprob": -0.09307407018706554, "compression_ratio": 1.993103448275862, "no_speech_prob": 0.0010000565089285374}, {"id": 218, "seek": 111784, "start": 1117.9199999999998, "end": 1121.6, "text": " observer's own internal representation is just an identity function or something. So there's no", "tokens": [50368, 27878, 311, 1065, 6920, 10290, 307, 445, 364, 6575, 2445, 420, 746, 13, 407, 456, 311, 572, 50552], "temperature": 0.0, "avg_logprob": -0.08588369316029772, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.000802882423158735}, {"id": 219, "seek": 111784, "start": 1121.6, "end": 1125.4399999999998, "text": " complexity there, but the system is doing something incredibly complex. It's doing some,", "tokens": [50552, 14024, 456, 11, 457, 264, 1185, 307, 884, 746, 6252, 3997, 13, 467, 311, 884, 512, 11, 50744], "temperature": 0.0, "avg_logprob": -0.08588369316029772, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.000802882423158735}, {"id": 220, "seek": 111784, "start": 1125.4399999999998, "end": 1129.9199999999998, "text": " some totally, some really sophisticated universal computation. And so that will also appear very", "tokens": [50744, 512, 3879, 11, 512, 534, 16950, 11455, 24903, 13, 400, 370, 300, 486, 611, 4204, 588, 50968], "temperature": 0.0, "avg_logprob": -0.08588369316029772, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.000802882423158735}, {"id": 221, "seek": 111784, "start": 1129.9199999999998, "end": 1136.0, "text": " complex to that observer. And so, and you can also have any kind of, any intermediate, you know,", "tokens": [50968, 3997, 281, 300, 27878, 13, 400, 370, 11, 293, 291, 393, 611, 362, 604, 733, 295, 11, 604, 19376, 11, 291, 458, 11, 51272], "temperature": 0.0, "avg_logprob": -0.08588369316029772, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.000802882423158735}, {"id": 222, "seek": 111784, "start": 1136.0, "end": 1142.56, "text": " there's this vast interstitial space between these two extremes. And so one thing that's kind of", "tokens": [51272, 456, 311, 341, 8369, 728, 372, 270, 831, 1901, 1296, 613, 732, 41119, 13, 400, 370, 472, 551, 300, 311, 733, 295, 51600], "temperature": 0.0, "avg_logprob": -0.08588369316029772, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.000802882423158735}, {"id": 223, "seek": 114256, "start": 1142.56, "end": 1148.32, "text": " always, one sort of philosophical problem that I've always kind of been interested in ever since", "tokens": [50364, 1009, 11, 472, 1333, 295, 25066, 1154, 300, 286, 600, 1009, 733, 295, 668, 3102, 294, 1562, 1670, 50652], "temperature": 0.0, "avg_logprob": -0.1507641526519275, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.007662143558263779}, {"id": 224, "seek": 114256, "start": 1148.32, "end": 1154.72, "text": " I was a kid, which is this, this sort of, this tension between empiricism versus rationalism,", "tokens": [50652, 286, 390, 257, 1636, 11, 597, 307, 341, 11, 341, 1333, 295, 11, 341, 8980, 1296, 25790, 26356, 5717, 15090, 1434, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1507641526519275, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.007662143558263779}, {"id": 225, "seek": 114256, "start": 1154.72, "end": 1159.6, "text": " right? You know, the question of, you know, on the white, if you look back at the history of,", "tokens": [50972, 558, 30, 509, 458, 11, 264, 1168, 295, 11, 291, 458, 11, 322, 264, 2418, 11, 498, 291, 574, 646, 412, 264, 2503, 295, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1507641526519275, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.007662143558263779}, {"id": 226, "seek": 114256, "start": 1159.6, "end": 1163.44, "text": " where, you know, early European philosophy, or, you know, it's certainly Western, you know,", "tokens": [51216, 689, 11, 291, 458, 11, 2440, 6473, 10675, 11, 420, 11, 291, 458, 11, 309, 311, 3297, 8724, 11, 291, 458, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1507641526519275, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.007662143558263779}, {"id": 227, "seek": 114256, "start": 1163.44, "end": 1168.1599999999999, "text": " Western post enlightenment philosophy, you had people like, you know, Descartes and Leibniz and,", "tokens": [51408, 8724, 2183, 34661, 10675, 11, 291, 632, 561, 411, 11, 291, 458, 11, 3885, 44672, 279, 293, 1456, 897, 77, 590, 293, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1507641526519275, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.007662143558263779}, {"id": 228, "seek": 116816, "start": 1168.24, "end": 1172.96, "text": " and so on, who were, you know, in a more sophisticated way, Bishop Barkley with subjective", "tokens": [50368, 293, 370, 322, 11, 567, 645, 11, 291, 458, 11, 294, 257, 544, 16950, 636, 11, 30113, 36275, 3420, 365, 25972, 50604], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 229, "seek": 116816, "start": 1172.96, "end": 1176.3200000000002, "text": " immaterialism, who were trying to push for this idea that, oh, you know, all the, all the", "tokens": [50604, 3397, 40364, 1434, 11, 567, 645, 1382, 281, 2944, 337, 341, 1558, 300, 11, 1954, 11, 291, 458, 11, 439, 264, 11, 439, 264, 50772], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 230, "seek": 116816, "start": 1176.3200000000002, "end": 1179.92, "text": " sophistication is what's going on inside the observer's head. And, you know, what goes on in", "tokens": [50772, 15572, 399, 307, 437, 311, 516, 322, 1854, 264, 27878, 311, 1378, 13, 400, 11, 291, 458, 11, 437, 1709, 322, 294, 50952], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 231, "seek": 116816, "start": 1179.92, "end": 1183.52, "text": " reality is somehow secondary. And then you had people like, you know, Locke and Hume and the", "tokens": [50952, 4103, 307, 6063, 11396, 13, 400, 550, 291, 632, 561, 411, 11, 291, 458, 11, 12859, 330, 293, 389, 2540, 293, 264, 51132], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 232, "seek": 116816, "start": 1183.52, "end": 1187.0400000000002, "text": " empiricists, who were saying, no, no, we should try and get the observer as much out of the picture", "tokens": [51132, 25790, 299, 1751, 11, 567, 645, 1566, 11, 572, 11, 572, 11, 321, 820, 853, 293, 483, 264, 27878, 382, 709, 484, 295, 264, 3036, 51308], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 233, "seek": 116816, "start": 1187.0400000000002, "end": 1191.0400000000002, "text": " as possible. And we should say all the sophistication is going on kind of in the external world.", "tokens": [51308, 382, 1944, 13, 400, 321, 820, 584, 439, 264, 15572, 399, 307, 516, 322, 733, 295, 294, 264, 8320, 1002, 13, 51508], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 234, "seek": 116816, "start": 1191.0400000000002, "end": 1195.0400000000002, "text": " And this in one, you know, one nice consequence of this is it gives one, one nice consequence of", "tokens": [51508, 400, 341, 294, 472, 11, 291, 458, 11, 472, 1481, 18326, 295, 341, 307, 309, 2709, 472, 11, 472, 1481, 18326, 295, 51708], "temperature": 0.0, "avg_logprob": -0.10269829854799833, "compression_ratio": 2.0060790273556233, "no_speech_prob": 0.006472748704254627}, {"id": 235, "seek": 119504, "start": 1195.04, "end": 1199.52, "text": " this formalism is it gives one actually an algebraic way of kind of parameterizing this", "tokens": [50364, 341, 9860, 1434, 307, 309, 2709, 472, 767, 364, 21989, 299, 636, 295, 733, 295, 13075, 3319, 341, 50588], "temperature": 0.0, "avg_logprob": -0.12414149408755096, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.004460474010556936}, {"id": 236, "seek": 119504, "start": 1199.52, "end": 1204.8799999999999, "text": " spectrum from rationalism to empiricism, right, that, that you can, you can choose the rationalist", "tokens": [50588, 11143, 490, 15090, 1434, 281, 25790, 26356, 11, 558, 11, 300, 11, 300, 291, 393, 11, 291, 393, 2826, 264, 15090, 468, 50856], "temperature": 0.0, "avg_logprob": -0.12414149408755096, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.004460474010556936}, {"id": 237, "seek": 119504, "start": 1204.8799999999999, "end": 1210.72, "text": " extreme where, you know, you know, you just have some, some space of all possible computations and,", "tokens": [50856, 8084, 689, 11, 291, 458, 11, 291, 458, 11, 291, 445, 362, 512, 11, 512, 1901, 295, 439, 1944, 2807, 763, 293, 11, 51148], "temperature": 0.0, "avg_logprob": -0.12414149408755096, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.004460474010556936}, {"id": 238, "seek": 119504, "start": 1210.72, "end": 1214.96, "text": " and the observer is doing all of the work to try to narrow down to a particular one,", "tokens": [51148, 293, 264, 27878, 307, 884, 439, 295, 264, 589, 281, 853, 281, 9432, 760, 281, 257, 1729, 472, 11, 51360], "temperature": 0.0, "avg_logprob": -0.12414149408755096, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.004460474010556936}, {"id": 239, "seek": 119504, "start": 1214.96, "end": 1219.84, "text": " or you can have the kind of empiricist extreme where, you know, the observer is a completely", "tokens": [51360, 420, 291, 393, 362, 264, 733, 295, 25790, 299, 468, 8084, 689, 11, 291, 458, 11, 264, 27878, 307, 257, 2584, 51604], "temperature": 0.0, "avg_logprob": -0.12414149408755096, "compression_ratio": 1.9173553719008265, "no_speech_prob": 0.004460474010556936}, {"id": 240, "seek": 121984, "start": 1219.84, "end": 1224.8, "text": " elementary system. And, you know, and everything and everything they observe is just being built", "tokens": [50364, 16429, 1185, 13, 400, 11, 291, 458, 11, 293, 1203, 293, 1203, 436, 11441, 307, 445, 885, 3094, 50612], "temperature": 0.0, "avg_logprob": -0.14485973971230642, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.010641857050359249}, {"id": 241, "seek": 121984, "start": 1224.8, "end": 1230.24, "text": " up from a kind of bottom up construction, or you can have anything in between. And in a sense,", "tokens": [50612, 493, 490, 257, 733, 295, 2767, 493, 6435, 11, 420, 291, 393, 362, 1340, 294, 1296, 13, 400, 294, 257, 2020, 11, 50884], "temperature": 0.0, "avg_logprob": -0.14485973971230642, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.010641857050359249}, {"id": 242, "seek": 121984, "start": 1230.24, "end": 1234.8799999999999, "text": " we now have, I think, the beginnings of a mathematical theory that explain, that's able", "tokens": [50884, 321, 586, 362, 11, 286, 519, 11, 264, 37281, 295, 257, 18894, 5261, 300, 2903, 11, 300, 311, 1075, 51116], "temperature": 0.0, "avg_logprob": -0.14485973971230642, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.010641857050359249}, {"id": 243, "seek": 121984, "start": 1234.8799999999999, "end": 1240.32, "text": " to explain how those complexities trade off in a very direct way. So I think there's potentially", "tokens": [51116, 281, 2903, 577, 729, 48705, 4923, 766, 294, 257, 588, 2047, 636, 13, 407, 286, 519, 456, 311, 7263, 51388], "temperature": 0.0, "avg_logprob": -0.14485973971230642, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.010641857050359249}, {"id": 244, "seek": 121984, "start": 1241.04, "end": 1246.3999999999999, "text": " places of mutual interest there in kind of the, in thinking about, yeah, as I say, cognition,", "tokens": [51424, 3190, 295, 16917, 1179, 456, 294, 733, 295, 264, 11, 294, 1953, 466, 11, 1338, 11, 382, 286, 584, 11, 46905, 11, 51692], "temperature": 0.0, "avg_logprob": -0.14485973971230642, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.010641857050359249}, {"id": 245, "seek": 124640, "start": 1246.4, "end": 1250.0800000000002, "text": " observation, measurement, scientific modeling, and so on, in fundamentally computational terms.", "tokens": [50364, 14816, 11, 13160, 11, 8134, 15983, 11, 293, 370, 322, 11, 294, 17879, 28270, 2115, 13, 50548], "temperature": 0.0, "avg_logprob": -0.12369579076766968, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.0018954204861074686}, {"id": 246, "seek": 124640, "start": 1250.0800000000002, "end": 1254.72, "text": " So I think that hopefully that will provide some, some context for, for a discussion.", "tokens": [50548, 407, 286, 519, 300, 4696, 300, 486, 2893, 512, 11, 512, 4319, 337, 11, 337, 257, 5017, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12369579076766968, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.0018954204861074686}, {"id": 247, "seek": 124640, "start": 1258.72, "end": 1265.68, "text": " Thank you. Great opening. There's so many places to spin in and jump through.", "tokens": [50980, 1044, 291, 13, 3769, 5193, 13, 821, 311, 370, 867, 3190, 281, 6060, 294, 293, 3012, 807, 13, 51328], "temperature": 0.0, "avg_logprob": -0.12369579076766968, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.0018954204861074686}, {"id": 248, "seek": 124640, "start": 1267.2, "end": 1271.6000000000001, "text": " I guess I'll start with the two things I wrote down were unity is plural and at", "tokens": [51404, 286, 2041, 286, 603, 722, 365, 264, 732, 721, 286, 4114, 760, 645, 18205, 307, 25377, 293, 412, 51624], "temperature": 0.0, "avg_logprob": -0.12369579076766968, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.0018954204861074686}, {"id": 249, "seek": 127160, "start": 1271.6, "end": 1276.48, "text": " minimum two and beauty is in the eye of the beholder and the way that these kinds of pieces", "tokens": [50364, 7285, 732, 293, 6643, 307, 294, 264, 3313, 295, 264, 312, 20480, 293, 264, 636, 300, 613, 3685, 295, 3755, 50608], "temperature": 0.0, "avg_logprob": -0.10100884664626349, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.09941580146551132}, {"id": 250, "seek": 127160, "start": 1276.48, "end": 1283.28, "text": " of timeless wisdom that describe that fundamentally relational component to observers in systems,", "tokens": [50608, 295, 41200, 10712, 300, 6786, 300, 17879, 38444, 6542, 281, 48090, 294, 3652, 11, 50948], "temperature": 0.0, "avg_logprob": -0.10100884664626349, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.09941580146551132}, {"id": 251, "seek": 127160, "start": 1283.28, "end": 1287.76, "text": " which are not all kinds of systems per se, but those kinds of systems, those things are true for,", "tokens": [50948, 597, 366, 406, 439, 3685, 295, 3652, 680, 369, 11, 457, 729, 3685, 295, 3652, 11, 729, 721, 366, 2074, 337, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10100884664626349, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.09941580146551132}, {"id": 252, "seek": 127160, "start": 1288.3999999999999, "end": 1295.1999999999998, "text": " and then the way in which along the formalism that you described with the system observer encoding", "tokens": [51204, 293, 550, 264, 636, 294, 597, 2051, 264, 9860, 1434, 300, 291, 7619, 365, 264, 1185, 27878, 43430, 51544], "temperature": 0.0, "avg_logprob": -0.10100884664626349, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.09941580146551132}, {"id": 253, "seek": 129520, "start": 1296.0, "end": 1302.32, "text": " freeway partition, and then the way that in free energy principle and the particular physics,", "tokens": [50404, 1737, 676, 24808, 11, 293, 550, 264, 636, 300, 294, 1737, 2281, 8665, 293, 264, 1729, 10649, 11, 50720], "temperature": 0.0, "avg_logprob": -0.13238399000052947, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.05183326080441475}, {"id": 254, "seek": 129520, "start": 1302.32, "end": 1308.64, "text": " that interface gets broken out from the agent's perspective into the incoming sensory and the", "tokens": [50720, 300, 9226, 2170, 5463, 484, 490, 264, 9461, 311, 4585, 666, 264, 22341, 27233, 293, 264, 51036], "temperature": 0.0, "avg_logprob": -0.13238399000052947, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.05183326080441475}, {"id": 255, "seek": 129520, "start": 1308.64, "end": 1316.48, "text": " outgoing action. So then that results in the fourfold particular partition. So maybe just to kind of,", "tokens": [51036, 41565, 3069, 13, 407, 550, 300, 3542, 294, 264, 1451, 18353, 1729, 24808, 13, 407, 1310, 445, 281, 733, 295, 11, 51428], "temperature": 0.0, "avg_logprob": -0.13238399000052947, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.05183326080441475}, {"id": 256, "seek": 129520, "start": 1317.44, "end": 1324.16, "text": " well, there, how do we partition the, from a category theory perspective or however,", "tokens": [51476, 731, 11, 456, 11, 577, 360, 321, 24808, 264, 11, 490, 257, 7719, 5261, 4585, 420, 4461, 11, 51812], "temperature": 0.0, "avg_logprob": -0.13238399000052947, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.05183326080441475}, {"id": 257, "seek": 132416, "start": 1324.16, "end": 1331.68, "text": " the action perception loop or the engagement loop? Like, how do we make a topology or", "tokens": [50364, 264, 3069, 12860, 6367, 420, 264, 8742, 6367, 30, 1743, 11, 577, 360, 321, 652, 257, 1192, 1793, 420, 50740], "temperature": 0.0, "avg_logprob": -0.13039757484613462, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00023777350725140423}, {"id": 258, "seek": 132416, "start": 1331.68, "end": 1339.1200000000001, "text": " compare a contrast different topologies and flows over this kind of seemingly pervasive or universal", "tokens": [50740, 6794, 257, 8712, 819, 1192, 6204, 293, 12867, 670, 341, 733, 295, 18709, 680, 39211, 420, 11455, 51112], "temperature": 0.0, "avg_logprob": -0.13039757484613462, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00023777350725140423}, {"id": 259, "seek": 132416, "start": 1339.1200000000001, "end": 1345.6000000000001, "text": " interface like concept? That's a fascinating question. So I don't have the answer and this is", "tokens": [51112, 9226, 411, 3410, 30, 663, 311, 257, 10343, 1168, 13, 407, 286, 500, 380, 362, 264, 1867, 293, 341, 307, 51436], "temperature": 0.0, "avg_logprob": -0.13039757484613462, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00023777350725140423}, {"id": 260, "seek": 132416, "start": 1345.6000000000001, "end": 1351.28, "text": " maybe a place where, where both you Daniel and perhaps David may have, you know, useful perspectives", "tokens": [51436, 1310, 257, 1081, 689, 11, 689, 1293, 291, 8033, 293, 4317, 4389, 815, 362, 11, 291, 458, 11, 4420, 16766, 51720], "temperature": 0.0, "avg_logprob": -0.13039757484613462, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00023777350725140423}, {"id": 261, "seek": 135128, "start": 1351.28, "end": 1357.76, "text": " on this because, you know, I'm, I'm, you know, I read Carl's work a few years back and so I have", "tokens": [50364, 322, 341, 570, 11, 291, 458, 11, 286, 478, 11, 286, 478, 11, 291, 458, 11, 286, 1401, 14256, 311, 589, 257, 1326, 924, 646, 293, 370, 286, 362, 50688], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 262, "seek": 135128, "start": 1357.76, "end": 1362.08, "text": " some familiarity with the terms, but I'm by no means a kind of, you know, an expert on free", "tokens": [50688, 512, 49828, 365, 264, 2115, 11, 457, 286, 478, 538, 572, 1355, 257, 733, 295, 11, 291, 458, 11, 364, 5844, 322, 1737, 50904], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 263, "seek": 135128, "start": 1362.08, "end": 1366.16, "text": " energy principle or, you know, active inference and those kinds of things. But I think it's a very", "tokens": [50904, 2281, 8665, 420, 11, 291, 458, 11, 4967, 38253, 293, 729, 3685, 295, 721, 13, 583, 286, 519, 309, 311, 257, 588, 51108], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 264, "seek": 135128, "start": 1366.16, "end": 1370.56, "text": " good point that you raise. And so, you know, I should begin by just being honest and say that,", "tokens": [51108, 665, 935, 300, 291, 5300, 13, 400, 370, 11, 291, 458, 11, 286, 820, 1841, 538, 445, 885, 3245, 293, 584, 300, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 265, "seek": 135128, "start": 1370.56, "end": 1375.2, "text": " you know, everything I'm doing, you know, all that I just described is of course an idealization", "tokens": [51328, 291, 458, 11, 1203, 286, 478, 884, 11, 291, 458, 11, 439, 300, 286, 445, 7619, 307, 295, 1164, 364, 7157, 2144, 51560], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 266, "seek": 135128, "start": 1375.2, "end": 1379.2, "text": " and that, you know, in reality, you know, in particular, it's an idealization, which I think", "tokens": [51560, 293, 300, 11, 291, 458, 11, 294, 4103, 11, 291, 458, 11, 294, 1729, 11, 309, 311, 364, 7157, 2144, 11, 597, 286, 519, 51760], "temperature": 0.0, "avg_logprob": -0.10337229443203871, "compression_ratio": 1.9066666666666667, "no_speech_prob": 0.007672519888728857}, {"id": 267, "seek": 137920, "start": 1379.28, "end": 1383.68, "text": " you were very right, Daniel, to kind of pick up on. It's an idealization in which we say the", "tokens": [50368, 291, 645, 588, 558, 11, 8033, 11, 281, 733, 295, 1888, 493, 322, 13, 467, 311, 364, 7157, 2144, 294, 597, 321, 584, 264, 50588], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 268, "seek": 137920, "start": 1383.68, "end": 1387.3600000000001, "text": " observer is completely kind of non-interacting with the world somehow, right? That, you know,", "tokens": [50588, 27878, 307, 2584, 733, 295, 2107, 12, 5106, 41090, 365, 264, 1002, 6063, 11, 558, 30, 663, 11, 291, 458, 11, 50772], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 269, "seek": 137920, "start": 1387.3600000000001, "end": 1392.0800000000002, "text": " in a sense that there's just input coming in and nothing, nothing coming out. But of course,", "tokens": [50772, 294, 257, 2020, 300, 456, 311, 445, 4846, 1348, 294, 293, 1825, 11, 1825, 1348, 484, 13, 583, 295, 1164, 11, 51008], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 270, "seek": 137920, "start": 1392.0800000000002, "end": 1395.6000000000001, "text": " we know that's not really how observation works. Observation is necessarily a kind of two-way", "tokens": [51008, 321, 458, 300, 311, 406, 534, 577, 14816, 1985, 13, 20707, 6864, 307, 4725, 257, 733, 295, 732, 12, 676, 51184], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 271, "seek": 137920, "start": 1395.6000000000001, "end": 1400.48, "text": " process. And so what's needed is not just this kind of very clean algebraic semantics that I've", "tokens": [51184, 1399, 13, 400, 370, 437, 311, 2978, 307, 406, 445, 341, 733, 295, 588, 2541, 21989, 299, 4361, 45298, 300, 286, 600, 51428], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 272, "seek": 137920, "start": 1400.48, "end": 1403.76, "text": " described here, which assumes that there's a essentially a one-way function from the world", "tokens": [51428, 7619, 510, 11, 597, 37808, 300, 456, 311, 257, 4476, 257, 472, 12, 676, 2445, 490, 264, 1002, 51592], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 273, "seek": 137920, "start": 1403.76, "end": 1408.64, "text": " to the observer, but actually something more like a kind of second-order cybernetics description", "tokens": [51592, 281, 264, 27878, 11, 457, 767, 746, 544, 411, 257, 733, 295, 1150, 12, 4687, 13411, 7129, 1167, 3855, 51836], "temperature": 0.0, "avg_logprob": -0.10295259952545166, "compression_ratio": 1.8149171270718232, "no_speech_prob": 0.008046699687838554}, {"id": 274, "seek": 140920, "start": 1409.44, "end": 1414.24, "text": " of, you know, what's really going on where you have, you know, first-order and second-order", "tokens": [50376, 295, 11, 291, 458, 11, 437, 311, 534, 516, 322, 689, 291, 362, 11, 291, 458, 11, 700, 12, 4687, 293, 1150, 12, 4687, 50616], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 275, "seek": 140920, "start": 1414.24, "end": 1417.76, "text": " interactions, whereas exactly as you say, you can get these sort of feedback loops from", "tokens": [50616, 13280, 11, 9735, 2293, 382, 291, 584, 11, 291, 393, 483, 613, 1333, 295, 5824, 16121, 490, 50792], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 276, "seek": 140920, "start": 1417.76, "end": 1422.16, "text": " observation to action and back again, which are probably, which is, I mean, still an idealization,", "tokens": [50792, 14816, 281, 3069, 293, 646, 797, 11, 597, 366, 1391, 11, 597, 307, 11, 286, 914, 11, 920, 364, 7157, 2144, 11, 51012], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 277, "seek": 140920, "start": 1422.16, "end": 1426.88, "text": " but probably a more realistic idealization for how real observers and real measurement apparatus", "tokens": [51012, 457, 1391, 257, 544, 12465, 7157, 2144, 337, 577, 957, 48090, 293, 957, 13160, 38573, 51248], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 278, "seek": 140920, "start": 1426.88, "end": 1432.16, "text": " work. So I just want to begin by saying that I don't know, right? And, you know, the question of", "tokens": [51248, 589, 13, 407, 286, 445, 528, 281, 1841, 538, 1566, 300, 286, 500, 380, 458, 11, 558, 30, 400, 11, 291, 458, 11, 264, 1168, 295, 51512], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 279, "seek": 140920, "start": 1432.16, "end": 1436.16, "text": " how this formalism into plays with things like second-order cybernetics and other areas where", "tokens": [51512, 577, 341, 9860, 1434, 666, 5749, 365, 721, 411, 1150, 12, 4687, 13411, 7129, 1167, 293, 661, 3179, 689, 51712], "temperature": 0.0, "avg_logprob": -0.08575499318812015, "compression_ratio": 1.7854889589905363, "no_speech_prob": 0.0003797837707679719}, {"id": 280, "seek": 143616, "start": 1436.16, "end": 1440.72, "text": " I know these kinds of questions have been explored, that's something I'm very interested to find out", "tokens": [50364, 286, 458, 613, 3685, 295, 1651, 362, 668, 24016, 11, 300, 311, 746, 286, 478, 588, 3102, 281, 915, 484, 50592], "temperature": 0.0, "avg_logprob": -0.11887027371314264, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.005543175619095564}, {"id": 281, "seek": 143616, "start": 1440.72, "end": 1450.64, "text": " about, you know, going forward. But I think, hmm, okay, so, yeah, so at some point, maybe you could", "tokens": [50592, 466, 11, 291, 458, 11, 516, 2128, 13, 583, 286, 519, 11, 16478, 11, 1392, 11, 370, 11, 1338, 11, 370, 412, 512, 935, 11, 1310, 291, 727, 51088], "temperature": 0.0, "avg_logprob": -0.11887027371314264, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.005543175619095564}, {"id": 282, "seek": 143616, "start": 1451.44, "end": 1455.44, "text": " help me understand, you know, potentially where things might fit in with, you know,", "tokens": [51128, 854, 385, 1223, 11, 291, 458, 11, 7263, 689, 721, 1062, 3318, 294, 365, 11, 291, 458, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11887027371314264, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.005543175619095564}, {"id": 283, "seek": 143616, "start": 1455.44, "end": 1463.92, "text": " with the kind of broader active inference framework. I'm not sure I necessarily have that", "tokens": [51328, 365, 264, 733, 295, 13227, 4967, 38253, 8388, 13, 286, 478, 406, 988, 286, 4725, 362, 300, 51752], "temperature": 0.0, "avg_logprob": -0.11887027371314264, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.005543175619095564}, {"id": 284, "seek": 146392, "start": 1463.92, "end": 1473.1200000000001, "text": " much more to comment on than that. Yeah, other than to say that, you know, in a sense, okay,", "tokens": [50364, 709, 544, 281, 2871, 322, 813, 300, 13, 865, 11, 661, 813, 281, 584, 300, 11, 291, 458, 11, 294, 257, 2020, 11, 1392, 11, 50824], "temperature": 0.0, "avg_logprob": -0.11435809704141879, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0021788389421999454}, {"id": 285, "seek": 146392, "start": 1473.1200000000001, "end": 1481.68, "text": " so maybe, you know, one further comment is that, because, I mean, you asked specifically about", "tokens": [50824, 370, 1310, 11, 291, 458, 11, 472, 3052, 2871, 307, 300, 11, 570, 11, 286, 914, 11, 291, 2351, 4682, 466, 51252], "temperature": 0.0, "avg_logprob": -0.11435809704141879, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0021788389421999454}, {"id": 286, "seek": 146392, "start": 1481.68, "end": 1486.8000000000002, "text": " how the kind of compositional category theoretic perspective might be useful. So I don't think", "tokens": [51252, 577, 264, 733, 295, 10199, 2628, 7719, 14308, 299, 4585, 1062, 312, 4420, 13, 407, 286, 500, 380, 519, 51508], "temperature": 0.0, "avg_logprob": -0.11435809704141879, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0021788389421999454}, {"id": 287, "seek": 146392, "start": 1486.8000000000002, "end": 1489.6000000000001, "text": " category theory in itself is going to be the complete answer. I think it will be category", "tokens": [51508, 7719, 5261, 294, 2564, 307, 516, 281, 312, 264, 3566, 1867, 13, 286, 519, 309, 486, 312, 7719, 51648], "temperature": 0.0, "avg_logprob": -0.11435809704141879, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0021788389421999454}, {"id": 288, "seek": 146392, "start": 1489.6000000000001, "end": 1493.1200000000001, "text": " theory augmented with some other things, computational complexity, probably second-order", "tokens": [51648, 5261, 36155, 365, 512, 661, 721, 11, 28270, 14024, 11, 1391, 1150, 12, 4687, 51824], "temperature": 0.0, "avg_logprob": -0.11435809704141879, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0021788389421999454}, {"id": 289, "seek": 149312, "start": 1493.12, "end": 1498.6399999999999, "text": " cybernetics, and some other things that I may not be aware of. But one place where I think that", "tokens": [50364, 13411, 7129, 1167, 11, 293, 512, 661, 721, 300, 286, 815, 406, 312, 3650, 295, 13, 583, 472, 1081, 689, 286, 519, 300, 50640], "temperature": 0.0, "avg_logprob": -0.06746901900081312, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0009093679254874587}, {"id": 290, "seek": 149312, "start": 1498.6399999999999, "end": 1505.28, "text": " viewpoint is useful, at least on a philosophical level, is the idea that comes about, that you", "tokens": [50640, 35248, 307, 4420, 11, 412, 1935, 322, 257, 25066, 1496, 11, 307, 264, 1558, 300, 1487, 466, 11, 300, 291, 50972], "temperature": 0.0, "avg_logprob": -0.06746901900081312, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0009093679254874587}, {"id": 291, "seek": 149312, "start": 1505.28, "end": 1509.36, "text": " really obtain by studying mathematical structures in a category theoretic way, which is that the", "tokens": [50972, 534, 12701, 538, 7601, 18894, 9227, 294, 257, 7719, 14308, 299, 636, 11, 597, 307, 300, 264, 51176], "temperature": 0.0, "avg_logprob": -0.06746901900081312, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0009093679254874587}, {"id": 292, "seek": 149312, "start": 1509.36, "end": 1516.3999999999999, "text": " identity of something, you can define it both in terms of its intrinsic properties, or you can", "tokens": [51176, 6575, 295, 746, 11, 291, 393, 6964, 309, 1293, 294, 2115, 295, 1080, 35698, 7221, 11, 420, 291, 393, 51528], "temperature": 0.0, "avg_logprob": -0.06746901900081312, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0009093679254874587}, {"id": 293, "seek": 149312, "start": 1516.3999999999999, "end": 1520.32, "text": " define it in terms of, you know, the stuff that you can do to it, right? So, you know, this was", "tokens": [51528, 6964, 309, 294, 2115, 295, 11, 291, 458, 11, 264, 1507, 300, 291, 393, 360, 281, 309, 11, 558, 30, 407, 11, 291, 458, 11, 341, 390, 51724], "temperature": 0.0, "avg_logprob": -0.06746901900081312, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0009093679254874587}, {"id": 294, "seek": 152032, "start": 1520.32, "end": 1524.08, "text": " really the transition that happened in the foundations of mathematics as a result of people", "tokens": [50364, 534, 264, 6034, 300, 2011, 294, 264, 22467, 295, 18666, 382, 257, 1874, 295, 561, 50552], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 295, "seek": 152032, "start": 1524.08, "end": 1529.6799999999998, "text": " like Samuel Alenberg and Saunders-McLean. So, you know, category theory has its origins in this", "tokens": [50552, 411, 23036, 967, 268, 6873, 293, 6299, 997, 433, 12, 46716, 11020, 282, 13, 407, 11, 291, 458, 11, 7719, 5261, 575, 1080, 22721, 294, 341, 50832], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 296, "seek": 152032, "start": 1529.6799999999998, "end": 1533.84, "text": " sort of slightly obstruous branch of algebraic topology. It was, you know, initially developed", "tokens": [50832, 1333, 295, 4748, 9579, 894, 563, 9819, 295, 21989, 299, 1192, 1793, 13, 467, 390, 11, 291, 458, 11, 9105, 4743, 51040], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 297, "seek": 152032, "start": 1533.84, "end": 1538.8799999999999, "text": " by people like Alexander Gordon-Deek and John P. S. Sear for doing homological algebra for,", "tokens": [51040, 538, 561, 411, 14845, 19369, 12, 11089, 916, 293, 2619, 430, 13, 318, 13, 1100, 289, 337, 884, 3655, 4383, 21989, 337, 11, 51292], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 298, "seek": 152032, "start": 1538.8799999999999, "end": 1542.8799999999999, "text": " you know, for reasoning about sort of the algebraic structure of topological spaces. But then later,", "tokens": [51292, 291, 458, 11, 337, 21577, 466, 1333, 295, 264, 21989, 299, 3877, 295, 1192, 4383, 7673, 13, 583, 550, 1780, 11, 51492], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 299, "seek": 152032, "start": 1542.8799999999999, "end": 1546.8, "text": " in the, I think, 1960s, 1970s, these two American mathematicians, Alenberg and McLean,", "tokens": [51492, 294, 264, 11, 286, 519, 11, 16157, 82, 11, 14577, 82, 11, 613, 732, 2665, 32811, 2567, 11, 967, 268, 6873, 293, 4050, 11020, 282, 11, 51688], "temperature": 0.0, "avg_logprob": -0.19268208333890732, "compression_ratio": 1.7186544342507646, "no_speech_prob": 0.004188869148492813}, {"id": 300, "seek": 154680, "start": 1546.8, "end": 1550.6399999999999, "text": " realized that it was useful not just for thinking about topology, but for thinking about kind of", "tokens": [50364, 5334, 300, 309, 390, 4420, 406, 445, 337, 1953, 466, 1192, 1793, 11, 457, 337, 1953, 466, 733, 295, 50556], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 301, "seek": 154680, "start": 1550.6399999999999, "end": 1554.72, "text": " mathematical structure in general. And then later on applied category theorists started saying,", "tokens": [50556, 18894, 3877, 294, 2674, 13, 400, 550, 1780, 322, 6456, 7719, 27423, 1751, 1409, 1566, 11, 50760], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 302, "seek": 154680, "start": 1554.72, "end": 1559.2, "text": " well, maybe it's useful for just thinking about structure in general. But, you know, the key kind", "tokens": [50760, 731, 11, 1310, 309, 311, 4420, 337, 445, 1953, 466, 3877, 294, 2674, 13, 583, 11, 291, 458, 11, 264, 2141, 733, 50984], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 303, "seek": 154680, "start": 1559.2, "end": 1565.52, "text": " of conceptual or philosophical shift that it imposes is, you know, historically, thanks to the work", "tokens": [50984, 295, 24106, 420, 25066, 5513, 300, 309, 704, 4201, 307, 11, 291, 458, 11, 16180, 11, 3231, 281, 264, 589, 51300], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 304, "seek": 154680, "start": 1565.52, "end": 1569.9199999999998, "text": " of people like Kantor and Frege and Russell and so on, people have thought about mathematical", "tokens": [51300, 295, 561, 411, 40927, 284, 293, 6142, 432, 293, 20937, 293, 370, 322, 11, 561, 362, 1194, 466, 18894, 51520], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 305, "seek": 154680, "start": 1569.9199999999998, "end": 1574.0, "text": " structures in the foundations of mathematics as, you know, in terms of set theory. And the idea", "tokens": [51520, 9227, 294, 264, 22467, 295, 18666, 382, 11, 291, 458, 11, 294, 2115, 295, 992, 5261, 13, 400, 264, 1558, 51724], "temperature": 0.0, "avg_logprob": -0.09671722558828501, "compression_ratio": 1.9528619528619529, "no_speech_prob": 0.01064340304583311}, {"id": 306, "seek": 157400, "start": 1574.0, "end": 1578.24, "text": " in set theory is you have this, you know, things like the axiom of extension that effectively say", "tokens": [50364, 294, 992, 5261, 307, 291, 362, 341, 11, 291, 458, 11, 721, 411, 264, 6360, 72, 298, 295, 10320, 300, 8659, 584, 50576], "temperature": 0.0, "avg_logprob": -0.09650860294218987, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0007549655856564641}, {"id": 307, "seek": 157400, "start": 1578.24, "end": 1583.52, "text": " set is defined by what's inside it, right? So in other words, you know, you, a mathematical", "tokens": [50576, 992, 307, 7642, 538, 437, 311, 1854, 309, 11, 558, 30, 407, 294, 661, 2283, 11, 291, 458, 11, 291, 11, 257, 18894, 50840], "temperature": 0.0, "avg_logprob": -0.09650860294218987, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0007549655856564641}, {"id": 308, "seek": 157400, "start": 1583.52, "end": 1587.68, "text": " structure obtains its identity by, you know, you break it apart and you look at what's inside.", "tokens": [50840, 3877, 7464, 2315, 1080, 6575, 538, 11, 291, 458, 11, 291, 1821, 309, 4936, 293, 291, 574, 412, 437, 311, 1854, 13, 51048], "temperature": 0.0, "avg_logprob": -0.09650860294218987, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0007549655856564641}, {"id": 309, "seek": 157400, "start": 1589.04, "end": 1594.32, "text": " In category theory, it's a completely different view. The view instead is you say, well, no,", "tokens": [51116, 682, 7719, 5261, 11, 309, 311, 257, 2584, 819, 1910, 13, 440, 1910, 2602, 307, 291, 584, 11, 731, 11, 572, 11, 51380], "temperature": 0.0, "avg_logprob": -0.09650860294218987, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0007549655856564641}, {"id": 310, "seek": 157400, "start": 1594.32, "end": 1598.08, "text": " you can't look inside, you know, it's a fundamental rule of category theory that you can't look inside", "tokens": [51380, 291, 393, 380, 574, 1854, 11, 291, 458, 11, 309, 311, 257, 8088, 4978, 295, 7719, 5261, 300, 291, 393, 380, 574, 1854, 51568], "temperature": 0.0, "avg_logprob": -0.09650860294218987, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0007549655856564641}, {"id": 311, "seek": 159808, "start": 1598.08, "end": 1603.6799999999998, "text": " an object. You know, it's internal structure, if it has any sort of outer bounds to you.", "tokens": [50364, 364, 2657, 13, 509, 458, 11, 309, 311, 6920, 3877, 11, 498, 309, 575, 604, 1333, 295, 10847, 29905, 281, 291, 13, 50644], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 312, "seek": 159808, "start": 1603.6799999999998, "end": 1609.04, "text": " And instead, you give that object identity in terms of how it relates to other objects", "tokens": [50644, 400, 2602, 11, 291, 976, 300, 2657, 6575, 294, 2115, 295, 577, 309, 16155, 281, 661, 6565, 50912], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 313, "seek": 159808, "start": 1609.04, "end": 1613.04, "text": " of the same type, right? So in other words, you know, you can ask, you know, what can I do to", "tokens": [50912, 295, 264, 912, 2010, 11, 558, 30, 407, 294, 661, 2283, 11, 291, 458, 11, 291, 393, 1029, 11, 291, 458, 11, 437, 393, 286, 360, 281, 51112], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 314, "seek": 159808, "start": 1613.04, "end": 1617.6799999999998, "text": " this? What functions can I apply to it? What functions can I apply to something else that", "tokens": [51112, 341, 30, 708, 6828, 393, 286, 3079, 281, 309, 30, 708, 6828, 393, 286, 3079, 281, 746, 1646, 300, 51344], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 315, "seek": 159808, "start": 1617.6799999999998, "end": 1622.32, "text": " map into this? So, you know, if I want to define, I don't know, the real numbers or the integers", "tokens": [51344, 4471, 666, 341, 30, 407, 11, 291, 458, 11, 498, 286, 528, 281, 6964, 11, 286, 500, 380, 458, 11, 264, 957, 3547, 420, 264, 41674, 51576], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 316, "seek": 159808, "start": 1622.32, "end": 1626.32, "text": " or something in the set theory, you know, from a set theory perspective, you would say that the,", "tokens": [51576, 420, 746, 294, 264, 992, 5261, 11, 291, 458, 11, 490, 257, 992, 5261, 4585, 11, 291, 576, 584, 300, 264, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09857258829129796, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.20162607729434967}, {"id": 317, "seek": 162632, "start": 1626.3999999999999, "end": 1630.72, "text": " you know, the essence of the real numbers are all the numbers that are inside that set or all", "tokens": [50368, 291, 458, 11, 264, 12801, 295, 264, 957, 3547, 366, 439, 264, 3547, 300, 366, 1854, 300, 992, 420, 439, 50584], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 318, "seek": 162632, "start": 1630.72, "end": 1635.4399999999998, "text": " the numbers that are inside are. Whereas the category theory perspective is no, the essence", "tokens": [50584, 264, 3547, 300, 366, 1854, 366, 13, 13813, 264, 7719, 5261, 4585, 307, 572, 11, 264, 12801, 50820], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 319, "seek": 162632, "start": 1635.4399999999998, "end": 1639.4399999999998, "text": " of the real numbers are all the functions that you can define that take real numbers to some", "tokens": [50820, 295, 264, 957, 3547, 366, 439, 264, 6828, 300, 291, 393, 6964, 300, 747, 957, 3547, 281, 512, 51020], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 320, "seek": 162632, "start": 1639.4399999999998, "end": 1643.6799999999998, "text": " other number system or real numbers to themselves or that take some other number system into the", "tokens": [51020, 661, 1230, 1185, 420, 957, 3547, 281, 2969, 420, 300, 747, 512, 661, 1230, 1185, 666, 264, 51232], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 321, "seek": 162632, "start": 1643.6799999999998, "end": 1648.6399999999999, "text": " real numbers or et cetera. And, you know, some of the deepest results in category theory, like the", "tokens": [51232, 957, 3547, 420, 1030, 11458, 13, 400, 11, 291, 458, 11, 512, 295, 264, 28288, 3542, 294, 7719, 5261, 11, 411, 264, 51480], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 322, "seek": 162632, "start": 1648.6399999999999, "end": 1654.8799999999999, "text": " Oneida Lemma and other things, are telling one in some very precise sense that these two perspectives", "tokens": [51480, 1485, 2887, 16905, 1696, 293, 661, 721, 11, 366, 3585, 472, 294, 512, 588, 13600, 2020, 300, 613, 732, 16766, 51792], "temperature": 0.0, "avg_logprob": -0.11340221017599106, "compression_ratio": 2.2857142857142856, "no_speech_prob": 0.0011691307881847024}, {"id": 323, "seek": 165488, "start": 1654.88, "end": 1660.0, "text": " are really the same at some fundamental level, but that, you know, identifying an object based", "tokens": [50364, 366, 534, 264, 912, 412, 512, 8088, 1496, 11, 457, 300, 11, 291, 458, 11, 16696, 364, 2657, 2361, 50620], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 324, "seek": 165488, "start": 1660.0, "end": 1664.0, "text": " on its internal structure, based on breaking it apart and asking what's inside and identifying", "tokens": [50620, 322, 1080, 6920, 3877, 11, 2361, 322, 7697, 309, 4936, 293, 3365, 437, 311, 1854, 293, 16696, 50820], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 325, "seek": 165488, "start": 1664.0, "end": 1669.2800000000002, "text": " an object by asking, what can I do to it? And what, you know, what can this object be transformed", "tokens": [50820, 364, 2657, 538, 3365, 11, 437, 393, 286, 360, 281, 309, 30, 400, 437, 11, 291, 458, 11, 437, 393, 341, 2657, 312, 16894, 51084], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 326, "seek": 165488, "start": 1669.2800000000002, "end": 1673.68, "text": " into and what things can be transformed into this object? Those give you exactly the same", "tokens": [51084, 666, 293, 437, 721, 393, 312, 16894, 666, 341, 2657, 30, 3950, 976, 291, 2293, 264, 912, 51304], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 327, "seek": 165488, "start": 1673.68, "end": 1677.44, "text": " information. It's far from obvious that that's true, you know, the Oneida Lemma is a very kind of,", "tokens": [51304, 1589, 13, 467, 311, 1400, 490, 6322, 300, 300, 311, 2074, 11, 291, 458, 11, 264, 1485, 2887, 16905, 1696, 307, 257, 588, 733, 295, 11, 51492], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 328, "seek": 165488, "start": 1678.5600000000002, "end": 1682.24, "text": " one of those results where you can never quite work out of its obvious or if it's incredibly,", "tokens": [51548, 472, 295, 729, 3542, 689, 291, 393, 1128, 1596, 589, 484, 295, 1080, 6322, 420, 498, 309, 311, 6252, 11, 51732], "temperature": 0.0, "avg_logprob": -0.0949753183518013, "compression_ratio": 1.881188118811881, "no_speech_prob": 0.01223082561045885}, {"id": 329, "seek": 168224, "start": 1682.24, "end": 1686.72, "text": " you know, mysterious. But I tend to fall on the side that it's incredibly mysterious. It's far", "tokens": [50364, 291, 458, 11, 13831, 13, 583, 286, 3928, 281, 2100, 322, 264, 1252, 300, 309, 311, 6252, 13831, 13, 467, 311, 1400, 50588], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 330, "seek": 168224, "start": 1686.72, "end": 1691.44, "text": " from evidence that those two perspectives would really be the same. And yet, the point you're", "tokens": [50588, 490, 4467, 300, 729, 732, 16766, 576, 534, 312, 264, 912, 13, 400, 1939, 11, 264, 935, 291, 434, 50824], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 331, "seek": 168224, "start": 1691.44, "end": 1698.8, "text": " making Daniel, I think, is that in a sense, historical ways of thinking about scientific", "tokens": [50824, 1455, 8033, 11, 286, 519, 11, 307, 300, 294, 257, 2020, 11, 8584, 2098, 295, 1953, 466, 8134, 51192], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 332, "seek": 168224, "start": 1698.8, "end": 1702.96, "text": " observation have tended towards the set theoretic viewpoint, tended towards the viewpoint that we", "tokens": [51192, 14816, 362, 34732, 3030, 264, 992, 14308, 299, 35248, 11, 34732, 3030, 264, 35248, 300, 321, 51400], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 333, "seek": 168224, "start": 1702.96, "end": 1706.16, "text": " understand systems based on kind of breaking them apart into constituent components.", "tokens": [51400, 1223, 3652, 2361, 322, 733, 295, 7697, 552, 4936, 666, 16085, 317, 6677, 13, 51560], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 334, "seek": 168224, "start": 1707.2, "end": 1711.44, "text": " But perhaps a more realistic view is something more like the category theory perspective,", "tokens": [51612, 583, 4317, 257, 544, 12465, 1910, 307, 746, 544, 411, 264, 7719, 5261, 4585, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11514892416485285, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004602776374667883}, {"id": 335, "seek": 171144, "start": 1711.44, "end": 1716.56, "text": " where we say, you know, I understand a system by interacting with it, right? By asking,", "tokens": [50364, 689, 321, 584, 11, 291, 458, 11, 286, 1223, 257, 1185, 538, 18017, 365, 309, 11, 558, 30, 3146, 3365, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 336, "seek": 171144, "start": 1716.56, "end": 1720.88, "text": " what can I do to it? And how does it behave as I, when I perform certain operations to it?", "tokens": [50620, 437, 393, 286, 360, 281, 309, 30, 400, 577, 775, 309, 15158, 382, 286, 11, 562, 286, 2042, 1629, 7705, 281, 309, 30, 50836], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 337, "seek": 171144, "start": 1720.88, "end": 1724.88, "text": " And that's a fundamentally, you know, that's a fundamentally two-way process that involves not", "tokens": [50836, 400, 300, 311, 257, 17879, 11, 291, 458, 11, 300, 311, 257, 17879, 732, 12, 676, 1399, 300, 11626, 406, 51036], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 338, "seek": 171144, "start": 1724.88, "end": 1729.52, "text": " just passive observation, but also kind of active participation. And somehow we need to develop", "tokens": [51036, 445, 14975, 14816, 11, 457, 611, 733, 295, 4967, 13487, 13, 400, 6063, 321, 643, 281, 1499, 51268], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 339, "seek": 171144, "start": 1729.52, "end": 1733.44, "text": " a formalism that kind of incorporates those two elements. And maybe, you know, maybe it already", "tokens": [51268, 257, 9860, 1434, 300, 733, 295, 50193, 729, 732, 4959, 13, 400, 1310, 11, 291, 458, 11, 1310, 309, 1217, 51464], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 340, "seek": 171144, "start": 1733.44, "end": 1738.16, "text": " exists. And it exists in this large literature tree of which I'm largely unaware. That's partly", "tokens": [51464, 8198, 13, 400, 309, 8198, 294, 341, 2416, 10394, 4230, 295, 597, 286, 478, 11611, 32065, 13, 663, 311, 17031, 51700], "temperature": 0.0, "avg_logprob": -0.10342556282326028, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0018656918546184897}, {"id": 341, "seek": 173816, "start": 1738.16, "end": 1741.92, "text": " why I want to be here to try and find out, you know, what things I missed, so to speak.", "tokens": [50364, 983, 286, 528, 281, 312, 510, 281, 853, 293, 915, 484, 11, 291, 458, 11, 437, 721, 286, 6721, 11, 370, 281, 1710, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11772795112765565, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.009264634922146797}, {"id": 342, "seek": 173816, "start": 1743.52, "end": 1751.44, "text": " Well, all right, a few points. Self-evident is far from evident. Also, I tend to the mysterious,", "tokens": [50632, 1042, 11, 439, 558, 11, 257, 1326, 2793, 13, 16348, 12, 13379, 1078, 307, 1400, 490, 16371, 13, 2743, 11, 286, 3928, 281, 264, 13831, 11, 51028], "temperature": 0.0, "avg_logprob": -0.11772795112765565, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.009264634922146797}, {"id": 343, "seek": 173816, "start": 1751.44, "end": 1756.72, "text": " which is to say, saying more with less, especially for these frameworks, because they're less", "tokens": [51028, 597, 307, 281, 584, 11, 1566, 544, 365, 1570, 11, 2318, 337, 613, 29834, 11, 570, 436, 434, 1570, 51292], "temperature": 0.0, "avg_logprob": -0.11772795112765565, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.009264634922146797}, {"id": 344, "seek": 173816, "start": 1756.72, "end": 1764.24, "text": " opinionated, so that their space of internal semantics can be larger. And then that description", "tokens": [51292, 4800, 770, 11, 370, 300, 641, 1901, 295, 6920, 4361, 45298, 393, 312, 4833, 13, 400, 550, 300, 3855, 51668], "temperature": 0.0, "avg_logprob": -0.11772795112765565, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.009264634922146797}, {"id": 345, "seek": 176424, "start": 1764.24, "end": 1769.04, "text": " that you provided with the relationship between the set and the category theory. So I kind of", "tokens": [50364, 300, 291, 5649, 365, 264, 2480, 1296, 264, 992, 293, 264, 7719, 5261, 13, 407, 286, 733, 295, 50604], "temperature": 0.0, "avg_logprob": -0.12110075839730196, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.07693261653184891}, {"id": 346, "seek": 176424, "start": 1769.04, "end": 1776.16, "text": " summarized it as set is to essential inclusion as category is to relational function. Now,", "tokens": [50604, 14611, 1602, 309, 382, 992, 307, 281, 7115, 15874, 382, 7719, 307, 281, 38444, 2445, 13, 823, 11, 50960], "temperature": 0.0, "avg_logprob": -0.12110075839730196, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.07693261653184891}, {"id": 347, "seek": 176424, "start": 1776.16, "end": 1784.72, "text": " if our concept of organismality or of action in the niche is constructive compositional material,", "tokens": [50960, 498, 527, 3410, 295, 24128, 1860, 420, 295, 3069, 294, 264, 19956, 307, 30223, 10199, 2628, 2527, 11, 51388], "temperature": 0.0, "avg_logprob": -0.12110075839730196, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.07693261653184891}, {"id": 348, "seek": 176424, "start": 1785.84, "end": 1791.52, "text": " then we are looking for, like, what is in or out? Is the microbiome in? Is the", "tokens": [51444, 550, 321, 366, 1237, 337, 11, 411, 11, 437, 307, 294, 420, 484, 30, 1119, 264, 33234, 423, 294, 30, 1119, 264, 51728], "temperature": 0.0, "avg_logprob": -0.12110075839730196, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.07693261653184891}, {"id": 349, "seek": 179152, "start": 1792.16, "end": 1796.96, "text": " pheromone in the ant colony in or out of that thing? Because it's looking for, like, a static", "tokens": [50396, 280, 511, 298, 546, 294, 264, 2511, 23028, 294, 420, 484, 295, 300, 551, 30, 1436, 309, 311, 1237, 337, 11, 411, 11, 257, 13437, 50636], "temperature": 0.0, "avg_logprob": -0.1084280419856944, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0017006519483402371}, {"id": 350, "seek": 179152, "start": 1796.96, "end": 1805.2, "text": " material answer. And then in contrast, the other side of that coin highlights the dynamic,", "tokens": [50636, 2527, 1867, 13, 400, 550, 294, 8712, 11, 264, 661, 1252, 295, 300, 11464, 14254, 264, 8546, 11, 51048], "temperature": 0.0, "avg_logprob": -0.1084280419856944, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0017006519483402371}, {"id": 351, "seek": 179152, "start": 1805.2, "end": 1810.48, "text": " like, whatever it is that self-organizing of the tornado is the tornado, whatever it is that", "tokens": [51048, 411, 11, 2035, 309, 307, 300, 2698, 12, 12372, 3319, 295, 264, 27935, 307, 264, 27935, 11, 2035, 309, 307, 300, 51312], "temperature": 0.0, "avg_logprob": -0.1084280419856944, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0017006519483402371}, {"id": 352, "seek": 179152, "start": 1810.48, "end": 1817.2, "text": " self-organizing for the ant is the ant. And then also this, like, hint slash", "tokens": [51312, 2698, 12, 12372, 3319, 337, 264, 2511, 307, 264, 2511, 13, 400, 550, 611, 341, 11, 411, 11, 12075, 17330, 51648], "temperature": 0.0, "avg_logprob": -0.1084280419856944, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0017006519483402371}, {"id": 353, "seek": 181720, "start": 1818.16, "end": 1824.4, "text": " mobius strip or something that those two in the moment are indistinguishable.", "tokens": [50412, 4298, 4872, 12828, 420, 746, 300, 729, 732, 294, 264, 1623, 366, 1016, 468, 7050, 742, 712, 13, 50724], "temperature": 0.0, "avg_logprob": -0.13377281859680845, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.031123297289013863}, {"id": 354, "seek": 181720, "start": 1825.28, "end": 1829.68, "text": " And yet systems that we choose to define one way or another, or keeping both open,", "tokens": [50768, 400, 1939, 3652, 300, 321, 2826, 281, 6964, 472, 636, 420, 1071, 11, 420, 5145, 1293, 1269, 11, 50988], "temperature": 0.0, "avg_logprob": -0.13377281859680845, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.031123297289013863}, {"id": 355, "seek": 181720, "start": 1830.48, "end": 1836.24, "text": " those design decisions do make all the difference, even if for real systems, as they're observed,", "tokens": [51028, 729, 1715, 5327, 360, 652, 439, 264, 2649, 11, 754, 498, 337, 957, 3652, 11, 382, 436, 434, 13095, 11, 51316], "temperature": 0.0, "avg_logprob": -0.13377281859680845, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.031123297289013863}, {"id": 356, "seek": 181720, "start": 1837.04, "end": 1844.96, "text": " there's indistinguishability. Right, right. I think that's a very important point. And one which,", "tokens": [51356, 456, 311, 1016, 468, 7050, 742, 2310, 13, 1779, 11, 558, 13, 286, 519, 300, 311, 257, 588, 1021, 935, 13, 400, 472, 597, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13377281859680845, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.031123297289013863}, {"id": 357, "seek": 184496, "start": 1845.92, "end": 1852.0, "text": " I mean, this is a, it's always a kind of concern I have whenever I start thinking about, you know,", "tokens": [50412, 286, 914, 11, 341, 307, 257, 11, 309, 311, 1009, 257, 733, 295, 3136, 286, 362, 5699, 286, 722, 1953, 466, 11, 291, 458, 11, 50716], "temperature": 0.0, "avg_logprob": -0.1269301308525933, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.001698182662948966}, {"id": 358, "seek": 184496, "start": 1852.0, "end": 1858.4, "text": " embodied cognition or, you know, extended phenotype type ideas, right? But in a sense, you know,", "tokens": [50716, 42046, 46905, 420, 11, 291, 458, 11, 10913, 7279, 13108, 2010, 3487, 11, 558, 30, 583, 294, 257, 2020, 11, 291, 458, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1269301308525933, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.001698182662948966}, {"id": 359, "seek": 184496, "start": 1859.28, "end": 1863.76, "text": " if what one is trying to do here is construct a kind of formalistic model of observation or", "tokens": [51080, 498, 437, 472, 307, 1382, 281, 360, 510, 307, 7690, 257, 733, 295, 9860, 3142, 2316, 295, 14816, 420, 51304], "temperature": 0.0, "avg_logprob": -0.1269301308525933, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.001698182662948966}, {"id": 360, "seek": 184496, "start": 1863.76, "end": 1868.96, "text": " of cognition or something, then as a kind of first order approximation, one has to start by", "tokens": [51304, 295, 46905, 420, 746, 11, 550, 382, 257, 733, 295, 700, 1668, 28023, 11, 472, 575, 281, 722, 538, 51564], "temperature": 0.0, "avg_logprob": -0.1269301308525933, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.001698182662948966}, {"id": 361, "seek": 184496, "start": 1868.96, "end": 1872.88, "text": " somehow decomposing the world into observers and systems. But of course, that, you know,", "tokens": [51564, 6063, 22867, 6110, 264, 1002, 666, 48090, 293, 3652, 13, 583, 295, 1164, 11, 300, 11, 291, 458, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1269301308525933, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.001698182662948966}, {"id": 362, "seek": 187288, "start": 1872.96, "end": 1877.2, "text": " we know that that decomposition is somehow arbitrarily imposed, right? And that, you know,", "tokens": [50368, 321, 458, 300, 300, 48356, 307, 6063, 19071, 3289, 26491, 11, 558, 30, 400, 300, 11, 291, 458, 11, 50580], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 363, "seek": 187288, "start": 1879.0400000000002, "end": 1882.72, "text": " if you take these things to their extremes, and you say, you know, you allow essentially", "tokens": [50672, 498, 291, 747, 613, 721, 281, 641, 41119, 11, 293, 291, 584, 11, 291, 458, 11, 291, 2089, 4476, 50856], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 364, "seek": 187288, "start": 1882.72, "end": 1886.3200000000002, "text": " everything that the agent is interacting with to be considered, you know, like,", "tokens": [50856, 1203, 300, 264, 9461, 307, 18017, 365, 281, 312, 4888, 11, 291, 458, 11, 411, 11, 51036], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 365, "seek": 187288, "start": 1886.3200000000002, "end": 1889.7600000000002, "text": " not just the microbiome, as you say, but also, you know, tools that they construct or", "tokens": [51036, 406, 445, 264, 33234, 423, 11, 382, 291, 584, 11, 457, 611, 11, 291, 458, 11, 3873, 300, 436, 7690, 420, 51208], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 366, "seek": 187288, "start": 1889.7600000000002, "end": 1893.8400000000001, "text": " environments in which they exist and so on. If you, as you start to consider all of that to be", "tokens": [51208, 12388, 294, 597, 436, 2514, 293, 370, 322, 13, 759, 291, 11, 382, 291, 722, 281, 1949, 439, 295, 300, 281, 312, 51412], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 367, "seek": 187288, "start": 1893.8400000000001, "end": 1897.8400000000001, "text": " a component of that organism, you know, of that agent's phenotype, which is a completely reasonable", "tokens": [51412, 257, 6542, 295, 300, 24128, 11, 291, 458, 11, 295, 300, 9461, 311, 7279, 13108, 11, 597, 307, 257, 2584, 10585, 51612], "temperature": 0.0, "avg_logprob": -0.10980005697770552, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0006457107956521213}, {"id": 368, "seek": 189784, "start": 1897.84, "end": 1905.12, "text": " thing to do, then, and you start to, you know, you start to say, okay, well, their cognitive", "tokens": [50364, 551, 281, 360, 11, 550, 11, 293, 291, 722, 281, 11, 291, 458, 11, 291, 722, 281, 584, 11, 1392, 11, 731, 11, 641, 15605, 50728], "temperature": 0.0, "avg_logprob": -0.094095931565466, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.07802683115005493}, {"id": 369, "seek": 189784, "start": 1905.12, "end": 1909.12, "text": " processes are not just localized to their brain or their spinal column, but are kind of somehow", "tokens": [50728, 7555, 366, 406, 445, 44574, 281, 641, 3567, 420, 641, 28022, 7738, 11, 457, 366, 733, 295, 6063, 50928], "temperature": 0.0, "avg_logprob": -0.094095931565466, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.07802683115005493}, {"id": 370, "seek": 189784, "start": 1909.12, "end": 1913.28, "text": " extended to, you know, the computers they use, the paper they write on, the books they read,", "tokens": [50928, 10913, 281, 11, 291, 458, 11, 264, 10807, 436, 764, 11, 264, 3035, 436, 2464, 322, 11, 264, 3642, 436, 1401, 11, 51136], "temperature": 0.0, "avg_logprob": -0.094095931565466, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.07802683115005493}, {"id": 371, "seek": 189784, "start": 1913.28, "end": 1919.9199999999998, "text": " et cetera. Again, perfectly reasonable thing to do and sort of somehow more descriptive of what's", "tokens": [51136, 1030, 11458, 13, 3764, 11, 6239, 10585, 551, 281, 360, 293, 1333, 295, 6063, 544, 42585, 295, 437, 311, 51468], "temperature": 0.0, "avg_logprob": -0.094095931565466, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.07802683115005493}, {"id": 372, "seek": 189784, "start": 1919.9199999999998, "end": 1924.6399999999999, "text": " really going on. But my fear is always, if you take that too far, then, you know, you end up", "tokens": [51468, 534, 516, 322, 13, 583, 452, 4240, 307, 1009, 11, 498, 291, 747, 300, 886, 1400, 11, 550, 11, 291, 458, 11, 291, 917, 493, 51704], "temperature": 0.0, "avg_logprob": -0.094095931565466, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.07802683115005493}, {"id": 373, "seek": 192464, "start": 1924.64, "end": 1928.96, "text": " destroying the whole assumption that the idealization was based on, which is that you can neatly", "tokens": [50364, 19926, 264, 1379, 15302, 300, 264, 7157, 2144, 390, 2361, 322, 11, 597, 307, 300, 291, 393, 36634, 50580], "temperature": 0.0, "avg_logprob": -0.08406828203771868, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.012810399755835533}, {"id": 374, "seek": 192464, "start": 1928.96, "end": 1934.4, "text": " decompose, you know, the world into observers and systems. And so, I always get a bit nervous", "tokens": [50580, 22867, 541, 11, 291, 458, 11, 264, 1002, 666, 48090, 293, 3652, 13, 400, 370, 11, 286, 1009, 483, 257, 857, 6296, 50852], "temperature": 0.0, "avg_logprob": -0.08406828203771868, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.012810399755835533}, {"id": 375, "seek": 192464, "start": 1934.4, "end": 1940.3200000000002, "text": " when thinking about that, that it's like, yes, you know, in a sense, you know, we know this is an", "tokens": [50852, 562, 1953, 466, 300, 11, 300, 309, 311, 411, 11, 2086, 11, 291, 458, 11, 294, 257, 2020, 11, 291, 458, 11, 321, 458, 341, 307, 364, 51148], "temperature": 0.0, "avg_logprob": -0.08406828203771868, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.012810399755835533}, {"id": 376, "seek": 192464, "start": 1940.3200000000002, "end": 1947.0400000000002, "text": " approximation and we know that that approximation is not really true, but how, you know, how much can", "tokens": [51148, 28023, 293, 321, 458, 300, 300, 28023, 307, 406, 534, 2074, 11, 457, 577, 11, 291, 458, 11, 577, 709, 393, 51484], "temperature": 0.0, "avg_logprob": -0.08406828203771868, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.012810399755835533}, {"id": 377, "seek": 192464, "start": 1947.0400000000002, "end": 1951.6000000000001, "text": " you afford to sort of loosen your grip on that approximation before the whole thing just kind", "tokens": [51484, 291, 6157, 281, 1333, 295, 26169, 428, 12007, 322, 300, 28023, 949, 264, 1379, 551, 445, 733, 51712], "temperature": 0.0, "avg_logprob": -0.08406828203771868, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.012810399755835533}, {"id": 378, "seek": 195160, "start": 1951.6, "end": 1955.76, "text": " of falls apart. I don't really know the answer to that question. But I think it's an interesting one.", "tokens": [50364, 295, 8804, 4936, 13, 286, 500, 380, 534, 458, 264, 1867, 281, 300, 1168, 13, 583, 286, 519, 309, 311, 364, 1880, 472, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13900536396464364, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.018543947488069534}, {"id": 379, "seek": 195160, "start": 1955.76, "end": 1961.76, "text": " Yeah. How about ask some questions from chat and then give your first thoughts and then we'll see", "tokens": [50572, 865, 13, 1012, 466, 1029, 512, 1651, 490, 5081, 293, 550, 976, 428, 700, 4598, 293, 550, 321, 603, 536, 50872], "temperature": 0.0, "avg_logprob": -0.13900536396464364, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.018543947488069534}, {"id": 380, "seek": 195160, "start": 1962.56, "end": 1967.36, "text": " maybe where that kind of lands with further questions or how it connects to active inference.", "tokens": [50912, 1310, 689, 300, 733, 295, 5949, 365, 3052, 1651, 420, 577, 309, 16967, 281, 4967, 38253, 13, 51152], "temperature": 0.0, "avg_logprob": -0.13900536396464364, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.018543947488069534}, {"id": 381, "seek": 195160, "start": 1967.36, "end": 1971.28, "text": " So that sounds good. Oh, by the way, should I keep my screen share on or should I?", "tokens": [51152, 407, 300, 3263, 665, 13, 876, 11, 538, 264, 636, 11, 820, 286, 1066, 452, 2568, 2073, 322, 420, 820, 286, 30, 51348], "temperature": 0.0, "avg_logprob": -0.13900536396464364, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.018543947488069534}, {"id": 382, "seek": 195160, "start": 1971.28, "end": 1977.84, "text": " Yeah, we might want to go to a figure. So it's fine. Okay. Yeah. All right. Quantum Bell wrote,", "tokens": [51348, 865, 11, 321, 1062, 528, 281, 352, 281, 257, 2573, 13, 407, 309, 311, 2489, 13, 1033, 13, 865, 13, 1057, 558, 13, 44964, 11485, 4114, 11, 51676], "temperature": 0.0, "avg_logprob": -0.13900536396464364, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.018543947488069534}, {"id": 383, "seek": 197784, "start": 1978.72, "end": 1981.6799999999998, "text": " how does this help us reason about causality?", "tokens": [50408, 577, 775, 341, 854, 505, 1778, 466, 3302, 1860, 30, 50556], "temperature": 0.0, "avg_logprob": -0.1289603732881092, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0019559995271265507}, {"id": 384, "seek": 197784, "start": 1983.4399999999998, "end": 1989.04, "text": " That's a fascinating question. Okay. So that's that's another major aspect of, you know, why I", "tokens": [50644, 663, 311, 257, 10343, 1168, 13, 1033, 13, 407, 300, 311, 300, 311, 1071, 2563, 4171, 295, 11, 291, 458, 11, 983, 286, 50924], "temperature": 0.0, "avg_logprob": -0.1289603732881092, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0019559995271265507}, {"id": 385, "seek": 197784, "start": 1989.04, "end": 1995.4399999999998, "text": " think this research program is exciting, because so again, this is something where I'm interested", "tokens": [50924, 519, 341, 2132, 1461, 307, 4670, 11, 570, 370, 797, 11, 341, 307, 746, 689, 286, 478, 3102, 51244], "temperature": 0.0, "avg_logprob": -0.1289603732881092, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0019559995271265507}, {"id": 386, "seek": 197784, "start": 1995.4399999999998, "end": 1998.8799999999999, "text": " to get the kind of active inference perspective, because I know this again, it's a topic in which", "tokens": [51244, 281, 483, 264, 733, 295, 4967, 38253, 4585, 11, 570, 286, 458, 341, 797, 11, 309, 311, 257, 4829, 294, 597, 51416], "temperature": 0.0, "avg_logprob": -0.1289603732881092, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0019559995271265507}, {"id": 387, "seek": 197784, "start": 1998.8799999999999, "end": 2006.48, "text": " much has been written and I'm largely ignorant. But yes, so one question you could ask is,", "tokens": [51416, 709, 575, 668, 3720, 293, 286, 478, 11611, 29374, 13, 583, 2086, 11, 370, 472, 1168, 291, 727, 1029, 307, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1289603732881092, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0019559995271265507}, {"id": 388, "seek": 200648, "start": 2006.96, "end": 2009.92, "text": " yeah, if you have a description of a computation like this, like,", "tokens": [50388, 1338, 11, 498, 291, 362, 257, 3855, 295, 257, 24903, 411, 341, 11, 411, 11, 50536], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 389, "seek": 200648, "start": 2010.72, "end": 2014.72, "text": " let's go back up to the Turing machine case, the single way Turing machine case that's", "tokens": [50576, 718, 311, 352, 646, 493, 281, 264, 314, 1345, 3479, 1389, 11, 264, 2167, 636, 314, 1345, 3479, 1389, 300, 311, 50776], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 390, "seek": 200648, "start": 2014.72, "end": 2017.76, "text": " relatively easy to analyze, although still far from obvious what's going on.", "tokens": [50776, 7226, 1858, 281, 12477, 11, 4878, 920, 1400, 490, 6322, 437, 311, 516, 322, 13, 50928], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 391, "seek": 200648, "start": 2018.48, "end": 2022.8, "text": " So suppose you have a computation of this kind, and you want to ask, what is its causal structure?", "tokens": [50964, 407, 7297, 291, 362, 257, 24903, 295, 341, 733, 11, 293, 291, 528, 281, 1029, 11, 437, 307, 1080, 38755, 3877, 30, 51180], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 392, "seek": 200648, "start": 2022.8, "end": 2026.96, "text": " In other words, you know, for each edge, each time I'm applying this Turing machine transition", "tokens": [51180, 682, 661, 2283, 11, 291, 458, 11, 337, 1184, 4691, 11, 1184, 565, 286, 478, 9275, 341, 314, 1345, 3479, 6034, 51388], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 393, "seek": 200648, "start": 2026.96, "end": 2032.0, "text": " function, can I construct some kind of graph, you know, some some directed graph representation that", "tokens": [51388, 2445, 11, 393, 286, 7690, 512, 733, 295, 4295, 11, 291, 458, 11, 512, 512, 12898, 4295, 10290, 300, 51640], "temperature": 0.0, "avg_logprob": -0.10369577182559517, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.0015966309001669288}, {"id": 394, "seek": 203200, "start": 2032.0, "end": 2037.6, "text": " tells me how these events are linked together. So in the, within the original research program,", "tokens": [50364, 5112, 385, 577, 613, 3931, 366, 9408, 1214, 13, 407, 294, 264, 11, 1951, 264, 3380, 2132, 1461, 11, 50644], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 395, "seek": 203200, "start": 2038.56, "end": 2042.24, "text": " the so-called Wolfram model research program that kind of started a lot of these investigations,", "tokens": [50692, 264, 370, 12, 11880, 16634, 2356, 2316, 2132, 1461, 300, 733, 295, 1409, 257, 688, 295, 613, 25582, 11, 50876], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 396, "seek": 203200, "start": 2042.24, "end": 2045.2, "text": " we were looking at this all the time, right? We were looking at kind of, you know, taking", "tokens": [50876, 321, 645, 1237, 412, 341, 439, 264, 565, 11, 558, 30, 492, 645, 1237, 412, 733, 295, 11, 291, 458, 11, 1940, 51024], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 397, "seek": 203200, "start": 2045.2, "end": 2049.52, "text": " computations and looking at that causal structure and trying to, you know, infer things about what", "tokens": [51024, 2807, 763, 293, 1237, 412, 300, 38755, 3877, 293, 1382, 281, 11, 291, 458, 11, 13596, 721, 466, 437, 51240], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 398, "seek": 203200, "start": 2049.52, "end": 2054.32, "text": " was going on about the, you know, the semantics of the computation based on causal relationships.", "tokens": [51240, 390, 516, 322, 466, 264, 11, 291, 458, 11, 264, 4361, 45298, 295, 264, 24903, 2361, 322, 38755, 6159, 13, 51480], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 399, "seek": 203200, "start": 2054.32, "end": 2057.6, "text": " And at a certain point, I started to realize, and I think other people had realized this before", "tokens": [51480, 400, 412, 257, 1629, 935, 11, 286, 1409, 281, 4325, 11, 293, 286, 519, 661, 561, 632, 5334, 341, 949, 51644], "temperature": 0.0, "avg_logprob": -0.09974452427455358, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0015723684336990118}, {"id": 400, "seek": 205760, "start": 2057.6, "end": 2062.96, "text": " I did, but I'm often slow to pick these things up. I, I, I and other people started to realize that", "tokens": [50364, 286, 630, 11, 457, 286, 478, 2049, 2964, 281, 1888, 613, 721, 493, 13, 286, 11, 286, 11, 286, 293, 661, 561, 1409, 281, 4325, 300, 50632], "temperature": 0.0, "avg_logprob": -0.101102259732032, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.002887774957343936}, {"id": 401, "seek": 205760, "start": 2062.96, "end": 2069.52, "text": " the notion of causality we were using was kind of nonsense. I mean, it was not completely hopeless,", "tokens": [50632, 264, 10710, 295, 3302, 1860, 321, 645, 1228, 390, 733, 295, 14925, 13, 286, 914, 11, 309, 390, 406, 2584, 27317, 11, 50960], "temperature": 0.0, "avg_logprob": -0.101102259732032, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.002887774957343936}, {"id": 402, "seek": 205760, "start": 2069.52, "end": 2073.68, "text": " but it wasn't really causality, or it couldn't really be called causality in any, in any definite", "tokens": [50960, 457, 309, 2067, 380, 534, 3302, 1860, 11, 420, 309, 2809, 380, 534, 312, 1219, 3302, 1860, 294, 604, 11, 294, 604, 25131, 51168], "temperature": 0.0, "avg_logprob": -0.101102259732032, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.002887774957343936}, {"id": 403, "seek": 205760, "start": 2073.68, "end": 2080.16, "text": " sense. So what do I mean by that? So first of all, it was a very technical problem. So if you're", "tokens": [51168, 2020, 13, 407, 437, 360, 286, 914, 538, 300, 30, 407, 700, 295, 439, 11, 309, 390, 257, 588, 6191, 1154, 13, 407, 498, 291, 434, 51492], "temperature": 0.0, "avg_logprob": -0.101102259732032, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.002887774957343936}, {"id": 404, "seek": 205760, "start": 2080.16, "end": 2084.16, "text": " looking at something like a Turing machine evolution or a hypergraphic writing system as we were,", "tokens": [51492, 1237, 412, 746, 411, 257, 314, 1345, 3479, 9303, 420, 257, 9848, 34091, 299, 3579, 1185, 382, 321, 645, 11, 51692], "temperature": 0.0, "avg_logprob": -0.101102259732032, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.002887774957343936}, {"id": 405, "seek": 208416, "start": 2085.12, "end": 2089.92, "text": " then there's a very tempting and apparently obvious natural definition of causality that", "tokens": [50412, 550, 456, 311, 257, 588, 37900, 293, 7970, 6322, 3303, 7123, 295, 3302, 1860, 300, 50652], "temperature": 0.0, "avg_logprob": -0.0798350984016351, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0019864500500261784}, {"id": 406, "seek": 208416, "start": 2089.92, "end": 2094.48, "text": " you can use, which is to ask, you know, when you split the world up into events that take,", "tokens": [50652, 291, 393, 764, 11, 597, 307, 281, 1029, 11, 291, 458, 11, 562, 291, 7472, 264, 1002, 493, 666, 3931, 300, 747, 11, 50880], "temperature": 0.0, "avg_logprob": -0.0798350984016351, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0019864500500261784}, {"id": 407, "seek": 208416, "start": 2094.48, "end": 2098.48, "text": " you know, some part of your data structure as input and output, you know, some other part of a", "tokens": [50880, 291, 458, 11, 512, 644, 295, 428, 1412, 3877, 382, 4846, 293, 5598, 11, 291, 458, 11, 512, 661, 644, 295, 257, 51080], "temperature": 0.0, "avg_logprob": -0.0798350984016351, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0019864500500261784}, {"id": 408, "seek": 208416, "start": 2098.48, "end": 2104.48, "text": " data structure as output, then you can very easily ask, well, does the output of one event", "tokens": [51080, 1412, 3877, 382, 5598, 11, 550, 291, 393, 588, 3612, 1029, 11, 731, 11, 775, 264, 5598, 295, 472, 2280, 51380], "temperature": 0.0, "avg_logprob": -0.0798350984016351, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0019864500500261784}, {"id": 409, "seek": 208416, "start": 2104.48, "end": 2109.6, "text": " intersect with the input of another event? So if I show the hypergraph example, it's perhaps easier", "tokens": [51380, 27815, 365, 264, 4846, 295, 1071, 2280, 30, 407, 498, 286, 855, 264, 9848, 34091, 1365, 11, 309, 311, 4317, 3571, 51636], "temperature": 0.0, "avg_logprob": -0.0798350984016351, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.0019864500500261784}, {"id": 410, "seek": 210960, "start": 2109.6, "end": 2115.04, "text": " to see. So you have a hypergraphy writing rule that looks like this, right? So you know, you have,", "tokens": [50364, 281, 536, 13, 407, 291, 362, 257, 9848, 34091, 88, 3579, 4978, 300, 1542, 411, 341, 11, 558, 30, 407, 291, 458, 11, 291, 362, 11, 50636], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 411, "seek": 210960, "start": 2115.04, "end": 2118.24, "text": " you say, if I have a piece of hypergraph that looks like that, I replace it with another piece of", "tokens": [50636, 291, 584, 11, 498, 286, 362, 257, 2522, 295, 9848, 34091, 300, 1542, 411, 300, 11, 286, 7406, 309, 365, 1071, 2522, 295, 50796], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 412, "seek": 210960, "start": 2118.24, "end": 2122.88, "text": " hypergraph that looks like this. So at each, each time you apply an event, you can think of that", "tokens": [50796, 9848, 34091, 300, 1542, 411, 341, 13, 407, 412, 1184, 11, 1184, 565, 291, 3079, 364, 2280, 11, 291, 393, 519, 295, 300, 51028], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 413, "seek": 210960, "start": 2122.88, "end": 2128.3199999999997, "text": " event as, you know, ingesting certain hyper edges and kind of, you know, replacing them with others.", "tokens": [51028, 2280, 382, 11, 291, 458, 11, 3957, 8714, 1629, 9848, 8819, 293, 733, 295, 11, 291, 458, 11, 19139, 552, 365, 2357, 13, 51300], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 414, "seek": 210960, "start": 2128.3199999999997, "end": 2132.56, "text": " So you can divide it up into a sort of the, the, the input hyper edges that are being ingested", "tokens": [51300, 407, 291, 393, 9845, 309, 493, 666, 257, 1333, 295, 264, 11, 264, 11, 264, 4846, 9848, 8819, 300, 366, 885, 3957, 21885, 51512], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 415, "seek": 210960, "start": 2132.56, "end": 2137.2, "text": " versus the output hyper edges that are being produced. And so then you can ask, well, do the", "tokens": [51512, 5717, 264, 5598, 9848, 8819, 300, 366, 885, 7126, 13, 400, 370, 550, 291, 393, 1029, 11, 731, 11, 360, 264, 51744], "temperature": 0.0, "avg_logprob": -0.07496705398060917, "compression_ratio": 2.1476014760147604, "no_speech_prob": 0.022961579263210297}, {"id": 416, "seek": 213720, "start": 2137.2, "end": 2144.64, "text": " output, did I use, did I subsequently ingest in some future event hyper edges that were output", "tokens": [50364, 5598, 11, 630, 286, 764, 11, 630, 286, 26514, 3957, 377, 294, 512, 2027, 2280, 9848, 8819, 300, 645, 5598, 50736], "temperature": 0.0, "avg_logprob": -0.13139934539794923, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0021813497878611088}, {"id": 417, "seek": 213720, "start": 2144.64, "end": 2148.3199999999997, "text": " in some previous event? Well, if the answer is yes, then pretty obviously that future event", "tokens": [50736, 294, 512, 3894, 2280, 30, 1042, 11, 498, 264, 1867, 307, 2086, 11, 550, 1238, 2745, 300, 2027, 2280, 50920], "temperature": 0.0, "avg_logprob": -0.13139934539794923, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0021813497878611088}, {"id": 418, "seek": 213720, "start": 2148.3199999999997, "end": 2152.08, "text": " couldn't have occurred unless the previous event had already occurred. So then you could say, well,", "tokens": [50920, 2809, 380, 362, 11068, 5969, 264, 3894, 2280, 632, 1217, 11068, 13, 407, 550, 291, 727, 584, 11, 731, 11, 51108], "temperature": 0.0, "avg_logprob": -0.13139934539794923, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0021813497878611088}, {"id": 419, "seek": 213720, "start": 2152.08, "end": 2156.48, "text": " then one of those events causes the other. So in general, you could say that two, you know,", "tokens": [51108, 550, 472, 295, 729, 3931, 7700, 264, 661, 13, 407, 294, 2674, 11, 291, 727, 584, 300, 732, 11, 291, 458, 11, 51328], "temperature": 0.0, "avg_logprob": -0.13139934539794923, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0021813497878611088}, {"id": 420, "seek": 213720, "start": 2156.48, "end": 2163.12, "text": " an event A causes event B, if it's the case that the output, that the collection of tokens that", "tokens": [51328, 364, 2280, 316, 7700, 2280, 363, 11, 498, 309, 311, 264, 1389, 300, 264, 5598, 11, 300, 264, 5765, 295, 22667, 300, 51660], "temperature": 0.0, "avg_logprob": -0.13139934539794923, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0021813497878611088}, {"id": 421, "seek": 216312, "start": 2163.12, "end": 2167.2, "text": " was produced in the output of event A has a non-zero intersection with the collection tokens", "tokens": [50364, 390, 7126, 294, 264, 5598, 295, 2280, 316, 575, 257, 2107, 12, 32226, 15236, 365, 264, 5765, 22667, 50568], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 422, "seek": 216312, "start": 2167.2, "end": 2171.3599999999997, "text": " that were ingested as part of the input of event B. And that's a very tempting, very natural", "tokens": [50568, 300, 645, 3957, 21885, 382, 644, 295, 264, 4846, 295, 2280, 363, 13, 400, 300, 311, 257, 588, 37900, 11, 588, 3303, 50776], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 423, "seek": 216312, "start": 2171.3599999999997, "end": 2175.92, "text": " definition of kind of causality in these systems. Turns out it doesn't really work. I mean, it works", "tokens": [50776, 7123, 295, 733, 295, 3302, 1860, 294, 613, 3652, 13, 29524, 484, 309, 1177, 380, 534, 589, 13, 286, 914, 11, 309, 1985, 51004], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 424, "seek": 216312, "start": 2176.72, "end": 2180.7999999999997, "text": " pretty well, but there are cases in which it fails and it fails pretty spectacularly. And", "tokens": [51044, 1238, 731, 11, 457, 456, 366, 3331, 294, 597, 309, 18199, 293, 309, 18199, 1238, 18149, 356, 13, 400, 51248], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 425, "seek": 216312, "start": 2180.7999999999997, "end": 2184.96, "text": " so the kind of canonical case where it fails spectacularly is that you can have events that", "tokens": [51248, 370, 264, 733, 295, 46491, 1389, 689, 309, 18199, 18149, 356, 307, 300, 291, 393, 362, 3931, 300, 51456], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 426, "seek": 216312, "start": 2184.96, "end": 2189.04, "text": " don't actually do anything, right? You can have events that just kind of touch an edge, touch a", "tokens": [51456, 500, 380, 767, 360, 1340, 11, 558, 30, 509, 393, 362, 3931, 300, 445, 733, 295, 2557, 364, 4691, 11, 2557, 257, 51660], "temperature": 0.0, "avg_logprob": -0.09709881333743825, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.005378460045903921}, {"id": 427, "seek": 218904, "start": 2189.04, "end": 2194.72, "text": " token and output it again unchanged, but maybe, you know, it modifies the name, it modifies the", "tokens": [50364, 14862, 293, 5598, 309, 797, 44553, 11, 457, 1310, 11, 291, 458, 11, 309, 1072, 11221, 264, 1315, 11, 309, 1072, 11221, 264, 50648], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 428, "seek": 218904, "start": 2194.72, "end": 2198.88, "text": " identifier, but it doesn't actually change anything about the structure of the hypergraph or the", "tokens": [50648, 45690, 11, 457, 309, 1177, 380, 767, 1319, 1340, 466, 264, 3877, 295, 264, 9848, 34091, 420, 264, 50856], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 429, "seek": 218904, "start": 2198.88, "end": 2203.44, "text": " Turing machine state or whatever. So pretty obviously that event doesn't matter. It shouldn't", "tokens": [50856, 314, 1345, 3479, 1785, 420, 2035, 13, 407, 1238, 2745, 300, 2280, 1177, 380, 1871, 13, 467, 4659, 380, 51084], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 430, "seek": 218904, "start": 2203.44, "end": 2208.72, "text": " be causally related to anything in the future. But because it ingested the edge and then didn't do", "tokens": [51084, 312, 3302, 379, 4077, 281, 1340, 294, 264, 2027, 13, 583, 570, 309, 3957, 21885, 264, 4691, 293, 550, 994, 380, 360, 51348], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 431, "seek": 218904, "start": 2208.72, "end": 2212.88, "text": " anything, you know, did some identity operation, but then, you know, produced it in the output again,", "tokens": [51348, 1340, 11, 291, 458, 11, 630, 512, 6575, 6916, 11, 457, 550, 11, 291, 458, 11, 7126, 309, 294, 264, 5598, 797, 11, 51556], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 432, "seek": 218904, "start": 2212.88, "end": 2216.88, "text": " it will kind of register as being causally related to any future event that used that", "tokens": [51556, 309, 486, 733, 295, 7280, 382, 885, 3302, 379, 4077, 281, 604, 2027, 2280, 300, 1143, 300, 51756], "temperature": 0.0, "avg_logprob": -0.10043139317456413, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.014953859150409698}, {"id": 433, "seek": 221688, "start": 2216.88, "end": 2221.2000000000003, "text": " edge, even though it didn't make any difference. That's just one very obvious example. There are", "tokens": [50364, 4691, 11, 754, 1673, 309, 994, 380, 652, 604, 2649, 13, 663, 311, 445, 472, 588, 6322, 1365, 13, 821, 366, 50580], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 434, "seek": 221688, "start": 2221.2000000000003, "end": 2225.44, "text": " other cases where it became clear that whatever this thing was, whatever this algorithm was", "tokens": [50580, 661, 3331, 689, 309, 3062, 1850, 300, 2035, 341, 551, 390, 11, 2035, 341, 9284, 390, 50792], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 435, "seek": 221688, "start": 2225.44, "end": 2230.08, "text": " detecting, it wasn't really causality. So I tried to think about, you know, what's a more sensible", "tokens": [50792, 40237, 11, 309, 2067, 380, 534, 3302, 1860, 13, 407, 286, 3031, 281, 519, 466, 11, 291, 458, 11, 437, 311, 257, 544, 25380, 51024], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 436, "seek": 221688, "start": 2230.08, "end": 2234.32, "text": " definition of causality. And I started working on things to do with, you know, a slightly kind of", "tokens": [51024, 7123, 295, 3302, 1860, 13, 400, 286, 1409, 1364, 322, 721, 281, 360, 365, 11, 291, 458, 11, 257, 4748, 733, 295, 51236], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 437, "seek": 221688, "start": 2234.32, "end": 2239.44, "text": " blockchain inspired ideas where you say, okay, well, rather than just arbitrarily assigning,", "tokens": [51236, 17176, 7547, 3487, 689, 291, 584, 11, 1392, 11, 731, 11, 2831, 813, 445, 19071, 3289, 49602, 11, 51492], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 438, "seek": 221688, "start": 2239.44, "end": 2244.7200000000003, "text": " you know, identifiers to these tokens every time they're created, what if I recursively", "tokens": [51492, 291, 458, 11, 2473, 23463, 281, 613, 22667, 633, 565, 436, 434, 2942, 11, 437, 498, 286, 20560, 3413, 51756], "temperature": 0.0, "avg_logprob": -0.07569914433493544, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.0033207195810973644}, {"id": 439, "seek": 224472, "start": 2245.6, "end": 2249.9199999999996, "text": " construct the identifier of the token based on its causal history. So in other words, each token,", "tokens": [50408, 7690, 264, 45690, 295, 264, 14862, 2361, 322, 1080, 38755, 2503, 13, 407, 294, 661, 2283, 11, 1184, 14862, 11, 50624], "temperature": 0.0, "avg_logprob": -0.13797783401777158, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.016906609758734703}, {"id": 440, "seek": 224472, "start": 2249.9199999999996, "end": 2255.52, "text": " like each hyper edge or each state in my each, each tape square on my Turing machine tape,", "tokens": [50624, 411, 1184, 9848, 4691, 420, 1184, 1785, 294, 452, 1184, 11, 1184, 7314, 3732, 322, 452, 314, 1345, 3479, 7314, 11, 50904], "temperature": 0.0, "avg_logprob": -0.13797783401777158, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.016906609758734703}, {"id": 441, "seek": 224472, "start": 2256.3999999999996, "end": 2261.3599999999997, "text": " I, the identifier is not just some random number that gets generated by my algorithm,", "tokens": [50948, 286, 11, 264, 45690, 307, 406, 445, 512, 4974, 1230, 300, 2170, 10833, 538, 452, 9284, 11, 51196], "temperature": 0.0, "avg_logprob": -0.13797783401777158, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.016906609758734703}, {"id": 442, "seek": 224472, "start": 2261.3599999999997, "end": 2267.2, "text": " but instead its identifier is a directed graph representation of its complete causal history.", "tokens": [51196, 457, 2602, 1080, 45690, 307, 257, 12898, 4295, 10290, 295, 1080, 3566, 38755, 2503, 13, 51488], "temperature": 0.0, "avg_logprob": -0.13797783401777158, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.016906609758734703}, {"id": 443, "seek": 224472, "start": 2267.2, "end": 2272.72, "text": " Well, then kind of recursively, it's identified can only change if the causal history was updated.", "tokens": [51488, 1042, 11, 550, 733, 295, 20560, 3413, 11, 309, 311, 9234, 393, 787, 1319, 498, 264, 38755, 2503, 390, 10588, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13797783401777158, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.016906609758734703}, {"id": 444, "seek": 227272, "start": 2272.7999999999997, "end": 2276.3999999999996, "text": " And so you don't end up with these kind of spurious causal relations that I described before.", "tokens": [50368, 400, 370, 291, 500, 380, 917, 493, 365, 613, 733, 295, 637, 24274, 38755, 2299, 300, 286, 7619, 949, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 445, "seek": 227272, "start": 2276.3999999999996, "end": 2281.6, "text": " So that seemed like one tempting way of resolving this problem. But then I realized,", "tokens": [50548, 407, 300, 6576, 411, 472, 37900, 636, 295, 49940, 341, 1154, 13, 583, 550, 286, 5334, 11, 50808], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 446, "seek": 227272, "start": 2281.6, "end": 2284.8799999999997, "text": " actually, there's a much more fundamental problem. There's a problem, there's a philosophical problem", "tokens": [50808, 767, 11, 456, 311, 257, 709, 544, 8088, 1154, 13, 821, 311, 257, 1154, 11, 456, 311, 257, 25066, 1154, 50972], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 447, "seek": 227272, "start": 2284.8799999999997, "end": 2289.6, "text": " with the way that we're thinking about causality, which is that it's not really, you know, so,", "tokens": [50972, 365, 264, 636, 300, 321, 434, 1953, 466, 3302, 1860, 11, 597, 307, 300, 309, 311, 406, 534, 11, 291, 458, 11, 370, 11, 51208], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 448, "seek": 227272, "start": 2290.9599999999996, "end": 2297.4399999999996, "text": " okay, this is a long tangent, which I'll talk about a little bit, but I won't get into the", "tokens": [51276, 1392, 11, 341, 307, 257, 938, 27747, 11, 597, 286, 603, 751, 466, 257, 707, 857, 11, 457, 286, 1582, 380, 483, 666, 264, 51600], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 449, "seek": 227272, "start": 2297.4399999999996, "end": 2301.12, "text": " complete details unless people are interested. But I ended up T I ended up talking to a bunch of", "tokens": [51600, 3566, 4365, 5969, 561, 366, 3102, 13, 583, 286, 4590, 493, 314, 286, 4590, 493, 1417, 281, 257, 3840, 295, 51784], "temperature": 0.0, "avg_logprob": -0.10394470983271976, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.0023956685326993465}, {"id": 450, "seek": 230112, "start": 2301.12, "end": 2304.7999999999997, "text": " philosophers who, you know, who worked on causality and people who worked on parallel", "tokens": [50364, 36839, 567, 11, 291, 458, 11, 567, 2732, 322, 3302, 1860, 293, 561, 567, 2732, 322, 8952, 50548], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 451, "seek": 230112, "start": 2304.7999999999997, "end": 2308.56, "text": " programming and quantum information theory and other places where causality was,", "tokens": [50548, 9410, 293, 13018, 1589, 5261, 293, 661, 3190, 689, 3302, 1860, 390, 11, 50736], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 452, "seek": 230112, "start": 2308.56, "end": 2312.48, "text": " was, was studied and asked them kind of basically what, what is, what, what do you mean by causality?", "tokens": [50736, 390, 11, 390, 9454, 293, 2351, 552, 733, 295, 1936, 437, 11, 437, 307, 11, 437, 11, 437, 360, 291, 914, 538, 3302, 1860, 30, 50932], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 453, "seek": 230112, "start": 2312.48, "end": 2319.12, "text": " What is causality? What is this thing we're trying to define? And in some form or another,", "tokens": [50932, 708, 307, 3302, 1860, 30, 708, 307, 341, 551, 321, 434, 1382, 281, 6964, 30, 400, 294, 512, 1254, 420, 1071, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 454, "seek": 230112, "start": 2319.12, "end": 2324.0, "text": " all of the definitions boiled down to, you know, event A causes event B if", "tokens": [51264, 439, 295, 264, 21988, 21058, 760, 281, 11, 291, 458, 11, 2280, 316, 7700, 2280, 363, 498, 51508], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 455, "seek": 230112, "start": 2324.96, "end": 2329.2, "text": " had event A not occurred, then event B would not have occurred. So in other words,", "tokens": [51556, 632, 2280, 316, 406, 11068, 11, 550, 2280, 363, 576, 406, 362, 11068, 13, 407, 294, 661, 2283, 11, 51768], "temperature": 0.0, "avg_logprob": -0.13684139101524054, "compression_ratio": 1.8731884057971016, "no_speech_prob": 0.005136766936630011}, {"id": 456, "seek": 232920, "start": 2329.2, "end": 2333.4399999999996, "text": " you need a counterfactual, you need some possible history, some possible world in which event A", "tokens": [50364, 291, 643, 257, 5682, 44919, 901, 11, 291, 643, 512, 1944, 2503, 11, 512, 1944, 1002, 294, 597, 2280, 316, 50576], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 457, "seek": 232920, "start": 2333.4399999999996, "end": 2337.52, "text": " didn't happen. But if you're reasoning about a purely deterministic event system like a Turing", "tokens": [50576, 994, 380, 1051, 13, 583, 498, 291, 434, 21577, 466, 257, 17491, 15957, 3142, 2280, 1185, 411, 257, 314, 1345, 50780], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 458, "seek": 232920, "start": 2337.52, "end": 2341.52, "text": " machine, that doesn't make any sense. Because if you're, you know, if you, if you have a single", "tokens": [50780, 3479, 11, 300, 1177, 380, 652, 604, 2020, 13, 1436, 498, 291, 434, 11, 291, 458, 11, 498, 291, 11, 498, 291, 362, 257, 2167, 50980], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 459, "seek": 232920, "start": 2341.52, "end": 2346.24, "text": " Turing machine transition function, there is no possible world in which that transition function", "tokens": [50980, 314, 1345, 3479, 6034, 2445, 11, 456, 307, 572, 1944, 1002, 294, 597, 300, 6034, 2445, 51216], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 460, "seek": 232920, "start": 2346.24, "end": 2349.3599999999997, "text": " didn't fire in that particular way. Because if it didn't fire in that particular way,", "tokens": [51216, 994, 380, 2610, 294, 300, 1729, 636, 13, 1436, 498, 309, 994, 380, 2610, 294, 300, 1729, 636, 11, 51372], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 461, "seek": 232920, "start": 2349.3599999999997, "end": 2352.48, "text": " you would not be reasoning about that Turing machine anymore, you'd be reasoning about", "tokens": [51372, 291, 576, 406, 312, 21577, 466, 300, 314, 1345, 3479, 3602, 11, 291, 1116, 312, 21577, 466, 51528], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 462, "seek": 232920, "start": 2352.48, "end": 2358.16, "text": " different Turing machine. So suddenly this, you know, to make sense of these notions of causality,", "tokens": [51528, 819, 314, 1345, 3479, 13, 407, 5800, 341, 11, 291, 458, 11, 281, 652, 2020, 295, 613, 35799, 295, 3302, 1860, 11, 51812], "temperature": 0.0, "avg_logprob": -0.0628507552608367, "compression_ratio": 2.176079734219269, "no_speech_prob": 0.01098156999796629}, {"id": 463, "seek": 235816, "start": 2358.16, "end": 2363.04, "text": " you need a kind of Leibnizian, you know, modality is view of reality that just doesn't exist for", "tokens": [50364, 291, 643, 257, 733, 295, 1456, 897, 77, 590, 952, 11, 291, 458, 11, 1072, 1860, 307, 1910, 295, 4103, 300, 445, 1177, 380, 2514, 337, 50608], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 464, "seek": 235816, "start": 2363.04, "end": 2367.7599999999998, "text": " these deterministic computational systems. So either you need to define computation,", "tokens": [50608, 613, 15957, 3142, 28270, 3652, 13, 407, 2139, 291, 643, 281, 6964, 24903, 11, 50844], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 465, "seek": 235816, "start": 2368.3999999999996, "end": 2373.8399999999997, "text": " you need to define causality only at the multiway level, only at the level where you have many", "tokens": [50876, 291, 643, 281, 6964, 3302, 1860, 787, 412, 264, 4825, 676, 1496, 11, 787, 412, 264, 1496, 689, 291, 362, 867, 51148], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 466, "seek": 235816, "start": 2373.8399999999997, "end": 2377.44, "text": " computations or possibly all computations happening in parallel, and then you can define", "tokens": [51148, 2807, 763, 420, 6264, 439, 2807, 763, 2737, 294, 8952, 11, 293, 550, 291, 393, 6964, 51328], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 467, "seek": 235816, "start": 2377.44, "end": 2381.7599999999998, "text": " causality relative to all of the, you know, relative between them, or you were kind of posed, right?", "tokens": [51328, 3302, 1860, 4972, 281, 439, 295, 264, 11, 291, 458, 11, 4972, 1296, 552, 11, 420, 291, 645, 733, 295, 31399, 11, 558, 30, 51544], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 468, "seek": 235816, "start": 2381.7599999999998, "end": 2385.68, "text": " There wasn't really, you know, that seemed like the only kind of the only get out, or you'd need", "tokens": [51544, 821, 2067, 380, 534, 11, 291, 458, 11, 300, 6576, 411, 264, 787, 733, 295, 264, 787, 483, 484, 11, 420, 291, 1116, 643, 51740], "temperature": 0.0, "avg_logprob": -0.11778223600318964, "compression_ratio": 1.961672473867596, "no_speech_prob": 0.005056700203567743}, {"id": 469, "seek": 238568, "start": 2385.68, "end": 2390.3199999999997, "text": " some fundamentally new philosophical theory of causality that I was not qualified to produce.", "tokens": [50364, 512, 17879, 777, 25066, 5261, 295, 3302, 1860, 300, 286, 390, 406, 15904, 281, 5258, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 470, "seek": 238568, "start": 2391.12, "end": 2396.24, "text": " And so that's, again, part of the reason, part of what motivated this general", "tokens": [50636, 400, 370, 300, 311, 11, 797, 11, 644, 295, 264, 1778, 11, 644, 295, 437, 14515, 341, 2674, 50892], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 471, "seek": 238568, "start": 2396.24, "end": 2401.3599999999997, "text": " research program, which is trying to think about this category of not just a single computation,", "tokens": [50892, 2132, 1461, 11, 597, 307, 1382, 281, 519, 466, 341, 7719, 295, 406, 445, 257, 2167, 24903, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 472, "seek": 238568, "start": 2401.9199999999996, "end": 2405.44, "text": " and with a single sequence of data structures, because it's clear that you can't, you know,", "tokens": [51176, 293, 365, 257, 2167, 8310, 295, 1412, 9227, 11, 570, 309, 311, 1850, 300, 291, 393, 380, 11, 291, 458, 11, 51352], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 473, "seek": 238568, "start": 2405.44, "end": 2410.8799999999997, "text": " philosophically meaningful way assign causality in that case, but rather, you know, looking at the", "tokens": [51352, 14529, 984, 10995, 636, 6269, 3302, 1860, 294, 300, 1389, 11, 457, 2831, 11, 291, 458, 11, 1237, 412, 264, 51624], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 474, "seek": 238568, "start": 2410.8799999999997, "end": 2414.64, "text": " algebraic structure of the category of all possible computations and all possible data", "tokens": [51624, 21989, 299, 3877, 295, 264, 7719, 295, 439, 1944, 2807, 763, 293, 439, 1944, 1412, 51812], "temperature": 0.0, "avg_logprob": -0.09030085704365715, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.002712146146222949}, {"id": 475, "seek": 241464, "start": 2414.64, "end": 2418.96, "text": " structures. And in that situation, there is a notion of causality you can equip that with,", "tokens": [50364, 9227, 13, 400, 294, 300, 2590, 11, 456, 307, 257, 10710, 295, 3302, 1860, 291, 393, 5037, 300, 365, 11, 50580], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 476, "seek": 241464, "start": 2418.96, "end": 2423.52, "text": " and there's a nice, again, a nice mathematical description in terms of, in terms of weak two", "tokens": [50580, 293, 456, 311, 257, 1481, 11, 797, 11, 257, 1481, 18894, 3855, 294, 2115, 295, 11, 294, 2115, 295, 5336, 732, 50808], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 477, "seek": 241464, "start": 2423.52, "end": 2428.56, "text": " categories and so on, which again, I can talk about if people are interested. But yeah, so it's", "tokens": [50808, 10479, 293, 370, 322, 11, 597, 797, 11, 286, 393, 751, 466, 498, 561, 366, 3102, 13, 583, 1338, 11, 370, 309, 311, 51060], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 478, "seek": 241464, "start": 2428.56, "end": 2432.8799999999997, "text": " clear that these things are very deeply related that this sort of theory of the category of", "tokens": [51060, 1850, 300, 613, 721, 366, 588, 8760, 4077, 300, 341, 1333, 295, 5261, 295, 264, 7719, 295, 51276], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 479, "seek": 241464, "start": 2432.8799999999997, "end": 2436.72, "text": " computations and data structures, and the theory of how you assign causality in a meaningful way,", "tokens": [51276, 2807, 763, 293, 1412, 9227, 11, 293, 264, 5261, 295, 577, 291, 6269, 3302, 1860, 294, 257, 10995, 636, 11, 51468], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 480, "seek": 241464, "start": 2437.7599999999998, "end": 2442.96, "text": " are very deeply related. And I'll just mention one other thing on that topic, which is again,", "tokens": [51520, 366, 588, 8760, 4077, 13, 400, 286, 603, 445, 2152, 472, 661, 551, 322, 300, 4829, 11, 597, 307, 797, 11, 51780], "temperature": 0.0, "avg_logprob": -0.11376023993772619, "compression_ratio": 1.902027027027027, "no_speech_prob": 0.002212281571701169}, {"id": 481, "seek": 244296, "start": 2442.96, "end": 2448.64, "text": " just the area which I find quite exciting, because it's an unexpected spin out of this program,", "tokens": [50364, 445, 264, 1859, 597, 286, 915, 1596, 4670, 11, 570, 309, 311, 364, 13106, 6060, 484, 295, 341, 1461, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10953093937465123, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0038209808990359306}, {"id": 482, "seek": 244296, "start": 2448.64, "end": 2453.76, "text": " which is that, so once you have a way of consistently applying causality at a per token", "tokens": [50648, 597, 307, 300, 11, 370, 1564, 291, 362, 257, 636, 295, 14961, 9275, 3302, 1860, 412, 257, 680, 14862, 50904], "temperature": 0.0, "avg_logprob": -0.10953093937465123, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0038209808990359306}, {"id": 483, "seek": 244296, "start": 2453.76, "end": 2458.8, "text": " level in these systems, it gives you a way of vastly generalizing what computation is.", "tokens": [50904, 1496, 294, 613, 3652, 11, 309, 2709, 291, 257, 636, 295, 41426, 2674, 3319, 437, 24903, 307, 13, 51156], "temperature": 0.0, "avg_logprob": -0.10953093937465123, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0038209808990359306}, {"id": 484, "seek": 244296, "start": 2460.4, "end": 2463.92, "text": " And you can, in particular, you can get, you can derive something which I call,", "tokens": [51236, 400, 291, 393, 11, 294, 1729, 11, 291, 393, 483, 11, 291, 393, 28446, 746, 597, 286, 818, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10953093937465123, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0038209808990359306}, {"id": 485, "seek": 244296, "start": 2465.28, "end": 2468.8, "text": " well, which I'm provisionally calling covariant computation, although it should probably have", "tokens": [51480, 731, 11, 597, 286, 478, 17225, 379, 5141, 49851, 394, 24903, 11, 4878, 309, 820, 1391, 362, 51656], "temperature": 0.0, "avg_logprob": -0.10953093937465123, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0038209808990359306}, {"id": 486, "seek": 246880, "start": 2468.8, "end": 2476.32, "text": " a better name than that, which is, so in our traditional kind of Turing Church type models", "tokens": [50364, 257, 1101, 1315, 813, 300, 11, 597, 307, 11, 370, 294, 527, 5164, 733, 295, 314, 1345, 7882, 2010, 5245, 50740], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 487, "seek": 246880, "start": 2476.32, "end": 2481.2000000000003, "text": " of computation, computation is a purely forwards in time operation. So at every point, you know,", "tokens": [50740, 295, 24903, 11, 24903, 307, 257, 17491, 30126, 294, 565, 6916, 13, 407, 412, 633, 935, 11, 291, 458, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 488, "seek": 246880, "start": 2481.2000000000003, "end": 2485.28, "text": " you have a complete data structure, and computation is about deriving what is the next state of that", "tokens": [50984, 291, 362, 257, 3566, 1412, 3877, 11, 293, 24903, 307, 466, 1163, 2123, 437, 307, 264, 958, 1785, 295, 300, 51188], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 489, "seek": 246880, "start": 2485.28, "end": 2488.96, "text": " data structure. So in a sense, it's only a forwards in time thing. You might be able to", "tokens": [51188, 1412, 3877, 13, 407, 294, 257, 2020, 11, 309, 311, 787, 257, 30126, 294, 565, 551, 13, 509, 1062, 312, 1075, 281, 51372], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 490, "seek": 246880, "start": 2488.96, "end": 2492.0, "text": " kind of reconstruct the initial conditions based on some subsequent data structures,", "tokens": [51372, 733, 295, 31499, 264, 5883, 4487, 2361, 322, 512, 19962, 1412, 9227, 11, 51524], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 491, "seek": 246880, "start": 2492.0, "end": 2494.8, "text": " you might be able to go backwards in time, but that's essentially what you're doing.", "tokens": [51524, 291, 1062, 312, 1075, 281, 352, 12204, 294, 565, 11, 457, 300, 311, 4476, 437, 291, 434, 884, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11244368553161621, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0023216507397592068}, {"id": 492, "seek": 249480, "start": 2495.76, "end": 2502.0, "text": " But then you could imagine, okay, suppose I don't know the complete state of my data structure,", "tokens": [50412, 583, 550, 291, 727, 3811, 11, 1392, 11, 7297, 286, 500, 380, 458, 264, 3566, 1785, 295, 452, 1412, 3877, 11, 50724], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 493, "seek": 249480, "start": 2502.0, "end": 2507.36, "text": " I know instead, I know one part of my data structure, but I know it, I know its history", "tokens": [50724, 286, 458, 2602, 11, 286, 458, 472, 644, 295, 452, 1412, 3877, 11, 457, 286, 458, 309, 11, 286, 458, 1080, 2503, 50992], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 494, "seek": 249480, "start": 2507.36, "end": 2511.6800000000003, "text": " throughout all of time. So you could imagine, say an elementary cellular automaton or a Turing", "tokens": [50992, 3710, 439, 295, 565, 13, 407, 291, 727, 3811, 11, 584, 364, 16429, 29267, 3553, 25781, 420, 257, 314, 1345, 51208], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 495, "seek": 249480, "start": 2511.6800000000003, "end": 2516.2400000000002, "text": " machine tape, where you'd know nothing about the tape, but you know the state of one cell,", "tokens": [51208, 3479, 7314, 11, 689, 291, 1116, 458, 1825, 466, 264, 7314, 11, 457, 291, 458, 264, 1785, 295, 472, 2815, 11, 51436], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 496, "seek": 249480, "start": 2516.2400000000002, "end": 2520.0800000000004, "text": " and you know it, you know, throughout all of time. And then the question is, what can you infer", "tokens": [51436, 293, 291, 458, 309, 11, 291, 458, 11, 3710, 439, 295, 565, 13, 400, 550, 264, 1168, 307, 11, 437, 393, 291, 13596, 51628], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 497, "seek": 249480, "start": 2520.0800000000004, "end": 2523.28, "text": " about the rest of the computation? And it turns out that for those kind of structured array type", "tokens": [51628, 466, 264, 1472, 295, 264, 24903, 30, 400, 309, 4523, 484, 300, 337, 729, 733, 295, 18519, 10225, 2010, 51788], "temperature": 0.0, "avg_logprob": -0.08697499547685895, "compression_ratio": 1.965034965034965, "no_speech_prob": 0.001925710472278297}, {"id": 498, "seek": 252328, "start": 2523.28, "end": 2527.76, "text": " systems, you can infer a lot, you can actually evolve the system, not forwards in time, but", "tokens": [50364, 3652, 11, 291, 393, 13596, 257, 688, 11, 291, 393, 767, 16693, 264, 1185, 11, 406, 30126, 294, 565, 11, 457, 50588], "temperature": 0.0, "avg_logprob": -0.09720715379292987, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.00394260510802269}, {"id": 499, "seek": 252328, "start": 2527.76, "end": 2533.92, "text": " sideways in space, and obtain a kind of causal diamond that so okay, the top left, top right,", "tokens": [50588, 26092, 294, 1901, 11, 293, 12701, 257, 733, 295, 38755, 16059, 300, 370, 1392, 11, 264, 1192, 1411, 11, 1192, 558, 11, 50896], "temperature": 0.0, "avg_logprob": -0.09720715379292987, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.00394260510802269}, {"id": 500, "seek": 252328, "start": 2533.92, "end": 2537.92, "text": " bottom left, bottom right corners are undetermined. But everything inside that diamond can be", "tokens": [50896, 2767, 1411, 11, 2767, 558, 12413, 366, 674, 35344, 2001, 13, 583, 1203, 1854, 300, 16059, 393, 312, 51096], "temperature": 0.0, "avg_logprob": -0.09720715379292987, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.00394260510802269}, {"id": 501, "seek": 252328, "start": 2537.92, "end": 2544.32, "text": " determined just from that one row, or that one column that you know, you know, sort of extended", "tokens": [51096, 9540, 445, 490, 300, 472, 5386, 11, 420, 300, 472, 7738, 300, 291, 458, 11, 291, 458, 11, 1333, 295, 10913, 51416], "temperature": 0.0, "avg_logprob": -0.09720715379292987, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.00394260510802269}, {"id": 502, "seek": 252328, "start": 2544.32, "end": 2549.52, "text": " throughout time. And so, you know, and that's a fundamentally different notion of computation.", "tokens": [51416, 3710, 565, 13, 400, 370, 11, 291, 458, 11, 293, 300, 311, 257, 17879, 819, 10710, 295, 24903, 13, 51676], "temperature": 0.0, "avg_logprob": -0.09720715379292987, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.00394260510802269}, {"id": 503, "seek": 254952, "start": 2549.52, "end": 2553.52, "text": " So it's a version of computation, which is not forwards in time, but sideways in space.", "tokens": [50364, 407, 309, 311, 257, 3037, 295, 24903, 11, 597, 307, 406, 30126, 294, 565, 11, 457, 26092, 294, 1901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 504, "seek": 254952, "start": 2553.52, "end": 2557.52, "text": " But you can also have version of computation that is sideways and branch your space where you", "tokens": [50564, 583, 291, 393, 611, 362, 3037, 295, 24903, 300, 307, 26092, 293, 9819, 428, 1901, 689, 291, 50764], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 505, "seek": 254952, "start": 2557.52, "end": 2562.56, "text": " know, you know, one complete state, you know, you know, one branch of the multiway system extended", "tokens": [50764, 458, 11, 291, 458, 11, 472, 3566, 1785, 11, 291, 458, 11, 291, 458, 11, 472, 9819, 295, 264, 4825, 676, 1185, 10913, 51016], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 506, "seek": 254952, "start": 2562.56, "end": 2565.68, "text": " throughout time. And then the question is, what else can you infer about the, you know, but the", "tokens": [51016, 3710, 565, 13, 400, 550, 264, 1168, 307, 11, 437, 1646, 393, 291, 13596, 466, 264, 11, 291, 458, 11, 457, 264, 51172], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 507, "seek": 254952, "start": 2565.68, "end": 2569.04, "text": " rest of the multiway system just from that one branch? And again, the answer turns out to be,", "tokens": [51172, 1472, 295, 264, 4825, 676, 1185, 445, 490, 300, 472, 9819, 30, 400, 797, 11, 264, 1867, 4523, 484, 281, 312, 11, 51340], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 508, "seek": 254952, "start": 2569.04, "end": 2575.2, "text": " you can infer a lot, but not everything. And so, just like in the reason I call this covariant", "tokens": [51340, 291, 393, 13596, 257, 688, 11, 457, 406, 1203, 13, 400, 370, 11, 445, 411, 294, 264, 1778, 286, 818, 341, 49851, 394, 51648], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 509, "seek": 254952, "start": 2575.2, "end": 2579.04, "text": " computation is because it's very analogous to what happens in relativity. So in relativity,", "tokens": [51648, 24903, 307, 570, 309, 311, 588, 16660, 563, 281, 437, 2314, 294, 45675, 13, 407, 294, 45675, 11, 51840], "temperature": 0.0, "avg_logprob": -0.11386231606051048, "compression_ratio": 2.0030487804878048, "no_speech_prob": 0.0062824818305671215}, {"id": 510, "seek": 257904, "start": 2579.12, "end": 2583.2, "text": " once you buy into this notion of general covariance and the notion that space and time are kind of", "tokens": [50368, 1564, 291, 2256, 666, 341, 10710, 295, 2674, 49851, 719, 293, 264, 10710, 300, 1901, 293, 565, 366, 733, 295, 50572], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 511, "seek": 257904, "start": 2583.2, "end": 2587.7599999999998, "text": " fundamentally the same thing, then you have to somehow relax your traditional view of what", "tokens": [50572, 17879, 264, 912, 551, 11, 550, 291, 362, 281, 6063, 5789, 428, 5164, 1910, 295, 437, 50800], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 512, "seek": 257904, "start": 2587.7599999999998, "end": 2592.0, "text": " dynamical systems do, which is, you know, we typically think of systems as evolving, you know,", "tokens": [50800, 5999, 804, 3652, 360, 11, 597, 307, 11, 291, 458, 11, 321, 5850, 519, 295, 3652, 382, 21085, 11, 291, 458, 11, 51012], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 513, "seek": 257904, "start": 2592.0, "end": 2596.88, "text": " you have a snapshot of your initial of your data at, you know, localized on a, you know,", "tokens": [51012, 291, 362, 257, 30163, 295, 428, 5883, 295, 428, 1412, 412, 11, 291, 458, 11, 44574, 322, 257, 11, 291, 458, 11, 51256], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 514, "seek": 257904, "start": 2596.88, "end": 2600.48, "text": " for a particular state of space, you know, on a particular space like hypersurface.", "tokens": [51256, 337, 257, 1729, 1785, 295, 1901, 11, 291, 458, 11, 322, 257, 1729, 1901, 411, 7420, 433, 374, 2868, 13, 51436], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 515, "seek": 257904, "start": 2600.48, "end": 2603.7599999999998, "text": " And then your laws of physics tell you how that space like hypersurface evolves forwards or", "tokens": [51436, 400, 550, 428, 6064, 295, 10649, 980, 291, 577, 300, 1901, 411, 7420, 433, 374, 2868, 43737, 30126, 420, 51600], "temperature": 0.0, "avg_logprob": -0.08119132823513862, "compression_ratio": 1.967741935483871, "no_speech_prob": 0.0012830669293180108}, {"id": 516, "seek": 260376, "start": 2603.76, "end": 2611.44, "text": " backwards in time. But in a covariant picture of physics, then you must also allow for, you know,", "tokens": [50364, 12204, 294, 565, 13, 583, 294, 257, 49851, 394, 3036, 295, 10649, 11, 550, 291, 1633, 611, 2089, 337, 11, 291, 458, 11, 50748], "temperature": 0.0, "avg_logprob": -0.09537951672663454, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.003937545698136091}, {"id": 517, "seek": 260376, "start": 2611.44, "end": 2616.1600000000003, "text": " your initial data to be defined on a time like hypersurface, and you for you to be able to evolve", "tokens": [50748, 428, 5883, 1412, 281, 312, 7642, 322, 257, 565, 411, 7420, 433, 374, 2868, 11, 293, 291, 337, 291, 281, 312, 1075, 281, 16693, 50984], "temperature": 0.0, "avg_logprob": -0.09537951672663454, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.003937545698136091}, {"id": 518, "seek": 260376, "start": 2616.1600000000003, "end": 2620.5600000000004, "text": " that time like hypersurface sideways in space or mixtures of the two and so on. And so it's clear", "tokens": [50984, 300, 565, 411, 7420, 433, 374, 2868, 26092, 294, 1901, 420, 2752, 37610, 295, 264, 732, 293, 370, 322, 13, 400, 370, 309, 311, 1850, 51204], "temperature": 0.0, "avg_logprob": -0.09537951672663454, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.003937545698136091}, {"id": 519, "seek": 260376, "start": 2620.5600000000004, "end": 2625.0400000000004, "text": " that there's a very general, a vast generalization of ordinary computation theory that you can,", "tokens": [51204, 300, 456, 311, 257, 588, 2674, 11, 257, 8369, 2674, 2144, 295, 10547, 24903, 5261, 300, 291, 393, 11, 51428], "temperature": 0.0, "avg_logprob": -0.09537951672663454, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.003937545698136091}, {"id": 520, "seek": 260376, "start": 2625.0400000000004, "end": 2629.5200000000004, "text": " that you can construct that's kind of physics inspired in that sense, in which you can have", "tokens": [51428, 300, 291, 393, 7690, 300, 311, 733, 295, 10649, 7547, 294, 300, 2020, 11, 294, 597, 291, 393, 362, 51652], "temperature": 0.0, "avg_logprob": -0.09537951672663454, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.003937545698136091}, {"id": 521, "seek": 262952, "start": 2629.6, "end": 2635.04, "text": " mixing of space time and kind of multiway directions in a completely consistent way.", "tokens": [50368, 11983, 295, 1901, 565, 293, 733, 295, 4825, 676, 11095, 294, 257, 2584, 8398, 636, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 522, "seek": 262952, "start": 2635.04, "end": 2640.16, "text": " But to make those things consistent, you need to have a definite way of assigning causality. You", "tokens": [50640, 583, 281, 652, 729, 721, 8398, 11, 291, 643, 281, 362, 257, 25131, 636, 295, 49602, 3302, 1860, 13, 509, 50896], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 523, "seek": 262952, "start": 2640.16, "end": 2644.32, "text": " need to because, you know, any computation that you do, even if it permutes the directions of", "tokens": [50896, 643, 281, 570, 11, 291, 458, 11, 604, 24903, 300, 291, 360, 11, 754, 498, 309, 4784, 1819, 264, 11095, 295, 51104], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 524, "seek": 262952, "start": 2644.32, "end": 2648.72, "text": " space and time and branching space and so on, must always somehow preserve the causal structure,", "tokens": [51104, 1901, 293, 565, 293, 9819, 278, 1901, 293, 370, 322, 11, 1633, 1009, 6063, 15665, 264, 38755, 3877, 11, 51324], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 525, "seek": 262952, "start": 2648.72, "end": 2653.2, "text": " has to respect the causal structure of what's going on, or else it's inconsistent. And so", "tokens": [51324, 575, 281, 3104, 264, 38755, 3877, 295, 437, 311, 516, 322, 11, 420, 1646, 309, 311, 36891, 13, 400, 370, 51548], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 526, "seek": 262952, "start": 2653.92, "end": 2657.92, "text": " this question of how you construct a covariant theory of computation is, it turns out,", "tokens": [51584, 341, 1168, 295, 577, 291, 7690, 257, 49851, 394, 5261, 295, 24903, 307, 11, 309, 4523, 484, 11, 51784], "temperature": 0.0, "avg_logprob": -0.10146249085664749, "compression_ratio": 1.88013698630137, "no_speech_prob": 0.0059026870876550674}, {"id": 527, "seek": 265792, "start": 2657.92, "end": 2662.64, "text": " intimately related to the question of how you take this category of computations and data structures", "tokens": [50364, 560, 5401, 4077, 281, 264, 1168, 295, 577, 291, 747, 341, 7719, 295, 2807, 763, 293, 1412, 9227, 50600], "temperature": 0.0, "avg_logprob": -0.1063954293190896, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.0009846256580203772}, {"id": 528, "seek": 265792, "start": 2662.64, "end": 2666.64, "text": " and equip it with a consistent notion of causality. So very interesting question,", "tokens": [50600, 293, 5037, 309, 365, 257, 8398, 10710, 295, 3302, 1860, 13, 407, 588, 1880, 1168, 11, 50800], "temperature": 0.0, "avg_logprob": -0.1063954293190896, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.0009846256580203772}, {"id": 529, "seek": 265792, "start": 2666.64, "end": 2673.84, "text": " we could talk about that at great length. Okay, to follow with a few pieces, it's very related to", "tokens": [50800, 321, 727, 751, 466, 300, 412, 869, 4641, 13, 1033, 11, 281, 1524, 365, 257, 1326, 3755, 11, 309, 311, 588, 4077, 281, 51160], "temperature": 0.0, "avg_logprob": -0.1063954293190896, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.0009846256580203772}, {"id": 530, "seek": 265792, "start": 2673.84, "end": 2680.0, "text": " Professor Mike Levin's notion of poly computing, and about the necessity for a causality concept to", "tokens": [51160, 8419, 6602, 1456, 4796, 311, 10710, 295, 6754, 15866, 11, 293, 466, 264, 24217, 337, 257, 3302, 1860, 3410, 281, 51468], "temperature": 0.0, "avg_logprob": -0.1063954293190896, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.0009846256580203772}, {"id": 531, "seek": 265792, "start": 2680.0, "end": 2686.0, "text": " be created or deployed when the question arises, was that me? Was that action or change due to me?", "tokens": [51468, 312, 2942, 420, 17826, 562, 264, 1168, 27388, 11, 390, 300, 385, 30, 3027, 300, 3069, 420, 1319, 3462, 281, 385, 30, 51768], "temperature": 0.0, "avg_logprob": -0.1063954293190896, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.0009846256580203772}, {"id": 532, "seek": 268600, "start": 2686.96, "end": 2693.28, "text": " Also, connectivities, even just in the neuroimaging setting, which is kind of the cradle", "tokens": [50412, 2743, 11, 1745, 43539, 11, 754, 445, 294, 264, 16499, 332, 3568, 3287, 11, 597, 307, 733, 295, 264, 48081, 50728], "temperature": 0.0, "avg_logprob": -0.10186087608337402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0042640636675059795}, {"id": 533, "seek": 268600, "start": 2693.28, "end": 2696.8, "text": " from which active inference and free energy principle arise from,", "tokens": [50728, 490, 597, 4967, 38253, 293, 1737, 2281, 8665, 20288, 490, 11, 50904], "temperature": 0.0, "avg_logprob": -0.10186087608337402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0042640636675059795}, {"id": 534, "seek": 268600, "start": 2696.8, "end": 2701.92, "text": " it's really important to distinguish the functional, effective, and anatomical connectivities.", "tokens": [50904, 309, 311, 534, 1021, 281, 20206, 264, 11745, 11, 4942, 11, 293, 21618, 298, 804, 1745, 43539, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10186087608337402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0042640636675059795}, {"id": 535, "seek": 268600, "start": 2701.92, "end": 2706.96, "text": " And that was one of the points that Toby St. Clair Smith made in his dissertation,", "tokens": [51160, 400, 300, 390, 472, 295, 264, 2793, 300, 40223, 745, 13, 12947, 347, 8538, 1027, 294, 702, 39555, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10186087608337402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0042640636675059795}, {"id": 536, "seek": 268600, "start": 2706.96, "end": 2713.28, "text": " which is that a lot of times the Bayesian graphs don't convey all of the necessary and sufficient", "tokens": [51412, 597, 307, 300, 257, 688, 295, 1413, 264, 7840, 42434, 24877, 500, 380, 16965, 439, 295, 264, 4818, 293, 11563, 51728], "temperature": 0.0, "avg_logprob": -0.10186087608337402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0042640636675059795}, {"id": 537, "seek": 271328, "start": 2713.28, "end": 2718.8, "text": " information to make the reproducible computation, which is one of those kind of what's missing from", "tokens": [50364, 1589, 281, 652, 264, 11408, 32128, 24903, 11, 597, 307, 472, 295, 729, 733, 295, 437, 311, 5361, 490, 50640], "temperature": 0.0, "avg_logprob": -0.0982375892938352, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0014103050343692303}, {"id": 538, "seek": 271328, "start": 2718.8, "end": 2723.52, "text": " the graph is what motivated a lot of the category theory developments in active inference,", "tokens": [50640, 264, 4295, 307, 437, 14515, 257, 688, 295, 264, 7719, 5261, 20862, 294, 4967, 38253, 11, 50876], "temperature": 0.0, "avg_logprob": -0.0982375892938352, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0014103050343692303}, {"id": 539, "seek": 271328, "start": 2723.52, "end": 2729.76, "text": " as well as some of the formal ontological works with Sumo and Dave here and Adam Pease, because", "tokens": [50876, 382, 731, 382, 512, 295, 264, 9860, 6592, 4383, 1985, 365, 8626, 78, 293, 11017, 510, 293, 7938, 2396, 651, 11, 570, 51188], "temperature": 0.0, "avg_logprob": -0.0982375892938352, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0014103050343692303}, {"id": 540, "seek": 271328, "start": 2730.8, "end": 2735.84, "text": " implementing modal and higher order logics is really important if it's a possible situation", "tokens": [51240, 18114, 39745, 293, 2946, 1668, 3565, 1167, 307, 534, 1021, 498, 309, 311, 257, 1944, 2590, 51492], "temperature": 0.0, "avg_logprob": -0.0982375892938352, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0014103050343692303}, {"id": 541, "seek": 271328, "start": 2735.84, "end": 2739.2000000000003, "text": " where a mind can have a perspective on a mind and all these things like that.", "tokens": [51492, 689, 257, 1575, 393, 362, 257, 4585, 322, 257, 1575, 293, 439, 613, 721, 411, 300, 13, 51660], "temperature": 0.0, "avg_logprob": -0.0982375892938352, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0014103050343692303}, {"id": 542, "seek": 273920, "start": 2740.16, "end": 2748.8799999999997, "text": " Then the ant-turing tape, the tape is the pheromone, and then the decision space is the nest", "tokens": [50412, 1396, 264, 2511, 12, 83, 1345, 7314, 11, 264, 7314, 307, 264, 280, 511, 298, 546, 11, 293, 550, 264, 3537, 1901, 307, 264, 15646, 50848], "temperature": 0.0, "avg_logprob": -0.17315296291076032, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0017006341367959976}, {"id": 543, "seek": 273920, "start": 2748.8799999999997, "end": 2757.12, "text": " mate's scrolling. So when you had a deterministic turing tape, that was like a movie because the", "tokens": [50848, 11709, 311, 29053, 13, 407, 562, 291, 632, 257, 15957, 3142, 256, 1345, 7314, 11, 300, 390, 411, 257, 3169, 570, 264, 51260], "temperature": 0.0, "avg_logprob": -0.17315296291076032, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0017006341367959976}, {"id": 544, "seek": 273920, "start": 2757.12, "end": 2762.56, "text": " nest mate couldn't make any choices, except for internal action, which is kind of side topic,", "tokens": [51260, 15646, 11709, 2809, 380, 652, 604, 7994, 11, 3993, 337, 6920, 3069, 11, 597, 307, 733, 295, 1252, 4829, 11, 51532], "temperature": 0.0, "avg_logprob": -0.17315296291076032, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0017006341367959976}, {"id": 545, "seek": 273920, "start": 2762.56, "end": 2768.16, "text": " but it couldn't make any choices on the tape. Whereas when there's a multi-way,", "tokens": [51532, 457, 309, 2809, 380, 652, 604, 7994, 322, 264, 7314, 13, 13813, 562, 456, 311, 257, 4825, 12, 676, 11, 51812], "temperature": 0.0, "avg_logprob": -0.17315296291076032, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0017006341367959976}, {"id": 546, "seek": 276816, "start": 2768.16, "end": 2773.44, "text": " which is basically in active inference, what we talk about in terms of affordances and the policy", "tokens": [50364, 597, 307, 1936, 294, 4967, 38253, 11, 437, 321, 751, 466, 294, 2115, 295, 6157, 2676, 293, 264, 3897, 50628], "temperature": 0.0, "avg_logprob": -0.08254042459190439, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0029806599486619234}, {"id": 547, "seek": 276816, "start": 2773.44, "end": 2779.12, "text": " space and the temporal depth of planning and counterfactuals on action and action-conditioned", "tokens": [50628, 1901, 293, 264, 30881, 7161, 295, 5038, 293, 5682, 44919, 901, 82, 322, 3069, 293, 3069, 12, 18882, 849, 292, 50912], "temperature": 0.0, "avg_logprob": -0.08254042459190439, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0029806599486619234}, {"id": 548, "seek": 276816, "start": 2779.12, "end": 2784.0, "text": " world transition states like the B matrix, all those kinds of topics come into play,", "tokens": [50912, 1002, 6034, 4368, 411, 264, 363, 8141, 11, 439, 729, 3685, 295, 8378, 808, 666, 862, 11, 51156], "temperature": 0.0, "avg_logprob": -0.08254042459190439, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0029806599486619234}, {"id": 549, "seek": 276816, "start": 2784.0, "end": 2789.7599999999998, "text": " because if you want to have a causal buffer or grasp on what is it that something that could do", "tokens": [51156, 570, 498, 291, 528, 281, 362, 257, 38755, 21762, 420, 21743, 322, 437, 307, 309, 300, 746, 300, 727, 360, 51444], "temperature": 0.0, "avg_logprob": -0.08254042459190439, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0029806599486619234}, {"id": 550, "seek": 276816, "start": 2789.7599999999998, "end": 2795.68, "text": " otherwise does, what does it cause to do when it does or doesn't do otherwise, you need something", "tokens": [51444, 5911, 775, 11, 437, 775, 309, 3082, 281, 360, 562, 309, 775, 420, 1177, 380, 360, 5911, 11, 291, 643, 746, 51740], "temperature": 0.0, "avg_logprob": -0.08254042459190439, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0029806599486619234}, {"id": 551, "seek": 279568, "start": 2795.68, "end": 2802.64, "text": " like a deterministic handle around what could be a probabilistic or deterministic,", "tokens": [50364, 411, 257, 15957, 3142, 4813, 926, 437, 727, 312, 257, 31959, 3142, 420, 15957, 3142, 11, 50712], "temperature": 0.0, "avg_logprob": -0.08837426336188066, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0028891556430608034}, {"id": 552, "seek": 279568, "start": 2802.64, "end": 2807.2799999999997, "text": " but at least multi-way map of some kind of cognitive territory.", "tokens": [50712, 457, 412, 1935, 4825, 12, 676, 4471, 295, 512, 733, 295, 15605, 11360, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08837426336188066, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0028891556430608034}, {"id": 553, "seek": 279568, "start": 2811.7599999999998, "end": 2818.3199999999997, "text": " That's a very, very interesting perspective. Again, I'm betraying my ignorance of active", "tokens": [51168, 663, 311, 257, 588, 11, 588, 1880, 4585, 13, 3764, 11, 286, 478, 15560, 278, 452, 25390, 295, 4967, 51496], "temperature": 0.0, "avg_logprob": -0.08837426336188066, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0028891556430608034}, {"id": 554, "seek": 281832, "start": 2818.32, "end": 2826.7200000000003, "text": " inference theory here, but it sounds almost like, when you have this kind of interplay between", "tokens": [50364, 38253, 5261, 510, 11, 457, 309, 3263, 1920, 411, 11, 562, 291, 362, 341, 733, 295, 728, 2858, 1296, 50784], "temperature": 0.0, "avg_logprob": -0.2102345343559019, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.018789948895573616}, {"id": 555, "seek": 281832, "start": 2828.0800000000004, "end": 2837.28, "text": " sort of epistemic versus pragmatic, there's two aspects of how this kind of speculative", "tokens": [50852, 1333, 295, 2388, 468, 3438, 5717, 46904, 11, 456, 311, 732, 7270, 295, 577, 341, 733, 295, 49415, 51312], "temperature": 0.0, "avg_logprob": -0.2102345343559019, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.018789948895573616}, {"id": 556, "seek": 281832, "start": 2837.28, "end": 2845.6800000000003, "text": " part of cognition works. This is something which I thought about in a completely different context", "tokens": [51312, 644, 295, 46905, 1985, 13, 639, 307, 746, 597, 286, 1194, 466, 294, 257, 2584, 819, 4319, 51732], "temperature": 0.0, "avg_logprob": -0.2102345343559019, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.018789948895573616}, {"id": 557, "seek": 284568, "start": 2845.68, "end": 2850.24, "text": " in relation to things like quantum information theory, but I wonder if there's a potential", "tokens": [50364, 294, 9721, 281, 721, 411, 13018, 1589, 5261, 11, 457, 286, 2441, 498, 456, 311, 257, 3995, 50592], "temperature": 0.0, "avg_logprob": -0.10006943668227598, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.006898168474435806}, {"id": 558, "seek": 284568, "start": 2850.24, "end": 2857.6, "text": " overlap there. There are certain situations when thinking about these kinds of systems,", "tokens": [50592, 19959, 456, 13, 821, 366, 1629, 6851, 562, 1953, 466, 613, 3685, 295, 3652, 11, 50960], "temperature": 0.0, "avg_logprob": -0.10006943668227598, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.006898168474435806}, {"id": 559, "seek": 284568, "start": 2858.3199999999997, "end": 2863.6, "text": " purely abstractly, where you kind of need two different notions of causality. You need a kind", "tokens": [50996, 17491, 12649, 356, 11, 689, 291, 733, 295, 643, 732, 819, 35799, 295, 3302, 1860, 13, 509, 643, 257, 733, 51260], "temperature": 0.0, "avg_logprob": -0.10006943668227598, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.006898168474435806}, {"id": 560, "seek": 284568, "start": 2863.6, "end": 2870.48, "text": " of speculative notion that's dynamic, that can be rewritten, and then you need a kind of definite", "tokens": [51260, 295, 49415, 10710, 300, 311, 8546, 11, 300, 393, 312, 319, 26859, 11, 293, 550, 291, 643, 257, 733, 295, 25131, 51604], "temperature": 0.0, "avg_logprob": -0.10006943668227598, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.006898168474435806}, {"id": 561, "seek": 287048, "start": 2870.48, "end": 2876.56, "text": " notion that's immutable. A classic example of this is for something like quantum information", "tokens": [50364, 10710, 300, 311, 3397, 32148, 13, 316, 7230, 1365, 295, 341, 307, 337, 746, 411, 13018, 1589, 50668], "temperature": 0.0, "avg_logprob": -0.11864295372596154, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0182347409427166}, {"id": 562, "seek": 287048, "start": 2876.56, "end": 2881.28, "text": " theory. You can have superpositions of causal orders, you can have quantum switches, you can have", "tokens": [50668, 5261, 13, 509, 393, 362, 1687, 30010, 2451, 295, 38755, 9470, 11, 291, 393, 362, 13018, 19458, 11, 291, 393, 362, 50904], "temperature": 0.0, "avg_logprob": -0.11864295372596154, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0182347409427166}, {"id": 563, "seek": 287048, "start": 2883.12, "end": 2888.32, "text": " causal structure that exists in superpositions of different kind of directed graph states,", "tokens": [50996, 38755, 3877, 300, 8198, 294, 1687, 30010, 2451, 295, 819, 733, 295, 12898, 4295, 4368, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11864295372596154, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0182347409427166}, {"id": 564, "seek": 287048, "start": 2888.32, "end": 2892.4, "text": " but then once you apply Hermitian operator, once you apply a measurement, the causal structure is", "tokens": [51256, 457, 550, 1564, 291, 3079, 21842, 270, 952, 12973, 11, 1564, 291, 3079, 257, 13160, 11, 264, 38755, 3877, 307, 51460], "temperature": 0.0, "avg_logprob": -0.11864295372596154, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0182347409427166}, {"id": 565, "seek": 287048, "start": 2892.4, "end": 2896.48, "text": " definite because then everything is relativistic and you have covariance. You have similar things,", "tokens": [51460, 25131, 570, 550, 1203, 307, 21960, 3142, 293, 291, 362, 49851, 719, 13, 509, 362, 2531, 721, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11864295372596154, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0182347409427166}, {"id": 566, "seek": 289648, "start": 2896.56, "end": 2903.76, "text": " as I understand, with distributed computing, with parallel computing, where you potentially allow", "tokens": [50368, 382, 286, 1223, 11, 365, 12631, 15866, 11, 365, 8952, 15866, 11, 689, 291, 7263, 2089, 50728], "temperature": 0.0, "avg_logprob": -0.1289201904745663, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.006090210750699043}, {"id": 567, "seek": 289648, "start": 2903.76, "end": 2907.52, "text": " for speculative execution for a certain number of steps where you're kind of treeing out this", "tokens": [50728, 337, 49415, 15058, 337, 257, 1629, 1230, 295, 4439, 689, 291, 434, 733, 295, 2192, 14667, 484, 341, 50916], "temperature": 0.0, "avg_logprob": -0.1289201904745663, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.006090210750699043}, {"id": 568, "seek": 289648, "start": 2907.52, "end": 2912.64, "text": " multi-way system and you have a superposition or at least a collection of possible causal histories,", "tokens": [50916, 4825, 12, 676, 1185, 293, 291, 362, 257, 1687, 38078, 420, 412, 1935, 257, 5765, 295, 1944, 38755, 30631, 11, 51172], "temperature": 0.0, "avg_logprob": -0.1289201904745663, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.006090210750699043}, {"id": 569, "seek": 289648, "start": 2912.64, "end": 2916.88, "text": " but then eventually you have to choose an actual operation to do and then the causal history,", "tokens": [51172, 457, 550, 4728, 291, 362, 281, 2826, 364, 3539, 6916, 281, 360, 293, 550, 264, 38755, 2503, 11, 51384], "temperature": 0.0, "avg_logprob": -0.1289201904745663, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.006090210750699043}, {"id": 570, "seek": 289648, "start": 2916.88, "end": 2922.2400000000002, "text": " you have this big block that gets laid down and then the causal history is somehow definite.", "tokens": [51384, 291, 362, 341, 955, 3461, 300, 2170, 9897, 760, 293, 550, 264, 38755, 2503, 307, 6063, 25131, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1289201904745663, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.006090210750699043}, {"id": 571, "seek": 292224, "start": 2922.4799999999996, "end": 2932.4799999999996, "text": " I wonder if there's a way of thinking about speculative execution of agents", "tokens": [50376, 286, 2441, 498, 456, 311, 257, 636, 295, 1953, 466, 49415, 15058, 295, 12554, 50876], "temperature": 0.0, "avg_logprob": -0.12464243249048161, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0006665790569968522}, {"id": 572, "seek": 292224, "start": 2932.4799999999996, "end": 2937.6, "text": " and the interrelation between that speculative execution and agent actions in terms of, again,", "tokens": [50876, 293, 264, 728, 4419, 399, 1296, 300, 49415, 15058, 293, 9461, 5909, 294, 2115, 295, 11, 797, 11, 51132], "temperature": 0.0, "avg_logprob": -0.12464243249048161, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0006665790569968522}, {"id": 573, "seek": 292224, "start": 2937.6, "end": 2941.6, "text": " this interplay between two different causal structures, between a dynamic one versus an", "tokens": [51132, 341, 728, 2858, 1296, 732, 819, 38755, 9227, 11, 1296, 257, 8546, 472, 5717, 364, 51332], "temperature": 0.0, "avg_logprob": -0.12464243249048161, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0006665790569968522}, {"id": 574, "seek": 292224, "start": 2941.6, "end": 2948.7999999999997, "text": " immutable one. Yeah. Well, one funny way to think about that is a single agent that has this", "tokens": [51332, 3397, 32148, 472, 13, 865, 13, 1042, 11, 472, 4074, 636, 281, 519, 466, 300, 307, 257, 2167, 9461, 300, 575, 341, 51692], "temperature": 0.0, "avg_logprob": -0.12464243249048161, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0006665790569968522}, {"id": 575, "seek": 294880, "start": 2948.8, "end": 2955.76, "text": " counterfactual contemplative ability could be in the center place foraging arena and then imagining", "tokens": [50364, 5682, 44919, 901, 19935, 1166, 3485, 727, 312, 294, 264, 3056, 1081, 337, 3568, 18451, 293, 550, 27798, 50712], "temperature": 0.0, "avg_logprob": -0.09910071768411775, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.01205193717032671}, {"id": 576, "seek": 294880, "start": 2955.76, "end": 2962.0, "text": " with discrete branching paths like a chess algorithm or like a probability distribution", "tokens": [50712, 365, 27706, 9819, 278, 14518, 411, 257, 24122, 9284, 420, 411, 257, 8482, 7316, 51024], "temperature": 0.0, "avg_logprob": -0.09910071768411775, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.01205193717032671}, {"id": 577, "seek": 294880, "start": 2962.0, "end": 2967.2000000000003, "text": " could be like imagining where it could go, but not all cognitive things or the kind of things that", "tokens": [51024, 727, 312, 411, 27798, 689, 309, 727, 352, 11, 457, 406, 439, 15605, 721, 420, 264, 733, 295, 721, 300, 51284], "temperature": 0.0, "avg_logprob": -0.09910071768411775, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.01205193717032671}, {"id": 578, "seek": 294880, "start": 2967.2000000000003, "end": 2974.2400000000002, "text": " make plans of their own actions, whereas like an ant colony has nest mates on the ground. So", "tokens": [51284, 652, 5482, 295, 641, 1065, 5909, 11, 9735, 411, 364, 2511, 23028, 575, 15646, 31488, 322, 264, 2727, 13, 407, 51636], "temperature": 0.0, "avg_logprob": -0.09910071768411775, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.01205193717032671}, {"id": 579, "seek": 297424, "start": 2974.24, "end": 2980.56, "text": " they're actually realizing in these finite trajectories, the real exoskeleton on the ground", "tokens": [50364, 436, 434, 767, 16734, 294, 613, 19362, 18257, 2083, 11, 264, 957, 454, 329, 330, 14806, 322, 264, 2727, 50680], "temperature": 0.0, "avg_logprob": -0.06632850310381722, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017006266862154007}, {"id": 580, "seek": 297424, "start": 2981.4399999999996, "end": 2988.3999999999996, "text": " that plays out, ending up with those simulated trajectories could have been simulated or could", "tokens": [50724, 300, 5749, 484, 11, 8121, 493, 365, 729, 41713, 18257, 2083, 727, 362, 668, 41713, 420, 727, 51072], "temperature": 0.0, "avg_logprob": -0.06632850310381722, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017006266862154007}, {"id": 581, "seek": 297424, "start": 2988.3999999999996, "end": 2992.9599999999996, "text": " have been probabilistically blurred, but that's kind of the difference between like the embodiment", "tokens": [51072, 362, 668, 31959, 20458, 43525, 11, 457, 300, 311, 733, 295, 264, 2649, 1296, 411, 264, 28935, 2328, 51300], "temperature": 0.0, "avg_logprob": -0.06632850310381722, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017006266862154007}, {"id": 582, "seek": 297424, "start": 2992.9599999999996, "end": 2998.4799999999996, "text": " and like the body moving there for a mammal or for an animal and then like the mind simulating it", "tokens": [51300, 293, 411, 264, 1772, 2684, 456, 337, 257, 49312, 420, 337, 364, 5496, 293, 550, 411, 264, 1575, 1034, 12162, 309, 51576], "temperature": 0.0, "avg_logprob": -0.06632850310381722, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017006266862154007}, {"id": 583, "seek": 299848, "start": 2998.56, "end": 3006.48, "text": " and then just to the epistemic and pragmatic tradeoff in decision making. So let's just say that", "tokens": [50368, 293, 550, 445, 281, 264, 2388, 468, 3438, 293, 46904, 4923, 4506, 294, 3537, 1455, 13, 407, 718, 311, 445, 584, 300, 50764], "temperature": 0.0, "avg_logprob": -0.1007983386516571, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017006638227030635}, {"id": 584, "seek": 299848, "start": 3006.48, "end": 3013.84, "text": " we're in that multi-way moment. We'll just have two options, two different slices of the B variable", "tokens": [50764, 321, 434, 294, 300, 4825, 12, 676, 1623, 13, 492, 603, 445, 362, 732, 3956, 11, 732, 819, 19793, 295, 264, 363, 7006, 51132], "temperature": 0.0, "avg_logprob": -0.1007983386516571, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017006638227030635}, {"id": 585, "seek": 299848, "start": 3013.84, "end": 3019.04, "text": " and the policy selection question is about which way are you going to go? Which affordance in the", "tokens": [51132, 293, 264, 3897, 9450, 1168, 307, 466, 597, 636, 366, 291, 516, 281, 352, 30, 3013, 6157, 719, 294, 264, 51392], "temperature": 0.0, "avg_logprob": -0.1007983386516571, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017006638227030635}, {"id": 586, "seek": 299848, "start": 3019.04, "end": 3024.8, "text": " moment? Policy is basically the affordances for the time horizon of planning, but if it's only one", "tokens": [51392, 1623, 30, 21708, 307, 1936, 264, 6157, 2676, 337, 264, 565, 18046, 295, 5038, 11, 457, 498, 309, 311, 787, 472, 51680], "temperature": 0.0, "avg_logprob": -0.1007983386516571, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017006638227030635}, {"id": 587, "seek": 302480, "start": 3024.8, "end": 3029.04, "text": " time step or just the next one, then the affordance space is just the actions that can be taken.", "tokens": [50364, 565, 1823, 420, 445, 264, 958, 472, 11, 550, 264, 6157, 719, 1901, 307, 445, 264, 5909, 300, 393, 312, 2726, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08004097544818843, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.004069914575666189}, {"id": 588, "seek": 302480, "start": 3030.96, "end": 3036.48, "text": " One way to make it so that what happens is the likeliest thing, path of least action, which is", "tokens": [50672, 1485, 636, 281, 652, 309, 370, 300, 437, 2314, 307, 264, 411, 16850, 551, 11, 3100, 295, 1935, 3069, 11, 597, 307, 50948], "temperature": 0.0, "avg_logprob": -0.08004097544818843, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.004069914575666189}, {"id": 589, "seek": 302480, "start": 3036.48, "end": 3041.36, "text": " kind of what opens up the whole physics of cognitive systems angle in contrast to like a", "tokens": [50948, 733, 295, 437, 9870, 493, 264, 1379, 10649, 295, 15605, 3652, 5802, 294, 8712, 281, 411, 257, 51192], "temperature": 0.0, "avg_logprob": -0.08004097544818843, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.004069914575666189}, {"id": 590, "seek": 302480, "start": 3041.36, "end": 3047.1200000000003, "text": " reinforcement or reward learning perspective, what makes it the likeliest thing is starting with", "tokens": [51192, 29280, 420, 7782, 2539, 4585, 11, 437, 1669, 309, 264, 411, 16850, 551, 307, 2891, 365, 51480], "temperature": 0.0, "avg_logprob": -0.08004097544818843, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.004069914575666189}, {"id": 591, "seek": 302480, "start": 3047.1200000000003, "end": 3053.52, "text": " habit. So it could just be drawn from a fixed habitual distribution. However, for adaptive action,", "tokens": [51480, 7164, 13, 407, 309, 727, 445, 312, 10117, 490, 257, 6806, 46883, 7316, 13, 2908, 11, 337, 27912, 3069, 11, 51800], "temperature": 0.0, "avg_logprob": -0.08004097544818843, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.004069914575666189}, {"id": 592, "seek": 305352, "start": 3054.08, "end": 3059.12, "text": " habit gets up-weighted with expected free energy, which is a functional that takes in", "tokens": [50392, 7164, 2170, 493, 12, 12329, 292, 365, 5176, 1737, 2281, 11, 597, 307, 257, 11745, 300, 2516, 294, 50644], "temperature": 0.0, "avg_logprob": -0.10032776103300207, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0019265906885266304}, {"id": 593, "seek": 305352, "start": 3059.92, "end": 3064.96, "text": " the policy space, which is summing up to one because there's a probability over actions,", "tokens": [50684, 264, 3897, 1901, 11, 597, 307, 2408, 2810, 493, 281, 472, 570, 456, 311, 257, 8482, 670, 5909, 11, 50936], "temperature": 0.0, "avg_logprob": -0.10032776103300207, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0019265906885266304}, {"id": 594, "seek": 305352, "start": 3064.96, "end": 3072.24, "text": " and then up-weighting policies according to their score on expected free energy, which is", "tokens": [50936, 293, 550, 493, 12, 12329, 278, 7657, 4650, 281, 641, 6175, 322, 5176, 1737, 2281, 11, 597, 307, 51300], "temperature": 0.0, "avg_logprob": -0.10032776103300207, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0019265906885266304}, {"id": 595, "seek": 305352, "start": 3072.8, "end": 3078.4, "text": " consisting of epistemic plus pragmatic value. So how much is it going to align the observations to", "tokens": [51328, 33921, 295, 2388, 468, 3438, 1804, 46904, 2158, 13, 407, 577, 709, 307, 309, 516, 281, 7975, 264, 18163, 281, 51608], "temperature": 0.0, "avg_logprob": -0.10032776103300207, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0019265906885266304}, {"id": 596, "seek": 307840, "start": 3078.4, "end": 3084.48, "text": " be what I like to see? That's pragmatic value with a preference. What is my expected information", "tokens": [50364, 312, 437, 286, 411, 281, 536, 30, 663, 311, 46904, 2158, 365, 257, 17502, 13, 708, 307, 452, 5176, 1589, 50668], "temperature": 0.0, "avg_logprob": -0.08562620206810963, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0031234791968017817}, {"id": 597, "seek": 307840, "start": 3084.48, "end": 3091.6800000000003, "text": " gain? That's the epistemic value. So how those are parameterized make the agent that always seeks", "tokens": [50668, 6052, 30, 663, 311, 264, 2388, 468, 3438, 2158, 13, 407, 577, 729, 366, 13075, 1602, 652, 264, 9461, 300, 1009, 28840, 51028], "temperature": 0.0, "avg_logprob": -0.08562620206810963, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0031234791968017817}, {"id": 598, "seek": 307840, "start": 3091.6800000000003, "end": 3097.52, "text": " out new information or always goes with habit or there's so much policy space because the knobs", "tokens": [51028, 484, 777, 1589, 420, 1009, 1709, 365, 7164, 420, 456, 311, 370, 709, 3897, 1901, 570, 264, 46999, 51320], "temperature": 0.0, "avg_logprob": -0.08562620206810963, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0031234791968017817}, {"id": 599, "seek": 307840, "start": 3098.2400000000002, "end": 3104.56, "text": " are not just simple sliders or there are multiple knobs, even though they are seemingly", "tokens": [51356, 366, 406, 445, 2199, 1061, 6936, 420, 456, 366, 3866, 46999, 11, 754, 1673, 436, 366, 18709, 51672], "temperature": 0.0, "avg_logprob": -0.08562620206810963, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0031234791968017817}, {"id": 600, "seek": 310456, "start": 3105.36, "end": 3113.12, "text": " quite conciliant and minimal, like it's hard to imagine less, yet especially when there's", "tokens": [50404, 1596, 1588, 2312, 394, 293, 13206, 11, 411, 309, 311, 1152, 281, 3811, 1570, 11, 1939, 2318, 562, 456, 311, 50792], "temperature": 0.0, "avg_logprob": -0.16459356800893718, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.006386617664247751}, {"id": 601, "seek": 310456, "start": 3113.12, "end": 3119.12, "text": " richness in the environment, even simple systems can have like enormously complex or", "tokens": [50792, 44506, 294, 264, 2823, 11, 754, 2199, 3652, 393, 362, 411, 39669, 3997, 420, 51092], "temperature": 0.0, "avg_logprob": -0.16459356800893718, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.006386617664247751}, {"id": 602, "seek": 310456, "start": 3119.12, "end": 3126.7999999999997, "text": " adaptive behaviors. I'll just leave it there. No, I never really thought of the, so okay,", "tokens": [51092, 27912, 15501, 13, 286, 603, 445, 1856, 309, 456, 13, 883, 11, 286, 1128, 534, 1194, 295, 264, 11, 370, 1392, 11, 51476], "temperature": 0.0, "avg_logprob": -0.16459356800893718, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.006386617664247751}, {"id": 603, "seek": 310456, "start": 3127.84, "end": 3133.2799999999997, "text": " yeah, two things, right? So first of all, the perspective of, you know, thinking of an ant", "tokens": [51528, 1338, 11, 732, 721, 11, 558, 30, 407, 700, 295, 439, 11, 264, 4585, 295, 11, 291, 458, 11, 1953, 295, 364, 2511, 51800], "temperature": 0.0, "avg_logprob": -0.16459356800893718, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.006386617664247751}, {"id": 604, "seek": 313328, "start": 3133.28, "end": 3137.6000000000004, "text": " colony or a termite colony or something as being akin to a mind, that's, you know,", "tokens": [50364, 23028, 420, 257, 1433, 642, 23028, 420, 746, 382, 885, 47540, 281, 257, 1575, 11, 300, 311, 11, 291, 458, 11, 50580], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 605, "seek": 313328, "start": 3138.6400000000003, "end": 3143.76, "text": " I was familiar with that perspective from people like Dan Dennett and so on. But the idea that the", "tokens": [50632, 286, 390, 4963, 365, 300, 4585, 490, 561, 411, 3394, 19027, 3093, 293, 370, 322, 13, 583, 264, 1558, 300, 264, 50888], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 606, "seek": 313328, "start": 3143.76, "end": 3149.6800000000003, "text": " individual ants in that colony are in a sense enact, they're kind of the hardware enacting the", "tokens": [50888, 2609, 23355, 294, 300, 23028, 366, 294, 257, 2020, 25909, 11, 436, 434, 733, 295, 264, 8837, 25909, 278, 264, 51184], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 607, "seek": 313328, "start": 3149.6800000000003, "end": 3155.1200000000003, "text": " speculative execution, that's a very interesting idea. It's not speculative for them. Well,", "tokens": [51184, 49415, 15058, 11, 300, 311, 257, 588, 1880, 1558, 13, 467, 311, 406, 49415, 337, 552, 13, 1042, 11, 51456], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 608, "seek": 313328, "start": 3155.1200000000003, "end": 3158.96, "text": " yeah, no, exactly. But it's sort of from the mind's perspective, I guess it's almost like", "tokens": [51456, 1338, 11, 572, 11, 2293, 13, 583, 309, 311, 1333, 295, 490, 264, 1575, 311, 4585, 11, 286, 2041, 309, 311, 1920, 411, 51648], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 609, "seek": 313328, "start": 3158.96, "end": 3162.88, "text": " speculative execution, but it's speculative execution that's being actuated in the physical", "tokens": [51648, 49415, 15058, 11, 457, 309, 311, 49415, 15058, 300, 311, 885, 605, 27275, 294, 264, 4001, 51844], "temperature": 0.0, "avg_logprob": -0.12213433678470441, "compression_ratio": 1.8835616438356164, "no_speech_prob": 0.004899680148810148}, {"id": 610, "seek": 316288, "start": 3162.88, "end": 3169.76, "text": " world, which is very interesting. I not really thought about that before. But then, yeah, okay,", "tokens": [50364, 1002, 11, 597, 307, 588, 1880, 13, 286, 406, 534, 1194, 466, 300, 949, 13, 583, 550, 11, 1338, 11, 1392, 11, 50708], "temperature": 0.0, "avg_logprob": -0.14652363459269205, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0025428999215364456}, {"id": 611, "seek": 316288, "start": 3169.76, "end": 3175.6800000000003, "text": " so then the point you're making about connection to free energy and sort of habit formation and so", "tokens": [50708, 370, 550, 264, 935, 291, 434, 1455, 466, 4984, 281, 1737, 2281, 293, 1333, 295, 7164, 11723, 293, 370, 51004], "temperature": 0.0, "avg_logprob": -0.14652363459269205, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0025428999215364456}, {"id": 612, "seek": 316288, "start": 3175.6800000000003, "end": 3185.52, "text": " on, okay, so I wonder if, you know, if we're thinking about a model of cognition in which", "tokens": [51004, 322, 11, 1392, 11, 370, 286, 2441, 498, 11, 291, 458, 11, 498, 321, 434, 1953, 466, 257, 2316, 295, 46905, 294, 597, 51496], "temperature": 0.0, "avg_logprob": -0.14652363459269205, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0025428999215364456}, {"id": 613, "seek": 316288, "start": 3185.52, "end": 3190.96, "text": " there are these two distinct causality notions, the immutable versus the dynamic one,", "tokens": [51496, 456, 366, 613, 732, 10644, 3302, 1860, 35799, 11, 264, 3397, 32148, 5717, 264, 8546, 472, 11, 51768], "temperature": 0.0, "avg_logprob": -0.14652363459269205, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0025428999215364456}, {"id": 614, "seek": 319288, "start": 3193.2000000000003, "end": 3199.52, "text": " I wonder if the, so you gave a very, very nice account of how habit formation", "tokens": [50380, 286, 2441, 498, 264, 11, 370, 291, 2729, 257, 588, 11, 588, 1481, 2696, 295, 577, 7164, 11723, 50696], "temperature": 0.0, "avg_logprob": -0.1330788820639424, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0004040211206302047}, {"id": 615, "seek": 319288, "start": 3200.08, "end": 3206.2400000000002, "text": " sort of works in these kinds of formalisms based on, you know, prior experience of expected", "tokens": [50724, 1333, 295, 1985, 294, 613, 3685, 295, 9860, 13539, 2361, 322, 11, 291, 458, 11, 4059, 1752, 295, 5176, 51032], "temperature": 0.0, "avg_logprob": -0.1330788820639424, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0004040211206302047}, {"id": 616, "seek": 319288, "start": 3206.2400000000002, "end": 3213.76, "text": " free energy. So I wonder if there's a way of describing that abstractly in terms of something", "tokens": [51032, 1737, 2281, 13, 407, 286, 2441, 498, 456, 311, 257, 636, 295, 16141, 300, 12649, 356, 294, 2115, 295, 746, 51408], "temperature": 0.0, "avg_logprob": -0.1330788820639424, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0004040211206302047}, {"id": 617, "seek": 319288, "start": 3213.76, "end": 3219.04, "text": " like, you know, you perform the speculative execution step where you're, you know, you're", "tokens": [51408, 411, 11, 291, 458, 11, 291, 2042, 264, 49415, 15058, 1823, 689, 291, 434, 11, 291, 458, 11, 291, 434, 51672], "temperature": 0.0, "avg_logprob": -0.1330788820639424, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0004040211206302047}, {"id": 618, "seek": 321904, "start": 3219.04, "end": 3224.08, "text": " treeing out several multiway possibilities. And initially, you kind of, you know, you know nothing", "tokens": [50364, 4230, 278, 484, 2940, 4825, 676, 12178, 13, 400, 9105, 11, 291, 733, 295, 11, 291, 458, 11, 291, 458, 1825, 50616], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 619, "seek": 321904, "start": 3224.08, "end": 3227.68, "text": " or you have no habits, you're just kind of, you're treeing everything out with kind of equal waiting.", "tokens": [50616, 420, 291, 362, 572, 14100, 11, 291, 434, 445, 733, 295, 11, 291, 434, 4230, 278, 1203, 484, 365, 733, 295, 2681, 3806, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 620, "seek": 321904, "start": 3228.24, "end": 3233.2799999999997, "text": " But then, you know, for each possible path, you're calculating either an actual or an expected", "tokens": [50824, 583, 550, 11, 291, 458, 11, 337, 1184, 1944, 3100, 11, 291, 434, 28258, 2139, 364, 3539, 420, 364, 5176, 51076], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 621, "seek": 321904, "start": 3233.2799999999997, "end": 3239.2, "text": " free energy. And then somehow, you know, in future speculative executions, you wait those", "tokens": [51076, 1737, 2281, 13, 400, 550, 6063, 11, 291, 458, 11, 294, 2027, 49415, 4454, 3666, 11, 291, 1699, 729, 51372], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 622, "seek": 321904, "start": 3239.2, "end": 3243.92, "text": " paths which you previously had found to have higher free energy as higher. And so, you know,", "tokens": [51372, 14518, 597, 291, 8046, 632, 1352, 281, 362, 2946, 1737, 2281, 382, 2946, 13, 400, 370, 11, 291, 458, 11, 51608], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 623, "seek": 321904, "start": 3243.92, "end": 3248.24, "text": " you're more likely to explore those and less likely explore ones which are, you know, which", "tokens": [51608, 291, 434, 544, 3700, 281, 6839, 729, 293, 1570, 3700, 6839, 2306, 597, 366, 11, 291, 458, 11, 597, 51824], "temperature": 0.0, "avg_logprob": -0.13042455520072993, "compression_ratio": 2.0, "no_speech_prob": 0.006282880902290344}, {"id": 624, "seek": 324824, "start": 3248.24, "end": 3254.08, "text": " have that lower expected value. It feels like something, something like that should fit very,", "tokens": [50364, 362, 300, 3126, 5176, 2158, 13, 467, 3417, 411, 746, 11, 746, 411, 300, 820, 3318, 588, 11, 50656], "temperature": 0.0, "avg_logprob": -0.13454733223750673, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.001284029334783554}, {"id": 625, "seek": 324824, "start": 3254.08, "end": 3258.7999999999997, "text": " very nicely into an algebraic semantics like this, which would be interesting.", "tokens": [50656, 588, 9594, 666, 364, 21989, 299, 4361, 45298, 411, 341, 11, 597, 576, 312, 1880, 13, 50892], "temperature": 0.0, "avg_logprob": -0.13454733223750673, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.001284029334783554}, {"id": 626, "seek": 324824, "start": 3258.7999999999997, "end": 3264.24, "text": " Oh, how about more questions from the chat? Okay, upcycle club writes,", "tokens": [50892, 876, 11, 577, 466, 544, 1651, 490, 264, 5081, 30, 1033, 11, 493, 14796, 6482, 13657, 11, 51164], "temperature": 0.0, "avg_logprob": -0.13454733223750673, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.001284029334783554}, {"id": 627, "seek": 324824, "start": 3265.52, "end": 3270.08, "text": " acknowledging the limitations of traditional entropy in multi computations", "tokens": [51228, 30904, 264, 15705, 295, 5164, 30867, 294, 4825, 2807, 763, 51456], "temperature": 0.0, "avg_logprob": -0.13454733223750673, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.001284029334783554}, {"id": 628, "seek": 324824, "start": 3270.72, "end": 3276.4799999999996, "text": " motivates us to develop context specific entropy metrics. Can you share some insights", "tokens": [51488, 42569, 505, 281, 1499, 4319, 2685, 30867, 16367, 13, 1664, 291, 2073, 512, 14310, 51776], "temperature": 0.0, "avg_logprob": -0.13454733223750673, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.001284029334783554}, {"id": 629, "seek": 327648, "start": 3276.48, "end": 3283.12, "text": " towards such efforts? Yeah, I can certainly try. So, so yes, I mean,", "tokens": [50364, 3030, 1270, 6484, 30, 865, 11, 286, 393, 3297, 853, 13, 407, 11, 370, 2086, 11, 286, 914, 11, 50696], "temperature": 0.0, "avg_logprob": -0.14606406325000829, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.003942705690860748}, {"id": 630, "seek": 327648, "start": 3284.56, "end": 3289.84, "text": " the first point is that, you know, it's, I think it was, there's that famous conversation between", "tokens": [50768, 264, 700, 935, 307, 300, 11, 291, 458, 11, 309, 311, 11, 286, 519, 309, 390, 11, 456, 311, 300, 4618, 3761, 1296, 51032], "temperature": 0.0, "avg_logprob": -0.14606406325000829, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.003942705690860748}, {"id": 631, "seek": 327648, "start": 3289.84, "end": 3293.68, "text": " John von Neumann and Claude Shannon, where I think von Neumann famously said that like,", "tokens": [51032, 2619, 2957, 1734, 449, 969, 293, 12947, 2303, 28974, 11, 689, 286, 519, 2957, 1734, 449, 969, 34360, 848, 300, 411, 11, 51224], "temperature": 0.0, "avg_logprob": -0.14606406325000829, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.003942705690860748}, {"id": 632, "seek": 327648, "start": 3293.68, "end": 3298.56, "text": " Shannon should call his measure entropy because no one knows what it means, right? And I submit that", "tokens": [51224, 28974, 820, 818, 702, 3481, 30867, 570, 572, 472, 3255, 437, 309, 1355, 11, 558, 30, 400, 286, 10315, 300, 51468], "temperature": 0.0, "avg_logprob": -0.14606406325000829, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.003942705690860748}, {"id": 633, "seek": 327648, "start": 3298.56, "end": 3303.2, "text": " the reason that no one knows what entropy means is because it's dependent. I mean, okay, one of", "tokens": [51468, 264, 1778, 300, 572, 472, 3255, 437, 30867, 1355, 307, 570, 309, 311, 12334, 13, 286, 914, 11, 1392, 11, 472, 295, 51700], "temperature": 0.0, "avg_logprob": -0.14606406325000829, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.003942705690860748}, {"id": 634, "seek": 330320, "start": 3303.2, "end": 3307.12, "text": " the reasons no one knows what entropy means is because it's dependent on exactly what we've", "tokens": [50364, 264, 4112, 572, 472, 3255, 437, 30867, 1355, 307, 570, 309, 311, 12334, 322, 2293, 437, 321, 600, 50560], "temperature": 0.0, "avg_logprob": -0.10801196893056234, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.002979477634653449}, {"id": 635, "seek": 330320, "start": 3307.12, "end": 3313.2, "text": " been talking about is dependent on the equivalence function of the observer. So it's one of these", "tokens": [50560, 668, 1417, 466, 307, 12334, 322, 264, 9052, 655, 2445, 295, 264, 27878, 13, 407, 309, 311, 472, 295, 613, 50864], "temperature": 0.0, "avg_logprob": -0.10801196893056234, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.002979477634653449}, {"id": 636, "seek": 330320, "start": 3313.2, "end": 3322.3199999999997, "text": " things like, I don't know, maybe this is a stupid analogy to use, but it's, sorry, I'm going to go", "tokens": [50864, 721, 411, 11, 286, 500, 380, 458, 11, 1310, 341, 307, 257, 6631, 21663, 281, 764, 11, 457, 309, 311, 11, 2597, 11, 286, 478, 516, 281, 352, 51320], "temperature": 0.0, "avg_logprob": -0.10801196893056234, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.002979477634653449}, {"id": 637, "seek": 330320, "start": 3322.3199999999997, "end": 3326.64, "text": " off on a tangent, but I promise it's sort of relevant. But so one thing that, okay, one thing", "tokens": [51320, 766, 322, 257, 27747, 11, 457, 286, 6228, 309, 311, 1333, 295, 7340, 13, 583, 370, 472, 551, 300, 11, 1392, 11, 472, 551, 51536], "temperature": 0.0, "avg_logprob": -0.10801196893056234, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.002979477634653449}, {"id": 638, "seek": 330320, "start": 3326.64, "end": 3331.2799999999997, "text": " that always breaks my brain is when I try and think about like actuaries and life insurance", "tokens": [51536, 300, 1009, 9857, 452, 3567, 307, 562, 286, 853, 293, 519, 466, 411, 34964, 4889, 293, 993, 7214, 51768], "temperature": 0.0, "avg_logprob": -0.10801196893056234, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.002979477634653449}, {"id": 639, "seek": 333128, "start": 3331.28, "end": 3340.6400000000003, "text": " policies, because it's one of those areas where those models only make sense if they're not", "tokens": [50364, 7657, 11, 570, 309, 311, 472, 295, 729, 3179, 689, 729, 5245, 787, 652, 2020, 498, 436, 434, 406, 50832], "temperature": 0.0, "avg_logprob": -0.11390140464713981, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.006898084655404091}, {"id": 640, "seek": 333128, "start": 3340.6400000000003, "end": 3345.76, "text": " perfect in a sense, right? Like, so if you had an actuary who knew exactly how long everyone was", "tokens": [50832, 2176, 294, 257, 2020, 11, 558, 30, 1743, 11, 370, 498, 291, 632, 364, 605, 6164, 567, 2586, 2293, 577, 938, 1518, 390, 51088], "temperature": 0.0, "avg_logprob": -0.11390140464713981, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.006898084655404091}, {"id": 641, "seek": 333128, "start": 3345.76, "end": 3351.0400000000004, "text": " going to live, and somehow that information was kind of openly available, there would be no,", "tokens": [51088, 516, 281, 1621, 11, 293, 6063, 300, 1589, 390, 733, 295, 23109, 2435, 11, 456, 576, 312, 572, 11, 51352], "temperature": 0.0, "avg_logprob": -0.11390140464713981, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.006898084655404091}, {"id": 642, "seek": 333128, "start": 3351.0400000000004, "end": 3356.7200000000003, "text": " like life insurance policies would be pointless. It's, but you know, whereas also if you had a", "tokens": [51352, 411, 993, 7214, 7657, 576, 312, 32824, 13, 467, 311, 11, 457, 291, 458, 11, 9735, 611, 498, 291, 632, 257, 51636], "temperature": 0.0, "avg_logprob": -0.11390140464713981, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.006898084655404091}, {"id": 643, "seek": 333128, "start": 3356.7200000000003, "end": 3360.1600000000003, "text": " model that was completely hopeless of predicting how long people would live, they would also be", "tokens": [51636, 2316, 300, 390, 2584, 27317, 295, 32884, 577, 938, 561, 576, 1621, 11, 436, 576, 611, 312, 51808], "temperature": 0.0, "avg_logprob": -0.11390140464713981, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.006898084655404091}, {"id": 644, "seek": 336016, "start": 3361.04, "end": 3367.92, "text": " life insurance policies would also be pointless. The very existence of actuarial science", "tokens": [50408, 993, 7214, 7657, 576, 611, 312, 32824, 13, 440, 588, 9123, 295, 605, 20766, 831, 3497, 50752], "temperature": 0.0, "avg_logprob": -0.10972928571271466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001808718778192997}, {"id": 645, "seek": 336016, "start": 3367.92, "end": 3373.6, "text": " relies on your model neither being perfect nor being awful. It somehow has to exist somewhere in", "tokens": [50752, 30910, 322, 428, 2316, 9662, 885, 2176, 6051, 885, 11232, 13, 467, 6063, 575, 281, 2514, 4079, 294, 51036], "temperature": 0.0, "avg_logprob": -0.10972928571271466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001808718778192997}, {"id": 646, "seek": 336016, "start": 3373.6, "end": 3379.04, "text": " between. And as I said, that's something which I, it's one of those topics where if I think about", "tokens": [51036, 1296, 13, 400, 382, 286, 848, 11, 300, 311, 746, 597, 286, 11, 309, 311, 472, 295, 729, 8378, 689, 498, 286, 519, 466, 51308], "temperature": 0.0, "avg_logprob": -0.10972928571271466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001808718778192997}, {"id": 647, "seek": 336016, "start": 3379.04, "end": 3383.6, "text": " it for too long, it all just stops making sense. And entropy has very much that same character,", "tokens": [51308, 309, 337, 886, 938, 11, 309, 439, 445, 10094, 1455, 2020, 13, 400, 30867, 575, 588, 709, 300, 912, 2517, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10972928571271466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001808718778192997}, {"id": 648, "seek": 336016, "start": 3383.6, "end": 3388.96, "text": " right? Because if you were Laplace's demon, if you had perfect information about the system that", "tokens": [51536, 558, 30, 1436, 498, 291, 645, 2369, 6742, 311, 14283, 11, 498, 291, 632, 2176, 1589, 466, 264, 1185, 300, 51804], "temperature": 0.0, "avg_logprob": -0.10972928571271466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001808718778192997}, {"id": 649, "seek": 338896, "start": 3388.96, "end": 3393.76, "text": " you were observing, there's no notion of entropy, right? It's just every, you know, you know every", "tokens": [50364, 291, 645, 22107, 11, 456, 311, 572, 10710, 295, 30867, 11, 558, 30, 467, 311, 445, 633, 11, 291, 458, 11, 291, 458, 633, 50604], "temperature": 0.0, "avg_logprob": -0.14443598122432313, "compression_ratio": 1.6875, "no_speech_prob": 0.001866554724983871}, {"id": 650, "seek": 338896, "start": 3393.76, "end": 3398.64, "text": " micro-estate. So, you know, the Boltzmann formula gives you an entropy value of zero,", "tokens": [50604, 4532, 12, 377, 473, 13, 407, 11, 291, 458, 11, 264, 37884, 89, 14912, 8513, 2709, 291, 364, 30867, 2158, 295, 4018, 11, 50848], "temperature": 0.0, "avg_logprob": -0.14443598122432313, "compression_ratio": 1.6875, "no_speech_prob": 0.001866554724983871}, {"id": 651, "seek": 338896, "start": 3399.76, "end": 3405.44, "text": " where, you know, the notion of entropy only exists once you take that perfect knowledge", "tokens": [50904, 689, 11, 291, 458, 11, 264, 10710, 295, 30867, 787, 8198, 1564, 291, 747, 300, 2176, 3601, 51188], "temperature": 0.0, "avg_logprob": -0.14443598122432313, "compression_ratio": 1.6875, "no_speech_prob": 0.001866554724983871}, {"id": 652, "seek": 338896, "start": 3405.44, "end": 3409.68, "text": " of a system and you coarse-grain it, you define, as I was describing earlier, you introduce an", "tokens": [51188, 295, 257, 1185, 293, 291, 39312, 12, 70, 7146, 309, 11, 291, 6964, 11, 382, 286, 390, 16141, 3071, 11, 291, 5366, 364, 51400], "temperature": 0.0, "avg_logprob": -0.14443598122432313, "compression_ratio": 1.6875, "no_speech_prob": 0.001866554724983871}, {"id": 653, "seek": 338896, "start": 3409.68, "end": 3415.68, "text": " encoding function that is not 100% subjective, so that now you are mapping certain distinct", "tokens": [51400, 43430, 2445, 300, 307, 406, 2319, 4, 25972, 11, 370, 300, 586, 291, 366, 18350, 1629, 10644, 51700], "temperature": 0.0, "avg_logprob": -0.14443598122432313, "compression_ratio": 1.6875, "no_speech_prob": 0.001866554724983871}, {"id": 654, "seek": 341568, "start": 3415.68, "end": 3420.08, "text": " micro-estates onto the same coarse-grained macro-estates. And then now you can ask, okay,", "tokens": [50364, 4532, 12, 377, 1024, 3911, 264, 912, 39312, 12, 20735, 2001, 18887, 12, 377, 1024, 13, 400, 550, 586, 291, 393, 1029, 11, 1392, 11, 50584], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 655, "seek": 341568, "start": 3420.08, "end": 3423.3599999999997, "text": " what's the number of micro-estates that, you know, certainly how coarse is my coarse-graining?", "tokens": [50584, 437, 311, 264, 1230, 295, 4532, 12, 377, 1024, 300, 11, 291, 458, 11, 3297, 577, 39312, 307, 452, 39312, 12, 20735, 1760, 30, 50748], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 656, "seek": 341568, "start": 3423.3599999999997, "end": 3427.3599999999997, "text": " What's the number of micro-estates consistent with this macro-estate? What's the number of different", "tokens": [50748, 708, 311, 264, 1230, 295, 4532, 12, 377, 1024, 8398, 365, 341, 18887, 12, 377, 473, 30, 708, 311, 264, 1230, 295, 819, 50948], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 657, "seek": 341568, "start": 3427.3599999999997, "end": 3431.52, "text": " values of my domain that gets mapped to a single point in my co-domain of my encoding function?", "tokens": [50948, 4190, 295, 452, 9274, 300, 2170, 33318, 281, 257, 2167, 935, 294, 452, 598, 12, 4121, 491, 295, 452, 43430, 2445, 30, 51156], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 658, "seek": 341568, "start": 3431.52, "end": 3437.3599999999997, "text": " And that's what entropy is. And so it's, it's very closely, I mean, it is effectively a measure of", "tokens": [51156, 400, 300, 311, 437, 30867, 307, 13, 400, 370, 309, 311, 11, 309, 311, 588, 8185, 11, 286, 914, 11, 309, 307, 8659, 257, 3481, 295, 51448], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 659, "seek": 341568, "start": 3437.3599999999997, "end": 3441.9199999999996, "text": " how good is my coarse-graining. So if you had perfect knowledge, there's no entropy. If you have no", "tokens": [51448, 577, 665, 307, 452, 39312, 12, 20735, 1760, 13, 407, 498, 291, 632, 2176, 3601, 11, 456, 311, 572, 30867, 13, 759, 291, 362, 572, 51676], "temperature": 0.0, "avg_logprob": -0.0875397208351759, "compression_ratio": 1.9661016949152543, "no_speech_prob": 0.006689693313091993}, {"id": 660, "seek": 344192, "start": 3441.92, "end": 3448.2400000000002, "text": " knowledge, there's no entropy. It relies upon you having a not completely trivial, but also not", "tokens": [50364, 3601, 11, 456, 311, 572, 30867, 13, 467, 30910, 3564, 291, 1419, 257, 406, 2584, 26703, 11, 457, 611, 406, 50680], "temperature": 0.0, "avg_logprob": -0.1238965214909734, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.0016993227181956172}, {"id": 661, "seek": 344192, "start": 3448.2400000000002, "end": 3453.6, "text": " 100%, you know, a slightly subjective, but not 100% subjective encoding function,", "tokens": [50680, 2319, 8923, 291, 458, 11, 257, 4748, 25972, 11, 457, 406, 2319, 4, 25972, 43430, 2445, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1238965214909734, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.0016993227181956172}, {"id": 662, "seek": 344192, "start": 3454.88, "end": 3461.28, "text": " just like with life insurance policies. But so the reason, the reason I'm stating that is because,", "tokens": [51012, 445, 411, 365, 993, 7214, 7657, 13, 583, 370, 264, 1778, 11, 264, 1778, 286, 478, 26688, 300, 307, 570, 11, 51332], "temperature": 0.0, "avg_logprob": -0.1238965214909734, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.0016993227181956172}, {"id": 663, "seek": 344192, "start": 3461.28, "end": 3466.0, "text": " so now it kind of, I think from that perspective, becomes a little bit clearer why there are all", "tokens": [51332, 370, 586, 309, 733, 295, 11, 286, 519, 490, 300, 4585, 11, 3643, 257, 707, 857, 26131, 983, 456, 366, 439, 51568], "temperature": 0.0, "avg_logprob": -0.1238965214909734, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.0016993227181956172}, {"id": 664, "seek": 344192, "start": 3466.0, "end": 3469.84, "text": " these different notions of entropy and why, as the questioner was alluding to, why entropy seems", "tokens": [51568, 613, 819, 35799, 295, 30867, 293, 983, 11, 382, 264, 1168, 260, 390, 439, 33703, 281, 11, 983, 30867, 2544, 51760], "temperature": 0.0, "avg_logprob": -0.1238965214909734, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.0016993227181956172}, {"id": 665, "seek": 346984, "start": 3469.84, "end": 3476.96, "text": " to be so, as a concept seems to be so domain and system specific, because every different system,", "tokens": [50364, 281, 312, 370, 11, 382, 257, 3410, 2544, 281, 312, 370, 9274, 293, 1185, 2685, 11, 570, 633, 819, 1185, 11, 50720], "temperature": 0.0, "avg_logprob": -0.09450826550474262, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.002250589430332184}, {"id": 666, "seek": 346984, "start": 3476.96, "end": 3481.1200000000003, "text": " every different observer will, in principle, have a different set of encoding functions,", "tokens": [50720, 633, 819, 27878, 486, 11, 294, 8665, 11, 362, 257, 819, 992, 295, 43430, 6828, 11, 50928], "temperature": 0.0, "avg_logprob": -0.09450826550474262, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.002250589430332184}, {"id": 667, "seek": 346984, "start": 3481.1200000000003, "end": 3484.96, "text": " a different set of equivalence functions, and each one will give rise to a different calculation", "tokens": [50928, 257, 819, 992, 295, 9052, 655, 6828, 11, 293, 1184, 472, 486, 976, 6272, 281, 257, 819, 17108, 51120], "temperature": 0.0, "avg_logprob": -0.09450826550474262, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.002250589430332184}, {"id": 668, "seek": 346984, "start": 3484.96, "end": 3490.0, "text": " of entropy. And so one way that you can think about this program, this program to try to", "tokens": [51120, 295, 30867, 13, 400, 370, 472, 636, 300, 291, 393, 519, 466, 341, 1461, 11, 341, 1461, 281, 853, 281, 51372], "temperature": 0.0, "avg_logprob": -0.09450826550474262, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.002250589430332184}, {"id": 669, "seek": 346984, "start": 3491.28, "end": 3495.04, "text": " understand the algebraic interplay between time complexity versus kind of equivalency", "tokens": [51436, 1223, 264, 21989, 299, 728, 2858, 1296, 565, 14024, 5717, 733, 295, 9052, 3020, 51624], "temperature": 0.0, "avg_logprob": -0.09450826550474262, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.002250589430332184}, {"id": 670, "seek": 349504, "start": 3495.04, "end": 3500.88, "text": " complexity or computational irreducibility versus multi-computational irreducibility.", "tokens": [50364, 14024, 420, 28270, 16014, 769, 537, 39802, 5717, 4825, 12, 1112, 2582, 1478, 16014, 769, 537, 39802, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11625404728269115, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.018522849306464195}, {"id": 671, "seek": 349504, "start": 3500.88, "end": 3506.4, "text": " In some sense, that is a program to try to understand how different definitions of entropy", "tokens": [50656, 682, 512, 2020, 11, 300, 307, 257, 1461, 281, 853, 281, 1223, 577, 819, 21988, 295, 30867, 50932], "temperature": 0.0, "avg_logprob": -0.11625404728269115, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.018522849306464195}, {"id": 672, "seek": 349504, "start": 3506.4, "end": 3512.08, "text": " relate to each other in these kinds of systems. How, you know, if I take one idealized observer", "tokens": [50932, 10961, 281, 1184, 661, 294, 613, 3685, 295, 3652, 13, 1012, 11, 291, 458, 11, 498, 286, 747, 472, 7157, 1602, 27878, 51216], "temperature": 0.0, "avg_logprob": -0.11625404728269115, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.018522849306464195}, {"id": 673, "seek": 349504, "start": 3512.08, "end": 3517.92, "text": " that has this equivalence function, and I ask, okay, suppose now they communicate with this", "tokens": [51216, 300, 575, 341, 9052, 655, 2445, 11, 293, 286, 1029, 11, 1392, 11, 7297, 586, 436, 7890, 365, 341, 51508], "temperature": 0.0, "avg_logprob": -0.11625404728269115, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.018522849306464195}, {"id": 674, "seek": 349504, "start": 3517.92, "end": 3521.6, "text": " different observer with a different equivalency function, they come to different understandings", "tokens": [51508, 819, 27878, 365, 257, 819, 9052, 3020, 2445, 11, 436, 808, 281, 819, 1223, 1109, 51692], "temperature": 0.0, "avg_logprob": -0.11625404728269115, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.018522849306464195}, {"id": 675, "seek": 352160, "start": 3521.68, "end": 3525.2799999999997, "text": " of what the entropy of the system is, but what is the relationship between their measured entropy", "tokens": [50368, 295, 437, 264, 30867, 295, 264, 1185, 307, 11, 457, 437, 307, 264, 2480, 1296, 641, 12690, 30867, 50548], "temperature": 0.0, "avg_logprob": -0.14812108694788922, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.018781369552016258}, {"id": 676, "seek": 352160, "start": 3525.2799999999997, "end": 3530.3199999999997, "text": " values? They clearly is one that depends algebraically on some details of the distinction", "tokens": [50548, 4190, 30, 814, 4448, 307, 472, 300, 5946, 21989, 984, 322, 512, 4365, 295, 264, 16844, 50800], "temperature": 0.0, "avg_logprob": -0.14812108694788922, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.018781369552016258}, {"id": 677, "seek": 352160, "start": 3530.3199999999997, "end": 3534.3199999999997, "text": " between their respective equivalence functions, but there doesn't seem to be yet any general", "tokens": [50800, 1296, 641, 23649, 9052, 655, 6828, 11, 457, 456, 1177, 380, 1643, 281, 312, 1939, 604, 2674, 51000], "temperature": 0.0, "avg_logprob": -0.14812108694788922, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.018781369552016258}, {"id": 678, "seek": 352160, "start": 3534.3199999999997, "end": 3540.64, "text": " theory for how those things are related, and that's part of the kind of raison d'\u00eatre of this", "tokens": [51000, 5261, 337, 577, 729, 721, 366, 4077, 11, 293, 300, 311, 644, 295, 264, 733, 295, 28402, 274, 6, 9498, 295, 341, 51316], "temperature": 0.0, "avg_logprob": -0.14812108694788922, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.018781369552016258}, {"id": 679, "seek": 354064, "start": 3541.3599999999997, "end": 3549.8399999999997, "text": " of this research program. But yeah, I mean, okay, so one thing that I will comment on,", "tokens": [50400, 295, 341, 2132, 1461, 13, 583, 1338, 11, 286, 914, 11, 1392, 11, 370, 472, 551, 300, 286, 486, 2871, 322, 11, 50824], "temperature": 0.0, "avg_logprob": -0.13962420021615377, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.12219901382923126}, {"id": 680, "seek": 354064, "start": 3551.3599999999997, "end": 3555.2, "text": " although this is a little bit more speculative, it can sort of quasi philosophical comment, but", "tokens": [50900, 4878, 341, 307, 257, 707, 857, 544, 49415, 11, 309, 393, 1333, 295, 20954, 25066, 2871, 11, 457, 51092], "temperature": 0.0, "avg_logprob": -0.13962420021615377, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.12219901382923126}, {"id": 681, "seek": 354064, "start": 3555.2, "end": 3561.6, "text": " so one place where these notions of entropy become one place where the fact that you have", "tokens": [51092, 370, 472, 1081, 689, 613, 35799, 295, 30867, 1813, 472, 1081, 689, 264, 1186, 300, 291, 362, 51412], "temperature": 0.0, "avg_logprob": -0.13962420021615377, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.12219901382923126}, {"id": 682, "seek": 354064, "start": 3561.6, "end": 3565.04, "text": " all these different notions of entropy becomes kind of interesting is in fundamental physics.", "tokens": [51412, 439, 613, 819, 35799, 295, 30867, 3643, 733, 295, 1880, 307, 294, 8088, 10649, 13, 51584], "temperature": 0.0, "avg_logprob": -0.13962420021615377, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.12219901382923126}, {"id": 683, "seek": 356504, "start": 3566.0, "end": 3570.56, "text": " So when you start to think about, if you try to model physics and the universe in these", "tokens": [50412, 407, 562, 291, 722, 281, 519, 466, 11, 498, 291, 853, 281, 2316, 10649, 293, 264, 6445, 294, 613, 50640], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 684, "seek": 356504, "start": 3570.56, "end": 3576.32, "text": " fundamentally computational terms, then one fairly generic sort of conclusion that you can reach", "tokens": [50640, 17879, 28270, 2115, 11, 550, 472, 6457, 19577, 1333, 295, 10063, 300, 291, 393, 2524, 50928], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 685, "seek": 356504, "start": 3576.32, "end": 3580.88, "text": " is that gravitation, general relativity is essentially an entropic phenomenon. I mean,", "tokens": [50928, 307, 300, 7427, 4614, 11, 2674, 45675, 307, 4476, 364, 948, 39173, 14029, 13, 286, 914, 11, 51156], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 686, "seek": 356504, "start": 3581.52, "end": 3585.6, "text": " Valinde and people have kind of talked about this in non computational contexts too,", "tokens": [51188, 7188, 8274, 293, 561, 362, 733, 295, 2825, 466, 341, 294, 2107, 28270, 30628, 886, 11, 51392], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 687, "seek": 356504, "start": 3585.6, "end": 3588.88, "text": " but it's very, very natural if you start to think about space like hypersurfaces being", "tokens": [51392, 457, 309, 311, 588, 11, 588, 3303, 498, 291, 722, 281, 519, 466, 1901, 411, 7420, 433, 21844, 2116, 885, 51556], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 688, "seek": 356504, "start": 3588.88, "end": 3593.52, "text": " a sort of hypergraphs, then, you know, in order to obtain a continuum geometry that's compatible", "tokens": [51556, 257, 1333, 295, 9848, 34091, 82, 11, 550, 11, 291, 458, 11, 294, 1668, 281, 12701, 257, 36120, 18426, 300, 311, 18218, 51788], "temperature": 0.0, "avg_logprob": -0.12958647565143863, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.004822051152586937}, {"id": 689, "seek": 359352, "start": 3593.52, "end": 3598.48, "text": " with the Einstein equations, you need to have certain ergodicity, you know, you need to be", "tokens": [50364, 365, 264, 23486, 11787, 11, 291, 643, 281, 362, 1629, 1189, 21787, 44198, 11, 291, 458, 11, 291, 643, 281, 312, 50612], "temperature": 0.0, "avg_logprob": -0.14024318905051694, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.004901141859591007}, {"id": 690, "seek": 359352, "start": 3598.48, "end": 3603.92, "text": " able to make certain ergodicity assumptions on the rewriting, which in turn, sort of implies", "tokens": [50612, 1075, 281, 652, 1629, 1189, 21787, 44198, 17695, 322, 264, 319, 19868, 11, 597, 294, 1261, 11, 1333, 295, 18779, 50884], "temperature": 0.0, "avg_logprob": -0.14024318905051694, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.004901141859591007}, {"id": 691, "seek": 359352, "start": 3603.92, "end": 3608.64, "text": " certain lower bounds on the entropy of the system. So somehow gravitation, general relativity,", "tokens": [50884, 1629, 3126, 29905, 322, 264, 30867, 295, 264, 1185, 13, 407, 6063, 7427, 4614, 11, 2674, 45675, 11, 51120], "temperature": 0.0, "avg_logprob": -0.14024318905051694, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.004901141859591007}, {"id": 692, "seek": 359352, "start": 3608.64, "end": 3613.28, "text": " is a coarse grained theory that you obtain in the limit as the entropy goes to infinity.", "tokens": [51120, 307, 257, 39312, 1295, 2001, 5261, 300, 291, 12701, 294, 264, 4948, 382, 264, 30867, 1709, 281, 13202, 13, 51352], "temperature": 0.0, "avg_logprob": -0.14024318905051694, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.004901141859591007}, {"id": 693, "seek": 359352, "start": 3613.84, "end": 3619.92, "text": " But something that's interesting is that quantum mechanics, on the other hand, is an idealization", "tokens": [51380, 583, 746, 300, 311, 1880, 307, 300, 13018, 12939, 11, 322, 264, 661, 1011, 11, 307, 364, 7157, 2144, 51684], "temperature": 0.0, "avg_logprob": -0.14024318905051694, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.004901141859591007}, {"id": 694, "seek": 361992, "start": 3619.92, "end": 3624.88, "text": " that you obtain in the limit as entropy goes to zero, because in kind of one of these purely", "tokens": [50364, 300, 291, 12701, 294, 264, 4948, 382, 30867, 1709, 281, 4018, 11, 570, 294, 733, 295, 472, 295, 613, 17491, 50612], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 695, "seek": 361992, "start": 3624.88, "end": 3629.12, "text": " computational models of physics, the quantum mechanical state of a system is described in", "tokens": [50612, 28270, 5245, 295, 10649, 11, 264, 13018, 12070, 1785, 295, 257, 1185, 307, 7619, 294, 50824], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 696, "seek": 361992, "start": 3629.12, "end": 3634.64, "text": " terms of its multi-way structure. It's described in terms of, you know, when I have a kind of a", "tokens": [50824, 2115, 295, 1080, 4825, 12, 676, 3877, 13, 467, 311, 7619, 294, 2115, 295, 11, 291, 458, 11, 562, 286, 362, 257, 733, 295, 257, 51100], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 697, "seek": 361992, "start": 3634.64, "end": 3640.32, "text": " branching program like this, let me find one of these like this, then, you know, I can divide it", "tokens": [51100, 9819, 278, 1461, 411, 341, 11, 718, 385, 915, 472, 295, 613, 411, 341, 11, 550, 11, 291, 458, 11, 286, 393, 9845, 309, 51384], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 698, "seek": 361992, "start": 3640.32, "end": 3645.6, "text": " up into these sort of, into these simultaneity surfaces. And if I associate each state of the", "tokens": [51384, 493, 666, 613, 1333, 295, 11, 666, 613, 13899, 1929, 507, 16130, 13, 400, 498, 286, 14644, 1184, 1785, 295, 264, 51648], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 699, "seek": 361992, "start": 3645.6, "end": 3649.36, "text": " program as being like the analog of a quantum eigen state, and the kind of path weightings", "tokens": [51648, 1461, 382, 885, 411, 264, 16660, 295, 257, 13018, 10446, 1785, 11, 293, 264, 733, 295, 3100, 3364, 1109, 51836], "temperature": 0.0, "avg_logprob": -0.10157068224920743, "compression_ratio": 1.8481848184818481, "no_speech_prob": 0.007805163040757179}, {"id": 700, "seek": 364936, "start": 3649.36, "end": 3653.84, "text": " as being an analogous to the amplitudes associated with the eigen states, I can quickly build up", "tokens": [50364, 382, 885, 364, 16660, 563, 281, 264, 9731, 16451, 6615, 365, 264, 10446, 4368, 11, 286, 393, 2661, 1322, 493, 50588], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 701, "seek": 364936, "start": 3653.84, "end": 3657.36, "text": " a description of this multi-way system in terms of the evolution of some discrete analog of the", "tokens": [50588, 257, 3855, 295, 341, 4825, 12, 676, 1185, 294, 2115, 295, 264, 9303, 295, 512, 27706, 16660, 295, 264, 50764], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 702, "seek": 364936, "start": 3657.36, "end": 3661.52, "text": " Schrodinger equation. And it turns out you get a theory that is kind of equivalent of the mathematically", "tokens": [50764, 2065, 340, 3584, 260, 5367, 13, 400, 309, 4523, 484, 291, 483, 257, 5261, 300, 307, 733, 295, 10344, 295, 264, 44003, 50972], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 703, "seek": 364936, "start": 3661.52, "end": 3666.88, "text": " isomorphic to standard quantum mechanics out of it. So quantum mechanics is sort of, you know,", "tokens": [50972, 307, 32702, 299, 281, 3832, 13018, 12939, 484, 295, 309, 13, 407, 13018, 12939, 307, 1333, 295, 11, 291, 458, 11, 51240], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 704, "seek": 364936, "start": 3666.88, "end": 3672.88, "text": " inextricably bound up with the phenomenon of the multi-way system. But if you take the entropy", "tokens": [51240, 294, 3828, 1341, 1188, 5472, 493, 365, 264, 14029, 295, 264, 4825, 12, 676, 1185, 13, 583, 498, 291, 747, 264, 30867, 51540], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 705, "seek": 364936, "start": 3672.88, "end": 3678.8, "text": " to infinity here, then you're effectively, then the sophistication of your equivalence function", "tokens": [51540, 281, 13202, 510, 11, 550, 291, 434, 8659, 11, 550, 264, 15572, 399, 295, 428, 9052, 655, 2445, 51836], "temperature": 0.0, "avg_logprob": -0.08622999752269071, "compression_ratio": 1.7883435582822085, "no_speech_prob": 0.0005526390159502625}, {"id": 706, "seek": 367880, "start": 3678.8, "end": 3682.96, "text": " becomes arbitrarily large, which means that you can describe any, essentially any pair of", "tokens": [50364, 3643, 19071, 3289, 2416, 11, 597, 1355, 300, 291, 393, 6786, 604, 11, 4476, 604, 6119, 295, 50572], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 707, "seek": 367880, "start": 3682.96, "end": 3689.28, "text": " states as being equivalent. And so it turns out that the, that actually the quantum mechanical", "tokens": [50572, 4368, 382, 885, 10344, 13, 400, 370, 309, 4523, 484, 300, 264, 11, 300, 767, 264, 13018, 12070, 50888], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 708, "seek": 367880, "start": 3689.28, "end": 3692.7200000000003, "text": " case corresponds to the zero entropy limit, whereas the kind of general relativistic case", "tokens": [50888, 1389, 23249, 281, 264, 4018, 30867, 4948, 11, 9735, 264, 733, 295, 2674, 21960, 3142, 1389, 51060], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 709, "seek": 367880, "start": 3692.7200000000003, "end": 3697.04, "text": " corresponds to the infinite entropy limit. But they're kind of two different, two fundamentally", "tokens": [51060, 23249, 281, 264, 13785, 30867, 4948, 13, 583, 436, 434, 733, 295, 732, 819, 11, 732, 17879, 51276], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 710, "seek": 367880, "start": 3697.04, "end": 3701.04, "text": " different notions of entropy, one of which exists at the single-way level, one of which exists at", "tokens": [51276, 819, 35799, 295, 30867, 11, 472, 295, 597, 8198, 412, 264, 2167, 12, 676, 1496, 11, 472, 295, 597, 8198, 412, 51476], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 711, "seek": 367880, "start": 3701.04, "end": 3706.32, "text": " the multi-way level. And again, the question of how these things into play is partly why we're,", "tokens": [51476, 264, 4825, 12, 676, 1496, 13, 400, 797, 11, 264, 1168, 295, 577, 613, 721, 666, 862, 307, 17031, 983, 321, 434, 11, 51740], "temperature": 0.0, "avg_logprob": -0.10306963088020446, "compression_ratio": 1.8737541528239203, "no_speech_prob": 0.00254813302308321}, {"id": 712, "seek": 370632, "start": 3706.32, "end": 3710.48, "text": " you know, why we're investigating this. And it's clear that that question has links to these", "tokens": [50364, 291, 458, 11, 983, 321, 434, 22858, 341, 13, 400, 309, 311, 1850, 300, 300, 1168, 575, 6123, 281, 613, 50572], "temperature": 0.0, "avg_logprob": -0.15797261988863032, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0022860157769173384}, {"id": 713, "seek": 370632, "start": 3710.48, "end": 3717.36, "text": " quite foundational questions in fundamental physics. The ideal point mass and the ideal", "tokens": [50572, 1596, 32195, 1651, 294, 8088, 10649, 13, 440, 7157, 935, 2758, 293, 264, 7157, 50916], "temperature": 0.0, "avg_logprob": -0.15797261988863032, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0022860157769173384}, {"id": 714, "seek": 370632, "start": 3717.36, "end": 3724.48, "text": " distribution with its center of gravity and all this, Dave, question for you, from you.", "tokens": [50916, 7316, 365, 1080, 3056, 295, 12110, 293, 439, 341, 11, 11017, 11, 1168, 337, 291, 11, 490, 291, 13, 51272], "temperature": 0.0, "avg_logprob": -0.15797261988863032, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0022860157769173384}, {"id": 715, "seek": 370632, "start": 3725.76, "end": 3736.1600000000003, "text": " Yes. Okay, I was, can you hear? Yeah, go for it. Okay, good. Yeah, I'd like to hear down to the", "tokens": [51336, 1079, 13, 1033, 11, 286, 390, 11, 393, 291, 1568, 30, 865, 11, 352, 337, 309, 13, 1033, 11, 665, 13, 865, 11, 286, 1116, 411, 281, 1568, 760, 281, 264, 51856], "temperature": 0.0, "avg_logprob": -0.15797261988863032, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0022860157769173384}, {"id": 716, "seek": 373616, "start": 3736.16, "end": 3742.56, "text": " low road during this discussion, maybe show business. What people are trying to explain,", "tokens": [50364, 2295, 3060, 1830, 341, 5017, 11, 1310, 855, 1606, 13, 708, 561, 366, 1382, 281, 2903, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12369531133900517, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.010485540144145489}, {"id": 717, "seek": 373616, "start": 3742.56, "end": 3748.3199999999997, "text": " what is computational reducibility or irreducibility? Often you'll see a graph that says, well,", "tokens": [50684, 437, 307, 28270, 2783, 537, 39802, 420, 16014, 769, 537, 39802, 30, 20043, 291, 603, 536, 257, 4295, 300, 1619, 11, 731, 11, 50972], "temperature": 0.0, "avg_logprob": -0.12369531133900517, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.010485540144145489}, {"id": 718, "seek": 373616, "start": 3748.3199999999997, "end": 3753.7599999999998, "text": " now here's the, the computation, our target running along, the fox is running along,", "tokens": [50972, 586, 510, 311, 264, 11, 264, 24903, 11, 527, 3779, 2614, 2051, 11, 264, 21026, 307, 2614, 2051, 11, 51244], "temperature": 0.0, "avg_logprob": -0.12369531133900517, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.010485540144145489}, {"id": 719, "seek": 373616, "start": 3753.7599999999998, "end": 3760.3199999999997, "text": " and behind it, there's a team of algorithms that would like to catch it. And yeah, it either does", "tokens": [51244, 293, 2261, 309, 11, 456, 311, 257, 1469, 295, 14642, 300, 576, 411, 281, 3745, 309, 13, 400, 1338, 11, 309, 2139, 775, 51572], "temperature": 0.0, "avg_logprob": -0.12369531133900517, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.010485540144145489}, {"id": 720, "seek": 376032, "start": 3760.4, "end": 3766.6400000000003, "text": " or it doesn't outrun all of them, but some can come, can seem to come pretty close to", "tokens": [50368, 420, 309, 1177, 380, 484, 12997, 439, 295, 552, 11, 457, 512, 393, 808, 11, 393, 1643, 281, 808, 1238, 1998, 281, 50680], "temperature": 0.0, "avg_logprob": -0.11154927459417605, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.09007498621940613}, {"id": 721, "seek": 376032, "start": 3767.6000000000004, "end": 3772.96, "text": " catching it. Now, there's something else people have been looking at for a few years, the Mandelbrot", "tokens": [50728, 16124, 309, 13, 823, 11, 456, 311, 746, 1646, 561, 362, 668, 1237, 412, 337, 257, 1326, 924, 11, 264, 15458, 338, 1443, 310, 50996], "temperature": 0.0, "avg_logprob": -0.11154927459417605, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.09007498621940613}, {"id": 722, "seek": 376032, "start": 3772.96, "end": 3779.44, "text": " set. The Mandelbrot set really is a determined, not only a deterministic computation, it's a crisp", "tokens": [50996, 992, 13, 440, 15458, 338, 1443, 310, 992, 534, 307, 257, 9540, 11, 406, 787, 257, 15957, 3142, 24903, 11, 309, 311, 257, 22952, 51320], "temperature": 0.0, "avg_logprob": -0.11154927459417605, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.09007498621940613}, {"id": 723, "seek": 376032, "start": 3779.44, "end": 3788.0, "text": " computation. Every set, every point, either is inside the safe zone, it's going to sit there,", "tokens": [51320, 24903, 13, 2048, 992, 11, 633, 935, 11, 2139, 307, 1854, 264, 3273, 6668, 11, 309, 311, 516, 281, 1394, 456, 11, 51748], "temperature": 0.0, "avg_logprob": -0.11154927459417605, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.09007498621940613}, {"id": 724, "seek": 378800, "start": 3788.0, "end": 3793.84, "text": " and it's happy and quiet, and you color it black, or it flies off to infinity.", "tokens": [50364, 293, 309, 311, 2055, 293, 5677, 11, 293, 291, 2017, 309, 2211, 11, 420, 309, 17414, 766, 281, 13202, 13, 50656], "temperature": 0.0, "avg_logprob": -0.10927269091972938, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.006096910685300827}, {"id": 725, "seek": 378800, "start": 3795.52, "end": 3800.48, "text": " So you could, it could just be a bunch of yes or no, but that's not the way people", "tokens": [50740, 407, 291, 727, 11, 309, 727, 445, 312, 257, 3840, 295, 2086, 420, 572, 11, 457, 300, 311, 406, 264, 636, 561, 50988], "temperature": 0.0, "avg_logprob": -0.10927269091972938, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.006096910685300827}, {"id": 726, "seek": 378800, "start": 3801.04, "end": 3806.32, "text": " trying to calm down after a day of work want to look at it. They want to say, hey, I want to see", "tokens": [51016, 1382, 281, 7151, 760, 934, 257, 786, 295, 589, 528, 281, 574, 412, 309, 13, 814, 528, 281, 584, 11, 4177, 11, 286, 528, 281, 536, 51280], "temperature": 0.0, "avg_logprob": -0.10927269091972938, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.006096910685300827}, {"id": 727, "seek": 378800, "start": 3806.32, "end": 3813.28, "text": " the 32 million deep Mandelbrot set. And I want to see it in colors. So when you get the colors,", "tokens": [51280, 264, 8858, 2459, 2452, 15458, 338, 1443, 310, 992, 13, 400, 286, 528, 281, 536, 309, 294, 4577, 13, 407, 562, 291, 483, 264, 4577, 11, 51628], "temperature": 0.0, "avg_logprob": -0.10927269091972938, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.006096910685300827}, {"id": 728, "seek": 381328, "start": 3813.28, "end": 3819.36, "text": " you ask, how hard did I have to work? How long did I have to grind along before I made that decision?", "tokens": [50364, 291, 1029, 11, 577, 1152, 630, 286, 362, 281, 589, 30, 1012, 938, 630, 286, 362, 281, 16700, 2051, 949, 286, 1027, 300, 3537, 30, 50668], "temperature": 0.0, "avg_logprob": -0.10923327288581329, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.01743934489786625}, {"id": 729, "seek": 381328, "start": 3819.36, "end": 3824.0, "text": " Oh, this is the point that's going to settle down and be quiet and uninteresting, or it's going to", "tokens": [50668, 876, 11, 341, 307, 264, 935, 300, 311, 516, 281, 11852, 760, 293, 312, 5677, 293, 49234, 8714, 11, 420, 309, 311, 516, 281, 50900], "temperature": 0.0, "avg_logprob": -0.10923327288581329, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.01743934489786625}, {"id": 730, "seek": 381328, "start": 3824.0, "end": 3831.84, "text": " fly off to infinity. So you put colors in. Now, I just wonder, would it be interesting to anyone", "tokens": [50900, 3603, 766, 281, 13202, 13, 407, 291, 829, 4577, 294, 13, 823, 11, 286, 445, 2441, 11, 576, 309, 312, 1880, 281, 2878, 51292], "temperature": 0.0, "avg_logprob": -0.10923327288581329, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.01743934489786625}, {"id": 731, "seek": 381328, "start": 3831.84, "end": 3840.6400000000003, "text": " to fuzzify these gorgeous causal and multi-causal graphs and just show this is the portion where", "tokens": [51292, 281, 283, 16740, 2505, 613, 12291, 38755, 293, 4825, 12, 496, 11765, 24877, 293, 445, 855, 341, 307, 264, 8044, 689, 51732], "temperature": 0.0, "avg_logprob": -0.10923327288581329, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.01743934489786625}, {"id": 732, "seek": 384064, "start": 3840.64, "end": 3848.0, "text": " we had to work really hard, rule 35 or whatever. We had to go 15,000 generations before we gave up", "tokens": [50364, 321, 632, 281, 589, 534, 1152, 11, 4978, 6976, 420, 2035, 13, 492, 632, 281, 352, 2119, 11, 1360, 10593, 949, 321, 2729, 493, 50732], "temperature": 0.0, "avg_logprob": -0.13278921300714666, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.002508290112018585}, {"id": 733, "seek": 384064, "start": 3848.0, "end": 3853.8399999999997, "text": " and said, we're not going to follow this anymore. The other one, oh, after 30 steps, I see.", "tokens": [50732, 293, 848, 11, 321, 434, 406, 516, 281, 1524, 341, 3602, 13, 440, 661, 472, 11, 1954, 11, 934, 2217, 4439, 11, 286, 536, 13, 51024], "temperature": 0.0, "avg_logprob": -0.13278921300714666, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.002508290112018585}, {"id": 734, "seek": 384064, "start": 3856.08, "end": 3861.92, "text": " That's a really interesting question. And I mean, yes. So a couple of comments on that. I mean,", "tokens": [51136, 663, 311, 257, 534, 1880, 1168, 13, 400, 286, 914, 11, 2086, 13, 407, 257, 1916, 295, 3053, 322, 300, 13, 286, 914, 11, 51428], "temperature": 0.0, "avg_logprob": -0.13278921300714666, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.002508290112018585}, {"id": 735, "seek": 384064, "start": 3861.92, "end": 3867.12, "text": " so one is, I mean, you're right in the sense that, yes, the Mandelbrot set is a very clean", "tokens": [51428, 370, 472, 307, 11, 286, 914, 11, 291, 434, 558, 294, 264, 2020, 300, 11, 2086, 11, 264, 15458, 338, 1443, 310, 992, 307, 257, 588, 2541, 51688], "temperature": 0.0, "avg_logprob": -0.13278921300714666, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.002508290112018585}, {"id": 736, "seek": 386712, "start": 3867.12, "end": 3872.48, "text": " thing to describe. I would say it's not, I mean, I would argue it's actually not a crisp", "tokens": [50364, 551, 281, 6786, 13, 286, 576, 584, 309, 311, 406, 11, 286, 914, 11, 286, 576, 9695, 309, 311, 767, 406, 257, 22952, 50632], "temperature": 0.0, "avg_logprob": -0.12206889511248388, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.044611673802137375}, {"id": 737, "seek": 386712, "start": 3873.3599999999997, "end": 3877.52, "text": " computation in that sense, precisely for reasons of computational irreducibility,", "tokens": [50676, 24903, 294, 300, 2020, 11, 13402, 337, 4112, 295, 28270, 16014, 769, 537, 39802, 11, 50884], "temperature": 0.0, "avg_logprob": -0.12206889511248388, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.044611673802137375}, {"id": 738, "seek": 386712, "start": 3877.52, "end": 3881.92, "text": " because as you go arbitrarily close to the boundary of the set, you can have complex", "tokens": [50884, 570, 382, 291, 352, 19071, 3289, 1998, 281, 264, 12866, 295, 264, 992, 11, 291, 393, 362, 3997, 51104], "temperature": 0.0, "avg_logprob": -0.12206889511248388, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.044611673802137375}, {"id": 739, "seek": 386712, "start": 3881.92, "end": 3886.0, "text": " numbers that stay, that have a kind of indefinite period of transient, right? There's no", "tokens": [51104, 3547, 300, 1754, 11, 300, 362, 257, 733, 295, 24162, 5194, 642, 2896, 295, 41998, 11, 558, 30, 821, 311, 572, 51308], "temperature": 0.0, "avg_logprob": -0.12206889511248388, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.044611673802137375}, {"id": 740, "seek": 386712, "start": 3886.0, "end": 3893.2, "text": " upper bound on how long the ZN squared plus C orbit can last before it either diverges or", "tokens": [51308, 6597, 5472, 322, 577, 938, 264, 1176, 45, 8889, 1804, 383, 13991, 393, 1036, 949, 309, 2139, 18558, 2880, 420, 51668], "temperature": 0.0, "avg_logprob": -0.12206889511248388, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.044611673802137375}, {"id": 741, "seek": 389320, "start": 3893.2, "end": 3901.12, "text": " converges to zero. And that statement that there will be points that can remain,", "tokens": [50364, 9652, 2880, 281, 4018, 13, 400, 300, 5629, 300, 456, 486, 312, 2793, 300, 393, 6222, 11, 50760], "temperature": 0.0, "avg_logprob": -0.14261862026747837, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0020808051340281963}, {"id": 742, "seek": 389320, "start": 3901.12, "end": 3905.3599999999997, "text": " that can get tied up in these orbits indefinitely is really a computational irreducibility statement.", "tokens": [50760, 300, 393, 483, 9601, 493, 294, 613, 43522, 24162, 10925, 307, 534, 257, 28270, 16014, 769, 537, 39802, 5629, 13, 50972], "temperature": 0.0, "avg_logprob": -0.14261862026747837, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0020808051340281963}, {"id": 743, "seek": 389320, "start": 3906.72, "end": 3914.0, "text": " But yeah, I mean, your question about, could you construct, I mean, that way of coloring", "tokens": [51040, 583, 1338, 11, 286, 914, 11, 428, 1168, 466, 11, 727, 291, 7690, 11, 286, 914, 11, 300, 636, 295, 23198, 51404], "temperature": 0.0, "avg_logprob": -0.14261862026747837, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0020808051340281963}, {"id": 744, "seek": 389320, "start": 3914.0, "end": 3919.4399999999996, "text": " points that are on the boundary of the Mandelbrot set in that way, the so-called escape time algorithm,", "tokens": [51404, 2793, 300, 366, 322, 264, 12866, 295, 264, 15458, 338, 1443, 310, 992, 294, 300, 636, 11, 264, 370, 12, 11880, 7615, 565, 9284, 11, 51676], "temperature": 0.0, "avg_logprob": -0.14261862026747837, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0020808051340281963}, {"id": 745, "seek": 391944, "start": 3919.52, "end": 3923.36, "text": " right? Where you kind of, where you color them based on how many steps that I need to", "tokens": [50368, 558, 30, 2305, 291, 733, 295, 11, 689, 291, 2017, 552, 2361, 322, 577, 867, 4439, 300, 286, 643, 281, 50560], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 746, "seek": 391944, "start": 3923.36, "end": 3928.8, "text": " do before it either converged or escaped off above some, or the complex numbers modulus", "tokens": [50560, 360, 949, 309, 2139, 9652, 3004, 420, 20397, 766, 3673, 512, 11, 420, 264, 3997, 3547, 42287, 50832], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 747, "seek": 391944, "start": 3929.44, "end": 3934.08, "text": " exceeded some value. Yeah, it's an interesting idea that you could try and kind of construct", "tokens": [50864, 38026, 512, 2158, 13, 865, 11, 309, 311, 364, 1880, 1558, 300, 291, 727, 853, 293, 733, 295, 7690, 51096], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 748, "seek": 391944, "start": 3934.08, "end": 3938.32, "text": " geometrical representations of the space of possible computations based on a kind of escape", "tokens": [51096, 12956, 15888, 33358, 295, 264, 1901, 295, 1944, 2807, 763, 2361, 322, 257, 733, 295, 7615, 51308], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 749, "seek": 391944, "start": 3938.32, "end": 3943.12, "text": " time algorithm for computational irreducibility. Yeah, so I mean, there are the possible ways", "tokens": [51308, 565, 9284, 337, 28270, 16014, 769, 537, 39802, 13, 865, 11, 370, 286, 914, 11, 456, 366, 264, 1944, 2098, 51548], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 750, "seek": 391944, "start": 3943.12, "end": 3948.32, "text": " that you could do that, right? That you said, we know, we've known since the days of Turing,", "tokens": [51548, 300, 291, 727, 360, 300, 11, 558, 30, 663, 291, 848, 11, 321, 458, 11, 321, 600, 2570, 1670, 264, 1708, 295, 314, 1345, 11, 51808], "temperature": 0.0, "avg_logprob": -0.1436635082914629, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.007571045774966478}, {"id": 751, "seek": 394832, "start": 3948.32, "end": 3952.2400000000002, "text": " that the halting problem for Turing machines is undecidable, right? That there's no finite", "tokens": [50364, 300, 264, 7523, 783, 1154, 337, 314, 1345, 8379, 307, 674, 3045, 38089, 11, 558, 30, 663, 456, 311, 572, 19362, 50560], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 752, "seek": 394832, "start": 3952.2400000000002, "end": 3955.84, "text": " computation that you can do that will determine over an arbitrary computer program will terminate", "tokens": [50560, 24903, 300, 291, 393, 360, 300, 486, 6997, 670, 364, 23211, 3820, 1461, 486, 10761, 473, 50740], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 753, "seek": 394832, "start": 3955.84, "end": 3960.0800000000004, "text": " in finite time. So you could do it, you could, if you once you have a way of kind of geometrizing", "tokens": [50740, 294, 19362, 565, 13, 407, 291, 727, 360, 309, 11, 291, 727, 11, 498, 291, 1564, 291, 362, 257, 636, 295, 733, 295, 12956, 81, 3319, 50952], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 754, "seek": 394832, "start": 3960.0800000000004, "end": 3964.96, "text": " the space of possible computations, which we, which we have now, as you say. Yeah, you could easily", "tokens": [50952, 264, 1901, 295, 1944, 2807, 763, 11, 597, 321, 11, 597, 321, 362, 586, 11, 382, 291, 584, 13, 865, 11, 291, 727, 3612, 51196], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 755, "seek": 394832, "start": 3964.96, "end": 3968.2400000000002, "text": " construct a kind of escape time algorithm where you know, you have your analog of the Mandelbrot", "tokens": [51196, 7690, 257, 733, 295, 7615, 565, 9284, 689, 291, 458, 11, 291, 362, 428, 16660, 295, 264, 15458, 338, 1443, 310, 51360], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 756, "seek": 394832, "start": 3968.2400000000002, "end": 3971.6000000000004, "text": " set, which is, you know, here's all the halting computations, here are all the definitely non", "tokens": [51360, 992, 11, 597, 307, 11, 291, 458, 11, 510, 311, 439, 264, 7523, 783, 2807, 763, 11, 510, 366, 439, 264, 2138, 2107, 51528], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 757, "seek": 394832, "start": 3971.6000000000004, "end": 3975.84, "text": " halting computations. And then there's some boundary of very fuzzy stuff where we kind of don't", "tokens": [51528, 7523, 783, 2807, 763, 13, 400, 550, 456, 311, 512, 12866, 295, 588, 34710, 1507, 689, 321, 733, 295, 500, 380, 51740], "temperature": 0.0, "avg_logprob": -0.123600891658238, "compression_ratio": 1.9228571428571428, "no_speech_prob": 0.0012637790059670806}, {"id": 758, "seek": 397584, "start": 3975.84, "end": 3980.08, "text": " really know, we have to do a lot of work. And even then it's only heuristic, which is, which", "tokens": [50364, 534, 458, 11, 321, 362, 281, 360, 257, 688, 295, 589, 13, 400, 754, 550, 309, 311, 787, 415, 374, 3142, 11, 597, 307, 11, 597, 50576], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 759, "seek": 397584, "start": 3980.08, "end": 3984.8, "text": " is so, yeah, I mean, a very directly analogous. And then, yeah, and then the question becomes,", "tokens": [50576, 307, 370, 11, 1338, 11, 286, 914, 11, 257, 588, 3838, 16660, 563, 13, 400, 550, 11, 1338, 11, 293, 550, 264, 1168, 3643, 11, 50812], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 760, "seek": 397584, "start": 3985.6000000000004, "end": 3991.52, "text": " you know, so in the case of the M set, you know, there is some underlying theory mostly,", "tokens": [50852, 291, 458, 11, 370, 294, 264, 1389, 295, 264, 376, 992, 11, 291, 458, 11, 456, 307, 512, 14217, 5261, 5240, 11, 51148], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 761, "seek": 397584, "start": 3991.52, "end": 3995.76, "text": " I think mostly due to like Dwadi and Hubbard and people that allows you to kind of predict,", "tokens": [51148, 286, 519, 5240, 3462, 281, 411, 41448, 5688, 293, 18986, 29984, 293, 561, 300, 4045, 291, 281, 733, 295, 6069, 11, 51360], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 762, "seek": 397584, "start": 3995.76, "end": 4000.56, "text": " like, you know, if you have a finite filament, you can, like, of the Mandelbrot set, you can", "tokens": [51360, 411, 11, 291, 458, 11, 498, 291, 362, 257, 19362, 44280, 11, 291, 393, 11, 411, 11, 295, 264, 15458, 338, 1443, 310, 992, 11, 291, 393, 51600], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 763, "seek": 397584, "start": 4000.56, "end": 4005.44, "text": " kind of predict where that filament is going to be based on some complicated complex analysis", "tokens": [51600, 733, 295, 6069, 689, 300, 44280, 307, 516, 281, 312, 2361, 322, 512, 6179, 3997, 5215, 51844], "temperature": 0.0, "avg_logprob": -0.14072432107483315, "compression_ratio": 1.8438538205980066, "no_speech_prob": 0.006373751442879438}, {"id": 764, "seek": 400544, "start": 4005.44, "end": 4009.84, "text": " argument. And, you know, I mean, the question is once you, if you once you have a geometrization", "tokens": [50364, 6770, 13, 400, 11, 291, 458, 11, 286, 914, 11, 264, 1168, 307, 1564, 291, 11, 498, 291, 1564, 291, 362, 257, 12956, 24959, 399, 50584], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 765, "seek": 400544, "start": 4009.84, "end": 4015.04, "text": " of the space of possible computations, can you construct some general theory like the theory,", "tokens": [50584, 295, 264, 1901, 295, 1944, 2807, 763, 11, 393, 291, 7690, 512, 2674, 5261, 411, 264, 5261, 11, 50844], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 766, "seek": 400544, "start": 4015.04, "end": 4018.48, "text": " you know, developed by Dwadi and Hubbard that tells you things about, you know, the topology", "tokens": [50844, 291, 458, 11, 4743, 538, 41448, 5688, 293, 18986, 29984, 300, 5112, 291, 721, 466, 11, 291, 458, 11, 264, 1192, 1793, 51016], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 767, "seek": 400544, "start": 4018.48, "end": 4023.12, "text": " of the computations, whether certain regions of the space are connected, whether they're compact,", "tokens": [51016, 295, 264, 2807, 763, 11, 1968, 1629, 10682, 295, 264, 1901, 366, 4582, 11, 1968, 436, 434, 14679, 11, 51248], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 768, "seek": 400544, "start": 4023.12, "end": 4026.4, "text": " whether you can make, you know, whether you can do a similar thing, or you can make predictions", "tokens": [51248, 1968, 291, 393, 652, 11, 291, 458, 11, 1968, 291, 393, 360, 257, 2531, 551, 11, 420, 291, 393, 652, 21264, 51412], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 769, "seek": 400544, "start": 4026.4, "end": 4030.48, "text": " based on the geometry of one part of the set, you know, where you can extrapolate to things about", "tokens": [51412, 2361, 322, 264, 18426, 295, 472, 644, 295, 264, 992, 11, 291, 458, 11, 689, 291, 393, 48224, 473, 281, 721, 466, 51616], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 770, "seek": 400544, "start": 4030.48, "end": 4035.2000000000003, "text": " the geometry of another part of the set. That's a really interesting question. And yeah, again,", "tokens": [51616, 264, 18426, 295, 1071, 644, 295, 264, 992, 13, 663, 311, 257, 534, 1880, 1168, 13, 400, 1338, 11, 797, 11, 51852], "temperature": 0.0, "avg_logprob": -0.10520508231186285, "compression_ratio": 2.058282208588957, "no_speech_prob": 0.001546400599181652}, {"id": 771, "seek": 403520, "start": 4035.7599999999998, "end": 4039.68, "text": " as with so many of these things, it's one of these, like, I don't know the answer, but it's a good", "tokens": [50392, 382, 365, 370, 867, 295, 613, 721, 11, 309, 311, 472, 295, 613, 11, 411, 11, 286, 500, 380, 458, 264, 1867, 11, 457, 309, 311, 257, 665, 50588], "temperature": 0.0, "avg_logprob": -0.09948791835619056, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.00033527560299262404}, {"id": 772, "seek": 403520, "start": 4039.68, "end": 4045.8399999999997, "text": " reason for investigating, you know, for pursuing the program, right? Yeah, from the Wolfram side,", "tokens": [50588, 1778, 337, 22858, 11, 291, 458, 11, 337, 20222, 264, 1461, 11, 558, 30, 865, 11, 490, 264, 16634, 2356, 1252, 11, 50896], "temperature": 0.0, "avg_logprob": -0.09948791835619056, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.00033527560299262404}, {"id": 773, "seek": 403520, "start": 4045.8399999999997, "end": 4052.72, "text": " as well as from the active inference side, there's both the information topology and the", "tokens": [50896, 382, 731, 382, 490, 264, 4967, 38253, 1252, 11, 456, 311, 1293, 264, 1589, 1192, 1793, 293, 264, 51240], "temperature": 0.0, "avg_logprob": -0.09948791835619056, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.00033527560299262404}, {"id": 774, "seek": 403520, "start": 4052.72, "end": 4058.48, "text": " information geometry side, and not just in the kind of topological deep learning, but rather", "tokens": [51240, 1589, 18426, 1252, 11, 293, 406, 445, 294, 264, 733, 295, 1192, 4383, 2452, 2539, 11, 457, 2831, 51528], "temperature": 0.0, "avg_logprob": -0.09948791835619056, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.00033527560299262404}, {"id": 775, "seek": 403520, "start": 4058.48, "end": 4064.8799999999997, "text": " looking at the topology of information flows, which has been heavily developed in the category", "tokens": [51528, 1237, 412, 264, 1192, 1793, 295, 1589, 12867, 11, 597, 575, 668, 10950, 4743, 294, 264, 7719, 51848], "temperature": 0.0, "avg_logprob": -0.09948791835619056, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.00033527560299262404}, {"id": 776, "seek": 406488, "start": 4065.12, "end": 4070.8, "text": " related to quantum information sciences. And then the information geometries that allow us to do,", "tokens": [50376, 4077, 281, 13018, 1589, 17677, 13, 400, 550, 264, 1589, 12956, 2244, 300, 2089, 505, 281, 360, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10411988857180574, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0013248055474832654}, {"id": 777, "seek": 406488, "start": 4070.8, "end": 4077.12, "text": " like, machine learning type, accelerated optimization ingredients, all these kinds of concepts that come", "tokens": [50660, 411, 11, 3479, 2539, 2010, 11, 29763, 19618, 6952, 11, 439, 613, 3685, 295, 10392, 300, 808, 50976], "temperature": 0.0, "avg_logprob": -0.10411988857180574, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0013248055474832654}, {"id": 778, "seek": 406488, "start": 4077.12, "end": 4084.6400000000003, "text": " into play with geometry, and we just don't get them from topology. So topology kind of sketches", "tokens": [50976, 666, 862, 365, 18426, 11, 293, 321, 445, 500, 380, 483, 552, 490, 1192, 1793, 13, 407, 1192, 1793, 733, 295, 34547, 51352], "temperature": 0.0, "avg_logprob": -0.10411988857180574, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0013248055474832654}, {"id": 779, "seek": 406488, "start": 4084.6400000000003, "end": 4090.56, "text": " the skeleton, and then with the computers that we have, the information geometry is at least on", "tokens": [51352, 264, 25204, 11, 293, 550, 365, 264, 10807, 300, 321, 362, 11, 264, 1589, 18426, 307, 412, 1935, 322, 51648], "temperature": 0.0, "avg_logprob": -0.10411988857180574, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0013248055474832654}, {"id": 780, "seek": 409056, "start": 4090.56, "end": 4095.84, "text": " the data sets and the ways that we have computation today, that's kind of like the quantitative", "tokens": [50364, 264, 1412, 6352, 293, 264, 2098, 300, 321, 362, 24903, 965, 11, 300, 311, 733, 295, 411, 264, 27778, 50628], "temperature": 0.0, "avg_logprob": -0.08628998607038016, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.09802986681461334}, {"id": 781, "seek": 409056, "start": 4095.84, "end": 4103.92, "text": " numerical versus the formal. One question is, are these all discrete state space, discrete time", "tokens": [50628, 29054, 5717, 264, 9860, 13, 1485, 1168, 307, 11, 366, 613, 439, 27706, 1785, 1901, 11, 27706, 565, 51032], "temperature": 0.0, "avg_logprob": -0.08628998607038016, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.09802986681461334}, {"id": 782, "seek": 409056, "start": 4103.92, "end": 4110.8, "text": " formalisms? Because in active inference, we often deal with hybrid models that have discrete and", "tokens": [51032, 9860, 13539, 30, 1436, 294, 4967, 38253, 11, 321, 2049, 2028, 365, 13051, 5245, 300, 362, 27706, 293, 51376], "temperature": 0.0, "avg_logprob": -0.08628998607038016, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.09802986681461334}, {"id": 783, "seek": 409056, "start": 4110.8, "end": 4115.44, "text": " continuous state spaces, and the same generative model or the same system of interest could be", "tokens": [51376, 10957, 1785, 7673, 11, 293, 264, 912, 1337, 1166, 2316, 420, 264, 912, 1185, 295, 1179, 727, 312, 51608], "temperature": 0.0, "avg_logprob": -0.08628998607038016, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.09802986681461334}, {"id": 784, "seek": 411544, "start": 4115.44, "end": 4121.36, "text": " modeled with like a discrete time chapter seven, or a continuous time chapter eight model. So how", "tokens": [50364, 37140, 365, 411, 257, 27706, 565, 7187, 3407, 11, 420, 257, 10957, 565, 7187, 3180, 2316, 13, 407, 577, 50660], "temperature": 0.0, "avg_logprob": -0.11880999478426847, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.018535740673542023}, {"id": 785, "seek": 411544, "start": 4121.36, "end": 4126.4, "text": " does this deal with that? That's a really good question. I mean, yeah, so most of what I've been", "tokens": [50660, 775, 341, 2028, 365, 300, 30, 663, 311, 257, 534, 665, 1168, 13, 286, 914, 11, 1338, 11, 370, 881, 295, 437, 286, 600, 668, 50912], "temperature": 0.0, "avg_logprob": -0.11880999478426847, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.018535740673542023}, {"id": 786, "seek": 411544, "start": 4127.44, "end": 4134.0, "text": " looking at so far has consisted of discrete time, discrete space models, for no particularly", "tokens": [50964, 1237, 412, 370, 1400, 575, 38227, 295, 27706, 565, 11, 27706, 1901, 5245, 11, 337, 572, 4098, 51292], "temperature": 0.0, "avg_logprob": -0.11880999478426847, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.018535740673542023}, {"id": 787, "seek": 411544, "start": 4134.0, "end": 4139.12, "text": " principled reason other than they're easier to analyze, right? For the most part, because you", "tokens": [51292, 3681, 15551, 1778, 661, 813, 436, 434, 3571, 281, 12477, 11, 558, 30, 1171, 264, 881, 644, 11, 570, 291, 51548], "temperature": 0.0, "avg_logprob": -0.11880999478426847, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.018535740673542023}, {"id": 788, "seek": 411544, "start": 4139.12, "end": 4143.04, "text": " can do explicit computations because they're kind of more amenable to constructive analysis.", "tokens": [51548, 393, 360, 13691, 2807, 763, 570, 436, 434, 733, 295, 544, 18497, 712, 281, 30223, 5215, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11880999478426847, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.018535740673542023}, {"id": 789, "seek": 414304, "start": 4143.76, "end": 4148.56, "text": " It's easier to do, but the beauty of a lot of this, that's one of the beauties of using kind of", "tokens": [50400, 467, 311, 3571, 281, 360, 11, 457, 264, 6643, 295, 257, 688, 295, 341, 11, 300, 311, 472, 295, 264, 1869, 530, 295, 1228, 733, 295, 50640], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 790, "seek": 414304, "start": 4148.56, "end": 4153.44, "text": " general mathematical formalism is that once you develop it, it's often quite easy to extend even", "tokens": [50640, 2674, 18894, 9860, 1434, 307, 300, 1564, 291, 1499, 309, 11, 309, 311, 2049, 1596, 1858, 281, 10101, 754, 50884], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 791, "seek": 414304, "start": 4153.44, "end": 4158.88, "text": " to cases that, or to extrapolate to the cases that you didn't explicitly analyze. So in principle,", "tokens": [50884, 281, 3331, 300, 11, 420, 281, 48224, 473, 281, 264, 3331, 300, 291, 994, 380, 20803, 12477, 13, 407, 294, 8665, 11, 51156], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 792, "seek": 414304, "start": 4158.88, "end": 4163.76, "text": " this formalism works for continuum space and continuum time systems as well,", "tokens": [51156, 341, 9860, 1434, 1985, 337, 36120, 1901, 293, 36120, 565, 3652, 382, 731, 11, 51400], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 793, "seek": 414304, "start": 4163.76, "end": 4168.32, "text": " just with some slight modifications. So rather than having say, branching and merging, you instead", "tokens": [51400, 445, 365, 512, 4036, 26881, 13, 407, 2831, 813, 1419, 584, 11, 9819, 278, 293, 44559, 11, 291, 2602, 51628], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 794, "seek": 414304, "start": 4168.32, "end": 4172.8, "text": " have, if you think about this thing as now being a dynamical system, described on the", "tokens": [51628, 362, 11, 498, 291, 519, 466, 341, 551, 382, 586, 885, 257, 5999, 804, 1185, 11, 7619, 322, 264, 51852], "temperature": 0.0, "avg_logprob": -0.12302261008355851, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.007803316693753004}, {"id": 795, "seek": 417280, "start": 4172.8, "end": 4176.64, "text": " on some symplectic manifold, then these kind of branching, merging operations of the multi-way", "tokens": [50364, 322, 512, 6697, 781, 15518, 47138, 11, 550, 613, 733, 295, 9819, 278, 11, 44559, 7705, 295, 264, 4825, 12, 676, 50556], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 796, "seek": 417280, "start": 4176.64, "end": 4182.8, "text": " system become effectively divergence and convergence, differential operators are defined on the", "tokens": [50556, 1185, 1813, 8659, 47387, 293, 32181, 11, 15756, 19077, 366, 7642, 322, 264, 50864], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 797, "seek": 417280, "start": 4182.8, "end": 4188.64, "text": " symplectic manifold. And so one place where we can start to analyze that explicitly, and which", "tokens": [50864, 6697, 781, 15518, 47138, 13, 400, 370, 472, 1081, 689, 321, 393, 722, 281, 12477, 300, 20803, 11, 293, 597, 51156], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 798, "seek": 417280, "start": 4188.64, "end": 4192.64, "text": " I've done a little bit of work on, but it's one of these things which I want to go back to very", "tokens": [51156, 286, 600, 1096, 257, 707, 857, 295, 589, 322, 11, 457, 309, 311, 472, 295, 613, 721, 597, 286, 528, 281, 352, 646, 281, 588, 51356], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 799, "seek": 417280, "start": 4192.64, "end": 4197.4400000000005, "text": " soon, is looking at PetriNets. So PetriNets are interesting because they are a discrete time,", "tokens": [51356, 2321, 11, 307, 1237, 412, 10472, 470, 45, 1385, 13, 407, 10472, 470, 45, 1385, 366, 1880, 570, 436, 366, 257, 27706, 565, 11, 51596], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 800, "seek": 417280, "start": 4197.4400000000005, "end": 4202.64, "text": " discrete space system, but they admit a continuum space, continuum time, description", "tokens": [51596, 27706, 1901, 1185, 11, 457, 436, 9796, 257, 36120, 1901, 11, 36120, 565, 11, 3855, 51856], "temperature": 0.0, "avg_logprob": -0.1335366422479803, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.0009689879370853305}, {"id": 801, "seek": 420264, "start": 4202.64, "end": 4206.96, "text": " in terms of ordinary differential equations and so on. So they're a nice example of a hybrid", "tokens": [50364, 294, 2115, 295, 10547, 15756, 11787, 293, 370, 322, 13, 407, 436, 434, 257, 1481, 1365, 295, 257, 13051, 50580], "temperature": 0.0, "avg_logprob": -0.06267362949894924, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.0005962615250609815}, {"id": 802, "seek": 420264, "start": 4206.96, "end": 4211.200000000001, "text": " kind of discrete event versus continuum event system, where it's clear that this formalism", "tokens": [50580, 733, 295, 27706, 2280, 5717, 36120, 2280, 1185, 11, 689, 309, 311, 1850, 300, 341, 9860, 1434, 50792], "temperature": 0.0, "avg_logprob": -0.06267362949894924, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.0005962615250609815}, {"id": 803, "seek": 420264, "start": 4211.200000000001, "end": 4217.04, "text": " can be used and is somehow agnostic as to whether the underlying system is discrete or continuous.", "tokens": [50792, 393, 312, 1143, 293, 307, 6063, 623, 77, 19634, 382, 281, 1968, 264, 14217, 1185, 307, 27706, 420, 10957, 13, 51084], "temperature": 0.0, "avg_logprob": -0.06267362949894924, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.0005962615250609815}, {"id": 804, "seek": 420264, "start": 4218.08, "end": 4221.04, "text": " Again, there's a broader philosophical point to make here, which is that", "tokens": [51136, 3764, 11, 456, 311, 257, 13227, 25066, 935, 281, 652, 510, 11, 597, 307, 300, 51284], "temperature": 0.0, "avg_logprob": -0.06267362949894924, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.0005962615250609815}, {"id": 805, "seek": 420264, "start": 4224.4800000000005, "end": 4228.320000000001, "text": " in a way, one of the reasons I don't feel embarrassed to be working primarily with discrete", "tokens": [51456, 294, 257, 636, 11, 472, 295, 264, 4112, 286, 500, 380, 841, 16843, 281, 312, 1364, 10029, 365, 27706, 51648], "temperature": 0.0, "avg_logprob": -0.06267362949894924, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.0005962615250609815}, {"id": 806, "seek": 422832, "start": 4228.32, "end": 4233.28, "text": " systems is because, again, once you start to think about things in terms of, okay,", "tokens": [50364, 3652, 307, 570, 11, 797, 11, 1564, 291, 722, 281, 519, 466, 721, 294, 2115, 295, 11, 1392, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12631057048666067, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.019073529168963432}, {"id": 807, "seek": 422832, "start": 4233.28, "end": 4236.5599999999995, "text": " you have to not just care about nature, you also have to care about the computations the observer", "tokens": [50612, 291, 362, 281, 406, 445, 1127, 466, 3687, 11, 291, 611, 362, 281, 1127, 466, 264, 2807, 763, 264, 27878, 50776], "temperature": 0.0, "avg_logprob": -0.12631057048666067, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.019073529168963432}, {"id": 808, "seek": 422832, "start": 4236.5599999999995, "end": 4241.92, "text": " can perform and what it's able to infer, then you quickly realize that in a sense, just like,", "tokens": [50776, 393, 2042, 293, 437, 309, 311, 1075, 281, 13596, 11, 550, 291, 2661, 4325, 300, 294, 257, 2020, 11, 445, 411, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12631057048666067, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.019073529168963432}, {"id": 809, "seek": 422832, "start": 4241.92, "end": 4245.5199999999995, "text": " whatever, as you were saying earlier, Danny, beauty is in the eye of the beholder. I think", "tokens": [51044, 2035, 11, 382, 291, 645, 1566, 3071, 11, 16682, 11, 6643, 307, 294, 264, 3313, 295, 264, 312, 20480, 13, 286, 519, 51224], "temperature": 0.0, "avg_logprob": -0.12631057048666067, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.019073529168963432}, {"id": 810, "seek": 422832, "start": 4245.5199999999995, "end": 4252.24, "text": " discreteness and continuity are also in the eye of the beholder, right? So if you have", "tokens": [51224, 2983, 35383, 442, 293, 23807, 366, 611, 294, 264, 3313, 295, 264, 312, 20480, 11, 558, 30, 407, 498, 291, 362, 51560], "temperature": 0.0, "avg_logprob": -0.12631057048666067, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.019073529168963432}, {"id": 811, "seek": 425224, "start": 4252.88, "end": 4262.4, "text": " a universe that is fundamentally continuous, that's described by a continuum, the Renzi and", "tokens": [50396, 257, 6445, 300, 307, 17879, 10957, 11, 300, 311, 7619, 538, 257, 36120, 11, 264, 12883, 3992, 293, 50872], "temperature": 0.0, "avg_logprob": -0.17821767216637022, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.005905299913138151}, {"id": 812, "seek": 425224, "start": 4262.4, "end": 4266.8, "text": " Manifold or something, but your constraints are that the only experiments you perform have", "tokens": [50872, 2458, 351, 2641, 420, 746, 11, 457, 428, 18491, 366, 300, 264, 787, 12050, 291, 2042, 362, 51092], "temperature": 0.0, "avg_logprob": -0.17821767216637022, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.005905299913138151}, {"id": 813, "seek": 425224, "start": 4266.8, "end": 4274.4, "text": " computable outcomes, have discrete outcomes, where the possible number of observables is", "tokens": [51092, 2807, 712, 10070, 11, 362, 27706, 10070, 11, 689, 264, 1944, 1230, 295, 9951, 2965, 307, 51472], "temperature": 0.0, "avg_logprob": -0.17821767216637022, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.005905299913138151}, {"id": 814, "seek": 425224, "start": 4274.4, "end": 4278.4, "text": " always countable, then in a sense, it doesn't matter, right? It's irrelevant to you as an", "tokens": [51472, 1009, 1207, 712, 11, 550, 294, 257, 2020, 11, 309, 1177, 380, 1871, 11, 558, 30, 467, 311, 28682, 281, 291, 382, 364, 51672], "temperature": 0.0, "avg_logprob": -0.17821767216637022, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.005905299913138151}, {"id": 815, "seek": 427840, "start": 4278.4, "end": 4283.599999999999, "text": " observer whether the system is discrete or continuous, because the only parts of it that", "tokens": [50364, 27878, 1968, 264, 1185, 307, 27706, 420, 10957, 11, 570, 264, 787, 3166, 295, 309, 300, 50624], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 816, "seek": 427840, "start": 4283.599999999999, "end": 4287.5199999999995, "text": " you can interface with and interact with are discrete, and so you could have replaced the", "tokens": [50624, 291, 393, 9226, 365, 293, 4648, 365, 366, 27706, 11, 293, 370, 291, 727, 362, 10772, 264, 50820], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 817, "seek": 427840, "start": 4287.5199999999995, "end": 4290.32, "text": " underlying substrate with a purely discrete mathematical structure and you wouldn't be able", "tokens": [50820, 14217, 27585, 365, 257, 17491, 27706, 18894, 3877, 293, 291, 2759, 380, 312, 1075, 50960], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 818, "seek": 427840, "start": 4290.32, "end": 4298.0, "text": " to tell. So in some sense, I don't feel too embarrassed dealing with discrete event systems", "tokens": [50960, 281, 980, 13, 407, 294, 512, 2020, 11, 286, 500, 380, 841, 886, 16843, 6260, 365, 27706, 2280, 3652, 51344], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 819, "seek": 427840, "start": 4298.0, "end": 4301.839999999999, "text": " because even if I don't necessarily believe that nature is discrete, because I don't think that's,", "tokens": [51344, 570, 754, 498, 286, 500, 380, 4725, 1697, 300, 3687, 307, 27706, 11, 570, 286, 500, 380, 519, 300, 311, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 820, "seek": 427840, "start": 4301.839999999999, "end": 4306.879999999999, "text": " I'm not even sure how we would be able to answer that, I'm reasonably convinced that", "tokens": [51536, 286, 478, 406, 754, 988, 577, 321, 576, 312, 1075, 281, 1867, 300, 11, 286, 478, 23551, 12561, 300, 51788], "temperature": 0.0, "avg_logprob": -0.10371412436167399, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.03402942791581154}, {"id": 821, "seek": 430688, "start": 4307.84, "end": 4313.2, "text": " the experiments that we can perform and the observations that we're able to perform are", "tokens": [50412, 264, 12050, 300, 321, 393, 2042, 293, 264, 18163, 300, 321, 434, 1075, 281, 2042, 366, 50680], "temperature": 0.0, "avg_logprob": -0.11490665435791016, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0015484053874388337}, {"id": 822, "seek": 430688, "start": 4313.2, "end": 4317.4400000000005, "text": " ultimately computable, and therefore, the underlying substrate might as well be discrete,", "tokens": [50680, 6284, 2807, 712, 11, 293, 4412, 11, 264, 14217, 27585, 1062, 382, 731, 312, 27706, 11, 50892], "temperature": 0.0, "avg_logprob": -0.11490665435791016, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0015484053874388337}, {"id": 823, "seek": 430688, "start": 4317.4400000000005, "end": 4323.12, "text": " even if it's not in reality, so to speak. Yeah, that's a great comment and definitely", "tokens": [50892, 754, 498, 309, 311, 406, 294, 4103, 11, 370, 281, 1710, 13, 865, 11, 300, 311, 257, 869, 2871, 293, 2138, 51176], "temperature": 0.0, "avg_logprob": -0.11490665435791016, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0015484053874388337}, {"id": 824, "seek": 430688, "start": 4323.12, "end": 4328.64, "text": " calls back to your earlier points about discretization being in the eye of the beholder,", "tokens": [51176, 5498, 646, 281, 428, 3071, 2793, 466, 25656, 2144, 885, 294, 264, 3313, 295, 264, 312, 20480, 11, 51452], "temperature": 0.0, "avg_logprob": -0.11490665435791016, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0015484053874388337}, {"id": 825, "seek": 430688, "start": 4328.64, "end": 4334.16, "text": " like in the active inference models, observations, raw data may already be discretized depending", "tokens": [51452, 411, 294, 264, 4967, 38253, 5245, 11, 18163, 11, 8936, 1412, 815, 1217, 312, 25656, 1602, 5413, 51728], "temperature": 0.0, "avg_logprob": -0.11490665435791016, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0015484053874388337}, {"id": 826, "seek": 433416, "start": 4334.16, "end": 4339.5199999999995, "text": " on the situation, but even if it weren't, like it were a continuous sensory perception or modeled", "tokens": [50364, 322, 264, 2590, 11, 457, 754, 498, 309, 4999, 380, 11, 411, 309, 645, 257, 10957, 27233, 12860, 420, 37140, 50632], "temperature": 0.0, "avg_logprob": -0.10226502475968327, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0288651455193758}, {"id": 827, "seek": 433416, "start": 4339.5199999999995, "end": 4346.8, "text": " as such analytically, still commonly models discretizing categorize as they move up cognitive", "tokens": [50632, 382, 1270, 10783, 984, 11, 920, 12719, 5245, 25656, 3319, 19250, 1125, 382, 436, 1286, 493, 15605, 50996], "temperature": 0.0, "avg_logprob": -0.10226502475968327, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0288651455193758}, {"id": 828, "seek": 433416, "start": 4346.8, "end": 4353.36, "text": " hierarchies, and that was like initially explored to get more of this discrete either or decision", "tokens": [50996, 35250, 530, 11, 293, 300, 390, 411, 9105, 24016, 281, 483, 544, 295, 341, 27706, 2139, 420, 3537, 51324], "temperature": 0.0, "avg_logprob": -0.10226502475968327, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0288651455193758}, {"id": 829, "seek": 433416, "start": 4353.36, "end": 4361.12, "text": " making, planning, all those kinds of properties. Well, there's many interesting angles like", "tokens": [51324, 1455, 11, 5038, 11, 439, 729, 3685, 295, 7221, 13, 1042, 11, 456, 311, 867, 1880, 14708, 411, 51712], "temperature": 0.0, "avg_logprob": -0.10226502475968327, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0288651455193758}, {"id": 830, "seek": 436112, "start": 4361.36, "end": 4368.72, "text": " I'm sure also it could be a multiplexed language model prompt, but like what are you working on", "tokens": [50376, 286, 478, 988, 611, 309, 727, 312, 257, 3311, 2021, 292, 2856, 2316, 12391, 11, 457, 411, 437, 366, 291, 1364, 322, 50744], "temperature": 0.0, "avg_logprob": -0.1491264482823814, "compression_ratio": 1.5224489795918368, "no_speech_prob": 0.0011691934196278453}, {"id": 831, "seek": 436112, "start": 4368.72, "end": 4377.44, "text": " or excited about for 2024? That's a good question. So I've kind of already given some hints about,", "tokens": [50744, 420, 2919, 466, 337, 45237, 30, 663, 311, 257, 665, 1168, 13, 407, 286, 600, 733, 295, 1217, 2212, 512, 27271, 466, 11, 51180], "temperature": 0.0, "avg_logprob": -0.1491264482823814, "compression_ratio": 1.5224489795918368, "no_speech_prob": 0.0011691934196278453}, {"id": 832, "seek": 436112, "start": 4377.44, "end": 4384.4, "text": " you know, like this general research program of trying to understand computational complexity", "tokens": [51180, 291, 458, 11, 411, 341, 2674, 2132, 1461, 295, 1382, 281, 1223, 28270, 14024, 51528], "temperature": 0.0, "avg_logprob": -0.1491264482823814, "compression_ratio": 1.5224489795918368, "no_speech_prob": 0.0011691934196278453}, {"id": 833, "seek": 436112, "start": 4384.4, "end": 4388.24, "text": " and algorithmic complexity and interplays between observers and systems through this", "tokens": [51528, 293, 9284, 299, 14024, 293, 728, 45755, 1296, 48090, 293, 3652, 807, 341, 51720], "temperature": 0.0, "avg_logprob": -0.1491264482823814, "compression_ratio": 1.5224489795918368, "no_speech_prob": 0.0011691934196278453}, {"id": 834, "seek": 438824, "start": 4388.24, "end": 4394.5599999999995, "text": " category theoretic lens. That's a major thing which I started on, say, about maybe a couple", "tokens": [50364, 7719, 14308, 299, 6765, 13, 663, 311, 257, 2563, 551, 597, 286, 1409, 322, 11, 584, 11, 466, 1310, 257, 1916, 50680], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 835, "seek": 438824, "start": 4394.5599999999995, "end": 4398.24, "text": " of years ago. I mean, in some form, I've been working on it for a long time, but this more", "tokens": [50680, 295, 924, 2057, 13, 286, 914, 11, 294, 512, 1254, 11, 286, 600, 668, 1364, 322, 309, 337, 257, 938, 565, 11, 457, 341, 544, 50864], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 836, "seek": 438824, "start": 4398.24, "end": 4402.48, "text": " recent perspective on it is maybe a couple of years old. But for the various reasons over the", "tokens": [50864, 5162, 4585, 322, 309, 307, 1310, 257, 1916, 295, 924, 1331, 13, 583, 337, 264, 3683, 4112, 670, 264, 51076], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 837, "seek": 438824, "start": 4402.48, "end": 4408.639999999999, "text": " last year or so, I've kind of put that to rest and I've been focused on these much more physics", "tokens": [51076, 1036, 1064, 420, 370, 11, 286, 600, 733, 295, 829, 300, 281, 1472, 293, 286, 600, 668, 5178, 322, 613, 709, 544, 10649, 51384], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 838, "seek": 438824, "start": 4408.639999999999, "end": 4412.5599999999995, "text": " oriented questions about discrete space time and understanding things like, you know, how do", "tokens": [51384, 21841, 1651, 466, 27706, 1901, 565, 293, 3701, 721, 411, 11, 291, 458, 11, 577, 360, 51580], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 839, "seek": 438824, "start": 4412.5599999999995, "end": 4416.4, "text": " black holes work and how does accretion work in discrete space time, which is also very important", "tokens": [51580, 2211, 8118, 589, 293, 577, 775, 1317, 1505, 313, 589, 294, 27706, 1901, 565, 11, 597, 307, 611, 588, 1021, 51772], "temperature": 0.0, "avg_logprob": -0.14530655957650448, "compression_ratio": 1.7873015873015874, "no_speech_prob": 0.007224611937999725}, {"id": 840, "seek": 441640, "start": 4416.4, "end": 4423.04, "text": " and very exciting. But I've sort of slightly been missing these more abstract directions. And so", "tokens": [50364, 293, 588, 4670, 13, 583, 286, 600, 1333, 295, 4748, 668, 5361, 613, 544, 12649, 11095, 13, 400, 370, 50696], "temperature": 0.0, "avg_logprob": -0.1106434627012773, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00033502274891361594}, {"id": 841, "seek": 441640, "start": 4423.04, "end": 4428.4, "text": " I have maybe one or two major physics related things that I need to finish off and then I", "tokens": [50696, 286, 362, 1310, 472, 420, 732, 2563, 10649, 4077, 721, 300, 286, 643, 281, 2413, 766, 293, 550, 286, 50964], "temperature": 0.0, "avg_logprob": -0.1106434627012773, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00033502274891361594}, {"id": 842, "seek": 441640, "start": 4428.4, "end": 4433.2, "text": " really want to go back to this to the greatest extent possible. And yeah, I mean, so one thing", "tokens": [50964, 534, 528, 281, 352, 646, 281, 341, 281, 264, 6636, 8396, 1944, 13, 400, 1338, 11, 286, 914, 11, 370, 472, 551, 51204], "temperature": 0.0, "avg_logprob": -0.1106434627012773, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00033502274891361594}, {"id": 843, "seek": 441640, "start": 4433.2, "end": 4441.44, "text": " is that's quite clear is that there's great interplay between this formalism and existing", "tokens": [51204, 307, 300, 311, 1596, 1850, 307, 300, 456, 311, 869, 728, 2858, 1296, 341, 9860, 1434, 293, 6741, 51616], "temperature": 0.0, "avg_logprob": -0.1106434627012773, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00033502274891361594}, {"id": 844, "seek": 444144, "start": 4441.44, "end": 4445.12, "text": " theories of computational and algorithmic complexity. So in particular, you know, so", "tokens": [50364, 13667, 295, 28270, 293, 9284, 299, 14024, 13, 407, 294, 1729, 11, 291, 458, 11, 370, 50548], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 845, "seek": 444144, "start": 4445.759999999999, "end": 4450.96, "text": " one very basic example is I mentioned before that, you know, you have this kind of coherence", "tokens": [50580, 472, 588, 3875, 1365, 307, 286, 2835, 949, 300, 11, 291, 458, 11, 291, 362, 341, 733, 295, 26528, 655, 50840], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 846, "seek": 444144, "start": 4450.96, "end": 4454.5599999999995, "text": " between these two different algebraic structures between your the operation of", "tokens": [50840, 1296, 613, 732, 819, 21989, 299, 9227, 1296, 428, 264, 6916, 295, 51020], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 847, "seek": 444144, "start": 4454.5599999999995, "end": 4459.2, "text": " time like composition versus the operation of kind of parallel composition. And these two", "tokens": [51020, 565, 411, 12686, 5717, 264, 6916, 295, 733, 295, 8952, 12686, 13, 400, 613, 732, 51252], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 848, "seek": 444144, "start": 4459.2, "end": 4464.0, "text": " algebraic structures are in general related, although the precise conditions that relate", "tokens": [51252, 21989, 299, 9227, 366, 294, 2674, 4077, 11, 4878, 264, 13600, 4487, 300, 10961, 51492], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 849, "seek": 444144, "start": 4464.0, "end": 4467.44, "text": " them are not clear. And that's partly what we're trying to what we're trying to understand.", "tokens": [51492, 552, 366, 406, 1850, 13, 400, 300, 311, 17031, 437, 321, 434, 1382, 281, 437, 321, 434, 1382, 281, 1223, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08889927182878767, "compression_ratio": 1.9094202898550725, "no_speech_prob": 0.12221088260412216}, {"id": 850, "seek": 446744, "start": 4468.4, "end": 4474.799999999999, "text": " But it turns out that degenerate cases of that of that question corresponds to unsolved problems", "tokens": [50412, 583, 309, 4523, 484, 300, 40520, 473, 3331, 295, 300, 295, 300, 1168, 23249, 281, 2693, 29110, 2740, 50732], "temperature": 0.0, "avg_logprob": -0.1316796769487097, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.006686845328658819}, {"id": 851, "seek": 446744, "start": 4474.799999999999, "end": 4479.839999999999, "text": " in computational complexity theory. So, for instance, the P bus NP problem can essentially", "tokens": [50732, 294, 28270, 14024, 5261, 13, 407, 11, 337, 5197, 11, 264, 430, 1255, 38611, 1154, 393, 4476, 50984], "temperature": 0.0, "avg_logprob": -0.1316796769487097, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.006686845328658819}, {"id": 852, "seek": 446744, "start": 4479.839999999999, "end": 4483.599999999999, "text": " be recast in these terms that you can recast the P bus NP problem is the question about", "tokens": [50984, 312, 850, 525, 294, 613, 2115, 300, 291, 393, 850, 525, 264, 430, 1255, 38611, 1154, 307, 264, 1168, 466, 51172], "temperature": 0.0, "avg_logprob": -0.1316796769487097, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.006686845328658819}, {"id": 853, "seek": 446744, "start": 4484.719999999999, "end": 4489.839999999999, "text": " is the coherence between the time like composition of computational complexity and", "tokens": [51228, 307, 264, 26528, 655, 1296, 264, 565, 411, 12686, 295, 28270, 14024, 293, 51484], "temperature": 0.0, "avg_logprob": -0.1316796769487097, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.006686845328658819}, {"id": 854, "seek": 446744, "start": 4489.839999999999, "end": 4493.679999999999, "text": " the parallel composition of computational complexity, which are what P and NP respectively", "tokens": [51484, 264, 8952, 12686, 295, 28270, 14024, 11, 597, 366, 437, 430, 293, 38611, 25009, 51676], "temperature": 0.0, "avg_logprob": -0.1316796769487097, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.006686845328658819}, {"id": 855, "seek": 449368, "start": 4493.76, "end": 4498.56, "text": " are really about, it are those coherence conditions, the strictest they can be,", "tokens": [50368, 366, 534, 466, 11, 309, 366, 729, 26528, 655, 4487, 11, 264, 10910, 377, 436, 393, 312, 11, 50608], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 856, "seek": 449368, "start": 4499.360000000001, "end": 4504.08, "text": " which would be the case that the P equals NP, or are they somehow more lax, which would be the", "tokens": [50648, 597, 576, 312, 264, 1389, 300, 264, 430, 6915, 38611, 11, 420, 366, 436, 6063, 544, 635, 87, 11, 597, 576, 312, 264, 50884], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 857, "seek": 449368, "start": 4504.08, "end": 4509.280000000001, "text": " case that P does not equal NP. And so there's, you know, that's that's one thing that we kind", "tokens": [50884, 1389, 300, 430, 775, 406, 2681, 38611, 13, 400, 370, 456, 311, 11, 291, 458, 11, 300, 311, 300, 311, 472, 551, 300, 321, 733, 51144], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 858, "seek": 449368, "start": 4509.280000000001, "end": 4512.8, "text": " of were already investigated, but it's clear that a whole bunch of questions about, you know,", "tokens": [51144, 295, 645, 1217, 30070, 11, 457, 309, 311, 1850, 300, 257, 1379, 3840, 295, 1651, 466, 11, 291, 458, 11, 51320], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 859, "seek": 449368, "start": 4513.76, "end": 4518.0, "text": " how the time complexity and space complexity trade off or how to come over of complexity and", "tokens": [51368, 577, 264, 565, 14024, 293, 1901, 14024, 4923, 766, 420, 577, 281, 808, 670, 295, 14024, 293, 51580], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 860, "seek": 449368, "start": 4518.0, "end": 4522.08, "text": " time complexity trade off, these, these are questions which can be recast in this kind of", "tokens": [51580, 565, 14024, 4923, 766, 11, 613, 11, 613, 366, 1651, 597, 393, 312, 850, 525, 294, 341, 733, 295, 51784], "temperature": 0.0, "avg_logprob": -0.15672926341786103, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.029729964211583138}, {"id": 861, "seek": 452208, "start": 4522.08, "end": 4528.24, "text": " more algebraic category, theoretic lens, and, and, and will hopefully give insight into this", "tokens": [50364, 544, 21989, 299, 7719, 11, 14308, 299, 6765, 11, 293, 11, 293, 11, 293, 486, 4696, 976, 11269, 666, 341, 50672], "temperature": 0.0, "avg_logprob": -0.16709762491205687, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.006189091131091118}, {"id": 862, "seek": 452208, "start": 4528.24, "end": 4532.48, "text": " general program of trying to understand observers and their relationship to the world. And those", "tokens": [50672, 2674, 1461, 295, 1382, 281, 1223, 48090, 293, 641, 2480, 281, 264, 1002, 13, 400, 729, 50884], "temperature": 0.0, "avg_logprob": -0.16709762491205687, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.006189091131091118}, {"id": 863, "seek": 452208, "start": 4532.48, "end": 4537.28, "text": " are kind of major, well, with any luck, those are major theorems that I hope we'll be able to", "tokens": [50884, 366, 733, 295, 2563, 11, 731, 11, 365, 604, 3668, 11, 729, 366, 2563, 10299, 2592, 300, 286, 1454, 321, 603, 312, 1075, 281, 51124], "temperature": 0.0, "avg_logprob": -0.16709762491205687, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.006189091131091118}, {"id": 864, "seek": 452208, "start": 4537.28, "end": 4544.64, "text": " prove at some point in 2024. That's that's that's that's awesome. And it makes me think about", "tokens": [51124, 7081, 412, 512, 935, 294, 45237, 13, 663, 311, 300, 311, 300, 311, 300, 311, 3476, 13, 400, 309, 1669, 385, 519, 466, 51492], "temperature": 0.0, "avg_logprob": -0.16709762491205687, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.006189091131091118}, {"id": 865, "seek": 454464, "start": 4544.64, "end": 4551.76, "text": " parallel, more nest mates, more CPU threads, deeper in times, more sequential, more planning,", "tokens": [50364, 8952, 11, 544, 15646, 31488, 11, 544, 13199, 19314, 11, 7731, 294, 1413, 11, 544, 42881, 11, 544, 5038, 11, 50720], "temperature": 0.0, "avg_logprob": -0.16121421541486466, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.13290055096149445}, {"id": 866, "seek": 454464, "start": 4551.76, "end": 4557.360000000001, "text": " and more cognitive single monolithic agent. And then the kind of question is like, can anything", "tokens": [50720, 293, 544, 15605, 2167, 1108, 42878, 9461, 13, 400, 550, 264, 733, 295, 1168, 307, 411, 11, 393, 1340, 51000], "temperature": 0.0, "avg_logprob": -0.16121421541486466, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.13290055096149445}, {"id": 867, "seek": 454464, "start": 4557.360000000001, "end": 4565.200000000001, "text": " that a single agent, mega matrix could do, cannot be decomposable at space advantage,", "tokens": [51000, 300, 257, 2167, 9461, 11, 17986, 8141, 727, 360, 11, 2644, 312, 22867, 329, 712, 412, 1901, 5002, 11, 51392], "temperature": 0.0, "avg_logprob": -0.16121421541486466, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.13290055096149445}, {"id": 868, "seek": 454464, "start": 4565.200000000001, "end": 4571.4400000000005, "text": " or even at space disadvantage in decomposed it into a single time step operation?", "tokens": [51392, 420, 754, 412, 1901, 24292, 294, 22867, 1744, 309, 666, 257, 2167, 565, 1823, 6916, 30, 51704], "temperature": 0.0, "avg_logprob": -0.16121421541486466, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.13290055096149445}, {"id": 869, "seek": 457144, "start": 4572.32, "end": 4577.28, "text": " Yeah, that's a super important question. And one that, you know, with the possible exception", "tokens": [50408, 865, 11, 300, 311, 257, 1687, 1021, 1168, 13, 400, 472, 300, 11, 291, 458, 11, 365, 264, 1944, 11183, 50656], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 870, "seek": 457144, "start": 4577.28, "end": 4582.879999999999, "text": " of this community, not many people have asked, right? I mean, so that's something which comes", "tokens": [50656, 295, 341, 1768, 11, 406, 867, 561, 362, 2351, 11, 558, 30, 286, 914, 11, 370, 300, 311, 746, 597, 1487, 50936], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 871, "seek": 457144, "start": 4582.879999999999, "end": 4588.5599999999995, "text": " up in quantum computing, right? So a lot of the hype around quantum computation comes from these", "tokens": [50936, 493, 294, 13018, 15866, 11, 558, 30, 407, 257, 688, 295, 264, 24144, 926, 13018, 24903, 1487, 490, 613, 51220], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 872, "seek": 457144, "start": 4588.5599999999995, "end": 4592.96, "text": " theoretical speedups that derive from the fact that you're able to, you know, you're able to", "tokens": [51220, 20864, 3073, 7528, 300, 28446, 490, 264, 1186, 300, 291, 434, 1075, 281, 11, 291, 458, 11, 291, 434, 1075, 281, 51440], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 873, "seek": 457144, "start": 4592.96, "end": 4597.679999999999, "text": " support these super positions of different, you know, where, you know, each, that each state of", "tokens": [51440, 1406, 613, 1687, 8432, 295, 819, 11, 291, 458, 11, 689, 11, 291, 458, 11, 1184, 11, 300, 1184, 1785, 295, 51676], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 874, "seek": 457144, "start": 4597.679999999999, "end": 4601.12, "text": " your data structure corresponds to a different eigenstate, and you're able to evolve some super", "tokens": [51676, 428, 1412, 3877, 23249, 281, 257, 819, 10446, 15406, 11, 293, 291, 434, 1075, 281, 16693, 512, 1687, 51848], "temperature": 0.0, "avg_logprob": -0.09529871015406366, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.0007090301369316876}, {"id": 875, "seek": 460112, "start": 4601.36, "end": 4604.72, "text": " superposition of those eigenstates. But then at the end, you have to actually come to a", "tokens": [50376, 1687, 38078, 295, 729, 10446, 372, 1024, 13, 583, 550, 412, 264, 917, 11, 291, 362, 281, 767, 808, 281, 257, 50544], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 876, "seek": 460112, "start": 4604.72, "end": 4608.32, "text": " definite conclusion about what the answer is, you have to perform some measurement operation.", "tokens": [50544, 25131, 10063, 466, 437, 264, 1867, 307, 11, 291, 362, 281, 2042, 512, 13160, 6916, 13, 50724], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 877, "seek": 460112, "start": 4608.32, "end": 4612.4, "text": " And that measurement operation is lossy, it's often non deterministic, you, you know, you often", "tokens": [50724, 400, 300, 13160, 6916, 307, 4470, 88, 11, 309, 311, 2049, 2107, 15957, 3142, 11, 291, 11, 291, 458, 11, 291, 2049, 50928], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 878, "seek": 460112, "start": 4612.4, "end": 4618.0, "text": " have to repeat it multiple times. And, you know, it's becoming increasingly clear that for a large", "tokens": [50928, 362, 281, 7149, 309, 3866, 1413, 13, 400, 11, 291, 458, 11, 309, 311, 5617, 12980, 1850, 300, 337, 257, 2416, 51208], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 879, "seek": 460112, "start": 4618.0, "end": 4622.64, "text": " class of operations that were previously thought to have quantum advantage, the additional complexity", "tokens": [51208, 1508, 295, 7705, 300, 645, 8046, 1194, 281, 362, 13018, 5002, 11, 264, 4497, 14024, 51440], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 880, "seek": 460112, "start": 4622.64, "end": 4626.5599999999995, "text": " of the measurement step really kills any quantum advantage that you may have had that you know,", "tokens": [51440, 295, 264, 13160, 1823, 534, 14563, 604, 13018, 5002, 300, 291, 815, 362, 632, 300, 291, 458, 11, 51636], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 881, "seek": 460112, "start": 4626.5599999999995, "end": 4631.04, "text": " you get some advantage by doing unitary evolution. But then you lose all of it by having to do", "tokens": [51636, 291, 483, 512, 5002, 538, 884, 517, 4109, 9303, 13, 583, 550, 291, 3624, 439, 295, 309, 538, 1419, 281, 360, 51860], "temperature": 0.0, "avg_logprob": -0.09332319259643555, "compression_ratio": 1.9676470588235293, "no_speech_prob": 0.0025495775043964386}, {"id": 882, "seek": 463104, "start": 4631.04, "end": 4636.16, "text": " the submission projection at the end. And that's really a story of, again, this interplay between", "tokens": [50364, 264, 23689, 22743, 412, 264, 917, 13, 400, 300, 311, 534, 257, 1657, 295, 11, 797, 11, 341, 728, 2858, 1296, 50620], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 883, "seek": 463104, "start": 4636.16, "end": 4641.04, "text": " the time complexity saving of doing a multi doing a computation of doing a multi computation in", "tokens": [50620, 264, 565, 14024, 6816, 295, 884, 257, 4825, 884, 257, 24903, 295, 884, 257, 4825, 24903, 294, 50864], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 884, "seek": 463104, "start": 4641.04, "end": 4645.92, "text": " parallel, versus the loss that comes from the complexity of the equivalence function that", "tokens": [50864, 8952, 11, 5717, 264, 4470, 300, 1487, 490, 264, 14024, 295, 264, 9052, 655, 2445, 300, 51108], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 885, "seek": 463104, "start": 4645.92, "end": 4650.64, "text": " you need to apply in order to get to some definite conclusion about what the, you know, about what", "tokens": [51108, 291, 643, 281, 3079, 294, 1668, 281, 483, 281, 512, 25131, 10063, 466, 437, 264, 11, 291, 458, 11, 466, 437, 51344], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 886, "seek": 463104, "start": 4650.64, "end": 4655.04, "text": " happened because, you know, ultimately, you need to somehow collapse that that that directed graph", "tokens": [51344, 2011, 570, 11, 291, 458, 11, 6284, 11, 291, 643, 281, 6063, 15584, 300, 300, 300, 12898, 4295, 51564], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 887, "seek": 463104, "start": 4655.04, "end": 4659.36, "text": " into a single thread of time in order to be able to have some coherent representation of what happened.", "tokens": [51564, 666, 257, 2167, 7207, 295, 565, 294, 1668, 281, 312, 1075, 281, 362, 512, 36239, 10290, 295, 437, 2011, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10560962346595103, "compression_ratio": 1.9435215946843853, "no_speech_prob": 0.0015968652442097664}, {"id": 888, "seek": 465936, "start": 4660.16, "end": 4663.839999999999, "text": " And so again, understand that, you know, that's a place where understanding these, you know,", "tokens": [50404, 400, 370, 797, 11, 1223, 300, 11, 291, 458, 11, 300, 311, 257, 1081, 689, 3701, 613, 11, 291, 458, 11, 50588], "temperature": 0.0, "avg_logprob": -0.14868891866583572, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0018091091187670827}, {"id": 889, "seek": 465936, "start": 4663.839999999999, "end": 4668.639999999999, "text": " the these tradeoffs will become very important. And as I say, in some limiting case that that gives", "tokens": [50588, 264, 613, 4923, 19231, 486, 1813, 588, 1021, 13, 400, 382, 286, 584, 11, 294, 512, 22083, 1389, 300, 300, 2709, 50828], "temperature": 0.0, "avg_logprob": -0.14868891866583572, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0018091091187670827}, {"id": 890, "seek": 465936, "start": 4669.28, "end": 4673.2, "text": " some perspective on your question, Daniel, which is, which I agree is a very interesting question", "tokens": [50860, 512, 4585, 322, 428, 1168, 11, 8033, 11, 597, 307, 11, 597, 286, 3986, 307, 257, 588, 1880, 1168, 51056], "temperature": 0.0, "avg_logprob": -0.14868891866583572, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0018091091187670827}, {"id": 891, "seek": 465936, "start": 4673.2, "end": 4679.599999999999, "text": " about, you know, in principle, we know that anything that a deterministic Turing machine", "tokens": [51056, 466, 11, 291, 458, 11, 294, 8665, 11, 321, 458, 300, 1340, 300, 257, 15957, 3142, 314, 1345, 3479, 51376], "temperature": 0.0, "avg_logprob": -0.14868891866583572, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0018091091187670827}, {"id": 892, "seek": 465936, "start": 4679.599999999999, "end": 4684.16, "text": " can do a non deterministic Turing machine can do and vice versa with some speed up or slow down.", "tokens": [51376, 393, 360, 257, 2107, 15957, 3142, 314, 1345, 3479, 393, 360, 293, 11964, 25650, 365, 512, 3073, 493, 420, 2964, 760, 13, 51604], "temperature": 0.0, "avg_logprob": -0.14868891866583572, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0018091091187670827}, {"id": 893, "seek": 468416, "start": 4685.12, "end": 4690.48, "text": " But that statement, which is a classic result in, you know, in computability theory, neglects", "tokens": [50412, 583, 300, 5629, 11, 597, 307, 257, 7230, 1874, 294, 11, 291, 458, 11, 294, 2807, 2310, 5261, 11, 17745, 82, 50680], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 894, "seek": 468416, "start": 4690.48, "end": 4693.84, "text": " all consideration of the equivalence function. So there may be cases where the equivalence", "tokens": [50680, 439, 12381, 295, 264, 9052, 655, 2445, 13, 407, 456, 815, 312, 3331, 689, 264, 9052, 655, 50848], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 895, "seek": 468416, "start": 4693.84, "end": 4699.04, "text": " function is so complex, that essentially, you know, that to do state equivalence becomes", "tokens": [50848, 2445, 307, 370, 3997, 11, 300, 4476, 11, 291, 458, 11, 300, 281, 360, 1785, 9052, 655, 3643, 51108], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 896, "seek": 468416, "start": 4699.04, "end": 4703.2, "text": " undecidable. And so in that case, you have a scenario where actually, you know, you've got a", "tokens": [51108, 674, 3045, 38089, 13, 400, 370, 294, 300, 1389, 11, 291, 362, 257, 9005, 689, 767, 11, 291, 458, 11, 291, 600, 658, 257, 51316], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 897, "seek": 468416, "start": 4703.2, "end": 4708.72, "text": " multi computational system, but to collapse it to one that's equivalent to a single way system", "tokens": [51316, 4825, 28270, 1185, 11, 457, 281, 15584, 309, 281, 472, 300, 311, 10344, 281, 257, 2167, 636, 1185, 51592], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 898, "seek": 468416, "start": 4708.72, "end": 4712.0, "text": " requires unbounded amounts of computational effort. And so that's so actually they become", "tokens": [51592, 7029, 517, 18767, 292, 11663, 295, 28270, 4630, 13, 400, 370, 300, 311, 370, 767, 436, 1813, 51756], "temperature": 0.0, "avg_logprob": -0.13455198716747668, "compression_ratio": 1.9198606271777003, "no_speech_prob": 0.007340951357036829}, {"id": 899, "seek": 471200, "start": 4712.08, "end": 4718.0, "text": " inequivalent, even though, you know, computability theory says they should be the same. So it's", "tokens": [50368, 294, 12816, 3576, 317, 11, 754, 1673, 11, 291, 458, 11, 2807, 2310, 5261, 1619, 436, 820, 312, 264, 912, 13, 407, 309, 311, 50664], "temperature": 0.0, "avg_logprob": -0.1081695556640625, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.0013243922730907798}, {"id": 900, "seek": 471200, "start": 4718.0, "end": 4723.28, "text": " clear that there's a there's a more rich, more subtle theory that's underlying here that we're", "tokens": [50664, 1850, 300, 456, 311, 257, 456, 311, 257, 544, 4593, 11, 544, 13743, 5261, 300, 311, 14217, 510, 300, 321, 434, 50928], "temperature": 0.0, "avg_logprob": -0.1081695556640625, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.0013243922730907798}, {"id": 901, "seek": 471200, "start": 4723.28, "end": 4726.72, "text": " just beginning to kind of glimpse, and that I hope will, you know, we'll be able to kind of", "tokens": [50928, 445, 2863, 281, 733, 295, 25838, 11, 293, 300, 286, 1454, 486, 11, 291, 458, 11, 321, 603, 312, 1075, 281, 733, 295, 51100], "temperature": 0.0, "avg_logprob": -0.1081695556640625, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.0013243922730907798}, {"id": 902, "seek": 471200, "start": 4726.72, "end": 4730.0, "text": " to prove some new limited results about soon, once we understand it a bit better.", "tokens": [51100, 281, 7081, 512, 777, 5567, 3542, 466, 2321, 11, 1564, 321, 1223, 309, 257, 857, 1101, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1081695556640625, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.0013243922730907798}, {"id": 903, "seek": 471200, "start": 4731.36, "end": 4737.52, "text": " Awesome. And I think definitely a special shout out to all of our colleagues on either like the", "tokens": [51332, 10391, 13, 400, 286, 519, 2138, 257, 2121, 8043, 484, 281, 439, 295, 527, 7734, 322, 2139, 411, 264, 51640], "temperature": 0.0, "avg_logprob": -0.1081695556640625, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.0013243922730907798}, {"id": 904, "seek": 473752, "start": 4737.52, "end": 4744.64, "text": " Wolfram and or active inference side, because we've seen few if any active inference models", "tokens": [50364, 16634, 2356, 293, 420, 4967, 38253, 1252, 11, 570, 321, 600, 1612, 1326, 498, 604, 4967, 38253, 5245, 50720], "temperature": 0.0, "avg_logprob": -0.10021333694458008, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.005059520713984966}, {"id": 905, "seek": 473752, "start": 4745.52, "end": 4752.64, "text": " phrased analytically or computationally with the Wolfram technology from studying complexity", "tokens": [50764, 7636, 1937, 10783, 984, 420, 24903, 379, 365, 264, 16634, 2356, 2899, 490, 7601, 14024, 51120], "temperature": 0.0, "avg_logprob": -0.10021333694458008, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.005059520713984966}, {"id": 906, "seek": 473752, "start": 4752.64, "end": 4759.200000000001, "text": " in other areas. It's really clear to see how productive and powerful the software and the", "tokens": [51120, 294, 661, 3179, 13, 467, 311, 534, 1850, 281, 536, 577, 13304, 293, 4005, 264, 4722, 293, 264, 51448], "temperature": 0.0, "avg_logprob": -0.10021333694458008, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.005059520713984966}, {"id": 907, "seek": 473752, "start": 4759.200000000001, "end": 4765.6, "text": " tools can be and changing and growing every day. So it's really interesting, maybe someone can", "tokens": [51448, 3873, 393, 312, 293, 4473, 293, 4194, 633, 786, 13, 407, 309, 311, 534, 1880, 11, 1310, 1580, 393, 51768], "temperature": 0.0, "avg_logprob": -0.10021333694458008, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.005059520713984966}, {"id": 908, "seek": 476560, "start": 4766.56, "end": 4773.200000000001, "text": " if they're listening this far in, like, go from one side to the other and back or make a Wolfram", "tokens": [50412, 498, 436, 434, 4764, 341, 1400, 294, 11, 411, 11, 352, 490, 472, 1252, 281, 264, 661, 293, 646, 420, 652, 257, 16634, 2356, 50744], "temperature": 0.0, "avg_logprob": -0.13089410146077474, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.003482869127765298}, {"id": 909, "seek": 476560, "start": 4773.200000000001, "end": 4781.04, "text": " active inference model, or do some other kind of combination, because it's very fruitful territory.", "tokens": [50744, 4967, 38253, 2316, 11, 420, 360, 512, 661, 733, 295, 6562, 11, 570, 309, 311, 588, 49795, 11360, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13089410146077474, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.003482869127765298}, {"id": 910, "seek": 476560, "start": 4781.92, "end": 4791.4400000000005, "text": " And we know that our elders have already spoken, they've okayed it. No, but really, it's so", "tokens": [51180, 400, 321, 458, 300, 527, 22737, 362, 1217, 10759, 11, 436, 600, 1392, 292, 309, 13, 883, 11, 457, 534, 11, 309, 311, 370, 51656], "temperature": 0.0, "avg_logprob": -0.13089410146077474, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.003482869127765298}, {"id": 911, "seek": 479144, "start": 4792.24, "end": 4799.28, "text": " rich with connections here between the areas that we're all studying and feeling like converging", "tokens": [50404, 4593, 365, 9271, 510, 1296, 264, 3179, 300, 321, 434, 439, 7601, 293, 2633, 411, 9652, 3249, 50756], "temperature": 0.0, "avg_logprob": -0.11093358735780458, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00039191567339003086}, {"id": 912, "seek": 479144, "start": 4800.0, "end": 4804.879999999999, "text": " on many common places to scaffold and jump off from together.", "tokens": [50792, 322, 867, 2689, 3190, 281, 44094, 293, 3012, 766, 490, 1214, 13, 51036], "temperature": 0.0, "avg_logprob": -0.11093358735780458, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00039191567339003086}, {"id": 913, "seek": 479144, "start": 4807.04, "end": 4813.44, "text": " Yeah, I agree. And I mean, at least the kinds of things that we were discussing here about,", "tokens": [51144, 865, 11, 286, 3986, 13, 400, 286, 914, 11, 412, 1935, 264, 3685, 295, 721, 300, 321, 645, 10850, 510, 466, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11093358735780458, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00039191567339003086}, {"id": 914, "seek": 479144, "start": 4813.44, "end": 4817.36, "text": " you know, speculative execution and behavior formation through free energy principle and", "tokens": [51464, 291, 458, 11, 49415, 15058, 293, 5223, 11723, 807, 1737, 2281, 8665, 293, 51660], "temperature": 0.0, "avg_logprob": -0.11093358735780458, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00039191567339003086}, {"id": 915, "seek": 481736, "start": 4817.36, "end": 4821.679999999999, "text": " so on, those things should be relatively easy to implement in the framework that's already been", "tokens": [50364, 370, 322, 11, 729, 721, 820, 312, 7226, 1858, 281, 4445, 294, 264, 8388, 300, 311, 1217, 668, 50580], "temperature": 0.0, "avg_logprob": -0.11686458587646484, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0199771448969841}, {"id": 916, "seek": 481736, "start": 4821.679999999999, "end": 4827.36, "text": " developed here. I mean, so that's just a question of just implementing some kind of computation of", "tokens": [50580, 4743, 510, 13, 286, 914, 11, 370, 300, 311, 445, 257, 1168, 295, 445, 18114, 512, 733, 295, 24903, 295, 50864], "temperature": 0.0, "avg_logprob": -0.11686458587646484, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0199771448969841}, {"id": 917, "seek": 481736, "start": 4827.36, "end": 4832.719999999999, "text": " expected free energy and using that to weight multiway paths in the speculative execution model.", "tokens": [50864, 5176, 1737, 2281, 293, 1228, 300, 281, 3364, 4825, 676, 14518, 294, 264, 49415, 15058, 2316, 13, 51132], "temperature": 0.0, "avg_logprob": -0.11686458587646484, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0199771448969841}, {"id": 918, "seek": 481736, "start": 4833.839999999999, "end": 4840.48, "text": " So at least the beginnings of that implementation, I think the path is pretty clear. And we will", "tokens": [51188, 407, 412, 1935, 264, 37281, 295, 300, 11420, 11, 286, 519, 264, 3100, 307, 1238, 1850, 13, 400, 321, 486, 51520], "temperature": 0.0, "avg_logprob": -0.11686458587646484, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0199771448969841}, {"id": 919, "seek": 481736, "start": 4840.48, "end": 4844.719999999999, "text": " probably end up doing at some point in the future anyway, as part of other research.", "tokens": [51520, 1391, 917, 493, 884, 412, 512, 935, 294, 264, 2027, 4033, 11, 382, 644, 295, 661, 2132, 13, 51732], "temperature": 0.0, "avg_logprob": -0.11686458587646484, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0199771448969841}, {"id": 920, "seek": 484472, "start": 4845.68, "end": 4850.4800000000005, "text": " So yeah, so I agree. It's a very exciting kind of point of interface.", "tokens": [50412, 407, 1338, 11, 370, 286, 3986, 13, 467, 311, 257, 588, 4670, 733, 295, 935, 295, 9226, 13, 50652], "temperature": 0.0, "avg_logprob": -0.10753815432628953, "compression_ratio": 1.511737089201878, "no_speech_prob": 0.008312368765473366}, {"id": 921, "seek": 484472, "start": 4851.04, "end": 4856.96, "text": " Yeah, well, I hope that we can stay in touch if you ever want to come back for a", "tokens": [50680, 865, 11, 731, 11, 286, 1454, 300, 321, 393, 1754, 294, 2557, 498, 291, 1562, 528, 281, 808, 646, 337, 257, 50976], "temperature": 0.0, "avg_logprob": -0.10753815432628953, "compression_ratio": 1.511737089201878, "no_speech_prob": 0.008312368765473366}, {"id": 922, "seek": 484472, "start": 4858.16, "end": 4864.4800000000005, "text": " 009.2, or if we want to even facilitate some kind of working group or some connections", "tokens": [51036, 7143, 24, 13, 17, 11, 420, 498, 321, 528, 281, 754, 20207, 512, 733, 295, 1364, 1594, 420, 512, 9271, 51352], "temperature": 0.0, "avg_logprob": -0.10753815432628953, "compression_ratio": 1.511737089201878, "no_speech_prob": 0.008312368765473366}, {"id": 923, "seek": 484472, "start": 4865.04, "end": 4870.400000000001, "text": " to really strengthen and like include the participation of more people in this super", "tokens": [51380, 281, 534, 17045, 293, 411, 4090, 264, 13487, 295, 544, 561, 294, 341, 1687, 51648], "temperature": 0.0, "avg_logprob": -0.10753815432628953, "compression_ratio": 1.511737089201878, "no_speech_prob": 0.008312368765473366}, {"id": 924, "seek": 487040, "start": 4870.4, "end": 4875.36, "text": " exciting area, that would be amazing. Yeah, that sounds fun. Let's let's try and set something up.", "tokens": [50364, 4670, 1859, 11, 300, 576, 312, 2243, 13, 865, 11, 300, 3263, 1019, 13, 961, 311, 718, 311, 853, 293, 992, 746, 493, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14986901534231087, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.017703557386994362}, {"id": 925, "seek": 487040, "start": 4876.16, "end": 4882.879999999999, "text": " Well, Dave, first penultimate comments, then Jonathan, you can have the kind of last comments.", "tokens": [50652, 1042, 11, 11017, 11, 700, 3435, 723, 2905, 3053, 11, 550, 15471, 11, 291, 393, 362, 264, 733, 295, 1036, 3053, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14986901534231087, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.017703557386994362}, {"id": 926, "seek": 487040, "start": 4883.679999999999, "end": 4891.12, "text": " Yes, I hope you do get to continue on the Wolfram physics side to think about a more general notion", "tokens": [51028, 1079, 11, 286, 1454, 291, 360, 483, 281, 2354, 322, 264, 16634, 2356, 10649, 1252, 281, 519, 466, 257, 544, 2674, 10710, 51400], "temperature": 0.0, "avg_logprob": -0.14986901534231087, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.017703557386994362}, {"id": 927, "seek": 487040, "start": 4891.12, "end": 4897.839999999999, "text": " of what these ultimate things are. Are they observers or does that already prejudice the case", "tokens": [51400, 295, 437, 613, 9705, 721, 366, 13, 2014, 436, 48090, 420, 775, 300, 1217, 34260, 264, 1389, 51736], "temperature": 0.0, "avg_logprob": -0.14986901534231087, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.017703557386994362}, {"id": 928, "seek": 489784, "start": 4897.84, "end": 4905.360000000001, "text": " of what you might find if you call them workers or actors or, you know, go back and ask Stuart", "tokens": [50364, 295, 437, 291, 1062, 915, 498, 291, 818, 552, 5600, 420, 10037, 420, 11, 291, 458, 11, 352, 646, 293, 1029, 36236, 50740], "temperature": 0.0, "avg_logprob": -0.1542719567176139, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0024716248735785484}, {"id": 929, "seek": 489784, "start": 4905.360000000001, "end": 4913.52, "text": " Kaufman, what must a mind do to earn its way in the world? Let's make this keep happening. Thank you.", "tokens": [50740, 44590, 1601, 11, 437, 1633, 257, 1575, 360, 281, 6012, 1080, 636, 294, 264, 1002, 30, 961, 311, 652, 341, 1066, 2737, 13, 1044, 291, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1542719567176139, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0024716248735785484}, {"id": 930, "seek": 489784, "start": 4914.400000000001, "end": 4919.92, "text": " Thank you, Dave, for suggesting it also. It was a great suggestion. Jonathan?", "tokens": [51192, 1044, 291, 11, 11017, 11, 337, 18094, 309, 611, 13, 467, 390, 257, 869, 16541, 13, 15471, 30, 51468], "temperature": 0.0, "avg_logprob": -0.1542719567176139, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0024716248735785484}, {"id": 931, "seek": 489784, "start": 4920.88, "end": 4926.72, "text": " No, I think that's a fantastic note to end on. I mean, in a sense, this idea that we should", "tokens": [51516, 883, 11, 286, 519, 300, 311, 257, 5456, 3637, 281, 917, 322, 13, 286, 914, 11, 294, 257, 2020, 11, 341, 1558, 300, 321, 820, 51808], "temperature": 0.0, "avg_logprob": -0.1542719567176139, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0024716248735785484}, {"id": 932, "seek": 492672, "start": 4926.72, "end": 4930.240000000001, "text": " start to move, I mean, so it's okay, big picture for a moment, like", "tokens": [50364, 722, 281, 1286, 11, 286, 914, 11, 370, 309, 311, 1392, 11, 955, 3036, 337, 257, 1623, 11, 411, 50540], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 933, "seek": 492672, "start": 4931.84, "end": 4936.240000000001, "text": " where, you know, this formalism is being developed, the formalism I've described in this", "tokens": [50620, 689, 11, 291, 458, 11, 341, 9860, 1434, 307, 885, 4743, 11, 264, 9860, 1434, 286, 600, 7619, 294, 341, 50840], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 934, "seek": 492672, "start": 4936.96, "end": 4940.96, "text": " in this discussion is being developed, you know, assuming a kind of purely passive observer", "tokens": [50876, 294, 341, 5017, 307, 885, 4743, 11, 291, 458, 11, 11926, 257, 733, 295, 17491, 14975, 27878, 51076], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 935, "seek": 492672, "start": 4940.96, "end": 4944.56, "text": " idealization. And that's already been incredibly difficult, right? This is clear, there's a lot", "tokens": [51076, 7157, 2144, 13, 400, 300, 311, 1217, 668, 6252, 2252, 11, 558, 30, 639, 307, 1850, 11, 456, 311, 257, 688, 51256], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 936, "seek": 492672, "start": 4944.56, "end": 4948.96, "text": " we don't understand at that. But of course, David is right that in a sense, you know, what, you know,", "tokens": [51256, 321, 500, 380, 1223, 412, 300, 13, 583, 295, 1164, 11, 4389, 307, 558, 300, 294, 257, 2020, 11, 291, 458, 11, 437, 11, 291, 458, 11, 51476], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 937, "seek": 492672, "start": 4948.96, "end": 4953.76, "text": " ultimately, we want to start transitioning to a participatory observer model, where you allow", "tokens": [51476, 6284, 11, 321, 528, 281, 722, 33777, 281, 257, 3421, 4745, 27878, 2316, 11, 689, 291, 2089, 51716], "temperature": 0.0, "avg_logprob": -0.1621381265145761, "compression_ratio": 1.758957654723127, "no_speech_prob": 0.003422751557081938}, {"id": 938, "seek": 495376, "start": 4953.76, "end": 4957.280000000001, "text": " for two-way interactions or, you know, higher order interactions between observers and systems", "tokens": [50364, 337, 732, 12, 676, 13280, 420, 11, 291, 458, 11, 2946, 1668, 13280, 1296, 48090, 293, 3652, 50540], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 939, "seek": 495376, "start": 4957.280000000001, "end": 4961.84, "text": " and things that don't just go in one direction. And yeah, you know, in a sense, I view a lot of", "tokens": [50540, 293, 721, 300, 500, 380, 445, 352, 294, 472, 3513, 13, 400, 1338, 11, 291, 458, 11, 294, 257, 2020, 11, 286, 1910, 257, 688, 295, 50768], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 940, "seek": 495376, "start": 4961.84, "end": 4965.6, "text": " what we're trying to do with, you know, trying to nail down these notions of causality, trying to", "tokens": [50768, 437, 321, 434, 1382, 281, 360, 365, 11, 291, 458, 11, 1382, 281, 10173, 760, 613, 35799, 295, 3302, 1860, 11, 1382, 281, 50956], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 941, "seek": 495376, "start": 4965.6, "end": 4969.76, "text": " understand these interplays between different complexity and entropy measures as, you know,", "tokens": [50956, 1223, 613, 728, 45755, 1296, 819, 14024, 293, 30867, 8000, 382, 11, 291, 458, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 942, "seek": 495376, "start": 4969.76, "end": 4974.400000000001, "text": " the necessary groundwork for developing that subsequent theory, right? That does, you know,", "tokens": [51164, 264, 4818, 2727, 1902, 337, 6416, 300, 19962, 5261, 11, 558, 30, 663, 775, 11, 291, 458, 11, 51396], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 943, "seek": 495376, "start": 4974.400000000001, "end": 4979.6, "text": " it's clear that if we want to have a version of the, you know, of this kind of compositional", "tokens": [51396, 309, 311, 1850, 300, 498, 321, 528, 281, 362, 257, 3037, 295, 264, 11, 291, 458, 11, 295, 341, 733, 295, 10199, 2628, 51656], "temperature": 0.0, "avg_logprob": -0.08550275073331945, "compression_ratio": 1.8344155844155845, "no_speech_prob": 0.004064952488988638}, {"id": 944, "seek": 497960, "start": 4979.6, "end": 4984.0, "text": " multi-way formalism that is also compatible with things like second-order cybernetics,", "tokens": [50364, 4825, 12, 676, 9860, 1434, 300, 307, 611, 18218, 365, 721, 411, 1150, 12, 4687, 13411, 7129, 1167, 11, 50584], "temperature": 0.0, "avg_logprob": -0.09749853004843502, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.043880727142095566}, {"id": 945, "seek": 497960, "start": 4984.0, "end": 4988.400000000001, "text": " then, you know, at the very least, we need to have a very coherent notion for what causality is", "tokens": [50584, 550, 11, 291, 458, 11, 412, 264, 588, 1935, 11, 321, 643, 281, 362, 257, 588, 36239, 10710, 337, 437, 3302, 1860, 307, 50804], "temperature": 0.0, "avg_logprob": -0.09749853004843502, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.043880727142095566}, {"id": 946, "seek": 497960, "start": 4988.400000000001, "end": 4993.76, "text": " and, you know, and a robust algebraic description of that that's not going to break or change.", "tokens": [50804, 293, 11, 291, 458, 11, 293, 257, 13956, 21989, 299, 3855, 295, 300, 300, 311, 406, 516, 281, 1821, 420, 1319, 13, 51072], "temperature": 0.0, "avg_logprob": -0.09749853004843502, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.043880727142095566}, {"id": 947, "seek": 497960, "start": 4993.76, "end": 5002.72, "text": " And so I think the nonparticipatory observer model is a useful starting point because it's one that", "tokens": [51072, 400, 370, 286, 519, 264, 2107, 6971, 6537, 4745, 27878, 2316, 307, 257, 4420, 2891, 935, 570, 309, 311, 472, 300, 51520], "temperature": 0.0, "avg_logprob": -0.09749853004843502, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.043880727142095566}, {"id": 948, "seek": 497960, "start": 5002.72, "end": 5008.160000000001, "text": " is just within our grasp of, you know, of being kind of mathematically tractable. And then the", "tokens": [51520, 307, 445, 1951, 527, 21743, 295, 11, 291, 458, 11, 295, 885, 733, 295, 44003, 24207, 712, 13, 400, 550, 264, 51792], "temperature": 0.0, "avg_logprob": -0.09749853004843502, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.043880727142095566}, {"id": 949, "seek": 500816, "start": 5008.16, "end": 5012.32, "text": " hope is that the technology and the ideas and the conceptual structure that we develop for", "tokens": [50364, 1454, 307, 300, 264, 2899, 293, 264, 3487, 293, 264, 24106, 3877, 300, 321, 1499, 337, 50572], "temperature": 0.0, "avg_logprob": -0.12322633128520871, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0011491732439026237}, {"id": 950, "seek": 500816, "start": 5012.32, "end": 5017.36, "text": " understanding that will then, as I say, lay the groundwork for developing something that's more", "tokens": [50572, 3701, 300, 486, 550, 11, 382, 286, 584, 11, 2360, 264, 2727, 1902, 337, 6416, 746, 300, 311, 544, 50824], "temperature": 0.0, "avg_logprob": -0.12322633128520871, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0011491732439026237}, {"id": 951, "seek": 500816, "start": 5017.36, "end": 5024.48, "text": " like what real, you know, active participatory observers do. And, and yeah, I mean, I think I", "tokens": [50824, 411, 437, 957, 11, 291, 458, 11, 4967, 3421, 4745, 48090, 360, 13, 400, 11, 293, 1338, 11, 286, 914, 11, 286, 519, 286, 51180], "temperature": 0.0, "avg_logprob": -0.12322633128520871, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0011491732439026237}, {"id": 952, "seek": 500816, "start": 5024.48, "end": 5029.76, "text": " don't, on the, on the scientific side, I'm not really sure I have any final comments apart from", "tokens": [51180, 500, 380, 11, 322, 264, 11, 322, 264, 8134, 1252, 11, 286, 478, 406, 534, 988, 286, 362, 604, 2572, 3053, 4936, 490, 51444], "temperature": 0.0, "avg_logprob": -0.12322633128520871, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0011491732439026237}, {"id": 953, "seek": 500816, "start": 5029.76, "end": 5033.36, "text": " just, you know, as obvious, this is a, you know, this is still a story that's being, that's being", "tokens": [51444, 445, 11, 291, 458, 11, 382, 6322, 11, 341, 307, 257, 11, 291, 458, 11, 341, 307, 920, 257, 1657, 300, 311, 885, 11, 300, 311, 885, 51624], "temperature": 0.0, "avg_logprob": -0.12322633128520871, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0011491732439026237}, {"id": 954, "seek": 503336, "start": 5033.44, "end": 5039.2, "text": " developed. And, and yeah, as Daniel alluded to, I hope that we can continue to interact and collaborate", "tokens": [50368, 4743, 13, 400, 11, 293, 1338, 11, 382, 8033, 33919, 281, 11, 286, 1454, 300, 321, 393, 2354, 281, 4648, 293, 18338, 50656], "temperature": 0.0, "avg_logprob": -0.14339825230785924, "compression_ratio": 1.7642857142857142, "no_speech_prob": 0.08350968360900879}, {"id": 955, "seek": 503336, "start": 5039.2, "end": 5044.32, "text": " where, where that makes sense. And, and yeah, at the very least, I think, in cooperation of kind", "tokens": [50656, 689, 11, 689, 300, 1669, 2020, 13, 400, 11, 293, 1338, 11, 412, 264, 588, 1935, 11, 286, 519, 11, 294, 14968, 295, 733, 50912], "temperature": 0.0, "avg_logprob": -0.14339825230785924, "compression_ratio": 1.7642857142857142, "no_speech_prob": 0.08350968360900879}, {"id": 956, "seek": 503336, "start": 5044.32, "end": 5049.599999999999, "text": " of these, you know, these active inference models within these discrete time, in the first instance,", "tokens": [50912, 295, 613, 11, 291, 458, 11, 613, 4967, 38253, 5245, 1951, 613, 27706, 565, 11, 294, 264, 700, 5197, 11, 51176], "temperature": 0.0, "avg_logprob": -0.14339825230785924, "compression_ratio": 1.7642857142857142, "no_speech_prob": 0.08350968360900879}, {"id": 957, "seek": 503336, "start": 5049.599999999999, "end": 5054.4, "text": " discrete time, you know, computational frameworks, and allowing things like speculative execution", "tokens": [51176, 27706, 565, 11, 291, 458, 11, 28270, 29834, 11, 293, 8293, 721, 411, 49415, 15058, 51416], "temperature": 0.0, "avg_logprob": -0.14339825230785924, "compression_ratio": 1.7642857142857142, "no_speech_prob": 0.08350968360900879}, {"id": 958, "seek": 503336, "start": 5054.4, "end": 5059.759999999999, "text": " and multi-way path waiting based on free energy estimates. I think that's, you know, that's a,", "tokens": [51416, 293, 4825, 12, 676, 3100, 3806, 2361, 322, 1737, 2281, 20561, 13, 286, 519, 300, 311, 11, 291, 458, 11, 300, 311, 257, 11, 51684], "temperature": 0.0, "avg_logprob": -0.14339825230785924, "compression_ratio": 1.7642857142857142, "no_speech_prob": 0.08350968360900879}, {"id": 959, "seek": 505976, "start": 5060.0, "end": 5065.84, "text": " that's a project that's of obvious mutual interest and, and something that I, that I hope will happen", "tokens": [50376, 300, 311, 257, 1716, 300, 311, 295, 6322, 16917, 1179, 293, 11, 293, 746, 300, 286, 11, 300, 286, 1454, 486, 1051, 50668], "temperature": 0.0, "avg_logprob": -0.2358335801112799, "compression_ratio": 1.5097087378640777, "no_speech_prob": 0.0027946571353822947}, {"id": 960, "seek": 505976, "start": 5065.84, "end": 5072.16, "text": " in the coming months. Thank you. We just speculatively executed active Wolfram inference.", "tokens": [50668, 294, 264, 1348, 2493, 13, 1044, 291, 13, 492, 445, 1608, 425, 19020, 17577, 4967, 16634, 2356, 38253, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2358335801112799, "compression_ratio": 1.5097087378640777, "no_speech_prob": 0.0027946571353822947}, {"id": 961, "seek": 505976, "start": 5073.52, "end": 5078.320000000001, "text": " Basically. Sounds very good. All right. Thank you, Jonathan. Thank you, Dave. Thank you, everyone.", "tokens": [51052, 8537, 13, 14576, 588, 665, 13, 1057, 558, 13, 1044, 291, 11, 15471, 13, 1044, 291, 11, 11017, 13, 1044, 291, 11, 1518, 13, 51292], "temperature": 0.0, "avg_logprob": -0.2358335801112799, "compression_ratio": 1.5097087378640777, "no_speech_prob": 0.0027946571353822947}, {"id": 962, "seek": 505976, "start": 5079.04, "end": 5082.16, "text": " See y'all next time.", "tokens": [51328, 3008, 288, 6, 336, 958, 565, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2358335801112799, "compression_ratio": 1.5097087378640777, "no_speech_prob": 0.0027946571353822947}, {"id": 963, "seek": 508976, "start": 5089.76, "end": 5091.14, "text": " you", "tokens": [50408, 291, 50433], "temperature": 0.0, "avg_logprob": -0.8688643574714661, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9090939164161682}, {"id": 964, "seek": 511976, "start": 5119.76, "end": 5121.14, "text": " you", "tokens": [50408, 291, 50433], "temperature": 0.0, "avg_logprob": -0.6419491767883301, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.7372705936431885}], "language": "en"}