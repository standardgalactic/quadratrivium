WEBVTT

00:00.000 --> 00:11.120
Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll

00:11.120 --> 00:17.520
be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent

00:17.520 --> 00:24.320
reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe,

00:24.320 --> 00:30.800
very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah,

00:30.800 --> 00:35.440
I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art

00:35.440 --> 00:43.280
language models into more probabilistic machinery and what that might bias. All right. Let's get

00:43.280 --> 00:49.120
right into it. All right. So some of the things that I'm hoping to discuss today include why we

00:49.120 --> 00:54.240
might use a language model for something that isn't long-form text generation and how we might do

00:54.240 --> 01:00.640
that, and then motivate a little bit kind of why we might need a world model, what simple

01:01.280 --> 01:06.960
self-organizing world models might look like, and even in the simplest cases how we might start to

01:06.960 --> 01:12.480
use those as effective recommendation engines in the wild today, and then a little bit of discussion

01:12.480 --> 01:22.560
about kind of where this research is going. All right. So what are language models good at?

01:23.520 --> 01:28.080
Today's especially large language models are trained on next token prediction.

01:28.800 --> 01:34.640
So this means given a sequence of tokens, we're going to estimate which token is most likely to

01:34.640 --> 01:41.040
come next. Maybe rephrasing that a little bit. We might also say that language models are trained

01:41.040 --> 01:49.040
to estimate which sequences of tokens or words are likely. The caveat being likely to appear in

01:49.040 --> 01:53.280
the training set, but today our training sets are quite extensive.

01:56.880 --> 02:02.160
So given this simple training objective, it's arguably surprising that we've seen language

02:02.160 --> 02:08.480
models do as well as they have in really impressive tasks. So they start to demonstrate

02:08.480 --> 02:13.760
like really great language understanding, meaning like the semantics of language itself,

02:14.400 --> 02:20.800
not just the syntax of which sentences make sense. They've also started to demonstrate some

02:20.800 --> 02:25.440
world knowledge, so implicitly learning kind of how the world works just through our own human

02:25.440 --> 02:33.760
abstraction and how we articulate that. We start to see language models falter at tasks,

02:33.760 --> 02:39.280
which look more like symbolic problem solving. So math in particular, we see this in programming,

02:39.280 --> 02:44.400
although we're getting better at this. And in general, any kind of long-term planning tasks,

02:44.400 --> 02:49.760
which require abstract reasoning. So we see this also in problems that look like this,

02:49.760 --> 02:57.200
which are word problems, but which are really about understanding abstractly how these abstract

02:57.200 --> 03:01.840
objects relate to each other. And again, this shouldn't be surprising that language models

03:01.840 --> 03:07.200
struggle to do things like this, since it's so far off from their original training objective.

03:07.600 --> 03:13.200
And in particular, even when we arrive at the right answer in some of these cases,

03:13.200 --> 03:17.520
it's really hard to know after the fact kind of the reasoning for how we ended up

03:17.520 --> 03:23.520
at the right answer in this particular case. And maybe more fundamentally, if we don't know

03:23.520 --> 03:28.000
where the abstraction is happening or where the reasoning is happening, it's very hard to guide

03:28.000 --> 03:33.840
that process. And so this starts to motivate the need for something that looks more like

03:33.840 --> 03:39.680
an explicit world model. And now, to start to borrow some words from Yasha Benjio,

03:39.680 --> 03:43.680
this is one of the kind of biggest issues with today's language models,

03:43.680 --> 03:47.760
is arguably that we're asking the language model to be both the inference machine

03:47.760 --> 03:54.800
and the world model implicitly, when this doesn't quite make sense. And so some things that we

03:54.800 --> 04:00.400
might hope for in a world model are to model causal relationships, to be really adapt at

04:00.400 --> 04:08.320
modeling, using uncertainty, and to be modular. Yann LeCun puts this in a similar way, so we

04:08.320 --> 04:13.200
might kind of ask a world model to be able to distinguish between which details are important

04:13.200 --> 04:18.000
versus irrelevant, and to be able to make predictions that can be performed in sort

04:18.000 --> 04:24.480
of this abstract space of representations. And so hopefully in the next following slides,

04:24.480 --> 04:30.000
we're going to motivate what it might look like to use language models as inference machines

04:30.000 --> 04:37.360
in maybe like the simplest case of self-organizing world models. And even in those most simple

04:37.360 --> 04:43.280
cases, how we start to get something much closer to the kind of explainable reasoning,

04:43.280 --> 04:45.600
which I think lots of us are grasping for right now.

04:50.240 --> 04:56.320
All right, so in a very simple case, we might think of a world model as simply a collection

04:56.320 --> 05:02.880
of hypotheses, where we model confidence over each of those hypotheses. And in particular,

05:02.880 --> 05:07.680
we care about predictive world models. So given some evidence or data that we've observed in the

05:07.680 --> 05:16.080
world, we'd like to propose the world models, which best explain the evidence that we've observed.

05:20.080 --> 05:26.080
And if that language sounded leading, it was indeed. We are proposing in this case to use

05:26.080 --> 05:31.520
Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given

05:31.520 --> 05:40.080
some new evidence. And so when our hypotheses are kind of these Bernoulli random variables,

05:40.080 --> 05:47.440
this bottom term can simply be expanded into this equation on the slide. And so the tricky part

05:47.440 --> 05:55.280
in this computation is the likelihood piece. So given this evidence, given that our hypothesis

05:55.280 --> 06:00.720
is true, what's the likelihood that we would have observed the evidence? And the claim that we're

06:00.720 --> 06:07.200
making is that language models are actually, this is like a natural task to ask of a language model.

06:08.160 --> 06:12.640
When our evidence and our hypothesis are both semantic objects, as we discussed, language

06:12.640 --> 06:20.560
models are kind of trained exactly to understand which sequences of text are likely. So asking it to

06:21.440 --> 06:27.360
prompting it in a clever way such that we can extract this particular likelihood

06:27.360 --> 06:34.800
is actually really natural. So how do we do this? So we come up with this kind of clever

06:34.800 --> 06:42.240
prompting scheme that allows us to extract again exactly how likely is this should have occurred.

06:42.880 --> 06:49.440
So given that our hypothesis is H in this setting and our evidence is this curly E,

06:50.640 --> 06:54.880
we only need to change our evidence into the conditional form and then phrase the question

06:54.880 --> 07:01.920
like this to the language model. So the input sequence of text goes like, given that potentially

07:01.920 --> 07:07.280
if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic,

07:07.280 --> 07:12.160
would we have observed the evidence that Walmart laid off 10% of the material staff?

07:14.000 --> 07:19.920
And so either using something like few shot prompting or guidance or some kind of control

07:19.920 --> 07:25.600
generation technique, we can ensure that our language model outputs either yes or no,

07:25.600 --> 07:30.320
and then use the logits from its answer to estimate that probability.

07:30.400 --> 07:38.560
And again, the claim here is that this is actually a really natural task to ask of a language model

07:38.560 --> 07:44.720
that leverages its innate reasoning engine better than just kind of allowing it to ramble

07:46.320 --> 07:56.160
using text sometimes. And we can even make this updating scheme a bit more sophisticated

07:56.160 --> 08:02.400
using things like precision waiting. So this will bias our posterior over hypothesis

08:02.400 --> 08:08.080
either towards our prior or our evidence that we've observed based on how confident we are

08:08.080 --> 08:20.240
in our prior and in our evidence. All right. So the world models that we've seen so far

08:20.800 --> 08:26.000
are fairly simple. In particular, we're modeling all our hypotheses as independent from each other.

08:26.880 --> 08:32.480
And this seems like a large simplifying assumption. So how might we support more complex world

08:32.480 --> 08:41.120
models where we condition our hypotheses on each other? All right. So another thing that

08:41.120 --> 08:45.520
hopefully you'll be familiar to this crowd, Bayes and Nets have been around for forever and

08:45.520 --> 08:53.040
are often used to model mechanistic failures and enlarge systems and can be understood simply as

08:53.120 --> 08:56.880
distribution where each variable depends on some small number of ancestor variables.

08:58.080 --> 09:01.680
Perhaps more intuitively, we can also think of these as directed graphs

09:03.200 --> 09:07.280
where we have edges between variables which are directed in the sense that

09:07.280 --> 09:13.680
the parent is conditioned on the child. So let's see an example.

09:15.600 --> 09:22.720
All right. So now we have two different kinds of hypotheses. The ones which are kind of parents

09:22.720 --> 09:29.360
in our simple setup are kind of qualitatively more abstract than the children. So our two

09:29.360 --> 09:35.680
children nodes are similar to the hypothesis we saw before. Mark a sentiment for Best Buy is Poor

09:35.680 --> 09:41.360
or Walmart will grow its physical footprint this year. And then the more abstract hypothesis

09:41.360 --> 09:49.200
being retailers were negatively impacted by COVID-19. And so then we specify this conditional

09:49.200 --> 09:54.400
structure either kind of in a classical sense by specifying all of the joint probabilities

09:54.400 --> 10:00.560
upfront or alternatively learning it given some healthy amount of training data. And again,

10:00.560 --> 10:05.840
we're proposing that instead of doing that intensive process, we can use language models

10:05.840 --> 10:12.960
to extract those probabilities in a natural way. And so one natural complaint at this point might

10:12.960 --> 10:18.560
be like, well, this space is going to start to get very large. If we're trying to update these

10:18.560 --> 10:24.720
beliefs in real time given some large data stream is going to become intractable given our current

10:24.720 --> 10:31.840
framework. So how might we start to augment that? Luckily, there's been a lot of work historically

10:31.840 --> 10:37.280
done on this problem. And we can start to use things like message passing to update our beliefs

10:37.280 --> 10:43.200
in an approximate way. So as long as our Bayes net has a tree dependency structure, we can use

10:43.200 --> 10:48.000
things like the sum product algorithm to update our beliefs. And I don't want you to worry too

10:48.000 --> 10:52.960
much about the equations on this slide. If you've seen this before, this kind of recursive structure

10:52.960 --> 10:58.640
will look familiar. But if not, the general kind of intuitive idea that we're going to compute these

10:58.640 --> 11:04.160
messages from all of the children or the neighboring nodes and use those to propagate through the graph

11:04.160 --> 11:12.400
to update each independent belief. Let's just see an example. All right, so suppose we observed

11:12.400 --> 11:18.080
some new evidence and we'd like to know how should we update probability over hypothesis A,

11:18.080 --> 11:24.960
which as you remember is the parent node in our graph. All right, so B here on the left hand side

11:24.960 --> 11:30.560
is the belief. XA is essentially like just the variable representing the hypothesis A.

11:31.680 --> 11:38.320
And then the fee and the side terms are specified by our language model or the

11:38.960 --> 11:44.400
the Bayes net in the classical sense. And so we're summing over all of the variables,

11:44.400 --> 11:48.640
all of the values that the variables XB and XC can take, which in our case, of course, is just zero

11:48.640 --> 11:55.520
and one. So if you, you might recognize at this point that this is indeed the exact marginal

11:55.520 --> 12:01.600
probability for the hypothesis A, and that's because our graph is simply connected in this example.

12:02.160 --> 12:06.480
So in general, this isn't true, but it turns out to be the case in our example.

12:06.720 --> 12:13.280
And again, we can compute these fee and side terms using the language model itself.

12:18.080 --> 12:22.720
All right, so because we're all lovers of free energy here, I'm going to walk through kind of

12:22.720 --> 12:30.640
how this is maybe the first example of a self evidencing or minimizing free energy kind of model.

12:31.200 --> 12:36.320
So as we noted, belief propagation isn't exact for more complicated graphs. And so

12:37.040 --> 12:42.640
it makes sense and might be useful to ask the question, you know, how far apart or when are

12:42.640 --> 12:49.920
our beliefs close to the exact marginals. And so we often use things like KL divergence to

12:49.920 --> 12:55.680
compare the difference between two probability distributions. And that is explicated on the

12:55.680 --> 13:04.720
right hand side. And then those of us who are like have a background in physics might recognize

13:04.720 --> 13:09.920
Boltzmann's law as well. So this is just the idea that we might represent the probability of a given

13:09.920 --> 13:15.040
state using an energy function. And we're not going to accept this as truth, maybe some of us

13:15.040 --> 13:19.360
have done in the past, but we are going to just use it as a definition for this energy function,

13:20.080 --> 13:27.200
such that when we plug in that term here, and expand out, we start to see these first two terms

13:27.200 --> 13:31.840
look a lot like the kind of energy and entropy functions which we are used to. And indeed,

13:31.840 --> 13:39.920
we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy

13:39.920 --> 13:46.080
to see this all coming together. And in particular, it might make sense just to note at this point that

13:46.800 --> 13:52.320
using world models which are self organizing in the sense seems to be very compelling,

13:52.320 --> 13:58.240
since the kind of world model which we want is one which promotes the evidence that we've seen so far.

14:03.920 --> 14:08.560
Why is it useful to formulate this in terms of free energy, besides the fact that we all

14:09.360 --> 14:13.760
find it compelling here? Well, you can make a lot of progress by constructing analytically

14:13.760 --> 14:19.680
tractable approximations of Gibbs free energy often. I'm not going to go into the details here,

14:20.800 --> 14:23.760
but here are two examples where that's been fruitful.

14:28.480 --> 14:33.440
All right. So now I'm going to chat briefly about kind of how we might use recommendation

14:33.440 --> 14:38.400
systems like, or how we might use these systems as recommendation systems, and indeed in the world

14:39.280 --> 14:47.440
today. All right. So one prime example for a system like this might be useful is a situation

14:47.440 --> 14:53.760
where we have lots of data incoming at very high frequencies, and we always want to have

14:53.760 --> 14:59.520
some set of naturally discrete hypotheses that we're modeling beliefs over which are

14:59.520 --> 15:04.960
being kept up to date at a very regular cadence. And so actually, a lot of the

15:05.600 --> 15:12.560
the muscle here is just reformatting documents or however our data comes in as evidences. This is

15:12.560 --> 15:18.400
not always obvious or easy to do. But once you've kind of figured out that part, and in particular,

15:19.200 --> 15:26.000
we've been using things like RAG, retrieval augmented generation, or embedding based systems to

15:26.000 --> 15:32.560
kind of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind

15:32.640 --> 15:39.520
of built up that machinery, the actual updating computations, as we've shown already, is actually

15:39.520 --> 15:44.960
pretty simple. So we do these likelihood computations and we update our beliefs. And then at any given

15:44.960 --> 15:51.600
time, we can query that model for our marginal distribution over any given hypothesis.

15:53.120 --> 15:57.760
And it turns out that this kind of setup has many practical applications.

15:57.760 --> 16:06.160
It's also noteworthy that even with very simple systems like these, these are like

16:06.160 --> 16:12.480
out of the box, controllable and explainable. So just by storing the magnitude and the direction

16:12.480 --> 16:18.160
of the update to the posterior for each piece of evidence that we observe, we have a very natural

16:18.160 --> 16:26.400
built in explanation for our belief at any given time. And that makes kind of like these applications

16:26.400 --> 16:31.520
where folks might really like to use a language model, but really require like a robust,

16:32.320 --> 16:39.200
like causal relationship between the outputs and the explanation, which you don't get from

16:39.200 --> 16:43.760
a language model on its own. A system like this can be very appealing in those situations.

16:48.800 --> 16:50.720
All right, so now on to further work.

16:51.440 --> 17:01.600
So everything that we've discussed today is early work towards integrating language models

17:01.600 --> 17:08.960
into more probabilistic frameworks. And there's been a lot of exciting work done in this vein

17:08.960 --> 17:14.080
right now. Some important questions which are especially interesting to me are which parts

17:14.080 --> 17:20.400
of the world model should be learned versus encoded? And how do we want intelligence to scale?

17:21.360 --> 17:26.560
Both in the sense of composing systems naturally, there should be some very like natural way that

17:26.560 --> 17:31.920
we can compose to intelligent systems and also such that we can scale them with compute.

17:33.040 --> 17:37.840
And I don't mean to restrict myself either to the kinds of compute that we have today.

17:37.840 --> 17:42.720
We're also working at some exciting new computing paradigms at normal, which might be more

17:42.720 --> 17:49.520
compatible with software of this nature. Also the two folks that we referenced at the beginning

17:49.520 --> 17:53.680
of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area,

17:55.280 --> 18:00.640
which is very inspiring. And so in particular, G flow nets are also probabilistic graphical

18:00.640 --> 18:08.240
models, which I think folks will find a natural next step in reading if you so desire.

18:11.360 --> 18:13.760
And that's it for me.

18:14.400 --> 18:23.360
Awesome. Thank you. Wow, very cool.

18:26.560 --> 18:31.040
Dan, do you want to give a first reflection or thought? And then meanwhile, anyone who's

18:31.040 --> 18:34.720
watching live, please feel free to write questions. I'll relay them in.

18:34.720 --> 18:45.040
Absolutely. So hi, I'm Dan. I work with Phoebe on this project. And yes, the

18:46.480 --> 18:53.120
I think the thing that's most exciting about this for me personally is sort of twofold.

18:53.840 --> 19:04.320
One of them is that it's a way of avoiding sort of having to trust a language model to

19:05.040 --> 19:14.800
understand and reason about text. Because they're not it's not that bad. The thing is that the

19:14.800 --> 19:20.080
extremely strange thing about language models is they're quite good at being almost good enough.

19:21.040 --> 19:27.360
But they're never quite what you could use. You could never use a language model to, I don't know,

19:30.000 --> 19:35.520
sort of triage, like an important sort of situation where a bunch of different things

19:35.520 --> 19:39.840
are coming in, you have to make a decision about which is important. The reason you can't do that

19:39.840 --> 19:47.200
is you simply cannot understand the encoded biases. You cannot get it to reliably generate

19:47.200 --> 19:52.080
reasoning. You can ask it for reasoning. But the thing that it prints out is not the reasoning

19:52.080 --> 19:59.280
that it used internally because it doesn't reason. Fundamentally, while these have input and output

19:59.280 --> 20:05.360
that are natural language, they are not artificially intelligent. They are just prediction machines.

20:06.560 --> 20:11.600
And so we have to be very careful about not anthropomorphizing them. So this is a way of

20:11.600 --> 20:18.720
using those incredibly powerful prediction machines in a framework where we can

20:20.320 --> 20:29.840
make sure that we essentially keep a record of what we're doing so that a human can look at it.

20:29.840 --> 20:34.400
Because, I mean, there's a lot of talk in this world about sort of post-human AI

20:35.040 --> 20:39.680
and those sorts of things. The idea that the machines will become intelligent enough,

20:39.680 --> 20:43.760
or the machines will rise up in a slightly more alarming type of way.

20:45.760 --> 20:49.840
And that's all great and wonderful, but that's not particularly interesting to us at normal.

20:49.840 --> 20:56.400
We're much more interested in sort of having mimicking explicit decision processes so that

20:56.400 --> 21:03.440
a human can audit them and can make these things work. That's kind of the area that we're coming from.

21:04.400 --> 21:11.280
Awesome. All right. I'll go to a question from the chat. So Josh asks,

21:11.840 --> 21:18.080
great talk. Where does hypothesis relevance enter the calculus? Is it folded into confidence?

21:18.080 --> 21:21.840
Not sure if it ought to be. Just saw hypothesis relevance mentioned.

21:25.360 --> 21:31.440
Hypothesis relevance. Does that mean like which hypotheses are conditioned on each other?

21:32.400 --> 21:36.560
Is it possible to ask a clarifying question there? Maybe Dan, you have a better idea.

21:36.560 --> 21:42.800
They can follow up. But yeah, I also wondered about this. You might know what was relevance.

21:43.360 --> 21:51.440
Maybe the temperature and the rainfall were relevant, but then how does this approach help us

21:52.080 --> 21:57.440
understand when one of those relevant factors no longer is relevant or when a new relevant

21:57.440 --> 22:05.120
factor comes into play? Yeah, these are great questions. So I think in terms of understanding

22:05.120 --> 22:10.480
in an automatic sense, when two hypotheses are relevant to each other, we can leverage

22:11.280 --> 22:18.480
embedding type language models for this kind of thing also. If we don't have a more kind of like

22:18.480 --> 22:26.240
structured human intuitive sense for when two hypotheses are related, in terms of like how

22:26.240 --> 22:30.720
those relationships evolve over time, this is something that's really interesting to me.

22:30.720 --> 22:36.160
And I think looking at the theory behind structure learning or when we propose to add new nodes to

22:36.160 --> 22:41.840
the network or propose to add a new edge to the network or things like this is a really exciting

22:41.840 --> 22:47.600
research direction. Although I don't have like a silver bullet answer to how we should do that.

22:48.240 --> 22:55.760
Just to like add a little bit more to that, it is like it is a really interesting research

22:55.760 --> 23:02.560
direction. Like one of the things that Phoebe mentioned in the talk is that there is a difficult

23:02.560 --> 23:06.720
step that we're not talking about, which is actually translating this natural language

23:06.720 --> 23:14.000
into reasonable hypotheses. So there is a step in there where you take essentially a chunk of text

23:14.000 --> 23:21.520
and you have to decide if this is a hypothesis, if this is a hypothesis we've seen before,

23:22.160 --> 23:29.280
if this is a sub hypothesis or a clarification of a hypothesis that we've seen before,

23:29.280 --> 23:36.160
and so on and so forth. So that in some sense part of the data processing and it is an important

23:36.720 --> 23:43.440
step and one that we are sort of continuing to work on and refine. The other thing like a different

23:44.160 --> 23:49.520
sort of interpretation of the question around relevance is around sort of

23:50.720 --> 23:56.160
is the hypothesis relevant to the thing that you're looking at? I mean we could have a hypothesis

23:56.160 --> 24:03.920
the sky is blue, but if we are deciding to deciding you know whether or not we need to

24:03.920 --> 24:11.600
check that part's oil, like the truth or not of the color of the sky is very irrelevant.

24:11.600 --> 24:17.840
And that then comes into the nice thing about having your world described as a

24:19.680 --> 24:25.440
collection of statements with truth values associated with them in that you can directly

24:25.440 --> 24:33.360
reason over them. So you can put a classical decision framework over that to take into account

24:33.920 --> 24:38.480
both the sort of the knowledge you have of the world and also which parts of these worlds are

24:39.120 --> 24:45.040
sort of unknown. So in that sort of situation the person using the world model to construct a

24:49.040 --> 24:58.480
sort of decision or an output will be responsible in some sense for assigning a weight or a cost

24:58.480 --> 25:03.920
to each hypothesis being true. And for some of those hypotheses obviously it will be zero

25:04.880 --> 25:08.720
because again we do not care about the color of the sky if all I want to know

25:08.720 --> 25:16.080
is if I need to change the oil in my car. So that's the sort of the other end of the answer.

25:16.080 --> 25:19.920
So there's a version of the answer at the start of the information flow and there's a version of

25:19.920 --> 25:25.120
the answer at the end of the information flow. But it is a tricky point and one that we are sort

25:25.120 --> 25:30.400
of continuing to iterate on to try and find sort of good ways on both ends of that.

25:30.560 --> 25:41.680
Yeah well a lot there. It's very interesting how in that presentation in response I heard

25:41.680 --> 25:51.120
both about probability distributions on rules and rules on probability distributions and like

25:52.080 --> 25:57.280
which one whether it's the tail wagging the dog or the horse in the cart how to design

25:57.280 --> 26:01.600
these synthetic intelligence systems that appropriately bring together

26:02.720 --> 26:09.920
aspects that are more symbolic more rule like and then more probabilistic more embedding like.

26:09.920 --> 26:17.280
So where does that end with you or how do you see the design of these systems with mixed symbolic

26:17.280 --> 26:22.960
and probabilistic components? Yeah yeah that's a great point and I think this really gets it like

26:23.040 --> 26:27.760
which parts of the world model should be learned or should be represented in some like more

26:27.760 --> 26:34.320
discrete space versus like encoded based on our own human intuition for rules and structure.

26:35.920 --> 26:40.800
And I think like maybe this would be fairly represented as a cop-out answer but I think

26:40.800 --> 26:45.680
it depends a lot on the application. I think like when we're developing systems like this and

26:45.760 --> 26:53.840
just trying to iterate through as many different hypotheses as you can quickly like choosing an

26:53.840 --> 27:00.960
application and benchmarking and testing and seeing like what actually works is a go-to strategy for

27:00.960 --> 27:09.200
us in terms of like well which parts should be fixed and are actually helpful to increase

27:09.200 --> 27:14.640
reliability such that like we can use our human intuition for how this particular you know system

27:14.640 --> 27:21.120
is built versus like well this is something that we we want uncertainty over that's like a really

27:21.120 --> 27:27.680
important part of the learning process for us in terms of yeah that kind of iteration so I think

27:27.680 --> 27:35.040
it probably depends on the application. Yeah it's um it it definitely depends on the application

27:35.040 --> 27:43.680
it's also like it depends on where the actual challenge points are so we've got like outside of

27:43.680 --> 27:50.000
this we've got sort of a few other things that we've released publicly that kind of look at this idea

27:50.000 --> 28:01.200
of there being like external rules to the system and whether or not we can add those in.

28:01.200 --> 28:06.560
So one of them is something called constrained generation where we sort of force the model

28:06.560 --> 28:13.280
to only produce something valid and that's sort of quite a useful way of removing

28:13.280 --> 28:19.680
one particular aspect of stress from the model which is that it may make sort of syntactically

28:20.320 --> 28:29.360
or sort of somehow incoherent outputs that don't follow the rules and then we can then

28:29.360 --> 28:34.800
focus with the rest of our energy we can then take that as given and focus with the rest of our

28:34.800 --> 28:42.880
energy on improving the bit that we don't have rules for. So those sorts of things and sort of a

28:42.880 --> 28:48.720
different version is trying to improve something by saying no you broke a rule we need to like go

28:48.720 --> 28:56.880
back and make this sort of true so this kind of sort of chain of thought prompting type of idea.

28:59.040 --> 29:02.960
So so yeah the the symbolic and the probabilistic

29:05.600 --> 29:08.800
I think in our minds live very closely together as

29:09.520 --> 29:17.200
two tools that don't completely solve the same problem and I think there's sort of in

29:17.200 --> 29:22.160
in the world of I'm not sure how familiar anyone in the audience is with language modeling but like

29:22.160 --> 29:27.120
in the world of language modeling before this sort of explosion of neural networks and artificial

29:27.120 --> 29:34.080
intelligence type methods there was a lot of work on symbolics of language and grammars and

29:34.080 --> 29:40.000
all of that sort of stuff and that work pushed quite a long way forward and this work is pushing

29:40.000 --> 29:44.240
quite a long way forward and I suspect the next thing is going to involve them joining up again

29:44.880 --> 29:51.840
because they each have good points they each have bad points and you know two wrongs don't

29:51.840 --> 29:58.480
necessarily make a right but they can make the less wrong. Nice yeah recently we heard from

29:58.480 --> 30:03.920
Elliot Murphy and talking about the neuro linguistics and about how the statistics of language

30:03.920 --> 30:09.520
are not the rules of language you can always come up with a new expression that's never been

30:09.520 --> 30:12.800
uttered that's not going to be in the training distribution or any distribution.

30:13.840 --> 30:18.000
Okay I'll ask a question in chat from Upcycle Club they wrote

30:19.040 --> 30:24.880
what are some of the key challenges associated with developing such Bayesian world models?

30:24.880 --> 30:32.560
Hmm I think we've touched on a bunch of them the ones that are most top of mind for me right now

30:32.560 --> 30:38.560
are the structure learning thing that came up so how do we understand like when to propose new

30:38.560 --> 30:45.360
hypotheses and how to integrate those into the models and then yeah just figuring out like

30:46.480 --> 30:51.840
yeah I guess this like proposal and evolution process of the nodes themselves since everything

30:51.840 --> 30:58.000
else like the framework like works pretty automatically and in a reasonable way thank you

30:58.000 --> 31:05.120
Bayes thank you to the development of language models but kind of moving from this like more

31:06.320 --> 31:12.240
discrete case into a continuous case which like more fully represents the space that

31:12.240 --> 31:20.560
we're learning over can be challenging. Yeah I would also say that like it's a sort of a

31:20.560 --> 31:28.480
maxism that max maxism what on earth did I just say there's a there's a common saying let's go in

31:28.480 --> 31:34.080
that direction there's a common saying in this world that um that sort of no model ever survives

31:34.080 --> 31:42.080
its first encounter with data um and that that becomes true here as well so there's lots of like

31:42.080 --> 31:47.440
as we've been building these things and using them we found lots of little spiky edge corners

31:47.440 --> 31:51.920
with sort of making sure that the language world is actually doing what we want it to do

31:51.920 --> 31:56.400
so there are a lot of questions in building these things around how do you actually test that the

31:57.360 --> 32:03.840
components of it are actually working the way you want and then on like a broader level how do you

32:04.720 --> 32:13.920
compare something that is fundamentally trying to solve a different problem to other methods so

32:14.720 --> 32:19.840
we are solving a problem under the constraints that we want a fully auditable system

32:22.240 --> 32:28.320
we could also solve all of these problems by a thing called in-context learning which is basically

32:28.320 --> 32:34.720
putting the context into the prompt of a large language model and asking it the answer and that

32:34.720 --> 32:41.440
also works especially when you've got things like GPT-4 which are just wonders and glories

32:41.600 --> 32:47.920
um it works really well so then we come to the question of how do we actually make the case

32:47.920 --> 32:56.320
from this from a like a bigger picture perspective can it be more than just a like can we find

32:56.320 --> 33:05.680
benchmarks that reflect um the structural advantages of this approach over something

33:05.680 --> 33:14.720
like in-context learning that don't come across as false so that's kind of like a a stranger answer

33:14.720 --> 33:20.640
because it's not really about like actually developing the world model it's about sort of

33:20.640 --> 33:28.320
convincing other people that it's a good idea um and that's you know that that is a thing that is

33:28.320 --> 33:35.200
true of essentially all of the things on this slide as well they are all quite complex and odd

33:35.200 --> 33:41.920
little methods um that you know there's a there's a degree to which well we definitely can solve this

33:41.920 --> 33:51.440
an easier way um so what is the thing that what is the the the application or the benchmark where

33:51.440 --> 33:57.040
we can say no if you do it the easier way you will fail at this measurable thing

33:57.680 --> 34:05.360
very interesting um so you mentioned the self-evidencing

34:07.360 --> 34:15.920
advantages of using world models that are self-evidencing rather than reward maximizing

34:15.920 --> 34:22.240
for example so how do you see that playing out and I can connect it back to active inference of

34:22.240 --> 34:28.320
course but how do you see this self-evidencing centrality play out in the kinds of models described

34:28.320 --> 34:33.840
here yeah I think there are a couple reasons why it's so compelling to me uh and the first just has

34:33.840 --> 34:39.040
to do with explainability right like it's really convincing to people to say like well why why did

34:39.040 --> 34:43.840
we predict this why do we believe this well this is the actual real world data that we've observed

34:43.840 --> 34:50.720
such that you know this this is the impact that that's had uh and then I think like uh I don't know

34:50.720 --> 34:55.200
you hear a lot about like designing these really complicated reward functions which are often very

34:55.200 --> 35:01.200
clever but which um often I feel like are close to being a pitfall because they very easily become

35:01.200 --> 35:07.200
like disconnected from like the complex world that we're trying to model and so you end up in like

35:07.200 --> 35:14.000
weird local maximums or minimums and um yeah you start like just kind of uh solving the problem

35:14.000 --> 35:19.440
that you've designed versus like the problem which actually exists and so um I just have always

35:19.440 --> 35:24.720
loved the idea that what we should be doing is um kind of self-evidencing and from an intuitive

35:24.720 --> 35:31.200
sense that feels like what it feels like what an intelligence system should do uh yeah

35:33.840 --> 35:37.760
yeah I actually don't have anything interesting to add to that I just agree with Fabie

35:39.760 --> 35:45.760
that explainability and the capacity to explicitly reference previous data including

35:45.760 --> 35:51.360
like leave one out so techniques from non-parametric statistics about the effect of adding in another

35:51.360 --> 35:59.840
piece of data or removing a piece of data um and then just like to bring it to like a homeostatic

35:59.840 --> 36:06.000
setting which is commonly considered an active inference like we're trying to be within a homeostatic

36:06.000 --> 36:15.440
temperature range of 37 yes we could propose reward functions but as those start to include

36:16.160 --> 36:21.280
open-endedness and exploration structure learning just like you described it Fabie like

36:21.280 --> 36:27.440
we're solving the problem as designed rather than the actual question of the homeostatic

36:27.440 --> 36:35.360
temperature and the sort of path of least action first principles physics grounded intelligence

36:35.360 --> 36:40.560
perspective from active inference is like make it the kind of thing that measures itself at 37

36:41.280 --> 36:48.080
and then as long as it is it is and when it isn't it's dead and that's the kind of mortal computing

36:48.880 --> 36:55.440
crossover which is like outside of its zone of surprise it it's not just that it's getting

36:55.440 --> 37:03.440
a bad grade in the class that is like a deeper failure signal than that and to understand okay

37:03.440 --> 37:10.640
when is it a yellow flag when is it a red flag in terms of the new scientific literature coming in

37:11.680 --> 37:24.080
those have plain straightforward ways to interpret that developing larger higher-order

37:24.160 --> 37:29.120
apparatuses will never return to that kind of basal simplicity

37:31.920 --> 37:33.120
yeah couldn't agree more

37:35.680 --> 37:43.120
yeah I mean absolutely it also like the other thing that it can do quite well is deal with

37:43.920 --> 37:54.880
essentially outlier studies so situations where you have a new piece of information that is

37:55.520 --> 38:02.320
strongly conflicting with all the previous pieces of information and trying to sort of

38:02.320 --> 38:08.880
work through what that really means and there's a like there's a degree to which

38:09.360 --> 38:17.440
we can even sort of extend this process to multiple agents that have these belief systems

38:17.440 --> 38:24.240
and then look at sort of consensus of experts or weighted consensus of experts so for instance

38:24.240 --> 38:31.040
you could have like a weather vane type of situation where somebody really over indexes to every new

38:31.040 --> 38:36.480
piece of information and you would do that with you know technically you do that with maybe a power

38:37.040 --> 38:43.040
posterior type thing or you can have somebody who's built in strong priors in a particular

38:43.040 --> 38:52.400
direction and you can then like take your consensus of artificial sort of decision making all of which

38:52.400 --> 39:00.400
has within their universe well-reasoned updates to the data and then you can look and try and

39:00.400 --> 39:10.560
work out what that swarm of experts can tell you and sort of do very empirical things like

39:11.520 --> 39:17.200
try and you know work out which of these experts is doing well at a particular moment in time

39:18.640 --> 39:24.480
because you know there could be there could be times when the world's very or the problem

39:24.480 --> 39:30.800
you're solving is very chaotic in which case the over indexing expert would probably be a pretty

39:30.800 --> 39:38.400
pretty solid bet while there are other times where sort of things are pretty stable and you

39:38.400 --> 39:43.520
probably it would be possible that the sort of the more conservative expert is more

39:45.600 --> 39:52.320
sort of empirically making good decisions and good recommendations so there's like a lot of

39:52.320 --> 40:01.600
ways that we can not just like incorporate these sort of homeostasis ideas but we can also change

40:01.600 --> 40:08.240
what that means for different agents and artificially like do that artificially and then combine them

40:08.240 --> 40:20.720
together to try and get a almost like a like a blanking on the word but you know

40:22.160 --> 40:28.080
a forecast under a sort of a hypothetical set of situations and we can actually sort of bring

40:28.080 --> 40:33.360
those ideas of the world forward and see what happens when they sort of meet with actual information.

40:33.680 --> 40:41.760
Yeah this angle of mixture of experts as it's sometimes called more in the language model space

40:41.760 --> 40:47.760
or ecosystems of shared intelligence or diverse intelligences in the active inference area like

40:47.760 --> 40:56.960
that's very interesting obviously has connections back to human teams and teams of beyond humans and

40:56.960 --> 41:08.000
so on a lot of this is still text based so maybe you did or didn't mention what representation the

41:08.000 --> 41:16.320
base graphs are but they're plain text like and there was a lot of discussion about bringing

41:16.320 --> 41:22.720
from natural language scientific papers or however it is into a structured form and then the

41:23.280 --> 41:30.160
explain method that you showed kind of taking the structured form and just giving a little

41:30.800 --> 41:40.240
syntactic fluency so it looks human readable so how do you see that essence coming into play with

41:40.240 --> 41:48.720
multimodal models and then with action in the world that isn't just developing the next text

41:48.720 --> 41:55.120
token but a robotic actuator or modifying some other control element of the world.

41:58.000 --> 42:02.720
Yeah that's a really good question. I honestly haven't thought much about multimodal stuff in

42:02.720 --> 42:09.760
this particular context but I think the framework is general enough at this point such that it

42:09.760 --> 42:16.720
it's definitely could support lots of different modalities. I'd be really curious to see how

42:16.720 --> 42:25.360
this did with something like audio in particular. Yeah and then to your point about like yeah this

42:25.360 --> 42:32.320
maybe like discrete versus continuous relationship I think I think that's like part of what we're

42:32.320 --> 42:37.600
learning is how to go from like these long natural text documents to a system which is

42:37.600 --> 42:43.840
appropriately discretizing our hypotheses such that we have these like meaningful explanations

42:43.840 --> 42:54.000
like you mentioned so I think yeah I think like continuing to develop like robust ways of

42:54.560 --> 43:00.880
surfacing those explanations is a big part of this as well like over time we're going to observe

43:00.880 --> 43:06.720
lots and lots and lots of evidence how do we make sure like hypotheses don't get stale and how do we

43:06.720 --> 43:12.480
use evidence to know when they are and things like this are part of that also I don't know if

43:12.480 --> 43:15.440
that directly answered your question but that's some of the stuff that I've been thinking about

43:15.440 --> 43:28.160
related to that. So in the I mean in examples like sort of moving towards robotics and sort of tech

43:28.160 --> 43:32.720
video generation and image generation other sort of audio other sort of multimodality is

43:32.960 --> 43:44.720
to be honest I think of these processes in general as enabling us like building a world

43:44.720 --> 43:52.640
model to enable a sort of sequential decision process so if that decision process happens to

43:52.640 --> 43:57.520
be should the robot turn left and that's what the sort of the decision process is it's it's

43:58.480 --> 44:04.080
multimodal in like a very classical sense that you can put any type of decision framework over

44:04.080 --> 44:09.920
the top but it's not sort of generatively multimodal I'm not saying write me a song that

44:09.920 --> 44:16.720
sounds like Beyonce and a song that sounds like Beyonce comes out I think this this sort of this

44:16.720 --> 44:22.080
sort of Bayesian world model layer is blocking towards that sort of thing but that's that's

44:22.080 --> 44:28.640
really not sort of the aim of what we're trying to do it's also like within normal like our

44:30.240 --> 44:36.240
almost I don't want to I don't want to say mantra or manifesto because that sounds culty

44:36.240 --> 44:44.480
and no one wants to sound culty but like our basic aim is to always center like humans within our

44:44.480 --> 44:56.480
process and so some of this multimodal stuff it's less clear where the human lives so for

44:56.480 --> 45:02.240
instance like a video generation type thing where does the human live so keeping it at this abstraction

45:02.240 --> 45:07.040
of sequential decision making then it's a decision that a human could do you know human with their

45:07.040 --> 45:14.640
thumbs could be moving a like a robot around and doing that sort of stuff but yeah it's it's

45:14.640 --> 45:19.280
really all about sort of controllability and auditability for us in sort of a sequential

45:19.280 --> 45:26.160
decision process so to the extent that that sort of leads in its multimodal world that's

45:27.680 --> 45:32.480
that that's sort of part of what we're doing and like to some some versions of multimodality

45:32.560 --> 45:42.080
um is we're just not swearing in that particular space um yeah not a great answer but a long one

45:45.200 --> 45:54.800
let them distill it down later um in the um auditability area it almost falls out to me

45:54.800 --> 45:59.520
to be like a syntax of auditability in a semantics at the syntactic level just

46:00.480 --> 46:06.800
tagging or versioning when a file comes in or when a given computation is executed that is

46:06.800 --> 46:14.400
basically transfer across all settings and then where I see you honing in on with with this work

46:14.400 --> 46:23.360
is kind of the semantic auditability which is actually how we compose our accounts

46:24.080 --> 46:31.280
I would have driven but I decided to walk because this happens and so bringing that

46:32.480 --> 46:42.800
different kind of trace to systems is gonna make it um what will it open up in science or

46:42.800 --> 46:52.560
education or how do you see this sitting at a console somebody is at now and making this

46:52.560 --> 47:00.560
different like over what timeline yeah I mean it's really quite nice for storytelling because

47:00.560 --> 47:08.400
as you said you can say things precisely like well you know because we observed this thing

47:08.400 --> 47:13.440
or because if we had observed something else you know like maybe you can even make statements which

47:13.440 --> 47:20.960
are um yeah conditional in that sense uh I think it does like empower whoever is sitting in front

47:21.040 --> 47:26.400
of this data to feel like really sure about again like that the reasoning engine that like

47:26.960 --> 47:31.840
that went on uh which to me is is pretty different from what it feels like to sit in

47:31.840 --> 47:37.040
front of chat gbt even though it's quite useful often um you know you you try the code and it

47:37.040 --> 47:42.720
works or doesn't work or you like you know ask your friend is this really true um and that feels

47:42.720 --> 47:48.480
pretty different to me from being able to to look at the evidences themselves and say like oh well

47:48.480 --> 47:53.440
actually if this is the reason you think that I know that that evidence is is not true or you

47:53.440 --> 47:59.840
know like you can bring your own human intuition or world model uh in terms of validating or um

48:00.400 --> 48:06.000
yeah super imposing what you believe on top of what this system believes and so that makes it

48:06.000 --> 48:14.160
really easy to make decisions um quickly I think there's also sort of a converse of this which is

48:14.240 --> 48:21.120
that it also makes it clear which evidence was not used to make a decision uh and that can be quite

48:21.120 --> 48:27.840
telling in these situations where you could be worried that a particular type of evidence isn't

48:27.840 --> 48:35.360
being weighted correctly or isn't being um sort of formatted correctly so again like if this is a

48:36.000 --> 48:46.080
sort of a like a system that builds an assistant um that sort of does surfaces all this information

48:46.080 --> 48:52.080
and sort of makes a recommendation with reasoning for a person that person can then look and be like

48:52.080 --> 48:57.440
and they know what the data is they can look at the deck and say you know why didn't you consider

48:57.440 --> 49:03.680
the make of the car or why didn't you consider this or why didn't you consider that and they can

49:03.680 --> 49:11.440
then use their understanding of what's not being prominently used by the model to

49:12.720 --> 49:19.600
sort of sense test like it's it's sort of I mean in some sense that usage of it is a reformulation

49:19.600 --> 49:24.080
of what Phoebe just said where you use your internal world model but it's like I think it's

49:24.240 --> 49:34.560
important to know when evidence is being used and this is like I think you simply cannot get

49:34.560 --> 49:41.440
um from from like a GPT type thing or any sort of like prompting type method we know for instance

49:41.440 --> 49:48.720
that like um the order of the order that you submit your evidence in is probably going to matter

49:48.720 --> 49:55.200
for a prompting based method okay that's obviously not true for a Bayesian update where we have this

49:55.200 --> 50:00.720
sort of this this coherence principle where if you shuffle your data and enter it in a different

50:00.720 --> 50:08.240
way you will get the same posterior up to computational artifacts um so so all of that

50:08.240 --> 50:13.920
is like in my mind is just as important to order ability as the ability to write a report that says

50:13.920 --> 50:23.600
I made this decision for these reasons yeah well that makes me think about this kind of view from

50:23.600 --> 50:28.560
the inside interpretability where the rules help and also knowing what evidence is not used is

50:28.560 --> 50:34.320
importance for compliance and knowing what information like in a healthcare setting was or wasn't

50:34.320 --> 50:47.280
used um what about thermodynamics we heard about free energy boltzmann came up how do you see the

50:48.000 --> 50:55.840
info thermo nexus what have we learned from the last hundred years of thermodynamics and

50:55.840 --> 51:06.080
information theory and all of this and on the software or hardware side how is that kind of a

51:06.080 --> 51:17.200
free energy nexus being used yeah i mean i i'm really excited about how all of this seems to be coming

51:17.200 --> 51:23.520
together um i the free energy just keeps showing up in all of these exciting areas to me we have

51:23.520 --> 51:27.840
like a book club for singular learning theory and like they talk all about free energy too and i

51:27.840 --> 51:33.680
think some of those ideas are really exciting um i mean at normal i think like the thing that i would

51:33.680 --> 51:39.280
highlight is like this idea of software hardware co-design um which is really special uh and so

51:39.280 --> 51:44.320
we're trying to do this hard and fun dance towards each other where we're like thinking about these

51:44.320 --> 51:51.200
new kinds of systems and how they might support each other and um empower each other and and yeah

51:51.200 --> 51:59.120
how to build full stack systems um which is really challenging and and also really exciting um yeah

51:59.120 --> 52:04.240
and i think like from like the first principles of thermodynamics perspective like like we're all

52:04.240 --> 52:11.200
just uh yeah we're all kind of like mathematics and physics people at heart so like going taking

52:11.200 --> 52:16.720
like uh you know all of what people have learned in language modeling and all of that like um very

52:16.720 --> 52:21.440
much to heart as well like i think approaching whatever problem that we're facing from a first

52:21.440 --> 52:26.080
principles how do physical systems work in the world what do we really what are the assumptions

52:26.080 --> 52:32.160
we're really comfortable with uh and building up from there um is definitely our our natural mode

52:33.040 --> 52:36.080
uh so i think that makes it easier to to start working together also

52:38.080 --> 52:45.200
um it's also probably worth saying that we have a sort of a secondary not secondary a very different

52:45.200 --> 52:50.800
stream of interest in thermodynamics as well which is the ways that we can use actual physical

52:50.800 --> 52:58.240
thermo dynamical principles to build hardware that is specifically has sort of noise in it as a

52:58.240 --> 53:05.040
first class citizen and because of that it is particularly well suited to probabilistic tasks

53:05.840 --> 53:12.720
um and so we've we've built if you if anyone wants to look we have a blog i believe the URLs

53:12.720 --> 53:20.240
blog dot normal computing dot ar um and amongst other things that are on it uh there is the very

53:20.240 --> 53:29.200
first demonstration of using physical thermodynamic hardware to actually do computations um is the

53:29.200 --> 53:33.680
computation the most vital computation that we will ever do it's inverting an eight by eight

53:33.680 --> 53:41.360
matrix so no we can do that otherwise um but it it is sort of building up towards this idea that

53:41.360 --> 53:47.040
we can use thermodynamics not just in our modeling and our understanding of the world but also in our

53:47.040 --> 53:57.840
sort of low energy compute stack to actually realize these things um so i think i it's i think

53:57.840 --> 54:06.560
it would be challenging to find a group of people on this earth who have more investment in thermodynamics

54:06.560 --> 54:15.680
and don't work in a physics department uh because we have investment all the way through from

54:15.680 --> 54:21.840
sort of active inference type things all the way down to this like this this hot thing goes there

54:23.920 --> 54:29.120
which is kind of cool i'm not a physicist so i have but but that's my that's my understanding

54:29.120 --> 54:35.520
of thermodynamics this hot thing goes there informative thing goes here hot thing over there

54:36.240 --> 54:49.280
call it a day um yep that it's a really cool fusion with the kind of parsimony and elegance

54:49.280 --> 54:58.240
and the aesthetic of math and physics and first principles and the different parsimony of pragmatism

54:58.800 --> 55:06.560
with the actual material basis like of a synapse the size of the synapse and the kind

55:06.560 --> 55:17.200
of stochasticity that that size alone um entails with like membranes and all of this those stochastic

55:17.200 --> 55:26.320
aspects are leveraged for the compute the synapse is not simply a variance reducing machine and so

55:29.120 --> 55:39.600
it's like both the platonic slash mathematical ish spirit it finds a common home in these real

55:40.480 --> 55:49.600
simple physical demonstrations and um today it feels like there's a big gap between the um

55:50.320 --> 55:58.320
mesoscale computational architectures that you described today that are very much running on

55:58.880 --> 56:06.880
the kind of von Neumann architecture turing completeness paradigm and yet very tantalizingly

56:06.880 --> 56:17.200
close like to a physical object that has a constrained rule de facto like only one thing

56:17.200 --> 56:26.320
can come out of this at a time as long as the funnel is this wide and so bringing the rules

56:26.320 --> 56:35.760
and the regularities of what we call physical things to bear with the fundamental and the

56:35.760 --> 56:48.640
imposed constraints on informational spaces it's very cool directions um one other note about

56:48.640 --> 56:54.800
just where active inference um an action plays a role is um and also you mentioned like the

56:54.800 --> 57:02.480
hypothesis going stale or like sort of data being over relied on um in the proactive stance

57:02.480 --> 57:09.760
where we're using expected free energy or something like it to to calculate future courses of action

57:09.760 --> 57:16.000
over observations that we haven't seen yet moves that haven't been made yet there's an explicit

57:16.000 --> 57:25.280
epistemic value and so that can be diagnosed and observed as a measure of where a given

57:26.160 --> 57:32.400
computation is on the continuum between purely pragmatic value just constraint

57:32.400 --> 57:37.840
satisfying and and realizing preferences and expectations and then the pure epistemic value

57:37.840 --> 57:45.280
where all outcomes are good and the more information gain the better and then being able to take

57:45.280 --> 57:52.080
control of that balance and know amidst changing situations again taking probabilistic or rule

57:52.080 --> 57:59.200
based approach there to when epistemic and pragmatic like gas and break kind of come into play

57:59.200 --> 58:09.520
these are very basal um control knobs or features in active inference that it's just

58:10.480 --> 58:17.600
not going to show up at the 50th layer of scaling is all you need

58:17.600 --> 58:32.640
yeah cool well do you have any other like thoughts or things you want to add or questions or where

58:32.640 --> 58:41.200
things are heading for your works nothing to add at this moment but certainly uh excited to keep

58:41.200 --> 58:51.440
in touch with this community and yeah collaborating yeah absolutely um and we sort of share I mean

58:51.440 --> 58:59.120
we write papers and stuff but we mostly like we share most of what we do be at academic in the

58:59.120 --> 59:05.680
sort of machine learning space or be it in the um sort of the physical hardware space uh on our

59:05.680 --> 59:13.520
blog which is blog.normalcomputing.ai um and yeah thank you so much for inviting us it's been very fun

59:14.400 --> 59:23.120
awesome thank you hope to speak again so peace bye thanks hi

59:35.680 --> 59:36.180
you

