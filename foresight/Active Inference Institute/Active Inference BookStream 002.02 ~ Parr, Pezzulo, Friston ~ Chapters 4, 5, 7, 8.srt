1
00:00:00,000 --> 00:00:16,400
Hello, it's July 28th, 2023, and we're in Active Inference Textbook Group slash Bookstream

2
00:00:16,400 --> 00:00:25,800
2.02. Thanks, Ali, for joining. So what we're going to do today is give a short overview

3
00:00:25,800 --> 00:00:35,480
of the chapters from the PAR at all 2022 book. We're going to do chapters four, five, seven, and eight.

4
00:00:37,560 --> 00:00:43,800
And we're just going to pause between them because then we'll clip them into the shorter videos,

5
00:00:43,800 --> 00:00:49,480
append that to the playlist, just so there's a first video overview of each of the chapters.

6
00:00:49,480 --> 00:00:59,800
And this is the second in that work. Alright, so we'll do chapter four. We'll just wait a few

7
00:00:59,800 --> 00:01:10,520
seconds and then start chapter four. Okay, chapter four is called the generative models of active

8
00:01:10,520 --> 00:01:14,680
inference. And it begins with a quotation, everything should be made as simple as possible,

9
00:01:14,680 --> 00:01:22,760
but not simpler by Albert Einstein. Ali, what is your overview thought or warning for chapter four?

10
00:01:25,320 --> 00:01:34,280
Okay, so after the preliminary materials in chapters two and three, which was basically

11
00:01:35,160 --> 00:01:43,000
largely based on providing some conceptual framework for developing the further

12
00:01:43,960 --> 00:01:50,920
theory, chapter four delves into much more detail in terms of mathematical formulation.

13
00:01:50,920 --> 00:01:59,480
And it unpacks a lot more the way that the central equations of active inference

14
00:02:00,280 --> 00:02:07,480
is derived and how to construct the important elements of active inference models.

15
00:02:08,440 --> 00:02:17,320
So say matrices A, B, C, and D and also how to put together generative models in

16
00:02:18,120 --> 00:02:24,040
different situations. So it basically lays out the foundation for

17
00:02:27,000 --> 00:02:35,080
constructing active inference models, both for discrete time situations and continuous time

18
00:02:35,080 --> 00:02:45,160
ones, which will be used later in chapters seven and eight. But this is probably one of the most

19
00:02:46,360 --> 00:02:54,280
challenging and at least mathematically dense chapters in the book. So I would personally

20
00:02:54,280 --> 00:03:03,320
suggest reading through this chapter really slowly. And even if we don't get to understand

21
00:03:03,320 --> 00:03:11,000
every single detail of the chapter, obviously we can return per required as we go through

22
00:03:11,720 --> 00:03:20,040
the textbook. Thank you, Ali. Yes. So let's look through the sections. Just to add on though,

23
00:03:20,040 --> 00:03:28,360
chapter four is one of the larger and more equation dense chapters, because it is the

24
00:03:28,360 --> 00:03:35,560
common kernel or basis that's then going to get applied in chapter five in the neurobiological

25
00:03:35,560 --> 00:03:41,400
case. There's a recipe for making chapter four in chapter six. That's the recipe for active

26
00:03:41,400 --> 00:03:47,560
inference modeling. Chapter seven and eight are about the discrete and the continuous time variant

27
00:03:47,560 --> 00:03:55,720
or subtype or motif of these kinds of things called generative models. So this is the real

28
00:03:55,720 --> 00:04:03,960
common root. And we'll just look at what the sections are. This chapter complements the

29
00:04:03,960 --> 00:04:08,520
preceding chapter's conceptual treatment of active inference with a more formal treatment.

30
00:04:09,720 --> 00:04:16,280
Section 4.2 from Bayesian inference to free energy. What would you say about this section, Ali?

31
00:04:16,280 --> 00:04:27,640
Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference.

32
00:04:28,360 --> 00:04:33,480
I mean, all the way back to Helmholtz theory about

33
00:04:35,720 --> 00:04:41,400
unconscious inference or something to that effect. I can't remember the exact term.

34
00:04:42,040 --> 00:04:52,360
But here, I mean, it provides in a bit more detail how we can derive free energy principle

35
00:04:53,320 --> 00:04:59,320
formalism using the established Bayesian inference formulation.

36
00:05:01,160 --> 00:05:08,760
And particularly, one of the key movements or at least one of the key decisions

37
00:05:09,400 --> 00:05:16,440
in through their derivation of free energy principle formulation is using Jane's inequality

38
00:05:16,440 --> 00:05:30,520
principle to derive an upper bound instead of just using the exact values to compute or to

39
00:05:30,520 --> 00:05:38,760
achieve required parameters. So that's basically, in my opinion, the key premise of

40
00:05:39,880 --> 00:05:51,640
section 4.2. And to see in a bit more detail how we can achieve those upper bounds using

41
00:05:51,640 --> 00:06:00,360
Jane's inequality directly by using, I mean, manipulations of Bayesian inference,

42
00:06:01,800 --> 00:06:03,480
Bayesian statistical formalism.

43
00:06:05,640 --> 00:06:11,240
Thanks. I'll just add one point from this section. Broadly, these are the problems of

44
00:06:11,240 --> 00:06:15,880
inferring states of the world perception and inferring a course of action planning.

45
00:06:15,880 --> 00:06:21,480
So this is again, referring to the perception and action. And everything that happens in

46
00:06:21,480 --> 00:06:26,760
between is the internal or the cognitive part of the inference. But this is like the blanket state

47
00:06:27,320 --> 00:06:35,160
cybernetic input output. And then let's look at the first equation or how much equations overall

48
00:06:35,800 --> 00:06:38,520
or what equations do you think we should highlight?

49
00:06:39,400 --> 00:06:55,000
Okay. So maybe we can, I mean, just as a general comment about these different equations, well,

50
00:06:55,000 --> 00:07:03,800
each of these equations provide a distinct step toward deriving the ultimate whole picture.

51
00:07:03,800 --> 00:07:13,560
So even if we don't quite understand how we can derive from each step to the other one,

52
00:07:15,320 --> 00:07:25,080
it's good to know that it's only required to understand how we get to that ultimate whole

53
00:07:25,080 --> 00:07:33,560
picture. But ultimately, what we would need in order to develop active inference models

54
00:07:34,360 --> 00:07:39,720
is the ultimate equation or ultimate whole picture. So this is just

55
00:07:42,120 --> 00:07:51,960
a way to elucidate the steps toward developing that whole picture. But again, it's not an

56
00:07:51,960 --> 00:08:00,840
essential requirement to understand the materials of the rest of the book. But if we go from,

57
00:08:01,800 --> 00:08:10,440
I mean, equations 4.1 toward the 4.4, or in other words, a variation of free energy,

58
00:08:10,440 --> 00:08:22,360
well, equation 4.1 is just a basic definition of some properties of probabilities in terms of

59
00:08:22,360 --> 00:08:30,120
conditional probability and so on. So equation 4.2 provides the central

60
00:08:31,000 --> 00:08:40,840
Jane's inequality principle and how it relates to, I mean, conditional probabilities and of

61
00:08:40,840 --> 00:08:50,120
course, joint probabilities. And then by using those two properties or those two equations,

62
00:08:50,120 --> 00:08:57,160
we ultimately get to 4.4, which is the definition of variational free energy parameter,

63
00:08:58,440 --> 00:09:05,640
which is the parameter of interest that needs to be optimized in order to inference to happen,

64
00:09:07,320 --> 00:09:11,000
or at least perceptual inference to happen in active inference models.

65
00:09:11,080 --> 00:09:21,080
Thanks. The only thing I'll add is f is the letter used for variational free energy. Think of it

66
00:09:21,080 --> 00:09:25,640
like a computer program and the arguments that it takes in or the variables that it takes in

67
00:09:26,360 --> 00:09:33,880
are q, which is the distribution that's under the statisticians control and y, which are the data,

68
00:09:33,880 --> 00:09:40,840
which are outside of the statisticians control. And do you want to describe more about anything

69
00:09:41,880 --> 00:09:43,560
in this equation or carry on?

70
00:09:46,040 --> 00:09:55,560
Just one thing that can probably be helpful is to somehow compare these steps with

71
00:09:56,760 --> 00:10:04,600
the initial picture we had from chapter 2, because variational free energy was first introduced

72
00:10:04,600 --> 00:10:13,560
in chapter 2. So it can be helpful to go back and forth between chapters 2 and 4 and try to

73
00:10:13,560 --> 00:10:27,720
connect the dots between the related points there. Section 4.3 generative models. All right,

74
00:10:27,720 --> 00:10:33,240
I'll read the first sentence, then you can give some thoughts. To calculate the free energy,

75
00:10:34,040 --> 00:10:40,440
we need three things. Data, a family of variational distributions, and a generative model comprising

76
00:10:40,440 --> 00:10:50,040
a prior and a likelihood. In this section, we outline two very general sorts of generative model

77
00:10:50,040 --> 00:10:53,640
used for active inference and the form the free energy takes in relation to each.

78
00:10:53,960 --> 00:11:10,040
Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time

79
00:11:10,040 --> 00:11:17,160
situations. So clearly, we would need two different types of generative models for each situation.

80
00:11:18,120 --> 00:11:25,480
And obviously, the generative models or the way to construct generative models for

81
00:11:25,480 --> 00:11:33,320
discrete time situations would vary quite a bit from the one for continuous time situations.

82
00:11:35,160 --> 00:11:43,000
But the general principle underlying those generative models are basically the same,

83
00:11:43,640 --> 00:11:52,680
which is, I mean, to somehow construct a model of the environment, I mean, either be it

84
00:11:54,760 --> 00:12:03,720
for the situation that is sequential in time or for the situations that need to be somehow,

85
00:12:05,800 --> 00:12:10,520
each moment of the situation needs to be accommodated in terms of a continuous time

86
00:12:10,520 --> 00:12:22,520
situation. So figure 4.2 provides some examples of both. So for, yes, let me see.

87
00:12:24,440 --> 00:12:35,720
Yes, so we have some examples of different kinds of generative models and case studies,

88
00:12:36,440 --> 00:12:45,720
if you like, and it provides various ways to show how the dependencies between variables

89
00:12:45,720 --> 00:12:54,280
can be modeled using these kinds of graphical probabilistic models. So one common way to

90
00:12:54,280 --> 00:13:02,440
represent generative models is to use these kinds of graphical probabilistic models in active inference

91
00:13:02,440 --> 00:13:12,200
literature, which is, at least in this case, the circles would represent the random variables,

92
00:13:12,200 --> 00:13:18,520
and the squares would represent the distributions, which would describe

93
00:13:19,960 --> 00:13:29,640
the dependencies between those random variables. So we can see the clear relationships between

94
00:13:29,640 --> 00:13:37,560
those parameters here, which is basically what this whole graph, what constitutes the

95
00:13:37,560 --> 00:13:45,960
generative model that needs to be used for different situations. And then in figure 4.3,

96
00:13:47,560 --> 00:13:53,400
we can compare the two different types of generative models based on

97
00:13:53,880 --> 00:14:02,120
whether it's discrete time or continuous time situations. So the upper picture is a generative

98
00:14:02,120 --> 00:14:11,560
model for the discrete time situation, and the lower picture is the parallel continuous time

99
00:14:11,560 --> 00:14:21,880
version of it. And as we can see, the general topology of these models are the same. The only

100
00:14:21,880 --> 00:14:32,760
things that differ is the use of parameters for policies or, I mean, discrete time policies or

101
00:14:32,760 --> 00:14:42,200
the continuous time ones. And we can obviously compare the different elements for both priors

102
00:14:42,200 --> 00:14:50,600
states and external states, internal states, and so on by comparing these two models here.

103
00:14:52,600 --> 00:14:57,480
Yeah, we often return to figure 4.3. It's kind of the Rosetta Stone

104
00:14:58,520 --> 00:15:04,280
of generative modeling for the context of this book, because it's then going to develop out into

105
00:15:04,280 --> 00:15:10,440
chapter seven and eight. And it represents a really fundamental decision made in modeling.

106
00:15:11,720 --> 00:15:18,040
And in the later chapters, it's also shown how it can be made into a hierarchical model

107
00:15:18,040 --> 00:15:23,240
that combines aspects of both. But within each level of modeling, still, these are the kinds

108
00:15:23,240 --> 00:15:28,440
of decisions that modelers are presented with when it comes to statistical modeling overall.

109
00:15:30,520 --> 00:15:38,200
So section 4.4 goes into essentially the top half of figure 4.3, discrete time.

110
00:15:39,800 --> 00:15:41,480
What would you say about discrete time?

111
00:15:42,440 --> 00:15:50,920
Okay, so the discrete time situation is obviously the archetype discrete time situation,

112
00:15:50,920 --> 00:15:59,320
which is the POMDP models. So at this point, I would very much like to recommend

113
00:16:00,280 --> 00:16:14,040
following the material from set by step paper, because in that paper, the way to construct

114
00:16:14,040 --> 00:16:22,360
POMDP models is described in a bit more detail. So if anyone feels like they should learn a bit

115
00:16:22,360 --> 00:16:31,560
more about the gaps in the details, I would very much like to recommend that particular paper.

116
00:16:32,360 --> 00:16:45,160
So yeah, I don't know how much detail we should go into, because it's, I mean, although it's not

117
00:16:45,160 --> 00:16:53,640
maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can

118
00:16:54,360 --> 00:17:02,280
construct these models using the concepts we've learned in previous chapters. So

119
00:17:03,880 --> 00:17:13,880
ultimately, we reach equations for point 13 and four point 14, which are basically the culmination

120
00:17:13,880 --> 00:17:21,720
of POMDP formulation using the vector notations and gradients and so on. So

121
00:17:25,640 --> 00:17:30,840
that's great. Then we go to continuous time. Yeah, great.

122
00:17:33,000 --> 00:17:40,360
A few things intervene in the continuous time chapter was that we'll just mention here, because

123
00:17:40,360 --> 00:17:46,200
they're kind of boxed or partitioned from the continuous time part, but they're following

124
00:17:46,200 --> 00:17:51,160
pages versus Markov blankets. We won't go into it here, but kind of footnote that or

125
00:17:51,160 --> 00:17:57,880
look at some other places where we talk about it outside of this chapter overview. Figure 4.4,

126
00:17:57,880 --> 00:18:07,080
Bayesian message passing. Again, a big topic. Let's kind of just go past it now. Back to the

127
00:18:07,160 --> 00:18:15,960
regularly scheduled continuous time generative model discussion. And then another box to the

128
00:18:15,960 --> 00:18:23,800
generalized coordinates of motion. So taking position plus derivatives of position. And that has

129
00:18:23,800 --> 00:18:33,400
some beneficial properties that are described and unpacked also elsewhere. Do you want to say

130
00:18:33,400 --> 00:18:47,480
anything about 4.5.2? Well, the only thing that comes to mind is although, as I said before,

131
00:18:48,760 --> 00:18:57,720
all the formulations here may look more, I mean, a bit too dense to understand at the first pass,

132
00:18:57,720 --> 00:19:12,840
but some of the key maybe components here could be obviously the material from box 4.2 and 4.3.

133
00:19:14,600 --> 00:19:25,080
I think are quite essential to understand the underlying principle behind deriving the continuous

134
00:19:25,080 --> 00:19:33,160
time situation because without Laplace approximation, what we would have in terms of free energy

135
00:19:33,160 --> 00:19:44,280
minimization would look very much like the Gibbs free energy. So I mean, the key distinction between

136
00:19:46,440 --> 00:19:52,120
the free energy principle as described in active inference literature, as opposed to Gibbs free

137
00:19:52,120 --> 00:20:02,200
energy, is the Laplace approximation. So this is what enables us to go from Gibbs free energy to,

138
00:20:03,560 --> 00:20:10,440
I mean, the variation of free energy. So yeah, that's, I mean, quite essential to

139
00:20:11,560 --> 00:20:19,400
make this, to be familiar with this essential approximation. And obviously, the concept of

140
00:20:19,400 --> 00:20:25,400
generalized coordinates of motion will come time and time again throughout the whole book,

141
00:20:25,400 --> 00:20:36,360
particularly in chapters eight and nine. So yeah, those two concepts, I believe, needs a bit more

142
00:20:36,920 --> 00:20:50,920
attention. So yeah, sounds good. Box 4.3 Laplace approximation equations, another message passing

143
00:20:51,560 --> 00:20:59,480
representation, and a summary. The key message to take away is that approximate Bayesian inference

144
00:20:59,480 --> 00:21:05,640
may be framed as minimizing a quantity known as variational free energy. This depends on a

145
00:21:05,640 --> 00:21:11,240
generative model that expresses our belief about how data are generated. Anything else you want to add?

146
00:21:17,000 --> 00:21:23,800
Nothing comes to mind at the moment, because, as I said, this is, I mean, we're still

147
00:21:24,840 --> 00:21:32,120
in the stage that we want to develop our essential tools to be used in the rest of the books. So

148
00:21:33,080 --> 00:21:40,040
here, up until now, I believe, by the end of chapter four, we have acquired all the essential

149
00:21:40,040 --> 00:21:49,800
necessary mathematical tools. And the next chapter, chapter five, is kind of acts like an interlude.

150
00:21:51,480 --> 00:22:00,920
And I don't think it's the direct, I mean, continuation of chapters one through four. So

151
00:22:01,880 --> 00:22:11,400
I believe the first section or the first part of the book, conceptually and mathematically ends here.

152
00:22:11,400 --> 00:22:19,000
So yeah, that's it. Yes, it's a little bit like the pragmatic modeling part gets foreshadowed

153
00:22:19,560 --> 00:22:26,840
or explored in five, now that we're all built up with four. All right, that's the end of the

154
00:22:26,840 --> 00:22:47,560
overview for four. Okay, chapter five is called message passing and neurobiology.

155
00:22:48,520 --> 00:22:51,560
What is your overview thought on chapter five?

156
00:22:54,840 --> 00:23:02,440
Okay, I mean, it's a kind of, I don't know, I had mixed feelings about this chapter, because

157
00:23:03,000 --> 00:23:10,840
on one hand, you see, as far as I understand active inference, although it originated as

158
00:23:11,560 --> 00:23:19,400
quote unquote, a unified theory of the brain, I don't think it's a neurobiological theory per se.

159
00:23:20,600 --> 00:23:29,080
Of course, there can be some correlations between neurobiological components or concepts

160
00:23:29,720 --> 00:23:40,440
with active inference, I mean, concepts. But I mean, it's not an essential premise

161
00:23:40,520 --> 00:23:49,160
of active inference theory to provide a theory, to provide a comprehensive theory about how

162
00:23:49,160 --> 00:24:01,000
the neurobiology of human brain or other organisms brain behave at a detailed and neuroanatomical

163
00:24:01,560 --> 00:24:12,360
level. But then again, it's nice to have these kinds of empirical correlations between the

164
00:24:12,360 --> 00:24:22,600
findings of neurobiology and the active inference theory. But I don't think it's one of active inference

165
00:24:23,320 --> 00:24:27,560
central assertions, at least to my understanding.

166
00:24:29,720 --> 00:24:36,760
Well said, very interesting framing. Well, chapter five definitely takes a very specific

167
00:24:37,560 --> 00:24:45,400
system of interest approach by highlighting one of the most studied areas, also one of the most

168
00:24:45,400 --> 00:24:56,520
relevant areas, which is mammalian neuroscience. And the chapter is going to introduce a few different

169
00:24:59,240 --> 00:25:06,360
motifs in the nervous system, and essentially build up towards figure 5.5,

170
00:25:07,080 --> 00:25:14,840
which is at the end of the chapter, and 5.5 wires together three specific neural systems

171
00:25:15,400 --> 00:25:22,520
that the chapter is going to focus on work in that area from. So Ali said it very well.

172
00:25:23,960 --> 00:25:30,840
Active inference was built up to in chapter four. Here is another level or type of science

173
00:25:30,840 --> 00:25:36,840
with assertions or with representations or mappings to any specific system. But this is the kind of

174
00:25:37,640 --> 00:25:45,560
modeling that has been built up and done by Friston, Par, Pizzolo, and others over the decades,

175
00:25:45,560 --> 00:25:53,720
with a focus coming from a human neuroimaging laboratory setting, a lot of focus and study

176
00:25:54,360 --> 00:25:58,920
and attention and funding and everything on the mammalian nervous system.

177
00:25:59,160 --> 00:26:10,840
But claims about the nervous system are not the basis of what active inference claims

178
00:26:11,400 --> 00:26:16,840
or how it's derived. But this is like an example case study in neurobiology,

179
00:26:17,560 --> 00:26:23,000
connecting back to some of the formalisms that we've just seen introduced in chapter four.

180
00:26:23,960 --> 00:26:32,440
Yeah, and to add a minor point to which I just said, I think it's important to draw attention to

181
00:26:33,080 --> 00:26:38,840
the last sentence of the last paragraph of the first page. It is important to draw a distinction

182
00:26:38,840 --> 00:26:44,680
between a principle, i.e. the minimization of free energy and a process theory about how this

183
00:26:44,680 --> 00:26:52,200
principle may be implemented in a certain kind of system. So I think this sentence here

184
00:26:53,000 --> 00:27:00,120
frames this chapter in relation to all the other technical chapters of this book. So if

185
00:27:01,000 --> 00:27:07,480
every other chapter is about developing, or at least up to now, was about developing

186
00:27:09,160 --> 00:27:16,280
the principled formalism of active inference, now chapter five provides a kind of preliminary

187
00:27:16,360 --> 00:27:23,400
sketch for the process theory of active inference, which is obviously far from an extensive

188
00:27:23,400 --> 00:27:29,960
theory, it's just a single chapter. But then again, it can provide some important

189
00:27:31,560 --> 00:27:38,360
signposts for anyone who wants to further investigate this area.

190
00:27:39,320 --> 00:27:48,200
Awesome. Free energy principle, Bayesian mechanics, all things in that area are on this principle,

191
00:27:49,000 --> 00:27:55,000
not responsive to empirical data. And then the process theory is about how the principle is

192
00:27:55,000 --> 00:28:02,440
implemented. So the specific generative models that are made, and how well they map, or how well

193
00:28:02,440 --> 00:28:07,960
they do in a portfolio of models that can have very different goals and assumptions and all of

194
00:28:07,960 --> 00:28:14,680
this. But the process theory implementation lets us develop hypotheses that are answerable to empirical

195
00:28:14,680 --> 00:28:25,080
data, like what is the kind of information or relationship between photons hitting the retina

196
00:28:25,720 --> 00:28:33,560
and changes in activity in neural systems. And that's an informational question or can be

197
00:28:33,560 --> 00:28:39,000
abstracted in a way to an informational question that it turns out does have empirical support

198
00:28:39,640 --> 00:28:44,920
and results in unique explanations and predictions. That doesn't mean that it always

199
00:28:44,920 --> 00:28:50,120
results in unique explanations and predictions, but a lot of citations are provided here.

200
00:28:51,640 --> 00:28:53,400
That's what we can explore in chapter five.

201
00:28:53,400 --> 00:29:03,720
The last paragraph of the first section describes that they're going to look at the

202
00:29:04,840 --> 00:29:11,080
three different neural systems. Okay, section 5.2, microcircuits and messages.

203
00:29:11,800 --> 00:29:24,600
What do you think, Oli? All right. So I mean, this chapter begins from how

204
00:29:25,320 --> 00:29:34,760
message passing happens in neurobiological terms and compare it to the way active inference

205
00:29:34,760 --> 00:29:44,040
frames this message passing mechanism. And specifically, if we look at figure 5.1 and

206
00:29:44,040 --> 00:29:53,800
compare this figure to the ones we've seen before in chapters one through four, I think it was in

207
00:29:53,800 --> 00:30:01,720
chapter four, we can see some clear parallels between how this kind of cortical message

208
00:30:01,720 --> 00:30:10,360
passing happens in the brain versus how it is framed in active inference literature. And as we

209
00:30:10,360 --> 00:30:20,440
can see, it's clearly inspired by the neurobiology of the brain. But then it's important to keep

210
00:30:20,440 --> 00:30:30,840
in mind that it's not a direct one-to-one mapping between these two models. This is just

211
00:30:32,120 --> 00:30:42,200
a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind

212
00:30:42,920 --> 00:30:52,200
to somehow be a bit more confident about the viability of the theory we want to use for

213
00:30:52,200 --> 00:31:00,360
message passing and active inference, which is to say it's not some haphazard theory that's just

214
00:31:00,440 --> 00:31:07,080
been developed for practical reasons. It has some basis in neurobiology, although

215
00:31:08,280 --> 00:31:16,040
it's not necessarily fully congruent with every detail of neurobiology.

216
00:31:19,400 --> 00:31:25,640
Great. The specific example is going to involve this one region of mammalian cortex tissue

217
00:31:26,360 --> 00:31:30,360
that has these six layers. And there's a ton of neurobiology.

218
00:31:32,680 --> 00:31:38,040
The big takeaway for figure 5.1 is that it's possible to graphically lay out

219
00:31:38,680 --> 00:31:45,800
nodes and variables and find some empirical correspondences. Again, some unique explanations

220
00:31:45,800 --> 00:31:53,320
and predictions in certain cases. And that's one kind of modeling where it's really trying to

221
00:31:53,320 --> 00:32:00,840
understand and improve the ability to do correlation and intervention and counterfactual

222
00:32:00,840 --> 00:32:08,120
causal type analysis with the real system of interest. Or in a more pedagogical setting or

223
00:32:08,120 --> 00:32:13,320
a research setting or an industrial setting, you might sweep across large families of structures

224
00:32:13,320 --> 00:32:22,120
of models and there's no need to be grounded to any biological structure at all. So this is just

225
00:32:22,120 --> 00:32:29,640
describing the specific neuroanatomical research that really arose out of the imaging work at UCL

226
00:32:29,640 --> 00:32:35,880
and the SPM package. So that's where a lot of this comes from. 5.2? Yeah, good.

227
00:32:36,600 --> 00:32:44,600
And sorry, just as a side note, I think watching one of Thomas Parr's lectures on neurobiology

228
00:32:44,600 --> 00:32:50,440
of active inference, which is available on YouTube, would really help to understand the materials of

229
00:32:50,440 --> 00:33:03,240
this chapter better. So I highly recommend watching that one. Thanks. Figure 5.2 gives a

230
00:33:03,240 --> 00:33:12,520
re-rendering of a kind of classical view of a hierarchical predictive coding system works.

231
00:33:13,480 --> 00:33:24,280
So here, abstracting a layer from the tissue six layer to just two layers here, computational layers

232
00:33:24,280 --> 00:33:32,680
now, and then showing how there's hierarchical communication within a layer, but also others,

233
00:33:32,680 --> 00:33:40,040
they're signaling within a layer and there's a hierarchy in Bayesian modeling with variables

234
00:33:40,120 --> 00:33:45,480
that are higher order predictions about other variables. And that's the basis of the predictive

235
00:33:45,480 --> 00:33:55,320
coding architecture. So 5.2 looks at some ways that the something that resonates with the cerebral

236
00:33:55,320 --> 00:34:03,000
cortical architecture enables what might computationally look like or have some really strong and

237
00:34:03,720 --> 00:34:12,280
explanatory values in actually relating to computationally a hierarchical Bayesian model,

238
00:34:12,280 --> 00:34:19,080
which could do various general tasks. All right, 5.3 is motor commands,

239
00:34:19,080 --> 00:34:27,720
leaving the prefrontal cortex going down to the butterfly looking cross section here. What is 5.3?

240
00:34:28,200 --> 00:34:38,920
Okay, so 5.3 moves to the other half of active inference framework, which is,

241
00:34:38,920 --> 00:34:49,960
I mean, how it can model the decision making and ultimately the movement of the agent in order to

242
00:34:49,960 --> 00:34:57,320
minimize the expected free energy as opposed to variational free energy that we saw in perceptual

243
00:34:58,040 --> 00:35:08,920
half of active inference. So it again provides a kind of correlation or analogy between the

244
00:35:08,920 --> 00:35:16,680
structural neural anatomy, particularly related to, I mean, the motor commands and

245
00:35:19,080 --> 00:35:26,520
how it can relate to active inference, particularly the continuous time active inference. So

246
00:35:27,720 --> 00:35:38,120
we can see that for, I mean, for the external event or, I'm sorry, for the external state,

247
00:35:38,680 --> 00:35:47,560
we can take, for example, the proprioceptive afferent, and then this proprioceptive afferent

248
00:35:47,560 --> 00:35:53,480
acts as a kind of Y for the continuous time active inference, which needs to be,

249
00:35:53,720 --> 00:36:03,640
I mean, processed in a way to optimize the expected free energy and how it relates to

250
00:36:04,200 --> 00:36:15,640
both attention and precision. We'll see a bit more detail about those terms and the relation

251
00:36:15,720 --> 00:36:26,200
between them in chapter eight, but I think here section 5.3 provides a good summary about the

252
00:36:26,200 --> 00:36:32,840
general paths through the motor command systems of neurobiology.

253
00:36:34,280 --> 00:36:41,960
Great. I'd say while the previous case study focused on how the connectivity within and between

254
00:36:42,840 --> 00:36:49,560
the cortical columns could have a computational relationship with a Bayesian

255
00:36:49,560 --> 00:36:56,120
hierarchical predictive coding architecture, the argument of the second case study is that a

256
00:36:56,120 --> 00:37:05,080
continuous input, continuous output, kind of set point seeking reflexive motor behavior

257
00:37:06,040 --> 00:37:13,320
with a moving set point with a descending moving set point enabling motion by changing ultimately

258
00:37:13,320 --> 00:37:19,640
the set point and enabling a variation in the strategies to reach that set point through

259
00:37:19,640 --> 00:37:26,360
different mechanisms. This is also describable in a compatible way.

260
00:37:27,080 --> 00:37:33,240
That's a shorter section. Now, section 5.4, subcortical structures.

261
00:37:34,840 --> 00:37:36,920
What would you say about this section?

262
00:37:38,920 --> 00:37:54,120
Okay. So, subcortical structures are very important in the decision making and, I mean,

263
00:37:55,080 --> 00:38:06,280
of the agents. So, obviously, here we need another kind of analogy between the way

264
00:38:07,400 --> 00:38:15,960
that these plannings and decision makings happen neuroanatomically with the way that

265
00:38:16,600 --> 00:38:23,080
that it's framed in active inference. But again, we can see it's clearly based on,

266
00:38:24,840 --> 00:38:30,840
I mean, at least some of the important elements we've seen from the previous chapters.

267
00:38:31,800 --> 00:38:41,000
So, for example, we saw how policy is described or how it relates to outcomes and preference and so

268
00:38:41,000 --> 00:38:49,480
on. We can see those elements are directly inspired by neuroanatomical structures. So,

269
00:38:52,520 --> 00:39:02,840
I guess that's, at least in my opinion, this section here 5.4 seems a bit more

270
00:39:03,160 --> 00:39:13,400
sketchy in the meaning that it doesn't go into quite the extensive details about how

271
00:39:14,760 --> 00:39:27,880
those structures can be compared. But for anyone who wants to further investigate these topics,

272
00:39:27,880 --> 00:39:38,120
there are some useful references put on here on pages 93 and 94. So, yeah.

273
00:39:39,640 --> 00:39:47,320
Thanks. Yeah, it's really abbreviated and over viewed. But we get an interlude from table 5.1

274
00:39:48,040 --> 00:39:55,000
with putative roles of neurotransmitters. So, same perspective that we took before on neuroanatomical

275
00:39:55,000 --> 00:40:03,240
functionalism here directly translates to neurotransmitter reductionism or essentialism

276
00:40:03,240 --> 00:40:06,920
or something like that. So, certainly all neurotransmitters and molecules that play

277
00:40:06,920 --> 00:40:13,800
variable roles in different settings. And this is the neat and scruffy

278
00:40:17,000 --> 00:40:23,880
manifold all over again. One person might say, well, we need a theory for every acetylcholine

279
00:40:23,880 --> 00:40:28,680
molecule in the world. They're all in a unique context. And someone else says,

280
00:40:28,680 --> 00:40:32,760
all neurotransmitters are described by one parameter in this model. I'm getting value

281
00:40:32,760 --> 00:40:38,920
from it. So, to me, that's an account. And somewhere in between is the work in this space,

282
00:40:39,640 --> 00:40:47,400
which is making an attempt to have a principled and falsifiable approach to

283
00:40:47,400 --> 00:40:56,120
model the computational aspects of specific regions and contexts and settings. And so,

284
00:40:56,120 --> 00:41:03,400
acetylcholine, noradrenaline, dopamine and serotonin are given a little mini review here.

285
00:41:04,600 --> 00:41:11,320
And so, it's not an exhaustive or an exclusive claim. It's kind of a provocation from computational

286
00:41:11,320 --> 00:41:17,240
and molecular neuroscience. And people can look into the papers and also ones that probably

287
00:41:17,240 --> 00:41:23,880
have been published since. 5.6 goes to continuous and discrete hierarchies,

288
00:41:24,760 --> 00:41:29,080
which is graphically overviewed in figure 5.5. So, what would you say about this?

289
00:41:31,800 --> 00:41:37,560
Yeah, one interesting thing about this section is the observation that

290
00:41:37,800 --> 00:41:46,440
our lower-level engagement with the environment can be most successfully

291
00:41:47,000 --> 00:41:54,520
characterized with continuous time formulations. But as we go up on the level of

292
00:41:56,120 --> 00:42:04,200
cognitive concepts or at the level of cognitive hierarchies, and we come to concepts such as,

293
00:42:04,280 --> 00:42:13,560
I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time

294
00:42:13,560 --> 00:42:23,720
situations would probably be more efficient to characterize the behavior of the agent. So, this

295
00:42:23,720 --> 00:42:34,120
multi-scale structure of active inference modeling is quite evident in the way that

296
00:42:35,080 --> 00:42:41,560
our message passing happens in our brain in terms of our lower-level data processing,

297
00:42:42,680 --> 00:42:50,200
often to consolidating the higher-level cognitive concepts and ontologies.

298
00:42:51,640 --> 00:42:59,560
Awesome. Thank you. To me, figure 5.5 demonstrates the kind of whole-of-body approach

299
00:42:59,960 --> 00:43:05,720
that you could imagine. There's so many organs and systems and phenomena for which

300
00:43:05,720 --> 00:43:09,640
there aren't specific generative models, so little can be said about situations where no

301
00:43:09,640 --> 00:43:15,480
generative model has been articulated. And here's one where it has, so it gives you also,

302
00:43:15,480 --> 00:43:21,000
it's kind of like reading a Drosophila melanogaster review paper relatively. It's like,

303
00:43:21,560 --> 00:43:26,600
this is how much work it takes to get to this state of knowledge in an insect. So then in

304
00:43:26,600 --> 00:43:33,160
another insect, do we know less about that insect empirically and genetically? So consider this to

305
00:43:33,160 --> 00:43:42,280
be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive

306
00:43:42,280 --> 00:43:48,200
systems, at least we know. So there's that additional kind of like self-reflexive aspect

307
00:43:48,200 --> 00:43:55,000
to this chapter that is not a cornerstone of active inference, but here it's just presented

308
00:43:55,000 --> 00:44:00,040
in a synthetic case study. Anything else you want to say about 5?

309
00:44:02,920 --> 00:44:05,640
Nothing particular comes to mind. Thank you.

310
00:44:05,640 --> 00:44:10,920
All right.

311
00:44:25,000 --> 00:44:34,920
Okay. Chapter seven is called active inference and discrete time.

312
00:44:35,880 --> 00:44:43,080
Chapter seven is the first in a pair of chapters with chapter eight on discrete and continuous

313
00:44:43,080 --> 00:44:51,320
time. So they're kind of like two forks of a river that we discussed in chapter four and before

314
00:44:51,880 --> 00:44:57,000
and described the recipe in chapter six. Now seven and eight are kind of like one level deeper,

315
00:44:57,720 --> 00:45:05,240
going from the kind of all of this group of animals to one level deeper into its classification scheme

316
00:45:05,960 --> 00:45:13,640
on the way to the specific generative model for which it's actually given in its totality.

317
00:45:13,640 --> 00:45:20,840
But everything prior to that is about the learning about its principles and this is kind of on the

318
00:45:20,840 --> 00:45:26,280
trunk of the path to discrete time modeling, just like chapter eight will be about continuous

319
00:45:26,280 --> 00:45:35,320
time modeling. What would you add in? Okay. So I think chapters seven and eight

320
00:45:35,960 --> 00:45:45,960
really helps to understand in a more practical way how the materials from particularly chapters one

321
00:45:45,960 --> 00:45:57,560
through five applies in real-time situations. So even if we somehow didn't get to understand

322
00:45:58,120 --> 00:46:04,680
every details of chapters one through four, when we come to chapters seven and eight,

323
00:46:05,640 --> 00:46:13,880
I think some of those uncertainties about our understandings can be clarified

324
00:46:16,840 --> 00:46:23,080
at least in a practical sense. So I believe these two chapters are really helpful

325
00:46:24,280 --> 00:46:28,280
in order to consolidate our understandings from the previous chapters.

326
00:46:28,680 --> 00:46:36,120
Awesome. Well said. So it's going to involve specifying some discrete time models.

327
00:46:38,920 --> 00:46:44,200
Seven point two goes into perceptual processing and the general structure of the chapter is going

328
00:46:44,200 --> 00:46:50,920
to walk through a series of examples that build in complexity where they first start with perception

329
00:46:51,000 --> 00:46:57,880
in seven point two, introduce decision making and then describe a few more types of motifs or

330
00:46:57,880 --> 00:47:06,120
cognitive structure or patterns and also check out step by step and model stream one where it's

331
00:47:06,120 --> 00:47:14,520
built up to in a different way. So the first example is I'll let you describe it since it's musical.

332
00:47:14,520 --> 00:47:26,760
Okay. So yeah, the first example is the situation in which we try to describe the performance

333
00:47:26,760 --> 00:47:35,160
of an amateur musician in terms of how we listen to the performance of an amateur musicians

334
00:47:35,400 --> 00:47:47,400
in terms of the predictions we get from our anticipation of the following notes as opposed to

335
00:47:47,400 --> 00:47:57,400
the actual notes that's being played. So these kinds of anticipatory reaction, listening reaction to

336
00:47:57,400 --> 00:48:04,200
the musician can be successfully formalized using discrete time active inference by

337
00:48:05,960 --> 00:48:20,440
putting together the matrices A for the states and matrix B for the transition between the states

338
00:48:20,440 --> 00:48:28,360
or the transition probabilities which in this case describes the probability from going from

339
00:48:28,360 --> 00:48:36,520
one note to the other and obviously the actual sequence that's been played which can be described

340
00:48:36,520 --> 00:48:45,960
with the matrix D. So and another point I wanted to point I wanted to mention is

341
00:48:47,160 --> 00:48:53,800
for anyone who has downloaded this chapter before, I don't know, I think about June or something,

342
00:48:54,680 --> 00:49:01,880
I recommend re-downloading it from MIT's website because they have corrected some of the typos

343
00:49:01,880 --> 00:49:07,000
that was previously present in this chapter, particularly in figure 7.2.

344
00:49:10,600 --> 00:49:18,680
Cool. So this graphical model where a person is listening, this is a general perceptual

345
00:49:18,680 --> 00:49:27,400
Bayesian framing, it's specified. Just like with any other equations, there's a lot to look into,

346
00:49:27,400 --> 00:49:34,600
but A indicates the probability of an outcome given a state. This is saying if it were all

347
00:49:35,640 --> 00:49:41,560
on the diagonal identity matrix, this is kind of a common motif, then states kind of map to

348
00:49:41,560 --> 00:49:53,400
themself. So in the context of, in the context of this model, A represents the mapping between the

349
00:49:53,400 --> 00:50:02,200
observed note and the underlying hidden true note. FNB describes the transition matrix of how those

350
00:50:02,200 --> 00:50:11,000
change to time D is the prior. They're specified. Figure 7.2, do you want to describe it?

351
00:50:15,240 --> 00:50:22,200
All right, so in figure 7.2, or at least the incomplete version of figure 7.2 we see here,

352
00:50:22,920 --> 00:50:34,600
well, at the upper left part of the picture, we see, I mean, the beliefs about each note

353
00:50:36,040 --> 00:50:45,480
at each step, at each time step. And at upper right, we somehow translate those beliefs into

354
00:50:46,280 --> 00:50:56,040
specific numerical values. So instead of just assigning some continuous values, we

355
00:50:57,960 --> 00:51:05,080
simplified the situation by assigning some discrete numerical values for each note.

356
00:51:05,800 --> 00:51:21,080
And then the lower left is supposed to show the free energy gradients over time or in other terms,

357
00:51:21,880 --> 00:51:31,960
the prediction errors we get from, I mean, comparing our predictions with the actual outcomes.

358
00:51:32,920 --> 00:51:41,720
So lastly, the lower right picture shows, in parallel to the upper right picture,

359
00:51:42,680 --> 00:51:53,800
determines the values of these errors. So we can see both the continuous, the initial,

360
00:51:54,360 --> 00:52:00,040
at least initial continuous assignment and values, and then the further

361
00:52:01,640 --> 00:52:09,880
discretizing of the values in order to get the discrete time situation or the more tractable

362
00:52:09,880 --> 00:52:18,680
discrete time situations. Okay, so it's a general passive inference task where there's

363
00:52:18,680 --> 00:52:23,240
priors about how states are going to change through time, and then there's real data coming in.

364
00:52:23,800 --> 00:52:29,000
So that's the kind of classical predictive coding, video compression, Kalman filter,

365
00:52:29,720 --> 00:52:39,560
Bayesian setting. 7.3 introduces a key motif, which is decision making and planning as inference.

366
00:52:40,200 --> 00:52:45,640
So this is the idea of having a Bayes graph where the variables can relate to different

367
00:52:45,640 --> 00:52:52,120
things. There's high composability. And here the idea is that a variable is going to be proposed

368
00:52:52,120 --> 00:52:59,000
that we can do inference about that describes the process of decision making or policy selection.

369
00:52:59,000 --> 00:53:10,360
So what would you say about 7.3? Okay, so 7.3 is obviously similar to what we

370
00:53:11,240 --> 00:53:20,520
saw in chapter four. And if I'm not mistaken, even the topology is exactly the same with that

371
00:53:20,520 --> 00:53:33,080
picture we saw previously. So this is the initial setup or which acts also as a review

372
00:53:34,120 --> 00:53:40,280
about how these different components upon DP generative models

373
00:53:40,360 --> 00:53:53,080
need to be described in such situations. But ultimately, the specific case study

374
00:53:54,200 --> 00:54:04,760
we come across in this section is the attempt to model the behavior of the mouse in a teammate,

375
00:54:04,840 --> 00:54:16,280
so the rat in a teammate. So especially teammates containing an aversive stimulus in one arm and

376
00:54:16,280 --> 00:54:23,800
an attractive stimulus on the other. So this is this can act as a kind of toy example to use

377
00:54:24,600 --> 00:54:32,120
this kind of probabilistic modeling to describe these situations.

378
00:54:35,720 --> 00:54:41,320
Thanks. So that leads us right to figure 7.4. Here's a visualization of the situation

379
00:54:42,040 --> 00:54:53,480
with the rat in this case, where there's a pleasant and aversive stimuli on each end of a

380
00:54:54,120 --> 00:55:03,240
decision point. And there's also a epistemic opportunity to receive some information

381
00:55:03,960 --> 00:55:13,160
about the context that the animal is in. And so that setting is described for both the case with

382
00:55:13,160 --> 00:55:19,480
white on the left, black on the right, and black on the left, white on the right. And those are shown

383
00:55:19,480 --> 00:55:24,840
in terms of their differences in the matrices, the explicit specification of the generative model.

384
00:55:27,720 --> 00:55:35,560
Visualizations show some of the slices of the B variable, which reflect different transition

385
00:55:35,560 --> 00:55:44,520
probabilities. C represents the preferences, which are expressed over the observable states.

386
00:55:45,400 --> 00:55:59,400
D reflects the priors on the different states that need priors. 7.4. What would you say about this?

387
00:55:59,400 --> 00:56:09,240
Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously

388
00:56:09,240 --> 00:56:22,120
saw in chapters 3 and sorry, 2 and 4, which is how the exact formulation for expected free energy

389
00:56:22,120 --> 00:56:29,720
can be used, sorry, variation free energy can be used to formulate the tradeoff between the

390
00:56:30,680 --> 00:56:39,960
I mean, information seeking and or at least between the epistemic value and information

391
00:56:39,960 --> 00:56:51,240
seeking. So here, it uses, again, that rad example in a bit more, more extended and elaborate form

392
00:56:51,240 --> 00:57:03,160
to formulate the epistemic value of observing Q in a given location. And figure 7.7 is a

393
00:57:03,160 --> 00:57:16,360
representation of this situation. But another situation that's been, let me see, yeah, in 7.9,

394
00:57:16,360 --> 00:57:26,840
another case study discussed here is the situation of the psychotic eye movements.

395
00:57:27,400 --> 00:57:36,920
And because it is something that can be quite successfully described or characterized

396
00:57:36,920 --> 00:57:44,120
in terms of information seeking versus the epistemic value. And the situation here is,

397
00:57:45,080 --> 00:57:55,880
let me see, yeah, shown visually in figure 7.9, which clearly shows how our visual

398
00:57:57,080 --> 00:58:06,920
psychotic eye movements can be described in such a way as to kind of trace the trajectory

399
00:58:07,880 --> 00:58:18,360
of our eye movement among different regions of the visual space. And how the information we gather

400
00:58:18,360 --> 00:58:30,040
from a given region can affect the, I mean, the subsequent trajectories of our psychotic eye movement.

401
00:58:30,040 --> 00:58:40,840
So, yeah, that's basically the main premise of this section, I guess.

402
00:58:41,880 --> 00:58:47,880
Nice, great. 7.5? What would you say about it?

403
00:58:48,440 --> 00:59:01,400
Okay, so 7.5, again, adds another dimension to the previous formulations. And this time,

404
00:59:02,120 --> 00:59:16,040
we get to update the generative models by learning. And so the generative models for

405
00:59:16,040 --> 00:59:23,240
this situation is a bit more complicated than the previous ones, because it now needs to

406
00:59:23,880 --> 00:59:32,520
account for a mechanism or a way to update the matrices we had before. So in the previous

407
00:59:32,520 --> 00:59:45,480
situations, we didn't account for learning, per se. But here, we directly update our general,

408
00:59:45,480 --> 00:59:55,240
sorry, the word update can be confusing here. We get to somehow improve our generative models to

409
00:59:56,200 --> 01:00:10,680
accommodate for these updating accounts. And yeah, so the situation here, or the case study

410
01:00:12,280 --> 01:00:24,600
here, which somehow elucidates the way that the learning can be accounted for with these models.

411
01:00:25,240 --> 01:00:39,800
Is again, a toy example of a creature in a simple world of black and white tiles, which kind of

412
01:00:39,800 --> 01:00:51,400
tries to find a path to reach a given destination, a certain destination. So it is more complicated

413
01:00:51,400 --> 01:01:00,520
than the situation we had for the rat example, because it only had, I mean, simple trajectories

414
01:01:00,520 --> 01:01:12,600
that needed to traverse. But here, the creature or the agent, in this case, needs to do lots

415
01:01:13,160 --> 01:01:21,000
of lots more learning and information seeking and so on. So all the previous elements

416
01:01:21,800 --> 01:01:29,480
is kind of combined in this example. And it's a really good example to see how the different

417
01:01:29,480 --> 01:01:38,680
components of active inference can be connected to each other. Nice. And 76 hierarchical or deep

418
01:01:38,680 --> 01:01:47,560
inference burst a box 7.3 interlude on structure learning boxed off topic and a lot to say.

419
01:01:48,200 --> 01:01:53,480
But structure learning broadly refers to learning the structure about a model,

420
01:01:54,680 --> 01:02:01,320
using the same types of methods that you might to do inference on, for example, a more observable

421
01:02:02,120 --> 01:02:13,240
sensor data reading, something like that. This section works towards the idea of nested inference

422
01:02:13,240 --> 01:02:18,280
or multi scale modeling. What would you say about figure seven 12?

423
01:02:18,600 --> 01:02:32,040
Okay, so again, this situation is, I think, the most complex situations of this chapter,

424
01:02:32,040 --> 01:02:40,920
which builds up from the previous sections. And this time, it adds another layer to accommodate

425
01:02:40,920 --> 01:02:52,120
for the inferences that happen in different time steps. So in this case, we have a multi

426
01:02:52,120 --> 01:03:01,480
time or multi scale inference and learning happening, both at the levels of learning

427
01:03:01,480 --> 01:03:09,400
and at the levels of information seeking. So this, this is represented in

428
01:03:11,800 --> 01:03:21,720
figure seven point 12, which represents how kind of this fractal generative model

429
01:03:22,520 --> 01:03:34,120
can be seen as a component in this multi scale, a bigger generative or as a kind of leaf in

430
01:03:34,120 --> 01:03:45,080
this bigger, bigger generative model. So it can be seen as a lower level inference happening at the

431
01:03:45,160 --> 01:03:54,280
leaf level, going up to the hierarchy and influencing, sorry, collaborating on the whole

432
01:03:54,280 --> 01:04:08,200
process of learning and inference at the higher level. So yeah, I guess that's somehow summarizes

433
01:04:08,200 --> 01:04:16,600
this figure. So if you have anything to add. That's, that's great. It's an example of the

434
01:04:16,600 --> 01:04:28,360
composability of generative models, what we've talked about and had Toby Sinclair Smith describe as

435
01:04:28,360 --> 01:04:34,920
as the compositional cognitive cartography, and just what kinds of connectors can and can't you do?

436
01:04:35,720 --> 01:04:42,600
And how can that motif that the discrete time model introduces? And then the rest of these

437
01:04:42,600 --> 01:04:48,760
features, including action and learning and so on get layered in on top. What can you do with that?

438
01:04:50,440 --> 01:04:57,400
713 gives another example. Do you want to say anything about it or maybe continue on?

439
01:04:58,360 --> 01:05:08,200
Yeah, so the case study here is the example of linguistic, I mean, language learning through

440
01:05:08,200 --> 01:05:18,520
reading. So not language learning. Maybe it's just what happens in reading. Yeah, in comprehension.

441
01:05:18,520 --> 01:05:29,640
So what happens when reading and in an anticipatory way, the words that that comes

442
01:05:31,000 --> 01:05:38,760
each after the other. So why this kind of situation can be most successfully characterized

443
01:05:38,760 --> 01:05:46,760
with this kind of modeling, because it involves different scales of learning and comprehension,

444
01:05:46,760 --> 01:05:57,160
both at the level of, I mean, reading at the level of somehow observing the letters and then

445
01:05:57,160 --> 01:06:05,640
going on to the words and then word groups and so on. So yeah, that's a really interesting way to,

446
01:06:06,600 --> 01:06:14,760
again, combine all of those elements into a single unified model to see how those different

447
01:06:14,760 --> 01:06:28,440
timescales, slow and fast timescales operate together to build this more encompassing model,

448
01:06:29,320 --> 01:06:31,720
more encompassing generative model of the situation.

449
01:06:33,560 --> 01:06:35,320
Great. Any closing thoughts on 7?

450
01:06:35,800 --> 01:06:41,000
Nothing particular, not bad. Thanks.

451
01:06:41,000 --> 01:06:55,880
All right. Next chapter is chapter eight, which is going to go into the continuous time.

452
01:07:06,040 --> 01:07:12,280
All right. Chapter eight is called active inference and continuous time begins with that

453
01:07:12,280 --> 01:07:17,880
timeless quote, everything flows, nothing stands still. So what would you say about chapter eight?

454
01:07:19,960 --> 01:07:27,560
All right. So this chapter probably is my most favorite chapter in the book,

455
01:07:27,560 --> 01:07:35,240
because of my own personal interest in, I don't know, the process materials and so on.

456
01:07:35,240 --> 01:07:45,400
But yeah, so chapter seven acts as a really good starting point for anyone who wants to

457
01:07:46,600 --> 01:07:51,400
develop the discrete time situations, to model discrete time situations

458
01:07:52,360 --> 01:07:59,320
within active inference framework. But in chapter eight, we kind of get to

459
01:08:01,640 --> 01:08:08,680
model a bit more interesting or, let's say, more involving situations.

460
01:08:09,640 --> 01:08:19,000
And they're not necessarily toy examples we saw at least at the beginning of chapter seven.

461
01:08:19,960 --> 01:08:27,960
So obviously, as the title suggests, this chapter deals with the continuous time situation.

462
01:08:27,960 --> 01:08:37,400
So in that case, we'll need to, maybe at this point, refresh our memory about

463
01:08:37,400 --> 01:08:42,280
what continuous time situation involves by reading the relevant parts,

464
01:08:43,160 --> 01:08:50,760
reading or reviewing relevant parts of chapter four. So yeah, in chapter four,

465
01:08:50,760 --> 01:08:58,920
we saw that the generative model for continuous time situation derives from the

466
01:09:00,120 --> 01:09:06,440
it is a stochastic calculus in terms of putting the whole process

467
01:09:06,760 --> 01:09:17,320
into two elements, two stochastic equations, one of which is the actual

468
01:09:18,120 --> 01:09:25,160
state, the condition of actual states or the behavior of the actual states. And the other one

469
01:09:25,800 --> 01:09:33,160
is the randomness that we need to account for in each real time continuous time situations.

470
01:09:33,240 --> 01:09:41,720
So that's what we get here in equation 8.1. And then, building up from that equation,

471
01:09:43,080 --> 01:09:54,440
we, it generalizes that equation to involve, I mean, the functionals of G and F instead of just

472
01:09:55,400 --> 01:10:05,000
the single valued functions of G and F. So then we get to

473
01:10:07,240 --> 01:10:15,400
put that into the situation that can be used for describing the behavior of dynamical systems,

474
01:10:16,040 --> 01:10:24,040
which is a very well known situation to use these kinds of stochastic equations.

475
01:10:24,680 --> 01:10:32,920
And it's widely studied how those, those kinds of dynamics can be characterized, especially

476
01:10:32,920 --> 01:10:42,680
in recent Bayesian mechanics paper by Dalton, Saktiv Atevel and others. So, and then it gets to

477
01:10:42,680 --> 01:10:51,320
some more specific examples such as Lothgabal-Terra dynamics and synchronicity and so on, in order

478
01:10:51,320 --> 01:11:04,280
to show how these kinds of dynamics can be elaborated upon and can be generalized to,

479
01:11:04,280 --> 01:11:12,440
and enables them to characterize more complex situations. So,

480
01:11:15,400 --> 01:11:21,640
yeah, that's a really short and brief overview of the whole chapter. Maybe

481
01:11:22,840 --> 01:11:25,560
we can talk about a bit more details as we go through it.

482
01:11:25,800 --> 01:11:34,600
Great. Well said. Well, I'm sure for another day, the philosophical implications of eight,

483
01:11:34,600 --> 01:11:39,320
seven and eight, and high road and low road, and all these other parts of the textbook, great topics.

484
01:11:40,200 --> 01:11:48,600
I agree. I would see chapter eight as demonstrating continuity with some classical

485
01:11:49,560 --> 01:11:55,880
continuous time modeling motifs from a few different areas of dynamical system science,

486
01:11:56,600 --> 01:12:01,240
which is applied in like many, many, many fields, but these are some classic examples.

487
01:12:01,240 --> 01:12:07,720
So, figure eight point one goes a little bit more into depth, or at least into more formalism

488
01:12:07,720 --> 01:12:15,480
detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive

489
01:12:15,560 --> 01:12:20,280
data coming in, and then a differential being calculated with the set point,

490
01:12:20,280 --> 01:12:27,400
which reflects a descending prediction from a decision making layer. And that can be viewed as

491
01:12:27,400 --> 01:12:35,320
this kind of mechanics that plays out in a phase space in continuous time, like a spring moving

492
01:12:35,320 --> 01:12:42,280
around with someone making a certain path with an attractor, and a spring being dragged around

493
01:12:42,280 --> 01:12:49,560
something in that area. Box eight point one goes into a very fascinating topic. Do you want to

494
01:12:49,560 --> 01:13:03,960
describe it? Well, it's maybe one of the most thought provoking pages of the whole book. And

495
01:13:03,960 --> 01:13:09,320
if I remember correctly, in all of the cohorts, this particular box

496
01:13:11,800 --> 01:13:18,840
I mean gives always gives rise to lots of questions, because of some of the interesting

497
01:13:18,840 --> 01:13:29,880
and at least initially counterintuitive claims here. But I don't want to spoil it. So

498
01:13:30,440 --> 01:13:46,520
but as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief

499
01:13:46,520 --> 01:13:53,800
discussion about the comparing these terms precision, attention, and sensory attenuation,

500
01:13:53,800 --> 01:14:00,680
and the relation and similarities and difference between these two, these three terms,

501
01:14:00,680 --> 01:14:06,760
and how each understanding each of them is essential to understanding the other ones.

502
01:14:07,640 --> 01:14:15,400
But as I said, it's a really interesting topic, which gives rise to lots of discussions.

503
01:14:15,560 --> 01:14:26,040
And I believe it's one of those topics that that's worth looking a bit more

504
01:14:27,400 --> 01:14:30,920
looking into in some other literature as well.

505
01:14:32,040 --> 01:14:40,120
Great. Well said. What a cliffhanger. Next, they go to a classic model family called Laka Volterra.

506
01:14:40,120 --> 01:14:46,920
These dynamics inherit from characterizations of predator prey dynamics in ecology. So it's kind

507
01:14:46,920 --> 01:14:55,000
of a classical ecology model shown in figure 8.2. On the top, it's actually the ecosystem model. Plants,

508
01:14:55,000 --> 01:15:02,040
herbivores and carnivores, which follow different kinds of oscillatory trends in continuous time.

509
01:15:02,680 --> 01:15:09,160
And so that also has enabled it to be applied for other so-called winnerless competitions.

510
01:15:09,880 --> 01:15:15,240
And that relates to topics like neural Darwinism and also neural dynamics, where things have

511
01:15:15,240 --> 01:15:21,800
kind of oscillatory relationships with each other, which are being modeled as a continuous time

512
01:15:21,800 --> 01:15:29,000
underlying process with a lot of measurement noise and discretization through space and time.

513
01:15:29,000 --> 01:15:33,880
Those are the kinds of algorithms that SPM explores more. And there's Laka Volterra and a

514
01:15:33,880 --> 01:15:41,960
lot of other dynamical systems theory in SPM. So active inference kind of adds action and more

515
01:15:42,680 --> 01:15:50,280
to what was laid out from a pure dynamical systems theory in SPM. Here, it really is just

516
01:15:50,280 --> 01:15:55,560
showing the ecology example and how you can project. If you have three different species,

517
01:15:55,560 --> 01:16:02,760
you can think about that motion in a cube or tetrahedron. And then you could project onto

518
01:16:03,400 --> 01:16:09,800
kind of like looking at a lower dimensional manifold relating just two of the three species.

519
01:16:10,520 --> 01:16:15,800
And that evinces this kind of oscillatory but also moving behavior.

520
01:16:16,920 --> 01:16:23,400
That gets connected in figure 8.3 to neurobiology. What would you say about this?

521
01:16:23,400 --> 01:16:35,560
Okay, so here in figure 8.3, we see some applications of Laka Volterra dynamics.

522
01:16:36,360 --> 01:16:50,920
So the left column here represents what happens in, I mean, in eye blinking, eye blink conditioning.

523
01:16:51,720 --> 01:17:08,360
So, of course, here we need to account for, I mean, the expected states of the sequences of events

524
01:17:08,360 --> 01:17:18,440
that happens in the eye blinking. So the upper left figure shows the expectations

525
01:17:19,320 --> 01:17:29,560
in terms of time. And then the parallel right hand side equation, sorry, right hand side figures,

526
01:17:30,280 --> 01:17:41,560
shows the Laka Volterra system that is applied in the handwriting situation. So as we can see,

527
01:17:41,560 --> 01:17:49,480
although the, I mean, mathematical technology is the same or at least the modeling technology is

528
01:17:49,480 --> 01:18:04,200
the same, the outcome of each situation varies drastically in two distinct neurobiological

529
01:18:04,280 --> 01:18:13,240
behavior, not neurobiological, but biological behavior. So, yeah, we can see how the same

530
01:18:13,240 --> 01:18:25,560
modeling framework can give rise to different outcomes based on what parameters needs to be

531
01:18:25,560 --> 01:18:34,120
optimized, what parameters are selected for modeling and so on. So I believe it's a quite

532
01:18:35,080 --> 01:18:45,720
interesting example to compare handwriting and the blinking together and how those can be compared

533
01:18:45,720 --> 01:18:55,160
to each other using the Laka Volterra dynamics. Great, thank you. Box 8.2 gives a variant on the

534
01:18:55,160 --> 01:19:00,760
learning here presented with the formalism for continuous models, kind of a technical aside.

535
01:19:01,720 --> 01:19:09,560
Section 8.4 is about generalized synchrony. So figure 8.4 is going to visualize one of the

536
01:19:09,560 --> 01:19:14,520
classic dynamical systems, which is the Lorenz attractor. So what would you say about this

537
01:19:16,120 --> 01:19:25,880
figure? Okay, so this section is truly interesting because when one thinks of active inference,

538
01:19:25,880 --> 01:19:35,000
probably the first situations that comes to mind is the situations in which we have quite well

539
01:19:35,000 --> 01:19:44,280
defined probability distributions for different parameters. But as we can see here in section

540
01:19:44,280 --> 01:19:53,400
8.4, actually some of the formalism of active inference can be successfully used to characterize

541
01:19:53,400 --> 01:20:01,080
even chaotic systems and in particular the way in which two chaotic systems can be synchronized

542
01:20:01,080 --> 01:20:13,160
with each other. So this is a classic example of a chaotic Lorenz system, and it draws upon

543
01:20:14,040 --> 01:20:22,920
from some of Professor Pristin's earlier work on birdsong synchrony. And as a side note, any

544
01:20:23,000 --> 01:20:31,000
literature before 2016 is considered earlier history in active inference literature because

545
01:20:31,000 --> 01:20:43,640
it evolves quite rapidly. So yeah, this kind of synchrony between two chaotic systems can be

546
01:20:44,600 --> 01:20:56,280
interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of

547
01:20:56,280 --> 01:21:11,080
mind in the sense that how exactly can we understand or can two agents can trace each

548
01:21:11,240 --> 01:21:23,640
other's trajectories without, I mean, engaging in any direct exchange of observations between

549
01:21:23,640 --> 01:21:31,320
their internal and external states. So yeah, that's a really good example and I believe one of the

550
01:21:31,320 --> 01:21:38,520
most interesting examples of how active inference can even account for these kinds of behavior.

551
01:21:39,160 --> 01:21:46,760
So the rest of the section goes into the details of how this kind of synchrony between

552
01:21:48,760 --> 01:22:00,920
multi-scale Lorenz systems can happen and how can we formulate it mathematically in terms of

553
01:22:00,920 --> 01:22:08,600
continuous time active inference. Awesome. And there's been more recent work on Mark

554
01:22:08,600 --> 01:22:16,280
Alblanket since stochastic chaos, but the bird example is a classic. 8.5 goes into hybrid discrete

555
01:22:16,280 --> 01:22:23,080
and continuous models. So this could be kind of like an in-between chapter of seven and eight,

556
01:22:23,080 --> 01:22:27,960
but now that we've been introduced to the pure form of discrete and the pure form of continuous

557
01:22:28,040 --> 01:22:34,840
models, here's shown that that composability extends to so-called hybrid models, where here

558
01:22:35,400 --> 01:22:43,800
the lower level visually is using the continuous time formalism and the higher level is describing

559
01:22:43,800 --> 01:22:51,080
the little line added here, the discrete time formalism. And this was the similar structure

560
01:22:51,720 --> 01:22:58,200
described by the authors of the paper, active inference does not contradict folk psychology,

561
01:22:59,240 --> 01:23:04,760
where they describe this lower level as motor active inference, which was closely

562
01:23:04,760 --> 01:23:13,160
allied with the spinal arc reflex shown above. And then this higher level, they call decision

563
01:23:13,160 --> 01:23:16,840
active inference, because in that case, it was referring to a discrete decision.

564
01:23:17,480 --> 01:23:24,600
And so they used that kind of basic motif of continuous activity or continuous time modeling

565
01:23:25,240 --> 01:23:32,440
at the more peripheral aspects of a cognitive entity. And like Ali said, more discretization

566
01:23:33,880 --> 01:23:39,400
and hybridization as well at higher levels of the cognitive modeling.

567
01:23:39,560 --> 01:23:48,200
And that type of an architecture here, instead of describing who wants the ice cream cone,

568
01:23:48,200 --> 01:23:55,320
I believe, here, it's going to be a mixed or hybrid model that is going to call back the

569
01:23:55,320 --> 01:24:07,240
isocade system, where there's a fixed point that is able to be moved as a set point. And then

570
01:24:07,240 --> 01:24:13,800
there's a continuous time isocade that pursues the new fixed point. And so that's analogous to a new

571
01:24:14,520 --> 01:24:20,600
set point or fixed point being specified from the top down muscle command about a new location

572
01:24:20,600 --> 01:24:27,720
for a muscle, followed by movement towards it. This is a muscular activity that is realizing

573
01:24:27,720 --> 01:24:34,680
that but but not in the elbow coming away from the hot stove. This is about the ice caking to an

574
01:24:34,680 --> 01:24:43,400
epistemic foraging location specified by top down hierarchical systems. 8.3 describes little

575
01:24:43,400 --> 01:24:51,800
technical aside on mixture of Gaussian Gaussian mixture models, kind of a technical modeling

576
01:24:51,800 --> 01:24:59,160
note. And 8.6 closes. It says it's a huge topic and much has been left out. And so they list in

577
01:24:59,160 --> 01:25:05,480
table 8.1 key advances in continuous time models. And those areas are synthetic bird song,

578
01:25:06,280 --> 01:25:12,440
ocular motor delays, conditioned reflexes, smooth pursuit, eye movement, psychosis, illusions,

579
01:25:13,240 --> 01:25:21,160
saccades, action observation, attention, hybrid models and self organization. And that's chapter 8.

580
01:25:21,880 --> 01:25:27,880
What else would you say? And also what would you kind of lead someone to in the philosophical

581
01:25:27,880 --> 01:25:39,560
implications of 8 because it sounds kind of cool? Okay, so well, the case of continuous time active

582
01:25:39,560 --> 01:25:52,680
inference. I think it leads to really interesting questions, both in terms of philosophical questions

583
01:25:52,680 --> 01:26:02,920
and also more practical modeling questions about what parameters needs to be accounted for and so

584
01:26:02,920 --> 01:26:14,920
on. And as I said, I believe it's a more more interesting way of if not interesting, but but

585
01:26:14,920 --> 01:26:24,120
at least more involved way of doing active inference modeling. But one thing that one of the

586
01:26:24,120 --> 01:26:37,960
philosophical questions that Mao and I have explored in our paper is how the processes of I mean,

587
01:26:38,040 --> 01:26:50,040
ontological processes can philosophically described using FPP assertions in terms of their

588
01:26:50,040 --> 01:27:00,200
interaction with the environment in which they co constitute themselves. And we don't necessarily

589
01:27:01,160 --> 01:27:09,160
distinguish between between the internal and the external states. So one obvious example of this

590
01:27:09,160 --> 01:27:17,000
is that generalized synchrony example that we saw in this chapter, in which we don't necessarily

591
01:27:17,000 --> 01:27:27,480
distinguish between which of the birds act as the agents and which one is the environment or the

592
01:27:27,480 --> 01:27:34,920
vice versa. So these kinds of co constitution of the environment and the agent, which gives rise

593
01:27:34,920 --> 01:27:43,000
to the partitioning of state space through a Markov blanket is one of the interesting

594
01:27:45,240 --> 01:27:52,520
philosophical points that I think needs to be elaborated a bit more using

595
01:27:52,920 --> 01:28:02,600
some of the recent advances in philosophy, such as the tools that's been developed in new materialism

596
01:28:03,640 --> 01:28:10,200
school or some other philosophical approach approaches. But yeah, these kinds of

597
01:28:11,560 --> 01:28:18,120
what exactly gives rise gives rise to emergence, what is the ontological status of emergent

598
01:28:18,200 --> 01:28:26,440
properties and so on, are some of the burning questions for many philosophers today. And I

599
01:28:26,440 --> 01:28:32,600
believe active inference and particularly continuous time active inference provides a clear,

600
01:28:34,360 --> 01:28:45,480
precise mathematical formalism. Even if not to answer these questions, but at least

601
01:28:45,480 --> 01:28:56,280
to explore it in a more rigorous and practical way, and also practical and a tractable way.

602
01:28:56,280 --> 01:29:07,560
So this is the area that I believe philosophy and science are beautifully intertwined into a coherent

603
01:29:07,560 --> 01:29:14,680
view of not only the phenomenon of interest, but even about the whole world.

604
01:29:17,480 --> 01:29:28,760
Wow. Wow. Pretty cool. Yeah, a lot to say about that topic. After completing chapter seven and eight,

605
01:29:29,560 --> 01:29:36,840
you've seen the kind of two major branches or two major motifs of just one kind of modeling.

606
01:29:36,840 --> 01:29:43,240
But these kind of models have so many different forms that that's why it's such a hands on process

607
01:29:43,240 --> 01:29:49,240
to specify the generative model in chapter six and fit it with data in chapter nine. Those are all

608
01:29:49,240 --> 01:29:56,280
what's required. And that's kind of the last mile of where these discussions about general motifs

609
01:29:56,280 --> 01:30:02,840
gets you. But also playing with these pedagogical models can be really helpful, because it will

610
01:30:02,840 --> 01:30:09,160
help you understand the basic patterns and relationships and start to see see different

611
01:30:09,160 --> 01:30:16,840
patterns in the graphical models and know from there what levels of technical processes can be

612
01:30:16,840 --> 01:30:31,960
kind of coarse grained over. All right. Okay, well, that's it. I guess next time we will do probably

613
01:30:32,840 --> 01:30:44,360
nine, 10, and maybe something else. All right, I'll end it now. Thanks, Holly. Thank you.

