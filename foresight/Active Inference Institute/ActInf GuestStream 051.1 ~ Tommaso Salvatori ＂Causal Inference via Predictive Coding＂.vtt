WEBVTT

00:00.000 --> 00:21.480
Hello and welcome.

00:21.480 --> 00:28.420
It's active inference gas stream number 51.1 on July 28th, 2023.

00:28.420 --> 00:34.520
We are here with Tomaso Salvatore and we will be having a presentation and a discussion

00:34.520 --> 00:39.420
on the recent work, causal inference via predictive coding.

00:39.420 --> 00:42.160
So thanks so much for joining.

00:42.160 --> 00:47.220
For those who are watching live, feel free to write questions in the live chat and off

00:47.220 --> 00:48.220
to you.

00:48.220 --> 00:49.220
Thank you.

00:49.220 --> 00:54.700
Thank you very much, Daniel, for inviting me.

00:54.700 --> 00:59.540
It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite

00:59.540 --> 01:04.500
excited to be here and be the one speaking this time.

01:04.500 --> 01:09.540
So I'm going to talk about this recent preprint that I put out, which has been the work of

01:09.540 --> 01:12.620
the last couple of months.

01:12.620 --> 01:22.220
And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz.

01:22.220 --> 01:27.820
It's basically a joint work between Versus, which is the company I work for, the University

01:27.820 --> 01:31.860
of Oxford and Theo Vien.

01:31.860 --> 01:42.040
So during this talk, I will, this is basically the outline of the talk, I will start talking

01:42.040 --> 01:49.340
about what predictive coding is and give an introduction of what it is, a brief historical

01:49.340 --> 01:55.820
introduction, why I think it's important to study predictive coding, even for example

01:55.820 --> 01:58.740
for the machine learning perspective.

01:58.740 --> 02:06.100
I will then provide a small intro to what causal inference is.

02:06.100 --> 02:12.300
And once we have all those informations together, I will then discuss why I wrote this paper,

02:12.300 --> 02:18.820
what was basically the research question that inspired me and the other collaborators.

02:19.820 --> 02:27.380
And present the main results, which are how to perform inference, so intervention and

02:27.380 --> 02:34.980
counterfactual inference, and how to learn the causal structures from a given data set

02:34.980 --> 02:37.060
using predictive coding.

02:37.060 --> 02:43.980
And then I will of course conclude with a small summary and some discussion on why I

02:44.020 --> 02:48.180
believe this work can be impactful in some future directions.

02:50.780 --> 02:53.460
So what is predictive coding?

02:53.460 --> 02:58.780
Predictive coding is in general famous for being a neuroscience inspired learning method,

02:58.780 --> 03:03.300
so a theory of how information processing in the brain works.

03:04.660 --> 03:11.380
And brain formally speaking, the theory of predictive coding can be described as basically

03:11.380 --> 03:18.860
having a hierarchical structure of neurons in the brain and you have two different families

03:18.860 --> 03:20.980
of neurons in the brain.

03:20.980 --> 03:27.540
The first family is the one in charge of sending prediction information, so neurons in a specific

03:27.540 --> 03:36.420
level of the hierarchy send information and predict the activity of the level below.

03:36.420 --> 03:40.100
And the second family of neurons is that of error neurons.

03:40.100 --> 03:45.140
And the error neurons, they send prediction error information up the hierarchy.

03:45.140 --> 03:49.580
So one level predicts the activity of the level below.

03:49.580 --> 03:54.460
This activity has some, this prediction has some mismatch, which we were actually going

03:54.460 --> 03:56.540
on in the level below.

03:56.540 --> 04:01.620
And the information about the prediction error gets sent up the hierarchy.

04:02.540 --> 04:10.540
However, predictive coding was actually not burned as a neuroscience, as a theory from

04:10.540 --> 04:15.740
neurosciences, but it was actually initially developed as a method for signal processing

04:15.740 --> 04:18.500
and compression back in the 50s.

04:18.500 --> 04:26.980
So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that

04:26.980 --> 04:34.660
once we have a predictor, a model that works that is well in predicting data, sending messages

04:34.660 --> 04:41.100
about the error in those predictions is actually much cheaper than sending the entire message

04:41.100 --> 04:43.460
every time.

04:43.460 --> 04:49.300
And this is how predictive coding was born, so as a signal processing and compression

04:49.300 --> 04:53.900
mechanism in information theory back in the 50s.

04:53.900 --> 05:04.060
He was actually in the 80s, that he became that exactly the same model was used in neuroscience.

05:04.060 --> 05:11.500
And so with the work from Mumford or other works, for example, explain how the rate enough

05:11.500 --> 05:15.980
processing formation, so we get prediction signals from the outside world, and we need

05:15.980 --> 05:23.020
to compress these representation and have this internal representation in our neurons.

05:23.020 --> 05:29.660
And the method is very similar, if not equivalent to the one that was developed by Elias and

05:29.660 --> 05:33.420
Oliver in the 50s.

05:33.420 --> 05:39.900
Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul

05:39.900 --> 05:47.100
and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical

05:47.100 --> 05:53.980
structures in the brain, where prediction information is top down and error information

05:53.980 --> 05:55.860
is bottom up.

05:55.860 --> 06:02.340
And something that they did that wasn't done before is that they explain and develop this

06:02.340 --> 06:08.700
theory about not only inference, but also about how learning works in the brain.

06:08.700 --> 06:13.660
So it's also a theory of how our synapses get updated.

06:13.660 --> 06:19.380
And the last big breakthrough that I'm going to talk about in this brief historical introduction

06:19.380 --> 06:28.620
is from 2003, but then it kept going in the years after, thanks to Carfriston, in which

06:28.620 --> 06:36.780
basically he took the theory of Raoul and Ballard, and he extended it and generalized

06:36.780 --> 06:40.740
it to the theory of generative models.

06:40.740 --> 06:47.140
So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization

06:47.140 --> 06:55.340
scheme of a specific kind of generative model, which I'm going to introduce later as well.

06:55.340 --> 07:04.220
So to make a brief summary, the first two kinds of predictive coding that I described,

07:04.220 --> 07:08.980
so signal processing and compression and the information processing in the retina and in

07:08.980 --> 07:12.820
the brain in general, they are inference methods.

07:12.820 --> 07:21.060
And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st

07:21.060 --> 07:25.100
century, is that predictive coding was seen as a learning algorithm.

07:25.100 --> 07:31.180
So we can first compress information and then update all the synapses or all the latent

07:31.180 --> 07:37.540
variables that we have in our generative model to improve our generative model itself.

07:38.540 --> 07:44.660
So let's give some definitions that are a little bit more formal.

07:44.660 --> 07:50.820
So predictive coding can be seen as a hierarchical Gaussian generative model.

07:50.820 --> 07:55.500
So here is a very simple figure in which we have this hierarchical structure, which can

07:55.500 --> 07:58.860
be as deep as we want.

07:58.860 --> 08:05.580
And prediction signals go from one latent variable, Xn, to the following one, and it

08:05.580 --> 08:10.460
gets transformed every time via function gn, or gi.

08:10.460 --> 08:19.540
And this is a generative model, as I said, and what's the marginal probability of this

08:19.540 --> 08:20.540
generative model?

08:20.540 --> 08:26.620
Well, it's simply the probability of the last, can you see my cursor?

08:26.620 --> 08:27.620
Yes, right?

08:27.620 --> 08:28.620
Yes, perfect.

08:28.620 --> 08:35.140
So it's the generative model of the last vertex, is the distribution of the last vertex, times

08:35.140 --> 08:40.740
the probability distribution of every other vertex, conditioned on the activity of the

08:40.740 --> 08:46.060
vertex before, or the latent variable before.

08:46.060 --> 08:50.980
I earlier said that it's a Gaussian generative model, which means that those probabilities

08:50.980 --> 09:02.380
they are in Gaussian form, and those function g, in general, and especially since, for

09:02.380 --> 09:07.060
example, in a round baller paper, and in all the papers that came afterwards, also because

09:07.060 --> 09:14.580
of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with

09:14.580 --> 09:23.580
activation functions, or nonlinear maps with activation function and an additive bias.

09:23.580 --> 09:29.420
So we can give a formal definition of predictive coding, and we can say that predictive coding

09:29.420 --> 09:35.180
is an inversion scheme for such a generative model, where its model evidence is maximized

09:35.180 --> 09:41.180
by minimizing a quantity that is called the variational free energy.

09:41.180 --> 09:46.420
In general, the goal of every generative model is to maximize model evidence, but this quantity

09:46.420 --> 09:55.420
is always intractable, and we have some techniques that allow us to approximate the solution.

09:55.420 --> 10:02.300
And the one that we use in predictive coding is minimizing a variational free energy, which

10:02.300 --> 10:05.580
is a lower bound of the model evidence.

10:05.580 --> 10:12.340
In this work, and actually in a lot of other ones, so is the standard way of doing it,

10:12.340 --> 10:19.860
this minimization is performed via gradient descent, and there are actually other methods

10:19.860 --> 10:24.780
such as expectation maximization, which is often equivalent, or you can use some other

10:24.780 --> 10:31.300
message-passing algorithms such as belief propagation, for example.

10:31.300 --> 10:36.380
And going a little bit back in time, so we're getting a little bit about the statistical

10:36.380 --> 10:44.460
generative models, we can see predictive coding, as I said already a couple of times, as a

10:44.460 --> 10:50.620
hierarchical model with neural activities, so with neurons, latent variables that represent

10:50.620 --> 10:56.780
neural activities, they send their signal down the hierarchy, and with error nodes or

10:56.780 --> 11:02.300
error neurons, they send their signal up the hierarchy, so they send the error information

11:02.300 --> 11:03.300
back.

11:03.300 --> 11:08.020
What's the variational free energy of these class-operated coding models?

11:08.020 --> 11:15.820
It's simply the sum of the mean square error of all the error neurons, so it's the sum

11:15.820 --> 11:22.460
of the total error squared.

11:22.460 --> 11:28.180
And this representation is going to be useful in the later slides, and I'm going to explain

11:28.180 --> 11:31.580
how to use predictive coding to model causal inference, for example.

11:31.580 --> 11:37.100
What do you think predictive coding is important and is a nice algorithm to study?

11:37.100 --> 11:41.260
Well, first of all, as I said earlier, it optimizes the correct objective, which is

11:41.260 --> 11:48.300
the model evidence or marginal likelihood, and then it does so by optimizing a lower bound,

11:48.300 --> 11:52.620
which is called the variational free energy, as I said, and the variational free energy

11:52.620 --> 11:58.900
is interesting because it can be written as a sum of two different terms, which are

11:58.900 --> 12:06.300
and each of those terms optimizing it as important impacts, for example, in machine learning

12:06.300 --> 12:09.300
tasks or in general in learning tasks.

12:09.300 --> 12:12.660
So one of those terms forces memorization.

12:12.660 --> 12:19.860
So the second term basically tells forces the model to fit a specific data set.

12:19.860 --> 12:24.860
And the first term forces the model to minimize the complexity.

12:24.860 --> 12:31.060
And as we know, for example, from the Occam's razor theory, if we have two different models

12:31.060 --> 12:35.980
that perform similarly on a specific training set, the one that we have to get and the one

12:35.980 --> 12:41.340
that is expected to generalize the most is the less complex one.

12:41.340 --> 12:48.860
So updating generative model via variational free energy allows us to basically converge

12:48.860 --> 12:55.900
to the optimal Occam razor model, which both memorizes a data set, but is also able to

12:55.900 --> 13:01.020
generalize very well on unseen data points.

13:01.020 --> 13:09.980
A second reason why predictive coding is important is that it actually doesn't have

13:09.980 --> 13:15.500
to be defined on a hierarchical structure, but it can be modeled on more complex and

13:15.500 --> 13:22.100
flexible architectures such as directed graphical model with any shape or generalized even more

13:22.100 --> 13:25.580
to networks with a lot of cycles that resemble brain region.

13:25.580 --> 13:31.500
And the underlying reason is that you're not learning and predicting with a forward

13:31.500 --> 13:36.900
pass and then back propagating the error, but you're minimizing an energy function.

13:36.900 --> 13:43.780
And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and

13:43.780 --> 13:46.580
allow to learn cycles.

13:46.580 --> 13:50.700
And this is actually quite important because the brain is full of cycles as we have some

13:50.700 --> 13:58.060
information from some recent papers that may manage to map completely the brain of some

13:58.060 --> 14:00.980
animals such as fruit fly.

14:00.980 --> 14:02.380
The brain is full of cycles.

14:02.380 --> 14:11.020
So it makes sense to train our machine learning models or our models in general with an algorithm

14:11.020 --> 14:17.500
that allows us to train using cyclic structures.

14:17.500 --> 14:22.100
The third reason why predictive coding is interesting is that it has been formally proven

14:22.100 --> 14:26.340
that it is more robust than standard neural networks trained with back propagation.

14:26.340 --> 14:31.900
So if you have a neural network and you want to perform classification tasks, you, predictive

14:31.900 --> 14:34.500
coding is more robust.

14:34.500 --> 14:40.340
And this is interesting in tasks such as online learning, training on small datasets

14:40.340 --> 14:42.900
or continuous learning tasks.

14:42.900 --> 14:48.300
And the theory basically comes from the fact that imperative coding has been proved to

14:48.300 --> 14:53.980
approximate implicit gradient descent, which is a different version of the explicit gradient

14:53.980 --> 15:00.100
descent, which is the standard gradient descent used in the, in every single model basically.

15:00.100 --> 15:06.020
And it's a variation that is more robust.

15:06.020 --> 15:08.900
I think, okay, I did a quite a long intro to predictive coding.

15:08.900 --> 15:15.020
I think I'm now moving to the second topic, which is causal inference and what's causal

15:15.020 --> 15:16.020
inference?

15:16.020 --> 15:21.660
Causal inference is a theory, is a very general theory that has been formalized the most by

15:21.660 --> 15:22.660
Judea Perl.

15:22.660 --> 15:26.740
He's definitely the most important person in the field of causal inference.

15:26.740 --> 15:28.780
He wrote some very nice books.

15:28.780 --> 15:35.700
For example, the book of why is highly recommended if you want to learn more about this topic.

15:35.700 --> 15:38.860
And it basically tackles the following problem.

15:38.860 --> 15:42.780
So let's assume we have a joint probability distribution, which is associated with a Bayesian

15:42.780 --> 15:43.780
network.

15:43.780 --> 15:49.940
This is going to be a little bit the running example through all the paper, especially with

15:49.940 --> 15:53.940
your net with Bayesian networks of this shape.

15:53.940 --> 16:00.940
Those Bayesian networks, the variables inside, they can represent different quantities.

16:00.940 --> 16:07.820
So for example, a Bayesian network with this shape can represent the quantities on the

16:07.820 --> 16:08.820
right.

16:08.820 --> 16:13.940
So a socio-economical statue of an individual, its education level, its intelligence, and

16:13.940 --> 16:17.580
its income level.

16:17.580 --> 16:24.900
Something the classical statistics is very good at, and it's a while most used application,

16:24.900 --> 16:27.780
is to model observations or correlations.

16:27.780 --> 16:34.700
A correlation basically answered the question, what is the, if we observe another variable

16:34.700 --> 16:35.820
C?

16:35.820 --> 16:40.140
So for example, in this case, what is, what's the income level, the expected income level

16:40.140 --> 16:44.540
of an individual, if I observe his education level?

16:44.540 --> 16:50.820
And of course, if that person has a higher degree of education, for example, a master

16:50.820 --> 16:56.340
or a PhD, I'm expecting general that person to have a higher income level.

16:56.340 --> 16:57.820
And this is a correlation.

16:57.820 --> 17:03.340
However, sometimes there are things that are very hard to observe, but they play a huge

17:03.340 --> 17:06.300
role in determining those quantities.

17:06.300 --> 17:12.940
So for example, it could be that the income level is much, much more defined by the intelligence

17:12.940 --> 17:15.980
of a specific person.

17:15.980 --> 17:21.700
And maybe that the intelligence, so if a person is intelligent, he's also most likely to have

17:21.700 --> 17:25.020
a higher education level.

17:25.020 --> 17:32.620
But still the real reason why the income is high is because of the IQ.

17:33.260 --> 17:39.260
This cannot be studied by simple correlations and has to be studied by a more advanced technique,

17:39.260 --> 17:40.900
which is called an intervention.

17:40.900 --> 17:46.900
An intervention basically answers the question, what is the, if we change C to a specific

17:46.900 --> 17:48.500
value?

17:48.500 --> 17:55.780
So for example, we can take an individual and check his income level, and then change

17:55.780 --> 18:01.060
its education level, so intervene on this word, and change his education level without

18:01.100 --> 18:06.660
touching his intelligence, and see how much his income changes.

18:06.660 --> 18:12.620
For example, if the income changes a lot, it means that the intelligence doesn't play

18:12.620 --> 18:16.020
a big role in this, but the education level does.

18:16.020 --> 18:20.220
If the income level doesn't change much, it means that maybe there's a hidden variable,

18:20.220 --> 18:26.300
in this case, the intelligence that determines the income level of a person.

18:26.300 --> 18:30.860
The third quantity important in causal inference is that of counterfactuals.

18:30.860 --> 18:36.780
So for example, a counterfactual answers the question, what would be, had we changed C

18:36.780 --> 18:39.420
to a different value in the past?

18:39.420 --> 18:43.220
So for example, we can see that the difference between interventions and counterfactuals is

18:43.220 --> 18:46.460
that interventions act in the future.

18:46.460 --> 18:51.300
So I'm interviewing in the world now to observe a change in the future.

18:51.300 --> 18:57.460
Well, counterfactual allow us to go back in time and change a variable back in time and

18:57.540 --> 19:03.180
see how the change would have influenced the world we live in now.

19:03.180 --> 19:08.300
And those are defined by Judea Perle as the three levels of causal inference.

19:08.300 --> 19:12.140
Correlation is the first level, intervention is the second level, and counterfactual is

19:12.140 --> 19:16.620
the third level.

19:16.620 --> 19:17.620
What are interventions?

19:17.620 --> 19:22.420
I'm going to define them more formally now, how that I gave an intuitive definition.

19:22.420 --> 19:28.220
And I'm using this notation here, which is the same actually throughout all the presentation.

19:28.220 --> 19:33.860
So X is always going to be a latent variable, SI is always going to be a data point or an

19:33.860 --> 19:38.180
observation, and VI is always going to be a vertex.

19:38.180 --> 19:45.580
So every time you see VI, we're only interested in the structure of the graph, for example.

19:45.580 --> 19:51.100
So let's assume we have a Bayesian model, which has the same structure as the Bayesian

19:51.100 --> 19:55.140
model we saw in the previous slide.

19:55.140 --> 20:01.780
Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the

20:01.780 --> 20:09.620
probability or the expectation of X4, which is the latent variable related to this vertex,

20:09.620 --> 20:12.620
given that X3 is equal to S3.

20:12.620 --> 20:20.620
To perform an intervention, we need a new kind of notation, which is called the do operation.

20:21.500 --> 20:28.820
So in this case, X4, we want to compute the probability of X4, given the fact that we

20:28.820 --> 20:33.540
intervene in the world and change X3 to S3.

20:33.540 --> 20:34.900
And how do we do this?

20:34.900 --> 20:41.520
To perform an intervention, Judea Perl tells us that we have to have an intermediate step

20:41.520 --> 20:47.900
before computing a correlation, is that first we have to remove all the incoming edges to

20:47.900 --> 20:50.460
V3.

20:50.460 --> 20:56.020
So we have to study not this Bayesian network, but this second one.

20:56.020 --> 21:03.580
And then at this point, we are allowed to compute a correlation, as we normally do.

21:03.580 --> 21:07.220
And this is an intervention.

21:07.220 --> 21:12.700
A counterfactual is a generalization of this that, as I said, lived in the past, and they

21:12.700 --> 21:15.580
are computing using structural causal models.

21:15.580 --> 21:22.460
A structural causal model is a tuple, which is conceptually similar to a Bayesian network.

21:22.460 --> 21:27.900
But basically, we have this new class of variables on top, which are the unobservable variables

21:27.900 --> 21:29.420
they use.

21:29.420 --> 21:33.500
So we have the Bayesian network that we had before, X1, X2, X3, S4.

21:33.500 --> 21:40.500
But we also have those unobservable or variables that depend on the environment.

21:40.500 --> 21:47.500
You cannot control them, you can infer them, but they are there.

21:47.500 --> 21:57.500
And F is a set of functions that depends on all the, basically, F of X3 depends on X1,

21:57.500 --> 22:01.820
because you have an arrow, on X2, because you have an arrow, and on the unobservable

22:01.820 --> 22:06.340
variable that also influences X3.

22:06.340 --> 22:14.100
So yes, intuitively, you can think of a structural causal model as a Bayesian network with those

22:14.100 --> 22:22.460
unobservable variables on top, and each unobservable variable only influences its own, its own

22:22.460 --> 22:23.460
related variable X.

22:23.460 --> 22:27.260
So, for example, IU will never touch X1 as well.

22:27.260 --> 22:35.260
U3 will only touch U3, U1 will only influence X1, and so forth, and so on.

22:35.260 --> 22:39.220
So performing counterfactual inference answers the following question.

22:39.220 --> 22:48.220
So what would X4 be at X3 being equal to another variable in a past situation, U?

22:48.220 --> 22:51.620
And computing this counterfactual requires three different steps.

22:51.620 --> 22:58.220
So abduction is the computation of all the background variables.

22:58.220 --> 23:03.340
So in this step, we want to go back in time and understand how the environment, the unobservable

23:03.340 --> 23:08.340
environment, was in that specific moment in time.

23:08.340 --> 23:17.260
And we do this by fixing all the latent variables X to some specific data that we already have,

23:17.260 --> 23:20.340
and performing this inference on the use.

23:20.340 --> 23:28.780
Then we're going to use the U to keep the U that we have learned, and perform an intervention.

23:28.780 --> 23:35.340
So a counterfactual can also be seen as an intervention back in time, in which we know

23:35.340 --> 23:43.620
the environment variables U1, U2, and U4 in that specific moment.

23:43.620 --> 23:46.980
And what's the missing step?

23:46.980 --> 23:54.060
So what would X4 be at X3 being equal to another data point in that specific situation?

23:54.620 --> 23:57.460
Now we can compute a correlation.

23:57.460 --> 24:03.660
And the correlation, we do it on the graph in which we have already performed an intervention

24:03.660 --> 24:10.420
using the environment variables that we have learned in the abduction step.

24:10.420 --> 24:15.780
And this is a counterfactual inference.

24:15.780 --> 24:22.380
This is the last slide of the causal inference introduction, and it's about structure learning.

24:22.380 --> 24:28.740
Basically, everything I've said so far relies on the fact that we know the causal dependencies

24:28.740 --> 24:30.540
among the data points.

24:30.540 --> 24:35.140
So we know the structure of the graph, we know which variable influences which one,

24:35.140 --> 24:37.580
we know the arrows in general.

24:37.580 --> 24:40.660
But in practice, this is actually not always possible.

24:40.660 --> 24:46.620
So we don't have access to the causal graph most of the times.

24:46.620 --> 24:50.940
And actually learning the best causal graph from data is still an open problem.

24:50.940 --> 24:51.940
We are improving in this.

24:51.940 --> 24:53.140
We are getting better.

24:53.140 --> 25:01.340
But how to perform this task exactly is still an open problem.

25:01.340 --> 25:05.860
So as I said, basically, the goal is to infer causal relationships from observational data.

25:05.860 --> 25:11.380
So given a data set, we want to infer the directed acyclic graph that describes the

25:11.380 --> 25:16.140
connectivity between the system and the variables of the data set.

25:16.140 --> 25:22.540
So for example here, we have an example that I guess we are all familiar with thanks because

25:22.540 --> 25:24.260
of the pandemic.

25:24.260 --> 25:31.740
So we have those four variables, age, vaccine, hospitalization, and CT.

25:31.740 --> 25:35.900
And we want to infer the causal dependencies among those variables.

25:35.900 --> 25:40.380
So for example, we want to learn directly from data that the probability of a person

25:40.380 --> 25:46.780
being hospitalized depends on its age and on the fact whether it's vaccinated or not,

25:46.780 --> 25:51.620
and so forth and so on.

25:51.620 --> 25:58.660
So this is the end of the long introduction, but I hope it was clear enough and I hope

25:58.660 --> 26:05.060
that I gave the basics to understand basically the results of the paper.

26:05.060 --> 26:07.940
And now we can go to the research questions.

26:07.940 --> 26:10.860
So the research questions are the following.

26:10.860 --> 26:17.300
First I want to see whether creative coding can be used to perform causal inference.

26:17.300 --> 26:23.700
So creative coding so far has only been used to perform two compute correlations in Bayesian

26:23.700 --> 26:25.380
networks.

26:25.380 --> 26:30.340
And the big question is, can we go beyond correlation and model intervention and counterfactual

26:30.340 --> 26:32.940
in a biological, plausible way?

26:32.940 --> 26:38.940
So in a way that it's, for example, simple, intuitive, and allow us to only play with

26:38.940 --> 26:44.220
the neurons and not touch, for example, the huge structure of the graph.

26:44.220 --> 26:48.820
And more in practice, more specifically, the question becomes, can we define a creative

26:48.820 --> 26:55.580
coding-based structural causal model to perform interventions and counterfactuals?

26:55.580 --> 27:01.320
The second question is, as I said, that having a structural causal model assumes that we

27:01.320 --> 27:04.520
know the structure of the Bayesian network.

27:04.520 --> 27:08.160
So it assumes that we have the arrows.

27:08.160 --> 27:12.000
Can we go beyond this and use creative coding networks to learn the causal structure of

27:12.000 --> 27:14.200
the graph?

27:14.200 --> 27:21.560
Basically, giving positive answers to both those questions would allow us to use creative

27:21.560 --> 27:27.400
coding as an end-to-end causal inference method, which basically takes a data set and allow

27:27.400 --> 27:37.040
us to test interventions and counterfactual predictions directly from this data set.

27:37.040 --> 27:42.040
So let's tackle the first problem, so causal inference via creative coding, which is also

27:42.040 --> 27:47.040
the section that gives the title to the paper, basically.

27:47.040 --> 27:53.040
And here I will show how to perform correlations with creative coding, which is already known,

27:53.040 --> 28:01.440
and how to perform interventional queries, which I think is the real question of the paper.

28:01.440 --> 28:07.560
So here is a causal graph, which is the usual graph that we had.

28:07.560 --> 28:10.520
And here is the corresponding creative coding model.

28:10.520 --> 28:15.960
So the axes are the latent variables and correspond to the neurons in a neural network

28:15.960 --> 28:18.360
model.

28:18.360 --> 28:25.880
And the black arrow passes prediction information from one neuron to the one down the hierarchy.

28:25.880 --> 28:31.880
And every vertex also has this error neuron, which passes information up the hierarchy.

28:31.880 --> 28:38.240
So the information of every error goes to the value node in the up the hierarchy and

28:38.240 --> 28:44.960
basically tells it to correct itself to change the prediction.

28:44.960 --> 28:49.120
So to perform a correlation using creative coding, what you have to do is that you take

28:49.120 --> 28:54.000
an observation and you simply fix the value of a specific neuron.

28:54.000 --> 28:59.840
So if you want to compute the probability of X4 given X3 equal to S3, we simply have

28:59.840 --> 29:06.600
to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy

29:06.600 --> 29:08.480
minimization.

29:08.480 --> 29:16.320
And this model, by minimizing, by updating the axes via a minimization of the variational

29:16.320 --> 29:21.080
free energy, allows the model to converge to a solution to this question.

29:21.080 --> 29:26.320
So the probability or the expected value of X4 given X3 equals 3.

29:26.320 --> 29:32.560
But how do I perform an intervention now without acting on the structure of the graph?

29:32.560 --> 29:37.200
Well, this is basically the first idea of the paper.

29:38.160 --> 29:40.520
This is still how to perform a correlation.

29:40.520 --> 29:45.240
So fix S3 equal to X3 is the first step in the algorithm.

29:45.240 --> 29:51.320
And the second one is to update the axes by minimizing the variational free energy.

29:51.320 --> 29:57.040
An intervention, which in theory corresponds in removing those arrows and answers to the

29:57.040 --> 30:04.120
question, the probability of X4 by performing an intervention, so do X3 equal S3?

30:04.120 --> 30:07.320
This coding can be performed as follows.

30:07.320 --> 30:10.120
So I'm going to write the algorithm here.

30:10.120 --> 30:19.160
So first, as in a correlation, you fix X3 equal to the observation that you get.

30:19.160 --> 30:21.560
Then this is the important step.

30:21.560 --> 30:27.360
You have to intervene not on the graph anymore, but on the prediction error and fix it equal

30:27.360 --> 30:29.360
to zero.

30:29.360 --> 30:36.840
Assuming a prediction error equal to zero basically makes sense, meaning less information

30:36.840 --> 30:41.360
up the hierarchy or actually sends no information up the hierarchy because it basically tells

30:41.360 --> 30:45.480
you that the prediction is always correct.

30:45.480 --> 30:51.000
And the third step is to, as we did before, to update the axes, the unconstrained axis

30:51.000 --> 30:55.960
or X1, X2, X4 by minimizing the variational free energy.

30:55.960 --> 31:01.680
As I will show now experimentally, by simply doing this little trick of setting a prediction

31:01.680 --> 31:09.080
error to be equal to zero, it prevents us to actually act on the structure of the graph

31:09.080 --> 31:17.920
as the theory of Duke-Alculus does and to infer the variables after an intervention

31:17.920 --> 31:25.000
by simply performing a variational free energy minimization.

31:25.000 --> 31:26.760
What about counterfactual inference?

31:26.760 --> 31:35.040
Counterfactual inference is actually easy once we have defined how to do an intervention.

31:35.040 --> 31:39.040
And this is because, as we saw earlier, performing a counterfactual is similar to performing an

31:39.040 --> 31:48.320
intervention in a past situation after you have inferred the unobservable variables.

31:48.320 --> 31:53.760
So as you can see in the plot I showed earlier about the abduction action and prediction

31:53.760 --> 31:59.920
steps, the action and prediction steps, they did not have those two arrows.

31:59.920 --> 32:01.680
They were removed.

32:01.680 --> 32:11.200
Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by

32:11.200 --> 32:16.240
simply performing an abduction step, as it was done earlier, an action step in which we

32:16.240 --> 32:19.400
simply perform an intervention on the single node.

32:19.400 --> 32:26.280
So we fix the value node and we set the error to zero and run the energy minimization, so

32:26.280 --> 32:32.960
minimizing the variational free energy to compute the prediction.

32:32.960 --> 32:41.560
So I think this is like an easy and elegant method to perform interventions and counterfactuals.

32:41.560 --> 32:47.800
And yeah, so I think the thing we have to show now is whether it works in practice or not.

32:47.800 --> 32:50.400
And we have a couple of experiments.

32:50.400 --> 32:53.560
And I'm going to show you now two different experiments.

32:53.560 --> 33:01.320
The first one is merely proof of concept experiment that shows that the predictive coding is able

33:01.320 --> 33:06.400
to perform intervention and counterfactuals.

33:06.400 --> 33:12.200
And the second one actually shows a simple application in how interventional queries

33:12.200 --> 33:17.520
can be used to improve the performance of classification tasks on a specific kind of

33:17.520 --> 33:22.720
predictive coding networks, which is that of a fully connected model.

33:22.720 --> 33:24.840
Let's start from the first one.

33:24.840 --> 33:26.440
So how do we do this task?

33:26.440 --> 33:34.600
So given a structural causal model, we generate training data and we use it to learn the weights,

33:34.600 --> 33:40.680
so to learn the functions of the structural causal models.

33:40.680 --> 33:46.360
And then we generate test data for both interventional and counterfactual queries.

33:46.360 --> 33:51.540
And we show whether we are able to converge to the correct test data using predictive

33:51.540 --> 33:53.760
coding.

33:53.760 --> 34:00.840
And for example here, those two plots represent the interventional and counterfactual queries

34:00.840 --> 34:06.280
of this specific graph, which is the butterfly bias graph, which is a graph that is often

34:06.280 --> 34:12.000
used in testing whether causal inference, whether interventional and counterfactual

34:12.000 --> 34:15.440
techniques work is as simple as that.

34:15.440 --> 34:19.520
But in the paper, you can find a lot of different graphs.

34:19.520 --> 34:29.480
But in general, those two plots show that the method works, show that the mean absolute

34:29.480 --> 34:36.240
error between the interventional and counterfactual quantities we compute and the interventional

34:36.720 --> 34:43.080
and counterfactual quantities from the original graph are close to each other.

34:43.080 --> 34:46.600
So the error is quite small.

34:46.600 --> 34:51.920
The second experiment is basically an extension of an experiment I proposed in an earlier

34:51.920 --> 34:59.320
paper, which is the learning on arbitrary graph topologies that I wrote last year.

34:59.320 --> 35:05.800
In that paper, I basically proposed this kind of network as a proof of concept, which is

35:05.800 --> 35:14.000
a fully connected network, which is in general the worst neural network you can have to perform

35:14.000 --> 35:25.520
machine learning experiments, because given a fixed set of neurons, basically every pair

35:25.520 --> 35:28.600
of neuron is connected by two different synapses.

35:28.600 --> 35:34.880
So it's the model with the highest complexity possible in general.

35:34.880 --> 35:38.320
The good thing is that since you have a lot of cycles, the model is extremely flexible

35:38.320 --> 35:43.920
in the sense that you can train it, for example, on a minst image and on a data point and on

35:43.920 --> 35:44.920
its label.

35:44.920 --> 35:50.760
But then the way you can query it, thanks to the information going back, is you can query

35:50.760 --> 35:51.760
in a lot of different ways.

35:51.760 --> 35:56.080
So you can form classification tasks in which you provide an image and you run the energy

35:56.080 --> 35:58.120
minimization and get the label.

35:58.120 --> 36:01.760
But you can also, for example, perform generation tasks in which you give the label, run the

36:01.760 --> 36:04.280
energy minimization and get the image.

36:04.280 --> 36:11.160
You can perform, for example, image completion, which should give half the image and let the

36:11.160 --> 36:13.800
model converge to the second half and so forth and so on.

36:13.800 --> 36:20.160
So it's basically a model that learns the statistics of the dataset in its entirety

36:20.160 --> 36:25.400
without being focused on classification or generation in general.

36:25.400 --> 36:28.160
So this flexibility is great.

36:28.160 --> 36:34.120
The problem is that because of this, every single task doesn't work well.

36:34.120 --> 36:39.080
So you can do a lot of different things, but none of them is done well.

36:39.080 --> 36:46.400
And here I want to show how using interventional queries instead of standard correlation queries

36:46.400 --> 36:52.200
or conditional queries slightly improves their results of those classification tasks.

36:52.200 --> 36:59.760
So what are the conjecture reasons of this test accuracy on those tasks not being so

36:59.760 --> 37:00.760
high?

37:00.760 --> 37:07.260
The first, the two reasons are that the model is distracted in correcting every single error.

37:07.260 --> 37:11.200
So basically you present an image and you would like to get a label, but the model is

37:11.200 --> 37:17.040
actually updating itself to also predict the error in the images.

37:17.040 --> 37:21.960
And the second reason, which is the one I said, is that the structure is far too complex.

37:21.960 --> 37:30.520
So again, from an Occam razor argumentation, this is the worst model you can have.

37:30.520 --> 37:34.200
So every time you have a model that fits a dataset, that model is going to be less complex

37:34.200 --> 37:36.640
than this one that is going to be preferred.

37:36.640 --> 37:43.800
But in general, just to start it, the idea is can querying this model be interventions

37:43.800 --> 37:48.160
be used to improve the performance of those fully connected models?

37:48.160 --> 37:51.320
Well, the answer is yes.

37:51.320 --> 37:53.680
So here is how I perform interventional queries.

37:53.680 --> 37:56.800
So I present an image to the network.

37:56.800 --> 38:00.160
I fix the error of the pixels to be equal to zero.

38:00.160 --> 38:03.480
So this error doesn't get propagated in the network.

38:03.480 --> 38:06.000
And then I compute the label.

38:06.000 --> 38:11.400
And as you can see, the accuracy improves, for example, from 89 using the standard query

38:11.400 --> 38:17.680
method of pretty difficult in networks to 92, which is the accuracy after the intervention

38:17.680 --> 38:22.000
and the same happens for fashion means.

38:22.000 --> 38:27.480
And I think that a very legit critic that probably everyone would think when seeing

38:27.480 --> 38:34.400
those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically.

38:34.400 --> 38:36.720
And yeah, it's true.

38:36.720 --> 38:40.920
And I'm actually in the later slides, I'm going to show how to act on the structure

38:40.920 --> 38:47.040
of this fully connected model will improve the results even more until the point they

38:47.720 --> 38:53.000
reach a performance that is not even close to state of the art performance, of course.

38:53.000 --> 39:02.240
But it's still up to a level that becomes basically acceptable and worth investigating.

39:02.240 --> 39:08.680
So yes, so this is the part about causal inference using predictive coding.

39:08.680 --> 39:16.600
And I guess to summarize, I can say that the interesting part of the results I just showed

39:17.240 --> 39:22.520
is that I showed that predictive coding is able to perform interventions in a very easy

39:22.520 --> 39:26.760
and intuitive way because you don't have to act on the structure of the old graph anymore.

39:26.760 --> 39:31.120
Sometimes those functions are not available, so forth and so on.

39:31.120 --> 39:41.880
But you simply have to intervene on a single neuron, set its prediction error to zero and

39:41.880 --> 39:44.680
perform an energy minimization process.

39:46.760 --> 39:53.040
And these extended allowed us to define predictive coding based structural causal models.

39:53.040 --> 39:59.680
Now we move to the second part of the work, which is about structure learning.

40:02.480 --> 40:08.000
So structure learning, as I said, deals with the problem of learning the causal structure

40:08.000 --> 40:12.080
of the model from observational data.

40:12.080 --> 40:21.000
This is actually no problem that has been around for decades and has always been, until

40:21.000 --> 40:25.800
a couple of years ago, tackled using combinatorial search methods.

40:25.800 --> 40:30.800
The problem with those combinatorial search methods is that their complexity grows double

40:30.800 --> 40:33.040
exponentially.

40:33.040 --> 40:40.200
So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn

40:40.200 --> 40:47.000
grows in size, learning it, it's incredibly slow.

40:47.000 --> 40:54.640
The new solution that came out actually a couple of years ago in a newspaper from 2018

40:54.640 --> 40:58.240
showed that it's possible to actually learn this structure, not using a combinatorial

40:58.240 --> 41:01.920
search method, but by using a gradient-based method.

41:01.920 --> 41:09.320
And this was basically this killed the problem in general because now you can simply apply

41:09.320 --> 41:13.440
your on the parameters, which is the prior proposed that I'm going to define a little

41:13.440 --> 41:17.640
bit better in this slide, around gradient descent.

41:17.640 --> 41:23.400
And even if you have a model that is double, triple the size, the algorithm is still incredibly

41:23.400 --> 41:25.800
fast.

41:25.800 --> 41:31.800
And for this reason, this paper is, yeah, I think it's kind of new and I think already

41:31.800 --> 41:35.800
has around 600 citations or things like that.

41:35.800 --> 41:39.560
And every paper that I'm seeing now about causal inference and learning causal structure

41:39.560 --> 41:42.320
of the graph uses their method.

41:42.320 --> 41:48.560
It just changes a little bit, they find faster or slightly better inference methods, but

41:48.560 --> 41:56.760
still they all use the prior, this paper defined, and I do as well, and we do as well.

41:56.760 --> 42:01.080
So here we define a new quantity, which is the agency matrix.

42:01.080 --> 42:05.800
The agency matrix is simply a matrix that encodes the connections of the model.

42:05.800 --> 42:10.560
So it's a binary matrix, and in general, it's a binary matrix.

42:10.560 --> 42:15.440
Then of course, when you do gradient-based optimization, you make it continuous and then

42:15.440 --> 42:21.960
you have some threshold at some point that basically kills an edge or set it to one.

42:22.280 --> 42:34.360
The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j

42:34.360 --> 42:35.760
or zero otherwise.

42:35.760 --> 42:40.400
So for example, this agency matrix here represents the connectivity structure of this Bayesian

42:40.400 --> 42:43.160
network.

42:43.160 --> 42:51.520
And basically this method tackles two problems that we want about learning the structure

42:51.560 --> 42:53.360
of the Bayesian network.

42:53.360 --> 43:00.200
The idea is that we start from a fully connected model, which conceptually is similar, actually

43:00.200 --> 43:04.680
is equivalent to the predictive coding network I defined earlier, which is fully connected.

43:04.680 --> 43:10.480
So you have a lot of vertices and every pair of vertices is connected by two different

43:10.480 --> 43:16.080
edges, and you simply want to prune the ones that are not needed.

43:16.080 --> 43:20.320
So it can be seen as a method that performs model reduction.

43:20.320 --> 43:23.080
You start from a big model and you want to make it small.

43:23.080 --> 43:26.640
So what's the first ingredient to reduce models?

43:26.640 --> 43:29.720
Well, it's of course sparse city.

43:29.720 --> 43:35.400
And what's the prior that everyone uses to make a model more sparse is the Laplace prior,

43:35.400 --> 43:41.240
which in machine learning is simply known as the L1 norm, which is defined here.

43:41.240 --> 43:47.120
The solution that this paper that I mentioned earlier proposed is to add a second prior

43:47.120 --> 43:56.440
on top, which enforces what's probably the biggest characteristic of Bayesian networks

43:56.440 --> 44:00.440
on which you want to perform causal inference, is that you want them to be acyclic.

44:00.440 --> 44:08.440
And basically they show that acyclicity can be imposed on an agency matrix as a prior,

44:08.440 --> 44:10.720
and it has this shape here.

44:10.720 --> 44:19.200
So it's the trace of the matrix that is the exponential of A times A, where A is the

44:19.200 --> 44:21.280
agency matrix again.

44:21.280 --> 44:28.840
And basically this quantity here is equal to zero if and only if the Bayesian network

44:28.840 --> 44:37.960
or whatever graph you're considering is acyclic.

44:37.960 --> 44:46.520
So I'm going to use these in some experiments, so force those two priors on different kinds

44:46.520 --> 44:51.280
of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier

44:51.280 --> 44:55.360
about performing causal inference via predictive coding.

44:55.360 --> 44:57.240
So I'm going to present two different experiments.

44:57.240 --> 45:03.560
So one is a proof of concept, which is the standard experiments showed in all the structural

45:03.560 --> 45:09.200
learning tasks, which is the inference of the correct Bayesian network from data.

45:09.200 --> 45:16.320
And then I'm going to build on top of the classification experiments I showed earlier,

45:16.320 --> 45:22.200
and show how actually those priors allow us to improve the classification accuracy, the

45:22.200 --> 45:29.720
test accuracy of fully connected predictive coding models.

45:29.720 --> 45:35.400
So let's move to the first experiment, which is to infer the structure of the graph.

45:35.400 --> 45:39.880
And the experiments, they all follow basically the same pipeline in all the papers in the

45:39.880 --> 45:40.880
field.

45:40.880 --> 45:46.320
The first step is to generate a Bayesian network from random graph.

45:46.320 --> 45:51.560
So basically normally the two random graphs that everyone tests are Erdos-Renis graphs

45:51.560 --> 45:53.680
and scale-free graphs.

45:53.680 --> 46:00.560
So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some

46:00.560 --> 46:04.920
edges that you sample randomly.

46:04.920 --> 46:08.600
And you use this graph to generate a data set.

46:08.600 --> 46:13.840
So you sample, for example, N, big N data points.

46:13.840 --> 46:18.480
And what you do is that you take the graph that you have generated earlier and you throw

46:18.480 --> 46:19.480
it away.

46:19.480 --> 46:21.160
You only keep the data set.

46:21.160 --> 46:27.920
And the task you want to solve now is to have a training algorithm that basically allows

46:27.920 --> 46:34.920
you to retrieve the structure of the graph you have thrown away.

46:34.920 --> 46:39.040
So the way we do it here is that we train a fully connected predictive coding model on

46:39.040 --> 46:46.320
this data set D, using both the sparse and the acyclic priors we have defined earlier.

46:46.320 --> 46:53.920
You can see whether actually the graph that we converge to, after pruning away the entries

46:53.920 --> 46:59.640
of the agency matrix that are smaller than a certain threshold, is similar to that of

46:59.640 --> 47:02.840
the initial graph.

47:02.840 --> 47:05.560
And the results show that this is actually the case.

47:05.560 --> 47:13.600
So this is an example and I show many different parametrizations and dimensions and things

47:13.600 --> 47:15.040
like that in the paper.

47:15.640 --> 47:19.800
But I think those two are the most representative examples with an air nosher in a graph and

47:19.800 --> 47:23.920
a free scale graph with 20 nodes.

47:23.920 --> 47:31.160
And here on the left, you can see the ground truth graph, which is the one sampled randomly.

47:31.160 --> 47:36.240
And on the right, you can see the graph, the predictive coding model as learned from the

47:36.240 --> 47:37.240
data set.

47:37.240 --> 47:40.760
And as you can see, they are quite similar.

47:40.760 --> 47:42.640
It's still not perfect.

47:42.640 --> 47:48.840
So there are some errors, but in general, the structures, they work quite well.

47:48.840 --> 47:54.240
We also have some quantitative experiments that I don't show here, because they're just

47:54.240 --> 47:58.240
huge tables with a lot of numbers and I thought it was maybe a little bit too much for the

47:58.240 --> 47:59.240
presentation.

47:59.240 --> 48:06.400
But there is also that they perform similarly to contemporary methods.

48:06.400 --> 48:12.520
Also because I have to say most of the quality comes from the acyclic priors that was introduced

48:12.520 --> 48:13.520
in 2018.

48:17.120 --> 48:22.760
The second class of experiments are classification experiments, which as I said, are the extensions

48:22.760 --> 48:25.880
of the one I shared earlier.

48:25.880 --> 48:31.000
And the idea is to use structure learning to improve the classification results on the

48:31.000 --> 48:36.960
means and fashion means data set, starting from a fully connected graph.

48:36.960 --> 48:44.000
So what I did is that I divided the fully connected graph in clusters of neurons.

48:44.000 --> 48:49.440
So 1B cluster is the one related to the input.

48:49.440 --> 48:55.680
And then we have some specific number of hidden clusters.

48:55.680 --> 49:02.200
And then we have the label cluster, which is the cluster of neurons that are supposed

49:02.200 --> 49:06.680
to give me the label predictions.

49:06.680 --> 49:10.400
And I've trained them using the first time, the sparse prior only.

49:10.400 --> 49:19.320
So the idea is, what if I prune the connections I don't need from a model and learn a sparser

49:19.320 --> 49:20.320
model?

49:20.320 --> 49:21.800
Does this work?

49:21.800 --> 49:23.480
Well, the answer is no.

49:23.480 --> 49:24.680
It doesn't work.

49:24.680 --> 49:31.840
And the reason why is that at the end, the graph that you converge with is actually degenerate.

49:31.840 --> 49:37.200
So basically, the model learns to predict the label based on the label itself.

49:37.200 --> 49:41.920
So it discards all the information from the input and only keeps the label.

49:41.920 --> 49:44.960
And as you can see here, the label y predicts itself.

49:44.960 --> 49:49.200
Or in other experiments, when you change the parameters, you have that y predicts at zero,

49:49.200 --> 49:53.000
that predicts x1, that predicts y again.

49:53.000 --> 49:56.360
So what's the solution to this problem?

49:56.360 --> 50:03.440
Well, the solution to this problem is that we have to converge to an acyclic graph.

50:03.440 --> 50:07.000
And so we have to add something that prevents acyclicity.

50:07.000 --> 50:08.920
And what is that?

50:08.920 --> 50:10.840
One is, of course, the one I already proposed.

50:10.840 --> 50:14.840
And then I show a second technique.

50:14.840 --> 50:19.600
So the first one uses the acyclic prior defined earlier.

50:19.600 --> 50:24.600
And the second one is a novel technique that actually makes use of negative examples.

50:24.600 --> 50:31.640
So a negative example in this case is simply a data point in which you have an image, but

50:31.640 --> 50:33.440
the label is wrong.

50:33.440 --> 50:37.360
So here, for example, you have an image of a 7, but the label that I'm giving the model

50:37.360 --> 50:41.360
is a 2.

50:41.360 --> 50:48.040
And the idea is very simple and has been used in a lot of works already.

50:48.040 --> 50:54.280
So every time the model sees a positive example, it has to minimize the variational free energy.

50:54.280 --> 50:59.120
And every time it sees a negative example, it has to increase it.

50:59.120 --> 51:05.680
So we will want this quantity to be minimized.

51:05.680 --> 51:10.680
And actually, with a lot of experiments and a lot of experimentations, we saw that the

51:10.680 --> 51:17.440
two techniques basically first lead to the same results and second lead to the same graph

51:17.440 --> 51:18.440
as well.

51:18.840 --> 51:26.480
So here are the new results on means and fashion means using the two techniques that I just

51:26.480 --> 51:28.640
proposed.

51:28.640 --> 51:34.480
And now we move to some which are still not great, but definitely more reasonable test

51:34.480 --> 51:35.560
accuracies.

51:35.560 --> 51:42.560
So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means.

51:42.680 --> 51:49.000
Actually, those results can be much improved by learning the structure of the graph on

51:49.000 --> 51:55.720
means and then fixing the structure of the graph and do some form of fine tuning.

51:55.720 --> 52:00.400
So if you fine tune the model on the correct hierarchical structure, at some point you

52:00.400 --> 52:04.720
reach the test accuracy, which is the one you would expect from a hierarchical model.

52:04.720 --> 52:11.120
But those ones are simply the one, the fully connected model as naturally converged to.

52:11.120 --> 52:17.400
So for example, from a test error of 18.32 of the fully connected model train on fashion

52:17.400 --> 52:23.560
means by simply performing correlations or conditional queries, which is the standard

52:23.560 --> 52:30.400
way of querying operative coding model, adding interventions and the acyclic prior together

52:30.400 --> 52:34.520
makes this test error much lower.

52:34.520 --> 52:38.120
And we can observe it for means as well.

52:39.120 --> 52:46.360
I'm now going a little bit into details on this last experiment and on how the acyclic

52:46.360 --> 52:49.760
prior acts on the structure of the graph.

52:49.760 --> 52:55.920
So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset,

52:55.920 --> 53:01.240
it may be too much, is the, I called it a two means dataset in which you have the input

53:01.240 --> 53:07.960
point is formed of two different images and the label only depends on the second image.

53:08.680 --> 53:09.840
On the first image story.

53:10.840 --> 53:17.040
So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and

53:17.040 --> 53:22.640
things like that able to recognize that the second half of the image is actually meaningless

53:22.800 --> 53:29.720
in, in performing, in learning the in performing classification.

53:31.280 --> 53:32.760
How does training behave in general?

53:32.760 --> 53:38.920
Like, for example, we have this input, input node, output node, and only the nodes are

53:38.920 --> 53:46.080
fully connected and the model converge to a hierarchical structure, which is the one

53:46.400 --> 53:50.040
that we know performs the best on, on classification tasks.

53:51.080 --> 53:55.960
Well, here is a, is an example of a training method of a training run.

53:56.480 --> 53:59.640
So that's C zero, which is the beginning of training.

54:00.880 --> 54:01.960
We have this model here.

54:02.040 --> 54:07.920
So as zero corresponds to the, to the seven, so to the first image as one corresponds to

54:07.920 --> 54:12.640
the second image, again, we have the label Y and all the latent variables X zero X one

54:12.640 --> 54:15.240
X two, and the model is fully connected.

54:15.280 --> 54:18.720
So the agency matrix is, is full of ones.

54:19.000 --> 54:20.160
There are, there are no zeros.

54:20.320 --> 54:22.040
We have self loops and things like that.

54:23.640 --> 54:30.000
We train them at the model for a couple of epochs until, and what we note immediately is

54:30.040 --> 54:34.680
that, for example, the, the model immediately understands that the four is not needed to

54:34.680 --> 54:35.760
perform classification.

54:36.240 --> 54:36.840
So it doesn't.

54:37.880 --> 54:42.880
So every outgoing node from the, from the second input cluster is removed.

54:44.160 --> 54:48.920
And something we didn't understand is that this is, this cluster is the one related to

54:48.920 --> 54:49.480
the output.

54:50.560 --> 54:58.280
So we have a, we have a linear map from S zero to Y directly, which is this part here.

54:59.240 --> 55:05.720
But we know that actually a linear map is not the best map for, for performing classification

55:05.720 --> 55:06.200
on means.

55:06.440 --> 55:07.880
So we, we need some hierarchy.

55:07.880 --> 55:10.520
We need some depth to, to improve the results.

55:10.920 --> 55:17.240
And as you can see, this line here is the, is the accuracy, which up to this point, so

55:17.240 --> 55:23.640
up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear

55:23.640 --> 55:24.440
classification.

55:25.400 --> 55:30.840
But once you go on with the training, the model understands that it needs some hierarchy

55:30.840 --> 55:32.000
to better fit the data.

55:32.840 --> 55:39.760
So you, you see that this arrow starts getting stronger and stronger over time until it, it

55:39.760 --> 55:44.480
understands that the linear map is not actually really needed and it removes it.

55:45.400 --> 55:50.360
And so the, so the model you converge with is a model that starts from a zero, goes to

55:50.680 --> 55:58.040
a hidden node and then goes to the, to the label with a very weak linear map, which actually

55:58.040 --> 56:03.480
gets removed if you, if you set that threshold of, if you set that threshold of, for example,

56:03.480 --> 56:06.920
0.1, 0.2, at some point, the linear map gets forgotten.

56:07.240 --> 56:11.800
And everything you end up with is with a, is with a hierarchical network.

56:13.400 --> 56:18.120
That is, that is, so it has learned the correct structure to, to perform classification tasks,

56:18.120 --> 56:19.080
which is a hierarchy.

56:19.480 --> 56:25.000
And it has also learned that the second image didn't play any role in defining the,

56:25.560 --> 56:26.840
the test accuracy.

56:26.840 --> 56:29.480
And this is all, this is all performed.

56:29.480 --> 56:35.320
So all those jobs are simply performed by, performed by one free energy minimization

56:35.320 --> 56:36.120
process.

56:36.120 --> 56:40.040
So you initialize the model, you define the free energy, you define the priors.

56:40.040 --> 56:46.120
So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge

56:46.120 --> 56:50.760
to hierarchical, to a hierarchical model, which is well able to perform classification on minced.

56:51.880 --> 56:56.760
And then if you then perform some fine tuning, you reach very competitive results as you do in

56:56.760 --> 56:59.720
feed forward networks with the, with back propagation.

56:59.720 --> 57:01.720
But I think that's not the interesting bit.

57:01.720 --> 57:05.400
The interesting bit is that you, like all this process, this process altogether

57:06.040 --> 57:11.880
of intervention and the acyclicity allows you to take a fully connected network

57:12.680 --> 57:18.040
and converge to a hierarchical one that is, that is able to perform classification with good results.

57:20.840 --> 57:24.600
And yeah, that's basically it.

57:24.600 --> 57:27.160
I'm now, oh yeah, wow, I've talked a lot.

57:27.160 --> 57:34.680
And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary.

57:34.680 --> 57:39.320
And I think the, the important takeaway if I have to give even one sentence of this paper

57:39.400 --> 57:45.560
is that predictive coding is a belief updating method that is able to perform end to end causal

57:45.560 --> 57:51.560
learning. So it's able to perform interventions to learn a structure from data and then perform

57:51.560 --> 57:59.080
interventions and counterfactuals. So causal inference in natural and efficiency model

57:59.080 --> 58:01.880
interventions by simply setting the prediction error to zero.

58:01.880 --> 58:05.800
So it's a, it's a very easy technique to perform interventions.

58:05.800 --> 58:09.480
And you simply only have to touch one neuron, you don't have to act on the structure of the graph.

58:10.920 --> 58:16.440
You can, you can use it to perform, to, to create structure causal models that are biologically

58:16.440 --> 58:24.440
plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already.

58:25.400 --> 58:29.480
And, and a couple of sentences about future works is that

58:30.200 --> 58:35.080
something that would be nice to do is to improve the performance of the model we,

58:35.080 --> 58:40.520
we have defined, because I think it performs reasonably well on a lot of tasks.

58:40.520 --> 58:45.880
So it performs reasonably well on structure learning on, for me, intervention and counterfactuals.

58:47.080 --> 58:51.480
But actually, if you look at state of the art model, there's always like a very specific method

58:51.480 --> 58:57.560
that performs better in a, in the single task. So it would be interesting to see if we can

58:58.040 --> 59:04.360
reach those level of performance in, in specific tasks by, by adding some tricks on, or some,

59:07.080 --> 59:12.600
or some new optimization methods, and to generalize it to, to dynamical systems,

59:12.600 --> 59:17.960
which are actually much more interesting, the static systems. So such as dynamical causal models

59:17.960 --> 59:24.600
and, or other techniques that allow you to perform causal inference in systems that move.

59:24.600 --> 59:31.080
So an action taken in a specific time step influences another node in a later time step,

59:31.080 --> 59:38.920
which is basically Granger causality. Yeah, that's it. And thank you very much.

59:47.560 --> 59:52.760
Thank you. Awesome. And very comprehensive presentation. That was really

59:53.480 --> 01:00:01.480
muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive

01:00:02.360 --> 01:00:07.480
presentation. There was really a lot there. And there was also a lot of great questions

01:00:07.480 --> 01:00:13.960
in the live chat. So maybe to warm into the questions, how did you come to study this

01:00:14.520 --> 01:00:20.200
topic? Were you studying causality and found predictive coding to be useful or vice versa?

01:00:20.200 --> 01:00:25.720
Or how did you come out this intersection? I actually have to say that the first person

01:00:25.720 --> 01:00:34.280
that came out with this idea was, was better. So, so like, like, I think a year and a half ago,

01:00:34.280 --> 01:00:40.600
even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up.

01:00:41.160 --> 01:00:45.880
And, and last summer, I started getting curious about causality and

01:00:46.120 --> 01:00:52.200
I read, for example, the book of why as I listen into podcasts, I don't know the

01:00:52.200 --> 01:00:56.520
standard way in which you get interested in a topic. And, and I remember this,

01:00:56.520 --> 01:01:02.360
this idea from Baron and proposed it to him. And I was like, why don't we expand it and,

01:01:02.360 --> 01:01:07.880
and actually make it a paper. So I, I involve some people to work with experiments and,

01:01:08.520 --> 01:01:13.560
and this is the final result at the end. Awesome. Cool. Yeah.

01:01:14.120 --> 01:01:19.640
Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different

01:01:19.640 --> 01:01:22.680
questions. And if anybody else wants to add more, I'm going to turn the light on first,

01:01:22.680 --> 01:01:26.200
because I'm, I think I'm getting in the dark more and more. Yes.

01:01:28.360 --> 01:01:33.400
Who said active inference can't solve the dark room issue? Oh, yes, here we are.

01:01:34.920 --> 01:01:37.880
So would you say the light switch caused it to be lighter?

01:01:38.200 --> 01:01:49.240
Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding,

01:01:49.240 --> 01:01:54.040
all distributions are usually Gaussian, the bottom up messages are precision weighted

01:01:54.040 --> 01:01:57.880
prediction errors where precision is the inverse of the Gaussian covariance.

01:01:58.520 --> 01:02:01.320
What if non Gaussian distributions are used?

01:02:01.640 --> 01:02:10.200
Is, um, basically the general method stays, the different, the main difference is that you,

01:02:11.000 --> 01:02:15.720
you don't have prediction errors, which, uh, as was correctly pointed out is the,

01:02:15.720 --> 01:02:21.320
basically the derivative of the variational free energy. If you have Gaussian assumptions,

01:02:22.920 --> 01:02:25.880
yeah, you don't have that single quantity to set to zero.

01:02:26.520 --> 01:02:32.120
And you probably will have to act on the structure of the graph to perform interventions.

01:02:34.040 --> 01:02:40.280
And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian

01:02:40.280 --> 01:02:42.680
distributions that, that looked at some of these issues, right?

01:02:43.880 --> 01:02:49.160
Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh,

01:02:50.520 --> 01:02:55.240
and we model transformers. That's the biggest motivation using predictive coding.

01:02:55.320 --> 01:03:00.680
And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end,

01:03:00.680 --> 01:03:06.520
and softmax calls to, uh, like not to Gaussian distribution, but to,

01:03:08.520 --> 01:03:12.200
yeah, to softmax distribution, the, I don't get the name now, but yes.

01:03:13.560 --> 01:03:17.080
And, uh, so yes, that's a generalization. It's a little bit

01:03:17.720 --> 01:03:21.000
tricky to call it. Once you remove the Gaston assumption is a little bit

01:03:21.000 --> 01:03:24.680
still tricky to call it predictive coding. So it's a,

01:03:26.520 --> 01:03:30.040
so for, for example, like talking to, uh, to Carl Freestone,

01:03:31.640 --> 01:03:36.120
like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions.

01:03:37.880 --> 01:03:40.440
But yes, that's more a philosophical debate than, uh,

01:03:42.760 --> 01:03:48.120
Interesting. And another, I think topic that, that's definitely of, of great interest is

01:03:48.120 --> 01:03:53.640
similarities and differences between the attention apparatus in transformers

01:03:54.360 --> 01:03:59.960
and the way that attention is described from a neurocognitive perspective and from a predictive

01:03:59.960 --> 01:04:04.040
processing precision waiting angle. What do you, what do you think about that?

01:04:06.440 --> 01:04:13.880
Well, the idea is that, um, yeah, I think about it is that in from a pretty processing

01:04:13.880 --> 01:04:18.440
and, uh, and also operational inference perspective, attention can be seen as a,

01:04:18.440 --> 01:04:22.280
as a kind of structure learning problem. There's a, I think there's a recent paper from,

01:04:23.080 --> 01:04:29.000
from Chris Buckley's group that shows that there should be, there should be a reprint on archive

01:04:29.560 --> 01:04:32.520
in which basically they show that the attention mechanism is simply

01:04:33.240 --> 01:04:39.720
learning the, the precision on the, on the weight parameters specific to out to a data point.

01:04:39.800 --> 01:04:45.160
So this precision is not a, is not a, is not a parameter that is in the structure of the model.

01:04:45.160 --> 01:04:49.480
So it's not a model specific parameter. It is a fast changing parameter like the value nodes

01:04:50.040 --> 01:04:53.080
that gets updated while minimizing the version of free energy.

01:04:53.720 --> 01:04:57.000
And once they, once you've minimized it and compute it, then you throw it away.

01:04:57.000 --> 01:04:59.960
And from the next data point, you have to really compute it from scratch.

01:05:00.760 --> 01:05:07.160
So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as

01:05:07.160 --> 01:05:13.080
a kind of structure learning, but a structure learning that is data point specific and not

01:05:13.080 --> 01:05:17.960
model specific. And I think if you want to generalize a little bit and go from,

01:05:18.920 --> 01:05:23.000
from the attention mechanism in transformers to the attention mechanism cognitive science,

01:05:24.200 --> 01:05:29.720
I feel they're probably too different to, like to draw similarities and, uh,

01:05:31.240 --> 01:05:36.840
I think the structure learning analogy and the, how important one connection in is

01:05:36.840 --> 01:05:39.800
with respect to another one probably does job much better.

01:05:42.040 --> 01:05:49.560
Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference

01:05:49.560 --> 01:05:53.640
between hidden variables X and unobserved variables U?

01:05:55.560 --> 01:06:02.440
The difference is that you can, uh, I think the main one is that you cannot observe the,

01:06:02.520 --> 01:06:07.480
the use. You can use them because you can, you can compute them and fix them,

01:06:08.200 --> 01:06:12.440
but you cannot, the idea is that you have no control over them. So the use,

01:06:12.440 --> 01:06:17.720
the use should be seen as a environment specific variables that they are there. They,

01:06:17.720 --> 01:06:22.840
they influence your process. Okay. Because the, for example, when you go back in time,

01:06:22.840 --> 01:06:25.720
the environment is different. So the idea is for example, if you,

01:06:26.520 --> 01:06:31.720
like going back to the, to the example before of the, of the expected income of a person with

01:06:31.720 --> 01:06:38.760
a specific intelligence of education, uh, uh, education degree, the idea is that if I want to,

01:06:39.320 --> 01:06:45.320
to see how much I will learn today with a, with a, with a, I don't know, with a master degree,

01:06:45.320 --> 01:06:51.000
is different with respect to how much I would earn 20 years ago with a master degree is different.

01:06:51.000 --> 01:06:56.440
For example, here in Italy with respect to other countries and all those variables that are not

01:06:56.440 --> 01:07:01.400
under your control, you can not model them using your vision network, but they are there.

01:07:01.400 --> 01:07:06.040
Okay. So you, you cannot ignore them when you, when you want to draw conclusions.

01:07:06.760 --> 01:07:11.720
So it's, yeah, it's basically everything that you cannot control. You can infer them. So you

01:07:11.720 --> 01:07:16.840
can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years

01:07:16.840 --> 01:07:23.400
ago, I would have earned this much if I, if I was disintelligent at this degree on average,

01:07:23.400 --> 01:07:29.000
of course. And, but it's not that I can change the government policies towards jobs or the,

01:07:29.640 --> 01:07:37.240
or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use.

01:07:38.440 --> 01:07:44.200
Awesome. All right. Have you implemented generalized coordinates in predictive coding?

01:07:45.960 --> 01:07:52.600
No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never

01:07:52.680 --> 01:07:59.880
implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable.

01:07:59.880 --> 01:08:06.200
I think that's the, that's the takeaway that I got from talking to people that have implemented them.

01:08:08.440 --> 01:08:13.960
But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that,

01:08:13.960 --> 01:08:18.840
that tested on some threshold encoder style. Actually, I think still from Baron,

01:08:19.480 --> 01:08:25.400
there's a, there's a paper out there that came out last summer, but no, I've never played them with

01:08:25.400 --> 01:08:33.960
them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction

01:08:33.960 --> 01:08:42.920
problem of predicting input? Adding more level in, uh, in which sense, because the

01:08:42.920 --> 01:08:48.600
destruction problem is given by cycles. So basically you provide an image and the fact that you have

01:08:48.840 --> 01:08:56.040
a, so edges going out of the image, going in the, in the neurons, and then other edges going back,

01:08:57.640 --> 01:09:03.400
the, this basically creates the fact that you have a, that the error of, that those basically,

01:09:03.960 --> 01:09:09.000
these ingoing edges to the pixels of the image, they create some prediction errors. So you have

01:09:09.000 --> 01:09:14.200
some prediction errors that get spread inside the model. And that's, yeah, and this problem,

01:09:14.200 --> 01:09:19.400
I think is general of cycles. And it's probably not related to hierarchy in general.

01:09:21.000 --> 01:09:26.360
So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges,

01:09:26.360 --> 01:09:33.800
you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic

01:09:33.800 --> 01:09:43.240
network through the trace operator, that's a very interesting technique. And when was that

01:09:43.320 --> 01:09:44.280
brought into play?

01:09:46.840 --> 01:09:53.320
As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know,

01:09:53.320 --> 01:09:59.000
at least in the causal inference literature, I'm, I'm not aware of any previous methods.

01:09:59.000 --> 01:10:04.120
I would say no, because that, I mean, that's the highly cited paper. So I would say they came out

01:10:04.120 --> 01:10:09.240
with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn

01:10:09.240 --> 01:10:15.000
the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like

01:10:15.000 --> 01:10:21.000
when you look at when different features of Bayesian inference and causal inference became

01:10:21.000 --> 01:10:28.680
available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal

01:10:28.680 --> 01:10:35.800
modeling framework? It's like, because there's only been like five to 25 years of this happening.

01:10:36.680 --> 01:10:42.360
And so that's very, very short. And also it's relatively technical. So there's relatively

01:10:42.360 --> 01:10:48.280
few research groups engaging in it. And it's just really cool what it's enabling.

01:10:49.800 --> 01:10:54.200
No, yes, yes, exactly. I mean, that's also, I think the exciting part of this field a little bit

01:10:54.200 --> 01:10:59.640
that is, I mean, there are definitely break breakthroughs out there that, that still have

01:10:59.640 --> 01:11:05.080
to be discovered and probably like, for example, like, or as much as a breakthrough that paper was

01:11:05.880 --> 01:11:12.920
they found like, they simply found out the right prior for acyclic structures. Okay, it's a

01:11:14.120 --> 01:11:19.640
yeah, I mean, I, I don't know exactly, but it may be an idea that you have in one afternoon.

01:11:19.640 --> 01:11:24.520
I don't know about the story of the, how the authors came up with that, but could potentially be

01:11:24.520 --> 01:11:29.080
that if they, they are there at the whiteboard, you're like, Oh, that actually works. That's a

01:11:29.080 --> 01:11:36.280
huge breakthrough. And I simply defined the prior. And also a lot of these breakthroughs,

01:11:37.160 --> 01:11:46.120
they, they don't just stack. It's not like a, a tower of blocks, they layer and they compose.

01:11:46.920 --> 01:11:51.000
So then something will be generalized to generalized coordinates or generalized

01:11:51.000 --> 01:11:57.800
synchrony or arbitrarily large graphs or sensor fusion with multimodal inputs. And it's like those

01:11:57.800 --> 01:12:04.920
all blend in really satisfying and effective ways. So, so even little things that again,

01:12:04.920 --> 01:12:13.160
someone can just come up with in a moment can really have impact. Okay, ML Don says,

01:12:13.160 --> 01:12:18.200
thanks a lot for asking my questions and thanks a million to Tomaso for the inspiring presentation.

01:12:18.200 --> 01:12:22.120
So nice. Thank you very much. And then Bert asks,

01:12:23.080 --> 01:12:28.120
how would language models using predictive coding differ from those using transformers?

01:12:32.520 --> 01:12:37.320
Okay, I think that actually, if I would have to build today a language model using predictive

01:12:37.320 --> 01:12:41.720
coding, I would still use transformers. So the idea is that, for example, if you have a,

01:12:42.760 --> 01:12:49.480
let's say this hierarchical graphical model, or this hierarchical Bayesian network,

01:12:50.440 --> 01:12:56.040
I've defined in the, in the very first slides, one arrow to encode a function, which is the linear

01:12:56.040 --> 01:13:01.720
map. Okay, so one arrow was simply the multiplication of a, of the vector encoded in the latent

01:13:01.720 --> 01:13:08.200
variables times the, this weight matrix that you can then make non-linear and things like that.

01:13:08.200 --> 01:13:12.200
But that can be actually something much more complex. The, the function encoded in the arrow

01:13:12.280 --> 01:13:19.560
can be a convolution, can be an attention mechanism. So, so actually how I would do it,

01:13:19.560 --> 01:13:26.680
I will still use the, I mean, which is actually the way we did it in, in, in the Oxford group last

01:13:26.680 --> 01:13:32.840
year is that we, we had exactly the structure. Every arrow is a transformer now. So one is

01:13:32.840 --> 01:13:37.240
the attention mechanism and the, the next one is the feed forward network as transformers.

01:13:38.280 --> 01:13:42.040
And basically the only difference that you have is that those variables you want to compute the

01:13:42.040 --> 01:13:47.240
posterior and you make those posterior's independence, independent via, via mean field

01:13:47.240 --> 01:13:52.360
approximation. So basically you follow all the steps that allow you to, to converge to the

01:13:52.360 --> 01:13:57.400
very, to the variational free energy of creative coding. But the, the way, the way you compute

01:13:57.400 --> 01:14:06.120
predictions and the way you, you send signals back is a, is done via transformer. So I will

01:14:06.120 --> 01:14:11.720
still use transformers in general. I mean, they work so well that I, I don't think that we can be

01:14:12.120 --> 01:14:16.440
arrogant and say, oh no, I'm going to do it better via a purely predictive coding way.

01:14:17.640 --> 01:14:21.720
Structure learning is a way to do it, but we'll still approximate transformers anyway.

01:14:22.520 --> 01:14:26.680
So you said structure learning would approximate the transformer approach?

01:14:27.560 --> 01:14:33.240
Yes. Destruction learning I mentioned earlier in, when, when someone asked the similarities

01:14:33.240 --> 01:14:36.040
between predictive coding and the attention mechanism.

01:14:38.360 --> 01:14:46.040
Very, yeah, very interesting. One thing I am wondering from MLBong, I could not see the

01:14:46.040 --> 01:14:50.600
concept of depth in the predictive coding networks you mentioned. Most likely I missed it. The

01:14:50.600 --> 01:14:57.800
definition provided for predictive coding involved the concept of depth. What did you mean by depth?

01:14:58.360 --> 01:15:05.560
No, yes, it's true. It's a, because the standard definition, as I said, multiple times is a,

01:15:05.560 --> 01:15:09.240
is hierarchical. You have predictions going one directions and prediction error going the

01:15:09.240 --> 01:15:15.720
opposite direction. Basically, what, what we did in, in this paper and also in the last one in

01:15:16.520 --> 01:15:20.360
which is called the learning on arbitrary graph topologies via predictive coding

01:15:21.080 --> 01:15:30.840
is that we can consider depth like as a, as independent, basically pair of latent variable,

01:15:30.840 --> 01:15:35.640
latent variable, and arrow. And you have predictions going that direction and prediction

01:15:35.640 --> 01:15:41.960
error going the other. But then you can compose these in how many, a lot of ways. So you can,

01:15:42.920 --> 01:15:47.960
you can, so basically this composition doesn't have to be hierarchical in the end.

01:15:48.680 --> 01:15:51.720
Can have cycles. So then you can, for example, plug in another,

01:15:53.000 --> 01:15:57.480
another latent variable to the first one, and then connect the other two. And you can have a

01:15:57.480 --> 01:16:03.320
structure that is as entangled as you want. So for example, in the, in the other paper, we train

01:16:03.320 --> 01:16:09.000
the, a network that has the shape of a brain structure. So we have a lot of brain regions

01:16:09.000 --> 01:16:15.000
that are sparsely connected inside and sparsely connected among each other. And, and there's,

01:16:15.000 --> 01:16:18.760
there's nothing hierarchical there at the end, but you can still train it by minimizing

01:16:18.760 --> 01:16:23.400
a ratio of free energy and by minimizing the, the total prediction error of the network.

01:16:25.800 --> 01:16:35.640
So you could have for a given motif in a entangled graph, you might see three successive layers

01:16:35.640 --> 01:16:39.240
that when you looked at them alone, you'd say, Oh, that's a three story building.

01:16:39.240 --> 01:16:44.520
That's a three layer model that has a depth of three. But then when you take a bigger picture

01:16:45.160 --> 01:16:50.840
there isn't like an explicit top or an explicit bottom to that network.

01:16:52.200 --> 01:16:55.880
Yes, exactly. And this is basically given by the, by the fact that every operation in

01:16:55.880 --> 01:17:01.640
creative coding networks is strictly local. So, so basically every message passing every

01:17:01.640 --> 01:17:06.760
prediction and every prediction error that you send, you only send it to the very nearby neurons.

01:17:07.320 --> 01:17:11.160
Okay. And whether the global structure is actually hierarchical or not,

01:17:11.880 --> 01:17:14.840
the, the single message passing doesn't even see that.

01:17:17.480 --> 01:17:27.000
I guess that's sort of the hope for learning new model architectures is the space of what is

01:17:27.880 --> 01:17:34.840
designed top down is very small and a lot of models in use today,

01:17:35.480 --> 01:17:42.600
albeit super effective models. Although you could ask effective per unit of compute or not,

01:17:42.600 --> 01:17:47.320
that's a second level question. But a lot of effective models today do not have some of these

01:17:47.320 --> 01:17:55.240
properties of predictive coding networks, like their capacity to use only local computations,

01:17:55.880 --> 01:18:04.760
which gives biological realism or just spatio temporal realism, but also may provide a lot

01:18:04.760 --> 01:18:08.920
of advantages in like federated compute or distributed computing settings.

01:18:10.600 --> 01:18:15.960
No, yes, exactly. I completely agree. I think the idea in general is that, and I don't know if

01:18:15.960 --> 01:18:20.120
that's going to be an advantage. I think it's very promising exactly for the reasons you said.

01:18:20.920 --> 01:18:24.920
And the reason is that today's models string with back propagation,

01:18:24.920 --> 01:18:32.840
you can basically summarize them as a model string back propagation is a function,

01:18:32.840 --> 01:18:37.640
because basically you have a map from input to output, and back propagation basically

01:18:38.280 --> 01:18:45.160
spreads information back from its computational graph. So every neural network model used today

01:18:46.040 --> 01:18:51.960
is a function. While predictive coding and not only predictive coding, like the whole class of

01:18:51.960 --> 01:18:58.360
functions, the class of methods that train in using local computations and actually work by

01:18:58.360 --> 01:19:05.000
minimizing a global energy function, they're not limited to model functions from input to output.

01:19:05.000 --> 01:19:09.960
They actually model something that kind of resembles physical systems. So you have a physical

01:19:09.960 --> 01:19:16.200
system, you fix some values to whatever input you have, and you let the system converge,

01:19:16.200 --> 01:19:21.320
and then you read some other value of neurons or variables that are supposed to be output.

01:19:21.320 --> 01:19:26.040
But this physical system doesn't have to be a feedforward map. It doesn't have to be a function

01:19:26.040 --> 01:19:32.200
that has an input space and an output space, and that's it. So the class of models that you can

01:19:32.200 --> 01:19:38.760
learn is also basically you can see like feedforward models and functions, and then a much bigger

01:19:38.760 --> 01:19:43.640
class, which is that of physical systems. Whether there's something interesting out here, I don't

01:19:43.640 --> 01:19:48.280
know yet, because the functions are working extremely well. We are seeing those days with

01:19:48.280 --> 01:19:54.440
back propagation, they work crazy well. So yeah, I don't know if there's anything interesting in

01:19:54.440 --> 01:20:00.520
the big part, but the big part is quite big. There are a lot of models that you cannot

01:20:01.160 --> 01:20:03.800
train with back propagation, and you can train with predictive coding,

01:20:04.680 --> 01:20:11.240
or a background propagation or other methods. That is super interesting. Certainly biological

01:20:11.240 --> 01:20:18.200
systems, physical systems solve all kinds of interesting problems. But there's still no free

01:20:18.200 --> 01:20:23.640
lunch, and ant species does really well in this environment might not do very well in another

01:20:23.640 --> 01:20:31.000
environment. And so out there in the in the hinterlands, there might be some really unique

01:20:31.080 --> 01:20:37.080
special algorithms that are not well described by being a function,

01:20:38.520 --> 01:20:48.040
yet still provide like a procedural way to to implement heuristics, which might be extremely,

01:20:48.040 --> 01:20:54.920
extremely effective. No, yes, yes, exactly. And yeah, and I think this has been most of my

01:20:55.720 --> 01:21:01.640
focus of research during my PhD, for example, like finding this application that is like out

01:21:01.640 --> 01:21:12.040
here and not inside the the functions. Cool. Well, where does this work go from here? Like,

01:21:12.040 --> 01:21:18.600
what directions are you excited about? And how do you see people in the active inference ecosystem

01:21:18.600 --> 01:21:25.160
getting involved in this type of work? I think every probably the most promising

01:21:25.720 --> 01:21:32.760
direction, which is something maybe I would like to explore a little bit is to, as I said,

01:21:32.760 --> 01:21:39.640
there is to go behind statistical models. So everything I've shown so far is about static

01:21:39.640 --> 01:21:45.560
data. So the data don't change over time, there's no time inside the definition of

01:21:45.560 --> 01:21:50.200
predictive coding as it is as I presented it here. However, you can, for example,

01:21:50.200 --> 01:21:55.400
generalize predictive coding to to work with temporal data using generalized coordinates,

01:21:55.400 --> 01:22:03.000
as you mentioned earlier, by by presenting it as a as a Kalman Kalman filter generative model.

01:22:04.280 --> 01:22:10.440
And and that's where, for example, the causal inference direction could be very useful,

01:22:10.440 --> 01:22:16.600
because at that model, in at that point, maybe you can be able to model Granger causality and

01:22:17.400 --> 01:22:20.040
and more complex and and useful

01:22:21.960 --> 01:22:27.480
dynamical causal models, basically. Because in general, the the due calculus and the

01:22:27.480 --> 01:22:35.000
interventional and counterfactual branch of science is mostly developed on on small models.

01:22:35.960 --> 01:22:43.400
So it's like you don't do interventions on gigantic models in general. So if you if you

01:22:43.400 --> 01:22:50.280
look at medical data, they use relatively small vision networks. And but of course,

01:22:50.280 --> 01:22:56.200
if you want to have a dynamical causal model, that models a specific environment or a specific

01:22:56.200 --> 01:23:01.080
reality, you have a lot of neurons inside, you have a lot of latent variables, they change over

01:23:01.080 --> 01:23:07.000
time and an intervention at some more at some moment creates an effect in a different time step.

01:23:07.000 --> 01:23:11.560
So maybe the next time step in 10 different time steps later. And I think that would be

01:23:11.560 --> 01:23:16.840
very interesting to develop like a biologically plausible way of passing information

01:23:17.960 --> 01:23:21.480
that is also able to model Granger causality, basically.

01:23:24.680 --> 01:23:27.400
Where do you see action in these models?

01:23:27.400 --> 01:23:39.080
Where do I see action? I didn't think of that. I think I see actions in those models,

01:23:39.080 --> 01:23:41.960
maybe in the same way as I as you see in other models, because

01:23:43.080 --> 01:23:48.680
creative coding is basically a model of perception. So so an action is you can see

01:23:48.680 --> 01:23:54.680
that's a consequence of what you're experiencing. So by changing the way you're you're

01:23:55.320 --> 01:24:00.840
experiencing something, then you can compute maybe you can simply perform a smarter action

01:24:00.840 --> 01:24:08.280
now that you have more information. But but yeah, I don't think action is very easy. Like,

01:24:08.280 --> 01:24:13.080
yeah, I don't see any explicit consequence of actions, besides the fact that this can allow

01:24:13.080 --> 01:24:20.200
you to basically maybe to simply draw better conclusions to then perform actions in the future.

01:24:20.760 --> 01:24:26.760
I'll add on to that a few ways that people have talked about predictive coding and action.

01:24:27.720 --> 01:24:35.560
First off, internal action or covert action is attention. So we can think about perception

01:24:35.560 --> 01:24:40.920
as an internal action that that's one approach. Another approach pretty micro is the outputs

01:24:40.920 --> 01:24:48.440
of a given node. We can understand that node as a particular thing with its own sensory cognitive

01:24:48.440 --> 01:24:55.720
and action states. And so in that sense, the output of a node. And then lastly, which we

01:24:55.720 --> 01:25:00.520
explored a little bit in live stream 43, on the theoretical review on predictive coding,

01:25:01.080 --> 01:25:05.240
we're reading all the way through. And it was all about perception all about perception. And then

01:25:05.240 --> 01:25:15.400
it was like section 5.3. If you have expectations about action, then action is just another variable

01:25:15.480 --> 01:25:20.680
in this architecture. And that's really aligned with inactive inference, where instead of having

01:25:20.680 --> 01:25:26.440
like a reward or utility function that we maximize, we select action based upon it being the

01:25:26.440 --> 01:25:31.240
likeliest course of action, the path of least action, that's Bayesian mechanics. And so it's

01:25:31.240 --> 01:25:39.400
actually very natural to bring in an action variable and utilize it essentially as it as if it were

01:25:40.120 --> 01:25:46.840
a prediction about something else. Exteroceptively in the world, because we're also expecting action.

01:25:48.520 --> 01:25:54.760
No, yes, yes, exactly. No, I like the way of defining actions a lot, actually. And I still

01:25:54.760 --> 01:26:01.080
think if it's been like, for example, there are not so many papers that apply this method. I think

01:26:01.080 --> 01:26:07.080
there are a couple from from Alexander Orobrie does something similar. But in practice, like

01:26:07.880 --> 01:26:12.360
outside of the pure active inference, like applying predictive coding and actions to

01:26:13.160 --> 01:26:17.080
solve practical problems hasn't been explored a lot.

01:26:19.720 --> 01:26:25.640
Well, thank you for this excellent presentation and discussion. Is there anything else that you

01:26:25.640 --> 01:26:33.640
want to say or point people towards? No, just a big thank you for inviting me. And

01:26:34.600 --> 01:26:38.760
it was really fun. And I hope to come back at some point for for some future works.

01:26:40.280 --> 01:26:50.600
Cool. Anytime, anytime. Thank you, Thomas. So thank you, Daniel. See you. Bye. Bye.

01:27:03.640 --> 01:27:05.100
You

