WEBVTT

00:00.000 --> 00:09.540
Hello and welcome, everyone, to the Active Inference Institute.

00:09.540 --> 00:16.640
This is Active Gueststream number 41.1 on April 25, 2023.

00:16.640 --> 00:20.460
We're here with Elliot Murphy and Steven Piantadosi.

00:20.460 --> 00:22.280
This is going to be quite a discussion.

00:22.280 --> 00:25.640
We will begin with opening statements from Steven and Elliot.

00:25.640 --> 00:29.880
Elliot will then lead with some questions and we'll have an open discussion at the

00:29.880 --> 00:30.880
end.

00:30.880 --> 00:36.080
So, Steven, please, thank you for joining and to your opening statement.

00:36.080 --> 00:37.080
Cool.

00:37.080 --> 00:39.360
Hi, so I'm Steve Piantadosi.

00:39.360 --> 00:45.320
I'm a professor in psychology and neuroscience at UC Berkeley.

00:45.320 --> 00:50.080
And I guess part of the reason that we're here is that I recently wrote a paper on large

00:50.080 --> 00:57.080
language models in part trying to convey some enthusiasm about what they've kind of accomplished

00:57.080 --> 01:01.000
in terms of learning syntax and semantics.

01:01.000 --> 01:05.600
And in part pointing out, I think that these models really change how we should think about

01:05.600 --> 01:12.240
language, how we should think about theories of linguistic representation and theories

01:12.240 --> 01:16.360
of grammar and likely also theories of learning.

01:16.360 --> 01:17.360
Yeah.

01:17.360 --> 01:18.360
Awesome.

01:18.360 --> 01:19.360
Yeah.

01:19.360 --> 01:21.360
So, I'm Elliot Murphy.

01:21.360 --> 01:25.680
I'm a postdoc in the Department of Neurosurgery at UC Health in Texas.

01:25.680 --> 01:29.600
I read Steven's paper with great interest and there's a lot of people.

01:29.600 --> 01:33.440
There were some areas of convergence, but the things I want to kind of focus on today

01:33.440 --> 01:38.520
in responding to Steven and kind of probing how to do with areas of divergence maybe.

01:38.520 --> 01:44.200
So, you know, Steven's paper is based on the idea that modern machine learning has subverted

01:44.200 --> 01:48.800
and bypassed the entire theoretical framework of Chomsky's approach.

01:48.800 --> 01:51.840
So I wanted to kind of respond to some of these main arguments and some other related

01:51.840 --> 01:56.560
arguments in the literature that some folks listening might have some insight and thoughts

01:56.560 --> 01:57.560
on.

01:57.560 --> 02:02.000
So, it's a very common criticism to say that large language models just predict the next

02:02.000 --> 02:05.800
token, which is obviously a bit of a cliche, right?

02:05.800 --> 02:07.000
It's not quite true.

02:07.000 --> 02:12.560
They don't just predict the next token, they also seem to confabulate, they seem to hallucinate,

02:12.560 --> 02:18.000
they maybe lie, they randomly provide different answers to the same question, and they seem

02:18.000 --> 02:21.120
to stochastically mimic language like structures.

02:21.120 --> 02:24.160
They sometimes correct themselves sometimes when they shouldn't.

02:24.160 --> 02:27.120
If you push them a little, they kind of change their mind sometimes.

02:27.120 --> 02:30.400
In fact, if Fox News is currently looking for a replacement for Tucker Carlson, they

02:30.400 --> 02:31.400
could do less.

02:31.400 --> 02:36.400
They could definitely do worse than using ChachiBT if they're looking for a similar

02:36.400 --> 02:37.400
caliber.

02:37.400 --> 02:41.880
So, these models seem to do all sorts of like wild things, and over the past 10 years, there's

02:41.880 --> 02:46.320
been a sequence of different, you know, systems developed like where to be, where to be, and

02:46.320 --> 02:49.840
each of them is based on a different neural net approach, but ultimately they all seem

02:49.840 --> 02:54.560
to take words and characterize them by lists of hundreds or thousands of numbers.

02:54.560 --> 03:01.800
So the GTP3 network has 175 billion weights, 96 attention heads in its architecture, and

03:01.800 --> 03:04.600
as far as what I know, maybe Stephen can correct me here.

03:04.600 --> 03:08.360
We don't really have a great idea of what these different parts really mean.

03:08.360 --> 03:10.080
It just seems to kind of work that way.

03:10.080 --> 03:15.040
Like attention heads in GTP3 can pay attention to much earlier tokens in the string in order

03:15.040 --> 03:18.840
to help them predict the next token, but the whole architecture from start to finish is

03:18.840 --> 03:24.760
kind of engineering-based motivations, and I always kind of wonder what about all the

03:24.760 --> 03:29.600
models that kind of failed from these LLMs, from the different tech companies.

03:29.600 --> 03:33.360
It's like these companies often seem to, you know, make it seem like they have these models

03:33.360 --> 03:38.840
that really work very well straight out the box, and they all seem to be named after some

03:38.840 --> 03:40.840
kind of famous artists, right?

03:40.840 --> 03:43.120
They have Dali after Salvador Dali.

03:43.120 --> 03:47.240
They have Da Vinci, maybe pretty soon one of these companies will release a large language

03:47.240 --> 03:51.520
model called Jesus or something, I don't know.

03:51.520 --> 03:54.040
But they always say, here's our new foundation model.

03:54.040 --> 03:55.040
It's called Picasso.

03:55.040 --> 03:56.040
It's the first one we tried.

03:56.040 --> 03:57.040
It works just great.

03:57.040 --> 04:00.720
No problems straight out the box, but I always wonder what about all the other black boxes

04:00.720 --> 04:02.960
that have kind of failed every time?

04:02.960 --> 04:07.360
That doesn't seem to be a kind of a very open and clear structure to the kind of scientific

04:07.360 --> 04:12.200
reasoning behind selecting, you know, one model or another, but again, I might be, I'm

04:12.200 --> 04:15.120
open to be corrected about that.

04:15.120 --> 04:20.520
So even basic language models do pretty well on basic word prediction.

04:20.520 --> 04:24.160
So the issue is whether these tools provide any insights into traditional psycholinguistic

04:24.160 --> 04:26.400
notions like grammar and parsing.

04:26.400 --> 04:30.680
So this is really why I kind of prefer the term corpus model rather language model, suggested

04:30.680 --> 04:33.240
by people like Sabra Varys.

04:33.240 --> 04:38.040
So as we pointed out that no one really thinks LLMs tell us anything profound about Python

04:38.040 --> 04:42.040
when they learn Python code just as well as natural language, but Python is a symbolic

04:42.040 --> 04:46.840
language with a phrase structure grammar and nobody says LLMs are unveiling the secrets

04:46.840 --> 04:47.840
of Python.

04:47.840 --> 04:48.840
Right.

04:48.840 --> 04:52.080
So just to quote Varys here, he says, if A and N models can be construed as explanatory

04:52.080 --> 04:56.240
theories for natural language based on their successes on language tasks, then in the absence

04:56.240 --> 04:59.520
of counter arguments, they should be good explanatory theories for computer language

04:59.520 --> 05:00.520
as well.

05:00.520 --> 05:05.400
Therefore, successful A and N models of natural language cannot be used as evidence against

05:05.400 --> 05:08.120
generative phrase structure grammars in language.

05:08.120 --> 05:12.160
So corpus model is really a more appropriate term for other reasons too.

05:12.160 --> 05:15.880
People like Emily Bender and some others have shown that features of the training corpus,

05:15.880 --> 05:20.200
in fact, I think Steven cites this, you cite this in your paper actually as a limitation.

05:20.200 --> 05:24.160
They show that features of the training corpus can heavily influence the learning process.

05:24.160 --> 05:27.400
So it's been shown that the performance of large language models on language tasks is

05:27.400 --> 05:31.800
really heavily influenced by the diversity of the training corpus.

05:31.800 --> 05:34.240
But natural language itself is not biased, right?

05:34.240 --> 05:36.800
It's just the computational system.

05:36.800 --> 05:40.160
Some beings can be biased in what they say and how they act.

05:40.160 --> 05:42.520
But natural language itself isn't biased, right?

05:42.520 --> 05:47.880
So large language models, therefore, it seems difficult for me to agree that they are being

05:47.880 --> 05:49.400
subject to all sorts of biases.

05:49.400 --> 05:52.920
They therefore can't really be models of language, they're models of something else.

05:52.920 --> 05:58.840
So just to kind of wrap up this argument, even though LLMs are clearly exposed to vastly

05:58.840 --> 06:02.560
more linguistic experience in children, again, this is something else that Steven concedes

06:02.560 --> 06:04.500
and talks about in his paper.

06:04.500 --> 06:09.020
And so their learning outcomes may still be relevant in addressing what grammatical generalizations

06:09.020 --> 06:10.660
are learnable in principle.

06:10.660 --> 06:14.220
So I do agree with this statement here, that in principle they can tell us something about

06:14.220 --> 06:18.900
learnability rather than things like broad acquisitionist frameworks.

06:18.900 --> 06:22.500
But that's about as much I think you can maybe say right now.

06:22.500 --> 06:27.140
Showing that some inductive biases are not necessary for learning is not really the same

06:27.140 --> 06:29.840
thing as showing that it isn't present in children.

06:29.840 --> 06:33.620
So there's been a long debate about whether negative evidence and instruction and correction

06:33.740 --> 06:39.300
and feedback during language learning are necessary or even useful for infants and children.

06:39.300 --> 06:43.340
But right now I kind of agree more with Eugene Choi and Gary Marcus and others who have highlighted

06:43.340 --> 06:46.820
how LLMs are currently very expensive to train.

06:46.820 --> 06:51.740
They're clearly an example of concentrated private power in the hands of a few tech companies.

06:51.740 --> 06:57.060
Their environment impact is massive and many of people have been less constrained and conservative

06:57.060 --> 07:01.940
in their assessment here, which is much less so than Gary Marcus and Eugene.

07:01.980 --> 07:08.820
So Bill Gates recently wrote that chatGPT is the biggest tech development since the

07:08.820 --> 07:10.820
graphical user interface, the GUI.

07:10.820 --> 07:16.100
And Henry Kissinger wrote in February in the Wall Street Journal that as chatGPTs capacities

07:16.100 --> 07:21.620
become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality

07:21.620 --> 07:24.180
and reorganize politics and society.

07:24.180 --> 07:29.180
Generative AI is poised to generate new forms of human consciousness, so very radical claims

07:29.180 --> 07:30.180
happening at the moment.

07:30.180 --> 07:36.100
I do wonder if sometimes all of the AI hype may have, you know, see it into certain portions

07:36.100 --> 07:39.940
of academia potentially, a lot of ground claims being made.

07:39.940 --> 07:43.220
But I think, you know, more concretely, just to put it back to Stephen here, I wanted to

07:43.220 --> 07:48.820
maybe raise the issue of there's a critique by Roscoe and Beaumont that I think he's read

07:48.820 --> 07:51.500
on Lingbos.

07:51.500 --> 07:56.140
I think you saw on Twitter that you don't like the response they gave because the objection

07:56.140 --> 07:59.620
that they made is that, you know, science is an example of deductible logic.

07:59.620 --> 08:03.340
Your objection is that science isn't deductive, it's inductive, right?

08:03.340 --> 08:08.380
But I think their general point might be more accurate, namely that you can't use the fact

08:08.380 --> 08:13.620
that language models do well predicting some linguistic behavior in humans and some neuroimaging

08:13.620 --> 08:14.860
responses.

08:14.860 --> 08:19.580
You can't use that alone to claim that they can yield a theory of human language.

08:19.580 --> 08:23.340
So in your paper, Stephen, you know that it seems that certain structures work better

08:23.340 --> 08:24.340
than others.

08:24.340 --> 08:28.460
The right attentional mechanism is important, prediction is important, semantic representations

08:28.460 --> 08:29.460
are important.

08:29.500 --> 08:32.620
And therefore, we can glean currently based on these models, right?

08:32.620 --> 08:35.900
But so far, that's really all I've been able to glean in the literature.

08:35.900 --> 08:37.940
I'm not sure if you have more insights here.

08:37.940 --> 08:43.700
So Roscoe and Beaumont use the example of poor prediction, but strong explanation, right?

08:43.700 --> 08:47.140
Explanatory power and not predictive accuracy forms the basis of modern science.

08:47.140 --> 08:50.780
I don't want to explore this a little bit later, maybe, but modern language models can

08:50.780 --> 08:54.540
accurately model parts of human language, but they can also perform very well on impossible

08:54.540 --> 08:59.420
languages and unnatural structures that humans can't learn and have great difficulty

08:59.500 --> 09:00.140
processing.

09:00.140 --> 09:02.220
And I know you're familiar with these with these criticisms, right?

09:03.180 --> 09:04.940
But you're definitely not alone here at the same time.

09:04.940 --> 09:12.220
So Ilya Tskeva, the chief scientist at OpenAI, he said in an interview recently, what does

09:12.220 --> 09:14.060
it mean to predict the next token well enough?

09:14.460 --> 09:19.020
It means that you understand the underlying reality that led to the creation of that token,

09:20.060 --> 09:23.420
which is quite divergent from a lot of more conservative claims in the literature here.

09:24.620 --> 09:28.940
And also, you know, I would just say in response to that, that different components of science

09:29.020 --> 09:32.220
can be either inductive or deductive, right?

09:32.220 --> 09:33.340
It's not really an either-or.

09:33.340 --> 09:34.700
You have an existing theory.

09:34.700 --> 09:36.300
You formulate a hypothesis.

09:36.300 --> 09:40.460
You collect data, you analyze it, and that's kind of a deductive process.

09:40.460 --> 09:43.340
But there's also cases where you start with a specific observation.

09:43.340 --> 09:46.460
You find some patterns and you induce general conclusions, right?

09:46.460 --> 09:53.020
And then there's abduction, where you magically invent hypotheses and reduce the hypothesis space.

09:53.020 --> 09:58.220
You wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific

09:58.220 --> 10:00.540
or abductive reasoning is unscientific, right?

10:00.540 --> 10:02.860
These are all just different ways of doing stuff.

10:02.860 --> 10:09.260
I mean, in your paper, you give the examples of using models to predict hurricanes and pandemics

10:09.260 --> 10:12.860
as being examples of stuff that is as rigorous as science gets.

10:12.860 --> 10:17.180
And then you employ a reader to conclude that the situation is no different for language models.

10:18.220 --> 10:22.380
But I guess for me, the issue is that models predicting hurricanes are not in the business

10:22.380 --> 10:25.980
of answering the question, what is the hurricane, right?

10:26.060 --> 10:29.420
Models accurately predicting the weather are very accurate, but they're not.

10:29.420 --> 10:33.420
They're aligned with the meteorology department, but they're not a substitute for it.

10:34.380 --> 10:36.140
So I guess I'll just hand it over to you.

10:37.820 --> 10:40.220
Yeah. Okay. Well, there's a lot there.

10:41.660 --> 10:48.860
I guess I could start just by saying that I agree with many of these criticisms, right,

10:48.940 --> 10:57.900
about these models being controlled by one or two companies that being very, very problematic.

11:00.300 --> 11:04.780
They have all kinds of biases that they've acquired because they're trained on text from the

11:04.780 --> 11:12.940
internet. That's hugely problematic. I certainly agree that there's things at least at present

11:12.940 --> 11:20.140
that the models don't do well, right? So I think it's easy to find examples of questions

11:20.140 --> 11:25.900
and problems that will trip them up. I think why I've been excited about them, though,

11:26.940 --> 11:34.060
is not necessarily in those terms, right, but in terms of performance on language,

11:34.780 --> 11:42.140
specifically syntax and semantics. I think they're far beyond kind of any other theory

11:42.140 --> 11:49.900
in any other domain, right? So there's no other theory out of linguistics or computer science

11:50.620 --> 11:57.340
which can generate long, coherent grammatical passages of text.

11:58.460 --> 12:07.260
And so kind of admitting all of their problems as tools or things which are deployed by companies,

12:08.220 --> 12:15.100
there's still this question of, like, how are they at dealing with language? And I think this

12:15.100 --> 12:20.140
is where a lot of the enthusiasm comes from, is there really hasn't been anything even remotely

12:20.140 --> 12:26.220
like them in terms of linguistic ability. And that's the thing that I think is exciting. So,

12:26.220 --> 12:33.100
yes, I agree with a bunch of these things you started with, but nonetheless, like I think in

12:33.100 --> 12:37.980
terms of syntax and semantics, there's just no other theory which is comparable to them.

12:40.220 --> 12:46.460
But so let me push that back then, right? So the main objection from a lot of people I've spoken

12:46.460 --> 12:51.740
to in the departments of linguistics who are like a lot of the general first of your paper

12:52.460 --> 12:57.900
is to really say, well, you're right, they do a wonderful job accurately modeling all aspects

12:57.980 --> 13:03.260
of a lot of aspects of syntax and semantics. However, I don't know if any real just like,

13:03.260 --> 13:06.860
you know, Chomsky talks about facts about language, which is an old fashioned notion.

13:07.580 --> 13:10.780
But I really think that's kind of an important notion too, right? Like,

13:10.780 --> 13:18.620
is there some discovery about language itself that LLMs can uniquely provide? So like, if LLMs

13:18.620 --> 13:24.780
made some prediction about, let's say you have a sentence structure type X being more difficult

13:24.860 --> 13:29.100
to process than sentence type Y. And this is a unique prediction that only they'd

13:29.660 --> 13:34.060
generate it. And no human linguist, Chomsky, Honesty, and Adger, none of these people had

13:34.060 --> 13:37.900
ever predicted that before. But it turns out to be true. You do eye tracking experiments,

13:37.900 --> 13:41.020
you do all sorts of different behavioral experiments. And it turns out, oh, you know,

13:41.020 --> 13:45.020
after all, it turns out to be true. This is the new insight about language processing,

13:45.020 --> 13:49.580
it's a new insight about language, you know, behavior. I just wonder, I'm not saying that

13:49.580 --> 13:53.660
this is not possible in principle, because it might happen in the near future. But that's,

13:53.660 --> 13:58.140
I guess, for me, the crux of why a lot of linguists speaking up, speaking on behalf of

13:58.780 --> 14:03.100
the entire linguistic community here. And, you know, I guess that would be one of the main

14:03.100 --> 14:11.100
objections. Yeah, I mean, I don't know of, I guess, I think of the insights they've provided as

14:11.100 --> 14:18.220
kind of general principles, right? So I think about these things like the power of memorizing

14:18.220 --> 14:23.260
chunks of language, right? So like, they seem to be very good at constructions, for example.

14:23.340 --> 14:27.500
And there's lots of linguistic theories, Chomsky's in particular, right, which are

14:28.380 --> 14:34.140
about trying to find kind of minimal amounts of structure to memorize, right, trying to derive

14:34.140 --> 14:40.780
as much as possible from some small set, some small collection of operations. And I think

14:40.780 --> 14:47.020
that hasn't gone well for those theories, right? Whereas this goes really well, right? So if we

14:47.020 --> 14:50.860
think about something which has the memorization abilities, if we think about theories of grammar,

14:50.860 --> 14:58.460
for example, which build on, you know, humans like really remarkable ability to memorize different

14:58.460 --> 15:02.780
constructions, right, or different words, you know, tens of thousands of words, tens of thousands of

15:02.780 --> 15:07.020
different constructions, sorry, tens of thousands of different idioms, maybe our theory of grammar

15:07.020 --> 15:12.140
should be integrated with that. And there in some sense, a kind of proof of principle that

15:12.140 --> 15:17.740
that kind of approach can work well, right? Can think about making other types of predictions

15:17.740 --> 15:23.500
with them, some of which people are currently doing, but for example, trying to use them to measure

15:24.300 --> 15:30.300
processing difficulty, measure surprise, for example, from these models, their surprise measures,

15:30.300 --> 15:36.620
right, are much better than, say, context-free grammars or other kinds of language models.

15:36.620 --> 15:41.980
And then it's an interesting question how those surprises or predictabilities relate to human

15:41.980 --> 15:47.180
processing, right? And it may capture some of it or might be nonlinear, or it might, you know,

15:47.180 --> 15:52.700
only capture a little bit of it or whatever. That's an interesting kind of other scientific

15:52.700 --> 15:57.580
question. But I think in principle, right, they can make predictions about, for example, the

15:57.580 --> 16:03.500
connections between sentences, right? So in the paper, I gave this example of, you know, converting

16:03.500 --> 16:09.820
a declaration into a question in 10 different ways, right? And presumably when it, when, you know,

16:09.820 --> 16:15.820
GPT or something is doing that, it's finding 10 different questions which are all in some way

16:15.820 --> 16:22.700
related, kind of nearby in the models underlying semantic or syntactic space. And so those kinds

16:22.700 --> 16:29.500
of things are of the type that I think, you know, some linguists might want, right, which is here's

16:29.500 --> 16:34.220
some hidden connection between sentences or their, or their structures. But as far as I know, they

16:34.220 --> 16:40.460
haven't been evaluated empirically yet. So, yeah, yeah, I mean, these kinds of models are only a

16:40.460 --> 16:45.820
few years old. So I think it's, it's reasonable to be excited about them, even though this kind

16:45.820 --> 16:51.260
of work hasn't been done yet. No, that's right. No, totally. Totally. I mean, I think that's

16:51.260 --> 16:55.820
the right perspective to take. But I think this gets to the issue of the, you mentioned surprise

16:55.820 --> 17:02.220
or you mentioned learnability, you know, LMS learn some syntax, but they do so. We've obviously way,

17:02.220 --> 17:08.700
way more data than infants do. Such that observations of potential structure in and of itself is not

17:08.780 --> 17:12.940
a refutation of the poverty of the stimulus, well, the weaker version, I should say, of the

17:12.940 --> 17:17.900
poverty of the stimulus argument. So the mere fact that LMS can do what they do without grammatical

17:17.900 --> 17:22.380
prize is very striking, I agree. And in fact, you wouldn't have predicted that maybe five or six or

17:22.380 --> 17:28.060
seven years ago. But it doesn't yet invalidate the claim that humans have such a prize and we

17:28.060 --> 17:32.380
bring those prizes with us. And so in order to see if computational linguistics can constrain

17:32.380 --> 17:36.140
hypotheses and theoretical linguistics, which I think it can do, by the way, this needs to be

17:36.140 --> 17:39.980
done with, you know, careful experiments in which different learning parameters are controlled.

17:40.700 --> 17:47.180
And gigantic language models like GPT3 are basically, you know, useless here. So this gets

17:47.180 --> 17:52.380
to some of Tarlin's complaints that we need something like a baby LM project, which I know

17:52.380 --> 17:56.700
you're interested in, where we have more, you know, ecologically valid training sets. You make the

17:56.700 --> 18:00.620
prediction in your paper that some structure will be learned from that. I suspect you might be right

18:00.620 --> 18:04.940
there. But, you know, even so, even with the baby LM challenge, there's still the kind of

18:05.020 --> 18:10.620
non-trivial issue of addressing more traditional issues like when the kids start to generalize

18:10.620 --> 18:15.420
based on the amount of current input, based on different factors, cross-linguistically. And that

18:15.420 --> 18:21.180
requires just traditional, you know, psycholinguistics and language acquisition. So LMs, you know, do

18:21.180 --> 18:25.260
care about things like frequency and surprise will, as you said, but there's a really nice paper by

18:25.260 --> 18:30.140
Sophie Slatzen, Andrea Martin, a really beautiful paper that I think you may have seen that shows

18:30.140 --> 18:34.860
very nicely that distributional statistics can sometimes be a cue to moments of structure

18:34.860 --> 18:39.340
building. But it doesn't replace these notions pertaining to composition. So I'll just read a

18:39.340 --> 18:45.820
quote from Chomsky 57, which sounds a lot like what Slatzen Martin said. Despite undeniable

18:45.820 --> 18:51.260
interest and importance of semantic and statistical models of language, they appear to have no direct

18:51.260 --> 18:55.180
relevance to the problem of determining or characterizing the set of grammatical utterances.

18:55.180 --> 18:59.020
I think that we are forced to conclude that grammar is autonomous and independent of meaning,

18:59.020 --> 19:03.260
and that probabilistic models give no particular insight into some of the basic problems of

19:03.260 --> 19:09.500
syntactic structure. So that second hedge of the second sentence turned out to be incorrect.

19:10.060 --> 19:12.460
But it's so it's true that, you know, what Chomsky said of available

19:13.020 --> 19:19.020
stat models in 57 is no longer accurate when applied to models today. That can make abstract

19:19.020 --> 19:22.300
generalizations about novel strings and distributional categories, as you mentioned,

19:22.300 --> 19:27.260
right? But the performance of a single model does not provide direct evidence for or against

19:27.260 --> 19:31.260
the landability of a particular structure. Like given the vast distance between any

19:31.260 --> 19:36.620
computational model available today and the human brain, model success does not mean that the

19:36.620 --> 19:42.620
structure is necessarily land and model failure also doesn't mean that the structure is not landable,

19:42.620 --> 19:50.140
right? Yeah, yeah. So I mean, I think it's maybe worth unpacking kind of a couple different versions

19:50.140 --> 19:55.500
of learnability arguments that people have made, because there have been very, very strong kind

19:55.580 --> 20:01.260
of impossibility claims coming out of kind of Chomsky's tradition, right, that were never

20:01.260 --> 20:06.940
claims about the amount of data that was required, right? They were claims about the logical problem

20:06.940 --> 20:12.860
of language learning and that it was just impossible, right? It was impossible without having

20:14.780 --> 20:19.580
kind of substantial constraints on the class of languages or the class of grammars that you

20:20.140 --> 20:25.580
would acquire. And people for a long time have been arguing against that version of things.

20:27.180 --> 20:31.180
You know, there's old work by Gold, and then there's whole kind of grammatical

20:31.180 --> 20:37.820
theories of acquisition built on that tradition that worry a lot about the kind of order in

20:37.820 --> 20:41.820
which you traverse through different hypotheses and consider different options and things.

20:42.780 --> 20:48.620
And my favorite reference in this is this paper by Nick Chater and Paul Vitani,

20:49.420 --> 20:55.260
called something like Ideal Learning of Natural Language, that basically shows that an unconstrained

20:55.260 --> 21:02.700
learner could, with enough data, acquire the kind of generating rules or the generating grammar

21:03.340 --> 21:10.300
just from observing strings, right? But that paper was really in response to this huge body of work

21:10.380 --> 21:15.980
that was arguing that learning from positive examples, so from just observing strings,

21:15.980 --> 21:24.380
was like logically impossible, right? So of course, you know, people in Chomsky's tradition

21:24.380 --> 21:31.020
really liked that form of argument because it was one that said you had to have something innately

21:31.740 --> 21:36.300
specified in order for language acquisition to work. It was like kind of a mathematical argument,

21:36.780 --> 21:42.300
that you had to have some kind of innate grammar, innate ordering of hypotheses or something,

21:42.300 --> 21:49.020
and all of that just turned out to be totally wrong. So if you move to slightly more kind of

21:49.020 --> 21:55.900
realistic learning settings, which Chater and Vitani do, then it turns out you like an idealized

21:55.900 --> 22:00.780
learner can acquire stuff, and there's no statements about the amount of data that's required even

22:00.780 --> 22:08.860
there, right? That's the kind of pure logical ability to learn, and that ability is what I think

22:08.860 --> 22:14.620
the big versions of large language models also speak to, right? So Chater and Vitani and other

22:14.620 --> 22:20.540
work kind of in that spirit is, you know, mathematical and kind of arguing in principle,

22:20.540 --> 22:28.220
but never created something which was really a grammar, right, or a real kind of implemented

22:28.220 --> 22:35.580
language model. So even, you know, a model which is trained on 100 million or 100 billion or however

22:35.580 --> 22:43.500
many tokens, right, even that kind of model I think is relevant to that version of the debate,

22:43.500 --> 22:50.620
right, and showing that language learning is not impossible from a very unconstrained space.

22:50.940 --> 22:58.380
Okay. And then there's a second version, right, which is can we learn language with the specific

22:58.380 --> 23:03.500
data that kids get, right, and that's both amount of data and form of the data,

23:04.700 --> 23:11.100
and so for people who don't know, the BabyLM Challenge is this,

23:11.580 --> 23:21.580
sorry, we think to call it a competition or a, I guess it is a challenge, trying to get people

23:21.580 --> 23:27.740
to train language models on human sized amounts of data. So that's something more like, I think

23:27.740 --> 23:33.420
there's two different versions, 10 or 100 million different, 10 or 100 million different words in

23:33.420 --> 23:42.060
the training set, which is like, you know, 100th or 1000th or something as big as these big AI

23:42.060 --> 23:49.660
companies are using for their language models. And I think actually it's like, that's exactly the

23:49.660 --> 23:53.820
right kind of thing and exactly what the field needs, right, because you might find that on

23:54.620 --> 24:00.940
a child sized amount of data, you can essentially learn syntax, right, which I think would be the

24:00.940 --> 24:05.500
strongest argument against these property of stimulus claims, you could alternatively find that

24:06.140 --> 24:12.540
maybe you can't learn very much, maybe you, you know, come up with a much crumbier kind of language

24:12.540 --> 24:19.020
model or it's lacking some syntactic or semantic abilities. I actually think that the failures,

24:19.020 --> 24:25.500
they are a little bit hard to interpret because kids data, when they're actually learning language,

24:25.500 --> 24:31.580
they get a lot more data than just strings of sentences, right, they're interacting in an

24:31.580 --> 24:37.500
environment. So there's stuff in the world in front of them. Their utterances are also interactive,

24:37.500 --> 24:41.820
right, so you can say something and see whether your parent brings you the thing that you asked for,

24:41.820 --> 24:49.420
for example, right, that's long been argued by people as a, you know, important cue in language

24:49.500 --> 24:58.460
acquisition. So in the baby LM challenge, there is an ability to train these models

24:59.180 --> 25:03.900
with kind of multimodal input, so I think you can give them as much video data as you want to give,

25:05.500 --> 25:10.540
but probably it's hard to kind of replicate exactly the type of setup and feedback that kids

25:10.540 --> 25:18.540
actually get. So I don't know, you know, I'm excited to see where that goes and how things pan

25:18.540 --> 25:26.860
out there. You know, I think that there is an interesting related question for large language

25:26.860 --> 25:34.220
models, which is like what, which is understanding exactly what all of the data is doing. So it

25:34.220 --> 25:40.060
could be that you need so much data for these models because they're effectively inventing

25:40.060 --> 25:46.780
some form of semantics internally, right, so they're both discovering the rules of syntax and they

25:46.780 --> 25:53.660
appear to be learning quite a bit about word meanings. And it's not, it's totally unclear,

25:53.660 --> 26:00.220
I think, how much of the data in these modern models is needed for syntax versus semantics.

26:00.220 --> 26:07.900
My own guess, I think, would be that the syntactic side is probably requires much less data than the

26:07.900 --> 26:13.180
semantic side. Actually, a student, a former student of mine, Frank Malica, and I wrote a paper a few

26:13.180 --> 26:18.300
years ago trying to estimate the amount of information a learner would necessarily have to

26:18.300 --> 26:24.140
acquire for learning the different aspects of language. So you have to learn all the words and

26:24.140 --> 26:27.980
you learn their forms, you learn their meanings, you probably know their frequencies, you have to

26:27.980 --> 26:34.940
learn syntax. And basically what we found in that analysis, that was, you know, basically just a kind

26:34.940 --> 26:41.020
of back of the envelope calculation for each of these domains is that syntax is actually very few

26:41.020 --> 26:48.060
bits of information, it doesn't take that much information to learn syntax. Whereas like most

26:48.060 --> 26:55.740
of the information you acquire is actually for semantics. So specifying, you know, 30 to 50,000

26:55.740 --> 27:03.340
different word meanings, you know, even if each meaning is just a few bits, right, like that requires

27:03.340 --> 27:10.220
a lot of information and probably each meaning is more than a few bits, right. So it could be,

27:10.220 --> 27:14.540
like, that would make me guess that what's happening with large language models is most

27:14.540 --> 27:19.500
of their training data is about word semantics. And you can think about other ways that kids get

27:19.500 --> 27:26.140
word semantics, right, that's not just kind of co-current patterns in text. But I agree, all of

27:26.140 --> 27:31.180
that is up in the air and really exciting to see what will happen. Yeah, I know that some of the

27:31.180 --> 27:38.060
earlier results from Linsen's lab suggest that at least restricted to equitably valid, you know,

27:38.060 --> 27:43.420
training set sides, models seem to generalize, you know, linear rules for English, probably

27:43.420 --> 27:47.500
has no question of formation, rather than the hierarchical rule, the correct hierarchical rule.

27:47.500 --> 27:53.020
So I think there's a real sense in which, you know, the space of the correct syntactic prize and

27:53.020 --> 27:57.900
inductive biases really is yet to be really settled on. But it seems, at least to me, pretty

27:57.900 --> 28:02.220
obvious that there has to be some. So there's also some evidence that children in English,

28:02.220 --> 28:06.700
going back to this frequency issue, that children in English sometimes spell out an intermediate

28:06.780 --> 28:12.380
copy of movement in the specified position of the lower complementizer position of a long-distance

28:12.380 --> 28:16.940
WH question. So there's a thesis by Thornton at some of the papers about this. So they say,

28:16.940 --> 28:21.580
which person do you think who did that, rather than which person do you think did that? So this

28:21.580 --> 28:25.820
is an interesting, you know, missetting, because some languages do actually spell out these intermediate

28:25.820 --> 28:30.940
copies, but English doesn't. So the kid makes the error in setting their grammar, but the frequency

28:30.940 --> 28:36.220
of the input is actually zero. So our mutual friend Gary Marcus also has an argument against

28:36.220 --> 28:41.580
frequency-determining a kid's output. In the case of German noun plurals, a more regular form of the

28:41.580 --> 28:45.580
setting kind is preferred, not the frequent one. And there's lots of examples like this. So it's

28:45.580 --> 28:50.300
sometimes claimed that subject-experiencer passives, where the subject is passively experiencing

28:50.300 --> 28:55.100
something, are very delayed in kids in comprehension studies until around eight, because they're not

28:55.100 --> 29:00.620
very frequent in the input. But Ken Wexler and colleagues have gone through subject-experiencer

29:00.620 --> 29:05.900
WH questions like, who likes Mary? And they discovered that these are as infrequent in the

29:05.900 --> 29:10.700
input as subject-experiencer passives. But kids have no problem in comprehension studies of these

29:10.700 --> 29:16.460
questions. But they do have problems comprehending subject-experiencer verbal passives. So frequency

29:16.460 --> 29:20.780
once again seems to be irrelevant. Or at least it's not explanatory, right? I guess it's not

29:20.780 --> 29:25.420
explanatory with respect to theory building. So how can LMS help with these, you know,

29:25.420 --> 29:30.300
diverging cases when there's clearly something else going on besides frequency? So LMS, you know,

29:30.300 --> 29:35.020
they seem to generalize just, again, going back to this issue of the cases that you have in your

29:35.020 --> 29:40.060
paper. You show that they generalize the structure of color screen ideas, which is obviously very

29:40.060 --> 29:45.100
cool. But the positive stimulus has never really been about not being able to learn language

29:45.100 --> 29:49.500
statistically. I know you made that claim, right? But Chomsky's point in the 50s about statistical

29:49.500 --> 29:55.020
models of the day is not true of commercial LMS in 2023. And that's correct. But we can't use that

29:55.020 --> 29:59.660
single point to undermine, you know, the entire generator enterprise. Chomsky's basic point was

29:59.660 --> 30:04.300
that you could have a grammatical structure wherein every background has zero frequency,

30:04.300 --> 30:08.700
and it also fails to provide clearly interpretable instructions to the conceptual interfaces.

30:08.700 --> 30:12.700
So interfaces with other systems of the mind. So as you're showing your paper, GPT

30:12.700 --> 30:18.780
mimics examples like full screen ideas. But, you know, again, this sentence yields over 150,000

30:18.780 --> 30:23.500
results on Google, and it's discussed extensively in the literature. It's able to mimic the fact

30:23.500 --> 30:27.340
that it can mimic this doesn't really tell us that much. At least we can't really say anything

30:27.340 --> 30:33.260
with much confidence. So, you know, Albeba behind University College Dublin has this quote recently,

30:33.260 --> 30:38.860
do not mistake your own vulnerability for an LMS intelligence. In fact, even Yanlacun wrote last

30:38.860 --> 30:44.460
year that critics are right to accuse LMS of being engaged in a kind of mimicry. And the example

30:44.460 --> 30:50.140
sentence is from chat GPT that you give in the paper. Actually, don't do a good job because,

30:50.140 --> 30:54.220
as you say, it's likely that, you know, meaningless language is rare in the training data, but they

30:54.220 --> 30:58.540
can either do it or they can't. But there's no middle ground in terms of giving us 10 examples

30:58.540 --> 31:04.860
like this. So, you have colourless green ideas, which are very different semantic objects from

31:04.860 --> 31:11.260
things like brown shimmering rabbits, white glittery bears, black shiny kangaroos, green

31:11.260 --> 31:16.460
glittering monkeys, yellow dazzling lions, red shimmering elephants, right? These are all like

31:16.460 --> 31:21.740
semantic, semantically weird and a bit strange, but they're still like legal structures. They're

31:21.740 --> 31:33.020
kind of meaningful semantic objects. Right? I just said, yeah. Yeah. I mean, so maybe I can

31:33.020 --> 31:38.540
respond to the first point first, right? So, you started off talking about these other

31:39.340 --> 31:45.660
kinds of acquisition patterns, which maybe don't map directly onto frequency. And I think it's

31:45.660 --> 31:53.500
actually a mistake to think that kind of modern learning models should be just based on frequency,

31:53.500 --> 32:00.220
because they're clearly learning like pretty complicated families of rules or constructions

32:00.220 --> 32:07.260
or something. And I think it's very likely that when they're learning that, they're in some sense

32:07.260 --> 32:13.500
searching for a simple or parsimonious explanation of the data that they've seen, right? And how that

32:13.500 --> 32:20.940
caches out in a neural network is maybe complicated and depends on parameters and the specifics of

32:20.940 --> 32:29.020
the learning algorithm and those kind of things. But I think it's, I'd suspect maybe that it's

32:29.020 --> 32:40.300
likely to be the case that they're learning over a complicated set of things, right? A complicated

32:40.540 --> 32:48.620
kind of family of rules and constructions. And that means I think that their generalizations,

32:48.620 --> 32:55.820
maybe like the examples of people that you gave, might be kind of discontinuous in the input,

32:55.820 --> 33:01.500
right? So, sometimes you could imagine seeing some strings which lead you to a grammar and

33:01.500 --> 33:07.100
the simplest grammar of the data that you've seen so far is one which predicts an unseen string,

33:07.100 --> 33:15.100
right? And if that happens, then you'll be taking the data, learning a representation

33:15.100 --> 33:21.740
which generalizes in some novel unseen way so far, purely because that generalization is

33:21.740 --> 33:25.660
sort of the simplest account of the data that you've seen to date, right? I think that's sort of

33:25.660 --> 33:30.220
what linguists try to do, right? Try to look at the data and come up with a theory of it,

33:30.220 --> 33:35.580
and then sometimes that theory predicts some new phenomenon, right? Or some new type of sentence.

33:36.460 --> 33:40.300
And so, if they're learning over as sufficiently rich space of theories,

33:41.180 --> 33:47.020
then it wouldn't be unreasonable or unexpected for them to also show those kinds of patterns. Now,

33:47.020 --> 33:53.420
whether they do or not I think is still an open empirical question, right? Because we have to

33:53.420 --> 33:58.140
train them on small amounts of data and test their generalizations and these kind of things.

33:58.140 --> 34:05.100
But I don't think like just the fact that humans do things which are not purely based on

34:05.100 --> 34:09.020
frequency is any evidence at all, either way, right? Because once you're learning over rich

34:09.020 --> 34:16.140
and interesting classes of theories, then that is the expected behavior. Actually, I had a paper

34:16.140 --> 34:25.900
about a year ago that I think you're familiar with, Yang and Pianta dosi, where we were looking at

34:27.740 --> 34:33.100
kind of what happens when you give a program learning model strings from different formal

34:33.100 --> 34:41.180
languages. So think of like giving a general model just 10 or 20 maybe simple strings that

34:41.180 --> 34:47.180
obey some pattern and then asking it to find a program which can explain that data, which often

34:47.180 --> 34:54.940
means finding some way of kind of programmatically writing down the pattern in the strings. And

34:54.940 --> 35:01.820
in that figure, we have a paper which is really relevant to this point where the generalizations

35:01.820 --> 35:07.820
that that kind of model makes are I think kind of qualitatively like the ones you're describing

35:07.820 --> 35:13.900
for people, right? Where you can give them a small amount of data and it will predict unseen

35:13.900 --> 35:19.340
strings with very high probability, even though there's zero frequency in the training input,

35:19.340 --> 35:24.460
right? And the reason it does that is that often the most concise computational description of the

35:24.460 --> 35:32.780
data that you've seen is one that predicts some particular new unseen output. So that model is

35:32.780 --> 35:39.020
essentially an implementation of the kind of Chater and Vitani program learning idea that I

35:39.020 --> 35:44.060
brought up earlier. But it's one that I think, you know, if you think about in the context of

35:44.060 --> 35:49.820
these arguments of kids saying unusual or unexpected things, like that is predicted by all of these

35:49.820 --> 35:55.180
kinds of accounts, right? Because as long as these things are effectively comparing an interesting

35:55.180 --> 36:03.180
space of grammars, then they'll show that kind of behavior, I think. Yeah. So, okay. So I guess,

36:03.180 --> 36:11.260
you know, the argument would be that, at least from the gender perspective, syntax is functioning

36:11.260 --> 36:16.780
separately, but it still maps to semantics, it informs pragmatics, right? So in the minimalist

36:16.780 --> 36:21.740
program, syntax is obviously minimalist, it's very small, it's just a linearization and labeling,

36:21.740 --> 36:26.700
they're the two only operations, you have a linearization algorithm to central motor systems

36:26.700 --> 36:33.180
and some kind of categorization algorithm at the conceptual systems. So Chomsky's architecture

36:33.180 --> 36:37.500
is kind of reliant on the process of mapping syntax to semantics, right? It's form meaning

36:37.500 --> 36:43.020
regulation, it's not just structure, and it's not just meaning. So LMS don't really have this mapping

36:43.020 --> 36:47.420
process, right? Like, where's the mapping to semantics? And if there is a mapping, what does

36:47.420 --> 36:52.460
the mapping process look like? What are the properties of its semantics? What are the properties of

36:52.460 --> 36:56.460
the semantics placed on their own sets of constraints on the mapping process? Like,

36:56.460 --> 37:02.220
they do for natural language? Do these kind of constraints inform each other? Is they kind of

37:02.220 --> 37:08.220
a back and forth process? Like, LMS don't really seem to describe this form meaning pairing,

37:08.940 --> 37:16.700
which means which strings, for example, right? Sorry, are you saying that they don't have semantics

37:16.700 --> 37:22.380
at all? Or are you saying that there's just not a clear delineation between how the structures

37:22.380 --> 37:26.860
get mapped onto the semantics? Yeah, the latter, right? So they clearly have some, potentially

37:26.860 --> 37:30.620
some kind of semantics. I know you've argued for conceptual role theory being relevant here, right?

37:30.620 --> 37:34.140
The rest of it is maybe a little bit more mysterious, but the actual, so in linguistics

37:34.460 --> 37:39.020
there's a theory of the mapping process itself, it's explicit, and you can see it in action,

37:39.020 --> 37:42.540
and you can test different theories of it in psycholinguistic models and what have you.

37:42.540 --> 37:48.220
The actual regulation, the kind of constrained ambiguity, ambiguity in the sense of one word,

37:48.220 --> 37:51.980
multiple meanings, or one structure, multiple interpretations, etc, right?

37:53.340 --> 37:58.700
Yeah, I mean, if you think they have semantics, then I think they have to have a mapping from

37:58.700 --> 38:04.620
the syntax to the semantics. I agree, it's not as like, nobody really understands how they're

38:04.620 --> 38:11.820
working on any deep level, right? So I agree, it's not as clear as, say, in generative syntax and

38:11.820 --> 38:19.180
semantics, right, where you kind of write down the rules of composition and can derive a compositional

38:19.180 --> 38:24.620
meaning from a sentence from the component parts or something, right? Like, that's not how they're

38:24.620 --> 38:30.940
working, right? But I just, I wouldn't take for granted that it has to be like that. Like,

38:32.940 --> 38:38.460
it could be that how they're working is actually how we work, right? That everything is represented

38:38.460 --> 38:45.100
in some high-dimensional vector space, and there's some complicated way in which that vector semantics

38:45.100 --> 38:53.500
gets updated with each additional word or whatever in a linguistic stream. But like, I think it's

38:53.500 --> 38:58.540
clear that they have some kind of representation of the semantics of a sentence, right? Like,

38:58.540 --> 39:03.420
they can answer questions, for example, at least approximately. I mean, it's not perfect, but

39:04.380 --> 39:11.260
it's not like a n-gram model or something, right? Which really doesn't have semantics. So I think

39:11.260 --> 39:20.620
that they're definitely representing semantics and, you know, updating that as they process

39:20.620 --> 39:26.140
language, it just happens not to look like these other formal theories. And I guess, I don't see

39:26.140 --> 39:29.900
why that's a problem, right? Like, those other formal theories could just be, you know, poor

39:29.900 --> 39:35.420
approximations or just totally wrong, right? Yeah, yeah, no, no, totally, totally. I mean,

39:35.420 --> 39:39.580
there's also ways in which some of the formal formal theories in semantics are already

39:39.580 --> 39:43.180
potentially compatible. We've got some of these things are doing, right? So another way to think

39:43.180 --> 39:48.940
about this is, you know, LMS are, well, LMS are compression algorithms, but natural language

39:48.940 --> 39:54.460
understanding is kind of more about decompression. It's disambiguating, meaning x, out of meanings,

39:54.460 --> 39:58.860
x, y, z. It's all about making inferences about, you know, meta relations between concepts that

39:58.860 --> 40:04.060
are not in the training data. So some examples that Millie Mitchell gives are things like on top of,

40:04.060 --> 40:09.500
you know, she's on top of a game, it's on top of the box, all of these kind of vary with context.

40:09.500 --> 40:13.180
So there's a lot of other things that are going on, right? And I think you discussed some of the

40:13.260 --> 40:19.500
examples on your paper. So, you know, but the fact that the language is still not, at least,

40:19.500 --> 40:24.860
again, under this theory of language, it's not about string generation. It's about this form,

40:24.860 --> 40:29.260
meaning, pairing machine. So some semantics in the generative tradition, even think all the

40:29.260 --> 40:34.620
rest of semantics is just and, right? So both Rasky's conjunctivist theories semantics is that

40:34.620 --> 40:40.780
human semantics is just and that's it. Which, again, is very simple, elegant. It's, it's,

40:40.780 --> 40:46.300
it's interpretable. It's compatible with other things that, you know, are maybe going on in your

40:46.300 --> 40:50.300
neck of the woods, right? But regardless, it's still, you know, natural language is still more

40:50.300 --> 40:55.660
compositional than things like, you know, formal languages just to make a clear distinction that's

40:55.660 --> 41:00.460
been made. They have a much richer compositional structure. There's more stuff going on, maybe.

41:00.460 --> 41:03.980
So it's important that before that, you know, things like attention-based machine mechanisms

41:03.980 --> 41:09.740
and transformers allow for combinations of discrete token bindings, which is more

41:09.740 --> 41:13.660
approximate to a merge-like operator than simple recurrent matrix multiplication.

41:14.860 --> 41:18.220
But, you know, the issue of binary branching, binary branching of merge, just to choose,

41:18.220 --> 41:22.940
for example, here to talk about the four meaning regulation, one principle. Binary branching in

41:22.940 --> 41:27.580
merge is an interesting question, but Gem2 grammar has always been open to different origins and

41:27.580 --> 41:32.220
locations of this apparent constraint in syntactic computation. Like, where does it come from?

41:32.220 --> 41:36.780
Maybe it's a condition on merge. Maybe it's imposed by a smooth system. Maybe it's a kind of prior,

41:36.780 --> 41:41.900
you know, who knows. And in fact, some more recent work in Gem2 grammar has tried to ground

41:42.860 --> 41:47.500
do away with a lot of the set theoretic assumptions of merge, right? Maybe set theory isn't the best

41:47.500 --> 41:51.980
way to model the Gem2 grammar. Maybe more logical accounts are more appropriate. There's lots of

41:51.980 --> 41:57.500
other recent ideas there, which are all compatible with the, with Chomsky's approach, right? In fact,

41:57.500 --> 42:00.860
one of the things that Chomsky likes the most is when he's, when he's proven wrong, right? A lot of

42:00.860 --> 42:06.620
these theories are going against the core mainstream minimalist architecture. But yeah,

42:06.620 --> 42:12.940
I think it's a very diverse, like, vibrant field. The people who are Adjah, Hornstein, you know,

42:12.940 --> 42:19.340
Petrosky, Haji Borre, they disagree in fundamental ways with a lot of what the mainstream of

42:19.340 --> 42:24.220
Gem2 grammar would say, but there's still more scope for disagreement. But it's still compatible

42:24.220 --> 42:28.460
with setting core assumptions, right? So a lot of David Adjah's work, for example, kind of deviates

42:28.460 --> 42:33.580
in this core respect, but it's still trying to ground these intuitions in different formal systems.

42:34.620 --> 42:41.500
So, you know, it's kind of, I want to get your thoughts again on, I mentioned Mitchell, right?

42:41.500 --> 42:47.340
So Mitchell and Bowers 2020, they have this paper, priorless recurrent networks laying curiously,

42:47.340 --> 42:50.780
but I think you might be aware of, right? So this is a really good example just to kind of get to

42:50.780 --> 42:54.940
the heart of the issue. So recurrent neural networks have been shown to accurately model,

42:54.940 --> 42:58.860
you know, non-veb number agreement, but Mitchell and Bowers show that these networks will also

42:58.860 --> 43:03.420
learn a number agreement with unnatural sentence structures. So structures that are not found

43:03.420 --> 43:07.740
in natural language, and which humans have a hard time processing, right? So the mode of learning

43:07.740 --> 43:14.220
for RNNs is, at least for RNNs, qualitatively distinct from infant, you know, infant homo sapiens,

43:14.220 --> 43:19.180
right? So the story is Mitchell and Bowers show that while their LSTM model has a good representation

43:19.180 --> 43:24.060
of singular variances, plural for individual sentences, there's no generalization going on,

43:24.060 --> 43:27.820
right? They can represent at the individual level. So the model doesn't have a representation of

43:27.820 --> 43:32.940
number as an abstraction. What number is? Only concrete instances of singular versus plural.

43:33.900 --> 43:40.060
So successfully predicting language behavior via LM, or successfully predicting neural responses

43:40.060 --> 43:43.340
in a similar way is obviously great. And maybe we can get into that issue later,

43:43.340 --> 43:47.100
but there's only one side of the coin here, right? The other side of the coin is explaining why this

43:47.100 --> 43:51.100
type of behavior and not some other behavior, why this structure and not some other, and that's

43:51.100 --> 43:58.620
maybe Chomsky's most important point, really, why this and not some other system. So linguistic

43:58.620 --> 44:02.620
theory kind of gives you that other side of the coin, right? Whereas LM's really don't. So the

44:02.620 --> 44:09.100
Mitchell and Bowers paper does something that- He does it! Well, yeah, so like, take Yael LaCrette's

44:09.100 --> 44:13.900
and Stanislas de Haines' work from 2019, right? They looked at number agreement in an LSTM and

44:13.900 --> 44:18.700
found two specialized units that encoded number agreement, but the overall contribution to performance

44:18.700 --> 44:24.540
was low. And then in 2021, Yael LaCrette's have this paper where they show that in the neural

44:24.540 --> 44:30.060
language model, it did not achieve genuine recursive processing of nested long range agreement,

44:30.060 --> 44:35.100
gender marking in Italian, I think, even if some hierarchical processing, you know, was achieved,

44:35.100 --> 44:39.660
as you've argued before, right? Some hierarchy was there, it was there. But the question is,

44:39.660 --> 44:44.140
is it the right mapping? Is it the right kind of hierarchy? They found that LSTM based models could

44:44.140 --> 44:48.780
learn subject-verb agreement over short spans, one degree of embedding, but they failed at some

44:48.780 --> 44:55.660
longer dependencies. And in their most recent paper, LaCrette set out with De Haines showed that

44:55.660 --> 45:01.580
they evaluated modern transformer LM's, including GPT2 XL, on the same task. And the transformers

45:01.580 --> 45:06.220
performed more similarly to humans than LSTMs did and performed above transfer overall, but they

45:06.220 --> 45:10.540
still performed below chance in one key condition, which is the, as I mentioned, the multiple embedding

45:10.540 --> 45:15.020
one, the difficult structures. So the reason why I mentioned these studies is because, you know,

45:15.820 --> 45:20.700
it's not just to explore the limits of LM's, which is an interesting question. But consider work by

45:20.700 --> 45:27.020
people like Neil Smith at UCL, right? He did work in the 90s with a polyglot, savant, and neurotypical

45:27.020 --> 45:31.900
controls comparing them. So he investigated second language learning of an artificial language

45:31.900 --> 45:35.740
containing both natural and unnatural ground structures, like the Michelin Bowers paper,

45:35.740 --> 45:39.500
right? The whole framework is natural versus unnatural. And they found that while both the

45:39.500 --> 45:44.860
savant, Christopher, the savant, and the controls could master the linguistically natural aspects,

45:44.860 --> 45:48.780
only the controls could eventually handle the structure dependent unnatural phenomena,

45:48.780 --> 45:53.100
and neither of them could master the structure independent aspects. So some weird rules where

45:53.100 --> 45:56.540
it's like, you know, you mark the emphasis on the third word of the sentence, things like that.

45:56.540 --> 46:01.660
So they argue that Christopher's abilities are entirely due to his intact linguistic faculties,

46:01.660 --> 46:06.860
but the controls could employ more domain general kind of cognitive resources, like, you know,

46:06.940 --> 46:10.940
tension control, etc., which is why they could deal with those difficult processes.

46:11.580 --> 46:17.340
But I just mentioned, you know, a minute ago, that the LSTM in the Michelin Bowers paper approaches

46:17.340 --> 46:22.140
natural and unnatural structures in pretty much the same way. So it's not, you know, it's not a

46:22.140 --> 46:27.420
psychologically plausible model, I would argue, for whatever humans are doing. And similar observations

46:27.420 --> 46:31.900
can apply to the limits of transformer models in Le Creta's work. And all of these themes are like,

46:31.900 --> 46:35.980
right up there, they're saying that there's all the way to the present. So another one of

46:35.980 --> 46:40.140
Tal Linsen's recent papers that he posted a few weeks ago, looking at child directed speech,

46:40.140 --> 46:45.420
showed that LSTMs and transformers limited to ecologically plausible amounts of data

46:45.420 --> 46:49.180
generalize, as I mentioned, the linear rules for English, right, rather than the abstract rules.

46:49.740 --> 46:55.260
And in fact, more recent work from Linsen's lab last week, looking at, well, last year, I should say,

46:55.260 --> 47:00.700
shows that looking at garden paths, surprise does not explain syntactic disambiguation

47:00.700 --> 47:05.180
difficulty, right? Surprise will underpredicts the size of the garden path effect across all

47:05.180 --> 47:08.540
constructions. And this gets to this issue that you mentioned before, you know, maybe surprise

47:08.540 --> 47:12.380
all this related to some aspects of syntax, but maybe not other ones, it's kind of a,

47:12.380 --> 47:16.220
it's a very nontrivial issue that is very much, it's open to discussion. It's not,

47:16.220 --> 47:20.620
it hasn't settled yet. But so Linsen showed that garden path effects are just way more

47:20.620 --> 47:24.780
difficult than you would expect from mere unpredictability. So another way of phrasing

47:24.780 --> 47:30.860
this argument is to quote a recent argument with Chomsky's to get at this natural basis,

47:30.860 --> 47:34.780
unnatural issue. He says, suppose we have an expanded periodic table that includes

47:34.780 --> 47:39.980
all the elements that do exist, all the elements that can possibly exist, and all the elements

47:39.980 --> 47:45.020
that cannot possibly exist. And let's say you have some model, some artificial model that fails

47:45.020 --> 47:49.500
to distinguish between these three categories, whatever this model is doing, it's not helping

47:49.500 --> 47:53.660
those understand chemistry, right? It's doing something else. It's doing something for sure,

47:53.660 --> 47:57.260
but whether or not it's helping those understand chemistry is something separate. And I know that

47:57.260 --> 48:00.700
you've said in response to some of these studies, I think you've said that, you know,

48:01.660 --> 48:05.180
in order to show that something is likely to be impossible, somewhere in your paper, I think you

48:05.180 --> 48:11.820
say, in order to show that something is impossible with normal bounds and false positives, you'd

48:11.820 --> 48:15.980
need to show, you need to look at something like 500 independently sampled languages. So you cite

48:15.980 --> 48:20.220
this in your paper, right? Which you probably can't do, that's just not, it's not a feasible thing to

48:20.220 --> 48:26.220
do. So, you know, I'm not too sure that this really refutes the principle argument that I'm

48:26.220 --> 48:30.700
making here, right? Because people like Mitchell and Bowers are making an argument about impossibility

48:30.700 --> 48:34.700
in principle, not in some kind of extensional sense, you know, just like searching across the

48:34.700 --> 48:39.420
world languages to see, to prove across every single language that it is impossible, right?

48:39.420 --> 48:43.660
That's kind of, it's a different argument, whether it's impossible in some random language in the

48:43.660 --> 48:48.060
Amazon, compared to actually impossible, based on the principles of what the language system is

48:48.060 --> 48:51.820
actually doing, like what it can do. So I would just say that, you know, all of these kind of

48:52.060 --> 48:58.140
I think that that that point is, is that you don't actually know what is typologically not

48:58.140 --> 49:03.020
possible, right? So people like to say things like, you know, there's no language that does X,

49:03.020 --> 49:09.020
therefore we have to build that restriction into our statistical models. But if it's not

49:09.020 --> 49:13.900
statistically justified that there is no language that does X, right? If you've only looked at

49:13.900 --> 49:17.740
20, 20 European languages or something, right? I mean, it's, it's not

49:18.300 --> 49:25.980
like that shouldn't motivate doing anything to the models, right? If it's, if it's not a

49:25.980 --> 49:33.420
statistically justified universal, I think. Well, you know, I think, you're totally right,

49:33.420 --> 49:36.940
but that just applies more generally to the social sciences and psychological sciences,

49:36.940 --> 49:40.940
right? Like typologically, it's very difficult to establish these things, right? So I guess

49:41.020 --> 49:45.900
you're, you're, I guess you're just kind of steelman you're a bit, you're saying that the strong

49:45.900 --> 49:51.500
claim is very difficult to prove, right? Like the reason the language that has X.

49:52.220 --> 49:56.860
The strong claim that something is not allowed in, in natural languages, I think very, very

49:56.860 --> 50:06.540
difficult to prove. And, you know, I think that there have been lots of, you know, strong attempts,

50:06.620 --> 50:13.020
there's been lots of strong claims from, often from, from generative syntax, right,

50:13.740 --> 50:20.700
about what all languages do. And I think that, you know, people have been very good at finding

50:20.700 --> 50:25.500
kind of counter examples to a lot of those things. I cite this paper by Evans and Levinson,

50:26.540 --> 50:31.420
which actually, you know, I had heard for years about how no language does X and that's what

50:31.420 --> 50:35.660
we're using to construct our theories. And that Evans and Levins paper, Evans and Levinson paper

50:35.660 --> 50:41.340
really kind of changed my mind about this, right? That like language is actually much more

50:42.140 --> 50:48.940
diverse than, than I think most, most syntacticians will, you know, try to construct theories for

50:48.940 --> 50:54.940
something. So, you know, I think we, going back to kind of the beginning of what you said, I think

50:55.180 --> 51:01.020
we, we'd agree that, that you need language architectures which learn the things that kids

51:01.020 --> 51:06.620
learn and learn it from data that they learn. And those architectures might, might be unlikely to

51:06.620 --> 51:12.780
be things like LSTMs or, you know, simple recurrent networks or, or whatever, right? Like, I think

51:12.780 --> 51:17.580
all of that work is, is very useful in, in kind of honing in on the right architecture.

51:18.300 --> 51:25.820
So, I'm just trying to, to remember all of, all of the points you were making. Oh, yeah. So,

51:26.620 --> 51:33.420
but I think this, that there, there's a kind of flip side to this, which is that I think that

51:33.420 --> 51:39.420
the space of things people can learn is actually kind of underestimated, right? Like, there's this

51:39.420 --> 51:46.300
bias to, to, to say, you know, people can't learn X, Y and Z. But people, at least outside of language

51:46.300 --> 51:50.940
have this, this really remarkable ability to learn different kinds of patterns, right? Like,

51:50.940 --> 51:57.980
the patterns you find in, in music or mathematics, for example, we can learn sophisticated types of,

51:57.980 --> 52:04.220
of algorithms, right? We can learn to, you know, fly a space shuttle or to, you know, tie knots in,

52:04.220 --> 52:09.420
for rock climbing or whatever, right? Like, there's all kinds of kind of procedural and

52:09.420 --> 52:14.860
algorithmic knowledge, which is structural that, that people are able to acquire. And I think that,

52:14.860 --> 52:21.740
that that notion very rightly kind of motivates looking for learning systems, which can work

52:21.740 --> 52:29.340
over pretty unrestricted spaces, right? So, you know, you, you, you might say that, okay, well,

52:29.340 --> 52:34.780
language is different because language is a restricted space. And it might be true that,

52:34.780 --> 52:38.380
that language is restricted, but it also might be true that the things we see in language come

52:38.380 --> 52:44.140
from other sources, right? It could be that languages, especially pragmatic, for example,

52:44.140 --> 52:49.180
compared to music or mathematics, right? And those kinds of pragmatic constraints

52:49.980 --> 52:53.900
are the things that constrain the, the form of language, right? Or language is communicative,

52:53.900 --> 52:58.540
it's probably more communicative than, than music, for example. And that might constrain the, the,

52:58.540 --> 53:03.500
the form of things. So, I mean, as, as you know, this is very old debate in, in linguistics about

53:03.500 --> 53:10.620
kind of where the, where the properties of, of natural language come from. And I guess what I'm

53:10.620 --> 53:15.820
trying to say is that there's one kind of perspective where you look at all of the things humans can do

53:15.820 --> 53:21.260
even outside of language, all of the rich structures and algorithms and processes we're able to learn

53:21.260 --> 53:26.460
about and internalize. And you say, okay, maybe language is like that. And then yes, language

53:26.460 --> 53:31.020
also has some of these other funny little properties. But, you know, maybe those come

53:31.020 --> 53:37.020
from some other, other pieces of, of where language comes from, right? It's, you know,

53:37.020 --> 53:43.260
we have pretty sophisticated pragmatic reasoning. We're using it to achieve certain communicative

53:43.260 --> 53:49.580
ends. You can find all kinds of kind of communicative features within the, the language system itself.

53:49.580 --> 53:54.700
And so, so maybe some of these other properties are, are properties that have some other origin.

53:55.580 --> 54:01.340
And that, that view, I think could be wrong, but it's, it's one that I think needs to be looked at

54:01.340 --> 54:11.100
to see if it's wrong, right? Like, I think it's been kind of dismissed by large chunks of, of

54:11.100 --> 54:15.180
linguists, right? Just, you know, I've heard people say stuff like, oh, well, communication

54:15.180 --> 54:19.980
doesn't really explain anything about language, right? And what they mean often is it doesn't

54:19.980 --> 54:24.780
explain like the particular island constraints or something that they're, that they're working on,

54:24.780 --> 54:28.620
right? But there's all kinds of other things in language that communicative pressures probably

54:28.620 --> 54:36.220
do explain. So I guess my, my pitch is always for, for kind of breadth in term, breadth in

54:36.220 --> 54:42.460
consideration of the forces that, that can shape language and not needing to put it all into some

54:42.460 --> 54:45.020
form of, of innate constraints or something like that.

54:45.020 --> 54:48.460
No, no, totally. And I think, I think a lot of that stuff is, is, is compatible with, with, with

54:48.460 --> 54:52.860
the minimalist program, because the minimalist program wants syntax to be minimal. It doesn't

54:52.860 --> 54:56.060
want it to be complicated. It doesn't want it to be, you know, any more complicated than it has to

54:56.060 --> 54:59.740
be. So there were some, you mentioned the curious properties, right? So there were some of the

54:59.740 --> 55:03.980
properties that need to be counted for in any model of language that are, I'll give you one

55:03.980 --> 55:09.020
example, right? The setting of Pearson features. And these Pearson features exhibit very non,

55:09.020 --> 55:13.180
non trivial generalizations that do not seem to be counted for via domain general learning

55:13.260 --> 55:17.500
mechanisms. So I'm citing here the work of Daniel Harbour at Queen Mary. So for example,

55:17.500 --> 55:22.220
the morphological composition of Pearson, its interaction of number, its connection to space,

55:22.860 --> 55:27.100
properties of its semantics and its linearization, they all appear to be strong candidates for our

55:27.100 --> 55:30.460
knowledge of language, right? What we mean by knowledge of language. But on the other hand,

55:30.460 --> 55:35.340
we have things like case and agreement and head movement. And these are all structural phenomena.

55:35.900 --> 55:43.180
However, they seem to resist a purely meaning based explanation in theoretical linguistics,

55:43.180 --> 55:46.940
right? It would be great if syntax were nothing but a computational engine

55:46.940 --> 55:51.260
that builds structured meaning. And that's the minimalist program, the goal. But that's not

55:51.260 --> 55:55.820
what we actually find. That's not in any actual minimalist, like concrete model, any concrete

55:55.820 --> 56:01.100
minimalist theory. The goal is just like, the program is language is perfect. Okay, that's the

56:01.100 --> 56:05.820
program. Is that what we find? No, obviously not. Okay, no, no linguist actually believes that.

56:06.860 --> 56:11.740
So it'd be great if syntax was like that. But I think, you know, the program is to look for

56:11.740 --> 56:16.860
perfection, but not always find it. So case and agreement and head movement are morphological,

56:16.860 --> 56:20.780
morphophonological phenomena, the properties of the performance systems, what's called

56:20.780 --> 56:24.380
performance systems. And so the minimalist program itself is really compatible with a lot

56:24.380 --> 56:28.220
of what you're saying about, you know, language, language, there are aspects of language that

56:28.220 --> 56:34.380
can be perfected and optimized for communicative efficiency. Absolutely. Totally. No doubt about

56:34.380 --> 56:39.820
it. But where is that locus of efficiency? Is it in the syntax itself? Or is it some kind of

56:39.820 --> 56:44.220
extra linguistic system? Is it in pragmatics? You know, is it in century motor? Is it in the

56:44.220 --> 56:49.980
speech? And property of speech and phonology? Probably, you know, I mean, who knows. But

56:49.980 --> 56:56.860
I think all of these things demand much more, you know, serious consideration into old fashioned

56:56.860 --> 57:00.620
notions like structure dependence, compositionality and what have you, things like that, which

57:00.620 --> 57:05.820
you can maybe find somewhere in the literature, but even just basic topics like, you know,

57:06.780 --> 57:13.100
quantifier raising, extended projections, adverbial hierarchies, all of these things

57:13.100 --> 57:18.220
in the minimalist program can be extra linguistic, right? They can actually be outside of syntax and

57:19.340 --> 57:24.140
very queer properties of the semantic conceptual systems, which are in themselves kind of domain

57:24.140 --> 57:29.180
general, weird leftovers from ancient primate cognition, right? The features of the way we

57:29.180 --> 57:33.260
pass events, the way we pass, you know, agents and patients, things like that. That's definitely not,

57:33.260 --> 57:38.380
that's not human specific. But, you know, the way that syntax provides instructions to these

57:38.380 --> 57:44.460
systems, you know, probably seems to be. So, you know, generative linguists have different theories of

57:44.460 --> 57:48.140
also language production too. I'll just talk about language production based on whether we

57:48.140 --> 57:52.220
store lemmas or whether we build words in the exact same way we will phrase and sentence this.

57:52.220 --> 57:55.660
So, I know that you make distinction between construction grammar and kind of generative

57:55.660 --> 58:00.220
grammar and, you know, the weight they place on memorizing constructions versus just building

58:00.220 --> 58:05.180
things from the bottom up, from the ground up, right? And so, you know, in some generative inspired

58:05.180 --> 58:09.900
models, mechanisms which generate syntactic structure make no distinctions between processes

58:09.900 --> 58:15.420
that apply above or below the word level. There's no point at which meaning syntax and form are

58:15.420 --> 58:20.060
all stored together as single atomic representations. Each stage in lexical access is a transition

58:20.060 --> 58:24.540
between different kinds of data structures, right? There's meaning, there's form and there's

58:24.540 --> 58:28.860
syntax. These three features kind of come in together and they don't always overlap. Different

58:28.860 --> 58:35.420
languages realize them in different ways. And so, you know, a word, the basic definition of a word

58:35.420 --> 58:41.340
is just this weird multi-system definition where lots of things, lots of different cognitive systems

58:41.340 --> 58:46.380
enrich the basis of every lexical item, right? You have, there's nothing like this really,

58:46.380 --> 58:51.100
this enrichment process anywhere else in linguistic theory, right? Or at least in

58:51.100 --> 58:58.060
what LLMs are doing. Like, so I guess, what, I guess I would ask you, what is your definition

58:58.060 --> 59:03.740
of a word, right? And what can LLMs really provide insights into weirdhood, right? Because if you

59:03.740 --> 59:07.500
can't, if you don't have a definition of what a word is, then you're really in trouble, right?

59:07.500 --> 59:12.860
Like, we have to at least use LLMs or artificial systems to inform what we mean by a word. Or

59:12.860 --> 59:17.740
maybe we don't need that anymore. I'm not sure what you think. I'm not sure what you mean. I mean,

59:20.700 --> 59:26.620
I don't have a... What is a word? Why does that matter? I mean, that's just a convention about

59:26.620 --> 59:32.300
how we use the term word, right? What, like, I mean, you could use, you know, lemmas or word

59:32.300 --> 59:37.580
firms or whatever. Like, that just feels like a conventional choice. I'm not sure what's at,

59:37.580 --> 59:43.500
what's at stake there. So how would you, I guess I would say, I agree, word is a conventionalization,

59:43.500 --> 59:49.740
you know. Our intuitive concept of word is often biased by orthography, the way we put spaces between

59:49.740 --> 59:54.300
things, right? So I agree with that criticism. You know, word in the intuitive sense is not really

59:54.300 --> 59:59.340
a scientific construct. However, I guess, let me rephrase my question. How would you, you know,

59:59.340 --> 01:00:03.500
decompose the intuitive concept of word into something that is more kind of, you know,

01:00:03.580 --> 01:00:07.420
scientifically amenable or psychologically plausible, which is exactly what genitive

01:00:07.420 --> 01:00:11.340
grammar tries to do by decomposing words into, you know, distinctive features,

01:00:11.900 --> 01:00:16.860
morphological categories, conceptual roots being matched with categorical features,

01:00:16.860 --> 01:00:22.380
you know, you get a concept, you know, and you match it with a noun or a verb category to get a noun

01:00:22.380 --> 01:00:27.740
or a verb. These different models make different predictions, right? Yeah, I mean, I think that

01:00:27.740 --> 01:00:32.540
general idea is likely to be right for large language models. Like, I think they kind of

01:00:32.540 --> 01:00:37.660
must have things that are kind of like part of speech categories, for example. And I think that

01:00:37.660 --> 01:00:45.500
they kind of must be able to update those, their categories based on the language that they've seen

01:00:45.500 --> 01:00:52.300
so far, right? So like, like, you know, GPT puts nouns and verbs in the right places. And to do

01:00:52.300 --> 01:00:57.580
that, you kind of need some representation of the nouns versus the verbs, and you need some ability to

01:00:58.380 --> 01:01:03.260
locate yourself in a string of other words and figure out if there's likely to be a noun or a

01:01:03.260 --> 01:01:10.620
verb next. So I think that on that level, those kinds of properties of words are very likely to

01:01:10.620 --> 01:01:17.420
be right. And there are also things which are very likely to be found kind of in the internal

01:01:17.420 --> 01:01:22.460
representations of these models. I don't see how it could be any other way other than that.

01:01:23.100 --> 01:01:32.140
But like, as far as I know, that's not where the main debates or disagreement, I think, is,

01:01:32.140 --> 01:01:39.580
right? Like, I think all theories of language have to have to say that there's different kinds of

01:01:39.580 --> 01:01:44.780
words that can show up in different places or something like that. Yeah. Okay, so how about

01:01:44.780 --> 01:01:50.620
the issue? You mentioned communication, right? So, you know, and you're totally right, when Trump

01:01:51.580 --> 01:01:55.580
says things like language is a thought system or, you know, language didn't evolve,

01:01:56.380 --> 01:01:59.660
he's kind of being a little bit cheeky. He doesn't really mean that. He kind of means it in a very

01:01:59.660 --> 01:02:04.620
specific sense, right? But, you know, when we say language is a thought system, what we mean is

01:02:05.740 --> 01:02:09.420
we're trying to get it an architectural claim. So if you look at the architecture of the minimalist

01:02:09.420 --> 01:02:15.180
program, the syntactic derivation and the conceptual systems are literally different systems, right?

01:02:15.180 --> 01:02:19.340
The conceptual systems take stuff from syntax and then does its own business with it and the

01:02:19.340 --> 01:02:24.140
CI systems have their own peculiar rules and principles, which is why thought and language

01:02:24.140 --> 01:02:29.500
are both similar symbolic compositional systems, but in different ways. Only a subset of thought

01:02:29.500 --> 01:02:35.580
is properly called the CI interface system, since the CI systems are by definition, you know,

01:02:35.580 --> 01:02:40.460
whatever conceptual systems you would have that can access and read out instructions from syntax.

01:02:40.460 --> 01:02:43.740
And we don't know what they are fully. They seem to have something to do with events and

01:02:43.740 --> 01:02:47.580
grammatical reference and definiteness. They seem to be the main categories that language,

01:02:47.580 --> 01:02:51.980
you know, cares about conceptually, but we don't really know. That's kind of just a hypothesis,

01:02:51.980 --> 01:02:58.060
right? But what we do know is that they don't seem to make use of color all that much. So no

01:02:58.060 --> 01:03:05.580
language morphologically marks shades of color. All the conceptual features like worry or concern,

01:03:05.580 --> 01:03:09.180
like no language morphologically marks a degree of worry or concern about an issue,

01:03:09.180 --> 01:03:15.180
but we do make use of epistemological notions like evidentiality and things like that.

01:03:15.180 --> 01:03:19.740
So, you know, I guess what I'm saying is the minimalist program does a good job of

01:03:20.460 --> 01:03:24.540
trying to figure out which aspects of thought language is intimately tied to,

01:03:24.540 --> 01:03:29.020
and which aspects of thought it's not tied to. So the minimalist program allows us to kind of carve

01:03:29.020 --> 01:03:33.580
that up quite neatly. And this is a much more nuanced framework than, you know, when Chomsky

01:03:33.580 --> 01:03:38.060
says language is thought, again, he doesn't, maybe he means it, maybe he doesn't, but that's not what

01:03:38.060 --> 01:03:43.100
the actual architecture of his theory says. It's a rhetorical device that is very, you know, useful

01:03:43.100 --> 01:03:48.860
and interesting to attract undergraduate audiences. But if you look at actual theories that are coming

01:03:48.860 --> 01:03:53.020
out of the minimalist program, no one really believes language equals thought, right? The language

01:03:53.020 --> 01:03:57.900
system seems to, it tries its best to access and reformat and manipulate various conceptual

01:03:57.900 --> 01:04:02.220
systems, but it has its limits, right? We know what systems, spell keys, core knowledge systems are

01:04:02.220 --> 01:04:08.860
hooked up to with respect to the syntax engine, and which ones are not. So, you know, this kind of

01:04:08.860 --> 01:04:13.820
gets back to the idea that lexicalization of a concept seems to maybe alter it in some way.

01:04:13.820 --> 01:04:18.300
It kind of imbues it with elements that are not there in the concept itself. So if you lexicalize

01:04:18.300 --> 01:04:22.220
the concept, you suddenly transform it a little bit, you give it a little extra, you sprinkle

01:04:22.220 --> 01:04:27.100
something else on top of it, and that seems to vary across different noun types. But these are all

01:04:27.100 --> 01:04:34.700
like very clear architectural claims within gem diagram that make very clear empirical predictions.

01:04:34.700 --> 01:04:38.860
So in other words, I guess what I'm saying is all these neuropsychology studies that are

01:04:38.860 --> 01:04:44.540
coincided, you know, in a lot of work in this fame, what does it really show? I think it shows

01:04:44.540 --> 01:04:49.900
that, you know, when language is damaged in the brain, it loses its particular sway or mode of

01:04:49.900 --> 01:04:54.860
influencing those systems. But there's no real prediction from within the gem to grammar enterprise

01:04:54.860 --> 01:04:59.020
that those non-linguistic systems should be impaired or should suddenly, you know, shut down

01:04:59.020 --> 01:05:03.420
if the core language system is compromised, right? In fact, if anything, that just

01:05:04.140 --> 01:05:09.580
emphasizes the principal divorce between the syntactic system and non-linguistic systems,

01:05:09.580 --> 01:05:13.420
right? So I think the, a lot of predictions here from the language and communication,

01:05:14.140 --> 01:05:17.340
you know, literature are kind of missing the point of the architectural claims.

01:05:19.340 --> 01:05:21.660
I can just give, or Daniel, do you want to go?

01:05:22.220 --> 01:05:22.860
Yeah, go ahead.

01:05:22.860 --> 01:05:27.740
Give a little bit of background there. So there's these papers from

01:05:28.780 --> 01:05:38.220
Ev Fedorenko and Rosemary Varley that are examining in part of them aphasic patients. So

01:05:38.220 --> 01:05:45.260
people who have impaired linguistic abilities, basically showing that with impaired linguistic

01:05:45.260 --> 01:05:53.020
abilities, you can still have preserved kind of reasoning abilities. So people like chess masters,

01:05:53.020 --> 01:06:00.700
chess grandmasters, for example, who are obviously very good at reasoning might not have kind of

01:06:00.700 --> 01:06:06.220
intact linguistic abilities. And then complimenting that kind of patient work, there's also work from

01:06:06.220 --> 01:06:15.660
Ed's lab showing that the parts of the brain that care about language are separable from

01:06:15.660 --> 01:06:20.060
the parts of the brain that care about other domains, even ones that seem kind of language-like.

01:06:20.060 --> 01:06:26.460
So things like music and mathematics tend not to happen in the language areas.

01:06:27.180 --> 01:06:34.220
So Ev and others have argued that this is basically evidence against the Chomsky

01:06:34.220 --> 01:06:40.940
inclaim that language is the medium for thinking, because there's thinking that can happen

01:06:40.940 --> 01:06:45.100
in the absence of language and the brain areas that care about language seem not to be the brain

01:06:45.100 --> 01:06:52.060
areas that care about thinking. I guess, Elliot, you're saying that people don't really believe that.

01:06:54.940 --> 01:06:57.020
They don't believe that distinction, I mean.

01:06:57.740 --> 01:07:05.340
And also, there's a lot of self-contradiction even within these arguments, right? So in your paper,

01:07:05.340 --> 01:07:09.180
you sometimes say that Chomsky thinks that language is a thought system, but then a few

01:07:09.180 --> 01:07:13.660
pages later, you'll say Chomsky also believes that syntax is some totally separate system from

01:07:13.660 --> 01:07:18.780
anything else, right? Your autonomy of syntax, etc. So does Chomsky think-

01:07:18.780 --> 01:07:21.340
That's not my contradiction. I mean, he said both of those things.

01:07:21.980 --> 01:07:26.860
Right, exactly. So therefore, you may want to ask yourself, does he really believe these things?

01:07:27.500 --> 01:07:31.260
Or what is the case if it arises from the architecture, right?

01:07:31.260 --> 01:07:35.660
So just saying language is a thought system, what does that mean? That doesn't mean anything.

01:07:35.660 --> 01:07:40.700
It's just a very vague statement. The question is how exactly is language contributing to thought

01:07:40.700 --> 01:07:41.900
and how is it not contributing?

01:07:43.980 --> 01:07:50.220
Yeah, I mean, I think his claim is mainly evolutionary or something, right, that this is the

01:07:50.220 --> 01:07:56.620
origins of the system, which I think is sort of equally hard to square with the kind of

01:07:56.620 --> 01:08:05.500
patient and neuroimaging data. But if he doesn't think that, then he shouldn't say it,

01:08:06.540 --> 01:08:08.460
or people will respond to what he said, I think.

01:08:10.220 --> 01:08:14.620
The argument is that language is a kind of thought system. It regulates some aspects of

01:08:14.620 --> 01:08:18.700
thought and it yields some aspects of thought that are clearly unique to humans,

01:08:18.700 --> 01:08:23.980
but it's not intrinsically or causally tied to it. The architecture of the system is very different

01:08:23.980 --> 01:08:28.460
from the kind of generalizations you can rhetorically evince from the architecture.

01:08:28.460 --> 01:08:32.620
So for instance, when you cite work from a phasic patient showing no deficits in complex

01:08:32.620 --> 01:08:36.460
reasoning, as you just mentioned, playing chess and so on, we would actually expect this under a

01:08:36.460 --> 01:08:41.740
kind of non-lexicalist framework of generative syntax, where meaning, as I said, meaning syntax

01:08:41.740 --> 01:08:46.700
and form, form just meaning anything that you can externalize language in, all these things

01:08:46.700 --> 01:08:51.100
are separate features and separate systems. The autonomy of syntax doesn't mean,

01:08:52.700 --> 01:08:56.380
what a lot of people think it means, it just means that there are certain syntactic operations

01:08:56.380 --> 01:09:00.540
that are not semantic. There are certain things you can do with syntax that you can only do with

01:09:00.540 --> 01:09:04.060
syntax and you can't do with semantics. So this gets back to the difference between

01:09:05.100 --> 01:09:11.180
Petrowski's theory that semantics is just and versus a lot of syntacticians' belief that

01:09:11.180 --> 01:09:15.340
there are certain peculiar weird things you can do with syntax that are just syntactic.

01:09:15.340 --> 01:09:21.340
So there is a divorce even within the kind of architectural framework. So it's not too surprising

01:09:21.340 --> 01:09:25.020
that you also find that divorce at the neuropsychological level, I would say.

01:09:26.060 --> 01:09:33.260
Well, I think I would want a prediction of the language's thought evolutionary idea then.

01:09:36.380 --> 01:09:43.980
If you're saying that doesn't predict that thought relies on language, then I think whoever

01:09:43.980 --> 01:09:50.380
likes that theory should come up with some predictions about what that theory actually

01:09:50.380 --> 01:09:55.500
means. I feel like those kinds of predictions are often really necessary for understanding the

01:09:55.500 --> 01:10:01.180
content of a prediction. So sorry, Daniel, your hand's been up for a while.

01:10:02.140 --> 01:10:14.300
No, it's all good. Just kind of wanted to bring a breath in and an opportunity for anyone to ask

01:10:14.300 --> 01:10:23.180
any other questions. But wow, thank you both for the many topics we've covered. We'll have,

01:10:23.180 --> 01:10:27.660
in the last minutes, a kind of conclusion in next steps. But Dave, would you like to

01:10:28.220 --> 01:10:30.860
ask a question or just give a short reflection?

01:10:37.020 --> 01:10:44.700
Okay, no. There are many comments in the chat, so I hope that both of you can read them

01:10:44.700 --> 01:10:51.980
on your own time to see what everyone added. Where do we go from here? As we

01:10:52.940 --> 01:11:01.740
roar into May 2023 and beyond, what can linguists, large language model developers

01:11:01.740 --> 01:11:06.940
and users, cognitive scientists, what do you each think are some of the most fruitful pathways

01:11:06.940 --> 01:11:16.140
forward? Well, I would say the most fruitful pathway forward is to really take cognitive

01:11:16.140 --> 01:11:22.620
psychology seriously. There's a lot of nice work recently trying to align things like chat

01:11:22.620 --> 01:11:27.180
GPT, Wolfram Alpha plugins, the way that chat GPT can interface with different kind of modules.

01:11:27.980 --> 01:11:34.300
The way of building a legitimate kind of AGI system doesn't necessarily have to be psychologically

01:11:34.300 --> 01:11:37.980
reliant on the kind of modules that human beings have, but I think it will benefit from it. So

01:11:37.980 --> 01:11:45.100
there have been some claims that large language models can maybe do all sorts of things. But I

01:11:45.100 --> 01:11:48.620
think in the long run, it's most likely going to be the case that LLMs can do something very

01:11:48.620 --> 01:11:52.780
important and very interesting, but it's only going to be one piece of the puzzle. So in fact,

01:11:52.780 --> 01:11:58.780
even OpenAI CEO Sam Altman said last week that what we can do with LLMs has really kind of been

01:11:58.780 --> 01:12:05.340
exhausted. We need new directions, new avenues and so on. I guess he was probably speaking to

01:12:05.340 --> 01:12:11.260
investors more than linguistic students here, but I think he's also right. LLMs can do something

01:12:11.260 --> 01:12:16.860
spectacular, but they're probably going to form a small part of the general AGI architecture,

01:12:16.860 --> 01:12:23.100
right? If you want to think about AGI as a potential, potential goal here. So, you know,

01:12:23.900 --> 01:12:31.340
I think a lot of the, so let me give me an example here. So Anna Ivanova, who's a very good

01:12:31.340 --> 01:12:35.980
cognitive scientist, she has a paper recently arguing for a kind of modular architecture for

01:12:35.980 --> 01:12:40.700
LLMs, which is a very nice framework, right? It's very cognitively plausible. It's exactly the

01:12:40.700 --> 01:12:44.380
kind of thing that we should be pushing for. It's compatible with Howard Gardner's, you know,

01:12:44.380 --> 01:12:48.620
notion of multiple intelligences and so on. But I think at the same time, just to finish this

01:12:48.620 --> 01:12:55.180
comment, there was a tech talk last week, I think, or maybe a few days ago, where a lot of this stuff

01:12:55.180 --> 01:13:01.740
can be conflated with AI hype in an unproductive way. So Greg Brockman from OpenAI, he gave one of

01:13:01.740 --> 01:13:06.700
his, one of these big TED talks where he showed different plugins that chat GPD can do. I mentioned

01:13:06.700 --> 01:13:11.580
Wolfram Alpert, right? But there's also things like image generation, Instacart shopping, where you

01:13:11.580 --> 01:13:17.180
can get chat GPD to buy you things and what have you. And again, this takes you back to the idea

01:13:17.180 --> 01:13:21.660
that multiple subsystems can do different sub-functions. So Brockman also showed an example

01:13:21.660 --> 01:13:28.540
of giving chat GPD an Excel file, a CSV file from an archive database of academic papers,

01:13:28.540 --> 01:13:33.900
where it just listed a bunch of papers and then titles and what have you, right? And he said that,

01:13:34.140 --> 01:13:40.060
using chat GPD, it uses world knowledge to infer what the titles of the columns mean. So we understood

01:13:40.060 --> 01:13:45.100
that title means the title of the paper. It understood that authors mean the number of authors

01:13:45.100 --> 01:13:50.220
per paper. It understood that created means the date the paper was submitted, right? And because

01:13:50.220 --> 01:13:55.500
it's a TED talk, the audience gave us a standing elevation, right? But the ability to describe

01:13:55.500 --> 01:14:02.780
labels on an Excel file is, I guess, nice. But I'm not sure you'd really call it world knowledge.

01:14:02.780 --> 01:14:07.980
So I guess, I would just say there's a lot of progress needs to be made alongside reducing

01:14:07.980 --> 01:14:12.300
anthropomorphism. You have to have the right balance of it. So like I said, you have to have

01:14:12.300 --> 01:14:17.260
the right balance of psychologically plausible kind of modular architecture, but you can't have too

01:14:17.260 --> 01:14:21.820
much anthropomorphism because then you'll get carried away. You have to find, we have to find

01:14:21.820 --> 01:14:27.100
the right balance between modeling kind of human-like modular systems, but not doing it

01:14:27.100 --> 01:14:31.580
to a degree that is a bit implausible or scientifically unhelpful.

01:14:35.580 --> 01:14:38.940
I mean, I think I agree with all of that. I'm really excited about these

01:14:39.660 --> 01:14:46.380
ways of kind of connecting language models to other forms of information processing,

01:14:46.380 --> 01:14:51.500
which does seem like what people have. I think I've been very surprised at the

01:14:52.460 --> 01:14:59.980
the things they are able to do just as language modeling, right? So different kinds of reasoning

01:14:59.980 --> 01:15:07.180
puzzles and things that they can solve, I think, is really fascinating and maybe will require us

01:15:07.180 --> 01:15:13.740
to rethink the relationships between language and thought and try to figure out a way of being

01:15:13.740 --> 01:15:19.500
specific about what it means for something to have a representation or to reason over that

01:15:19.500 --> 01:15:27.180
representation. But ultimately, I think I agree that people have different modes of thinking about

01:15:27.180 --> 01:15:36.060
things and that seems important for intelligence. I'm also super excited about the BabyLM challenge.

01:15:36.060 --> 01:15:43.660
So I think on the kind of linguistic side, that's exactly the right thing of seeing how far we can

01:15:43.660 --> 01:15:53.660
get with smaller data sets and maybe eventually after that trying to understand some more about

01:15:53.660 --> 01:15:59.820
the kinds of semantics that kids acquire and where they get it from and how kind of external semantics

01:15:59.820 --> 01:16:08.060
can inform language learning or specifically maybe grammar and syntax learning. I guess my other

01:16:08.940 --> 01:16:17.180
path forward point would be that there's... I feel like these kinds of models have

01:16:17.180 --> 01:16:24.940
have really gone far beyond people's expectations for this kind of class of model, right? Kind of

01:16:24.940 --> 01:16:33.100
ground up statistical learning, discovering patterns in text seems to give really pretty

01:16:33.100 --> 01:16:39.260
remarkable results. And that for me going forward, I think has just introduced a huge wave of

01:16:39.260 --> 01:16:45.740
uncertainty over theories. So I think that our theories of basically everything in language for

01:16:45.740 --> 01:16:52.460
sure, but cognition, probably neuroscience, like all of those things I think are going to be reworked

01:16:52.460 --> 01:16:59.660
when we really come to kind of understand the ability of really general kinds of learning

01:16:59.660 --> 01:17:08.380
systems like these. So that makes it on the one hand kind of a bummer for past theories,

01:17:08.380 --> 01:17:16.620
especially theories which relied on learning not being able to work well. But on the upside,

01:17:16.620 --> 01:17:23.260
I think it makes it a very exciting time both for AI and cognitive science and linguistics,

01:17:23.980 --> 01:17:29.500
where now there's these really, really powerful tools that seem like a qualitatively

01:17:30.540 --> 01:17:36.940
different size step towards human abilities. And I think kind of integrating them and taking

01:17:37.820 --> 01:17:43.820
both the kind of engineering lessons and the kind of philosophical lessons about how they're made

01:17:43.820 --> 01:17:49.260
and what kinds of principles go into designing intelligent systems. I think that those things

01:17:49.260 --> 01:17:56.780
will really shape the field over the next five or 10 years. And also, I would just say in the

01:17:56.780 --> 01:18:00.940
context of broader themes here, right, like you're totally right, like I remember when I was reading

01:18:00.940 --> 01:18:07.820
about when Deep Blue, the Kasparov, was it, the chess thing, right? And there were some commentators

01:18:07.820 --> 01:18:13.260
who said, you know, chess is over. If an AI can beat a human, then it's game over. What's the

01:18:13.260 --> 01:18:18.860
point in studying chess? You know, there's no need of boring anymore. And I guess if AI has achieved

01:18:18.860 --> 01:18:22.380
seemingly everything that humans need to do to play chess, what's the point of playing it?

01:18:23.180 --> 01:18:26.620
But I think, you know, if anything, it turned out to increase the popularity of chess, right?

01:18:26.620 --> 01:18:31.260
There are now many chess celebrities as well, worldwide tournaments. And I would predict that

01:18:31.260 --> 01:18:34.860
the same is probably going to happen with language too. You know, LLMs do not mean

01:18:34.860 --> 01:18:38.460
it's the end of language, no more language, no more linguistics. I would actually push back

01:18:38.460 --> 01:18:43.340
and say maybe it would be the opposite. You know, the success of LLMs will increase general

01:18:43.340 --> 01:18:47.900
interest in linguistic theory, due to their apparent, you know, weird constraints and apparent

01:18:47.900 --> 01:18:53.100
limitations, right? Because I would also say, you know, scale, at this point, the chess issue,

01:18:53.740 --> 01:18:59.420
scale is kind of definitely far from all that's needed. What is lacking is an ability of LLMs to,

01:18:59.420 --> 01:19:03.580
you know, really abstract their knowledge and experiences in order to make robust predictions

01:19:03.580 --> 01:19:07.340
and generalizations and so on. I gave some examples, but there's some others in the literature

01:19:07.340 --> 01:19:11.100
where it doesn't seem to really be good at generalizing. It can kind of mimic particular

01:19:11.100 --> 01:19:16.300
token types. But I would, you know, I would guess my final, my final thing would be that,

01:19:16.300 --> 01:19:21.820
you know, the language acquisition literature doesn't necessarily need LLMs though. You know,

01:19:21.820 --> 01:19:26.460
cognitive scientists don't really need LLMs. We could potentially, you know,

01:19:26.460 --> 01:19:30.460
me and Stephen obviously disagree here, but I would say big tech companies

01:19:30.460 --> 01:19:34.620
profiting off LLMs need LLMs, right? They're the only ones that really do. It may be the case

01:19:34.620 --> 01:19:39.180
that the mind is a very, I will say, you know, the mind is a very diverse space. It may be that

01:19:39.180 --> 01:19:43.260
there are certain forms of behavior and learning that might be captured by processes similar to

01:19:43.260 --> 01:19:46.540
what LLMs are doing. So Stephen has given some interesting examples in his papers about

01:19:46.540 --> 01:19:51.260
magnetism and we're kind of rules of learning that are very, very general and very quick and

01:19:51.260 --> 01:19:55.900
very mysterious. So, you know, maybe for those sorts of things, that kind of learning will be

01:19:55.900 --> 01:20:00.300
relevant. But I still think it's unlikely that one of the candidates will be natural language,

01:20:01.420 --> 01:20:05.100
at least the way natural language works in its full glory in terms of the four meaning

01:20:05.100 --> 01:20:09.740
regulation and what have you. So I guess I would, you know, it kind of reminds me of where you,

01:20:10.540 --> 01:20:14.540
you know, you have this image of, I saw John with chapter four recently, right? And he has this,

01:20:14.540 --> 01:20:17.580
there's this scene where he's walking in the desert and he's not sure if he's seen

01:20:17.580 --> 01:20:21.740
this guy that he wants to assassinate. It's kind of like when you walk in the desert

01:20:22.700 --> 01:20:27.100
and you have an illusion of seeing an oasis because it turns out you're hallucinating.

01:20:27.100 --> 01:20:30.860
But then you realize that, you know, sometimes before it's too late that you actually are

01:20:30.860 --> 01:20:34.460
hallucinating. It's you're not seeing an oasis. You're still in the desert. And I think that's

01:20:34.460 --> 01:20:39.580
kind of maybe the situation we're in right now with linguistic competence of laws of language

01:20:39.580 --> 01:20:45.500
models. We have the illusion of linguistic competence. But, you know, you always see the

01:20:45.500 --> 01:20:49.660
illusion before you find the oasis, right? So I think, I think right now we're in the

01:20:49.660 --> 01:20:54.540
hallucinating stage of the desert where we're seeing potential sparks of linguistic competence,

01:20:54.540 --> 01:20:59.660
but it's still not very clear and I'm robust. And we haven't actually reached the oasis yet.

01:21:00.620 --> 01:21:08.700
Um, just a rapid fire question. So see if you can give a short response. So

01:21:09.660 --> 01:21:15.580
Sphinode, you know, writes question, is it correct to say that large language models have no priors?

01:21:18.620 --> 01:21:22.700
Do large language models have priors? I'd say yes, they definitely do.

01:21:23.100 --> 01:21:30.540
Um, and there, I think the difference to how people, you know, are used to thinking about

01:21:30.540 --> 01:21:35.420
priors in Bayesian inference, for example, if you like write down a Bayesian statistical model,

01:21:35.420 --> 01:21:39.740
you say like, you know, here's the parameters and here's what the priors are on the parameters.

01:21:40.780 --> 01:21:45.020
Large language models, I think the priors are and maybe neural nets in general, I think that the

01:21:45.020 --> 01:21:49.980
that the priors are much more implicit, right? So there's some functions which they find easier

01:21:49.980 --> 01:21:55.500
to learn than other functions. And there's even some work trying to discover, you know,

01:21:55.500 --> 01:22:01.020
some statement of what those kind of implicit priors are. But that's actually how I think about,

01:22:03.100 --> 01:22:06.460
you know, comparison of different neural network architectures, right,

01:22:07.580 --> 01:22:12.060
which is maybe something Elliot and I might agree on, right? Like you have to find priors which

01:22:12.060 --> 01:22:18.220
allow them to learn the things that kids learn, right? And not all architectures will do that.

01:22:18.860 --> 01:22:23.900
Even among architectures which are turn complete or capable of learning any kind of function,

01:22:23.900 --> 01:22:31.500
not all of them will do it, even on kind of huge data set sizes. So I think of this sort of search

01:22:31.500 --> 01:22:38.060
over neural net architectures as really one of a search over priors. But it's not priors or,

01:22:38.060 --> 01:22:41.740
I mean, you could think of it as a search over universal grammar or something, right? But it's,

01:22:41.740 --> 01:22:47.660
it's, it's not priors or universal grammar in the sense that people have talked about it as like

01:22:47.660 --> 01:22:51.980
an explicit statement about what kinds of rules are allowed or an explicit statement about what

01:22:51.980 --> 01:22:56.540
kinds of functions or high probability or something like that. It's all implicitly coded there.

01:22:57.260 --> 01:23:01.660
Yeah, totally. I think, I think that's right. I mean, you know, the real question is reducing

01:23:01.660 --> 01:23:06.860
the space of what those priors are like. And if it's anything remotely like what human beings

01:23:06.860 --> 01:23:11.660
are doing, so LLMs like, I would, I would at least say that things like GPT-3 are an existence

01:23:11.660 --> 01:23:17.260
proof of, you know, that building fully functioning syntactic categories from surface

01:23:17.260 --> 01:23:23.500
distributional analysis alone is possible. That's, yes, that is correct. But, you know,

01:23:23.500 --> 01:23:29.900
even so, I would say most syntacticians don't really believe that syntactic categories are innate.

01:23:29.900 --> 01:23:34.780
So the prior issue is slightly less relevant here. It's the operations that are said to be innate.

01:23:34.780 --> 01:23:39.900
So the, in the syntax domain, it's particular linguistic computations that are said to be innate

01:23:39.900 --> 01:23:43.980
and categories themselves. In fact, even Charles Young has admitted in the last couple of years

01:23:43.980 --> 01:23:50.060
that they are maybe innate, but maybe not. So people have given, I know of a relevant prize,

01:23:50.060 --> 01:23:54.460
they are things like, you know, me and Gary Marcus have talked about compositionality.

01:23:54.460 --> 01:23:59.340
That seems to be a big problem. So people have given chat GPT BBC news articles asking it to

01:23:59.340 --> 01:24:05.740
compress it and then re-explain it. So one example I saw was Peter Smith 58 is being arrested on

01:24:05.740 --> 01:24:11.580
charges of manslaughter and you get it to compress it and re-explain it. And it comes out as 58 people

01:24:11.580 --> 01:24:14.860
are being charged with manslaughter. All right. That's a pretty clear example of a lack of

01:24:14.860 --> 01:24:19.100
compositionality being built into whatever compression it's doing. And there's no example

01:24:19.100 --> 01:24:23.580
where there's been, there's some examples of potential analogical reasoning. So in Bing chat,

01:24:23.580 --> 01:24:28.700
you know, Bing has this, this chat function. The question is, is it just finding meta relations

01:24:28.700 --> 01:24:33.180
that have already been documented by humans or is it genuinely creating new relations that the new

01:24:33.180 --> 01:24:40.380
stuff that is being built. So, you know, someone asked through me a table comparing Jesus Christ

01:24:41.020 --> 01:24:47.340
with the Nokia 9910, right, the cell phone Nokia 9910. And it said, you know, it compared the

01:24:47.340 --> 01:24:54.540
release dates. It compared the size, the weight. It compared the CPU with Jesus's all-powerful

01:24:54.540 --> 01:24:59.580
knowledge. It compared the memory of the phone with the all-knowing nature of God, right.

01:24:59.900 --> 01:25:04.940
And it also, I think it said that they were both resurrected because the Nokia was re-released a

01:25:04.940 --> 01:25:09.020
couple of times, right. So the Nokia. That sounds like a great answer. What's wrong with that?

01:25:09.020 --> 01:25:14.620
Okay. It may be. It sounds a lot like analogical reasoning, but then it also had some quite weird

01:25:14.620 --> 01:25:19.100
ones where it was like, you know, for the camera, it said, no, it just gave Jesus's description,

01:25:19.100 --> 01:25:23.980
but it's not really what a camera is. There's some kind of things that look like analogical

01:25:23.980 --> 01:25:32.060
reasoning, maybe, but it's unclear. Yeah. I think that sounds like an awesome answer to me.

01:25:34.620 --> 01:25:39.100
I was going to say, like, you said large-language models learn they're an existence proof of part

01:25:39.100 --> 01:25:43.580
of speech categories, but like, they don't just output part of speech categories, right. Like,

01:25:43.580 --> 01:25:51.500
they have a lot of grammatical syntactic knowledge. And moreover, like, they have a lot of semantic

01:25:51.500 --> 01:25:57.260
knowledge and probably some pragmatic knowledge. And, you know, they're not bad at translation.

01:25:57.260 --> 01:26:03.020
And like, it's way more that they have discovered than just part of speech categories.

01:26:04.860 --> 01:26:08.300
Well, sorry, I said syntactic. I'm sorry. It's like syntactic categories.

01:26:09.100 --> 01:26:12.380
Right. Well, sorry. Yeah. Yeah. But they've discovered way more than that.

01:26:12.940 --> 01:26:21.980
Yeah. I'm going to, as a teaser slash motivator for hopefully both of you to join again in the

01:26:21.980 --> 01:26:27.660
future with or without other guests, a few of the exciting questions just for us to include in this

01:26:27.660 --> 01:26:31.260
transcript. And then thank you both, Elliott and Stephen, for joining. So just a few of the last

01:26:31.260 --> 01:26:35.900
questions that were asked, Juan asked, how do small transformers, Zhang et al. 2020,

01:26:35.900 --> 01:26:41.260
compared with children learning language? 96 asked, what are your thoughts on implicit

01:26:41.260 --> 01:26:47.420
priors versus animal instinct? Rojda asked, what constraints that space in LLMs, don't

01:26:47.420 --> 01:26:52.300
they get there by training? So are they discovering it? That's not what they implement at the start

01:26:52.300 --> 01:26:59.180
maybe. And there's many more questions. So I hope that we can all review and reread each other's

01:26:59.180 --> 01:27:05.500
works and come together for 41.2 in some future time. Thank you, Elliott and Stephen,

01:27:05.500 --> 01:27:09.980
for this excellent stream. Thank you, Dave. Thank you both. Yeah. Thank you so much.

01:27:09.980 --> 01:27:16.860
Very well. Bye. See you.

