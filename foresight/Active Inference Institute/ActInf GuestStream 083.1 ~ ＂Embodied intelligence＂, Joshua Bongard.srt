1
00:00:00,000 --> 00:00:17,120
Hello and welcome. It is July 17th, 2024. We're in active inference guest stream 83.1 with

2
00:00:17,120 --> 00:00:23,360
Josh Bongard. Thank you for joining. This should be very exciting. We'll have a presentation

3
00:00:23,360 --> 00:00:28,160
and then some discussion. So if you're in the live chat, please feel free to write any

4
00:00:28,160 --> 00:00:34,000
questions. And thank you again, Josh, looking forward to this. Yeah, thank you, Daniel. And thanks

5
00:00:34,000 --> 00:00:39,280
to those of you that are attending online. So my name is Josh Bongard. I'm a professor of computer

6
00:00:39,280 --> 00:00:45,760
science here at the University of Vermont. And my bread and butter in my lab is the study of

7
00:00:45,760 --> 00:00:54,240
robotics and AI. And obviously, we're in the middle of the current AI summer. So what I wanted to do

8
00:00:54,240 --> 00:00:58,640
today is show a couple of highlights from my group, things that I've worked on in the past and

9
00:00:58,640 --> 00:01:04,240
that we're working on at the moment, that I hope in the long term will help us realize sort of the

10
00:01:04,240 --> 00:01:09,360
long term vision for a lot of those trying to create intelligent machines, which is to create

11
00:01:09,360 --> 00:01:15,920
machines that are helpful, but but also safe. We're part of the way there. But as anyone who's

12
00:01:15,920 --> 00:01:22,240
worked with chat GPT or stable diffusion knows or even has a robot vacuum cleaner at home,

13
00:01:22,240 --> 00:01:27,280
there are some limitations to our current technology. It's hard to create machines that are

14
00:01:27,280 --> 00:01:33,920
autonomous and useful and safe all simultaneously. So what are the things that we're missing?

15
00:01:34,640 --> 00:01:39,840
That's what I wanted to sort of seed the pool with today and hopefully we can move on to an

16
00:01:39,840 --> 00:01:45,360
interactive discussion about it. So I'm going to leave this slide up and just sort of talk over

17
00:01:45,360 --> 00:01:49,520
this slide for a few minutes that will hopefully generate some food for thought and then questions.

18
00:01:49,760 --> 00:01:55,840
This is a snapshot from some of the projects I've worked on over the years. First thing you'll

19
00:01:55,840 --> 00:02:00,560
probably notice is there's a lot of different robots that have very different structural

20
00:02:00,560 --> 00:02:06,720
properties. They not only act differently, but look very different. And that is a fundamental

21
00:02:06,720 --> 00:02:13,040
foundation in everything we do in my research group, which is to try and understand how the

22
00:02:13,280 --> 00:02:21,760
body facilitates cognition. Years ago with my PhD advisor, we wrote a popular science book called

23
00:02:21,760 --> 00:02:28,640
How the Body Shapes the Way We Think. And we can mean that literally or figuratively. We wrote that

24
00:02:28,640 --> 00:02:33,680
book a while ago. We made some arguments about how the body shapes the way we think. And since that

25
00:02:33,680 --> 00:02:40,320
time, my group and others have formulated other arguments for why or how the body shapes the

26
00:02:40,320 --> 00:02:47,200
way we think. And I'm hoping to survey some of those today. So as I mentioned, you can see

27
00:02:47,200 --> 00:02:52,000
a lot of different robots here, very different structure. They've got very different form and

28
00:02:52,000 --> 00:02:58,560
function. But across each of the pair of videos that you see here, you'll notice that there's also

29
00:02:58,560 --> 00:03:03,840
a common pattern, which is on the left side, you tend to see something that's virtual. And on the

30
00:03:03,840 --> 00:03:09,520
right side of the video pair, you tend to see something that's physical. And this illustrates

31
00:03:09,520 --> 00:03:15,200
the basic approach that my group takes to understanding how the body shapes the way we think,

32
00:03:16,000 --> 00:03:24,240
which is to create AI that creates robots, creates embodied AI. So what do I mean by that?

33
00:03:24,960 --> 00:03:31,120
What I mean is that in all of the projects that you see here, we create an AI that searches the

34
00:03:31,120 --> 00:03:37,840
space of all possible robots that could solve the tasks that we want the robots to solve.

35
00:03:38,800 --> 00:03:44,240
Most people that are familiar with AI and robotics and autonomous cars and drones have

36
00:03:44,240 --> 00:03:51,520
a rough understanding that AI is somehow optimizing or tuning the brain of the autonomous car or the

37
00:03:51,520 --> 00:03:57,280
drone or the robot, what have you. There's an underlying assumption in all of that current,

38
00:03:57,280 --> 00:04:03,440
in most current robotics, which is that the AI tunes the brain, but does not tune the body.

39
00:04:03,440 --> 00:04:11,200
Tesla cars are dreamed up mostly by humans and an AI tunes their brain or their control policy.

40
00:04:11,760 --> 00:04:16,160
But of course, nature doesn't work that way. Nature produced us and all the other intelligent

41
00:04:16,160 --> 00:04:23,920
organisms on this planet by carefully tuning body and brain simultaneously. Certain bodies

42
00:04:23,920 --> 00:04:28,640
facilitate certain kinds of behaviors and certain kinds of intelligent behavior,

43
00:04:28,640 --> 00:04:33,760
and other bodies don't. They obstruct that particular behavior or that intelligent behavior.

44
00:04:34,560 --> 00:04:41,360
So in all of our work, we ask the AI not just to tune the brain of a robot, but its body

45
00:04:41,360 --> 00:04:48,960
simultaneously. And as you can see visually here, the AI often comes up with bodies that are well

46
00:04:48,960 --> 00:04:55,200
suited to whatever we want them to do. So if you direct your attention to the very top left of

47
00:04:55,200 --> 00:05:01,040
the screen for a moment, this is now a 22 year old experiment, but I think it still visualizes

48
00:05:01,040 --> 00:05:07,200
the potential of this approach. In this case, we were interested in creating a robot that can

49
00:05:07,200 --> 00:05:13,920
brachiate, that can swim, swing across beams or tree branches or electrical wires for various

50
00:05:13,920 --> 00:05:19,840
inspection tasks. And you'll notice that in this case, the AI came up with a solution that in

51
00:05:19,840 --> 00:05:26,560
retrospect seems intuitive. The robot has to carry a very heavy battery, which you can see in

52
00:05:26,560 --> 00:05:32,960
the physical robot in the top left there, the black box that's at the bottom. And the AI has

53
00:05:32,960 --> 00:05:40,160
figured out how to design the body of this robot so that it's actually able to exploit the forward

54
00:05:40,160 --> 00:05:47,280
momentum of this heavy object, this battery, to facilitate its movement or brachiation across

55
00:05:47,280 --> 00:05:53,680
this physical beam. So it's a simple example, a simple robot, a simple task, but it demonstrates

56
00:05:54,560 --> 00:06:03,600
this interplay between AI, robotics, body and brain. If the AI was not free to place the body,

57
00:06:04,080 --> 00:06:09,920
to place the battery at a particular place on the robot's body, it would be much harder,

58
00:06:09,920 --> 00:06:15,280
it would require more energy, it would require a more complex brain for the robot to figure out how

59
00:06:15,280 --> 00:06:22,880
to move its heavy weight across this beam. So that idea of tuning body and brain has

60
00:06:22,880 --> 00:06:29,680
suffused everything that we work on. Some other examples you can see in the top center and the

61
00:06:29,680 --> 00:06:37,920
top right. Here we have a robot that suffers damage, its body changes over time. So now the AI has to

62
00:06:37,920 --> 00:06:43,440
grapple with not just designing a body, but grappling with a body that changes. One of the

63
00:06:43,440 --> 00:06:49,920
things that we as intelligent organisms here in the world and all embodied AI, all autonomous cars,

64
00:06:49,920 --> 00:06:56,080
all drones, all robots have to deal with is entropy. The world throws a lot of stuff at us,

65
00:06:56,080 --> 00:07:02,400
we have to deal with wear and tear, injury. In our case, we grow from a single cell into about

66
00:07:02,400 --> 00:07:09,840
10 to the 12 cells. We change massively in terms of our physical magnitude. How do you continue

67
00:07:09,840 --> 00:07:17,040
to operate, keep yourself alive and do whatever it is you need to do across massive morphological

68
00:07:17,040 --> 00:07:23,280
change? That is not an easy thing to do. And again, it requires an AI, if it's going to do this with

69
00:07:23,280 --> 00:07:30,320
robots, it's got to figure out how to carefully tune body and brain to deal with the generation

70
00:07:30,320 --> 00:07:37,120
of behavior inside a body that is changing, either unexpectedly due to injury and wear and tear,

71
00:07:38,080 --> 00:07:44,800
or intentionally. You can see in the very center panel, this is again a pretty good

72
00:07:44,800 --> 00:07:52,560
visualization of where designing body along with brain comes in handy. If we want to make

73
00:07:52,560 --> 00:07:58,640
flying machines or swimming machines, we have to very, very carefully tune the geometry and

74
00:07:58,640 --> 00:08:05,600
material properties of the body itself to realize flight. So what you're seeing in this middle panel

75
00:08:05,600 --> 00:08:11,600
here, this is partway through the AI experimenting with the design of different kinds of wings

76
00:08:11,600 --> 00:08:20,480
for an ornithopter, a drone that flies by flapping its wings. This flexible wing that you see here

77
00:08:20,480 --> 00:08:26,000
in the center of the screen, this is a bit of a transition from traditional robots that are made

78
00:08:26,000 --> 00:08:33,360
of rigid structures, like you can see in the top row, into a more modern era in robotics,

79
00:08:33,360 --> 00:08:39,280
which is sometimes referred to as soft robotics. Material science has come a long way in the last

80
00:08:39,280 --> 00:08:46,800
20 years. So we can now start to build robots embodied AI. We can start to build robots out of

81
00:08:46,800 --> 00:08:53,840
materials other than rigid plastic and metals. And we can again, we can start to move into an era

82
00:08:53,840 --> 00:09:00,320
in which robots like organisms can exploit the material properties of their bodies

83
00:09:01,120 --> 00:09:07,280
to facilitate whatever behaviors they need to do to survive or be useful to humans and so on.

84
00:09:08,400 --> 00:09:14,560
So in the middle right panel, this is a highlight of some work that my group has done in collaboration

85
00:09:14,560 --> 00:09:22,080
with Rebecca Kramer, Bottiglio's lab at Yale. Rebecca's lab is famously advancing the state of

86
00:09:22,080 --> 00:09:28,320
the art in soft robotics. What can you get robots to do when they're made out of soft materials?

87
00:09:28,320 --> 00:09:33,760
You can see an example of some of those soft robots. Middle right and a very different soft

88
00:09:33,760 --> 00:09:41,120
robot lower left, which is exploiting its body properties in order to move in interesting

89
00:09:41,120 --> 00:09:48,080
ways. One of the interesting things about soft robotics in my perspective is that it starts to

90
00:09:48,080 --> 00:09:55,840
usher in an era in which robots can actually grow and complexify their bodies. You can see these

91
00:09:56,480 --> 00:10:03,680
hollow cubes in the middle right and these hollow sort of chambers in the bottom left

92
00:10:03,680 --> 00:10:11,520
expanding and contracting as we push and pull air into and then out of the body of the robot.

93
00:10:12,640 --> 00:10:17,440
Suddenly, now you have robots that can change their geometry. They can change their volume.

94
00:10:18,480 --> 00:10:24,640
They become what's known as thermodynamically open. It's a fancy term for meaning that they can

95
00:10:24,640 --> 00:10:33,040
draw new material and new energy into themselves. The thermodynamically open machines that you see

96
00:10:33,040 --> 00:10:39,200
middle right and lower left, the only thing they're drawing into their body is more air,

97
00:10:39,200 --> 00:10:46,960
but it's a start. I mentioned already that humans grow from a single cell into 10 to the 12 cells.

98
00:10:47,600 --> 00:10:54,880
Every organism on this planet, with a few exceptions, starts small and gets bigger over its

99
00:10:54,880 --> 00:11:03,360
lifetime. That fundamental morphological change starts small, starts simple and gradually grow

100
00:11:03,360 --> 00:11:13,440
in size and complexity. That provides scaffolding. It provides a gradient on which to learn how to

101
00:11:13,440 --> 00:11:18,800
gradually grapple with the world around you. Most organisms, again, there are exceptions,

102
00:11:18,800 --> 00:11:25,280
are not thrust into this world with all of their machinery online from the beginning.

103
00:11:26,320 --> 00:11:32,080
Just the way I'm phrasing this is obviously intentional to sort of dichotomize growing

104
00:11:32,080 --> 00:11:39,440
organisms and robots with fixed bodies. Autonomous cars are still very dangerous.

105
00:11:39,440 --> 00:11:46,640
Autonomous drones are still very dangerous to be around because 99.99% of the time,

106
00:11:46,640 --> 00:11:51,280
they do the right thing, but every once in a while, they don't know what to do,

107
00:11:52,000 --> 00:12:00,240
and no one knows what they're going to do in those uncertain circumstances. That is a very

108
00:12:00,240 --> 00:12:07,840
concerning situation as we start to now deploy robots and autonomous vehicles into everyday

109
00:12:07,840 --> 00:12:14,640
environments where they are in close proximity to humans. Why is it that even with state-of-the-art

110
00:12:14,640 --> 00:12:21,360
AI and with all of Google's data centers and AI training algorithms, we still can't rub out that

111
00:12:21,360 --> 00:12:30,080
0.001% where no one knows what's going to happen? Part of the reason, again, is these machines

112
00:12:30,160 --> 00:12:39,600
are born with complex bodies. We drop a controller into a one-ton autonomous car made of metal and

113
00:12:39,600 --> 00:12:47,120
plastics. It's very dangerous. We don't grow autonomous cars from a very small, simple,

114
00:12:47,120 --> 00:12:53,280
lightweight machine that can't cause anyone any harm whatsoever, and then when that simple,

115
00:12:53,360 --> 00:13:00,240
small machine demonstrates and verifies to us that it's safe, then we allow it to become larger

116
00:13:00,240 --> 00:13:06,320
and more complex. It sounds like silly sci-fi. Why would we build a machine like that? But again,

117
00:13:06,320 --> 00:13:12,400
every organism starts simple and grows in complexity. If it doesn't do the right thing,

118
00:13:12,400 --> 00:13:18,720
if it performs dangerous actions that are harmful to itself or fatal to itself, by definition,

119
00:13:18,720 --> 00:13:25,360
it doesn't get any further. That's, again, one of the ways of thinking about how the body shapes

120
00:13:25,360 --> 00:13:32,240
the way we think. In my personal and professional opinion, any physical machines that we deploy

121
00:13:32,240 --> 00:13:38,000
into the real world, they should start as very small, lightweight machines that can't harm anyone.

122
00:13:38,880 --> 00:13:43,760
They have a very limited number of actions that they can perform, and they sort of cycle

123
00:13:43,760 --> 00:13:51,040
through all those actions and verify everything, and only then can they take more mass, more energy

124
00:13:51,040 --> 00:13:57,680
into themselves. Can they recruit more material? They can sort of be allowed to be thermodynamically

125
00:13:57,680 --> 00:14:04,160
open and grow and complexify themselves. There's lots of ways in which we're starting to create

126
00:14:04,160 --> 00:14:10,240
machines that grow and complexify themselves. I just talked about these soft robots that can pull

127
00:14:10,240 --> 00:14:17,440
in air or pass possibly fluids. They could be hydrodynamic machines. They could mechanically

128
00:14:17,440 --> 00:14:23,920
or magnetically connect to other robots. That sort of swarm robotics. That's another path

129
00:14:23,920 --> 00:14:31,200
to growing machines. At the moment, most of these machines are still restricted to academic labs.

130
00:14:31,200 --> 00:14:36,480
They also are not safe yet, but I think in the long run, they're going to be a safer alternative to

131
00:14:37,360 --> 00:14:43,360
dropping AI into very large, complex, heavy, dangerous machines and crossing our fingers

132
00:14:43,360 --> 00:14:50,000
and hoping for the best. I've talked a little bit about rigid robots and soft robots.

133
00:14:50,720 --> 00:14:54,800
I want to try and talk as little as possible. There's some time for a good discussion,

134
00:14:54,800 --> 00:15:00,480
but I want to talk about what I see as sort of a third era of robotics and embodied AI,

135
00:15:00,480 --> 00:15:07,760
which is just starting to open up in the last few years, which is biobotics or creating biobots.

136
00:15:08,400 --> 00:15:17,360
You can see two biobots on display at the bottom center of my slide here. A biobot is a robot

137
00:15:17,360 --> 00:15:25,360
that's made up of only biological components, no technological components whatsoever. In the bottom

138
00:15:25,360 --> 00:15:32,320
left here, Kriegman, Blackiston, Levin and myself in 2020 published a paper demonstrating the first

139
00:15:32,320 --> 00:15:40,080
biobot. This became known after publication in the popular media as Xenobots, X-E-N-O,

140
00:15:40,880 --> 00:15:49,520
Xeno, Xenobots, because these Xenobots are built from about 2,000 frog cells and the cells were

141
00:15:49,520 --> 00:15:56,880
taken from a particular species of frog, which is Xenopus laevis. Michael Levin, our biology

142
00:15:56,880 --> 00:16:03,760
colleague at Tufts University, is world-renowned for demonstrating that you can reconfigure

143
00:16:03,760 --> 00:16:11,840
genetically unmodified materials like, for example, frog cells, and that rearrangement

144
00:16:11,840 --> 00:16:17,520
of living tissues not only does not kill the organism, the organism is able to in some cases

145
00:16:18,080 --> 00:16:24,320
continue on doing what it does, what it needs to do, ingest materials, survive,

146
00:16:24,320 --> 00:16:31,440
reproduce in this reconfigured state. There's a lot of biological implications for that.

147
00:16:31,440 --> 00:16:38,160
One of the biological implications is that frog DNA does not code for frog. What you're looking at

148
00:16:38,160 --> 00:16:43,360
in the bottom left, the Xenobot is about a millimeter in diameter, so it looks like a speck

149
00:16:43,360 --> 00:16:50,640
of pepper to the unmagnified eye, and yet it's able to walk around the bottom of a petri dish.

150
00:16:50,640 --> 00:16:56,000
It doesn't have all the features of a living organism, but it's got enough of them that it's

151
00:16:56,000 --> 00:17:05,040
motile. It's able to get itself from point A to point B. One of the other implications for AI

152
00:17:05,040 --> 00:17:10,880
of this biological discovery that you could rearrange genetically unmodified living tissues

153
00:17:10,880 --> 00:17:19,120
is that maybe we can task an AI with discovering novel rearrangements of living tissue to produce

154
00:17:19,760 --> 00:17:25,840
robots, to produce something that moves around and does something useful on a human's behalf.

155
00:17:27,120 --> 00:17:32,800
That's what I mean by a biobot, a biobot that's made from, in this case, genetically unmodified

156
00:17:33,360 --> 00:17:41,120
cells. The swarm of Xenobots that you see in the bottom right, as you can see,

157
00:17:41,120 --> 00:17:48,400
they're sort of pushing around some material in their dish. This sort of visually hints at

158
00:17:48,400 --> 00:17:54,480
applications for this type of robotic technology, which is they might be able to act like very,

159
00:17:54,480 --> 00:18:00,480
very small Roomba robot vacuum cleaners in the future. They might be able to collect

160
00:18:00,480 --> 00:18:09,360
microplastics out of waterways or cancer cells out of bloodstreams. The swarm that you see that's

161
00:18:09,360 --> 00:18:15,200
cleaning up in this slide at the bottom right, the material that they're cleaning up is actually

162
00:18:16,000 --> 00:18:23,440
other frog cells. It turns out that if the AI designs this swarm just right and the swarm

163
00:18:23,440 --> 00:18:30,000
that you're looking at, this is an AI designed swarm, the AI came up with the shape for each

164
00:18:30,000 --> 00:18:36,880
member of the swarm. This swarm is pushing these little white circles, which are individual frog

165
00:18:36,880 --> 00:18:43,280
cells, pushing them into piles. Turns out these individual frog skin cells at a certain stage

166
00:18:43,280 --> 00:18:50,080
of development are sticky, and they clump together into a pile. Some of these piles,

167
00:18:50,080 --> 00:18:55,680
if they're big enough, if they contain enough frog cells, they will grow very small hairs

168
00:18:55,680 --> 00:19:01,840
on the surface cells, the cells that are on the surface of the pile. Those little small hairs are

169
00:19:01,840 --> 00:19:11,360
called cilia. They're usually used to pull dirt and pathogens off the body of frogs, adult frogs.

170
00:19:11,360 --> 00:19:19,200
Here, when those cilia grow on small piles of frog cells, they're able to exert enough force

171
00:19:19,200 --> 00:19:27,360
against the surrounding water that these piles start to move. What you have in essence is a child

172
00:19:27,360 --> 00:19:33,840
xenobot. This swarm pushes cells together and in essence makes copies of themselves.

173
00:19:34,640 --> 00:19:40,400
This is another implication of this work, is that in this case the AI has figured out how to design

174
00:19:40,400 --> 00:19:47,600
robots that replicate. They make copies of themselves by finding raw material in their

175
00:19:47,600 --> 00:19:53,440
environment and constructing copies of themselves. This has been a long-standing dream in robotics,

176
00:19:55,520 --> 00:20:01,440
dating back a very long time to John von Neumann in the 1950s who had a thought experiment. It would

177
00:20:01,440 --> 00:20:06,880
be great if we could create robots that would create copies of themselves, which would create

178
00:20:06,880 --> 00:20:14,400
copies of themselves. If those robots do useful work for humans as a side effect, for von Neumann

179
00:20:14,400 --> 00:20:20,560
that was creating moon bases and then Mars bases and then colonizing the galaxy, which sounds great.

180
00:20:22,160 --> 00:20:30,160
But the seed of this idea is if we want robots to really be useful at scale, instead of manually

181
00:20:30,160 --> 00:20:36,320
constructing billions of robots and then deploying them to do something useful, which is expensive,

182
00:20:36,320 --> 00:20:40,320
it would be much cheaper to make one robot that does something useful for us.

183
00:20:41,200 --> 00:20:47,040
By the way, it also makes two copies of itself, which does more useful work for us in four and eight

184
00:20:47,040 --> 00:20:53,440
and sixteen and so on. We're not there yet with the Xenobots, but it's a demonstration that that

185
00:20:53,440 --> 00:20:59,920
is possible. Again, all of that becomes possible because the AI is designing both the bodies

186
00:20:59,920 --> 00:21:07,040
and the brains of these robots. This is very far now from the traditional view of AI and robotics,

187
00:21:07,040 --> 00:21:14,000
where we build a robot body, we humans build a robot body, and then the AI tinkers with the brain

188
00:21:14,000 --> 00:21:20,960
of that fixed machine. Part of the reason why I'm here today and part of the message of my group is

189
00:21:20,960 --> 00:21:28,320
we need to think more broadly about how to combine AI and robotics and possibly synthetic biology.

190
00:21:29,200 --> 00:21:35,920
When we do and we think more broadly, there are whole new paths that open up to ways in which

191
00:21:35,920 --> 00:21:43,120
we might create in the future, not yet, but in the future, create intelligent, useful, and safe

192
00:21:43,120 --> 00:21:50,720
machines. In the current era in AI, there is one particular approach, which is auto completion of

193
00:21:50,720 --> 00:21:57,040
tokens, which has come to dominate the field and come to dominate the popular imagination. We all

194
00:21:57,040 --> 00:22:03,760
kind of have an understanding more or less of what chat GPT is doing, and there are some very strong

195
00:22:03,760 --> 00:22:09,520
lobbying organizations out there that are bent on convincing us that if we just do this with

196
00:22:09,520 --> 00:22:16,320
more compute, more data, we will eventually get to safe machines. My contention is there just

197
00:22:16,320 --> 00:22:24,160
isn't enough data out there to make non-embodied AI like chat GPT and stable diffusion and all the

198
00:22:24,160 --> 00:22:32,160
rest to make them safe. We have to think differently about designing bodies and brains of machines

199
00:22:32,160 --> 00:22:40,160
simultaneously to realize this long-term goal. Okay, I've been talking for a while. I'm going to

200
00:22:40,160 --> 00:22:45,280
stop and I'm happy to take questions or engage in some discussion, and I'm happy to come back to

201
00:22:45,280 --> 00:22:51,120
any of these experiments and provide more detail if that's helpful. Over to you, Daniel.

202
00:22:51,920 --> 00:23:00,080
Thank you. Wow. Awesome. What a cornucopia of bodies and minds.

203
00:23:03,120 --> 00:23:09,840
It was a great overview. I was really struck by some of the similarities and the convergence on

204
00:23:10,480 --> 00:23:17,040
whole of lifecycle design and kind of holistic design coming from, on one hand, a systems engineering

205
00:23:17,040 --> 00:23:23,680
and a materials perspective, and on the other hand, from the biology perspective with like eco-evo-devo

206
00:23:24,400 --> 00:23:31,840
and this convergence upon needing to think about how the end-to-end function maybe even past the

207
00:23:31,840 --> 00:23:39,280
point of functionality like into the planned graceful decay of a robot as well. So it brings in a lot

208
00:23:39,280 --> 00:23:45,600
of topics that even from an outsider's perspective seem to be put as kind of secondary. So that's

209
00:23:45,680 --> 00:23:50,960
very cool. Okay, great. I'm looking forward to what people in the live chat, right?

210
00:23:52,320 --> 00:24:02,080
My first question is how over these 22 years, how have the materials, the theories, like the

211
00:24:02,080 --> 00:24:08,960
contexts, advances in turing computation, all these kinds of things, how have they intersected

212
00:24:09,280 --> 00:24:13,120
just what has the ride been like as you pursued these questions?

213
00:24:14,560 --> 00:24:22,560
Yeah, I think the short answer again is focusing on the physical aspect of AI and robotics. So the

214
00:24:22,560 --> 00:24:30,480
materials from which we can build machines has changed over 20 years. And from my perspective,

215
00:24:31,040 --> 00:24:34,320
the experiment, the top left there, that was something I did as part of my PhD,

216
00:24:35,040 --> 00:24:40,000
you know, the materials at the time, it was very hard to build a robot. You bought some

217
00:24:40,000 --> 00:24:45,040
motors, you bought a battery, you bought some metal, you bought some wires, and you wired

218
00:24:45,040 --> 00:24:50,400
everything up. There was the assumption that bodies were fixed. And not only that, but they were

219
00:24:50,400 --> 00:24:55,600
difficult to make. So once you made one, you were very careful with it to make sure it didn't change,

220
00:24:55,600 --> 00:25:02,560
that it didn't become damaged. And that seemed to comport with a lot of the theory in AI and

221
00:25:02,560 --> 00:25:09,840
neuroscience, which had the same sort of idea that the brain, or in the case of robotics,

222
00:25:09,840 --> 00:25:16,960
the control policy is the puppeteer. It's something that's pulling the strings of a fixed thing,

223
00:25:16,960 --> 00:25:25,120
either the body of an organism or a robot. And if you look at a lot of theory in both fields,

224
00:25:25,200 --> 00:25:31,040
AI and neuroscience, that assumption runs so deep. So for example, an active

225
00:25:31,920 --> 00:25:37,360
inference, you know, the free energy principle, we want to reduce surprise all that there's a

226
00:25:37,360 --> 00:25:43,360
fixed set of actions that we perform to try and reduce the surprise between what we're sensing

227
00:25:43,360 --> 00:25:49,680
and what we predicted we would sense given the past action. Where do those actions come from?

228
00:25:49,680 --> 00:25:57,440
Why are they fixed? Does the set of actions grow over time? Maybe the sensory data that's coming

229
00:25:57,440 --> 00:26:02,720
is coming from a new sensor that's just coming online or a sensor that's recently duplicated.

230
00:26:02,720 --> 00:26:09,120
Now there's two of them, but they're not quite reporting exactly the same thing. There's a whole

231
00:26:09,120 --> 00:26:16,640
bunch of assumptions underlying a lot of the theory about active inference, predictive coding.

232
00:26:17,600 --> 00:26:22,560
You name it, you pick your concept from neuroscience or cognitive science or AI.

233
00:26:25,360 --> 00:26:32,080
Once you peel back those assumptions, imagine the robot's body changes. Imagine the robot splits

234
00:26:32,080 --> 00:26:40,960
in two and becomes two copies of itself. A lot of the theory and the formal underpinnings of

235
00:26:41,040 --> 00:26:46,240
that theory break down. You start to get into ill posed questions, which force you to now

236
00:26:46,240 --> 00:26:52,800
think about how do you fundamentally change the theory? If you have a hierarchy of actions,

237
00:26:52,800 --> 00:26:58,160
like in predictive coding or active inference, what if that hierarchy is growing and changing

238
00:26:58,160 --> 00:27:04,400
over itself is growing and changing over time? How do we address that in a formal manner? So

239
00:27:04,400 --> 00:27:09,040
to get back to your question, I think these advances in what we can do physically, we can

240
00:27:09,040 --> 00:27:14,720
build robots now out of soft materials. We can build robots out of living materials,

241
00:27:15,520 --> 00:27:21,120
which on their own will grow and divide and seek out energy and material on their own.

242
00:27:22,800 --> 00:27:28,160
Those physical machines, these odd new kinds of creatures, are militating. They're pushing

243
00:27:28,160 --> 00:27:34,080
against the theory. Specifically, they're pushing against these unspoken assumptions that lie

244
00:27:34,080 --> 00:27:41,760
underneath a lot of this theory about what's required to act intelligently in a complex world.

245
00:27:43,120 --> 00:27:50,400
That's awesome. Like the real world and the territory expanding into our unknown unknown.

246
00:27:50,400 --> 00:27:54,640
Okay, there's a bunch of questions in the live chat. So I'm just going to go for them,

247
00:27:54,640 --> 00:27:57,360
give any answer that you like. Okay.

248
00:27:57,360 --> 00:28:02,400
Sure. David Williams wrote,

249
00:28:03,280 --> 00:28:09,280
How do you think about the controllers in your robotics? Embedded AI at least today is rather

250
00:28:09,280 --> 00:28:15,760
hard. Batteries and chips, PCBs, not soft and not easily synthesized locally. So how do you

251
00:28:15,760 --> 00:28:21,120
think about the controllers in your robotics? Yeah, great question. So, right, exactly. The

252
00:28:21,120 --> 00:28:27,840
controllers are dealing with hard rigid fixed components. We need to start thinking about

253
00:28:27,840 --> 00:28:34,400
controllers that can, in which, for example, the input layer and the output layer can grow and shrink

254
00:28:34,400 --> 00:28:41,360
over time. There may be new sensors or new input coordinates that are growing or being attached

255
00:28:41,360 --> 00:28:48,480
to a machine. And the controller needs to be able to carefully deal with those new input channels

256
00:28:48,480 --> 00:28:55,040
while the machine is operating. Same thing goes for the output channel. There may be new

257
00:28:55,040 --> 00:29:04,480
actions or new dimensions of action along which the machine can act. And control policies,

258
00:29:04,480 --> 00:29:10,560
reinforcement learning, all the rest of it does all those assumptions that make reinforcement

259
00:29:10,560 --> 00:29:16,240
learning work, which is what drives most autonomous vehicles at the moment, assumes

260
00:29:16,240 --> 00:29:21,200
that the dimensionality of input and the dimensionality of output, the things that the machine

261
00:29:21,200 --> 00:29:29,120
can do and sense, are fixed during training or during behavior generation, during execution.

262
00:29:29,120 --> 00:29:35,520
That is absolutely not true in any organism on the planet. And that's becoming increasingly untrue

263
00:29:36,640 --> 00:29:43,040
for our coming machines. Now, how to do it well? I don't have any answers, but we have to figure

264
00:29:43,040 --> 00:29:49,040
it out. You were asking a question about thinking about controllers. That's a concrete example

265
00:29:49,040 --> 00:29:55,040
about how we have to rethink control policy optimization, even if we're not thinking directly

266
00:29:55,040 --> 00:30:00,400
about the body, even if we just focus on the control policy and ask what happens as the input

267
00:30:00,400 --> 00:30:06,480
and output channel, the dimensionality of the input and output channels change during behavior

268
00:30:06,480 --> 00:30:14,400
execution. Yeah, just one short point on that. It's like training with a fixed set of perceptual

269
00:30:14,400 --> 00:30:20,640
elements or of affordances or actuators. It's like training on one point in a larger space of the

270
00:30:20,640 --> 00:30:27,200
adjacent possible of bodies or of architectures. So then, okay, we're bringing all this compute

271
00:30:27,200 --> 00:30:34,800
to train a special case in the fixed setting. And that's not even how the smallest organism works.

272
00:30:34,800 --> 00:30:41,680
So that just again, kind of shows that point. Okay. Sorry, before we move on from that point,

273
00:30:41,680 --> 00:30:47,680
just to again, illustrate how the body shapes the way we think. In the case of a growing biological

274
00:30:47,680 --> 00:30:53,600
body, there are new input channels that come online throughout our lifetime, but they don't

275
00:30:53,600 --> 00:30:59,280
appear de novo. Whatever it is, whatever that new input channel is, as we're growing, we just have

276
00:30:59,360 --> 00:31:08,560
more sense cells. The signals that they're sending into the peripheral and central nervous system

277
00:31:08,560 --> 00:31:14,960
are not orthogonal to whatever else is already coming in as input, because new input channels

278
00:31:14,960 --> 00:31:21,760
or new cells are slowly dividing. And at the moment of division, they're providing exactly the same

279
00:31:21,760 --> 00:31:29,040
signal as some other sensory channel that already exists. So the body, or in this case, biological

280
00:31:29,040 --> 00:31:36,400
growth provides an immediate scaffold, a gradient. In robotics, it can be very scary to think about

281
00:31:36,400 --> 00:31:42,000
like attaching a sensor to an autonomous vehicle. What the hell does it do with this new information

282
00:31:42,000 --> 00:31:48,880
that's coming in? Because we haven't thought carefully about how to add that new sense modality

283
00:31:48,880 --> 00:31:57,280
to the machine. Again, we have to look to nature that every new sense modality is gradually coming

284
00:31:57,280 --> 00:32:04,800
online and gradually drifts away or becomes increasingly orthogonal to the starting input

285
00:32:04,800 --> 00:32:12,080
modality. So that's how we should, if we did that physically with machines, it would simplify

286
00:32:13,200 --> 00:32:16,960
reinforcement learning or would make it easier for reinforcement learning or what have you,

287
00:32:18,240 --> 00:32:23,280
sorry, let me reshare my screen here, it would simply make it easier for the,

288
00:32:23,280 --> 00:32:28,480
sorry, something seems to have gone wrong here, give me a moment.

289
00:32:36,320 --> 00:32:43,040
Yep. Okay, all right. Yeah, it makes things easier on the control policy optimization process

290
00:32:43,760 --> 00:32:50,080
if new sense organs and new motor outputs are coming online, but they resemble things that already exist.

291
00:32:53,280 --> 00:32:59,280
That's super interesting. Brings up a lot of questions about like self and non self recognition

292
00:32:59,280 --> 00:33:06,560
and what is a self as new and different senses and actions come online. Sure. Okay. Prakash

293
00:33:06,560 --> 00:33:14,800
Kavi asks, do these bio bots have any sense of agency? What is your sense? I'm quite intrigued

294
00:33:14,800 --> 00:33:20,800
by the idea that beyond a critical point, they start growing hair. And do these bio bots act

295
00:33:20,800 --> 00:33:25,520
independently of each other? And also what happens at a group level? So what's your sense

296
00:33:25,520 --> 00:33:31,200
of agency in bio bots? And I guess the bio bot and the group level?

297
00:33:32,400 --> 00:33:37,440
Yeah, it's a great question. So I'll start with the disclaimer. I'm not a biologist. I'm a computer

298
00:33:37,440 --> 00:33:44,720
scientist by formal training. So I can only say so much about what the cells are doing and what

299
00:33:44,720 --> 00:33:54,640
they want to do. I definitely follow in the footsteps of the late Daniel Dennett in that

300
00:33:54,640 --> 00:33:59,360
when we talk about agency, we each of us individually has to decide whether or not we

301
00:33:59,360 --> 00:34:07,200
take the intentional stance or not. It's in my opinion, it's a point of view. If it's easier to

302
00:34:07,200 --> 00:34:15,280
explain what the Xenobots are doing by talking about what the cells want to do, like grow cilia

303
00:34:15,280 --> 00:34:22,160
and coordinate their actions, fine. If it's easier to explain what the Xenobots are doing by not

304
00:34:22,160 --> 00:34:28,560
taking the intentional stance and describing cells as mechanical components that are transforming

305
00:34:28,560 --> 00:34:35,920
input into actions, that's fine too. This is something also that comes from my colleague,

306
00:34:35,920 --> 00:34:41,520
Mike Levin. It depends. As scientists, if we want to try and explain and understand what these

307
00:34:41,520 --> 00:34:47,600
machines are doing, if taking the intentional stance makes explanation easier, fine. If not,

308
00:34:47,600 --> 00:34:55,840
then not. But attributing agency is sort of an objective property of the bots or the cells themselves.

309
00:34:55,840 --> 00:35:03,520
Independent of us is observers. To me, that's philosophically and practically problematic.

310
00:35:04,480 --> 00:35:12,320
As far as I know, there is no objective measure of agency in cells, let alone inorganic robots.

311
00:35:15,280 --> 00:35:20,320
Super interesting. That's like the second order cybernetics or the observer theory

312
00:35:20,320 --> 00:35:26,080
or the poly computing question, which is to say just looking at something and then

313
00:35:26,080 --> 00:35:32,560
treating one's perspective as objectively. The case, it is objectively the subjective experience.

314
00:35:33,600 --> 00:35:38,800
Absolutely. Now, that being said, again, there is an empirical side to this. We can make some

315
00:35:38,800 --> 00:35:48,400
progress in understanding the Xenobots by comparing them against a control. So instead of cells,

316
00:35:48,400 --> 00:35:56,720
if these were magnets or some complex mechanical system in which more of us are comfortable in

317
00:35:56,720 --> 00:36:05,920
saying there is no agency, it's just a bucket of cogs doing something, and that control does not

318
00:36:05,920 --> 00:36:11,440
exhibit kinematic self-replication, for example, or it's much harder for the AI to figure out how

319
00:36:11,440 --> 00:36:20,640
to put together non-agential components to do what it is, then I feel a little bit more comfortable

320
00:36:20,640 --> 00:36:25,200
by saying the cells are doing something more. Now, I don't know whether it's agential or they

321
00:36:25,200 --> 00:36:31,760
want to do something, or if it's free will or consciousness, I don't know. But if we can point

322
00:36:31,760 --> 00:36:38,640
at biobots or machines that are built from biological components and say it's easier to get them to do

323
00:36:38,640 --> 00:36:46,560
things because they become complicit in the overall goal compared to mechanical parts,

324
00:36:46,560 --> 00:36:54,720
which don't, okay. And again, as a roboticist, the top and the middle rows that you see on my

325
00:36:54,720 --> 00:36:59,840
slide here, when we do build things out of metal and rubber and plastics and ceramics,

326
00:37:00,640 --> 00:37:05,280
it's usually super hard. It's really hard to get them to do whatever we want them to do.

327
00:37:06,640 --> 00:37:11,680
We've been working on robotics since the end of the Second World War, and we've got the Roomba,

328
00:37:11,680 --> 00:37:16,880
and maybe we've got autonomous cars, we're getting there. It's taken a really long time

329
00:37:16,880 --> 00:37:23,440
because robotics is really hard. It's really hard to convince physical materials to adapt and do

330
00:37:23,440 --> 00:37:28,480
something useful and safe. On the flip side, we've been working on Xenobots at the bottom here.

331
00:37:28,480 --> 00:37:34,560
We've been working on them for about five or six years, and we've got Roombas. We're making faster

332
00:37:34,560 --> 00:37:42,560
progress in robotics when we build from cells than when we build from metal suggests the cells are

333
00:37:42,560 --> 00:37:47,200
somehow helping. I don't know that they want to help. We've got to be careful there. That's the

334
00:37:47,200 --> 00:37:55,840
intentional stance, but when you try and compose machines from smart machines and cells are smart

335
00:37:55,840 --> 00:38:03,200
machines, I know I'm biased, but I think we're making faster progress than when we build machines

336
00:38:03,200 --> 00:38:11,840
out of inert materials. Yeah, super interesting. Okay, I'll read some comments from David Clement.

337
00:38:11,840 --> 00:38:21,280
David wrote, does your work incorporate a gentile hierarchies? For example, does Xenobots grow by

338
00:38:21,280 --> 00:38:28,320
replicating the initial seed cell into a higher order system? And is it critical for lower order

339
00:38:28,320 --> 00:38:34,720
systems to act as a component of a virtual machine, meaning that they have a target behavior that is

340
00:38:34,720 --> 00:38:39,840
less than the higher order system? And that's kind of related to Prakash's question as well. Like,

341
00:38:39,840 --> 00:38:45,920
how do you bridge that from the individual component into the swarm or the aggregate?

342
00:38:47,040 --> 00:38:53,840
Yeah, it's a really good question. So, absolutely, I think that when we started working on the Xenobots

343
00:38:53,840 --> 00:38:59,680
and Mike Levin started to talk about machines made of machines, made of machines, that definitely

344
00:38:59,680 --> 00:39:05,440
has influenced the work in my group to focus on this issue of hierarchy. I don't know about a

345
00:39:05,440 --> 00:39:10,560
gentile hierarchy. Again, we just talked about a gentile agency, that's maybe a

346
00:39:11,520 --> 00:39:17,600
subjective stance. But definitely, you know, why would you want to build machines out of

347
00:39:17,600 --> 00:39:23,760
machines out of machines? At the moment, our state of the art robots, like autonomous vehicles,

348
00:39:23,760 --> 00:39:32,160
are not hierarchical. The control policy operates at the level of the machine as a whole.

349
00:39:32,960 --> 00:39:39,520
For example, if there's an emergency blowout of the tire, an autonomous vehicle, the tire itself,

350
00:39:39,520 --> 00:39:45,840
the rubber that makes up the tire, doesn't deform and try and fix or reduce surprise all locally.

351
00:39:45,840 --> 00:39:52,960
It can't. It's rubber. It's inert material. We don't have machines built of machines built

352
00:39:52,960 --> 00:39:59,280
of machines yet. But as biology in general and the Xenobots in particular demonstrate,

353
00:39:59,360 --> 00:40:04,720
there's an adaptive advantage to being a hierarchy of physical things, of physical machines.

354
00:40:05,760 --> 00:40:09,760
If there is a surprising event at the level of the machine as a whole,

355
00:40:10,800 --> 00:40:17,760
but that surprise trickles down through the hierarchy, it's unlikely that everyone at every

356
00:40:17,760 --> 00:40:23,040
level of the hierarchy is going to be surprised. Someone somewhere in the hierarchy is going to

357
00:40:23,040 --> 00:40:30,160
say, from my local view at least on this bigger surprising issue, I know what to do. So let me

358
00:40:30,160 --> 00:40:36,720
start to communicate to my peers and up the hierarchy to deal with surprise. That would be,

359
00:40:37,440 --> 00:40:44,480
from an engineering point of view, that would be a good thing to have in big, heavy, fast-moving

360
00:40:44,480 --> 00:40:51,440
robots that are near humans. There's always going to be some surprising event that the vehicle

361
00:40:51,440 --> 00:40:57,040
as a whole has never seen before. There's great YouTube videos of horrifyingly

362
00:40:58,000 --> 00:41:03,600
scary surprising edge cases for autonomous vehicles. Okay, we're never going to fix every

363
00:41:03,600 --> 00:41:11,920
edge case. What we can fix is to make hierarchies and maybe agential hierarchies where local surprise

364
00:41:11,920 --> 00:41:18,400
can be handled or global surprise can be broken down into local surprising events, which can be

365
00:41:18,480 --> 00:41:25,920
handled locally. If I understood the second part of your question is how do we design that

366
00:41:25,920 --> 00:41:31,840
hierarchy? Should the smaller parts be trying to pull in the same direction or be trying to solve

367
00:41:31,840 --> 00:41:37,680
some part of the goal of the higher level? I think that's a super interesting question

368
00:41:37,680 --> 00:41:43,920
and I don't think that the answer is obvious. It may be that smaller parts pursuing orthogonal

369
00:41:43,920 --> 00:41:56,320
goals may end up being useful. Just to give you a quick example, if there's a surprising event

370
00:41:56,320 --> 00:42:02,080
and you've got a whole bunch of semi-independent machines organized in a hierarchy, I would argue

371
00:42:02,080 --> 00:42:07,040
that every single one of those members of the hierarchy should have a slightly different

372
00:42:07,040 --> 00:42:11,280
body and brain. It should have a slightly different form and function. You don't want a

373
00:42:11,280 --> 00:42:16,000
monoculture. You don't want all the parts being smaller versions of the bigger parts

374
00:42:16,560 --> 00:42:23,440
and trying to achieve smaller versions of the same goals because then you've basically got

375
00:42:23,440 --> 00:42:29,200
a committee in which everybody thinks and feels the same way. As we know from humans, that's a

376
00:42:29,200 --> 00:42:35,360
dangerous thing. You get group think or group act. You actually want a hetero culture. You

377
00:42:35,360 --> 00:42:40,080
want a whole bunch of things that are unique in terms of form and function and that maximizes

378
00:42:40,080 --> 00:42:44,480
your chances that someone somewhere in the hierarchy says, just because of the way I'm

379
00:42:44,480 --> 00:42:50,400
built and the way I think with my local control policy, I know what's going on and I have the

380
00:42:50,400 --> 00:42:55,680
seed of a solution. Here's the seed. You all figure out what you need to do to make it a

381
00:42:55,680 --> 00:43:01,600
reality at the larger level. That's another aspect of where the body comes into play.

382
00:43:02,320 --> 00:43:12,240
Yeah. Thank you. Like everywhere is the last mile from somewhere. Things have to be addressed

383
00:43:12,240 --> 00:43:16,800
locally. No matter how you think about a communications architecture distally,

384
00:43:17,680 --> 00:43:23,440
everything and embodiment calls our attention back to that. It has to be somewhere locally.

385
00:43:23,440 --> 00:43:29,360
So then why not take that as the starting point instead of this resource challenge

386
00:43:29,360 --> 00:43:38,400
and then about the multiple subunits when there's a damage to the nest of an ant colony or

387
00:43:38,400 --> 00:43:44,720
there's some things spilled on the surface. It's not that every single nest makes a perfect

388
00:43:44,720 --> 00:43:52,960
pebble move. It's that 51% accuracy with a bunch of nonspecific flurrying of activity,

389
00:43:52,960 --> 00:43:58,880
just like kind of stress or these more generic higher order signaling. That is what allows

390
00:43:59,440 --> 00:44:05,680
nest mates with different brains and bodies to fulfill their own paths of least action.

391
00:44:05,680 --> 00:44:11,440
And then colonies for which that doesn't clean up the mess or it cleans up too well and there's

392
00:44:11,440 --> 00:44:16,800
externalities, those colonies are swept off a table. And then we see the persistence of

393
00:44:16,800 --> 00:44:22,800
collective systems that could figure that out in their growth from a little colony to a big colony

394
00:44:23,760 --> 00:44:30,800
Absolutely, great, great example. I had a question you mentioned, both safety as well as

395
00:44:30,800 --> 00:44:40,240
like reliability. And how do you think about capacities and evaluations on diverse intelligences?

396
00:44:40,800 --> 00:44:48,240
We're all familiar with RAM, CPU, hard drive storage, some of the von Neumann type architectural

397
00:44:48,320 --> 00:44:55,440
descriptors. However, how do we even think about what does that rubric or report card

398
00:44:55,440 --> 00:45:00,160
even start to look like when we know that there's complex interactions with the niche

399
00:45:00,160 --> 00:45:04,240
and when the kinds of capacities that we're talking about may have even open-endedness?

400
00:45:05,520 --> 00:45:12,320
Yeah, great point, great point. So we are the beneficiaries of two big revolutions,

401
00:45:12,320 --> 00:45:17,360
one of them is the AI revolution, but then the older one is the digital electronics revolution.

402
00:45:18,480 --> 00:45:23,760
Digital electronics works, we all have a super computer in our pocket, like there's no arguing

403
00:45:23,760 --> 00:45:29,840
with it. It's an incredibly powerful way to make machines that internally communicate quickly and

404
00:45:29,840 --> 00:45:36,560
richly and then can communicate with other machines. I mean, that's it, that's the information age

405
00:45:36,560 --> 00:45:43,440
that we're in. It's been so successful that it's hard to think about alternatives or why we would

406
00:45:43,440 --> 00:45:50,960
even bother thinking about alternatives. But again, living systems, a lot of what cells do,

407
00:45:50,960 --> 00:45:56,480
they rely on electrical communication, but they also rely on mechanical communication,

408
00:45:56,480 --> 00:46:04,800
chemical communication, thermal communication. Cells are using all physical modalities,

409
00:46:04,800 --> 00:46:10,720
not all physical modalities, as many as they can get their hands on simultaneously all the time.

410
00:46:10,720 --> 00:46:15,040
Why? Why don't they just abandon everything and do everything purely electrically,

411
00:46:15,040 --> 00:46:20,640
like our modern civilization has done? Because it's dangerous, you don't have a diversified

412
00:46:20,640 --> 00:46:26,800
portfolio. So one panel here that I haven't talked about is the one in the bottom right,

413
00:46:27,360 --> 00:46:34,000
which is basically just what you're looking at is what's called a granular material. It's a material

414
00:46:34,000 --> 00:46:39,120
that's made up of a bunch of grains. In this case, the blue circles that you see, these are little

415
00:46:39,120 --> 00:46:46,240
just rubber pucks. And there is an oscillation being supplied at the left hand side. And you can

416
00:46:46,240 --> 00:46:54,000
see that this leads to interesting non-linear vibrational behavior within this material.

417
00:46:54,560 --> 00:47:01,280
What does that have to do with robotics or AI? If you view the vibrations as the carrier of

418
00:47:01,280 --> 00:47:08,640
information, so if a puck is vibrating, that's a one. If the puck is not vibrating, that's a zero.

419
00:47:09,840 --> 00:47:15,440
Now you can start to imagine creating materials that communicate Shannon information

420
00:47:16,240 --> 00:47:22,880
throughout the physical structure, not with electricity, but with a different modality,

421
00:47:22,880 --> 00:47:30,080
dynamics or vibration. And it turns out that you can actually compose these meta materials to

422
00:47:30,080 --> 00:47:37,600
embody logic gates. If you vibrate one particle or another particle, but not both and not neither,

423
00:47:37,680 --> 00:47:44,000
you can watch a third particle and it will either vibrate or not in accordance with an exclusive

424
00:47:44,000 --> 00:47:51,360
OR gate. And you can build this up. Now again, why would you do that? We can make an XOR gate

425
00:47:51,360 --> 00:47:56,800
that's vanishingly small and vanishingly fast in digital electronics. Why would you ever want to

426
00:47:56,800 --> 00:48:03,680
do something different? Because it turns out there are advantages of communicating with vibration

427
00:48:03,680 --> 00:48:10,000
rather than electricity under certain conditions. Having a machine that can communicate between

428
00:48:10,000 --> 00:48:15,840
distant parts of its body through mechanical vibration as well as electricity has an advantage

429
00:48:15,840 --> 00:48:22,640
over a machine that can only communicate long distances within its body electrically. I won't

430
00:48:22,640 --> 00:48:28,320
go into the reasons, but you can intuitively start to understand that. So again, I think we need to,

431
00:48:28,880 --> 00:48:35,280
if we're serious about creating safe and useful autonomous machines, we have to break out of the

432
00:48:35,280 --> 00:48:40,400
digital electronics assumption that that's the only way to do things. We have to break out of

433
00:48:40,400 --> 00:48:46,160
the assumption that non-embodied cognition is the way to go and it's easy to just drop it into a

434
00:48:46,160 --> 00:48:51,920
physical body and we're good to go. We have to peel back some of these very deep assumptions about

435
00:48:51,920 --> 00:48:57,200
the right way to do things that have built up in our society since the Second World War,

436
00:48:57,200 --> 00:49:02,160
because a lot of those technologies have been very successful, nothing wrong with them. But when

437
00:49:02,160 --> 00:49:09,120
we come to apply them to creating safe and useful machines, not always the right thing or the only

438
00:49:09,120 --> 00:49:17,280
way to approach things. That's really interesting. It's like a sort of generalized compute concept

439
00:49:17,280 --> 00:49:23,840
where we could talk about, well, these are the chemicals that it can detect with this fidelity

440
00:49:23,840 --> 00:49:29,120
and here's its tactile interface, here's its electromagnetic capacity for sending and receiving.

441
00:49:29,120 --> 00:49:36,240
That's what kind of motivates or complements the generic theory like free energy principle,

442
00:49:36,240 --> 00:49:42,240
which doesn't tell us about how anything is in particular, but then sets us up with kind of

443
00:49:42,240 --> 00:49:49,440
the framework to plug in these different modules. And then it's an empirical question. And then

444
00:49:49,440 --> 00:49:59,120
right here is sort of the virtual body and a real body. And so that's also very interesting.

445
00:49:59,760 --> 00:50:07,280
How does that work in a collaboration or with a graduate student? How do you balance

446
00:50:07,920 --> 00:50:17,760
this digital adjacent possible off of the material and the more costly implementation with embodiment?

447
00:50:18,720 --> 00:50:24,480
Yeah, well with grad students and postdocs or whoever I'm collaborating with that's kind of

448
00:50:24,480 --> 00:50:29,520
starting out, this can be a very frightening prospect for someone who's trying to get into

449
00:50:29,520 --> 00:50:35,920
AI and robotics because it looks like everything's been done, it's solved. We just have to wait for

450
00:50:35,920 --> 00:50:42,160
Google and Microsoft to buy more compute and data and they're going to finish off the last 1% of

451
00:50:42,480 --> 00:50:49,920
dangerous behavior. So if you're trying to contribute to society's goal of making useful

452
00:50:49,920 --> 00:50:56,400
autonomous safe machines as starting out, what do you do? It looks like this massive brick wall,

453
00:50:56,400 --> 00:51:03,120
there's no entry point. So my take on this is again is that we may be going about this all wrong,

454
00:51:03,120 --> 00:51:09,920
right? The assumption that electricity should be the carrier of information inside an autonomous

455
00:51:09,920 --> 00:51:16,160
machine, that's an assumption. Why electricity? Why not vibration? Why not something else? So

456
00:51:17,440 --> 00:51:21,520
even if you start to think about the alternatives, the immediate reaction as well, it's not going to

457
00:51:21,520 --> 00:51:27,520
be as good, maybe, maybe. But if you think about vibration, you were just mentioning like compute.

458
00:51:27,520 --> 00:51:34,560
We can use vibration for compute, but vibration is movement. So the minute you start to think

459
00:51:34,560 --> 00:51:41,120
about vibration as the carrier of Shannon information, you're now conflating action with

460
00:51:41,120 --> 00:51:49,120
computation. They cannot be separated. Descartes convinced the West 400 years ago that they're

461
00:51:49,120 --> 00:51:58,480
separate. They just are. And you look at AI and robotics, what a surprise. These two are attempts

462
00:51:58,560 --> 00:52:05,680
to create, you know, AGI is bicameral. There's one team that says it's going to happen in computers

463
00:52:05,680 --> 00:52:11,280
and the other side that says it's going to happen in physical machines. That's the Cartesian legacy,

464
00:52:11,280 --> 00:52:16,160
that they're separate. But the minute you look at some very humble material like the one in the

465
00:52:16,160 --> 00:52:22,560
bottom right, it's a bunch, it's 12 hockey pucks next to one another. There's no Cartesian division

466
00:52:22,560 --> 00:52:28,720
anymore between body and brain. There is a body and there is a brain there, but it looks very

467
00:52:28,720 --> 00:52:36,720
different from anything we would usually consider. And there's no value judgment here. It's not

468
00:52:36,720 --> 00:52:43,600
better or worse, maybe it is depending on what your metric is. It's just very different. And so

469
00:52:43,600 --> 00:52:50,400
with grad students and postdocs, I encourage them to pursue that. Could we do things completely

470
00:52:50,400 --> 00:52:57,120
differently? And in the long run, might that be a better way to do things? Who knows? We'll see.

471
00:52:58,880 --> 00:53:05,120
Awesome. I'll make one comment and then ask a last question. You brought up Descartes and that's

472
00:53:05,120 --> 00:53:14,560
the rest extensa, rest cognitive dualism between the thinking and the non-thinking substance and

473
00:53:15,120 --> 00:53:22,800
embodied cognition, embodied intelligence provides both an operational, instrumental,

474
00:53:22,800 --> 00:53:29,200
and an ontological counter argument or complementary perspective, which is just, well,

475
00:53:29,200 --> 00:53:36,640
in practice and in actuality, take it or leave it, they are inseparable. And so at the very least,

476
00:53:36,640 --> 00:53:42,640
that that starts to ratchet and leapfrog the discussion about what is mind and body.

477
00:53:43,440 --> 00:53:49,200
And you started with pointing out how important it was to co-evolve and the complementarity of

478
00:53:49,200 --> 00:53:53,520
mind and body. And it's like, there are two separate things that need to be complementary

479
00:53:53,520 --> 00:54:01,120
and tangoing. And also, maybe they're integrated and blurred in even deeper ways than the dance.

480
00:54:01,120 --> 00:54:08,800
So it's an empirical entry point into what otherwise is a thought experiment, which can

481
00:54:08,880 --> 00:54:16,240
have utility, but also can be just arbitrarily misleading. Absolutely. One of my former mentors,

482
00:54:16,240 --> 00:54:21,600
Inman Harvey at the University of Sussex, used to talk about robotics as philosophy with a screwdriver.

483
00:54:22,960 --> 00:54:27,040
It's not just armchair philosophy. It's when you start to build some of these machines,

484
00:54:27,600 --> 00:54:33,280
maybe in retrospect, in the case of the Metamaterials project, for me in retrospect,

485
00:54:33,280 --> 00:54:38,640
I said, oh my God, most action and cognition are not complementary, separate things that

486
00:54:38,640 --> 00:54:43,440
are complementary. They're one and the same thing. It's not embodied cognition. It's not

487
00:54:43,440 --> 00:54:51,600
an adjective of a noun. It's embodiment is cognition. There are not two things here.

488
00:54:51,600 --> 00:54:57,760
There's just one thing. Very hard to think about. It's so alien to a Western mind, but

489
00:54:58,880 --> 00:55:04,640
it just is. Sometimes I think about that in terms of adjectives getting added in front of a word

490
00:55:04,720 --> 00:55:11,680
and then the term expanding, and then it just encompassing, oh, of course, cognition is ecological,

491
00:55:11,680 --> 00:55:16,080
embodied, enacted, et cetera, et cetera, et cetera, et cetera. And then so it kind of like needs to be

492
00:55:16,080 --> 00:55:25,200
distinguished. And then it subsumes again. And that's part of in closing, like, what are you excited

493
00:55:25,200 --> 00:55:31,840
about? Where can people continue to learn more? What would you say to a person who's wanting to

494
00:55:31,840 --> 00:55:41,520
like go in this area? Yeah. Okay. Great question. So Google my name and it'll take you into lectures

495
00:55:41,520 --> 00:55:47,200
and papers and tutorials. And people want folks want to email me. That's perfectly fine. Again,

496
00:55:47,200 --> 00:55:50,720
Google my name. You can find my email. Happy to put you in touch with the right people.

497
00:55:52,240 --> 00:55:57,520
I think it's, you know, it's easier than ever to get started. You can go to chat GPT and say,

498
00:55:57,600 --> 00:56:04,720
you know, create some tutorials for me to start coding up robots. You know, they're, ironically,

499
00:56:04,720 --> 00:56:10,640
non embodied AI provides a good on ramp now, not just for reading about these ideas or listening to

500
00:56:10,640 --> 00:56:16,480
people talk about these ideas. You can start coding them up in a way that was easier than ever.

501
00:56:16,480 --> 00:56:21,040
That's easier than ever. The old days, you know, you had to learn C and then, you know,

502
00:56:21,040 --> 00:56:26,960
go on from there. Very easy to get your hands dirty, maybe not with physical materials,

503
00:56:26,960 --> 00:56:32,480
but you can create like you see in the left of each of these panels. You can create machines that

504
00:56:32,480 --> 00:56:37,280
are virtual. They're not physical, but they're embodied. It's another point that's important

505
00:56:37,280 --> 00:56:44,240
to make embodiment does not imply physicality. You need to be able to push against the world and

506
00:56:44,240 --> 00:56:49,600
observe how the world pushes back, but the world and you may be virtual like you see on the left

507
00:56:49,600 --> 00:56:55,520
or physical like you see on the right. So you can actually relatively quickly get your hands dirty

508
00:56:55,520 --> 00:57:01,440
with playing around with embodied AI these days. And I encourage everyone to do so.

509
00:57:02,880 --> 00:57:04,880
Cool. Any last comments?

510
00:57:06,320 --> 00:57:12,800
I would just say I was at the computer vision and pattern recognition conference CVPR a few

511
00:57:12,800 --> 00:57:18,720
weeks back. This is one of the flagship AI conferences, 15,000 attendees. And after my

512
00:57:18,720 --> 00:57:22,880
talk, a lot of grad students came up and they said, listen, I, you know, you, you sort of

513
00:57:22,880 --> 00:57:28,000
demonstrated there's another path here that I was feeling depressed or anxious about how to make

514
00:57:28,000 --> 00:57:33,440
progress in AI when these goliaths are, you know, have these data centers at their beck and call.

515
00:57:34,160 --> 00:57:40,160
I would just encourage everyone that when you think differently about all this stuff,

516
00:57:40,720 --> 00:57:45,520
there are new paths that open up. They may not in the long run be the right path, but there are

517
00:57:45,520 --> 00:57:52,080
alternatives to this monolithic predict the next token idea, which is currently, you know,

518
00:57:52,080 --> 00:57:58,960
in vogue. It's, it may be the beginning of the end, but I think this is just the end of the

519
00:57:58,960 --> 00:58:04,080
beginning. We've, we figured a few things out. There are some things that work, but they're

520
00:58:04,080 --> 00:58:10,160
still producing not quite useful and definitely dangerous machines. There is room for improvement.

521
00:58:10,160 --> 00:58:15,120
And there's nothing that says that only Google, Google with all its resources is going to be the

522
00:58:15,120 --> 00:58:20,720
one that can figure out these improvements. Think differently, try some of these alternative

523
00:58:20,720 --> 00:58:28,800
approaches and maybe you will be the one, you know, that comes up with the answer, whatever it is.

524
00:58:29,840 --> 00:58:36,240
Cool. Good luck. Awesome. Thank you, Josh. Really appreciate it. Thanks for having me.

525
00:58:36,240 --> 00:58:42,000
Yeah. And until next time. Thank you. Bye.

526
00:58:50,720 --> 00:58:52,100
you

527
00:59:20,720 --> 00:59:22,100
you

