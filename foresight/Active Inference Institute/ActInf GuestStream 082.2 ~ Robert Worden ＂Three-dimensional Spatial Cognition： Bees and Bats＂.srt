1
00:00:00,000 --> 00:00:16,240
Hello and welcome. It is June 3, 2024. We're in active inference guest stream 82.2 with

2
00:00:16,240 --> 00:00:22,880
Robert Warden. Today we're going to be discussing three dimensional spatial cognition, bees

3
00:00:22,880 --> 00:00:29,000
and bats. So thank you Robert for joining again. To you for the presentation and looking

4
00:00:29,000 --> 00:00:35,000
forward to it. Thanks Dan. Okay, well as Dan said, I'm talking about three dimensional

5
00:00:35,000 --> 00:00:42,320
spatial cognition in small animals, particularly like bees and bats for examples. And what I'm

6
00:00:42,320 --> 00:00:47,400
going to be doing is showing you a demonstration program that does this. And so you can find

7
00:00:47,400 --> 00:00:51,840
the demonstration program at this link at the bottom of the picture, and you can download

8
00:00:51,840 --> 00:00:56,560
it and try it yourself. Or you can read a couple of papers about this work, which are

9
00:00:56,560 --> 00:01:06,360
there an archive at that address. So that is the introduction to get straight into it.

10
00:01:06,360 --> 00:01:12,000
What this work represents, I think is a challenge for classical neuroscience. And by classical

11
00:01:12,000 --> 00:01:18,040
neuroscience, I mean the assumption that all that happens in the brain or cognition is

12
00:01:18,040 --> 00:01:23,480
done by neurons connecting to each other by synapses and so on. And the challenge, which

13
00:01:23,480 --> 00:01:28,800
I think comes out of this work, is that the main result is that neurons are actually not

14
00:01:28,800 --> 00:01:34,240
capable of that. They cannot represent three dimensional space, because they're too imprecise

15
00:01:34,240 --> 00:01:39,880
and too slow. So the resulting challenge for neuroscience is to show that this idea is

16
00:01:39,880 --> 00:01:45,560
wrong. Everybody thinks it's wrong. Everybody thinks neurons do everything. So you have

17
00:01:45,560 --> 00:01:52,240
to show it's wrong by building a working model of neural computation model of 3d space and

18
00:01:52,280 --> 00:01:58,800
checking that it really scales and can perform somewhat like animals perform. I think just

19
00:01:58,800 --> 00:02:02,480
writing papers and talking about it is not enough. You've got to build a model and show

20
00:02:02,480 --> 00:02:10,240
it works. And for building that model, the FEP neural process model is the best starting

21
00:02:10,240 --> 00:02:20,520
point I believe. So that's where this talk is going. It gets there by 3d spatial cognition.

22
00:02:20,520 --> 00:02:29,040
So what is that? Spatial cognition, as first say, is a very important problem. The primary

23
00:02:29,040 --> 00:02:34,080
task of any animal brain is to control its movements, physical movements, its limbs,

24
00:02:34,080 --> 00:02:40,280
in 3d. And that's a 3d problem. And it has to do that at all times of the day. And for

25
00:02:40,280 --> 00:02:46,640
most animals, most of their brain is devoted to this problem. And we believe, being Bayesian,

26
00:02:46,800 --> 00:02:55,960
they do this by building and using Bayesian maximum likelihood model of 3d spatial space. And my

27
00:02:55,960 --> 00:03:01,240
previous live stream was about the subject how animals build models in general. But the

28
00:03:01,240 --> 00:03:06,920
particular 3d model of space, that's been important since the Cambrian era, when animals

29
00:03:06,920 --> 00:03:14,800
first started having precise sense data, like good eyes and capable limbs. And that there's

30
00:03:14,840 --> 00:03:21,760
been huge and sustained selection pressure on all animal species since then, to do it well. And

31
00:03:21,760 --> 00:03:27,240
what we believe is that animals do do it rather well. For instance, our own conscious awareness

32
00:03:27,240 --> 00:03:33,120
of 3d space must come from something going on in our brains. And that is a rather precise model

33
00:03:33,120 --> 00:03:39,160
in our conscious awareness of 3d space around us. So that must be quite a good model of space

34
00:03:39,280 --> 00:03:45,240
in our heads. And going from us to small insects, even the small insect can land very

35
00:03:45,240 --> 00:03:52,240
skillfully on the room of a coffee cup or any other surface. So that's why I say modeling

36
00:03:52,240 --> 00:03:59,640
3d spatial cognition is the top priority for neuroscience. And we may look at all sorts of

37
00:03:59,640 --> 00:04:05,480
problems in neuroscience. This is the one, this is the hard part of the problem, the thing we

38
00:04:05,480 --> 00:04:11,800
really need to get right. So how do you do it? How do animals build a 3d model of local space

39
00:04:11,800 --> 00:04:17,560
around them? And the immediate problem is that most of their sense data in their vision is

40
00:04:17,560 --> 00:04:22,720
two dimensional. So how do you get from two dimensional vision to three dimensional model of

41
00:04:22,720 --> 00:04:28,840
space? There are some constraints and people obviously think of stereopsis with two eyes,

42
00:04:28,840 --> 00:04:34,360
where you can tell the depth of things or proprioception and touch. But those constraints

43
00:04:34,840 --> 00:04:40,280
only apply to restricted regions of the space around an animal. So for the rest of it,

44
00:04:41,240 --> 00:04:46,120
what I believe animals do is they build a model of the space around them by moving in space. And

45
00:04:46,120 --> 00:04:51,640
this is a form of active entrance, if you like, that you have to move in a moving space to find

46
00:04:51,640 --> 00:05:00,040
out about space. And this is based on a very strong Bayesian probability that as an animal's move,

47
00:05:00,760 --> 00:05:06,600
most of the things around it do not move. So the world is like a big rigid moving body

48
00:05:06,600 --> 00:05:12,120
around the animal. So the animal can compute object locations by what is called structure from

49
00:05:12,120 --> 00:05:18,120
motion, SFM. And when you build a computational model of this, it's actually fairly simple

50
00:05:18,120 --> 00:05:26,280
computation to do. What you can do is fairly simple 3d matrix operations to maximize the likelihood

51
00:05:26,360 --> 00:05:29,880
of an object being in a certain position. And that's what this program does.

52
00:05:30,680 --> 00:05:36,920
But if you're doing that, shape from motion, structure from motion, it requires a short

53
00:05:36,920 --> 00:05:42,120
term spatial memory of working memory for positions of things. And that's what this

54
00:05:42,120 --> 00:05:47,400
demonstration program does. So the demonstration program

55
00:05:48,360 --> 00:05:54,680
that gives them both the echo delay and it gives them a Doppler shift. And we'll talk about that

56
00:05:54,680 --> 00:06:01,320
later. So both of those animals move fast among static objects. So this 3d shape from motion

57
00:06:01,320 --> 00:06:05,320
is an applicable way of working out where the objects are. So we're going to go ahead and

58
00:06:05,320 --> 00:06:10,520
move on to the next slide. So we're going to go ahead and move on to the next slide.

59
00:06:10,840 --> 00:06:16,520
So this 3d shape from motion is an applicable way of working out where the objects are.

60
00:06:17,640 --> 00:06:21,320
So the program we're going to show you builds a 3d model of space in three

61
00:06:22,120 --> 00:06:28,440
different ways. Firstly, it can build an optimal Bayesian shape from model model model.

62
00:06:29,080 --> 00:06:36,920
That is what I call a brute force calculation. I've discussed those in my previous live stream.

63
00:06:37,640 --> 00:06:42,040
And it's very hard for animals to do. It's not the way we think animals do it. But

64
00:06:42,040 --> 00:06:46,520
the interesting thing about it is it gives you the very best possible Bayesian model

65
00:06:47,320 --> 00:06:53,640
based on the sense data. So that's the first way. The second way is by dynamical object tracking

66
00:06:53,640 --> 00:06:59,800
where an animal makes an estimate of where each object is in three dimensions. And then it keeps

67
00:06:59,800 --> 00:07:05,480
updating their estimate every time, from every step along the track it takes. It updates that

68
00:07:05,480 --> 00:07:14,360
estimate from its sense data. And the third model is doing that same object tracking,

69
00:07:14,360 --> 00:07:20,440
but doing it in the presence of neural memory noise. I should say this computational model is

70
00:07:20,440 --> 00:07:26,520
built at Mars level, David Mars level two. That is, it's not a neural implementation,

71
00:07:26,520 --> 00:07:33,880
but I have made it so that you can add simulated neural noise to it. So if you want to see this

72
00:07:33,880 --> 00:07:40,680
program after you've seen demonstrated, download it from this web address and you can

73
00:07:40,680 --> 00:07:48,440
quite simply unzip it and start it running. So I will now switch to demonstrating this program.

74
00:07:49,960 --> 00:07:58,360
So I end the show and there is the program. That's what it looks like. And what you see is

75
00:07:58,360 --> 00:08:04,200
three different windows, the left center and right. The left hand window is going to be a

76
00:08:04,200 --> 00:08:10,520
three dimensional view of some space in which a B or a bat is moving. The center view is always just

77
00:08:10,520 --> 00:08:16,920
help information and it tries to tell you how to use all these sliders and buttons and controls.

78
00:08:16,920 --> 00:08:22,920
And the right hand view is just various graphs and we'll see some of those as we go along.

79
00:08:22,920 --> 00:08:29,960
So what happens when you press the start button is you see some three dimensional space and inside

80
00:08:29,960 --> 00:08:39,080
it there on the left hand side is an animal this time of B and the colored circles are objects

81
00:08:39,080 --> 00:08:46,600
randomly spaced in that space. And so the line is going from the B to the objects are lines of

82
00:08:46,600 --> 00:08:52,760
sight. And this is a three dimensional view so you can rotate it, see the three dimensions and

83
00:08:52,760 --> 00:08:59,480
that's what happens when you rotate the objects rotate. And so there at the moment we've got

84
00:08:59,480 --> 00:09:05,320
nothing about the B's internal model of space. We've only got actual space itself shown in the

85
00:09:05,320 --> 00:09:11,960
view. In order to show the B's internal model of space we press the run button and I'll do this

86
00:09:11,960 --> 00:09:17,240
and you can see what happens. So as you press run the B starts moving that's the green line.

87
00:09:17,800 --> 00:09:23,800
It gets new lines of sight and from the new lines of sight it estimates the positions of all those

88
00:09:23,800 --> 00:09:32,680
objects. So I'll restart and do that again. And what you'll see this time is that the estimates

89
00:09:32,680 --> 00:09:40,120
of the positions appear as small circles with error bars. So the error bars are the gray lines

90
00:09:40,120 --> 00:09:45,320
the small circles are where the B thinks the object is. So you can see the B is building

91
00:09:45,320 --> 00:09:51,400
rather an accurate model of where the objects are from its sense data. I'll restart again and this

92
00:09:51,400 --> 00:09:59,080
time we'll step through it one step at a time to see how the B's model of space evolves with time.

93
00:09:59,080 --> 00:10:06,280
So one step and you can see in the very early steps we'll rotate it a bit to see show what's

94
00:10:06,280 --> 00:10:11,960
going on. The B starts making really quite good estimates of where the objects are.

95
00:10:12,920 --> 00:10:19,000
Each little white circle is quite close to the blue circle but the estimates have error bars

96
00:10:19,000 --> 00:10:24,120
and the error bars are in three dimensions showing the uncertainty of the location estimate in all

97
00:10:24,120 --> 00:10:30,920
those dimensions. So those estimates of position that enable the B to move where it wants to do

98
00:10:31,000 --> 00:10:35,160
suppose these are flowers it can go to the flowers and get pollen or whatever it wants to do.

99
00:10:36,120 --> 00:10:42,280
Now I said the program computes three different kinds of model and these three is always doing

100
00:10:42,280 --> 00:10:48,200
this as you go through the steps on the track and these three models are a full Bayesian model

101
00:10:48,200 --> 00:10:53,480
and that's what we're showing at the moment full Bayesian. We can switch to a tracking model

102
00:10:54,200 --> 00:11:01,960
that's the object dynamic tracking and there if I switch that you can see the error bars and the

103
00:11:01,960 --> 00:11:08,600
objects hardly move at all they do move a little bit but this is one result of the program that

104
00:11:08,600 --> 00:11:15,160
doing that tracking model which is simpler to do than the full Bayesian model gives you nearly the

105
00:11:15,160 --> 00:11:22,840
same estimates and same error bars and the third model it computes is a tracking model with noise

106
00:11:22,920 --> 00:11:30,600
and I switch to this one and again it's not moving very much actually but I'll move it on a bit and

107
00:11:30,600 --> 00:11:37,800
you'll see that noisy tracking often is very different from tracking without noise so we're

108
00:11:37,800 --> 00:11:46,680
on tracking without any noise at the moment we step forward a bit and the B keeps updating its

109
00:11:46,680 --> 00:11:52,920
estimates of these positions as they go and I now switch from tracking to noisy tracking

110
00:11:54,040 --> 00:11:59,880
and you can see the noisy tracking is significantly worse than tracking they noisy

111
00:11:59,880 --> 00:12:06,520
estimates have drifted away from the true positions of the object and that is really the second

112
00:12:08,600 --> 00:12:13,880
by the way I'm running this with a fairly small level of neural noise at the moment you can adjust

113
00:12:13,880 --> 00:12:19,080
the level of neural noise you're going to adjust the bees visual acuity you adjust all sorts of

114
00:12:19,080 --> 00:12:24,680
things using these sliders and try running the program again this is running with a fairly small

115
00:12:24,680 --> 00:12:35,400
level of noise so I keep stepping and the noisy tracking estimate keeps getting worse but I go

116
00:12:35,400 --> 00:12:42,360
back to the tracking estimate the tracking estimate without neural noise is pretty much dead on so

117
00:12:43,320 --> 00:12:50,440
I'll restart again and I'll run again because then we can show the graph at the end of the run

118
00:12:50,440 --> 00:12:54,920
which shows what's happened to these errors so if I run again

119
00:12:55,720 --> 00:13:12,840
now you look at the graph on the right and what that is doing is comparing tracking which is the

120
00:13:12,840 --> 00:13:20,440
black curve versus noisy tracking which is the red curve and these are steps along the bees

121
00:13:20,440 --> 00:13:26,920
track those are the same steps we saw there but these vertical axis is the level of error in the

122
00:13:26,920 --> 00:13:33,720
depth of the bee estimates and you can see that tracking is rather small errors it homes in on

123
00:13:33,720 --> 00:13:39,480
true position of the object whereas with memory noise you get much bigger errors and in fact the

124
00:13:39,480 --> 00:13:47,960
errors are very unpredictable if you rerun we'll just rerun it again you find that the errors

125
00:13:47,960 --> 00:13:54,440
coming from noisy tracking they do seem to vary quite a lot from one run to the next noisy tracking

126
00:13:54,440 --> 00:13:59,240
is pretty unpredictable basically so there the errors have gone right up and they've come down

127
00:13:59,240 --> 00:14:05,560
again whereas ordinary tracking without noise is nice convergence towards the true positions

128
00:14:06,440 --> 00:14:14,120
so that is really the second key result of the simulation that is that a tracking model is a

129
00:14:14,120 --> 00:14:20,920
realistic model of how animals estimate positions of things around them but if you add noise to it

130
00:14:20,920 --> 00:14:25,960
even adding a small amount of noise completely messes up the tracking model now there are more

131
00:14:25,960 --> 00:14:32,840
things you can show with this model you can show for instance how animals use their model to detect

132
00:14:32,840 --> 00:14:38,760
what is moving around them because obviously when an animal is moving it is quite hard for it to

133
00:14:38,840 --> 00:14:44,600
detect motion from its visual field because when it's moving everything is moving in this

134
00:14:44,600 --> 00:14:51,240
visual field so it has to use the three dimensional model to work out what is actually moving and

135
00:14:51,240 --> 00:14:58,680
if you also plot the efficiency of motion detection you find that too is very much poorer

136
00:14:58,680 --> 00:15:06,760
when you add memory noise so I think I'll stop at this point the program also does a simulation

137
00:15:06,760 --> 00:15:11,720
of bats but I won't step straight into that perhaps we could come back to that at the end of the talk

138
00:15:11,720 --> 00:15:18,680
if somebody wants to hear about it but for the moment let's just step back and find what we

139
00:15:18,680 --> 00:15:31,800
have concluded from the bee's model now where is my presentation now I've got to resume the

140
00:15:31,800 --> 00:15:38,520
presentation somehow how do I do that yeah you get back to the presentation

141
00:15:44,600 --> 00:15:50,680
so the key points that I think I may have shown you is that in the 3d view you can see the track

142
00:15:50,680 --> 00:15:55,560
of the animal you can see it's the lines of sight you can see sorry my phone is ringing I better

143
00:15:55,560 --> 00:16:03,320
go and shoot it up

144
00:16:15,160 --> 00:16:19,720
I've shown you the three I've shown you where the real objects are those are the circles the

145
00:16:19,800 --> 00:16:26,600
colored circles shown you the objects as located in the internal model I've shown you shape from

146
00:16:26,600 --> 00:16:33,640
motion I've shown you what the error bars are and you can rotate the 3d view I've shown you

147
00:16:33,640 --> 00:16:38,920
the three different models of 3d space the full Bayesian model the dynamic object tracking model

148
00:16:38,920 --> 00:16:47,320
and the tracking model with memory errors as I say when you run this program you can change

149
00:16:47,320 --> 00:16:54,040
all sorts of parameters to see how it's sensitive to the parameters and I've shown you the bee's

150
00:16:54,040 --> 00:16:59,080
spatial model I've shown you how the error bars are particularly the depth dimension

151
00:16:59,880 --> 00:17:10,360
and I've shown you how memory noise degrades the model so here are the key results so the best

152
00:17:10,360 --> 00:17:16,520
possible model and any animal could build from its sense is a very good model in fact it's more

153
00:17:16,520 --> 00:17:25,240
precise than the sense data because if the animal can assume that objects don't move then over time

154
00:17:25,240 --> 00:17:30,200
an animal can build up a very good understanding of where objects are better than its raw sense data

155
00:17:31,400 --> 00:17:37,480
animals can't do any better than that but dynamical object tracking works pretty well it's almost as

156
00:17:37,480 --> 00:17:44,840
good as the full Bayesian model and it only works if spatial memory has very high precision and I

157
00:17:44,840 --> 00:17:52,120
didn't say the levels of precision that I put into the program which start to spoil the tracking model

158
00:17:52,120 --> 00:17:59,560
they're about one part in a hundred and that I believe is much more precise the most neural

159
00:17:59,560 --> 00:18:05,960
representation of space can give so that's the next part of this talk how do we do that modeling

160
00:18:05,960 --> 00:18:13,720
with a neural model of the brain the classical neural model so how do you build a neural spatial

161
00:18:13,720 --> 00:18:21,880
memory and this is a changed quote from Animal Farm where they said two legs bad four legs good

162
00:18:21,880 --> 00:18:27,480
two dimensions is easy three dimensions is hard because two dimensions you can easily do a sheet

163
00:18:27,480 --> 00:18:33,000
of neurons representing two dimensions whereas in three dimensions you don't have that option

164
00:18:33,560 --> 00:18:38,680
and there are several possible memory designs you can have a two-dimensional sheet of neurons

165
00:18:38,680 --> 00:18:44,840
and you can represent the third dimension by depth depth by the third yeah you can have some other

166
00:18:44,840 --> 00:18:50,040
variable representing depth or you can have a three-dimensional clump of neurons where

167
00:18:50,040 --> 00:18:55,480
position in the clump represents a 3d position or you can represent all three dimensions of

168
00:18:55,480 --> 00:19:01,880
an object position by neural firing rates that none of these work well for object tracking I think

169
00:19:01,880 --> 00:19:08,920
we can quite simply eliminate the 3d clump model but the other two models have a problem with neural

170
00:19:08,920 --> 00:19:14,840
error rates and if neural information is encoded as firing rates of neurons

171
00:19:19,560 --> 00:19:27,560
typically you have a neuron firing n times in some time interval t and then the precision

172
00:19:27,560 --> 00:19:33,960
with which it can represent some real quantity is of the order of one part in square root of n

173
00:19:34,840 --> 00:19:41,800
now n over t is typically five or between five and fifty pulses per second on most animal brains

174
00:19:42,920 --> 00:19:49,880
but for insects and small mammals the time they've got to have to update their internal model of

175
00:19:49,880 --> 00:19:56,120
space is very small typically less than a tenth of a second so if the time there's a tenth of a second

176
00:19:56,120 --> 00:20:03,160
then you get n less than 10 and that gives you errors of the order of 30 percent which are much

177
00:20:03,160 --> 00:20:09,880
bigger than the 1 percent errors which I said are needed for tracking structure from motion

178
00:20:10,920 --> 00:20:17,000
so the conclusion of this is that the neurons when they represent space there's a trade-off

179
00:20:17,000 --> 00:20:23,720
between speed and precision faster you have the less precise it is and this trade-off is just too

180
00:20:23,720 --> 00:20:34,360
hard so I believe there is no working neural model of 3d spatial cognition now the three

181
00:20:34,360 --> 00:20:41,000
the three points in blue I've said before they say spatial cognition is very important and animals

182
00:20:41,000 --> 00:20:47,560
do it well and we've had this problem for a long time in his book vision 40 years ago

183
00:20:47,560 --> 00:20:52,760
David Ma identified the challenge and he started work on it he defined what he called a two and

184
00:20:52,760 --> 00:20:59,320
a half d sketch and various other models now I believe that in terms of building neural models

185
00:20:59,320 --> 00:21:04,360
of how spatial cognition works people have really not moved beyond this

186
00:21:06,760 --> 00:21:11,320
why I think the main reason is that the memory problem is just too hard that memory gives them

187
00:21:11,320 --> 00:21:17,000
two big errors and is too slow and I suspect that over the years there have been many people who look

188
00:21:17,000 --> 00:21:24,120
at this problem and they decide to move on and do something else instead but the result is that

189
00:21:25,560 --> 00:21:31,080
spatial cognition is the central problem of neuroscience we don't have a model of it and so

190
00:21:31,080 --> 00:21:37,960
this is rather like theory of planetary motion without a sun or a theory of the atom with no nucleus

191
00:21:38,840 --> 00:21:48,040
so how does this relate to active vision I think active vision is one way to explore this problem

192
00:21:48,920 --> 00:21:56,040
active vision describes how a 3d spatial model can be inferred from vision and there are quite a

193
00:21:56,040 --> 00:22:01,640
few papers on it they focused on various aspects of it they focused on 2d scene classification

194
00:22:02,120 --> 00:22:08,200
they focused on the trade-offs between various objectives like the choice of visual saccades

195
00:22:08,840 --> 00:22:14,120
they focused on 3d robotics as far as I know none of them have really focused on how

196
00:22:14,120 --> 00:22:22,040
animal brains practically build a 3d model and I believe that existing active vision models

197
00:22:22,040 --> 00:22:28,520
do not address the issue of neural error rates one reason for this is that the standard active

198
00:22:28,600 --> 00:22:35,640
inference toolkit in MATLAB I believe it doesn't model neural error rates it assumes I believe

199
00:22:35,640 --> 00:22:43,400
an abstract perfect neuron with very precise representation of quantities and error rates

200
00:22:43,400 --> 00:22:48,120
are actually not an issue for many of these applications they're not an issue for robotics

201
00:22:48,760 --> 00:22:57,080
and they're not an issue really for making discrete choices but as I've said in this talk so far

202
00:22:57,080 --> 00:23:02,680
the accuracy with 3d model really matters and neural error rates are the big problem

203
00:23:04,280 --> 00:23:11,560
if we set that problem on one side for a moment there is the issue of active inference trade-offs

204
00:23:11,560 --> 00:23:18,840
and there are many interesting trade-offs you can examine in active vision and the key trade-off I

205
00:23:18,840 --> 00:23:26,760
believe is one between freezing and moving as I've shown in the demonstration the animal

206
00:23:26,760 --> 00:23:33,160
has to move by has to move in order to infer the 3d positions things around it by shape from motion

207
00:23:33,160 --> 00:23:37,800
it also of course has to move to achieve practical goals like feeding and fleeing and mating and

208
00:23:37,800 --> 00:23:45,560
so on on the island it can freeze and freezing it may conserve energy it may be able to detect

209
00:23:45,560 --> 00:23:51,480
what's moving simply directly from its visual field which is much easier and it may itself

210
00:23:51,480 --> 00:23:57,480
avoid detection so these are very key trade-offs they're absolutely essential for lifetime fitness

211
00:23:57,480 --> 00:24:04,520
for many animals animals have to make this trade-off or these trade-offs any moment of the day

212
00:24:04,520 --> 00:24:10,600
and so we've got plenty of empirical data about it and I think it'll be a very useful area to explore

213
00:24:10,840 --> 00:24:19,320
now I'm going to switch to something completely different having said that neural storage

214
00:24:19,320 --> 00:24:26,040
of spatial positions is a very hard problem I'm going to talk about an alternative possible

215
00:24:26,040 --> 00:24:36,040
alternative way of storing spatial data and so if you assume there's a round some round region

216
00:24:36,040 --> 00:24:44,200
inside the brain of a fairly large diameter d and this holds waves with a minimum wavelength

217
00:24:44,200 --> 00:24:50,840
which are called lambda and the neurons can couple to the waves both as transmitters and receivers

218
00:24:50,840 --> 00:24:57,400
and the wave can persist at least for fractions of a second so the wave can act as a working memory

219
00:24:57,400 --> 00:25:03,960
for positions and the number of object positions you can store in the wave can be up to d over lambda

220
00:25:03,960 --> 00:25:10,280
cube and that's a can be a very large number the spatial precision which one object position

221
00:25:10,280 --> 00:25:17,560
is stored can be one part in d over lambda and I think that d over lambda can be very large so

222
00:25:17,560 --> 00:25:23,720
you can easily get precision better than one part in 100 which is what you need to build the spatial

223
00:25:23,720 --> 00:25:31,160
model so in summary wave storage of 3d positions may have a lot of computational benefits you can

224
00:25:31,160 --> 00:25:37,880
give a natural fit to the problem it can give high precision and high capability it can give you

225
00:25:37,880 --> 00:25:43,720
very fast response times low spatial distortion and some other benefits which are described in the

226
00:25:43,720 --> 00:25:52,440
papers so apart from its computational benefits is there any evidence for wave storage in the brain

227
00:25:54,760 --> 00:25:59,320
I believe there are two quite powerful lines of evidence one of which comes from the insect

228
00:25:59,320 --> 00:26:05,240
central body the central body of the insect brain is a very small part of the brain in the middle of

229
00:26:05,240 --> 00:26:11,560
it and it consists of a fan shaped body and the elliptical body and it has this shape which is

230
00:26:11,560 --> 00:26:17,960
remarkably well conserved across all insect species and there's an insect brain database and I've

231
00:26:17,960 --> 00:26:23,160
gone to the insect's brain database and pulled from it the shapes of the central body from a few

232
00:26:23,160 --> 00:26:29,240
typical insect species and here you can see the fan shaped body and the elliptical body and it's

233
00:26:29,240 --> 00:26:36,280
very constant across all kinds of insects and you can see it's approximately a round shape so it's

234
00:26:36,280 --> 00:26:45,640
well suited to hold three dimensional wave and it does multi-sensory integration and so it's quite

235
00:26:45,640 --> 00:26:51,560
likely quite probable that it holds spatial positions and insects have very few neurons

236
00:26:51,560 --> 00:26:58,840
in their brain to do it in any other way and what I think is significant is how constant and round

237
00:26:58,840 --> 00:27:03,000
the insect central body is compared with all the other parts of the insect brain

238
00:27:04,840 --> 00:27:09,720
so that's one piece of evidence from the insect central body the other piece of evidence comes

239
00:27:09,720 --> 00:27:17,880
from the mammalian thalamus as you may know the thalamus of most mammals all animals is

240
00:27:17,880 --> 00:27:24,680
approximately spherical and is connected to all sense data and all cortical reasons

241
00:27:25,880 --> 00:27:28,360
but the important thing is that the shape of the thalamus

242
00:27:31,400 --> 00:27:38,840
is highly conserved across all species and there's an important aspect of the thalamus anatomy

243
00:27:39,800 --> 00:27:46,920
that unless you assume a wave really it doesn't make sense because the thalamus consists of a

244
00:27:46,920 --> 00:27:53,480
number of independent nuclei like the pulvinar and the lgn and so on and so forth and the connections

245
00:27:53,480 --> 00:28:01,800
across within the thalamus between these nuclei are very weak or even non-existent so you could

246
00:28:01,800 --> 00:28:09,080
have this picture here that the thumb where's my pointer here's my pointer the thalamic

247
00:28:09,080 --> 00:28:16,120
nuclei which do have white circles here they all connect in two ways the cortex but they don't

248
00:28:16,120 --> 00:28:23,800
connect to each other so one thalamic nucleus here could easily start moving out to towards the

249
00:28:23,800 --> 00:28:31,640
cortex and the distance the length of its axons could decrease and its other connections it doesn't

250
00:28:31,640 --> 00:28:38,440
need other connections so all the nuclei could migrate outward towards the cortex and you could

251
00:28:38,440 --> 00:28:48,760
still have the same neural synaptic connectivity and the same computational capability if neurons

252
00:28:48,760 --> 00:28:57,640
only compute by synaptic computation so this way you could save a lot of energy in shorter axon

253
00:28:57,640 --> 00:29:04,760
lengths so in summary a compact thalamus and it makes sense if all the nuclei need to be immersed

254
00:29:04,760 --> 00:29:13,320
in the same wave so we now have three pieces of evidence for a wave in the brain firstly there's

255
00:29:13,320 --> 00:29:19,480
the computational neuroscience that it's a very difficult problem to build a 3d model about it

256
00:29:20,040 --> 00:29:25,000
but you can build a 3d spatial model if you assume there's a wave storing positions

257
00:29:26,440 --> 00:29:32,600
secondly the insect's central body is nearly round in all insects very well suited to hold a

258
00:29:32,600 --> 00:29:38,920
wave and it appears to be in the right part of the brain to do that and thirdly the mammalian

259
00:29:38,920 --> 00:29:46,120
thalamus which again has this round shape very well suited to hold a wave and the important thing

260
00:29:46,120 --> 00:29:54,360
here is that without a wave the anatomy of the thalamus doesn't make sense so I would like you

261
00:29:54,360 --> 00:30:00,120
if you remember only one thing about this talk remember this slide there is quite a lot of evidence

262
00:30:00,120 --> 00:30:08,520
for a wave in the brain one thing I will say is that the wave is probably not an electromagnetic

263
00:30:08,520 --> 00:30:14,600
wave because there's quite a lot of interest in electromagnetic fields in the wave from researchers

264
00:30:14,600 --> 00:30:22,280
like Miller and McFadden and so on but electromagnetic field can't play the role that this wave is

265
00:30:22,280 --> 00:30:28,280
supposed to is needed to play in other words the key thing that this the wave is supposed to do in

266
00:30:28,280 --> 00:30:34,840
this model is to store information of fraction of a second but an electromagnetic field in the

267
00:30:34,840 --> 00:30:39,560
wave and there certainly are electric in the met in the brain there certainly are electromagnetic

268
00:30:39,560 --> 00:30:45,720
fields in the brain they cannot store information of fractions of a second and they cannot represent

269
00:30:45,720 --> 00:30:51,480
3d space like a holibra and just to say a little more about this if there's an electromagnetic

270
00:30:51,560 --> 00:30:57,720
wave in the brain it has to obey Maxwell's equation so the wavelength times the frequency

271
00:30:57,720 --> 00:31:05,560
is equal to the speed of light lambda f equals c and that means that 40 hertz typical frequencies

272
00:31:05,560 --> 00:31:13,720
of these waves the wavelength is 8000 kilometers as large as half the earth so it's the conclusion

273
00:31:13,720 --> 00:31:21,240
is that at 40 hertz electromagnetic field is not a wave it's a static field and is driven entirely

274
00:31:21,240 --> 00:31:27,400
by neuron firing so it doesn't store the information so in summary we're looking for something not

275
00:31:27,400 --> 00:31:34,200
electromagnetic and possibly some quantized excitation something a bit exotic um in the

276
00:31:34,200 --> 00:31:39,800
field of quantum biology i think we shouldn't despair here because we know evolution is a lot

277
00:31:39,800 --> 00:31:46,920
smarter than we are at discovering these things and exploding them so here are some take home

278
00:31:46,920 --> 00:31:54,360
questions does 3d spatial cognition use a wave in the brain in other words in the light of the

279
00:31:54,360 --> 00:32:00,280
evidence i've shown you what is the Bayesian probability of that hypothesis being true now i

280
00:32:00,280 --> 00:32:04,840
say these take home questions because i didn't expect you to have an answer immediately but

281
00:32:04,840 --> 00:32:09,720
perhaps you'd like to look at the papers and see what the evidence is and try and assess it in your

282
00:32:09,720 --> 00:32:15,560
own mind or do you know some slam dunk killer reason why they can't be a wave in the brain

283
00:32:15,560 --> 00:32:22,280
if you do know reason what is that reason and how do brains compute space how do neurons

284
00:32:22,280 --> 00:32:29,080
on their own represent 3d space with enough precision on the other hand if there might be

285
00:32:29,080 --> 00:32:34,600
a wave in the brain wouldn't that be a a rather exciting and revolutionary development it would

286
00:32:34,600 --> 00:32:39,240
actually change the neuroscience and it could address this central unsolved problem of how

287
00:32:39,320 --> 00:32:45,880
spatial cognition takes place so i believe that possibility should be explored particularly

288
00:32:45,880 --> 00:32:51,480
for young researchers this is attractive it's greenfield research it's not a well-trodden path

289
00:32:51,480 --> 00:32:57,640
of classical neuroscience the classical neuroscience model of maculic pits neurons and

290
00:32:58,200 --> 00:33:04,920
hebbian synapses that's 75 years old now so i would like to encourage people to get out and explore

291
00:33:05,000 --> 00:33:15,480
or again come back to the earlier slide here a crisis in neuroscience the result of this work

292
00:33:15,480 --> 00:33:22,360
i think is that neurons can't represent 3d space because they're too imprecise and too slow so

293
00:33:22,360 --> 00:33:29,880
the crisis is can you show this is wrong can you show it by building a working neural computational

294
00:33:29,880 --> 00:33:37,320
model and checking its scales properly so the fep neural process model is the starting point for

295
00:33:37,320 --> 00:33:43,160
that i think it's a good problem to work on because it is a crisis and big crisis big advances in

296
00:33:43,160 --> 00:33:51,160
science tend to come out of crises so what i'm advocating this is my last slide is a twin track

297
00:33:51,160 --> 00:33:58,840
research program to build two different active vision models of 3d spatial cognition one is a pure

298
00:33:58,840 --> 00:34:06,840
neural model which is a classic fep neural process model can this be made to work or are the neural

299
00:34:06,840 --> 00:34:12,520
memory errors going to kill it and secondly to try to build a hybrid wave and neural model

300
00:34:13,720 --> 00:34:18,600
and when you're building those models we can explore the trade-offs that active inference

301
00:34:18,600 --> 00:34:25,560
is so good at computing and particularly the trade-off between freezing and moving so

302
00:34:25,560 --> 00:34:31,560
there are a couple of candidate projects for the active inference institute okay that's it

303
00:34:35,800 --> 00:34:43,080
awesome thank you robert i have some questions and some people have asked questions in the

304
00:34:43,080 --> 00:34:51,560
live chat so i'll uh i'll ask them so first just while i'm recropping everything how would you

305
00:34:51,560 --> 00:34:57,960
connect this to the requirements equation earlier work because you mentioned that there was a

306
00:34:59,240 --> 00:35:05,720
requirements equation driven calibration of the optimal navigation so what does that look like

307
00:35:05,720 --> 00:35:13,720
to have the optimal navigation according to the requirements equation well to summarize on the

308
00:35:13,720 --> 00:35:22,440
requirements equation you can model how brains evolve and this is the previous live stream and

309
00:35:22,440 --> 00:35:29,880
you can show that they evolve towards making a purely Bayesian calculation of their best model

310
00:35:29,880 --> 00:35:38,440
of the world from the sense data but that purely Bayesian calculation is rather expensive and it's

311
00:35:38,440 --> 00:35:44,600
been well known in FEP that full Bayesian calculations is intractable for most animals

312
00:35:45,480 --> 00:35:51,880
and so that is a very expensive calculation and it's probably not the way animals do it but it

313
00:35:51,880 --> 00:35:58,680
is it can be done on digital computers and it can be done in this model i've showed you and the first

314
00:35:58,680 --> 00:36:05,640
model the full Bayesian model is actually computing the requirement equation from the bees or the bats

315
00:36:05,640 --> 00:36:13,000
sense data the second model a tracking model is a is an approximation to that which is a lot cheaper

316
00:36:13,000 --> 00:36:15,480
but seems to give very nearly the same results

317
00:36:19,560 --> 00:36:28,680
okay awesome let us dive into a few mammal and insect neuroanatomy questions so i'll start with

318
00:36:28,680 --> 00:36:34,840
the set of questions from the the live chat this is going to be about mammal neuroanatomy

319
00:36:35,640 --> 00:36:42,520
okay tim ritter asks do you assume this wave property for all phylimic nuclei primary and

320
00:36:42,520 --> 00:36:47,640
secondary or for specific ones e.g polvanar or mediodorsal

321
00:36:50,360 --> 00:36:56,200
very good question i don't know the answer i mean at this stage i believe the whole

322
00:36:56,200 --> 00:37:02,600
thalamus is around spherical near spherical volume with the wave going through all of it so

323
00:37:02,600 --> 00:37:08,280
they are all immersed in that same wave so even the polvanar the polvanar certainly is

324
00:37:08,280 --> 00:37:15,080
even the lgn which is rather small and is a pass through nucleus i think they all are so i think

325
00:37:17,400 --> 00:37:22,120
for instance i think people always say the thalamus is a relay

326
00:37:23,560 --> 00:37:30,040
sense data gets the cortex by account thalamus but people don't have a very good reason why

327
00:37:30,040 --> 00:37:35,960
it has to go through these relays nuclei in order to get there i think it's doing something

328
00:37:36,600 --> 00:37:42,840
about locating about i think the wave has some involvement there but this is very early days

329
00:37:42,840 --> 00:37:49,080
i don't know the answer at all okay another question from tim on mammalian neuroanatomy

330
00:37:50,200 --> 00:37:57,400
what about 2d orientation would you expect similar waves in hippocampal instead of thalamic

331
00:37:57,400 --> 00:38:06,040
regions or is 2d sufficiently easy to get by without yeah basically i believe 2d is easy enough

332
00:38:06,040 --> 00:38:11,880
to get by and the hippocampus is by no means suitable to hold a wave they all sorts hippocampi

333
00:38:11,880 --> 00:38:20,040
have all sorts of different shapes so yes i think that was a core theme between the mammal and

334
00:38:20,120 --> 00:38:29,240
insect areas is the conserved shape and then also the allometric differences over evolution

335
00:38:29,880 --> 00:38:37,960
where the size differential of the insect central body changes much less than other primary sensory

336
00:38:37,960 --> 00:38:47,000
regions and that was in your paper yeah that's right yeah and that kind of implies that that small

337
00:38:47,080 --> 00:38:52,760
size of the central body it's only a few percent of the whole insect brain seems to be enough

338
00:38:54,680 --> 00:39:03,240
yeah and that the properties which it hosts or enables might be related to its physical

339
00:39:03,240 --> 00:39:11,400
extent or like to its surface area it's a volume ratio and not a function like for example in the

340
00:39:12,360 --> 00:39:18,520
antennel lobe where the olfactory information are coming in there are these little glomeruli

341
00:39:18,520 --> 00:39:24,360
and different species have from several tens to several hundred of these olfactory glomeruli

342
00:39:24,360 --> 00:39:29,080
like ants have many and they have more olfactory receptors in their genome and they have more

343
00:39:30,120 --> 00:39:38,040
olfactory glomeruli in that region or insects with more compound eye sections they have larger

344
00:39:38,120 --> 00:39:47,080
optic lobes so the primary sensory regions have very large variation amongst species but then

345
00:39:47,080 --> 00:39:56,360
as you get into the central body you get much more conserved anatomy and size and then the

346
00:39:56,360 --> 00:40:00,840
mushroom body on the top part of the brain is something a little bit in between that might

347
00:40:00,840 --> 00:40:08,680
have more of an analogy to like mammalian cortex where there actually is the possibility to scale

348
00:40:08,680 --> 00:40:14,440
its cognitive capacities through size changes because it has some kind of like repetitive or

349
00:40:14,440 --> 00:40:23,560
stereotyped layout yeah I mean there are a load of fascinating questions in your anatomy which

350
00:40:23,560 --> 00:40:29,240
relate to this and and if you pursue the this hypothesis then there's all those interesting

351
00:40:29,240 --> 00:40:35,480
question I'm not an expert on insect or mammalian neuroanatomy but there's a load of interesting

352
00:40:35,480 --> 00:40:47,480
questions in there cool so about the bee spatial cognition so we know that bees use a variety of

353
00:40:47,480 --> 00:40:54,760
visual cues ranging from the landmark and the landscape recognition to polarization of light

354
00:40:54,760 --> 00:41:01,560
and so on and also as you pointed out the central body does multisensory integration so

355
00:41:02,680 --> 00:41:11,160
how do we think about the possibly complementary or redundant information provided by these

356
00:41:11,160 --> 00:41:17,400
different aspects of the visual fields and what does your simulation focus in on

357
00:41:17,960 --> 00:41:25,880
well I mean I believe that what the central body and the thalamus both do is multisensory

358
00:41:25,880 --> 00:41:32,760
integration in other words animals should they they need to make the best 3d model of space they

359
00:41:32,760 --> 00:41:39,000
can and they need to use all their sensitivities to use it apart from possibly smell that's an

360
00:41:39,000 --> 00:41:46,280
interesting question and so both of them do multisense integration and ideally one would

361
00:41:46,280 --> 00:41:52,040
put in a simulation one would have things like stereopsis one would have object recognition one

362
00:41:52,040 --> 00:41:59,400
would have light polarization all sorts of sources this program only does simple vision or simple

363
00:41:59,400 --> 00:42:05,640
echolocation at the moment but it should do all multisensory integration in in a single

364
00:42:06,040 --> 00:42:13,320
maximum likelihood model of the whole all sense data coming in at the moment

365
00:42:16,200 --> 00:42:22,120
interesting yeah with sounds or with smells it would be interesting to see how those come into

366
00:42:22,120 --> 00:42:30,200
play and and how do you think about in in the simulations presented here egocentric and allocentric

367
00:42:30,280 --> 00:42:37,000
navigation because you mentioned how the kind of simplifying assumption is that the world

368
00:42:37,000 --> 00:42:42,760
is a rigid fixed body so you can have these kind of duality where like I'm moving and the world is

369
00:42:42,760 --> 00:42:48,200
fixed and then there's sort of like I'm fixed and the world is moving so how does that relate

370
00:42:48,200 --> 00:42:56,760
to that to that egocentric allocentric distinction yeah very good question I mean I think the frame

371
00:42:56,760 --> 00:43:04,680
of reference used for the model should be as much allocentric as it can be because the wave has to

372
00:43:04,680 --> 00:43:11,400
persist and if the wave just persists it represents an object as a constant position so you want to

373
00:43:11,400 --> 00:43:17,880
have a frame of reference where most objects are at constant positions so I think that makes

374
00:43:17,880 --> 00:43:22,760
allocentric but obviously has to change from time to time every few seconds it has to switch

375
00:43:23,080 --> 00:43:26,120
because it can't just stay allocentric

376
00:43:30,760 --> 00:43:38,920
hmm interesting so now to connect that to what you brought up about move or stay that kind of

377
00:43:38,920 --> 00:43:48,360
fundamental uh animal or fundamental mobile organismal nervous system question I thought about

378
00:43:49,080 --> 00:43:58,840
different body plans where the eyes or the visual component are unable to move separately from the

379
00:43:58,840 --> 00:44:07,720
body like a bee can turn its body but it can't rotate its eyes whereas in humans for example we

380
00:44:07,720 --> 00:44:16,360
have optic saccade so there that stay or move yes we have turning our posture and moving through

381
00:44:16,360 --> 00:44:23,640
space but also we see like this microcosm where when the gaze is fixed there's high precision

382
00:44:24,360 --> 00:44:30,360
and then movement in the world is associated with movement of objects and then whereas when an

383
00:44:30,360 --> 00:44:37,560
eye saccade is dispatched during the saccade our visual attention is alleviated and then

384
00:44:37,560 --> 00:44:44,200
it's because during that time all the movement of pixels essentially is ascribed to the movement of

385
00:44:44,200 --> 00:44:50,760
the eye so we see kind of like a microcosm of the two modes of movement and stability

386
00:44:51,400 --> 00:44:58,600
in motion detection in the saccading but for other organisms that don't have eye saccade

387
00:44:59,240 --> 00:45:04,520
the only way that they can get that kind of alternating movement and stability is by moving

388
00:45:04,520 --> 00:45:13,960
their body yeah yeah I mean I always think of eye saccades as particularly predators if you like but

389
00:45:13,960 --> 00:45:20,840
want some high resolution in some direction some particular direction whereas for most insects

390
00:45:20,840 --> 00:45:28,920
as you say that there is not the option of a high resolution fovea but I think of saccades as being

391
00:45:28,920 --> 00:45:36,840
cheap there's I mean the freeze move trade-off is a real trade-off that if an animal moves

392
00:45:37,480 --> 00:45:45,000
it can be detected as moving and it can't detect motion itself as well as if it's stationary

393
00:45:45,560 --> 00:45:51,080
and so that's a real hard trade-off an animal has to make whereas saccades you can make them

394
00:45:51,080 --> 00:46:01,800
cheaply whenever you like yes yes and also it's really interesting like how often the behavioral

395
00:46:01,800 --> 00:46:06,760
studies just look at the direction of movement but there's this whole timing of movement and so

396
00:46:06,760 --> 00:46:15,000
there's definitely a lot of empirical studies about whether fear-based movements like in a predator

397
00:46:15,000 --> 00:46:24,440
prey or different kinds of movement choices where would you say attention comes into play

398
00:46:25,320 --> 00:46:30,600
in the sense that the bee or the bat was just kind of taking it all in it didn't have like some

399
00:46:30,600 --> 00:46:36,120
restricted scope so it's kind of like a uniform attention across objects and across space and

400
00:46:36,120 --> 00:46:44,360
time but then we know that we do have this visual attention phenomena well yeah attention is very

401
00:46:44,360 --> 00:46:53,400
important and I think naively it's a search time model of attention in other words the wave

402
00:46:53,400 --> 00:46:58,920
representation of all space represents all the space around an animal but the animal can focus

403
00:46:58,920 --> 00:47:08,040
attention on a region of the space and what that's doing is tuning the receptors in the thalamus so

404
00:47:08,040 --> 00:47:15,400
they are sensitive to wave vectors in a certain region so there's a whole load of issues there

405
00:47:15,400 --> 00:47:23,560
about how the wave works as to whether it can how signals are rooted from sensitive inputs to

406
00:47:23,560 --> 00:47:28,760
specialist regions of the cortex and I think attention is that rooting of information

407
00:47:29,720 --> 00:47:32,280
so again loads of big questions there

408
00:47:35,480 --> 00:47:42,840
yes with the way if I was kind of thinking about the insect brain visual input flowing in

409
00:47:43,400 --> 00:47:51,160
and also other potentially inputs and all of these are crashing on the shores of the central

410
00:47:51,160 --> 00:48:01,960
body and then there's this kind of stabilized dynamical wave representation such that information

411
00:48:01,960 --> 00:48:11,320
coming in differently changes the the the resting shape of the wave and then that opens up like you're

412
00:48:11,320 --> 00:48:18,520
now suggesting recurrent connections or or other connections into that wave hosting region

413
00:48:19,320 --> 00:48:26,680
and recurrent connections can modify the shape of the wave attentionally and then also the

414
00:48:26,680 --> 00:48:34,120
oh yeah the resting shape of the wave can route or augment or suppress other sensory information

415
00:48:34,120 --> 00:48:40,200
coming in that'd be like water kind of dumping to where there's already a high water point

416
00:48:40,760 --> 00:48:47,320
versus water going to where there's low water yeah yeah I'm I think key role of the wave is to

417
00:48:47,320 --> 00:48:55,160
persist a background model of all 3d space and then against that background model new

418
00:48:55,160 --> 00:49:03,000
central information that comes in particularly movement is best evaluated a piece of nuisance

419
00:49:03,000 --> 00:49:09,000
data you evaluate you much better if you compare and contrast it with what you had before that is

420
00:49:09,000 --> 00:49:17,000
attention and it's it's the foulness if you like telling the cortex here pay attention to here

421
00:49:17,000 --> 00:49:21,800
here's your old information from this place in space here's your new information so tell me what's

422
00:49:21,800 --> 00:49:30,520
changed well this connection with frame differencing is very powerful predictive processing predictive

423
00:49:30,520 --> 00:49:38,680
coding algorithms were built by computer scientists and in compression engineers looking to make

424
00:49:38,680 --> 00:49:42,840
video compression work and doing the frame differencing because that's the optimal way to

425
00:49:42,840 --> 00:49:48,840
compress video and then that got brought also back into neuroscience where there's a lot of

426
00:49:48,840 --> 00:49:54,280
focus on things like edge detection and these other 2d visual phenomena and then as you're

427
00:49:54,280 --> 00:50:02,200
pointing to there's this kind of sun at the solar systems center that's not really being discussed

428
00:50:02,200 --> 00:50:09,560
which is like okay yes we have neurons in different visual regions that are excitable by

429
00:50:09,560 --> 00:50:17,960
vertical lines by diagonal lines and so on but this is all flat phenomena and the question of

430
00:50:18,520 --> 00:50:24,920
not just shape recognition but the question that's most approximately relevant for movement

431
00:50:24,920 --> 00:50:30,280
and the fitness related decisions for the organism in the niche has to do with its

432
00:50:30,520 --> 00:50:39,960
spatial navigation not it's like eyesight at the eye doctor yeah

433
00:50:42,840 --> 00:50:48,280
not only is spatial navigation how it moves its limbs you know how where it puts it foot it's foot

434
00:50:48,280 --> 00:50:54,120
next that sort of thing and the 3d model i think does all of that

435
00:50:58,280 --> 00:51:01,000
okay i'll read a question from the live chat

436
00:51:02,840 --> 00:51:09,320
how might the cicade relate to a matrix of inputs versus a human based visual system

437
00:51:09,880 --> 00:51:13,080
movement on the matrix may give different spatial dynamics

438
00:51:13,080 --> 00:51:24,840
i'm not sure what we mean by matrix of inputs there but as you said during a cicade visual

439
00:51:24,840 --> 00:51:31,720
input is kind of blocked while the eye is getting from a to b whereas the wave persists and the 3d

440
00:51:31,720 --> 00:51:39,960
spatial model stays constant and after a cicade the eye has to update the 3d spatial model in

441
00:51:39,960 --> 00:51:47,880
some different place so um i'm not i don't think i've answered the question but perhaps you

442
00:51:49,560 --> 00:51:56,360
i think you understand my matrix it's what you enter um

443
00:51:58,600 --> 00:52:06,440
it's making me think about the experiments where the for example the fruit fly is placed on a ping

444
00:52:06,440 --> 00:52:14,520
pong ball in a harness in a virtual reality setting so it's getting custom visual input

445
00:52:14,520 --> 00:52:21,800
and its movements on the ping pong ball just kind of scrolls the ball so it's basically fixed but it

446
00:52:21,800 --> 00:52:31,320
gives a lot of degrees of experimental freedom around the um orienting of the body and what

447
00:52:31,320 --> 00:52:38,440
visual inputs it gets so i wonder if anywhere there we know about the the time

448
00:52:40,360 --> 00:52:49,880
the timescale of spatial orientation updating because that would be very critical to understand

449
00:52:49,880 --> 00:52:58,280
the nature of the wave but if it was something that was um for example closer to the diffusion rate

450
00:52:59,240 --> 00:53:08,280
of ions then we might be looking more towards like a channel or pore-based hypothesis if it was

451
00:53:08,280 --> 00:53:15,240
something that was faster than neural signaling it would suggest something more like the direct

452
00:53:15,240 --> 00:53:25,320
coupling or other kinds of of action so what what makes you feel as you suggested that it is not an

453
00:53:25,400 --> 00:53:35,480
electromagnetic stabilized wave field well as i say the physics i mean for electromagnetic

454
00:53:35,480 --> 00:53:42,520
wave we do understand the physics and the electromagnetic waves that measured by eeg for

455
00:53:42,520 --> 00:53:49,080
instance they are a purely passive consequence of neural activity they don't persist in the

456
00:53:49,080 --> 00:53:56,840
information for any time at all whereas this wave i'm talking about has to persist information

457
00:53:56,840 --> 00:54:02,840
for fractions of a second so this constant spatial model is kept persisted while the

458
00:54:02,840 --> 00:54:10,360
circuits go on on the animal moves while it computes shape from motion so a pure electric

459
00:54:10,360 --> 00:54:16,040
field we we know the physics it's Maxwell's equations and it does not store energy store

460
00:54:16,040 --> 00:54:22,600
information so it's purely a passive reflection of what's going on on the neurons it's not a memory

461
00:54:25,320 --> 00:54:34,520
hmm so the neurons are especially if we think about the several like thousand to tens of thousand

462
00:54:34,520 --> 00:54:43,880
let's say in the insect central body there's too few and they're too sparse and noisy to in a purely

463
00:54:43,880 --> 00:54:52,200
connectionist neural framework to support the kinds of empirical results that we see

464
00:54:52,920 --> 00:55:02,280
on the other hand a purely field-based approach has some issues that you just laid out so it's

465
00:55:02,280 --> 00:55:09,960
it's very interesting that to at least of the well-known mechanisms the local field potential

466
00:55:09,960 --> 00:55:16,920
and the firing rate rate coding type models that both of them seem to have some limitations

467
00:55:17,800 --> 00:55:26,120
and yet there's a very strong anatomical evidence for the functional role of that region

468
00:55:27,960 --> 00:55:37,960
oh yeah it's absolutely vital region but i believe that just looking at electric fields

469
00:55:37,960 --> 00:55:42,600
magnetic fields in the brain is not going to give you memory and that's the key thing

470
00:55:42,600 --> 00:55:48,120
that i think is needed to do structure from motion you've got to have short-term working memory

471
00:55:48,840 --> 00:55:52,600
to hold a little model of space for a fraction of a second

472
00:55:55,880 --> 00:56:02,520
do you see that as a kind of special type of short-term memory or do you think this is

473
00:56:03,480 --> 00:56:14,200
the same memory pool that like short-term audio memory gets entered into

474
00:56:15,960 --> 00:56:19,480
oh it's it's special it's different from that yeah definitely

475
00:56:21,560 --> 00:56:26,840
to say it starts from how do you can how do you conclude a 3d spatial model

476
00:56:26,840 --> 00:56:34,200
or how do you do structure from motion you need short-term working memory for that purpose specifically

477
00:56:40,360 --> 00:56:46,040
interesting um the kind that enables us to

478
00:56:46,920 --> 00:56:57,720
check for difference in in visually changing systems or what what does this visual working memory

479
00:56:59,240 --> 00:56:59,640
have

480
00:57:03,240 --> 00:57:13,000
well basically it's it's not vision because it's 3d and uh checking for difference in the visual field

481
00:57:13,960 --> 00:57:19,160
is you can do it quicker you can look directly at the visual field whereas this model i think

482
00:57:20,200 --> 00:57:29,400
the 3d model is um it's maintained by a loop between in mammals between the thalamus and the cortex

483
00:57:29,400 --> 00:57:36,520
and it's a 40 hertz cycle that maintains it so it takes time to build the 3d model

484
00:57:37,800 --> 00:57:41,640
and it's a bit slower than direct visual change detection

485
00:57:43,560 --> 00:57:51,800
interesting it's this tension with like visual being what is seen versus kind of a broader

486
00:57:52,760 --> 00:58:02,600
imagine the imagination of vision um what about action in your model so how were the paths set

487
00:58:03,960 --> 00:58:10,360
yeah i said that the model was a bit like active inference and the animal has to move in order to

488
00:58:10,360 --> 00:58:16,280
understand space but it's not really the program has not modeled the kind of active

489
00:58:16,280 --> 00:58:21,960
inference choices of how should i move to get the best understanding of space and you could do that

490
00:58:21,960 --> 00:58:28,120
you could make the b choose this trajectory to get the best understanding of where the flowers are

491
00:58:28,680 --> 00:58:33,960
or you make the b choose this trajectory for all sorts those are the active inference trade-offs

492
00:58:33,960 --> 00:58:39,080
that the program has not yet looked at and which can be looked at and i think are very interesting

493
00:58:39,800 --> 00:58:45,000
that's awesome yeah it's almost like the b in this situation it's like on a train

494
00:58:45,800 --> 00:58:51,880
trying to reduce its uncertainty about the location of landmarks but it's just on a it's

495
00:58:51,880 --> 00:58:59,080
on a rail it doesn't have policy decision whereas once we start to close that loop and ask which

496
00:58:59,080 --> 00:59:07,000
direction of movement or none given the costs would reduce uncertainty about resolving this

497
00:59:07,960 --> 00:59:15,080
kind of spatial relationship then that that's where the perception actions start to like come

498
00:59:15,080 --> 00:59:24,040
into play in benefit of each other and potentially there's several choice successive moves can greatly

499
00:59:24,040 --> 00:59:30,200
reduce uncertainty through active sampling just as we see in skating and all other situations

500
00:59:31,000 --> 00:59:38,120
and that would be like a heuristic or strategy that really does work

501
00:59:39,720 --> 00:59:47,000
yeah i mean another example that i use somewhere is i believe that predatory birds like hawks when

502
00:59:47,000 --> 00:59:50,840
they're approaching their target they don't go in a straight line they move on a curve

503
00:59:51,720 --> 00:59:53,560
to reduce the uncertainty

504
00:59:55,720 --> 01:00:01,800
so that they continue to get more information continue to see the range of of the target

505
01:00:03,400 --> 01:00:07,720
if they just went straight to the target they wouldn't get a range fix on it

506
01:00:12,760 --> 01:00:17,640
now what about the difference between the b and the bat

507
01:00:18,440 --> 01:00:28,040
whereas a b is simply receiving the reflected photons let's just say the bat is sending out

508
01:00:29,320 --> 01:00:37,240
a invisible signal so how is this kind of radar echolocation setting

509
01:00:38,040 --> 01:00:40,840
similar and different to the vision setting

510
01:00:42,520 --> 01:00:46,120
oh it's very different i could show you that with the program if you like

511
01:00:47,880 --> 01:00:51,960
what happens with the bat is from the delay of an

512
01:00:52,680 --> 01:00:58,200
for a particular insect that the bat is tracking from the delay of the echo it sees

513
01:00:58,920 --> 01:01:03,960
the range of the insect so it gets the insect constrained on a sphere in its 3d model

514
01:01:04,840 --> 01:01:11,320
and then from the Doppler shift it actually sees the perceives the cosine of the angle between

515
01:01:11,320 --> 01:01:16,360
its directional movement and the direction of the insect so what it gets there is it

516
01:01:16,360 --> 01:01:23,000
constrains the sphere surface of a sphere down to one circle in space so what the bat gets is a

517
01:01:23,000 --> 01:01:29,400
series of circles in space which constrain the position of the insects and i could show that

518
01:01:29,400 --> 01:01:37,400
on the program if you like sure okay um well how do i do that how do i share again yeah

519
01:01:39,000 --> 01:01:44,760
i'm curious is the circle something like is it like a hoop that you could throw a basketball

520
01:01:44,760 --> 01:01:53,640
through or is it well i'll show you if i'm now i can't get rid of this something on my screen it's

521
01:01:53,640 --> 01:02:01,560
the problem um i've got a big black screen with a two on it and i can't get rid of it

522
01:02:01,880 --> 01:02:07,400
um end show it's where the end show right no

523
01:02:09,720 --> 01:02:17,560
am i sharing my screen or not not yet um now how do i find where i do that how do i find

524
01:02:17,560 --> 01:02:25,800
where i am in space yeah absolutely that's an interesting question also how these mechanisms

525
01:02:25,960 --> 01:02:31,400
google windows somewhere zoom windows somewhere oh zm that'll be it

526
01:02:33,400 --> 01:02:42,440
how are these mechanisms repurposed for digital navigation semantic navigation narrative navigation

527
01:02:45,080 --> 01:02:53,960
uh just a minute let me find oh god get away oh chef screen to chef

528
01:02:54,920 --> 01:03:03,080
okay right you got that we're back okay so what we do is we change from beta bat

529
01:03:04,280 --> 01:03:12,360
and we start again so what you have now is the bat and several insects which are colored

530
01:03:12,360 --> 01:03:21,000
circles here and what the bat has is its echolocation take the blue insect its echolocation

531
01:03:21,000 --> 01:03:27,720
constrains it to a sphere in space and that's the delay of the echolocation and the Doppler shift

532
01:03:27,720 --> 01:03:33,800
can constrain the sphere down to a circle so if we rotate these circles you see that blue

533
01:03:33,800 --> 01:03:42,120
insect is on a circle from one point in the bats trajectory so as the bat moves it gets

534
01:03:42,920 --> 01:03:50,440
successive circles of the same insect so what we have is the bat is moving and all these

535
01:03:50,440 --> 01:03:56,520
circles highly confusing and it gradually locates all the insects better and better

536
01:03:58,600 --> 01:04:07,800
but what if i restart and step again if i step of i've restarted right now if i step

537
01:04:08,840 --> 01:04:17,480
what i can do is focus on one insect if i focus on that insect then i only see the circles from

538
01:04:17,480 --> 01:04:25,000
echolocation of that one insect and i can step again and get a new circle from a new step

539
01:04:25,640 --> 01:04:31,640
and all the time the bat is optimizing the position of that insect to get the best fit to

540
01:04:31,640 --> 01:04:37,000
all the circles it has for that insect so it's very different from vision but it can build

541
01:04:38,040 --> 01:04:44,360
a very good 3d model from its echolocation okay so to kind of confirm what's happening here

542
01:04:45,080 --> 01:04:55,960
there's there's for a given snapshot ping with the sonar what is returned is a circle

543
01:04:55,960 --> 01:05:02,680
of equiprobable locations that are kind of like the maximum likelihood ridge that's right

544
01:05:02,680 --> 01:05:09,160
constraints these are really sort of gaussian's gaussian donuts if you like okay likelihood

545
01:05:09,160 --> 01:05:17,560
and then successive pings enable you to look at the intersection point of the circles

546
01:05:18,360 --> 01:05:22,040
to find out yeah through successive approximation the likeliest location

547
01:05:22,840 --> 01:05:27,960
this is what's happening here on that light blue insect has got these three circles

548
01:05:28,680 --> 01:05:33,880
and that's the most likely they're not very well intersecting but that's the most likely

549
01:05:33,960 --> 01:05:40,600
intersection point that's i go on this bat it's it's like a gaussian mixture model you have those

550
01:05:40,600 --> 01:05:46,280
three that's peaks and then when you summit those three peaks that the the point that interpolates

551
01:05:46,280 --> 01:05:52,040
them or just is the the geometric average is the single maximum likelihood estimate point

552
01:05:53,000 --> 01:05:59,080
that's exactly it yeah and again with the bat i can go from full basian model which is this one

553
01:05:59,240 --> 01:06:09,000
full basian tracking or tracking with noise and again the noise tends to have nasty effect

554
01:06:11,320 --> 01:06:19,080
or i look back at this one here that's that dark blue insect and i can rotate to see how

555
01:06:19,080 --> 01:06:28,200
the circles go so how is this how is this similar or different than for example radar

556
01:06:28,200 --> 01:06:34,040
navigation algorithms for planes or ships i think it's quite similar

557
01:06:36,440 --> 01:06:43,800
i think yeah i mean they are making maximum likelihood inferences from radar signal

558
01:06:46,040 --> 01:06:50,840
and have you with this software looked at the computational

559
01:06:51,720 --> 01:06:58,360
like the runtime complexity or the resource use associated with scaling the number of insects

560
01:06:58,360 --> 01:07:05,240
or scaling the resolution of vision yeah you can scale the number of insects for instance if i scale

561
01:07:05,240 --> 01:07:18,680
up here the 30 insects i can run the model with 30 insects and um it's it runs perfectly well

562
01:07:21,720 --> 01:07:29,320
so the model is quite efficient because what it has to do for each object

563
01:07:30,600 --> 01:07:38,360
um and for each time step it simply has to do and this Gaussian optimization which turns out to be

564
01:07:38,360 --> 01:07:44,600
just a 3d matrix operation three were three matrices and that's very quick to do

565
01:07:45,080 --> 01:07:53,080
and the so that's one of the advantages of it that if animals are trying to

566
01:07:53,720 --> 01:07:59,800
track a load of things around them which they probably are it's quite a quick efficient computation

567
01:08:01,080 --> 01:08:05,320
but this is not the neural implementation the problem is going from here this

568
01:08:05,320 --> 01:08:11,000
mars level 2 computational model to a neural implementation that is where i think all the

569
01:08:11,000 --> 01:08:17,080
interesting problems lie at least you have at this level two model is a kind of starting one

570
01:08:17,080 --> 01:08:21,160
and the basian model would be trying to implement that in your

571
01:08:25,400 --> 01:08:37,480
yes makes sense this is kind of the as if basian algorithmic map and the question is what

572
01:08:37,480 --> 01:08:48,040
proximate mechanisms are capable of doing these algorithms functionally and neuro anatomy has

573
01:08:48,040 --> 01:08:59,960
basically localized to the region in mammals and in invertebrates and so now we're in a space of

574
01:09:00,920 --> 01:09:06,360
winnowing possibilities and leaving the door open for unconventional opportunities

575
01:09:06,360 --> 01:09:12,520
for how those regions actually do it it's a very targeted specific agenda

576
01:09:13,400 --> 01:09:21,560
that that connects a first principles grounding about how well the navigation can proceed

577
01:09:21,560 --> 01:09:28,280
with a requirements equation on through to the empirical patterns that we see

578
01:09:29,080 --> 01:09:33,400
those empirical patterns might have some like behavioral experiments well

579
01:09:33,400 --> 01:09:37,960
sometimes can be hard to interpret as well though for example there's a common experiment

580
01:09:37,960 --> 01:09:45,800
where a wall that's shaped like an L is set up and then people will use it to test if an animal

581
01:09:45,800 --> 01:09:53,960
will go on the hypotenuse to reduce the travel distance or whether it will take the the around

582
01:09:53,960 --> 01:10:00,920
the wall however even if there was a conceptualization of the ability to move on the hypotenuse

583
01:10:00,920 --> 01:10:08,280
direction an animal might like prefer to move along walls so then by the time you get to the

584
01:10:08,280 --> 01:10:19,160
real animals movements it's very tied up with not just trying to be the optimal landmark resolving

585
01:10:19,160 --> 01:10:26,520
visual algorithm it's actually engaging in all these other drives that can make it look like it's

586
01:10:27,320 --> 01:10:32,120
lower efficiency or even like supernaturally efficient on a given domain

587
01:10:34,440 --> 01:10:39,960
yeah i i think there's all sorts of animal experiments what one could do but interpreting them

588
01:10:39,960 --> 01:10:47,080
is not easy but basically if you're looking in these experiments to to to measure how good

589
01:10:47,800 --> 01:10:54,840
the animals internal 3d model is i would like to with insects for instance how good is that

590
01:10:54,920 --> 01:11:00,440
movement detection if it's movement in depth that needs the 3d model to resolve it rather than

591
01:11:00,440 --> 01:11:08,840
just the visual field that's very interesting especially in insects potentially where there's

592
01:11:08,840 --> 01:11:19,880
little overlap between their two compound eyes yeah yes um okay i'll read a question from the chat

593
01:11:19,880 --> 01:11:28,200
as we kind of head towards the end use equal wrote how would perception of ability to conceive

594
01:11:28,200 --> 01:11:33,320
of other domains of time as a cognitive function change spatial awareness

595
01:11:35,960 --> 01:11:44,280
um other dimensions of time i i'm not sure i understand that question other domains of time

596
01:11:45,000 --> 01:11:53,000
domains what's other domains of time um well this is all a very short term

597
01:11:53,880 --> 01:12:01,000
question it's fractions of a second and time outside that into all just doesn't come into it at all

598
01:12:01,000 --> 01:12:11,480
really does that answer the question you think it might i think it might also be pointing to our

599
01:12:12,440 --> 01:12:21,160
awareness of how we handle our perception of time and space and what does that open up or enable

600
01:12:21,880 --> 01:12:27,560
for example for our perception of space if we have a different perception or conception of time

601
01:12:28,760 --> 01:12:34,680
well that is a very deep question i mean i think our perception of chronological time the long

602
01:12:34,680 --> 01:12:41,400
term time is something completely different from more anything else in the animal kingdom

603
01:12:42,120 --> 01:12:47,880
i think most animals live in the moment really they're aware of what's happening right now

604
01:12:47,880 --> 01:12:51,320
and what they got to do right now and the rest just doesn't matter

605
01:12:54,760 --> 01:13:01,880
whereas we exit what matters in the moments to ruminate and speculate yeah absolutely

606
01:13:02,120 --> 01:13:12,280
um well what other directions or or ways are you hoping to take this work

607
01:13:14,120 --> 01:13:21,640
well one way that is particularly important i think although particularly problematic is

608
01:13:21,640 --> 01:13:28,200
theories of consciousness in other words consciousness most of our consciousness

609
01:13:28,840 --> 01:13:35,720
at any moment is consciousness of the space around us and it seems to come from our internal

610
01:13:35,720 --> 01:13:43,560
Bayesian model of space around us so this work is very related to why we are conscious and there's

611
01:13:43,560 --> 01:13:49,720
another paper and i'd like to give another live stream just on that subject so i think this gives

612
01:13:49,720 --> 01:13:55,880
a way forward to a theory of consciousness that can be in many ways more satisfactory than we get

613
01:13:55,880 --> 01:14:07,960
from purely classical neural models and brain that's awesome i'm also personally very excited

614
01:14:07,960 --> 01:14:16,760
on the empirical insect neuroanatomy side to look oh there are a huge number of ways of

615
01:14:16,760 --> 01:14:22,600
going forward i think insects are particularly productive because you know they've got to do it

616
01:14:22,600 --> 01:14:29,160
with a really small number of neurons so it's really there's a lot of experimental work on

617
01:14:29,160 --> 01:14:37,240
insects that can illuminate this question yeah absolutely cool yes the brains are fun to dissect

618
01:14:37,240 --> 01:14:47,640
you can see it transparently all there and no backstage there and then also this on on the more

619
01:14:48,520 --> 01:14:57,880
transferable outside of entomology i think the active inference loop closure with policy

620
01:14:57,880 --> 01:15:03,080
selection of movements including stay go no go decisions and which way to go

621
01:15:03,960 --> 01:15:09,800
and then understanding how like well here's the trajectory that would have been the most

622
01:15:09,880 --> 01:15:18,760
information gain on resolving this location um here's the trajectory that maximized safety

623
01:15:19,480 --> 01:15:22,840
not moving or something and then here's the trajectory that that would have um

624
01:15:23,560 --> 01:15:29,400
done some other thing and then being able to look at realized empirical trajectories

625
01:15:30,200 --> 01:15:37,720
and then break those down or look at their component loading according to safety visual

626
01:15:37,720 --> 01:15:44,840
information gain other kinds of heuristics or or impulses to understand moments or kind of

627
01:15:44,840 --> 01:15:51,160
fragments of trajectories like something looks to briefly resolve its uncertainty and then it

628
01:15:51,160 --> 01:15:57,160
doesn't need then the the overall demand to resolve uncertainty drops again and then it continues

629
01:15:57,800 --> 01:16:02,920
just with inertia and then maybe that's a very simple decision to make and then there's probably

630
01:16:03,000 --> 01:16:09,960
all kinds of cool patterns and ways to go yeah there's a huge amount to investigate

631
01:16:12,280 --> 01:16:17,880
another another topic by the way is we've looked at my most we've looked at insects right at the

632
01:16:17,880 --> 01:16:23,480
officer ends of the spectrum there's a whole load of stuff in between i think octopus and

633
01:16:23,480 --> 01:16:29,160
squid are particularly interesting but there's all the other species one can look at say how do

634
01:16:29,160 --> 01:16:36,360
they do space they have unique bodies and different bodies but there's also bringing in there the

635
01:16:36,360 --> 01:16:46,040
question of underwater or space or fluid media and then that and a bat flies but it has the mammalian

636
01:16:46,040 --> 01:16:52,040
neuroanatomy so then there could be like bird neuroanatomy with its slight differences from

637
01:16:52,040 --> 01:16:58,040
the mammal then there could be this question of underwater and maybe there's some mammals

638
01:16:59,160 --> 01:17:09,160
that have underwater navigation maybe dolphins would be like bats but in another fluid media

639
01:17:12,840 --> 01:17:15,960
i hope people download the dot zip and play around

640
01:17:20,280 --> 01:17:26,840
so do i any last comments uh

641
01:17:30,120 --> 01:17:38,360
not really no i think i said it all thank you thank you robert well i i am greatly enjoying

642
01:17:38,360 --> 01:17:45,000
learning about the work and seeing about how the requirements equation

643
01:17:47,400 --> 01:17:58,040
scopes a given problem setting and kind of puts a meter stick on whatever the inferential problem

644
01:17:58,040 --> 01:18:07,880
is inference plus action problem is and then from there the mar the marian research program

645
01:18:08,520 --> 01:18:14,600
is just kind of laid out you can pursue it from the mechanistic or from the algorithmic elements

646
01:18:15,160 --> 01:18:20,600
but they all are connected through on one hand in theory the requirements equation

647
01:18:20,600 --> 01:18:23,640
and on the other hand the empirical results that we actually see

648
01:18:24,120 --> 01:18:26,120
yeah

649
01:18:29,640 --> 01:18:35,720
cool so thank you again um i come back to the Feynman well Richard Feynman on his

650
01:18:35,720 --> 01:18:40,120
backboard said if you can't build it you don't understand it this is all about building it

651
01:18:43,080 --> 01:18:44,040
we can't build a bug

652
01:18:47,240 --> 01:18:53,320
not yet um thank you robert see you for dot three

653
01:18:53,320 --> 01:18:56,200
bye

