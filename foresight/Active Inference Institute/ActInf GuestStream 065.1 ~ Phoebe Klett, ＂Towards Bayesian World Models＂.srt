1
00:00:00,000 --> 00:00:11,120
Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll

2
00:00:11,120 --> 00:00:17,520
be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent

3
00:00:17,520 --> 00:00:24,320
reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe,

4
00:00:24,320 --> 00:00:30,800
very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah,

5
00:00:30,800 --> 00:00:35,440
I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art

6
00:00:35,440 --> 00:00:43,280
language models into more probabilistic machinery and what that might bias. All right. Let's get

7
00:00:43,280 --> 00:00:49,120
right into it. All right. So some of the things that I'm hoping to discuss today include why we

8
00:00:49,120 --> 00:00:54,240
might use a language model for something that isn't long-form text generation and how we might do

9
00:00:54,240 --> 00:01:00,640
that, and then motivate a little bit kind of why we might need a world model, what simple

10
00:01:01,280 --> 00:01:06,960
self-organizing world models might look like, and even in the simplest cases how we might start to

11
00:01:06,960 --> 00:01:12,480
use those as effective recommendation engines in the wild today, and then a little bit of discussion

12
00:01:12,480 --> 00:01:22,560
about kind of where this research is going. All right. So what are language models good at?

13
00:01:23,520 --> 00:01:28,080
Today's especially large language models are trained on next token prediction.

14
00:01:28,800 --> 00:01:34,640
So this means given a sequence of tokens, we're going to estimate which token is most likely to

15
00:01:34,640 --> 00:01:41,040
come next. Maybe rephrasing that a little bit. We might also say that language models are trained

16
00:01:41,040 --> 00:01:49,040
to estimate which sequences of tokens or words are likely. The caveat being likely to appear in

17
00:01:49,040 --> 00:01:53,280
the training set, but today our training sets are quite extensive.

18
00:01:56,880 --> 00:02:02,160
So given this simple training objective, it's arguably surprising that we've seen language

19
00:02:02,160 --> 00:02:08,480
models do as well as they have in really impressive tasks. So they start to demonstrate

20
00:02:08,480 --> 00:02:13,760
like really great language understanding, meaning like the semantics of language itself,

21
00:02:14,400 --> 00:02:20,800
not just the syntax of which sentences make sense. They've also started to demonstrate some

22
00:02:20,800 --> 00:02:25,440
world knowledge, so implicitly learning kind of how the world works just through our own human

23
00:02:25,440 --> 00:02:33,760
abstraction and how we articulate that. We start to see language models falter at tasks,

24
00:02:33,760 --> 00:02:39,280
which look more like symbolic problem solving. So math in particular, we see this in programming,

25
00:02:39,280 --> 00:02:44,400
although we're getting better at this. And in general, any kind of long-term planning tasks,

26
00:02:44,400 --> 00:02:49,760
which require abstract reasoning. So we see this also in problems that look like this,

27
00:02:49,760 --> 00:02:57,200
which are word problems, but which are really about understanding abstractly how these abstract

28
00:02:57,200 --> 00:03:01,840
objects relate to each other. And again, this shouldn't be surprising that language models

29
00:03:01,840 --> 00:03:07,200
struggle to do things like this, since it's so far off from their original training objective.

30
00:03:07,600 --> 00:03:13,200
And in particular, even when we arrive at the right answer in some of these cases,

31
00:03:13,200 --> 00:03:17,520
it's really hard to know after the fact kind of the reasoning for how we ended up

32
00:03:17,520 --> 00:03:23,520
at the right answer in this particular case. And maybe more fundamentally, if we don't know

33
00:03:23,520 --> 00:03:28,000
where the abstraction is happening or where the reasoning is happening, it's very hard to guide

34
00:03:28,000 --> 00:03:33,840
that process. And so this starts to motivate the need for something that looks more like

35
00:03:33,840 --> 00:03:39,680
an explicit world model. And now, to start to borrow some words from Yasha Benjio,

36
00:03:39,680 --> 00:03:43,680
this is one of the kind of biggest issues with today's language models,

37
00:03:43,680 --> 00:03:47,760
is arguably that we're asking the language model to be both the inference machine

38
00:03:47,760 --> 00:03:54,800
and the world model implicitly, when this doesn't quite make sense. And so some things that we

39
00:03:54,800 --> 00:04:00,400
might hope for in a world model are to model causal relationships, to be really adapt at

40
00:04:00,400 --> 00:04:08,320
modeling, using uncertainty, and to be modular. Yann LeCun puts this in a similar way, so we

41
00:04:08,320 --> 00:04:13,200
might kind of ask a world model to be able to distinguish between which details are important

42
00:04:13,200 --> 00:04:18,000
versus irrelevant, and to be able to make predictions that can be performed in sort

43
00:04:18,000 --> 00:04:24,480
of this abstract space of representations. And so hopefully in the next following slides,

44
00:04:24,480 --> 00:04:30,000
we're going to motivate what it might look like to use language models as inference machines

45
00:04:30,000 --> 00:04:37,360
in maybe like the simplest case of self-organizing world models. And even in those most simple

46
00:04:37,360 --> 00:04:43,280
cases, how we start to get something much closer to the kind of explainable reasoning,

47
00:04:43,280 --> 00:04:45,600
which I think lots of us are grasping for right now.

48
00:04:50,240 --> 00:04:56,320
All right, so in a very simple case, we might think of a world model as simply a collection

49
00:04:56,320 --> 00:05:02,880
of hypotheses, where we model confidence over each of those hypotheses. And in particular,

50
00:05:02,880 --> 00:05:07,680
we care about predictive world models. So given some evidence or data that we've observed in the

51
00:05:07,680 --> 00:05:16,080
world, we'd like to propose the world models, which best explain the evidence that we've observed.

52
00:05:20,080 --> 00:05:26,080
And if that language sounded leading, it was indeed. We are proposing in this case to use

53
00:05:26,080 --> 00:05:31,520
Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given

54
00:05:31,520 --> 00:05:40,080
some new evidence. And so when our hypotheses are kind of these Bernoulli random variables,

55
00:05:40,080 --> 00:05:47,440
this bottom term can simply be expanded into this equation on the slide. And so the tricky part

56
00:05:47,440 --> 00:05:55,280
in this computation is the likelihood piece. So given this evidence, given that our hypothesis

57
00:05:55,280 --> 00:06:00,720
is true, what's the likelihood that we would have observed the evidence? And the claim that we're

58
00:06:00,720 --> 00:06:07,200
making is that language models are actually, this is like a natural task to ask of a language model.

59
00:06:08,160 --> 00:06:12,640
When our evidence and our hypothesis are both semantic objects, as we discussed, language

60
00:06:12,640 --> 00:06:20,560
models are kind of trained exactly to understand which sequences of text are likely. So asking it to

61
00:06:21,440 --> 00:06:27,360
prompting it in a clever way such that we can extract this particular likelihood

62
00:06:27,360 --> 00:06:34,800
is actually really natural. So how do we do this? So we come up with this kind of clever

63
00:06:34,800 --> 00:06:42,240
prompting scheme that allows us to extract again exactly how likely is this should have occurred.

64
00:06:42,880 --> 00:06:49,440
So given that our hypothesis is H in this setting and our evidence is this curly E,

65
00:06:50,640 --> 00:06:54,880
we only need to change our evidence into the conditional form and then phrase the question

66
00:06:54,880 --> 00:07:01,920
like this to the language model. So the input sequence of text goes like, given that potentially

67
00:07:01,920 --> 00:07:07,280
if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic,

68
00:07:07,280 --> 00:07:12,160
would we have observed the evidence that Walmart laid off 10% of the material staff?

69
00:07:14,000 --> 00:07:19,920
And so either using something like few shot prompting or guidance or some kind of control

70
00:07:19,920 --> 00:07:25,600
generation technique, we can ensure that our language model outputs either yes or no,

71
00:07:25,600 --> 00:07:30,320
and then use the logits from its answer to estimate that probability.

72
00:07:30,400 --> 00:07:38,560
And again, the claim here is that this is actually a really natural task to ask of a language model

73
00:07:38,560 --> 00:07:44,720
that leverages its innate reasoning engine better than just kind of allowing it to ramble

74
00:07:46,320 --> 00:07:56,160
using text sometimes. And we can even make this updating scheme a bit more sophisticated

75
00:07:56,160 --> 00:08:02,400
using things like precision waiting. So this will bias our posterior over hypothesis

76
00:08:02,400 --> 00:08:08,080
either towards our prior or our evidence that we've observed based on how confident we are

77
00:08:08,080 --> 00:08:20,240
in our prior and in our evidence. All right. So the world models that we've seen so far

78
00:08:20,800 --> 00:08:26,000
are fairly simple. In particular, we're modeling all our hypotheses as independent from each other.

79
00:08:26,880 --> 00:08:32,480
And this seems like a large simplifying assumption. So how might we support more complex world

80
00:08:32,480 --> 00:08:41,120
models where we condition our hypotheses on each other? All right. So another thing that

81
00:08:41,120 --> 00:08:45,520
hopefully you'll be familiar to this crowd, Bayes and Nets have been around for forever and

82
00:08:45,520 --> 00:08:53,040
are often used to model mechanistic failures and enlarge systems and can be understood simply as

83
00:08:53,120 --> 00:08:56,880
distribution where each variable depends on some small number of ancestor variables.

84
00:08:58,080 --> 00:09:01,680
Perhaps more intuitively, we can also think of these as directed graphs

85
00:09:03,200 --> 00:09:07,280
where we have edges between variables which are directed in the sense that

86
00:09:07,280 --> 00:09:13,680
the parent is conditioned on the child. So let's see an example.

87
00:09:15,600 --> 00:09:22,720
All right. So now we have two different kinds of hypotheses. The ones which are kind of parents

88
00:09:22,720 --> 00:09:29,360
in our simple setup are kind of qualitatively more abstract than the children. So our two

89
00:09:29,360 --> 00:09:35,680
children nodes are similar to the hypothesis we saw before. Mark a sentiment for Best Buy is Poor

90
00:09:35,680 --> 00:09:41,360
or Walmart will grow its physical footprint this year. And then the more abstract hypothesis

91
00:09:41,360 --> 00:09:49,200
being retailers were negatively impacted by COVID-19. And so then we specify this conditional

92
00:09:49,200 --> 00:09:54,400
structure either kind of in a classical sense by specifying all of the joint probabilities

93
00:09:54,400 --> 00:10:00,560
upfront or alternatively learning it given some healthy amount of training data. And again,

94
00:10:00,560 --> 00:10:05,840
we're proposing that instead of doing that intensive process, we can use language models

95
00:10:05,840 --> 00:10:12,960
to extract those probabilities in a natural way. And so one natural complaint at this point might

96
00:10:12,960 --> 00:10:18,560
be like, well, this space is going to start to get very large. If we're trying to update these

97
00:10:18,560 --> 00:10:24,720
beliefs in real time given some large data stream is going to become intractable given our current

98
00:10:24,720 --> 00:10:31,840
framework. So how might we start to augment that? Luckily, there's been a lot of work historically

99
00:10:31,840 --> 00:10:37,280
done on this problem. And we can start to use things like message passing to update our beliefs

100
00:10:37,280 --> 00:10:43,200
in an approximate way. So as long as our Bayes net has a tree dependency structure, we can use

101
00:10:43,200 --> 00:10:48,000
things like the sum product algorithm to update our beliefs. And I don't want you to worry too

102
00:10:48,000 --> 00:10:52,960
much about the equations on this slide. If you've seen this before, this kind of recursive structure

103
00:10:52,960 --> 00:10:58,640
will look familiar. But if not, the general kind of intuitive idea that we're going to compute these

104
00:10:58,640 --> 00:11:04,160
messages from all of the children or the neighboring nodes and use those to propagate through the graph

105
00:11:04,160 --> 00:11:12,400
to update each independent belief. Let's just see an example. All right, so suppose we observed

106
00:11:12,400 --> 00:11:18,080
some new evidence and we'd like to know how should we update probability over hypothesis A,

107
00:11:18,080 --> 00:11:24,960
which as you remember is the parent node in our graph. All right, so B here on the left hand side

108
00:11:24,960 --> 00:11:30,560
is the belief. XA is essentially like just the variable representing the hypothesis A.

109
00:11:31,680 --> 00:11:38,320
And then the fee and the side terms are specified by our language model or the

110
00:11:38,960 --> 00:11:44,400
the Bayes net in the classical sense. And so we're summing over all of the variables,

111
00:11:44,400 --> 00:11:48,640
all of the values that the variables XB and XC can take, which in our case, of course, is just zero

112
00:11:48,640 --> 00:11:55,520
and one. So if you, you might recognize at this point that this is indeed the exact marginal

113
00:11:55,520 --> 00:12:01,600
probability for the hypothesis A, and that's because our graph is simply connected in this example.

114
00:12:02,160 --> 00:12:06,480
So in general, this isn't true, but it turns out to be the case in our example.

115
00:12:06,720 --> 00:12:13,280
And again, we can compute these fee and side terms using the language model itself.

116
00:12:18,080 --> 00:12:22,720
All right, so because we're all lovers of free energy here, I'm going to walk through kind of

117
00:12:22,720 --> 00:12:30,640
how this is maybe the first example of a self evidencing or minimizing free energy kind of model.

118
00:12:31,200 --> 00:12:36,320
So as we noted, belief propagation isn't exact for more complicated graphs. And so

119
00:12:37,040 --> 00:12:42,640
it makes sense and might be useful to ask the question, you know, how far apart or when are

120
00:12:42,640 --> 00:12:49,920
our beliefs close to the exact marginals. And so we often use things like KL divergence to

121
00:12:49,920 --> 00:12:55,680
compare the difference between two probability distributions. And that is explicated on the

122
00:12:55,680 --> 00:13:04,720
right hand side. And then those of us who are like have a background in physics might recognize

123
00:13:04,720 --> 00:13:09,920
Boltzmann's law as well. So this is just the idea that we might represent the probability of a given

124
00:13:09,920 --> 00:13:15,040
state using an energy function. And we're not going to accept this as truth, maybe some of us

125
00:13:15,040 --> 00:13:19,360
have done in the past, but we are going to just use it as a definition for this energy function,

126
00:13:20,080 --> 00:13:27,200
such that when we plug in that term here, and expand out, we start to see these first two terms

127
00:13:27,200 --> 00:13:31,840
look a lot like the kind of energy and entropy functions which we are used to. And indeed,

128
00:13:31,840 --> 00:13:39,920
we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy

129
00:13:39,920 --> 00:13:46,080
to see this all coming together. And in particular, it might make sense just to note at this point that

130
00:13:46,800 --> 00:13:52,320
using world models which are self organizing in the sense seems to be very compelling,

131
00:13:52,320 --> 00:13:58,240
since the kind of world model which we want is one which promotes the evidence that we've seen so far.

132
00:14:03,920 --> 00:14:08,560
Why is it useful to formulate this in terms of free energy, besides the fact that we all

133
00:14:09,360 --> 00:14:13,760
find it compelling here? Well, you can make a lot of progress by constructing analytically

134
00:14:13,760 --> 00:14:19,680
tractable approximations of Gibbs free energy often. I'm not going to go into the details here,

135
00:14:20,800 --> 00:14:23,760
but here are two examples where that's been fruitful.

136
00:14:28,480 --> 00:14:33,440
All right. So now I'm going to chat briefly about kind of how we might use recommendation

137
00:14:33,440 --> 00:14:38,400
systems like, or how we might use these systems as recommendation systems, and indeed in the world

138
00:14:39,280 --> 00:14:47,440
today. All right. So one prime example for a system like this might be useful is a situation

139
00:14:47,440 --> 00:14:53,760
where we have lots of data incoming at very high frequencies, and we always want to have

140
00:14:53,760 --> 00:14:59,520
some set of naturally discrete hypotheses that we're modeling beliefs over which are

141
00:14:59,520 --> 00:15:04,960
being kept up to date at a very regular cadence. And so actually, a lot of the

142
00:15:05,600 --> 00:15:12,560
the muscle here is just reformatting documents or however our data comes in as evidences. This is

143
00:15:12,560 --> 00:15:18,400
not always obvious or easy to do. But once you've kind of figured out that part, and in particular,

144
00:15:19,200 --> 00:15:26,000
we've been using things like RAG, retrieval augmented generation, or embedding based systems to

145
00:15:26,000 --> 00:15:32,560
kind of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind

146
00:15:32,640 --> 00:15:39,520
of built up that machinery, the actual updating computations, as we've shown already, is actually

147
00:15:39,520 --> 00:15:44,960
pretty simple. So we do these likelihood computations and we update our beliefs. And then at any given

148
00:15:44,960 --> 00:15:51,600
time, we can query that model for our marginal distribution over any given hypothesis.

149
00:15:53,120 --> 00:15:57,760
And it turns out that this kind of setup has many practical applications.

150
00:15:57,760 --> 00:16:06,160
It's also noteworthy that even with very simple systems like these, these are like

151
00:16:06,160 --> 00:16:12,480
out of the box, controllable and explainable. So just by storing the magnitude and the direction

152
00:16:12,480 --> 00:16:18,160
of the update to the posterior for each piece of evidence that we observe, we have a very natural

153
00:16:18,160 --> 00:16:26,400
built in explanation for our belief at any given time. And that makes kind of like these applications

154
00:16:26,400 --> 00:16:31,520
where folks might really like to use a language model, but really require like a robust,

155
00:16:32,320 --> 00:16:39,200
like causal relationship between the outputs and the explanation, which you don't get from

156
00:16:39,200 --> 00:16:43,760
a language model on its own. A system like this can be very appealing in those situations.

157
00:16:48,800 --> 00:16:50,720
All right, so now on to further work.

158
00:16:51,440 --> 00:17:01,600
So everything that we've discussed today is early work towards integrating language models

159
00:17:01,600 --> 00:17:08,960
into more probabilistic frameworks. And there's been a lot of exciting work done in this vein

160
00:17:08,960 --> 00:17:14,080
right now. Some important questions which are especially interesting to me are which parts

161
00:17:14,080 --> 00:17:20,400
of the world model should be learned versus encoded? And how do we want intelligence to scale?

162
00:17:21,360 --> 00:17:26,560
Both in the sense of composing systems naturally, there should be some very like natural way that

163
00:17:26,560 --> 00:17:31,920
we can compose to intelligent systems and also such that we can scale them with compute.

164
00:17:33,040 --> 00:17:37,840
And I don't mean to restrict myself either to the kinds of compute that we have today.

165
00:17:37,840 --> 00:17:42,720
We're also working at some exciting new computing paradigms at normal, which might be more

166
00:17:42,720 --> 00:17:49,520
compatible with software of this nature. Also the two folks that we referenced at the beginning

167
00:17:49,520 --> 00:17:53,680
of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area,

168
00:17:55,280 --> 00:18:00,640
which is very inspiring. And so in particular, G flow nets are also probabilistic graphical

169
00:18:00,640 --> 00:18:08,240
models, which I think folks will find a natural next step in reading if you so desire.

170
00:18:11,360 --> 00:18:13,760
And that's it for me.

171
00:18:14,400 --> 00:18:23,360
Awesome. Thank you. Wow, very cool.

172
00:18:26,560 --> 00:18:31,040
Dan, do you want to give a first reflection or thought? And then meanwhile, anyone who's

173
00:18:31,040 --> 00:18:34,720
watching live, please feel free to write questions. I'll relay them in.

174
00:18:34,720 --> 00:18:45,040
Absolutely. So hi, I'm Dan. I work with Phoebe on this project. And yes, the

175
00:18:46,480 --> 00:18:53,120
I think the thing that's most exciting about this for me personally is sort of twofold.

176
00:18:53,840 --> 00:19:04,320
One of them is that it's a way of avoiding sort of having to trust a language model to

177
00:19:05,040 --> 00:19:14,800
understand and reason about text. Because they're not it's not that bad. The thing is that the

178
00:19:14,800 --> 00:19:20,080
extremely strange thing about language models is they're quite good at being almost good enough.

179
00:19:21,040 --> 00:19:27,360
But they're never quite what you could use. You could never use a language model to, I don't know,

180
00:19:30,000 --> 00:19:35,520
sort of triage, like an important sort of situation where a bunch of different things

181
00:19:35,520 --> 00:19:39,840
are coming in, you have to make a decision about which is important. The reason you can't do that

182
00:19:39,840 --> 00:19:47,200
is you simply cannot understand the encoded biases. You cannot get it to reliably generate

183
00:19:47,200 --> 00:19:52,080
reasoning. You can ask it for reasoning. But the thing that it prints out is not the reasoning

184
00:19:52,080 --> 00:19:59,280
that it used internally because it doesn't reason. Fundamentally, while these have input and output

185
00:19:59,280 --> 00:20:05,360
that are natural language, they are not artificially intelligent. They are just prediction machines.

186
00:20:06,560 --> 00:20:11,600
And so we have to be very careful about not anthropomorphizing them. So this is a way of

187
00:20:11,600 --> 00:20:18,720
using those incredibly powerful prediction machines in a framework where we can

188
00:20:20,320 --> 00:20:29,840
make sure that we essentially keep a record of what we're doing so that a human can look at it.

189
00:20:29,840 --> 00:20:34,400
Because, I mean, there's a lot of talk in this world about sort of post-human AI

190
00:20:35,040 --> 00:20:39,680
and those sorts of things. The idea that the machines will become intelligent enough,

191
00:20:39,680 --> 00:20:43,760
or the machines will rise up in a slightly more alarming type of way.

192
00:20:45,760 --> 00:20:49,840
And that's all great and wonderful, but that's not particularly interesting to us at normal.

193
00:20:49,840 --> 00:20:56,400
We're much more interested in sort of having mimicking explicit decision processes so that

194
00:20:56,400 --> 00:21:03,440
a human can audit them and can make these things work. That's kind of the area that we're coming from.

195
00:21:04,400 --> 00:21:11,280
Awesome. All right. I'll go to a question from the chat. So Josh asks,

196
00:21:11,840 --> 00:21:18,080
great talk. Where does hypothesis relevance enter the calculus? Is it folded into confidence?

197
00:21:18,080 --> 00:21:21,840
Not sure if it ought to be. Just saw hypothesis relevance mentioned.

198
00:21:25,360 --> 00:21:31,440
Hypothesis relevance. Does that mean like which hypotheses are conditioned on each other?

199
00:21:32,400 --> 00:21:36,560
Is it possible to ask a clarifying question there? Maybe Dan, you have a better idea.

200
00:21:36,560 --> 00:21:42,800
They can follow up. But yeah, I also wondered about this. You might know what was relevance.

201
00:21:43,360 --> 00:21:51,440
Maybe the temperature and the rainfall were relevant, but then how does this approach help us

202
00:21:52,080 --> 00:21:57,440
understand when one of those relevant factors no longer is relevant or when a new relevant

203
00:21:57,440 --> 00:22:05,120
factor comes into play? Yeah, these are great questions. So I think in terms of understanding

204
00:22:05,120 --> 00:22:10,480
in an automatic sense, when two hypotheses are relevant to each other, we can leverage

205
00:22:11,280 --> 00:22:18,480
embedding type language models for this kind of thing also. If we don't have a more kind of like

206
00:22:18,480 --> 00:22:26,240
structured human intuitive sense for when two hypotheses are related, in terms of like how

207
00:22:26,240 --> 00:22:30,720
those relationships evolve over time, this is something that's really interesting to me.

208
00:22:30,720 --> 00:22:36,160
And I think looking at the theory behind structure learning or when we propose to add new nodes to

209
00:22:36,160 --> 00:22:41,840
the network or propose to add a new edge to the network or things like this is a really exciting

210
00:22:41,840 --> 00:22:47,600
research direction. Although I don't have like a silver bullet answer to how we should do that.

211
00:22:48,240 --> 00:22:55,760
Just to like add a little bit more to that, it is like it is a really interesting research

212
00:22:55,760 --> 00:23:02,560
direction. Like one of the things that Phoebe mentioned in the talk is that there is a difficult

213
00:23:02,560 --> 00:23:06,720
step that we're not talking about, which is actually translating this natural language

214
00:23:06,720 --> 00:23:14,000
into reasonable hypotheses. So there is a step in there where you take essentially a chunk of text

215
00:23:14,000 --> 00:23:21,520
and you have to decide if this is a hypothesis, if this is a hypothesis we've seen before,

216
00:23:22,160 --> 00:23:29,280
if this is a sub hypothesis or a clarification of a hypothesis that we've seen before,

217
00:23:29,280 --> 00:23:36,160
and so on and so forth. So that in some sense part of the data processing and it is an important

218
00:23:36,720 --> 00:23:43,440
step and one that we are sort of continuing to work on and refine. The other thing like a different

219
00:23:44,160 --> 00:23:49,520
sort of interpretation of the question around relevance is around sort of

220
00:23:50,720 --> 00:23:56,160
is the hypothesis relevant to the thing that you're looking at? I mean we could have a hypothesis

221
00:23:56,160 --> 00:24:03,920
the sky is blue, but if we are deciding to deciding you know whether or not we need to

222
00:24:03,920 --> 00:24:11,600
check that part's oil, like the truth or not of the color of the sky is very irrelevant.

223
00:24:11,600 --> 00:24:17,840
And that then comes into the nice thing about having your world described as a

224
00:24:19,680 --> 00:24:25,440
collection of statements with truth values associated with them in that you can directly

225
00:24:25,440 --> 00:24:33,360
reason over them. So you can put a classical decision framework over that to take into account

226
00:24:33,920 --> 00:24:38,480
both the sort of the knowledge you have of the world and also which parts of these worlds are

227
00:24:39,120 --> 00:24:45,040
sort of unknown. So in that sort of situation the person using the world model to construct a

228
00:24:49,040 --> 00:24:58,480
sort of decision or an output will be responsible in some sense for assigning a weight or a cost

229
00:24:58,480 --> 00:25:03,920
to each hypothesis being true. And for some of those hypotheses obviously it will be zero

230
00:25:04,880 --> 00:25:08,720
because again we do not care about the color of the sky if all I want to know

231
00:25:08,720 --> 00:25:16,080
is if I need to change the oil in my car. So that's the sort of the other end of the answer.

232
00:25:16,080 --> 00:25:19,920
So there's a version of the answer at the start of the information flow and there's a version of

233
00:25:19,920 --> 00:25:25,120
the answer at the end of the information flow. But it is a tricky point and one that we are sort

234
00:25:25,120 --> 00:25:30,400
of continuing to iterate on to try and find sort of good ways on both ends of that.

235
00:25:30,560 --> 00:25:41,680
Yeah well a lot there. It's very interesting how in that presentation in response I heard

236
00:25:41,680 --> 00:25:51,120
both about probability distributions on rules and rules on probability distributions and like

237
00:25:52,080 --> 00:25:57,280
which one whether it's the tail wagging the dog or the horse in the cart how to design

238
00:25:57,280 --> 00:26:01,600
these synthetic intelligence systems that appropriately bring together

239
00:26:02,720 --> 00:26:09,920
aspects that are more symbolic more rule like and then more probabilistic more embedding like.

240
00:26:09,920 --> 00:26:17,280
So where does that end with you or how do you see the design of these systems with mixed symbolic

241
00:26:17,280 --> 00:26:22,960
and probabilistic components? Yeah yeah that's a great point and I think this really gets it like

242
00:26:23,040 --> 00:26:27,760
which parts of the world model should be learned or should be represented in some like more

243
00:26:27,760 --> 00:26:34,320
discrete space versus like encoded based on our own human intuition for rules and structure.

244
00:26:35,920 --> 00:26:40,800
And I think like maybe this would be fairly represented as a cop-out answer but I think

245
00:26:40,800 --> 00:26:45,680
it depends a lot on the application. I think like when we're developing systems like this and

246
00:26:45,760 --> 00:26:53,840
just trying to iterate through as many different hypotheses as you can quickly like choosing an

247
00:26:53,840 --> 00:27:00,960
application and benchmarking and testing and seeing like what actually works is a go-to strategy for

248
00:27:00,960 --> 00:27:09,200
us in terms of like well which parts should be fixed and are actually helpful to increase

249
00:27:09,200 --> 00:27:14,640
reliability such that like we can use our human intuition for how this particular you know system

250
00:27:14,640 --> 00:27:21,120
is built versus like well this is something that we we want uncertainty over that's like a really

251
00:27:21,120 --> 00:27:27,680
important part of the learning process for us in terms of yeah that kind of iteration so I think

252
00:27:27,680 --> 00:27:35,040
it probably depends on the application. Yeah it's um it it definitely depends on the application

253
00:27:35,040 --> 00:27:43,680
it's also like it depends on where the actual challenge points are so we've got like outside of

254
00:27:43,680 --> 00:27:50,000
this we've got sort of a few other things that we've released publicly that kind of look at this idea

255
00:27:50,000 --> 00:28:01,200
of there being like external rules to the system and whether or not we can add those in.

256
00:28:01,200 --> 00:28:06,560
So one of them is something called constrained generation where we sort of force the model

257
00:28:06,560 --> 00:28:13,280
to only produce something valid and that's sort of quite a useful way of removing

258
00:28:13,280 --> 00:28:19,680
one particular aspect of stress from the model which is that it may make sort of syntactically

259
00:28:20,320 --> 00:28:29,360
or sort of somehow incoherent outputs that don't follow the rules and then we can then

260
00:28:29,360 --> 00:28:34,800
focus with the rest of our energy we can then take that as given and focus with the rest of our

261
00:28:34,800 --> 00:28:42,880
energy on improving the bit that we don't have rules for. So those sorts of things and sort of a

262
00:28:42,880 --> 00:28:48,720
different version is trying to improve something by saying no you broke a rule we need to like go

263
00:28:48,720 --> 00:28:56,880
back and make this sort of true so this kind of sort of chain of thought prompting type of idea.

264
00:28:59,040 --> 00:29:02,960
So so yeah the the symbolic and the probabilistic

265
00:29:05,600 --> 00:29:08,800
I think in our minds live very closely together as

266
00:29:09,520 --> 00:29:17,200
two tools that don't completely solve the same problem and I think there's sort of in

267
00:29:17,200 --> 00:29:22,160
in the world of I'm not sure how familiar anyone in the audience is with language modeling but like

268
00:29:22,160 --> 00:29:27,120
in the world of language modeling before this sort of explosion of neural networks and artificial

269
00:29:27,120 --> 00:29:34,080
intelligence type methods there was a lot of work on symbolics of language and grammars and

270
00:29:34,080 --> 00:29:40,000
all of that sort of stuff and that work pushed quite a long way forward and this work is pushing

271
00:29:40,000 --> 00:29:44,240
quite a long way forward and I suspect the next thing is going to involve them joining up again

272
00:29:44,880 --> 00:29:51,840
because they each have good points they each have bad points and you know two wrongs don't

273
00:29:51,840 --> 00:29:58,480
necessarily make a right but they can make the less wrong. Nice yeah recently we heard from

274
00:29:58,480 --> 00:30:03,920
Elliot Murphy and talking about the neuro linguistics and about how the statistics of language

275
00:30:03,920 --> 00:30:09,520
are not the rules of language you can always come up with a new expression that's never been

276
00:30:09,520 --> 00:30:12,800
uttered that's not going to be in the training distribution or any distribution.

277
00:30:13,840 --> 00:30:18,000
Okay I'll ask a question in chat from Upcycle Club they wrote

278
00:30:19,040 --> 00:30:24,880
what are some of the key challenges associated with developing such Bayesian world models?

279
00:30:24,880 --> 00:30:32,560
Hmm I think we've touched on a bunch of them the ones that are most top of mind for me right now

280
00:30:32,560 --> 00:30:38,560
are the structure learning thing that came up so how do we understand like when to propose new

281
00:30:38,560 --> 00:30:45,360
hypotheses and how to integrate those into the models and then yeah just figuring out like

282
00:30:46,480 --> 00:30:51,840
yeah I guess this like proposal and evolution process of the nodes themselves since everything

283
00:30:51,840 --> 00:30:58,000
else like the framework like works pretty automatically and in a reasonable way thank you

284
00:30:58,000 --> 00:31:05,120
Bayes thank you to the development of language models but kind of moving from this like more

285
00:31:06,320 --> 00:31:12,240
discrete case into a continuous case which like more fully represents the space that

286
00:31:12,240 --> 00:31:20,560
we're learning over can be challenging. Yeah I would also say that like it's a sort of a

287
00:31:20,560 --> 00:31:28,480
maxism that max maxism what on earth did I just say there's a there's a common saying let's go in

288
00:31:28,480 --> 00:31:34,080
that direction there's a common saying in this world that um that sort of no model ever survives

289
00:31:34,080 --> 00:31:42,080
its first encounter with data um and that that becomes true here as well so there's lots of like

290
00:31:42,080 --> 00:31:47,440
as we've been building these things and using them we found lots of little spiky edge corners

291
00:31:47,440 --> 00:31:51,920
with sort of making sure that the language world is actually doing what we want it to do

292
00:31:51,920 --> 00:31:56,400
so there are a lot of questions in building these things around how do you actually test that the

293
00:31:57,360 --> 00:32:03,840
components of it are actually working the way you want and then on like a broader level how do you

294
00:32:04,720 --> 00:32:13,920
compare something that is fundamentally trying to solve a different problem to other methods so

295
00:32:14,720 --> 00:32:19,840
we are solving a problem under the constraints that we want a fully auditable system

296
00:32:22,240 --> 00:32:28,320
we could also solve all of these problems by a thing called in-context learning which is basically

297
00:32:28,320 --> 00:32:34,720
putting the context into the prompt of a large language model and asking it the answer and that

298
00:32:34,720 --> 00:32:41,440
also works especially when you've got things like GPT-4 which are just wonders and glories

299
00:32:41,600 --> 00:32:47,920
um it works really well so then we come to the question of how do we actually make the case

300
00:32:47,920 --> 00:32:56,320
from this from a like a bigger picture perspective can it be more than just a like can we find

301
00:32:56,320 --> 00:33:05,680
benchmarks that reflect um the structural advantages of this approach over something

302
00:33:05,680 --> 00:33:14,720
like in-context learning that don't come across as false so that's kind of like a a stranger answer

303
00:33:14,720 --> 00:33:20,640
because it's not really about like actually developing the world model it's about sort of

304
00:33:20,640 --> 00:33:28,320
convincing other people that it's a good idea um and that's you know that that is a thing that is

305
00:33:28,320 --> 00:33:35,200
true of essentially all of the things on this slide as well they are all quite complex and odd

306
00:33:35,200 --> 00:33:41,920
little methods um that you know there's a there's a degree to which well we definitely can solve this

307
00:33:41,920 --> 00:33:51,440
an easier way um so what is the thing that what is the the the application or the benchmark where

308
00:33:51,440 --> 00:33:57,040
we can say no if you do it the easier way you will fail at this measurable thing

309
00:33:57,680 --> 00:34:05,360
very interesting um so you mentioned the self-evidencing

310
00:34:07,360 --> 00:34:15,920
advantages of using world models that are self-evidencing rather than reward maximizing

311
00:34:15,920 --> 00:34:22,240
for example so how do you see that playing out and I can connect it back to active inference of

312
00:34:22,240 --> 00:34:28,320
course but how do you see this self-evidencing centrality play out in the kinds of models described

313
00:34:28,320 --> 00:34:33,840
here yeah I think there are a couple reasons why it's so compelling to me uh and the first just has

314
00:34:33,840 --> 00:34:39,040
to do with explainability right like it's really convincing to people to say like well why why did

315
00:34:39,040 --> 00:34:43,840
we predict this why do we believe this well this is the actual real world data that we've observed

316
00:34:43,840 --> 00:34:50,720
such that you know this this is the impact that that's had uh and then I think like uh I don't know

317
00:34:50,720 --> 00:34:55,200
you hear a lot about like designing these really complicated reward functions which are often very

318
00:34:55,200 --> 00:35:01,200
clever but which um often I feel like are close to being a pitfall because they very easily become

319
00:35:01,200 --> 00:35:07,200
like disconnected from like the complex world that we're trying to model and so you end up in like

320
00:35:07,200 --> 00:35:14,000
weird local maximums or minimums and um yeah you start like just kind of uh solving the problem

321
00:35:14,000 --> 00:35:19,440
that you've designed versus like the problem which actually exists and so um I just have always

322
00:35:19,440 --> 00:35:24,720
loved the idea that what we should be doing is um kind of self-evidencing and from an intuitive

323
00:35:24,720 --> 00:35:31,200
sense that feels like what it feels like what an intelligence system should do uh yeah

324
00:35:33,840 --> 00:35:37,760
yeah I actually don't have anything interesting to add to that I just agree with Fabie

325
00:35:39,760 --> 00:35:45,760
that explainability and the capacity to explicitly reference previous data including

326
00:35:45,760 --> 00:35:51,360
like leave one out so techniques from non-parametric statistics about the effect of adding in another

327
00:35:51,360 --> 00:35:59,840
piece of data or removing a piece of data um and then just like to bring it to like a homeostatic

328
00:35:59,840 --> 00:36:06,000
setting which is commonly considered an active inference like we're trying to be within a homeostatic

329
00:36:06,000 --> 00:36:15,440
temperature range of 37 yes we could propose reward functions but as those start to include

330
00:36:16,160 --> 00:36:21,280
open-endedness and exploration structure learning just like you described it Fabie like

331
00:36:21,280 --> 00:36:27,440
we're solving the problem as designed rather than the actual question of the homeostatic

332
00:36:27,440 --> 00:36:35,360
temperature and the sort of path of least action first principles physics grounded intelligence

333
00:36:35,360 --> 00:36:40,560
perspective from active inference is like make it the kind of thing that measures itself at 37

334
00:36:41,280 --> 00:36:48,080
and then as long as it is it is and when it isn't it's dead and that's the kind of mortal computing

335
00:36:48,880 --> 00:36:55,440
crossover which is like outside of its zone of surprise it it's not just that it's getting

336
00:36:55,440 --> 00:37:03,440
a bad grade in the class that is like a deeper failure signal than that and to understand okay

337
00:37:03,440 --> 00:37:10,640
when is it a yellow flag when is it a red flag in terms of the new scientific literature coming in

338
00:37:11,680 --> 00:37:24,080
those have plain straightforward ways to interpret that developing larger higher-order

339
00:37:24,160 --> 00:37:29,120
apparatuses will never return to that kind of basal simplicity

340
00:37:31,920 --> 00:37:33,120
yeah couldn't agree more

341
00:37:35,680 --> 00:37:43,120
yeah I mean absolutely it also like the other thing that it can do quite well is deal with

342
00:37:43,920 --> 00:37:54,880
essentially outlier studies so situations where you have a new piece of information that is

343
00:37:55,520 --> 00:38:02,320
strongly conflicting with all the previous pieces of information and trying to sort of

344
00:38:02,320 --> 00:38:08,880
work through what that really means and there's a like there's a degree to which

345
00:38:09,360 --> 00:38:17,440
we can even sort of extend this process to multiple agents that have these belief systems

346
00:38:17,440 --> 00:38:24,240
and then look at sort of consensus of experts or weighted consensus of experts so for instance

347
00:38:24,240 --> 00:38:31,040
you could have like a weather vane type of situation where somebody really over indexes to every new

348
00:38:31,040 --> 00:38:36,480
piece of information and you would do that with you know technically you do that with maybe a power

349
00:38:37,040 --> 00:38:43,040
posterior type thing or you can have somebody who's built in strong priors in a particular

350
00:38:43,040 --> 00:38:52,400
direction and you can then like take your consensus of artificial sort of decision making all of which

351
00:38:52,400 --> 00:39:00,400
has within their universe well-reasoned updates to the data and then you can look and try and

352
00:39:00,400 --> 00:39:10,560
work out what that swarm of experts can tell you and sort of do very empirical things like

353
00:39:11,520 --> 00:39:17,200
try and you know work out which of these experts is doing well at a particular moment in time

354
00:39:18,640 --> 00:39:24,480
because you know there could be there could be times when the world's very or the problem

355
00:39:24,480 --> 00:39:30,800
you're solving is very chaotic in which case the over indexing expert would probably be a pretty

356
00:39:30,800 --> 00:39:38,400
pretty solid bet while there are other times where sort of things are pretty stable and you

357
00:39:38,400 --> 00:39:43,520
probably it would be possible that the sort of the more conservative expert is more

358
00:39:45,600 --> 00:39:52,320
sort of empirically making good decisions and good recommendations so there's like a lot of

359
00:39:52,320 --> 00:40:01,600
ways that we can not just like incorporate these sort of homeostasis ideas but we can also change

360
00:40:01,600 --> 00:40:08,240
what that means for different agents and artificially like do that artificially and then combine them

361
00:40:08,240 --> 00:40:20,720
together to try and get a almost like a like a blanking on the word but you know

362
00:40:22,160 --> 00:40:28,080
a forecast under a sort of a hypothetical set of situations and we can actually sort of bring

363
00:40:28,080 --> 00:40:33,360
those ideas of the world forward and see what happens when they sort of meet with actual information.

364
00:40:33,680 --> 00:40:41,760
Yeah this angle of mixture of experts as it's sometimes called more in the language model space

365
00:40:41,760 --> 00:40:47,760
or ecosystems of shared intelligence or diverse intelligences in the active inference area like

366
00:40:47,760 --> 00:40:56,960
that's very interesting obviously has connections back to human teams and teams of beyond humans and

367
00:40:56,960 --> 00:41:08,000
so on a lot of this is still text based so maybe you did or didn't mention what representation the

368
00:41:08,000 --> 00:41:16,320
base graphs are but they're plain text like and there was a lot of discussion about bringing

369
00:41:16,320 --> 00:41:22,720
from natural language scientific papers or however it is into a structured form and then the

370
00:41:23,280 --> 00:41:30,160
explain method that you showed kind of taking the structured form and just giving a little

371
00:41:30,800 --> 00:41:40,240
syntactic fluency so it looks human readable so how do you see that essence coming into play with

372
00:41:40,240 --> 00:41:48,720
multimodal models and then with action in the world that isn't just developing the next text

373
00:41:48,720 --> 00:41:55,120
token but a robotic actuator or modifying some other control element of the world.

374
00:41:58,000 --> 00:42:02,720
Yeah that's a really good question. I honestly haven't thought much about multimodal stuff in

375
00:42:02,720 --> 00:42:09,760
this particular context but I think the framework is general enough at this point such that it

376
00:42:09,760 --> 00:42:16,720
it's definitely could support lots of different modalities. I'd be really curious to see how

377
00:42:16,720 --> 00:42:25,360
this did with something like audio in particular. Yeah and then to your point about like yeah this

378
00:42:25,360 --> 00:42:32,320
maybe like discrete versus continuous relationship I think I think that's like part of what we're

379
00:42:32,320 --> 00:42:37,600
learning is how to go from like these long natural text documents to a system which is

380
00:42:37,600 --> 00:42:43,840
appropriately discretizing our hypotheses such that we have these like meaningful explanations

381
00:42:43,840 --> 00:42:54,000
like you mentioned so I think yeah I think like continuing to develop like robust ways of

382
00:42:54,560 --> 00:43:00,880
surfacing those explanations is a big part of this as well like over time we're going to observe

383
00:43:00,880 --> 00:43:06,720
lots and lots and lots of evidence how do we make sure like hypotheses don't get stale and how do we

384
00:43:06,720 --> 00:43:12,480
use evidence to know when they are and things like this are part of that also I don't know if

385
00:43:12,480 --> 00:43:15,440
that directly answered your question but that's some of the stuff that I've been thinking about

386
00:43:15,440 --> 00:43:28,160
related to that. So in the I mean in examples like sort of moving towards robotics and sort of tech

387
00:43:28,160 --> 00:43:32,720
video generation and image generation other sort of audio other sort of multimodality is

388
00:43:32,960 --> 00:43:44,720
to be honest I think of these processes in general as enabling us like building a world

389
00:43:44,720 --> 00:43:52,640
model to enable a sort of sequential decision process so if that decision process happens to

390
00:43:52,640 --> 00:43:57,520
be should the robot turn left and that's what the sort of the decision process is it's it's

391
00:43:58,480 --> 00:44:04,080
multimodal in like a very classical sense that you can put any type of decision framework over

392
00:44:04,080 --> 00:44:09,920
the top but it's not sort of generatively multimodal I'm not saying write me a song that

393
00:44:09,920 --> 00:44:16,720
sounds like Beyonce and a song that sounds like Beyonce comes out I think this this sort of this

394
00:44:16,720 --> 00:44:22,080
sort of Bayesian world model layer is blocking towards that sort of thing but that's that's

395
00:44:22,080 --> 00:44:28,640
really not sort of the aim of what we're trying to do it's also like within normal like our

396
00:44:30,240 --> 00:44:36,240
almost I don't want to I don't want to say mantra or manifesto because that sounds culty

397
00:44:36,240 --> 00:44:44,480
and no one wants to sound culty but like our basic aim is to always center like humans within our

398
00:44:44,480 --> 00:44:56,480
process and so some of this multimodal stuff it's less clear where the human lives so for

399
00:44:56,480 --> 00:45:02,240
instance like a video generation type thing where does the human live so keeping it at this abstraction

400
00:45:02,240 --> 00:45:07,040
of sequential decision making then it's a decision that a human could do you know human with their

401
00:45:07,040 --> 00:45:14,640
thumbs could be moving a like a robot around and doing that sort of stuff but yeah it's it's

402
00:45:14,640 --> 00:45:19,280
really all about sort of controllability and auditability for us in sort of a sequential

403
00:45:19,280 --> 00:45:26,160
decision process so to the extent that that sort of leads in its multimodal world that's

404
00:45:27,680 --> 00:45:32,480
that that's sort of part of what we're doing and like to some some versions of multimodality

405
00:45:32,560 --> 00:45:42,080
um is we're just not swearing in that particular space um yeah not a great answer but a long one

406
00:45:45,200 --> 00:45:54,800
let them distill it down later um in the um auditability area it almost falls out to me

407
00:45:54,800 --> 00:45:59,520
to be like a syntax of auditability in a semantics at the syntactic level just

408
00:46:00,480 --> 00:46:06,800
tagging or versioning when a file comes in or when a given computation is executed that is

409
00:46:06,800 --> 00:46:14,400
basically transfer across all settings and then where I see you honing in on with with this work

410
00:46:14,400 --> 00:46:23,360
is kind of the semantic auditability which is actually how we compose our accounts

411
00:46:24,080 --> 00:46:31,280
I would have driven but I decided to walk because this happens and so bringing that

412
00:46:32,480 --> 00:46:42,800
different kind of trace to systems is gonna make it um what will it open up in science or

413
00:46:42,800 --> 00:46:52,560
education or how do you see this sitting at a console somebody is at now and making this

414
00:46:52,560 --> 00:47:00,560
different like over what timeline yeah I mean it's really quite nice for storytelling because

415
00:47:00,560 --> 00:47:08,400
as you said you can say things precisely like well you know because we observed this thing

416
00:47:08,400 --> 00:47:13,440
or because if we had observed something else you know like maybe you can even make statements which

417
00:47:13,440 --> 00:47:20,960
are um yeah conditional in that sense uh I think it does like empower whoever is sitting in front

418
00:47:21,040 --> 00:47:26,400
of this data to feel like really sure about again like that the reasoning engine that like

419
00:47:26,960 --> 00:47:31,840
that went on uh which to me is is pretty different from what it feels like to sit in

420
00:47:31,840 --> 00:47:37,040
front of chat gbt even though it's quite useful often um you know you you try the code and it

421
00:47:37,040 --> 00:47:42,720
works or doesn't work or you like you know ask your friend is this really true um and that feels

422
00:47:42,720 --> 00:47:48,480
pretty different to me from being able to to look at the evidences themselves and say like oh well

423
00:47:48,480 --> 00:47:53,440
actually if this is the reason you think that I know that that evidence is is not true or you

424
00:47:53,440 --> 00:47:59,840
know like you can bring your own human intuition or world model uh in terms of validating or um

425
00:48:00,400 --> 00:48:06,000
yeah super imposing what you believe on top of what this system believes and so that makes it

426
00:48:06,000 --> 00:48:14,160
really easy to make decisions um quickly I think there's also sort of a converse of this which is

427
00:48:14,240 --> 00:48:21,120
that it also makes it clear which evidence was not used to make a decision uh and that can be quite

428
00:48:21,120 --> 00:48:27,840
telling in these situations where you could be worried that a particular type of evidence isn't

429
00:48:27,840 --> 00:48:35,360
being weighted correctly or isn't being um sort of formatted correctly so again like if this is a

430
00:48:36,000 --> 00:48:46,080
sort of a like a system that builds an assistant um that sort of does surfaces all this information

431
00:48:46,080 --> 00:48:52,080
and sort of makes a recommendation with reasoning for a person that person can then look and be like

432
00:48:52,080 --> 00:48:57,440
and they know what the data is they can look at the deck and say you know why didn't you consider

433
00:48:57,440 --> 00:49:03,680
the make of the car or why didn't you consider this or why didn't you consider that and they can

434
00:49:03,680 --> 00:49:11,440
then use their understanding of what's not being prominently used by the model to

435
00:49:12,720 --> 00:49:19,600
sort of sense test like it's it's sort of I mean in some sense that usage of it is a reformulation

436
00:49:19,600 --> 00:49:24,080
of what Phoebe just said where you use your internal world model but it's like I think it's

437
00:49:24,240 --> 00:49:34,560
important to know when evidence is being used and this is like I think you simply cannot get

438
00:49:34,560 --> 00:49:41,440
um from from like a GPT type thing or any sort of like prompting type method we know for instance

439
00:49:41,440 --> 00:49:48,720
that like um the order of the order that you submit your evidence in is probably going to matter

440
00:49:48,720 --> 00:49:55,200
for a prompting based method okay that's obviously not true for a Bayesian update where we have this

441
00:49:55,200 --> 00:50:00,720
sort of this this coherence principle where if you shuffle your data and enter it in a different

442
00:50:00,720 --> 00:50:08,240
way you will get the same posterior up to computational artifacts um so so all of that

443
00:50:08,240 --> 00:50:13,920
is like in my mind is just as important to order ability as the ability to write a report that says

444
00:50:13,920 --> 00:50:23,600
I made this decision for these reasons yeah well that makes me think about this kind of view from

445
00:50:23,600 --> 00:50:28,560
the inside interpretability where the rules help and also knowing what evidence is not used is

446
00:50:28,560 --> 00:50:34,320
importance for compliance and knowing what information like in a healthcare setting was or wasn't

447
00:50:34,320 --> 00:50:47,280
used um what about thermodynamics we heard about free energy boltzmann came up how do you see the

448
00:50:48,000 --> 00:50:55,840
info thermo nexus what have we learned from the last hundred years of thermodynamics and

449
00:50:55,840 --> 00:51:06,080
information theory and all of this and on the software or hardware side how is that kind of a

450
00:51:06,080 --> 00:51:17,200
free energy nexus being used yeah i mean i i'm really excited about how all of this seems to be coming

451
00:51:17,200 --> 00:51:23,520
together um i the free energy just keeps showing up in all of these exciting areas to me we have

452
00:51:23,520 --> 00:51:27,840
like a book club for singular learning theory and like they talk all about free energy too and i

453
00:51:27,840 --> 00:51:33,680
think some of those ideas are really exciting um i mean at normal i think like the thing that i would

454
00:51:33,680 --> 00:51:39,280
highlight is like this idea of software hardware co-design um which is really special uh and so

455
00:51:39,280 --> 00:51:44,320
we're trying to do this hard and fun dance towards each other where we're like thinking about these

456
00:51:44,320 --> 00:51:51,200
new kinds of systems and how they might support each other and um empower each other and and yeah

457
00:51:51,200 --> 00:51:59,120
how to build full stack systems um which is really challenging and and also really exciting um yeah

458
00:51:59,120 --> 00:52:04,240
and i think like from like the first principles of thermodynamics perspective like like we're all

459
00:52:04,240 --> 00:52:11,200
just uh yeah we're all kind of like mathematics and physics people at heart so like going taking

460
00:52:11,200 --> 00:52:16,720
like uh you know all of what people have learned in language modeling and all of that like um very

461
00:52:16,720 --> 00:52:21,440
much to heart as well like i think approaching whatever problem that we're facing from a first

462
00:52:21,440 --> 00:52:26,080
principles how do physical systems work in the world what do we really what are the assumptions

463
00:52:26,080 --> 00:52:32,160
we're really comfortable with uh and building up from there um is definitely our our natural mode

464
00:52:33,040 --> 00:52:36,080
uh so i think that makes it easier to to start working together also

465
00:52:38,080 --> 00:52:45,200
um it's also probably worth saying that we have a sort of a secondary not secondary a very different

466
00:52:45,200 --> 00:52:50,800
stream of interest in thermodynamics as well which is the ways that we can use actual physical

467
00:52:50,800 --> 00:52:58,240
thermo dynamical principles to build hardware that is specifically has sort of noise in it as a

468
00:52:58,240 --> 00:53:05,040
first class citizen and because of that it is particularly well suited to probabilistic tasks

469
00:53:05,840 --> 00:53:12,720
um and so we've we've built if you if anyone wants to look we have a blog i believe the URLs

470
00:53:12,720 --> 00:53:20,240
blog dot normal computing dot ar um and amongst other things that are on it uh there is the very

471
00:53:20,240 --> 00:53:29,200
first demonstration of using physical thermodynamic hardware to actually do computations um is the

472
00:53:29,200 --> 00:53:33,680
computation the most vital computation that we will ever do it's inverting an eight by eight

473
00:53:33,680 --> 00:53:41,360
matrix so no we can do that otherwise um but it it is sort of building up towards this idea that

474
00:53:41,360 --> 00:53:47,040
we can use thermodynamics not just in our modeling and our understanding of the world but also in our

475
00:53:47,040 --> 00:53:57,840
sort of low energy compute stack to actually realize these things um so i think i it's i think

476
00:53:57,840 --> 00:54:06,560
it would be challenging to find a group of people on this earth who have more investment in thermodynamics

477
00:54:06,560 --> 00:54:15,680
and don't work in a physics department uh because we have investment all the way through from

478
00:54:15,680 --> 00:54:21,840
sort of active inference type things all the way down to this like this this hot thing goes there

479
00:54:23,920 --> 00:54:29,120
which is kind of cool i'm not a physicist so i have but but that's my that's my understanding

480
00:54:29,120 --> 00:54:35,520
of thermodynamics this hot thing goes there informative thing goes here hot thing over there

481
00:54:36,240 --> 00:54:49,280
call it a day um yep that it's a really cool fusion with the kind of parsimony and elegance

482
00:54:49,280 --> 00:54:58,240
and the aesthetic of math and physics and first principles and the different parsimony of pragmatism

483
00:54:58,800 --> 00:55:06,560
with the actual material basis like of a synapse the size of the synapse and the kind

484
00:55:06,560 --> 00:55:17,200
of stochasticity that that size alone um entails with like membranes and all of this those stochastic

485
00:55:17,200 --> 00:55:26,320
aspects are leveraged for the compute the synapse is not simply a variance reducing machine and so

486
00:55:29,120 --> 00:55:39,600
it's like both the platonic slash mathematical ish spirit it finds a common home in these real

487
00:55:40,480 --> 00:55:49,600
simple physical demonstrations and um today it feels like there's a big gap between the um

488
00:55:50,320 --> 00:55:58,320
mesoscale computational architectures that you described today that are very much running on

489
00:55:58,880 --> 00:56:06,880
the kind of von Neumann architecture turing completeness paradigm and yet very tantalizingly

490
00:56:06,880 --> 00:56:17,200
close like to a physical object that has a constrained rule de facto like only one thing

491
00:56:17,200 --> 00:56:26,320
can come out of this at a time as long as the funnel is this wide and so bringing the rules

492
00:56:26,320 --> 00:56:35,760
and the regularities of what we call physical things to bear with the fundamental and the

493
00:56:35,760 --> 00:56:48,640
imposed constraints on informational spaces it's very cool directions um one other note about

494
00:56:48,640 --> 00:56:54,800
just where active inference um an action plays a role is um and also you mentioned like the

495
00:56:54,800 --> 00:57:02,480
hypothesis going stale or like sort of data being over relied on um in the proactive stance

496
00:57:02,480 --> 00:57:09,760
where we're using expected free energy or something like it to to calculate future courses of action

497
00:57:09,760 --> 00:57:16,000
over observations that we haven't seen yet moves that haven't been made yet there's an explicit

498
00:57:16,000 --> 00:57:25,280
epistemic value and so that can be diagnosed and observed as a measure of where a given

499
00:57:26,160 --> 00:57:32,400
computation is on the continuum between purely pragmatic value just constraint

500
00:57:32,400 --> 00:57:37,840
satisfying and and realizing preferences and expectations and then the pure epistemic value

501
00:57:37,840 --> 00:57:45,280
where all outcomes are good and the more information gain the better and then being able to take

502
00:57:45,280 --> 00:57:52,080
control of that balance and know amidst changing situations again taking probabilistic or rule

503
00:57:52,080 --> 00:57:59,200
based approach there to when epistemic and pragmatic like gas and break kind of come into play

504
00:57:59,200 --> 00:58:09,520
these are very basal um control knobs or features in active inference that it's just

505
00:58:10,480 --> 00:58:17,600
not going to show up at the 50th layer of scaling is all you need

506
00:58:17,600 --> 00:58:32,640
yeah cool well do you have any other like thoughts or things you want to add or questions or where

507
00:58:32,640 --> 00:58:41,200
things are heading for your works nothing to add at this moment but certainly uh excited to keep

508
00:58:41,200 --> 00:58:51,440
in touch with this community and yeah collaborating yeah absolutely um and we sort of share I mean

509
00:58:51,440 --> 00:58:59,120
we write papers and stuff but we mostly like we share most of what we do be at academic in the

510
00:58:59,120 --> 00:59:05,680
sort of machine learning space or be it in the um sort of the physical hardware space uh on our

511
00:59:05,680 --> 00:59:13,520
blog which is blog.normalcomputing.ai um and yeah thank you so much for inviting us it's been very fun

512
00:59:14,400 --> 00:59:23,120
awesome thank you hope to speak again so peace bye thanks hi

513
00:59:35,680 --> 00:59:36,180
you

