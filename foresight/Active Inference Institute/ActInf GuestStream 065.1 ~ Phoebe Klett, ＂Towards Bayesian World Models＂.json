{"text": " Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe, very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah, I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art language models into more probabilistic machinery and what that might bias. All right. Let's get right into it. All right. So some of the things that I'm hoping to discuss today include why we might use a language model for something that isn't long-form text generation and how we might do that, and then motivate a little bit kind of why we might need a world model, what simple self-organizing world models might look like, and even in the simplest cases how we might start to use those as effective recommendation engines in the wild today, and then a little bit of discussion about kind of where this research is going. All right. So what are language models good at? Today's especially large language models are trained on next token prediction. So this means given a sequence of tokens, we're going to estimate which token is most likely to come next. Maybe rephrasing that a little bit. We might also say that language models are trained to estimate which sequences of tokens or words are likely. The caveat being likely to appear in the training set, but today our training sets are quite extensive. So given this simple training objective, it's arguably surprising that we've seen language models do as well as they have in really impressive tasks. So they start to demonstrate like really great language understanding, meaning like the semantics of language itself, not just the syntax of which sentences make sense. They've also started to demonstrate some world knowledge, so implicitly learning kind of how the world works just through our own human abstraction and how we articulate that. We start to see language models falter at tasks, which look more like symbolic problem solving. So math in particular, we see this in programming, although we're getting better at this. And in general, any kind of long-term planning tasks, which require abstract reasoning. So we see this also in problems that look like this, which are word problems, but which are really about understanding abstractly how these abstract objects relate to each other. And again, this shouldn't be surprising that language models struggle to do things like this, since it's so far off from their original training objective. And in particular, even when we arrive at the right answer in some of these cases, it's really hard to know after the fact kind of the reasoning for how we ended up at the right answer in this particular case. And maybe more fundamentally, if we don't know where the abstraction is happening or where the reasoning is happening, it's very hard to guide that process. And so this starts to motivate the need for something that looks more like an explicit world model. And now, to start to borrow some words from Yasha Benjio, this is one of the kind of biggest issues with today's language models, is arguably that we're asking the language model to be both the inference machine and the world model implicitly, when this doesn't quite make sense. And so some things that we might hope for in a world model are to model causal relationships, to be really adapt at modeling, using uncertainty, and to be modular. Yann LeCun puts this in a similar way, so we might kind of ask a world model to be able to distinguish between which details are important versus irrelevant, and to be able to make predictions that can be performed in sort of this abstract space of representations. And so hopefully in the next following slides, we're going to motivate what it might look like to use language models as inference machines in maybe like the simplest case of self-organizing world models. And even in those most simple cases, how we start to get something much closer to the kind of explainable reasoning, which I think lots of us are grasping for right now. All right, so in a very simple case, we might think of a world model as simply a collection of hypotheses, where we model confidence over each of those hypotheses. And in particular, we care about predictive world models. So given some evidence or data that we've observed in the world, we'd like to propose the world models, which best explain the evidence that we've observed. And if that language sounded leading, it was indeed. We are proposing in this case to use Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given some new evidence. And so when our hypotheses are kind of these Bernoulli random variables, this bottom term can simply be expanded into this equation on the slide. And so the tricky part in this computation is the likelihood piece. So given this evidence, given that our hypothesis is true, what's the likelihood that we would have observed the evidence? And the claim that we're making is that language models are actually, this is like a natural task to ask of a language model. When our evidence and our hypothesis are both semantic objects, as we discussed, language models are kind of trained exactly to understand which sequences of text are likely. So asking it to prompting it in a clever way such that we can extract this particular likelihood is actually really natural. So how do we do this? So we come up with this kind of clever prompting scheme that allows us to extract again exactly how likely is this should have occurred. So given that our hypothesis is H in this setting and our evidence is this curly E, we only need to change our evidence into the conditional form and then phrase the question like this to the language model. So the input sequence of text goes like, given that potentially if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic, would we have observed the evidence that Walmart laid off 10% of the material staff? And so either using something like few shot prompting or guidance or some kind of control generation technique, we can ensure that our language model outputs either yes or no, and then use the logits from its answer to estimate that probability. And again, the claim here is that this is actually a really natural task to ask of a language model that leverages its innate reasoning engine better than just kind of allowing it to ramble using text sometimes. And we can even make this updating scheme a bit more sophisticated using things like precision waiting. So this will bias our posterior over hypothesis either towards our prior or our evidence that we've observed based on how confident we are in our prior and in our evidence. All right. So the world models that we've seen so far are fairly simple. In particular, we're modeling all our hypotheses as independent from each other. And this seems like a large simplifying assumption. So how might we support more complex world models where we condition our hypotheses on each other? All right. So another thing that hopefully you'll be familiar to this crowd, Bayes and Nets have been around for forever and are often used to model mechanistic failures and enlarge systems and can be understood simply as distribution where each variable depends on some small number of ancestor variables. Perhaps more intuitively, we can also think of these as directed graphs where we have edges between variables which are directed in the sense that the parent is conditioned on the child. So let's see an example. All right. So now we have two different kinds of hypotheses. The ones which are kind of parents in our simple setup are kind of qualitatively more abstract than the children. So our two children nodes are similar to the hypothesis we saw before. Mark a sentiment for Best Buy is Poor or Walmart will grow its physical footprint this year. And then the more abstract hypothesis being retailers were negatively impacted by COVID-19. And so then we specify this conditional structure either kind of in a classical sense by specifying all of the joint probabilities upfront or alternatively learning it given some healthy amount of training data. And again, we're proposing that instead of doing that intensive process, we can use language models to extract those probabilities in a natural way. And so one natural complaint at this point might be like, well, this space is going to start to get very large. If we're trying to update these beliefs in real time given some large data stream is going to become intractable given our current framework. So how might we start to augment that? Luckily, there's been a lot of work historically done on this problem. And we can start to use things like message passing to update our beliefs in an approximate way. So as long as our Bayes net has a tree dependency structure, we can use things like the sum product algorithm to update our beliefs. And I don't want you to worry too much about the equations on this slide. If you've seen this before, this kind of recursive structure will look familiar. But if not, the general kind of intuitive idea that we're going to compute these messages from all of the children or the neighboring nodes and use those to propagate through the graph to update each independent belief. Let's just see an example. All right, so suppose we observed some new evidence and we'd like to know how should we update probability over hypothesis A, which as you remember is the parent node in our graph. All right, so B here on the left hand side is the belief. XA is essentially like just the variable representing the hypothesis A. And then the fee and the side terms are specified by our language model or the the Bayes net in the classical sense. And so we're summing over all of the variables, all of the values that the variables XB and XC can take, which in our case, of course, is just zero and one. So if you, you might recognize at this point that this is indeed the exact marginal probability for the hypothesis A, and that's because our graph is simply connected in this example. So in general, this isn't true, but it turns out to be the case in our example. And again, we can compute these fee and side terms using the language model itself. All right, so because we're all lovers of free energy here, I'm going to walk through kind of how this is maybe the first example of a self evidencing or minimizing free energy kind of model. So as we noted, belief propagation isn't exact for more complicated graphs. And so it makes sense and might be useful to ask the question, you know, how far apart or when are our beliefs close to the exact marginals. And so we often use things like KL divergence to compare the difference between two probability distributions. And that is explicated on the right hand side. And then those of us who are like have a background in physics might recognize Boltzmann's law as well. So this is just the idea that we might represent the probability of a given state using an energy function. And we're not going to accept this as truth, maybe some of us have done in the past, but we are going to just use it as a definition for this energy function, such that when we plug in that term here, and expand out, we start to see these first two terms look a lot like the kind of energy and entropy functions which we are used to. And indeed, we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy to see this all coming together. And in particular, it might make sense just to note at this point that using world models which are self organizing in the sense seems to be very compelling, since the kind of world model which we want is one which promotes the evidence that we've seen so far. Why is it useful to formulate this in terms of free energy, besides the fact that we all find it compelling here? Well, you can make a lot of progress by constructing analytically tractable approximations of Gibbs free energy often. I'm not going to go into the details here, but here are two examples where that's been fruitful. All right. So now I'm going to chat briefly about kind of how we might use recommendation systems like, or how we might use these systems as recommendation systems, and indeed in the world today. All right. So one prime example for a system like this might be useful is a situation where we have lots of data incoming at very high frequencies, and we always want to have some set of naturally discrete hypotheses that we're modeling beliefs over which are being kept up to date at a very regular cadence. And so actually, a lot of the the muscle here is just reformatting documents or however our data comes in as evidences. This is not always obvious or easy to do. But once you've kind of figured out that part, and in particular, we've been using things like RAG, retrieval augmented generation, or embedding based systems to kind of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind of built up that machinery, the actual updating computations, as we've shown already, is actually pretty simple. So we do these likelihood computations and we update our beliefs. And then at any given time, we can query that model for our marginal distribution over any given hypothesis. And it turns out that this kind of setup has many practical applications. It's also noteworthy that even with very simple systems like these, these are like out of the box, controllable and explainable. So just by storing the magnitude and the direction of the update to the posterior for each piece of evidence that we observe, we have a very natural built in explanation for our belief at any given time. And that makes kind of like these applications where folks might really like to use a language model, but really require like a robust, like causal relationship between the outputs and the explanation, which you don't get from a language model on its own. A system like this can be very appealing in those situations. All right, so now on to further work. So everything that we've discussed today is early work towards integrating language models into more probabilistic frameworks. And there's been a lot of exciting work done in this vein right now. Some important questions which are especially interesting to me are which parts of the world model should be learned versus encoded? And how do we want intelligence to scale? Both in the sense of composing systems naturally, there should be some very like natural way that we can compose to intelligent systems and also such that we can scale them with compute. And I don't mean to restrict myself either to the kinds of compute that we have today. We're also working at some exciting new computing paradigms at normal, which might be more compatible with software of this nature. Also the two folks that we referenced at the beginning of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area, which is very inspiring. And so in particular, G flow nets are also probabilistic graphical models, which I think folks will find a natural next step in reading if you so desire. And that's it for me. Awesome. Thank you. Wow, very cool. Dan, do you want to give a first reflection or thought? And then meanwhile, anyone who's watching live, please feel free to write questions. I'll relay them in. Absolutely. So hi, I'm Dan. I work with Phoebe on this project. And yes, the I think the thing that's most exciting about this for me personally is sort of twofold. One of them is that it's a way of avoiding sort of having to trust a language model to understand and reason about text. Because they're not it's not that bad. The thing is that the extremely strange thing about language models is they're quite good at being almost good enough. But they're never quite what you could use. You could never use a language model to, I don't know, sort of triage, like an important sort of situation where a bunch of different things are coming in, you have to make a decision about which is important. The reason you can't do that is you simply cannot understand the encoded biases. You cannot get it to reliably generate reasoning. You can ask it for reasoning. But the thing that it prints out is not the reasoning that it used internally because it doesn't reason. Fundamentally, while these have input and output that are natural language, they are not artificially intelligent. They are just prediction machines. And so we have to be very careful about not anthropomorphizing them. So this is a way of using those incredibly powerful prediction machines in a framework where we can make sure that we essentially keep a record of what we're doing so that a human can look at it. Because, I mean, there's a lot of talk in this world about sort of post-human AI and those sorts of things. The idea that the machines will become intelligent enough, or the machines will rise up in a slightly more alarming type of way. And that's all great and wonderful, but that's not particularly interesting to us at normal. We're much more interested in sort of having mimicking explicit decision processes so that a human can audit them and can make these things work. That's kind of the area that we're coming from. Awesome. All right. I'll go to a question from the chat. So Josh asks, great talk. Where does hypothesis relevance enter the calculus? Is it folded into confidence? Not sure if it ought to be. Just saw hypothesis relevance mentioned. Hypothesis relevance. Does that mean like which hypotheses are conditioned on each other? Is it possible to ask a clarifying question there? Maybe Dan, you have a better idea. They can follow up. But yeah, I also wondered about this. You might know what was relevance. Maybe the temperature and the rainfall were relevant, but then how does this approach help us understand when one of those relevant factors no longer is relevant or when a new relevant factor comes into play? Yeah, these are great questions. So I think in terms of understanding in an automatic sense, when two hypotheses are relevant to each other, we can leverage embedding type language models for this kind of thing also. If we don't have a more kind of like structured human intuitive sense for when two hypotheses are related, in terms of like how those relationships evolve over time, this is something that's really interesting to me. And I think looking at the theory behind structure learning or when we propose to add new nodes to the network or propose to add a new edge to the network or things like this is a really exciting research direction. Although I don't have like a silver bullet answer to how we should do that. Just to like add a little bit more to that, it is like it is a really interesting research direction. Like one of the things that Phoebe mentioned in the talk is that there is a difficult step that we're not talking about, which is actually translating this natural language into reasonable hypotheses. So there is a step in there where you take essentially a chunk of text and you have to decide if this is a hypothesis, if this is a hypothesis we've seen before, if this is a sub hypothesis or a clarification of a hypothesis that we've seen before, and so on and so forth. So that in some sense part of the data processing and it is an important step and one that we are sort of continuing to work on and refine. The other thing like a different sort of interpretation of the question around relevance is around sort of is the hypothesis relevant to the thing that you're looking at? I mean we could have a hypothesis the sky is blue, but if we are deciding to deciding you know whether or not we need to check that part's oil, like the truth or not of the color of the sky is very irrelevant. And that then comes into the nice thing about having your world described as a collection of statements with truth values associated with them in that you can directly reason over them. So you can put a classical decision framework over that to take into account both the sort of the knowledge you have of the world and also which parts of these worlds are sort of unknown. So in that sort of situation the person using the world model to construct a sort of decision or an output will be responsible in some sense for assigning a weight or a cost to each hypothesis being true. And for some of those hypotheses obviously it will be zero because again we do not care about the color of the sky if all I want to know is if I need to change the oil in my car. So that's the sort of the other end of the answer. So there's a version of the answer at the start of the information flow and there's a version of the answer at the end of the information flow. But it is a tricky point and one that we are sort of continuing to iterate on to try and find sort of good ways on both ends of that. Yeah well a lot there. It's very interesting how in that presentation in response I heard both about probability distributions on rules and rules on probability distributions and like which one whether it's the tail wagging the dog or the horse in the cart how to design these synthetic intelligence systems that appropriately bring together aspects that are more symbolic more rule like and then more probabilistic more embedding like. So where does that end with you or how do you see the design of these systems with mixed symbolic and probabilistic components? Yeah yeah that's a great point and I think this really gets it like which parts of the world model should be learned or should be represented in some like more discrete space versus like encoded based on our own human intuition for rules and structure. And I think like maybe this would be fairly represented as a cop-out answer but I think it depends a lot on the application. I think like when we're developing systems like this and just trying to iterate through as many different hypotheses as you can quickly like choosing an application and benchmarking and testing and seeing like what actually works is a go-to strategy for us in terms of like well which parts should be fixed and are actually helpful to increase reliability such that like we can use our human intuition for how this particular you know system is built versus like well this is something that we we want uncertainty over that's like a really important part of the learning process for us in terms of yeah that kind of iteration so I think it probably depends on the application. Yeah it's um it it definitely depends on the application it's also like it depends on where the actual challenge points are so we've got like outside of this we've got sort of a few other things that we've released publicly that kind of look at this idea of there being like external rules to the system and whether or not we can add those in. So one of them is something called constrained generation where we sort of force the model to only produce something valid and that's sort of quite a useful way of removing one particular aspect of stress from the model which is that it may make sort of syntactically or sort of somehow incoherent outputs that don't follow the rules and then we can then focus with the rest of our energy we can then take that as given and focus with the rest of our energy on improving the bit that we don't have rules for. So those sorts of things and sort of a different version is trying to improve something by saying no you broke a rule we need to like go back and make this sort of true so this kind of sort of chain of thought prompting type of idea. So so yeah the the symbolic and the probabilistic I think in our minds live very closely together as two tools that don't completely solve the same problem and I think there's sort of in in the world of I'm not sure how familiar anyone in the audience is with language modeling but like in the world of language modeling before this sort of explosion of neural networks and artificial intelligence type methods there was a lot of work on symbolics of language and grammars and all of that sort of stuff and that work pushed quite a long way forward and this work is pushing quite a long way forward and I suspect the next thing is going to involve them joining up again because they each have good points they each have bad points and you know two wrongs don't necessarily make a right but they can make the less wrong. Nice yeah recently we heard from Elliot Murphy and talking about the neuro linguistics and about how the statistics of language are not the rules of language you can always come up with a new expression that's never been uttered that's not going to be in the training distribution or any distribution. Okay I'll ask a question in chat from Upcycle Club they wrote what are some of the key challenges associated with developing such Bayesian world models? Hmm I think we've touched on a bunch of them the ones that are most top of mind for me right now are the structure learning thing that came up so how do we understand like when to propose new hypotheses and how to integrate those into the models and then yeah just figuring out like yeah I guess this like proposal and evolution process of the nodes themselves since everything else like the framework like works pretty automatically and in a reasonable way thank you Bayes thank you to the development of language models but kind of moving from this like more discrete case into a continuous case which like more fully represents the space that we're learning over can be challenging. Yeah I would also say that like it's a sort of a maxism that max maxism what on earth did I just say there's a there's a common saying let's go in that direction there's a common saying in this world that um that sort of no model ever survives its first encounter with data um and that that becomes true here as well so there's lots of like as we've been building these things and using them we found lots of little spiky edge corners with sort of making sure that the language world is actually doing what we want it to do so there are a lot of questions in building these things around how do you actually test that the components of it are actually working the way you want and then on like a broader level how do you compare something that is fundamentally trying to solve a different problem to other methods so we are solving a problem under the constraints that we want a fully auditable system we could also solve all of these problems by a thing called in-context learning which is basically putting the context into the prompt of a large language model and asking it the answer and that also works especially when you've got things like GPT-4 which are just wonders and glories um it works really well so then we come to the question of how do we actually make the case from this from a like a bigger picture perspective can it be more than just a like can we find benchmarks that reflect um the structural advantages of this approach over something like in-context learning that don't come across as false so that's kind of like a a stranger answer because it's not really about like actually developing the world model it's about sort of convincing other people that it's a good idea um and that's you know that that is a thing that is true of essentially all of the things on this slide as well they are all quite complex and odd little methods um that you know there's a there's a degree to which well we definitely can solve this an easier way um so what is the thing that what is the the the application or the benchmark where we can say no if you do it the easier way you will fail at this measurable thing very interesting um so you mentioned the self-evidencing advantages of using world models that are self-evidencing rather than reward maximizing for example so how do you see that playing out and I can connect it back to active inference of course but how do you see this self-evidencing centrality play out in the kinds of models described here yeah I think there are a couple reasons why it's so compelling to me uh and the first just has to do with explainability right like it's really convincing to people to say like well why why did we predict this why do we believe this well this is the actual real world data that we've observed such that you know this this is the impact that that's had uh and then I think like uh I don't know you hear a lot about like designing these really complicated reward functions which are often very clever but which um often I feel like are close to being a pitfall because they very easily become like disconnected from like the complex world that we're trying to model and so you end up in like weird local maximums or minimums and um yeah you start like just kind of uh solving the problem that you've designed versus like the problem which actually exists and so um I just have always loved the idea that what we should be doing is um kind of self-evidencing and from an intuitive sense that feels like what it feels like what an intelligence system should do uh yeah yeah I actually don't have anything interesting to add to that I just agree with Fabie that explainability and the capacity to explicitly reference previous data including like leave one out so techniques from non-parametric statistics about the effect of adding in another piece of data or removing a piece of data um and then just like to bring it to like a homeostatic setting which is commonly considered an active inference like we're trying to be within a homeostatic temperature range of 37 yes we could propose reward functions but as those start to include open-endedness and exploration structure learning just like you described it Fabie like we're solving the problem as designed rather than the actual question of the homeostatic temperature and the sort of path of least action first principles physics grounded intelligence perspective from active inference is like make it the kind of thing that measures itself at 37 and then as long as it is it is and when it isn't it's dead and that's the kind of mortal computing crossover which is like outside of its zone of surprise it it's not just that it's getting a bad grade in the class that is like a deeper failure signal than that and to understand okay when is it a yellow flag when is it a red flag in terms of the new scientific literature coming in those have plain straightforward ways to interpret that developing larger higher-order apparatuses will never return to that kind of basal simplicity yeah couldn't agree more yeah I mean absolutely it also like the other thing that it can do quite well is deal with essentially outlier studies so situations where you have a new piece of information that is strongly conflicting with all the previous pieces of information and trying to sort of work through what that really means and there's a like there's a degree to which we can even sort of extend this process to multiple agents that have these belief systems and then look at sort of consensus of experts or weighted consensus of experts so for instance you could have like a weather vane type of situation where somebody really over indexes to every new piece of information and you would do that with you know technically you do that with maybe a power posterior type thing or you can have somebody who's built in strong priors in a particular direction and you can then like take your consensus of artificial sort of decision making all of which has within their universe well-reasoned updates to the data and then you can look and try and work out what that swarm of experts can tell you and sort of do very empirical things like try and you know work out which of these experts is doing well at a particular moment in time because you know there could be there could be times when the world's very or the problem you're solving is very chaotic in which case the over indexing expert would probably be a pretty pretty solid bet while there are other times where sort of things are pretty stable and you probably it would be possible that the sort of the more conservative expert is more sort of empirically making good decisions and good recommendations so there's like a lot of ways that we can not just like incorporate these sort of homeostasis ideas but we can also change what that means for different agents and artificially like do that artificially and then combine them together to try and get a almost like a like a blanking on the word but you know a forecast under a sort of a hypothetical set of situations and we can actually sort of bring those ideas of the world forward and see what happens when they sort of meet with actual information. Yeah this angle of mixture of experts as it's sometimes called more in the language model space or ecosystems of shared intelligence or diverse intelligences in the active inference area like that's very interesting obviously has connections back to human teams and teams of beyond humans and so on a lot of this is still text based so maybe you did or didn't mention what representation the base graphs are but they're plain text like and there was a lot of discussion about bringing from natural language scientific papers or however it is into a structured form and then the explain method that you showed kind of taking the structured form and just giving a little syntactic fluency so it looks human readable so how do you see that essence coming into play with multimodal models and then with action in the world that isn't just developing the next text token but a robotic actuator or modifying some other control element of the world. Yeah that's a really good question. I honestly haven't thought much about multimodal stuff in this particular context but I think the framework is general enough at this point such that it it's definitely could support lots of different modalities. I'd be really curious to see how this did with something like audio in particular. Yeah and then to your point about like yeah this maybe like discrete versus continuous relationship I think I think that's like part of what we're learning is how to go from like these long natural text documents to a system which is appropriately discretizing our hypotheses such that we have these like meaningful explanations like you mentioned so I think yeah I think like continuing to develop like robust ways of surfacing those explanations is a big part of this as well like over time we're going to observe lots and lots and lots of evidence how do we make sure like hypotheses don't get stale and how do we use evidence to know when they are and things like this are part of that also I don't know if that directly answered your question but that's some of the stuff that I've been thinking about related to that. So in the I mean in examples like sort of moving towards robotics and sort of tech video generation and image generation other sort of audio other sort of multimodality is to be honest I think of these processes in general as enabling us like building a world model to enable a sort of sequential decision process so if that decision process happens to be should the robot turn left and that's what the sort of the decision process is it's it's multimodal in like a very classical sense that you can put any type of decision framework over the top but it's not sort of generatively multimodal I'm not saying write me a song that sounds like Beyonce and a song that sounds like Beyonce comes out I think this this sort of this sort of Bayesian world model layer is blocking towards that sort of thing but that's that's really not sort of the aim of what we're trying to do it's also like within normal like our almost I don't want to I don't want to say mantra or manifesto because that sounds culty and no one wants to sound culty but like our basic aim is to always center like humans within our process and so some of this multimodal stuff it's less clear where the human lives so for instance like a video generation type thing where does the human live so keeping it at this abstraction of sequential decision making then it's a decision that a human could do you know human with their thumbs could be moving a like a robot around and doing that sort of stuff but yeah it's it's really all about sort of controllability and auditability for us in sort of a sequential decision process so to the extent that that sort of leads in its multimodal world that's that that's sort of part of what we're doing and like to some some versions of multimodality um is we're just not swearing in that particular space um yeah not a great answer but a long one let them distill it down later um in the um auditability area it almost falls out to me to be like a syntax of auditability in a semantics at the syntactic level just tagging or versioning when a file comes in or when a given computation is executed that is basically transfer across all settings and then where I see you honing in on with with this work is kind of the semantic auditability which is actually how we compose our accounts I would have driven but I decided to walk because this happens and so bringing that different kind of trace to systems is gonna make it um what will it open up in science or education or how do you see this sitting at a console somebody is at now and making this different like over what timeline yeah I mean it's really quite nice for storytelling because as you said you can say things precisely like well you know because we observed this thing or because if we had observed something else you know like maybe you can even make statements which are um yeah conditional in that sense uh I think it does like empower whoever is sitting in front of this data to feel like really sure about again like that the reasoning engine that like that went on uh which to me is is pretty different from what it feels like to sit in front of chat gbt even though it's quite useful often um you know you you try the code and it works or doesn't work or you like you know ask your friend is this really true um and that feels pretty different to me from being able to to look at the evidences themselves and say like oh well actually if this is the reason you think that I know that that evidence is is not true or you know like you can bring your own human intuition or world model uh in terms of validating or um yeah super imposing what you believe on top of what this system believes and so that makes it really easy to make decisions um quickly I think there's also sort of a converse of this which is that it also makes it clear which evidence was not used to make a decision uh and that can be quite telling in these situations where you could be worried that a particular type of evidence isn't being weighted correctly or isn't being um sort of formatted correctly so again like if this is a sort of a like a system that builds an assistant um that sort of does surfaces all this information and sort of makes a recommendation with reasoning for a person that person can then look and be like and they know what the data is they can look at the deck and say you know why didn't you consider the make of the car or why didn't you consider this or why didn't you consider that and they can then use their understanding of what's not being prominently used by the model to sort of sense test like it's it's sort of I mean in some sense that usage of it is a reformulation of what Phoebe just said where you use your internal world model but it's like I think it's important to know when evidence is being used and this is like I think you simply cannot get um from from like a GPT type thing or any sort of like prompting type method we know for instance that like um the order of the order that you submit your evidence in is probably going to matter for a prompting based method okay that's obviously not true for a Bayesian update where we have this sort of this this coherence principle where if you shuffle your data and enter it in a different way you will get the same posterior up to computational artifacts um so so all of that is like in my mind is just as important to order ability as the ability to write a report that says I made this decision for these reasons yeah well that makes me think about this kind of view from the inside interpretability where the rules help and also knowing what evidence is not used is importance for compliance and knowing what information like in a healthcare setting was or wasn't used um what about thermodynamics we heard about free energy boltzmann came up how do you see the info thermo nexus what have we learned from the last hundred years of thermodynamics and information theory and all of this and on the software or hardware side how is that kind of a free energy nexus being used yeah i mean i i'm really excited about how all of this seems to be coming together um i the free energy just keeps showing up in all of these exciting areas to me we have like a book club for singular learning theory and like they talk all about free energy too and i think some of those ideas are really exciting um i mean at normal i think like the thing that i would highlight is like this idea of software hardware co-design um which is really special uh and so we're trying to do this hard and fun dance towards each other where we're like thinking about these new kinds of systems and how they might support each other and um empower each other and and yeah how to build full stack systems um which is really challenging and and also really exciting um yeah and i think like from like the first principles of thermodynamics perspective like like we're all just uh yeah we're all kind of like mathematics and physics people at heart so like going taking like uh you know all of what people have learned in language modeling and all of that like um very much to heart as well like i think approaching whatever problem that we're facing from a first principles how do physical systems work in the world what do we really what are the assumptions we're really comfortable with uh and building up from there um is definitely our our natural mode uh so i think that makes it easier to to start working together also um it's also probably worth saying that we have a sort of a secondary not secondary a very different stream of interest in thermodynamics as well which is the ways that we can use actual physical thermo dynamical principles to build hardware that is specifically has sort of noise in it as a first class citizen and because of that it is particularly well suited to probabilistic tasks um and so we've we've built if you if anyone wants to look we have a blog i believe the URLs blog dot normal computing dot ar um and amongst other things that are on it uh there is the very first demonstration of using physical thermodynamic hardware to actually do computations um is the computation the most vital computation that we will ever do it's inverting an eight by eight matrix so no we can do that otherwise um but it it is sort of building up towards this idea that we can use thermodynamics not just in our modeling and our understanding of the world but also in our sort of low energy compute stack to actually realize these things um so i think i it's i think it would be challenging to find a group of people on this earth who have more investment in thermodynamics and don't work in a physics department uh because we have investment all the way through from sort of active inference type things all the way down to this like this this hot thing goes there which is kind of cool i'm not a physicist so i have but but that's my that's my understanding of thermodynamics this hot thing goes there informative thing goes here hot thing over there call it a day um yep that it's a really cool fusion with the kind of parsimony and elegance and the aesthetic of math and physics and first principles and the different parsimony of pragmatism with the actual material basis like of a synapse the size of the synapse and the kind of stochasticity that that size alone um entails with like membranes and all of this those stochastic aspects are leveraged for the compute the synapse is not simply a variance reducing machine and so it's like both the platonic slash mathematical ish spirit it finds a common home in these real simple physical demonstrations and um today it feels like there's a big gap between the um mesoscale computational architectures that you described today that are very much running on the kind of von Neumann architecture turing completeness paradigm and yet very tantalizingly close like to a physical object that has a constrained rule de facto like only one thing can come out of this at a time as long as the funnel is this wide and so bringing the rules and the regularities of what we call physical things to bear with the fundamental and the imposed constraints on informational spaces it's very cool directions um one other note about just where active inference um an action plays a role is um and also you mentioned like the hypothesis going stale or like sort of data being over relied on um in the proactive stance where we're using expected free energy or something like it to to calculate future courses of action over observations that we haven't seen yet moves that haven't been made yet there's an explicit epistemic value and so that can be diagnosed and observed as a measure of where a given computation is on the continuum between purely pragmatic value just constraint satisfying and and realizing preferences and expectations and then the pure epistemic value where all outcomes are good and the more information gain the better and then being able to take control of that balance and know amidst changing situations again taking probabilistic or rule based approach there to when epistemic and pragmatic like gas and break kind of come into play these are very basal um control knobs or features in active inference that it's just not going to show up at the 50th layer of scaling is all you need yeah cool well do you have any other like thoughts or things you want to add or questions or where things are heading for your works nothing to add at this moment but certainly uh excited to keep in touch with this community and yeah collaborating yeah absolutely um and we sort of share I mean we write papers and stuff but we mostly like we share most of what we do be at academic in the sort of machine learning space or be it in the um sort of the physical hardware space uh on our blog which is blog.normalcomputing.ai um and yeah thank you so much for inviting us it's been very fun awesome thank you hope to speak again so peace bye thanks hi you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.120000000000001, "text": " Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll", "tokens": [50364, 2425, 293, 2928, 13, 639, 307, 26635, 682, 5158, 31988, 1572, 11624, 13, 16, 13, 467, 311, 7687, 1386, 11, 44377, 13, 492, 603, 50920], "temperature": 0.0, "avg_logprob": -0.2657571712010343, "compression_ratio": 1.3459715639810426, "no_speech_prob": 0.27652958035469055}, {"id": 1, "seek": 0, "start": 11.120000000000001, "end": 17.52, "text": " be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent", "tokens": [50920, 312, 4763, 490, 14936, 48593, 591, 32547, 293, 3394, 38184, 322, 7840, 42434, 1002, 5245, 337, 2903, 712, 12737, 51240], "temperature": 0.0, "avg_logprob": -0.2657571712010343, "compression_ratio": 1.3459715639810426, "no_speech_prob": 0.27652958035469055}, {"id": 2, "seek": 0, "start": 17.52, "end": 24.32, "text": " reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe,", "tokens": [51240, 21577, 13, 821, 486, 312, 257, 5860, 6263, 538, 257, 5017, 13, 407, 1309, 291, 337, 5549, 14936, 48593, 11, 51580], "temperature": 0.0, "avg_logprob": -0.2657571712010343, "compression_ratio": 1.3459715639810426, "no_speech_prob": 0.27652958035469055}, {"id": 3, "seek": 2432, "start": 24.32, "end": 30.8, "text": " very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah,", "tokens": [50364, 588, 709, 13, 11053, 2128, 281, 264, 5860, 281, 291, 13, 10391, 13, 2561, 337, 1419, 505, 13, 865, 11, 50688], "temperature": 0.0, "avg_logprob": -0.10627709404896882, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.023670291528105736}, {"id": 4, "seek": 2432, "start": 30.8, "end": 35.44, "text": " I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art", "tokens": [50688, 286, 478, 534, 2919, 281, 5081, 365, 291, 439, 466, 577, 321, 1062, 722, 281, 13365, 965, 311, 1785, 12, 2670, 12, 3322, 12, 446, 50920], "temperature": 0.0, "avg_logprob": -0.10627709404896882, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.023670291528105736}, {"id": 5, "seek": 2432, "start": 35.44, "end": 43.28, "text": " language models into more probabilistic machinery and what that might bias. All right. Let's get", "tokens": [50920, 2856, 5245, 666, 544, 31959, 3142, 27302, 293, 437, 300, 1062, 12577, 13, 1057, 558, 13, 961, 311, 483, 51312], "temperature": 0.0, "avg_logprob": -0.10627709404896882, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.023670291528105736}, {"id": 6, "seek": 2432, "start": 43.28, "end": 49.120000000000005, "text": " right into it. All right. So some of the things that I'm hoping to discuss today include why we", "tokens": [51312, 558, 666, 309, 13, 1057, 558, 13, 407, 512, 295, 264, 721, 300, 286, 478, 7159, 281, 2248, 965, 4090, 983, 321, 51604], "temperature": 0.0, "avg_logprob": -0.10627709404896882, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.023670291528105736}, {"id": 7, "seek": 2432, "start": 49.120000000000005, "end": 54.24, "text": " might use a language model for something that isn't long-form text generation and how we might do", "tokens": [51604, 1062, 764, 257, 2856, 2316, 337, 746, 300, 1943, 380, 938, 12, 837, 2487, 5125, 293, 577, 321, 1062, 360, 51860], "temperature": 0.0, "avg_logprob": -0.10627709404896882, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.023670291528105736}, {"id": 8, "seek": 5424, "start": 54.24, "end": 60.64, "text": " that, and then motivate a little bit kind of why we might need a world model, what simple", "tokens": [50364, 300, 11, 293, 550, 28497, 257, 707, 857, 733, 295, 983, 321, 1062, 643, 257, 1002, 2316, 11, 437, 2199, 50684], "temperature": 0.0, "avg_logprob": -0.09175837441776576, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0005355877219699323}, {"id": 9, "seek": 5424, "start": 61.28, "end": 66.96000000000001, "text": " self-organizing world models might look like, and even in the simplest cases how we might start to", "tokens": [50716, 2698, 12, 12372, 3319, 1002, 5245, 1062, 574, 411, 11, 293, 754, 294, 264, 22811, 3331, 577, 321, 1062, 722, 281, 51000], "temperature": 0.0, "avg_logprob": -0.09175837441776576, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0005355877219699323}, {"id": 10, "seek": 5424, "start": 66.96000000000001, "end": 72.48, "text": " use those as effective recommendation engines in the wild today, and then a little bit of discussion", "tokens": [51000, 764, 729, 382, 4942, 11879, 12982, 294, 264, 4868, 965, 11, 293, 550, 257, 707, 857, 295, 5017, 51276], "temperature": 0.0, "avg_logprob": -0.09175837441776576, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0005355877219699323}, {"id": 11, "seek": 5424, "start": 72.48, "end": 82.56, "text": " about kind of where this research is going. All right. So what are language models good at?", "tokens": [51276, 466, 733, 295, 689, 341, 2132, 307, 516, 13, 1057, 558, 13, 407, 437, 366, 2856, 5245, 665, 412, 30, 51780], "temperature": 0.0, "avg_logprob": -0.09175837441776576, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0005355877219699323}, {"id": 12, "seek": 8256, "start": 83.52, "end": 88.08, "text": " Today's especially large language models are trained on next token prediction.", "tokens": [50412, 2692, 311, 2318, 2416, 2856, 5245, 366, 8895, 322, 958, 14862, 17630, 13, 50640], "temperature": 0.0, "avg_logprob": -0.08892564657257825, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009108553058467805}, {"id": 13, "seek": 8256, "start": 88.8, "end": 94.64, "text": " So this means given a sequence of tokens, we're going to estimate which token is most likely to", "tokens": [50676, 407, 341, 1355, 2212, 257, 8310, 295, 22667, 11, 321, 434, 516, 281, 12539, 597, 14862, 307, 881, 3700, 281, 50968], "temperature": 0.0, "avg_logprob": -0.08892564657257825, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009108553058467805}, {"id": 14, "seek": 8256, "start": 94.64, "end": 101.04, "text": " come next. Maybe rephrasing that a little bit. We might also say that language models are trained", "tokens": [50968, 808, 958, 13, 2704, 319, 44598, 3349, 300, 257, 707, 857, 13, 492, 1062, 611, 584, 300, 2856, 5245, 366, 8895, 51288], "temperature": 0.0, "avg_logprob": -0.08892564657257825, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009108553058467805}, {"id": 15, "seek": 8256, "start": 101.04, "end": 109.04, "text": " to estimate which sequences of tokens or words are likely. The caveat being likely to appear in", "tokens": [51288, 281, 12539, 597, 22978, 295, 22667, 420, 2283, 366, 3700, 13, 440, 43012, 885, 3700, 281, 4204, 294, 51688], "temperature": 0.0, "avg_logprob": -0.08892564657257825, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009108553058467805}, {"id": 16, "seek": 10904, "start": 109.04, "end": 113.28, "text": " the training set, but today our training sets are quite extensive.", "tokens": [50364, 264, 3097, 992, 11, 457, 965, 527, 3097, 6352, 366, 1596, 13246, 13, 50576], "temperature": 0.0, "avg_logprob": -0.07562468757092113, "compression_ratio": 1.6616915422885572, "no_speech_prob": 5.1437062211334705e-05}, {"id": 17, "seek": 10904, "start": 116.88000000000001, "end": 122.16000000000001, "text": " So given this simple training objective, it's arguably surprising that we've seen language", "tokens": [50756, 407, 2212, 341, 2199, 3097, 10024, 11, 309, 311, 26771, 8830, 300, 321, 600, 1612, 2856, 51020], "temperature": 0.0, "avg_logprob": -0.07562468757092113, "compression_ratio": 1.6616915422885572, "no_speech_prob": 5.1437062211334705e-05}, {"id": 18, "seek": 10904, "start": 122.16000000000001, "end": 128.48000000000002, "text": " models do as well as they have in really impressive tasks. So they start to demonstrate", "tokens": [51020, 5245, 360, 382, 731, 382, 436, 362, 294, 534, 8992, 9608, 13, 407, 436, 722, 281, 11698, 51336], "temperature": 0.0, "avg_logprob": -0.07562468757092113, "compression_ratio": 1.6616915422885572, "no_speech_prob": 5.1437062211334705e-05}, {"id": 19, "seek": 10904, "start": 128.48000000000002, "end": 133.76, "text": " like really great language understanding, meaning like the semantics of language itself,", "tokens": [51336, 411, 534, 869, 2856, 3701, 11, 3620, 411, 264, 4361, 45298, 295, 2856, 2564, 11, 51600], "temperature": 0.0, "avg_logprob": -0.07562468757092113, "compression_ratio": 1.6616915422885572, "no_speech_prob": 5.1437062211334705e-05}, {"id": 20, "seek": 13376, "start": 134.39999999999998, "end": 140.79999999999998, "text": " not just the syntax of which sentences make sense. They've also started to demonstrate some", "tokens": [50396, 406, 445, 264, 28431, 295, 597, 16579, 652, 2020, 13, 814, 600, 611, 1409, 281, 11698, 512, 50716], "temperature": 0.0, "avg_logprob": -0.1010498186437095, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.0015485149342566729}, {"id": 21, "seek": 13376, "start": 140.79999999999998, "end": 145.44, "text": " world knowledge, so implicitly learning kind of how the world works just through our own human", "tokens": [50716, 1002, 3601, 11, 370, 26947, 356, 2539, 733, 295, 577, 264, 1002, 1985, 445, 807, 527, 1065, 1952, 50948], "temperature": 0.0, "avg_logprob": -0.1010498186437095, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.0015485149342566729}, {"id": 22, "seek": 13376, "start": 145.44, "end": 153.76, "text": " abstraction and how we articulate that. We start to see language models falter at tasks,", "tokens": [50948, 37765, 293, 577, 321, 30305, 300, 13, 492, 722, 281, 536, 2856, 5245, 3704, 391, 412, 9608, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1010498186437095, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.0015485149342566729}, {"id": 23, "seek": 13376, "start": 153.76, "end": 159.28, "text": " which look more like symbolic problem solving. So math in particular, we see this in programming,", "tokens": [51364, 597, 574, 544, 411, 25755, 1154, 12606, 13, 407, 5221, 294, 1729, 11, 321, 536, 341, 294, 9410, 11, 51640], "temperature": 0.0, "avg_logprob": -0.1010498186437095, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.0015485149342566729}, {"id": 24, "seek": 15928, "start": 159.28, "end": 164.4, "text": " although we're getting better at this. And in general, any kind of long-term planning tasks,", "tokens": [50364, 4878, 321, 434, 1242, 1101, 412, 341, 13, 400, 294, 2674, 11, 604, 733, 295, 938, 12, 7039, 5038, 9608, 11, 50620], "temperature": 0.0, "avg_logprob": -0.0892625790016324, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.0017003216780722141}, {"id": 25, "seek": 15928, "start": 164.4, "end": 169.76, "text": " which require abstract reasoning. So we see this also in problems that look like this,", "tokens": [50620, 597, 3651, 12649, 21577, 13, 407, 321, 536, 341, 611, 294, 2740, 300, 574, 411, 341, 11, 50888], "temperature": 0.0, "avg_logprob": -0.0892625790016324, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.0017003216780722141}, {"id": 26, "seek": 15928, "start": 169.76, "end": 177.2, "text": " which are word problems, but which are really about understanding abstractly how these abstract", "tokens": [50888, 597, 366, 1349, 2740, 11, 457, 597, 366, 534, 466, 3701, 12649, 356, 577, 613, 12649, 51260], "temperature": 0.0, "avg_logprob": -0.0892625790016324, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.0017003216780722141}, {"id": 27, "seek": 15928, "start": 177.2, "end": 181.84, "text": " objects relate to each other. And again, this shouldn't be surprising that language models", "tokens": [51260, 6565, 10961, 281, 1184, 661, 13, 400, 797, 11, 341, 4659, 380, 312, 8830, 300, 2856, 5245, 51492], "temperature": 0.0, "avg_logprob": -0.0892625790016324, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.0017003216780722141}, {"id": 28, "seek": 15928, "start": 181.84, "end": 187.2, "text": " struggle to do things like this, since it's so far off from their original training objective.", "tokens": [51492, 7799, 281, 360, 721, 411, 341, 11, 1670, 309, 311, 370, 1400, 766, 490, 641, 3380, 3097, 10024, 13, 51760], "temperature": 0.0, "avg_logprob": -0.0892625790016324, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.0017003216780722141}, {"id": 29, "seek": 18720, "start": 187.6, "end": 193.2, "text": " And in particular, even when we arrive at the right answer in some of these cases,", "tokens": [50384, 400, 294, 1729, 11, 754, 562, 321, 8881, 412, 264, 558, 1867, 294, 512, 295, 613, 3331, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0780652971828685, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005192563985474408}, {"id": 30, "seek": 18720, "start": 193.2, "end": 197.51999999999998, "text": " it's really hard to know after the fact kind of the reasoning for how we ended up", "tokens": [50664, 309, 311, 534, 1152, 281, 458, 934, 264, 1186, 733, 295, 264, 21577, 337, 577, 321, 4590, 493, 50880], "temperature": 0.0, "avg_logprob": -0.0780652971828685, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005192563985474408}, {"id": 31, "seek": 18720, "start": 197.51999999999998, "end": 203.51999999999998, "text": " at the right answer in this particular case. And maybe more fundamentally, if we don't know", "tokens": [50880, 412, 264, 558, 1867, 294, 341, 1729, 1389, 13, 400, 1310, 544, 17879, 11, 498, 321, 500, 380, 458, 51180], "temperature": 0.0, "avg_logprob": -0.0780652971828685, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005192563985474408}, {"id": 32, "seek": 18720, "start": 203.51999999999998, "end": 208.0, "text": " where the abstraction is happening or where the reasoning is happening, it's very hard to guide", "tokens": [51180, 689, 264, 37765, 307, 2737, 420, 689, 264, 21577, 307, 2737, 11, 309, 311, 588, 1152, 281, 5934, 51404], "temperature": 0.0, "avg_logprob": -0.0780652971828685, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005192563985474408}, {"id": 33, "seek": 18720, "start": 208.0, "end": 213.83999999999997, "text": " that process. And so this starts to motivate the need for something that looks more like", "tokens": [51404, 300, 1399, 13, 400, 370, 341, 3719, 281, 28497, 264, 643, 337, 746, 300, 1542, 544, 411, 51696], "temperature": 0.0, "avg_logprob": -0.0780652971828685, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005192563985474408}, {"id": 34, "seek": 21384, "start": 213.84, "end": 219.68, "text": " an explicit world model. And now, to start to borrow some words from Yasha Benjio,", "tokens": [50364, 364, 13691, 1002, 2316, 13, 400, 586, 11, 281, 722, 281, 11172, 512, 2283, 490, 398, 12137, 3964, 73, 1004, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10571038489248238, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0029796359594911337}, {"id": 35, "seek": 21384, "start": 219.68, "end": 223.68, "text": " this is one of the kind of biggest issues with today's language models,", "tokens": [50656, 341, 307, 472, 295, 264, 733, 295, 3880, 2663, 365, 965, 311, 2856, 5245, 11, 50856], "temperature": 0.0, "avg_logprob": -0.10571038489248238, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0029796359594911337}, {"id": 36, "seek": 21384, "start": 223.68, "end": 227.76, "text": " is arguably that we're asking the language model to be both the inference machine", "tokens": [50856, 307, 26771, 300, 321, 434, 3365, 264, 2856, 2316, 281, 312, 1293, 264, 38253, 3479, 51060], "temperature": 0.0, "avg_logprob": -0.10571038489248238, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0029796359594911337}, {"id": 37, "seek": 21384, "start": 227.76, "end": 234.8, "text": " and the world model implicitly, when this doesn't quite make sense. And so some things that we", "tokens": [51060, 293, 264, 1002, 2316, 26947, 356, 11, 562, 341, 1177, 380, 1596, 652, 2020, 13, 400, 370, 512, 721, 300, 321, 51412], "temperature": 0.0, "avg_logprob": -0.10571038489248238, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0029796359594911337}, {"id": 38, "seek": 21384, "start": 234.8, "end": 240.4, "text": " might hope for in a world model are to model causal relationships, to be really adapt at", "tokens": [51412, 1062, 1454, 337, 294, 257, 1002, 2316, 366, 281, 2316, 38755, 6159, 11, 281, 312, 534, 6231, 412, 51692], "temperature": 0.0, "avg_logprob": -0.10571038489248238, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0029796359594911337}, {"id": 39, "seek": 24040, "start": 240.4, "end": 248.32, "text": " modeling, using uncertainty, and to be modular. Yann LeCun puts this in a similar way, so we", "tokens": [50364, 15983, 11, 1228, 15697, 11, 293, 281, 312, 31111, 13, 398, 969, 1456, 34, 409, 8137, 341, 294, 257, 2531, 636, 11, 370, 321, 50760], "temperature": 0.0, "avg_logprob": -0.12850513645246917, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0013881432823836803}, {"id": 40, "seek": 24040, "start": 248.32, "end": 253.20000000000002, "text": " might kind of ask a world model to be able to distinguish between which details are important", "tokens": [50760, 1062, 733, 295, 1029, 257, 1002, 2316, 281, 312, 1075, 281, 20206, 1296, 597, 4365, 366, 1021, 51004], "temperature": 0.0, "avg_logprob": -0.12850513645246917, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0013881432823836803}, {"id": 41, "seek": 24040, "start": 253.20000000000002, "end": 258.0, "text": " versus irrelevant, and to be able to make predictions that can be performed in sort", "tokens": [51004, 5717, 28682, 11, 293, 281, 312, 1075, 281, 652, 21264, 300, 393, 312, 10332, 294, 1333, 51244], "temperature": 0.0, "avg_logprob": -0.12850513645246917, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0013881432823836803}, {"id": 42, "seek": 24040, "start": 258.0, "end": 264.48, "text": " of this abstract space of representations. And so hopefully in the next following slides,", "tokens": [51244, 295, 341, 12649, 1901, 295, 33358, 13, 400, 370, 4696, 294, 264, 958, 3480, 9788, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12850513645246917, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0013881432823836803}, {"id": 43, "seek": 24040, "start": 264.48, "end": 270.0, "text": " we're going to motivate what it might look like to use language models as inference machines", "tokens": [51568, 321, 434, 516, 281, 28497, 437, 309, 1062, 574, 411, 281, 764, 2856, 5245, 382, 38253, 8379, 51844], "temperature": 0.0, "avg_logprob": -0.12850513645246917, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0013881432823836803}, {"id": 44, "seek": 27000, "start": 270.0, "end": 277.36, "text": " in maybe like the simplest case of self-organizing world models. And even in those most simple", "tokens": [50364, 294, 1310, 411, 264, 22811, 1389, 295, 2698, 12, 12372, 3319, 1002, 5245, 13, 400, 754, 294, 729, 881, 2199, 50732], "temperature": 0.0, "avg_logprob": -0.08404525314889304, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.00010551036393735558}, {"id": 45, "seek": 27000, "start": 277.36, "end": 283.28, "text": " cases, how we start to get something much closer to the kind of explainable reasoning,", "tokens": [50732, 3331, 11, 577, 321, 722, 281, 483, 746, 709, 4966, 281, 264, 733, 295, 2903, 712, 21577, 11, 51028], "temperature": 0.0, "avg_logprob": -0.08404525314889304, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.00010551036393735558}, {"id": 46, "seek": 27000, "start": 283.28, "end": 285.6, "text": " which I think lots of us are grasping for right now.", "tokens": [51028, 597, 286, 519, 3195, 295, 505, 366, 29444, 3381, 337, 558, 586, 13, 51144], "temperature": 0.0, "avg_logprob": -0.08404525314889304, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.00010551036393735558}, {"id": 47, "seek": 27000, "start": 290.24, "end": 296.32, "text": " All right, so in a very simple case, we might think of a world model as simply a collection", "tokens": [51376, 1057, 558, 11, 370, 294, 257, 588, 2199, 1389, 11, 321, 1062, 519, 295, 257, 1002, 2316, 382, 2935, 257, 5765, 51680], "temperature": 0.0, "avg_logprob": -0.08404525314889304, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.00010551036393735558}, {"id": 48, "seek": 29632, "start": 296.32, "end": 302.88, "text": " of hypotheses, where we model confidence over each of those hypotheses. And in particular,", "tokens": [50364, 295, 49969, 11, 689, 321, 2316, 6687, 670, 1184, 295, 729, 49969, 13, 400, 294, 1729, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08748749799506608, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001700294786132872}, {"id": 49, "seek": 29632, "start": 302.88, "end": 307.68, "text": " we care about predictive world models. So given some evidence or data that we've observed in the", "tokens": [50692, 321, 1127, 466, 35521, 1002, 5245, 13, 407, 2212, 512, 4467, 420, 1412, 300, 321, 600, 13095, 294, 264, 50932], "temperature": 0.0, "avg_logprob": -0.08748749799506608, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001700294786132872}, {"id": 50, "seek": 29632, "start": 307.68, "end": 316.08, "text": " world, we'd like to propose the world models, which best explain the evidence that we've observed.", "tokens": [50932, 1002, 11, 321, 1116, 411, 281, 17421, 264, 1002, 5245, 11, 597, 1151, 2903, 264, 4467, 300, 321, 600, 13095, 13, 51352], "temperature": 0.0, "avg_logprob": -0.08748749799506608, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001700294786132872}, {"id": 51, "seek": 29632, "start": 320.08, "end": 326.08, "text": " And if that language sounded leading, it was indeed. We are proposing in this case to use", "tokens": [51552, 400, 498, 300, 2856, 17714, 5775, 11, 309, 390, 6451, 13, 492, 366, 29939, 294, 341, 1389, 281, 764, 51852], "temperature": 0.0, "avg_logprob": -0.08748749799506608, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001700294786132872}, {"id": 52, "seek": 32608, "start": 326.08, "end": 331.52, "text": " Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given", "tokens": [50364, 7840, 279, 6, 4978, 11, 597, 321, 439, 458, 293, 959, 11, 597, 5112, 505, 2293, 577, 281, 5623, 527, 13585, 2212, 50636], "temperature": 0.0, "avg_logprob": -0.09542943143296516, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0003798912512138486}, {"id": 53, "seek": 32608, "start": 331.52, "end": 340.08, "text": " some new evidence. And so when our hypotheses are kind of these Bernoulli random variables,", "tokens": [50636, 512, 777, 4467, 13, 400, 370, 562, 527, 49969, 366, 733, 295, 613, 10781, 263, 16320, 4974, 9102, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09542943143296516, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0003798912512138486}, {"id": 54, "seek": 32608, "start": 340.08, "end": 347.44, "text": " this bottom term can simply be expanded into this equation on the slide. And so the tricky part", "tokens": [51064, 341, 2767, 1433, 393, 2935, 312, 14342, 666, 341, 5367, 322, 264, 4137, 13, 400, 370, 264, 12414, 644, 51432], "temperature": 0.0, "avg_logprob": -0.09542943143296516, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0003798912512138486}, {"id": 55, "seek": 32608, "start": 347.44, "end": 355.28, "text": " in this computation is the likelihood piece. So given this evidence, given that our hypothesis", "tokens": [51432, 294, 341, 24903, 307, 264, 22119, 2522, 13, 407, 2212, 341, 4467, 11, 2212, 300, 527, 17291, 51824], "temperature": 0.0, "avg_logprob": -0.09542943143296516, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0003798912512138486}, {"id": 56, "seek": 35528, "start": 355.28, "end": 360.71999999999997, "text": " is true, what's the likelihood that we would have observed the evidence? And the claim that we're", "tokens": [50364, 307, 2074, 11, 437, 311, 264, 22119, 300, 321, 576, 362, 13095, 264, 4467, 30, 400, 264, 3932, 300, 321, 434, 50636], "temperature": 0.0, "avg_logprob": -0.09025624702716696, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.00013980688527226448}, {"id": 57, "seek": 35528, "start": 360.71999999999997, "end": 367.2, "text": " making is that language models are actually, this is like a natural task to ask of a language model.", "tokens": [50636, 1455, 307, 300, 2856, 5245, 366, 767, 11, 341, 307, 411, 257, 3303, 5633, 281, 1029, 295, 257, 2856, 2316, 13, 50960], "temperature": 0.0, "avg_logprob": -0.09025624702716696, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.00013980688527226448}, {"id": 58, "seek": 35528, "start": 368.15999999999997, "end": 372.64, "text": " When our evidence and our hypothesis are both semantic objects, as we discussed, language", "tokens": [51008, 1133, 527, 4467, 293, 527, 17291, 366, 1293, 47982, 6565, 11, 382, 321, 7152, 11, 2856, 51232], "temperature": 0.0, "avg_logprob": -0.09025624702716696, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.00013980688527226448}, {"id": 59, "seek": 35528, "start": 372.64, "end": 380.55999999999995, "text": " models are kind of trained exactly to understand which sequences of text are likely. So asking it to", "tokens": [51232, 5245, 366, 733, 295, 8895, 2293, 281, 1223, 597, 22978, 295, 2487, 366, 3700, 13, 407, 3365, 309, 281, 51628], "temperature": 0.0, "avg_logprob": -0.09025624702716696, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.00013980688527226448}, {"id": 60, "seek": 38056, "start": 381.44, "end": 387.36, "text": " prompting it in a clever way such that we can extract this particular likelihood", "tokens": [50408, 12391, 278, 309, 294, 257, 13494, 636, 1270, 300, 321, 393, 8947, 341, 1729, 22119, 50704], "temperature": 0.0, "avg_logprob": -0.11489272886706937, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00012730514572467655}, {"id": 61, "seek": 38056, "start": 387.36, "end": 394.8, "text": " is actually really natural. So how do we do this? So we come up with this kind of clever", "tokens": [50704, 307, 767, 534, 3303, 13, 407, 577, 360, 321, 360, 341, 30, 407, 321, 808, 493, 365, 341, 733, 295, 13494, 51076], "temperature": 0.0, "avg_logprob": -0.11489272886706937, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00012730514572467655}, {"id": 62, "seek": 38056, "start": 394.8, "end": 402.24, "text": " prompting scheme that allows us to extract again exactly how likely is this should have occurred.", "tokens": [51076, 12391, 278, 12232, 300, 4045, 505, 281, 8947, 797, 2293, 577, 3700, 307, 341, 820, 362, 11068, 13, 51448], "temperature": 0.0, "avg_logprob": -0.11489272886706937, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00012730514572467655}, {"id": 63, "seek": 40224, "start": 402.88, "end": 409.44, "text": " So given that our hypothesis is H in this setting and our evidence is this curly E,", "tokens": [50396, 407, 2212, 300, 527, 17291, 307, 389, 294, 341, 3287, 293, 527, 4467, 307, 341, 32066, 462, 11, 50724], "temperature": 0.0, "avg_logprob": -0.09966169429730766, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.002472280291840434}, {"id": 64, "seek": 40224, "start": 410.64, "end": 414.88, "text": " we only need to change our evidence into the conditional form and then phrase the question", "tokens": [50784, 321, 787, 643, 281, 1319, 527, 4467, 666, 264, 27708, 1254, 293, 550, 9535, 264, 1168, 50996], "temperature": 0.0, "avg_logprob": -0.09966169429730766, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.002472280291840434}, {"id": 65, "seek": 40224, "start": 414.88, "end": 421.92, "text": " like this to the language model. So the input sequence of text goes like, given that potentially", "tokens": [50996, 411, 341, 281, 264, 2856, 2316, 13, 407, 264, 4846, 8310, 295, 2487, 1709, 411, 11, 2212, 300, 7263, 51348], "temperature": 0.0, "avg_logprob": -0.09966169429730766, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.002472280291840434}, {"id": 66, "seek": 40224, "start": 421.92, "end": 427.28000000000003, "text": " if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic,", "tokens": [51348, 498, 527, 17291, 307, 411, 25237, 575, 668, 26271, 15653, 538, 4566, 12, 3405, 5388, 11, 51616], "temperature": 0.0, "avg_logprob": -0.09966169429730766, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.002472280291840434}, {"id": 67, "seek": 42728, "start": 427.28, "end": 432.15999999999997, "text": " would we have observed the evidence that Walmart laid off 10% of the material staff?", "tokens": [50364, 576, 321, 362, 13095, 264, 4467, 300, 25237, 9897, 766, 1266, 4, 295, 264, 2527, 3525, 30, 50608], "temperature": 0.0, "avg_logprob": -0.09731793403625488, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.00013981085794512182}, {"id": 68, "seek": 42728, "start": 434.0, "end": 439.91999999999996, "text": " And so either using something like few shot prompting or guidance or some kind of control", "tokens": [50700, 400, 370, 2139, 1228, 746, 411, 1326, 3347, 12391, 278, 420, 10056, 420, 512, 733, 295, 1969, 50996], "temperature": 0.0, "avg_logprob": -0.09731793403625488, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.00013981085794512182}, {"id": 69, "seek": 42728, "start": 439.91999999999996, "end": 445.59999999999997, "text": " generation technique, we can ensure that our language model outputs either yes or no,", "tokens": [50996, 5125, 6532, 11, 321, 393, 5586, 300, 527, 2856, 2316, 23930, 2139, 2086, 420, 572, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09731793403625488, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.00013981085794512182}, {"id": 70, "seek": 42728, "start": 445.59999999999997, "end": 450.32, "text": " and then use the logits from its answer to estimate that probability.", "tokens": [51280, 293, 550, 764, 264, 3565, 1208, 490, 1080, 1867, 281, 12539, 300, 8482, 13, 51516], "temperature": 0.0, "avg_logprob": -0.09731793403625488, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.00013981085794512182}, {"id": 71, "seek": 45032, "start": 450.4, "end": 458.56, "text": " And again, the claim here is that this is actually a really natural task to ask of a language model", "tokens": [50368, 400, 797, 11, 264, 3932, 510, 307, 300, 341, 307, 767, 257, 534, 3303, 5633, 281, 1029, 295, 257, 2856, 2316, 50776], "temperature": 0.0, "avg_logprob": -0.16211968083535472, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.000570278090890497}, {"id": 72, "seek": 45032, "start": 458.56, "end": 464.71999999999997, "text": " that leverages its innate reasoning engine better than just kind of allowing it to ramble", "tokens": [50776, 300, 12451, 1660, 1080, 41766, 21577, 2848, 1101, 813, 445, 733, 295, 8293, 309, 281, 10211, 638, 51084], "temperature": 0.0, "avg_logprob": -0.16211968083535472, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.000570278090890497}, {"id": 73, "seek": 45032, "start": 466.32, "end": 476.15999999999997, "text": " using text sometimes. And we can even make this updating scheme a bit more sophisticated", "tokens": [51164, 1228, 2487, 2171, 13, 400, 321, 393, 754, 652, 341, 25113, 12232, 257, 857, 544, 16950, 51656], "temperature": 0.0, "avg_logprob": -0.16211968083535472, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.000570278090890497}, {"id": 74, "seek": 47616, "start": 476.16, "end": 482.40000000000003, "text": " using things like precision waiting. So this will bias our posterior over hypothesis", "tokens": [50364, 1228, 721, 411, 18356, 3806, 13, 407, 341, 486, 12577, 527, 33529, 670, 17291, 50676], "temperature": 0.0, "avg_logprob": -0.10476534366607666, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0010321300942450762}, {"id": 75, "seek": 47616, "start": 482.40000000000003, "end": 488.08000000000004, "text": " either towards our prior or our evidence that we've observed based on how confident we are", "tokens": [50676, 2139, 3030, 527, 4059, 420, 527, 4467, 300, 321, 600, 13095, 2361, 322, 577, 6679, 321, 366, 50960], "temperature": 0.0, "avg_logprob": -0.10476534366607666, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0010321300942450762}, {"id": 76, "seek": 47616, "start": 488.08000000000004, "end": 500.24, "text": " in our prior and in our evidence. All right. So the world models that we've seen so far", "tokens": [50960, 294, 527, 4059, 293, 294, 527, 4467, 13, 1057, 558, 13, 407, 264, 1002, 5245, 300, 321, 600, 1612, 370, 1400, 51568], "temperature": 0.0, "avg_logprob": -0.10476534366607666, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0010321300942450762}, {"id": 77, "seek": 47616, "start": 500.8, "end": 506.0, "text": " are fairly simple. In particular, we're modeling all our hypotheses as independent from each other.", "tokens": [51596, 366, 6457, 2199, 13, 682, 1729, 11, 321, 434, 15983, 439, 527, 49969, 382, 6695, 490, 1184, 661, 13, 51856], "temperature": 0.0, "avg_logprob": -0.10476534366607666, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0010321300942450762}, {"id": 78, "seek": 50616, "start": 506.88000000000005, "end": 512.48, "text": " And this seems like a large simplifying assumption. So how might we support more complex world", "tokens": [50400, 400, 341, 2544, 411, 257, 2416, 6883, 5489, 15302, 13, 407, 577, 1062, 321, 1406, 544, 3997, 1002, 50680], "temperature": 0.0, "avg_logprob": -0.12896066688629518, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.475290527101606e-05}, {"id": 79, "seek": 50616, "start": 512.48, "end": 521.12, "text": " models where we condition our hypotheses on each other? All right. So another thing that", "tokens": [50680, 5245, 689, 321, 4188, 527, 49969, 322, 1184, 661, 30, 1057, 558, 13, 407, 1071, 551, 300, 51112], "temperature": 0.0, "avg_logprob": -0.12896066688629518, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.475290527101606e-05}, {"id": 80, "seek": 50616, "start": 521.12, "end": 525.52, "text": " hopefully you'll be familiar to this crowd, Bayes and Nets have been around for forever and", "tokens": [51112, 4696, 291, 603, 312, 4963, 281, 341, 6919, 11, 7840, 279, 293, 426, 1385, 362, 668, 926, 337, 5680, 293, 51332], "temperature": 0.0, "avg_logprob": -0.12896066688629518, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.475290527101606e-05}, {"id": 81, "seek": 50616, "start": 525.52, "end": 533.0400000000001, "text": " are often used to model mechanistic failures and enlarge systems and can be understood simply as", "tokens": [51332, 366, 2049, 1143, 281, 2316, 4236, 3142, 20774, 293, 31976, 432, 3652, 293, 393, 312, 7320, 2935, 382, 51708], "temperature": 0.0, "avg_logprob": -0.12896066688629518, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.475290527101606e-05}, {"id": 82, "seek": 53304, "start": 533.12, "end": 536.88, "text": " distribution where each variable depends on some small number of ancestor variables.", "tokens": [50368, 7316, 689, 1184, 7006, 5946, 322, 512, 1359, 1230, 295, 40032, 9102, 13, 50556], "temperature": 0.0, "avg_logprob": -0.0766161042590474, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0003569245454855263}, {"id": 83, "seek": 53304, "start": 538.0799999999999, "end": 541.68, "text": " Perhaps more intuitively, we can also think of these as directed graphs", "tokens": [50616, 10517, 544, 46506, 11, 321, 393, 611, 519, 295, 613, 382, 12898, 24877, 50796], "temperature": 0.0, "avg_logprob": -0.0766161042590474, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0003569245454855263}, {"id": 84, "seek": 53304, "start": 543.1999999999999, "end": 547.28, "text": " where we have edges between variables which are directed in the sense that", "tokens": [50872, 689, 321, 362, 8819, 1296, 9102, 597, 366, 12898, 294, 264, 2020, 300, 51076], "temperature": 0.0, "avg_logprob": -0.0766161042590474, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0003569245454855263}, {"id": 85, "seek": 53304, "start": 547.28, "end": 553.68, "text": " the parent is conditioned on the child. So let's see an example.", "tokens": [51076, 264, 2596, 307, 35833, 322, 264, 1440, 13, 407, 718, 311, 536, 364, 1365, 13, 51396], "temperature": 0.0, "avg_logprob": -0.0766161042590474, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0003569245454855263}, {"id": 86, "seek": 53304, "start": 555.5999999999999, "end": 562.7199999999999, "text": " All right. So now we have two different kinds of hypotheses. The ones which are kind of parents", "tokens": [51492, 1057, 558, 13, 407, 586, 321, 362, 732, 819, 3685, 295, 49969, 13, 440, 2306, 597, 366, 733, 295, 3152, 51848], "temperature": 0.0, "avg_logprob": -0.0766161042590474, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0003569245454855263}, {"id": 87, "seek": 56272, "start": 562.72, "end": 569.36, "text": " in our simple setup are kind of qualitatively more abstract than the children. So our two", "tokens": [50364, 294, 527, 2199, 8657, 366, 733, 295, 31312, 356, 544, 12649, 813, 264, 2227, 13, 407, 527, 732, 50696], "temperature": 0.0, "avg_logprob": -0.13187168836593627, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.00019710547348950058}, {"id": 88, "seek": 56272, "start": 569.36, "end": 575.6800000000001, "text": " children nodes are similar to the hypothesis we saw before. Mark a sentiment for Best Buy is Poor", "tokens": [50696, 2227, 13891, 366, 2531, 281, 264, 17291, 321, 1866, 949, 13, 3934, 257, 16149, 337, 9752, 19146, 307, 23591, 51012], "temperature": 0.0, "avg_logprob": -0.13187168836593627, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.00019710547348950058}, {"id": 89, "seek": 56272, "start": 575.6800000000001, "end": 581.36, "text": " or Walmart will grow its physical footprint this year. And then the more abstract hypothesis", "tokens": [51012, 420, 25237, 486, 1852, 1080, 4001, 24222, 341, 1064, 13, 400, 550, 264, 544, 12649, 17291, 51296], "temperature": 0.0, "avg_logprob": -0.13187168836593627, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.00019710547348950058}, {"id": 90, "seek": 56272, "start": 581.36, "end": 589.2, "text": " being retailers were negatively impacted by COVID-19. And so then we specify this conditional", "tokens": [51296, 885, 33519, 645, 29519, 15653, 538, 4566, 12, 3405, 13, 400, 370, 550, 321, 16500, 341, 27708, 51688], "temperature": 0.0, "avg_logprob": -0.13187168836593627, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.00019710547348950058}, {"id": 91, "seek": 58920, "start": 589.2, "end": 594.4000000000001, "text": " structure either kind of in a classical sense by specifying all of the joint probabilities", "tokens": [50364, 3877, 2139, 733, 295, 294, 257, 13735, 2020, 538, 1608, 5489, 439, 295, 264, 7225, 33783, 50624], "temperature": 0.0, "avg_logprob": -0.06852421106076707, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0004878150357399136}, {"id": 92, "seek": 58920, "start": 594.4000000000001, "end": 600.5600000000001, "text": " upfront or alternatively learning it given some healthy amount of training data. And again,", "tokens": [50624, 30264, 420, 8535, 356, 2539, 309, 2212, 512, 4627, 2372, 295, 3097, 1412, 13, 400, 797, 11, 50932], "temperature": 0.0, "avg_logprob": -0.06852421106076707, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0004878150357399136}, {"id": 93, "seek": 58920, "start": 600.5600000000001, "end": 605.84, "text": " we're proposing that instead of doing that intensive process, we can use language models", "tokens": [50932, 321, 434, 29939, 300, 2602, 295, 884, 300, 18957, 1399, 11, 321, 393, 764, 2856, 5245, 51196], "temperature": 0.0, "avg_logprob": -0.06852421106076707, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0004878150357399136}, {"id": 94, "seek": 58920, "start": 605.84, "end": 612.96, "text": " to extract those probabilities in a natural way. And so one natural complaint at this point might", "tokens": [51196, 281, 8947, 729, 33783, 294, 257, 3303, 636, 13, 400, 370, 472, 3303, 20100, 412, 341, 935, 1062, 51552], "temperature": 0.0, "avg_logprob": -0.06852421106076707, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0004878150357399136}, {"id": 95, "seek": 58920, "start": 612.96, "end": 618.5600000000001, "text": " be like, well, this space is going to start to get very large. If we're trying to update these", "tokens": [51552, 312, 411, 11, 731, 11, 341, 1901, 307, 516, 281, 722, 281, 483, 588, 2416, 13, 759, 321, 434, 1382, 281, 5623, 613, 51832], "temperature": 0.0, "avg_logprob": -0.06852421106076707, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0004878150357399136}, {"id": 96, "seek": 61856, "start": 618.56, "end": 624.7199999999999, "text": " beliefs in real time given some large data stream is going to become intractable given our current", "tokens": [50364, 13585, 294, 957, 565, 2212, 512, 2416, 1412, 4309, 307, 516, 281, 1813, 560, 1897, 712, 2212, 527, 2190, 50672], "temperature": 0.0, "avg_logprob": -0.07724940038360326, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0002165229234378785}, {"id": 97, "seek": 61856, "start": 624.7199999999999, "end": 631.8399999999999, "text": " framework. So how might we start to augment that? Luckily, there's been a lot of work historically", "tokens": [50672, 8388, 13, 407, 577, 1062, 321, 722, 281, 29919, 300, 30, 19726, 11, 456, 311, 668, 257, 688, 295, 589, 16180, 51028], "temperature": 0.0, "avg_logprob": -0.07724940038360326, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0002165229234378785}, {"id": 98, "seek": 61856, "start": 631.8399999999999, "end": 637.28, "text": " done on this problem. And we can start to use things like message passing to update our beliefs", "tokens": [51028, 1096, 322, 341, 1154, 13, 400, 321, 393, 722, 281, 764, 721, 411, 3636, 8437, 281, 5623, 527, 13585, 51300], "temperature": 0.0, "avg_logprob": -0.07724940038360326, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0002165229234378785}, {"id": 99, "seek": 61856, "start": 637.28, "end": 643.1999999999999, "text": " in an approximate way. So as long as our Bayes net has a tree dependency structure, we can use", "tokens": [51300, 294, 364, 30874, 636, 13, 407, 382, 938, 382, 527, 7840, 279, 2533, 575, 257, 4230, 33621, 3877, 11, 321, 393, 764, 51596], "temperature": 0.0, "avg_logprob": -0.07724940038360326, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0002165229234378785}, {"id": 100, "seek": 61856, "start": 643.1999999999999, "end": 648.0, "text": " things like the sum product algorithm to update our beliefs. And I don't want you to worry too", "tokens": [51596, 721, 411, 264, 2408, 1674, 9284, 281, 5623, 527, 13585, 13, 400, 286, 500, 380, 528, 291, 281, 3292, 886, 51836], "temperature": 0.0, "avg_logprob": -0.07724940038360326, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0002165229234378785}, {"id": 101, "seek": 64800, "start": 648.0, "end": 652.96, "text": " much about the equations on this slide. If you've seen this before, this kind of recursive structure", "tokens": [50364, 709, 466, 264, 11787, 322, 341, 4137, 13, 759, 291, 600, 1612, 341, 949, 11, 341, 733, 295, 20560, 488, 3877, 50612], "temperature": 0.0, "avg_logprob": -0.08536921077304416, "compression_ratio": 1.572549019607843, "no_speech_prob": 0.0004728178901132196}, {"id": 102, "seek": 64800, "start": 652.96, "end": 658.64, "text": " will look familiar. But if not, the general kind of intuitive idea that we're going to compute these", "tokens": [50612, 486, 574, 4963, 13, 583, 498, 406, 11, 264, 2674, 733, 295, 21769, 1558, 300, 321, 434, 516, 281, 14722, 613, 50896], "temperature": 0.0, "avg_logprob": -0.08536921077304416, "compression_ratio": 1.572549019607843, "no_speech_prob": 0.0004728178901132196}, {"id": 103, "seek": 64800, "start": 658.64, "end": 664.16, "text": " messages from all of the children or the neighboring nodes and use those to propagate through the graph", "tokens": [50896, 7897, 490, 439, 295, 264, 2227, 420, 264, 31521, 13891, 293, 764, 729, 281, 48256, 807, 264, 4295, 51172], "temperature": 0.0, "avg_logprob": -0.08536921077304416, "compression_ratio": 1.572549019607843, "no_speech_prob": 0.0004728178901132196}, {"id": 104, "seek": 64800, "start": 664.16, "end": 672.4, "text": " to update each independent belief. Let's just see an example. All right, so suppose we observed", "tokens": [51172, 281, 5623, 1184, 6695, 7107, 13, 961, 311, 445, 536, 364, 1365, 13, 1057, 558, 11, 370, 7297, 321, 13095, 51584], "temperature": 0.0, "avg_logprob": -0.08536921077304416, "compression_ratio": 1.572549019607843, "no_speech_prob": 0.0004728178901132196}, {"id": 105, "seek": 67240, "start": 672.4, "end": 678.0799999999999, "text": " some new evidence and we'd like to know how should we update probability over hypothesis A,", "tokens": [50364, 512, 777, 4467, 293, 321, 1116, 411, 281, 458, 577, 820, 321, 5623, 8482, 670, 17291, 316, 11, 50648], "temperature": 0.0, "avg_logprob": -0.15691522189549037, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.0035929305013269186}, {"id": 106, "seek": 67240, "start": 678.0799999999999, "end": 684.9599999999999, "text": " which as you remember is the parent node in our graph. All right, so B here on the left hand side", "tokens": [50648, 597, 382, 291, 1604, 307, 264, 2596, 9984, 294, 527, 4295, 13, 1057, 558, 11, 370, 363, 510, 322, 264, 1411, 1011, 1252, 50992], "temperature": 0.0, "avg_logprob": -0.15691522189549037, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.0035929305013269186}, {"id": 107, "seek": 67240, "start": 684.9599999999999, "end": 690.56, "text": " is the belief. XA is essentially like just the variable representing the hypothesis A.", "tokens": [50992, 307, 264, 7107, 13, 1783, 32, 307, 4476, 411, 445, 264, 7006, 13460, 264, 17291, 316, 13, 51272], "temperature": 0.0, "avg_logprob": -0.15691522189549037, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.0035929305013269186}, {"id": 108, "seek": 67240, "start": 691.68, "end": 698.3199999999999, "text": " And then the fee and the side terms are specified by our language model or the", "tokens": [51328, 400, 550, 264, 12054, 293, 264, 1252, 2115, 366, 22206, 538, 527, 2856, 2316, 420, 264, 51660], "temperature": 0.0, "avg_logprob": -0.15691522189549037, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.0035929305013269186}, {"id": 109, "seek": 69832, "start": 698.96, "end": 704.4000000000001, "text": " the Bayes net in the classical sense. And so we're summing over all of the variables,", "tokens": [50396, 264, 7840, 279, 2533, 294, 264, 13735, 2020, 13, 400, 370, 321, 434, 2408, 2810, 670, 439, 295, 264, 9102, 11, 50668], "temperature": 0.0, "avg_logprob": -0.09921194124622505, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0003149957046844065}, {"id": 110, "seek": 69832, "start": 704.4000000000001, "end": 708.6400000000001, "text": " all of the values that the variables XB and XC can take, which in our case, of course, is just zero", "tokens": [50668, 439, 295, 264, 4190, 300, 264, 9102, 1783, 33, 293, 1783, 34, 393, 747, 11, 597, 294, 527, 1389, 11, 295, 1164, 11, 307, 445, 4018, 50880], "temperature": 0.0, "avg_logprob": -0.09921194124622505, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0003149957046844065}, {"id": 111, "seek": 69832, "start": 708.6400000000001, "end": 715.5200000000001, "text": " and one. So if you, you might recognize at this point that this is indeed the exact marginal", "tokens": [50880, 293, 472, 13, 407, 498, 291, 11, 291, 1062, 5521, 412, 341, 935, 300, 341, 307, 6451, 264, 1900, 16885, 51224], "temperature": 0.0, "avg_logprob": -0.09921194124622505, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0003149957046844065}, {"id": 112, "seek": 69832, "start": 715.5200000000001, "end": 721.6, "text": " probability for the hypothesis A, and that's because our graph is simply connected in this example.", "tokens": [51224, 8482, 337, 264, 17291, 316, 11, 293, 300, 311, 570, 527, 4295, 307, 2935, 4582, 294, 341, 1365, 13, 51528], "temperature": 0.0, "avg_logprob": -0.09921194124622505, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0003149957046844065}, {"id": 113, "seek": 69832, "start": 722.1600000000001, "end": 726.48, "text": " So in general, this isn't true, but it turns out to be the case in our example.", "tokens": [51556, 407, 294, 2674, 11, 341, 1943, 380, 2074, 11, 457, 309, 4523, 484, 281, 312, 264, 1389, 294, 527, 1365, 13, 51772], "temperature": 0.0, "avg_logprob": -0.09921194124622505, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0003149957046844065}, {"id": 114, "seek": 72648, "start": 726.72, "end": 733.28, "text": " And again, we can compute these fee and side terms using the language model itself.", "tokens": [50376, 400, 797, 11, 321, 393, 14722, 613, 12054, 293, 1252, 2115, 1228, 264, 2856, 2316, 2564, 13, 50704], "temperature": 0.0, "avg_logprob": -0.15137629722481344, "compression_ratio": 1.478494623655914, "no_speech_prob": 8.74979596119374e-05}, {"id": 115, "seek": 72648, "start": 738.08, "end": 742.72, "text": " All right, so because we're all lovers of free energy here, I'm going to walk through kind of", "tokens": [50944, 1057, 558, 11, 370, 570, 321, 434, 439, 22697, 295, 1737, 2281, 510, 11, 286, 478, 516, 281, 1792, 807, 733, 295, 51176], "temperature": 0.0, "avg_logprob": -0.15137629722481344, "compression_ratio": 1.478494623655914, "no_speech_prob": 8.74979596119374e-05}, {"id": 116, "seek": 72648, "start": 742.72, "end": 750.64, "text": " how this is maybe the first example of a self evidencing or minimizing free energy kind of model.", "tokens": [51176, 577, 341, 307, 1310, 264, 700, 1365, 295, 257, 2698, 43699, 2175, 420, 46608, 1737, 2281, 733, 295, 2316, 13, 51572], "temperature": 0.0, "avg_logprob": -0.15137629722481344, "compression_ratio": 1.478494623655914, "no_speech_prob": 8.74979596119374e-05}, {"id": 117, "seek": 75064, "start": 751.1999999999999, "end": 756.3199999999999, "text": " So as we noted, belief propagation isn't exact for more complicated graphs. And so", "tokens": [50392, 407, 382, 321, 12964, 11, 7107, 38377, 1943, 380, 1900, 337, 544, 6179, 24877, 13, 400, 370, 50648], "temperature": 0.0, "avg_logprob": -0.13082504272460938, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0003920254239346832}, {"id": 118, "seek": 75064, "start": 757.04, "end": 762.64, "text": " it makes sense and might be useful to ask the question, you know, how far apart or when are", "tokens": [50684, 309, 1669, 2020, 293, 1062, 312, 4420, 281, 1029, 264, 1168, 11, 291, 458, 11, 577, 1400, 4936, 420, 562, 366, 50964], "temperature": 0.0, "avg_logprob": -0.13082504272460938, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0003920254239346832}, {"id": 119, "seek": 75064, "start": 762.64, "end": 769.92, "text": " our beliefs close to the exact marginals. And so we often use things like KL divergence to", "tokens": [50964, 527, 13585, 1998, 281, 264, 1900, 10270, 1124, 13, 400, 370, 321, 2049, 764, 721, 411, 47991, 47387, 281, 51328], "temperature": 0.0, "avg_logprob": -0.13082504272460938, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0003920254239346832}, {"id": 120, "seek": 75064, "start": 769.92, "end": 775.68, "text": " compare the difference between two probability distributions. And that is explicated on the", "tokens": [51328, 6794, 264, 2649, 1296, 732, 8482, 37870, 13, 400, 300, 307, 1490, 3587, 322, 264, 51616], "temperature": 0.0, "avg_logprob": -0.13082504272460938, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0003920254239346832}, {"id": 121, "seek": 77568, "start": 775.68, "end": 784.7199999999999, "text": " right hand side. And then those of us who are like have a background in physics might recognize", "tokens": [50364, 558, 1011, 1252, 13, 400, 550, 729, 295, 505, 567, 366, 411, 362, 257, 3678, 294, 10649, 1062, 5521, 50816], "temperature": 0.0, "avg_logprob": -0.09022522976523951, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0021155206486582756}, {"id": 122, "seek": 77568, "start": 784.7199999999999, "end": 789.92, "text": " Boltzmann's law as well. So this is just the idea that we might represent the probability of a given", "tokens": [50816, 37884, 89, 14912, 311, 2101, 382, 731, 13, 407, 341, 307, 445, 264, 1558, 300, 321, 1062, 2906, 264, 8482, 295, 257, 2212, 51076], "temperature": 0.0, "avg_logprob": -0.09022522976523951, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0021155206486582756}, {"id": 123, "seek": 77568, "start": 789.92, "end": 795.04, "text": " state using an energy function. And we're not going to accept this as truth, maybe some of us", "tokens": [51076, 1785, 1228, 364, 2281, 2445, 13, 400, 321, 434, 406, 516, 281, 3241, 341, 382, 3494, 11, 1310, 512, 295, 505, 51332], "temperature": 0.0, "avg_logprob": -0.09022522976523951, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0021155206486582756}, {"id": 124, "seek": 77568, "start": 795.04, "end": 799.3599999999999, "text": " have done in the past, but we are going to just use it as a definition for this energy function,", "tokens": [51332, 362, 1096, 294, 264, 1791, 11, 457, 321, 366, 516, 281, 445, 764, 309, 382, 257, 7123, 337, 341, 2281, 2445, 11, 51548], "temperature": 0.0, "avg_logprob": -0.09022522976523951, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0021155206486582756}, {"id": 125, "seek": 79936, "start": 800.08, "end": 807.2, "text": " such that when we plug in that term here, and expand out, we start to see these first two terms", "tokens": [50400, 1270, 300, 562, 321, 5452, 294, 300, 1433, 510, 11, 293, 5268, 484, 11, 321, 722, 281, 536, 613, 700, 732, 2115, 50756], "temperature": 0.0, "avg_logprob": -0.06419683010020155, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004609188996255398}, {"id": 126, "seek": 79936, "start": 807.2, "end": 811.84, "text": " look a lot like the kind of energy and entropy functions which we are used to. And indeed,", "tokens": [50756, 574, 257, 688, 411, 264, 733, 295, 2281, 293, 30867, 6828, 597, 321, 366, 1143, 281, 13, 400, 6451, 11, 50988], "temperature": 0.0, "avg_logprob": -0.06419683010020155, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004609188996255398}, {"id": 127, "seek": 79936, "start": 811.84, "end": 819.92, "text": " we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy", "tokens": [50988, 321, 393, 445, 33872, 729, 732, 2115, 382, 264, 30199, 1737, 2281, 2445, 13, 865, 11, 597, 1669, 385, 2055, 51392], "temperature": 0.0, "avg_logprob": -0.06419683010020155, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004609188996255398}, {"id": 128, "seek": 79936, "start": 819.92, "end": 826.08, "text": " to see this all coming together. And in particular, it might make sense just to note at this point that", "tokens": [51392, 281, 536, 341, 439, 1348, 1214, 13, 400, 294, 1729, 11, 309, 1062, 652, 2020, 445, 281, 3637, 412, 341, 935, 300, 51700], "temperature": 0.0, "avg_logprob": -0.06419683010020155, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004609188996255398}, {"id": 129, "seek": 82608, "start": 826.8000000000001, "end": 832.32, "text": " using world models which are self organizing in the sense seems to be very compelling,", "tokens": [50400, 1228, 1002, 5245, 597, 366, 2698, 17608, 294, 264, 2020, 2544, 281, 312, 588, 20050, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09089615765739889, "compression_ratio": 1.64, "no_speech_prob": 0.00018520664889365435}, {"id": 130, "seek": 82608, "start": 832.32, "end": 838.24, "text": " since the kind of world model which we want is one which promotes the evidence that we've seen so far.", "tokens": [50676, 1670, 264, 733, 295, 1002, 2316, 597, 321, 528, 307, 472, 597, 36015, 264, 4467, 300, 321, 600, 1612, 370, 1400, 13, 50972], "temperature": 0.0, "avg_logprob": -0.09089615765739889, "compression_ratio": 1.64, "no_speech_prob": 0.00018520664889365435}, {"id": 131, "seek": 82608, "start": 843.9200000000001, "end": 848.5600000000001, "text": " Why is it useful to formulate this in terms of free energy, besides the fact that we all", "tokens": [51256, 1545, 307, 309, 4420, 281, 47881, 341, 294, 2115, 295, 1737, 2281, 11, 11868, 264, 1186, 300, 321, 439, 51488], "temperature": 0.0, "avg_logprob": -0.09089615765739889, "compression_ratio": 1.64, "no_speech_prob": 0.00018520664889365435}, {"id": 132, "seek": 82608, "start": 849.36, "end": 853.76, "text": " find it compelling here? Well, you can make a lot of progress by constructing analytically", "tokens": [51528, 915, 309, 20050, 510, 30, 1042, 11, 291, 393, 652, 257, 688, 295, 4205, 538, 39969, 10783, 984, 51748], "temperature": 0.0, "avg_logprob": -0.09089615765739889, "compression_ratio": 1.64, "no_speech_prob": 0.00018520664889365435}, {"id": 133, "seek": 85376, "start": 853.76, "end": 859.68, "text": " tractable approximations of Gibbs free energy often. I'm not going to go into the details here,", "tokens": [50364, 24207, 712, 8542, 763, 295, 30199, 1737, 2281, 2049, 13, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 510, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1289378046989441, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0008039973909035325}, {"id": 134, "seek": 85376, "start": 860.8, "end": 863.76, "text": " but here are two examples where that's been fruitful.", "tokens": [50716, 457, 510, 366, 732, 5110, 689, 300, 311, 668, 49795, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1289378046989441, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0008039973909035325}, {"id": 135, "seek": 85376, "start": 868.48, "end": 873.4399999999999, "text": " All right. So now I'm going to chat briefly about kind of how we might use recommendation", "tokens": [51100, 1057, 558, 13, 407, 586, 286, 478, 516, 281, 5081, 10515, 466, 733, 295, 577, 321, 1062, 764, 11879, 51348], "temperature": 0.0, "avg_logprob": -0.1289378046989441, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0008039973909035325}, {"id": 136, "seek": 85376, "start": 873.4399999999999, "end": 878.4, "text": " systems like, or how we might use these systems as recommendation systems, and indeed in the world", "tokens": [51348, 3652, 411, 11, 420, 577, 321, 1062, 764, 613, 3652, 382, 11879, 3652, 11, 293, 6451, 294, 264, 1002, 51596], "temperature": 0.0, "avg_logprob": -0.1289378046989441, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0008039973909035325}, {"id": 137, "seek": 87840, "start": 879.28, "end": 887.4399999999999, "text": " today. All right. So one prime example for a system like this might be useful is a situation", "tokens": [50408, 965, 13, 1057, 558, 13, 407, 472, 5835, 1365, 337, 257, 1185, 411, 341, 1062, 312, 4420, 307, 257, 2590, 50816], "temperature": 0.0, "avg_logprob": -0.11820917953679591, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0008040486718527973}, {"id": 138, "seek": 87840, "start": 887.4399999999999, "end": 893.76, "text": " where we have lots of data incoming at very high frequencies, and we always want to have", "tokens": [50816, 689, 321, 362, 3195, 295, 1412, 22341, 412, 588, 1090, 20250, 11, 293, 321, 1009, 528, 281, 362, 51132], "temperature": 0.0, "avg_logprob": -0.11820917953679591, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0008040486718527973}, {"id": 139, "seek": 87840, "start": 893.76, "end": 899.52, "text": " some set of naturally discrete hypotheses that we're modeling beliefs over which are", "tokens": [51132, 512, 992, 295, 8195, 27706, 49969, 300, 321, 434, 15983, 13585, 670, 597, 366, 51420], "temperature": 0.0, "avg_logprob": -0.11820917953679591, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0008040486718527973}, {"id": 140, "seek": 87840, "start": 899.52, "end": 904.96, "text": " being kept up to date at a very regular cadence. And so actually, a lot of the", "tokens": [51420, 885, 4305, 493, 281, 4002, 412, 257, 588, 3890, 46109, 13, 400, 370, 767, 11, 257, 688, 295, 264, 51692], "temperature": 0.0, "avg_logprob": -0.11820917953679591, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0008040486718527973}, {"id": 141, "seek": 90496, "start": 905.6, "end": 912.5600000000001, "text": " the muscle here is just reformatting documents or however our data comes in as evidences. This is", "tokens": [50396, 264, 8679, 510, 307, 445, 8290, 267, 783, 8512, 420, 4461, 527, 1412, 1487, 294, 382, 1073, 41298, 13, 639, 307, 50744], "temperature": 0.0, "avg_logprob": -0.12400379586727062, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0004044602101203054}, {"id": 142, "seek": 90496, "start": 912.5600000000001, "end": 918.4000000000001, "text": " not always obvious or easy to do. But once you've kind of figured out that part, and in particular,", "tokens": [50744, 406, 1009, 6322, 420, 1858, 281, 360, 13, 583, 1564, 291, 600, 733, 295, 8932, 484, 300, 644, 11, 293, 294, 1729, 11, 51036], "temperature": 0.0, "avg_logprob": -0.12400379586727062, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0004044602101203054}, {"id": 143, "seek": 90496, "start": 919.2, "end": 926.0, "text": " we've been using things like RAG, retrieval augmented generation, or embedding based systems to", "tokens": [51076, 321, 600, 668, 1228, 721, 411, 14626, 38, 11, 19817, 3337, 36155, 5125, 11, 420, 12240, 3584, 2361, 3652, 281, 51416], "temperature": 0.0, "avg_logprob": -0.12400379586727062, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0004044602101203054}, {"id": 144, "seek": 90496, "start": 926.0, "end": 932.5600000000001, "text": " kind of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind", "tokens": [51416, 733, 295, 2573, 484, 562, 1412, 300, 311, 22341, 307, 7340, 281, 257, 2212, 17291, 13, 3443, 291, 600, 733, 51744], "temperature": 0.0, "avg_logprob": -0.12400379586727062, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0004044602101203054}, {"id": 145, "seek": 93256, "start": 932.64, "end": 939.52, "text": " of built up that machinery, the actual updating computations, as we've shown already, is actually", "tokens": [50368, 295, 3094, 493, 300, 27302, 11, 264, 3539, 25113, 2807, 763, 11, 382, 321, 600, 4898, 1217, 11, 307, 767, 50712], "temperature": 0.0, "avg_logprob": -0.0846384189746998, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0010986123234033585}, {"id": 146, "seek": 93256, "start": 939.52, "end": 944.9599999999999, "text": " pretty simple. So we do these likelihood computations and we update our beliefs. And then at any given", "tokens": [50712, 1238, 2199, 13, 407, 321, 360, 613, 22119, 2807, 763, 293, 321, 5623, 527, 13585, 13, 400, 550, 412, 604, 2212, 50984], "temperature": 0.0, "avg_logprob": -0.0846384189746998, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0010986123234033585}, {"id": 147, "seek": 93256, "start": 944.9599999999999, "end": 951.5999999999999, "text": " time, we can query that model for our marginal distribution over any given hypothesis.", "tokens": [50984, 565, 11, 321, 393, 14581, 300, 2316, 337, 527, 16885, 7316, 670, 604, 2212, 17291, 13, 51316], "temperature": 0.0, "avg_logprob": -0.0846384189746998, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0010986123234033585}, {"id": 148, "seek": 93256, "start": 953.1199999999999, "end": 957.76, "text": " And it turns out that this kind of setup has many practical applications.", "tokens": [51392, 400, 309, 4523, 484, 300, 341, 733, 295, 8657, 575, 867, 8496, 5821, 13, 51624], "temperature": 0.0, "avg_logprob": -0.0846384189746998, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0010986123234033585}, {"id": 149, "seek": 95776, "start": 957.76, "end": 966.16, "text": " It's also noteworthy that even with very simple systems like these, these are like", "tokens": [50364, 467, 311, 611, 406, 1023, 2652, 88, 300, 754, 365, 588, 2199, 3652, 411, 613, 11, 613, 366, 411, 50784], "temperature": 0.0, "avg_logprob": -0.10512661933898926, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0002868278243113309}, {"id": 150, "seek": 95776, "start": 966.16, "end": 972.48, "text": " out of the box, controllable and explainable. So just by storing the magnitude and the direction", "tokens": [50784, 484, 295, 264, 2424, 11, 45159, 712, 293, 2903, 712, 13, 407, 445, 538, 26085, 264, 15668, 293, 264, 3513, 51100], "temperature": 0.0, "avg_logprob": -0.10512661933898926, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0002868278243113309}, {"id": 151, "seek": 95776, "start": 972.48, "end": 978.16, "text": " of the update to the posterior for each piece of evidence that we observe, we have a very natural", "tokens": [51100, 295, 264, 5623, 281, 264, 33529, 337, 1184, 2522, 295, 4467, 300, 321, 11441, 11, 321, 362, 257, 588, 3303, 51384], "temperature": 0.0, "avg_logprob": -0.10512661933898926, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0002868278243113309}, {"id": 152, "seek": 95776, "start": 978.16, "end": 986.4, "text": " built in explanation for our belief at any given time. And that makes kind of like these applications", "tokens": [51384, 3094, 294, 10835, 337, 527, 7107, 412, 604, 2212, 565, 13, 400, 300, 1669, 733, 295, 411, 613, 5821, 51796], "temperature": 0.0, "avg_logprob": -0.10512661933898926, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0002868278243113309}, {"id": 153, "seek": 98640, "start": 986.4, "end": 991.52, "text": " where folks might really like to use a language model, but really require like a robust,", "tokens": [50364, 689, 4024, 1062, 534, 411, 281, 764, 257, 2856, 2316, 11, 457, 534, 3651, 411, 257, 13956, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09008740725582592, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.0006460464792326093}, {"id": 154, "seek": 98640, "start": 992.3199999999999, "end": 999.1999999999999, "text": " like causal relationship between the outputs and the explanation, which you don't get from", "tokens": [50660, 411, 38755, 2480, 1296, 264, 23930, 293, 264, 10835, 11, 597, 291, 500, 380, 483, 490, 51004], "temperature": 0.0, "avg_logprob": -0.09008740725582592, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.0006460464792326093}, {"id": 155, "seek": 98640, "start": 999.1999999999999, "end": 1003.76, "text": " a language model on its own. A system like this can be very appealing in those situations.", "tokens": [51004, 257, 2856, 2316, 322, 1080, 1065, 13, 316, 1185, 411, 341, 393, 312, 588, 23842, 294, 729, 6851, 13, 51232], "temperature": 0.0, "avg_logprob": -0.09008740725582592, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.0006460464792326093}, {"id": 156, "seek": 98640, "start": 1008.8, "end": 1010.72, "text": " All right, so now on to further work.", "tokens": [51484, 1057, 558, 11, 370, 586, 322, 281, 3052, 589, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09008740725582592, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.0006460464792326093}, {"id": 157, "seek": 101072, "start": 1011.44, "end": 1021.6, "text": " So everything that we've discussed today is early work towards integrating language models", "tokens": [50400, 407, 1203, 300, 321, 600, 7152, 965, 307, 2440, 589, 3030, 26889, 2856, 5245, 50908], "temperature": 0.0, "avg_logprob": -0.16080565330309746, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0006460368167608976}, {"id": 158, "seek": 101072, "start": 1021.6, "end": 1028.96, "text": " into more probabilistic frameworks. And there's been a lot of exciting work done in this vein", "tokens": [50908, 666, 544, 31959, 3142, 29834, 13, 400, 456, 311, 668, 257, 688, 295, 4670, 589, 1096, 294, 341, 30669, 51276], "temperature": 0.0, "avg_logprob": -0.16080565330309746, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0006460368167608976}, {"id": 159, "seek": 101072, "start": 1028.96, "end": 1034.08, "text": " right now. Some important questions which are especially interesting to me are which parts", "tokens": [51276, 558, 586, 13, 2188, 1021, 1651, 597, 366, 2318, 1880, 281, 385, 366, 597, 3166, 51532], "temperature": 0.0, "avg_logprob": -0.16080565330309746, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0006460368167608976}, {"id": 160, "seek": 101072, "start": 1034.08, "end": 1040.4, "text": " of the world model should be learned versus encoded? And how do we want intelligence to scale?", "tokens": [51532, 295, 264, 1002, 2316, 820, 312, 3264, 5717, 2058, 12340, 30, 400, 577, 360, 321, 528, 7599, 281, 4373, 30, 51848], "temperature": 0.0, "avg_logprob": -0.16080565330309746, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0006460368167608976}, {"id": 161, "seek": 104040, "start": 1041.3600000000001, "end": 1046.5600000000002, "text": " Both in the sense of composing systems naturally, there should be some very like natural way that", "tokens": [50412, 6767, 294, 264, 2020, 295, 715, 6110, 3652, 8195, 11, 456, 820, 312, 512, 588, 411, 3303, 636, 300, 50672], "temperature": 0.0, "avg_logprob": -0.07661572243403462, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00028681184630841017}, {"id": 162, "seek": 104040, "start": 1046.5600000000002, "end": 1051.92, "text": " we can compose to intelligent systems and also such that we can scale them with compute.", "tokens": [50672, 321, 393, 35925, 281, 13232, 3652, 293, 611, 1270, 300, 321, 393, 4373, 552, 365, 14722, 13, 50940], "temperature": 0.0, "avg_logprob": -0.07661572243403462, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00028681184630841017}, {"id": 163, "seek": 104040, "start": 1053.0400000000002, "end": 1057.8400000000001, "text": " And I don't mean to restrict myself either to the kinds of compute that we have today.", "tokens": [50996, 400, 286, 500, 380, 914, 281, 7694, 2059, 2139, 281, 264, 3685, 295, 14722, 300, 321, 362, 965, 13, 51236], "temperature": 0.0, "avg_logprob": -0.07661572243403462, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00028681184630841017}, {"id": 164, "seek": 104040, "start": 1057.8400000000001, "end": 1062.72, "text": " We're also working at some exciting new computing paradigms at normal, which might be more", "tokens": [51236, 492, 434, 611, 1364, 412, 512, 4670, 777, 15866, 13480, 328, 2592, 412, 2710, 11, 597, 1062, 312, 544, 51480], "temperature": 0.0, "avg_logprob": -0.07661572243403462, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00028681184630841017}, {"id": 165, "seek": 104040, "start": 1062.72, "end": 1069.52, "text": " compatible with software of this nature. Also the two folks that we referenced at the beginning", "tokens": [51480, 18218, 365, 4722, 295, 341, 3687, 13, 2743, 264, 732, 4024, 300, 321, 32734, 412, 264, 2863, 51820], "temperature": 0.0, "avg_logprob": -0.07661572243403462, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00028681184630841017}, {"id": 166, "seek": 106952, "start": 1069.52, "end": 1073.68, "text": " of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area,", "tokens": [50364, 295, 264, 751, 11, 7506, 389, 12442, 293, 398, 969, 1456, 34, 409, 362, 1096, 534, 4670, 589, 294, 341, 1859, 11, 50572], "temperature": 0.0, "avg_logprob": -0.16154522644846062, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.006687607616186142}, {"id": 167, "seek": 106952, "start": 1075.28, "end": 1080.6399999999999, "text": " which is very inspiring. And so in particular, G flow nets are also probabilistic graphical", "tokens": [50652, 597, 307, 588, 15883, 13, 400, 370, 294, 1729, 11, 460, 3095, 36170, 366, 611, 31959, 3142, 35942, 50920], "temperature": 0.0, "avg_logprob": -0.16154522644846062, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.006687607616186142}, {"id": 168, "seek": 106952, "start": 1080.6399999999999, "end": 1088.24, "text": " models, which I think folks will find a natural next step in reading if you so desire.", "tokens": [50920, 5245, 11, 597, 286, 519, 4024, 486, 915, 257, 3303, 958, 1823, 294, 3760, 498, 291, 370, 7516, 13, 51300], "temperature": 0.0, "avg_logprob": -0.16154522644846062, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.006687607616186142}, {"id": 169, "seek": 106952, "start": 1091.36, "end": 1093.76, "text": " And that's it for me.", "tokens": [51456, 400, 300, 311, 309, 337, 385, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16154522644846062, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.006687607616186142}, {"id": 170, "seek": 109376, "start": 1094.4, "end": 1103.36, "text": " Awesome. Thank you. Wow, very cool.", "tokens": [50396, 10391, 13, 1044, 291, 13, 3153, 11, 588, 1627, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2266852237560131, "compression_ratio": 1.2980132450331126, "no_speech_prob": 0.01690487004816532}, {"id": 171, "seek": 109376, "start": 1106.56, "end": 1111.04, "text": " Dan, do you want to give a first reflection or thought? And then meanwhile, anyone who's", "tokens": [51004, 3394, 11, 360, 291, 528, 281, 976, 257, 700, 12914, 420, 1194, 30, 400, 550, 29252, 11, 2878, 567, 311, 51228], "temperature": 0.0, "avg_logprob": -0.2266852237560131, "compression_ratio": 1.2980132450331126, "no_speech_prob": 0.01690487004816532}, {"id": 172, "seek": 109376, "start": 1111.04, "end": 1114.72, "text": " watching live, please feel free to write questions. I'll relay them in.", "tokens": [51228, 1976, 1621, 11, 1767, 841, 1737, 281, 2464, 1651, 13, 286, 603, 24214, 552, 294, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2266852237560131, "compression_ratio": 1.2980132450331126, "no_speech_prob": 0.01690487004816532}, {"id": 173, "seek": 111472, "start": 1114.72, "end": 1125.04, "text": " Absolutely. So hi, I'm Dan. I work with Phoebe on this project. And yes, the", "tokens": [50364, 7021, 13, 407, 4879, 11, 286, 478, 3394, 13, 286, 589, 365, 14936, 48593, 322, 341, 1716, 13, 400, 2086, 11, 264, 50880], "temperature": 0.0, "avg_logprob": -0.13760034074174596, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013014369644224644}, {"id": 174, "seek": 111472, "start": 1126.48, "end": 1133.1200000000001, "text": " I think the thing that's most exciting about this for me personally is sort of twofold.", "tokens": [50952, 286, 519, 264, 551, 300, 311, 881, 4670, 466, 341, 337, 385, 5665, 307, 1333, 295, 732, 18353, 13, 51284], "temperature": 0.0, "avg_logprob": -0.13760034074174596, "compression_ratio": 1.251908396946565, "no_speech_prob": 0.013014369644224644}, {"id": 175, "seek": 113312, "start": 1133.84, "end": 1144.32, "text": " One of them is that it's a way of avoiding sort of having to trust a language model to", "tokens": [50400, 1485, 295, 552, 307, 300, 309, 311, 257, 636, 295, 20220, 1333, 295, 1419, 281, 3361, 257, 2856, 2316, 281, 50924], "temperature": 0.0, "avg_logprob": -0.1414422266411059, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.43748700618743896}, {"id": 176, "seek": 113312, "start": 1145.04, "end": 1154.8, "text": " understand and reason about text. Because they're not it's not that bad. The thing is that the", "tokens": [50960, 1223, 293, 1778, 466, 2487, 13, 1436, 436, 434, 406, 309, 311, 406, 300, 1578, 13, 440, 551, 307, 300, 264, 51448], "temperature": 0.0, "avg_logprob": -0.1414422266411059, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.43748700618743896}, {"id": 177, "seek": 113312, "start": 1154.8, "end": 1160.08, "text": " extremely strange thing about language models is they're quite good at being almost good enough.", "tokens": [51448, 4664, 5861, 551, 466, 2856, 5245, 307, 436, 434, 1596, 665, 412, 885, 1920, 665, 1547, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1414422266411059, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.43748700618743896}, {"id": 178, "seek": 116008, "start": 1161.04, "end": 1167.36, "text": " But they're never quite what you could use. You could never use a language model to, I don't know,", "tokens": [50412, 583, 436, 434, 1128, 1596, 437, 291, 727, 764, 13, 509, 727, 1128, 764, 257, 2856, 2316, 281, 11, 286, 500, 380, 458, 11, 50728], "temperature": 0.0, "avg_logprob": -0.07968241373697917, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.004330414813011885}, {"id": 179, "seek": 116008, "start": 1170.0, "end": 1175.52, "text": " sort of triage, like an important sort of situation where a bunch of different things", "tokens": [50860, 1333, 295, 1376, 609, 11, 411, 364, 1021, 1333, 295, 2590, 689, 257, 3840, 295, 819, 721, 51136], "temperature": 0.0, "avg_logprob": -0.07968241373697917, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.004330414813011885}, {"id": 180, "seek": 116008, "start": 1175.52, "end": 1179.84, "text": " are coming in, you have to make a decision about which is important. The reason you can't do that", "tokens": [51136, 366, 1348, 294, 11, 291, 362, 281, 652, 257, 3537, 466, 597, 307, 1021, 13, 440, 1778, 291, 393, 380, 360, 300, 51352], "temperature": 0.0, "avg_logprob": -0.07968241373697917, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.004330414813011885}, {"id": 181, "seek": 116008, "start": 1179.84, "end": 1187.1999999999998, "text": " is you simply cannot understand the encoded biases. You cannot get it to reliably generate", "tokens": [51352, 307, 291, 2935, 2644, 1223, 264, 2058, 12340, 32152, 13, 509, 2644, 483, 309, 281, 49927, 8460, 51720], "temperature": 0.0, "avg_logprob": -0.07968241373697917, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.004330414813011885}, {"id": 182, "seek": 118720, "start": 1187.2, "end": 1192.0800000000002, "text": " reasoning. You can ask it for reasoning. But the thing that it prints out is not the reasoning", "tokens": [50364, 21577, 13, 509, 393, 1029, 309, 337, 21577, 13, 583, 264, 551, 300, 309, 22305, 484, 307, 406, 264, 21577, 50608], "temperature": 0.0, "avg_logprob": -0.08816419558578663, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.010982668027281761}, {"id": 183, "seek": 118720, "start": 1192.0800000000002, "end": 1199.28, "text": " that it used internally because it doesn't reason. Fundamentally, while these have input and output", "tokens": [50608, 300, 309, 1143, 19501, 570, 309, 1177, 380, 1778, 13, 13493, 2466, 379, 11, 1339, 613, 362, 4846, 293, 5598, 50968], "temperature": 0.0, "avg_logprob": -0.08816419558578663, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.010982668027281761}, {"id": 184, "seek": 118720, "start": 1199.28, "end": 1205.3600000000001, "text": " that are natural language, they are not artificially intelligent. They are just prediction machines.", "tokens": [50968, 300, 366, 3303, 2856, 11, 436, 366, 406, 39905, 2270, 13232, 13, 814, 366, 445, 17630, 8379, 13, 51272], "temperature": 0.0, "avg_logprob": -0.08816419558578663, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.010982668027281761}, {"id": 185, "seek": 118720, "start": 1206.56, "end": 1211.6000000000001, "text": " And so we have to be very careful about not anthropomorphizing them. So this is a way of", "tokens": [51332, 400, 370, 321, 362, 281, 312, 588, 5026, 466, 406, 22727, 32702, 3319, 552, 13, 407, 341, 307, 257, 636, 295, 51584], "temperature": 0.0, "avg_logprob": -0.08816419558578663, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.010982668027281761}, {"id": 186, "seek": 121160, "start": 1211.6, "end": 1218.7199999999998, "text": " using those incredibly powerful prediction machines in a framework where we can", "tokens": [50364, 1228, 729, 6252, 4005, 17630, 8379, 294, 257, 8388, 689, 321, 393, 50720], "temperature": 0.0, "avg_logprob": -0.09679811383471076, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.030199838802218437}, {"id": 187, "seek": 121160, "start": 1220.32, "end": 1229.84, "text": " make sure that we essentially keep a record of what we're doing so that a human can look at it.", "tokens": [50800, 652, 988, 300, 321, 4476, 1066, 257, 2136, 295, 437, 321, 434, 884, 370, 300, 257, 1952, 393, 574, 412, 309, 13, 51276], "temperature": 0.0, "avg_logprob": -0.09679811383471076, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.030199838802218437}, {"id": 188, "seek": 121160, "start": 1229.84, "end": 1234.3999999999999, "text": " Because, I mean, there's a lot of talk in this world about sort of post-human AI", "tokens": [51276, 1436, 11, 286, 914, 11, 456, 311, 257, 688, 295, 751, 294, 341, 1002, 466, 1333, 295, 2183, 12, 18796, 7318, 51504], "temperature": 0.0, "avg_logprob": -0.09679811383471076, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.030199838802218437}, {"id": 189, "seek": 121160, "start": 1235.04, "end": 1239.6799999999998, "text": " and those sorts of things. The idea that the machines will become intelligent enough,", "tokens": [51536, 293, 729, 7527, 295, 721, 13, 440, 1558, 300, 264, 8379, 486, 1813, 13232, 1547, 11, 51768], "temperature": 0.0, "avg_logprob": -0.09679811383471076, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.030199838802218437}, {"id": 190, "seek": 123968, "start": 1239.68, "end": 1243.76, "text": " or the machines will rise up in a slightly more alarming type of way.", "tokens": [50364, 420, 264, 8379, 486, 6272, 493, 294, 257, 4748, 544, 44043, 2010, 295, 636, 13, 50568], "temperature": 0.0, "avg_logprob": -0.10856449036371141, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.003073473460972309}, {"id": 191, "seek": 123968, "start": 1245.76, "end": 1249.8400000000001, "text": " And that's all great and wonderful, but that's not particularly interesting to us at normal.", "tokens": [50668, 400, 300, 311, 439, 869, 293, 3715, 11, 457, 300, 311, 406, 4098, 1880, 281, 505, 412, 2710, 13, 50872], "temperature": 0.0, "avg_logprob": -0.10856449036371141, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.003073473460972309}, {"id": 192, "seek": 123968, "start": 1249.8400000000001, "end": 1256.4, "text": " We're much more interested in sort of having mimicking explicit decision processes so that", "tokens": [50872, 492, 434, 709, 544, 3102, 294, 1333, 295, 1419, 12247, 10401, 13691, 3537, 7555, 370, 300, 51200], "temperature": 0.0, "avg_logprob": -0.10856449036371141, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.003073473460972309}, {"id": 193, "seek": 123968, "start": 1256.4, "end": 1263.44, "text": " a human can audit them and can make these things work. That's kind of the area that we're coming from.", "tokens": [51200, 257, 1952, 393, 17748, 552, 293, 393, 652, 613, 721, 589, 13, 663, 311, 733, 295, 264, 1859, 300, 321, 434, 1348, 490, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10856449036371141, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.003073473460972309}, {"id": 194, "seek": 126344, "start": 1264.4, "end": 1271.28, "text": " Awesome. All right. I'll go to a question from the chat. So Josh asks,", "tokens": [50412, 10391, 13, 1057, 558, 13, 286, 603, 352, 281, 257, 1168, 490, 264, 5081, 13, 407, 9785, 8962, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1376557783647017, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.008845726028084755}, {"id": 195, "seek": 126344, "start": 1271.8400000000001, "end": 1278.0800000000002, "text": " great talk. Where does hypothesis relevance enter the calculus? Is it folded into confidence?", "tokens": [50784, 869, 751, 13, 2305, 775, 17291, 32684, 3242, 264, 33400, 30, 1119, 309, 23940, 666, 6687, 30, 51096], "temperature": 0.0, "avg_logprob": -0.1376557783647017, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.008845726028084755}, {"id": 196, "seek": 126344, "start": 1278.0800000000002, "end": 1281.8400000000001, "text": " Not sure if it ought to be. Just saw hypothesis relevance mentioned.", "tokens": [51096, 1726, 988, 498, 309, 13416, 281, 312, 13, 1449, 1866, 17291, 32684, 2835, 13, 51284], "temperature": 0.0, "avg_logprob": -0.1376557783647017, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.008845726028084755}, {"id": 197, "seek": 126344, "start": 1285.3600000000001, "end": 1291.44, "text": " Hypothesis relevance. Does that mean like which hypotheses are conditioned on each other?", "tokens": [51460, 45649, 4624, 271, 32684, 13, 4402, 300, 914, 411, 597, 49969, 366, 35833, 322, 1184, 661, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1376557783647017, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.008845726028084755}, {"id": 198, "seek": 129144, "start": 1292.4, "end": 1296.56, "text": " Is it possible to ask a clarifying question there? Maybe Dan, you have a better idea.", "tokens": [50412, 1119, 309, 1944, 281, 1029, 257, 6093, 5489, 1168, 456, 30, 2704, 3394, 11, 291, 362, 257, 1101, 1558, 13, 50620], "temperature": 0.0, "avg_logprob": -0.14096139726184664, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.0006877922569401562}, {"id": 199, "seek": 129144, "start": 1296.56, "end": 1302.8, "text": " They can follow up. But yeah, I also wondered about this. You might know what was relevance.", "tokens": [50620, 814, 393, 1524, 493, 13, 583, 1338, 11, 286, 611, 17055, 466, 341, 13, 509, 1062, 458, 437, 390, 32684, 13, 50932], "temperature": 0.0, "avg_logprob": -0.14096139726184664, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.0006877922569401562}, {"id": 200, "seek": 129144, "start": 1303.3600000000001, "end": 1311.44, "text": " Maybe the temperature and the rainfall were relevant, but then how does this approach help us", "tokens": [50960, 2704, 264, 4292, 293, 264, 29382, 645, 7340, 11, 457, 550, 577, 775, 341, 3109, 854, 505, 51364], "temperature": 0.0, "avg_logprob": -0.14096139726184664, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.0006877922569401562}, {"id": 201, "seek": 129144, "start": 1312.0800000000002, "end": 1317.44, "text": " understand when one of those relevant factors no longer is relevant or when a new relevant", "tokens": [51396, 1223, 562, 472, 295, 729, 7340, 6771, 572, 2854, 307, 7340, 420, 562, 257, 777, 7340, 51664], "temperature": 0.0, "avg_logprob": -0.14096139726184664, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.0006877922569401562}, {"id": 202, "seek": 131744, "start": 1317.44, "end": 1325.1200000000001, "text": " factor comes into play? Yeah, these are great questions. So I think in terms of understanding", "tokens": [50364, 5952, 1487, 666, 862, 30, 865, 11, 613, 366, 869, 1651, 13, 407, 286, 519, 294, 2115, 295, 3701, 50748], "temperature": 0.0, "avg_logprob": -0.09021973042261033, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00028684092103503644}, {"id": 203, "seek": 131744, "start": 1325.1200000000001, "end": 1330.48, "text": " in an automatic sense, when two hypotheses are relevant to each other, we can leverage", "tokens": [50748, 294, 364, 12509, 2020, 11, 562, 732, 49969, 366, 7340, 281, 1184, 661, 11, 321, 393, 13982, 51016], "temperature": 0.0, "avg_logprob": -0.09021973042261033, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00028684092103503644}, {"id": 204, "seek": 131744, "start": 1331.28, "end": 1338.48, "text": " embedding type language models for this kind of thing also. If we don't have a more kind of like", "tokens": [51056, 12240, 3584, 2010, 2856, 5245, 337, 341, 733, 295, 551, 611, 13, 759, 321, 500, 380, 362, 257, 544, 733, 295, 411, 51416], "temperature": 0.0, "avg_logprob": -0.09021973042261033, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00028684092103503644}, {"id": 205, "seek": 131744, "start": 1338.48, "end": 1346.24, "text": " structured human intuitive sense for when two hypotheses are related, in terms of like how", "tokens": [51416, 18519, 1952, 21769, 2020, 337, 562, 732, 49969, 366, 4077, 11, 294, 2115, 295, 411, 577, 51804], "temperature": 0.0, "avg_logprob": -0.09021973042261033, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00028684092103503644}, {"id": 206, "seek": 134624, "start": 1346.24, "end": 1350.72, "text": " those relationships evolve over time, this is something that's really interesting to me.", "tokens": [50364, 729, 6159, 16693, 670, 565, 11, 341, 307, 746, 300, 311, 534, 1880, 281, 385, 13, 50588], "temperature": 0.0, "avg_logprob": -0.08856505225686466, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015485485782846808}, {"id": 207, "seek": 134624, "start": 1350.72, "end": 1356.16, "text": " And I think looking at the theory behind structure learning or when we propose to add new nodes to", "tokens": [50588, 400, 286, 519, 1237, 412, 264, 5261, 2261, 3877, 2539, 420, 562, 321, 17421, 281, 909, 777, 13891, 281, 50860], "temperature": 0.0, "avg_logprob": -0.08856505225686466, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015485485782846808}, {"id": 208, "seek": 134624, "start": 1356.16, "end": 1361.84, "text": " the network or propose to add a new edge to the network or things like this is a really exciting", "tokens": [50860, 264, 3209, 420, 17421, 281, 909, 257, 777, 4691, 281, 264, 3209, 420, 721, 411, 341, 307, 257, 534, 4670, 51144], "temperature": 0.0, "avg_logprob": -0.08856505225686466, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015485485782846808}, {"id": 209, "seek": 134624, "start": 1361.84, "end": 1367.6, "text": " research direction. Although I don't have like a silver bullet answer to how we should do that.", "tokens": [51144, 2132, 3513, 13, 5780, 286, 500, 380, 362, 411, 257, 8753, 11632, 1867, 281, 577, 321, 820, 360, 300, 13, 51432], "temperature": 0.0, "avg_logprob": -0.08856505225686466, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015485485782846808}, {"id": 210, "seek": 136760, "start": 1368.24, "end": 1375.76, "text": " Just to like add a little bit more to that, it is like it is a really interesting research", "tokens": [50396, 1449, 281, 411, 909, 257, 707, 857, 544, 281, 300, 11, 309, 307, 411, 309, 307, 257, 534, 1880, 2132, 50772], "temperature": 0.0, "avg_logprob": -0.10426310130528041, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.016908809542655945}, {"id": 211, "seek": 136760, "start": 1375.76, "end": 1382.56, "text": " direction. Like one of the things that Phoebe mentioned in the talk is that there is a difficult", "tokens": [50772, 3513, 13, 1743, 472, 295, 264, 721, 300, 14936, 48593, 2835, 294, 264, 751, 307, 300, 456, 307, 257, 2252, 51112], "temperature": 0.0, "avg_logprob": -0.10426310130528041, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.016908809542655945}, {"id": 212, "seek": 136760, "start": 1382.56, "end": 1386.7199999999998, "text": " step that we're not talking about, which is actually translating this natural language", "tokens": [51112, 1823, 300, 321, 434, 406, 1417, 466, 11, 597, 307, 767, 35030, 341, 3303, 2856, 51320], "temperature": 0.0, "avg_logprob": -0.10426310130528041, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.016908809542655945}, {"id": 213, "seek": 136760, "start": 1386.7199999999998, "end": 1394.0, "text": " into reasonable hypotheses. So there is a step in there where you take essentially a chunk of text", "tokens": [51320, 666, 10585, 49969, 13, 407, 456, 307, 257, 1823, 294, 456, 689, 291, 747, 4476, 257, 16635, 295, 2487, 51684], "temperature": 0.0, "avg_logprob": -0.10426310130528041, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.016908809542655945}, {"id": 214, "seek": 139400, "start": 1394.0, "end": 1401.52, "text": " and you have to decide if this is a hypothesis, if this is a hypothesis we've seen before,", "tokens": [50364, 293, 291, 362, 281, 4536, 498, 341, 307, 257, 17291, 11, 498, 341, 307, 257, 17291, 321, 600, 1612, 949, 11, 50740], "temperature": 0.0, "avg_logprob": -0.09124152214972528, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00060691824182868}, {"id": 215, "seek": 139400, "start": 1402.16, "end": 1409.28, "text": " if this is a sub hypothesis or a clarification of a hypothesis that we've seen before,", "tokens": [50772, 498, 341, 307, 257, 1422, 17291, 420, 257, 34449, 295, 257, 17291, 300, 321, 600, 1612, 949, 11, 51128], "temperature": 0.0, "avg_logprob": -0.09124152214972528, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00060691824182868}, {"id": 216, "seek": 139400, "start": 1409.28, "end": 1416.16, "text": " and so on and so forth. So that in some sense part of the data processing and it is an important", "tokens": [51128, 293, 370, 322, 293, 370, 5220, 13, 407, 300, 294, 512, 2020, 644, 295, 264, 1412, 9007, 293, 309, 307, 364, 1021, 51472], "temperature": 0.0, "avg_logprob": -0.09124152214972528, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00060691824182868}, {"id": 217, "seek": 139400, "start": 1416.72, "end": 1423.44, "text": " step and one that we are sort of continuing to work on and refine. The other thing like a different", "tokens": [51500, 1823, 293, 472, 300, 321, 366, 1333, 295, 9289, 281, 589, 322, 293, 33906, 13, 440, 661, 551, 411, 257, 819, 51836], "temperature": 0.0, "avg_logprob": -0.09124152214972528, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00060691824182868}, {"id": 218, "seek": 142400, "start": 1424.16, "end": 1429.52, "text": " sort of interpretation of the question around relevance is around sort of", "tokens": [50372, 1333, 295, 14174, 295, 264, 1168, 926, 32684, 307, 926, 1333, 295, 50640], "temperature": 0.0, "avg_logprob": -0.13294825496443782, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.001596940914168954}, {"id": 219, "seek": 142400, "start": 1430.72, "end": 1436.16, "text": " is the hypothesis relevant to the thing that you're looking at? I mean we could have a hypothesis", "tokens": [50700, 307, 264, 17291, 7340, 281, 264, 551, 300, 291, 434, 1237, 412, 30, 286, 914, 321, 727, 362, 257, 17291, 50972], "temperature": 0.0, "avg_logprob": -0.13294825496443782, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.001596940914168954}, {"id": 220, "seek": 142400, "start": 1436.16, "end": 1443.92, "text": " the sky is blue, but if we are deciding to deciding you know whether or not we need to", "tokens": [50972, 264, 5443, 307, 3344, 11, 457, 498, 321, 366, 17990, 281, 17990, 291, 458, 1968, 420, 406, 321, 643, 281, 51360], "temperature": 0.0, "avg_logprob": -0.13294825496443782, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.001596940914168954}, {"id": 221, "seek": 142400, "start": 1443.92, "end": 1451.6, "text": " check that part's oil, like the truth or not of the color of the sky is very irrelevant.", "tokens": [51360, 1520, 300, 644, 311, 3184, 11, 411, 264, 3494, 420, 406, 295, 264, 2017, 295, 264, 5443, 307, 588, 28682, 13, 51744], "temperature": 0.0, "avg_logprob": -0.13294825496443782, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.001596940914168954}, {"id": 222, "seek": 145160, "start": 1451.6, "end": 1457.84, "text": " And that then comes into the nice thing about having your world described as a", "tokens": [50364, 400, 300, 550, 1487, 666, 264, 1481, 551, 466, 1419, 428, 1002, 7619, 382, 257, 50676], "temperature": 0.0, "avg_logprob": -0.07837924204374615, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00039193572592921555}, {"id": 223, "seek": 145160, "start": 1459.6799999999998, "end": 1465.4399999999998, "text": " collection of statements with truth values associated with them in that you can directly", "tokens": [50768, 5765, 295, 12363, 365, 3494, 4190, 6615, 365, 552, 294, 300, 291, 393, 3838, 51056], "temperature": 0.0, "avg_logprob": -0.07837924204374615, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00039193572592921555}, {"id": 224, "seek": 145160, "start": 1465.4399999999998, "end": 1473.36, "text": " reason over them. So you can put a classical decision framework over that to take into account", "tokens": [51056, 1778, 670, 552, 13, 407, 291, 393, 829, 257, 13735, 3537, 8388, 670, 300, 281, 747, 666, 2696, 51452], "temperature": 0.0, "avg_logprob": -0.07837924204374615, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00039193572592921555}, {"id": 225, "seek": 145160, "start": 1473.9199999999998, "end": 1478.48, "text": " both the sort of the knowledge you have of the world and also which parts of these worlds are", "tokens": [51480, 1293, 264, 1333, 295, 264, 3601, 291, 362, 295, 264, 1002, 293, 611, 597, 3166, 295, 613, 13401, 366, 51708], "temperature": 0.0, "avg_logprob": -0.07837924204374615, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00039193572592921555}, {"id": 226, "seek": 147848, "start": 1479.1200000000001, "end": 1485.04, "text": " sort of unknown. So in that sort of situation the person using the world model to construct a", "tokens": [50396, 1333, 295, 9841, 13, 407, 294, 300, 1333, 295, 2590, 264, 954, 1228, 264, 1002, 2316, 281, 7690, 257, 50692], "temperature": 0.0, "avg_logprob": -0.10282763223799449, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.005058813374489546}, {"id": 227, "seek": 147848, "start": 1489.04, "end": 1498.48, "text": " sort of decision or an output will be responsible in some sense for assigning a weight or a cost", "tokens": [50892, 1333, 295, 3537, 420, 364, 5598, 486, 312, 6250, 294, 512, 2020, 337, 49602, 257, 3364, 420, 257, 2063, 51364], "temperature": 0.0, "avg_logprob": -0.10282763223799449, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.005058813374489546}, {"id": 228, "seek": 147848, "start": 1498.48, "end": 1503.92, "text": " to each hypothesis being true. And for some of those hypotheses obviously it will be zero", "tokens": [51364, 281, 1184, 17291, 885, 2074, 13, 400, 337, 512, 295, 729, 49969, 2745, 309, 486, 312, 4018, 51636], "temperature": 0.0, "avg_logprob": -0.10282763223799449, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.005058813374489546}, {"id": 229, "seek": 150392, "start": 1504.88, "end": 1508.72, "text": " because again we do not care about the color of the sky if all I want to know", "tokens": [50412, 570, 797, 321, 360, 406, 1127, 466, 264, 2017, 295, 264, 5443, 498, 439, 286, 528, 281, 458, 50604], "temperature": 0.0, "avg_logprob": -0.0750213117680998, "compression_ratio": 1.973568281938326, "no_speech_prob": 0.036162152886390686}, {"id": 230, "seek": 150392, "start": 1508.72, "end": 1516.0800000000002, "text": " is if I need to change the oil in my car. So that's the sort of the other end of the answer.", "tokens": [50604, 307, 498, 286, 643, 281, 1319, 264, 3184, 294, 452, 1032, 13, 407, 300, 311, 264, 1333, 295, 264, 661, 917, 295, 264, 1867, 13, 50972], "temperature": 0.0, "avg_logprob": -0.0750213117680998, "compression_ratio": 1.973568281938326, "no_speech_prob": 0.036162152886390686}, {"id": 231, "seek": 150392, "start": 1516.0800000000002, "end": 1519.92, "text": " So there's a version of the answer at the start of the information flow and there's a version of", "tokens": [50972, 407, 456, 311, 257, 3037, 295, 264, 1867, 412, 264, 722, 295, 264, 1589, 3095, 293, 456, 311, 257, 3037, 295, 51164], "temperature": 0.0, "avg_logprob": -0.0750213117680998, "compression_ratio": 1.973568281938326, "no_speech_prob": 0.036162152886390686}, {"id": 232, "seek": 150392, "start": 1519.92, "end": 1525.1200000000001, "text": " the answer at the end of the information flow. But it is a tricky point and one that we are sort", "tokens": [51164, 264, 1867, 412, 264, 917, 295, 264, 1589, 3095, 13, 583, 309, 307, 257, 12414, 935, 293, 472, 300, 321, 366, 1333, 51424], "temperature": 0.0, "avg_logprob": -0.0750213117680998, "compression_ratio": 1.973568281938326, "no_speech_prob": 0.036162152886390686}, {"id": 233, "seek": 150392, "start": 1525.1200000000001, "end": 1530.4, "text": " of continuing to iterate on to try and find sort of good ways on both ends of that.", "tokens": [51424, 295, 9289, 281, 44497, 322, 281, 853, 293, 915, 1333, 295, 665, 2098, 322, 1293, 5314, 295, 300, 13, 51688], "temperature": 0.0, "avg_logprob": -0.0750213117680998, "compression_ratio": 1.973568281938326, "no_speech_prob": 0.036162152886390686}, {"id": 234, "seek": 153040, "start": 1530.5600000000002, "end": 1541.68, "text": " Yeah well a lot there. It's very interesting how in that presentation in response I heard", "tokens": [50372, 865, 731, 257, 688, 456, 13, 467, 311, 588, 1880, 577, 294, 300, 5860, 294, 4134, 286, 2198, 50928], "temperature": 0.0, "avg_logprob": -0.14400959014892578, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.002082554157823324}, {"id": 235, "seek": 153040, "start": 1541.68, "end": 1551.1200000000001, "text": " both about probability distributions on rules and rules on probability distributions and like", "tokens": [50928, 1293, 466, 8482, 37870, 322, 4474, 293, 4474, 322, 8482, 37870, 293, 411, 51400], "temperature": 0.0, "avg_logprob": -0.14400959014892578, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.002082554157823324}, {"id": 236, "seek": 153040, "start": 1552.0800000000002, "end": 1557.2800000000002, "text": " which one whether it's the tail wagging the dog or the horse in the cart how to design", "tokens": [51448, 597, 472, 1968, 309, 311, 264, 6838, 36854, 3249, 264, 3000, 420, 264, 6832, 294, 264, 5467, 577, 281, 1715, 51708], "temperature": 0.0, "avg_logprob": -0.14400959014892578, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.002082554157823324}, {"id": 237, "seek": 155728, "start": 1557.28, "end": 1561.6, "text": " these synthetic intelligence systems that appropriately bring together", "tokens": [50364, 613, 23420, 7599, 3652, 300, 23505, 1565, 1214, 50580], "temperature": 0.0, "avg_logprob": -0.13036998949552836, "compression_ratio": 1.7028301886792452, "no_speech_prob": 0.0026312193367630243}, {"id": 238, "seek": 155728, "start": 1562.72, "end": 1569.92, "text": " aspects that are more symbolic more rule like and then more probabilistic more embedding like.", "tokens": [50636, 7270, 300, 366, 544, 25755, 544, 4978, 411, 293, 550, 544, 31959, 3142, 544, 12240, 3584, 411, 13, 50996], "temperature": 0.0, "avg_logprob": -0.13036998949552836, "compression_ratio": 1.7028301886792452, "no_speech_prob": 0.0026312193367630243}, {"id": 239, "seek": 155728, "start": 1569.92, "end": 1577.28, "text": " So where does that end with you or how do you see the design of these systems with mixed symbolic", "tokens": [50996, 407, 689, 775, 300, 917, 365, 291, 420, 577, 360, 291, 536, 264, 1715, 295, 613, 3652, 365, 7467, 25755, 51364], "temperature": 0.0, "avg_logprob": -0.13036998949552836, "compression_ratio": 1.7028301886792452, "no_speech_prob": 0.0026312193367630243}, {"id": 240, "seek": 155728, "start": 1577.28, "end": 1582.96, "text": " and probabilistic components? Yeah yeah that's a great point and I think this really gets it like", "tokens": [51364, 293, 31959, 3142, 6677, 30, 865, 1338, 300, 311, 257, 869, 935, 293, 286, 519, 341, 534, 2170, 309, 411, 51648], "temperature": 0.0, "avg_logprob": -0.13036998949552836, "compression_ratio": 1.7028301886792452, "no_speech_prob": 0.0026312193367630243}, {"id": 241, "seek": 158296, "start": 1583.04, "end": 1587.76, "text": " which parts of the world model should be learned or should be represented in some like more", "tokens": [50368, 597, 3166, 295, 264, 1002, 2316, 820, 312, 3264, 420, 820, 312, 10379, 294, 512, 411, 544, 50604], "temperature": 0.0, "avg_logprob": -0.0868984431755252, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.003272460773587227}, {"id": 242, "seek": 158296, "start": 1587.76, "end": 1594.32, "text": " discrete space versus like encoded based on our own human intuition for rules and structure.", "tokens": [50604, 27706, 1901, 5717, 411, 2058, 12340, 2361, 322, 527, 1065, 1952, 24002, 337, 4474, 293, 3877, 13, 50932], "temperature": 0.0, "avg_logprob": -0.0868984431755252, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.003272460773587227}, {"id": 243, "seek": 158296, "start": 1595.92, "end": 1600.8, "text": " And I think like maybe this would be fairly represented as a cop-out answer but I think", "tokens": [51012, 400, 286, 519, 411, 1310, 341, 576, 312, 6457, 10379, 382, 257, 2971, 12, 346, 1867, 457, 286, 519, 51256], "temperature": 0.0, "avg_logprob": -0.0868984431755252, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.003272460773587227}, {"id": 244, "seek": 158296, "start": 1600.8, "end": 1605.68, "text": " it depends a lot on the application. I think like when we're developing systems like this and", "tokens": [51256, 309, 5946, 257, 688, 322, 264, 3861, 13, 286, 519, 411, 562, 321, 434, 6416, 3652, 411, 341, 293, 51500], "temperature": 0.0, "avg_logprob": -0.0868984431755252, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.003272460773587227}, {"id": 245, "seek": 160568, "start": 1605.76, "end": 1613.8400000000001, "text": " just trying to iterate through as many different hypotheses as you can quickly like choosing an", "tokens": [50368, 445, 1382, 281, 44497, 807, 382, 867, 819, 49969, 382, 291, 393, 2661, 411, 10875, 364, 50772], "temperature": 0.0, "avg_logprob": -0.07578717002385779, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.0010004076175391674}, {"id": 246, "seek": 160568, "start": 1613.8400000000001, "end": 1620.96, "text": " application and benchmarking and testing and seeing like what actually works is a go-to strategy for", "tokens": [50772, 3861, 293, 18927, 278, 293, 4997, 293, 2577, 411, 437, 767, 1985, 307, 257, 352, 12, 1353, 5206, 337, 51128], "temperature": 0.0, "avg_logprob": -0.07578717002385779, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.0010004076175391674}, {"id": 247, "seek": 160568, "start": 1620.96, "end": 1629.2, "text": " us in terms of like well which parts should be fixed and are actually helpful to increase", "tokens": [51128, 505, 294, 2115, 295, 411, 731, 597, 3166, 820, 312, 6806, 293, 366, 767, 4961, 281, 3488, 51540], "temperature": 0.0, "avg_logprob": -0.07578717002385779, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.0010004076175391674}, {"id": 248, "seek": 160568, "start": 1629.2, "end": 1634.64, "text": " reliability such that like we can use our human intuition for how this particular you know system", "tokens": [51540, 24550, 1270, 300, 411, 321, 393, 764, 527, 1952, 24002, 337, 577, 341, 1729, 291, 458, 1185, 51812], "temperature": 0.0, "avg_logprob": -0.07578717002385779, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.0010004076175391674}, {"id": 249, "seek": 163464, "start": 1634.64, "end": 1641.1200000000001, "text": " is built versus like well this is something that we we want uncertainty over that's like a really", "tokens": [50364, 307, 3094, 5717, 411, 731, 341, 307, 746, 300, 321, 321, 528, 15697, 670, 300, 311, 411, 257, 534, 50688], "temperature": 0.0, "avg_logprob": -0.09447461505268895, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.00039196162833832204}, {"id": 250, "seek": 163464, "start": 1641.1200000000001, "end": 1647.68, "text": " important part of the learning process for us in terms of yeah that kind of iteration so I think", "tokens": [50688, 1021, 644, 295, 264, 2539, 1399, 337, 505, 294, 2115, 295, 1338, 300, 733, 295, 24784, 370, 286, 519, 51016], "temperature": 0.0, "avg_logprob": -0.09447461505268895, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.00039196162833832204}, {"id": 251, "seek": 163464, "start": 1647.68, "end": 1655.0400000000002, "text": " it probably depends on the application. Yeah it's um it it definitely depends on the application", "tokens": [51016, 309, 1391, 5946, 322, 264, 3861, 13, 865, 309, 311, 1105, 309, 309, 2138, 5946, 322, 264, 3861, 51384], "temperature": 0.0, "avg_logprob": -0.09447461505268895, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.00039196162833832204}, {"id": 252, "seek": 163464, "start": 1655.0400000000002, "end": 1663.68, "text": " it's also like it depends on where the actual challenge points are so we've got like outside of", "tokens": [51384, 309, 311, 611, 411, 309, 5946, 322, 689, 264, 3539, 3430, 2793, 366, 370, 321, 600, 658, 411, 2380, 295, 51816], "temperature": 0.0, "avg_logprob": -0.09447461505268895, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.00039196162833832204}, {"id": 253, "seek": 166368, "start": 1663.68, "end": 1670.0, "text": " this we've got sort of a few other things that we've released publicly that kind of look at this idea", "tokens": [50364, 341, 321, 600, 658, 1333, 295, 257, 1326, 661, 721, 300, 321, 600, 4736, 14843, 300, 733, 295, 574, 412, 341, 1558, 50680], "temperature": 0.0, "avg_logprob": -0.07318287585155074, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.004903302062302828}, {"id": 254, "seek": 166368, "start": 1670.0, "end": 1681.2, "text": " of there being like external rules to the system and whether or not we can add those in.", "tokens": [50680, 295, 456, 885, 411, 8320, 4474, 281, 264, 1185, 293, 1968, 420, 406, 321, 393, 909, 729, 294, 13, 51240], "temperature": 0.0, "avg_logprob": -0.07318287585155074, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.004903302062302828}, {"id": 255, "seek": 166368, "start": 1681.2, "end": 1686.5600000000002, "text": " So one of them is something called constrained generation where we sort of force the model", "tokens": [51240, 407, 472, 295, 552, 307, 746, 1219, 38901, 5125, 689, 321, 1333, 295, 3464, 264, 2316, 51508], "temperature": 0.0, "avg_logprob": -0.07318287585155074, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.004903302062302828}, {"id": 256, "seek": 166368, "start": 1686.5600000000002, "end": 1693.28, "text": " to only produce something valid and that's sort of quite a useful way of removing", "tokens": [51508, 281, 787, 5258, 746, 7363, 293, 300, 311, 1333, 295, 1596, 257, 4420, 636, 295, 12720, 51844], "temperature": 0.0, "avg_logprob": -0.07318287585155074, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.004903302062302828}, {"id": 257, "seek": 169328, "start": 1693.28, "end": 1699.68, "text": " one particular aspect of stress from the model which is that it may make sort of syntactically", "tokens": [50364, 472, 1729, 4171, 295, 4244, 490, 264, 2316, 597, 307, 300, 309, 815, 652, 1333, 295, 23980, 578, 984, 50684], "temperature": 0.0, "avg_logprob": -0.07169969703840173, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0012059321161359549}, {"id": 258, "seek": 169328, "start": 1700.32, "end": 1709.36, "text": " or sort of somehow incoherent outputs that don't follow the rules and then we can then", "tokens": [50716, 420, 1333, 295, 6063, 834, 78, 511, 317, 23930, 300, 500, 380, 1524, 264, 4474, 293, 550, 321, 393, 550, 51168], "temperature": 0.0, "avg_logprob": -0.07169969703840173, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0012059321161359549}, {"id": 259, "seek": 169328, "start": 1709.36, "end": 1714.8, "text": " focus with the rest of our energy we can then take that as given and focus with the rest of our", "tokens": [51168, 1879, 365, 264, 1472, 295, 527, 2281, 321, 393, 550, 747, 300, 382, 2212, 293, 1879, 365, 264, 1472, 295, 527, 51440], "temperature": 0.0, "avg_logprob": -0.07169969703840173, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0012059321161359549}, {"id": 260, "seek": 169328, "start": 1714.8, "end": 1722.8799999999999, "text": " energy on improving the bit that we don't have rules for. So those sorts of things and sort of a", "tokens": [51440, 2281, 322, 11470, 264, 857, 300, 321, 500, 380, 362, 4474, 337, 13, 407, 729, 7527, 295, 721, 293, 1333, 295, 257, 51844], "temperature": 0.0, "avg_logprob": -0.07169969703840173, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0012059321161359549}, {"id": 261, "seek": 172288, "start": 1722.88, "end": 1728.72, "text": " different version is trying to improve something by saying no you broke a rule we need to like go", "tokens": [50364, 819, 3037, 307, 1382, 281, 3470, 746, 538, 1566, 572, 291, 6902, 257, 4978, 321, 643, 281, 411, 352, 50656], "temperature": 0.0, "avg_logprob": -0.10711007722666566, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.0009840080747380853}, {"id": 262, "seek": 172288, "start": 1728.72, "end": 1736.88, "text": " back and make this sort of true so this kind of sort of chain of thought prompting type of idea.", "tokens": [50656, 646, 293, 652, 341, 1333, 295, 2074, 370, 341, 733, 295, 1333, 295, 5021, 295, 1194, 12391, 278, 2010, 295, 1558, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10711007722666566, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.0009840080747380853}, {"id": 263, "seek": 172288, "start": 1739.0400000000002, "end": 1742.96, "text": " So so yeah the the symbolic and the probabilistic", "tokens": [51172, 407, 370, 1338, 264, 264, 25755, 293, 264, 31959, 3142, 51368], "temperature": 0.0, "avg_logprob": -0.10711007722666566, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.0009840080747380853}, {"id": 264, "seek": 172288, "start": 1745.6000000000001, "end": 1748.8000000000002, "text": " I think in our minds live very closely together as", "tokens": [51500, 286, 519, 294, 527, 9634, 1621, 588, 8185, 1214, 382, 51660], "temperature": 0.0, "avg_logprob": -0.10711007722666566, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.0009840080747380853}, {"id": 265, "seek": 174880, "start": 1749.52, "end": 1757.2, "text": " two tools that don't completely solve the same problem and I think there's sort of in", "tokens": [50400, 732, 3873, 300, 500, 380, 2584, 5039, 264, 912, 1154, 293, 286, 519, 456, 311, 1333, 295, 294, 50784], "temperature": 0.0, "avg_logprob": -0.08906562153886004, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.004902574233710766}, {"id": 266, "seek": 174880, "start": 1757.2, "end": 1762.1599999999999, "text": " in the world of I'm not sure how familiar anyone in the audience is with language modeling but like", "tokens": [50784, 294, 264, 1002, 295, 286, 478, 406, 988, 577, 4963, 2878, 294, 264, 4034, 307, 365, 2856, 15983, 457, 411, 51032], "temperature": 0.0, "avg_logprob": -0.08906562153886004, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.004902574233710766}, {"id": 267, "seek": 174880, "start": 1762.1599999999999, "end": 1767.12, "text": " in the world of language modeling before this sort of explosion of neural networks and artificial", "tokens": [51032, 294, 264, 1002, 295, 2856, 15983, 949, 341, 1333, 295, 15673, 295, 18161, 9590, 293, 11677, 51280], "temperature": 0.0, "avg_logprob": -0.08906562153886004, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.004902574233710766}, {"id": 268, "seek": 174880, "start": 1767.12, "end": 1774.08, "text": " intelligence type methods there was a lot of work on symbolics of language and grammars and", "tokens": [51280, 7599, 2010, 7150, 456, 390, 257, 688, 295, 589, 322, 5986, 1167, 295, 2856, 293, 17570, 685, 293, 51628], "temperature": 0.0, "avg_logprob": -0.08906562153886004, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.004902574233710766}, {"id": 269, "seek": 177408, "start": 1774.08, "end": 1780.0, "text": " all of that sort of stuff and that work pushed quite a long way forward and this work is pushing", "tokens": [50364, 439, 295, 300, 1333, 295, 1507, 293, 300, 589, 9152, 1596, 257, 938, 636, 2128, 293, 341, 589, 307, 7380, 50660], "temperature": 0.0, "avg_logprob": -0.07436238887698152, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.014058055356144905}, {"id": 270, "seek": 177408, "start": 1780.0, "end": 1784.24, "text": " quite a long way forward and I suspect the next thing is going to involve them joining up again", "tokens": [50660, 1596, 257, 938, 636, 2128, 293, 286, 9091, 264, 958, 551, 307, 516, 281, 9494, 552, 5549, 493, 797, 50872], "temperature": 0.0, "avg_logprob": -0.07436238887698152, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.014058055356144905}, {"id": 271, "seek": 177408, "start": 1784.8799999999999, "end": 1791.84, "text": " because they each have good points they each have bad points and you know two wrongs don't", "tokens": [50904, 570, 436, 1184, 362, 665, 2793, 436, 1184, 362, 1578, 2793, 293, 291, 458, 732, 2085, 82, 500, 380, 51252], "temperature": 0.0, "avg_logprob": -0.07436238887698152, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.014058055356144905}, {"id": 272, "seek": 177408, "start": 1791.84, "end": 1798.48, "text": " necessarily make a right but they can make the less wrong. Nice yeah recently we heard from", "tokens": [51252, 4725, 652, 257, 558, 457, 436, 393, 652, 264, 1570, 2085, 13, 5490, 1338, 3938, 321, 2198, 490, 51584], "temperature": 0.0, "avg_logprob": -0.07436238887698152, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.014058055356144905}, {"id": 273, "seek": 179848, "start": 1798.48, "end": 1803.92, "text": " Elliot Murphy and talking about the neuro linguistics and about how the statistics of language", "tokens": [50364, 38986, 28549, 293, 1417, 466, 264, 16499, 21766, 6006, 293, 466, 577, 264, 12523, 295, 2856, 50636], "temperature": 0.0, "avg_logprob": -0.10199900596372542, "compression_ratio": 1.64453125, "no_speech_prob": 0.042700644582509995}, {"id": 274, "seek": 179848, "start": 1803.92, "end": 1809.52, "text": " are not the rules of language you can always come up with a new expression that's never been", "tokens": [50636, 366, 406, 264, 4474, 295, 2856, 291, 393, 1009, 808, 493, 365, 257, 777, 6114, 300, 311, 1128, 668, 50916], "temperature": 0.0, "avg_logprob": -0.10199900596372542, "compression_ratio": 1.64453125, "no_speech_prob": 0.042700644582509995}, {"id": 275, "seek": 179848, "start": 1809.52, "end": 1812.8, "text": " uttered that's not going to be in the training distribution or any distribution.", "tokens": [50916, 17567, 292, 300, 311, 406, 516, 281, 312, 294, 264, 3097, 7316, 420, 604, 7316, 13, 51080], "temperature": 0.0, "avg_logprob": -0.10199900596372542, "compression_ratio": 1.64453125, "no_speech_prob": 0.042700644582509995}, {"id": 276, "seek": 179848, "start": 1813.84, "end": 1818.0, "text": " Okay I'll ask a question in chat from Upcycle Club they wrote", "tokens": [51132, 1033, 286, 603, 1029, 257, 1168, 294, 5081, 490, 5858, 14796, 11288, 436, 4114, 51340], "temperature": 0.0, "avg_logprob": -0.10199900596372542, "compression_ratio": 1.64453125, "no_speech_prob": 0.042700644582509995}, {"id": 277, "seek": 179848, "start": 1819.04, "end": 1824.88, "text": " what are some of the key challenges associated with developing such Bayesian world models?", "tokens": [51392, 437, 366, 512, 295, 264, 2141, 4759, 6615, 365, 6416, 1270, 7840, 42434, 1002, 5245, 30, 51684], "temperature": 0.0, "avg_logprob": -0.10199900596372542, "compression_ratio": 1.64453125, "no_speech_prob": 0.042700644582509995}, {"id": 278, "seek": 182488, "start": 1824.88, "end": 1832.5600000000002, "text": " Hmm I think we've touched on a bunch of them the ones that are most top of mind for me right now", "tokens": [50364, 8239, 286, 519, 321, 600, 9828, 322, 257, 3840, 295, 552, 264, 2306, 300, 366, 881, 1192, 295, 1575, 337, 385, 558, 586, 50748], "temperature": 0.0, "avg_logprob": -0.11115890595971084, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0009696040651760995}, {"id": 279, "seek": 182488, "start": 1832.5600000000002, "end": 1838.5600000000002, "text": " are the structure learning thing that came up so how do we understand like when to propose new", "tokens": [50748, 366, 264, 3877, 2539, 551, 300, 1361, 493, 370, 577, 360, 321, 1223, 411, 562, 281, 17421, 777, 51048], "temperature": 0.0, "avg_logprob": -0.11115890595971084, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0009696040651760995}, {"id": 280, "seek": 182488, "start": 1838.5600000000002, "end": 1845.3600000000001, "text": " hypotheses and how to integrate those into the models and then yeah just figuring out like", "tokens": [51048, 49969, 293, 577, 281, 13365, 729, 666, 264, 5245, 293, 550, 1338, 445, 15213, 484, 411, 51388], "temperature": 0.0, "avg_logprob": -0.11115890595971084, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0009696040651760995}, {"id": 281, "seek": 182488, "start": 1846.48, "end": 1851.8400000000001, "text": " yeah I guess this like proposal and evolution process of the nodes themselves since everything", "tokens": [51444, 1338, 286, 2041, 341, 411, 11494, 293, 9303, 1399, 295, 264, 13891, 2969, 1670, 1203, 51712], "temperature": 0.0, "avg_logprob": -0.11115890595971084, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0009696040651760995}, {"id": 282, "seek": 185184, "start": 1851.84, "end": 1858.0, "text": " else like the framework like works pretty automatically and in a reasonable way thank you", "tokens": [50364, 1646, 411, 264, 8388, 411, 1985, 1238, 6772, 293, 294, 257, 10585, 636, 1309, 291, 50672], "temperature": 0.0, "avg_logprob": -0.10812173745571038, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0009688154677860439}, {"id": 283, "seek": 185184, "start": 1858.0, "end": 1865.12, "text": " Bayes thank you to the development of language models but kind of moving from this like more", "tokens": [50672, 7840, 279, 1309, 291, 281, 264, 3250, 295, 2856, 5245, 457, 733, 295, 2684, 490, 341, 411, 544, 51028], "temperature": 0.0, "avg_logprob": -0.10812173745571038, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0009688154677860439}, {"id": 284, "seek": 185184, "start": 1866.32, "end": 1872.24, "text": " discrete case into a continuous case which like more fully represents the space that", "tokens": [51088, 27706, 1389, 666, 257, 10957, 1389, 597, 411, 544, 4498, 8855, 264, 1901, 300, 51384], "temperature": 0.0, "avg_logprob": -0.10812173745571038, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0009688154677860439}, {"id": 285, "seek": 185184, "start": 1872.24, "end": 1880.56, "text": " we're learning over can be challenging. Yeah I would also say that like it's a sort of a", "tokens": [51384, 321, 434, 2539, 670, 393, 312, 7595, 13, 865, 286, 576, 611, 584, 300, 411, 309, 311, 257, 1333, 295, 257, 51800], "temperature": 0.0, "avg_logprob": -0.10812173745571038, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0009688154677860439}, {"id": 286, "seek": 188056, "start": 1880.56, "end": 1888.48, "text": " maxism that max maxism what on earth did I just say there's a there's a common saying let's go in", "tokens": [50364, 11469, 1434, 300, 11469, 11469, 1434, 437, 322, 4120, 630, 286, 445, 584, 456, 311, 257, 456, 311, 257, 2689, 1566, 718, 311, 352, 294, 50760], "temperature": 0.0, "avg_logprob": -0.10519789634866918, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.012427554465830326}, {"id": 287, "seek": 188056, "start": 1888.48, "end": 1894.08, "text": " that direction there's a common saying in this world that um that sort of no model ever survives", "tokens": [50760, 300, 3513, 456, 311, 257, 2689, 1566, 294, 341, 1002, 300, 1105, 300, 1333, 295, 572, 2316, 1562, 46231, 51040], "temperature": 0.0, "avg_logprob": -0.10519789634866918, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.012427554465830326}, {"id": 288, "seek": 188056, "start": 1894.08, "end": 1902.08, "text": " its first encounter with data um and that that becomes true here as well so there's lots of like", "tokens": [51040, 1080, 700, 8593, 365, 1412, 1105, 293, 300, 300, 3643, 2074, 510, 382, 731, 370, 456, 311, 3195, 295, 411, 51440], "temperature": 0.0, "avg_logprob": -0.10519789634866918, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.012427554465830326}, {"id": 289, "seek": 188056, "start": 1902.08, "end": 1907.44, "text": " as we've been building these things and using them we found lots of little spiky edge corners", "tokens": [51440, 382, 321, 600, 668, 2390, 613, 721, 293, 1228, 552, 321, 1352, 3195, 295, 707, 637, 1035, 88, 4691, 12413, 51708], "temperature": 0.0, "avg_logprob": -0.10519789634866918, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.012427554465830326}, {"id": 290, "seek": 190744, "start": 1907.44, "end": 1911.92, "text": " with sort of making sure that the language world is actually doing what we want it to do", "tokens": [50364, 365, 1333, 295, 1455, 988, 300, 264, 2856, 1002, 307, 767, 884, 437, 321, 528, 309, 281, 360, 50588], "temperature": 0.0, "avg_logprob": -0.10463477344047732, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.005552456248551607}, {"id": 291, "seek": 190744, "start": 1911.92, "end": 1916.4, "text": " so there are a lot of questions in building these things around how do you actually test that the", "tokens": [50588, 370, 456, 366, 257, 688, 295, 1651, 294, 2390, 613, 721, 926, 577, 360, 291, 767, 1500, 300, 264, 50812], "temperature": 0.0, "avg_logprob": -0.10463477344047732, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.005552456248551607}, {"id": 292, "seek": 190744, "start": 1917.3600000000001, "end": 1923.8400000000001, "text": " components of it are actually working the way you want and then on like a broader level how do you", "tokens": [50860, 6677, 295, 309, 366, 767, 1364, 264, 636, 291, 528, 293, 550, 322, 411, 257, 13227, 1496, 577, 360, 291, 51184], "temperature": 0.0, "avg_logprob": -0.10463477344047732, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.005552456248551607}, {"id": 293, "seek": 190744, "start": 1924.72, "end": 1933.92, "text": " compare something that is fundamentally trying to solve a different problem to other methods so", "tokens": [51228, 6794, 746, 300, 307, 17879, 1382, 281, 5039, 257, 819, 1154, 281, 661, 7150, 370, 51688], "temperature": 0.0, "avg_logprob": -0.10463477344047732, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.005552456248551607}, {"id": 294, "seek": 193392, "start": 1934.72, "end": 1939.8400000000001, "text": " we are solving a problem under the constraints that we want a fully auditable system", "tokens": [50404, 321, 366, 12606, 257, 1154, 833, 264, 18491, 300, 321, 528, 257, 4498, 2379, 16772, 1185, 50660], "temperature": 0.0, "avg_logprob": -0.06213987441289993, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.0009108848171308637}, {"id": 295, "seek": 193392, "start": 1942.24, "end": 1948.3200000000002, "text": " we could also solve all of these problems by a thing called in-context learning which is basically", "tokens": [50780, 321, 727, 611, 5039, 439, 295, 613, 2740, 538, 257, 551, 1219, 294, 12, 9000, 3828, 2539, 597, 307, 1936, 51084], "temperature": 0.0, "avg_logprob": -0.06213987441289993, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.0009108848171308637}, {"id": 296, "seek": 193392, "start": 1948.3200000000002, "end": 1954.72, "text": " putting the context into the prompt of a large language model and asking it the answer and that", "tokens": [51084, 3372, 264, 4319, 666, 264, 12391, 295, 257, 2416, 2856, 2316, 293, 3365, 309, 264, 1867, 293, 300, 51404], "temperature": 0.0, "avg_logprob": -0.06213987441289993, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.0009108848171308637}, {"id": 297, "seek": 193392, "start": 1954.72, "end": 1961.44, "text": " also works especially when you've got things like GPT-4 which are just wonders and glories", "tokens": [51404, 611, 1985, 2318, 562, 291, 600, 658, 721, 411, 26039, 51, 12, 19, 597, 366, 445, 27348, 293, 1563, 2083, 51740], "temperature": 0.0, "avg_logprob": -0.06213987441289993, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.0009108848171308637}, {"id": 298, "seek": 196144, "start": 1961.6000000000001, "end": 1967.92, "text": " um it works really well so then we come to the question of how do we actually make the case", "tokens": [50372, 1105, 309, 1985, 534, 731, 370, 550, 321, 808, 281, 264, 1168, 295, 577, 360, 321, 767, 652, 264, 1389, 50688], "temperature": 0.0, "avg_logprob": -0.08327359358469645, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.009264432825148106}, {"id": 299, "seek": 196144, "start": 1967.92, "end": 1976.3200000000002, "text": " from this from a like a bigger picture perspective can it be more than just a like can we find", "tokens": [50688, 490, 341, 490, 257, 411, 257, 3801, 3036, 4585, 393, 309, 312, 544, 813, 445, 257, 411, 393, 321, 915, 51108], "temperature": 0.0, "avg_logprob": -0.08327359358469645, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.009264432825148106}, {"id": 300, "seek": 196144, "start": 1976.3200000000002, "end": 1985.68, "text": " benchmarks that reflect um the structural advantages of this approach over something", "tokens": [51108, 43751, 300, 5031, 1105, 264, 15067, 14906, 295, 341, 3109, 670, 746, 51576], "temperature": 0.0, "avg_logprob": -0.08327359358469645, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.009264432825148106}, {"id": 301, "seek": 198568, "start": 1985.68, "end": 1994.72, "text": " like in-context learning that don't come across as false so that's kind of like a a stranger answer", "tokens": [50364, 411, 294, 12, 9000, 3828, 2539, 300, 500, 380, 808, 2108, 382, 7908, 370, 300, 311, 733, 295, 411, 257, 257, 18834, 1867, 50816], "temperature": 0.0, "avg_logprob": -0.08441450831654308, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.05338432639837265}, {"id": 302, "seek": 198568, "start": 1994.72, "end": 2000.64, "text": " because it's not really about like actually developing the world model it's about sort of", "tokens": [50816, 570, 309, 311, 406, 534, 466, 411, 767, 6416, 264, 1002, 2316, 309, 311, 466, 1333, 295, 51112], "temperature": 0.0, "avg_logprob": -0.08441450831654308, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.05338432639837265}, {"id": 303, "seek": 198568, "start": 2000.64, "end": 2008.3200000000002, "text": " convincing other people that it's a good idea um and that's you know that that is a thing that is", "tokens": [51112, 24823, 661, 561, 300, 309, 311, 257, 665, 1558, 1105, 293, 300, 311, 291, 458, 300, 300, 307, 257, 551, 300, 307, 51496], "temperature": 0.0, "avg_logprob": -0.08441450831654308, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.05338432639837265}, {"id": 304, "seek": 198568, "start": 2008.3200000000002, "end": 2015.2, "text": " true of essentially all of the things on this slide as well they are all quite complex and odd", "tokens": [51496, 2074, 295, 4476, 439, 295, 264, 721, 322, 341, 4137, 382, 731, 436, 366, 439, 1596, 3997, 293, 7401, 51840], "temperature": 0.0, "avg_logprob": -0.08441450831654308, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.05338432639837265}, {"id": 305, "seek": 201520, "start": 2015.2, "end": 2021.92, "text": " little methods um that you know there's a there's a degree to which well we definitely can solve this", "tokens": [50364, 707, 7150, 1105, 300, 291, 458, 456, 311, 257, 456, 311, 257, 4314, 281, 597, 731, 321, 2138, 393, 5039, 341, 50700], "temperature": 0.0, "avg_logprob": -0.0918398757479084, "compression_ratio": 1.7177914110429449, "no_speech_prob": 0.0014097514795139432}, {"id": 306, "seek": 201520, "start": 2021.92, "end": 2031.44, "text": " an easier way um so what is the thing that what is the the the application or the benchmark where", "tokens": [50700, 364, 3571, 636, 1105, 370, 437, 307, 264, 551, 300, 437, 307, 264, 264, 264, 3861, 420, 264, 18927, 689, 51176], "temperature": 0.0, "avg_logprob": -0.0918398757479084, "compression_ratio": 1.7177914110429449, "no_speech_prob": 0.0014097514795139432}, {"id": 307, "seek": 201520, "start": 2031.44, "end": 2037.04, "text": " we can say no if you do it the easier way you will fail at this measurable thing", "tokens": [51176, 321, 393, 584, 572, 498, 291, 360, 309, 264, 3571, 636, 291, 486, 3061, 412, 341, 43615, 551, 51456], "temperature": 0.0, "avg_logprob": -0.0918398757479084, "compression_ratio": 1.7177914110429449, "no_speech_prob": 0.0014097514795139432}, {"id": 308, "seek": 203704, "start": 2037.68, "end": 2045.36, "text": " very interesting um so you mentioned the self-evidencing", "tokens": [50396, 588, 1880, 1105, 370, 291, 2835, 264, 2698, 12, 13379, 4380, 2175, 50780], "temperature": 0.0, "avg_logprob": -0.11721337469000566, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00426363805308938}, {"id": 309, "seek": 203704, "start": 2047.36, "end": 2055.92, "text": " advantages of using world models that are self-evidencing rather than reward maximizing", "tokens": [50880, 14906, 295, 1228, 1002, 5245, 300, 366, 2698, 12, 13379, 4380, 2175, 2831, 813, 7782, 5138, 3319, 51308], "temperature": 0.0, "avg_logprob": -0.11721337469000566, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00426363805308938}, {"id": 310, "seek": 203704, "start": 2055.92, "end": 2062.24, "text": " for example so how do you see that playing out and I can connect it back to active inference of", "tokens": [51308, 337, 1365, 370, 577, 360, 291, 536, 300, 2433, 484, 293, 286, 393, 1745, 309, 646, 281, 4967, 38253, 295, 51624], "temperature": 0.0, "avg_logprob": -0.11721337469000566, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00426363805308938}, {"id": 311, "seek": 206224, "start": 2062.24, "end": 2068.3199999999997, "text": " course but how do you see this self-evidencing centrality play out in the kinds of models described", "tokens": [50364, 1164, 457, 577, 360, 291, 536, 341, 2698, 12, 13379, 4380, 2175, 32199, 1860, 862, 484, 294, 264, 3685, 295, 5245, 7619, 50668], "temperature": 0.0, "avg_logprob": -0.06376020908355713, "compression_ratio": 1.7686832740213523, "no_speech_prob": 0.002396330237388611}, {"id": 312, "seek": 206224, "start": 2068.3199999999997, "end": 2073.8399999999997, "text": " here yeah I think there are a couple reasons why it's so compelling to me uh and the first just has", "tokens": [50668, 510, 1338, 286, 519, 456, 366, 257, 1916, 4112, 983, 309, 311, 370, 20050, 281, 385, 2232, 293, 264, 700, 445, 575, 50944], "temperature": 0.0, "avg_logprob": -0.06376020908355713, "compression_ratio": 1.7686832740213523, "no_speech_prob": 0.002396330237388611}, {"id": 313, "seek": 206224, "start": 2073.8399999999997, "end": 2079.04, "text": " to do with explainability right like it's really convincing to people to say like well why why did", "tokens": [50944, 281, 360, 365, 2903, 2310, 558, 411, 309, 311, 534, 24823, 281, 561, 281, 584, 411, 731, 983, 983, 630, 51204], "temperature": 0.0, "avg_logprob": -0.06376020908355713, "compression_ratio": 1.7686832740213523, "no_speech_prob": 0.002396330237388611}, {"id": 314, "seek": 206224, "start": 2079.04, "end": 2083.8399999999997, "text": " we predict this why do we believe this well this is the actual real world data that we've observed", "tokens": [51204, 321, 6069, 341, 983, 360, 321, 1697, 341, 731, 341, 307, 264, 3539, 957, 1002, 1412, 300, 321, 600, 13095, 51444], "temperature": 0.0, "avg_logprob": -0.06376020908355713, "compression_ratio": 1.7686832740213523, "no_speech_prob": 0.002396330237388611}, {"id": 315, "seek": 206224, "start": 2083.8399999999997, "end": 2090.72, "text": " such that you know this this is the impact that that's had uh and then I think like uh I don't know", "tokens": [51444, 1270, 300, 291, 458, 341, 341, 307, 264, 2712, 300, 300, 311, 632, 2232, 293, 550, 286, 519, 411, 2232, 286, 500, 380, 458, 51788], "temperature": 0.0, "avg_logprob": -0.06376020908355713, "compression_ratio": 1.7686832740213523, "no_speech_prob": 0.002396330237388611}, {"id": 316, "seek": 209072, "start": 2090.72, "end": 2095.2, "text": " you hear a lot about like designing these really complicated reward functions which are often very", "tokens": [50364, 291, 1568, 257, 688, 466, 411, 14685, 613, 534, 6179, 7782, 6828, 597, 366, 2049, 588, 50588], "temperature": 0.0, "avg_logprob": -0.048389475300626934, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.014949687756597996}, {"id": 317, "seek": 209072, "start": 2095.2, "end": 2101.2, "text": " clever but which um often I feel like are close to being a pitfall because they very easily become", "tokens": [50588, 13494, 457, 597, 1105, 2049, 286, 841, 411, 366, 1998, 281, 885, 257, 10147, 6691, 570, 436, 588, 3612, 1813, 50888], "temperature": 0.0, "avg_logprob": -0.048389475300626934, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.014949687756597996}, {"id": 318, "seek": 209072, "start": 2101.2, "end": 2107.2, "text": " like disconnected from like the complex world that we're trying to model and so you end up in like", "tokens": [50888, 411, 29426, 490, 411, 264, 3997, 1002, 300, 321, 434, 1382, 281, 2316, 293, 370, 291, 917, 493, 294, 411, 51188], "temperature": 0.0, "avg_logprob": -0.048389475300626934, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.014949687756597996}, {"id": 319, "seek": 209072, "start": 2107.2, "end": 2114.0, "text": " weird local maximums or minimums and um yeah you start like just kind of uh solving the problem", "tokens": [51188, 3657, 2654, 6674, 82, 420, 7285, 82, 293, 1105, 1338, 291, 722, 411, 445, 733, 295, 2232, 12606, 264, 1154, 51528], "temperature": 0.0, "avg_logprob": -0.048389475300626934, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.014949687756597996}, {"id": 320, "seek": 209072, "start": 2114.0, "end": 2119.4399999999996, "text": " that you've designed versus like the problem which actually exists and so um I just have always", "tokens": [51528, 300, 291, 600, 4761, 5717, 411, 264, 1154, 597, 767, 8198, 293, 370, 1105, 286, 445, 362, 1009, 51800], "temperature": 0.0, "avg_logprob": -0.048389475300626934, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.014949687756597996}, {"id": 321, "seek": 211944, "start": 2119.44, "end": 2124.7200000000003, "text": " loved the idea that what we should be doing is um kind of self-evidencing and from an intuitive", "tokens": [50364, 4333, 264, 1558, 300, 437, 321, 820, 312, 884, 307, 1105, 733, 295, 2698, 12, 13379, 4380, 2175, 293, 490, 364, 21769, 50628], "temperature": 0.0, "avg_logprob": -0.07819975339449368, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0006461477023549378}, {"id": 322, "seek": 211944, "start": 2124.7200000000003, "end": 2131.2000000000003, "text": " sense that feels like what it feels like what an intelligence system should do uh yeah", "tokens": [50628, 2020, 300, 3417, 411, 437, 309, 3417, 411, 437, 364, 7599, 1185, 820, 360, 2232, 1338, 50952], "temperature": 0.0, "avg_logprob": -0.07819975339449368, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0006461477023549378}, {"id": 323, "seek": 211944, "start": 2133.84, "end": 2137.76, "text": " yeah I actually don't have anything interesting to add to that I just agree with Fabie", "tokens": [51084, 1338, 286, 767, 500, 380, 362, 1340, 1880, 281, 909, 281, 300, 286, 445, 3986, 365, 17440, 414, 51280], "temperature": 0.0, "avg_logprob": -0.07819975339449368, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0006461477023549378}, {"id": 324, "seek": 211944, "start": 2139.76, "end": 2145.76, "text": " that explainability and the capacity to explicitly reference previous data including", "tokens": [51380, 300, 2903, 2310, 293, 264, 6042, 281, 20803, 6408, 3894, 1412, 3009, 51680], "temperature": 0.0, "avg_logprob": -0.07819975339449368, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.0006461477023549378}, {"id": 325, "seek": 214576, "start": 2145.76, "end": 2151.36, "text": " like leave one out so techniques from non-parametric statistics about the effect of adding in another", "tokens": [50364, 411, 1856, 472, 484, 370, 7512, 490, 2107, 12, 2181, 335, 17475, 12523, 466, 264, 1802, 295, 5127, 294, 1071, 50644], "temperature": 0.0, "avg_logprob": -0.05357992308480399, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.014951126649975777}, {"id": 326, "seek": 214576, "start": 2151.36, "end": 2159.84, "text": " piece of data or removing a piece of data um and then just like to bring it to like a homeostatic", "tokens": [50644, 2522, 295, 1412, 420, 12720, 257, 2522, 295, 1412, 1105, 293, 550, 445, 411, 281, 1565, 309, 281, 411, 257, 1280, 555, 2399, 51068], "temperature": 0.0, "avg_logprob": -0.05357992308480399, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.014951126649975777}, {"id": 327, "seek": 214576, "start": 2159.84, "end": 2166.0, "text": " setting which is commonly considered an active inference like we're trying to be within a homeostatic", "tokens": [51068, 3287, 597, 307, 12719, 4888, 364, 4967, 38253, 411, 321, 434, 1382, 281, 312, 1951, 257, 1280, 555, 2399, 51376], "temperature": 0.0, "avg_logprob": -0.05357992308480399, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.014951126649975777}, {"id": 328, "seek": 216600, "start": 2166.0, "end": 2175.44, "text": " temperature range of 37 yes we could propose reward functions but as those start to include", "tokens": [50364, 4292, 3613, 295, 13435, 2086, 321, 727, 17421, 7782, 6828, 457, 382, 729, 722, 281, 4090, 50836], "temperature": 0.0, "avg_logprob": -0.07809608929777799, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.01941787451505661}, {"id": 329, "seek": 216600, "start": 2176.16, "end": 2181.28, "text": " open-endedness and exploration structure learning just like you described it Fabie like", "tokens": [50872, 1269, 12, 3502, 1287, 293, 16197, 3877, 2539, 445, 411, 291, 7619, 309, 17440, 414, 411, 51128], "temperature": 0.0, "avg_logprob": -0.07809608929777799, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.01941787451505661}, {"id": 330, "seek": 216600, "start": 2181.28, "end": 2187.44, "text": " we're solving the problem as designed rather than the actual question of the homeostatic", "tokens": [51128, 321, 434, 12606, 264, 1154, 382, 4761, 2831, 813, 264, 3539, 1168, 295, 264, 1280, 555, 2399, 51436], "temperature": 0.0, "avg_logprob": -0.07809608929777799, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.01941787451505661}, {"id": 331, "seek": 216600, "start": 2187.44, "end": 2195.36, "text": " temperature and the sort of path of least action first principles physics grounded intelligence", "tokens": [51436, 4292, 293, 264, 1333, 295, 3100, 295, 1935, 3069, 700, 9156, 10649, 23535, 7599, 51832], "temperature": 0.0, "avg_logprob": -0.07809608929777799, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.01941787451505661}, {"id": 332, "seek": 219536, "start": 2195.36, "end": 2200.56, "text": " perspective from active inference is like make it the kind of thing that measures itself at 37", "tokens": [50364, 4585, 490, 4967, 38253, 307, 411, 652, 309, 264, 733, 295, 551, 300, 8000, 2564, 412, 13435, 50624], "temperature": 0.0, "avg_logprob": -0.037243045700920956, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0024724756367504597}, {"id": 333, "seek": 219536, "start": 2201.28, "end": 2208.08, "text": " and then as long as it is it is and when it isn't it's dead and that's the kind of mortal computing", "tokens": [50660, 293, 550, 382, 938, 382, 309, 307, 309, 307, 293, 562, 309, 1943, 380, 309, 311, 3116, 293, 300, 311, 264, 733, 295, 27624, 15866, 51000], "temperature": 0.0, "avg_logprob": -0.037243045700920956, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0024724756367504597}, {"id": 334, "seek": 219536, "start": 2208.88, "end": 2215.44, "text": " crossover which is like outside of its zone of surprise it it's not just that it's getting", "tokens": [51040, 33837, 597, 307, 411, 2380, 295, 1080, 6668, 295, 6365, 309, 309, 311, 406, 445, 300, 309, 311, 1242, 51368], "temperature": 0.0, "avg_logprob": -0.037243045700920956, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0024724756367504597}, {"id": 335, "seek": 219536, "start": 2215.44, "end": 2223.44, "text": " a bad grade in the class that is like a deeper failure signal than that and to understand okay", "tokens": [51368, 257, 1578, 7204, 294, 264, 1508, 300, 307, 411, 257, 7731, 7763, 6358, 813, 300, 293, 281, 1223, 1392, 51768], "temperature": 0.0, "avg_logprob": -0.037243045700920956, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0024724756367504597}, {"id": 336, "seek": 222344, "start": 2223.44, "end": 2230.64, "text": " when is it a yellow flag when is it a red flag in terms of the new scientific literature coming in", "tokens": [50364, 562, 307, 309, 257, 5566, 7166, 562, 307, 309, 257, 2182, 7166, 294, 2115, 295, 264, 777, 8134, 10394, 1348, 294, 50724], "temperature": 0.0, "avg_logprob": -0.10128487348556518, "compression_ratio": 1.48, "no_speech_prob": 0.0015977172879502177}, {"id": 337, "seek": 222344, "start": 2231.68, "end": 2244.08, "text": " those have plain straightforward ways to interpret that developing larger higher-order", "tokens": [50776, 729, 362, 11121, 15325, 2098, 281, 7302, 300, 6416, 4833, 2946, 12, 4687, 51396], "temperature": 0.0, "avg_logprob": -0.10128487348556518, "compression_ratio": 1.48, "no_speech_prob": 0.0015977172879502177}, {"id": 338, "seek": 224408, "start": 2244.16, "end": 2249.12, "text": " apparatuses will never return to that kind of basal simplicity", "tokens": [50368, 36564, 8355, 486, 1128, 2736, 281, 300, 733, 295, 987, 304, 25632, 50616], "temperature": 0.0, "avg_logprob": -0.09151421893726695, "compression_ratio": 1.390625, "no_speech_prob": 0.05660571530461311}, {"id": 339, "seek": 224408, "start": 2251.92, "end": 2253.12, "text": " yeah couldn't agree more", "tokens": [50756, 1338, 2809, 380, 3986, 544, 50816], "temperature": 0.0, "avg_logprob": -0.09151421893726695, "compression_ratio": 1.390625, "no_speech_prob": 0.05660571530461311}, {"id": 340, "seek": 224408, "start": 2255.68, "end": 2263.12, "text": " yeah I mean absolutely it also like the other thing that it can do quite well is deal with", "tokens": [50944, 1338, 286, 914, 3122, 309, 611, 411, 264, 661, 551, 300, 309, 393, 360, 1596, 731, 307, 2028, 365, 51316], "temperature": 0.0, "avg_logprob": -0.09151421893726695, "compression_ratio": 1.390625, "no_speech_prob": 0.05660571530461311}, {"id": 341, "seek": 226312, "start": 2263.92, "end": 2274.88, "text": " essentially outlier studies so situations where you have a new piece of information that is", "tokens": [50404, 4476, 484, 2753, 5313, 370, 6851, 689, 291, 362, 257, 777, 2522, 295, 1589, 300, 307, 50952], "temperature": 0.0, "avg_logprob": -0.13517513275146484, "compression_ratio": 1.61875, "no_speech_prob": 0.03961950168013573}, {"id": 342, "seek": 226312, "start": 2275.52, "end": 2282.3199999999997, "text": " strongly conflicting with all the previous pieces of information and trying to sort of", "tokens": [50984, 10613, 43784, 365, 439, 264, 3894, 3755, 295, 1589, 293, 1382, 281, 1333, 295, 51324], "temperature": 0.0, "avg_logprob": -0.13517513275146484, "compression_ratio": 1.61875, "no_speech_prob": 0.03961950168013573}, {"id": 343, "seek": 226312, "start": 2282.3199999999997, "end": 2288.88, "text": " work through what that really means and there's a like there's a degree to which", "tokens": [51324, 589, 807, 437, 300, 534, 1355, 293, 456, 311, 257, 411, 456, 311, 257, 4314, 281, 597, 51652], "temperature": 0.0, "avg_logprob": -0.13517513275146484, "compression_ratio": 1.61875, "no_speech_prob": 0.03961950168013573}, {"id": 344, "seek": 228888, "start": 2289.36, "end": 2297.44, "text": " we can even sort of extend this process to multiple agents that have these belief systems", "tokens": [50388, 321, 393, 754, 1333, 295, 10101, 341, 1399, 281, 3866, 12554, 300, 362, 613, 7107, 3652, 50792], "temperature": 0.0, "avg_logprob": -0.16430490772898604, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.010325363837182522}, {"id": 345, "seek": 228888, "start": 2297.44, "end": 2304.2400000000002, "text": " and then look at sort of consensus of experts or weighted consensus of experts so for instance", "tokens": [50792, 293, 550, 574, 412, 1333, 295, 19115, 295, 8572, 420, 32807, 19115, 295, 8572, 370, 337, 5197, 51132], "temperature": 0.0, "avg_logprob": -0.16430490772898604, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.010325363837182522}, {"id": 346, "seek": 228888, "start": 2304.2400000000002, "end": 2311.04, "text": " you could have like a weather vane type of situation where somebody really over indexes to every new", "tokens": [51132, 291, 727, 362, 411, 257, 5503, 371, 1929, 2010, 295, 2590, 689, 2618, 534, 670, 8186, 279, 281, 633, 777, 51472], "temperature": 0.0, "avg_logprob": -0.16430490772898604, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.010325363837182522}, {"id": 347, "seek": 228888, "start": 2311.04, "end": 2316.48, "text": " piece of information and you would do that with you know technically you do that with maybe a power", "tokens": [51472, 2522, 295, 1589, 293, 291, 576, 360, 300, 365, 291, 458, 12120, 291, 360, 300, 365, 1310, 257, 1347, 51744], "temperature": 0.0, "avg_logprob": -0.16430490772898604, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.010325363837182522}, {"id": 348, "seek": 231648, "start": 2317.04, "end": 2323.04, "text": " posterior type thing or you can have somebody who's built in strong priors in a particular", "tokens": [50392, 33529, 2010, 551, 420, 291, 393, 362, 2618, 567, 311, 3094, 294, 2068, 1790, 830, 294, 257, 1729, 50692], "temperature": 0.0, "avg_logprob": -0.09448478405292217, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.005728152114897966}, {"id": 349, "seek": 231648, "start": 2323.04, "end": 2332.4, "text": " direction and you can then like take your consensus of artificial sort of decision making all of which", "tokens": [50692, 3513, 293, 291, 393, 550, 411, 747, 428, 19115, 295, 11677, 1333, 295, 3537, 1455, 439, 295, 597, 51160], "temperature": 0.0, "avg_logprob": -0.09448478405292217, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.005728152114897966}, {"id": 350, "seek": 231648, "start": 2332.4, "end": 2340.4, "text": " has within their universe well-reasoned updates to the data and then you can look and try and", "tokens": [51160, 575, 1951, 641, 6445, 731, 12, 265, 1258, 292, 9205, 281, 264, 1412, 293, 550, 291, 393, 574, 293, 853, 293, 51560], "temperature": 0.0, "avg_logprob": -0.09448478405292217, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.005728152114897966}, {"id": 351, "seek": 234040, "start": 2340.4, "end": 2350.56, "text": " work out what that swarm of experts can tell you and sort of do very empirical things like", "tokens": [50364, 589, 484, 437, 300, 49839, 295, 8572, 393, 980, 291, 293, 1333, 295, 360, 588, 31886, 721, 411, 50872], "temperature": 0.0, "avg_logprob": -0.06750952251373775, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.1207456886768341}, {"id": 352, "seek": 234040, "start": 2351.52, "end": 2357.2000000000003, "text": " try and you know work out which of these experts is doing well at a particular moment in time", "tokens": [50920, 853, 293, 291, 458, 589, 484, 597, 295, 613, 8572, 307, 884, 731, 412, 257, 1729, 1623, 294, 565, 51204], "temperature": 0.0, "avg_logprob": -0.06750952251373775, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.1207456886768341}, {"id": 353, "seek": 234040, "start": 2358.64, "end": 2364.48, "text": " because you know there could be there could be times when the world's very or the problem", "tokens": [51276, 570, 291, 458, 456, 727, 312, 456, 727, 312, 1413, 562, 264, 1002, 311, 588, 420, 264, 1154, 51568], "temperature": 0.0, "avg_logprob": -0.06750952251373775, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.1207456886768341}, {"id": 354, "seek": 236448, "start": 2364.48, "end": 2370.8, "text": " you're solving is very chaotic in which case the over indexing expert would probably be a pretty", "tokens": [50364, 291, 434, 12606, 307, 588, 27013, 294, 597, 1389, 264, 670, 8186, 278, 5844, 576, 1391, 312, 257, 1238, 50680], "temperature": 0.0, "avg_logprob": -0.08858743080726036, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.21190118789672852}, {"id": 355, "seek": 236448, "start": 2370.8, "end": 2378.4, "text": " pretty solid bet while there are other times where sort of things are pretty stable and you", "tokens": [50680, 1238, 5100, 778, 1339, 456, 366, 661, 1413, 689, 1333, 295, 721, 366, 1238, 8351, 293, 291, 51060], "temperature": 0.0, "avg_logprob": -0.08858743080726036, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.21190118789672852}, {"id": 356, "seek": 236448, "start": 2378.4, "end": 2383.52, "text": " probably it would be possible that the sort of the more conservative expert is more", "tokens": [51060, 1391, 309, 576, 312, 1944, 300, 264, 1333, 295, 264, 544, 13780, 5844, 307, 544, 51316], "temperature": 0.0, "avg_logprob": -0.08858743080726036, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.21190118789672852}, {"id": 357, "seek": 236448, "start": 2385.6, "end": 2392.32, "text": " sort of empirically making good decisions and good recommendations so there's like a lot of", "tokens": [51420, 1333, 295, 25790, 984, 1455, 665, 5327, 293, 665, 10434, 370, 456, 311, 411, 257, 688, 295, 51756], "temperature": 0.0, "avg_logprob": -0.08858743080726036, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.21190118789672852}, {"id": 358, "seek": 239232, "start": 2392.32, "end": 2401.6000000000004, "text": " ways that we can not just like incorporate these sort of homeostasis ideas but we can also change", "tokens": [50364, 2098, 300, 321, 393, 406, 445, 411, 16091, 613, 1333, 295, 1280, 555, 26632, 3487, 457, 321, 393, 611, 1319, 50828], "temperature": 0.0, "avg_logprob": -0.10813478990034624, "compression_ratio": 1.5426356589147288, "no_speech_prob": 0.010007097385823727}, {"id": 359, "seek": 239232, "start": 2401.6000000000004, "end": 2408.2400000000002, "text": " what that means for different agents and artificially like do that artificially and then combine them", "tokens": [50828, 437, 300, 1355, 337, 819, 12554, 293, 39905, 2270, 411, 360, 300, 39905, 2270, 293, 550, 10432, 552, 51160], "temperature": 0.0, "avg_logprob": -0.10813478990034624, "compression_ratio": 1.5426356589147288, "no_speech_prob": 0.010007097385823727}, {"id": 360, "seek": 240824, "start": 2408.24, "end": 2420.72, "text": " together to try and get a almost like a like a blanking on the word but you know", "tokens": [50364, 1214, 281, 853, 293, 483, 257, 1920, 411, 257, 411, 257, 8247, 278, 322, 264, 1349, 457, 291, 458, 50988], "temperature": 0.0, "avg_logprob": -0.13124635815620422, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.1274474412202835}, {"id": 361, "seek": 240824, "start": 2422.16, "end": 2428.08, "text": " a forecast under a sort of a hypothetical set of situations and we can actually sort of bring", "tokens": [51060, 257, 14330, 833, 257, 1333, 295, 257, 33053, 992, 295, 6851, 293, 321, 393, 767, 1333, 295, 1565, 51356], "temperature": 0.0, "avg_logprob": -0.13124635815620422, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.1274474412202835}, {"id": 362, "seek": 240824, "start": 2428.08, "end": 2433.3599999999997, "text": " those ideas of the world forward and see what happens when they sort of meet with actual information.", "tokens": [51356, 729, 3487, 295, 264, 1002, 2128, 293, 536, 437, 2314, 562, 436, 1333, 295, 1677, 365, 3539, 1589, 13, 51620], "temperature": 0.0, "avg_logprob": -0.13124635815620422, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.1274474412202835}, {"id": 363, "seek": 243336, "start": 2433.6800000000003, "end": 2441.76, "text": " Yeah this angle of mixture of experts as it's sometimes called more in the language model space", "tokens": [50380, 865, 341, 5802, 295, 9925, 295, 8572, 382, 309, 311, 2171, 1219, 544, 294, 264, 2856, 2316, 1901, 50784], "temperature": 0.0, "avg_logprob": -0.10754503052810142, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0046080900356173515}, {"id": 364, "seek": 243336, "start": 2441.76, "end": 2447.76, "text": " or ecosystems of shared intelligence or diverse intelligences in the active inference area like", "tokens": [50784, 420, 32647, 295, 5507, 7599, 420, 9521, 5613, 2667, 294, 264, 4967, 38253, 1859, 411, 51084], "temperature": 0.0, "avg_logprob": -0.10754503052810142, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0046080900356173515}, {"id": 365, "seek": 243336, "start": 2447.76, "end": 2456.96, "text": " that's very interesting obviously has connections back to human teams and teams of beyond humans and", "tokens": [51084, 300, 311, 588, 1880, 2745, 575, 9271, 646, 281, 1952, 5491, 293, 5491, 295, 4399, 6255, 293, 51544], "temperature": 0.0, "avg_logprob": -0.10754503052810142, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0046080900356173515}, {"id": 366, "seek": 245696, "start": 2456.96, "end": 2468.0, "text": " so on a lot of this is still text based so maybe you did or didn't mention what representation the", "tokens": [50364, 370, 322, 257, 688, 295, 341, 307, 920, 2487, 2361, 370, 1310, 291, 630, 420, 994, 380, 2152, 437, 10290, 264, 50916], "temperature": 0.0, "avg_logprob": -0.06564746584211077, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.006902735214680433}, {"id": 367, "seek": 245696, "start": 2468.0, "end": 2476.32, "text": " base graphs are but they're plain text like and there was a lot of discussion about bringing", "tokens": [50916, 3096, 24877, 366, 457, 436, 434, 11121, 2487, 411, 293, 456, 390, 257, 688, 295, 5017, 466, 5062, 51332], "temperature": 0.0, "avg_logprob": -0.06564746584211077, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.006902735214680433}, {"id": 368, "seek": 245696, "start": 2476.32, "end": 2482.7200000000003, "text": " from natural language scientific papers or however it is into a structured form and then the", "tokens": [51332, 490, 3303, 2856, 8134, 10577, 420, 4461, 309, 307, 666, 257, 18519, 1254, 293, 550, 264, 51652], "temperature": 0.0, "avg_logprob": -0.06564746584211077, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.006902735214680433}, {"id": 369, "seek": 248272, "start": 2483.2799999999997, "end": 2490.16, "text": " explain method that you showed kind of taking the structured form and just giving a little", "tokens": [50392, 2903, 3170, 300, 291, 4712, 733, 295, 1940, 264, 18519, 1254, 293, 445, 2902, 257, 707, 50736], "temperature": 0.0, "avg_logprob": -0.06087486706082783, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0028892250265926123}, {"id": 370, "seek": 248272, "start": 2490.7999999999997, "end": 2500.24, "text": " syntactic fluency so it looks human readable so how do you see that essence coming into play with", "tokens": [50768, 23980, 19892, 5029, 3020, 370, 309, 1542, 1952, 49857, 370, 577, 360, 291, 536, 300, 12801, 1348, 666, 862, 365, 51240], "temperature": 0.0, "avg_logprob": -0.06087486706082783, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0028892250265926123}, {"id": 371, "seek": 248272, "start": 2500.24, "end": 2508.72, "text": " multimodal models and then with action in the world that isn't just developing the next text", "tokens": [51240, 32972, 378, 304, 5245, 293, 550, 365, 3069, 294, 264, 1002, 300, 1943, 380, 445, 6416, 264, 958, 2487, 51664], "temperature": 0.0, "avg_logprob": -0.06087486706082783, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0028892250265926123}, {"id": 372, "seek": 250872, "start": 2508.72, "end": 2515.12, "text": " token but a robotic actuator or modifying some other control element of the world.", "tokens": [50364, 14862, 457, 257, 30468, 34964, 1639, 420, 42626, 512, 661, 1969, 4478, 295, 264, 1002, 13, 50684], "temperature": 0.0, "avg_logprob": -0.10050424715367759, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010985017288476229}, {"id": 373, "seek": 250872, "start": 2518.0, "end": 2522.72, "text": " Yeah that's a really good question. I honestly haven't thought much about multimodal stuff in", "tokens": [50828, 865, 300, 311, 257, 534, 665, 1168, 13, 286, 6095, 2378, 380, 1194, 709, 466, 32972, 378, 304, 1507, 294, 51064], "temperature": 0.0, "avg_logprob": -0.10050424715367759, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010985017288476229}, {"id": 374, "seek": 250872, "start": 2522.72, "end": 2529.7599999999998, "text": " this particular context but I think the framework is general enough at this point such that it", "tokens": [51064, 341, 1729, 4319, 457, 286, 519, 264, 8388, 307, 2674, 1547, 412, 341, 935, 1270, 300, 309, 51416], "temperature": 0.0, "avg_logprob": -0.10050424715367759, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010985017288476229}, {"id": 375, "seek": 250872, "start": 2529.7599999999998, "end": 2536.72, "text": " it's definitely could support lots of different modalities. I'd be really curious to see how", "tokens": [51416, 309, 311, 2138, 727, 1406, 3195, 295, 819, 1072, 16110, 13, 286, 1116, 312, 534, 6369, 281, 536, 577, 51764], "temperature": 0.0, "avg_logprob": -0.10050424715367759, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010985017288476229}, {"id": 376, "seek": 253672, "start": 2536.72, "end": 2545.3599999999997, "text": " this did with something like audio in particular. Yeah and then to your point about like yeah this", "tokens": [50364, 341, 630, 365, 746, 411, 6278, 294, 1729, 13, 865, 293, 550, 281, 428, 935, 466, 411, 1338, 341, 50796], "temperature": 0.0, "avg_logprob": -0.098538689799123, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.001031988300383091}, {"id": 377, "seek": 253672, "start": 2545.3599999999997, "end": 2552.3199999999997, "text": " maybe like discrete versus continuous relationship I think I think that's like part of what we're", "tokens": [50796, 1310, 411, 27706, 5717, 10957, 2480, 286, 519, 286, 519, 300, 311, 411, 644, 295, 437, 321, 434, 51144], "temperature": 0.0, "avg_logprob": -0.098538689799123, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.001031988300383091}, {"id": 378, "seek": 253672, "start": 2552.3199999999997, "end": 2557.6, "text": " learning is how to go from like these long natural text documents to a system which is", "tokens": [51144, 2539, 307, 577, 281, 352, 490, 411, 613, 938, 3303, 2487, 8512, 281, 257, 1185, 597, 307, 51408], "temperature": 0.0, "avg_logprob": -0.098538689799123, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.001031988300383091}, {"id": 379, "seek": 253672, "start": 2557.6, "end": 2563.8399999999997, "text": " appropriately discretizing our hypotheses such that we have these like meaningful explanations", "tokens": [51408, 23505, 25656, 3319, 527, 49969, 1270, 300, 321, 362, 613, 411, 10995, 28708, 51720], "temperature": 0.0, "avg_logprob": -0.098538689799123, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.001031988300383091}, {"id": 380, "seek": 256384, "start": 2563.84, "end": 2574.0, "text": " like you mentioned so I think yeah I think like continuing to develop like robust ways of", "tokens": [50364, 411, 291, 2835, 370, 286, 519, 1338, 286, 519, 411, 9289, 281, 1499, 411, 13956, 2098, 295, 50872], "temperature": 0.0, "avg_logprob": -0.09015370463276957, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.001754334312863648}, {"id": 381, "seek": 256384, "start": 2574.56, "end": 2580.88, "text": " surfacing those explanations is a big part of this as well like over time we're going to observe", "tokens": [50900, 9684, 5615, 729, 28708, 307, 257, 955, 644, 295, 341, 382, 731, 411, 670, 565, 321, 434, 516, 281, 11441, 51216], "temperature": 0.0, "avg_logprob": -0.09015370463276957, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.001754334312863648}, {"id": 382, "seek": 256384, "start": 2580.88, "end": 2586.7200000000003, "text": " lots and lots and lots of evidence how do we make sure like hypotheses don't get stale and how do we", "tokens": [51216, 3195, 293, 3195, 293, 3195, 295, 4467, 577, 360, 321, 652, 988, 411, 49969, 500, 380, 483, 342, 1220, 293, 577, 360, 321, 51508], "temperature": 0.0, "avg_logprob": -0.09015370463276957, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.001754334312863648}, {"id": 383, "seek": 256384, "start": 2586.7200000000003, "end": 2592.48, "text": " use evidence to know when they are and things like this are part of that also I don't know if", "tokens": [51508, 764, 4467, 281, 458, 562, 436, 366, 293, 721, 411, 341, 366, 644, 295, 300, 611, 286, 500, 380, 458, 498, 51796], "temperature": 0.0, "avg_logprob": -0.09015370463276957, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.001754334312863648}, {"id": 384, "seek": 259248, "start": 2592.48, "end": 2595.44, "text": " that directly answered your question but that's some of the stuff that I've been thinking about", "tokens": [50364, 300, 3838, 10103, 428, 1168, 457, 300, 311, 512, 295, 264, 1507, 300, 286, 600, 668, 1953, 466, 50512], "temperature": 0.0, "avg_logprob": -0.1350886027018229, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008557469118386507}, {"id": 385, "seek": 259248, "start": 2595.44, "end": 2608.16, "text": " related to that. So in the I mean in examples like sort of moving towards robotics and sort of tech", "tokens": [50512, 4077, 281, 300, 13, 407, 294, 264, 286, 914, 294, 5110, 411, 1333, 295, 2684, 3030, 34145, 293, 1333, 295, 7553, 51148], "temperature": 0.0, "avg_logprob": -0.1350886027018229, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008557469118386507}, {"id": 386, "seek": 259248, "start": 2608.16, "end": 2612.72, "text": " video generation and image generation other sort of audio other sort of multimodality is", "tokens": [51148, 960, 5125, 293, 3256, 5125, 661, 1333, 295, 6278, 661, 1333, 295, 32972, 378, 1860, 307, 51376], "temperature": 0.0, "avg_logprob": -0.1350886027018229, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008557469118386507}, {"id": 387, "seek": 261272, "start": 2612.9599999999996, "end": 2624.72, "text": " to be honest I think of these processes in general as enabling us like building a world", "tokens": [50376, 281, 312, 3245, 286, 519, 295, 613, 7555, 294, 2674, 382, 23148, 505, 411, 2390, 257, 1002, 50964], "temperature": 0.0, "avg_logprob": -0.14253082583027502, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.035127803683280945}, {"id": 388, "seek": 261272, "start": 2624.72, "end": 2632.64, "text": " model to enable a sort of sequential decision process so if that decision process happens to", "tokens": [50964, 2316, 281, 9528, 257, 1333, 295, 42881, 3537, 1399, 370, 498, 300, 3537, 1399, 2314, 281, 51360], "temperature": 0.0, "avg_logprob": -0.14253082583027502, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.035127803683280945}, {"id": 389, "seek": 261272, "start": 2632.64, "end": 2637.52, "text": " be should the robot turn left and that's what the sort of the decision process is it's it's", "tokens": [51360, 312, 820, 264, 7881, 1261, 1411, 293, 300, 311, 437, 264, 1333, 295, 264, 3537, 1399, 307, 309, 311, 309, 311, 51604], "temperature": 0.0, "avg_logprob": -0.14253082583027502, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.035127803683280945}, {"id": 390, "seek": 263752, "start": 2638.48, "end": 2644.08, "text": " multimodal in like a very classical sense that you can put any type of decision framework over", "tokens": [50412, 32972, 378, 304, 294, 411, 257, 588, 13735, 2020, 300, 291, 393, 829, 604, 2010, 295, 3537, 8388, 670, 50692], "temperature": 0.0, "avg_logprob": -0.058949486593182165, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.006690625566989183}, {"id": 391, "seek": 263752, "start": 2644.08, "end": 2649.92, "text": " the top but it's not sort of generatively multimodal I'm not saying write me a song that", "tokens": [50692, 264, 1192, 457, 309, 311, 406, 1333, 295, 1337, 19020, 32972, 378, 304, 286, 478, 406, 1566, 2464, 385, 257, 2153, 300, 50984], "temperature": 0.0, "avg_logprob": -0.058949486593182165, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.006690625566989183}, {"id": 392, "seek": 263752, "start": 2649.92, "end": 2656.72, "text": " sounds like Beyonce and a song that sounds like Beyonce comes out I think this this sort of this", "tokens": [50984, 3263, 411, 48416, 293, 257, 2153, 300, 3263, 411, 48416, 1487, 484, 286, 519, 341, 341, 1333, 295, 341, 51324], "temperature": 0.0, "avg_logprob": -0.058949486593182165, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.006690625566989183}, {"id": 393, "seek": 263752, "start": 2656.72, "end": 2662.08, "text": " sort of Bayesian world model layer is blocking towards that sort of thing but that's that's", "tokens": [51324, 1333, 295, 7840, 42434, 1002, 2316, 4583, 307, 17776, 3030, 300, 1333, 295, 551, 457, 300, 311, 300, 311, 51592], "temperature": 0.0, "avg_logprob": -0.058949486593182165, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.006690625566989183}, {"id": 394, "seek": 266208, "start": 2662.08, "end": 2668.64, "text": " really not sort of the aim of what we're trying to do it's also like within normal like our", "tokens": [50364, 534, 406, 1333, 295, 264, 5939, 295, 437, 321, 434, 1382, 281, 360, 309, 311, 611, 411, 1951, 2710, 411, 527, 50692], "temperature": 0.0, "avg_logprob": -0.10168961404075086, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.09396239370107651}, {"id": 395, "seek": 266208, "start": 2670.24, "end": 2676.24, "text": " almost I don't want to I don't want to say mantra or manifesto because that sounds culty", "tokens": [50772, 1920, 286, 500, 380, 528, 281, 286, 500, 380, 528, 281, 584, 32094, 420, 10067, 78, 570, 300, 3263, 2376, 88, 51072], "temperature": 0.0, "avg_logprob": -0.10168961404075086, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.09396239370107651}, {"id": 396, "seek": 266208, "start": 2676.24, "end": 2684.48, "text": " and no one wants to sound culty but like our basic aim is to always center like humans within our", "tokens": [51072, 293, 572, 472, 2738, 281, 1626, 2376, 88, 457, 411, 527, 3875, 5939, 307, 281, 1009, 3056, 411, 6255, 1951, 527, 51484], "temperature": 0.0, "avg_logprob": -0.10168961404075086, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.09396239370107651}, {"id": 397, "seek": 268448, "start": 2684.48, "end": 2696.48, "text": " process and so some of this multimodal stuff it's less clear where the human lives so for", "tokens": [50364, 1399, 293, 370, 512, 295, 341, 32972, 378, 304, 1507, 309, 311, 1570, 1850, 689, 264, 1952, 2909, 370, 337, 50964], "temperature": 0.0, "avg_logprob": -0.08324583493746245, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.09804558008909225}, {"id": 398, "seek": 268448, "start": 2696.48, "end": 2702.2400000000002, "text": " instance like a video generation type thing where does the human live so keeping it at this abstraction", "tokens": [50964, 5197, 411, 257, 960, 5125, 2010, 551, 689, 775, 264, 1952, 1621, 370, 5145, 309, 412, 341, 37765, 51252], "temperature": 0.0, "avg_logprob": -0.08324583493746245, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.09804558008909225}, {"id": 399, "seek": 268448, "start": 2702.2400000000002, "end": 2707.04, "text": " of sequential decision making then it's a decision that a human could do you know human with their", "tokens": [51252, 295, 42881, 3537, 1455, 550, 309, 311, 257, 3537, 300, 257, 1952, 727, 360, 291, 458, 1952, 365, 641, 51492], "temperature": 0.0, "avg_logprob": -0.08324583493746245, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.09804558008909225}, {"id": 400, "seek": 270704, "start": 2707.04, "end": 2714.64, "text": " thumbs could be moving a like a robot around and doing that sort of stuff but yeah it's it's", "tokens": [50364, 8838, 727, 312, 2684, 257, 411, 257, 7881, 926, 293, 884, 300, 1333, 295, 1507, 457, 1338, 309, 311, 309, 311, 50744], "temperature": 0.0, "avg_logprob": -0.08023497191342441, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.07260648906230927}, {"id": 401, "seek": 270704, "start": 2714.64, "end": 2719.2799999999997, "text": " really all about sort of controllability and auditability for us in sort of a sequential", "tokens": [50744, 534, 439, 466, 1333, 295, 45159, 2310, 293, 17748, 2310, 337, 505, 294, 1333, 295, 257, 42881, 50976], "temperature": 0.0, "avg_logprob": -0.08023497191342441, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.07260648906230927}, {"id": 402, "seek": 270704, "start": 2719.2799999999997, "end": 2726.16, "text": " decision process so to the extent that that sort of leads in its multimodal world that's", "tokens": [50976, 3537, 1399, 370, 281, 264, 8396, 300, 300, 1333, 295, 6689, 294, 1080, 32972, 378, 304, 1002, 300, 311, 51320], "temperature": 0.0, "avg_logprob": -0.08023497191342441, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.07260648906230927}, {"id": 403, "seek": 270704, "start": 2727.68, "end": 2732.48, "text": " that that's sort of part of what we're doing and like to some some versions of multimodality", "tokens": [51396, 300, 300, 311, 1333, 295, 644, 295, 437, 321, 434, 884, 293, 411, 281, 512, 512, 9606, 295, 32972, 378, 1860, 51636], "temperature": 0.0, "avg_logprob": -0.08023497191342441, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.07260648906230927}, {"id": 404, "seek": 273248, "start": 2732.56, "end": 2742.08, "text": " um is we're just not swearing in that particular space um yeah not a great answer but a long one", "tokens": [50368, 1105, 307, 321, 434, 445, 406, 2484, 1921, 294, 300, 1729, 1901, 1105, 1338, 406, 257, 869, 1867, 457, 257, 938, 472, 50844], "temperature": 0.0, "avg_logprob": -0.1398055660190867, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.0007321596494875848}, {"id": 405, "seek": 273248, "start": 2745.2, "end": 2754.8, "text": " let them distill it down later um in the um auditability area it almost falls out to me", "tokens": [51000, 718, 552, 42923, 309, 760, 1780, 1105, 294, 264, 1105, 17748, 2310, 1859, 309, 1920, 8804, 484, 281, 385, 51480], "temperature": 0.0, "avg_logprob": -0.1398055660190867, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.0007321596494875848}, {"id": 406, "seek": 273248, "start": 2754.8, "end": 2759.52, "text": " to be like a syntax of auditability in a semantics at the syntactic level just", "tokens": [51480, 281, 312, 411, 257, 28431, 295, 17748, 2310, 294, 257, 4361, 45298, 412, 264, 23980, 19892, 1496, 445, 51716], "temperature": 0.0, "avg_logprob": -0.1398055660190867, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.0007321596494875848}, {"id": 407, "seek": 275952, "start": 2760.48, "end": 2766.8, "text": " tagging or versioning when a file comes in or when a given computation is executed that is", "tokens": [50412, 6162, 3249, 420, 3037, 278, 562, 257, 3991, 1487, 294, 420, 562, 257, 2212, 24903, 307, 17577, 300, 307, 50728], "temperature": 0.0, "avg_logprob": -0.06754629729224033, "compression_ratio": 1.5976331360946745, "no_speech_prob": 0.0023965458385646343}, {"id": 408, "seek": 275952, "start": 2766.8, "end": 2774.4, "text": " basically transfer across all settings and then where I see you honing in on with with this work", "tokens": [50728, 1936, 5003, 2108, 439, 6257, 293, 550, 689, 286, 536, 291, 2157, 278, 294, 322, 365, 365, 341, 589, 51108], "temperature": 0.0, "avg_logprob": -0.06754629729224033, "compression_ratio": 1.5976331360946745, "no_speech_prob": 0.0023965458385646343}, {"id": 409, "seek": 275952, "start": 2774.4, "end": 2783.36, "text": " is kind of the semantic auditability which is actually how we compose our accounts", "tokens": [51108, 307, 733, 295, 264, 47982, 17748, 2310, 597, 307, 767, 577, 321, 35925, 527, 9402, 51556], "temperature": 0.0, "avg_logprob": -0.06754629729224033, "compression_ratio": 1.5976331360946745, "no_speech_prob": 0.0023965458385646343}, {"id": 410, "seek": 278336, "start": 2784.08, "end": 2791.28, "text": " I would have driven but I decided to walk because this happens and so bringing that", "tokens": [50400, 286, 576, 362, 9555, 457, 286, 3047, 281, 1792, 570, 341, 2314, 293, 370, 5062, 300, 50760], "temperature": 0.0, "avg_logprob": -0.10140764517862288, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.011156631633639336}, {"id": 411, "seek": 278336, "start": 2792.48, "end": 2802.8, "text": " different kind of trace to systems is gonna make it um what will it open up in science or", "tokens": [50820, 819, 733, 295, 13508, 281, 3652, 307, 799, 652, 309, 1105, 437, 486, 309, 1269, 493, 294, 3497, 420, 51336], "temperature": 0.0, "avg_logprob": -0.10140764517862288, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.011156631633639336}, {"id": 412, "seek": 278336, "start": 2802.8, "end": 2812.56, "text": " education or how do you see this sitting at a console somebody is at now and making this", "tokens": [51336, 3309, 420, 577, 360, 291, 536, 341, 3798, 412, 257, 11076, 2618, 307, 412, 586, 293, 1455, 341, 51824], "temperature": 0.0, "avg_logprob": -0.10140764517862288, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.011156631633639336}, {"id": 413, "seek": 281256, "start": 2812.56, "end": 2820.56, "text": " different like over what timeline yeah I mean it's really quite nice for storytelling because", "tokens": [50364, 819, 411, 670, 437, 12933, 1338, 286, 914, 309, 311, 534, 1596, 1481, 337, 21479, 570, 50764], "temperature": 0.0, "avg_logprob": -0.06681961417198182, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.005138836335390806}, {"id": 414, "seek": 281256, "start": 2820.56, "end": 2828.4, "text": " as you said you can say things precisely like well you know because we observed this thing", "tokens": [50764, 382, 291, 848, 291, 393, 584, 721, 13402, 411, 731, 291, 458, 570, 321, 13095, 341, 551, 51156], "temperature": 0.0, "avg_logprob": -0.06681961417198182, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.005138836335390806}, {"id": 415, "seek": 281256, "start": 2828.4, "end": 2833.44, "text": " or because if we had observed something else you know like maybe you can even make statements which", "tokens": [51156, 420, 570, 498, 321, 632, 13095, 746, 1646, 291, 458, 411, 1310, 291, 393, 754, 652, 12363, 597, 51408], "temperature": 0.0, "avg_logprob": -0.06681961417198182, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.005138836335390806}, {"id": 416, "seek": 281256, "start": 2833.44, "end": 2840.96, "text": " are um yeah conditional in that sense uh I think it does like empower whoever is sitting in front", "tokens": [51408, 366, 1105, 1338, 27708, 294, 300, 2020, 2232, 286, 519, 309, 775, 411, 11071, 11387, 307, 3798, 294, 1868, 51784], "temperature": 0.0, "avg_logprob": -0.06681961417198182, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.005138836335390806}, {"id": 417, "seek": 284096, "start": 2841.04, "end": 2846.4, "text": " of this data to feel like really sure about again like that the reasoning engine that like", "tokens": [50368, 295, 341, 1412, 281, 841, 411, 534, 988, 466, 797, 411, 300, 264, 21577, 2848, 300, 411, 50636], "temperature": 0.0, "avg_logprob": -0.07389885670429952, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.021605242043733597}, {"id": 418, "seek": 284096, "start": 2846.96, "end": 2851.84, "text": " that went on uh which to me is is pretty different from what it feels like to sit in", "tokens": [50664, 300, 1437, 322, 2232, 597, 281, 385, 307, 307, 1238, 819, 490, 437, 309, 3417, 411, 281, 1394, 294, 50908], "temperature": 0.0, "avg_logprob": -0.07389885670429952, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.021605242043733597}, {"id": 419, "seek": 284096, "start": 2851.84, "end": 2857.04, "text": " front of chat gbt even though it's quite useful often um you know you you try the code and it", "tokens": [50908, 1868, 295, 5081, 290, 4517, 754, 1673, 309, 311, 1596, 4420, 2049, 1105, 291, 458, 291, 291, 853, 264, 3089, 293, 309, 51168], "temperature": 0.0, "avg_logprob": -0.07389885670429952, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.021605242043733597}, {"id": 420, "seek": 284096, "start": 2857.04, "end": 2862.7200000000003, "text": " works or doesn't work or you like you know ask your friend is this really true um and that feels", "tokens": [51168, 1985, 420, 1177, 380, 589, 420, 291, 411, 291, 458, 1029, 428, 1277, 307, 341, 534, 2074, 1105, 293, 300, 3417, 51452], "temperature": 0.0, "avg_logprob": -0.07389885670429952, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.021605242043733597}, {"id": 421, "seek": 284096, "start": 2862.7200000000003, "end": 2868.48, "text": " pretty different to me from being able to to look at the evidences themselves and say like oh well", "tokens": [51452, 1238, 819, 281, 385, 490, 885, 1075, 281, 281, 574, 412, 264, 1073, 41298, 2969, 293, 584, 411, 1954, 731, 51740], "temperature": 0.0, "avg_logprob": -0.07389885670429952, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.021605242043733597}, {"id": 422, "seek": 286848, "start": 2868.48, "end": 2873.44, "text": " actually if this is the reason you think that I know that that evidence is is not true or you", "tokens": [50364, 767, 498, 341, 307, 264, 1778, 291, 519, 300, 286, 458, 300, 300, 4467, 307, 307, 406, 2074, 420, 291, 50612], "temperature": 0.0, "avg_logprob": -0.0591061142053497, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0015974945854395628}, {"id": 423, "seek": 286848, "start": 2873.44, "end": 2879.84, "text": " know like you can bring your own human intuition or world model uh in terms of validating or um", "tokens": [50612, 458, 411, 291, 393, 1565, 428, 1065, 1952, 24002, 420, 1002, 2316, 2232, 294, 2115, 295, 7363, 990, 420, 1105, 50932], "temperature": 0.0, "avg_logprob": -0.0591061142053497, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0015974945854395628}, {"id": 424, "seek": 286848, "start": 2880.4, "end": 2886.0, "text": " yeah super imposing what you believe on top of what this system believes and so that makes it", "tokens": [50960, 1338, 1687, 40288, 437, 291, 1697, 322, 1192, 295, 437, 341, 1185, 12307, 293, 370, 300, 1669, 309, 51240], "temperature": 0.0, "avg_logprob": -0.0591061142053497, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0015974945854395628}, {"id": 425, "seek": 286848, "start": 2886.0, "end": 2894.16, "text": " really easy to make decisions um quickly I think there's also sort of a converse of this which is", "tokens": [51240, 534, 1858, 281, 652, 5327, 1105, 2661, 286, 519, 456, 311, 611, 1333, 295, 257, 416, 4308, 295, 341, 597, 307, 51648], "temperature": 0.0, "avg_logprob": -0.0591061142053497, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0015974945854395628}, {"id": 426, "seek": 289416, "start": 2894.24, "end": 2901.12, "text": " that it also makes it clear which evidence was not used to make a decision uh and that can be quite", "tokens": [50368, 300, 309, 611, 1669, 309, 1850, 597, 4467, 390, 406, 1143, 281, 652, 257, 3537, 2232, 293, 300, 393, 312, 1596, 50712], "temperature": 0.0, "avg_logprob": -0.06078311891266794, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.04811277613043785}, {"id": 427, "seek": 289416, "start": 2901.12, "end": 2907.8399999999997, "text": " telling in these situations where you could be worried that a particular type of evidence isn't", "tokens": [50712, 3585, 294, 613, 6851, 689, 291, 727, 312, 5804, 300, 257, 1729, 2010, 295, 4467, 1943, 380, 51048], "temperature": 0.0, "avg_logprob": -0.06078311891266794, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.04811277613043785}, {"id": 428, "seek": 289416, "start": 2907.8399999999997, "end": 2915.3599999999997, "text": " being weighted correctly or isn't being um sort of formatted correctly so again like if this is a", "tokens": [51048, 885, 32807, 8944, 420, 1943, 380, 885, 1105, 1333, 295, 1254, 32509, 8944, 370, 797, 411, 498, 341, 307, 257, 51424], "temperature": 0.0, "avg_logprob": -0.06078311891266794, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.04811277613043785}, {"id": 429, "seek": 291536, "start": 2916.0, "end": 2926.08, "text": " sort of a like a system that builds an assistant um that sort of does surfaces all this information", "tokens": [50396, 1333, 295, 257, 411, 257, 1185, 300, 15182, 364, 10994, 1105, 300, 1333, 295, 775, 16130, 439, 341, 1589, 50900], "temperature": 0.0, "avg_logprob": -0.06143325826396113, "compression_ratio": 2.0572916666666665, "no_speech_prob": 0.06369531154632568}, {"id": 430, "seek": 291536, "start": 2926.08, "end": 2932.08, "text": " and sort of makes a recommendation with reasoning for a person that person can then look and be like", "tokens": [50900, 293, 1333, 295, 1669, 257, 11879, 365, 21577, 337, 257, 954, 300, 954, 393, 550, 574, 293, 312, 411, 51200], "temperature": 0.0, "avg_logprob": -0.06143325826396113, "compression_ratio": 2.0572916666666665, "no_speech_prob": 0.06369531154632568}, {"id": 431, "seek": 291536, "start": 2932.08, "end": 2937.44, "text": " and they know what the data is they can look at the deck and say you know why didn't you consider", "tokens": [51200, 293, 436, 458, 437, 264, 1412, 307, 436, 393, 574, 412, 264, 9341, 293, 584, 291, 458, 983, 994, 380, 291, 1949, 51468], "temperature": 0.0, "avg_logprob": -0.06143325826396113, "compression_ratio": 2.0572916666666665, "no_speech_prob": 0.06369531154632568}, {"id": 432, "seek": 291536, "start": 2937.44, "end": 2943.6800000000003, "text": " the make of the car or why didn't you consider this or why didn't you consider that and they can", "tokens": [51468, 264, 652, 295, 264, 1032, 420, 983, 994, 380, 291, 1949, 341, 420, 983, 994, 380, 291, 1949, 300, 293, 436, 393, 51780], "temperature": 0.0, "avg_logprob": -0.06143325826396113, "compression_ratio": 2.0572916666666665, "no_speech_prob": 0.06369531154632568}, {"id": 433, "seek": 294368, "start": 2943.68, "end": 2951.44, "text": " then use their understanding of what's not being prominently used by the model to", "tokens": [50364, 550, 764, 641, 3701, 295, 437, 311, 406, 885, 39225, 2276, 1143, 538, 264, 2316, 281, 50752], "temperature": 0.0, "avg_logprob": -0.06580066680908203, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.007008441258221865}, {"id": 434, "seek": 294368, "start": 2952.72, "end": 2959.6, "text": " sort of sense test like it's it's sort of I mean in some sense that usage of it is a reformulation", "tokens": [50816, 1333, 295, 2020, 1500, 411, 309, 311, 309, 311, 1333, 295, 286, 914, 294, 512, 2020, 300, 14924, 295, 309, 307, 257, 8290, 2776, 51160], "temperature": 0.0, "avg_logprob": -0.06580066680908203, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.007008441258221865}, {"id": 435, "seek": 294368, "start": 2959.6, "end": 2964.08, "text": " of what Phoebe just said where you use your internal world model but it's like I think it's", "tokens": [51160, 295, 437, 14936, 48593, 445, 848, 689, 291, 764, 428, 6920, 1002, 2316, 457, 309, 311, 411, 286, 519, 309, 311, 51384], "temperature": 0.0, "avg_logprob": -0.06580066680908203, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.007008441258221865}, {"id": 436, "seek": 296408, "start": 2964.24, "end": 2974.56, "text": " important to know when evidence is being used and this is like I think you simply cannot get", "tokens": [50372, 1021, 281, 458, 562, 4467, 307, 885, 1143, 293, 341, 307, 411, 286, 519, 291, 2935, 2644, 483, 50888], "temperature": 0.0, "avg_logprob": -0.08638947757322397, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0460217259824276}, {"id": 437, "seek": 296408, "start": 2974.56, "end": 2981.44, "text": " um from from like a GPT type thing or any sort of like prompting type method we know for instance", "tokens": [50888, 1105, 490, 490, 411, 257, 26039, 51, 2010, 551, 420, 604, 1333, 295, 411, 12391, 278, 2010, 3170, 321, 458, 337, 5197, 51232], "temperature": 0.0, "avg_logprob": -0.08638947757322397, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0460217259824276}, {"id": 438, "seek": 296408, "start": 2981.44, "end": 2988.72, "text": " that like um the order of the order that you submit your evidence in is probably going to matter", "tokens": [51232, 300, 411, 1105, 264, 1668, 295, 264, 1668, 300, 291, 10315, 428, 4467, 294, 307, 1391, 516, 281, 1871, 51596], "temperature": 0.0, "avg_logprob": -0.08638947757322397, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0460217259824276}, {"id": 439, "seek": 298872, "start": 2988.72, "end": 2995.2, "text": " for a prompting based method okay that's obviously not true for a Bayesian update where we have this", "tokens": [50364, 337, 257, 12391, 278, 2361, 3170, 1392, 300, 311, 2745, 406, 2074, 337, 257, 7840, 42434, 5623, 689, 321, 362, 341, 50688], "temperature": 0.0, "avg_logprob": -0.06826576861468228, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.012235652655363083}, {"id": 440, "seek": 298872, "start": 2995.2, "end": 3000.72, "text": " sort of this this coherence principle where if you shuffle your data and enter it in a different", "tokens": [50688, 1333, 295, 341, 341, 26528, 655, 8665, 689, 498, 291, 39426, 428, 1412, 293, 3242, 309, 294, 257, 819, 50964], "temperature": 0.0, "avg_logprob": -0.06826576861468228, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.012235652655363083}, {"id": 441, "seek": 298872, "start": 3000.72, "end": 3008.24, "text": " way you will get the same posterior up to computational artifacts um so so all of that", "tokens": [50964, 636, 291, 486, 483, 264, 912, 33529, 493, 281, 28270, 24617, 1105, 370, 370, 439, 295, 300, 51340], "temperature": 0.0, "avg_logprob": -0.06826576861468228, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.012235652655363083}, {"id": 442, "seek": 298872, "start": 3008.24, "end": 3013.9199999999996, "text": " is like in my mind is just as important to order ability as the ability to write a report that says", "tokens": [51340, 307, 411, 294, 452, 1575, 307, 445, 382, 1021, 281, 1668, 3485, 382, 264, 3485, 281, 2464, 257, 2275, 300, 1619, 51624], "temperature": 0.0, "avg_logprob": -0.06826576861468228, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.012235652655363083}, {"id": 443, "seek": 301392, "start": 3013.92, "end": 3023.6, "text": " I made this decision for these reasons yeah well that makes me think about this kind of view from", "tokens": [50364, 286, 1027, 341, 3537, 337, 613, 4112, 1338, 731, 300, 1669, 385, 519, 466, 341, 733, 295, 1910, 490, 50848], "temperature": 0.0, "avg_logprob": -0.07364282608032227, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00172715715598315}, {"id": 444, "seek": 301392, "start": 3023.6, "end": 3028.56, "text": " the inside interpretability where the rules help and also knowing what evidence is not used is", "tokens": [50848, 264, 1854, 7302, 2310, 689, 264, 4474, 854, 293, 611, 5276, 437, 4467, 307, 406, 1143, 307, 51096], "temperature": 0.0, "avg_logprob": -0.07364282608032227, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00172715715598315}, {"id": 445, "seek": 301392, "start": 3028.56, "end": 3034.32, "text": " importance for compliance and knowing what information like in a healthcare setting was or wasn't", "tokens": [51096, 7379, 337, 15882, 293, 5276, 437, 1589, 411, 294, 257, 8884, 3287, 390, 420, 2067, 380, 51384], "temperature": 0.0, "avg_logprob": -0.07364282608032227, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.00172715715598315}, {"id": 446, "seek": 303432, "start": 3034.32, "end": 3047.28, "text": " used um what about thermodynamics we heard about free energy boltzmann came up how do you see the", "tokens": [50364, 1143, 1105, 437, 466, 8810, 35483, 321, 2198, 466, 1737, 2281, 13436, 89, 14912, 1361, 493, 577, 360, 291, 536, 264, 51012], "temperature": 0.0, "avg_logprob": -0.1342245101928711, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.28446686267852783}, {"id": 447, "seek": 303432, "start": 3048.0, "end": 3055.84, "text": " info thermo nexus what have we learned from the last hundred years of thermodynamics and", "tokens": [51048, 13614, 8810, 78, 408, 32618, 437, 362, 321, 3264, 490, 264, 1036, 3262, 924, 295, 8810, 35483, 293, 51440], "temperature": 0.0, "avg_logprob": -0.1342245101928711, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.28446686267852783}, {"id": 448, "seek": 305584, "start": 3055.84, "end": 3066.08, "text": " information theory and all of this and on the software or hardware side how is that kind of a", "tokens": [50364, 1589, 5261, 293, 439, 295, 341, 293, 322, 264, 4722, 420, 8837, 1252, 577, 307, 300, 733, 295, 257, 50876], "temperature": 0.0, "avg_logprob": -0.08486800193786621, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.01150666642934084}, {"id": 449, "seek": 305584, "start": 3066.08, "end": 3077.2000000000003, "text": " free energy nexus being used yeah i mean i i'm really excited about how all of this seems to be coming", "tokens": [50876, 1737, 2281, 408, 32618, 885, 1143, 1338, 741, 914, 741, 741, 478, 534, 2919, 466, 577, 439, 295, 341, 2544, 281, 312, 1348, 51432], "temperature": 0.0, "avg_logprob": -0.08486800193786621, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.01150666642934084}, {"id": 450, "seek": 305584, "start": 3077.2000000000003, "end": 3083.52, "text": " together um i the free energy just keeps showing up in all of these exciting areas to me we have", "tokens": [51432, 1214, 1105, 741, 264, 1737, 2281, 445, 5965, 4099, 493, 294, 439, 295, 613, 4670, 3179, 281, 385, 321, 362, 51748], "temperature": 0.0, "avg_logprob": -0.08486800193786621, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.01150666642934084}, {"id": 451, "seek": 308352, "start": 3083.52, "end": 3087.84, "text": " like a book club for singular learning theory and like they talk all about free energy too and i", "tokens": [50364, 411, 257, 1446, 6482, 337, 20010, 2539, 5261, 293, 411, 436, 751, 439, 466, 1737, 2281, 886, 293, 741, 50580], "temperature": 0.0, "avg_logprob": -0.05800249769880965, "compression_ratio": 1.849624060150376, "no_speech_prob": 0.012620653957128525}, {"id": 452, "seek": 308352, "start": 3087.84, "end": 3093.68, "text": " think some of those ideas are really exciting um i mean at normal i think like the thing that i would", "tokens": [50580, 519, 512, 295, 729, 3487, 366, 534, 4670, 1105, 741, 914, 412, 2710, 741, 519, 411, 264, 551, 300, 741, 576, 50872], "temperature": 0.0, "avg_logprob": -0.05800249769880965, "compression_ratio": 1.849624060150376, "no_speech_prob": 0.012620653957128525}, {"id": 453, "seek": 308352, "start": 3093.68, "end": 3099.28, "text": " highlight is like this idea of software hardware co-design um which is really special uh and so", "tokens": [50872, 5078, 307, 411, 341, 1558, 295, 4722, 8837, 598, 12, 14792, 788, 1105, 597, 307, 534, 2121, 2232, 293, 370, 51152], "temperature": 0.0, "avg_logprob": -0.05800249769880965, "compression_ratio": 1.849624060150376, "no_speech_prob": 0.012620653957128525}, {"id": 454, "seek": 308352, "start": 3099.28, "end": 3104.32, "text": " we're trying to do this hard and fun dance towards each other where we're like thinking about these", "tokens": [51152, 321, 434, 1382, 281, 360, 341, 1152, 293, 1019, 4489, 3030, 1184, 661, 689, 321, 434, 411, 1953, 466, 613, 51404], "temperature": 0.0, "avg_logprob": -0.05800249769880965, "compression_ratio": 1.849624060150376, "no_speech_prob": 0.012620653957128525}, {"id": 455, "seek": 308352, "start": 3104.32, "end": 3111.2, "text": " new kinds of systems and how they might support each other and um empower each other and and yeah", "tokens": [51404, 777, 3685, 295, 3652, 293, 577, 436, 1062, 1406, 1184, 661, 293, 1105, 11071, 1184, 661, 293, 293, 1338, 51748], "temperature": 0.0, "avg_logprob": -0.05800249769880965, "compression_ratio": 1.849624060150376, "no_speech_prob": 0.012620653957128525}, {"id": 456, "seek": 311120, "start": 3111.2, "end": 3119.12, "text": " how to build full stack systems um which is really challenging and and also really exciting um yeah", "tokens": [50364, 577, 281, 1322, 1577, 8630, 3652, 1105, 597, 307, 534, 7595, 293, 293, 611, 534, 4670, 1105, 1338, 50760], "temperature": 0.0, "avg_logprob": -0.09424358816707835, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0049039749428629875}, {"id": 457, "seek": 311120, "start": 3119.12, "end": 3124.24, "text": " and i think like from like the first principles of thermodynamics perspective like like we're all", "tokens": [50760, 293, 741, 519, 411, 490, 411, 264, 700, 9156, 295, 8810, 35483, 4585, 411, 411, 321, 434, 439, 51016], "temperature": 0.0, "avg_logprob": -0.09424358816707835, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0049039749428629875}, {"id": 458, "seek": 311120, "start": 3124.24, "end": 3131.2, "text": " just uh yeah we're all kind of like mathematics and physics people at heart so like going taking", "tokens": [51016, 445, 2232, 1338, 321, 434, 439, 733, 295, 411, 18666, 293, 10649, 561, 412, 1917, 370, 411, 516, 1940, 51364], "temperature": 0.0, "avg_logprob": -0.09424358816707835, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0049039749428629875}, {"id": 459, "seek": 311120, "start": 3131.2, "end": 3136.72, "text": " like uh you know all of what people have learned in language modeling and all of that like um very", "tokens": [51364, 411, 2232, 291, 458, 439, 295, 437, 561, 362, 3264, 294, 2856, 15983, 293, 439, 295, 300, 411, 1105, 588, 51640], "temperature": 0.0, "avg_logprob": -0.09424358816707835, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0049039749428629875}, {"id": 460, "seek": 313672, "start": 3136.72, "end": 3141.4399999999996, "text": " much to heart as well like i think approaching whatever problem that we're facing from a first", "tokens": [50364, 709, 281, 1917, 382, 731, 411, 741, 519, 14908, 2035, 1154, 300, 321, 434, 7170, 490, 257, 700, 50600], "temperature": 0.0, "avg_logprob": -0.09898729035348604, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0034824893809854984}, {"id": 461, "seek": 313672, "start": 3141.4399999999996, "end": 3146.08, "text": " principles how do physical systems work in the world what do we really what are the assumptions", "tokens": [50600, 9156, 577, 360, 4001, 3652, 589, 294, 264, 1002, 437, 360, 321, 534, 437, 366, 264, 17695, 50832], "temperature": 0.0, "avg_logprob": -0.09898729035348604, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0034824893809854984}, {"id": 462, "seek": 313672, "start": 3146.08, "end": 3152.16, "text": " we're really comfortable with uh and building up from there um is definitely our our natural mode", "tokens": [50832, 321, 434, 534, 4619, 365, 2232, 293, 2390, 493, 490, 456, 1105, 307, 2138, 527, 527, 3303, 4391, 51136], "temperature": 0.0, "avg_logprob": -0.09898729035348604, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0034824893809854984}, {"id": 463, "seek": 313672, "start": 3153.04, "end": 3156.08, "text": " uh so i think that makes it easier to to start working together also", "tokens": [51180, 2232, 370, 741, 519, 300, 1669, 309, 3571, 281, 281, 722, 1364, 1214, 611, 51332], "temperature": 0.0, "avg_logprob": -0.09898729035348604, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0034824893809854984}, {"id": 464, "seek": 313672, "start": 3158.08, "end": 3165.2, "text": " um it's also probably worth saying that we have a sort of a secondary not secondary a very different", "tokens": [51432, 1105, 309, 311, 611, 1391, 3163, 1566, 300, 321, 362, 257, 1333, 295, 257, 11396, 406, 11396, 257, 588, 819, 51788], "temperature": 0.0, "avg_logprob": -0.09898729035348604, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0034824893809854984}, {"id": 465, "seek": 316520, "start": 3165.2, "end": 3170.7999999999997, "text": " stream of interest in thermodynamics as well which is the ways that we can use actual physical", "tokens": [50364, 4309, 295, 1179, 294, 8810, 35483, 382, 731, 597, 307, 264, 2098, 300, 321, 393, 764, 3539, 4001, 50644], "temperature": 0.0, "avg_logprob": -0.08091653779495595, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0229648444801569}, {"id": 466, "seek": 316520, "start": 3170.7999999999997, "end": 3178.24, "text": " thermo dynamical principles to build hardware that is specifically has sort of noise in it as a", "tokens": [50644, 8810, 78, 5999, 804, 9156, 281, 1322, 8837, 300, 307, 4682, 575, 1333, 295, 5658, 294, 309, 382, 257, 51016], "temperature": 0.0, "avg_logprob": -0.08091653779495595, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0229648444801569}, {"id": 467, "seek": 316520, "start": 3178.24, "end": 3185.04, "text": " first class citizen and because of that it is particularly well suited to probabilistic tasks", "tokens": [51016, 700, 1508, 13326, 293, 570, 295, 300, 309, 307, 4098, 731, 24736, 281, 31959, 3142, 9608, 51356], "temperature": 0.0, "avg_logprob": -0.08091653779495595, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0229648444801569}, {"id": 468, "seek": 316520, "start": 3185.8399999999997, "end": 3192.72, "text": " um and so we've we've built if you if anyone wants to look we have a blog i believe the URLs", "tokens": [51396, 1105, 293, 370, 321, 600, 321, 600, 3094, 498, 291, 498, 2878, 2738, 281, 574, 321, 362, 257, 6968, 741, 1697, 264, 43267, 51740], "temperature": 0.0, "avg_logprob": -0.08091653779495595, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0229648444801569}, {"id": 469, "seek": 319272, "start": 3192.72, "end": 3200.24, "text": " blog dot normal computing dot ar um and amongst other things that are on it uh there is the very", "tokens": [50364, 6968, 5893, 2710, 15866, 5893, 594, 1105, 293, 12918, 661, 721, 300, 366, 322, 309, 2232, 456, 307, 264, 588, 50740], "temperature": 0.0, "avg_logprob": -0.09325838088989258, "compression_ratio": 1.7660550458715596, "no_speech_prob": 0.010814891196787357}, {"id": 470, "seek": 319272, "start": 3200.24, "end": 3209.2, "text": " first demonstration of using physical thermodynamic hardware to actually do computations um is the", "tokens": [50740, 700, 16520, 295, 1228, 4001, 8810, 34988, 8837, 281, 767, 360, 2807, 763, 1105, 307, 264, 51188], "temperature": 0.0, "avg_logprob": -0.09325838088989258, "compression_ratio": 1.7660550458715596, "no_speech_prob": 0.010814891196787357}, {"id": 471, "seek": 319272, "start": 3209.2, "end": 3213.68, "text": " computation the most vital computation that we will ever do it's inverting an eight by eight", "tokens": [51188, 24903, 264, 881, 11707, 24903, 300, 321, 486, 1562, 360, 309, 311, 28653, 783, 364, 3180, 538, 3180, 51412], "temperature": 0.0, "avg_logprob": -0.09325838088989258, "compression_ratio": 1.7660550458715596, "no_speech_prob": 0.010814891196787357}, {"id": 472, "seek": 319272, "start": 3213.68, "end": 3221.3599999999997, "text": " matrix so no we can do that otherwise um but it it is sort of building up towards this idea that", "tokens": [51412, 8141, 370, 572, 321, 393, 360, 300, 5911, 1105, 457, 309, 309, 307, 1333, 295, 2390, 493, 3030, 341, 1558, 300, 51796], "temperature": 0.0, "avg_logprob": -0.09325838088989258, "compression_ratio": 1.7660550458715596, "no_speech_prob": 0.010814891196787357}, {"id": 473, "seek": 322136, "start": 3221.36, "end": 3227.04, "text": " we can use thermodynamics not just in our modeling and our understanding of the world but also in our", "tokens": [50364, 321, 393, 764, 8810, 35483, 406, 445, 294, 527, 15983, 293, 527, 3701, 295, 264, 1002, 457, 611, 294, 527, 50648], "temperature": 0.0, "avg_logprob": -0.09103540813221651, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00047266704495996237}, {"id": 474, "seek": 322136, "start": 3227.04, "end": 3237.84, "text": " sort of low energy compute stack to actually realize these things um so i think i it's i think", "tokens": [50648, 1333, 295, 2295, 2281, 14722, 8630, 281, 767, 4325, 613, 721, 1105, 370, 741, 519, 741, 309, 311, 741, 519, 51188], "temperature": 0.0, "avg_logprob": -0.09103540813221651, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00047266704495996237}, {"id": 475, "seek": 322136, "start": 3237.84, "end": 3246.56, "text": " it would be challenging to find a group of people on this earth who have more investment in thermodynamics", "tokens": [51188, 309, 576, 312, 7595, 281, 915, 257, 1594, 295, 561, 322, 341, 4120, 567, 362, 544, 6078, 294, 8810, 35483, 51624], "temperature": 0.0, "avg_logprob": -0.09103540813221651, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00047266704495996237}, {"id": 476, "seek": 324656, "start": 3246.56, "end": 3255.68, "text": " and don't work in a physics department uh because we have investment all the way through from", "tokens": [50364, 293, 500, 380, 589, 294, 257, 10649, 5882, 2232, 570, 321, 362, 6078, 439, 264, 636, 807, 490, 50820], "temperature": 0.0, "avg_logprob": -0.07745700723984662, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.009557453915476799}, {"id": 477, "seek": 324656, "start": 3255.68, "end": 3261.84, "text": " sort of active inference type things all the way down to this like this this hot thing goes there", "tokens": [50820, 1333, 295, 4967, 38253, 2010, 721, 439, 264, 636, 760, 281, 341, 411, 341, 341, 2368, 551, 1709, 456, 51128], "temperature": 0.0, "avg_logprob": -0.07745700723984662, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.009557453915476799}, {"id": 478, "seek": 324656, "start": 3263.92, "end": 3269.12, "text": " which is kind of cool i'm not a physicist so i have but but that's my that's my understanding", "tokens": [51232, 597, 307, 733, 295, 1627, 741, 478, 406, 257, 42466, 370, 741, 362, 457, 457, 300, 311, 452, 300, 311, 452, 3701, 51492], "temperature": 0.0, "avg_logprob": -0.07745700723984662, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.009557453915476799}, {"id": 479, "seek": 324656, "start": 3269.12, "end": 3275.52, "text": " of thermodynamics this hot thing goes there informative thing goes here hot thing over there", "tokens": [51492, 295, 8810, 35483, 341, 2368, 551, 1709, 456, 27759, 551, 1709, 510, 2368, 551, 670, 456, 51812], "temperature": 0.0, "avg_logprob": -0.07745700723984662, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.009557453915476799}, {"id": 480, "seek": 327552, "start": 3276.24, "end": 3289.28, "text": " call it a day um yep that it's a really cool fusion with the kind of parsimony and elegance", "tokens": [50400, 818, 309, 257, 786, 1105, 18633, 300, 309, 311, 257, 534, 1627, 23100, 365, 264, 733, 295, 21156, 332, 2526, 293, 14459, 719, 51052], "temperature": 0.0, "avg_logprob": -0.06099943238861707, "compression_ratio": 1.5118110236220472, "no_speech_prob": 0.0026723972987383604}, {"id": 481, "seek": 327552, "start": 3289.28, "end": 3298.24, "text": " and the aesthetic of math and physics and first principles and the different parsimony of pragmatism", "tokens": [51052, 293, 264, 20092, 295, 5221, 293, 10649, 293, 700, 9156, 293, 264, 819, 21156, 332, 2526, 295, 33394, 15677, 1434, 51500], "temperature": 0.0, "avg_logprob": -0.06099943238861707, "compression_ratio": 1.5118110236220472, "no_speech_prob": 0.0026723972987383604}, {"id": 482, "seek": 329824, "start": 3298.7999999999997, "end": 3306.56, "text": " with the actual material basis like of a synapse the size of the synapse and the kind", "tokens": [50392, 365, 264, 3539, 2527, 5143, 411, 295, 257, 5451, 11145, 264, 2744, 295, 264, 5451, 11145, 293, 264, 733, 50780], "temperature": 0.0, "avg_logprob": -0.07549870715421789, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.017172202467918396}, {"id": 483, "seek": 329824, "start": 3306.56, "end": 3317.2, "text": " of stochasticity that that size alone um entails with like membranes and all of this those stochastic", "tokens": [50780, 295, 342, 8997, 2750, 507, 300, 300, 2744, 3312, 1105, 50133, 365, 411, 15595, 12779, 293, 439, 295, 341, 729, 342, 8997, 2750, 51312], "temperature": 0.0, "avg_logprob": -0.07549870715421789, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.017172202467918396}, {"id": 484, "seek": 329824, "start": 3317.2, "end": 3326.3199999999997, "text": " aspects are leveraged for the compute the synapse is not simply a variance reducing machine and so", "tokens": [51312, 7270, 366, 12451, 2980, 337, 264, 14722, 264, 5451, 11145, 307, 406, 2935, 257, 21977, 12245, 3479, 293, 370, 51768], "temperature": 0.0, "avg_logprob": -0.07549870715421789, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.017172202467918396}, {"id": 485, "seek": 332824, "start": 3329.12, "end": 3339.6, "text": " it's like both the platonic slash mathematical ish spirit it finds a common home in these real", "tokens": [50408, 309, 311, 411, 1293, 264, 3403, 11630, 17330, 18894, 307, 71, 3797, 309, 10704, 257, 2689, 1280, 294, 613, 957, 50932], "temperature": 0.0, "avg_logprob": -0.13618702112242234, "compression_ratio": 1.4566929133858268, "no_speech_prob": 0.0005192827666178346}, {"id": 486, "seek": 332824, "start": 3340.4799999999996, "end": 3349.6, "text": " simple physical demonstrations and um today it feels like there's a big gap between the um", "tokens": [50976, 2199, 4001, 34714, 293, 1105, 965, 309, 3417, 411, 456, 311, 257, 955, 7417, 1296, 264, 1105, 51432], "temperature": 0.0, "avg_logprob": -0.13618702112242234, "compression_ratio": 1.4566929133858268, "no_speech_prob": 0.0005192827666178346}, {"id": 487, "seek": 334960, "start": 3350.3199999999997, "end": 3358.3199999999997, "text": " mesoscale computational architectures that you described today that are very much running on", "tokens": [50400, 3813, 10466, 1220, 28270, 6331, 1303, 300, 291, 7619, 965, 300, 366, 588, 709, 2614, 322, 50800], "temperature": 0.0, "avg_logprob": -0.12156841403148214, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008186866529285908}, {"id": 488, "seek": 334960, "start": 3358.88, "end": 3366.88, "text": " the kind of von Neumann architecture turing completeness paradigm and yet very tantalizingly", "tokens": [50828, 264, 733, 295, 2957, 1734, 449, 969, 9482, 256, 1345, 1557, 15264, 24709, 293, 1939, 588, 12095, 304, 3319, 356, 51228], "temperature": 0.0, "avg_logprob": -0.12156841403148214, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008186866529285908}, {"id": 489, "seek": 334960, "start": 3366.88, "end": 3377.2, "text": " close like to a physical object that has a constrained rule de facto like only one thing", "tokens": [51228, 1998, 411, 281, 257, 4001, 2657, 300, 575, 257, 38901, 4978, 368, 42225, 411, 787, 472, 551, 51744], "temperature": 0.0, "avg_logprob": -0.12156841403148214, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.008186866529285908}, {"id": 490, "seek": 337720, "start": 3377.2, "end": 3386.3199999999997, "text": " can come out of this at a time as long as the funnel is this wide and so bringing the rules", "tokens": [50364, 393, 808, 484, 295, 341, 412, 257, 565, 382, 938, 382, 264, 24515, 307, 341, 4874, 293, 370, 5062, 264, 4474, 50820], "temperature": 0.0, "avg_logprob": -0.016103556210344486, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.010165498591959476}, {"id": 491, "seek": 337720, "start": 3386.3199999999997, "end": 3395.7599999999998, "text": " and the regularities of what we call physical things to bear with the fundamental and the", "tokens": [50820, 293, 264, 3890, 1088, 295, 437, 321, 818, 4001, 721, 281, 6155, 365, 264, 8088, 293, 264, 51292], "temperature": 0.0, "avg_logprob": -0.016103556210344486, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.010165498591959476}, {"id": 492, "seek": 339576, "start": 3395.76, "end": 3408.6400000000003, "text": " imposed constraints on informational spaces it's very cool directions um one other note about", "tokens": [50364, 26491, 18491, 322, 49391, 7673, 309, 311, 588, 1627, 11095, 1105, 472, 661, 3637, 466, 51008], "temperature": 0.0, "avg_logprob": -0.08534506619986841, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.041452545672655106}, {"id": 493, "seek": 339576, "start": 3408.6400000000003, "end": 3414.8, "text": " just where active inference um an action plays a role is um and also you mentioned like the", "tokens": [51008, 445, 689, 4967, 38253, 1105, 364, 3069, 5749, 257, 3090, 307, 1105, 293, 611, 291, 2835, 411, 264, 51316], "temperature": 0.0, "avg_logprob": -0.08534506619986841, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.041452545672655106}, {"id": 494, "seek": 339576, "start": 3414.8, "end": 3422.48, "text": " hypothesis going stale or like sort of data being over relied on um in the proactive stance", "tokens": [51316, 17291, 516, 342, 1220, 420, 411, 1333, 295, 1412, 885, 670, 35463, 322, 1105, 294, 264, 28028, 21033, 51700], "temperature": 0.0, "avg_logprob": -0.08534506619986841, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.041452545672655106}, {"id": 495, "seek": 342248, "start": 3422.48, "end": 3429.76, "text": " where we're using expected free energy or something like it to to calculate future courses of action", "tokens": [50364, 689, 321, 434, 1228, 5176, 1737, 2281, 420, 746, 411, 309, 281, 281, 8873, 2027, 7712, 295, 3069, 50728], "temperature": 0.0, "avg_logprob": -0.05662539228796959, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.004331027623265982}, {"id": 496, "seek": 342248, "start": 3429.76, "end": 3436.0, "text": " over observations that we haven't seen yet moves that haven't been made yet there's an explicit", "tokens": [50728, 670, 18163, 300, 321, 2378, 380, 1612, 1939, 6067, 300, 2378, 380, 668, 1027, 1939, 456, 311, 364, 13691, 51040], "temperature": 0.0, "avg_logprob": -0.05662539228796959, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.004331027623265982}, {"id": 497, "seek": 342248, "start": 3436.0, "end": 3445.28, "text": " epistemic value and so that can be diagnosed and observed as a measure of where a given", "tokens": [51040, 2388, 468, 3438, 2158, 293, 370, 300, 393, 312, 16899, 293, 13095, 382, 257, 3481, 295, 689, 257, 2212, 51504], "temperature": 0.0, "avg_logprob": -0.05662539228796959, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.004331027623265982}, {"id": 498, "seek": 344528, "start": 3446.1600000000003, "end": 3452.4, "text": " computation is on the continuum between purely pragmatic value just constraint", "tokens": [50408, 24903, 307, 322, 264, 36120, 1296, 17491, 46904, 2158, 445, 25534, 50720], "temperature": 0.0, "avg_logprob": -0.0687847478049142, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0618596114218235}, {"id": 499, "seek": 344528, "start": 3452.4, "end": 3457.84, "text": " satisfying and and realizing preferences and expectations and then the pure epistemic value", "tokens": [50720, 18348, 293, 293, 16734, 21910, 293, 9843, 293, 550, 264, 6075, 2388, 468, 3438, 2158, 50992], "temperature": 0.0, "avg_logprob": -0.0687847478049142, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0618596114218235}, {"id": 500, "seek": 344528, "start": 3457.84, "end": 3465.28, "text": " where all outcomes are good and the more information gain the better and then being able to take", "tokens": [50992, 689, 439, 10070, 366, 665, 293, 264, 544, 1589, 6052, 264, 1101, 293, 550, 885, 1075, 281, 747, 51364], "temperature": 0.0, "avg_logprob": -0.0687847478049142, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0618596114218235}, {"id": 501, "seek": 344528, "start": 3465.28, "end": 3472.0800000000004, "text": " control of that balance and know amidst changing situations again taking probabilistic or rule", "tokens": [51364, 1969, 295, 300, 4772, 293, 458, 30153, 372, 4473, 6851, 797, 1940, 31959, 3142, 420, 4978, 51704], "temperature": 0.0, "avg_logprob": -0.0687847478049142, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0618596114218235}, {"id": 502, "seek": 347208, "start": 3472.08, "end": 3479.2, "text": " based approach there to when epistemic and pragmatic like gas and break kind of come into play", "tokens": [50364, 2361, 3109, 456, 281, 562, 2388, 468, 3438, 293, 46904, 411, 4211, 293, 1821, 733, 295, 808, 666, 862, 50720], "temperature": 0.0, "avg_logprob": -0.07337987422943115, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.033067043870687485}, {"id": 503, "seek": 347208, "start": 3479.2, "end": 3489.52, "text": " these are very basal um control knobs or features in active inference that it's just", "tokens": [50720, 613, 366, 588, 987, 304, 1105, 1969, 46999, 420, 4122, 294, 4967, 38253, 300, 309, 311, 445, 51236], "temperature": 0.0, "avg_logprob": -0.07337987422943115, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.033067043870687485}, {"id": 504, "seek": 347208, "start": 3490.48, "end": 3497.6, "text": " not going to show up at the 50th layer of scaling is all you need", "tokens": [51284, 406, 516, 281, 855, 493, 412, 264, 2625, 392, 4583, 295, 21589, 307, 439, 291, 643, 51640], "temperature": 0.0, "avg_logprob": -0.07337987422943115, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.033067043870687485}, {"id": 505, "seek": 349760, "start": 3497.6, "end": 3512.64, "text": " yeah cool well do you have any other like thoughts or things you want to add or questions or where", "tokens": [50364, 1338, 1627, 731, 360, 291, 362, 604, 661, 411, 4598, 420, 721, 291, 528, 281, 909, 420, 1651, 420, 689, 51116], "temperature": 0.0, "avg_logprob": -0.1631878072565252, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.008843991905450821}, {"id": 506, "seek": 349760, "start": 3512.64, "end": 3521.2, "text": " things are heading for your works nothing to add at this moment but certainly uh excited to keep", "tokens": [51116, 721, 366, 9864, 337, 428, 1985, 1825, 281, 909, 412, 341, 1623, 457, 3297, 2232, 2919, 281, 1066, 51544], "temperature": 0.0, "avg_logprob": -0.1631878072565252, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.008843991905450821}, {"id": 507, "seek": 352120, "start": 3521.2, "end": 3531.4399999999996, "text": " in touch with this community and yeah collaborating yeah absolutely um and we sort of share I mean", "tokens": [50364, 294, 2557, 365, 341, 1768, 293, 1338, 30188, 1338, 3122, 1105, 293, 321, 1333, 295, 2073, 286, 914, 50876], "temperature": 0.0, "avg_logprob": -0.101420423877773, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.18925008177757263}, {"id": 508, "seek": 352120, "start": 3531.4399999999996, "end": 3539.12, "text": " we write papers and stuff but we mostly like we share most of what we do be at academic in the", "tokens": [50876, 321, 2464, 10577, 293, 1507, 457, 321, 5240, 411, 321, 2073, 881, 295, 437, 321, 360, 312, 412, 7778, 294, 264, 51260], "temperature": 0.0, "avg_logprob": -0.101420423877773, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.18925008177757263}, {"id": 509, "seek": 352120, "start": 3539.12, "end": 3545.68, "text": " sort of machine learning space or be it in the um sort of the physical hardware space uh on our", "tokens": [51260, 1333, 295, 3479, 2539, 1901, 420, 312, 309, 294, 264, 1105, 1333, 295, 264, 4001, 8837, 1901, 2232, 322, 527, 51588], "temperature": 0.0, "avg_logprob": -0.101420423877773, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.18925008177757263}, {"id": 510, "seek": 354568, "start": 3545.68, "end": 3553.52, "text": " blog which is blog.normalcomputing.ai um and yeah thank you so much for inviting us it's been very fun", "tokens": [50364, 6968, 597, 307, 6968, 13, 23157, 1112, 2582, 278, 13, 1301, 1105, 293, 1338, 1309, 291, 370, 709, 337, 18202, 505, 309, 311, 668, 588, 1019, 50756], "temperature": 0.0, "avg_logprob": -0.16484378104986147, "compression_ratio": 1.38135593220339, "no_speech_prob": 0.03661072254180908}, {"id": 511, "seek": 354568, "start": 3554.3999999999996, "end": 3563.12, "text": " awesome thank you hope to speak again so peace bye thanks hi", "tokens": [50800, 3476, 1309, 291, 1454, 281, 1710, 797, 370, 4336, 6543, 3231, 4879, 51236], "temperature": 0.0, "avg_logprob": -0.16484378104986147, "compression_ratio": 1.38135593220339, "no_speech_prob": 0.03661072254180908}, {"id": 512, "seek": 357568, "start": 3575.68, "end": 3576.18, "text": " you", "tokens": [50364, 291, 50389], "temperature": 0.0, "avg_logprob": -0.8467538952827454, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.34980010986328125}], "language": "en"}