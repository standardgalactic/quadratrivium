1
00:00:00,000 --> 00:00:09,540
Hello and welcome, everyone, to the Active Inference Institute.

2
00:00:09,540 --> 00:00:16,640
This is Active Gueststream number 41.1 on April 25, 2023.

3
00:00:16,640 --> 00:00:20,460
We're here with Elliot Murphy and Steven Piantadosi.

4
00:00:20,460 --> 00:00:22,280
This is going to be quite a discussion.

5
00:00:22,280 --> 00:00:25,640
We will begin with opening statements from Steven and Elliot.

6
00:00:25,640 --> 00:00:29,880
Elliot will then lead with some questions and we'll have an open discussion at the

7
00:00:29,880 --> 00:00:30,880
end.

8
00:00:30,880 --> 00:00:36,080
So, Steven, please, thank you for joining and to your opening statement.

9
00:00:36,080 --> 00:00:37,080
Cool.

10
00:00:37,080 --> 00:00:39,360
Hi, so I'm Steve Piantadosi.

11
00:00:39,360 --> 00:00:45,320
I'm a professor in psychology and neuroscience at UC Berkeley.

12
00:00:45,320 --> 00:00:50,080
And I guess part of the reason that we're here is that I recently wrote a paper on large

13
00:00:50,080 --> 00:00:57,080
language models in part trying to convey some enthusiasm about what they've kind of accomplished

14
00:00:57,080 --> 00:01:01,000
in terms of learning syntax and semantics.

15
00:01:01,000 --> 00:01:05,600
And in part pointing out, I think that these models really change how we should think about

16
00:01:05,600 --> 00:01:12,240
language, how we should think about theories of linguistic representation and theories

17
00:01:12,240 --> 00:01:16,360
of grammar and likely also theories of learning.

18
00:01:16,360 --> 00:01:17,360
Yeah.

19
00:01:17,360 --> 00:01:18,360
Awesome.

20
00:01:18,360 --> 00:01:19,360
Yeah.

21
00:01:19,360 --> 00:01:21,360
So, I'm Elliot Murphy.

22
00:01:21,360 --> 00:01:25,680
I'm a postdoc in the Department of Neurosurgery at UC Health in Texas.

23
00:01:25,680 --> 00:01:29,600
I read Steven's paper with great interest and there's a lot of people.

24
00:01:29,600 --> 00:01:33,440
There were some areas of convergence, but the things I want to kind of focus on today

25
00:01:33,440 --> 00:01:38,520
in responding to Steven and kind of probing how to do with areas of divergence maybe.

26
00:01:38,520 --> 00:01:44,200
So, you know, Steven's paper is based on the idea that modern machine learning has subverted

27
00:01:44,200 --> 00:01:48,800
and bypassed the entire theoretical framework of Chomsky's approach.

28
00:01:48,800 --> 00:01:51,840
So I wanted to kind of respond to some of these main arguments and some other related

29
00:01:51,840 --> 00:01:56,560
arguments in the literature that some folks listening might have some insight and thoughts

30
00:01:56,560 --> 00:01:57,560
on.

31
00:01:57,560 --> 00:02:02,000
So, it's a very common criticism to say that large language models just predict the next

32
00:02:02,000 --> 00:02:05,800
token, which is obviously a bit of a cliche, right?

33
00:02:05,800 --> 00:02:07,000
It's not quite true.

34
00:02:07,000 --> 00:02:12,560
They don't just predict the next token, they also seem to confabulate, they seem to hallucinate,

35
00:02:12,560 --> 00:02:18,000
they maybe lie, they randomly provide different answers to the same question, and they seem

36
00:02:18,000 --> 00:02:21,120
to stochastically mimic language like structures.

37
00:02:21,120 --> 00:02:24,160
They sometimes correct themselves sometimes when they shouldn't.

38
00:02:24,160 --> 00:02:27,120
If you push them a little, they kind of change their mind sometimes.

39
00:02:27,120 --> 00:02:30,400
In fact, if Fox News is currently looking for a replacement for Tucker Carlson, they

40
00:02:30,400 --> 00:02:31,400
could do less.

41
00:02:31,400 --> 00:02:36,400
They could definitely do worse than using ChachiBT if they're looking for a similar

42
00:02:36,400 --> 00:02:37,400
caliber.

43
00:02:37,400 --> 00:02:41,880
So, these models seem to do all sorts of like wild things, and over the past 10 years, there's

44
00:02:41,880 --> 00:02:46,320
been a sequence of different, you know, systems developed like where to be, where to be, and

45
00:02:46,320 --> 00:02:49,840
each of them is based on a different neural net approach, but ultimately they all seem

46
00:02:49,840 --> 00:02:54,560
to take words and characterize them by lists of hundreds or thousands of numbers.

47
00:02:54,560 --> 00:03:01,800
So the GTP3 network has 175 billion weights, 96 attention heads in its architecture, and

48
00:03:01,800 --> 00:03:04,600
as far as what I know, maybe Stephen can correct me here.

49
00:03:04,600 --> 00:03:08,360
We don't really have a great idea of what these different parts really mean.

50
00:03:08,360 --> 00:03:10,080
It just seems to kind of work that way.

51
00:03:10,080 --> 00:03:15,040
Like attention heads in GTP3 can pay attention to much earlier tokens in the string in order

52
00:03:15,040 --> 00:03:18,840
to help them predict the next token, but the whole architecture from start to finish is

53
00:03:18,840 --> 00:03:24,760
kind of engineering-based motivations, and I always kind of wonder what about all the

54
00:03:24,760 --> 00:03:29,600
models that kind of failed from these LLMs, from the different tech companies.

55
00:03:29,600 --> 00:03:33,360
It's like these companies often seem to, you know, make it seem like they have these models

56
00:03:33,360 --> 00:03:38,840
that really work very well straight out the box, and they all seem to be named after some

57
00:03:38,840 --> 00:03:40,840
kind of famous artists, right?

58
00:03:40,840 --> 00:03:43,120
They have Dali after Salvador Dali.

59
00:03:43,120 --> 00:03:47,240
They have Da Vinci, maybe pretty soon one of these companies will release a large language

60
00:03:47,240 --> 00:03:51,520
model called Jesus or something, I don't know.

61
00:03:51,520 --> 00:03:54,040
But they always say, here's our new foundation model.

62
00:03:54,040 --> 00:03:55,040
It's called Picasso.

63
00:03:55,040 --> 00:03:56,040
It's the first one we tried.

64
00:03:56,040 --> 00:03:57,040
It works just great.

65
00:03:57,040 --> 00:04:00,720
No problems straight out the box, but I always wonder what about all the other black boxes

66
00:04:00,720 --> 00:04:02,960
that have kind of failed every time?

67
00:04:02,960 --> 00:04:07,360
That doesn't seem to be a kind of a very open and clear structure to the kind of scientific

68
00:04:07,360 --> 00:04:12,200
reasoning behind selecting, you know, one model or another, but again, I might be, I'm

69
00:04:12,200 --> 00:04:15,120
open to be corrected about that.

70
00:04:15,120 --> 00:04:20,520
So even basic language models do pretty well on basic word prediction.

71
00:04:20,520 --> 00:04:24,160
So the issue is whether these tools provide any insights into traditional psycholinguistic

72
00:04:24,160 --> 00:04:26,400
notions like grammar and parsing.

73
00:04:26,400 --> 00:04:30,680
So this is really why I kind of prefer the term corpus model rather language model, suggested

74
00:04:30,680 --> 00:04:33,240
by people like Sabra Varys.

75
00:04:33,240 --> 00:04:38,040
So as we pointed out that no one really thinks LLMs tell us anything profound about Python

76
00:04:38,040 --> 00:04:42,040
when they learn Python code just as well as natural language, but Python is a symbolic

77
00:04:42,040 --> 00:04:46,840
language with a phrase structure grammar and nobody says LLMs are unveiling the secrets

78
00:04:46,840 --> 00:04:47,840
of Python.

79
00:04:47,840 --> 00:04:48,840
Right.

80
00:04:48,840 --> 00:04:52,080
So just to quote Varys here, he says, if A and N models can be construed as explanatory

81
00:04:52,080 --> 00:04:56,240
theories for natural language based on their successes on language tasks, then in the absence

82
00:04:56,240 --> 00:04:59,520
of counter arguments, they should be good explanatory theories for computer language

83
00:04:59,520 --> 00:05:00,520
as well.

84
00:05:00,520 --> 00:05:05,400
Therefore, successful A and N models of natural language cannot be used as evidence against

85
00:05:05,400 --> 00:05:08,120
generative phrase structure grammars in language.

86
00:05:08,120 --> 00:05:12,160
So corpus model is really a more appropriate term for other reasons too.

87
00:05:12,160 --> 00:05:15,880
People like Emily Bender and some others have shown that features of the training corpus,

88
00:05:15,880 --> 00:05:20,200
in fact, I think Steven cites this, you cite this in your paper actually as a limitation.

89
00:05:20,200 --> 00:05:24,160
They show that features of the training corpus can heavily influence the learning process.

90
00:05:24,160 --> 00:05:27,400
So it's been shown that the performance of large language models on language tasks is

91
00:05:27,400 --> 00:05:31,800
really heavily influenced by the diversity of the training corpus.

92
00:05:31,800 --> 00:05:34,240
But natural language itself is not biased, right?

93
00:05:34,240 --> 00:05:36,800
It's just the computational system.

94
00:05:36,800 --> 00:05:40,160
Some beings can be biased in what they say and how they act.

95
00:05:40,160 --> 00:05:42,520
But natural language itself isn't biased, right?

96
00:05:42,520 --> 00:05:47,880
So large language models, therefore, it seems difficult for me to agree that they are being

97
00:05:47,880 --> 00:05:49,400
subject to all sorts of biases.

98
00:05:49,400 --> 00:05:52,920
They therefore can't really be models of language, they're models of something else.

99
00:05:52,920 --> 00:05:58,840
So just to kind of wrap up this argument, even though LLMs are clearly exposed to vastly

100
00:05:58,840 --> 00:06:02,560
more linguistic experience in children, again, this is something else that Steven concedes

101
00:06:02,560 --> 00:06:04,500
and talks about in his paper.

102
00:06:04,500 --> 00:06:09,020
And so their learning outcomes may still be relevant in addressing what grammatical generalizations

103
00:06:09,020 --> 00:06:10,660
are learnable in principle.

104
00:06:10,660 --> 00:06:14,220
So I do agree with this statement here, that in principle they can tell us something about

105
00:06:14,220 --> 00:06:18,900
learnability rather than things like broad acquisitionist frameworks.

106
00:06:18,900 --> 00:06:22,500
But that's about as much I think you can maybe say right now.

107
00:06:22,500 --> 00:06:27,140
Showing that some inductive biases are not necessary for learning is not really the same

108
00:06:27,140 --> 00:06:29,840
thing as showing that it isn't present in children.

109
00:06:29,840 --> 00:06:33,620
So there's been a long debate about whether negative evidence and instruction and correction

110
00:06:33,740 --> 00:06:39,300
and feedback during language learning are necessary or even useful for infants and children.

111
00:06:39,300 --> 00:06:43,340
But right now I kind of agree more with Eugene Choi and Gary Marcus and others who have highlighted

112
00:06:43,340 --> 00:06:46,820
how LLMs are currently very expensive to train.

113
00:06:46,820 --> 00:06:51,740
They're clearly an example of concentrated private power in the hands of a few tech companies.

114
00:06:51,740 --> 00:06:57,060
Their environment impact is massive and many of people have been less constrained and conservative

115
00:06:57,060 --> 00:07:01,940
in their assessment here, which is much less so than Gary Marcus and Eugene.

116
00:07:01,980 --> 00:07:08,820
So Bill Gates recently wrote that chatGPT is the biggest tech development since the

117
00:07:08,820 --> 00:07:10,820
graphical user interface, the GUI.

118
00:07:10,820 --> 00:07:16,100
And Henry Kissinger wrote in February in the Wall Street Journal that as chatGPTs capacities

119
00:07:16,100 --> 00:07:21,620
become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality

120
00:07:21,620 --> 00:07:24,180
and reorganize politics and society.

121
00:07:24,180 --> 00:07:29,180
Generative AI is poised to generate new forms of human consciousness, so very radical claims

122
00:07:29,180 --> 00:07:30,180
happening at the moment.

123
00:07:30,180 --> 00:07:36,100
I do wonder if sometimes all of the AI hype may have, you know, see it into certain portions

124
00:07:36,100 --> 00:07:39,940
of academia potentially, a lot of ground claims being made.

125
00:07:39,940 --> 00:07:43,220
But I think, you know, more concretely, just to put it back to Stephen here, I wanted to

126
00:07:43,220 --> 00:07:48,820
maybe raise the issue of there's a critique by Roscoe and Beaumont that I think he's read

127
00:07:48,820 --> 00:07:51,500
on Lingbos.

128
00:07:51,500 --> 00:07:56,140
I think you saw on Twitter that you don't like the response they gave because the objection

129
00:07:56,140 --> 00:07:59,620
that they made is that, you know, science is an example of deductible logic.

130
00:07:59,620 --> 00:08:03,340
Your objection is that science isn't deductive, it's inductive, right?

131
00:08:03,340 --> 00:08:08,380
But I think their general point might be more accurate, namely that you can't use the fact

132
00:08:08,380 --> 00:08:13,620
that language models do well predicting some linguistic behavior in humans and some neuroimaging

133
00:08:13,620 --> 00:08:14,860
responses.

134
00:08:14,860 --> 00:08:19,580
You can't use that alone to claim that they can yield a theory of human language.

135
00:08:19,580 --> 00:08:23,340
So in your paper, Stephen, you know that it seems that certain structures work better

136
00:08:23,340 --> 00:08:24,340
than others.

137
00:08:24,340 --> 00:08:28,460
The right attentional mechanism is important, prediction is important, semantic representations

138
00:08:28,460 --> 00:08:29,460
are important.

139
00:08:29,500 --> 00:08:32,620
And therefore, we can glean currently based on these models, right?

140
00:08:32,620 --> 00:08:35,900
But so far, that's really all I've been able to glean in the literature.

141
00:08:35,900 --> 00:08:37,940
I'm not sure if you have more insights here.

142
00:08:37,940 --> 00:08:43,700
So Roscoe and Beaumont use the example of poor prediction, but strong explanation, right?

143
00:08:43,700 --> 00:08:47,140
Explanatory power and not predictive accuracy forms the basis of modern science.

144
00:08:47,140 --> 00:08:50,780
I don't want to explore this a little bit later, maybe, but modern language models can

145
00:08:50,780 --> 00:08:54,540
accurately model parts of human language, but they can also perform very well on impossible

146
00:08:54,540 --> 00:08:59,420
languages and unnatural structures that humans can't learn and have great difficulty

147
00:08:59,500 --> 00:09:00,140
processing.

148
00:09:00,140 --> 00:09:02,220
And I know you're familiar with these with these criticisms, right?

149
00:09:03,180 --> 00:09:04,940
But you're definitely not alone here at the same time.

150
00:09:04,940 --> 00:09:12,220
So Ilya Tskeva, the chief scientist at OpenAI, he said in an interview recently, what does

151
00:09:12,220 --> 00:09:14,060
it mean to predict the next token well enough?

152
00:09:14,460 --> 00:09:19,020
It means that you understand the underlying reality that led to the creation of that token,

153
00:09:20,060 --> 00:09:23,420
which is quite divergent from a lot of more conservative claims in the literature here.

154
00:09:24,620 --> 00:09:28,940
And also, you know, I would just say in response to that, that different components of science

155
00:09:29,020 --> 00:09:32,220
can be either inductive or deductive, right?

156
00:09:32,220 --> 00:09:33,340
It's not really an either-or.

157
00:09:33,340 --> 00:09:34,700
You have an existing theory.

158
00:09:34,700 --> 00:09:36,300
You formulate a hypothesis.

159
00:09:36,300 --> 00:09:40,460
You collect data, you analyze it, and that's kind of a deductive process.

160
00:09:40,460 --> 00:09:43,340
But there's also cases where you start with a specific observation.

161
00:09:43,340 --> 00:09:46,460
You find some patterns and you induce general conclusions, right?

162
00:09:46,460 --> 00:09:53,020
And then there's abduction, where you magically invent hypotheses and reduce the hypothesis space.

163
00:09:53,020 --> 00:09:58,220
You wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific

164
00:09:58,220 --> 00:10:00,540
or abductive reasoning is unscientific, right?

165
00:10:00,540 --> 00:10:02,860
These are all just different ways of doing stuff.

166
00:10:02,860 --> 00:10:09,260
I mean, in your paper, you give the examples of using models to predict hurricanes and pandemics

167
00:10:09,260 --> 00:10:12,860
as being examples of stuff that is as rigorous as science gets.

168
00:10:12,860 --> 00:10:17,180
And then you employ a reader to conclude that the situation is no different for language models.

169
00:10:18,220 --> 00:10:22,380
But I guess for me, the issue is that models predicting hurricanes are not in the business

170
00:10:22,380 --> 00:10:25,980
of answering the question, what is the hurricane, right?

171
00:10:26,060 --> 00:10:29,420
Models accurately predicting the weather are very accurate, but they're not.

172
00:10:29,420 --> 00:10:33,420
They're aligned with the meteorology department, but they're not a substitute for it.

173
00:10:34,380 --> 00:10:36,140
So I guess I'll just hand it over to you.

174
00:10:37,820 --> 00:10:40,220
Yeah. Okay. Well, there's a lot there.

175
00:10:41,660 --> 00:10:48,860
I guess I could start just by saying that I agree with many of these criticisms, right,

176
00:10:48,940 --> 00:10:57,900
about these models being controlled by one or two companies that being very, very problematic.

177
00:11:00,300 --> 00:11:04,780
They have all kinds of biases that they've acquired because they're trained on text from the

178
00:11:04,780 --> 00:11:12,940
internet. That's hugely problematic. I certainly agree that there's things at least at present

179
00:11:12,940 --> 00:11:20,140
that the models don't do well, right? So I think it's easy to find examples of questions

180
00:11:20,140 --> 00:11:25,900
and problems that will trip them up. I think why I've been excited about them, though,

181
00:11:26,940 --> 00:11:34,060
is not necessarily in those terms, right, but in terms of performance on language,

182
00:11:34,780 --> 00:11:42,140
specifically syntax and semantics. I think they're far beyond kind of any other theory

183
00:11:42,140 --> 00:11:49,900
in any other domain, right? So there's no other theory out of linguistics or computer science

184
00:11:50,620 --> 00:11:57,340
which can generate long, coherent grammatical passages of text.

185
00:11:58,460 --> 00:12:07,260
And so kind of admitting all of their problems as tools or things which are deployed by companies,

186
00:12:08,220 --> 00:12:15,100
there's still this question of, like, how are they at dealing with language? And I think this

187
00:12:15,100 --> 00:12:20,140
is where a lot of the enthusiasm comes from, is there really hasn't been anything even remotely

188
00:12:20,140 --> 00:12:26,220
like them in terms of linguistic ability. And that's the thing that I think is exciting. So,

189
00:12:26,220 --> 00:12:33,100
yes, I agree with a bunch of these things you started with, but nonetheless, like I think in

190
00:12:33,100 --> 00:12:37,980
terms of syntax and semantics, there's just no other theory which is comparable to them.

191
00:12:40,220 --> 00:12:46,460
But so let me push that back then, right? So the main objection from a lot of people I've spoken

192
00:12:46,460 --> 00:12:51,740
to in the departments of linguistics who are like a lot of the general first of your paper

193
00:12:52,460 --> 00:12:57,900
is to really say, well, you're right, they do a wonderful job accurately modeling all aspects

194
00:12:57,980 --> 00:13:03,260
of a lot of aspects of syntax and semantics. However, I don't know if any real just like,

195
00:13:03,260 --> 00:13:06,860
you know, Chomsky talks about facts about language, which is an old fashioned notion.

196
00:13:07,580 --> 00:13:10,780
But I really think that's kind of an important notion too, right? Like,

197
00:13:10,780 --> 00:13:18,620
is there some discovery about language itself that LLMs can uniquely provide? So like, if LLMs

198
00:13:18,620 --> 00:13:24,780
made some prediction about, let's say you have a sentence structure type X being more difficult

199
00:13:24,860 --> 00:13:29,100
to process than sentence type Y. And this is a unique prediction that only they'd

200
00:13:29,660 --> 00:13:34,060
generate it. And no human linguist, Chomsky, Honesty, and Adger, none of these people had

201
00:13:34,060 --> 00:13:37,900
ever predicted that before. But it turns out to be true. You do eye tracking experiments,

202
00:13:37,900 --> 00:13:41,020
you do all sorts of different behavioral experiments. And it turns out, oh, you know,

203
00:13:41,020 --> 00:13:45,020
after all, it turns out to be true. This is the new insight about language processing,

204
00:13:45,020 --> 00:13:49,580
it's a new insight about language, you know, behavior. I just wonder, I'm not saying that

205
00:13:49,580 --> 00:13:53,660
this is not possible in principle, because it might happen in the near future. But that's,

206
00:13:53,660 --> 00:13:58,140
I guess, for me, the crux of why a lot of linguists speaking up, speaking on behalf of

207
00:13:58,780 --> 00:14:03,100
the entire linguistic community here. And, you know, I guess that would be one of the main

208
00:14:03,100 --> 00:14:11,100
objections. Yeah, I mean, I don't know of, I guess, I think of the insights they've provided as

209
00:14:11,100 --> 00:14:18,220
kind of general principles, right? So I think about these things like the power of memorizing

210
00:14:18,220 --> 00:14:23,260
chunks of language, right? So like, they seem to be very good at constructions, for example.

211
00:14:23,340 --> 00:14:27,500
And there's lots of linguistic theories, Chomsky's in particular, right, which are

212
00:14:28,380 --> 00:14:34,140
about trying to find kind of minimal amounts of structure to memorize, right, trying to derive

213
00:14:34,140 --> 00:14:40,780
as much as possible from some small set, some small collection of operations. And I think

214
00:14:40,780 --> 00:14:47,020
that hasn't gone well for those theories, right? Whereas this goes really well, right? So if we

215
00:14:47,020 --> 00:14:50,860
think about something which has the memorization abilities, if we think about theories of grammar,

216
00:14:50,860 --> 00:14:58,460
for example, which build on, you know, humans like really remarkable ability to memorize different

217
00:14:58,460 --> 00:15:02,780
constructions, right, or different words, you know, tens of thousands of words, tens of thousands of

218
00:15:02,780 --> 00:15:07,020
different constructions, sorry, tens of thousands of different idioms, maybe our theory of grammar

219
00:15:07,020 --> 00:15:12,140
should be integrated with that. And there in some sense, a kind of proof of principle that

220
00:15:12,140 --> 00:15:17,740
that kind of approach can work well, right? Can think about making other types of predictions

221
00:15:17,740 --> 00:15:23,500
with them, some of which people are currently doing, but for example, trying to use them to measure

222
00:15:24,300 --> 00:15:30,300
processing difficulty, measure surprise, for example, from these models, their surprise measures,

223
00:15:30,300 --> 00:15:36,620
right, are much better than, say, context-free grammars or other kinds of language models.

224
00:15:36,620 --> 00:15:41,980
And then it's an interesting question how those surprises or predictabilities relate to human

225
00:15:41,980 --> 00:15:47,180
processing, right? And it may capture some of it or might be nonlinear, or it might, you know,

226
00:15:47,180 --> 00:15:52,700
only capture a little bit of it or whatever. That's an interesting kind of other scientific

227
00:15:52,700 --> 00:15:57,580
question. But I think in principle, right, they can make predictions about, for example, the

228
00:15:57,580 --> 00:16:03,500
connections between sentences, right? So in the paper, I gave this example of, you know, converting

229
00:16:03,500 --> 00:16:09,820
a declaration into a question in 10 different ways, right? And presumably when it, when, you know,

230
00:16:09,820 --> 00:16:15,820
GPT or something is doing that, it's finding 10 different questions which are all in some way

231
00:16:15,820 --> 00:16:22,700
related, kind of nearby in the models underlying semantic or syntactic space. And so those kinds

232
00:16:22,700 --> 00:16:29,500
of things are of the type that I think, you know, some linguists might want, right, which is here's

233
00:16:29,500 --> 00:16:34,220
some hidden connection between sentences or their, or their structures. But as far as I know, they

234
00:16:34,220 --> 00:16:40,460
haven't been evaluated empirically yet. So, yeah, yeah, I mean, these kinds of models are only a

235
00:16:40,460 --> 00:16:45,820
few years old. So I think it's, it's reasonable to be excited about them, even though this kind

236
00:16:45,820 --> 00:16:51,260
of work hasn't been done yet. No, that's right. No, totally. Totally. I mean, I think that's

237
00:16:51,260 --> 00:16:55,820
the right perspective to take. But I think this gets to the issue of the, you mentioned surprise

238
00:16:55,820 --> 00:17:02,220
or you mentioned learnability, you know, LMS learn some syntax, but they do so. We've obviously way,

239
00:17:02,220 --> 00:17:08,700
way more data than infants do. Such that observations of potential structure in and of itself is not

240
00:17:08,780 --> 00:17:12,940
a refutation of the poverty of the stimulus, well, the weaker version, I should say, of the

241
00:17:12,940 --> 00:17:17,900
poverty of the stimulus argument. So the mere fact that LMS can do what they do without grammatical

242
00:17:17,900 --> 00:17:22,380
prize is very striking, I agree. And in fact, you wouldn't have predicted that maybe five or six or

243
00:17:22,380 --> 00:17:28,060
seven years ago. But it doesn't yet invalidate the claim that humans have such a prize and we

244
00:17:28,060 --> 00:17:32,380
bring those prizes with us. And so in order to see if computational linguistics can constrain

245
00:17:32,380 --> 00:17:36,140
hypotheses and theoretical linguistics, which I think it can do, by the way, this needs to be

246
00:17:36,140 --> 00:17:39,980
done with, you know, careful experiments in which different learning parameters are controlled.

247
00:17:40,700 --> 00:17:47,180
And gigantic language models like GPT3 are basically, you know, useless here. So this gets

248
00:17:47,180 --> 00:17:52,380
to some of Tarlin's complaints that we need something like a baby LM project, which I know

249
00:17:52,380 --> 00:17:56,700
you're interested in, where we have more, you know, ecologically valid training sets. You make the

250
00:17:56,700 --> 00:18:00,620
prediction in your paper that some structure will be learned from that. I suspect you might be right

251
00:18:00,620 --> 00:18:04,940
there. But, you know, even so, even with the baby LM challenge, there's still the kind of

252
00:18:05,020 --> 00:18:10,620
non-trivial issue of addressing more traditional issues like when the kids start to generalize

253
00:18:10,620 --> 00:18:15,420
based on the amount of current input, based on different factors, cross-linguistically. And that

254
00:18:15,420 --> 00:18:21,180
requires just traditional, you know, psycholinguistics and language acquisition. So LMs, you know, do

255
00:18:21,180 --> 00:18:25,260
care about things like frequency and surprise will, as you said, but there's a really nice paper by

256
00:18:25,260 --> 00:18:30,140
Sophie Slatzen, Andrea Martin, a really beautiful paper that I think you may have seen that shows

257
00:18:30,140 --> 00:18:34,860
very nicely that distributional statistics can sometimes be a cue to moments of structure

258
00:18:34,860 --> 00:18:39,340
building. But it doesn't replace these notions pertaining to composition. So I'll just read a

259
00:18:39,340 --> 00:18:45,820
quote from Chomsky 57, which sounds a lot like what Slatzen Martin said. Despite undeniable

260
00:18:45,820 --> 00:18:51,260
interest and importance of semantic and statistical models of language, they appear to have no direct

261
00:18:51,260 --> 00:18:55,180
relevance to the problem of determining or characterizing the set of grammatical utterances.

262
00:18:55,180 --> 00:18:59,020
I think that we are forced to conclude that grammar is autonomous and independent of meaning,

263
00:18:59,020 --> 00:19:03,260
and that probabilistic models give no particular insight into some of the basic problems of

264
00:19:03,260 --> 00:19:09,500
syntactic structure. So that second hedge of the second sentence turned out to be incorrect.

265
00:19:10,060 --> 00:19:12,460
But it's so it's true that, you know, what Chomsky said of available

266
00:19:13,020 --> 00:19:19,020
stat models in 57 is no longer accurate when applied to models today. That can make abstract

267
00:19:19,020 --> 00:19:22,300
generalizations about novel strings and distributional categories, as you mentioned,

268
00:19:22,300 --> 00:19:27,260
right? But the performance of a single model does not provide direct evidence for or against

269
00:19:27,260 --> 00:19:31,260
the landability of a particular structure. Like given the vast distance between any

270
00:19:31,260 --> 00:19:36,620
computational model available today and the human brain, model success does not mean that the

271
00:19:36,620 --> 00:19:42,620
structure is necessarily land and model failure also doesn't mean that the structure is not landable,

272
00:19:42,620 --> 00:19:50,140
right? Yeah, yeah. So I mean, I think it's maybe worth unpacking kind of a couple different versions

273
00:19:50,140 --> 00:19:55,500
of learnability arguments that people have made, because there have been very, very strong kind

274
00:19:55,580 --> 00:20:01,260
of impossibility claims coming out of kind of Chomsky's tradition, right, that were never

275
00:20:01,260 --> 00:20:06,940
claims about the amount of data that was required, right? They were claims about the logical problem

276
00:20:06,940 --> 00:20:12,860
of language learning and that it was just impossible, right? It was impossible without having

277
00:20:14,780 --> 00:20:19,580
kind of substantial constraints on the class of languages or the class of grammars that you

278
00:20:20,140 --> 00:20:25,580
would acquire. And people for a long time have been arguing against that version of things.

279
00:20:27,180 --> 00:20:31,180
You know, there's old work by Gold, and then there's whole kind of grammatical

280
00:20:31,180 --> 00:20:37,820
theories of acquisition built on that tradition that worry a lot about the kind of order in

281
00:20:37,820 --> 00:20:41,820
which you traverse through different hypotheses and consider different options and things.

282
00:20:42,780 --> 00:20:48,620
And my favorite reference in this is this paper by Nick Chater and Paul Vitani,

283
00:20:49,420 --> 00:20:55,260
called something like Ideal Learning of Natural Language, that basically shows that an unconstrained

284
00:20:55,260 --> 00:21:02,700
learner could, with enough data, acquire the kind of generating rules or the generating grammar

285
00:21:03,340 --> 00:21:10,300
just from observing strings, right? But that paper was really in response to this huge body of work

286
00:21:10,380 --> 00:21:15,980
that was arguing that learning from positive examples, so from just observing strings,

287
00:21:15,980 --> 00:21:24,380
was like logically impossible, right? So of course, you know, people in Chomsky's tradition

288
00:21:24,380 --> 00:21:31,020
really liked that form of argument because it was one that said you had to have something innately

289
00:21:31,740 --> 00:21:36,300
specified in order for language acquisition to work. It was like kind of a mathematical argument,

290
00:21:36,780 --> 00:21:42,300
that you had to have some kind of innate grammar, innate ordering of hypotheses or something,

291
00:21:42,300 --> 00:21:49,020
and all of that just turned out to be totally wrong. So if you move to slightly more kind of

292
00:21:49,020 --> 00:21:55,900
realistic learning settings, which Chater and Vitani do, then it turns out you like an idealized

293
00:21:55,900 --> 00:22:00,780
learner can acquire stuff, and there's no statements about the amount of data that's required even

294
00:22:00,780 --> 00:22:08,860
there, right? That's the kind of pure logical ability to learn, and that ability is what I think

295
00:22:08,860 --> 00:22:14,620
the big versions of large language models also speak to, right? So Chater and Vitani and other

296
00:22:14,620 --> 00:22:20,540
work kind of in that spirit is, you know, mathematical and kind of arguing in principle,

297
00:22:20,540 --> 00:22:28,220
but never created something which was really a grammar, right, or a real kind of implemented

298
00:22:28,220 --> 00:22:35,580
language model. So even, you know, a model which is trained on 100 million or 100 billion or however

299
00:22:35,580 --> 00:22:43,500
many tokens, right, even that kind of model I think is relevant to that version of the debate,

300
00:22:43,500 --> 00:22:50,620
right, and showing that language learning is not impossible from a very unconstrained space.

301
00:22:50,940 --> 00:22:58,380
Okay. And then there's a second version, right, which is can we learn language with the specific

302
00:22:58,380 --> 00:23:03,500
data that kids get, right, and that's both amount of data and form of the data,

303
00:23:04,700 --> 00:23:11,100
and so for people who don't know, the BabyLM Challenge is this,

304
00:23:11,580 --> 00:23:21,580
sorry, we think to call it a competition or a, I guess it is a challenge, trying to get people

305
00:23:21,580 --> 00:23:27,740
to train language models on human sized amounts of data. So that's something more like, I think

306
00:23:27,740 --> 00:23:33,420
there's two different versions, 10 or 100 million different, 10 or 100 million different words in

307
00:23:33,420 --> 00:23:42,060
the training set, which is like, you know, 100th or 1000th or something as big as these big AI

308
00:23:42,060 --> 00:23:49,660
companies are using for their language models. And I think actually it's like, that's exactly the

309
00:23:49,660 --> 00:23:53,820
right kind of thing and exactly what the field needs, right, because you might find that on

310
00:23:54,620 --> 00:24:00,940
a child sized amount of data, you can essentially learn syntax, right, which I think would be the

311
00:24:00,940 --> 00:24:05,500
strongest argument against these property of stimulus claims, you could alternatively find that

312
00:24:06,140 --> 00:24:12,540
maybe you can't learn very much, maybe you, you know, come up with a much crumbier kind of language

313
00:24:12,540 --> 00:24:19,020
model or it's lacking some syntactic or semantic abilities. I actually think that the failures,

314
00:24:19,020 --> 00:24:25,500
they are a little bit hard to interpret because kids data, when they're actually learning language,

315
00:24:25,500 --> 00:24:31,580
they get a lot more data than just strings of sentences, right, they're interacting in an

316
00:24:31,580 --> 00:24:37,500
environment. So there's stuff in the world in front of them. Their utterances are also interactive,

317
00:24:37,500 --> 00:24:41,820
right, so you can say something and see whether your parent brings you the thing that you asked for,

318
00:24:41,820 --> 00:24:49,420
for example, right, that's long been argued by people as a, you know, important cue in language

319
00:24:49,500 --> 00:24:58,460
acquisition. So in the baby LM challenge, there is an ability to train these models

320
00:24:59,180 --> 00:25:03,900
with kind of multimodal input, so I think you can give them as much video data as you want to give,

321
00:25:05,500 --> 00:25:10,540
but probably it's hard to kind of replicate exactly the type of setup and feedback that kids

322
00:25:10,540 --> 00:25:18,540
actually get. So I don't know, you know, I'm excited to see where that goes and how things pan

323
00:25:18,540 --> 00:25:26,860
out there. You know, I think that there is an interesting related question for large language

324
00:25:26,860 --> 00:25:34,220
models, which is like what, which is understanding exactly what all of the data is doing. So it

325
00:25:34,220 --> 00:25:40,060
could be that you need so much data for these models because they're effectively inventing

326
00:25:40,060 --> 00:25:46,780
some form of semantics internally, right, so they're both discovering the rules of syntax and they

327
00:25:46,780 --> 00:25:53,660
appear to be learning quite a bit about word meanings. And it's not, it's totally unclear,

328
00:25:53,660 --> 00:26:00,220
I think, how much of the data in these modern models is needed for syntax versus semantics.

329
00:26:00,220 --> 00:26:07,900
My own guess, I think, would be that the syntactic side is probably requires much less data than the

330
00:26:07,900 --> 00:26:13,180
semantic side. Actually, a student, a former student of mine, Frank Malica, and I wrote a paper a few

331
00:26:13,180 --> 00:26:18,300
years ago trying to estimate the amount of information a learner would necessarily have to

332
00:26:18,300 --> 00:26:24,140
acquire for learning the different aspects of language. So you have to learn all the words and

333
00:26:24,140 --> 00:26:27,980
you learn their forms, you learn their meanings, you probably know their frequencies, you have to

334
00:26:27,980 --> 00:26:34,940
learn syntax. And basically what we found in that analysis, that was, you know, basically just a kind

335
00:26:34,940 --> 00:26:41,020
of back of the envelope calculation for each of these domains is that syntax is actually very few

336
00:26:41,020 --> 00:26:48,060
bits of information, it doesn't take that much information to learn syntax. Whereas like most

337
00:26:48,060 --> 00:26:55,740
of the information you acquire is actually for semantics. So specifying, you know, 30 to 50,000

338
00:26:55,740 --> 00:27:03,340
different word meanings, you know, even if each meaning is just a few bits, right, like that requires

339
00:27:03,340 --> 00:27:10,220
a lot of information and probably each meaning is more than a few bits, right. So it could be,

340
00:27:10,220 --> 00:27:14,540
like, that would make me guess that what's happening with large language models is most

341
00:27:14,540 --> 00:27:19,500
of their training data is about word semantics. And you can think about other ways that kids get

342
00:27:19,500 --> 00:27:26,140
word semantics, right, that's not just kind of co-current patterns in text. But I agree, all of

343
00:27:26,140 --> 00:27:31,180
that is up in the air and really exciting to see what will happen. Yeah, I know that some of the

344
00:27:31,180 --> 00:27:38,060
earlier results from Linsen's lab suggest that at least restricted to equitably valid, you know,

345
00:27:38,060 --> 00:27:43,420
training set sides, models seem to generalize, you know, linear rules for English, probably

346
00:27:43,420 --> 00:27:47,500
has no question of formation, rather than the hierarchical rule, the correct hierarchical rule.

347
00:27:47,500 --> 00:27:53,020
So I think there's a real sense in which, you know, the space of the correct syntactic prize and

348
00:27:53,020 --> 00:27:57,900
inductive biases really is yet to be really settled on. But it seems, at least to me, pretty

349
00:27:57,900 --> 00:28:02,220
obvious that there has to be some. So there's also some evidence that children in English,

350
00:28:02,220 --> 00:28:06,700
going back to this frequency issue, that children in English sometimes spell out an intermediate

351
00:28:06,780 --> 00:28:12,380
copy of movement in the specified position of the lower complementizer position of a long-distance

352
00:28:12,380 --> 00:28:16,940
WH question. So there's a thesis by Thornton at some of the papers about this. So they say,

353
00:28:16,940 --> 00:28:21,580
which person do you think who did that, rather than which person do you think did that? So this

354
00:28:21,580 --> 00:28:25,820
is an interesting, you know, missetting, because some languages do actually spell out these intermediate

355
00:28:25,820 --> 00:28:30,940
copies, but English doesn't. So the kid makes the error in setting their grammar, but the frequency

356
00:28:30,940 --> 00:28:36,220
of the input is actually zero. So our mutual friend Gary Marcus also has an argument against

357
00:28:36,220 --> 00:28:41,580
frequency-determining a kid's output. In the case of German noun plurals, a more regular form of the

358
00:28:41,580 --> 00:28:45,580
setting kind is preferred, not the frequent one. And there's lots of examples like this. So it's

359
00:28:45,580 --> 00:28:50,300
sometimes claimed that subject-experiencer passives, where the subject is passively experiencing

360
00:28:50,300 --> 00:28:55,100
something, are very delayed in kids in comprehension studies until around eight, because they're not

361
00:28:55,100 --> 00:29:00,620
very frequent in the input. But Ken Wexler and colleagues have gone through subject-experiencer

362
00:29:00,620 --> 00:29:05,900
WH questions like, who likes Mary? And they discovered that these are as infrequent in the

363
00:29:05,900 --> 00:29:10,700
input as subject-experiencer passives. But kids have no problem in comprehension studies of these

364
00:29:10,700 --> 00:29:16,460
questions. But they do have problems comprehending subject-experiencer verbal passives. So frequency

365
00:29:16,460 --> 00:29:20,780
once again seems to be irrelevant. Or at least it's not explanatory, right? I guess it's not

366
00:29:20,780 --> 00:29:25,420
explanatory with respect to theory building. So how can LMS help with these, you know,

367
00:29:25,420 --> 00:29:30,300
diverging cases when there's clearly something else going on besides frequency? So LMS, you know,

368
00:29:30,300 --> 00:29:35,020
they seem to generalize just, again, going back to this issue of the cases that you have in your

369
00:29:35,020 --> 00:29:40,060
paper. You show that they generalize the structure of color screen ideas, which is obviously very

370
00:29:40,060 --> 00:29:45,100
cool. But the positive stimulus has never really been about not being able to learn language

371
00:29:45,100 --> 00:29:49,500
statistically. I know you made that claim, right? But Chomsky's point in the 50s about statistical

372
00:29:49,500 --> 00:29:55,020
models of the day is not true of commercial LMS in 2023. And that's correct. But we can't use that

373
00:29:55,020 --> 00:29:59,660
single point to undermine, you know, the entire generator enterprise. Chomsky's basic point was

374
00:29:59,660 --> 00:30:04,300
that you could have a grammatical structure wherein every background has zero frequency,

375
00:30:04,300 --> 00:30:08,700
and it also fails to provide clearly interpretable instructions to the conceptual interfaces.

376
00:30:08,700 --> 00:30:12,700
So interfaces with other systems of the mind. So as you're showing your paper, GPT

377
00:30:12,700 --> 00:30:18,780
mimics examples like full screen ideas. But, you know, again, this sentence yields over 150,000

378
00:30:18,780 --> 00:30:23,500
results on Google, and it's discussed extensively in the literature. It's able to mimic the fact

379
00:30:23,500 --> 00:30:27,340
that it can mimic this doesn't really tell us that much. At least we can't really say anything

380
00:30:27,340 --> 00:30:33,260
with much confidence. So, you know, Albeba behind University College Dublin has this quote recently,

381
00:30:33,260 --> 00:30:38,860
do not mistake your own vulnerability for an LMS intelligence. In fact, even Yanlacun wrote last

382
00:30:38,860 --> 00:30:44,460
year that critics are right to accuse LMS of being engaged in a kind of mimicry. And the example

383
00:30:44,460 --> 00:30:50,140
sentence is from chat GPT that you give in the paper. Actually, don't do a good job because,

384
00:30:50,140 --> 00:30:54,220
as you say, it's likely that, you know, meaningless language is rare in the training data, but they

385
00:30:54,220 --> 00:30:58,540
can either do it or they can't. But there's no middle ground in terms of giving us 10 examples

386
00:30:58,540 --> 00:31:04,860
like this. So, you have colourless green ideas, which are very different semantic objects from

387
00:31:04,860 --> 00:31:11,260
things like brown shimmering rabbits, white glittery bears, black shiny kangaroos, green

388
00:31:11,260 --> 00:31:16,460
glittering monkeys, yellow dazzling lions, red shimmering elephants, right? These are all like

389
00:31:16,460 --> 00:31:21,740
semantic, semantically weird and a bit strange, but they're still like legal structures. They're

390
00:31:21,740 --> 00:31:33,020
kind of meaningful semantic objects. Right? I just said, yeah. Yeah. I mean, so maybe I can

391
00:31:33,020 --> 00:31:38,540
respond to the first point first, right? So, you started off talking about these other

392
00:31:39,340 --> 00:31:45,660
kinds of acquisition patterns, which maybe don't map directly onto frequency. And I think it's

393
00:31:45,660 --> 00:31:53,500
actually a mistake to think that kind of modern learning models should be just based on frequency,

394
00:31:53,500 --> 00:32:00,220
because they're clearly learning like pretty complicated families of rules or constructions

395
00:32:00,220 --> 00:32:07,260
or something. And I think it's very likely that when they're learning that, they're in some sense

396
00:32:07,260 --> 00:32:13,500
searching for a simple or parsimonious explanation of the data that they've seen, right? And how that

397
00:32:13,500 --> 00:32:20,940
caches out in a neural network is maybe complicated and depends on parameters and the specifics of

398
00:32:20,940 --> 00:32:29,020
the learning algorithm and those kind of things. But I think it's, I'd suspect maybe that it's

399
00:32:29,020 --> 00:32:40,300
likely to be the case that they're learning over a complicated set of things, right? A complicated

400
00:32:40,540 --> 00:32:48,620
kind of family of rules and constructions. And that means I think that their generalizations,

401
00:32:48,620 --> 00:32:55,820
maybe like the examples of people that you gave, might be kind of discontinuous in the input,

402
00:32:55,820 --> 00:33:01,500
right? So, sometimes you could imagine seeing some strings which lead you to a grammar and

403
00:33:01,500 --> 00:33:07,100
the simplest grammar of the data that you've seen so far is one which predicts an unseen string,

404
00:33:07,100 --> 00:33:15,100
right? And if that happens, then you'll be taking the data, learning a representation

405
00:33:15,100 --> 00:33:21,740
which generalizes in some novel unseen way so far, purely because that generalization is

406
00:33:21,740 --> 00:33:25,660
sort of the simplest account of the data that you've seen to date, right? I think that's sort of

407
00:33:25,660 --> 00:33:30,220
what linguists try to do, right? Try to look at the data and come up with a theory of it,

408
00:33:30,220 --> 00:33:35,580
and then sometimes that theory predicts some new phenomenon, right? Or some new type of sentence.

409
00:33:36,460 --> 00:33:40,300
And so, if they're learning over as sufficiently rich space of theories,

410
00:33:41,180 --> 00:33:47,020
then it wouldn't be unreasonable or unexpected for them to also show those kinds of patterns. Now,

411
00:33:47,020 --> 00:33:53,420
whether they do or not I think is still an open empirical question, right? Because we have to

412
00:33:53,420 --> 00:33:58,140
train them on small amounts of data and test their generalizations and these kind of things.

413
00:33:58,140 --> 00:34:05,100
But I don't think like just the fact that humans do things which are not purely based on

414
00:34:05,100 --> 00:34:09,020
frequency is any evidence at all, either way, right? Because once you're learning over rich

415
00:34:09,020 --> 00:34:16,140
and interesting classes of theories, then that is the expected behavior. Actually, I had a paper

416
00:34:16,140 --> 00:34:25,900
about a year ago that I think you're familiar with, Yang and Pianta dosi, where we were looking at

417
00:34:27,740 --> 00:34:33,100
kind of what happens when you give a program learning model strings from different formal

418
00:34:33,100 --> 00:34:41,180
languages. So think of like giving a general model just 10 or 20 maybe simple strings that

419
00:34:41,180 --> 00:34:47,180
obey some pattern and then asking it to find a program which can explain that data, which often

420
00:34:47,180 --> 00:34:54,940
means finding some way of kind of programmatically writing down the pattern in the strings. And

421
00:34:54,940 --> 00:35:01,820
in that figure, we have a paper which is really relevant to this point where the generalizations

422
00:35:01,820 --> 00:35:07,820
that that kind of model makes are I think kind of qualitatively like the ones you're describing

423
00:35:07,820 --> 00:35:13,900
for people, right? Where you can give them a small amount of data and it will predict unseen

424
00:35:13,900 --> 00:35:19,340
strings with very high probability, even though there's zero frequency in the training input,

425
00:35:19,340 --> 00:35:24,460
right? And the reason it does that is that often the most concise computational description of the

426
00:35:24,460 --> 00:35:32,780
data that you've seen is one that predicts some particular new unseen output. So that model is

427
00:35:32,780 --> 00:35:39,020
essentially an implementation of the kind of Chater and Vitani program learning idea that I

428
00:35:39,020 --> 00:35:44,060
brought up earlier. But it's one that I think, you know, if you think about in the context of

429
00:35:44,060 --> 00:35:49,820
these arguments of kids saying unusual or unexpected things, like that is predicted by all of these

430
00:35:49,820 --> 00:35:55,180
kinds of accounts, right? Because as long as these things are effectively comparing an interesting

431
00:35:55,180 --> 00:36:03,180
space of grammars, then they'll show that kind of behavior, I think. Yeah. So, okay. So I guess,

432
00:36:03,180 --> 00:36:11,260
you know, the argument would be that, at least from the gender perspective, syntax is functioning

433
00:36:11,260 --> 00:36:16,780
separately, but it still maps to semantics, it informs pragmatics, right? So in the minimalist

434
00:36:16,780 --> 00:36:21,740
program, syntax is obviously minimalist, it's very small, it's just a linearization and labeling,

435
00:36:21,740 --> 00:36:26,700
they're the two only operations, you have a linearization algorithm to central motor systems

436
00:36:26,700 --> 00:36:33,180
and some kind of categorization algorithm at the conceptual systems. So Chomsky's architecture

437
00:36:33,180 --> 00:36:37,500
is kind of reliant on the process of mapping syntax to semantics, right? It's form meaning

438
00:36:37,500 --> 00:36:43,020
regulation, it's not just structure, and it's not just meaning. So LMS don't really have this mapping

439
00:36:43,020 --> 00:36:47,420
process, right? Like, where's the mapping to semantics? And if there is a mapping, what does

440
00:36:47,420 --> 00:36:52,460
the mapping process look like? What are the properties of its semantics? What are the properties of

441
00:36:52,460 --> 00:36:56,460
the semantics placed on their own sets of constraints on the mapping process? Like,

442
00:36:56,460 --> 00:37:02,220
they do for natural language? Do these kind of constraints inform each other? Is they kind of

443
00:37:02,220 --> 00:37:08,220
a back and forth process? Like, LMS don't really seem to describe this form meaning pairing,

444
00:37:08,940 --> 00:37:16,700
which means which strings, for example, right? Sorry, are you saying that they don't have semantics

445
00:37:16,700 --> 00:37:22,380
at all? Or are you saying that there's just not a clear delineation between how the structures

446
00:37:22,380 --> 00:37:26,860
get mapped onto the semantics? Yeah, the latter, right? So they clearly have some, potentially

447
00:37:26,860 --> 00:37:30,620
some kind of semantics. I know you've argued for conceptual role theory being relevant here, right?

448
00:37:30,620 --> 00:37:34,140
The rest of it is maybe a little bit more mysterious, but the actual, so in linguistics

449
00:37:34,460 --> 00:37:39,020
there's a theory of the mapping process itself, it's explicit, and you can see it in action,

450
00:37:39,020 --> 00:37:42,540
and you can test different theories of it in psycholinguistic models and what have you.

451
00:37:42,540 --> 00:37:48,220
The actual regulation, the kind of constrained ambiguity, ambiguity in the sense of one word,

452
00:37:48,220 --> 00:37:51,980
multiple meanings, or one structure, multiple interpretations, etc, right?

453
00:37:53,340 --> 00:37:58,700
Yeah, I mean, if you think they have semantics, then I think they have to have a mapping from

454
00:37:58,700 --> 00:38:04,620
the syntax to the semantics. I agree, it's not as like, nobody really understands how they're

455
00:38:04,620 --> 00:38:11,820
working on any deep level, right? So I agree, it's not as clear as, say, in generative syntax and

456
00:38:11,820 --> 00:38:19,180
semantics, right, where you kind of write down the rules of composition and can derive a compositional

457
00:38:19,180 --> 00:38:24,620
meaning from a sentence from the component parts or something, right? Like, that's not how they're

458
00:38:24,620 --> 00:38:30,940
working, right? But I just, I wouldn't take for granted that it has to be like that. Like,

459
00:38:32,940 --> 00:38:38,460
it could be that how they're working is actually how we work, right? That everything is represented

460
00:38:38,460 --> 00:38:45,100
in some high-dimensional vector space, and there's some complicated way in which that vector semantics

461
00:38:45,100 --> 00:38:53,500
gets updated with each additional word or whatever in a linguistic stream. But like, I think it's

462
00:38:53,500 --> 00:38:58,540
clear that they have some kind of representation of the semantics of a sentence, right? Like,

463
00:38:58,540 --> 00:39:03,420
they can answer questions, for example, at least approximately. I mean, it's not perfect, but

464
00:39:04,380 --> 00:39:11,260
it's not like a n-gram model or something, right? Which really doesn't have semantics. So I think

465
00:39:11,260 --> 00:39:20,620
that they're definitely representing semantics and, you know, updating that as they process

466
00:39:20,620 --> 00:39:26,140
language, it just happens not to look like these other formal theories. And I guess, I don't see

467
00:39:26,140 --> 00:39:29,900
why that's a problem, right? Like, those other formal theories could just be, you know, poor

468
00:39:29,900 --> 00:39:35,420
approximations or just totally wrong, right? Yeah, yeah, no, no, totally, totally. I mean,

469
00:39:35,420 --> 00:39:39,580
there's also ways in which some of the formal formal theories in semantics are already

470
00:39:39,580 --> 00:39:43,180
potentially compatible. We've got some of these things are doing, right? So another way to think

471
00:39:43,180 --> 00:39:48,940
about this is, you know, LMS are, well, LMS are compression algorithms, but natural language

472
00:39:48,940 --> 00:39:54,460
understanding is kind of more about decompression. It's disambiguating, meaning x, out of meanings,

473
00:39:54,460 --> 00:39:58,860
x, y, z. It's all about making inferences about, you know, meta relations between concepts that

474
00:39:58,860 --> 00:40:04,060
are not in the training data. So some examples that Millie Mitchell gives are things like on top of,

475
00:40:04,060 --> 00:40:09,500
you know, she's on top of a game, it's on top of the box, all of these kind of vary with context.

476
00:40:09,500 --> 00:40:13,180
So there's a lot of other things that are going on, right? And I think you discussed some of the

477
00:40:13,260 --> 00:40:19,500
examples on your paper. So, you know, but the fact that the language is still not, at least,

478
00:40:19,500 --> 00:40:24,860
again, under this theory of language, it's not about string generation. It's about this form,

479
00:40:24,860 --> 00:40:29,260
meaning, pairing machine. So some semantics in the generative tradition, even think all the

480
00:40:29,260 --> 00:40:34,620
rest of semantics is just and, right? So both Rasky's conjunctivist theories semantics is that

481
00:40:34,620 --> 00:40:40,780
human semantics is just and that's it. Which, again, is very simple, elegant. It's, it's,

482
00:40:40,780 --> 00:40:46,300
it's interpretable. It's compatible with other things that, you know, are maybe going on in your

483
00:40:46,300 --> 00:40:50,300
neck of the woods, right? But regardless, it's still, you know, natural language is still more

484
00:40:50,300 --> 00:40:55,660
compositional than things like, you know, formal languages just to make a clear distinction that's

485
00:40:55,660 --> 00:41:00,460
been made. They have a much richer compositional structure. There's more stuff going on, maybe.

486
00:41:00,460 --> 00:41:03,980
So it's important that before that, you know, things like attention-based machine mechanisms

487
00:41:03,980 --> 00:41:09,740
and transformers allow for combinations of discrete token bindings, which is more

488
00:41:09,740 --> 00:41:13,660
approximate to a merge-like operator than simple recurrent matrix multiplication.

489
00:41:14,860 --> 00:41:18,220
But, you know, the issue of binary branching, binary branching of merge, just to choose,

490
00:41:18,220 --> 00:41:22,940
for example, here to talk about the four meaning regulation, one principle. Binary branching in

491
00:41:22,940 --> 00:41:27,580
merge is an interesting question, but Gem2 grammar has always been open to different origins and

492
00:41:27,580 --> 00:41:32,220
locations of this apparent constraint in syntactic computation. Like, where does it come from?

493
00:41:32,220 --> 00:41:36,780
Maybe it's a condition on merge. Maybe it's imposed by a smooth system. Maybe it's a kind of prior,

494
00:41:36,780 --> 00:41:41,900
you know, who knows. And in fact, some more recent work in Gem2 grammar has tried to ground

495
00:41:42,860 --> 00:41:47,500
do away with a lot of the set theoretic assumptions of merge, right? Maybe set theory isn't the best

496
00:41:47,500 --> 00:41:51,980
way to model the Gem2 grammar. Maybe more logical accounts are more appropriate. There's lots of

497
00:41:51,980 --> 00:41:57,500
other recent ideas there, which are all compatible with the, with Chomsky's approach, right? In fact,

498
00:41:57,500 --> 00:42:00,860
one of the things that Chomsky likes the most is when he's, when he's proven wrong, right? A lot of

499
00:42:00,860 --> 00:42:06,620
these theories are going against the core mainstream minimalist architecture. But yeah,

500
00:42:06,620 --> 00:42:12,940
I think it's a very diverse, like, vibrant field. The people who are Adjah, Hornstein, you know,

501
00:42:12,940 --> 00:42:19,340
Petrosky, Haji Borre, they disagree in fundamental ways with a lot of what the mainstream of

502
00:42:19,340 --> 00:42:24,220
Gem2 grammar would say, but there's still more scope for disagreement. But it's still compatible

503
00:42:24,220 --> 00:42:28,460
with setting core assumptions, right? So a lot of David Adjah's work, for example, kind of deviates

504
00:42:28,460 --> 00:42:33,580
in this core respect, but it's still trying to ground these intuitions in different formal systems.

505
00:42:34,620 --> 00:42:41,500
So, you know, it's kind of, I want to get your thoughts again on, I mentioned Mitchell, right?

506
00:42:41,500 --> 00:42:47,340
So Mitchell and Bowers 2020, they have this paper, priorless recurrent networks laying curiously,

507
00:42:47,340 --> 00:42:50,780
but I think you might be aware of, right? So this is a really good example just to kind of get to

508
00:42:50,780 --> 00:42:54,940
the heart of the issue. So recurrent neural networks have been shown to accurately model,

509
00:42:54,940 --> 00:42:58,860
you know, non-veb number agreement, but Mitchell and Bowers show that these networks will also

510
00:42:58,860 --> 00:43:03,420
learn a number agreement with unnatural sentence structures. So structures that are not found

511
00:43:03,420 --> 00:43:07,740
in natural language, and which humans have a hard time processing, right? So the mode of learning

512
00:43:07,740 --> 00:43:14,220
for RNNs is, at least for RNNs, qualitatively distinct from infant, you know, infant homo sapiens,

513
00:43:14,220 --> 00:43:19,180
right? So the story is Mitchell and Bowers show that while their LSTM model has a good representation

514
00:43:19,180 --> 00:43:24,060
of singular variances, plural for individual sentences, there's no generalization going on,

515
00:43:24,060 --> 00:43:27,820
right? They can represent at the individual level. So the model doesn't have a representation of

516
00:43:27,820 --> 00:43:32,940
number as an abstraction. What number is? Only concrete instances of singular versus plural.

517
00:43:33,900 --> 00:43:40,060
So successfully predicting language behavior via LM, or successfully predicting neural responses

518
00:43:40,060 --> 00:43:43,340
in a similar way is obviously great. And maybe we can get into that issue later,

519
00:43:43,340 --> 00:43:47,100
but there's only one side of the coin here, right? The other side of the coin is explaining why this

520
00:43:47,100 --> 00:43:51,100
type of behavior and not some other behavior, why this structure and not some other, and that's

521
00:43:51,100 --> 00:43:58,620
maybe Chomsky's most important point, really, why this and not some other system. So linguistic

522
00:43:58,620 --> 00:44:02,620
theory kind of gives you that other side of the coin, right? Whereas LM's really don't. So the

523
00:44:02,620 --> 00:44:09,100
Mitchell and Bowers paper does something that- He does it! Well, yeah, so like, take Yael LaCrette's

524
00:44:09,100 --> 00:44:13,900
and Stanislas de Haines' work from 2019, right? They looked at number agreement in an LSTM and

525
00:44:13,900 --> 00:44:18,700
found two specialized units that encoded number agreement, but the overall contribution to performance

526
00:44:18,700 --> 00:44:24,540
was low. And then in 2021, Yael LaCrette's have this paper where they show that in the neural

527
00:44:24,540 --> 00:44:30,060
language model, it did not achieve genuine recursive processing of nested long range agreement,

528
00:44:30,060 --> 00:44:35,100
gender marking in Italian, I think, even if some hierarchical processing, you know, was achieved,

529
00:44:35,100 --> 00:44:39,660
as you've argued before, right? Some hierarchy was there, it was there. But the question is,

530
00:44:39,660 --> 00:44:44,140
is it the right mapping? Is it the right kind of hierarchy? They found that LSTM based models could

531
00:44:44,140 --> 00:44:48,780
learn subject-verb agreement over short spans, one degree of embedding, but they failed at some

532
00:44:48,780 --> 00:44:55,660
longer dependencies. And in their most recent paper, LaCrette set out with De Haines showed that

533
00:44:55,660 --> 00:45:01,580
they evaluated modern transformer LM's, including GPT2 XL, on the same task. And the transformers

534
00:45:01,580 --> 00:45:06,220
performed more similarly to humans than LSTMs did and performed above transfer overall, but they

535
00:45:06,220 --> 00:45:10,540
still performed below chance in one key condition, which is the, as I mentioned, the multiple embedding

536
00:45:10,540 --> 00:45:15,020
one, the difficult structures. So the reason why I mentioned these studies is because, you know,

537
00:45:15,820 --> 00:45:20,700
it's not just to explore the limits of LM's, which is an interesting question. But consider work by

538
00:45:20,700 --> 00:45:27,020
people like Neil Smith at UCL, right? He did work in the 90s with a polyglot, savant, and neurotypical

539
00:45:27,020 --> 00:45:31,900
controls comparing them. So he investigated second language learning of an artificial language

540
00:45:31,900 --> 00:45:35,740
containing both natural and unnatural ground structures, like the Michelin Bowers paper,

541
00:45:35,740 --> 00:45:39,500
right? The whole framework is natural versus unnatural. And they found that while both the

542
00:45:39,500 --> 00:45:44,860
savant, Christopher, the savant, and the controls could master the linguistically natural aspects,

543
00:45:44,860 --> 00:45:48,780
only the controls could eventually handle the structure dependent unnatural phenomena,

544
00:45:48,780 --> 00:45:53,100
and neither of them could master the structure independent aspects. So some weird rules where

545
00:45:53,100 --> 00:45:56,540
it's like, you know, you mark the emphasis on the third word of the sentence, things like that.

546
00:45:56,540 --> 00:46:01,660
So they argue that Christopher's abilities are entirely due to his intact linguistic faculties,

547
00:46:01,660 --> 00:46:06,860
but the controls could employ more domain general kind of cognitive resources, like, you know,

548
00:46:06,940 --> 00:46:10,940
tension control, etc., which is why they could deal with those difficult processes.

549
00:46:11,580 --> 00:46:17,340
But I just mentioned, you know, a minute ago, that the LSTM in the Michelin Bowers paper approaches

550
00:46:17,340 --> 00:46:22,140
natural and unnatural structures in pretty much the same way. So it's not, you know, it's not a

551
00:46:22,140 --> 00:46:27,420
psychologically plausible model, I would argue, for whatever humans are doing. And similar observations

552
00:46:27,420 --> 00:46:31,900
can apply to the limits of transformer models in Le Creta's work. And all of these themes are like,

553
00:46:31,900 --> 00:46:35,980
right up there, they're saying that there's all the way to the present. So another one of

554
00:46:35,980 --> 00:46:40,140
Tal Linsen's recent papers that he posted a few weeks ago, looking at child directed speech,

555
00:46:40,140 --> 00:46:45,420
showed that LSTMs and transformers limited to ecologically plausible amounts of data

556
00:46:45,420 --> 00:46:49,180
generalize, as I mentioned, the linear rules for English, right, rather than the abstract rules.

557
00:46:49,740 --> 00:46:55,260
And in fact, more recent work from Linsen's lab last week, looking at, well, last year, I should say,

558
00:46:55,260 --> 00:47:00,700
shows that looking at garden paths, surprise does not explain syntactic disambiguation

559
00:47:00,700 --> 00:47:05,180
difficulty, right? Surprise will underpredicts the size of the garden path effect across all

560
00:47:05,180 --> 00:47:08,540
constructions. And this gets to this issue that you mentioned before, you know, maybe surprise

561
00:47:08,540 --> 00:47:12,380
all this related to some aspects of syntax, but maybe not other ones, it's kind of a,

562
00:47:12,380 --> 00:47:16,220
it's a very nontrivial issue that is very much, it's open to discussion. It's not,

563
00:47:16,220 --> 00:47:20,620
it hasn't settled yet. But so Linsen showed that garden path effects are just way more

564
00:47:20,620 --> 00:47:24,780
difficult than you would expect from mere unpredictability. So another way of phrasing

565
00:47:24,780 --> 00:47:30,860
this argument is to quote a recent argument with Chomsky's to get at this natural basis,

566
00:47:30,860 --> 00:47:34,780
unnatural issue. He says, suppose we have an expanded periodic table that includes

567
00:47:34,780 --> 00:47:39,980
all the elements that do exist, all the elements that can possibly exist, and all the elements

568
00:47:39,980 --> 00:47:45,020
that cannot possibly exist. And let's say you have some model, some artificial model that fails

569
00:47:45,020 --> 00:47:49,500
to distinguish between these three categories, whatever this model is doing, it's not helping

570
00:47:49,500 --> 00:47:53,660
those understand chemistry, right? It's doing something else. It's doing something for sure,

571
00:47:53,660 --> 00:47:57,260
but whether or not it's helping those understand chemistry is something separate. And I know that

572
00:47:57,260 --> 00:48:00,700
you've said in response to some of these studies, I think you've said that, you know,

573
00:48:01,660 --> 00:48:05,180
in order to show that something is likely to be impossible, somewhere in your paper, I think you

574
00:48:05,180 --> 00:48:11,820
say, in order to show that something is impossible with normal bounds and false positives, you'd

575
00:48:11,820 --> 00:48:15,980
need to show, you need to look at something like 500 independently sampled languages. So you cite

576
00:48:15,980 --> 00:48:20,220
this in your paper, right? Which you probably can't do, that's just not, it's not a feasible thing to

577
00:48:20,220 --> 00:48:26,220
do. So, you know, I'm not too sure that this really refutes the principle argument that I'm

578
00:48:26,220 --> 00:48:30,700
making here, right? Because people like Mitchell and Bowers are making an argument about impossibility

579
00:48:30,700 --> 00:48:34,700
in principle, not in some kind of extensional sense, you know, just like searching across the

580
00:48:34,700 --> 00:48:39,420
world languages to see, to prove across every single language that it is impossible, right?

581
00:48:39,420 --> 00:48:43,660
That's kind of, it's a different argument, whether it's impossible in some random language in the

582
00:48:43,660 --> 00:48:48,060
Amazon, compared to actually impossible, based on the principles of what the language system is

583
00:48:48,060 --> 00:48:51,820
actually doing, like what it can do. So I would just say that, you know, all of these kind of

584
00:48:52,060 --> 00:48:58,140
I think that that that point is, is that you don't actually know what is typologically not

585
00:48:58,140 --> 00:49:03,020
possible, right? So people like to say things like, you know, there's no language that does X,

586
00:49:03,020 --> 00:49:09,020
therefore we have to build that restriction into our statistical models. But if it's not

587
00:49:09,020 --> 00:49:13,900
statistically justified that there is no language that does X, right? If you've only looked at

588
00:49:13,900 --> 00:49:17,740
20, 20 European languages or something, right? I mean, it's, it's not

589
00:49:18,300 --> 00:49:25,980
like that shouldn't motivate doing anything to the models, right? If it's, if it's not a

590
00:49:25,980 --> 00:49:33,420
statistically justified universal, I think. Well, you know, I think, you're totally right,

591
00:49:33,420 --> 00:49:36,940
but that just applies more generally to the social sciences and psychological sciences,

592
00:49:36,940 --> 00:49:40,940
right? Like typologically, it's very difficult to establish these things, right? So I guess

593
00:49:41,020 --> 00:49:45,900
you're, you're, I guess you're just kind of steelman you're a bit, you're saying that the strong

594
00:49:45,900 --> 00:49:51,500
claim is very difficult to prove, right? Like the reason the language that has X.

595
00:49:52,220 --> 00:49:56,860
The strong claim that something is not allowed in, in natural languages, I think very, very

596
00:49:56,860 --> 00:50:06,540
difficult to prove. And, you know, I think that there have been lots of, you know, strong attempts,

597
00:50:06,620 --> 00:50:13,020
there's been lots of strong claims from, often from, from generative syntax, right,

598
00:50:13,740 --> 00:50:20,700
about what all languages do. And I think that, you know, people have been very good at finding

599
00:50:20,700 --> 00:50:25,500
kind of counter examples to a lot of those things. I cite this paper by Evans and Levinson,

600
00:50:26,540 --> 00:50:31,420
which actually, you know, I had heard for years about how no language does X and that's what

601
00:50:31,420 --> 00:50:35,660
we're using to construct our theories. And that Evans and Levins paper, Evans and Levinson paper

602
00:50:35,660 --> 00:50:41,340
really kind of changed my mind about this, right? That like language is actually much more

603
00:50:42,140 --> 00:50:48,940
diverse than, than I think most, most syntacticians will, you know, try to construct theories for

604
00:50:48,940 --> 00:50:54,940
something. So, you know, I think we, going back to kind of the beginning of what you said, I think

605
00:50:55,180 --> 00:51:01,020
we, we'd agree that, that you need language architectures which learn the things that kids

606
00:51:01,020 --> 00:51:06,620
learn and learn it from data that they learn. And those architectures might, might be unlikely to

607
00:51:06,620 --> 00:51:12,780
be things like LSTMs or, you know, simple recurrent networks or, or whatever, right? Like, I think

608
00:51:12,780 --> 00:51:17,580
all of that work is, is very useful in, in kind of honing in on the right architecture.

609
00:51:18,300 --> 00:51:25,820
So, I'm just trying to, to remember all of, all of the points you were making. Oh, yeah. So,

610
00:51:26,620 --> 00:51:33,420
but I think this, that there, there's a kind of flip side to this, which is that I think that

611
00:51:33,420 --> 00:51:39,420
the space of things people can learn is actually kind of underestimated, right? Like, there's this

612
00:51:39,420 --> 00:51:46,300
bias to, to, to say, you know, people can't learn X, Y and Z. But people, at least outside of language

613
00:51:46,300 --> 00:51:50,940
have this, this really remarkable ability to learn different kinds of patterns, right? Like,

614
00:51:50,940 --> 00:51:57,980
the patterns you find in, in music or mathematics, for example, we can learn sophisticated types of,

615
00:51:57,980 --> 00:52:04,220
of algorithms, right? We can learn to, you know, fly a space shuttle or to, you know, tie knots in,

616
00:52:04,220 --> 00:52:09,420
for rock climbing or whatever, right? Like, there's all kinds of kind of procedural and

617
00:52:09,420 --> 00:52:14,860
algorithmic knowledge, which is structural that, that people are able to acquire. And I think that,

618
00:52:14,860 --> 00:52:21,740
that that notion very rightly kind of motivates looking for learning systems, which can work

619
00:52:21,740 --> 00:52:29,340
over pretty unrestricted spaces, right? So, you know, you, you, you might say that, okay, well,

620
00:52:29,340 --> 00:52:34,780
language is different because language is a restricted space. And it might be true that,

621
00:52:34,780 --> 00:52:38,380
that language is restricted, but it also might be true that the things we see in language come

622
00:52:38,380 --> 00:52:44,140
from other sources, right? It could be that languages, especially pragmatic, for example,

623
00:52:44,140 --> 00:52:49,180
compared to music or mathematics, right? And those kinds of pragmatic constraints

624
00:52:49,980 --> 00:52:53,900
are the things that constrain the, the form of language, right? Or language is communicative,

625
00:52:53,900 --> 00:52:58,540
it's probably more communicative than, than music, for example. And that might constrain the, the,

626
00:52:58,540 --> 00:53:03,500
the form of things. So, I mean, as, as you know, this is very old debate in, in linguistics about

627
00:53:03,500 --> 00:53:10,620
kind of where the, where the properties of, of natural language come from. And I guess what I'm

628
00:53:10,620 --> 00:53:15,820
trying to say is that there's one kind of perspective where you look at all of the things humans can do

629
00:53:15,820 --> 00:53:21,260
even outside of language, all of the rich structures and algorithms and processes we're able to learn

630
00:53:21,260 --> 00:53:26,460
about and internalize. And you say, okay, maybe language is like that. And then yes, language

631
00:53:26,460 --> 00:53:31,020
also has some of these other funny little properties. But, you know, maybe those come

632
00:53:31,020 --> 00:53:37,020
from some other, other pieces of, of where language comes from, right? It's, you know,

633
00:53:37,020 --> 00:53:43,260
we have pretty sophisticated pragmatic reasoning. We're using it to achieve certain communicative

634
00:53:43,260 --> 00:53:49,580
ends. You can find all kinds of kind of communicative features within the, the language system itself.

635
00:53:49,580 --> 00:53:54,700
And so, so maybe some of these other properties are, are properties that have some other origin.

636
00:53:55,580 --> 00:54:01,340
And that, that view, I think could be wrong, but it's, it's one that I think needs to be looked at

637
00:54:01,340 --> 00:54:11,100
to see if it's wrong, right? Like, I think it's been kind of dismissed by large chunks of, of

638
00:54:11,100 --> 00:54:15,180
linguists, right? Just, you know, I've heard people say stuff like, oh, well, communication

639
00:54:15,180 --> 00:54:19,980
doesn't really explain anything about language, right? And what they mean often is it doesn't

640
00:54:19,980 --> 00:54:24,780
explain like the particular island constraints or something that they're, that they're working on,

641
00:54:24,780 --> 00:54:28,620
right? But there's all kinds of other things in language that communicative pressures probably

642
00:54:28,620 --> 00:54:36,220
do explain. So I guess my, my pitch is always for, for kind of breadth in term, breadth in

643
00:54:36,220 --> 00:54:42,460
consideration of the forces that, that can shape language and not needing to put it all into some

644
00:54:42,460 --> 00:54:45,020
form of, of innate constraints or something like that.

645
00:54:45,020 --> 00:54:48,460
No, no, totally. And I think, I think a lot of that stuff is, is, is compatible with, with, with

646
00:54:48,460 --> 00:54:52,860
the minimalist program, because the minimalist program wants syntax to be minimal. It doesn't

647
00:54:52,860 --> 00:54:56,060
want it to be complicated. It doesn't want it to be, you know, any more complicated than it has to

648
00:54:56,060 --> 00:54:59,740
be. So there were some, you mentioned the curious properties, right? So there were some of the

649
00:54:59,740 --> 00:55:03,980
properties that need to be counted for in any model of language that are, I'll give you one

650
00:55:03,980 --> 00:55:09,020
example, right? The setting of Pearson features. And these Pearson features exhibit very non,

651
00:55:09,020 --> 00:55:13,180
non trivial generalizations that do not seem to be counted for via domain general learning

652
00:55:13,260 --> 00:55:17,500
mechanisms. So I'm citing here the work of Daniel Harbour at Queen Mary. So for example,

653
00:55:17,500 --> 00:55:22,220
the morphological composition of Pearson, its interaction of number, its connection to space,

654
00:55:22,860 --> 00:55:27,100
properties of its semantics and its linearization, they all appear to be strong candidates for our

655
00:55:27,100 --> 00:55:30,460
knowledge of language, right? What we mean by knowledge of language. But on the other hand,

656
00:55:30,460 --> 00:55:35,340
we have things like case and agreement and head movement. And these are all structural phenomena.

657
00:55:35,900 --> 00:55:43,180
However, they seem to resist a purely meaning based explanation in theoretical linguistics,

658
00:55:43,180 --> 00:55:46,940
right? It would be great if syntax were nothing but a computational engine

659
00:55:46,940 --> 00:55:51,260
that builds structured meaning. And that's the minimalist program, the goal. But that's not

660
00:55:51,260 --> 00:55:55,820
what we actually find. That's not in any actual minimalist, like concrete model, any concrete

661
00:55:55,820 --> 00:56:01,100
minimalist theory. The goal is just like, the program is language is perfect. Okay, that's the

662
00:56:01,100 --> 00:56:05,820
program. Is that what we find? No, obviously not. Okay, no, no linguist actually believes that.

663
00:56:06,860 --> 00:56:11,740
So it'd be great if syntax was like that. But I think, you know, the program is to look for

664
00:56:11,740 --> 00:56:16,860
perfection, but not always find it. So case and agreement and head movement are morphological,

665
00:56:16,860 --> 00:56:20,780
morphophonological phenomena, the properties of the performance systems, what's called

666
00:56:20,780 --> 00:56:24,380
performance systems. And so the minimalist program itself is really compatible with a lot

667
00:56:24,380 --> 00:56:28,220
of what you're saying about, you know, language, language, there are aspects of language that

668
00:56:28,220 --> 00:56:34,380
can be perfected and optimized for communicative efficiency. Absolutely. Totally. No doubt about

669
00:56:34,380 --> 00:56:39,820
it. But where is that locus of efficiency? Is it in the syntax itself? Or is it some kind of

670
00:56:39,820 --> 00:56:44,220
extra linguistic system? Is it in pragmatics? You know, is it in century motor? Is it in the

671
00:56:44,220 --> 00:56:49,980
speech? And property of speech and phonology? Probably, you know, I mean, who knows. But

672
00:56:49,980 --> 00:56:56,860
I think all of these things demand much more, you know, serious consideration into old fashioned

673
00:56:56,860 --> 00:57:00,620
notions like structure dependence, compositionality and what have you, things like that, which

674
00:57:00,620 --> 00:57:05,820
you can maybe find somewhere in the literature, but even just basic topics like, you know,

675
00:57:06,780 --> 00:57:13,100
quantifier raising, extended projections, adverbial hierarchies, all of these things

676
00:57:13,100 --> 00:57:18,220
in the minimalist program can be extra linguistic, right? They can actually be outside of syntax and

677
00:57:19,340 --> 00:57:24,140
very queer properties of the semantic conceptual systems, which are in themselves kind of domain

678
00:57:24,140 --> 00:57:29,180
general, weird leftovers from ancient primate cognition, right? The features of the way we

679
00:57:29,180 --> 00:57:33,260
pass events, the way we pass, you know, agents and patients, things like that. That's definitely not,

680
00:57:33,260 --> 00:57:38,380
that's not human specific. But, you know, the way that syntax provides instructions to these

681
00:57:38,380 --> 00:57:44,460
systems, you know, probably seems to be. So, you know, generative linguists have different theories of

682
00:57:44,460 --> 00:57:48,140
also language production too. I'll just talk about language production based on whether we

683
00:57:48,140 --> 00:57:52,220
store lemmas or whether we build words in the exact same way we will phrase and sentence this.

684
00:57:52,220 --> 00:57:55,660
So, I know that you make distinction between construction grammar and kind of generative

685
00:57:55,660 --> 00:58:00,220
grammar and, you know, the weight they place on memorizing constructions versus just building

686
00:58:00,220 --> 00:58:05,180
things from the bottom up, from the ground up, right? And so, you know, in some generative inspired

687
00:58:05,180 --> 00:58:09,900
models, mechanisms which generate syntactic structure make no distinctions between processes

688
00:58:09,900 --> 00:58:15,420
that apply above or below the word level. There's no point at which meaning syntax and form are

689
00:58:15,420 --> 00:58:20,060
all stored together as single atomic representations. Each stage in lexical access is a transition

690
00:58:20,060 --> 00:58:24,540
between different kinds of data structures, right? There's meaning, there's form and there's

691
00:58:24,540 --> 00:58:28,860
syntax. These three features kind of come in together and they don't always overlap. Different

692
00:58:28,860 --> 00:58:35,420
languages realize them in different ways. And so, you know, a word, the basic definition of a word

693
00:58:35,420 --> 00:58:41,340
is just this weird multi-system definition where lots of things, lots of different cognitive systems

694
00:58:41,340 --> 00:58:46,380
enrich the basis of every lexical item, right? You have, there's nothing like this really,

695
00:58:46,380 --> 00:58:51,100
this enrichment process anywhere else in linguistic theory, right? Or at least in

696
00:58:51,100 --> 00:58:58,060
what LLMs are doing. Like, so I guess, what, I guess I would ask you, what is your definition

697
00:58:58,060 --> 00:59:03,740
of a word, right? And what can LLMs really provide insights into weirdhood, right? Because if you

698
00:59:03,740 --> 00:59:07,500
can't, if you don't have a definition of what a word is, then you're really in trouble, right?

699
00:59:07,500 --> 00:59:12,860
Like, we have to at least use LLMs or artificial systems to inform what we mean by a word. Or

700
00:59:12,860 --> 00:59:17,740
maybe we don't need that anymore. I'm not sure what you think. I'm not sure what you mean. I mean,

701
00:59:20,700 --> 00:59:26,620
I don't have a... What is a word? Why does that matter? I mean, that's just a convention about

702
00:59:26,620 --> 00:59:32,300
how we use the term word, right? What, like, I mean, you could use, you know, lemmas or word

703
00:59:32,300 --> 00:59:37,580
firms or whatever. Like, that just feels like a conventional choice. I'm not sure what's at,

704
00:59:37,580 --> 00:59:43,500
what's at stake there. So how would you, I guess I would say, I agree, word is a conventionalization,

705
00:59:43,500 --> 00:59:49,740
you know. Our intuitive concept of word is often biased by orthography, the way we put spaces between

706
00:59:49,740 --> 00:59:54,300
things, right? So I agree with that criticism. You know, word in the intuitive sense is not really

707
00:59:54,300 --> 00:59:59,340
a scientific construct. However, I guess, let me rephrase my question. How would you, you know,

708
00:59:59,340 --> 01:00:03,500
decompose the intuitive concept of word into something that is more kind of, you know,

709
01:00:03,580 --> 01:00:07,420
scientifically amenable or psychologically plausible, which is exactly what genitive

710
01:00:07,420 --> 01:00:11,340
grammar tries to do by decomposing words into, you know, distinctive features,

711
01:00:11,900 --> 01:00:16,860
morphological categories, conceptual roots being matched with categorical features,

712
01:00:16,860 --> 01:00:22,380
you know, you get a concept, you know, and you match it with a noun or a verb category to get a noun

713
01:00:22,380 --> 01:00:27,740
or a verb. These different models make different predictions, right? Yeah, I mean, I think that

714
01:00:27,740 --> 01:00:32,540
general idea is likely to be right for large language models. Like, I think they kind of

715
01:00:32,540 --> 01:00:37,660
must have things that are kind of like part of speech categories, for example. And I think that

716
01:00:37,660 --> 01:00:45,500
they kind of must be able to update those, their categories based on the language that they've seen

717
01:00:45,500 --> 01:00:52,300
so far, right? So like, like, you know, GPT puts nouns and verbs in the right places. And to do

718
01:00:52,300 --> 01:00:57,580
that, you kind of need some representation of the nouns versus the verbs, and you need some ability to

719
01:00:58,380 --> 01:01:03,260
locate yourself in a string of other words and figure out if there's likely to be a noun or a

720
01:01:03,260 --> 01:01:10,620
verb next. So I think that on that level, those kinds of properties of words are very likely to

721
01:01:10,620 --> 01:01:17,420
be right. And there are also things which are very likely to be found kind of in the internal

722
01:01:17,420 --> 01:01:22,460
representations of these models. I don't see how it could be any other way other than that.

723
01:01:23,100 --> 01:01:32,140
But like, as far as I know, that's not where the main debates or disagreement, I think, is,

724
01:01:32,140 --> 01:01:39,580
right? Like, I think all theories of language have to have to say that there's different kinds of

725
01:01:39,580 --> 01:01:44,780
words that can show up in different places or something like that. Yeah. Okay, so how about

726
01:01:44,780 --> 01:01:50,620
the issue? You mentioned communication, right? So, you know, and you're totally right, when Trump

727
01:01:51,580 --> 01:01:55,580
says things like language is a thought system or, you know, language didn't evolve,

728
01:01:56,380 --> 01:01:59,660
he's kind of being a little bit cheeky. He doesn't really mean that. He kind of means it in a very

729
01:01:59,660 --> 01:02:04,620
specific sense, right? But, you know, when we say language is a thought system, what we mean is

730
01:02:05,740 --> 01:02:09,420
we're trying to get it an architectural claim. So if you look at the architecture of the minimalist

731
01:02:09,420 --> 01:02:15,180
program, the syntactic derivation and the conceptual systems are literally different systems, right?

732
01:02:15,180 --> 01:02:19,340
The conceptual systems take stuff from syntax and then does its own business with it and the

733
01:02:19,340 --> 01:02:24,140
CI systems have their own peculiar rules and principles, which is why thought and language

734
01:02:24,140 --> 01:02:29,500
are both similar symbolic compositional systems, but in different ways. Only a subset of thought

735
01:02:29,500 --> 01:02:35,580
is properly called the CI interface system, since the CI systems are by definition, you know,

736
01:02:35,580 --> 01:02:40,460
whatever conceptual systems you would have that can access and read out instructions from syntax.

737
01:02:40,460 --> 01:02:43,740
And we don't know what they are fully. They seem to have something to do with events and

738
01:02:43,740 --> 01:02:47,580
grammatical reference and definiteness. They seem to be the main categories that language,

739
01:02:47,580 --> 01:02:51,980
you know, cares about conceptually, but we don't really know. That's kind of just a hypothesis,

740
01:02:51,980 --> 01:02:58,060
right? But what we do know is that they don't seem to make use of color all that much. So no

741
01:02:58,060 --> 01:03:05,580
language morphologically marks shades of color. All the conceptual features like worry or concern,

742
01:03:05,580 --> 01:03:09,180
like no language morphologically marks a degree of worry or concern about an issue,

743
01:03:09,180 --> 01:03:15,180
but we do make use of epistemological notions like evidentiality and things like that.

744
01:03:15,180 --> 01:03:19,740
So, you know, I guess what I'm saying is the minimalist program does a good job of

745
01:03:20,460 --> 01:03:24,540
trying to figure out which aspects of thought language is intimately tied to,

746
01:03:24,540 --> 01:03:29,020
and which aspects of thought it's not tied to. So the minimalist program allows us to kind of carve

747
01:03:29,020 --> 01:03:33,580
that up quite neatly. And this is a much more nuanced framework than, you know, when Chomsky

748
01:03:33,580 --> 01:03:38,060
says language is thought, again, he doesn't, maybe he means it, maybe he doesn't, but that's not what

749
01:03:38,060 --> 01:03:43,100
the actual architecture of his theory says. It's a rhetorical device that is very, you know, useful

750
01:03:43,100 --> 01:03:48,860
and interesting to attract undergraduate audiences. But if you look at actual theories that are coming

751
01:03:48,860 --> 01:03:53,020
out of the minimalist program, no one really believes language equals thought, right? The language

752
01:03:53,020 --> 01:03:57,900
system seems to, it tries its best to access and reformat and manipulate various conceptual

753
01:03:57,900 --> 01:04:02,220
systems, but it has its limits, right? We know what systems, spell keys, core knowledge systems are

754
01:04:02,220 --> 01:04:08,860
hooked up to with respect to the syntax engine, and which ones are not. So, you know, this kind of

755
01:04:08,860 --> 01:04:13,820
gets back to the idea that lexicalization of a concept seems to maybe alter it in some way.

756
01:04:13,820 --> 01:04:18,300
It kind of imbues it with elements that are not there in the concept itself. So if you lexicalize

757
01:04:18,300 --> 01:04:22,220
the concept, you suddenly transform it a little bit, you give it a little extra, you sprinkle

758
01:04:22,220 --> 01:04:27,100
something else on top of it, and that seems to vary across different noun types. But these are all

759
01:04:27,100 --> 01:04:34,700
like very clear architectural claims within gem diagram that make very clear empirical predictions.

760
01:04:34,700 --> 01:04:38,860
So in other words, I guess what I'm saying is all these neuropsychology studies that are

761
01:04:38,860 --> 01:04:44,540
coincided, you know, in a lot of work in this fame, what does it really show? I think it shows

762
01:04:44,540 --> 01:04:49,900
that, you know, when language is damaged in the brain, it loses its particular sway or mode of

763
01:04:49,900 --> 01:04:54,860
influencing those systems. But there's no real prediction from within the gem to grammar enterprise

764
01:04:54,860 --> 01:04:59,020
that those non-linguistic systems should be impaired or should suddenly, you know, shut down

765
01:04:59,020 --> 01:05:03,420
if the core language system is compromised, right? In fact, if anything, that just

766
01:05:04,140 --> 01:05:09,580
emphasizes the principal divorce between the syntactic system and non-linguistic systems,

767
01:05:09,580 --> 01:05:13,420
right? So I think the, a lot of predictions here from the language and communication,

768
01:05:14,140 --> 01:05:17,340
you know, literature are kind of missing the point of the architectural claims.

769
01:05:19,340 --> 01:05:21,660
I can just give, or Daniel, do you want to go?

770
01:05:22,220 --> 01:05:22,860
Yeah, go ahead.

771
01:05:22,860 --> 01:05:27,740
Give a little bit of background there. So there's these papers from

772
01:05:28,780 --> 01:05:38,220
Ev Fedorenko and Rosemary Varley that are examining in part of them aphasic patients. So

773
01:05:38,220 --> 01:05:45,260
people who have impaired linguistic abilities, basically showing that with impaired linguistic

774
01:05:45,260 --> 01:05:53,020
abilities, you can still have preserved kind of reasoning abilities. So people like chess masters,

775
01:05:53,020 --> 01:06:00,700
chess grandmasters, for example, who are obviously very good at reasoning might not have kind of

776
01:06:00,700 --> 01:06:06,220
intact linguistic abilities. And then complimenting that kind of patient work, there's also work from

777
01:06:06,220 --> 01:06:15,660
Ed's lab showing that the parts of the brain that care about language are separable from

778
01:06:15,660 --> 01:06:20,060
the parts of the brain that care about other domains, even ones that seem kind of language-like.

779
01:06:20,060 --> 01:06:26,460
So things like music and mathematics tend not to happen in the language areas.

780
01:06:27,180 --> 01:06:34,220
So Ev and others have argued that this is basically evidence against the Chomsky

781
01:06:34,220 --> 01:06:40,940
inclaim that language is the medium for thinking, because there's thinking that can happen

782
01:06:40,940 --> 01:06:45,100
in the absence of language and the brain areas that care about language seem not to be the brain

783
01:06:45,100 --> 01:06:52,060
areas that care about thinking. I guess, Elliot, you're saying that people don't really believe that.

784
01:06:54,940 --> 01:06:57,020
They don't believe that distinction, I mean.

785
01:06:57,740 --> 01:07:05,340
And also, there's a lot of self-contradiction even within these arguments, right? So in your paper,

786
01:07:05,340 --> 01:07:09,180
you sometimes say that Chomsky thinks that language is a thought system, but then a few

787
01:07:09,180 --> 01:07:13,660
pages later, you'll say Chomsky also believes that syntax is some totally separate system from

788
01:07:13,660 --> 01:07:18,780
anything else, right? Your autonomy of syntax, etc. So does Chomsky think-

789
01:07:18,780 --> 01:07:21,340
That's not my contradiction. I mean, he said both of those things.

790
01:07:21,980 --> 01:07:26,860
Right, exactly. So therefore, you may want to ask yourself, does he really believe these things?

791
01:07:27,500 --> 01:07:31,260
Or what is the case if it arises from the architecture, right?

792
01:07:31,260 --> 01:07:35,660
So just saying language is a thought system, what does that mean? That doesn't mean anything.

793
01:07:35,660 --> 01:07:40,700
It's just a very vague statement. The question is how exactly is language contributing to thought

794
01:07:40,700 --> 01:07:41,900
and how is it not contributing?

795
01:07:43,980 --> 01:07:50,220
Yeah, I mean, I think his claim is mainly evolutionary or something, right, that this is the

796
01:07:50,220 --> 01:07:56,620
origins of the system, which I think is sort of equally hard to square with the kind of

797
01:07:56,620 --> 01:08:05,500
patient and neuroimaging data. But if he doesn't think that, then he shouldn't say it,

798
01:08:06,540 --> 01:08:08,460
or people will respond to what he said, I think.

799
01:08:10,220 --> 01:08:14,620
The argument is that language is a kind of thought system. It regulates some aspects of

800
01:08:14,620 --> 01:08:18,700
thought and it yields some aspects of thought that are clearly unique to humans,

801
01:08:18,700 --> 01:08:23,980
but it's not intrinsically or causally tied to it. The architecture of the system is very different

802
01:08:23,980 --> 01:08:28,460
from the kind of generalizations you can rhetorically evince from the architecture.

803
01:08:28,460 --> 01:08:32,620
So for instance, when you cite work from a phasic patient showing no deficits in complex

804
01:08:32,620 --> 01:08:36,460
reasoning, as you just mentioned, playing chess and so on, we would actually expect this under a

805
01:08:36,460 --> 01:08:41,740
kind of non-lexicalist framework of generative syntax, where meaning, as I said, meaning syntax

806
01:08:41,740 --> 01:08:46,700
and form, form just meaning anything that you can externalize language in, all these things

807
01:08:46,700 --> 01:08:51,100
are separate features and separate systems. The autonomy of syntax doesn't mean,

808
01:08:52,700 --> 01:08:56,380
what a lot of people think it means, it just means that there are certain syntactic operations

809
01:08:56,380 --> 01:09:00,540
that are not semantic. There are certain things you can do with syntax that you can only do with

810
01:09:00,540 --> 01:09:04,060
syntax and you can't do with semantics. So this gets back to the difference between

811
01:09:05,100 --> 01:09:11,180
Petrowski's theory that semantics is just and versus a lot of syntacticians' belief that

812
01:09:11,180 --> 01:09:15,340
there are certain peculiar weird things you can do with syntax that are just syntactic.

813
01:09:15,340 --> 01:09:21,340
So there is a divorce even within the kind of architectural framework. So it's not too surprising

814
01:09:21,340 --> 01:09:25,020
that you also find that divorce at the neuropsychological level, I would say.

815
01:09:26,060 --> 01:09:33,260
Well, I think I would want a prediction of the language's thought evolutionary idea then.

816
01:09:36,380 --> 01:09:43,980
If you're saying that doesn't predict that thought relies on language, then I think whoever

817
01:09:43,980 --> 01:09:50,380
likes that theory should come up with some predictions about what that theory actually

818
01:09:50,380 --> 01:09:55,500
means. I feel like those kinds of predictions are often really necessary for understanding the

819
01:09:55,500 --> 01:10:01,180
content of a prediction. So sorry, Daniel, your hand's been up for a while.

820
01:10:02,140 --> 01:10:14,300
No, it's all good. Just kind of wanted to bring a breath in and an opportunity for anyone to ask

821
01:10:14,300 --> 01:10:23,180
any other questions. But wow, thank you both for the many topics we've covered. We'll have,

822
01:10:23,180 --> 01:10:27,660
in the last minutes, a kind of conclusion in next steps. But Dave, would you like to

823
01:10:28,220 --> 01:10:30,860
ask a question or just give a short reflection?

824
01:10:37,020 --> 01:10:44,700
Okay, no. There are many comments in the chat, so I hope that both of you can read them

825
01:10:44,700 --> 01:10:51,980
on your own time to see what everyone added. Where do we go from here? As we

826
01:10:52,940 --> 01:11:01,740
roar into May 2023 and beyond, what can linguists, large language model developers

827
01:11:01,740 --> 01:11:06,940
and users, cognitive scientists, what do you each think are some of the most fruitful pathways

828
01:11:06,940 --> 01:11:16,140
forward? Well, I would say the most fruitful pathway forward is to really take cognitive

829
01:11:16,140 --> 01:11:22,620
psychology seriously. There's a lot of nice work recently trying to align things like chat

830
01:11:22,620 --> 01:11:27,180
GPT, Wolfram Alpha plugins, the way that chat GPT can interface with different kind of modules.

831
01:11:27,980 --> 01:11:34,300
The way of building a legitimate kind of AGI system doesn't necessarily have to be psychologically

832
01:11:34,300 --> 01:11:37,980
reliant on the kind of modules that human beings have, but I think it will benefit from it. So

833
01:11:37,980 --> 01:11:45,100
there have been some claims that large language models can maybe do all sorts of things. But I

834
01:11:45,100 --> 01:11:48,620
think in the long run, it's most likely going to be the case that LLMs can do something very

835
01:11:48,620 --> 01:11:52,780
important and very interesting, but it's only going to be one piece of the puzzle. So in fact,

836
01:11:52,780 --> 01:11:58,780
even OpenAI CEO Sam Altman said last week that what we can do with LLMs has really kind of been

837
01:11:58,780 --> 01:12:05,340
exhausted. We need new directions, new avenues and so on. I guess he was probably speaking to

838
01:12:05,340 --> 01:12:11,260
investors more than linguistic students here, but I think he's also right. LLMs can do something

839
01:12:11,260 --> 01:12:16,860
spectacular, but they're probably going to form a small part of the general AGI architecture,

840
01:12:16,860 --> 01:12:23,100
right? If you want to think about AGI as a potential, potential goal here. So, you know,

841
01:12:23,900 --> 01:12:31,340
I think a lot of the, so let me give me an example here. So Anna Ivanova, who's a very good

842
01:12:31,340 --> 01:12:35,980
cognitive scientist, she has a paper recently arguing for a kind of modular architecture for

843
01:12:35,980 --> 01:12:40,700
LLMs, which is a very nice framework, right? It's very cognitively plausible. It's exactly the

844
01:12:40,700 --> 01:12:44,380
kind of thing that we should be pushing for. It's compatible with Howard Gardner's, you know,

845
01:12:44,380 --> 01:12:48,620
notion of multiple intelligences and so on. But I think at the same time, just to finish this

846
01:12:48,620 --> 01:12:55,180
comment, there was a tech talk last week, I think, or maybe a few days ago, where a lot of this stuff

847
01:12:55,180 --> 01:13:01,740
can be conflated with AI hype in an unproductive way. So Greg Brockman from OpenAI, he gave one of

848
01:13:01,740 --> 01:13:06,700
his, one of these big TED talks where he showed different plugins that chat GPD can do. I mentioned

849
01:13:06,700 --> 01:13:11,580
Wolfram Alpert, right? But there's also things like image generation, Instacart shopping, where you

850
01:13:11,580 --> 01:13:17,180
can get chat GPD to buy you things and what have you. And again, this takes you back to the idea

851
01:13:17,180 --> 01:13:21,660
that multiple subsystems can do different sub-functions. So Brockman also showed an example

852
01:13:21,660 --> 01:13:28,540
of giving chat GPD an Excel file, a CSV file from an archive database of academic papers,

853
01:13:28,540 --> 01:13:33,900
where it just listed a bunch of papers and then titles and what have you, right? And he said that,

854
01:13:34,140 --> 01:13:40,060
using chat GPD, it uses world knowledge to infer what the titles of the columns mean. So we understood

855
01:13:40,060 --> 01:13:45,100
that title means the title of the paper. It understood that authors mean the number of authors

856
01:13:45,100 --> 01:13:50,220
per paper. It understood that created means the date the paper was submitted, right? And because

857
01:13:50,220 --> 01:13:55,500
it's a TED talk, the audience gave us a standing elevation, right? But the ability to describe

858
01:13:55,500 --> 01:14:02,780
labels on an Excel file is, I guess, nice. But I'm not sure you'd really call it world knowledge.

859
01:14:02,780 --> 01:14:07,980
So I guess, I would just say there's a lot of progress needs to be made alongside reducing

860
01:14:07,980 --> 01:14:12,300
anthropomorphism. You have to have the right balance of it. So like I said, you have to have

861
01:14:12,300 --> 01:14:17,260
the right balance of psychologically plausible kind of modular architecture, but you can't have too

862
01:14:17,260 --> 01:14:21,820
much anthropomorphism because then you'll get carried away. You have to find, we have to find

863
01:14:21,820 --> 01:14:27,100
the right balance between modeling kind of human-like modular systems, but not doing it

864
01:14:27,100 --> 01:14:31,580
to a degree that is a bit implausible or scientifically unhelpful.

865
01:14:35,580 --> 01:14:38,940
I mean, I think I agree with all of that. I'm really excited about these

866
01:14:39,660 --> 01:14:46,380
ways of kind of connecting language models to other forms of information processing,

867
01:14:46,380 --> 01:14:51,500
which does seem like what people have. I think I've been very surprised at the

868
01:14:52,460 --> 01:14:59,980
the things they are able to do just as language modeling, right? So different kinds of reasoning

869
01:14:59,980 --> 01:15:07,180
puzzles and things that they can solve, I think, is really fascinating and maybe will require us

870
01:15:07,180 --> 01:15:13,740
to rethink the relationships between language and thought and try to figure out a way of being

871
01:15:13,740 --> 01:15:19,500
specific about what it means for something to have a representation or to reason over that

872
01:15:19,500 --> 01:15:27,180
representation. But ultimately, I think I agree that people have different modes of thinking about

873
01:15:27,180 --> 01:15:36,060
things and that seems important for intelligence. I'm also super excited about the BabyLM challenge.

874
01:15:36,060 --> 01:15:43,660
So I think on the kind of linguistic side, that's exactly the right thing of seeing how far we can

875
01:15:43,660 --> 01:15:53,660
get with smaller data sets and maybe eventually after that trying to understand some more about

876
01:15:53,660 --> 01:15:59,820
the kinds of semantics that kids acquire and where they get it from and how kind of external semantics

877
01:15:59,820 --> 01:16:08,060
can inform language learning or specifically maybe grammar and syntax learning. I guess my other

878
01:16:08,940 --> 01:16:17,180
path forward point would be that there's... I feel like these kinds of models have

879
01:16:17,180 --> 01:16:24,940
have really gone far beyond people's expectations for this kind of class of model, right? Kind of

880
01:16:24,940 --> 01:16:33,100
ground up statistical learning, discovering patterns in text seems to give really pretty

881
01:16:33,100 --> 01:16:39,260
remarkable results. And that for me going forward, I think has just introduced a huge wave of

882
01:16:39,260 --> 01:16:45,740
uncertainty over theories. So I think that our theories of basically everything in language for

883
01:16:45,740 --> 01:16:52,460
sure, but cognition, probably neuroscience, like all of those things I think are going to be reworked

884
01:16:52,460 --> 01:16:59,660
when we really come to kind of understand the ability of really general kinds of learning

885
01:16:59,660 --> 01:17:08,380
systems like these. So that makes it on the one hand kind of a bummer for past theories,

886
01:17:08,380 --> 01:17:16,620
especially theories which relied on learning not being able to work well. But on the upside,

887
01:17:16,620 --> 01:17:23,260
I think it makes it a very exciting time both for AI and cognitive science and linguistics,

888
01:17:23,980 --> 01:17:29,500
where now there's these really, really powerful tools that seem like a qualitatively

889
01:17:30,540 --> 01:17:36,940
different size step towards human abilities. And I think kind of integrating them and taking

890
01:17:37,820 --> 01:17:43,820
both the kind of engineering lessons and the kind of philosophical lessons about how they're made

891
01:17:43,820 --> 01:17:49,260
and what kinds of principles go into designing intelligent systems. I think that those things

892
01:17:49,260 --> 01:17:56,780
will really shape the field over the next five or 10 years. And also, I would just say in the

893
01:17:56,780 --> 01:18:00,940
context of broader themes here, right, like you're totally right, like I remember when I was reading

894
01:18:00,940 --> 01:18:07,820
about when Deep Blue, the Kasparov, was it, the chess thing, right? And there were some commentators

895
01:18:07,820 --> 01:18:13,260
who said, you know, chess is over. If an AI can beat a human, then it's game over. What's the

896
01:18:13,260 --> 01:18:18,860
point in studying chess? You know, there's no need of boring anymore. And I guess if AI has achieved

897
01:18:18,860 --> 01:18:22,380
seemingly everything that humans need to do to play chess, what's the point of playing it?

898
01:18:23,180 --> 01:18:26,620
But I think, you know, if anything, it turned out to increase the popularity of chess, right?

899
01:18:26,620 --> 01:18:31,260
There are now many chess celebrities as well, worldwide tournaments. And I would predict that

900
01:18:31,260 --> 01:18:34,860
the same is probably going to happen with language too. You know, LLMs do not mean

901
01:18:34,860 --> 01:18:38,460
it's the end of language, no more language, no more linguistics. I would actually push back

902
01:18:38,460 --> 01:18:43,340
and say maybe it would be the opposite. You know, the success of LLMs will increase general

903
01:18:43,340 --> 01:18:47,900
interest in linguistic theory, due to their apparent, you know, weird constraints and apparent

904
01:18:47,900 --> 01:18:53,100
limitations, right? Because I would also say, you know, scale, at this point, the chess issue,

905
01:18:53,740 --> 01:18:59,420
scale is kind of definitely far from all that's needed. What is lacking is an ability of LLMs to,

906
01:18:59,420 --> 01:19:03,580
you know, really abstract their knowledge and experiences in order to make robust predictions

907
01:19:03,580 --> 01:19:07,340
and generalizations and so on. I gave some examples, but there's some others in the literature

908
01:19:07,340 --> 01:19:11,100
where it doesn't seem to really be good at generalizing. It can kind of mimic particular

909
01:19:11,100 --> 01:19:16,300
token types. But I would, you know, I would guess my final, my final thing would be that,

910
01:19:16,300 --> 01:19:21,820
you know, the language acquisition literature doesn't necessarily need LLMs though. You know,

911
01:19:21,820 --> 01:19:26,460
cognitive scientists don't really need LLMs. We could potentially, you know,

912
01:19:26,460 --> 01:19:30,460
me and Stephen obviously disagree here, but I would say big tech companies

913
01:19:30,460 --> 01:19:34,620
profiting off LLMs need LLMs, right? They're the only ones that really do. It may be the case

914
01:19:34,620 --> 01:19:39,180
that the mind is a very, I will say, you know, the mind is a very diverse space. It may be that

915
01:19:39,180 --> 01:19:43,260
there are certain forms of behavior and learning that might be captured by processes similar to

916
01:19:43,260 --> 01:19:46,540
what LLMs are doing. So Stephen has given some interesting examples in his papers about

917
01:19:46,540 --> 01:19:51,260
magnetism and we're kind of rules of learning that are very, very general and very quick and

918
01:19:51,260 --> 01:19:55,900
very mysterious. So, you know, maybe for those sorts of things, that kind of learning will be

919
01:19:55,900 --> 01:20:00,300
relevant. But I still think it's unlikely that one of the candidates will be natural language,

920
01:20:01,420 --> 01:20:05,100
at least the way natural language works in its full glory in terms of the four meaning

921
01:20:05,100 --> 01:20:09,740
regulation and what have you. So I guess I would, you know, it kind of reminds me of where you,

922
01:20:10,540 --> 01:20:14,540
you know, you have this image of, I saw John with chapter four recently, right? And he has this,

923
01:20:14,540 --> 01:20:17,580
there's this scene where he's walking in the desert and he's not sure if he's seen

924
01:20:17,580 --> 01:20:21,740
this guy that he wants to assassinate. It's kind of like when you walk in the desert

925
01:20:22,700 --> 01:20:27,100
and you have an illusion of seeing an oasis because it turns out you're hallucinating.

926
01:20:27,100 --> 01:20:30,860
But then you realize that, you know, sometimes before it's too late that you actually are

927
01:20:30,860 --> 01:20:34,460
hallucinating. It's you're not seeing an oasis. You're still in the desert. And I think that's

928
01:20:34,460 --> 01:20:39,580
kind of maybe the situation we're in right now with linguistic competence of laws of language

929
01:20:39,580 --> 01:20:45,500
models. We have the illusion of linguistic competence. But, you know, you always see the

930
01:20:45,500 --> 01:20:49,660
illusion before you find the oasis, right? So I think, I think right now we're in the

931
01:20:49,660 --> 01:20:54,540
hallucinating stage of the desert where we're seeing potential sparks of linguistic competence,

932
01:20:54,540 --> 01:20:59,660
but it's still not very clear and I'm robust. And we haven't actually reached the oasis yet.

933
01:21:00,620 --> 01:21:08,700
Um, just a rapid fire question. So see if you can give a short response. So

934
01:21:09,660 --> 01:21:15,580
Sphinode, you know, writes question, is it correct to say that large language models have no priors?

935
01:21:18,620 --> 01:21:22,700
Do large language models have priors? I'd say yes, they definitely do.

936
01:21:23,100 --> 01:21:30,540
Um, and there, I think the difference to how people, you know, are used to thinking about

937
01:21:30,540 --> 01:21:35,420
priors in Bayesian inference, for example, if you like write down a Bayesian statistical model,

938
01:21:35,420 --> 01:21:39,740
you say like, you know, here's the parameters and here's what the priors are on the parameters.

939
01:21:40,780 --> 01:21:45,020
Large language models, I think the priors are and maybe neural nets in general, I think that the

940
01:21:45,020 --> 01:21:49,980
that the priors are much more implicit, right? So there's some functions which they find easier

941
01:21:49,980 --> 01:21:55,500
to learn than other functions. And there's even some work trying to discover, you know,

942
01:21:55,500 --> 01:22:01,020
some statement of what those kind of implicit priors are. But that's actually how I think about,

943
01:22:03,100 --> 01:22:06,460
you know, comparison of different neural network architectures, right,

944
01:22:07,580 --> 01:22:12,060
which is maybe something Elliot and I might agree on, right? Like you have to find priors which

945
01:22:12,060 --> 01:22:18,220
allow them to learn the things that kids learn, right? And not all architectures will do that.

946
01:22:18,860 --> 01:22:23,900
Even among architectures which are turn complete or capable of learning any kind of function,

947
01:22:23,900 --> 01:22:31,500
not all of them will do it, even on kind of huge data set sizes. So I think of this sort of search

948
01:22:31,500 --> 01:22:38,060
over neural net architectures as really one of a search over priors. But it's not priors or,

949
01:22:38,060 --> 01:22:41,740
I mean, you could think of it as a search over universal grammar or something, right? But it's,

950
01:22:41,740 --> 01:22:47,660
it's, it's not priors or universal grammar in the sense that people have talked about it as like

951
01:22:47,660 --> 01:22:51,980
an explicit statement about what kinds of rules are allowed or an explicit statement about what

952
01:22:51,980 --> 01:22:56,540
kinds of functions or high probability or something like that. It's all implicitly coded there.

953
01:22:57,260 --> 01:23:01,660
Yeah, totally. I think, I think that's right. I mean, you know, the real question is reducing

954
01:23:01,660 --> 01:23:06,860
the space of what those priors are like. And if it's anything remotely like what human beings

955
01:23:06,860 --> 01:23:11,660
are doing, so LLMs like, I would, I would at least say that things like GPT-3 are an existence

956
01:23:11,660 --> 01:23:17,260
proof of, you know, that building fully functioning syntactic categories from surface

957
01:23:17,260 --> 01:23:23,500
distributional analysis alone is possible. That's, yes, that is correct. But, you know,

958
01:23:23,500 --> 01:23:29,900
even so, I would say most syntacticians don't really believe that syntactic categories are innate.

959
01:23:29,900 --> 01:23:34,780
So the prior issue is slightly less relevant here. It's the operations that are said to be innate.

960
01:23:34,780 --> 01:23:39,900
So the, in the syntax domain, it's particular linguistic computations that are said to be innate

961
01:23:39,900 --> 01:23:43,980
and categories themselves. In fact, even Charles Young has admitted in the last couple of years

962
01:23:43,980 --> 01:23:50,060
that they are maybe innate, but maybe not. So people have given, I know of a relevant prize,

963
01:23:50,060 --> 01:23:54,460
they are things like, you know, me and Gary Marcus have talked about compositionality.

964
01:23:54,460 --> 01:23:59,340
That seems to be a big problem. So people have given chat GPT BBC news articles asking it to

965
01:23:59,340 --> 01:24:05,740
compress it and then re-explain it. So one example I saw was Peter Smith 58 is being arrested on

966
01:24:05,740 --> 01:24:11,580
charges of manslaughter and you get it to compress it and re-explain it. And it comes out as 58 people

967
01:24:11,580 --> 01:24:14,860
are being charged with manslaughter. All right. That's a pretty clear example of a lack of

968
01:24:14,860 --> 01:24:19,100
compositionality being built into whatever compression it's doing. And there's no example

969
01:24:19,100 --> 01:24:23,580
where there's been, there's some examples of potential analogical reasoning. So in Bing chat,

970
01:24:23,580 --> 01:24:28,700
you know, Bing has this, this chat function. The question is, is it just finding meta relations

971
01:24:28,700 --> 01:24:33,180
that have already been documented by humans or is it genuinely creating new relations that the new

972
01:24:33,180 --> 01:24:40,380
stuff that is being built. So, you know, someone asked through me a table comparing Jesus Christ

973
01:24:41,020 --> 01:24:47,340
with the Nokia 9910, right, the cell phone Nokia 9910. And it said, you know, it compared the

974
01:24:47,340 --> 01:24:54,540
release dates. It compared the size, the weight. It compared the CPU with Jesus's all-powerful

975
01:24:54,540 --> 01:24:59,580
knowledge. It compared the memory of the phone with the all-knowing nature of God, right.

976
01:24:59,900 --> 01:25:04,940
And it also, I think it said that they were both resurrected because the Nokia was re-released a

977
01:25:04,940 --> 01:25:09,020
couple of times, right. So the Nokia. That sounds like a great answer. What's wrong with that?

978
01:25:09,020 --> 01:25:14,620
Okay. It may be. It sounds a lot like analogical reasoning, but then it also had some quite weird

979
01:25:14,620 --> 01:25:19,100
ones where it was like, you know, for the camera, it said, no, it just gave Jesus's description,

980
01:25:19,100 --> 01:25:23,980
but it's not really what a camera is. There's some kind of things that look like analogical

981
01:25:23,980 --> 01:25:32,060
reasoning, maybe, but it's unclear. Yeah. I think that sounds like an awesome answer to me.

982
01:25:34,620 --> 01:25:39,100
I was going to say, like, you said large-language models learn they're an existence proof of part

983
01:25:39,100 --> 01:25:43,580
of speech categories, but like, they don't just output part of speech categories, right. Like,

984
01:25:43,580 --> 01:25:51,500
they have a lot of grammatical syntactic knowledge. And moreover, like, they have a lot of semantic

985
01:25:51,500 --> 01:25:57,260
knowledge and probably some pragmatic knowledge. And, you know, they're not bad at translation.

986
01:25:57,260 --> 01:26:03,020
And like, it's way more that they have discovered than just part of speech categories.

987
01:26:04,860 --> 01:26:08,300
Well, sorry, I said syntactic. I'm sorry. It's like syntactic categories.

988
01:26:09,100 --> 01:26:12,380
Right. Well, sorry. Yeah. Yeah. But they've discovered way more than that.

989
01:26:12,940 --> 01:26:21,980
Yeah. I'm going to, as a teaser slash motivator for hopefully both of you to join again in the

990
01:26:21,980 --> 01:26:27,660
future with or without other guests, a few of the exciting questions just for us to include in this

991
01:26:27,660 --> 01:26:31,260
transcript. And then thank you both, Elliott and Stephen, for joining. So just a few of the last

992
01:26:31,260 --> 01:26:35,900
questions that were asked, Juan asked, how do small transformers, Zhang et al. 2020,

993
01:26:35,900 --> 01:26:41,260
compared with children learning language? 96 asked, what are your thoughts on implicit

994
01:26:41,260 --> 01:26:47,420
priors versus animal instinct? Rojda asked, what constraints that space in LLMs, don't

995
01:26:47,420 --> 01:26:52,300
they get there by training? So are they discovering it? That's not what they implement at the start

996
01:26:52,300 --> 01:26:59,180
maybe. And there's many more questions. So I hope that we can all review and reread each other's

997
01:26:59,180 --> 01:27:05,500
works and come together for 41.2 in some future time. Thank you, Elliott and Stephen,

998
01:27:05,500 --> 01:27:09,980
for this excellent stream. Thank you, Dave. Thank you both. Yeah. Thank you so much.

999
01:27:09,980 --> 01:27:16,860
Very well. Bye. See you.

