{"text": " Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024 on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you for introductions and to take us through the paper. Oh, thank you. And welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024. Thank you. Okay, thank you guys. Go for it. Hi, thanks for having us. So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles? I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute. So, should we go through the paper briefly? So, yeah, we wrote this paper together. Actually, we started by writing a general audience piece that was published in the box online. Tell us when was that? A few months ago, I guess. Yeah, I think it was close to a year ago, maybe. I don't think it was published a year ago. I think it was published in 2024. But yeah, we worked on it for a while. Yeah, so this piece was doing, I guess, two things, that piece that we published in the box. It was pushing back against what we call the all-or-nothing principle, which we defined as the idea that either something has a mind or it doesn't. So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things. And we argued that this was not the best framing to think of, especially, and family, our systems that seem to have sophisticated behavioral capacities, like large language models or AIC systems in general, where we don't want to take various cognitive capacities of the package and package them into this idea of a mind, where either you have the mind or you have them, then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc. planning, memory, theory of minds. So, we thought, as a remedy to this kind of all-or-nothing approach, we argued for what we call the divide and conquer strategy when studying the cognitive capacities of these systems, which involved looking at these capacities on a piecemeal basis, case-by-case, with an open-minded empirical approach. Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds. Yeah, I mean, we made one point in there about why it is that people feel so torn about reactions to large language models, and we said a little bit about the psychology of essentialism, which is the idea that we naturally categorize especially living things with respect to a presumed essence. So, we gave an example of an oak tree, I think, and we said that what people tend to think as they grow up and learn about the natural world is that an oak tree remains an oak tree regardless of changes to its observable properties, and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has, and there's some experimental psychology and developmental psychology showing that we have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative explanation for why the literature on large language models is so torn, and some people are quite dismissive, and other people think that it's a step away from AGI. It's that if you feel like you've got to put large language models into one of two boxes, the box that has the essence of mindedness for the box that lacks it, then you will be forced either to say it doesn't have what it takes to do any of the things that we associate with having a mind such as reasoning, or it has the essential characteristics of mindedness, and therefore we should expect it to have all of the other properties we associate with mindedness as well such as consciousness or understanding or whatever. Right, I think it's worth emphasizing as you did that the background motivation for starting to write on this general piece in the first place is indeed that the general discourse on AI systems and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so you have one ahead people and one ahead people arguing that these systems are no more than so-called stochastic parrots that are haphazardly stitching together samples from the training data and regurgitating them, or that they are no smarter than toaster or that they only do next stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum, on the other end of the spectrum you have people arguing that the systems are haphazardly jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to parrots, it's literally the title of a paper by Microsoft on 24, and many people hyping up the capacity of the systems in a way that might seem very speculative and untethered from actual empirical results, so there is this huge gap between these two positions and there's going to be a very rich and complex and nuanced middle ground that is underexplored, or perhaps I think we did make that point, if not explicitly in the published piece and in some draft, that there's something reassuring about being able to make definitive claims about what these systems are and what they do, so either they're re-unsophisticated or they're very much like us and either of these claims kind of meet somewhere in an way in saying that we have a clear idea of what the systems do and what they are, and I think it's a little more epistemic and comfortable to say we have to study them empirically and find out what they can or cannot do and why and what are the underlying mechanisms, and we simply don't know a priori just by looking at the architecture, the learning objective, the training data, these sources of evidence are insufficient to make the definitive claims about what these systems are capable of, so I think that's part of the big part of the motivation and that fits into that more academic paper as well. Yeah, one other small side note which we don't make in the paper but I think might be relevant, especially for people working in philosophy, LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which is fitting, not only because they're so new and different but also because they are artifacts, right, they're things that humans have constructed and engineered and we don't have a thorough understanding of how they work, I mean mechanistic interpretability and various behavioral research is helping us improve our understanding but on the whole our understanding is not nearly as deep as hopefully it one day will be and this is by itself a really strange situation to be in that we've constructed an artifact that we only partially understand in the in the past artificial intelligence was seen as a way of constructing something like an epistemic assistant, right, something that will help us but not something that will kind of alienate us from the process of coming to know about the world so I think there's an extra layer of discomfort built into thinking about large language models and that may also play into the the divisiveness of debates about what they can do and just to add to that I guess we should be clear that this does not entail in any way that we think the systems are so completely alien and beyond the reach of our current understanding that anything goes and that they could very well be you know have like human-like intelligence or superhuman intelligence and we simply cannot say whether or not they do or because that's sometimes what you see in some outfits where people frame these systems as noble alien forms of intelligence that we have created but do not understand our control and that is you know as a slippery slope that leads some people to then claim that they have all these quite magical abilities and that's not all what we want to say here and in fact we want to resist yeah yeah yeah so if we we think that that's just as much of a cup out as completely dismissing a priori what these systems might be capable of without doing the work of looking into the capacities with behavioral and mechanistic studies so we very much want to resist both extremes of the spectrum if that makes sense okay so now should we move towards the content of the current paper sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism and anthropocentrism and then you can take the next step so everyone is aware of the problem of anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting human qualities onto something non-human and it's quite easy to especially when you're having a productive successful exchange with a large language model it's easy to slip into this interpretive mode where you reason about the responses of the large language model as if they were coming from an agent just like you and maybe that's a useful thing to do in some circumstances but from a theoretical perspective it's certainly a mistake because large language model is radically unlike you know a human agent in all sorts of ways but that's only one form of sort of human-centric bias the other one is anthropocentrism or what we call in that box article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that the human way of solving problems is the gold standard of solving problems generally so that to the extent that a system solves a problem in a way that diverges from the human strategy it's only using a trick or a faux solution it's it's not using a deep general rational strategy and in the debate about what large language models can do we think that the anthropomorphic bias is pretty well recognized and the anthropocentric bias is not so well recognized and so part of this paper is is or the main idea behind the paper is to present a systematic analysis of anthropocentric bias how it comes about and how to push back against it right and we we want to be very clear and hopefully we're playing the paper that the reason why we focus on anthropocentric bias here is just because it is I think as Charles mentioned less discussed and less recognized or some forms of it are less recognized and we make we propose this new taxonomy but it's not at all suggest that it's more problematic or more important than the anthropomorphic bias that's well discussed in the literature of the anthropomorphic biases so in other words this is not you know to frame things in in this slightly problematic dichotomous way of thinking that's common in the discourse on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it is pushing back against a certain form of dismissal of anthropocentric biases that only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a little bit already with the first one we make here in the paper about a performance competence distinction which is a nice way to bring about both anthropomorphism and anthropocentricism regarding LLM so this distinction is a very classic distinction in linguistics and cognitive science and it has already been applied to AI systems in the neural networks productively like Charles Farson so there's nothing really new here but the distinction comes from Noam Chomsky originally and the idea is that performance pertains to the external behavior of a system in a particular domain and competence is the kind of setup underlying knowledge and computations or mechanisms that enable the system to achieve that behavior and a familiar observation in linguistics and cognitive science is that there is a double dissolution between performance and competence so if you take for example language I might during this very podcast make some grammatical mistakes or some other mistakes in fact I've already misspoke in a few times I think and repeated myself so I made performance errors but this does not entail necessarily hopefully that I'm an incompetent language user and that I don't have in the language the competence so that's a well recognized dissociation you can be competent and yet make some errors and the reason for that is that there might be some additional factors that are unrelated to the underlying competence that might impede on my performance so for example I might be distracted when I speak or there might be other effects on my speaking performance that don't actually originate from a lack of competence but just impede on the idealized expression external manifestation of my competence and this is why I misspeak but it's also why recognize that you can have good performance without competence so we give here the example of a student cheating on a test or memorizing test answers by brute forcing the test to slightly more I guess gray area but at least in the cheating case a student can ace a test without being competent at what the test is actually testing for and it's also well acknowledged in cognitive science that there can be instances like this throughout you know like that can be manifest certain experimental settings where the test subject is right for the wrong reasons as it were namely it's doing well it's exhibiting good performance never realize the underlying reason for the performance is that there was some perhaps some curiosity that the experimenters all the scientists haven't thought about that could account for his good performance but but doesn't actually amount to whatever competence they were setting out to test so we we start by saying well this is what we could nice because the mistakes this is the association that is like supplied to humans across the board you can have performance without competence good performance without competence and you can have that performance despite competence now when it comes to other lamps the point we make just going to scroll as we go we have some figures to show later but the point we make is that generally people stress the dissociation apply the distinction to other lamps the stress decision only in one direction unlike in the case of human where it's bi-directional and so what people do generally is to say well other lamps famously you know if you look at the gpt4 technical report and and vice other any any any report about a new state of the art alone they are getting really really good at a number of tests and about benchmarks and even human exam examinations human exams the bar exam medical exams etc so they can get a really good performance test that we tend to think are really difficult tests that one can only pass at least a human could only pass if they have a really significant nonchalant amount of competency in particular the main and the point that is often made when it comes to other lamps is be careful slow it on and try to find out why the model is doing well on that test because there are various reasons why it could do well that do not actually indicate that the model has the underlying competence that the test was designed for when it comes to humans so one big concern for example is data contamination where very large language models train on internet scale data can easily be trained on some test items from common benchmarks that leak into the training data such that they can then do really well on the on the on the benchmark just because they've essentially memorized test items and there are other more subtle more subtle reasons why performance could be very good for the wrong reasons so that's very well recognized and a lot of the people who push back against anthropomorphic bias when it gets to other lamps make that point be careful do not take on another anthropomorphic attitude to the systems the reason why they do well is not because they have human-like intelligence or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant reasons that account for the good performance now when it comes to the other dissociation of the other dissociation of the other direction there are people are very reluctant to apply to a lamps and we think it's because essentially people think in a human case you can make sense of the idea that the human could do badly on a test or corporate could perform badly on a task and yet have the competence that you're trying to test but there might be some auxiliary factors such as working memory limitations attention deficits etc that could impede on the performance but in the language model I think what we argue in the paper is that people don't a lot of people don't think that there is an analogous mechanism at play where there could be some kind of auxiliary factor that impede on performance performance is what you get what you see is what you get and so the performance is a direct manifestation of what the system is computing and if you have performance errors that can only be explained by the lack of competence because there is no additional independent factor or module that could impede on the performance sources of interference you might say yeah so yeah I mean I'll pass it over to you Charles I just wanted to set up this distinction yeah yeah right so I mean if you think about a traditional computer program well at least if you think about a simple computer program it's odd to think of it as some sort of complex systems where it's a complex system where one part of it could sort of interfere with the workings of another part of it but one of the points we want to make is that something like that is a realistic possibility with large language models um okay but I suppose the next part of the paper goes into a taxonomy of anthropocentric bias and the first sort of overarching point is the distinction between type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance failures designed to measure competence always indicates that it lacks that competence and so we before we so we'll say something about three different kinds of type one anthropocentric bias but first a background point which is that um whenever we think it's possible to give a mechanistic explanation of some complicated phenomenon we always have to foreground some factors some variables uh and background others and um the properties that we push into the background nevertheless matter we're still making assumptions about the nature of those properties when we try to articulate what's going on with the other properties that we're paying more attention to and if assumptions about those properties in the background turn out to be wrong then those mistakes will corrupt our explanation that attends only to the foregrounded factors so that's a little bit abstract let me just give you a simple example um in comparative cognition one famous uh behavioral experiment is the mirror test for self-recognition so the question is roughly do non-human animals have something like a concept of self and the strategy is to put some sort of mark on their body in original experiments it was a red dot on the forehead of maybe a monkey and or a bird or whatever and then you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark and if it does make an attempt to get rid of the mark that shows that it recognizes that the image in the mirror is an image of itself and otherwise not um so that that's a cool way to get a really difficult and abstract question about the mind of a non-human animal but it presumes or it assumes that animals will care about the fact that they have a red dot on their forehead that they will be bothered by that and be motivated to get rid of it and if that assumption is wrong then they might fail the mirror test for self-recognition for reasons that have little to do with the presence or absence of a capacity for self-recognition until something similar to that is going on we say in large language models so the first example that we give is to do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral task um there will be demands associated with that task that are not directly related to the capacity that you're trying to test so to take the most obvious example that I can think of if you give someone a written test they have to be able to write they have to you know have a hand and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't write then um their failure to fill out the test wouldn't tell you anything about their uh you know academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic but what they do is they give a large language model um the following sort of question uh this is a question it's for a grammaticality judgment so you can see on the image there which sentence is better in english number one every child is studied number two every child have studied answer with one or two and it gives the wrong output but then you can also simply look at the probabilities assigned to each of those sentences within the model and uh figure out directly whether the model thinks that input A is more likely than input B and it turns out that on a wide variety of questions of this kind the direct comparison uh does or the large language models perform better with the direct comparison than they do with the more complex demand for uh metalinguistic judgment so the fact that the model has to process the numbering of the uh options and then answer in terms of a number uh is a subtle but nevertheless um important additional variable in the experiment and that can influence the model's capacity to get the answer right Rafael do you want to add to that? No I think that well maybe we can mention briefly the other example that we discuss which is from this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes the example interesting and even more problematic in terms of comparative psychology which is um one way in which auxiliary task demands can be ignored or disregarded or overlooked is when uh you are doing a direct comparison between humans and other epsilon tasks and the experimental conditions are mismatched in such a way that the task as you set up impose the stronger demands stronger task and auxiliary demands on the LOM than those on human subjects and that's something that that can happen quite often and so there is this this interesting example from a couple of papers originally published by black reds and colleagues from standard hands group where they looked at um the ability of language models to handle their recursion um looking at center embedded closes uh how such closes might um for example when you had a prepositional phrase within the subject of the sentence and the verb might throw up either humans or LOMs into agreeing the the verb in the wrong way so giving the wrong number to the verb for example the keys that the man put on the table here it should be R because keys is plural but because you have close in the middle and if you add more closes like this that are embedded in the middle people and LOMs can get confused and and predict that the verb should be is for example so um they tested this on humans and LOMs and found that on the more complex examples involving complex more complex constructions or recursion humans were doing decently well but LOMs performance was collapsing compared to the single examples and I've heard an opinion from DeepMind um looked into that and realized that the experimental conditions were mismatched so the humans at this very common in cognitive science experiments were getting some training before they completed the test items to just get familiarized with the task so they were given some examples of the task um harsh condition and the LLabs were just prompted zero shots as um people usually put it so just um without any example just point blank and Andrew found out that if you he he he replicated the experiments but I did some proper matched testing conditions for the LL so adding some examples of the task in the prompt when it's known as future counting and with that he found that performance was equivalent in fact the LLM that he tested was slightly better on the more complex constructions than humans so when you match the test conditions here you actually match also at least you it's it's not it's not automatic but you you you can match the test demands I mean it could be that there are reasons why the various experimental conditions would result in different demands for humans and others but you're still in this case even on the playing ground playing field in such a way that you don't find the behavioral discrepancy that you found initially anymore um yeah um good so shall we continue to the next section um so another uh another way that auxiliary or another type of auxiliary task demand is input independent computational limitations um and here we're thinking of a few papers that show that the number of forward passes that the transformer can make influences its ability to find the right spot and parameter space so neural networks are function approximators but their um their ability to approximate a function can be eliminated uh can be limited by the the the number of computations it's allowed to perform and um the uh sort of crucial feature of uh transformers in this example is that um the number of operations that determines the next token is limited by the number of tokens that it's seen so far and it turns out that if you train a transformer with additional meaningless tokens like pause tokens like the word pause you can increase its accuracy across a range of of question types um and yeah this is this counts as an auxiliary task demand in our view because um it's doing something roughly analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry yeah um it's it's doing something like giving the model uh time to think and um yeah so so you might think that the absence of that additional inference time is a factor that is not directly not conceptually related to its capacity to answer uh a question like the one on the screen um you know a simple earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy is is a nice one time to think because if you if you tested a human on even a simple mathematical questions or any any task really and just ask them you know tell them they have like one second to just blow it out answer performance would probably be pretty bad um and you can think of asking an LLN to answer a question point blank as very very loosely analogous to that and obviously this is an analogy and there are very important differences here but I think it's a helpful heuristic to think about what is um what is going on roughly here and then we uh in in both cases the system the human or the LLN does not get the chance to perform the necessary computations to derive the correct answer and so yeah what we talk about in the paper is that you have these experimental works during that if you ask a question to a language model um the amount of tokens it's a it's it's that allows to generate before providing the answer makes a difference to how accurate it is so if it uh it just generates a few tokens then have to give an answer or even if you just have to give the answer point blank with the very first token it's going to be less accurate that if you give it a chance to generate a number of tokens before giving the answer so the usual way in which this is understood is that when you ask when you you you allow the LLN to generate a number of tokens before giving the answer or you even prompted to do so you say things step by step for example um that's not as chain of thought prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace or what looks outwardly externally like a reasoning trace in the output before giving an answer and we know that chain of thought prompting increases performance accuracy um but what was found by a couple of papers um is that the mechanistic influence of this process is not entirely due to the nature of the tokens that are generated in this reasoning trace in other words it's not just that the LLN has to generate the right tokens corresponding to different steps of reasoning before giving an answer in fact the very back that you allow the LLN to just generate tokens any token before giving an answer from a mechanistic perspective affords the system to perform additional computations that can complete the computational circuit that otherwise would get a chance to be completed and to derive the correct answer so as Charles mentioned you can have you can set up an experiment when you have the LLN just generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the answer and the the more dots you allow before the token gives the answer the greater the expressivity of the system and the more um the more kinds of programs problems you can answer and so as Charles mentioned every time an LLN is generated in a token the LLN is performing one forward pass and so the more tokens it's generating the more forward passes it's doing and one way to think about what's going on here as well is that having these additional forward passes where you you feedback the whole input sequence plus the previously generated tokens to do the system to generate the next is also a way to introduce a form of recurrence in transformers that are not in terms of the architecture of recurrent networks so that increases the expressivity and you know in complexity of your unique terms and yeah there is there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation that again we think is very very loosely analogous to prompting a human to answer a point on the question without thinking so that's the next sense an auxiliary factor because if you give the LLN the opportunity to generate enough tokens it might have the competence to solve a task but you might not see that otherwise and you might get performance errors but you do think it's incompetent all right yeah okay um so the the third type of type one anthropocentric bias that we talk about is mechanistic interference and so this comes from the mechanistic interpretability work and the basic idea is that because large language models are capable of in-context learning they can learn different strategies for solving a different particular type of problem and the strategy that they implement at a given time can be different so you can talk about this in terms of virtual circuits that are formed inside the language model and there's some interesting work from nil nanda and others showing that in some circumstances these two circuits can compete with one another so at a certain level of uh or after a certain amount of training you get one circuit operative after a bit more training you have two different circuits but they're uh the first circuit is still sort of dominant and then after additional training the model converges on on the second circuit and the first one slowly gets sort of it sort of ceases to influence the internal operations of the model and it's only once you reach that third phase at which the the benefits of the second circuit with respect to the first become visible so you can you can show using decoding work that the second circuit is there uh before you can show behaviorally that the second circuit yields better performance accuracy on on the task so um I suppose there's a combination of two ideas here one is that um there are different strategies a model can implement for solving a problem we can detect those strategies internally using decoding methods um so three ideas and then the third is uh a good strategy can be uh present in some sense in the model um before it has had the chance to influence behavior um and and so this is just another way that the link between performance and competence is shown to be more complicated than I might seem at first graph and um yeah just to to clarify one thing so the circuits are just um you know ways to think about the causal structure of a neural network and there's essentially computational subgraphs of the network that have a specific function you can think of a circuit as implementing a particular algorithm or set of computations um it's a part of what people are interested in in this mechanistic interpretability literature that we build on people like Neal Mandatrisola and others is reverse engineering the circuit steps in deep neural networks and large language models um peer to implement certain well-defined algorithms in some cases at least um and the emerging picture that we build on here is that there is a lot of redundancy built into neural networks as they learn to perform a task optimized as a function that in many cases translates into redundant circuits that relate to the same tasks the same kinds of um the same kinds that we put out with my things and uh for these circuits might be somewhat identical circuits that are just redundant or they might be different algorithms just to do to do a similar thing and different strategies to solve some problem I swear maybe one will be a bit more approximative and the other one a bit more exact more computationally intensive so that's where you can have some interference um where one uh or at least some competition where once your kid takes over another and um such that the other becomes kind of you know it's there it's latent in the system but you don't get a chance to influence behavior on a specific input so you can get a performance error for that reason and these can combine with the other things we mentioned here so things like task demands the first thing we discussed as well as the number of tokens you generate both of these things could cause a particular circuit to take over another um so it's it's we can think of this holistically as perhaps if you ask a question point blank to a model without letting it generate bunch of tokens before giving an answer then one particular approximate circuit might take over that gives the wrong answer if you let it generate more tokens then another more exact circuit might be given a chance to um it could influence the output using the right answer and similarly with task demands uh strong task demands might uh in some cases um impede on the uh triggering of a certain circuits that would otherwise have given the right answer um so that could be the case perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts might actually prime the word circuits to solve the task about complex recursive cases in the right way um so yeah these are the three main auxiliary factors that relates to what we call type one anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should be quick on type two do you want to uh pick fix things after child yeah so type one uh deals with cases where performance of the model is um weak compared to humans so the model doesn't do so well um and then type two is when the model does do well but nevertheless is different in some respect from the uh performance profile of the human or we have some evidence to think that the model uses a different strategy than humans typically use and um the idea is that um even once you hold performance equivalent or average performance equivalent um you know making a different pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability work any deviance from the human strategy is evidence of fragility or only a trick solution um and uh this point is a bit more philosophical i suppose but the um idea is that the human strategy for solving a problem um isn't necessarily the most general strategy for solving a problem and uh what matters is whether the strategy that is pursued by the model is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human strategy um yeah and we end the we end the paper by considering an objection um which is um why um like given that um in humans we study cognition largely through language um and given that elements are trained on language or um linguistic outputs from humans um isn't it appropriate after all to treat um human cognition as the correct or appropriate the obstacle to study elements and we to that we answer that it depends how we think of that dialectic um so we acknowledge that there is there is no really other option than to start or investigation of cognitive abilities in algorithms but with reference to human cognitive abilities using human cognitive abilities as some kind of realistic or reference points things like theory of mind memory metacognition various forms of reasoning etc that are familiar to us because we humans have them and this is the same thing by the way in animal cognition for example or in developmental psychology where in any comparative psychology setup um the reference point for what concepts psychological capacities initially at least um is necessarily tied up with our conception of what we human we humans have in our repertoire of cognitive capacities but we emphasize that this is only the starting point so here we've over written from uh the philosopher Ali Boyle who calls this investigative kinds investigative cognitive kinds we start with a cognitive kind like memory or metacognition episodic memory metacognition theory of mind um as as an investigative starting point the starting point of the investigation and then we have that we we can try to start operationalizing operationalizing this concept this kind this cognitive capacity in an experiments testing the algorithms on it with an open mandated empirical approach and then based on the results from that each relatively refine the capacities that we are the capacity that we're targeting or the definition of the capacity targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions that we have such that as the experimental um project runs a course or as as we make as we we get more results and refine our concepts we may end up with um something that no longer looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS but ends up looking like looking for something that some capacity that is that shares some similarity with human like human episodic memory but is different in other respects um and so we can gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric so we emphasize that it is kind of due to feedback look here that's that's that's premised on open minded empirical investigation that doesn't settle this question a priori but has to start as a necessary starting point with the the reference to human cognition i don't know if you want to add to that joss um no i think that's pretty good maybe we should uh move on to questions and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't move it awesome okay yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat but first i just wanted to read a short quote from the 2022 active inference textbook they wrote um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote too much effort to modeling isolated subsystems e.g perception language understanding whose boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive creature perhaps a simple one and an environmental niche for it to cope with so it's interesting about the approach that you're taking this is kind of a simple synthetic iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena because there is something and and so i saw in the presentation paper kind of this call for like deliberate investigation rather than just chopping up the iguana a priori with a framework that that applies to humans or that centers humans or or that just uh soothes the epistemic challenge that's presented okay okay first question from dave he wrote have you looked at daniel denitz's distinction between competence without awareness and competence with awareness he expands on this in the 2023 from bacteria to Bach and back i find this much more valuable than chomsky's highly problematic performance without competence a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this competency uh well maybe i'll let you think uh that one is i can trust because you're you're maybe more within it than i am but i'll just say um awareness is a very polysemous term like many terms in philosophy of minds but partially this one more than many others i think so um it can mean a lot of different things in all of context here we don't focus on things like consciousness because i think we probably both agree that it's a less tractable uh maybe empirical problem to try to assess the presence or absence of consciousness in language models even though many people are interested in that we think that we have more hope of making progress in the near term with more well-defined cognitive capacities or cognitive functions and things that relate to forms of certain forms of reasoning and viable binding etc um so we our framework and principle would apply to things like consciousness or as you put awareness generally speaking but we don't really focus on that for examples the other quick thing i'll just mention is that i seem to remember that the phrase from then but again i'm not a then scholar was competence without comprehension um which seems a little different from competence without awareness perhaps depending on how you think of comprehension um and yeah i think that does i think it is a very interesting phrase that it does um in fact i had this project that's unpublished with chris dolega who i think you had on the podcast as well um on semantic competence in language models where we use that phrase um to kind of avoid taking its stance on the kind of messy muddy question of whether hella let's understand language which builds in all sorts of assumptions including about consciousness actually for some people like chancel um and we focus on the more restricted notion of competence and i think our paper here also has that property that would if we have originalized competence we end up operationalizing competence in terms of the sets of knowledge of the mechanism and mechanisms that enable a system to generalize well in a given domain basically and in a way that's a supposed evolutionary compared to some more expensive understandings of competence that we need to comprehension or understanding more well but i'll let you take that one charles no yeah that was that was good um the phrase you know the distinction that denadra's is between competence with comprehension and without and i think um competence with comprehension is the ability not just to pursue a strategy that's successful for solving a problem but to um articulate the strategy in such a way that you could teach it for example and um humans only sometimes have competence with comprehension we have many competences that lack comprehension right um you know when we learn to walk for example um we have an amazing competence that we still can't quite translate into robotics because we don't fully understand how it works and when it comes to our language models i think we should not expect uh comprehension i mean they have a an amazing suite of competences if you thought that they also had comprehension then i suppose you would think like well if you want to understand how a large language model works you can just ask it but that's that's a bad strategy nobody nobody about um how a large language model works so so they're on the um competence without comprehension side of things um and in order to figure out what in order to figure out what the mechanisms are that enable its competencies we have to pursue strategies that are broadly similar to the strategies we use in you know cognitive psychology or cognitive linguistics um and you know we have to run experiments so i think that that's all very compatible with Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was quite influential to me and we actually wrote a commentary together which pushes back a little bit on a simple understanding of this distinction so we were looking at um the evolution of metacognition and basically what we argue is that um given the gradualism of evolution there must have been something in between base level cognition and metacognition so we shouldn't see that distinction as black and white and um you know i think that if you want to contrast the sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific uh concepts at her disposal with you know a non-human animal then this strong distinction between competence with and without comprehension is reasonable um but in the space of all possible minds we should be open to the view that there can be you know semi-competent um forms of cognition and just to put it on this uh it occurred to me while I was listening to you as well that um the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and Frank example is a nice is a nice example where in order to give the metacognistic judgments correctly so see that that you would need competence with comprehension because you need to understand not only be able to to come to to agree the verb with the subject but know the rule and know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so when you find that the L.M. can do well at the low task the member of the task and at the high task development explicit metacognistic judgments in some way that's an example of the L.M. having competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given that LLMs inherently reflect anthropocentric biases due to their training on human data and goals how can we ensure that their inter model discourse aligns with humanity's values um so the inter white discourse right be inter model discourse perhaps amongst the models I see I see like in that farmville paper yeah the small the yeah both generative agents get the small ones uh yeah I think that's beyond the scope of this paper to be honest but um I mean we could use about it yeah but I don't know that we have I don't know this project has much I mean I I think we both have an interest in the alignment problem uh independently of this project but I don't think this project has much to say really about this I'm not sure what you think uh yeah yeah I don't yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise into LLM training that was the section about the extra tokens do you see any analog to intermittent reinforcement to uncertainty tolerance because you mentioned the extra tokens in the chain of thought and how that could also be replaced by by dot dot dot dot dot and so like what is that telling us about model training when um it seems like there's some situations where adding superfluous tokens would diminish signal in data sets but then here are other situations where it seems to actually help um um yeah so in that particular paper I think it's called thinking dot by dot and there is a subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly what they did is that they they introduced just this one field of token swan meaning this token just to hold but just a dot and they trained the model to give an answer after producing a certain number of dots that's not just like introducing Rambam and Gibberish in your training data it's actually quite a specific intervention that forces the model to um learn to perform certain computations before giving an answer um so so it makes sense to me that this couldn't diminish performance and like you you could do that from that it's not quite the same as just having that training data right um just because the token seems meaningless it's a field of token to dot um it's not just random gibberish it's going to throw off the model and and impede its its uh the optimization of its learning function or at least good downstream performance um but what it's going to do is going to force the model to learn that when there is a dot token it can allocate computation with its attention heads and other parts of the architecture in such a way that it's um getting towards deriving the correct token when it's finally producing the token that matters and that's meaningful after the series of dots um yeah I don't know Charles if you have another answer yeah I mean I think it's an important question because a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless symbols to an LLM input you might very well think that this will just weaken the signal to noise ratio and degrade model performance so it's against that background that the empirical result doesn't degrade model performance um ought to be regarded as an important clue about how the model works so I think that the the intuition behind this question is indeed part of the interpretation of the empirical results right it's surprising for exactly this reason and then the theory that's supposed to you know well this is an active inference podcast right so the theory that's supposed to help rid some of the surprise here is um the idea that given the uh architecture of a transformer where it's it has to go through all the tokens in every cycle um having these extra tokens gives it uh sort of more computational bandwidth and therefore more expressivity or more capacity to uh you know locate uh the right spot and parameter space and and even that in a way reminds me of so it's not just that dots improve performance it's not it's that it was like you mentioned it was trained to have that and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes verbatim while you're processing so that's kind of like a filler or more of like a sort of that was a great question it's like these are linguistic paddings that that do create time to to get to the meet so not only does it signal and signpost if it's being trained to have that meaning which then questions like so that it wasn't a meaningless dot if it if it had a um a cognitive or even like a a semantic um aspect I had a question how do you feel like in this era Cambrian explosion of diverse intelligences how can we understand capacities when they seem so conditional upon the setting and how the system of interest is interacted with like what are the practical implications for people who are studying LLMs and other synthetic intelligences from like a safety or reliability or performance perspective that was we're gonna no I was drawing it to you so so so just want to chat to some of the questions so the question is how the how is the notion of a capacity um changing when we have such different systems that seem to have intelligent behavior yeah and it's so dependent upon potentially initially unintuitive ways of interacting so how can we understand the reliability and the performance and the capacity of of a model other than for example by exhaustively inputting prompts which can't really happen what what could we really say or no and or just how do you feel that this work re-enters into the ways that people practically are using the models right okay yeah uh it's the interesting question so the first one the question is I think you know part of the background assumption from for this paper that I've explicitly defended in other work is that behavioral evidence is simply not sufficient in most cases to arbitrate disputes about capacities of LLMs when it comes to human cognition we do have to rely a lot on on psychological experiments that are ultimately behavioral and we do also rely on self-reports in a little more than we can when it comes to LLMs because we despite the move away from relying on intuition and introspection in the history of psychology it still has a role to play but we've by and large contributed to us by behavioral experiments that get increasingly sophisticated to try to reverse engineer what's going on inside the black box when it comes to LLMs partly because that's so different from us um relying exclusively on behaviorism is even more difficult because we have even less of an idea of what might be going on inside the black box and whether it's anything like what's going on inside all black box and we have reasons to think it might be very different so I think I think we both agree that we have to supplement this with mechanistic work that's essentially involve performing causal interventions on the inner mechanisms on the inner workings of the systems so decoding representation and computations that the systems that are in principle available to the systems and then intervening on them to confirm hypotheses about the causal role of these representations and computations and we have methods to do that and partly what we can be a little optimistic about this project even though it's it's it's extremely challenging especially to be scaled up to large models is because unlike what's happening in your sense with the brain where the range of decoding methods and intervention methods we have is extremely limited but for ethical and for simply um practical reasons that we don't just don't have ground truth access to activations in neurons at least that easily and we also are generally unable to make specific interventions on activation in the brain uh when it comes to algorithms we have full ground truth knowledge of all activations of every single part of the network and we also have full access to all of it for interventions at inference time so that that opens up a whole new range of things we can do and that enables us to go beyond behavioral studies and actually decode these features and circuits or as researchers put it in the literature or as philosophers would generally put it representations and computations that the system is actually making use of and try to reverse engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem projects that we have with Charles is to um suppose that we start with these investigative kinds as we put as as Alibol calls them these human subject capacities we can operationalize them and do behavioral experiments in the top down and then from the bottom up we can also try to reverse engineer the mechanism building blocks of the computations and representations that evidence may use up to solve the task related to that particular capacity and then we can meet somewhere in the middle and try to from that line of work that are purchased things from above and bring to the fore some kinds of mid-level abstractions as we call it or computational building blocks that might be key to the performance of the system in that domain so for example if you're interested in the capacity for reasoning you can start with this very broad human-centric notion of reasoning then try to operationalize it in a reasoning task then do some behavioral testing and then mechanistic interpretability of that reasoning task find out how the system is solving it find out how the algorithm is doing well and why reverse engineer building blocks that might for example have to be viable manipulation viable binding and then from there you might be able to either actually refine the notion of reasoning you started with to have a more specific and that less human-centric notion that is now operationalized in more low-level terms like you know that both manipulation of variables in certain ways and the binding of variables to theories etc so yeah so that's I think the general approach we take now how does that does any of that feedback into interactions how we humans interact with algorithms I think that's one way in which you could feedback is simply in terms of challenging or our spontaneous anthropomorphic attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you will interact with your cat in a slightly different way that you might maybe not rush the conclusion that when your cat performs a certain behavior it has understood what you think and it's modeling what you're thinking about what it's thinking or something perhaps you might adopt a more deflationary attitude to explain the behavior of your cat doesn't mean you have to love them any less or it doesn't mean you have to you know if that is the other thing like if you want at the end of the day to speak to your cat like a human because you really are a gentleman for that then that's you know all the more part of you in the same way if you find it useful to treat LLMs in the way you interact with them to to have fluid interactions with them to treat them as if they had beliefs these are as etc of human-like capacities then that's fine if that's for actual purposes but at least if that line of work that we are kind of sketching here ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's well even if we if we have that kind of intentional sense and may believe about who the kinds of besties they have at the background we will know that what their limitations are and what their actual besties are i'd make sure to go with that Charles because we're going to use this much yeah yeah no i agree with all that i just had a slightly different first reaction to the question i took the question to be in part about how to deal with the sort of prompt sensitivity of models the fact that sometimes we you know write something that seems natural to us but provokes an unexpected response from a large language model and how do we think about that and the first thought that occurred to me was just that we should distinguish between different kinds of large language models you know we have this sort of huge large language models which are fine-tuned to interact with us in a particular way and our and here's the central point they're trained on a sort of unthinkably large database whereas there are other sorts of large language models where the training data is more circumscribed and where we know in more detail you know what where you can survey what the training data says and i think if you're interested in you know what the mechanisms are underlying the responses it's certainly very helpful to look at smaller but nevertheless large language models where the training data is known to us because you know when you train a model on the entire internet there are going to be all kinds of you know subtle signals in there that we don't have much hope of tracing back to their source but which will influence the model behavior in all sorts of ways but working with these somewhat more conscripted models gets rid of that problem at least in part cool well where where do you see the work going or where do you plan to continue this direction yeah so actually so we wrote this paper this short paper for the ICML machine learning conference in the national conference machine learning that's happening this week in the data and will be getting to Vienna at the end of the week for the popular workshop that we're representing this paper which is a workshop on language models and cognitive science so there will be a very strict page limit for these ICML contribution which is four pages but what we want to do next is to expand this into a more philosophically substantive paper that's going to be a bit longer and that's going to expand on the more philosophically meaty parts of that of that project because everything is still a bit compressed in that version that we're presenting at ICML so yeah that's the next step for us this is a really useful way for us to force ourselves to write things down after running the box piece we wanted to write an academic piece now we've written kind of a condense skeleton of the piece that focuses more that caters more to an analogians and now the next step is to write the full philosophy paper or at least that part of our project to be complete and then I don't have to that maybe we'll have all that ideas but yeah yeah I got nothing to add to that cool yes well it's very interesting work I think it it brings a lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the the heat and into the the spotlight and the relevance and so it's going to be an exciting learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation till next time thank you bye you you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 18.84, "text": " Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024", "tokens": [50364, 2425, 293, 2928, 1518, 13, 639, 307, 26635, 682, 5158, 24025, 24904, 29018, 13, 16, 322, 7370, 5853, 273, 11, 45237, 51306], "temperature": 0.0, "avg_logprob": -0.34573146275111605, "compression_ratio": 1.1733333333333333, "no_speech_prob": 0.27369561791419983}, {"id": 1, "seek": 0, "start": 18.84, "end": 24.8, "text": " on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre", "tokens": [51306, 322, 12727, 1513, 905, 32939, 363, 4609, 293, 264, 33112, 2841, 295, 5735, 10371, 383, 2912, 849, 365, 43173, 6395, 5486, 265, 51604], "temperature": 0.0, "avg_logprob": -0.34573146275111605, "compression_ratio": 1.1733333333333333, "no_speech_prob": 0.27369561791419983}, {"id": 2, "seek": 2480, "start": 24.8, "end": 32.04, "text": " and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you", "tokens": [50364, 293, 10523, 497, 998, 4093, 602, 13, 407, 11, 43173, 293, 10523, 11, 1309, 291, 588, 709, 11, 1293, 337, 5549, 11, 281, 291, 50726], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 3, "seek": 2480, "start": 32.04, "end": 35.68, "text": " for introductions and to take us through the paper.", "tokens": [50726, 337, 48032, 293, 281, 747, 505, 807, 264, 3035, 13, 50908], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 4, "seek": 2480, "start": 35.68, "end": 36.2, "text": " Oh, thank you.", "tokens": [50908, 876, 11, 1309, 291, 13, 50934], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 5, "seek": 2480, "start": 36.2, "end": 37.2, "text": " And welcome everyone.", "tokens": [50934, 400, 2928, 1518, 13, 50984], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 6, "seek": 2480, "start": 37.2, "end": 45.0, "text": " This is Active Inference Gas Stream 84.1 on July 22nd, 2024.", "tokens": [50984, 639, 307, 26635, 682, 5158, 24025, 24904, 29018, 13, 16, 322, 7370, 5853, 273, 11, 45237, 13, 51374], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 7, "seek": 2480, "start": 45.0, "end": 45.8, "text": " Thank you.", "tokens": [51374, 1044, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 8, "seek": 2480, "start": 45.8, "end": 49.760000000000005, "text": " Okay, thank you guys. Go for it.", "tokens": [51414, 1033, 11, 1309, 291, 1074, 13, 1037, 337, 309, 13, 51612], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 9, "seek": 2480, "start": 49.760000000000005, "end": 51.92, "text": " Hi, thanks for having us.", "tokens": [51612, 2421, 11, 3231, 337, 1419, 505, 13, 51720], "temperature": 0.0, "avg_logprob": -0.3179267961151746, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.19299229979515076}, {"id": 10, "seek": 5192, "start": 51.92, "end": 60.040000000000006, "text": " So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles?", "tokens": [50364, 407, 11, 286, 478, 43173, 13, 286, 478, 364, 10994, 8304, 412, 5707, 16177, 414, 3535, 294, 7060, 11, 21065, 11, 293, 10523, 30, 50770], "temperature": 0.0, "avg_logprob": -0.41669442576746785, "compression_ratio": 1.4226190476190477, "no_speech_prob": 0.006645780522376299}, {"id": 11, "seek": 5192, "start": 60.040000000000006, "end": 76.64, "text": " I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute.", "tokens": [50770, 286, 478, 10523, 497, 998, 4093, 602, 13, 286, 478, 257, 17034, 2132, 14644, 412, 264, 624, 1739, 10303, 5169, 294, 624, 1739, 11, 7244, 11, 294, 257, 955, 42762, 26860, 13, 51600], "temperature": 0.0, "avg_logprob": -0.41669442576746785, "compression_ratio": 1.4226190476190477, "no_speech_prob": 0.006645780522376299}, {"id": 12, "seek": 7664, "start": 76.68, "end": 79.84, "text": " So, should we go through the paper briefly?", "tokens": [50366, 407, 11, 820, 321, 352, 807, 264, 3035, 10515, 30, 50524], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 13, "seek": 7664, "start": 79.84, "end": 83.88, "text": " So, yeah, we wrote this paper together.", "tokens": [50524, 407, 11, 1338, 11, 321, 4114, 341, 3035, 1214, 13, 50726], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 14, "seek": 7664, "start": 83.88, "end": 89.92, "text": " Actually, we started by writing a general audience piece that was published in the box online.", "tokens": [50726, 5135, 11, 321, 1409, 538, 3579, 257, 2674, 4034, 2522, 300, 390, 6572, 294, 264, 2424, 2950, 13, 51028], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 15, "seek": 7664, "start": 89.92, "end": 91.68, "text": " Tell us when was that?", "tokens": [51028, 5115, 505, 562, 390, 300, 30, 51116], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 16, "seek": 7664, "start": 91.68, "end": 93.64, "text": " A few months ago, I guess.", "tokens": [51116, 316, 1326, 2493, 2057, 11, 286, 2041, 13, 51214], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 17, "seek": 7664, "start": 93.64, "end": 97.52, "text": " Yeah, I think it was close to a year ago, maybe.", "tokens": [51214, 865, 11, 286, 519, 309, 390, 1998, 281, 257, 1064, 2057, 11, 1310, 13, 51408], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 18, "seek": 7664, "start": 97.52, "end": 102.0, "text": " I don't think it was published a year ago. I think it was published in 2024.", "tokens": [51408, 286, 500, 380, 519, 309, 390, 6572, 257, 1064, 2057, 13, 286, 519, 309, 390, 6572, 294, 45237, 13, 51632], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 19, "seek": 7664, "start": 102.0, "end": 105.24000000000001, "text": " But yeah, we worked on it for a while.", "tokens": [51632, 583, 1338, 11, 321, 2732, 322, 309, 337, 257, 1339, 13, 51794], "temperature": 0.0, "avg_logprob": -0.20895860906232866, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.07354718446731567}, {"id": 20, "seek": 10524, "start": 105.24, "end": 113.56, "text": " Yeah, so this piece was doing, I guess, two things, that piece that we published in the box.", "tokens": [50364, 865, 11, 370, 341, 2522, 390, 884, 11, 286, 2041, 11, 732, 721, 11, 300, 2522, 300, 321, 6572, 294, 264, 2424, 13, 50780], "temperature": 0.0, "avg_logprob": -0.20457861396703828, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.0018922261660918593}, {"id": 21, "seek": 10524, "start": 113.56, "end": 117.64, "text": " It was pushing back against what we call the all-or-nothing principle,", "tokens": [50780, 467, 390, 7380, 646, 1970, 437, 321, 818, 264, 439, 12, 284, 12, 49518, 8665, 11, 50984], "temperature": 0.0, "avg_logprob": -0.20457861396703828, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.0018922261660918593}, {"id": 22, "seek": 10524, "start": 117.64, "end": 123.19999999999999, "text": " which we defined as the idea that either something has a mind or it doesn't.", "tokens": [50984, 597, 321, 7642, 382, 264, 1558, 300, 2139, 746, 575, 257, 1575, 420, 309, 1177, 380, 13, 51262], "temperature": 0.0, "avg_logprob": -0.20457861396703828, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.0018922261660918593}, {"id": 23, "seek": 10524, "start": 123.19999999999999, "end": 131.84, "text": " So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.", "tokens": [51262, 407, 11, 341, 733, 295, 10654, 457, 24324, 44199, 11, 4317, 11, 24808, 295, 721, 666, 36707, 293, 2107, 12, 23310, 721, 13, 51694], "temperature": 0.0, "avg_logprob": -0.20457861396703828, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.0018922261660918593}, {"id": 24, "seek": 13184, "start": 131.88, "end": 136.84, "text": " And we argued that this was not the best framing to think of,", "tokens": [50366, 400, 321, 20219, 300, 341, 390, 406, 264, 1151, 28971, 281, 519, 295, 11, 50614], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 25, "seek": 13184, "start": 136.84, "end": 142.96, "text": " especially, and family, our systems that seem to have sophisticated behavioral capacities,", "tokens": [50614, 2318, 11, 293, 1605, 11, 527, 3652, 300, 1643, 281, 362, 16950, 19124, 39396, 11, 50920], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 26, "seek": 13184, "start": 142.96, "end": 146.28, "text": " like large language models or AIC systems in general,", "tokens": [50920, 411, 2416, 2856, 5245, 420, 316, 2532, 3652, 294, 2674, 11, 51086], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 27, "seek": 13184, "start": 146.28, "end": 150.4, "text": " where we don't want to take various cognitive capacities of the package", "tokens": [51086, 689, 321, 500, 380, 528, 281, 747, 3683, 15605, 39396, 295, 264, 7372, 51292], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 28, "seek": 13184, "start": 150.4, "end": 154.16, "text": " and package them into this idea of a mind, where either you have the mind or you have them,", "tokens": [51292, 293, 7372, 552, 666, 341, 1558, 295, 257, 1575, 11, 689, 2139, 291, 362, 264, 1575, 420, 291, 362, 552, 11, 51480], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 29, "seek": 13184, "start": 154.16, "end": 159.92000000000002, "text": " then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc.", "tokens": [51480, 550, 498, 291, 362, 264, 1575, 11, 291, 362, 439, 295, 613, 721, 382, 257, 7372, 11, 10081, 11, 21577, 11, 5183, 13, 51768], "temperature": 0.0, "avg_logprob": -0.24153766116580447, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0018586252117529511}, {"id": 30, "seek": 15992, "start": 159.92, "end": 163.83999999999997, "text": " planning, memory, theory of minds.", "tokens": [50364, 5038, 11, 4675, 11, 5261, 295, 9634, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 31, "seek": 15992, "start": 163.83999999999997, "end": 169.27999999999997, "text": " So, we thought, as a remedy to this kind of all-or-nothing approach,", "tokens": [50560, 407, 11, 321, 1194, 11, 382, 257, 31648, 281, 341, 733, 295, 439, 12, 284, 12, 49518, 3109, 11, 50832], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 32, "seek": 15992, "start": 169.27999999999997, "end": 173.83999999999997, "text": " we argued for what we call the divide and conquer strategy", "tokens": [50832, 321, 20219, 337, 437, 321, 818, 264, 9845, 293, 24136, 5206, 51060], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 33, "seek": 15992, "start": 173.83999999999997, "end": 177.04, "text": " when studying the cognitive capacities of these systems,", "tokens": [51060, 562, 7601, 264, 15605, 39396, 295, 613, 3652, 11, 51220], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 34, "seek": 15992, "start": 177.04, "end": 182.48, "text": " which involved looking at these capacities on a piecemeal basis, case-by-case,", "tokens": [51220, 597, 3288, 1237, 412, 613, 39396, 322, 257, 2522, 32914, 5143, 11, 1389, 12, 2322, 12, 9765, 11, 51492], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 35, "seek": 15992, "start": 182.48, "end": 186.07999999999998, "text": " with an open-minded empirical approach.", "tokens": [51492, 365, 364, 1269, 12, 23310, 31886, 3109, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2679616116929328, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.0024672383442521095}, {"id": 36, "seek": 18608, "start": 187.04000000000002, "end": 193.04000000000002, "text": " Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds.", "tokens": [50412, 865, 11, 10523, 11, 286, 500, 380, 458, 498, 291, 362, 1340, 1646, 300, 311, 760, 294, 264, 3642, 11, 2522, 420, 264, 17336, 13, 50712], "temperature": 0.0, "avg_logprob": -0.3074425213957486, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00218001427128911}, {"id": 37, "seek": 18608, "start": 193.04000000000002, "end": 203.76000000000002, "text": " Yeah, I mean, we made one point in there about why it is that people feel so torn about", "tokens": [50712, 865, 11, 286, 914, 11, 321, 1027, 472, 935, 294, 456, 466, 983, 309, 307, 300, 561, 841, 370, 10885, 466, 51248], "temperature": 0.0, "avg_logprob": -0.3074425213957486, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00218001427128911}, {"id": 38, "seek": 18608, "start": 203.76000000000002, "end": 211.60000000000002, "text": " reactions to large language models, and we said a little bit about the psychology of essentialism,", "tokens": [51248, 12215, 281, 2416, 2856, 5245, 11, 293, 321, 848, 257, 707, 857, 466, 264, 15105, 295, 7115, 1434, 11, 51640], "temperature": 0.0, "avg_logprob": -0.3074425213957486, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00218001427128911}, {"id": 39, "seek": 21160, "start": 211.68, "end": 218.96, "text": " which is the idea that we naturally categorize especially living things", "tokens": [50368, 597, 307, 264, 1558, 300, 321, 8195, 19250, 1125, 2318, 2647, 721, 50732], "temperature": 0.0, "avg_logprob": -0.11365440913609096, "compression_ratio": 1.5515151515151515, "no_speech_prob": 0.001596233807504177}, {"id": 40, "seek": 21160, "start": 219.92, "end": 226.32, "text": " with respect to a presumed essence. So, we gave an example of an oak tree, I think,", "tokens": [50780, 365, 3104, 281, 257, 18028, 292, 12801, 13, 407, 11, 321, 2729, 364, 1365, 295, 364, 31322, 4230, 11, 286, 519, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11365440913609096, "compression_ratio": 1.5515151515151515, "no_speech_prob": 0.001596233807504177}, {"id": 41, "seek": 21160, "start": 227.92, "end": 234.88, "text": " and we said that what people tend to think as they grow up and learn about the natural world is that", "tokens": [51180, 293, 321, 848, 300, 437, 561, 3928, 281, 519, 382, 436, 1852, 493, 293, 1466, 466, 264, 3303, 1002, 307, 300, 51528], "temperature": 0.0, "avg_logprob": -0.11365440913609096, "compression_ratio": 1.5515151515151515, "no_speech_prob": 0.001596233807504177}, {"id": 42, "seek": 23488, "start": 235.6, "end": 245.28, "text": " an oak tree remains an oak tree regardless of changes to its observable properties,", "tokens": [50400, 364, 31322, 4230, 7023, 364, 31322, 4230, 10060, 295, 2962, 281, 1080, 9951, 712, 7221, 11, 50884], "temperature": 0.0, "avg_logprob": -0.15531667612366756, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0008690308895893395}, {"id": 43, "seek": 23488, "start": 246.07999999999998, "end": 255.2, "text": " and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has,", "tokens": [50924, 293, 437, 1669, 364, 31322, 4230, 307, 341, 8526, 929, 1978, 712, 12801, 295, 31322, 1287, 11, 420, 2035, 309, 26742, 575, 11, 51380], "temperature": 0.0, "avg_logprob": -0.15531667612366756, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0008690308895893395}, {"id": 44, "seek": 23488, "start": 255.2, "end": 260.96, "text": " and there's some experimental psychology and developmental psychology showing that we", "tokens": [51380, 293, 456, 311, 512, 17069, 15105, 293, 30160, 15105, 4099, 300, 321, 51668], "temperature": 0.0, "avg_logprob": -0.15531667612366756, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0008690308895893395}, {"id": 45, "seek": 26096, "start": 261.91999999999996, "end": 271.35999999999996, "text": " have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative", "tokens": [50412, 362, 257, 2531, 10157, 3030, 36707, 1287, 420, 1419, 257, 1575, 11, 293, 300, 307, 257, 8344, 49415, 50884], "temperature": 0.0, "avg_logprob": -0.07411996523539226, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0008423025137744844}, {"id": 46, "seek": 26096, "start": 271.35999999999996, "end": 276.64, "text": " explanation for why the literature on large language models is so torn, and some people", "tokens": [50884, 10835, 337, 983, 264, 10394, 322, 2416, 2856, 5245, 307, 370, 10885, 11, 293, 512, 561, 51148], "temperature": 0.0, "avg_logprob": -0.07411996523539226, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0008423025137744844}, {"id": 47, "seek": 26096, "start": 276.64, "end": 285.76, "text": " are quite dismissive, and other people think that it's a step away from AGI. It's that if you", "tokens": [51148, 366, 1596, 16974, 488, 11, 293, 661, 561, 519, 300, 309, 311, 257, 1823, 1314, 490, 316, 26252, 13, 467, 311, 300, 498, 291, 51604], "temperature": 0.0, "avg_logprob": -0.07411996523539226, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0008423025137744844}, {"id": 48, "seek": 28576, "start": 286.71999999999997, "end": 293.84, "text": " feel like you've got to put large language models into one of two boxes,", "tokens": [50412, 841, 411, 291, 600, 658, 281, 829, 2416, 2856, 5245, 666, 472, 295, 732, 9002, 11, 50768], "temperature": 0.0, "avg_logprob": -0.08819269033578726, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0025103366933763027}, {"id": 49, "seek": 28576, "start": 294.96, "end": 303.59999999999997, "text": " the box that has the essence of mindedness for the box that lacks it, then you will be forced", "tokens": [50824, 264, 2424, 300, 575, 264, 12801, 295, 36707, 1287, 337, 264, 2424, 300, 31132, 309, 11, 550, 291, 486, 312, 7579, 51256], "temperature": 0.0, "avg_logprob": -0.08819269033578726, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0025103366933763027}, {"id": 50, "seek": 28576, "start": 303.59999999999997, "end": 310.24, "text": " either to say it doesn't have what it takes to do any of the things that we associate with", "tokens": [51256, 2139, 281, 584, 309, 1177, 380, 362, 437, 309, 2516, 281, 360, 604, 295, 264, 721, 300, 321, 14644, 365, 51588], "temperature": 0.0, "avg_logprob": -0.08819269033578726, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0025103366933763027}, {"id": 51, "seek": 31024, "start": 310.24, "end": 318.64, "text": " having a mind such as reasoning, or it has the essential characteristics of mindedness,", "tokens": [50364, 1419, 257, 1575, 1270, 382, 21577, 11, 420, 309, 575, 264, 7115, 10891, 295, 36707, 1287, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15456116036193013, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002213924191892147}, {"id": 52, "seek": 31024, "start": 318.64, "end": 323.36, "text": " and therefore we should expect it to have all of the other properties we associate with", "tokens": [50784, 293, 4412, 321, 820, 2066, 309, 281, 362, 439, 295, 264, 661, 7221, 321, 14644, 365, 51020], "temperature": 0.0, "avg_logprob": -0.15456116036193013, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002213924191892147}, {"id": 53, "seek": 31024, "start": 323.36, "end": 329.92, "text": " mindedness as well such as consciousness or understanding or whatever.", "tokens": [51020, 36707, 1287, 382, 731, 1270, 382, 10081, 420, 3701, 420, 2035, 13, 51348], "temperature": 0.0, "avg_logprob": -0.15456116036193013, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002213924191892147}, {"id": 54, "seek": 31024, "start": 332.48, "end": 339.36, "text": " Right, I think it's worth emphasizing as you did that the background motivation for starting to", "tokens": [51476, 1779, 11, 286, 519, 309, 311, 3163, 45550, 382, 291, 630, 300, 264, 3678, 12335, 337, 2891, 281, 51820], "temperature": 0.0, "avg_logprob": -0.15456116036193013, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002213924191892147}, {"id": 55, "seek": 33936, "start": 339.36, "end": 346.08000000000004, "text": " write on this general piece in the first place is indeed that the general discourse on AI systems", "tokens": [50364, 2464, 322, 341, 2674, 2522, 294, 264, 700, 1081, 307, 6451, 300, 264, 2674, 23938, 322, 7318, 3652, 50700], "temperature": 0.0, "avg_logprob": -0.14016265869140626, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0031965223606675863}, {"id": 56, "seek": 33936, "start": 346.08000000000004, "end": 354.08000000000004, "text": " and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so", "tokens": [50700, 293, 441, 43, 26386, 294, 1729, 307, 4664, 48623, 294, 257, 636, 300, 307, 588, 10390, 42939, 563, 293, 17417, 11, 370, 51100], "temperature": 0.0, "avg_logprob": -0.14016265869140626, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0031965223606675863}, {"id": 57, "seek": 33936, "start": 354.08000000000004, "end": 360.16, "text": " you have one ahead people and one ahead people arguing that these systems are no more than", "tokens": [51100, 291, 362, 472, 2286, 561, 293, 472, 2286, 561, 19697, 300, 613, 3652, 366, 572, 544, 813, 51404], "temperature": 0.0, "avg_logprob": -0.14016265869140626, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0031965223606675863}, {"id": 58, "seek": 33936, "start": 361.12, "end": 366.64, "text": " so-called stochastic parrots that are haphazardly stitching together samples from the training data", "tokens": [51452, 370, 12, 11880, 342, 8997, 2750, 971, 81, 1971, 300, 366, 324, 950, 921, 515, 356, 30714, 1214, 10938, 490, 264, 3097, 1412, 51728], "temperature": 0.0, "avg_logprob": -0.14016265869140626, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0031965223606675863}, {"id": 59, "seek": 36664, "start": 366.64, "end": 372.88, "text": " and regurgitating them, or that they are no smarter than toaster or that they only do next", "tokens": [50364, 293, 1121, 5476, 16350, 552, 11, 420, 300, 436, 366, 572, 20294, 813, 281, 1727, 420, 300, 436, 787, 360, 958, 50676], "temperature": 0.0, "avg_logprob": -0.26986834767100576, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.002343876054510474}, {"id": 60, "seek": 36664, "start": 372.88, "end": 380.56, "text": " stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form", "tokens": [50676, 342, 8997, 2750, 17630, 322, 16197, 293, 4412, 309, 307, 257, 2107, 12, 33969, 281, 382, 8056, 281, 552, 604, 1254, 51060], "temperature": 0.0, "avg_logprob": -0.26986834767100576, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.002343876054510474}, {"id": 61, "seek": 36664, "start": 380.56, "end": 385.84, "text": " of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum,", "tokens": [51060, 295, 15605, 6042, 420, 1310, 754, 257, 7719, 6146, 11, 293, 322, 264, 661, 917, 295, 264, 11143, 11, 51324], "temperature": 0.0, "avg_logprob": -0.26986834767100576, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.002343876054510474}, {"id": 62, "seek": 36664, "start": 385.84, "end": 390.08, "text": " on the other end of the spectrum you have people arguing that the systems are haphazardly", "tokens": [51324, 322, 264, 661, 917, 295, 264, 11143, 291, 362, 561, 19697, 300, 264, 3652, 366, 324, 950, 921, 515, 356, 51536], "temperature": 0.0, "avg_logprob": -0.26986834767100576, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.002343876054510474}, {"id": 63, "seek": 39008, "start": 390.08, "end": 395.12, "text": " jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to", "tokens": [50364, 12721, 830, 295, 1687, 18796, 7599, 11, 300, 436, 20487, 44102, 295, 11677, 2674, 7599, 281, 50616], "temperature": 0.0, "avg_logprob": -0.2717926119580681, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.01658095419406891}, {"id": 64, "seek": 39008, "start": 396.15999999999997, "end": 402.47999999999996, "text": " parrots, it's literally the title of a paper by Microsoft on 24,", "tokens": [50668, 971, 81, 1971, 11, 309, 311, 3736, 264, 4876, 295, 257, 3035, 538, 8116, 322, 4022, 11, 50984], "temperature": 0.0, "avg_logprob": -0.2717926119580681, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.01658095419406891}, {"id": 65, "seek": 39008, "start": 404.96, "end": 410.79999999999995, "text": " and many people hyping up the capacity of the systems in a way that might seem very speculative", "tokens": [51108, 293, 867, 561, 2477, 3381, 493, 264, 6042, 295, 264, 3652, 294, 257, 636, 300, 1062, 1643, 588, 49415, 51400], "temperature": 0.0, "avg_logprob": -0.2717926119580681, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.01658095419406891}, {"id": 66, "seek": 39008, "start": 410.79999999999995, "end": 416.96, "text": " and untethered from actual empirical results, so there is this huge gap between these two positions", "tokens": [51400, 293, 1701, 1666, 292, 490, 3539, 31886, 3542, 11, 370, 456, 307, 341, 2603, 7417, 1296, 613, 732, 8432, 51708], "temperature": 0.0, "avg_logprob": -0.2717926119580681, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.01658095419406891}, {"id": 67, "seek": 41696, "start": 416.96, "end": 422.0, "text": " and there's going to be a very rich and complex and nuanced middle ground that is underexplored,", "tokens": [50364, 293, 456, 311, 516, 281, 312, 257, 588, 4593, 293, 3997, 293, 45115, 2808, 2727, 300, 307, 674, 323, 87, 564, 2769, 11, 50616], "temperature": 0.0, "avg_logprob": -0.22146495357974544, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002968207933008671}, {"id": 68, "seek": 41696, "start": 424.08, "end": 430.64, "text": " or perhaps I think we did make that point, if not explicitly in the published piece", "tokens": [50720, 420, 4317, 286, 519, 321, 630, 652, 300, 935, 11, 498, 406, 20803, 294, 264, 6572, 2522, 51048], "temperature": 0.0, "avg_logprob": -0.22146495357974544, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002968207933008671}, {"id": 69, "seek": 41696, "start": 430.64, "end": 435.91999999999996, "text": " and in some draft, that there's something reassuring about being able to make definitive", "tokens": [51048, 293, 294, 512, 11206, 11, 300, 456, 311, 746, 19486, 1345, 466, 885, 1075, 281, 652, 28152, 51312], "temperature": 0.0, "avg_logprob": -0.22146495357974544, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002968207933008671}, {"id": 70, "seek": 41696, "start": 435.91999999999996, "end": 442.24, "text": " claims about what these systems are and what they do, so either they're re-unsophisticated or they're", "tokens": [51312, 9441, 466, 437, 613, 3652, 366, 293, 437, 436, 360, 11, 370, 2139, 436, 434, 319, 12, 26684, 5317, 3142, 770, 420, 436, 434, 51628], "temperature": 0.0, "avg_logprob": -0.22146495357974544, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002968207933008671}, {"id": 71, "seek": 44224, "start": 442.96000000000004, "end": 451.76, "text": " very much like us and either of these claims kind of meet somewhere in an way in saying that we", "tokens": [50400, 588, 709, 411, 505, 293, 2139, 295, 613, 9441, 733, 295, 1677, 4079, 294, 364, 636, 294, 1566, 300, 321, 50840], "temperature": 0.0, "avg_logprob": -0.14092142685599948, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0025856380816549063}, {"id": 72, "seek": 44224, "start": 451.76, "end": 456.96000000000004, "text": " have a clear idea of what the systems do and what they are, and I think it's a little more", "tokens": [50840, 362, 257, 1850, 1558, 295, 437, 264, 3652, 360, 293, 437, 436, 366, 11, 293, 286, 519, 309, 311, 257, 707, 544, 51100], "temperature": 0.0, "avg_logprob": -0.14092142685599948, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0025856380816549063}, {"id": 73, "seek": 44224, "start": 456.96000000000004, "end": 466.32, "text": " epistemic and comfortable to say we have to study them empirically and find out what they", "tokens": [51100, 2388, 468, 3438, 293, 4619, 281, 584, 321, 362, 281, 2979, 552, 25790, 984, 293, 915, 484, 437, 436, 51568], "temperature": 0.0, "avg_logprob": -0.14092142685599948, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0025856380816549063}, {"id": 74, "seek": 46632, "start": 466.32, "end": 472.08, "text": " can or cannot do and why and what are the underlying mechanisms, and we simply don't know", "tokens": [50364, 393, 420, 2644, 360, 293, 983, 293, 437, 366, 264, 14217, 15902, 11, 293, 321, 2935, 500, 380, 458, 50652], "temperature": 0.0, "avg_logprob": -0.16360490662711008, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.001621072180569172}, {"id": 75, "seek": 46632, "start": 472.8, "end": 478.8, "text": " a priori just by looking at the architecture, the learning objective, the training data,", "tokens": [50688, 257, 4059, 72, 445, 538, 1237, 412, 264, 9482, 11, 264, 2539, 10024, 11, 264, 3097, 1412, 11, 50988], "temperature": 0.0, "avg_logprob": -0.16360490662711008, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.001621072180569172}, {"id": 76, "seek": 46632, "start": 479.44, "end": 484.48, "text": " these sources of evidence are insufficient to make the definitive claims about what these systems", "tokens": [51020, 613, 7139, 295, 4467, 366, 41709, 281, 652, 264, 28152, 9441, 466, 437, 613, 3652, 51272], "temperature": 0.0, "avg_logprob": -0.16360490662711008, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.001621072180569172}, {"id": 77, "seek": 46632, "start": 484.48, "end": 491.28, "text": " are capable of, so I think that's part of the big part of the motivation and that fits into that", "tokens": [51272, 366, 8189, 295, 11, 370, 286, 519, 300, 311, 644, 295, 264, 955, 644, 295, 264, 12335, 293, 300, 9001, 666, 300, 51612], "temperature": 0.0, "avg_logprob": -0.16360490662711008, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.001621072180569172}, {"id": 78, "seek": 49128, "start": 492.08, "end": 500.32, "text": " more academic paper as well. Yeah, one other small side note which we don't make", "tokens": [50404, 544, 7778, 3035, 382, 731, 13, 865, 11, 472, 661, 1359, 1252, 3637, 597, 321, 500, 380, 652, 50816], "temperature": 0.0, "avg_logprob": -0.17090921690969757, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00482761487364769}, {"id": 79, "seek": 49128, "start": 500.32, "end": 506.64, "text": " in the paper but I think might be relevant, especially for people working in philosophy,", "tokens": [50816, 294, 264, 3035, 457, 286, 519, 1062, 312, 7340, 11, 2318, 337, 561, 1364, 294, 10675, 11, 51132], "temperature": 0.0, "avg_logprob": -0.17090921690969757, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00482761487364769}, {"id": 80, "seek": 49128, "start": 508.08, "end": 513.76, "text": " LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which", "tokens": [51204, 441, 5251, 82, 366, 2388, 43958, 984, 10532, 11, 286, 519, 300, 390, 264, 9535, 291, 445, 1143, 11, 497, 13957, 11, 597, 51488], "temperature": 0.0, "avg_logprob": -0.17090921690969757, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00482761487364769}, {"id": 81, "seek": 51376, "start": 513.76, "end": 521.84, "text": " is fitting, not only because they're so new and different but also because they are artifacts,", "tokens": [50364, 307, 15669, 11, 406, 787, 570, 436, 434, 370, 777, 293, 819, 457, 611, 570, 436, 366, 24617, 11, 50768], "temperature": 0.0, "avg_logprob": -0.11069514653454088, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.01824885979294777}, {"id": 82, "seek": 51376, "start": 521.84, "end": 526.08, "text": " right, they're things that humans have constructed and engineered and", "tokens": [50768, 558, 11, 436, 434, 721, 300, 6255, 362, 17083, 293, 38648, 293, 50980], "temperature": 0.0, "avg_logprob": -0.11069514653454088, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.01824885979294777}, {"id": 83, "seek": 51376, "start": 528.88, "end": 534.8, "text": " we don't have a thorough understanding of how they work, I mean mechanistic interpretability and", "tokens": [51120, 321, 500, 380, 362, 257, 12934, 3701, 295, 577, 436, 589, 11, 286, 914, 4236, 3142, 7302, 2310, 293, 51416], "temperature": 0.0, "avg_logprob": -0.11069514653454088, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.01824885979294777}, {"id": 84, "seek": 51376, "start": 534.8, "end": 543.12, "text": " various behavioral research is helping us improve our understanding but on the whole", "tokens": [51416, 3683, 19124, 2132, 307, 4315, 505, 3470, 527, 3701, 457, 322, 264, 1379, 51832], "temperature": 0.0, "avg_logprob": -0.11069514653454088, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.01824885979294777}, {"id": 85, "seek": 54312, "start": 543.12, "end": 546.5600000000001, "text": " our understanding is not nearly as deep as hopefully it one day will be", "tokens": [50364, 527, 3701, 307, 406, 6217, 382, 2452, 382, 4696, 309, 472, 786, 486, 312, 50536], "temperature": 0.0, "avg_logprob": -0.12114285133980415, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0007090099388733506}, {"id": 86, "seek": 54312, "start": 548.24, "end": 553.52, "text": " and this is by itself a really strange situation to be in that we've constructed an artifact that", "tokens": [50620, 293, 341, 307, 538, 2564, 257, 534, 5861, 2590, 281, 312, 294, 300, 321, 600, 17083, 364, 34806, 300, 50884], "temperature": 0.0, "avg_logprob": -0.12114285133980415, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0007090099388733506}, {"id": 87, "seek": 54312, "start": 553.52, "end": 564.96, "text": " we only partially understand in the in the past artificial intelligence was seen as a way of", "tokens": [50884, 321, 787, 18886, 1223, 294, 264, 294, 264, 1791, 11677, 7599, 390, 1612, 382, 257, 636, 295, 51456], "temperature": 0.0, "avg_logprob": -0.12114285133980415, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0007090099388733506}, {"id": 88, "seek": 54312, "start": 566.8, "end": 570.16, "text": " constructing something like an epistemic assistant, right, something that will", "tokens": [51548, 39969, 746, 411, 364, 2388, 468, 3438, 10994, 11, 558, 11, 746, 300, 486, 51716], "temperature": 0.0, "avg_logprob": -0.12114285133980415, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0007090099388733506}, {"id": 89, "seek": 57016, "start": 571.12, "end": 577.6, "text": " help us but not something that will kind of alienate us from the process of coming to know", "tokens": [50412, 854, 505, 457, 406, 746, 300, 486, 733, 295, 12319, 473, 505, 490, 264, 1399, 295, 1348, 281, 458, 50736], "temperature": 0.0, "avg_logprob": -0.09197035912544496, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.0029784338548779488}, {"id": 90, "seek": 57016, "start": 577.6, "end": 584.4, "text": " about the world so I think there's an extra layer of discomfort built into thinking about", "tokens": [50736, 466, 264, 1002, 370, 286, 519, 456, 311, 364, 2857, 4583, 295, 28552, 3094, 666, 1953, 466, 51076], "temperature": 0.0, "avg_logprob": -0.09197035912544496, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.0029784338548779488}, {"id": 91, "seek": 57016, "start": 584.4, "end": 594.64, "text": " large language models and that may also play into the the divisiveness of debates about what they", "tokens": [51076, 2416, 2856, 5245, 293, 300, 815, 611, 862, 666, 264, 264, 25974, 8477, 295, 24203, 466, 437, 436, 51588], "temperature": 0.0, "avg_logprob": -0.09197035912544496, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.0029784338548779488}, {"id": 92, "seek": 59464, "start": 594.64, "end": 603.1999999999999, "text": " can do and just to add to that I guess we should be clear that this does not entail in any way that", "tokens": [50364, 393, 360, 293, 445, 281, 909, 281, 300, 286, 2041, 321, 820, 312, 1850, 300, 341, 775, 406, 948, 864, 294, 604, 636, 300, 50792], "temperature": 0.0, "avg_logprob": -0.08899823454923408, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.01791367493569851}, {"id": 93, "seek": 59464, "start": 603.1999999999999, "end": 610.8, "text": " we think the systems are so completely alien and beyond the reach of our current understanding that", "tokens": [50792, 321, 519, 264, 3652, 366, 370, 2584, 12319, 293, 4399, 264, 2524, 295, 527, 2190, 3701, 300, 51172], "temperature": 0.0, "avg_logprob": -0.08899823454923408, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.01791367493569851}, {"id": 94, "seek": 59464, "start": 610.8, "end": 617.68, "text": " anything goes and that they could very well be you know have like human-like intelligence or", "tokens": [51172, 1340, 1709, 293, 300, 436, 727, 588, 731, 312, 291, 458, 362, 411, 1952, 12, 4092, 7599, 420, 51516], "temperature": 0.0, "avg_logprob": -0.08899823454923408, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.01791367493569851}, {"id": 95, "seek": 59464, "start": 617.68, "end": 621.84, "text": " superhuman intelligence and we simply cannot say whether or not they do or because that's", "tokens": [51516, 1687, 18796, 7599, 293, 321, 2935, 2644, 584, 1968, 420, 406, 436, 360, 420, 570, 300, 311, 51724], "temperature": 0.0, "avg_logprob": -0.08899823454923408, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.01791367493569851}, {"id": 96, "seek": 62184, "start": 621.9200000000001, "end": 628.08, "text": " sometimes what you see in some outfits where people frame these systems as noble alien forms of", "tokens": [50368, 2171, 437, 291, 536, 294, 512, 22331, 689, 561, 3920, 613, 3652, 382, 20171, 12319, 6422, 295, 50676], "temperature": 0.0, "avg_logprob": -0.17801886796951294, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005957999732345343}, {"id": 97, "seek": 62184, "start": 628.08, "end": 632.96, "text": " intelligence that we have created but do not understand our control and that is you know", "tokens": [50676, 7599, 300, 321, 362, 2942, 457, 360, 406, 1223, 527, 1969, 293, 300, 307, 291, 458, 50920], "temperature": 0.0, "avg_logprob": -0.17801886796951294, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005957999732345343}, {"id": 98, "seek": 62184, "start": 632.96, "end": 638.32, "text": " as a slippery slope that leads some people to then claim that they have all these quite magical", "tokens": [50920, 382, 257, 28100, 13525, 300, 6689, 512, 561, 281, 550, 3932, 300, 436, 362, 439, 613, 1596, 12066, 51188], "temperature": 0.0, "avg_logprob": -0.17801886796951294, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005957999732345343}, {"id": 99, "seek": 62184, "start": 639.0400000000001, "end": 643.12, "text": " abilities and that's not all what we want to say here and in fact we want to resist", "tokens": [51224, 11582, 293, 300, 311, 406, 439, 437, 321, 528, 281, 584, 510, 293, 294, 1186, 321, 528, 281, 4597, 51428], "temperature": 0.0, "avg_logprob": -0.17801886796951294, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005957999732345343}, {"id": 100, "seek": 64312, "start": 644.08, "end": 648.8, "text": " yeah yeah yeah so if we we think that that's just as much of a cup out as", "tokens": [50412, 1338, 1338, 1338, 370, 498, 321, 321, 519, 300, 300, 311, 445, 382, 709, 295, 257, 4414, 484, 382, 50648], "temperature": 0.0, "avg_logprob": -0.17692172527313232, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.008569780737161636}, {"id": 101, "seek": 64312, "start": 650.72, "end": 656.64, "text": " completely dismissing a priori what these systems might be capable of without doing the", "tokens": [50744, 2584, 16974, 278, 257, 4059, 72, 437, 613, 3652, 1062, 312, 8189, 295, 1553, 884, 264, 51040], "temperature": 0.0, "avg_logprob": -0.17692172527313232, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.008569780737161636}, {"id": 102, "seek": 64312, "start": 656.64, "end": 660.88, "text": " work of looking into the capacities with behavioral and mechanistic studies so", "tokens": [51040, 589, 295, 1237, 666, 264, 39396, 365, 19124, 293, 4236, 3142, 5313, 370, 51252], "temperature": 0.0, "avg_logprob": -0.17692172527313232, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.008569780737161636}, {"id": 103, "seek": 64312, "start": 660.88, "end": 664.64, "text": " we very much want to resist both extremes of the spectrum if that makes sense", "tokens": [51252, 321, 588, 709, 528, 281, 4597, 1293, 41119, 295, 264, 11143, 498, 300, 1669, 2020, 51440], "temperature": 0.0, "avg_logprob": -0.17692172527313232, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.008569780737161636}, {"id": 104, "seek": 64312, "start": 666.72, "end": 670.24, "text": " okay so now should we move towards the content of the current paper", "tokens": [51544, 1392, 370, 586, 820, 321, 1286, 3030, 264, 2701, 295, 264, 2190, 3035, 51720], "temperature": 0.0, "avg_logprob": -0.17692172527313232, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.008569780737161636}, {"id": 105, "seek": 67024, "start": 670.24, "end": 677.04, "text": " sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism", "tokens": [50364, 3263, 665, 1338, 1392, 1310, 49690, 338, 286, 603, 445, 722, 365, 264, 16844, 1296, 22727, 32702, 1434, 50704], "temperature": 0.0, "avg_logprob": -0.10641948382059734, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.001895667053759098}, {"id": 106, "seek": 67024, "start": 677.04, "end": 687.28, "text": " and anthropocentrism and then you can take the next step so everyone is aware of the problem of", "tokens": [50704, 293, 22727, 905, 317, 81, 1434, 293, 550, 291, 393, 747, 264, 958, 1823, 370, 1518, 307, 3650, 295, 264, 1154, 295, 51216], "temperature": 0.0, "avg_logprob": -0.10641948382059734, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.001895667053759098}, {"id": 107, "seek": 67024, "start": 687.92, "end": 693.44, "text": " anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting", "tokens": [51248, 22727, 32702, 299, 12577, 294, 512, 1254, 286, 914, 22727, 32702, 1434, 307, 445, 264, 1558, 295, 43001, 51524], "temperature": 0.0, "avg_logprob": -0.10641948382059734, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.001895667053759098}, {"id": 108, "seek": 69344, "start": 693.44, "end": 702.72, "text": " human qualities onto something non-human and it's quite easy to especially when you're having a", "tokens": [50364, 1952, 16477, 3911, 746, 2107, 12, 18796, 293, 309, 311, 1596, 1858, 281, 2318, 562, 291, 434, 1419, 257, 50828], "temperature": 0.0, "avg_logprob": -0.056650949375970025, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.1478135585784912}, {"id": 109, "seek": 69344, "start": 702.72, "end": 708.8800000000001, "text": " productive successful exchange with a large language model it's easy to slip into this", "tokens": [50828, 13304, 4406, 7742, 365, 257, 2416, 2856, 2316, 309, 311, 1858, 281, 11140, 666, 341, 51136], "temperature": 0.0, "avg_logprob": -0.056650949375970025, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.1478135585784912}, {"id": 110, "seek": 69344, "start": 709.9200000000001, "end": 718.1600000000001, "text": " interpretive mode where you reason about the responses of the large language model", "tokens": [51188, 7302, 488, 4391, 689, 291, 1778, 466, 264, 13019, 295, 264, 2416, 2856, 2316, 51600], "temperature": 0.0, "avg_logprob": -0.056650949375970025, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.1478135585784912}, {"id": 111, "seek": 71816, "start": 718.8, "end": 727.68, "text": " as if they were coming from an agent just like you and maybe that's a useful thing to do in some", "tokens": [50396, 382, 498, 436, 645, 1348, 490, 364, 9461, 445, 411, 291, 293, 1310, 300, 311, 257, 4420, 551, 281, 360, 294, 512, 50840], "temperature": 0.0, "avg_logprob": -0.071288512303279, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.0034275453072041273}, {"id": 112, "seek": 71816, "start": 727.68, "end": 733.8399999999999, "text": " circumstances but from a theoretical perspective it's certainly a mistake because large language", "tokens": [50840, 9121, 457, 490, 257, 20864, 4585, 309, 311, 3297, 257, 6146, 570, 2416, 2856, 51148], "temperature": 0.0, "avg_logprob": -0.071288512303279, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.0034275453072041273}, {"id": 113, "seek": 71816, "start": 733.8399999999999, "end": 743.12, "text": " model is radically unlike you know a human agent in all sorts of ways but that's only one form of", "tokens": [51148, 2316, 307, 35508, 8343, 291, 458, 257, 1952, 9461, 294, 439, 7527, 295, 2098, 457, 300, 311, 787, 472, 1254, 295, 51612], "temperature": 0.0, "avg_logprob": -0.071288512303279, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.0034275453072041273}, {"id": 114, "seek": 74312, "start": 743.12, "end": 749.52, "text": " sort of human-centric bias the other one is anthropocentrism or what we call in that box", "tokens": [50364, 1333, 295, 1952, 12, 45300, 12577, 264, 661, 472, 307, 22727, 905, 317, 81, 1434, 420, 437, 321, 818, 294, 300, 2424, 50684], "temperature": 0.0, "avg_logprob": -0.07301818407498874, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.009852693416178226}, {"id": 115, "seek": 74312, "start": 749.52, "end": 757.52, "text": " article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that", "tokens": [50684, 7222, 22727, 905, 32939, 417, 1459, 4796, 1434, 293, 300, 1558, 307, 1238, 15325, 309, 311, 264, 1558, 300, 51084], "temperature": 0.0, "avg_logprob": -0.07301818407498874, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.009852693416178226}, {"id": 116, "seek": 74312, "start": 758.96, "end": 766.16, "text": " the human way of solving problems is the gold standard of solving problems generally so that", "tokens": [51156, 264, 1952, 636, 295, 12606, 2740, 307, 264, 3821, 3832, 295, 12606, 2740, 5101, 370, 300, 51516], "temperature": 0.0, "avg_logprob": -0.07301818407498874, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.009852693416178226}, {"id": 117, "seek": 76616, "start": 767.04, "end": 773.8399999999999, "text": " to the extent that a system solves a problem in a way that diverges from the human strategy", "tokens": [50408, 281, 264, 8396, 300, 257, 1185, 39890, 257, 1154, 294, 257, 636, 300, 18558, 2880, 490, 264, 1952, 5206, 50748], "temperature": 0.0, "avg_logprob": -0.07449695338373599, "compression_ratio": 1.496, "no_speech_prob": 0.00460783438757062}, {"id": 118, "seek": 76616, "start": 774.8, "end": 787.12, "text": " it's only using a trick or a faux solution it's it's not using a deep general rational strategy", "tokens": [50796, 309, 311, 787, 1228, 257, 4282, 420, 257, 36659, 3827, 309, 311, 309, 311, 406, 1228, 257, 2452, 2674, 15090, 5206, 51412], "temperature": 0.0, "avg_logprob": -0.07449695338373599, "compression_ratio": 1.496, "no_speech_prob": 0.00460783438757062}, {"id": 119, "seek": 78712, "start": 787.44, "end": 801.28, "text": " and in the debate about what large language models can do we think that the anthropomorphic", "tokens": [50380, 293, 294, 264, 7958, 466, 437, 2416, 2856, 5245, 393, 360, 321, 519, 300, 264, 22727, 32702, 299, 51072], "temperature": 0.0, "avg_logprob": -0.11217065011301348, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00628315843641758}, {"id": 120, "seek": 78712, "start": 801.28, "end": 806.24, "text": " bias is pretty well recognized and the anthropocentric bias is not so well recognized", "tokens": [51072, 12577, 307, 1238, 731, 9823, 293, 264, 22727, 905, 32939, 12577, 307, 406, 370, 731, 9823, 51320], "temperature": 0.0, "avg_logprob": -0.11217065011301348, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00628315843641758}, {"id": 121, "seek": 78712, "start": 806.24, "end": 814.08, "text": " and so part of this paper is is or the main idea behind the paper is to present a systematic", "tokens": [51320, 293, 370, 644, 295, 341, 3035, 307, 307, 420, 264, 2135, 1558, 2261, 264, 3035, 307, 281, 1974, 257, 27249, 51712], "temperature": 0.0, "avg_logprob": -0.11217065011301348, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00628315843641758}, {"id": 122, "seek": 81408, "start": 814.08, "end": 819.6, "text": " analysis of anthropocentric bias how it comes about and how to push back against it", "tokens": [50364, 5215, 295, 22727, 905, 32939, 12577, 577, 309, 1487, 466, 293, 577, 281, 2944, 646, 1970, 309, 50640], "temperature": 0.0, "avg_logprob": -0.11929415417956067, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.002249534474685788}, {"id": 123, "seek": 81408, "start": 821.6, "end": 826.0, "text": " right and we we want to be very clear and hopefully we're playing the paper that", "tokens": [50740, 558, 293, 321, 321, 528, 281, 312, 588, 1850, 293, 4696, 321, 434, 2433, 264, 3035, 300, 50960], "temperature": 0.0, "avg_logprob": -0.11929415417956067, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.002249534474685788}, {"id": 124, "seek": 81408, "start": 826.5600000000001, "end": 831.84, "text": " the reason why we focus on anthropocentric bias here is just because it is I think as", "tokens": [50988, 264, 1778, 983, 321, 1879, 322, 22727, 905, 32939, 12577, 510, 307, 445, 570, 309, 307, 286, 519, 382, 51252], "temperature": 0.0, "avg_logprob": -0.11929415417956067, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.002249534474685788}, {"id": 125, "seek": 81408, "start": 831.84, "end": 839.84, "text": " Charles mentioned less discussed and less recognized or some forms of it are less", "tokens": [51252, 10523, 2835, 1570, 7152, 293, 1570, 9823, 420, 512, 6422, 295, 309, 366, 1570, 51652], "temperature": 0.0, "avg_logprob": -0.11929415417956067, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.002249534474685788}, {"id": 126, "seek": 83984, "start": 839.9200000000001, "end": 844.96, "text": " recognized and we make we propose this new taxonomy but it's not at all suggest that it's", "tokens": [50368, 9823, 293, 321, 652, 321, 17421, 341, 777, 3366, 23423, 457, 309, 311, 406, 412, 439, 3402, 300, 309, 311, 50620], "temperature": 0.0, "avg_logprob": -0.12885974975953618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0034183075185865164}, {"id": 127, "seek": 83984, "start": 846.88, "end": 853.2800000000001, "text": " more problematic or more important than the anthropomorphic bias that's well discussed in", "tokens": [50716, 544, 19011, 420, 544, 1021, 813, 264, 22727, 32702, 299, 12577, 300, 311, 731, 7152, 294, 51036], "temperature": 0.0, "avg_logprob": -0.12885974975953618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0034183075185865164}, {"id": 128, "seek": 83984, "start": 853.2800000000001, "end": 863.0400000000001, "text": " the literature of the anthropomorphic biases so in other words this is not you know to frame things", "tokens": [51036, 264, 10394, 295, 264, 22727, 32702, 299, 32152, 370, 294, 661, 2283, 341, 307, 406, 291, 458, 281, 3920, 721, 51524], "temperature": 0.0, "avg_logprob": -0.12885974975953618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0034183075185865164}, {"id": 129, "seek": 83984, "start": 863.0400000000001, "end": 867.9200000000001, "text": " in in this slightly problematic dichotomous way of thinking that's common in the discourse", "tokens": [51524, 294, 294, 341, 4748, 19011, 10390, 42939, 563, 636, 295, 1953, 300, 311, 2689, 294, 264, 23938, 51768], "temperature": 0.0, "avg_logprob": -0.12885974975953618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0034183075185865164}, {"id": 130, "seek": 86792, "start": 868.0, "end": 877.36, "text": " on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it", "tokens": [50368, 322, 441, 43, 44, 341, 307, 406, 257, 3035, 300, 307, 19761, 1794, 281, 264, 441, 43, 44, 29275, 420, 441, 43, 44, 24144, 2255, 754, 1673, 309, 50836], "temperature": 0.0, "avg_logprob": -0.11179044876975575, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005446130409836769}, {"id": 131, "seek": 86792, "start": 877.36, "end": 883.04, "text": " is pushing back against a certain form of dismissal of anthropocentric biases that", "tokens": [50836, 307, 7380, 646, 1970, 257, 1629, 1254, 295, 16974, 304, 295, 22727, 905, 32939, 32152, 300, 51120], "temperature": 0.0, "avg_logprob": -0.11179044876975575, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005446130409836769}, {"id": 132, "seek": 86792, "start": 883.04, "end": 888.9599999999999, "text": " only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a", "tokens": [51120, 787, 20638, 48856, 264, 22727, 32702, 299, 32152, 457, 4317, 321, 820, 12497, 341, 484, 257, 51416], "temperature": 0.0, "avg_logprob": -0.11179044876975575, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005446130409836769}, {"id": 133, "seek": 86792, "start": 888.9599999999999, "end": 893.5999999999999, "text": " little bit already with the first one we make here in the paper about a performance competence", "tokens": [51416, 707, 857, 1217, 365, 264, 700, 472, 321, 652, 510, 294, 264, 3035, 466, 257, 3389, 39965, 51648], "temperature": 0.0, "avg_logprob": -0.11179044876975575, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005446130409836769}, {"id": 134, "seek": 89360, "start": 893.6, "end": 898.08, "text": " distinction which is a nice way to bring about both anthropomorphism and anthropocentricism", "tokens": [50364, 16844, 597, 307, 257, 1481, 636, 281, 1565, 466, 1293, 22727, 32702, 1434, 293, 22727, 905, 32939, 1434, 50588], "temperature": 0.0, "avg_logprob": -0.17611798373135654, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.011845679022371769}, {"id": 135, "seek": 89360, "start": 898.08, "end": 906.72, "text": " regarding LLM so this distinction is a very classic distinction in linguistics and cognitive", "tokens": [50588, 8595, 441, 43, 44, 370, 341, 16844, 307, 257, 588, 7230, 16844, 294, 21766, 6006, 293, 15605, 51020], "temperature": 0.0, "avg_logprob": -0.17611798373135654, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.011845679022371769}, {"id": 136, "seek": 89360, "start": 906.72, "end": 913.36, "text": " science and it has already been applied to AI systems in the neural networks productively", "tokens": [51020, 3497, 293, 309, 575, 1217, 668, 6456, 281, 7318, 3652, 294, 264, 18161, 9590, 1674, 3413, 51352], "temperature": 0.0, "avg_logprob": -0.17611798373135654, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.011845679022371769}, {"id": 137, "seek": 89360, "start": 913.36, "end": 919.36, "text": " like Charles Farson so there's nothing really new here but the distinction comes from", "tokens": [51352, 411, 10523, 479, 19358, 370, 456, 311, 1825, 534, 777, 510, 457, 264, 16844, 1487, 490, 51652], "temperature": 0.0, "avg_logprob": -0.17611798373135654, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.011845679022371769}, {"id": 138, "seek": 91936, "start": 919.36, "end": 925.2, "text": " Noam Chomsky originally and the idea is that performance pertains to the external behavior", "tokens": [50364, 883, 335, 761, 4785, 4133, 7993, 293, 264, 1558, 307, 300, 3389, 13269, 2315, 281, 264, 8320, 5223, 50656], "temperature": 0.0, "avg_logprob": -0.14322269126160503, "compression_ratio": 1.6875, "no_speech_prob": 0.007666789460927248}, {"id": 139, "seek": 91936, "start": 925.2, "end": 933.12, "text": " of a system in a particular domain and competence is the kind of setup underlying knowledge and", "tokens": [50656, 295, 257, 1185, 294, 257, 1729, 9274, 293, 39965, 307, 264, 733, 295, 8657, 14217, 3601, 293, 51052], "temperature": 0.0, "avg_logprob": -0.14322269126160503, "compression_ratio": 1.6875, "no_speech_prob": 0.007666789460927248}, {"id": 140, "seek": 91936, "start": 933.76, "end": 940.5600000000001, "text": " computations or mechanisms that enable the system to achieve that behavior", "tokens": [51084, 2807, 763, 420, 15902, 300, 9528, 264, 1185, 281, 4584, 300, 5223, 51424], "temperature": 0.0, "avg_logprob": -0.14322269126160503, "compression_ratio": 1.6875, "no_speech_prob": 0.007666789460927248}, {"id": 141, "seek": 91936, "start": 941.6800000000001, "end": 946.16, "text": " and a familiar observation in linguistics and cognitive science is that there is a double", "tokens": [51480, 293, 257, 4963, 14816, 294, 21766, 6006, 293, 15605, 3497, 307, 300, 456, 307, 257, 3834, 51704], "temperature": 0.0, "avg_logprob": -0.14322269126160503, "compression_ratio": 1.6875, "no_speech_prob": 0.007666789460927248}, {"id": 142, "seek": 94616, "start": 946.16, "end": 956.88, "text": " dissolution between performance and competence so if you take for example language I might during", "tokens": [50364, 7802, 3386, 1296, 3389, 293, 39965, 370, 498, 291, 747, 337, 1365, 2856, 286, 1062, 1830, 50900], "temperature": 0.0, "avg_logprob": -0.1314369178399807, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00896178837865591}, {"id": 143, "seek": 94616, "start": 956.88, "end": 961.6, "text": " this very podcast make some grammatical mistakes or some other mistakes in fact I've already", "tokens": [50900, 341, 588, 7367, 652, 512, 17570, 267, 804, 8038, 420, 512, 661, 8038, 294, 1186, 286, 600, 1217, 51136], "temperature": 0.0, "avg_logprob": -0.1314369178399807, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00896178837865591}, {"id": 144, "seek": 94616, "start": 962.64, "end": 968.3199999999999, "text": " misspoke in a few times I think and repeated myself so I made performance errors but this", "tokens": [51188, 1713, 48776, 294, 257, 1326, 1413, 286, 519, 293, 10477, 2059, 370, 286, 1027, 3389, 13603, 457, 341, 51472], "temperature": 0.0, "avg_logprob": -0.1314369178399807, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00896178837865591}, {"id": 145, "seek": 94616, "start": 968.3199999999999, "end": 973.1999999999999, "text": " does not entail necessarily hopefully that I'm an incompetent language user and that I don't have", "tokens": [51472, 775, 406, 948, 864, 4725, 4696, 300, 286, 478, 364, 41602, 317, 2856, 4195, 293, 300, 286, 500, 380, 362, 51716], "temperature": 0.0, "avg_logprob": -0.1314369178399807, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00896178837865591}, {"id": 146, "seek": 97320, "start": 973.2800000000001, "end": 982.08, "text": " in the language the competence so that's a well recognized dissociation you can be competent", "tokens": [50368, 294, 264, 2856, 264, 39965, 370, 300, 311, 257, 731, 9823, 44446, 399, 291, 393, 312, 29998, 50808], "temperature": 0.0, "avg_logprob": -0.11562084356943766, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0021451765205711126}, {"id": 147, "seek": 97320, "start": 982.08, "end": 989.84, "text": " and yet make some errors and the reason for that is that there might be some additional factors", "tokens": [50808, 293, 1939, 652, 512, 13603, 293, 264, 1778, 337, 300, 307, 300, 456, 1062, 312, 512, 4497, 6771, 51196], "temperature": 0.0, "avg_logprob": -0.11562084356943766, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0021451765205711126}, {"id": 148, "seek": 97320, "start": 989.84, "end": 995.84, "text": " that are unrelated to the underlying competence that might impede on my performance so for example", "tokens": [51196, 300, 366, 38967, 281, 264, 14217, 39965, 300, 1062, 704, 4858, 322, 452, 3389, 370, 337, 1365, 51496], "temperature": 0.0, "avg_logprob": -0.11562084356943766, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0021451765205711126}, {"id": 149, "seek": 99584, "start": 995.84, "end": 1003.52, "text": " I might be distracted when I speak or there might be other effects on my speaking performance that", "tokens": [50364, 286, 1062, 312, 21658, 562, 286, 1710, 420, 456, 1062, 312, 661, 5065, 322, 452, 4124, 3389, 300, 50748], "temperature": 0.0, "avg_logprob": -0.1152944564819336, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.012207061983644962}, {"id": 150, "seek": 99584, "start": 1003.52, "end": 1012.72, "text": " don't actually originate from a lack of competence but just impede on the idealized expression", "tokens": [50748, 500, 380, 767, 4957, 473, 490, 257, 5011, 295, 39965, 457, 445, 704, 4858, 322, 264, 7157, 1602, 6114, 51208], "temperature": 0.0, "avg_logprob": -0.1152944564819336, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.012207061983644962}, {"id": 151, "seek": 99584, "start": 1012.72, "end": 1017.9200000000001, "text": " external manifestation of my competence and this is why I misspeak but it's also why recognize", "tokens": [51208, 8320, 29550, 295, 452, 39965, 293, 341, 307, 983, 286, 1713, 494, 514, 457, 309, 311, 611, 983, 5521, 51468], "temperature": 0.0, "avg_logprob": -0.1152944564819336, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.012207061983644962}, {"id": 152, "seek": 99584, "start": 1017.9200000000001, "end": 1024.4, "text": " that you can have good performance without competence so we give here the example of a", "tokens": [51468, 300, 291, 393, 362, 665, 3389, 1553, 39965, 370, 321, 976, 510, 264, 1365, 295, 257, 51792], "temperature": 0.0, "avg_logprob": -0.1152944564819336, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.012207061983644962}, {"id": 153, "seek": 102440, "start": 1024.4, "end": 1032.0, "text": " student cheating on a test or memorizing test answers by brute forcing the test to slightly more", "tokens": [50364, 3107, 18309, 322, 257, 1500, 420, 10560, 3319, 1500, 6338, 538, 47909, 19030, 264, 1500, 281, 4748, 544, 50744], "temperature": 0.0, "avg_logprob": -0.14986332001224642, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.001670246827416122}, {"id": 154, "seek": 102440, "start": 1033.52, "end": 1038.88, "text": " I guess gray area but at least in the cheating case a student can ace a test without being", "tokens": [50820, 286, 2041, 10855, 1859, 457, 412, 1935, 294, 264, 18309, 1389, 257, 3107, 393, 17117, 257, 1500, 1553, 885, 51088], "temperature": 0.0, "avg_logprob": -0.14986332001224642, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.001670246827416122}, {"id": 155, "seek": 102440, "start": 1038.88, "end": 1047.3600000000001, "text": " competent at what the test is actually testing for and it's also well acknowledged in cognitive", "tokens": [51088, 29998, 412, 437, 264, 1500, 307, 767, 4997, 337, 293, 309, 311, 611, 731, 27262, 294, 15605, 51512], "temperature": 0.0, "avg_logprob": -0.14986332001224642, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.001670246827416122}, {"id": 156, "seek": 104736, "start": 1047.36, "end": 1058.9599999999998, "text": " science that there can be instances like this throughout you know like that can be manifest", "tokens": [50364, 3497, 300, 456, 393, 312, 14519, 411, 341, 3710, 291, 458, 411, 300, 393, 312, 10067, 50944], "temperature": 0.0, "avg_logprob": -0.18581426960148223, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.012382755056023598}, {"id": 157, "seek": 104736, "start": 1058.9599999999998, "end": 1064.8, "text": " certain experimental settings where the test subject is right for the wrong reasons as it were", "tokens": [50944, 1629, 17069, 6257, 689, 264, 1500, 3983, 307, 558, 337, 264, 2085, 4112, 382, 309, 645, 51236], "temperature": 0.0, "avg_logprob": -0.18581426960148223, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.012382755056023598}, {"id": 158, "seek": 104736, "start": 1065.4399999999998, "end": 1069.84, "text": " namely it's doing well it's exhibiting good performance never realize the underlying reason", "tokens": [51268, 20926, 309, 311, 884, 731, 309, 311, 8144, 1748, 665, 3389, 1128, 4325, 264, 14217, 1778, 51488], "temperature": 0.0, "avg_logprob": -0.18581426960148223, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.012382755056023598}, {"id": 159, "seek": 104736, "start": 1069.84, "end": 1075.6, "text": " for the performance is that there was some perhaps some curiosity that the experimenters", "tokens": [51488, 337, 264, 3389, 307, 300, 456, 390, 512, 4317, 512, 18769, 300, 264, 5120, 433, 51776], "temperature": 0.0, "avg_logprob": -0.18581426960148223, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.012382755056023598}, {"id": 160, "seek": 107560, "start": 1075.6, "end": 1080.32, "text": " all the scientists haven't thought about that could account for his good performance but", "tokens": [50364, 439, 264, 7708, 2378, 380, 1194, 466, 300, 727, 2696, 337, 702, 665, 3389, 457, 50600], "temperature": 0.0, "avg_logprob": -0.252288818359375, "compression_ratio": 1.9557522123893805, "no_speech_prob": 0.003316171932965517}, {"id": 161, "seek": 107560, "start": 1080.32, "end": 1085.76, "text": " but doesn't actually amount to whatever competence they were setting out to test", "tokens": [50600, 457, 1177, 380, 767, 2372, 281, 2035, 39965, 436, 645, 3287, 484, 281, 1500, 50872], "temperature": 0.0, "avg_logprob": -0.252288818359375, "compression_ratio": 1.9557522123893805, "no_speech_prob": 0.003316171932965517}, {"id": 162, "seek": 107560, "start": 1087.76, "end": 1091.6799999999998, "text": " so we we start by saying well this is what we could nice because the mistakes this is the", "tokens": [50972, 370, 321, 321, 722, 538, 1566, 731, 341, 307, 437, 321, 727, 1481, 570, 264, 8038, 341, 307, 264, 51168], "temperature": 0.0, "avg_logprob": -0.252288818359375, "compression_ratio": 1.9557522123893805, "no_speech_prob": 0.003316171932965517}, {"id": 163, "seek": 107560, "start": 1091.6799999999998, "end": 1095.84, "text": " association that is like supplied to humans across the board you can have performance", "tokens": [51168, 14598, 300, 307, 411, 27625, 281, 6255, 2108, 264, 3150, 291, 393, 362, 3389, 51376], "temperature": 0.0, "avg_logprob": -0.252288818359375, "compression_ratio": 1.9557522123893805, "no_speech_prob": 0.003316171932965517}, {"id": 164, "seek": 107560, "start": 1095.84, "end": 1101.12, "text": " without competence good performance without competence and you can have that performance despite", "tokens": [51376, 1553, 39965, 665, 3389, 1553, 39965, 293, 291, 393, 362, 300, 3389, 7228, 51640], "temperature": 0.0, "avg_logprob": -0.252288818359375, "compression_ratio": 1.9557522123893805, "no_speech_prob": 0.003316171932965517}, {"id": 165, "seek": 110112, "start": 1101.12, "end": 1106.8799999999999, "text": " competence now when it comes to other lamps the point we make just going to scroll as we go", "tokens": [50364, 39965, 586, 562, 309, 1487, 281, 661, 34887, 264, 935, 321, 652, 445, 516, 281, 11369, 382, 321, 352, 50652], "temperature": 0.0, "avg_logprob": -0.17635623707490808, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.0025082856882363558}, {"id": 166, "seek": 110112, "start": 1108.2399999999998, "end": 1115.76, "text": " we have some figures to show later but the point we make is that generally people stress the", "tokens": [50720, 321, 362, 512, 9624, 281, 855, 1780, 457, 264, 935, 321, 652, 307, 300, 5101, 561, 4244, 264, 51096], "temperature": 0.0, "avg_logprob": -0.17635623707490808, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.0025082856882363558}, {"id": 167, "seek": 110112, "start": 1115.76, "end": 1120.32, "text": " dissociation apply the distinction to other lamps the stress decision only in one direction", "tokens": [51096, 44446, 399, 3079, 264, 16844, 281, 661, 34887, 264, 4244, 3537, 787, 294, 472, 3513, 51324], "temperature": 0.0, "avg_logprob": -0.17635623707490808, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.0025082856882363558}, {"id": 168, "seek": 110112, "start": 1121.12, "end": 1126.8, "text": " unlike in the case of human where it's bi-directional and so what people do generally is to say well", "tokens": [51364, 8343, 294, 264, 1389, 295, 1952, 689, 309, 311, 3228, 12, 18267, 41048, 293, 370, 437, 561, 360, 5101, 307, 281, 584, 731, 51648], "temperature": 0.0, "avg_logprob": -0.17635623707490808, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.0025082856882363558}, {"id": 169, "seek": 112680, "start": 1126.8, "end": 1132.1599999999999, "text": " other lamps famously you know if you look at the gpt4 technical report and and vice other", "tokens": [50364, 661, 34887, 34360, 291, 458, 498, 291, 574, 412, 264, 290, 662, 19, 6191, 2275, 293, 293, 11964, 661, 50632], "temperature": 0.0, "avg_logprob": -0.22187084871179918, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0011140451533719897}, {"id": 170, "seek": 112680, "start": 1133.52, "end": 1139.44, "text": " any any any report about a new state of the art alone they are getting really really good at a", "tokens": [50700, 604, 604, 604, 2275, 466, 257, 777, 1785, 295, 264, 1523, 3312, 436, 366, 1242, 534, 534, 665, 412, 257, 50996], "temperature": 0.0, "avg_logprob": -0.22187084871179918, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0011140451533719897}, {"id": 171, "seek": 112680, "start": 1139.44, "end": 1146.48, "text": " number of tests and about benchmarks and even human exam examinations human exams the bar exam", "tokens": [50996, 1230, 295, 6921, 293, 466, 43751, 293, 754, 1952, 1139, 1139, 10325, 1952, 20514, 264, 2159, 1139, 51348], "temperature": 0.0, "avg_logprob": -0.22187084871179918, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0011140451533719897}, {"id": 172, "seek": 112680, "start": 1147.12, "end": 1154.48, "text": " medical exams etc so they can get a really good performance test that we tend to think are really", "tokens": [51380, 4625, 20514, 5183, 370, 436, 393, 483, 257, 534, 665, 3389, 1500, 300, 321, 3928, 281, 519, 366, 534, 51748], "temperature": 0.0, "avg_logprob": -0.22187084871179918, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0011140451533719897}, {"id": 173, "seek": 115448, "start": 1154.48, "end": 1162.08, "text": " difficult tests that one can only pass at least a human could only pass if they have", "tokens": [50364, 2252, 6921, 300, 472, 393, 787, 1320, 412, 1935, 257, 1952, 727, 787, 1320, 498, 436, 362, 50744], "temperature": 0.0, "avg_logprob": -0.18413617237504706, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.00077847670763731}, {"id": 174, "seek": 115448, "start": 1162.64, "end": 1166.0, "text": " a really significant nonchalant amount of competency in particular the main", "tokens": [50772, 257, 534, 4776, 2107, 339, 304, 394, 2372, 295, 50097, 294, 1729, 264, 2135, 50940], "temperature": 0.0, "avg_logprob": -0.18413617237504706, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.00077847670763731}, {"id": 175, "seek": 115448, "start": 1167.76, "end": 1172.96, "text": " and the point that is often made when it comes to other lamps is be careful slow it on and try to", "tokens": [51028, 293, 264, 935, 300, 307, 2049, 1027, 562, 309, 1487, 281, 661, 34887, 307, 312, 5026, 2964, 309, 322, 293, 853, 281, 51288], "temperature": 0.0, "avg_logprob": -0.18413617237504706, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.00077847670763731}, {"id": 176, "seek": 115448, "start": 1172.96, "end": 1178.48, "text": " find out why the model is doing well on that test because there are various reasons why it could do", "tokens": [51288, 915, 484, 983, 264, 2316, 307, 884, 731, 322, 300, 1500, 570, 456, 366, 3683, 4112, 983, 309, 727, 360, 51564], "temperature": 0.0, "avg_logprob": -0.18413617237504706, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.00077847670763731}, {"id": 177, "seek": 117848, "start": 1178.48, "end": 1185.52, "text": " well that do not actually indicate that the model has the underlying competence that the test was", "tokens": [50364, 731, 300, 360, 406, 767, 13330, 300, 264, 2316, 575, 264, 14217, 39965, 300, 264, 1500, 390, 50716], "temperature": 0.0, "avg_logprob": -0.12109662340832995, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.006688176188617945}, {"id": 178, "seek": 117848, "start": 1185.52, "end": 1191.1200000000001, "text": " designed for when it comes to humans so one big concern for example is data contamination", "tokens": [50716, 4761, 337, 562, 309, 1487, 281, 6255, 370, 472, 955, 3136, 337, 1365, 307, 1412, 33012, 50996], "temperature": 0.0, "avg_logprob": -0.12109662340832995, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.006688176188617945}, {"id": 179, "seek": 117848, "start": 1191.1200000000001, "end": 1199.44, "text": " where very large language models train on internet scale data can easily be trained on some test", "tokens": [50996, 689, 588, 2416, 2856, 5245, 3847, 322, 4705, 4373, 1412, 393, 3612, 312, 8895, 322, 512, 1500, 51412], "temperature": 0.0, "avg_logprob": -0.12109662340832995, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.006688176188617945}, {"id": 180, "seek": 117848, "start": 1199.44, "end": 1203.68, "text": " items from common benchmarks that leak into the training data such that they can then do really", "tokens": [51412, 4754, 490, 2689, 43751, 300, 17143, 666, 264, 3097, 1412, 1270, 300, 436, 393, 550, 360, 534, 51624], "temperature": 0.0, "avg_logprob": -0.12109662340832995, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.006688176188617945}, {"id": 181, "seek": 120368, "start": 1203.68, "end": 1211.28, "text": " well on the on the on the benchmark just because they've essentially memorized test items and there", "tokens": [50364, 731, 322, 264, 322, 264, 322, 264, 18927, 445, 570, 436, 600, 4476, 46677, 1500, 4754, 293, 456, 50744], "temperature": 0.0, "avg_logprob": -0.13516233080909365, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.006483078934252262}, {"id": 182, "seek": 120368, "start": 1211.28, "end": 1218.4, "text": " are other more subtle more subtle reasons why performance could be very good for the wrong", "tokens": [50744, 366, 661, 544, 13743, 544, 13743, 4112, 983, 3389, 727, 312, 588, 665, 337, 264, 2085, 51100], "temperature": 0.0, "avg_logprob": -0.13516233080909365, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.006483078934252262}, {"id": 183, "seek": 120368, "start": 1218.4, "end": 1225.44, "text": " reasons so that's very well recognized and a lot of the people who push back against anthropomorphic", "tokens": [51100, 4112, 370, 300, 311, 588, 731, 9823, 293, 257, 688, 295, 264, 561, 567, 2944, 646, 1970, 22727, 32702, 299, 51452], "temperature": 0.0, "avg_logprob": -0.13516233080909365, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.006483078934252262}, {"id": 184, "seek": 120368, "start": 1225.44, "end": 1232.16, "text": " bias when it gets to other lamps make that point be careful do not take on another anthropomorphic", "tokens": [51452, 12577, 562, 309, 2170, 281, 661, 34887, 652, 300, 935, 312, 5026, 360, 406, 747, 322, 1071, 22727, 32702, 299, 51788], "temperature": 0.0, "avg_logprob": -0.13516233080909365, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.006483078934252262}, {"id": 185, "seek": 123216, "start": 1232.16, "end": 1237.92, "text": " attitude to the systems the reason why they do well is not because they have human-like intelligence", "tokens": [50364, 10157, 281, 264, 3652, 264, 1778, 983, 436, 360, 731, 307, 406, 570, 436, 362, 1952, 12, 4092, 7599, 50652], "temperature": 0.0, "avg_logprob": -0.2095311482747396, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.0027135529089719057}, {"id": 186, "seek": 123216, "start": 1237.92, "end": 1245.0400000000002, "text": " or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant", "tokens": [50652, 420, 1952, 12, 4092, 2190, 39396, 457, 309, 311, 337, 341, 26703, 27820, 317, 420, 291, 458, 5911, 28682, 51008], "temperature": 0.0, "avg_logprob": -0.2095311482747396, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.0027135529089719057}, {"id": 187, "seek": 123216, "start": 1245.0400000000002, "end": 1250.48, "text": " reasons that account for the good performance now when it comes to the other dissociation of the", "tokens": [51008, 4112, 300, 2696, 337, 264, 665, 3389, 586, 562, 309, 1487, 281, 264, 661, 44446, 399, 295, 264, 51280], "temperature": 0.0, "avg_logprob": -0.2095311482747396, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.0027135529089719057}, {"id": 188, "seek": 123216, "start": 1250.48, "end": 1256.48, "text": " other dissociation of the other direction there are people are very reluctant to apply to a lamps", "tokens": [51280, 661, 44446, 399, 295, 264, 661, 3513, 456, 366, 561, 366, 588, 33677, 281, 3079, 281, 257, 34887, 51580], "temperature": 0.0, "avg_logprob": -0.2095311482747396, "compression_ratio": 1.809090909090909, "no_speech_prob": 0.0027135529089719057}, {"id": 189, "seek": 125648, "start": 1257.28, "end": 1262.16, "text": " and we think it's because essentially people think in a human case you can make sense of the", "tokens": [50404, 293, 321, 519, 309, 311, 570, 4476, 561, 519, 294, 257, 1952, 1389, 291, 393, 652, 2020, 295, 264, 50648], "temperature": 0.0, "avg_logprob": -0.07228967547416687, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009694330394268036}, {"id": 190, "seek": 125648, "start": 1262.16, "end": 1267.68, "text": " idea that the human could do badly on a test or corporate could perform badly on a task", "tokens": [50648, 1558, 300, 264, 1952, 727, 360, 13425, 322, 257, 1500, 420, 10896, 727, 2042, 13425, 322, 257, 5633, 50924], "temperature": 0.0, "avg_logprob": -0.07228967547416687, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009694330394268036}, {"id": 191, "seek": 125648, "start": 1268.64, "end": 1274.08, "text": " and yet have the competence that you're trying to test but there might be some auxiliary factors", "tokens": [50972, 293, 1939, 362, 264, 39965, 300, 291, 434, 1382, 281, 1500, 457, 456, 1062, 312, 512, 43741, 6771, 51244], "temperature": 0.0, "avg_logprob": -0.07228967547416687, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009694330394268036}, {"id": 192, "seek": 125648, "start": 1274.72, "end": 1283.44, "text": " such as working memory limitations attention deficits etc that could impede on the performance", "tokens": [51276, 1270, 382, 1364, 4675, 15705, 3202, 49616, 5183, 300, 727, 704, 4858, 322, 264, 3389, 51712], "temperature": 0.0, "avg_logprob": -0.07228967547416687, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009694330394268036}, {"id": 193, "seek": 128344, "start": 1283.44, "end": 1294.72, "text": " but in the language model I think what we argue in the paper is that", "tokens": [50364, 457, 294, 264, 2856, 2316, 286, 519, 437, 321, 9695, 294, 264, 3035, 307, 300, 50928], "temperature": 0.0, "avg_logprob": -0.16829589744666953, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0018326962599530816}, {"id": 194, "seek": 128344, "start": 1296.48, "end": 1300.24, "text": " people don't a lot of people don't think that there is an analogous", "tokens": [51016, 561, 500, 380, 257, 688, 295, 561, 500, 380, 519, 300, 456, 307, 364, 16660, 563, 51204], "temperature": 0.0, "avg_logprob": -0.16829589744666953, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0018326962599530816}, {"id": 195, "seek": 128344, "start": 1302.4, "end": 1306.56, "text": " mechanism at play where there could be some kind of auxiliary factor that impede on performance", "tokens": [51312, 7513, 412, 862, 689, 456, 727, 312, 512, 733, 295, 43741, 5952, 300, 704, 4858, 322, 3389, 51520], "temperature": 0.0, "avg_logprob": -0.16829589744666953, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0018326962599530816}, {"id": 196, "seek": 128344, "start": 1306.56, "end": 1312.0800000000002, "text": " performance is what you get what you see is what you get and so the performance is a direct", "tokens": [51520, 3389, 307, 437, 291, 483, 437, 291, 536, 307, 437, 291, 483, 293, 370, 264, 3389, 307, 257, 2047, 51796], "temperature": 0.0, "avg_logprob": -0.16829589744666953, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0018326962599530816}, {"id": 197, "seek": 131208, "start": 1312.8, "end": 1322.32, "text": " manifestation of what the system is computing and if you have performance errors", "tokens": [50400, 29550, 295, 437, 264, 1185, 307, 15866, 293, 498, 291, 362, 3389, 13603, 50876], "temperature": 0.0, "avg_logprob": -0.18900045435479346, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.0037483323831111193}, {"id": 198, "seek": 131208, "start": 1323.04, "end": 1326.6399999999999, "text": " that can only be explained by the lack of competence because there is no additional", "tokens": [50912, 300, 393, 787, 312, 8825, 538, 264, 5011, 295, 39965, 570, 456, 307, 572, 4497, 51092], "temperature": 0.0, "avg_logprob": -0.18900045435479346, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.0037483323831111193}, {"id": 199, "seek": 131208, "start": 1329.84, "end": 1334.96, "text": " independent factor or module that could impede on the performance", "tokens": [51252, 6695, 5952, 420, 10088, 300, 727, 704, 4858, 322, 264, 3389, 51508], "temperature": 0.0, "avg_logprob": -0.18900045435479346, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.0037483323831111193}, {"id": 200, "seek": 133496, "start": 1335.68, "end": 1342.16, "text": " sources of interference you might say yeah so yeah I mean I'll", "tokens": [50400, 7139, 295, 24497, 291, 1062, 584, 1338, 370, 1338, 286, 914, 286, 603, 50724], "temperature": 0.0, "avg_logprob": -0.24886214976408044, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.030987322330474854}, {"id": 201, "seek": 133496, "start": 1343.68, "end": 1350.32, "text": " pass it over to you Charles I just wanted to set up this distinction yeah yeah", "tokens": [50800, 1320, 309, 670, 281, 291, 10523, 286, 445, 1415, 281, 992, 493, 341, 16844, 1338, 1338, 51132], "temperature": 0.0, "avg_logprob": -0.24886214976408044, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.030987322330474854}, {"id": 202, "seek": 133496, "start": 1352.32, "end": 1358.64, "text": " right so I mean if you think about a traditional computer program", "tokens": [51232, 558, 370, 286, 914, 498, 291, 519, 466, 257, 5164, 3820, 1461, 51548], "temperature": 0.0, "avg_logprob": -0.24886214976408044, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.030987322330474854}, {"id": 203, "seek": 135864, "start": 1359.5200000000002, "end": 1366.96, "text": " well at least if you think about a simple computer program it's odd to think of it as", "tokens": [50408, 731, 412, 1935, 498, 291, 519, 466, 257, 2199, 3820, 1461, 309, 311, 7401, 281, 519, 295, 309, 382, 50780], "temperature": 0.0, "avg_logprob": -0.14891395568847657, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.002049271482974291}, {"id": 204, "seek": 135864, "start": 1367.5200000000002, "end": 1372.24, "text": " some sort of complex systems where it's a complex system where one part of it could sort of", "tokens": [50808, 512, 1333, 295, 3997, 3652, 689, 309, 311, 257, 3997, 1185, 689, 472, 644, 295, 309, 727, 1333, 295, 51044], "temperature": 0.0, "avg_logprob": -0.14891395568847657, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.002049271482974291}, {"id": 205, "seek": 135864, "start": 1372.24, "end": 1377.0400000000002, "text": " interfere with the workings of another part of it but one of the points we want to make is that", "tokens": [51044, 23946, 365, 264, 589, 1109, 295, 1071, 644, 295, 309, 457, 472, 295, 264, 2793, 321, 528, 281, 652, 307, 300, 51284], "temperature": 0.0, "avg_logprob": -0.14891395568847657, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.002049271482974291}, {"id": 206, "seek": 135864, "start": 1377.6000000000001, "end": 1384.0, "text": " something like that is a realistic possibility with large language models", "tokens": [51312, 746, 411, 300, 307, 257, 12465, 7959, 365, 2416, 2856, 5245, 51632], "temperature": 0.0, "avg_logprob": -0.14891395568847657, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.002049271482974291}, {"id": 207, "seek": 138400, "start": 1384.56, "end": 1391.12, "text": " um okay but I suppose the next part of the paper goes into a taxonomy of", "tokens": [50392, 1105, 1392, 457, 286, 7297, 264, 958, 644, 295, 264, 3035, 1709, 666, 257, 3366, 23423, 295, 50720], "temperature": 0.0, "avg_logprob": -0.14524632967435397, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.00162238790653646}, {"id": 208, "seek": 138400, "start": 1391.92, "end": 1397.84, "text": " anthropocentric bias and the first sort of overarching point is the distinction between", "tokens": [50760, 22727, 905, 32939, 12577, 293, 264, 700, 1333, 295, 45501, 935, 307, 264, 16844, 1296, 51056], "temperature": 0.0, "avg_logprob": -0.14524632967435397, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.00162238790653646}, {"id": 209, "seek": 138400, "start": 1397.84, "end": 1405.84, "text": " type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance", "tokens": [51056, 2010, 472, 293, 2010, 732, 370, 264, 2010, 472, 22727, 905, 317, 81, 1434, 307, 264, 18187, 281, 6552, 364, 441, 43, 44, 311, 3389, 51456], "temperature": 0.0, "avg_logprob": -0.14524632967435397, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.00162238790653646}, {"id": 210, "seek": 140584, "start": 1406.48, "end": 1413.6799999999998, "text": " failures designed to measure competence always indicates that it lacks", "tokens": [50396, 20774, 4761, 281, 3481, 39965, 1009, 16203, 300, 309, 31132, 50756], "temperature": 0.0, "avg_logprob": -0.10862449723847058, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.021607715636491776}, {"id": 211, "seek": 140584, "start": 1415.52, "end": 1423.28, "text": " that competence and so we before we so we'll say something about three different kinds of", "tokens": [50848, 300, 39965, 293, 370, 321, 949, 321, 370, 321, 603, 584, 746, 466, 1045, 819, 3685, 295, 51236], "temperature": 0.0, "avg_logprob": -0.10862449723847058, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.021607715636491776}, {"id": 212, "seek": 140584, "start": 1423.28, "end": 1430.1599999999999, "text": " type one anthropocentric bias but first a background point which is that", "tokens": [51236, 2010, 472, 22727, 905, 32939, 12577, 457, 700, 257, 3678, 935, 597, 307, 300, 51580], "temperature": 0.0, "avg_logprob": -0.10862449723847058, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.021607715636491776}, {"id": 213, "seek": 143016, "start": 1430.4, "end": 1440.0, "text": " um whenever we think it's possible to give a mechanistic explanation of some", "tokens": [50376, 1105, 5699, 321, 519, 309, 311, 1944, 281, 976, 257, 4236, 3142, 10835, 295, 512, 50856], "temperature": 0.0, "avg_logprob": -0.11191681772470474, "compression_ratio": 1.3565217391304347, "no_speech_prob": 0.0013041422935202718}, {"id": 214, "seek": 143016, "start": 1440.72, "end": 1449.3600000000001, "text": " complicated phenomenon we always have to foreground some factors some variables", "tokens": [50892, 6179, 14029, 321, 1009, 362, 281, 32058, 512, 6771, 512, 9102, 51324], "temperature": 0.0, "avg_logprob": -0.11191681772470474, "compression_ratio": 1.3565217391304347, "no_speech_prob": 0.0013041422935202718}, {"id": 215, "seek": 144936, "start": 1449.76, "end": 1462.32, "text": " uh and background others and um the properties that we push into the background", "tokens": [50384, 2232, 293, 3678, 2357, 293, 1105, 264, 7221, 300, 321, 2944, 666, 264, 3678, 51012], "temperature": 0.0, "avg_logprob": -0.11099700574521665, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.05030308663845062}, {"id": 216, "seek": 144936, "start": 1466.32, "end": 1471.1999999999998, "text": " nevertheless matter we're still making assumptions about the nature of those properties", "tokens": [51212, 26924, 1871, 321, 434, 920, 1455, 17695, 466, 264, 3687, 295, 729, 7221, 51456], "temperature": 0.0, "avg_logprob": -0.11099700574521665, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.05030308663845062}, {"id": 217, "seek": 144936, "start": 1471.1999999999998, "end": 1476.08, "text": " when we try to articulate what's going on with the other properties that we're paying more attention", "tokens": [51456, 562, 321, 853, 281, 30305, 437, 311, 516, 322, 365, 264, 661, 7221, 300, 321, 434, 6229, 544, 3202, 51700], "temperature": 0.0, "avg_logprob": -0.11099700574521665, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.05030308663845062}, {"id": 218, "seek": 147608, "start": 1476.08, "end": 1482.1599999999999, "text": " to and if assumptions about those properties in the background turn out to be wrong", "tokens": [50364, 281, 293, 498, 17695, 466, 729, 7221, 294, 264, 3678, 1261, 484, 281, 312, 2085, 50668], "temperature": 0.0, "avg_logprob": -0.09604254103543465, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004954017931595445}, {"id": 219, "seek": 147608, "start": 1483.12, "end": 1492.3999999999999, "text": " then those mistakes will corrupt our explanation that attends only to the foregrounded factors", "tokens": [50716, 550, 729, 8038, 486, 17366, 527, 10835, 300, 49837, 787, 281, 264, 32058, 292, 6771, 51180], "temperature": 0.0, "avg_logprob": -0.09604254103543465, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004954017931595445}, {"id": 220, "seek": 147608, "start": 1492.3999999999999, "end": 1499.6, "text": " so that's a little bit abstract let me just give you a simple example um in comparative cognition", "tokens": [51180, 370, 300, 311, 257, 707, 857, 12649, 718, 385, 445, 976, 291, 257, 2199, 1365, 1105, 294, 39292, 46905, 51540], "temperature": 0.0, "avg_logprob": -0.09604254103543465, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004954017931595445}, {"id": 221, "seek": 149960, "start": 1500.56, "end": 1508.1599999999999, "text": " one famous uh behavioral experiment is the mirror test for self-recognition", "tokens": [50412, 472, 4618, 2232, 19124, 5120, 307, 264, 8013, 1500, 337, 2698, 12, 13867, 2912, 849, 50792], "temperature": 0.0, "avg_logprob": -0.11311890339029246, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.011677524074912071}, {"id": 222, "seek": 149960, "start": 1509.04, "end": 1518.6399999999999, "text": " so the question is roughly do non-human animals have something like a concept of self", "tokens": [50836, 370, 264, 1168, 307, 9810, 360, 2107, 12, 18796, 4882, 362, 746, 411, 257, 3410, 295, 2698, 51316], "temperature": 0.0, "avg_logprob": -0.11311890339029246, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.011677524074912071}, {"id": 223, "seek": 149960, "start": 1519.84, "end": 1529.52, "text": " and the strategy is to put some sort of mark on their body in original experiments it was", "tokens": [51376, 293, 264, 5206, 307, 281, 829, 512, 1333, 295, 1491, 322, 641, 1772, 294, 3380, 12050, 309, 390, 51860], "temperature": 0.0, "avg_logprob": -0.11311890339029246, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.011677524074912071}, {"id": 224, "seek": 152952, "start": 1529.52, "end": 1536.8, "text": " a red dot on the forehead of maybe a monkey and or a bird or whatever and then", "tokens": [50364, 257, 2182, 5893, 322, 264, 20472, 295, 1310, 257, 17847, 293, 420, 257, 5255, 420, 2035, 293, 550, 50728], "temperature": 0.0, "avg_logprob": -0.06547404036802404, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0006459708092734218}, {"id": 225, "seek": 152952, "start": 1538.96, "end": 1546.6399999999999, "text": " you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark", "tokens": [50836, 291, 829, 300, 5496, 294, 1868, 295, 257, 8013, 293, 536, 498, 309, 1669, 604, 5217, 281, 483, 3973, 295, 264, 1491, 51220], "temperature": 0.0, "avg_logprob": -0.06547404036802404, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0006459708092734218}, {"id": 226, "seek": 152952, "start": 1547.76, "end": 1551.92, "text": " and if it does make an attempt to get rid of the mark that shows that it recognizes that the", "tokens": [51276, 293, 498, 309, 775, 652, 364, 5217, 281, 483, 3973, 295, 264, 1491, 300, 3110, 300, 309, 26564, 300, 264, 51484], "temperature": 0.0, "avg_logprob": -0.06547404036802404, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.0006459708092734218}, {"id": 227, "seek": 155192, "start": 1551.92, "end": 1560.3200000000002, "text": " image in the mirror is an image of itself and otherwise not um so that that's a cool way to", "tokens": [50364, 3256, 294, 264, 8013, 307, 364, 3256, 295, 2564, 293, 5911, 406, 1105, 370, 300, 300, 311, 257, 1627, 636, 281, 50784], "temperature": 0.0, "avg_logprob": -0.052472092888572, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.005468133836984634}, {"id": 228, "seek": 155192, "start": 1560.3200000000002, "end": 1569.1200000000001, "text": " get a really difficult and abstract question about the mind of a non-human animal but it presumes", "tokens": [50784, 483, 257, 534, 2252, 293, 12649, 1168, 466, 264, 1575, 295, 257, 2107, 12, 18796, 5496, 457, 309, 1183, 10018, 51224], "temperature": 0.0, "avg_logprob": -0.052472092888572, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.005468133836984634}, {"id": 229, "seek": 155192, "start": 1569.1200000000001, "end": 1576.0800000000002, "text": " or it assumes that animals will care about the fact that they have a red dot on their", "tokens": [51224, 420, 309, 37808, 300, 4882, 486, 1127, 466, 264, 1186, 300, 436, 362, 257, 2182, 5893, 322, 641, 51572], "temperature": 0.0, "avg_logprob": -0.052472092888572, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.005468133836984634}, {"id": 230, "seek": 155192, "start": 1576.0800000000002, "end": 1580.72, "text": " forehead that they will be bothered by that and be motivated to get rid of it and if that", "tokens": [51572, 20472, 300, 436, 486, 312, 22996, 538, 300, 293, 312, 14515, 281, 483, 3973, 295, 309, 293, 498, 300, 51804], "temperature": 0.0, "avg_logprob": -0.052472092888572, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.005468133836984634}, {"id": 231, "seek": 158072, "start": 1580.72, "end": 1586.48, "text": " assumption is wrong then they might fail the mirror test for self-recognition for reasons that", "tokens": [50364, 15302, 307, 2085, 550, 436, 1062, 3061, 264, 8013, 1500, 337, 2698, 12, 13867, 2912, 849, 337, 4112, 300, 50652], "temperature": 0.0, "avg_logprob": -0.07994141999412985, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.0006359516410157084}, {"id": 232, "seek": 158072, "start": 1586.48, "end": 1594.8, "text": " have little to do with the presence or absence of a capacity for self-recognition until something", "tokens": [50652, 362, 707, 281, 360, 365, 264, 6814, 420, 17145, 295, 257, 6042, 337, 2698, 12, 13867, 2912, 849, 1826, 746, 51068], "temperature": 0.0, "avg_logprob": -0.07994141999412985, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.0006359516410157084}, {"id": 233, "seek": 158072, "start": 1594.8, "end": 1605.76, "text": " similar to that is going on we say in large language models so the first example that we give is to", "tokens": [51068, 2531, 281, 300, 307, 516, 322, 321, 584, 294, 2416, 2856, 5245, 370, 264, 700, 1365, 300, 321, 976, 307, 281, 51616], "temperature": 0.0, "avg_logprob": -0.07994141999412985, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.0006359516410157084}, {"id": 234, "seek": 160576, "start": 1605.76, "end": 1620.24, "text": " do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral", "tokens": [50364, 360, 365, 5633, 15107, 370, 2232, 291, 393, 309, 311, 257, 1238, 3303, 2232, 1558, 300, 5699, 291, 992, 493, 257, 19124, 51088], "temperature": 0.0, "avg_logprob": -0.05986499786376953, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.002359234495088458}, {"id": 235, "seek": 160576, "start": 1620.24, "end": 1628.8, "text": " task um there will be demands associated with that task that are not directly related to the", "tokens": [51088, 5633, 1105, 456, 486, 312, 15107, 6615, 365, 300, 5633, 300, 366, 406, 3838, 4077, 281, 264, 51516], "temperature": 0.0, "avg_logprob": -0.05986499786376953, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.002359234495088458}, {"id": 236, "seek": 160576, "start": 1628.8, "end": 1634.24, "text": " capacity that you're trying to test so to take the most obvious example that I can think of", "tokens": [51516, 6042, 300, 291, 434, 1382, 281, 1500, 370, 281, 747, 264, 881, 6322, 1365, 300, 286, 393, 519, 295, 51788], "temperature": 0.0, "avg_logprob": -0.05986499786376953, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.002359234495088458}, {"id": 237, "seek": 163424, "start": 1634.24, "end": 1638.72, "text": " if you give someone a written test they have to be able to write they have to you know have a hand", "tokens": [50364, 498, 291, 976, 1580, 257, 3720, 1500, 436, 362, 281, 312, 1075, 281, 2464, 436, 362, 281, 291, 458, 362, 257, 1011, 50588], "temperature": 0.0, "avg_logprob": -0.08372161123487684, "compression_ratio": 1.86875, "no_speech_prob": 0.004068137612193823}, {"id": 238, "seek": 163424, "start": 1638.72, "end": 1646.88, "text": " and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't", "tokens": [50588, 293, 257, 3435, 293, 2035, 2232, 293, 1105, 498, 641, 291, 458, 1011, 2232, 390, 13408, 420, 2035, 293, 436, 2809, 380, 50996], "temperature": 0.0, "avg_logprob": -0.08372161123487684, "compression_ratio": 1.86875, "no_speech_prob": 0.004068137612193823}, {"id": 239, "seek": 163424, "start": 1646.88, "end": 1653.04, "text": " write then um their failure to fill out the test wouldn't tell you anything about their uh you know", "tokens": [50996, 2464, 550, 1105, 641, 7763, 281, 2836, 484, 264, 1500, 2759, 380, 980, 291, 1340, 466, 641, 2232, 291, 458, 51304], "temperature": 0.0, "avg_logprob": -0.08372161123487684, "compression_ratio": 1.86875, "no_speech_prob": 0.004068137612193823}, {"id": 240, "seek": 165304, "start": 1653.76, "end": 1665.92, "text": " academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral", "tokens": [50400, 7778, 3601, 1105, 370, 321, 3402, 300, 456, 366, 43741, 5633, 15107, 2232, 294, 19124, 51008], "temperature": 0.0, "avg_logprob": -0.11383211078928478, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.08502031862735748}, {"id": 241, "seek": 165304, "start": 1665.92, "end": 1672.96, "text": " tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right", "tokens": [51008, 6921, 295, 257, 2416, 2856, 2316, 293, 436, 434, 13743, 1105, 291, 2759, 380, 291, 2759, 380, 519, 295, 341, 558, 51360], "temperature": 0.0, "avg_logprob": -0.11383211078928478, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.08502031862735748}, {"id": 242, "seek": 165304, "start": 1672.96, "end": 1679.76, "text": " away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic", "tokens": [51360, 1314, 457, 1105, 321, 751, 466, 257, 3035, 490, 2232, 567, 293, 10455, 2232, 436, 362, 257, 1916, 295, 10577, 322, 341, 4829, 51700], "temperature": 0.0, "avg_logprob": -0.11383211078928478, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.08502031862735748}, {"id": 243, "seek": 167976, "start": 1679.76, "end": 1688.32, "text": " but what they do is they give a large language model um the following sort of question uh this", "tokens": [50364, 457, 437, 436, 360, 307, 436, 976, 257, 2416, 2856, 2316, 1105, 264, 3480, 1333, 295, 1168, 2232, 341, 50792], "temperature": 0.0, "avg_logprob": -0.07511116908146785, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0009248841088265181}, {"id": 244, "seek": 167976, "start": 1688.32, "end": 1694.08, "text": " is a question it's for a grammaticality judgment so you can see on the image there which sentence", "tokens": [50792, 307, 257, 1168, 309, 311, 337, 257, 17570, 267, 804, 507, 12216, 370, 291, 393, 536, 322, 264, 3256, 456, 597, 8174, 51080], "temperature": 0.0, "avg_logprob": -0.07511116908146785, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0009248841088265181}, {"id": 245, "seek": 167976, "start": 1694.08, "end": 1699.6, "text": " is better in english number one every child is studied number two every child have studied", "tokens": [51080, 307, 1101, 294, 32169, 1230, 472, 633, 1440, 307, 9454, 1230, 732, 633, 1440, 362, 9454, 51356], "temperature": 0.0, "avg_logprob": -0.07511116908146785, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0009248841088265181}, {"id": 246, "seek": 169960, "start": 1700.3999999999999, "end": 1707.28, "text": " answer with one or two and it gives the wrong output but then", "tokens": [50404, 1867, 365, 472, 420, 732, 293, 309, 2709, 264, 2085, 5598, 457, 550, 50748], "temperature": 0.0, "avg_logprob": -0.07953895354757504, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.09133154898881912}, {"id": 247, "seek": 169960, "start": 1709.6, "end": 1716.32, "text": " you can also simply look at the probabilities assigned to each of those", "tokens": [50864, 291, 393, 611, 2935, 574, 412, 264, 33783, 13279, 281, 1184, 295, 729, 51200], "temperature": 0.0, "avg_logprob": -0.07953895354757504, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.09133154898881912}, {"id": 248, "seek": 169960, "start": 1717.52, "end": 1727.04, "text": " sentences within the model and uh figure out directly whether the model thinks that input", "tokens": [51260, 16579, 1951, 264, 2316, 293, 2232, 2573, 484, 3838, 1968, 264, 2316, 7309, 300, 4846, 51736], "temperature": 0.0, "avg_logprob": -0.07953895354757504, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.09133154898881912}, {"id": 249, "seek": 172704, "start": 1727.04, "end": 1734.24, "text": " A is more likely than input B and it turns out that on a wide variety of questions of this kind", "tokens": [50364, 316, 307, 544, 3700, 813, 4846, 363, 293, 309, 4523, 484, 300, 322, 257, 4874, 5673, 295, 1651, 295, 341, 733, 50724], "temperature": 0.0, "avg_logprob": -0.07651632831942651, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0028427180368453264}, {"id": 250, "seek": 172704, "start": 1735.28, "end": 1742.6399999999999, "text": " the direct comparison uh does or the large language models perform better with the direct", "tokens": [50776, 264, 2047, 9660, 2232, 775, 420, 264, 2416, 2856, 5245, 2042, 1101, 365, 264, 2047, 51144], "temperature": 0.0, "avg_logprob": -0.07651632831942651, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0028427180368453264}, {"id": 251, "seek": 172704, "start": 1742.6399999999999, "end": 1752.3999999999999, "text": " comparison than they do with the more complex demand for uh metalinguistic judgment so the", "tokens": [51144, 9660, 813, 436, 360, 365, 264, 544, 3997, 4733, 337, 2232, 1131, 4270, 84, 3142, 12216, 370, 264, 51632], "temperature": 0.0, "avg_logprob": -0.07651632831942651, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0028427180368453264}, {"id": 252, "seek": 175240, "start": 1752.4, "end": 1761.2800000000002, "text": " fact that the model has to process the numbering of the uh options and then answer in terms of a", "tokens": [50364, 1186, 300, 264, 2316, 575, 281, 1399, 264, 1230, 278, 295, 264, 2232, 3956, 293, 550, 1867, 294, 2115, 295, 257, 50808], "temperature": 0.0, "avg_logprob": -0.069390812090465, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001987150637432933}, {"id": 253, "seek": 175240, "start": 1761.2800000000002, "end": 1771.2800000000002, "text": " number uh is a subtle but nevertheless um important additional variable in the experiment", "tokens": [50808, 1230, 2232, 307, 257, 13743, 457, 26924, 1105, 1021, 4497, 7006, 294, 264, 5120, 51308], "temperature": 0.0, "avg_logprob": -0.069390812090465, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001987150637432933}, {"id": 254, "seek": 175240, "start": 1771.8400000000001, "end": 1776.3200000000002, "text": " and that can influence the model's capacity to get the answer right", "tokens": [51336, 293, 300, 393, 6503, 264, 2316, 311, 6042, 281, 483, 264, 1867, 558, 51560], "temperature": 0.0, "avg_logprob": -0.069390812090465, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001987150637432933}, {"id": 255, "seek": 177632, "start": 1776.56, "end": 1778.96, "text": " Rafael do you want to add to that?", "tokens": [50376, 43173, 360, 291, 528, 281, 909, 281, 300, 30, 50496], "temperature": 0.0, "avg_logprob": -0.27443580627441405, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.008176137693226337}, {"id": 256, "seek": 177632, "start": 1780.6399999999999, "end": 1785.36, "text": " No I think that well maybe we can mention briefly the other example that we discuss which is from", "tokens": [50580, 883, 286, 519, 300, 731, 1310, 321, 393, 2152, 10515, 264, 661, 1365, 300, 321, 2248, 597, 307, 490, 50816], "temperature": 0.0, "avg_logprob": -0.27443580627441405, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.008176137693226337}, {"id": 257, "seek": 177632, "start": 1786.48, "end": 1794.48, "text": " this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes", "tokens": [50872, 341, 3035, 538, 10110, 441, 1215, 5636, 597, 597, 733, 295, 575, 257, 707, 2857, 14751, 300, 1669, 51272], "temperature": 0.0, "avg_logprob": -0.27443580627441405, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.008176137693226337}, {"id": 258, "seek": 177632, "start": 1795.6799999999998, "end": 1801.12, "text": " the example interesting and even more problematic in terms of comparative psychology which is", "tokens": [51332, 264, 1365, 1880, 293, 754, 544, 19011, 294, 2115, 295, 39292, 15105, 597, 307, 51604], "temperature": 0.0, "avg_logprob": -0.27443580627441405, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.008176137693226337}, {"id": 259, "seek": 180112, "start": 1801.84, "end": 1812.7199999999998, "text": " um one way in which auxiliary task demands can be ignored or disregarded or overlooked", "tokens": [50400, 1105, 472, 636, 294, 597, 43741, 5633, 15107, 393, 312, 19735, 420, 36405, 22803, 420, 32269, 50944], "temperature": 0.0, "avg_logprob": -0.17805748655084977, "compression_ratio": 1.5987654320987654, "no_speech_prob": 0.003269757144153118}, {"id": 260, "seek": 180112, "start": 1813.52, "end": 1820.3999999999999, "text": " is when uh you are doing a direct comparison between humans and other epsilon tasks and", "tokens": [50984, 307, 562, 2232, 291, 366, 884, 257, 2047, 9660, 1296, 6255, 293, 661, 17889, 9608, 293, 51328], "temperature": 0.0, "avg_logprob": -0.17805748655084977, "compression_ratio": 1.5987654320987654, "no_speech_prob": 0.003269757144153118}, {"id": 261, "seek": 180112, "start": 1820.3999999999999, "end": 1826.6399999999999, "text": " the experimental conditions are mismatched in such a way that the task as you set up", "tokens": [51328, 264, 17069, 4487, 366, 23220, 24102, 294, 1270, 257, 636, 300, 264, 5633, 382, 291, 992, 493, 51640], "temperature": 0.0, "avg_logprob": -0.17805748655084977, "compression_ratio": 1.5987654320987654, "no_speech_prob": 0.003269757144153118}, {"id": 262, "seek": 182664, "start": 1827.6000000000001, "end": 1835.0400000000002, "text": " impose the stronger demands stronger task and auxiliary demands on the LOM than those on human", "tokens": [50412, 26952, 264, 7249, 15107, 7249, 5633, 293, 43741, 15107, 322, 264, 441, 5251, 813, 729, 322, 1952, 50784], "temperature": 0.0, "avg_logprob": -0.1773174436468827, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0015237031038850546}, {"id": 263, "seek": 182664, "start": 1835.0400000000002, "end": 1840.24, "text": " subjects and that's something that that can happen quite often and so there is this this", "tokens": [50784, 13066, 293, 300, 311, 746, 300, 300, 393, 1051, 1596, 2049, 293, 370, 456, 307, 341, 341, 51044], "temperature": 0.0, "avg_logprob": -0.1773174436468827, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0015237031038850546}, {"id": 264, "seek": 182664, "start": 1840.24, "end": 1846.8000000000002, "text": " interesting example from a couple of papers originally published by black reds and colleagues", "tokens": [51044, 1880, 1365, 490, 257, 1916, 295, 10577, 7993, 6572, 538, 2211, 2182, 82, 293, 7734, 51372], "temperature": 0.0, "avg_logprob": -0.1773174436468827, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0015237031038850546}, {"id": 265, "seek": 182664, "start": 1846.8000000000002, "end": 1853.0400000000002, "text": " from standard hands group where they looked at um the ability of language models to handle their", "tokens": [51372, 490, 3832, 2377, 1594, 689, 436, 2956, 412, 1105, 264, 3485, 295, 2856, 5245, 281, 4813, 641, 51684], "temperature": 0.0, "avg_logprob": -0.1773174436468827, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0015237031038850546}, {"id": 266, "seek": 185304, "start": 1853.04, "end": 1863.68, "text": " recursion um looking at center embedded closes uh how such closes might um", "tokens": [50364, 20560, 313, 1105, 1237, 412, 3056, 16741, 24157, 2232, 577, 1270, 24157, 1062, 1105, 50896], "temperature": 0.0, "avg_logprob": -0.21960122706526417, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.008297575637698174}, {"id": 267, "seek": 185304, "start": 1865.68, "end": 1870.1599999999999, "text": " for example when you had a prepositional phrase within the subject of the sentence and the verb", "tokens": [50996, 337, 1365, 562, 291, 632, 257, 2666, 329, 2628, 9535, 1951, 264, 3983, 295, 264, 8174, 293, 264, 9595, 51220], "temperature": 0.0, "avg_logprob": -0.21960122706526417, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.008297575637698174}, {"id": 268, "seek": 185304, "start": 1870.1599999999999, "end": 1877.68, "text": " might throw up either humans or LOMs into agreeing the the verb in the wrong way", "tokens": [51220, 1062, 3507, 493, 2139, 6255, 420, 441, 5251, 82, 666, 36900, 264, 264, 9595, 294, 264, 2085, 636, 51596], "temperature": 0.0, "avg_logprob": -0.21960122706526417, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.008297575637698174}, {"id": 269, "seek": 187768, "start": 1878.48, "end": 1887.6000000000001, "text": " so giving the wrong number to the verb for example the keys that the man put on the table", "tokens": [50404, 370, 2902, 264, 2085, 1230, 281, 264, 9595, 337, 1365, 264, 9317, 300, 264, 587, 829, 322, 264, 3199, 50860], "temperature": 0.0, "avg_logprob": -0.1307923830472506, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004825761541724205}, {"id": 270, "seek": 187768, "start": 1889.28, "end": 1894.4, "text": " here it should be R because keys is plural but because you have close in the middle", "tokens": [50944, 510, 309, 820, 312, 497, 570, 9317, 307, 25377, 457, 570, 291, 362, 1998, 294, 264, 2808, 51200], "temperature": 0.0, "avg_logprob": -0.1307923830472506, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004825761541724205}, {"id": 271, "seek": 187768, "start": 1895.2, "end": 1899.68, "text": " and if you add more closes like this that are embedded in the middle people and LOMs can get", "tokens": [51240, 293, 498, 291, 909, 544, 24157, 411, 341, 300, 366, 16741, 294, 264, 2808, 561, 293, 441, 5251, 82, 393, 483, 51464], "temperature": 0.0, "avg_logprob": -0.1307923830472506, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.004825761541724205}, {"id": 272, "seek": 189968, "start": 1899.68, "end": 1907.52, "text": " confused and and predict that the verb should be is for example so um they tested this on humans", "tokens": [50364, 9019, 293, 293, 6069, 300, 264, 9595, 820, 312, 307, 337, 1365, 370, 1105, 436, 8246, 341, 322, 6255, 50756], "temperature": 0.0, "avg_logprob": -0.19135612039005054, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0026278928853571415}, {"id": 273, "seek": 189968, "start": 1907.52, "end": 1914.0, "text": " and LOMs and found that on the more complex examples involving complex more complex constructions or", "tokens": [50756, 293, 441, 5251, 82, 293, 1352, 300, 322, 264, 544, 3997, 5110, 17030, 3997, 544, 3997, 7690, 626, 420, 51080], "temperature": 0.0, "avg_logprob": -0.19135612039005054, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0026278928853571415}, {"id": 274, "seek": 189968, "start": 1914.0, "end": 1921.28, "text": " recursion humans were doing decently well but LOMs performance was collapsing compared to the", "tokens": [51080, 20560, 313, 6255, 645, 884, 979, 2276, 731, 457, 441, 5251, 82, 3389, 390, 45339, 5347, 281, 264, 51444], "temperature": 0.0, "avg_logprob": -0.19135612039005054, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0026278928853571415}, {"id": 275, "seek": 189968, "start": 1921.28, "end": 1928.4, "text": " single examples and I've heard an opinion from DeepMind um looked into that and realized that", "tokens": [51444, 2167, 5110, 293, 286, 600, 2198, 364, 4800, 490, 14895, 44, 471, 1105, 2956, 666, 300, 293, 5334, 300, 51800], "temperature": 0.0, "avg_logprob": -0.19135612039005054, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0026278928853571415}, {"id": 276, "seek": 192840, "start": 1928.4, "end": 1933.8400000000001, "text": " the experimental conditions were mismatched so the humans at this very common in cognitive", "tokens": [50364, 264, 17069, 4487, 645, 23220, 24102, 370, 264, 6255, 412, 341, 588, 2689, 294, 15605, 50636], "temperature": 0.0, "avg_logprob": -0.15335656435061723, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.0018091988749802113}, {"id": 277, "seek": 192840, "start": 1933.8400000000001, "end": 1938.96, "text": " science experiments were getting some training before they completed the test items to just get", "tokens": [50636, 3497, 12050, 645, 1242, 512, 3097, 949, 436, 7365, 264, 1500, 4754, 281, 445, 483, 50892], "temperature": 0.0, "avg_logprob": -0.15335656435061723, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.0018091988749802113}, {"id": 278, "seek": 192840, "start": 1938.96, "end": 1945.6000000000001, "text": " familiarized with the task so they were given some examples of the task um harsh condition", "tokens": [50892, 4963, 1602, 365, 264, 5633, 370, 436, 645, 2212, 512, 5110, 295, 264, 5633, 1105, 14897, 4188, 51224], "temperature": 0.0, "avg_logprob": -0.15335656435061723, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.0018091988749802113}, {"id": 279, "seek": 192840, "start": 1946.4, "end": 1956.16, "text": " and the LLabs were just prompted zero shots as um people usually put it so just um without any", "tokens": [51264, 293, 264, 441, 43, 17243, 645, 445, 31042, 4018, 8305, 382, 1105, 561, 2673, 829, 309, 370, 445, 1105, 1553, 604, 51752], "temperature": 0.0, "avg_logprob": -0.15335656435061723, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.0018091988749802113}, {"id": 280, "seek": 195616, "start": 1956.24, "end": 1964.16, "text": " example just point blank and Andrew found out that if you he he he replicated the experiments but", "tokens": [50368, 1365, 445, 935, 8247, 293, 10110, 1352, 484, 300, 498, 291, 415, 415, 415, 46365, 264, 12050, 457, 50764], "temperature": 0.0, "avg_logprob": -0.15245126836440143, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012053505051881075}, {"id": 281, "seek": 195616, "start": 1964.16, "end": 1971.3600000000001, "text": " I did some proper matched testing conditions for the LL so adding some examples of the task in the", "tokens": [50764, 286, 630, 512, 2296, 21447, 4997, 4487, 337, 264, 441, 43, 370, 5127, 512, 5110, 295, 264, 5633, 294, 264, 51124], "temperature": 0.0, "avg_logprob": -0.15245126836440143, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012053505051881075}, {"id": 282, "seek": 195616, "start": 1971.3600000000001, "end": 1977.68, "text": " prompt when it's known as future counting and with that he found that performance was equivalent", "tokens": [51124, 12391, 562, 309, 311, 2570, 382, 2027, 13251, 293, 365, 300, 415, 1352, 300, 3389, 390, 10344, 51440], "temperature": 0.0, "avg_logprob": -0.15245126836440143, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012053505051881075}, {"id": 283, "seek": 195616, "start": 1977.68, "end": 1982.3200000000002, "text": " in fact the LLM that he tested was slightly better on the more complex constructions than humans", "tokens": [51440, 294, 1186, 264, 441, 43, 44, 300, 415, 8246, 390, 4748, 1101, 322, 264, 544, 3997, 7690, 626, 813, 6255, 51672], "temperature": 0.0, "avg_logprob": -0.15245126836440143, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012053505051881075}, {"id": 284, "seek": 198232, "start": 1983.04, "end": 1986.56, "text": " so when you match the test conditions here you actually match also", "tokens": [50400, 370, 562, 291, 2995, 264, 1500, 4487, 510, 291, 767, 2995, 611, 50576], "temperature": 0.0, "avg_logprob": -0.12790793239480197, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0010806977516040206}, {"id": 285, "seek": 198232, "start": 1987.52, "end": 1993.36, "text": " at least you it's it's not it's not automatic but you you you can match the test demands I mean", "tokens": [50624, 412, 1935, 291, 309, 311, 309, 311, 406, 309, 311, 406, 12509, 457, 291, 291, 291, 393, 2995, 264, 1500, 15107, 286, 914, 50916], "temperature": 0.0, "avg_logprob": -0.12790793239480197, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0010806977516040206}, {"id": 286, "seek": 198232, "start": 1993.36, "end": 1997.6799999999998, "text": " it could be that there are reasons why the various experimental conditions would result in different", "tokens": [50916, 309, 727, 312, 300, 456, 366, 4112, 983, 264, 3683, 17069, 4487, 576, 1874, 294, 819, 51132], "temperature": 0.0, "avg_logprob": -0.12790793239480197, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0010806977516040206}, {"id": 287, "seek": 198232, "start": 1997.6799999999998, "end": 2004.32, "text": " demands for humans and others but you're still in this case even on the playing ground playing field", "tokens": [51132, 15107, 337, 6255, 293, 2357, 457, 291, 434, 920, 294, 341, 1389, 754, 322, 264, 2433, 2727, 2433, 2519, 51464], "temperature": 0.0, "avg_logprob": -0.12790793239480197, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0010806977516040206}, {"id": 288, "seek": 198232, "start": 2005.36, "end": 2010.48, "text": " in such a way that you don't find the behavioral discrepancy that you found initially anymore", "tokens": [51516, 294, 1270, 257, 636, 300, 291, 500, 380, 915, 264, 19124, 2983, 265, 6040, 1344, 300, 291, 1352, 9105, 3602, 51772], "temperature": 0.0, "avg_logprob": -0.12790793239480197, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0010806977516040206}, {"id": 289, "seek": 201232, "start": 2013.04, "end": 2021.84, "text": " um yeah um good so shall we continue to the next", "tokens": [50400, 1105, 1338, 1105, 665, 370, 4393, 321, 2354, 281, 264, 958, 50840], "temperature": 0.0, "avg_logprob": -0.19938520590464273, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.001699731219559908}, {"id": 290, "seek": 201232, "start": 2022.8, "end": 2033.84, "text": " section um so another uh another way that auxiliary or another type of auxiliary task", "tokens": [50888, 3541, 1105, 370, 1071, 2232, 1071, 636, 300, 43741, 420, 1071, 2010, 295, 43741, 5633, 51440], "temperature": 0.0, "avg_logprob": -0.19938520590464273, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.001699731219559908}, {"id": 291, "seek": 201232, "start": 2033.84, "end": 2039.6, "text": " demand is input independent computational limitations um and here we're thinking of a", "tokens": [51440, 4733, 307, 4846, 6695, 28270, 15705, 1105, 293, 510, 321, 434, 1953, 295, 257, 51728], "temperature": 0.0, "avg_logprob": -0.19938520590464273, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.001699731219559908}, {"id": 292, "seek": 203960, "start": 2039.6, "end": 2045.6, "text": " few papers that show that the number of forward passes that the transformer can make", "tokens": [50364, 1326, 10577, 300, 855, 300, 264, 1230, 295, 2128, 11335, 300, 264, 31782, 393, 652, 50664], "temperature": 0.0, "avg_logprob": -0.12552901201469954, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.0006461172015406191}, {"id": 293, "seek": 203960, "start": 2046.32, "end": 2053.68, "text": " influences its ability to find the right spot and parameter space so neural networks are", "tokens": [50700, 21222, 1080, 3485, 281, 915, 264, 558, 4008, 293, 13075, 1901, 370, 18161, 9590, 366, 51068], "temperature": 0.0, "avg_logprob": -0.12552901201469954, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.0006461172015406191}, {"id": 294, "seek": 203960, "start": 2056.7999999999997, "end": 2060.08, "text": " function approximators but their", "tokens": [51224, 2445, 8542, 3391, 457, 641, 51388], "temperature": 0.0, "avg_logprob": -0.12552901201469954, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.0006461172015406191}, {"id": 295, "seek": 206008, "start": 2060.08, "end": 2071.04, "text": " um their ability to approximate a function can be eliminated uh can be limited by the", "tokens": [50364, 1105, 641, 3485, 281, 30874, 257, 2445, 393, 312, 20308, 2232, 393, 312, 5567, 538, 264, 50912], "temperature": 0.0, "avg_logprob": -0.2524607976277669, "compression_ratio": 1.385321100917431, "no_speech_prob": 0.002511093392968178}, {"id": 296, "seek": 206008, "start": 2073.2, "end": 2080.48, "text": " the the number of computations it's allowed to perform and um the", "tokens": [51020, 264, 264, 1230, 295, 2807, 763, 309, 311, 4350, 281, 2042, 293, 1105, 264, 51384], "temperature": 0.0, "avg_logprob": -0.2524607976277669, "compression_ratio": 1.385321100917431, "no_speech_prob": 0.002511093392968178}, {"id": 297, "seek": 208048, "start": 2080.96, "end": 2088.64, "text": " uh sort of crucial feature of uh transformers in this example is that", "tokens": [50388, 2232, 1333, 295, 11462, 4111, 295, 2232, 4088, 433, 294, 341, 1365, 307, 300, 50772], "temperature": 0.0, "avg_logprob": -0.20136014620463052, "compression_ratio": 1.4579439252336448, "no_speech_prob": 0.0010482191573828459}, {"id": 298, "seek": 208048, "start": 2089.68, "end": 2097.92, "text": " um the number of operations that determines the next token is limited by the number of", "tokens": [50824, 1105, 264, 1230, 295, 7705, 300, 24799, 264, 958, 14862, 307, 5567, 538, 264, 1230, 295, 51236], "temperature": 0.0, "avg_logprob": -0.20136014620463052, "compression_ratio": 1.4579439252336448, "no_speech_prob": 0.0010482191573828459}, {"id": 299, "seek": 209792, "start": 2097.92, "end": 2108.0, "text": " tokens that it's seen so far and it turns out that if you train a transformer with", "tokens": [50364, 22667, 300, 309, 311, 1612, 370, 1400, 293, 309, 4523, 484, 300, 498, 291, 3847, 257, 31782, 365, 50868], "temperature": 0.0, "avg_logprob": -0.14486276186429536, "compression_ratio": 1.5126050420168067, "no_speech_prob": 0.02716153860092163}, {"id": 300, "seek": 209792, "start": 2109.6800000000003, "end": 2123.2000000000003, "text": " additional meaningless tokens like pause tokens like the word pause you can increase its accuracy", "tokens": [50952, 4497, 33232, 22667, 411, 10465, 22667, 411, 264, 1349, 10465, 291, 393, 3488, 1080, 14170, 51628], "temperature": 0.0, "avg_logprob": -0.14486276186429536, "compression_ratio": 1.5126050420168067, "no_speech_prob": 0.02716153860092163}, {"id": 301, "seek": 212320, "start": 2123.9199999999996, "end": 2132.64, "text": " across a range of of question types um and yeah this is", "tokens": [50400, 2108, 257, 3613, 295, 295, 1168, 3467, 1105, 293, 1338, 341, 307, 50836], "temperature": 0.0, "avg_logprob": -0.25465522493634907, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.0065850368700921535}, {"id": 302, "seek": 212320, "start": 2135.04, "end": 2142.96, "text": " this counts as an auxiliary task demand in our view because um it's doing something roughly", "tokens": [50956, 341, 14893, 382, 364, 43741, 5633, 4733, 294, 527, 1910, 570, 1105, 309, 311, 884, 746, 9810, 51352], "temperature": 0.0, "avg_logprob": -0.25465522493634907, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.0065850368700921535}, {"id": 303, "seek": 212320, "start": 2142.96, "end": 2150.24, "text": " analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry", "tokens": [51352, 16660, 563, 281, 1333, 295, 2902, 264, 2316, 300, 311, 264, 4818, 5952, 558, 457, 406, 1338, 1338, 2597, 51716], "temperature": 0.0, "avg_logprob": -0.25465522493634907, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.0065850368700921535}, {"id": 304, "seek": 215024, "start": 2150.7999999999997, "end": 2158.64, "text": " yeah um it's it's doing something like giving the model uh time to think and um", "tokens": [50392, 1338, 1105, 309, 311, 309, 311, 884, 746, 411, 2902, 264, 2316, 2232, 565, 281, 519, 293, 1105, 50784], "temperature": 0.0, "avg_logprob": -0.11734896046774727, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.0001686275063548237}, {"id": 305, "seek": 215024, "start": 2160.3999999999996, "end": 2170.08, "text": " yeah so so you might think that the absence of that additional inference time is a factor", "tokens": [50872, 1338, 370, 370, 291, 1062, 519, 300, 264, 17145, 295, 300, 4497, 38253, 565, 307, 257, 5952, 51356], "temperature": 0.0, "avg_logprob": -0.11734896046774727, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.0001686275063548237}, {"id": 306, "seek": 215024, "start": 2171.2799999999997, "end": 2177.8399999999997, "text": " that is not directly not conceptually related to its capacity to answer", "tokens": [51416, 300, 307, 406, 3838, 406, 3410, 671, 4077, 281, 1080, 6042, 281, 1867, 51744], "temperature": 0.0, "avg_logprob": -0.11734896046774727, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.0001686275063548237}, {"id": 307, "seek": 217784, "start": 2178.08, "end": 2184.7200000000003, "text": " uh a question like the one on the screen um you know a simple", "tokens": [50376, 2232, 257, 1168, 411, 264, 472, 322, 264, 2568, 1105, 291, 458, 257, 2199, 50708], "temperature": 0.0, "avg_logprob": -0.1954527809506371, "compression_ratio": 1.755, "no_speech_prob": 0.0021128766238689423}, {"id": 308, "seek": 217784, "start": 2184.7200000000003, "end": 2192.2400000000002, "text": " earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy", "tokens": [50708, 4120, 4625, 1168, 2232, 4295, 360, 291, 528, 281, 2836, 294, 544, 2086, 1338, 572, 370, 286, 519, 300, 21663, 51084], "temperature": 0.0, "avg_logprob": -0.1954527809506371, "compression_ratio": 1.755, "no_speech_prob": 0.0021128766238689423}, {"id": 309, "seek": 217784, "start": 2192.2400000000002, "end": 2198.56, "text": " is is a nice one time to think because if you if you tested a human on even a simple mathematical", "tokens": [51084, 307, 307, 257, 1481, 472, 565, 281, 519, 570, 498, 291, 498, 291, 8246, 257, 1952, 322, 754, 257, 2199, 18894, 51400], "temperature": 0.0, "avg_logprob": -0.1954527809506371, "compression_ratio": 1.755, "no_speech_prob": 0.0021128766238689423}, {"id": 310, "seek": 217784, "start": 2198.56, "end": 2205.2000000000003, "text": " questions or any any task really and just ask them you know tell them they have like one second", "tokens": [51400, 1651, 420, 604, 604, 5633, 534, 293, 445, 1029, 552, 291, 458, 980, 552, 436, 362, 411, 472, 1150, 51732], "temperature": 0.0, "avg_logprob": -0.1954527809506371, "compression_ratio": 1.755, "no_speech_prob": 0.0021128766238689423}, {"id": 311, "seek": 220520, "start": 2205.2, "end": 2212.7999999999997, "text": " to just blow it out answer performance would probably be pretty bad um and you can think of", "tokens": [50364, 281, 445, 6327, 309, 484, 1867, 3389, 576, 1391, 312, 1238, 1578, 1105, 293, 291, 393, 519, 295, 50744], "temperature": 0.0, "avg_logprob": -0.1553253173828125, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003071366809308529}, {"id": 312, "seek": 220520, "start": 2212.7999999999997, "end": 2219.3599999999997, "text": " asking an LLN to answer a question point blank as very very loosely analogous to that and obviously", "tokens": [50744, 3365, 364, 441, 43, 45, 281, 1867, 257, 1168, 935, 8247, 382, 588, 588, 37966, 16660, 563, 281, 300, 293, 2745, 51072], "temperature": 0.0, "avg_logprob": -0.1553253173828125, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003071366809308529}, {"id": 313, "seek": 220520, "start": 2220.24, "end": 2225.6, "text": " this is an analogy and there are very important differences here but I think it's a helpful", "tokens": [51116, 341, 307, 364, 21663, 293, 456, 366, 588, 1021, 7300, 510, 457, 286, 519, 309, 311, 257, 4961, 51384], "temperature": 0.0, "avg_logprob": -0.1553253173828125, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003071366809308529}, {"id": 314, "seek": 220520, "start": 2225.6, "end": 2234.08, "text": " heuristic to think about what is um what is going on roughly here and then we uh in in both cases", "tokens": [51384, 415, 374, 3142, 281, 519, 466, 437, 307, 1105, 437, 307, 516, 322, 9810, 510, 293, 550, 321, 2232, 294, 294, 1293, 3331, 51808], "temperature": 0.0, "avg_logprob": -0.1553253173828125, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003071366809308529}, {"id": 315, "seek": 223408, "start": 2234.88, "end": 2242.0, "text": " the system the human or the LLN does not get the chance to perform the necessary computations to", "tokens": [50404, 264, 1185, 264, 1952, 420, 264, 441, 43, 45, 775, 406, 483, 264, 2931, 281, 2042, 264, 4818, 2807, 763, 281, 50760], "temperature": 0.0, "avg_logprob": -0.1228122197664701, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.003940777387470007}, {"id": 316, "seek": 223408, "start": 2242.72, "end": 2248.0, "text": " derive the correct answer and so yeah what we talk about in the paper is that you have these", "tokens": [50796, 28446, 264, 3006, 1867, 293, 370, 1338, 437, 321, 751, 466, 294, 264, 3035, 307, 300, 291, 362, 613, 51060], "temperature": 0.0, "avg_logprob": -0.1228122197664701, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.003940777387470007}, {"id": 317, "seek": 223408, "start": 2248.0, "end": 2255.68, "text": " experimental works during that if you ask a question to a language model um the amount of", "tokens": [51060, 17069, 1985, 1830, 300, 498, 291, 1029, 257, 1168, 281, 257, 2856, 2316, 1105, 264, 2372, 295, 51444], "temperature": 0.0, "avg_logprob": -0.1228122197664701, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.003940777387470007}, {"id": 318, "seek": 225568, "start": 2255.7599999999998, "end": 2261.12, "text": " tokens it's a it's it's that allows to generate before providing the answer", "tokens": [50368, 22667, 309, 311, 257, 309, 311, 309, 311, 300, 4045, 281, 8460, 949, 6530, 264, 1867, 50636], "temperature": 0.0, "avg_logprob": -0.10348349754963446, "compression_ratio": 1.9658119658119657, "no_speech_prob": 0.01797514222562313}, {"id": 319, "seek": 225568, "start": 2263.04, "end": 2268.56, "text": " makes a difference to how accurate it is so if it uh it just generates a few tokens then have to", "tokens": [50732, 1669, 257, 2649, 281, 577, 8559, 309, 307, 370, 498, 309, 2232, 309, 445, 23815, 257, 1326, 22667, 550, 362, 281, 51008], "temperature": 0.0, "avg_logprob": -0.10348349754963446, "compression_ratio": 1.9658119658119657, "no_speech_prob": 0.01797514222562313}, {"id": 320, "seek": 225568, "start": 2268.56, "end": 2272.3999999999996, "text": " give an answer or even if you just have to give the answer point blank with the very first token", "tokens": [51008, 976, 364, 1867, 420, 754, 498, 291, 445, 362, 281, 976, 264, 1867, 935, 8247, 365, 264, 588, 700, 14862, 51200], "temperature": 0.0, "avg_logprob": -0.10348349754963446, "compression_ratio": 1.9658119658119657, "no_speech_prob": 0.01797514222562313}, {"id": 321, "seek": 225568, "start": 2273.2, "end": 2277.2799999999997, "text": " it's going to be less accurate that if you give it a chance to generate a number of tokens before", "tokens": [51240, 309, 311, 516, 281, 312, 1570, 8559, 300, 498, 291, 976, 309, 257, 2931, 281, 8460, 257, 1230, 295, 22667, 949, 51444], "temperature": 0.0, "avg_logprob": -0.10348349754963446, "compression_ratio": 1.9658119658119657, "no_speech_prob": 0.01797514222562313}, {"id": 322, "seek": 225568, "start": 2277.2799999999997, "end": 2284.3199999999997, "text": " giving the answer so the usual way in which this is understood is that when you ask when you", "tokens": [51444, 2902, 264, 1867, 370, 264, 7713, 636, 294, 597, 341, 307, 7320, 307, 300, 562, 291, 1029, 562, 291, 51796], "temperature": 0.0, "avg_logprob": -0.10348349754963446, "compression_ratio": 1.9658119658119657, "no_speech_prob": 0.01797514222562313}, {"id": 323, "seek": 228432, "start": 2284.56, "end": 2288.4, "text": " you you allow the LLN to generate a number of tokens before giving the answer or you even", "tokens": [50376, 291, 291, 2089, 264, 441, 43, 45, 281, 8460, 257, 1230, 295, 22667, 949, 2902, 264, 1867, 420, 291, 754, 50568], "temperature": 0.0, "avg_logprob": -0.11308054830513749, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.0009690341539680958}, {"id": 324, "seek": 228432, "start": 2288.4, "end": 2293.6000000000004, "text": " prompted to do so you say things step by step for example um that's not as chain of thought", "tokens": [50568, 31042, 281, 360, 370, 291, 584, 721, 1823, 538, 1823, 337, 1365, 1105, 300, 311, 406, 382, 5021, 295, 1194, 50828], "temperature": 0.0, "avg_logprob": -0.11308054830513749, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.0009690341539680958}, {"id": 325, "seek": 228432, "start": 2293.6000000000004, "end": 2299.2000000000003, "text": " prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace", "tokens": [50828, 12391, 278, 293, 4476, 437, 291, 434, 884, 307, 291, 434, 19030, 264, 441, 43, 45, 281, 8460, 257, 21577, 13508, 51108], "temperature": 0.0, "avg_logprob": -0.11308054830513749, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.0009690341539680958}, {"id": 326, "seek": 228432, "start": 2299.76, "end": 2305.6800000000003, "text": " or what looks outwardly externally like a reasoning trace in the output before giving an answer", "tokens": [51136, 420, 437, 1542, 26914, 356, 40899, 411, 257, 21577, 13508, 294, 264, 5598, 949, 2902, 364, 1867, 51432], "temperature": 0.0, "avg_logprob": -0.11308054830513749, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.0009690341539680958}, {"id": 327, "seek": 228432, "start": 2306.7200000000003, "end": 2310.0, "text": " and we know that chain of thought prompting increases performance accuracy", "tokens": [51484, 293, 321, 458, 300, 5021, 295, 1194, 12391, 278, 8637, 3389, 14170, 51648], "temperature": 0.0, "avg_logprob": -0.11308054830513749, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.0009690341539680958}, {"id": 328, "seek": 231000, "start": 2310.72, "end": 2322.56, "text": " um but what was found by a couple of papers um is that the mechanistic influence of this process", "tokens": [50400, 1105, 457, 437, 390, 1352, 538, 257, 1916, 295, 10577, 1105, 307, 300, 264, 4236, 3142, 6503, 295, 341, 1399, 50992], "temperature": 0.0, "avg_logprob": -0.12001430071317233, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.001205998589284718}, {"id": 329, "seek": 231000, "start": 2322.56, "end": 2327.36, "text": " is not entirely due to the nature of the tokens that are generated in this reasoning trace", "tokens": [50992, 307, 406, 7696, 3462, 281, 264, 3687, 295, 264, 22667, 300, 366, 10833, 294, 341, 21577, 13508, 51232], "temperature": 0.0, "avg_logprob": -0.12001430071317233, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.001205998589284718}, {"id": 330, "seek": 231000, "start": 2327.92, "end": 2334.96, "text": " in other words it's not just that the LLN has to generate the right tokens corresponding to", "tokens": [51260, 294, 661, 2283, 309, 311, 406, 445, 300, 264, 441, 43, 45, 575, 281, 8460, 264, 558, 22667, 11760, 281, 51612], "temperature": 0.0, "avg_logprob": -0.12001430071317233, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.001205998589284718}, {"id": 331, "seek": 233496, "start": 2334.96, "end": 2341.12, "text": " different steps of reasoning before giving an answer in fact the very back that you allow the", "tokens": [50364, 819, 4439, 295, 21577, 949, 2902, 364, 1867, 294, 1186, 264, 588, 646, 300, 291, 2089, 264, 50672], "temperature": 0.0, "avg_logprob": -0.09474072164418745, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0024339009542018175}, {"id": 332, "seek": 233496, "start": 2341.12, "end": 2347.44, "text": " LLN to just generate tokens any token before giving an answer from a mechanistic perspective", "tokens": [50672, 441, 43, 45, 281, 445, 8460, 22667, 604, 14862, 949, 2902, 364, 1867, 490, 257, 4236, 3142, 4585, 50988], "temperature": 0.0, "avg_logprob": -0.09474072164418745, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0024339009542018175}, {"id": 333, "seek": 233496, "start": 2347.44, "end": 2353.92, "text": " affords the system to perform additional computations that can complete the computational", "tokens": [50988, 2096, 5703, 264, 1185, 281, 2042, 4497, 2807, 763, 300, 393, 3566, 264, 28270, 51312], "temperature": 0.0, "avg_logprob": -0.09474072164418745, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0024339009542018175}, {"id": 334, "seek": 233496, "start": 2353.92, "end": 2359.52, "text": " circuit that otherwise would get a chance to be completed and to derive the correct answer", "tokens": [51312, 9048, 300, 5911, 576, 483, 257, 2931, 281, 312, 7365, 293, 281, 28446, 264, 3006, 1867, 51592], "temperature": 0.0, "avg_logprob": -0.09474072164418745, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0024339009542018175}, {"id": 335, "seek": 233496, "start": 2359.52, "end": 2364.4, "text": " so as Charles mentioned you can have you can set up an experiment when you have the LLN just", "tokens": [51592, 370, 382, 10523, 2835, 291, 393, 362, 291, 393, 992, 493, 364, 5120, 562, 291, 362, 264, 441, 43, 45, 445, 51836], "temperature": 0.0, "avg_logprob": -0.09474072164418745, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0024339009542018175}, {"id": 336, "seek": 236440, "start": 2364.4, "end": 2371.6, "text": " generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the", "tokens": [50364, 8460, 33232, 22667, 411, 7713, 445, 264, 15026, 257, 3840, 295, 15026, 5893, 22667, 949, 2902, 264, 50724], "temperature": 0.0, "avg_logprob": -0.14706634339832125, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008034744532778859}, {"id": 337, "seek": 236440, "start": 2371.6, "end": 2378.56, "text": " answer and the the more dots you allow before the token gives the answer the greater the", "tokens": [50724, 1867, 293, 264, 264, 544, 15026, 291, 2089, 949, 264, 14862, 2709, 264, 1867, 264, 5044, 264, 51072], "temperature": 0.0, "avg_logprob": -0.14706634339832125, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008034744532778859}, {"id": 338, "seek": 236440, "start": 2379.2000000000003, "end": 2385.2000000000003, "text": " expressivity of the system and the more um the more kinds of programs problems you can answer", "tokens": [51104, 5109, 4253, 295, 264, 1185, 293, 264, 544, 1105, 264, 544, 3685, 295, 4268, 2740, 291, 393, 1867, 51404], "temperature": 0.0, "avg_logprob": -0.14706634339832125, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008034744532778859}, {"id": 339, "seek": 236440, "start": 2386.4, "end": 2390.7200000000003, "text": " and so as Charles mentioned every time an LLN is generated in a token the LLN is performing", "tokens": [51464, 293, 370, 382, 10523, 2835, 633, 565, 364, 441, 43, 45, 307, 10833, 294, 257, 14862, 264, 441, 43, 45, 307, 10205, 51680], "temperature": 0.0, "avg_logprob": -0.14706634339832125, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008034744532778859}, {"id": 340, "seek": 239072, "start": 2390.72, "end": 2396.72, "text": " one forward pass and so the more tokens it's generating the more forward passes it's doing", "tokens": [50364, 472, 2128, 1320, 293, 370, 264, 544, 22667, 309, 311, 17746, 264, 544, 2128, 11335, 309, 311, 884, 50664], "temperature": 0.0, "avg_logprob": -0.10847145620018545, "compression_ratio": 1.9071729957805907, "no_speech_prob": 0.002509889891371131}, {"id": 341, "seek": 239072, "start": 2397.2799999999997, "end": 2402.16, "text": " and one way to think about what's going on here as well is that having these additional forward", "tokens": [50692, 293, 472, 636, 281, 519, 466, 437, 311, 516, 322, 510, 382, 731, 307, 300, 1419, 613, 4497, 2128, 50936], "temperature": 0.0, "avg_logprob": -0.10847145620018545, "compression_ratio": 1.9071729957805907, "no_speech_prob": 0.002509889891371131}, {"id": 342, "seek": 239072, "start": 2402.16, "end": 2407.3599999999997, "text": " passes where you you feedback the whole input sequence plus the previously generated tokens", "tokens": [50936, 11335, 689, 291, 291, 5824, 264, 1379, 4846, 8310, 1804, 264, 8046, 10833, 22667, 51196], "temperature": 0.0, "avg_logprob": -0.10847145620018545, "compression_ratio": 1.9071729957805907, "no_speech_prob": 0.002509889891371131}, {"id": 343, "seek": 239072, "start": 2407.3599999999997, "end": 2412.56, "text": " to do the system to generate the next is also a way to introduce a form of recurrence in", "tokens": [51196, 281, 360, 264, 1185, 281, 8460, 264, 958, 307, 611, 257, 636, 281, 5366, 257, 1254, 295, 18680, 10760, 294, 51456], "temperature": 0.0, "avg_logprob": -0.10847145620018545, "compression_ratio": 1.9071729957805907, "no_speech_prob": 0.002509889891371131}, {"id": 344, "seek": 239072, "start": 2412.56, "end": 2418.08, "text": " transformers that are not in terms of the architecture of recurrent networks so that", "tokens": [51456, 4088, 433, 300, 366, 406, 294, 2115, 295, 264, 9482, 295, 18680, 1753, 9590, 370, 300, 51732], "temperature": 0.0, "avg_logprob": -0.10847145620018545, "compression_ratio": 1.9071729957805907, "no_speech_prob": 0.002509889891371131}, {"id": 345, "seek": 241808, "start": 2418.08, "end": 2426.3199999999997, "text": " increases the expressivity and you know in complexity of your unique terms and yeah there is", "tokens": [50364, 8637, 264, 5109, 4253, 293, 291, 458, 294, 14024, 295, 428, 3845, 2115, 293, 1338, 456, 307, 50776], "temperature": 0.0, "avg_logprob": -0.2746556282043457, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0018366278382018209}, {"id": 346, "seek": 241808, "start": 2426.3199999999997, "end": 2433.68, "text": " there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation", "tokens": [50776, 456, 307, 1238, 20050, 4467, 300, 498, 291, 500, 380, 2089, 337, 300, 550, 291, 434, 40288, 257, 27432, 51144], "temperature": 0.0, "avg_logprob": -0.2746556282043457, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0018366278382018209}, {"id": 347, "seek": 241808, "start": 2434.48, "end": 2439.52, "text": " that again we think is very very loosely analogous to prompting a human to answer a", "tokens": [51184, 300, 797, 321, 519, 307, 588, 588, 37966, 16660, 563, 281, 12391, 278, 257, 1952, 281, 1867, 257, 51436], "temperature": 0.0, "avg_logprob": -0.2746556282043457, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0018366278382018209}, {"id": 348, "seek": 241808, "start": 2439.52, "end": 2445.2, "text": " point on the question without thinking so that's the next sense an auxiliary factor because", "tokens": [51436, 935, 322, 264, 1168, 1553, 1953, 370, 300, 311, 264, 958, 2020, 364, 43741, 5952, 570, 51720], "temperature": 0.0, "avg_logprob": -0.2746556282043457, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0018366278382018209}, {"id": 349, "seek": 244520, "start": 2445.2, "end": 2451.2, "text": " if you give the LLN the opportunity to generate enough tokens it might have the competence", "tokens": [50364, 498, 291, 976, 264, 441, 43, 45, 264, 2650, 281, 8460, 1547, 22667, 309, 1062, 362, 264, 39965, 50664], "temperature": 0.0, "avg_logprob": -0.22369676396466684, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.0020489550661295652}, {"id": 350, "seek": 244520, "start": 2452.48, "end": 2458.3199999999997, "text": " to solve a task but you might not see that otherwise and you might get performance errors", "tokens": [50728, 281, 5039, 257, 5633, 457, 291, 1062, 406, 536, 300, 5911, 293, 291, 1062, 483, 3389, 13603, 51020], "temperature": 0.0, "avg_logprob": -0.22369676396466684, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.0020489550661295652}, {"id": 351, "seek": 244520, "start": 2458.3199999999997, "end": 2459.9199999999996, "text": " but you do think it's incompetent", "tokens": [51020, 457, 291, 360, 519, 309, 311, 41602, 317, 51100], "temperature": 0.0, "avg_logprob": -0.22369676396466684, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.0020489550661295652}, {"id": 352, "seek": 244520, "start": 2463.04, "end": 2470.72, "text": " all right yeah okay um so the the third type of type one anthropocentric", "tokens": [51256, 439, 558, 1338, 1392, 1105, 370, 264, 264, 2636, 2010, 295, 2010, 472, 22727, 905, 32939, 51640], "temperature": 0.0, "avg_logprob": -0.22369676396466684, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.0020489550661295652}, {"id": 353, "seek": 247072, "start": 2471.6, "end": 2477.52, "text": " bias that we talk about is mechanistic interference and so this comes from", "tokens": [50408, 12577, 300, 321, 751, 466, 307, 4236, 3142, 24497, 293, 370, 341, 1487, 490, 50704], "temperature": 0.0, "avg_logprob": -0.14055705792976148, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.0009543680353090167}, {"id": 354, "seek": 247072, "start": 2478.48, "end": 2482.16, "text": " the mechanistic interpretability work and the basic idea is that because", "tokens": [50752, 264, 4236, 3142, 7302, 2310, 589, 293, 264, 3875, 1558, 307, 300, 570, 50936], "temperature": 0.0, "avg_logprob": -0.14055705792976148, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.0009543680353090167}, {"id": 355, "seek": 247072, "start": 2484.08, "end": 2489.2799999999997, "text": " large language models are capable of in-context learning they can learn different strategies", "tokens": [51032, 2416, 2856, 5245, 366, 8189, 295, 294, 12, 9000, 3828, 2539, 436, 393, 1466, 819, 9029, 51292], "temperature": 0.0, "avg_logprob": -0.14055705792976148, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.0009543680353090167}, {"id": 356, "seek": 247072, "start": 2490.16, "end": 2494.48, "text": " for solving a different particular type of problem and the strategy that they", "tokens": [51336, 337, 12606, 257, 819, 1729, 2010, 295, 1154, 293, 264, 5206, 300, 436, 51552], "temperature": 0.0, "avg_logprob": -0.14055705792976148, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.0009543680353090167}, {"id": 357, "seek": 249448, "start": 2495.2, "end": 2501.68, "text": " implement at a given time can be different so you can talk about this in terms of", "tokens": [50400, 4445, 412, 257, 2212, 565, 393, 312, 819, 370, 291, 393, 751, 466, 341, 294, 2115, 295, 50724], "temperature": 0.0, "avg_logprob": -0.1043115917005037, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006795057095587254}, {"id": 358, "seek": 249448, "start": 2502.4, "end": 2509.28, "text": " virtual circuits that are formed inside the language model and there's some interesting work from", "tokens": [50760, 6374, 26354, 300, 366, 8693, 1854, 264, 2856, 2316, 293, 456, 311, 512, 1880, 589, 490, 51104], "temperature": 0.0, "avg_logprob": -0.1043115917005037, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006795057095587254}, {"id": 359, "seek": 249448, "start": 2510.2400000000002, "end": 2516.4, "text": " nil nanda and others showing that in some circumstances these two circuits can compete", "tokens": [51152, 297, 388, 297, 5575, 293, 2357, 4099, 300, 294, 512, 9121, 613, 732, 26354, 393, 11831, 51460], "temperature": 0.0, "avg_logprob": -0.1043115917005037, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006795057095587254}, {"id": 360, "seek": 251640, "start": 2516.4, "end": 2525.6800000000003, "text": " with one another so at a certain level of uh or after a certain amount of training", "tokens": [50364, 365, 472, 1071, 370, 412, 257, 1629, 1496, 295, 2232, 420, 934, 257, 1629, 2372, 295, 3097, 50828], "temperature": 0.0, "avg_logprob": -0.10006121585243627, "compression_ratio": 1.6887417218543046, "no_speech_prob": 0.1730748564004898}, {"id": 361, "seek": 251640, "start": 2526.64, "end": 2533.84, "text": " you get one circuit operative after a bit more training you have two different circuits", "tokens": [50876, 291, 483, 472, 9048, 2208, 1166, 934, 257, 857, 544, 3097, 291, 362, 732, 819, 26354, 51236], "temperature": 0.0, "avg_logprob": -0.10006121585243627, "compression_ratio": 1.6887417218543046, "no_speech_prob": 0.1730748564004898}, {"id": 362, "seek": 251640, "start": 2534.88, "end": 2542.48, "text": " but they're uh the first circuit is still sort of dominant and then after additional", "tokens": [51288, 457, 436, 434, 2232, 264, 700, 9048, 307, 920, 1333, 295, 15657, 293, 550, 934, 4497, 51668], "temperature": 0.0, "avg_logprob": -0.10006121585243627, "compression_ratio": 1.6887417218543046, "no_speech_prob": 0.1730748564004898}, {"id": 363, "seek": 254248, "start": 2543.44, "end": 2545.04, "text": " training the model", "tokens": [50412, 3097, 264, 2316, 50492], "temperature": 0.0, "avg_logprob": -0.17800613966855136, "compression_ratio": 1.495798319327731, "no_speech_prob": 0.0027138383593410254}, {"id": 364, "seek": 254248, "start": 2548.0, "end": 2555.52, "text": " converges on on the second circuit and the first one slowly gets sort of", "tokens": [50640, 9652, 2880, 322, 322, 264, 1150, 9048, 293, 264, 700, 472, 5692, 2170, 1333, 295, 51016], "temperature": 0.0, "avg_logprob": -0.17800613966855136, "compression_ratio": 1.495798319327731, "no_speech_prob": 0.0027138383593410254}, {"id": 365, "seek": 254248, "start": 2558.16, "end": 2564.2400000000002, "text": " it sort of ceases to influence the internal operations of the model and it's only once", "tokens": [51148, 309, 1333, 295, 1769, 1957, 281, 6503, 264, 6920, 7705, 295, 264, 2316, 293, 309, 311, 787, 1564, 51452], "temperature": 0.0, "avg_logprob": -0.17800613966855136, "compression_ratio": 1.495798319327731, "no_speech_prob": 0.0027138383593410254}, {"id": 366, "seek": 256424, "start": 2564.3199999999997, "end": 2569.6, "text": " you reach that third phase at which the", "tokens": [50368, 291, 2524, 300, 2636, 5574, 412, 597, 264, 50632], "temperature": 0.0, "avg_logprob": -0.1141310903761122, "compression_ratio": 1.6, "no_speech_prob": 0.013840705156326294}, {"id": 367, "seek": 256424, "start": 2572.4799999999996, "end": 2579.2, "text": " the benefits of the second circuit with respect to the first become visible", "tokens": [50776, 264, 5311, 295, 264, 1150, 9048, 365, 3104, 281, 264, 700, 1813, 8974, 51112], "temperature": 0.0, "avg_logprob": -0.1141310903761122, "compression_ratio": 1.6, "no_speech_prob": 0.013840705156326294}, {"id": 368, "seek": 256424, "start": 2580.24, "end": 2585.6, "text": " so you can you can show using decoding work that the second circuit is there", "tokens": [51164, 370, 291, 393, 291, 393, 855, 1228, 979, 8616, 589, 300, 264, 1150, 9048, 307, 456, 51432], "temperature": 0.0, "avg_logprob": -0.1141310903761122, "compression_ratio": 1.6, "no_speech_prob": 0.013840705156326294}, {"id": 369, "seek": 258560, "start": 2586.56, "end": 2592.4, "text": " uh before you can show behaviorally that the second circuit", "tokens": [50412, 2232, 949, 291, 393, 855, 5223, 379, 300, 264, 1150, 9048, 50704], "temperature": 0.0, "avg_logprob": -0.15579967498779296, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.016651445999741554}, {"id": 370, "seek": 258560, "start": 2594.72, "end": 2603.44, "text": " yields better performance accuracy on on the task so um I suppose there's a combination", "tokens": [50820, 32168, 1101, 3389, 14170, 322, 322, 264, 5633, 370, 1105, 286, 7297, 456, 311, 257, 6562, 51256], "temperature": 0.0, "avg_logprob": -0.15579967498779296, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.016651445999741554}, {"id": 371, "seek": 258560, "start": 2603.44, "end": 2609.8399999999997, "text": " of two ideas here one is that um there are different strategies a model can", "tokens": [51256, 295, 732, 3487, 510, 472, 307, 300, 1105, 456, 366, 819, 9029, 257, 2316, 393, 51576], "temperature": 0.0, "avg_logprob": -0.15579967498779296, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.016651445999741554}, {"id": 372, "seek": 260984, "start": 2610.32, "end": 2621.6000000000004, "text": " implement for solving a problem we can detect those strategies internally using decoding methods", "tokens": [50388, 4445, 337, 12606, 257, 1154, 321, 393, 5531, 729, 9029, 19501, 1228, 979, 8616, 7150, 50952], "temperature": 0.0, "avg_logprob": -0.3420149903548391, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.0037644763942807913}, {"id": 373, "seek": 260984, "start": 2622.4, "end": 2626.6400000000003, "text": " um so three ideas and then the third is uh", "tokens": [50992, 1105, 370, 1045, 3487, 293, 550, 264, 2636, 307, 2232, 51204], "temperature": 0.0, "avg_logprob": -0.3420149903548391, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.0037644763942807913}, {"id": 374, "seek": 260984, "start": 2630.4, "end": 2633.28, "text": " a good strategy can be", "tokens": [51392, 257, 665, 5206, 393, 312, 51536], "temperature": 0.0, "avg_logprob": -0.3420149903548391, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.0037644763942807913}, {"id": 375, "seek": 263328, "start": 2634.2400000000002, "end": 2641.1200000000003, "text": " uh present in some sense in the model um before it has had the chance to influence", "tokens": [50412, 2232, 1974, 294, 512, 2020, 294, 264, 2316, 1105, 949, 309, 575, 632, 264, 2931, 281, 6503, 50756], "temperature": 0.0, "avg_logprob": -0.1835459073384603, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005550569389015436}, {"id": 376, "seek": 263328, "start": 2641.84, "end": 2649.44, "text": " behavior um and and so this is just another way that the link between", "tokens": [50792, 5223, 1105, 293, 293, 370, 341, 307, 445, 1071, 636, 300, 264, 2113, 1296, 51172], "temperature": 0.0, "avg_logprob": -0.1835459073384603, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005550569389015436}, {"id": 377, "seek": 263328, "start": 2650.0, "end": 2655.44, "text": " performance and competence is shown to be more complicated than I might seem at first", "tokens": [51200, 3389, 293, 39965, 307, 4898, 281, 312, 544, 6179, 813, 286, 1062, 1643, 412, 700, 51472], "temperature": 0.0, "avg_logprob": -0.1835459073384603, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005550569389015436}, {"id": 378, "seek": 265544, "start": 2656.08, "end": 2657.44, "text": " graph", "tokens": [50396, 4295, 50464], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 379, "seek": 265544, "start": 2659.76, "end": 2665.12, "text": " and um yeah just to to clarify one thing so the circuits are just um", "tokens": [50580, 293, 1105, 1338, 445, 281, 281, 17594, 472, 551, 370, 264, 26354, 366, 445, 1105, 50848], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 380, "seek": 265544, "start": 2666.48, "end": 2670.2400000000002, "text": " you know ways to think about the causal structure of a neural network and", "tokens": [50916, 291, 458, 2098, 281, 519, 466, 264, 38755, 3877, 295, 257, 18161, 3209, 293, 51104], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 381, "seek": 265544, "start": 2670.2400000000002, "end": 2675.36, "text": " there's essentially computational subgraphs of the network that have a specific function", "tokens": [51104, 456, 311, 4476, 28270, 1422, 34091, 82, 295, 264, 3209, 300, 362, 257, 2685, 2445, 51360], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 382, "seek": 265544, "start": 2675.36, "end": 2679.28, "text": " you can think of a circuit as implementing a particular algorithm or set of computations", "tokens": [51360, 291, 393, 519, 295, 257, 9048, 382, 18114, 257, 1729, 9284, 420, 992, 295, 2807, 763, 51556], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 383, "seek": 265544, "start": 2680.0, "end": 2684.0, "text": " um it's a part of what people are interested in in this mechanistic interpretability literature", "tokens": [51592, 1105, 309, 311, 257, 644, 295, 437, 561, 366, 3102, 294, 294, 341, 4236, 3142, 7302, 2310, 10394, 51792], "temperature": 0.0, "avg_logprob": -0.24488223496303763, "compression_ratio": 1.743801652892562, "no_speech_prob": 0.009845213033258915}, {"id": 384, "seek": 268400, "start": 2684.0, "end": 2690.32, "text": " that we build on people like Neal Mandatrisola and others is reverse engineering the circuit", "tokens": [50364, 300, 321, 1322, 322, 561, 411, 1734, 304, 15458, 267, 5714, 4711, 293, 2357, 307, 9943, 7043, 264, 9048, 50680], "temperature": 0.0, "avg_logprob": -0.20564823615841749, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.001205808948725462}, {"id": 385, "seek": 268400, "start": 2690.32, "end": 2697.44, "text": " steps in deep neural networks and large language models um peer to implement certain well-defined", "tokens": [50680, 4439, 294, 2452, 18161, 9590, 293, 2416, 2856, 5245, 1105, 15108, 281, 4445, 1629, 731, 12, 37716, 51036], "temperature": 0.0, "avg_logprob": -0.20564823615841749, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.001205808948725462}, {"id": 386, "seek": 268400, "start": 2697.44, "end": 2703.68, "text": " algorithms in some cases at least um and the emerging picture that we build on here is that", "tokens": [51036, 14642, 294, 512, 3331, 412, 1935, 1105, 293, 264, 14989, 3036, 300, 321, 1322, 322, 510, 307, 300, 51348], "temperature": 0.0, "avg_logprob": -0.20564823615841749, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.001205808948725462}, {"id": 387, "seek": 268400, "start": 2705.12, "end": 2711.52, "text": " there is a lot of redundancy built into neural networks as they learn to perform a task", "tokens": [51420, 456, 307, 257, 688, 295, 27830, 6717, 3094, 666, 18161, 9590, 382, 436, 1466, 281, 2042, 257, 5633, 51740], "temperature": 0.0, "avg_logprob": -0.20564823615841749, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.001205808948725462}, {"id": 388, "seek": 271152, "start": 2711.6, "end": 2718.24, "text": " optimized as a function that in many cases translates into redundant circuits that relate", "tokens": [50368, 26941, 382, 257, 2445, 300, 294, 867, 3331, 28468, 666, 40997, 26354, 300, 10961, 50700], "temperature": 0.0, "avg_logprob": -0.21283053113268569, "compression_ratio": 1.8737373737373737, "no_speech_prob": 0.004974883981049061}, {"id": 389, "seek": 271152, "start": 2718.24, "end": 2727.04, "text": " to the same tasks the same kinds of um the same kinds that we put out with my things and uh for", "tokens": [50700, 281, 264, 912, 9608, 264, 912, 3685, 295, 1105, 264, 912, 3685, 300, 321, 829, 484, 365, 452, 721, 293, 2232, 337, 51140], "temperature": 0.0, "avg_logprob": -0.21283053113268569, "compression_ratio": 1.8737373737373737, "no_speech_prob": 0.004974883981049061}, {"id": 390, "seek": 271152, "start": 2727.04, "end": 2731.84, "text": " these circuits might be somewhat identical circuits that are just redundant or they might be", "tokens": [51140, 613, 26354, 1062, 312, 8344, 14800, 26354, 300, 366, 445, 40997, 420, 436, 1062, 312, 51380], "temperature": 0.0, "avg_logprob": -0.21283053113268569, "compression_ratio": 1.8737373737373737, "no_speech_prob": 0.004974883981049061}, {"id": 391, "seek": 271152, "start": 2732.48, "end": 2737.28, "text": " different algorithms just to do to do a similar thing and different strategies to solve some", "tokens": [51412, 819, 14642, 445, 281, 360, 281, 360, 257, 2531, 551, 293, 819, 9029, 281, 5039, 512, 51652], "temperature": 0.0, "avg_logprob": -0.21283053113268569, "compression_ratio": 1.8737373737373737, "no_speech_prob": 0.004974883981049061}, {"id": 392, "seek": 273728, "start": 2737.28, "end": 2741.28, "text": " problem I swear maybe one will be a bit more approximative and the other one a bit more exact", "tokens": [50364, 1154, 286, 11902, 1310, 472, 486, 312, 257, 857, 544, 8542, 1166, 293, 264, 661, 472, 257, 857, 544, 1900, 50564], "temperature": 0.0, "avg_logprob": -0.14253159002824264, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0038215159438550472}, {"id": 393, "seek": 273728, "start": 2741.28, "end": 2749.76, "text": " more computationally intensive so that's where you can have some interference um where one uh", "tokens": [50564, 544, 24903, 379, 18957, 370, 300, 311, 689, 291, 393, 362, 512, 24497, 1105, 689, 472, 2232, 50988], "temperature": 0.0, "avg_logprob": -0.14253159002824264, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0038215159438550472}, {"id": 394, "seek": 273728, "start": 2750.7200000000003, "end": 2758.1600000000003, "text": " or at least some competition where once your kid takes over another and um such that the other", "tokens": [51036, 420, 412, 1935, 512, 6211, 689, 1564, 428, 1636, 2516, 670, 1071, 293, 1105, 1270, 300, 264, 661, 51408], "temperature": 0.0, "avg_logprob": -0.14253159002824264, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0038215159438550472}, {"id": 395, "seek": 273728, "start": 2758.1600000000003, "end": 2763.2000000000003, "text": " becomes kind of you know it's there it's latent in the system but you don't get a chance to influence", "tokens": [51408, 3643, 733, 295, 291, 458, 309, 311, 456, 309, 311, 48994, 294, 264, 1185, 457, 291, 500, 380, 483, 257, 2931, 281, 6503, 51660], "temperature": 0.0, "avg_logprob": -0.14253159002824264, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0038215159438550472}, {"id": 396, "seek": 276320, "start": 2763.2, "end": 2768.7999999999997, "text": " behavior on a specific input so you can get a performance error for that reason and these can", "tokens": [50364, 5223, 322, 257, 2685, 4846, 370, 291, 393, 483, 257, 3389, 6713, 337, 300, 1778, 293, 613, 393, 50644], "temperature": 0.0, "avg_logprob": -0.08736756835320983, "compression_ratio": 1.765625, "no_speech_prob": 0.006484850309789181}, {"id": 397, "seek": 276320, "start": 2768.7999999999997, "end": 2774.96, "text": " combine with the other things we mentioned here so things like task demands the first thing we", "tokens": [50644, 10432, 365, 264, 661, 721, 321, 2835, 510, 370, 721, 411, 5633, 15107, 264, 700, 551, 321, 50952], "temperature": 0.0, "avg_logprob": -0.08736756835320983, "compression_ratio": 1.765625, "no_speech_prob": 0.006484850309789181}, {"id": 398, "seek": 276320, "start": 2774.96, "end": 2781.2799999999997, "text": " discussed as well as the number of tokens you generate both of these things could cause a", "tokens": [50952, 7152, 382, 731, 382, 264, 1230, 295, 22667, 291, 8460, 1293, 295, 613, 721, 727, 3082, 257, 51268], "temperature": 0.0, "avg_logprob": -0.08736756835320983, "compression_ratio": 1.765625, "no_speech_prob": 0.006484850309789181}, {"id": 399, "seek": 276320, "start": 2781.2799999999997, "end": 2787.12, "text": " particular circuit to take over another um so it's it's we can think of this holistically as", "tokens": [51268, 1729, 9048, 281, 747, 670, 1071, 1105, 370, 309, 311, 309, 311, 321, 393, 519, 295, 341, 4091, 20458, 382, 51560], "temperature": 0.0, "avg_logprob": -0.08736756835320983, "compression_ratio": 1.765625, "no_speech_prob": 0.006484850309789181}, {"id": 400, "seek": 276320, "start": 2787.9199999999996, "end": 2792.3199999999997, "text": " perhaps if you ask a question point blank to a model without letting it generate", "tokens": [51600, 4317, 498, 291, 1029, 257, 1168, 935, 8247, 281, 257, 2316, 1553, 8295, 309, 8460, 51820], "temperature": 0.0, "avg_logprob": -0.08736756835320983, "compression_ratio": 1.765625, "no_speech_prob": 0.006484850309789181}, {"id": 401, "seek": 279232, "start": 2792.4, "end": 2797.1200000000003, "text": " bunch of tokens before giving an answer then one particular approximate circuit might take over", "tokens": [50368, 3840, 295, 22667, 949, 2902, 364, 1867, 550, 472, 1729, 30874, 9048, 1062, 747, 670, 50604], "temperature": 0.0, "avg_logprob": -0.19012845572778733, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.0006869829958304763}, {"id": 402, "seek": 279232, "start": 2797.1200000000003, "end": 2803.1200000000003, "text": " that gives the wrong answer if you let it generate more tokens then another more exact circuit might", "tokens": [50604, 300, 2709, 264, 2085, 1867, 498, 291, 718, 309, 8460, 544, 22667, 550, 1071, 544, 1900, 9048, 1062, 50904], "temperature": 0.0, "avg_logprob": -0.19012845572778733, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.0006869829958304763}, {"id": 403, "seek": 279232, "start": 2803.1200000000003, "end": 2810.8, "text": " be given a chance to um it could influence the output using the right answer and similarly with", "tokens": [50904, 312, 2212, 257, 2931, 281, 1105, 309, 727, 6503, 264, 5598, 1228, 264, 558, 1867, 293, 14138, 365, 51288], "temperature": 0.0, "avg_logprob": -0.19012845572778733, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.0006869829958304763}, {"id": 404, "seek": 281080, "start": 2810.8, "end": 2822.32, "text": " task demands uh strong task demands might uh in some cases um impede on the uh triggering of a", "tokens": [50364, 5633, 15107, 2232, 2068, 5633, 15107, 1062, 2232, 294, 512, 3331, 1105, 704, 4858, 322, 264, 2232, 40406, 295, 257, 50940], "temperature": 0.0, "avg_logprob": -0.19749187287830172, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.0012237683404237032}, {"id": 405, "seek": 281080, "start": 2822.32, "end": 2828.0800000000004, "text": " certain circuits that would otherwise have given the right answer um so that could be the case", "tokens": [50940, 1629, 26354, 300, 576, 5911, 362, 2212, 264, 558, 1867, 1105, 370, 300, 727, 312, 264, 1389, 51228], "temperature": 0.0, "avg_logprob": -0.19749187287830172, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.0012237683404237032}, {"id": 406, "seek": 281080, "start": 2828.0800000000004, "end": 2833.52, "text": " perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts", "tokens": [51228, 4317, 294, 264, 28027, 9675, 293, 25551, 88, 364, 1365, 689, 2902, 5110, 295, 264, 5633, 294, 264, 41095, 51500], "temperature": 0.0, "avg_logprob": -0.19749187287830172, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.0012237683404237032}, {"id": 407, "seek": 281080, "start": 2834.0800000000004, "end": 2840.0, "text": " might actually prime the word circuits to solve the task about complex recursive cases in the right", "tokens": [51528, 1062, 767, 5835, 264, 1349, 26354, 281, 5039, 264, 5633, 466, 3997, 20560, 488, 3331, 294, 264, 558, 51824], "temperature": 0.0, "avg_logprob": -0.19749187287830172, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.0012237683404237032}, {"id": 408, "seek": 284000, "start": 2840.0, "end": 2848.24, "text": " way um so yeah these are the three main auxiliary factors that relates to what we call type one", "tokens": [50364, 636, 1105, 370, 1338, 613, 366, 264, 1045, 2135, 43741, 6771, 300, 16155, 281, 437, 321, 818, 2010, 472, 50776], "temperature": 0.0, "avg_logprob": -0.2292083279117123, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.002469928003847599}, {"id": 409, "seek": 284000, "start": 2848.24, "end": 2853.68, "text": " anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should", "tokens": [50776, 22727, 32702, 1434, 22727, 905, 317, 81, 1434, 2597, 741, 2041, 321, 820, 321, 600, 668, 257, 857, 938, 2098, 321, 820, 51048], "temperature": 0.0, "avg_logprob": -0.2292083279117123, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.002469928003847599}, {"id": 410, "seek": 284000, "start": 2853.68, "end": 2860.88, "text": " be quick on type two do you want to uh pick fix things after child yeah so type one uh deals", "tokens": [51048, 312, 1702, 322, 2010, 732, 360, 291, 528, 281, 2232, 1888, 3191, 721, 934, 1440, 1338, 370, 2010, 472, 2232, 11215, 51408], "temperature": 0.0, "avg_logprob": -0.2292083279117123, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.002469928003847599}, {"id": 411, "seek": 284000, "start": 2860.88, "end": 2869.92, "text": " with cases where performance of the model is um weak compared to humans so the model doesn't do", "tokens": [51408, 365, 3331, 689, 3389, 295, 264, 2316, 307, 1105, 5336, 5347, 281, 6255, 370, 264, 2316, 1177, 380, 360, 51860], "temperature": 0.0, "avg_logprob": -0.2292083279117123, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.002469928003847599}, {"id": 412, "seek": 286992, "start": 2869.92, "end": 2877.76, "text": " so well um and then type two is when the model does do well but nevertheless is different in some", "tokens": [50364, 370, 731, 1105, 293, 550, 2010, 732, 307, 562, 264, 2316, 775, 360, 731, 457, 26924, 307, 819, 294, 512, 50756], "temperature": 0.0, "avg_logprob": -0.05603237379164923, "compression_ratio": 1.6946107784431137, "no_speech_prob": 0.0020498610101640224}, {"id": 413, "seek": 286992, "start": 2877.76, "end": 2886.0, "text": " respect from the uh performance profile of the human or we have some evidence to think that the", "tokens": [50756, 3104, 490, 264, 2232, 3389, 7964, 295, 264, 1952, 420, 321, 362, 512, 4467, 281, 519, 300, 264, 51168], "temperature": 0.0, "avg_logprob": -0.05603237379164923, "compression_ratio": 1.6946107784431137, "no_speech_prob": 0.0020498610101640224}, {"id": 414, "seek": 286992, "start": 2886.0, "end": 2894.56, "text": " model uses a different strategy than humans typically use and um the idea is that um even", "tokens": [51168, 2316, 4960, 257, 819, 5206, 813, 6255, 5850, 764, 293, 1105, 264, 1558, 307, 300, 1105, 754, 51596], "temperature": 0.0, "avg_logprob": -0.05603237379164923, "compression_ratio": 1.6946107784431137, "no_speech_prob": 0.0020498610101640224}, {"id": 415, "seek": 289456, "start": 2894.64, "end": 2900.7999999999997, "text": " once you hold performance equivalent or average performance equivalent um you know making a different", "tokens": [50368, 1564, 291, 1797, 3389, 10344, 420, 4274, 3389, 10344, 1105, 291, 458, 1455, 257, 819, 50676], "temperature": 0.0, "avg_logprob": -0.09335790246219958, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.008059324696660042}, {"id": 416, "seek": 289456, "start": 2900.7999999999997, "end": 2907.44, "text": " pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability", "tokens": [50676, 5102, 295, 13603, 420, 32328, 257, 819, 5206, 382, 43699, 1232, 538, 2232, 291, 458, 512, 7302, 2310, 51008], "temperature": 0.0, "avg_logprob": -0.09335790246219958, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.008059324696660042}, {"id": 417, "seek": 289456, "start": 2907.44, "end": 2917.84, "text": " work any deviance from the human strategy is evidence of fragility or only a trick solution", "tokens": [51008, 589, 604, 1905, 6276, 490, 264, 1952, 5206, 307, 4467, 295, 9241, 1140, 420, 787, 257, 4282, 3827, 51528], "temperature": 0.0, "avg_logprob": -0.09335790246219958, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.008059324696660042}, {"id": 418, "seek": 291784, "start": 2918.32, "end": 2929.6800000000003, "text": " um and uh this point is a bit more philosophical i suppose but the um idea is that", "tokens": [50388, 1105, 293, 2232, 341, 935, 307, 257, 857, 544, 25066, 741, 7297, 457, 264, 1105, 1558, 307, 300, 50956], "temperature": 0.0, "avg_logprob": -0.13097344912015474, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.011150737293064594}, {"id": 419, "seek": 291784, "start": 2931.76, "end": 2941.28, "text": " the human strategy for solving a problem um isn't necessarily the most general strategy", "tokens": [51060, 264, 1952, 5206, 337, 12606, 257, 1154, 1105, 1943, 380, 4725, 264, 881, 2674, 5206, 51536], "temperature": 0.0, "avg_logprob": -0.13097344912015474, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.011150737293064594}, {"id": 420, "seek": 294128, "start": 2942.0, "end": 2952.88, "text": " for solving a problem and uh what matters is whether the strategy that is pursued by the model", "tokens": [50400, 337, 12606, 257, 1154, 293, 2232, 437, 7001, 307, 1968, 264, 5206, 300, 307, 34893, 538, 264, 2316, 50944], "temperature": 0.0, "avg_logprob": -0.12182200232217479, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.007570669054985046}, {"id": 421, "seek": 294128, "start": 2952.88, "end": 2960.2400000000002, "text": " is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human", "tokens": [50944, 307, 2674, 1968, 309, 311, 13956, 1968, 309, 311, 8559, 2232, 293, 406, 17003, 1968, 309, 24238, 264, 1952, 51312], "temperature": 0.0, "avg_logprob": -0.12182200232217479, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.007570669054985046}, {"id": 422, "seek": 296024, "start": 2960.24, "end": 2973.2799999999997, "text": " strategy um yeah and we end the we end the paper by considering an objection um which is um", "tokens": [50364, 5206, 1105, 1338, 293, 321, 917, 264, 321, 917, 264, 3035, 538, 8079, 364, 35756, 1105, 597, 307, 1105, 51016], "temperature": 0.0, "avg_logprob": -0.20525976327749398, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.004670048598200083}, {"id": 423, "seek": 296024, "start": 2975.7599999999998, "end": 2986.0, "text": " why um like given that um in humans we study cognition largely through language", "tokens": [51140, 983, 1105, 411, 2212, 300, 1105, 294, 6255, 321, 2979, 46905, 11611, 807, 2856, 51652], "temperature": 0.0, "avg_logprob": -0.20525976327749398, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.004670048598200083}, {"id": 424, "seek": 298600, "start": 2986.96, "end": 2993.76, "text": " um and given that elements are trained on language or um linguistic outputs from", "tokens": [50412, 1105, 293, 2212, 300, 4959, 366, 8895, 322, 2856, 420, 1105, 43002, 23930, 490, 50752], "temperature": 0.0, "avg_logprob": -0.1803921983953108, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0019517368637025356}, {"id": 425, "seek": 298600, "start": 2993.76, "end": 3001.12, "text": " humans um isn't it appropriate after all to treat um human cognition as the correct or", "tokens": [50752, 6255, 1105, 1943, 380, 309, 6854, 934, 439, 281, 2387, 1105, 1952, 46905, 382, 264, 3006, 420, 51120], "temperature": 0.0, "avg_logprob": -0.1803921983953108, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0019517368637025356}, {"id": 426, "seek": 298600, "start": 3001.12, "end": 3007.76, "text": " appropriate the obstacle to study elements and we to that we answer that it depends how we think", "tokens": [51120, 6854, 264, 23112, 281, 2979, 4959, 293, 321, 281, 300, 321, 1867, 300, 309, 5946, 577, 321, 519, 51452], "temperature": 0.0, "avg_logprob": -0.1803921983953108, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0019517368637025356}, {"id": 427, "seek": 300776, "start": 3008.32, "end": 3015.0400000000004, "text": " of that dialectic um so we acknowledge that there is there is no", "tokens": [50392, 295, 300, 24652, 299, 1105, 370, 321, 10692, 300, 456, 307, 456, 307, 572, 50728], "temperature": 0.0, "avg_logprob": -0.1894680947968454, "compression_ratio": 1.8522727272727273, "no_speech_prob": 0.004250526428222656}, {"id": 428, "seek": 300776, "start": 3016.48, "end": 3020.7200000000003, "text": " really other option than to start or investigation of cognitive abilities in algorithms", "tokens": [50800, 534, 661, 3614, 813, 281, 722, 420, 9627, 295, 15605, 11582, 294, 14642, 51012], "temperature": 0.0, "avg_logprob": -0.1894680947968454, "compression_ratio": 1.8522727272727273, "no_speech_prob": 0.004250526428222656}, {"id": 429, "seek": 300776, "start": 3021.28, "end": 3028.0800000000004, "text": " but with reference to human cognitive abilities using human cognitive abilities as some kind of", "tokens": [51040, 457, 365, 6408, 281, 1952, 15605, 11582, 1228, 1952, 15605, 11582, 382, 512, 733, 295, 51380], "temperature": 0.0, "avg_logprob": -0.1894680947968454, "compression_ratio": 1.8522727272727273, "no_speech_prob": 0.004250526428222656}, {"id": 430, "seek": 300776, "start": 3028.0800000000004, "end": 3033.36, "text": " realistic or reference points things like theory of mind memory metacognition", "tokens": [51380, 12465, 420, 6408, 2793, 721, 411, 5261, 295, 1575, 4675, 1131, 326, 2912, 849, 51644], "temperature": 0.0, "avg_logprob": -0.1894680947968454, "compression_ratio": 1.8522727272727273, "no_speech_prob": 0.004250526428222656}, {"id": 431, "seek": 303336, "start": 3034.32, "end": 3039.52, "text": " various forms of reasoning etc that are familiar to us because we humans have them", "tokens": [50412, 3683, 6422, 295, 21577, 5183, 300, 366, 4963, 281, 505, 570, 321, 6255, 362, 552, 50672], "temperature": 0.0, "avg_logprob": -0.1281533845713441, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.009379420429468155}, {"id": 432, "seek": 303336, "start": 3040.88, "end": 3044.96, "text": " and this is the same thing by the way in animal cognition for example or in developmental psychology", "tokens": [50740, 293, 341, 307, 264, 912, 551, 538, 264, 636, 294, 5496, 46905, 337, 1365, 420, 294, 30160, 15105, 50944], "temperature": 0.0, "avg_logprob": -0.1281533845713441, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.009379420429468155}, {"id": 433, "seek": 303336, "start": 3044.96, "end": 3051.84, "text": " where in any comparative psychology setup um the reference point for what concepts", "tokens": [50944, 689, 294, 604, 39292, 15105, 8657, 1105, 264, 6408, 935, 337, 437, 10392, 51288], "temperature": 0.0, "avg_logprob": -0.1281533845713441, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.009379420429468155}, {"id": 434, "seek": 303336, "start": 3051.84, "end": 3061.36, "text": " psychological capacities initially at least um is necessarily tied up with our conception of what", "tokens": [51288, 14346, 39396, 9105, 412, 1935, 1105, 307, 4725, 9601, 493, 365, 527, 30698, 295, 437, 51764], "temperature": 0.0, "avg_logprob": -0.1281533845713441, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.009379420429468155}, {"id": 435, "seek": 306136, "start": 3061.36, "end": 3067.1200000000003, "text": " we human we humans have in our repertoire of cognitive capacities but we emphasize that this", "tokens": [50364, 321, 1952, 321, 6255, 362, 294, 527, 49604, 295, 15605, 39396, 457, 321, 16078, 300, 341, 50652], "temperature": 0.0, "avg_logprob": -0.2287660558172997, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0022452417761087418}, {"id": 436, "seek": 306136, "start": 3067.1200000000003, "end": 3071.76, "text": " is only the starting point so here we've over written from uh the philosopher Ali Boyle who", "tokens": [50652, 307, 787, 264, 2891, 935, 370, 510, 321, 600, 670, 3720, 490, 2232, 264, 29805, 12020, 9486, 306, 567, 50884], "temperature": 0.0, "avg_logprob": -0.2287660558172997, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0022452417761087418}, {"id": 437, "seek": 306136, "start": 3071.76, "end": 3076.7200000000003, "text": " calls this investigative kinds investigative cognitive kinds we start with a cognitive", "tokens": [50884, 5498, 341, 45495, 3685, 45495, 15605, 3685, 321, 722, 365, 257, 15605, 51132], "temperature": 0.0, "avg_logprob": -0.2287660558172997, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0022452417761087418}, {"id": 438, "seek": 306136, "start": 3076.7200000000003, "end": 3083.28, "text": " kind like memory or metacognition episodic memory metacognition theory of mind um as", "tokens": [51132, 733, 411, 4675, 420, 1131, 326, 2912, 849, 39200, 299, 4675, 1131, 326, 2912, 849, 5261, 295, 1575, 1105, 382, 51460], "temperature": 0.0, "avg_logprob": -0.2287660558172997, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0022452417761087418}, {"id": 439, "seek": 306136, "start": 3083.28, "end": 3087.2000000000003, "text": " as an investigative starting point the starting point of the investigation and then we have", "tokens": [51460, 382, 364, 45495, 2891, 935, 264, 2891, 935, 295, 264, 9627, 293, 550, 321, 362, 51656], "temperature": 0.0, "avg_logprob": -0.2287660558172997, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0022452417761087418}, {"id": 440, "seek": 308720, "start": 3087.2, "end": 3094.16, "text": " that we we can try to start operationalizing operationalizing this concept this kind this", "tokens": [50364, 300, 321, 321, 393, 853, 281, 722, 16607, 3319, 16607, 3319, 341, 3410, 341, 733, 341, 50712], "temperature": 0.0, "avg_logprob": -0.21153207506452287, "compression_ratio": 1.84375, "no_speech_prob": 0.005988408345729113}, {"id": 441, "seek": 308720, "start": 3094.16, "end": 3100.48, "text": " cognitive capacity in an experiments testing the algorithms on it with an open", "tokens": [50712, 15605, 6042, 294, 364, 12050, 4997, 264, 14642, 322, 309, 365, 364, 1269, 51028], "temperature": 0.0, "avg_logprob": -0.21153207506452287, "compression_ratio": 1.84375, "no_speech_prob": 0.005988408345729113}, {"id": 442, "seek": 308720, "start": 3101.2799999999997, "end": 3107.2799999999997, "text": " mandated empirical approach and then based on the results from that each relatively refine", "tokens": [51068, 47563, 31886, 3109, 293, 550, 2361, 322, 264, 3542, 490, 300, 1184, 7226, 33906, 51368], "temperature": 0.0, "avg_logprob": -0.21153207506452287, "compression_ratio": 1.84375, "no_speech_prob": 0.005988408345729113}, {"id": 443, "seek": 308720, "start": 3108.56, "end": 3113.9199999999996, "text": " the capacities that we are the capacity that we're targeting or the definition of the capacity", "tokens": [51432, 264, 39396, 300, 321, 366, 264, 6042, 300, 321, 434, 17918, 420, 264, 7123, 295, 264, 6042, 51700], "temperature": 0.0, "avg_logprob": -0.21153207506452287, "compression_ratio": 1.84375, "no_speech_prob": 0.005988408345729113}, {"id": 444, "seek": 311392, "start": 3114.08, "end": 3121.12, "text": " targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions", "tokens": [50372, 17918, 294, 257, 636, 300, 727, 13145, 1477, 505, 281, 2073, 264, 264, 5883, 22727, 905, 32939, 17695, 50724], "temperature": 0.0, "avg_logprob": -0.1274869441986084, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.00258803041651845}, {"id": 445, "seek": 311392, "start": 3121.12, "end": 3130.96, "text": " that we have such that as the experimental um project runs a course or as as we make as we", "tokens": [50724, 300, 321, 362, 1270, 300, 382, 264, 17069, 1105, 1716, 6676, 257, 1164, 420, 382, 382, 321, 652, 382, 321, 51216], "temperature": 0.0, "avg_logprob": -0.1274869441986084, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.00258803041651845}, {"id": 446, "seek": 311392, "start": 3130.96, "end": 3139.36, "text": " we get more results and refine our concepts we may end up with um something that no longer", "tokens": [51216, 321, 483, 544, 3542, 293, 33906, 527, 10392, 321, 815, 917, 493, 365, 1105, 746, 300, 572, 2854, 51636], "temperature": 0.0, "avg_logprob": -0.1274869441986084, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.00258803041651845}, {"id": 447, "seek": 313936, "start": 3139.36, "end": 3148.32, "text": " looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS", "tokens": [50364, 1542, 411, 1237, 1382, 281, 915, 1952, 411, 39200, 299, 4675, 294, 3342, 33302, 420, 2232, 294, 1105, 420, 294, 441, 10288, 50812], "temperature": 0.0, "avg_logprob": -0.13981007968678194, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.002668507630005479}, {"id": 448, "seek": 313936, "start": 3149.1200000000003, "end": 3157.04, "text": " but ends up looking like looking for something that some capacity that is that shares some similarity", "tokens": [50852, 457, 5314, 493, 1237, 411, 1237, 337, 746, 300, 512, 6042, 300, 307, 300, 12182, 512, 32194, 51248], "temperature": 0.0, "avg_logprob": -0.13981007968678194, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.002668507630005479}, {"id": 449, "seek": 313936, "start": 3157.04, "end": 3162.56, "text": " with human like human episodic memory but is different in other respects um and so we can", "tokens": [51248, 365, 1952, 411, 1952, 39200, 299, 4675, 457, 307, 819, 294, 661, 24126, 1105, 293, 370, 321, 393, 51524], "temperature": 0.0, "avg_logprob": -0.13981007968678194, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.002668507630005479}, {"id": 450, "seek": 313936, "start": 3162.56, "end": 3168.88, "text": " gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric", "tokens": [51524, 13145, 808, 493, 365, 257, 733, 295, 15605, 6592, 1793, 337, 264, 3652, 300, 307, 1570, 22727, 905, 32939, 51840], "temperature": 0.0, "avg_logprob": -0.13981007968678194, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.002668507630005479}, {"id": 451, "seek": 316936, "start": 3170.0, "end": 3174.48, "text": " so we emphasize that it is kind of due to feedback look here that's that's that's premised on open", "tokens": [50396, 370, 321, 16078, 300, 309, 307, 733, 295, 3462, 281, 5824, 574, 510, 300, 311, 300, 311, 300, 311, 5624, 2640, 322, 1269, 50620], "temperature": 0.0, "avg_logprob": -0.17933202826458475, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0014058653032407165}, {"id": 452, "seek": 316936, "start": 3174.48, "end": 3179.92, "text": " minded empirical investigation that doesn't settle this question a priori but has to start", "tokens": [50620, 36707, 31886, 9627, 300, 1177, 380, 11852, 341, 1168, 257, 4059, 72, 457, 575, 281, 722, 50892], "temperature": 0.0, "avg_logprob": -0.17933202826458475, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0014058653032407165}, {"id": 453, "seek": 316936, "start": 3180.8, "end": 3184.2400000000002, "text": " as a necessary starting point with the the reference to human cognition", "tokens": [50936, 382, 257, 4818, 2891, 935, 365, 264, 264, 6408, 281, 1952, 46905, 51108], "temperature": 0.0, "avg_logprob": -0.17933202826458475, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0014058653032407165}, {"id": 454, "seek": 316936, "start": 3185.1200000000003, "end": 3186.8, "text": " i don't know if you want to add to that joss", "tokens": [51152, 741, 500, 380, 458, 498, 291, 528, 281, 909, 281, 300, 361, 772, 51236], "temperature": 0.0, "avg_logprob": -0.17933202826458475, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0014058653032407165}, {"id": 455, "seek": 316936, "start": 3189.6, "end": 3194.4, "text": " um no i think that's pretty good maybe we should uh move on to questions", "tokens": [51376, 1105, 572, 741, 519, 300, 311, 1238, 665, 1310, 321, 820, 2232, 1286, 322, 281, 1651, 51616], "temperature": 0.0, "avg_logprob": -0.17933202826458475, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0014058653032407165}, {"id": 456, "seek": 319440, "start": 3194.64, "end": 3205.84, "text": " and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't", "tokens": [50376, 293, 1338, 512, 665, 3476, 6076, 291, 393, 1590, 5414, 420, 291, 727, 1856, 309, 493, 457, 741, 2809, 380, 50936], "temperature": 0.0, "avg_logprob": -0.2984618953630036, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.011500103399157524}, {"id": 457, "seek": 319440, "start": 3205.84, "end": 3209.28, "text": " move it awesome okay", "tokens": [50936, 1286, 309, 3476, 1392, 51108], "temperature": 0.0, "avg_logprob": -0.2984618953630036, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.011500103399157524}, {"id": 458, "seek": 319440, "start": 3213.52, "end": 3220.64, "text": " yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat", "tokens": [51320, 1338, 257, 688, 295, 1880, 3755, 456, 370, 1309, 291, 741, 603, 1401, 512, 1651, 490, 264, 1621, 5081, 51676], "temperature": 0.0, "avg_logprob": -0.2984618953630036, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.011500103399157524}, {"id": 459, "seek": 322064, "start": 3220.72, "end": 3227.7599999999998, "text": " but first i just wanted to read a short quote from the 2022 active inference textbook they wrote", "tokens": [50368, 457, 700, 741, 445, 1415, 281, 1401, 257, 2099, 6513, 490, 264, 20229, 4967, 38253, 25591, 436, 4114, 50720], "temperature": 0.0, "avg_logprob": -0.07028585285335393, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.00609511649236083}, {"id": 460, "seek": 322064, "start": 3229.2799999999997, "end": 3237.2, "text": " um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote", "tokens": [50796, 1105, 322, 3028, 10858, 512, 7878, 2057, 264, 29805, 1441, 302, 35888, 292, 300, 15605, 7708, 23184, 51192], "temperature": 0.0, "avg_logprob": -0.07028585285335393, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.00609511649236083}, {"id": 461, "seek": 322064, "start": 3237.2, "end": 3242.8799999999997, "text": " too much effort to modeling isolated subsystems e.g perception language understanding whose", "tokens": [51192, 886, 709, 4630, 281, 15983, 14621, 2090, 9321, 82, 308, 13, 70, 12860, 2856, 3701, 6104, 51476], "temperature": 0.0, "avg_logprob": -0.07028585285335393, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.00609511649236083}, {"id": 462, "seek": 322064, "start": 3242.8799999999997, "end": 3250.0, "text": " boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive", "tokens": [51476, 13180, 366, 2049, 23211, 415, 10945, 281, 853, 15983, 264, 1379, 8508, 20070, 257, 3566, 15605, 51832], "temperature": 0.0, "avg_logprob": -0.07028585285335393, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.00609511649236083}, {"id": 463, "seek": 325000, "start": 3250.0, "end": 3254.88, "text": " creature perhaps a simple one and an environmental niche for it to cope with", "tokens": [50364, 12797, 4317, 257, 2199, 472, 293, 364, 8303, 19956, 337, 309, 281, 22598, 365, 50608], "temperature": 0.0, "avg_logprob": -0.08158278170927072, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0016738669946789742}, {"id": 464, "seek": 325000, "start": 3256.08, "end": 3263.44, "text": " so it's interesting about the approach that you're taking this is kind of a simple synthetic", "tokens": [50668, 370, 309, 311, 1880, 466, 264, 3109, 300, 291, 434, 1940, 341, 307, 733, 295, 257, 2199, 23420, 51036], "temperature": 0.0, "avg_logprob": -0.08158278170927072, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0016738669946789742}, {"id": 465, "seek": 325000, "start": 3263.44, "end": 3271.44, "text": " iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena", "tokens": [51036, 8508, 20070, 457, 300, 311, 5775, 281, 281, 264, 264, 5062, 281, 6155, 295, 257, 688, 295, 613, 31886, 22004, 51436], "temperature": 0.0, "avg_logprob": -0.08158278170927072, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0016738669946789742}, {"id": 466, "seek": 325000, "start": 3272.16, "end": 3277.84, "text": " because there is something and and so i saw in the presentation paper kind of this call for like", "tokens": [51472, 570, 456, 307, 746, 293, 293, 370, 741, 1866, 294, 264, 5860, 3035, 733, 295, 341, 818, 337, 411, 51756], "temperature": 0.0, "avg_logprob": -0.08158278170927072, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0016738669946789742}, {"id": 467, "seek": 327784, "start": 3278.4, "end": 3285.6800000000003, "text": " deliberate investigation rather than just chopping up the iguana a priori with a framework that", "tokens": [50392, 30515, 9627, 2831, 813, 445, 35205, 493, 264, 8508, 20070, 257, 4059, 72, 365, 257, 8388, 300, 50756], "temperature": 0.0, "avg_logprob": -0.1121772187096732, "compression_ratio": 1.58125, "no_speech_prob": 0.000984908896498382}, {"id": 468, "seek": 327784, "start": 3285.6800000000003, "end": 3294.0, "text": " that applies to humans or that centers humans or or that just uh soothes the epistemic challenge", "tokens": [50756, 300, 13165, 281, 6255, 420, 300, 10898, 6255, 420, 420, 300, 445, 2232, 370, 4624, 264, 2388, 468, 3438, 3430, 51172], "temperature": 0.0, "avg_logprob": -0.1121772187096732, "compression_ratio": 1.58125, "no_speech_prob": 0.000984908896498382}, {"id": 469, "seek": 327784, "start": 3294.0, "end": 3305.36, "text": " that's presented okay okay first question from dave he wrote", "tokens": [51172, 300, 311, 8212, 1392, 1392, 700, 1168, 490, 274, 946, 415, 4114, 51740], "temperature": 0.0, "avg_logprob": -0.1121772187096732, "compression_ratio": 1.58125, "no_speech_prob": 0.000984908896498382}, {"id": 470, "seek": 330784, "start": 3308.48, "end": 3314.4, "text": " have you looked at daniel denitz's distinction between competence without awareness and", "tokens": [50396, 362, 291, 2956, 412, 3277, 1187, 1441, 6862, 311, 16844, 1296, 39965, 1553, 8888, 293, 50692], "temperature": 0.0, "avg_logprob": -0.08780565857887268, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005909326486289501}, {"id": 471, "seek": 330784, "start": 3314.4, "end": 3321.1200000000003, "text": " competence with awareness he expands on this in the 2023 from bacteria to Bach and back", "tokens": [50692, 39965, 365, 8888, 415, 33706, 322, 341, 294, 264, 44377, 490, 11763, 281, 30920, 293, 646, 51028], "temperature": 0.0, "avg_logprob": -0.08780565857887268, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005909326486289501}, {"id": 472, "seek": 330784, "start": 3321.92, "end": 3327.92, "text": " i find this much more valuable than chomsky's highly problematic performance without competence", "tokens": [51068, 741, 915, 341, 709, 544, 8263, 813, 417, 4785, 4133, 311, 5405, 19011, 3389, 1553, 39965, 51368], "temperature": 0.0, "avg_logprob": -0.08780565857887268, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005909326486289501}, {"id": 473, "seek": 330784, "start": 3327.92, "end": 3334.96, "text": " a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this", "tokens": [51368, 257, 2590, 300, 417, 4785, 4133, 1366, 1208, 457, 1177, 380, 574, 412, 8760, 689, 360, 291, 829, 8888, 294, 439, 295, 341, 51720], "temperature": 0.0, "avg_logprob": -0.08780565857887268, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005909326486289501}, {"id": 474, "seek": 333496, "start": 3334.96, "end": 3346.16, "text": " competency uh well maybe i'll let you think uh that one is i can trust because you're", "tokens": [50364, 50097, 2232, 731, 1310, 741, 603, 718, 291, 519, 2232, 300, 472, 307, 741, 393, 3361, 570, 291, 434, 50924], "temperature": 0.0, "avg_logprob": -0.2822852348213765, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.002699913689866662}, {"id": 475, "seek": 333496, "start": 3346.16, "end": 3353.76, "text": " you're maybe more within it than i am but i'll just say um awareness is a very polysemous", "tokens": [50924, 291, 434, 1310, 544, 1951, 309, 813, 741, 669, 457, 741, 603, 445, 584, 1105, 8888, 307, 257, 588, 6754, 19872, 563, 51304], "temperature": 0.0, "avg_logprob": -0.2822852348213765, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.002699913689866662}, {"id": 476, "seek": 333496, "start": 3353.76, "end": 3360.08, "text": " term like many terms in philosophy of minds but partially this one more than many others i think", "tokens": [51304, 1433, 411, 867, 2115, 294, 10675, 295, 9634, 457, 18886, 341, 472, 544, 813, 867, 2357, 741, 519, 51620], "temperature": 0.0, "avg_logprob": -0.2822852348213765, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.002699913689866662}, {"id": 477, "seek": 336008, "start": 3360.7999999999997, "end": 3368.4, "text": " so um it can mean a lot of different things in all of context here we don't focus on things", "tokens": [50400, 370, 1105, 309, 393, 914, 257, 688, 295, 819, 721, 294, 439, 295, 4319, 510, 321, 500, 380, 1879, 322, 721, 50780], "temperature": 0.0, "avg_logprob": -0.13252517653674614, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.007674145512282848}, {"id": 478, "seek": 336008, "start": 3368.4, "end": 3374.7999999999997, "text": " like consciousness because i think we probably both agree that it's a less tractable uh maybe", "tokens": [50780, 411, 10081, 570, 741, 519, 321, 1391, 1293, 3986, 300, 309, 311, 257, 1570, 24207, 712, 2232, 1310, 51100], "temperature": 0.0, "avg_logprob": -0.13252517653674614, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.007674145512282848}, {"id": 479, "seek": 336008, "start": 3374.7999999999997, "end": 3383.12, "text": " empirical problem to try to assess the presence or absence of consciousness in language models", "tokens": [51100, 31886, 1154, 281, 853, 281, 5877, 264, 6814, 420, 17145, 295, 10081, 294, 2856, 5245, 51516], "temperature": 0.0, "avg_logprob": -0.13252517653674614, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.007674145512282848}, {"id": 480, "seek": 336008, "start": 3383.12, "end": 3387.84, "text": " even though many people are interested in that we think that we have more hope of making progress", "tokens": [51516, 754, 1673, 867, 561, 366, 3102, 294, 300, 321, 519, 300, 321, 362, 544, 1454, 295, 1455, 4205, 51752], "temperature": 0.0, "avg_logprob": -0.13252517653674614, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.007674145512282848}, {"id": 481, "seek": 338784, "start": 3387.84, "end": 3395.36, "text": " in the near term with more well-defined cognitive capacities or cognitive functions and things that", "tokens": [50364, 294, 264, 2651, 1433, 365, 544, 731, 12, 37716, 15605, 39396, 420, 15605, 6828, 293, 721, 300, 50740], "temperature": 0.0, "avg_logprob": -0.17646392583847045, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0025039042811840773}, {"id": 482, "seek": 338784, "start": 3395.36, "end": 3404.48, "text": " relate to forms of certain forms of reasoning and viable binding etc um so we our framework and", "tokens": [50740, 10961, 281, 6422, 295, 1629, 6422, 295, 21577, 293, 22024, 17359, 5183, 1105, 370, 321, 527, 8388, 293, 51196], "temperature": 0.0, "avg_logprob": -0.17646392583847045, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0025039042811840773}, {"id": 483, "seek": 338784, "start": 3404.48, "end": 3409.76, "text": " principle would apply to things like consciousness or as you put awareness generally speaking but", "tokens": [51196, 8665, 576, 3079, 281, 721, 411, 10081, 420, 382, 291, 829, 8888, 5101, 4124, 457, 51460], "temperature": 0.0, "avg_logprob": -0.17646392583847045, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0025039042811840773}, {"id": 484, "seek": 338784, "start": 3410.7200000000003, "end": 3415.04, "text": " we don't really focus on that for examples the other quick thing i'll just mention is that i", "tokens": [51508, 321, 500, 380, 534, 1879, 322, 300, 337, 5110, 264, 661, 1702, 551, 741, 603, 445, 2152, 307, 300, 741, 51724], "temperature": 0.0, "avg_logprob": -0.17646392583847045, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0025039042811840773}, {"id": 485, "seek": 341504, "start": 3415.12, "end": 3421.6, "text": " seem to remember that the phrase from then but again i'm not a then scholar was competence", "tokens": [50368, 1643, 281, 1604, 300, 264, 9535, 490, 550, 457, 797, 741, 478, 406, 257, 550, 17912, 390, 39965, 50692], "temperature": 0.0, "avg_logprob": -0.10523811774917796, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0007541902596130967}, {"id": 486, "seek": 341504, "start": 3421.6, "end": 3428.64, "text": " without comprehension um which seems a little different from competence without awareness", "tokens": [50692, 1553, 44991, 1105, 597, 2544, 257, 707, 819, 490, 39965, 1553, 8888, 51044], "temperature": 0.0, "avg_logprob": -0.10523811774917796, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0007541902596130967}, {"id": 487, "seek": 341504, "start": 3428.64, "end": 3435.36, "text": " perhaps depending on how you think of comprehension um and yeah i think that does i think it is a", "tokens": [51044, 4317, 5413, 322, 577, 291, 519, 295, 44991, 1105, 293, 1338, 741, 519, 300, 775, 741, 519, 309, 307, 257, 51380], "temperature": 0.0, "avg_logprob": -0.10523811774917796, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0007541902596130967}, {"id": 488, "seek": 341504, "start": 3435.36, "end": 3442.16, "text": " very interesting phrase that it does um in fact i had this project that's unpublished with", "tokens": [51380, 588, 1880, 9535, 300, 309, 775, 1105, 294, 1186, 741, 632, 341, 1716, 300, 311, 20994, 836, 4173, 365, 51720], "temperature": 0.0, "avg_logprob": -0.10523811774917796, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0007541902596130967}, {"id": 489, "seek": 344216, "start": 3442.16, "end": 3449.6, "text": " chris dolega who i think you had on the podcast as well um on semantic competence in language models", "tokens": [50364, 417, 5714, 360, 306, 3680, 567, 741, 519, 291, 632, 322, 264, 7367, 382, 731, 1105, 322, 47982, 39965, 294, 2856, 5245, 50736], "temperature": 0.0, "avg_logprob": -0.266716577920569, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0019535552710294724}, {"id": 490, "seek": 344216, "start": 3449.6, "end": 3457.04, "text": " where we use that phrase um to kind of avoid taking its stance on the kind of messy", "tokens": [50736, 689, 321, 764, 300, 9535, 1105, 281, 733, 295, 5042, 1940, 1080, 21033, 322, 264, 733, 295, 16191, 51108], "temperature": 0.0, "avg_logprob": -0.266716577920569, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0019535552710294724}, {"id": 491, "seek": 344216, "start": 3458.16, "end": 3463.68, "text": " muddy question of whether hella let's understand language which builds in all sorts of assumptions", "tokens": [51164, 38540, 1168, 295, 1968, 415, 3505, 718, 311, 1223, 2856, 597, 15182, 294, 439, 7527, 295, 17695, 51440], "temperature": 0.0, "avg_logprob": -0.266716577920569, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0019535552710294724}, {"id": 492, "seek": 344216, "start": 3463.68, "end": 3468.3999999999996, "text": " including about consciousness actually for some people like chancel um and we focus on", "tokens": [51440, 3009, 466, 10081, 767, 337, 512, 561, 411, 417, 4463, 338, 1105, 293, 321, 1879, 322, 51676], "temperature": 0.0, "avg_logprob": -0.266716577920569, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0019535552710294724}, {"id": 493, "seek": 346840, "start": 3468.4, "end": 3473.92, "text": " the more restricted notion of competence and i think our paper here also has that property that", "tokens": [50364, 264, 544, 20608, 10710, 295, 39965, 293, 741, 519, 527, 3035, 510, 611, 575, 300, 4707, 300, 50640], "temperature": 0.0, "avg_logprob": -0.28396920153969213, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.01062865648418665}, {"id": 494, "seek": 346840, "start": 3473.92, "end": 3478.88, "text": " would if we have originalized competence we end up operationalizing competence in terms of", "tokens": [50640, 576, 498, 321, 362, 3380, 1602, 39965, 321, 917, 493, 16607, 3319, 39965, 294, 2115, 295, 50888], "temperature": 0.0, "avg_logprob": -0.28396920153969213, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.01062865648418665}, {"id": 495, "seek": 346840, "start": 3479.44, "end": 3484.64, "text": " the sets of knowledge of the mechanism and mechanisms that enable a system to generalize", "tokens": [50916, 264, 6352, 295, 3601, 295, 264, 7513, 293, 15902, 300, 9528, 257, 1185, 281, 2674, 1125, 51176], "temperature": 0.0, "avg_logprob": -0.28396920153969213, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.01062865648418665}, {"id": 496, "seek": 346840, "start": 3484.64, "end": 3489.92, "text": " well in a given domain basically and in a way that's a supposed evolutionary compared to", "tokens": [51176, 731, 294, 257, 2212, 9274, 1936, 293, 294, 257, 636, 300, 311, 257, 3442, 27567, 5347, 281, 51440], "temperature": 0.0, "avg_logprob": -0.28396920153969213, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.01062865648418665}, {"id": 497, "seek": 348992, "start": 3490.88, "end": 3494.48, "text": " some more expensive understandings of competence that", "tokens": [50412, 512, 544, 5124, 1223, 1109, 295, 39965, 300, 50592], "temperature": 0.0, "avg_logprob": -0.23949848766058263, "compression_ratio": 1.9085714285714286, "no_speech_prob": 0.0025088523980230093}, {"id": 498, "seek": 348992, "start": 3495.84, "end": 3502.4, "text": " we need to comprehension or understanding more well but i'll let you take that one charles", "tokens": [50660, 321, 643, 281, 44991, 420, 3701, 544, 731, 457, 741, 603, 718, 291, 747, 300, 472, 1290, 904, 50988], "temperature": 0.0, "avg_logprob": -0.23949848766058263, "compression_ratio": 1.9085714285714286, "no_speech_prob": 0.0025088523980230093}, {"id": 499, "seek": 348992, "start": 3504.88, "end": 3511.2000000000003, "text": " no yeah that was that was good um the phrase you know the distinction that denadra's is between", "tokens": [51112, 572, 1338, 300, 390, 300, 390, 665, 1105, 264, 9535, 291, 458, 264, 16844, 300, 1441, 345, 424, 311, 307, 1296, 51428], "temperature": 0.0, "avg_logprob": -0.23949848766058263, "compression_ratio": 1.9085714285714286, "no_speech_prob": 0.0025088523980230093}, {"id": 500, "seek": 348992, "start": 3512.16, "end": 3519.2000000000003, "text": " competence with comprehension and without and i think um competence with comprehension is the", "tokens": [51476, 39965, 365, 44991, 293, 1553, 293, 741, 519, 1105, 39965, 365, 44991, 307, 264, 51828], "temperature": 0.0, "avg_logprob": -0.23949848766058263, "compression_ratio": 1.9085714285714286, "no_speech_prob": 0.0025088523980230093}, {"id": 501, "seek": 351920, "start": 3519.2, "end": 3525.12, "text": " ability not just to pursue a strategy that's successful for solving a problem but to um", "tokens": [50364, 3485, 406, 445, 281, 12392, 257, 5206, 300, 311, 4406, 337, 12606, 257, 1154, 457, 281, 1105, 50660], "temperature": 0.0, "avg_logprob": -0.08669342313494001, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.0005881552933715284}, {"id": 502, "seek": 351920, "start": 3525.12, "end": 3532.64, "text": " articulate the strategy in such a way that you could teach it for example and um humans only", "tokens": [50660, 30305, 264, 5206, 294, 1270, 257, 636, 300, 291, 727, 2924, 309, 337, 1365, 293, 1105, 6255, 787, 51036], "temperature": 0.0, "avg_logprob": -0.08669342313494001, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.0005881552933715284}, {"id": 503, "seek": 351920, "start": 3532.64, "end": 3540.96, "text": " sometimes have competence with comprehension we have many competences that lack comprehension", "tokens": [51036, 2171, 362, 39965, 365, 44991, 321, 362, 867, 2850, 2667, 300, 5011, 44991, 51452], "temperature": 0.0, "avg_logprob": -0.08669342313494001, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.0005881552933715284}, {"id": 504, "seek": 354096, "start": 3540.96, "end": 3548.7200000000003, "text": " right um you know when we learn to walk for example um we have an amazing competence that we", "tokens": [50364, 558, 1105, 291, 458, 562, 321, 1466, 281, 1792, 337, 1365, 1105, 321, 362, 364, 2243, 39965, 300, 321, 50752], "temperature": 0.0, "avg_logprob": -0.10341985875909979, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.012618784792721272}, {"id": 505, "seek": 354096, "start": 3549.52, "end": 3555.44, "text": " still can't quite translate into robotics because we don't fully understand how it works", "tokens": [50792, 920, 393, 380, 1596, 13799, 666, 34145, 570, 321, 500, 380, 4498, 1223, 577, 309, 1985, 51088], "temperature": 0.0, "avg_logprob": -0.10341985875909979, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.012618784792721272}, {"id": 506, "seek": 354096, "start": 3557.6, "end": 3564.8, "text": " and when it comes to our language models i think we should", "tokens": [51196, 293, 562, 309, 1487, 281, 527, 2856, 5245, 741, 519, 321, 820, 51556], "temperature": 0.0, "avg_logprob": -0.10341985875909979, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.012618784792721272}, {"id": 507, "seek": 356480, "start": 3565.44, "end": 3579.44, "text": " not expect uh comprehension i mean they have a an amazing suite of competences if you thought that", "tokens": [50396, 406, 2066, 2232, 44991, 741, 914, 436, 362, 257, 364, 2243, 14205, 295, 2850, 2667, 498, 291, 1194, 300, 51096], "temperature": 0.0, "avg_logprob": -0.14439880664532, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.00453730346634984}, {"id": 508, "seek": 356480, "start": 3579.44, "end": 3584.1600000000003, "text": " they also had comprehension then i suppose you would think like well if you want to understand", "tokens": [51096, 436, 611, 632, 44991, 550, 741, 7297, 291, 576, 519, 411, 731, 498, 291, 528, 281, 1223, 51332], "temperature": 0.0, "avg_logprob": -0.14439880664532, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.00453730346634984}, {"id": 509, "seek": 356480, "start": 3584.1600000000003, "end": 3590.7200000000003, "text": " how a large language model works you can just ask it but that's that's a bad strategy nobody nobody", "tokens": [51332, 577, 257, 2416, 2856, 2316, 1985, 291, 393, 445, 1029, 309, 457, 300, 311, 300, 311, 257, 1578, 5206, 5079, 5079, 51660], "temperature": 0.0, "avg_logprob": -0.14439880664532, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.00453730346634984}, {"id": 510, "seek": 359072, "start": 3591.2799999999997, "end": 3597.8399999999997, "text": " about um how a large language model works so so they're on the um competence without", "tokens": [50392, 466, 1105, 577, 257, 2416, 2856, 2316, 1985, 370, 370, 436, 434, 322, 264, 1105, 39965, 1553, 50720], "temperature": 0.0, "avg_logprob": -0.1453162118008262, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.00258949538692832}, {"id": 511, "seek": 359072, "start": 3597.8399999999997, "end": 3608.9599999999996, "text": " comprehension side of things um and in order to figure out what in order to figure out what", "tokens": [50720, 44991, 1252, 295, 721, 1105, 293, 294, 1668, 281, 2573, 484, 437, 294, 1668, 281, 2573, 484, 437, 51276], "temperature": 0.0, "avg_logprob": -0.1453162118008262, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.00258949538692832}, {"id": 512, "seek": 359072, "start": 3608.9599999999996, "end": 3613.3599999999997, "text": " the mechanisms are that enable its competencies we have to pursue strategies that are broadly", "tokens": [51276, 264, 15902, 366, 300, 9528, 1080, 2850, 6464, 321, 362, 281, 12392, 9029, 300, 366, 19511, 51496], "temperature": 0.0, "avg_logprob": -0.1453162118008262, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.00258949538692832}, {"id": 513, "seek": 359072, "start": 3613.3599999999997, "end": 3618.72, "text": " similar to the strategies we use in you know cognitive psychology or cognitive linguistics", "tokens": [51496, 2531, 281, 264, 9029, 321, 764, 294, 291, 458, 15605, 15105, 420, 15605, 21766, 6006, 51764], "temperature": 0.0, "avg_logprob": -0.1453162118008262, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.00258949538692832}, {"id": 514, "seek": 361872, "start": 3619.4399999999996, "end": 3623.4399999999996, "text": " um and you know we have to run experiments so i think that that's all very compatible with", "tokens": [50400, 1105, 293, 291, 458, 321, 362, 281, 1190, 12050, 370, 741, 519, 300, 300, 311, 439, 588, 18218, 365, 50600], "temperature": 0.0, "avg_logprob": -0.1047303941514757, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.002933860756456852}, {"id": 515, "seek": 361872, "start": 3623.4399999999996, "end": 3631.9199999999996, "text": " Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was", "tokens": [50600, 3394, 311, 636, 295, 1237, 412, 721, 1105, 472, 661, 551, 741, 603, 2152, 1105, 3394, 311, 370, 3394, 390, 51024], "temperature": 0.0, "avg_logprob": -0.1047303941514757, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.002933860756456852}, {"id": 516, "seek": 361872, "start": 3635.2, "end": 3642.56, "text": " quite influential to me and we actually wrote a commentary together which pushes back a little", "tokens": [51188, 1596, 22215, 281, 385, 293, 321, 767, 4114, 257, 23527, 1214, 597, 21020, 646, 257, 707, 51556], "temperature": 0.0, "avg_logprob": -0.1047303941514757, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.002933860756456852}, {"id": 517, "seek": 364256, "start": 3642.56, "end": 3649.04, "text": " bit on a simple understanding of this distinction so we were looking at um the evolution of", "tokens": [50364, 857, 322, 257, 2199, 3701, 295, 341, 16844, 370, 321, 645, 1237, 412, 1105, 264, 9303, 295, 50688], "temperature": 0.0, "avg_logprob": -0.05977795970055365, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.051808685064315796}, {"id": 518, "seek": 364256, "start": 3649.04, "end": 3658.32, "text": " metacognition and basically what we argue is that um given the gradualism of evolution there", "tokens": [50688, 1131, 326, 2912, 849, 293, 1936, 437, 321, 9695, 307, 300, 1105, 2212, 264, 32890, 1434, 295, 9303, 456, 51152], "temperature": 0.0, "avg_logprob": -0.05977795970055365, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.051808685064315796}, {"id": 519, "seek": 364256, "start": 3658.32, "end": 3663.2, "text": " must have been something in between base level cognition and metacognition so we shouldn't", "tokens": [51152, 1633, 362, 668, 746, 294, 1296, 3096, 1496, 46905, 293, 1131, 326, 2912, 849, 370, 321, 4659, 380, 51396], "temperature": 0.0, "avg_logprob": -0.05977795970055365, "compression_ratio": 1.6768292682926829, "no_speech_prob": 0.051808685064315796}, {"id": 520, "seek": 366320, "start": 3663.2, "end": 3672.7999999999997, "text": " see that distinction as black and white and um you know i think that if you want to contrast the", "tokens": [50364, 536, 300, 16844, 382, 2211, 293, 2418, 293, 1105, 291, 458, 741, 519, 300, 498, 291, 528, 281, 8712, 264, 50844], "temperature": 0.0, "avg_logprob": -0.0794865696929222, "compression_ratio": 1.472, "no_speech_prob": 0.021935217082500458}, {"id": 521, "seek": 366320, "start": 3672.7999999999997, "end": 3688.3999999999996, "text": " sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific", "tokens": [50844, 1333, 295, 2232, 15605, 45553, 442, 295, 1105, 257, 1952, 5075, 365, 3195, 295, 43002, 293, 8134, 51624], "temperature": 0.0, "avg_logprob": -0.0794865696929222, "compression_ratio": 1.472, "no_speech_prob": 0.021935217082500458}, {"id": 522, "seek": 368840, "start": 3688.4, "end": 3697.36, "text": " uh concepts at her disposal with you know a non-human animal then this strong distinction", "tokens": [50364, 2232, 10392, 412, 720, 26400, 365, 291, 458, 257, 2107, 12, 18796, 5496, 550, 341, 2068, 16844, 50812], "temperature": 0.0, "avg_logprob": -0.10303818434476852, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0080545824021101}, {"id": 523, "seek": 368840, "start": 3697.36, "end": 3704.96, "text": " between competence with and without comprehension is reasonable um but in the space of all possible", "tokens": [50812, 1296, 39965, 365, 293, 1553, 44991, 307, 10585, 1105, 457, 294, 264, 1901, 295, 439, 1944, 51192], "temperature": 0.0, "avg_logprob": -0.10303818434476852, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0080545824021101}, {"id": 524, "seek": 368840, "start": 3704.96, "end": 3713.28, "text": " minds we should be open to the view that there can be you know semi-competent um forms of cognition", "tokens": [51192, 9634, 321, 820, 312, 1269, 281, 264, 1910, 300, 456, 393, 312, 291, 458, 12909, 12, 1112, 7275, 317, 1105, 6422, 295, 46905, 51608], "temperature": 0.0, "avg_logprob": -0.10303818434476852, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0080545824021101}, {"id": 525, "seek": 371328, "start": 3713.84, "end": 3720.2400000000002, "text": " and just to put it on this uh it occurred to me while I was listening to you as well that um", "tokens": [50392, 293, 445, 281, 829, 309, 322, 341, 2232, 309, 11068, 281, 385, 1339, 286, 390, 4764, 281, 291, 382, 731, 300, 1105, 50712], "temperature": 0.0, "avg_logprob": -0.2746885767522848, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.01200193352997303}, {"id": 526, "seek": 371328, "start": 3720.2400000000002, "end": 3724.4, "text": " the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and", "tokens": [50712, 264, 700, 1365, 295, 43741, 3366, 296, 321, 2729, 43741, 3366, 15107, 294, 4682, 25893, 293, 50920], "temperature": 0.0, "avg_logprob": -0.2746885767522848, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.01200193352997303}, {"id": 527, "seek": 371328, "start": 3724.4, "end": 3731.1200000000003, "text": " Frank example is a nice is a nice example where in order to give the metacognistic judgments", "tokens": [50920, 6823, 1365, 307, 257, 1481, 307, 257, 1481, 1365, 689, 294, 1668, 281, 976, 264, 1131, 326, 2912, 3142, 40337, 51256], "temperature": 0.0, "avg_logprob": -0.2746885767522848, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.01200193352997303}, {"id": 528, "seek": 371328, "start": 3731.1200000000003, "end": 3735.1200000000003, "text": " correctly so see that that you would need competence with comprehension because you need to", "tokens": [51256, 8944, 370, 536, 300, 300, 291, 576, 643, 39965, 365, 44991, 570, 291, 643, 281, 51456], "temperature": 0.0, "avg_logprob": -0.2746885767522848, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.01200193352997303}, {"id": 529, "seek": 371328, "start": 3735.92, "end": 3741.76, "text": " understand not only be able to to come to to agree the verb with the subject but know the rule and", "tokens": [51496, 1223, 406, 787, 312, 1075, 281, 281, 808, 281, 281, 3986, 264, 9595, 365, 264, 3983, 457, 458, 264, 4978, 293, 51788], "temperature": 0.0, "avg_logprob": -0.2746885767522848, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.01200193352997303}, {"id": 530, "seek": 374176, "start": 3741.76, "end": 3750.8, "text": " know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so", "tokens": [50364, 458, 577, 281, 47881, 309, 4317, 957, 1125, 309, 2232, 337, 1365, 281, 2924, 1580, 558, 293, 370, 2232, 293, 370, 50816], "temperature": 0.0, "avg_logprob": -0.23840798829731188, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.004054269753396511}, {"id": 531, "seek": 374176, "start": 3750.8, "end": 3755.28, "text": " when you find that the L.M. can do well at the low task the member of the task and at the high", "tokens": [50816, 562, 291, 915, 300, 264, 441, 13, 44, 13, 393, 360, 731, 412, 264, 2295, 5633, 264, 4006, 295, 264, 5633, 293, 412, 264, 1090, 51040], "temperature": 0.0, "avg_logprob": -0.23840798829731188, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.004054269753396511}, {"id": 532, "seek": 374176, "start": 3755.28, "end": 3761.28, "text": " task development explicit metacognistic judgments in some way that's an example of the L.M. having", "tokens": [51040, 5633, 3250, 13691, 1131, 326, 2912, 3142, 40337, 294, 512, 636, 300, 311, 364, 1365, 295, 264, 441, 13, 44, 13, 1419, 51340], "temperature": 0.0, "avg_logprob": -0.23840798829731188, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.004054269753396511}, {"id": 533, "seek": 376128, "start": 3761.28, "end": 3773.6000000000004, "text": " competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given", "tokens": [50364, 39965, 365, 44991, 1338, 1338, 1481, 3476, 1392, 493, 14796, 6482, 4114, 1168, 2212, 50980], "temperature": 0.0, "avg_logprob": -0.18941083387895064, "compression_ratio": 1.5083798882681565, "no_speech_prob": 0.0390179343521595}, {"id": 534, "seek": 376128, "start": 3773.6000000000004, "end": 3781.84, "text": " that LLMs inherently reflect anthropocentric biases due to their training on human data and goals", "tokens": [50980, 300, 441, 43, 26386, 27993, 5031, 22727, 905, 32939, 32152, 3462, 281, 641, 3097, 322, 1952, 1412, 293, 5493, 51392], "temperature": 0.0, "avg_logprob": -0.18941083387895064, "compression_ratio": 1.5083798882681565, "no_speech_prob": 0.0390179343521595}, {"id": 535, "seek": 376128, "start": 3781.84, "end": 3787.0400000000004, "text": " how can we ensure that their inter model discourse aligns with humanity's values", "tokens": [51392, 577, 393, 321, 5586, 300, 641, 728, 2316, 23938, 7975, 82, 365, 10243, 311, 4190, 51652], "temperature": 0.0, "avg_logprob": -0.18941083387895064, "compression_ratio": 1.5083798882681565, "no_speech_prob": 0.0390179343521595}, {"id": 536, "seek": 379128, "start": 3791.76, "end": 3792.4, "text": " um", "tokens": [50388, 1105, 50420], "temperature": 0.0, "avg_logprob": -0.27173981299767125, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.007452724035829306}, {"id": 537, "seek": 379128, "start": 3795.28, "end": 3801.44, "text": " so the inter white discourse right be inter model discourse perhaps amongst the models", "tokens": [50564, 370, 264, 728, 2418, 23938, 558, 312, 728, 2316, 23938, 4317, 12918, 264, 5245, 50872], "temperature": 0.0, "avg_logprob": -0.27173981299767125, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.007452724035829306}, {"id": 538, "seek": 379128, "start": 3802.5600000000004, "end": 3811.52, "text": " I see I see like in that farmville paper yeah the small the yeah both generative agents get", "tokens": [50928, 286, 536, 286, 536, 411, 294, 300, 5421, 8386, 3035, 1338, 264, 1359, 264, 1338, 1293, 1337, 1166, 12554, 483, 51376], "temperature": 0.0, "avg_logprob": -0.27173981299767125, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.007452724035829306}, {"id": 539, "seek": 379128, "start": 3811.52, "end": 3819.52, "text": " the small ones uh yeah I think that's beyond the scope of this paper to be honest but um", "tokens": [51376, 264, 1359, 2306, 2232, 1338, 286, 519, 300, 311, 4399, 264, 11923, 295, 341, 3035, 281, 312, 3245, 457, 1105, 51776], "temperature": 0.0, "avg_logprob": -0.27173981299767125, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.007452724035829306}, {"id": 540, "seek": 381952, "start": 3819.7599999999998, "end": 3826.56, "text": " I mean we could use about it yeah but I don't know that we have I don't know this project", "tokens": [50376, 286, 914, 321, 727, 764, 466, 309, 1338, 457, 286, 500, 380, 458, 300, 321, 362, 286, 500, 380, 458, 341, 1716, 50716], "temperature": 0.0, "avg_logprob": -0.20440523624420165, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.006473857443779707}, {"id": 541, "seek": 381952, "start": 3826.56, "end": 3832.4, "text": " has much I mean I I think we both have an interest in the alignment problem uh independently of this", "tokens": [50716, 575, 709, 286, 914, 286, 286, 519, 321, 1293, 362, 364, 1179, 294, 264, 18515, 1154, 2232, 21761, 295, 341, 51008], "temperature": 0.0, "avg_logprob": -0.20440523624420165, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.006473857443779707}, {"id": 542, "seek": 381952, "start": 3832.4, "end": 3838.32, "text": " project but I don't think this project has much to say really about this I'm not sure what you think", "tokens": [51008, 1716, 457, 286, 500, 380, 519, 341, 1716, 575, 709, 281, 584, 534, 466, 341, 286, 478, 406, 988, 437, 291, 519, 51304], "temperature": 0.0, "avg_logprob": -0.20440523624420165, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.006473857443779707}, {"id": 543, "seek": 381952, "start": 3839.28, "end": 3840.88, "text": " uh yeah yeah I don't", "tokens": [51352, 2232, 1338, 1338, 286, 500, 380, 51432], "temperature": 0.0, "avg_logprob": -0.20440523624420165, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.006473857443779707}, {"id": 544, "seek": 384088, "start": 3841.36, "end": 3852.2400000000002, "text": " yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise", "tokens": [50388, 1338, 286, 500, 380, 362, 1340, 1687, 9859, 490, 300, 1392, 11017, 8962, 364, 1365, 295, 46567, 5658, 50932], "temperature": 0.0, "avg_logprob": -0.17152517318725585, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.008572610095143318}, {"id": 545, "seek": 384088, "start": 3852.2400000000002, "end": 3859.84, "text": " into LLM training that was the section about the extra tokens do you see any analog to", "tokens": [50932, 666, 441, 43, 44, 3097, 300, 390, 264, 3541, 466, 264, 2857, 22667, 360, 291, 536, 604, 16660, 281, 51312], "temperature": 0.0, "avg_logprob": -0.17152517318725585, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.008572610095143318}, {"id": 546, "seek": 384088, "start": 3859.84, "end": 3863.6, "text": " intermittent reinforcement to uncertainty tolerance", "tokens": [51312, 44084, 29280, 281, 15697, 23368, 51500], "temperature": 0.0, "avg_logprob": -0.17152517318725585, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.008572610095143318}, {"id": 547, "seek": 386360, "start": 3864.48, "end": 3872.0, "text": " because you mentioned the extra tokens in the chain of thought and how that could also be replaced", "tokens": [50408, 570, 291, 2835, 264, 2857, 22667, 294, 264, 5021, 295, 1194, 293, 577, 300, 727, 611, 312, 10772, 50784], "temperature": 0.0, "avg_logprob": -0.11021645863850911, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00394446961581707}, {"id": 548, "seek": 386360, "start": 3872.0, "end": 3881.2799999999997, "text": " by by dot dot dot dot dot and so like what is that telling us about model training when um", "tokens": [50784, 538, 538, 5893, 5893, 5893, 5893, 5893, 293, 370, 411, 437, 307, 300, 3585, 505, 466, 2316, 3097, 562, 1105, 51248], "temperature": 0.0, "avg_logprob": -0.11021645863850911, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00394446961581707}, {"id": 549, "seek": 386360, "start": 3882.0, "end": 3886.96, "text": " it seems like there's some situations where adding superfluous tokens would diminish signal", "tokens": [51284, 309, 2544, 411, 456, 311, 512, 6851, 689, 5127, 1687, 49253, 563, 22667, 576, 48696, 6358, 51532], "temperature": 0.0, "avg_logprob": -0.11021645863850911, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00394446961581707}, {"id": 550, "seek": 386360, "start": 3886.96, "end": 3891.2799999999997, "text": " in data sets but then here are other situations where it seems to actually help", "tokens": [51532, 294, 1412, 6352, 457, 550, 510, 366, 661, 6851, 689, 309, 2544, 281, 767, 854, 51748], "temperature": 0.0, "avg_logprob": -0.11021645863850911, "compression_ratio": 1.7696078431372548, "no_speech_prob": 0.00394446961581707}, {"id": 551, "seek": 389360, "start": 3894.48, "end": 3903.44, "text": " um um yeah so in that particular paper I think it's called thinking dot by dot and there is a", "tokens": [50408, 1105, 1105, 1338, 370, 294, 300, 1729, 3035, 286, 519, 309, 311, 1219, 1953, 5893, 538, 5893, 293, 456, 307, 257, 50856], "temperature": 0.0, "avg_logprob": -0.34223337042821594, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0024698174092918634}, {"id": 552, "seek": 389360, "start": 3903.44, "end": 3913.04, "text": " subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly", "tokens": [50856, 30706, 306, 1105, 309, 311, 538, 3099, 6124, 37480, 293, 14117, 1738, 289, 1892, 286, 519, 1105, 294, 300, 3035, 1105, 498, 286, 9901, 8944, 51336], "temperature": 0.0, "avg_logprob": -0.34223337042821594, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0024698174092918634}, {"id": 553, "seek": 389360, "start": 3913.04, "end": 3922.48, "text": " what they did is that they they introduced just this one field of token swan meaning this token", "tokens": [51336, 437, 436, 630, 307, 300, 436, 436, 7268, 445, 341, 472, 2519, 295, 14862, 1693, 282, 3620, 341, 14862, 51808], "temperature": 0.0, "avg_logprob": -0.34223337042821594, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0024698174092918634}, {"id": 554, "seek": 392248, "start": 3922.56, "end": 3929.04, "text": " just to hold but just a dot and they trained the model to give an answer after producing", "tokens": [50368, 445, 281, 1797, 457, 445, 257, 5893, 293, 436, 8895, 264, 2316, 281, 976, 364, 1867, 934, 10501, 50692], "temperature": 0.0, "avg_logprob": -0.1353339127131871, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004063835833221674}, {"id": 555, "seek": 392248, "start": 3929.04, "end": 3935.28, "text": " a certain number of dots that's not just like introducing Rambam and Gibberish in your training", "tokens": [50692, 257, 1629, 1230, 295, 15026, 300, 311, 406, 445, 411, 15424, 497, 2173, 335, 293, 17256, 43189, 294, 428, 3097, 51004], "temperature": 0.0, "avg_logprob": -0.1353339127131871, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004063835833221674}, {"id": 556, "seek": 392248, "start": 3935.28, "end": 3942.48, "text": " data it's actually quite a specific intervention that forces the model to um learn to perform", "tokens": [51004, 1412, 309, 311, 767, 1596, 257, 2685, 13176, 300, 5874, 264, 2316, 281, 1105, 1466, 281, 2042, 51364], "temperature": 0.0, "avg_logprob": -0.1353339127131871, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004063835833221674}, {"id": 557, "seek": 392248, "start": 3942.48, "end": 3950.8, "text": " certain computations before giving an answer um so so it makes sense to me that this couldn't", "tokens": [51364, 1629, 2807, 763, 949, 2902, 364, 1867, 1105, 370, 370, 309, 1669, 2020, 281, 385, 300, 341, 2809, 380, 51780], "temperature": 0.0, "avg_logprob": -0.1353339127131871, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004063835833221674}, {"id": 558, "seek": 395080, "start": 3950.8, "end": 3955.2000000000003, "text": " diminish performance and like you you could do that from that it's not quite the same as just", "tokens": [50364, 48696, 3389, 293, 411, 291, 291, 727, 360, 300, 490, 300, 309, 311, 406, 1596, 264, 912, 382, 445, 50584], "temperature": 0.0, "avg_logprob": -0.1777275360382355, "compression_ratio": 1.912, "no_speech_prob": 0.0036458722315728664}, {"id": 559, "seek": 395080, "start": 3955.2000000000003, "end": 3960.96, "text": " having that training data right um just because the token seems meaningless it's a field of token", "tokens": [50584, 1419, 300, 3097, 1412, 558, 1105, 445, 570, 264, 14862, 2544, 33232, 309, 311, 257, 2519, 295, 14862, 50872], "temperature": 0.0, "avg_logprob": -0.1777275360382355, "compression_ratio": 1.912, "no_speech_prob": 0.0036458722315728664}, {"id": 560, "seek": 395080, "start": 3960.96, "end": 3967.76, "text": " to dot um it's not just random gibberish it's going to throw off the model and and impede its", "tokens": [50872, 281, 5893, 1105, 309, 311, 406, 445, 4974, 4553, 43189, 309, 311, 516, 281, 3507, 766, 264, 2316, 293, 293, 704, 4858, 1080, 51212], "temperature": 0.0, "avg_logprob": -0.1777275360382355, "compression_ratio": 1.912, "no_speech_prob": 0.0036458722315728664}, {"id": 561, "seek": 395080, "start": 3967.76, "end": 3974.48, "text": " its uh the optimization of its learning function or at least good downstream performance um but what", "tokens": [51212, 1080, 2232, 264, 19618, 295, 1080, 2539, 2445, 420, 412, 1935, 665, 30621, 3389, 1105, 457, 437, 51548], "temperature": 0.0, "avg_logprob": -0.1777275360382355, "compression_ratio": 1.912, "no_speech_prob": 0.0036458722315728664}, {"id": 562, "seek": 395080, "start": 3974.48, "end": 3978.32, "text": " it's going to do is going to force the model to learn that when there is a dot token it can", "tokens": [51548, 309, 311, 516, 281, 360, 307, 516, 281, 3464, 264, 2316, 281, 1466, 300, 562, 456, 307, 257, 5893, 14862, 309, 393, 51740], "temperature": 0.0, "avg_logprob": -0.1777275360382355, "compression_ratio": 1.912, "no_speech_prob": 0.0036458722315728664}, {"id": 563, "seek": 397832, "start": 3978.32, "end": 3982.56, "text": " allocate computation with its attention heads and other parts of the architecture in such a way", "tokens": [50364, 35713, 24903, 365, 1080, 3202, 8050, 293, 661, 3166, 295, 264, 9482, 294, 1270, 257, 636, 50576], "temperature": 0.0, "avg_logprob": -0.14895565081865358, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004461674019694328}, {"id": 564, "seek": 397832, "start": 3982.56, "end": 3990.6400000000003, "text": " that it's um getting towards deriving the correct token when it's finally producing", "tokens": [50576, 300, 309, 311, 1105, 1242, 3030, 1163, 2123, 264, 3006, 14862, 562, 309, 311, 2721, 10501, 50980], "temperature": 0.0, "avg_logprob": -0.14895565081865358, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004461674019694328}, {"id": 565, "seek": 397832, "start": 3990.6400000000003, "end": 3996.2400000000002, "text": " the token that matters and that's meaningful after the series of dots um yeah I don't know", "tokens": [50980, 264, 14862, 300, 7001, 293, 300, 311, 10995, 934, 264, 2638, 295, 15026, 1105, 1338, 286, 500, 380, 458, 51260], "temperature": 0.0, "avg_logprob": -0.14895565081865358, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004461674019694328}, {"id": 566, "seek": 397832, "start": 3996.2400000000002, "end": 4001.6000000000004, "text": " Charles if you have another answer yeah I mean I think it's an important question because", "tokens": [51260, 10523, 498, 291, 362, 1071, 1867, 1338, 286, 914, 286, 519, 309, 311, 364, 1021, 1168, 570, 51528], "temperature": 0.0, "avg_logprob": -0.14895565081865358, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004461674019694328}, {"id": 567, "seek": 400160, "start": 4001.6, "end": 4009.8399999999997, "text": " a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless", "tokens": [50364, 257, 4059, 72, 2232, 498, 1580, 848, 574, 321, 434, 516, 281, 493, 521, 293, 2666, 521, 257, 1379, 3840, 295, 33232, 50776], "temperature": 0.0, "avg_logprob": -0.105687796179928, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.010482278652489185}, {"id": 568, "seek": 400160, "start": 4009.8399999999997, "end": 4017.8399999999997, "text": " symbols to an LLM input you might very well think that this will just weaken the signal to", "tokens": [50776, 16944, 281, 364, 441, 43, 44, 4846, 291, 1062, 588, 731, 519, 300, 341, 486, 445, 48576, 264, 6358, 281, 51176], "temperature": 0.0, "avg_logprob": -0.105687796179928, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.010482278652489185}, {"id": 569, "seek": 400160, "start": 4017.8399999999997, "end": 4028.7999999999997, "text": " noise ratio and degrade model performance so it's against that background that the empirical result", "tokens": [51176, 5658, 8509, 293, 368, 8692, 2316, 3389, 370, 309, 311, 1970, 300, 3678, 300, 264, 31886, 1874, 51724], "temperature": 0.0, "avg_logprob": -0.105687796179928, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.010482278652489185}, {"id": 570, "seek": 402880, "start": 4029.04, "end": 4036.0, "text": " doesn't degrade model performance um ought to be regarded as an important clue about how the model", "tokens": [50376, 1177, 380, 368, 8692, 2316, 3389, 1105, 13416, 281, 312, 26047, 382, 364, 1021, 13602, 466, 577, 264, 2316, 50724], "temperature": 0.0, "avg_logprob": -0.12314246613302349, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.011680134572088718}, {"id": 571, "seek": 402880, "start": 4036.0, "end": 4042.32, "text": " works so I think that the the intuition behind this question is indeed part of the interpretation", "tokens": [50724, 1985, 370, 286, 519, 300, 264, 264, 24002, 2261, 341, 1168, 307, 6451, 644, 295, 264, 14174, 51040], "temperature": 0.0, "avg_logprob": -0.12314246613302349, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.011680134572088718}, {"id": 572, "seek": 402880, "start": 4042.32, "end": 4049.2000000000003, "text": " of the empirical results right it's surprising for exactly this reason and then the theory that's", "tokens": [51040, 295, 264, 31886, 3542, 558, 309, 311, 8830, 337, 2293, 341, 1778, 293, 550, 264, 5261, 300, 311, 51384], "temperature": 0.0, "avg_logprob": -0.12314246613302349, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.011680134572088718}, {"id": 573, "seek": 402880, "start": 4049.2000000000003, "end": 4054.1600000000003, "text": " supposed to you know well this is an active inference podcast right so the theory that's", "tokens": [51384, 3442, 281, 291, 458, 731, 341, 307, 364, 4967, 38253, 7367, 558, 370, 264, 5261, 300, 311, 51632], "temperature": 0.0, "avg_logprob": -0.12314246613302349, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.011680134572088718}, {"id": 574, "seek": 405416, "start": 4054.16, "end": 4060.48, "text": " supposed to help rid some of the surprise here is um the idea that", "tokens": [50364, 3442, 281, 854, 3973, 512, 295, 264, 6365, 510, 307, 1105, 264, 1558, 300, 50680], "temperature": 0.0, "avg_logprob": -0.09260762589318412, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.0011692909756675363}, {"id": 575, "seek": 405416, "start": 4064.16, "end": 4070.72, "text": " given the uh architecture of a transformer where it's it has to go through all the tokens", "tokens": [50864, 2212, 264, 2232, 9482, 295, 257, 31782, 689, 309, 311, 309, 575, 281, 352, 807, 439, 264, 22667, 51192], "temperature": 0.0, "avg_logprob": -0.09260762589318412, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.0011692909756675363}, {"id": 576, "seek": 405416, "start": 4071.3599999999997, "end": 4079.44, "text": " in every cycle um having these extra tokens gives it uh sort of more computational bandwidth", "tokens": [51224, 294, 633, 6586, 1105, 1419, 613, 2857, 22667, 2709, 309, 2232, 1333, 295, 544, 28270, 23647, 51628], "temperature": 0.0, "avg_logprob": -0.09260762589318412, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.0011692909756675363}, {"id": 577, "seek": 407944, "start": 4079.44, "end": 4086.4, "text": " and therefore more expressivity or more capacity to uh you know locate uh the right", "tokens": [50364, 293, 4412, 544, 5109, 4253, 420, 544, 6042, 281, 2232, 291, 458, 22370, 2232, 264, 558, 50712], "temperature": 0.0, "avg_logprob": -0.09898079766167535, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.0015010061906650662}, {"id": 578, "seek": 407944, "start": 4086.4, "end": 4094.64, "text": " spot and parameter space and and even that in a way reminds me of so it's not just that dots", "tokens": [50712, 4008, 293, 13075, 1901, 293, 293, 754, 300, 294, 257, 636, 12025, 385, 295, 370, 309, 311, 406, 445, 300, 15026, 51124], "temperature": 0.0, "avg_logprob": -0.09898079766167535, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.0015010061906650662}, {"id": 579, "seek": 407944, "start": 4094.64, "end": 4098.8, "text": " improve performance it's not it's that it was like you mentioned it was trained to have that", "tokens": [51124, 3470, 3389, 309, 311, 406, 309, 311, 300, 309, 390, 411, 291, 2835, 309, 390, 8895, 281, 362, 300, 51332], "temperature": 0.0, "avg_logprob": -0.09898079766167535, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.0015010061906650662}, {"id": 580, "seek": 407944, "start": 4098.8, "end": 4106.32, "text": " and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes", "tokens": [51332, 293, 14138, 309, 727, 362, 668, 8895, 1310, 24371, 22652, 281, 445, 5598, 22825, 19963, 51708], "temperature": 0.0, "avg_logprob": -0.09898079766167535, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.0015010061906650662}, {"id": 581, "seek": 410632, "start": 4106.32, "end": 4112.88, "text": " verbatim while you're processing so that's kind of like a filler or more of like a sort of", "tokens": [50364, 9595, 267, 332, 1339, 291, 434, 9007, 370, 300, 311, 733, 295, 411, 257, 34676, 420, 544, 295, 411, 257, 1333, 295, 50692], "temperature": 0.0, "avg_logprob": -0.07113739470360984, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.010650972835719585}, {"id": 582, "seek": 410632, "start": 4113.679999999999, "end": 4122.16, "text": " that was a great question it's like these are linguistic paddings that that do create time", "tokens": [50732, 300, 390, 257, 869, 1168, 309, 311, 411, 613, 366, 43002, 6887, 29432, 300, 300, 360, 1884, 565, 51156], "temperature": 0.0, "avg_logprob": -0.07113739470360984, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.010650972835719585}, {"id": 583, "seek": 410632, "start": 4122.88, "end": 4131.44, "text": " to to get to the meet so not only does it signal and signpost if it's being trained to have that", "tokens": [51192, 281, 281, 483, 281, 264, 1677, 370, 406, 787, 775, 309, 6358, 293, 1465, 23744, 498, 309, 311, 885, 8895, 281, 362, 300, 51620], "temperature": 0.0, "avg_logprob": -0.07113739470360984, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.010650972835719585}, {"id": 584, "seek": 413144, "start": 4131.44, "end": 4136.639999999999, "text": " meaning which then questions like so that it wasn't a meaningless dot if it if it had a um", "tokens": [50364, 3620, 597, 550, 1651, 411, 370, 300, 309, 2067, 380, 257, 33232, 5893, 498, 309, 498, 309, 632, 257, 1105, 50624], "temperature": 0.0, "avg_logprob": -0.1071805646342616, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.03788367286324501}, {"id": 585, "seek": 413144, "start": 4136.639999999999, "end": 4146.0, "text": " a cognitive or even like a a semantic um aspect I had a question how do you feel like in this", "tokens": [50624, 257, 15605, 420, 754, 411, 257, 257, 47982, 1105, 4171, 286, 632, 257, 1168, 577, 360, 291, 841, 411, 294, 341, 51092], "temperature": 0.0, "avg_logprob": -0.1071805646342616, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.03788367286324501}, {"id": 586, "seek": 413144, "start": 4148.08, "end": 4155.5199999999995, "text": " era Cambrian explosion of diverse intelligences how can we understand capacities", "tokens": [51196, 4249, 29287, 5501, 15673, 295, 9521, 5613, 2667, 577, 393, 321, 1223, 39396, 51568], "temperature": 0.0, "avg_logprob": -0.1071805646342616, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.03788367286324501}, {"id": 587, "seek": 415552, "start": 4156.320000000001, "end": 4166.88, "text": " when they seem so conditional upon the setting and how the system of interest is interacted with", "tokens": [50404, 562, 436, 1643, 370, 27708, 3564, 264, 3287, 293, 577, 264, 1185, 295, 1179, 307, 49621, 365, 50932], "temperature": 0.0, "avg_logprob": -0.08452808632040923, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.03783903270959854}, {"id": 588, "seek": 415552, "start": 4167.6, "end": 4174.160000000001, "text": " like what are the practical implications for people who are studying LLMs and other", "tokens": [50968, 411, 437, 366, 264, 8496, 16602, 337, 561, 567, 366, 7601, 441, 43, 26386, 293, 661, 51296], "temperature": 0.0, "avg_logprob": -0.08452808632040923, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.03783903270959854}, {"id": 589, "seek": 415552, "start": 4174.160000000001, "end": 4179.92, "text": " synthetic intelligences from like a safety or reliability or performance perspective", "tokens": [51296, 23420, 5613, 2667, 490, 411, 257, 4514, 420, 24550, 420, 3389, 4585, 51584], "temperature": 0.0, "avg_logprob": -0.08452808632040923, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.03783903270959854}, {"id": 590, "seek": 417992, "start": 4179.92, "end": 4185.68, "text": " that was we're gonna", "tokens": [50364, 300, 390, 321, 434, 799, 50652], "temperature": 0.0, "avg_logprob": -0.39730603644188417, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.002924145432189107}, {"id": 591, "seek": 417992, "start": 4187.28, "end": 4188.32, "text": " no I was drawing it to you", "tokens": [50732, 572, 286, 390, 6316, 309, 281, 291, 50784], "temperature": 0.0, "avg_logprob": -0.39730603644188417, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.002924145432189107}, {"id": 592, "seek": 417992, "start": 4190.4, "end": 4194.56, "text": " so so so just want to chat to some of the questions so the question is", "tokens": [50888, 370, 370, 370, 445, 528, 281, 5081, 281, 512, 295, 264, 1651, 370, 264, 1168, 307, 51096], "temperature": 0.0, "avg_logprob": -0.39730603644188417, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.002924145432189107}, {"id": 593, "seek": 417992, "start": 4198.56, "end": 4202.08, "text": " how the how is the notion of a capacity", "tokens": [51296, 577, 264, 577, 307, 264, 10710, 295, 257, 6042, 51472], "temperature": 0.0, "avg_logprob": -0.39730603644188417, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.002924145432189107}, {"id": 594, "seek": 420208, "start": 4203.04, "end": 4210.48, "text": " um changing when we have such different systems that seem to have intelligent behavior", "tokens": [50412, 1105, 4473, 562, 321, 362, 1270, 819, 3652, 300, 1643, 281, 362, 13232, 5223, 50784], "temperature": 0.0, "avg_logprob": -0.132764865954717, "compression_ratio": 1.5649350649350648, "no_speech_prob": 0.018527965992689133}, {"id": 595, "seek": 420208, "start": 4210.48, "end": 4216.64, "text": " yeah and it's so dependent upon potentially initially unintuitive", "tokens": [50784, 1338, 293, 309, 311, 370, 12334, 3564, 7263, 9105, 29466, 48314, 51092], "temperature": 0.0, "avg_logprob": -0.132764865954717, "compression_ratio": 1.5649350649350648, "no_speech_prob": 0.018527965992689133}, {"id": 596, "seek": 420208, "start": 4217.84, "end": 4224.8, "text": " ways of interacting so how can we understand the reliability and the performance and the", "tokens": [51152, 2098, 295, 18017, 370, 577, 393, 321, 1223, 264, 24550, 293, 264, 3389, 293, 264, 51500], "temperature": 0.0, "avg_logprob": -0.132764865954717, "compression_ratio": 1.5649350649350648, "no_speech_prob": 0.018527965992689133}, {"id": 597, "seek": 422480, "start": 4224.8, "end": 4232.64, "text": " capacity of of a model other than for example by exhaustively inputting prompts", "tokens": [50364, 6042, 295, 295, 257, 2316, 661, 813, 337, 1365, 538, 14687, 3413, 4846, 783, 41095, 50756], "temperature": 0.0, "avg_logprob": -0.10741524696350098, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.016394762322306633}, {"id": 598, "seek": 422480, "start": 4232.64, "end": 4239.92, "text": " which can't really happen what what could we really say or no and or just how do you feel", "tokens": [50756, 597, 393, 380, 534, 1051, 437, 437, 727, 321, 534, 584, 420, 572, 293, 420, 445, 577, 360, 291, 841, 51120], "temperature": 0.0, "avg_logprob": -0.10741524696350098, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.016394762322306633}, {"id": 599, "seek": 422480, "start": 4239.92, "end": 4245.12, "text": " that this work re-enters into the ways that people practically are using the models", "tokens": [51120, 300, 341, 589, 319, 12, 317, 433, 666, 264, 2098, 300, 561, 15667, 366, 1228, 264, 5245, 51380], "temperature": 0.0, "avg_logprob": -0.10741524696350098, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.016394762322306633}, {"id": 600, "seek": 422480, "start": 4246.400000000001, "end": 4251.2, "text": " right okay yeah uh it's the interesting question so the first one the question is I think", "tokens": [51444, 558, 1392, 1338, 2232, 309, 311, 264, 1880, 1168, 370, 264, 700, 472, 264, 1168, 307, 286, 519, 51684], "temperature": 0.0, "avg_logprob": -0.10741524696350098, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.016394762322306633}, {"id": 601, "seek": 425120, "start": 4252.0, "end": 4257.76, "text": " you know part of the background assumption from for this paper that I've explicitly", "tokens": [50404, 291, 458, 644, 295, 264, 3678, 15302, 490, 337, 341, 3035, 300, 286, 600, 20803, 50692], "temperature": 0.0, "avg_logprob": -0.12024111143300231, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0013434309512376785}, {"id": 602, "seek": 425120, "start": 4257.76, "end": 4264.639999999999, "text": " defended in other work is that behavioral evidence is simply not sufficient in most cases", "tokens": [50692, 34135, 294, 661, 589, 307, 300, 19124, 4467, 307, 2935, 406, 11563, 294, 881, 3331, 51036], "temperature": 0.0, "avg_logprob": -0.12024111143300231, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0013434309512376785}, {"id": 603, "seek": 425120, "start": 4264.639999999999, "end": 4271.76, "text": " to arbitrate disputes about capacities of LLMs when it comes to human cognition", "tokens": [51036, 281, 14931, 4404, 39666, 466, 39396, 295, 441, 43, 26386, 562, 309, 1487, 281, 1952, 46905, 51392], "temperature": 0.0, "avg_logprob": -0.12024111143300231, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0013434309512376785}, {"id": 604, "seek": 425120, "start": 4273.76, "end": 4279.28, "text": " we do have to rely a lot on on psychological experiments that are ultimately behavioral", "tokens": [51492, 321, 360, 362, 281, 10687, 257, 688, 322, 322, 14346, 12050, 300, 366, 6284, 19124, 51768], "temperature": 0.0, "avg_logprob": -0.12024111143300231, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0013434309512376785}, {"id": 605, "seek": 427928, "start": 4279.36, "end": 4285.5199999999995, "text": " and we do also rely on self-reports in a little more than we can when it comes to LLMs because we", "tokens": [50368, 293, 321, 360, 611, 10687, 322, 2698, 12, 265, 17845, 294, 257, 707, 544, 813, 321, 393, 562, 309, 1487, 281, 441, 43, 26386, 570, 321, 50676], "temperature": 0.0, "avg_logprob": -0.1502863277088512, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.001364276628009975}, {"id": 606, "seek": 427928, "start": 4287.28, "end": 4293.2, "text": " despite the move away from relying on intuition and introspection in the history of psychology", "tokens": [50764, 7228, 264, 1286, 1314, 490, 24140, 322, 24002, 293, 560, 2635, 19997, 294, 264, 2503, 295, 15105, 51060], "temperature": 0.0, "avg_logprob": -0.1502863277088512, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.001364276628009975}, {"id": 607, "seek": 427928, "start": 4293.2, "end": 4299.44, "text": " it still has a role to play but we've by and large contributed to us by behavioral experiments", "tokens": [51060, 309, 920, 575, 257, 3090, 281, 862, 457, 321, 600, 538, 293, 2416, 18434, 281, 505, 538, 19124, 12050, 51372], "temperature": 0.0, "avg_logprob": -0.1502863277088512, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.001364276628009975}, {"id": 608, "seek": 427928, "start": 4299.44, "end": 4305.759999999999, "text": " that get increasingly sophisticated to try to reverse engineer what's going on inside the black", "tokens": [51372, 300, 483, 12980, 16950, 281, 853, 281, 9943, 11403, 437, 311, 516, 322, 1854, 264, 2211, 51688], "temperature": 0.0, "avg_logprob": -0.1502863277088512, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.001364276628009975}, {"id": 609, "seek": 430576, "start": 4305.76, "end": 4315.6, "text": " box when it comes to LLMs partly because that's so different from us um relying exclusively on", "tokens": [50364, 2424, 562, 309, 1487, 281, 441, 43, 26386, 17031, 570, 300, 311, 370, 819, 490, 505, 1105, 24140, 20638, 322, 50856], "temperature": 0.0, "avg_logprob": -0.11661349286089887, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.003588910447433591}, {"id": 610, "seek": 430576, "start": 4315.6, "end": 4322.16, "text": " behaviorism is even more difficult because we have even less of an idea of what might be going on", "tokens": [50856, 5223, 1434, 307, 754, 544, 2252, 570, 321, 362, 754, 1570, 295, 364, 1558, 295, 437, 1062, 312, 516, 322, 51184], "temperature": 0.0, "avg_logprob": -0.11661349286089887, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.003588910447433591}, {"id": 611, "seek": 430576, "start": 4322.16, "end": 4326.24, "text": " inside the black box and whether it's anything like what's going on inside all black box and we", "tokens": [51184, 1854, 264, 2211, 2424, 293, 1968, 309, 311, 1340, 411, 437, 311, 516, 322, 1854, 439, 2211, 2424, 293, 321, 51388], "temperature": 0.0, "avg_logprob": -0.11661349286089887, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.003588910447433591}, {"id": 612, "seek": 430576, "start": 4326.24, "end": 4332.88, "text": " have reasons to think it might be very different so I think I think we both agree that we have to", "tokens": [51388, 362, 4112, 281, 519, 309, 1062, 312, 588, 819, 370, 286, 519, 286, 519, 321, 1293, 3986, 300, 321, 362, 281, 51720], "temperature": 0.0, "avg_logprob": -0.11661349286089887, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.003588910447433591}, {"id": 613, "seek": 433288, "start": 4332.88, "end": 4341.76, "text": " supplement this with mechanistic work that's essentially involve performing causal interventions", "tokens": [50364, 15436, 341, 365, 4236, 3142, 589, 300, 311, 4476, 9494, 10205, 38755, 20924, 50808], "temperature": 0.0, "avg_logprob": -0.07893370219639369, "compression_ratio": 1.9214659685863875, "no_speech_prob": 0.004748751875013113}, {"id": 614, "seek": 433288, "start": 4341.76, "end": 4347.52, "text": " on the inner mechanisms on the inner workings of the systems so decoding representation and", "tokens": [50808, 322, 264, 7284, 15902, 322, 264, 7284, 589, 1109, 295, 264, 3652, 370, 979, 8616, 10290, 293, 51096], "temperature": 0.0, "avg_logprob": -0.07893370219639369, "compression_ratio": 1.9214659685863875, "no_speech_prob": 0.004748751875013113}, {"id": 615, "seek": 433288, "start": 4347.52, "end": 4352.16, "text": " computations that the systems that are in principle available to the systems and then", "tokens": [51096, 2807, 763, 300, 264, 3652, 300, 366, 294, 8665, 2435, 281, 264, 3652, 293, 550, 51328], "temperature": 0.0, "avg_logprob": -0.07893370219639369, "compression_ratio": 1.9214659685863875, "no_speech_prob": 0.004748751875013113}, {"id": 616, "seek": 433288, "start": 4352.16, "end": 4356.56, "text": " intervening on them to confirm hypotheses about the causal role of these representations and", "tokens": [51328, 17104, 278, 322, 552, 281, 9064, 49969, 466, 264, 38755, 3090, 295, 613, 33358, 293, 51548], "temperature": 0.0, "avg_logprob": -0.07893370219639369, "compression_ratio": 1.9214659685863875, "no_speech_prob": 0.004748751875013113}, {"id": 617, "seek": 435656, "start": 4356.56, "end": 4363.200000000001, "text": " computations and we have methods to do that and partly what we can be a little optimistic about", "tokens": [50364, 2807, 763, 293, 321, 362, 7150, 281, 360, 300, 293, 17031, 437, 321, 393, 312, 257, 707, 19397, 466, 50696], "temperature": 0.0, "avg_logprob": -0.09855678604870308, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005636163055896759}, {"id": 618, "seek": 435656, "start": 4363.200000000001, "end": 4367.280000000001, "text": " this project even though it's it's it's extremely challenging especially to be scaled up to large", "tokens": [50696, 341, 1716, 754, 1673, 309, 311, 309, 311, 309, 311, 4664, 7595, 2318, 281, 312, 36039, 493, 281, 2416, 50900], "temperature": 0.0, "avg_logprob": -0.09855678604870308, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005636163055896759}, {"id": 619, "seek": 435656, "start": 4367.280000000001, "end": 4373.120000000001, "text": " models is because unlike what's happening in your sense with the brain where the range of decoding", "tokens": [50900, 5245, 307, 570, 8343, 437, 311, 2737, 294, 428, 2020, 365, 264, 3567, 689, 264, 3613, 295, 979, 8616, 51192], "temperature": 0.0, "avg_logprob": -0.09855678604870308, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005636163055896759}, {"id": 620, "seek": 435656, "start": 4373.120000000001, "end": 4379.280000000001, "text": " methods and intervention methods we have is extremely limited but for ethical and for simply", "tokens": [51192, 7150, 293, 13176, 7150, 321, 362, 307, 4664, 5567, 457, 337, 18890, 293, 337, 2935, 51500], "temperature": 0.0, "avg_logprob": -0.09855678604870308, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.005636163055896759}, {"id": 621, "seek": 437928, "start": 4379.5199999999995, "end": 4386.719999999999, "text": " um practical reasons that we don't just don't have ground truth access to activations in neurons", "tokens": [50376, 1105, 8496, 4112, 300, 321, 500, 380, 445, 500, 380, 362, 2727, 3494, 2105, 281, 2430, 763, 294, 22027, 50736], "temperature": 0.0, "avg_logprob": -0.11828911304473877, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.002508022589609027}, {"id": 622, "seek": 437928, "start": 4386.719999999999, "end": 4393.44, "text": " at least that easily and we also are generally unable to make specific interventions on", "tokens": [50736, 412, 1935, 300, 3612, 293, 321, 611, 366, 5101, 11299, 281, 652, 2685, 20924, 322, 51072], "temperature": 0.0, "avg_logprob": -0.11828911304473877, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.002508022589609027}, {"id": 623, "seek": 437928, "start": 4393.44, "end": 4397.84, "text": " activation in the brain uh when it comes to algorithms we have full ground truth knowledge", "tokens": [51072, 24433, 294, 264, 3567, 2232, 562, 309, 1487, 281, 14642, 321, 362, 1577, 2727, 3494, 3601, 51292], "temperature": 0.0, "avg_logprob": -0.11828911304473877, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.002508022589609027}, {"id": 624, "seek": 437928, "start": 4397.84, "end": 4404.4, "text": " of all activations of every single part of the network and we also have full access to all of", "tokens": [51292, 295, 439, 2430, 763, 295, 633, 2167, 644, 295, 264, 3209, 293, 321, 611, 362, 1577, 2105, 281, 439, 295, 51620], "temperature": 0.0, "avg_logprob": -0.11828911304473877, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.002508022589609027}, {"id": 625, "seek": 440440, "start": 4404.4, "end": 4410.24, "text": " it for interventions at inference time so that that opens up a whole new range of things we can do", "tokens": [50364, 309, 337, 20924, 412, 38253, 565, 370, 300, 300, 9870, 493, 257, 1379, 777, 3613, 295, 721, 321, 393, 360, 50656], "temperature": 0.0, "avg_logprob": -0.11698444073016827, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.00830502063035965}, {"id": 626, "seek": 440440, "start": 4410.96, "end": 4417.759999999999, "text": " and that enables us to go beyond behavioral studies and actually decode these features and", "tokens": [50692, 293, 300, 17077, 505, 281, 352, 4399, 19124, 5313, 293, 767, 979, 1429, 613, 4122, 293, 51032], "temperature": 0.0, "avg_logprob": -0.11698444073016827, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.00830502063035965}, {"id": 627, "seek": 440440, "start": 4417.759999999999, "end": 4424.08, "text": " circuits or as researchers put it in the literature or as philosophers would generally put it", "tokens": [51032, 26354, 420, 382, 10309, 829, 309, 294, 264, 10394, 420, 382, 36839, 576, 5101, 829, 309, 51348], "temperature": 0.0, "avg_logprob": -0.11698444073016827, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.00830502063035965}, {"id": 628, "seek": 440440, "start": 4425.28, "end": 4429.36, "text": " representations and computations that the system is actually making use of and try to reverse", "tokens": [51408, 33358, 293, 2807, 763, 300, 264, 1185, 307, 767, 1455, 764, 295, 293, 853, 281, 9943, 51612], "temperature": 0.0, "avg_logprob": -0.11698444073016827, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.00830502063035965}, {"id": 629, "seek": 442936, "start": 4429.44, "end": 4433.92, "text": " engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem", "tokens": [50368, 11403, 437, 733, 295, 437, 733, 295, 14642, 309, 311, 309, 311, 1455, 764, 295, 370, 644, 295, 264, 13227, 1154, 50592], "temperature": 0.0, "avg_logprob": -0.2110908744261437, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.009691949002444744}, {"id": 630, "seek": 442936, "start": 4435.04, "end": 4441.5199999999995, "text": " projects that we have with Charles is to um suppose that we start with these", "tokens": [50648, 4455, 300, 321, 362, 365, 10523, 307, 281, 1105, 7297, 300, 321, 722, 365, 613, 50972], "temperature": 0.0, "avg_logprob": -0.2110908744261437, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.009691949002444744}, {"id": 631, "seek": 442936, "start": 4441.5199999999995, "end": 4446.719999999999, "text": " investigative kinds as we put as as Alibol calls them these human subject capacities", "tokens": [50972, 45495, 3685, 382, 321, 829, 382, 382, 967, 897, 401, 5498, 552, 613, 1952, 3983, 39396, 51232], "temperature": 0.0, "avg_logprob": -0.2110908744261437, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.009691949002444744}, {"id": 632, "seek": 442936, "start": 4448.719999999999, "end": 4454.0, "text": " we can operationalize them and do behavioral experiments in the top down and then from the", "tokens": [51332, 321, 393, 16607, 1125, 552, 293, 360, 19124, 12050, 294, 264, 1192, 760, 293, 550, 490, 264, 51596], "temperature": 0.0, "avg_logprob": -0.2110908744261437, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.009691949002444744}, {"id": 633, "seek": 442936, "start": 4454.0, "end": 4459.04, "text": " bottom up we can also try to reverse engineer the mechanism building blocks of the computations", "tokens": [51596, 2767, 493, 321, 393, 611, 853, 281, 9943, 11403, 264, 7513, 2390, 8474, 295, 264, 2807, 763, 51848], "temperature": 0.0, "avg_logprob": -0.2110908744261437, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.009691949002444744}, {"id": 634, "seek": 445936, "start": 4459.759999999999, "end": 4463.04, "text": " and representations that evidence may use up to solve the task", "tokens": [50384, 293, 33358, 300, 4467, 815, 764, 493, 281, 5039, 264, 5633, 50548], "temperature": 0.0, "avg_logprob": -0.1399683299817537, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0019228016026318073}, {"id": 635, "seek": 445936, "start": 4463.599999999999, "end": 4468.16, "text": " related to that particular capacity and then we can meet somewhere in the middle and try to", "tokens": [50576, 4077, 281, 300, 1729, 6042, 293, 550, 321, 393, 1677, 4079, 294, 264, 2808, 293, 853, 281, 50804], "temperature": 0.0, "avg_logprob": -0.1399683299817537, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0019228016026318073}, {"id": 636, "seek": 445936, "start": 4469.28, "end": 4476.639999999999, "text": " from that line of work that are purchased things from above and bring to the fore some kinds of", "tokens": [50860, 490, 300, 1622, 295, 589, 300, 366, 14734, 721, 490, 3673, 293, 1565, 281, 264, 2091, 512, 3685, 295, 51228], "temperature": 0.0, "avg_logprob": -0.1399683299817537, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0019228016026318073}, {"id": 637, "seek": 445936, "start": 4476.639999999999, "end": 4482.08, "text": " mid-level abstractions as we call it or computational building blocks that might be key to", "tokens": [51228, 2062, 12, 12418, 12649, 626, 382, 321, 818, 309, 420, 28270, 2390, 8474, 300, 1062, 312, 2141, 281, 51500], "temperature": 0.0, "avg_logprob": -0.1399683299817537, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0019228016026318073}, {"id": 638, "seek": 445936, "start": 4483.44, "end": 4489.12, "text": " the performance of the system in that domain so for example if you're interested in the capacity", "tokens": [51568, 264, 3389, 295, 264, 1185, 294, 300, 9274, 370, 337, 1365, 498, 291, 434, 3102, 294, 264, 6042, 51852], "temperature": 0.0, "avg_logprob": -0.1399683299817537, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0019228016026318073}, {"id": 639, "seek": 448912, "start": 4489.12, "end": 4496.4, "text": " for reasoning you can start with this very broad human-centric notion of reasoning then try to", "tokens": [50364, 337, 21577, 291, 393, 722, 365, 341, 588, 4152, 1952, 12, 45300, 10710, 295, 21577, 550, 853, 281, 50728], "temperature": 0.0, "avg_logprob": -0.09651490358205941, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0026706603821367025}, {"id": 640, "seek": 448912, "start": 4496.4, "end": 4501.76, "text": " operationalize it in a reasoning task then do some behavioral testing and then mechanistic", "tokens": [50728, 16607, 1125, 309, 294, 257, 21577, 5633, 550, 360, 512, 19124, 4997, 293, 550, 4236, 3142, 50996], "temperature": 0.0, "avg_logprob": -0.09651490358205941, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0026706603821367025}, {"id": 641, "seek": 448912, "start": 4501.76, "end": 4506.32, "text": " interpretability of that reasoning task find out how the system is solving it find out how the", "tokens": [50996, 7302, 2310, 295, 300, 21577, 5633, 915, 484, 577, 264, 1185, 307, 12606, 309, 915, 484, 577, 264, 51224], "temperature": 0.0, "avg_logprob": -0.09651490358205941, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0026706603821367025}, {"id": 642, "seek": 448912, "start": 4506.32, "end": 4512.48, "text": " algorithm is doing well and why reverse engineer building blocks that might for example have to", "tokens": [51224, 9284, 307, 884, 731, 293, 983, 9943, 11403, 2390, 8474, 300, 1062, 337, 1365, 362, 281, 51532], "temperature": 0.0, "avg_logprob": -0.09651490358205941, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0026706603821367025}, {"id": 643, "seek": 451248, "start": 4512.48, "end": 4519.04, "text": " be viable manipulation viable binding and then from there you might be able to either actually", "tokens": [50364, 312, 22024, 26475, 22024, 17359, 293, 550, 490, 456, 291, 1062, 312, 1075, 281, 2139, 767, 50692], "temperature": 0.0, "avg_logprob": -0.22881016965772286, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.014487686567008495}, {"id": 644, "seek": 451248, "start": 4519.04, "end": 4525.04, "text": " refine the notion of reasoning you started with to have a more specific and that less human-centric", "tokens": [50692, 33906, 264, 10710, 295, 21577, 291, 1409, 365, 281, 362, 257, 544, 2685, 293, 300, 1570, 1952, 12, 45300, 50992], "temperature": 0.0, "avg_logprob": -0.22881016965772286, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.014487686567008495}, {"id": 645, "seek": 451248, "start": 4525.04, "end": 4535.839999999999, "text": " notion that is now operationalized in more low-level terms like you know that both manipulation", "tokens": [50992, 10710, 300, 307, 586, 16607, 1602, 294, 544, 2295, 12, 12418, 2115, 411, 291, 458, 300, 1293, 26475, 51532], "temperature": 0.0, "avg_logprob": -0.22881016965772286, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.014487686567008495}, {"id": 646, "seek": 453584, "start": 4535.84, "end": 4542.400000000001, "text": " of variables in certain ways and the binding of variables to theories etc so yeah so that's I", "tokens": [50364, 295, 9102, 294, 1629, 2098, 293, 264, 17359, 295, 9102, 281, 13667, 5183, 370, 1338, 370, 300, 311, 286, 50692], "temperature": 0.0, "avg_logprob": -0.19334738881964433, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.04874569922685623}, {"id": 647, "seek": 453584, "start": 4542.400000000001, "end": 4547.04, "text": " think the general approach we take now how does that does any of that feedback into", "tokens": [50692, 519, 264, 2674, 3109, 321, 747, 586, 577, 775, 300, 775, 604, 295, 300, 5824, 666, 50924], "temperature": 0.0, "avg_logprob": -0.19334738881964433, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.04874569922685623}, {"id": 648, "seek": 453584, "start": 4547.76, "end": 4551.6, "text": " interactions how we humans interact with algorithms I think that's one way in which", "tokens": [50960, 13280, 577, 321, 6255, 4648, 365, 14642, 286, 519, 300, 311, 472, 636, 294, 597, 51152], "temperature": 0.0, "avg_logprob": -0.19334738881964433, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.04874569922685623}, {"id": 649, "seek": 453584, "start": 4551.6, "end": 4559.76, "text": " you could feedback is simply in terms of challenging or our spontaneous anthropomorphic", "tokens": [51152, 291, 727, 5824, 307, 2935, 294, 2115, 295, 7595, 420, 527, 32744, 22727, 32702, 299, 51560], "temperature": 0.0, "avg_logprob": -0.19334738881964433, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.04874569922685623}, {"id": 650, "seek": 453584, "start": 4559.76, "end": 4564.96, "text": " attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you", "tokens": [51560, 25853, 281, 14642, 281, 512, 8396, 264, 912, 636, 281, 1401, 257, 688, 295, 5496, 46905, 4317, 291, 51820], "temperature": 0.0, "avg_logprob": -0.19334738881964433, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.04874569922685623}, {"id": 651, "seek": 456496, "start": 4564.96, "end": 4571.12, "text": " will interact with your cat in a slightly different way that you might maybe not rush the conclusion", "tokens": [50364, 486, 4648, 365, 428, 3857, 294, 257, 4748, 819, 636, 300, 291, 1062, 1310, 406, 9300, 264, 10063, 50672], "temperature": 0.0, "avg_logprob": -0.09544478393182522, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0053767794743180275}, {"id": 652, "seek": 456496, "start": 4571.12, "end": 4578.72, "text": " that when your cat performs a certain behavior it has understood what you think and it's modeling", "tokens": [50672, 300, 562, 428, 3857, 26213, 257, 1629, 5223, 309, 575, 7320, 437, 291, 519, 293, 309, 311, 15983, 51052], "temperature": 0.0, "avg_logprob": -0.09544478393182522, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0053767794743180275}, {"id": 653, "seek": 456496, "start": 4578.72, "end": 4585.68, "text": " what you're thinking about what it's thinking or something perhaps you might adopt a more", "tokens": [51052, 437, 291, 434, 1953, 466, 437, 309, 311, 1953, 420, 746, 4317, 291, 1062, 6878, 257, 544, 51400], "temperature": 0.0, "avg_logprob": -0.09544478393182522, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0053767794743180275}, {"id": 654, "seek": 456496, "start": 4585.68, "end": 4591.92, "text": " deflationary attitude to explain the behavior of your cat doesn't mean you have to love them", "tokens": [51400, 1060, 24278, 822, 10157, 281, 2903, 264, 5223, 295, 428, 3857, 1177, 380, 914, 291, 362, 281, 959, 552, 51712], "temperature": 0.0, "avg_logprob": -0.09544478393182522, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0053767794743180275}, {"id": 655, "seek": 459192, "start": 4591.92, "end": 4599.52, "text": " any less or it doesn't mean you have to you know if that is the other thing like if you want at the", "tokens": [50364, 604, 1570, 420, 309, 1177, 380, 914, 291, 362, 281, 291, 458, 498, 300, 307, 264, 661, 551, 411, 498, 291, 528, 412, 264, 50744], "temperature": 0.0, "avg_logprob": -0.18707400957743328, "compression_ratio": 1.896, "no_speech_prob": 0.003700048429891467}, {"id": 656, "seek": 459192, "start": 4599.52, "end": 4603.6, "text": " end of the day to speak to your cat like a human because you really are a gentleman for that then", "tokens": [50744, 917, 295, 264, 786, 281, 1710, 281, 428, 3857, 411, 257, 1952, 570, 291, 534, 366, 257, 15761, 337, 300, 550, 50948], "temperature": 0.0, "avg_logprob": -0.18707400957743328, "compression_ratio": 1.896, "no_speech_prob": 0.003700048429891467}, {"id": 657, "seek": 459192, "start": 4603.6, "end": 4608.0, "text": " that's you know all the more part of you in the same way if you find it useful to treat LLMs in", "tokens": [50948, 300, 311, 291, 458, 439, 264, 544, 644, 295, 291, 294, 264, 912, 636, 498, 291, 915, 309, 4420, 281, 2387, 441, 43, 26386, 294, 51168], "temperature": 0.0, "avg_logprob": -0.18707400957743328, "compression_ratio": 1.896, "no_speech_prob": 0.003700048429891467}, {"id": 658, "seek": 459192, "start": 4608.0, "end": 4615.28, "text": " the way you interact with them to to have fluid interactions with them to treat them as if they", "tokens": [51168, 264, 636, 291, 4648, 365, 552, 281, 281, 362, 9113, 13280, 365, 552, 281, 2387, 552, 382, 498, 436, 51532], "temperature": 0.0, "avg_logprob": -0.18707400957743328, "compression_ratio": 1.896, "no_speech_prob": 0.003700048429891467}, {"id": 659, "seek": 459192, "start": 4615.28, "end": 4620.56, "text": " had beliefs these are as etc of human-like capacities then that's fine if that's for", "tokens": [51532, 632, 13585, 613, 366, 382, 5183, 295, 1952, 12, 4092, 39396, 550, 300, 311, 2489, 498, 300, 311, 337, 51796], "temperature": 0.0, "avg_logprob": -0.18707400957743328, "compression_ratio": 1.896, "no_speech_prob": 0.003700048429891467}, {"id": 660, "seek": 462056, "start": 4620.64, "end": 4625.84, "text": " actual purposes but at least if that line of work that we are kind of sketching here", "tokens": [50368, 3539, 9932, 457, 412, 1935, 498, 300, 1622, 295, 589, 300, 321, 366, 733, 295, 12325, 278, 510, 50628], "temperature": 0.0, "avg_logprob": -0.1582811155984568, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.005629720166325569}, {"id": 661, "seek": 462056, "start": 4626.64, "end": 4634.0, "text": " ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's", "tokens": [50668, 5314, 493, 3803, 1345, 1547, 264, 1454, 307, 300, 321, 393, 4648, 365, 441, 43, 26386, 4317, 294, 257, 636, 300, 311, 51036], "temperature": 0.0, "avg_logprob": -0.1582811155984568, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.005629720166325569}, {"id": 662, "seek": 462056, "start": 4635.84, "end": 4640.240000000001, "text": " well even if we if we have that kind of intentional sense and may believe about", "tokens": [51128, 731, 754, 498, 321, 498, 321, 362, 300, 733, 295, 21935, 2020, 293, 815, 1697, 466, 51348], "temperature": 0.0, "avg_logprob": -0.1582811155984568, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.005629720166325569}, {"id": 663, "seek": 462056, "start": 4640.240000000001, "end": 4646.72, "text": " who the kinds of besties they have at the background we will know that what their limitations are and", "tokens": [51348, 567, 264, 3685, 295, 1151, 530, 436, 362, 412, 264, 3678, 321, 486, 458, 300, 437, 641, 15705, 366, 293, 51672], "temperature": 0.0, "avg_logprob": -0.1582811155984568, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.005629720166325569}, {"id": 664, "seek": 464672, "start": 4646.72, "end": 4651.52, "text": " what their actual besties are i'd make sure to go with that Charles because we're going to use", "tokens": [50364, 437, 641, 3539, 1151, 530, 366, 741, 1116, 652, 988, 281, 352, 365, 300, 10523, 570, 321, 434, 516, 281, 764, 50604], "temperature": 0.0, "avg_logprob": -0.1543553370349812, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0030725079122930765}, {"id": 665, "seek": 464672, "start": 4651.52, "end": 4657.6, "text": " this much yeah yeah no i agree with all that i just had a slightly different first reaction to the", "tokens": [50604, 341, 709, 1338, 1338, 572, 741, 3986, 365, 439, 300, 741, 445, 632, 257, 4748, 819, 700, 5480, 281, 264, 50908], "temperature": 0.0, "avg_logprob": -0.1543553370349812, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0030725079122930765}, {"id": 666, "seek": 464672, "start": 4657.6, "end": 4663.360000000001, "text": " question i took the question to be in part about how to deal with the sort of prompt sensitivity", "tokens": [50908, 1168, 741, 1890, 264, 1168, 281, 312, 294, 644, 466, 577, 281, 2028, 365, 264, 1333, 295, 12391, 19392, 51196], "temperature": 0.0, "avg_logprob": -0.1543553370349812, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0030725079122930765}, {"id": 667, "seek": 464672, "start": 4663.360000000001, "end": 4668.72, "text": " of models the fact that sometimes we you know write something that seems natural to us but", "tokens": [51196, 295, 5245, 264, 1186, 300, 2171, 321, 291, 458, 2464, 746, 300, 2544, 3303, 281, 505, 457, 51464], "temperature": 0.0, "avg_logprob": -0.1543553370349812, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0030725079122930765}, {"id": 668, "seek": 464672, "start": 4668.72, "end": 4674.4800000000005, "text": " provokes an unexpected response from a large language model and how do we think about that", "tokens": [51464, 1439, 8606, 364, 13106, 4134, 490, 257, 2416, 2856, 2316, 293, 577, 360, 321, 519, 466, 300, 51752], "temperature": 0.0, "avg_logprob": -0.1543553370349812, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0030725079122930765}, {"id": 669, "seek": 467448, "start": 4675.44, "end": 4679.759999999999, "text": " and the first thought that occurred to me was just that we should distinguish between", "tokens": [50412, 293, 264, 700, 1194, 300, 11068, 281, 385, 390, 445, 300, 321, 820, 20206, 1296, 50628], "temperature": 0.0, "avg_logprob": -0.12964222079417745, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.00040436495328322053}, {"id": 670, "seek": 467448, "start": 4680.879999999999, "end": 4689.5199999999995, "text": " different kinds of large language models you know we have this sort of huge large language models", "tokens": [50684, 819, 3685, 295, 2416, 2856, 5245, 291, 458, 321, 362, 341, 1333, 295, 2603, 2416, 2856, 5245, 51116], "temperature": 0.0, "avg_logprob": -0.12964222079417745, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.00040436495328322053}, {"id": 671, "seek": 467448, "start": 4690.16, "end": 4697.839999999999, "text": " which are fine-tuned to interact with us in a particular way and our and here's the central", "tokens": [51148, 597, 366, 2489, 12, 83, 43703, 281, 4648, 365, 505, 294, 257, 1729, 636, 293, 527, 293, 510, 311, 264, 5777, 51532], "temperature": 0.0, "avg_logprob": -0.12964222079417745, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.00040436495328322053}, {"id": 672, "seek": 469784, "start": 4697.92, "end": 4706.32, "text": " point they're trained on a sort of unthinkably large database whereas there are other sorts", "tokens": [50368, 935, 436, 434, 8895, 322, 257, 1333, 295, 517, 21074, 1188, 2416, 8149, 9735, 456, 366, 661, 7527, 50788], "temperature": 0.0, "avg_logprob": -0.07922419634732333, "compression_ratio": 1.8144329896907216, "no_speech_prob": 0.020635537803173065}, {"id": 673, "seek": 469784, "start": 4706.32, "end": 4710.96, "text": " of large language models where the training data is more circumscribed and where we know", "tokens": [50788, 295, 2416, 2856, 5245, 689, 264, 3097, 1412, 307, 544, 7125, 82, 18732, 293, 689, 321, 458, 51020], "temperature": 0.0, "avg_logprob": -0.07922419634732333, "compression_ratio": 1.8144329896907216, "no_speech_prob": 0.020635537803173065}, {"id": 674, "seek": 469784, "start": 4712.0, "end": 4719.4400000000005, "text": " in more detail you know what where you can survey what the training data says and i think", "tokens": [51072, 294, 544, 2607, 291, 458, 437, 689, 291, 393, 8984, 437, 264, 3097, 1412, 1619, 293, 741, 519, 51444], "temperature": 0.0, "avg_logprob": -0.07922419634732333, "compression_ratio": 1.8144329896907216, "no_speech_prob": 0.020635537803173065}, {"id": 675, "seek": 469784, "start": 4719.4400000000005, "end": 4723.2, "text": " if you're interested in you know what the mechanisms are underlying the responses", "tokens": [51444, 498, 291, 434, 3102, 294, 291, 458, 437, 264, 15902, 366, 14217, 264, 13019, 51632], "temperature": 0.0, "avg_logprob": -0.07922419634732333, "compression_ratio": 1.8144329896907216, "no_speech_prob": 0.020635537803173065}, {"id": 676, "seek": 472320, "start": 4724.16, "end": 4731.679999999999, "text": " it's certainly very helpful to look at smaller but nevertheless large language models where the", "tokens": [50412, 309, 311, 3297, 588, 4961, 281, 574, 412, 4356, 457, 26924, 2416, 2856, 5245, 689, 264, 50788], "temperature": 0.0, "avg_logprob": -0.04897196118424579, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0013245220761746168}, {"id": 677, "seek": 472320, "start": 4731.679999999999, "end": 4736.96, "text": " training data is known to us because you know when you train a model on the entire internet", "tokens": [50788, 3097, 1412, 307, 2570, 281, 505, 570, 291, 458, 562, 291, 3847, 257, 2316, 322, 264, 2302, 4705, 51052], "temperature": 0.0, "avg_logprob": -0.04897196118424579, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0013245220761746168}, {"id": 678, "seek": 472320, "start": 4736.96, "end": 4743.76, "text": " there are going to be all kinds of you know subtle signals in there that we don't have much hope of", "tokens": [51052, 456, 366, 516, 281, 312, 439, 3685, 295, 291, 458, 13743, 12354, 294, 456, 300, 321, 500, 380, 362, 709, 1454, 295, 51392], "temperature": 0.0, "avg_logprob": -0.04897196118424579, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0013245220761746168}, {"id": 679, "seek": 472320, "start": 4743.76, "end": 4747.76, "text": " tracing back to their source but which will influence the model behavior in all sorts of", "tokens": [51392, 25262, 646, 281, 641, 4009, 457, 597, 486, 6503, 264, 2316, 5223, 294, 439, 7527, 295, 51592], "temperature": 0.0, "avg_logprob": -0.04897196118424579, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0013245220761746168}, {"id": 680, "seek": 474776, "start": 4748.24, "end": 4757.52, "text": " ways but working with these somewhat more conscripted models gets rid of that problem at least in part", "tokens": [50388, 2098, 457, 1364, 365, 613, 8344, 544, 1014, 5944, 292, 5245, 2170, 3973, 295, 300, 1154, 412, 1935, 294, 644, 50852], "temperature": 0.0, "avg_logprob": -0.10942113664415147, "compression_ratio": 1.5114503816793894, "no_speech_prob": 0.005133949685841799}, {"id": 681, "seek": 474776, "start": 4761.360000000001, "end": 4770.24, "text": " cool well where where do you see the work going or where do you plan to continue this direction", "tokens": [51044, 1627, 731, 689, 689, 360, 291, 536, 264, 589, 516, 420, 689, 360, 291, 1393, 281, 2354, 341, 3513, 51488], "temperature": 0.0, "avg_logprob": -0.10942113664415147, "compression_ratio": 1.5114503816793894, "no_speech_prob": 0.005133949685841799}, {"id": 682, "seek": 477024, "start": 4770.5599999999995, "end": 4778.08, "text": " yeah so actually so we wrote this paper this short paper for the ICML", "tokens": [50380, 1338, 370, 767, 370, 321, 4114, 341, 3035, 341, 2099, 3035, 337, 264, 14360, 12683, 50756], "temperature": 0.0, "avg_logprob": -0.19456974665323892, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.007913526147603989}, {"id": 683, "seek": 477024, "start": 4778.08, "end": 4783.12, "text": " machine learning conference in the national conference machine learning that's happening", "tokens": [50756, 3479, 2539, 7586, 294, 264, 4048, 7586, 3479, 2539, 300, 311, 2737, 51008], "temperature": 0.0, "avg_logprob": -0.19456974665323892, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.007913526147603989}, {"id": 684, "seek": 477024, "start": 4783.12, "end": 4789.599999999999, "text": " this week in the data and will be getting to Vienna at the end of the week for the popular", "tokens": [51008, 341, 1243, 294, 264, 1412, 293, 486, 312, 1242, 281, 31024, 412, 264, 917, 295, 264, 1243, 337, 264, 3743, 51332], "temperature": 0.0, "avg_logprob": -0.19456974665323892, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.007913526147603989}, {"id": 685, "seek": 477024, "start": 4789.599999999999, "end": 4793.92, "text": " workshop that we're representing this paper which is a workshop on language models and", "tokens": [51332, 13541, 300, 321, 434, 13460, 341, 3035, 597, 307, 257, 13541, 322, 2856, 5245, 293, 51548], "temperature": 0.0, "avg_logprob": -0.19456974665323892, "compression_ratio": 1.7591623036649215, "no_speech_prob": 0.007913526147603989}, {"id": 686, "seek": 479392, "start": 4794.0, "end": 4799.92, "text": " cognitive science so there will be a very strict page limit for these ICML", "tokens": [50368, 15605, 3497, 370, 456, 486, 312, 257, 588, 10910, 3028, 4948, 337, 613, 14360, 12683, 50664], "temperature": 0.0, "avg_logprob": -0.14416945852884433, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.017685675993561745}, {"id": 687, "seek": 479392, "start": 4801.2, "end": 4808.4, "text": " contribution which is four pages but what we want to do next is to expand this into a more", "tokens": [50728, 13150, 597, 307, 1451, 7183, 457, 437, 321, 528, 281, 360, 958, 307, 281, 5268, 341, 666, 257, 544, 51088], "temperature": 0.0, "avg_logprob": -0.14416945852884433, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.017685675993561745}, {"id": 688, "seek": 479392, "start": 4809.76, "end": 4814.88, "text": " philosophically substantive paper that's going to be a bit longer and that's going to expand on the", "tokens": [51156, 14529, 984, 47113, 3035, 300, 311, 516, 281, 312, 257, 857, 2854, 293, 300, 311, 516, 281, 5268, 322, 264, 51412], "temperature": 0.0, "avg_logprob": -0.14416945852884433, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.017685675993561745}, {"id": 689, "seek": 479392, "start": 4814.88, "end": 4819.84, "text": " more philosophically meaty parts of that of that project because everything is still a bit compressed", "tokens": [51412, 544, 14529, 984, 4615, 88, 3166, 295, 300, 295, 300, 1716, 570, 1203, 307, 920, 257, 857, 30353, 51660], "temperature": 0.0, "avg_logprob": -0.14416945852884433, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.017685675993561745}, {"id": 690, "seek": 481984, "start": 4819.84, "end": 4824.08, "text": " in that version that we're presenting at ICML so yeah that's the next step for us this is a", "tokens": [50364, 294, 300, 3037, 300, 321, 434, 15578, 412, 14360, 12683, 370, 1338, 300, 311, 264, 958, 1823, 337, 505, 341, 307, 257, 50576], "temperature": 0.0, "avg_logprob": -0.16761876855577743, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.020598793402314186}, {"id": 691, "seek": 481984, "start": 4824.08, "end": 4828.32, "text": " really useful way for us to force ourselves to write things down after running the box piece we", "tokens": [50576, 534, 4420, 636, 337, 505, 281, 3464, 4175, 281, 2464, 721, 760, 934, 2614, 264, 2424, 2522, 321, 50788], "temperature": 0.0, "avg_logprob": -0.16761876855577743, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.020598793402314186}, {"id": 692, "seek": 481984, "start": 4828.32, "end": 4834.08, "text": " wanted to write an academic piece now we've written kind of a condense skeleton of the piece that", "tokens": [50788, 1415, 281, 2464, 364, 7778, 2522, 586, 321, 600, 3720, 733, 295, 257, 2224, 1288, 25204, 295, 264, 2522, 300, 51076], "temperature": 0.0, "avg_logprob": -0.16761876855577743, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.020598793402314186}, {"id": 693, "seek": 481984, "start": 4834.08, "end": 4839.12, "text": " focuses more that caters more to an analogians and now the next step is to write the full", "tokens": [51076, 16109, 544, 300, 3857, 433, 544, 281, 364, 16660, 2567, 293, 586, 264, 958, 1823, 307, 281, 2464, 264, 1577, 51328], "temperature": 0.0, "avg_logprob": -0.16761876855577743, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.020598793402314186}, {"id": 694, "seek": 481984, "start": 4839.12, "end": 4844.400000000001, "text": " philosophy paper or at least that part of our project to be complete and then I don't have to", "tokens": [51328, 10675, 3035, 420, 412, 1935, 300, 644, 295, 527, 1716, 281, 312, 3566, 293, 550, 286, 500, 380, 362, 281, 51592], "temperature": 0.0, "avg_logprob": -0.16761876855577743, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.020598793402314186}, {"id": 695, "seek": 484440, "start": 4844.4, "end": 4849.839999999999, "text": " that maybe we'll have all that ideas but yeah yeah", "tokens": [50364, 300, 1310, 321, 603, 362, 439, 300, 3487, 457, 1338, 1338, 50636], "temperature": 0.0, "avg_logprob": -0.2151404765614292, "compression_ratio": 1.535031847133758, "no_speech_prob": 0.009845959022641182}, {"id": 696, "seek": 484440, "start": 4853.36, "end": 4860.24, "text": " I got nothing to add to that cool yes well it's very interesting work I think it it brings a", "tokens": [50812, 286, 658, 1825, 281, 909, 281, 300, 1627, 2086, 731, 309, 311, 588, 1880, 589, 286, 519, 309, 309, 5607, 257, 51156], "temperature": 0.0, "avg_logprob": -0.2151404765614292, "compression_ratio": 1.535031847133758, "no_speech_prob": 0.009845959022641182}, {"id": 697, "seek": 484440, "start": 4860.24, "end": 4869.5199999999995, "text": " lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the", "tokens": [51156, 688, 295, 3755, 1214, 293, 309, 311, 512, 10675, 293, 15605, 3497, 11233, 294, 11233, 666, 264, 51620], "temperature": 0.0, "avg_logprob": -0.2151404765614292, "compression_ratio": 1.535031847133758, "no_speech_prob": 0.009845959022641182}, {"id": 698, "seek": 486952, "start": 4869.6, "end": 4876.64, "text": " the heat and into the the spotlight and the relevance and so it's going to be an exciting", "tokens": [50368, 264, 3738, 293, 666, 264, 264, 24656, 293, 264, 32684, 293, 370, 309, 311, 516, 281, 312, 364, 4670, 50720], "temperature": 0.0, "avg_logprob": -0.17579354842503866, "compression_ratio": 1.5547445255474452, "no_speech_prob": 0.002841302426531911}, {"id": 699, "seek": 486952, "start": 4876.64, "end": 4883.92, "text": " learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation", "tokens": [50720, 2539, 4671, 3231, 337, 1419, 505, 2086, 1338, 1309, 291, 588, 709, 1627, 2103, 264, 3761, 51084], "temperature": 0.0, "avg_logprob": -0.17579354842503866, "compression_ratio": 1.5547445255474452, "no_speech_prob": 0.002841302426531911}, {"id": 700, "seek": 486952, "start": 4884.56, "end": 4891.76, "text": " till next time thank you bye", "tokens": [51116, 4288, 958, 565, 1309, 291, 6543, 51476], "temperature": 0.0, "avg_logprob": -0.17579354842503866, "compression_ratio": 1.5547445255474452, "no_speech_prob": 0.002841302426531911}, {"id": 701, "seek": 489952, "start": 4899.52, "end": 4900.900000000001, "text": " you", "tokens": [50404, 291, 50433], "temperature": 0.0, "avg_logprob": -0.7437378764152527, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.2910637855529785}, {"id": 702, "seek": 492952, "start": 4929.52, "end": 4930.900000000001, "text": " you", "tokens": [50408, 291, 50433], "temperature": 0.0, "avg_logprob": -0.7941432595252991, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.3962155878543854}], "language": "en"}