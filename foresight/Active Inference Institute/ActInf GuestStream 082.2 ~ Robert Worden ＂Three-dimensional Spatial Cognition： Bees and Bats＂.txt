Hello and welcome. It is June 3, 2024. We're in active inference guest stream 82.2 with
Robert Warden. Today we're going to be discussing three dimensional spatial cognition, bees
and bats. So thank you Robert for joining again. To you for the presentation and looking
forward to it. Thanks Dan. Okay, well as Dan said, I'm talking about three dimensional
spatial cognition in small animals, particularly like bees and bats for examples. And what I'm
going to be doing is showing you a demonstration program that does this. And so you can find
the demonstration program at this link at the bottom of the picture, and you can download
it and try it yourself. Or you can read a couple of papers about this work, which are
there an archive at that address. So that is the introduction to get straight into it.
What this work represents, I think is a challenge for classical neuroscience. And by classical
neuroscience, I mean the assumption that all that happens in the brain or cognition is
done by neurons connecting to each other by synapses and so on. And the challenge, which
I think comes out of this work, is that the main result is that neurons are actually not
capable of that. They cannot represent three dimensional space, because they're too imprecise
and too slow. So the resulting challenge for neuroscience is to show that this idea is
wrong. Everybody thinks it's wrong. Everybody thinks neurons do everything. So you have
to show it's wrong by building a working model of neural computation model of 3d space and
checking that it really scales and can perform somewhat like animals perform. I think just
writing papers and talking about it is not enough. You've got to build a model and show
it works. And for building that model, the FEP neural process model is the best starting
point I believe. So that's where this talk is going. It gets there by 3d spatial cognition.
So what is that? Spatial cognition, as first say, is a very important problem. The primary
task of any animal brain is to control its movements, physical movements, its limbs,
in 3d. And that's a 3d problem. And it has to do that at all times of the day. And for
most animals, most of their brain is devoted to this problem. And we believe, being Bayesian,
they do this by building and using Bayesian maximum likelihood model of 3d spatial space. And my
previous live stream was about the subject how animals build models in general. But the
particular 3d model of space, that's been important since the Cambrian era, when animals
first started having precise sense data, like good eyes and capable limbs. And that there's
been huge and sustained selection pressure on all animal species since then, to do it well. And
what we believe is that animals do do it rather well. For instance, our own conscious awareness
of 3d space must come from something going on in our brains. And that is a rather precise model
in our conscious awareness of 3d space around us. So that must be quite a good model of space
in our heads. And going from us to small insects, even the small insect can land very
skillfully on the room of a coffee cup or any other surface. So that's why I say modeling
3d spatial cognition is the top priority for neuroscience. And we may look at all sorts of
problems in neuroscience. This is the one, this is the hard part of the problem, the thing we
really need to get right. So how do you do it? How do animals build a 3d model of local space
around them? And the immediate problem is that most of their sense data in their vision is
two dimensional. So how do you get from two dimensional vision to three dimensional model of
space? There are some constraints and people obviously think of stereopsis with two eyes,
where you can tell the depth of things or proprioception and touch. But those constraints
only apply to restricted regions of the space around an animal. So for the rest of it,
what I believe animals do is they build a model of the space around them by moving in space. And
this is a form of active entrance, if you like, that you have to move in a moving space to find
out about space. And this is based on a very strong Bayesian probability that as an animal's move,
most of the things around it do not move. So the world is like a big rigid moving body
around the animal. So the animal can compute object locations by what is called structure from
motion, SFM. And when you build a computational model of this, it's actually fairly simple
computation to do. What you can do is fairly simple 3d matrix operations to maximize the likelihood
of an object being in a certain position. And that's what this program does.
But if you're doing that, shape from motion, structure from motion, it requires a short
term spatial memory of working memory for positions of things. And that's what this
demonstration program does. So the demonstration program
that gives them both the echo delay and it gives them a Doppler shift. And we'll talk about that
later. So both of those animals move fast among static objects. So this 3d shape from motion
is an applicable way of working out where the objects are. So we're going to go ahead and
move on to the next slide. So we're going to go ahead and move on to the next slide.
So this 3d shape from motion is an applicable way of working out where the objects are.
So the program we're going to show you builds a 3d model of space in three
different ways. Firstly, it can build an optimal Bayesian shape from model model model.
That is what I call a brute force calculation. I've discussed those in my previous live stream.
And it's very hard for animals to do. It's not the way we think animals do it. But
the interesting thing about it is it gives you the very best possible Bayesian model
based on the sense data. So that's the first way. The second way is by dynamical object tracking
where an animal makes an estimate of where each object is in three dimensions. And then it keeps
updating their estimate every time, from every step along the track it takes. It updates that
estimate from its sense data. And the third model is doing that same object tracking,
but doing it in the presence of neural memory noise. I should say this computational model is
built at Mars level, David Mars level two. That is, it's not a neural implementation,
but I have made it so that you can add simulated neural noise to it. So if you want to see this
program after you've seen demonstrated, download it from this web address and you can
quite simply unzip it and start it running. So I will now switch to demonstrating this program.
So I end the show and there is the program. That's what it looks like. And what you see is
three different windows, the left center and right. The left hand window is going to be a
three dimensional view of some space in which a B or a bat is moving. The center view is always just
help information and it tries to tell you how to use all these sliders and buttons and controls.
And the right hand view is just various graphs and we'll see some of those as we go along.
So what happens when you press the start button is you see some three dimensional space and inside
it there on the left hand side is an animal this time of B and the colored circles are objects
randomly spaced in that space. And so the line is going from the B to the objects are lines of
sight. And this is a three dimensional view so you can rotate it, see the three dimensions and
that's what happens when you rotate the objects rotate. And so there at the moment we've got
nothing about the B's internal model of space. We've only got actual space itself shown in the
view. In order to show the B's internal model of space we press the run button and I'll do this
and you can see what happens. So as you press run the B starts moving that's the green line.
It gets new lines of sight and from the new lines of sight it estimates the positions of all those
objects. So I'll restart and do that again. And what you'll see this time is that the estimates
of the positions appear as small circles with error bars. So the error bars are the gray lines
the small circles are where the B thinks the object is. So you can see the B is building
rather an accurate model of where the objects are from its sense data. I'll restart again and this
time we'll step through it one step at a time to see how the B's model of space evolves with time.
So one step and you can see in the very early steps we'll rotate it a bit to see show what's
going on. The B starts making really quite good estimates of where the objects are.
Each little white circle is quite close to the blue circle but the estimates have error bars
and the error bars are in three dimensions showing the uncertainty of the location estimate in all
those dimensions. So those estimates of position that enable the B to move where it wants to do
suppose these are flowers it can go to the flowers and get pollen or whatever it wants to do.
Now I said the program computes three different kinds of model and these three is always doing
this as you go through the steps on the track and these three models are a full Bayesian model
and that's what we're showing at the moment full Bayesian. We can switch to a tracking model
that's the object dynamic tracking and there if I switch that you can see the error bars and the
objects hardly move at all they do move a little bit but this is one result of the program that
doing that tracking model which is simpler to do than the full Bayesian model gives you nearly the
same estimates and same error bars and the third model it computes is a tracking model with noise
and I switch to this one and again it's not moving very much actually but I'll move it on a bit and
you'll see that noisy tracking often is very different from tracking without noise so we're
on tracking without any noise at the moment we step forward a bit and the B keeps updating its
estimates of these positions as they go and I now switch from tracking to noisy tracking
and you can see the noisy tracking is significantly worse than tracking they noisy
estimates have drifted away from the true positions of the object and that is really the second
by the way I'm running this with a fairly small level of neural noise at the moment you can adjust
the level of neural noise you're going to adjust the bees visual acuity you adjust all sorts of
things using these sliders and try running the program again this is running with a fairly small
level of noise so I keep stepping and the noisy tracking estimate keeps getting worse but I go
back to the tracking estimate the tracking estimate without neural noise is pretty much dead on so
I'll restart again and I'll run again because then we can show the graph at the end of the run
which shows what's happened to these errors so if I run again
now you look at the graph on the right and what that is doing is comparing tracking which is the
black curve versus noisy tracking which is the red curve and these are steps along the bees
track those are the same steps we saw there but these vertical axis is the level of error in the
depth of the bee estimates and you can see that tracking is rather small errors it homes in on
true position of the object whereas with memory noise you get much bigger errors and in fact the
errors are very unpredictable if you rerun we'll just rerun it again you find that the errors
coming from noisy tracking they do seem to vary quite a lot from one run to the next noisy tracking
is pretty unpredictable basically so there the errors have gone right up and they've come down
again whereas ordinary tracking without noise is nice convergence towards the true positions
so that is really the second key result of the simulation that is that a tracking model is a
realistic model of how animals estimate positions of things around them but if you add noise to it
even adding a small amount of noise completely messes up the tracking model now there are more
things you can show with this model you can show for instance how animals use their model to detect
what is moving around them because obviously when an animal is moving it is quite hard for it to
detect motion from its visual field because when it's moving everything is moving in this
visual field so it has to use the three dimensional model to work out what is actually moving and
if you also plot the efficiency of motion detection you find that too is very much poorer
when you add memory noise so I think I'll stop at this point the program also does a simulation
of bats but I won't step straight into that perhaps we could come back to that at the end of the talk
if somebody wants to hear about it but for the moment let's just step back and find what we
have concluded from the bee's model now where is my presentation now I've got to resume the
presentation somehow how do I do that yeah you get back to the presentation
so the key points that I think I may have shown you is that in the 3d view you can see the track
of the animal you can see it's the lines of sight you can see sorry my phone is ringing I better
go and shoot it up
I've shown you the three I've shown you where the real objects are those are the circles the
colored circles shown you the objects as located in the internal model I've shown you shape from
motion I've shown you what the error bars are and you can rotate the 3d view I've shown you
the three different models of 3d space the full Bayesian model the dynamic object tracking model
and the tracking model with memory errors as I say when you run this program you can change
all sorts of parameters to see how it's sensitive to the parameters and I've shown you the bee's
spatial model I've shown you how the error bars are particularly the depth dimension
and I've shown you how memory noise degrades the model so here are the key results so the best
possible model and any animal could build from its sense is a very good model in fact it's more
precise than the sense data because if the animal can assume that objects don't move then over time
an animal can build up a very good understanding of where objects are better than its raw sense data
animals can't do any better than that but dynamical object tracking works pretty well it's almost as
good as the full Bayesian model and it only works if spatial memory has very high precision and I
didn't say the levels of precision that I put into the program which start to spoil the tracking model
they're about one part in a hundred and that I believe is much more precise the most neural
representation of space can give so that's the next part of this talk how do we do that modeling
with a neural model of the brain the classical neural model so how do you build a neural spatial
memory and this is a changed quote from Animal Farm where they said two legs bad four legs good
two dimensions is easy three dimensions is hard because two dimensions you can easily do a sheet
of neurons representing two dimensions whereas in three dimensions you don't have that option
and there are several possible memory designs you can have a two-dimensional sheet of neurons
and you can represent the third dimension by depth depth by the third yeah you can have some other
variable representing depth or you can have a three-dimensional clump of neurons where
position in the clump represents a 3d position or you can represent all three dimensions of
an object position by neural firing rates that none of these work well for object tracking I think
we can quite simply eliminate the 3d clump model but the other two models have a problem with neural
error rates and if neural information is encoded as firing rates of neurons
typically you have a neuron firing n times in some time interval t and then the precision
with which it can represent some real quantity is of the order of one part in square root of n
now n over t is typically five or between five and fifty pulses per second on most animal brains
but for insects and small mammals the time they've got to have to update their internal model of
space is very small typically less than a tenth of a second so if the time there's a tenth of a second
then you get n less than 10 and that gives you errors of the order of 30 percent which are much
bigger than the 1 percent errors which I said are needed for tracking structure from motion
so the conclusion of this is that the neurons when they represent space there's a trade-off
between speed and precision faster you have the less precise it is and this trade-off is just too
hard so I believe there is no working neural model of 3d spatial cognition now the three
the three points in blue I've said before they say spatial cognition is very important and animals
do it well and we've had this problem for a long time in his book vision 40 years ago
David Ma identified the challenge and he started work on it he defined what he called a two and
a half d sketch and various other models now I believe that in terms of building neural models
of how spatial cognition works people have really not moved beyond this
why I think the main reason is that the memory problem is just too hard that memory gives them
two big errors and is too slow and I suspect that over the years there have been many people who look
at this problem and they decide to move on and do something else instead but the result is that
spatial cognition is the central problem of neuroscience we don't have a model of it and so
this is rather like theory of planetary motion without a sun or a theory of the atom with no nucleus
so how does this relate to active vision I think active vision is one way to explore this problem
active vision describes how a 3d spatial model can be inferred from vision and there are quite a
few papers on it they focused on various aspects of it they focused on 2d scene classification
they focused on the trade-offs between various objectives like the choice of visual saccades
they focused on 3d robotics as far as I know none of them have really focused on how
animal brains practically build a 3d model and I believe that existing active vision models
do not address the issue of neural error rates one reason for this is that the standard active
inference toolkit in MATLAB I believe it doesn't model neural error rates it assumes I believe
an abstract perfect neuron with very precise representation of quantities and error rates
are actually not an issue for many of these applications they're not an issue for robotics
and they're not an issue really for making discrete choices but as I've said in this talk so far
the accuracy with 3d model really matters and neural error rates are the big problem
if we set that problem on one side for a moment there is the issue of active inference trade-offs
and there are many interesting trade-offs you can examine in active vision and the key trade-off I
believe is one between freezing and moving as I've shown in the demonstration the animal
has to move by has to move in order to infer the 3d positions things around it by shape from motion
it also of course has to move to achieve practical goals like feeding and fleeing and mating and
so on on the island it can freeze and freezing it may conserve energy it may be able to detect
what's moving simply directly from its visual field which is much easier and it may itself
avoid detection so these are very key trade-offs they're absolutely essential for lifetime fitness
for many animals animals have to make this trade-off or these trade-offs any moment of the day
and so we've got plenty of empirical data about it and I think it'll be a very useful area to explore
now I'm going to switch to something completely different having said that neural storage
of spatial positions is a very hard problem I'm going to talk about an alternative possible
alternative way of storing spatial data and so if you assume there's a round some round region
inside the brain of a fairly large diameter d and this holds waves with a minimum wavelength
which are called lambda and the neurons can couple to the waves both as transmitters and receivers
and the wave can persist at least for fractions of a second so the wave can act as a working memory
for positions and the number of object positions you can store in the wave can be up to d over lambda
cube and that's a can be a very large number the spatial precision which one object position
is stored can be one part in d over lambda and I think that d over lambda can be very large so
you can easily get precision better than one part in 100 which is what you need to build the spatial
model so in summary wave storage of 3d positions may have a lot of computational benefits you can
give a natural fit to the problem it can give high precision and high capability it can give you
very fast response times low spatial distortion and some other benefits which are described in the
papers so apart from its computational benefits is there any evidence for wave storage in the brain
I believe there are two quite powerful lines of evidence one of which comes from the insect
central body the central body of the insect brain is a very small part of the brain in the middle of
it and it consists of a fan shaped body and the elliptical body and it has this shape which is
remarkably well conserved across all insect species and there's an insect brain database and I've
gone to the insect's brain database and pulled from it the shapes of the central body from a few
typical insect species and here you can see the fan shaped body and the elliptical body and it's
very constant across all kinds of insects and you can see it's approximately a round shape so it's
well suited to hold three dimensional wave and it does multi-sensory integration and so it's quite
likely quite probable that it holds spatial positions and insects have very few neurons
in their brain to do it in any other way and what I think is significant is how constant and round
the insect central body is compared with all the other parts of the insect brain
so that's one piece of evidence from the insect central body the other piece of evidence comes
from the mammalian thalamus as you may know the thalamus of most mammals all animals is
approximately spherical and is connected to all sense data and all cortical reasons
but the important thing is that the shape of the thalamus
is highly conserved across all species and there's an important aspect of the thalamus anatomy
that unless you assume a wave really it doesn't make sense because the thalamus consists of a
number of independent nuclei like the pulvinar and the lgn and so on and so forth and the connections
across within the thalamus between these nuclei are very weak or even non-existent so you could
have this picture here that the thumb where's my pointer here's my pointer the thalamic
nuclei which do have white circles here they all connect in two ways the cortex but they don't
connect to each other so one thalamic nucleus here could easily start moving out to towards the
cortex and the distance the length of its axons could decrease and its other connections it doesn't
need other connections so all the nuclei could migrate outward towards the cortex and you could
still have the same neural synaptic connectivity and the same computational capability if neurons
only compute by synaptic computation so this way you could save a lot of energy in shorter axon
lengths so in summary a compact thalamus and it makes sense if all the nuclei need to be immersed
in the same wave so we now have three pieces of evidence for a wave in the brain firstly there's
the computational neuroscience that it's a very difficult problem to build a 3d model about it
but you can build a 3d spatial model if you assume there's a wave storing positions
secondly the insect's central body is nearly round in all insects very well suited to hold a
wave and it appears to be in the right part of the brain to do that and thirdly the mammalian
thalamus which again has this round shape very well suited to hold a wave and the important thing
here is that without a wave the anatomy of the thalamus doesn't make sense so I would like you
if you remember only one thing about this talk remember this slide there is quite a lot of evidence
for a wave in the brain one thing I will say is that the wave is probably not an electromagnetic
wave because there's quite a lot of interest in electromagnetic fields in the wave from researchers
like Miller and McFadden and so on but electromagnetic field can't play the role that this wave is
supposed to is needed to play in other words the key thing that this the wave is supposed to do in
this model is to store information of fraction of a second but an electromagnetic field in the
wave and there certainly are electric in the met in the brain there certainly are electromagnetic
fields in the brain they cannot store information of fractions of a second and they cannot represent
3d space like a holibra and just to say a little more about this if there's an electromagnetic
wave in the brain it has to obey Maxwell's equation so the wavelength times the frequency
is equal to the speed of light lambda f equals c and that means that 40 hertz typical frequencies
of these waves the wavelength is 8000 kilometers as large as half the earth so it's the conclusion
is that at 40 hertz electromagnetic field is not a wave it's a static field and is driven entirely
by neuron firing so it doesn't store the information so in summary we're looking for something not
electromagnetic and possibly some quantized excitation something a bit exotic um in the
field of quantum biology i think we shouldn't despair here because we know evolution is a lot
smarter than we are at discovering these things and exploding them so here are some take home
questions does 3d spatial cognition use a wave in the brain in other words in the light of the
evidence i've shown you what is the Bayesian probability of that hypothesis being true now i
say these take home questions because i didn't expect you to have an answer immediately but
perhaps you'd like to look at the papers and see what the evidence is and try and assess it in your
own mind or do you know some slam dunk killer reason why they can't be a wave in the brain
if you do know reason what is that reason and how do brains compute space how do neurons
on their own represent 3d space with enough precision on the other hand if there might be
a wave in the brain wouldn't that be a a rather exciting and revolutionary development it would
actually change the neuroscience and it could address this central unsolved problem of how
spatial cognition takes place so i believe that possibility should be explored particularly
for young researchers this is attractive it's greenfield research it's not a well-trodden path
of classical neuroscience the classical neuroscience model of maculic pits neurons and
hebbian synapses that's 75 years old now so i would like to encourage people to get out and explore
or again come back to the earlier slide here a crisis in neuroscience the result of this work
i think is that neurons can't represent 3d space because they're too imprecise and too slow so
the crisis is can you show this is wrong can you show it by building a working neural computational
model and checking its scales properly so the fep neural process model is the starting point for
that i think it's a good problem to work on because it is a crisis and big crisis big advances in
science tend to come out of crises so what i'm advocating this is my last slide is a twin track
research program to build two different active vision models of 3d spatial cognition one is a pure
neural model which is a classic fep neural process model can this be made to work or are the neural
memory errors going to kill it and secondly to try to build a hybrid wave and neural model
and when you're building those models we can explore the trade-offs that active inference
is so good at computing and particularly the trade-off between freezing and moving so
there are a couple of candidate projects for the active inference institute okay that's it
awesome thank you robert i have some questions and some people have asked questions in the
live chat so i'll uh i'll ask them so first just while i'm recropping everything how would you
connect this to the requirements equation earlier work because you mentioned that there was a
requirements equation driven calibration of the optimal navigation so what does that look like
to have the optimal navigation according to the requirements equation well to summarize on the
requirements equation you can model how brains evolve and this is the previous live stream and
you can show that they evolve towards making a purely Bayesian calculation of their best model
of the world from the sense data but that purely Bayesian calculation is rather expensive and it's
been well known in FEP that full Bayesian calculations is intractable for most animals
and so that is a very expensive calculation and it's probably not the way animals do it but it
is it can be done on digital computers and it can be done in this model i've showed you and the first
model the full Bayesian model is actually computing the requirement equation from the bees or the bats
sense data the second model a tracking model is a is an approximation to that which is a lot cheaper
but seems to give very nearly the same results
okay awesome let us dive into a few mammal and insect neuroanatomy questions so i'll start with
the set of questions from the the live chat this is going to be about mammal neuroanatomy
okay tim ritter asks do you assume this wave property for all phylimic nuclei primary and
secondary or for specific ones e.g polvanar or mediodorsal
very good question i don't know the answer i mean at this stage i believe the whole
thalamus is around spherical near spherical volume with the wave going through all of it so
they are all immersed in that same wave so even the polvanar the polvanar certainly is
even the lgn which is rather small and is a pass through nucleus i think they all are so i think
for instance i think people always say the thalamus is a relay
sense data gets the cortex by account thalamus but people don't have a very good reason why
it has to go through these relays nuclei in order to get there i think it's doing something
about locating about i think the wave has some involvement there but this is very early days
i don't know the answer at all okay another question from tim on mammalian neuroanatomy
what about 2d orientation would you expect similar waves in hippocampal instead of thalamic
regions or is 2d sufficiently easy to get by without yeah basically i believe 2d is easy enough
to get by and the hippocampus is by no means suitable to hold a wave they all sorts hippocampi
have all sorts of different shapes so yes i think that was a core theme between the mammal and
insect areas is the conserved shape and then also the allometric differences over evolution
where the size differential of the insect central body changes much less than other primary sensory
regions and that was in your paper yeah that's right yeah and that kind of implies that that small
size of the central body it's only a few percent of the whole insect brain seems to be enough
yeah and that the properties which it hosts or enables might be related to its physical
extent or like to its surface area it's a volume ratio and not a function like for example in the
antennel lobe where the olfactory information are coming in there are these little glomeruli
and different species have from several tens to several hundred of these olfactory glomeruli
like ants have many and they have more olfactory receptors in their genome and they have more
olfactory glomeruli in that region or insects with more compound eye sections they have larger
optic lobes so the primary sensory regions have very large variation amongst species but then
as you get into the central body you get much more conserved anatomy and size and then the
mushroom body on the top part of the brain is something a little bit in between that might
have more of an analogy to like mammalian cortex where there actually is the possibility to scale
its cognitive capacities through size changes because it has some kind of like repetitive or
stereotyped layout yeah I mean there are a load of fascinating questions in your anatomy which
relate to this and and if you pursue the this hypothesis then there's all those interesting
question I'm not an expert on insect or mammalian neuroanatomy but there's a load of interesting
questions in there cool so about the bee spatial cognition so we know that bees use a variety of
visual cues ranging from the landmark and the landscape recognition to polarization of light
and so on and also as you pointed out the central body does multisensory integration so
how do we think about the possibly complementary or redundant information provided by these
different aspects of the visual fields and what does your simulation focus in on
well I mean I believe that what the central body and the thalamus both do is multisensory
integration in other words animals should they they need to make the best 3d model of space they
can and they need to use all their sensitivities to use it apart from possibly smell that's an
interesting question and so both of them do multisense integration and ideally one would
put in a simulation one would have things like stereopsis one would have object recognition one
would have light polarization all sorts of sources this program only does simple vision or simple
echolocation at the moment but it should do all multisensory integration in in a single
maximum likelihood model of the whole all sense data coming in at the moment
interesting yeah with sounds or with smells it would be interesting to see how those come into
play and and how do you think about in in the simulations presented here egocentric and allocentric
navigation because you mentioned how the kind of simplifying assumption is that the world
is a rigid fixed body so you can have these kind of duality where like I'm moving and the world is
fixed and then there's sort of like I'm fixed and the world is moving so how does that relate
to that to that egocentric allocentric distinction yeah very good question I mean I think the frame
of reference used for the model should be as much allocentric as it can be because the wave has to
persist and if the wave just persists it represents an object as a constant position so you want to
have a frame of reference where most objects are at constant positions so I think that makes
allocentric but obviously has to change from time to time every few seconds it has to switch
because it can't just stay allocentric
hmm interesting so now to connect that to what you brought up about move or stay that kind of
fundamental uh animal or fundamental mobile organismal nervous system question I thought about
different body plans where the eyes or the visual component are unable to move separately from the
body like a bee can turn its body but it can't rotate its eyes whereas in humans for example we
have optic saccade so there that stay or move yes we have turning our posture and moving through
space but also we see like this microcosm where when the gaze is fixed there's high precision
and then movement in the world is associated with movement of objects and then whereas when an
eye saccade is dispatched during the saccade our visual attention is alleviated and then
it's because during that time all the movement of pixels essentially is ascribed to the movement of
the eye so we see kind of like a microcosm of the two modes of movement and stability
in motion detection in the saccading but for other organisms that don't have eye saccade
the only way that they can get that kind of alternating movement and stability is by moving
their body yeah yeah I mean I always think of eye saccades as particularly predators if you like but
want some high resolution in some direction some particular direction whereas for most insects
as you say that there is not the option of a high resolution fovea but I think of saccades as being
cheap there's I mean the freeze move trade-off is a real trade-off that if an animal moves
it can be detected as moving and it can't detect motion itself as well as if it's stationary
and so that's a real hard trade-off an animal has to make whereas saccades you can make them
cheaply whenever you like yes yes and also it's really interesting like how often the behavioral
studies just look at the direction of movement but there's this whole timing of movement and so
there's definitely a lot of empirical studies about whether fear-based movements like in a predator
prey or different kinds of movement choices where would you say attention comes into play
in the sense that the bee or the bat was just kind of taking it all in it didn't have like some
restricted scope so it's kind of like a uniform attention across objects and across space and
time but then we know that we do have this visual attention phenomena well yeah attention is very
important and I think naively it's a search time model of attention in other words the wave
representation of all space represents all the space around an animal but the animal can focus
attention on a region of the space and what that's doing is tuning the receptors in the thalamus so
they are sensitive to wave vectors in a certain region so there's a whole load of issues there
about how the wave works as to whether it can how signals are rooted from sensitive inputs to
specialist regions of the cortex and I think attention is that rooting of information
so again loads of big questions there
yes with the way if I was kind of thinking about the insect brain visual input flowing in
and also other potentially inputs and all of these are crashing on the shores of the central
body and then there's this kind of stabilized dynamical wave representation such that information
coming in differently changes the the the resting shape of the wave and then that opens up like you're
now suggesting recurrent connections or or other connections into that wave hosting region
and recurrent connections can modify the shape of the wave attentionally and then also the
oh yeah the resting shape of the wave can route or augment or suppress other sensory information
coming in that'd be like water kind of dumping to where there's already a high water point
versus water going to where there's low water yeah yeah I'm I think key role of the wave is to
persist a background model of all 3d space and then against that background model new
central information that comes in particularly movement is best evaluated a piece of nuisance
data you evaluate you much better if you compare and contrast it with what you had before that is
attention and it's it's the foulness if you like telling the cortex here pay attention to here
here's your old information from this place in space here's your new information so tell me what's
changed well this connection with frame differencing is very powerful predictive processing predictive
coding algorithms were built by computer scientists and in compression engineers looking to make
video compression work and doing the frame differencing because that's the optimal way to
compress video and then that got brought also back into neuroscience where there's a lot of
focus on things like edge detection and these other 2d visual phenomena and then as you're
pointing to there's this kind of sun at the solar systems center that's not really being discussed
which is like okay yes we have neurons in different visual regions that are excitable by
vertical lines by diagonal lines and so on but this is all flat phenomena and the question of
not just shape recognition but the question that's most approximately relevant for movement
and the fitness related decisions for the organism in the niche has to do with its
spatial navigation not it's like eyesight at the eye doctor yeah
not only is spatial navigation how it moves its limbs you know how where it puts it foot it's foot
next that sort of thing and the 3d model i think does all of that
okay i'll read a question from the live chat
how might the cicade relate to a matrix of inputs versus a human based visual system
movement on the matrix may give different spatial dynamics
i'm not sure what we mean by matrix of inputs there but as you said during a cicade visual
input is kind of blocked while the eye is getting from a to b whereas the wave persists and the 3d
spatial model stays constant and after a cicade the eye has to update the 3d spatial model in
some different place so um i'm not i don't think i've answered the question but perhaps you
i think you understand my matrix it's what you enter um
it's making me think about the experiments where the for example the fruit fly is placed on a ping
pong ball in a harness in a virtual reality setting so it's getting custom visual input
and its movements on the ping pong ball just kind of scrolls the ball so it's basically fixed but it
gives a lot of degrees of experimental freedom around the um orienting of the body and what
visual inputs it gets so i wonder if anywhere there we know about the the time
the timescale of spatial orientation updating because that would be very critical to understand
the nature of the wave but if it was something that was um for example closer to the diffusion rate
of ions then we might be looking more towards like a channel or pore-based hypothesis if it was
something that was faster than neural signaling it would suggest something more like the direct
coupling or other kinds of of action so what what makes you feel as you suggested that it is not an
electromagnetic stabilized wave field well as i say the physics i mean for electromagnetic
wave we do understand the physics and the electromagnetic waves that measured by eeg for
instance they are a purely passive consequence of neural activity they don't persist in the
information for any time at all whereas this wave i'm talking about has to persist information
for fractions of a second so this constant spatial model is kept persisted while the
circuits go on on the animal moves while it computes shape from motion so a pure electric
field we we know the physics it's Maxwell's equations and it does not store energy store
information so it's purely a passive reflection of what's going on on the neurons it's not a memory
hmm so the neurons are especially if we think about the several like thousand to tens of thousand
let's say in the insect central body there's too few and they're too sparse and noisy to in a purely
connectionist neural framework to support the kinds of empirical results that we see
on the other hand a purely field-based approach has some issues that you just laid out so it's
it's very interesting that to at least of the well-known mechanisms the local field potential
and the firing rate rate coding type models that both of them seem to have some limitations
and yet there's a very strong anatomical evidence for the functional role of that region
oh yeah it's absolutely vital region but i believe that just looking at electric fields
magnetic fields in the brain is not going to give you memory and that's the key thing
that i think is needed to do structure from motion you've got to have short-term working memory
to hold a little model of space for a fraction of a second
do you see that as a kind of special type of short-term memory or do you think this is
the same memory pool that like short-term audio memory gets entered into
oh it's it's special it's different from that yeah definitely
to say it starts from how do you can how do you conclude a 3d spatial model
or how do you do structure from motion you need short-term working memory for that purpose specifically
interesting um the kind that enables us to
check for difference in in visually changing systems or what what does this visual working memory
have
well basically it's it's not vision because it's 3d and uh checking for difference in the visual field
is you can do it quicker you can look directly at the visual field whereas this model i think
the 3d model is um it's maintained by a loop between in mammals between the thalamus and the cortex
and it's a 40 hertz cycle that maintains it so it takes time to build the 3d model
and it's a bit slower than direct visual change detection
interesting it's this tension with like visual being what is seen versus kind of a broader
imagine the imagination of vision um what about action in your model so how were the paths set
yeah i said that the model was a bit like active inference and the animal has to move in order to
understand space but it's not really the program has not modeled the kind of active
inference choices of how should i move to get the best understanding of space and you could do that
you could make the b choose this trajectory to get the best understanding of where the flowers are
or you make the b choose this trajectory for all sorts those are the active inference trade-offs
that the program has not yet looked at and which can be looked at and i think are very interesting
that's awesome yeah it's almost like the b in this situation it's like on a train
trying to reduce its uncertainty about the location of landmarks but it's just on a it's
on a rail it doesn't have policy decision whereas once we start to close that loop and ask which
direction of movement or none given the costs would reduce uncertainty about resolving this
kind of spatial relationship then that that's where the perception actions start to like come
into play in benefit of each other and potentially there's several choice successive moves can greatly
reduce uncertainty through active sampling just as we see in skating and all other situations
and that would be like a heuristic or strategy that really does work
yeah i mean another example that i use somewhere is i believe that predatory birds like hawks when
they're approaching their target they don't go in a straight line they move on a curve
to reduce the uncertainty
so that they continue to get more information continue to see the range of of the target
if they just went straight to the target they wouldn't get a range fix on it
now what about the difference between the b and the bat
whereas a b is simply receiving the reflected photons let's just say the bat is sending out
a invisible signal so how is this kind of radar echolocation setting
similar and different to the vision setting
oh it's very different i could show you that with the program if you like
what happens with the bat is from the delay of an
for a particular insect that the bat is tracking from the delay of the echo it sees
the range of the insect so it gets the insect constrained on a sphere in its 3d model
and then from the Doppler shift it actually sees the perceives the cosine of the angle between
its directional movement and the direction of the insect so what it gets there is it
constrains the sphere surface of a sphere down to one circle in space so what the bat gets is a
series of circles in space which constrain the position of the insects and i could show that
on the program if you like sure okay um well how do i do that how do i share again yeah
i'm curious is the circle something like is it like a hoop that you could throw a basketball
through or is it well i'll show you if i'm now i can't get rid of this something on my screen it's
the problem um i've got a big black screen with a two on it and i can't get rid of it
um end show it's where the end show right no
am i sharing my screen or not not yet um now how do i find where i do that how do i find
where i am in space yeah absolutely that's an interesting question also how these mechanisms
google windows somewhere zoom windows somewhere oh zm that'll be it
how are these mechanisms repurposed for digital navigation semantic navigation narrative navigation
uh just a minute let me find oh god get away oh chef screen to chef
okay right you got that we're back okay so what we do is we change from beta bat
and we start again so what you have now is the bat and several insects which are colored
circles here and what the bat has is its echolocation take the blue insect its echolocation
constrains it to a sphere in space and that's the delay of the echolocation and the Doppler shift
can constrain the sphere down to a circle so if we rotate these circles you see that blue
insect is on a circle from one point in the bats trajectory so as the bat moves it gets
successive circles of the same insect so what we have is the bat is moving and all these
circles highly confusing and it gradually locates all the insects better and better
but what if i restart and step again if i step of i've restarted right now if i step
what i can do is focus on one insect if i focus on that insect then i only see the circles from
echolocation of that one insect and i can step again and get a new circle from a new step
and all the time the bat is optimizing the position of that insect to get the best fit to
all the circles it has for that insect so it's very different from vision but it can build
a very good 3d model from its echolocation okay so to kind of confirm what's happening here
there's there's for a given snapshot ping with the sonar what is returned is a circle
of equiprobable locations that are kind of like the maximum likelihood ridge that's right
constraints these are really sort of gaussian's gaussian donuts if you like okay likelihood
and then successive pings enable you to look at the intersection point of the circles
to find out yeah through successive approximation the likeliest location
this is what's happening here on that light blue insect has got these three circles
and that's the most likely they're not very well intersecting but that's the most likely
intersection point that's i go on this bat it's it's like a gaussian mixture model you have those
three that's peaks and then when you summit those three peaks that the the point that interpolates
them or just is the the geometric average is the single maximum likelihood estimate point
that's exactly it yeah and again with the bat i can go from full basian model which is this one
full basian tracking or tracking with noise and again the noise tends to have nasty effect
or i look back at this one here that's that dark blue insect and i can rotate to see how
the circles go so how is this how is this similar or different than for example radar
navigation algorithms for planes or ships i think it's quite similar
i think yeah i mean they are making maximum likelihood inferences from radar signal
and have you with this software looked at the computational
like the runtime complexity or the resource use associated with scaling the number of insects
or scaling the resolution of vision yeah you can scale the number of insects for instance if i scale
up here the 30 insects i can run the model with 30 insects and um it's it runs perfectly well
so the model is quite efficient because what it has to do for each object
um and for each time step it simply has to do and this Gaussian optimization which turns out to be
just a 3d matrix operation three were three matrices and that's very quick to do
and the so that's one of the advantages of it that if animals are trying to
track a load of things around them which they probably are it's quite a quick efficient computation
but this is not the neural implementation the problem is going from here this
mars level 2 computational model to a neural implementation that is where i think all the
interesting problems lie at least you have at this level two model is a kind of starting one
and the basian model would be trying to implement that in your
yes makes sense this is kind of the as if basian algorithmic map and the question is what
proximate mechanisms are capable of doing these algorithms functionally and neuro anatomy has
basically localized to the region in mammals and in invertebrates and so now we're in a space of
winnowing possibilities and leaving the door open for unconventional opportunities
for how those regions actually do it it's a very targeted specific agenda
that that connects a first principles grounding about how well the navigation can proceed
with a requirements equation on through to the empirical patterns that we see
those empirical patterns might have some like behavioral experiments well
sometimes can be hard to interpret as well though for example there's a common experiment
where a wall that's shaped like an L is set up and then people will use it to test if an animal
will go on the hypotenuse to reduce the travel distance or whether it will take the the around
the wall however even if there was a conceptualization of the ability to move on the hypotenuse
direction an animal might like prefer to move along walls so then by the time you get to the
real animals movements it's very tied up with not just trying to be the optimal landmark resolving
visual algorithm it's actually engaging in all these other drives that can make it look like it's
lower efficiency or even like supernaturally efficient on a given domain
yeah i i think there's all sorts of animal experiments what one could do but interpreting them
is not easy but basically if you're looking in these experiments to to to measure how good
the animals internal 3d model is i would like to with insects for instance how good is that
movement detection if it's movement in depth that needs the 3d model to resolve it rather than
just the visual field that's very interesting especially in insects potentially where there's
little overlap between their two compound eyes yeah yes um okay i'll read a question from the chat
as we kind of head towards the end use equal wrote how would perception of ability to conceive
of other domains of time as a cognitive function change spatial awareness
um other dimensions of time i i'm not sure i understand that question other domains of time
domains what's other domains of time um well this is all a very short term
question it's fractions of a second and time outside that into all just doesn't come into it at all
really does that answer the question you think it might i think it might also be pointing to our
awareness of how we handle our perception of time and space and what does that open up or enable
for example for our perception of space if we have a different perception or conception of time
well that is a very deep question i mean i think our perception of chronological time the long
term time is something completely different from more anything else in the animal kingdom
i think most animals live in the moment really they're aware of what's happening right now
and what they got to do right now and the rest just doesn't matter
whereas we exit what matters in the moments to ruminate and speculate yeah absolutely
um well what other directions or or ways are you hoping to take this work
well one way that is particularly important i think although particularly problematic is
theories of consciousness in other words consciousness most of our consciousness
at any moment is consciousness of the space around us and it seems to come from our internal
Bayesian model of space around us so this work is very related to why we are conscious and there's
another paper and i'd like to give another live stream just on that subject so i think this gives
a way forward to a theory of consciousness that can be in many ways more satisfactory than we get
from purely classical neural models and brain that's awesome i'm also personally very excited
on the empirical insect neuroanatomy side to look oh there are a huge number of ways of
going forward i think insects are particularly productive because you know they've got to do it
with a really small number of neurons so it's really there's a lot of experimental work on
insects that can illuminate this question yeah absolutely cool yes the brains are fun to dissect
you can see it transparently all there and no backstage there and then also this on on the more
transferable outside of entomology i think the active inference loop closure with policy
selection of movements including stay go no go decisions and which way to go
and then understanding how like well here's the trajectory that would have been the most
information gain on resolving this location um here's the trajectory that maximized safety
not moving or something and then here's the trajectory that that would have um
done some other thing and then being able to look at realized empirical trajectories
and then break those down or look at their component loading according to safety visual
information gain other kinds of heuristics or or impulses to understand moments or kind of
fragments of trajectories like something looks to briefly resolve its uncertainty and then it
doesn't need then the the overall demand to resolve uncertainty drops again and then it continues
just with inertia and then maybe that's a very simple decision to make and then there's probably
all kinds of cool patterns and ways to go yeah there's a huge amount to investigate
another another topic by the way is we've looked at my most we've looked at insects right at the
officer ends of the spectrum there's a whole load of stuff in between i think octopus and
squid are particularly interesting but there's all the other species one can look at say how do
they do space they have unique bodies and different bodies but there's also bringing in there the
question of underwater or space or fluid media and then that and a bat flies but it has the mammalian
neuroanatomy so then there could be like bird neuroanatomy with its slight differences from
the mammal then there could be this question of underwater and maybe there's some mammals
that have underwater navigation maybe dolphins would be like bats but in another fluid media
i hope people download the dot zip and play around
so do i any last comments uh
not really no i think i said it all thank you thank you robert well i i am greatly enjoying
learning about the work and seeing about how the requirements equation
scopes a given problem setting and kind of puts a meter stick on whatever the inferential problem
is inference plus action problem is and then from there the mar the marian research program
is just kind of laid out you can pursue it from the mechanistic or from the algorithmic elements
but they all are connected through on one hand in theory the requirements equation
and on the other hand the empirical results that we actually see
yeah
cool so thank you again um i come back to the Feynman well Richard Feynman on his
backboard said if you can't build it you don't understand it this is all about building it
we can't build a bug
not yet um thank you robert see you for dot three
bye
