Hello and welcome, everyone. It's January 26th, 2024. We're here in active inference,
MathStream 8.1 with Richard Sarajevan. And we're going to have an interesting presentation
and discussion today on introduction to Bayesian mechanics, free energy principle and the state
based formalism. This is part one. So Richard, thank you for joining. Looking forward to this
presentation and discussion. So to you. Hi, everybody. So yeah, my name is Richard Sarajevan.
I'm French working in Switzerland. I'm a PhD student at EPFL in Lausanne. And just to bring a bit
of context, I'm not working on Bayesian mechanics. We are, we do have a physics background, but we
are interested in modeling bacterial evolution and ecology. And what happened is that something like,
I mean, the free energy principle was always in the corner of my head. And one year and a half ago,
I decided to really read about the free energy principle, especially if I wanted to transition
to the field. And I do want to transition to the field after my PhD. And so I started to ask
many questions to the people from the FEP community. And I'm so grateful. Thanks for them.
And also on the discord of the of the active inference Institute. And at some point, I said that
I was preparing a lab meeting about the free energy principle. And Daniel proposed to have this
discussed on the live stream, because there isn't such material to specifically learn about
Bayesian mechanics and the actual physics underlying the free energy principle.
And so here I am. So once again, I'm not an expert on the matter. So always refer to the
original papers. But hopefully I gonna, I gonna do a decent job. So without further ado, let's,
let's start. I'm not going to tell you what we where we are heading, what questions we would like to
to address or whatever. I'm rather rather going to start building the framework right away.
And at some point, what we're doing, doing will become clear. So
as you may know, there are two formulations or formalisms
of the free energy principles, the so called state based formulation, and the so called
path based formulation. So today, we will focus on the state based formalism. It's not like the old
versus the new formulation. In fact, thinking in terms of path, or so called generalized
coordinates of motion, I've been around forever, but in the literature, but it kind of came back
to the front scene of the Bayesian mechanics literature, I think. Anyway, today, we will
focus on the state based form, formalism. So the very starting point is to write down
Langevin equation, a generic Langevin equation. So it's literally like saying, let's consider
a random dynamical system. Very briefly for the people not acquainted with such an equation,
x here is the state of your system. So it could be a simple scalar if you are
considering a one dimensional process. But in general, x would be a vector. For instance,
if I don't know, you want to, to model the 3d diffusion of a Brownian particle immersed in
a liquid, x would be a 3d vector was components are the coordinates of your Brownian particle.
And you can see on the left hand side that we have dx over dt, the time derivative of the state
vector. So that's such an equation really describes or specifies the dynamics of the system.
So many things can influence indeed the dynamics of the system. If I stick to my
Brownian particle example, maybe it is subject to an external force. So whatever is relevant here,
you put it in F, the so-called deterministic term or flow, we will refer to it as the flow
for the presentation. However, in some cases, there is stuff you don't want to explicitly model.
For instance, if I stick with my Brownian particle example, it is constantly hit by
the molecules of the medium surrounding it. Hence it's a Brownian motion, right?
And it would be so if you want to take into account this thermal fluctuations,
it would be mission impossible to explicitly model every single molecule of the millions,
if not billions of the molecules surrounding it. So a convenient way to still take into account
these fluctuations, which are literally thermal fluctuations in my example, a convenient way
to proceed is just to add a noisy term to the equation. So omega here is a random variable
was value changes with time with the appropriate statistics. Okay, so two brief remarks before
moving on. If you assume that the state of your system changes slowly compared to the time relaxation
of your fluctuations, you can write the autocorrelation function of the
noise like that, where gamma is the diffusion matrix and delta is the delta direct function.
So what it means, it's just that in that case, your noise is super rough and it's not correlated
in time basically. Also, second remark, you can, I mean, you can use the central limit theorem
to argue that it makes sense to assume that omega is normally distributed. So that in the end,
the noise is a Gaussian white noise, but not that in the next live stream where we will
discuss the path based formulation of the FEP, we will relax the white noise assumption.
Anyway, so we have this random dynamical system. And we can do something cool with the flow. So
the flow F is a vector, it has the same dimension than the state vector, because each component
of the state vector has its own longitudinal equation, if you will. And you can decompose it
into a solenoidal and a gradient terms. So before telling you what this decomposition is all about,
on a technical note, just notice that first Q here is the so-called solenoidal matrix. Gamma
is the diffusion matrix just as before. And the I here with the nabla I, this I of X here,
is a negative log of a density. So it's a self-information or surprise we will refer to
it as the surprise throughout the presentation. And the density at play here in this negative
log density is the steady state or nest for non-equilibrium steady state density of the system.
So we assume that there is such nest density that exists, so that if you from a given initial
initial state you let your system evolve, it will reach at some point a unique well-defined
nest density. And second remark before telling you what this decomposition is all about,
note that usually in the papers the divergent terms here and here are put together in a third term
which is sometimes called housekeeping or correction term. But actually if Q and gamma
are not state dependent, these divergent terms vanish anyway and we end up with these the two
remaining terms which can be nicely factorized like that. Also a last thing I want to say is that
if you you consider the solenoidal term, the first term, it is indeed a solenoidal term,
you can indeed write it as the rotational of some potential. I'm saying that because sometimes
people get confused when they see a gradient in both terms. Anyway, what this decomposition is all
about is quite in fact simple. Let's consider this nice 2D single-moded nest density. Okay, so the flow
and more specifically the gradient component of the flow which is here the vertical flow will drive
the system towards its mode while fluctuations kind of push it away. But it's not the only flow,
there is also the solenoidal flow which is here called the horizontal flow which
will make the system kind of converge to its mode with ever-decreasing cycles. And so if you want to
get some more intuitions on this solenoidal flow what we can do is to remove the fluctuations.
So all the entries of gamma, the diffusion matrix go to zero and this means that we would not have
any gradient flow anymore. We end up with only the solenoidal flow and if we do that
the system will just follow an isocontour circulation on the nest density that's the
bottom right panel here where the solenoidal flow kind of drives the system on this circulation here.
So a small remark about this solenoidal flow because it kind of drives the system in this simple
example in either clockwise or anti-clockwise direction in an irreversible fashion, irreversible
in the statistical physics sense. So it breaks detail balance and so on. People sometimes view
this solenoidal flow as underwriting the symmetry breaking ubiquitous in living systems. Anyway,
okay so before using this decomposition of the flow to do some cool stuff I need to introduce
some stuff. So I will have to go through a couple of things of notions one after the other
and afterwards we will put everything together and actually derive the free energy principle.
So the first thing I want to introduce is the notion of sparse coupling. So let's say that
in my state vector x here I have a subset of variables this mu here we refer to as the
internal states and they specify the state of some subsystem called mu. So I mean you get the idea
that we have like an organism, an agent, the bacteria in my schematic and these variables here
literally specifies the internal states of my bacteria and this bacteria is in a given
environment, niche, whatever. So there is this other subsets of variable we refer to as the
external states and which corresponds to the external world, the external states of the
bacteria. And the idea here is that these two subsystems are not connected to each other. So
when I'm saying that two variables are not connected to each other I just mean that
their respective flows do not take the other one state as arguments so they do not influence
each other basically. In fact they are indirectly connected to each other thanks
or through a third subsystem we refer to as the marcov blanket so that these guys here
are called the blanket states and we will see in a minute that it really corresponds to a
marcov blanket in a statistical sense. Okay so we have this architecture, this
path coping architecture here and in fact we can even go a bit further and assume that
within the blanket there are two more systems, the so-called sensory states and the so-called
active states A. So basically the idea here is that the external states eta they influence the
sensory states S and these sensory states S influence the internal states mu but not the
other way around and the internal states mu they influence the active states A which influence
the external states eta but not the other way around. So it's really a path coping architecture
inspired by the so-called action perception loop however you could ask questions like why do the
sensory states influence the external states or why do the active states influence the internal
states etc. So we don't have really time to discuss this I guess you can think of some
qualitative example in biology but I just want to point out that even though this architecture is
quite canonical it's not a definitive feature of the free-nury principle and in fact in the next
time when we will discuss the other formalism we will do a bit of zoology and we will look at
other path coping architecture. Okay so on a technical note just notice that such
path couplings are encoded by zero entries in the Jacobian matrix of the flow.
Anyway so thanks to this path coupling architecture we have this system of four
coupled long-run equations which respectively describes or specifies
the dynamics of the external states eta the sensory and active states s and a and
of the internal states mu. Okay so I want to say here about the Markov-Blanket thing
that under some conditions I'm not going to discuss here so for the people acquainted it
involves having no solenoidal couplings between autonomous and not autonomous states but anyway
I'm not going to go into this let's say under some conditions the external states eta and the
internal states mu are conditional independent so they are independent when conditioned upon b
which makes sense because all the informations kind of transit through b however not that when
I'm talking about conditional independences here I'm talking about conditional independences
in the stationary density so basically if you fix p and you have this joint
uh conditional stationary density here for x i and x j if these two guys are conditional
independent it just means that you can write this joint density like that and so that such
conditional uh independences are encoded by zero entries in the ACN matrix of surprizo.
Okay so now just a bit more of vocabulary before moving on um note that if we
put together a and mu so we consider the couple active states and internal states
we refer to to these guys are as the autonomous states alpha and the cool thing about the
autonomous states a and mu or alpha is that they are conditionally independent of the external
states so autonomous and external states are independent when conditioned upon uh sensory
states and if you add the sensory state s to the autonomous states so you consider the whole thing
the whole marco blanket and the act and the internal states we refer to to these guys as the
particular states pi and pi constitutes a particle a particle in a generic sense of course
so an organism an agent whatever a bacteria in my schematic okay so here I just want to make a
point to make a bit more clear what we are doing what what this this approach is all about
so basically here we kind of define what it means for something a bacteria any whatever to exist in
the sense that it has its own internal dynamics statistically separated from the the external
it does have a marco blanket it does have its own physical integrity so we have no clue of how it
maintains indeed it's it's uh it's integrity in the sense that if you're considering uh real systems
like like an like an actual bacteria or a human being or whatever it it does survive at a given
in a given time scale um right for instance this playing I don't know like active processes
contouring dissipation for instance here we don't say anything about how it does survive it just
does we do have this path coupling architecture and from there from the starting point we are going
to derive the the necessary uh the consequences of such sparse coupling so basically we kind of ask
or answer to or try to answer the questions if things exist what must they do and so if you're
a bit confused don't worry we're going to go back to this idea later but I just want first to show
you this quote here which tells you many theories in the biological sciences are answers to the
question what must things do in order to exist the FEP turned this question on its head and
asks if things exist what must they do but once again we are going to go back to this idea later
but that's kind of the idea of this approach in a nutshell so as I told you I still have a couple
of things to present so I will have to go through each of them one after the other and finally we
will put everything together and finally derive the free energy principle so the next thing I need
to introduce is the notion of synchronization map so very uh very generally speaking I'm not
specifically here talking about our random dynamical system if you have a linear map
uh g mu here which gives you mu from b and g eta here which gives you eta from b then
if g mu here is injected so that basically you can go back to the pre-image from the image
you can use the pseudo inverse of g mu so that from mu you go back to b and from b you can go back
to you can go to eta so the successive application of the pseudo inverse of g mu and then of g eta
is called the synchronization map and it basically allows you to directly go to eta from mu
okay so now let's try to uh to use this idea in the context of our system so b here corresponds to
the blanket states so if I fix the blanket states I have a corresponding conditional
densities for mu and eta I have p of mu given b and p of eta given b and I call and what I
I am and their their modes are bold mu and bold eta so in virtue of this synchronization map
I can go back to the external mode from the internal mode thanks to once again this
synchronization map here and I'm going to give an example in a sec which is going to to make
to clarify a bit more what we are doing here but first I just want to say that
in this nice paper by Lenz Dacosta about this this this synchronization map
basically everything was Gaussian but sometimes I mean if it is not the case a Laplace approximation
which which is literally a Gaussian approximation might be necessary to derive a synchronization
map of closed form but don't worry we'll go back to this idea of Laplace approximation later just
remember that we have this synchronization map here which allows you to go to the external map
external mode sorry from the internal mode so for instance if given b given the blanket states
the corresponding p of eta given b follows this nice normal distribution where bold
bold eta here corresponds to the mode then in virtue of the synchronization map I can view
the internal mode mu as parameterizing a density I write it that way q mu which is equal to this
nice normal distribution where the mode is just the synchronization map applied to the internal
mode to itself and by construction of the synchronization map it is equal to the true
external density so you can view the internal mode as parameterizing a distribution over
external states basically thanks to the synchronization map that's why what the synchronization map
is all about so just a small point because maybe some of you are a bit confused here because we're
talking about modes as opposed to actual states so we will talk about that later but indeed I mean
if I take the actual internal states at a given time t they are not necessarily equal to their modes
just because of fluctuations or whatever so that if I apply the synchronization map on the actual
internal states it might not give you the true external mode but anyway we will discuss this
a bit more later so that was the notion of synchronization map in a nutshell basically
so last thing I want to to introduce before finally putting everything together and actually
derive the free energy principle is the notion of variational inference so very simply let's say
that you have some latent variables or hidden variable or some latent generative process
causing some data s so you have a prior p of eta over the state of these hidden causes of data
and you are also equipped with a generative model which just designates this joint distribution
here p of eta and s so you can view it as a model of how the latent variables cause the data
so the idea here is the following you sample some data s and you want to compute the posterior
distribution p of eta given s so in a way you want to refine your belief about the
the hidden cause of data thanks to a new sample data so it's very simple you in principle because
you just have to apply base theorem right however in practical settings the denominator here p of s
so the marginal density over sensory data usually requires a monstrous marginalization so it's just
not tractable so we can't just apply a base theorem so we need we need a method uh which
given some variational sorry which given some some variational distribution q also called recognition
density uh gives us uh i mean we want a method that makes it as close as possible if not equal
to the true distribution we want ultimately to compute namely p of eta given s and these two
density so q our variational distribution and the true distribution p of eta given s are equal or
more or less equal if their divergence cal divergence here is zero because this quantity here the cal
divergence basically measure the difference between two distributions so that's what i wrote here on
the top of the slides finding an accurate distribution q in the sense of finding a q as close as possible
if not equal to the true target density is equal to minimize to minimizing these divergence
however these divergence i mean there is the target density appearing here we can't
do anything directly with it we can't compute it or whatever we need a proxy for this target
divergence and there in the proxy called variational free energy f in green in my in my uh slide here
so f is is equal to this divergence here between q and the generative model and the idea here is
that you can decompose these divergence into the true the target sorry into the target divergence
in red here plus something so it is indeed a proxy for the target divergence
and note that interestingly enough the second term here is the surprise over sensory data or
negative log p of s so that f is it can be viewed as an upper bound or lower bound depending on how
you define it uh unsurprise okay so what i just said here is that minimizing the target
divergence just means minimizing f so that's basically what variational inference uh is all
about and note that usually algorithms require q to be Gaussian or require a mean field approximation
or whatever and if q is required to be Gaussian even though the target density is not Gaussian
we would end up with the best Gaussian approximation of the target density basically
and in practice it would mean working with a so-called Laplace encoded free energy
okay so before moving on i just want to say that uh this quantity the variational free energy
is in itself a quite rich and interesting quantity so you can decompose it in many ways
and each decompose uh provide interesting uh interpretations for instance if you look at
the second line here you can see that minimizing free free energy means
maximizing this accuracy term here you basically want to explain the data i would say but at the
same time you you want q to differ the least possible from a prior distribution so that's
that's um an interesting quantity anyway now let's finally go back to our sparsely coupled
random dynamica system and use everything we we talked about and finally let's uh derive the
free energy principle so here is our system and we have this four langen equations and the first
thing to do is just to apply the decomposition we talked about in the beginning so basically
the flows of each of them can be written like that so i just directly applied the Helmholtz
decomposition we talked about in the beginning okay so now um let's try to understand uh how
it works let's talk about the dynamics of the system let's say that uh so there is a momentary
instantiated uh uh sensory state and let's fix let's say that the sensory states are fixed
and there is a corresponding autonomous density uh autonomous mode toward which the autonomous
states are going to converge and stay in the vicinity of the of their mode in the closed
vicinity if uh fluctuations are not too large okay but in fact sensory states with time changes
so that the the mode of the autonomous state move as well and in fact it it moves on its
corresponding autonomous manifold so i'm not going to go into the details but just
have in mind that the autonomous mode moves on a so-called autonomous manifold which can
be viewed as a statistical manifold and which can also be viewed as a so-called center or
center manifold so if i kind of rephrase what i am saying here is that the flow of the of the
autonomous states can be decomposed into uh off manifold flow and uh on manifold flow which
corresponds to the path of the mode itself on the manifold okay so just to be a bit more clear
let's say in my bottom right uh illustration diagram here the autonomous states are here
and i'm i'm interested in the the off manifold flow so basically i have the this component here
which corresponds to the gradient flow towards the manifold towards the mode basically here it's
pretty much like what we discussed in the beginning and at the same time there is here this orthogonal
component which corresponds to the solenoidal flow so that's basically the way the autonomous state
are going to reach their mode here it can be viewed as this ever decreasing cycle towards
the the manifold on which the autonomous mode move okay so that's a bit dense i guess so
i recommend to check the paper the free energy principle made simpler but not too simple which
kind of discuss all this this idea about center manifolds and stuff so here the interesting point
is that if you assume a separation of timescale between the fast flow of the manifold as opposed
to the slow flow on the manifold basically the autonomous state always are always in the vicinity
of their modes and if you want to characterize the overall dynamics of the autonomous states
you can focus on the autonomous mode on the path of the mode and in the next slides we will indeed
focus on the autonomous mode and and by definition as we already discussed the autonomous mode
the autonomous mode is or corresponds to the autonomous states which minimize
surprise here in the last two launch variations because the autonomous mode corresponds to the
least surprise of autonomous states before moving on i just want to say something
we can maybe discuss afterwards because i'm not sure to fully understand but basically
if i'm here in my bottom right schematic and so i have this gradient flow towards the manifold and
this solenoidal flow parallel to the manifold and if i remove fluctuations so all the corresponding
entries in the diffusion matrix go to zero as we saw in the beginning it means that there is no
gradient component anymore and what the system will be doing is kind of
um orbiting or oscillating around a point which is which moves on the manifold so that's
interesting and i guess that if we do the exact same reasoning but starting already on the mode
then the world flow reduces to the unmanifold flow and i guess that in that case the autonomous
states follow and in fact coincide with their mode but anyway maybe we we can discuss about that
afterwards so okay so let's use the various things we talked about and especially the notion of
synchronization map we as we said the internal mode parametrize indeed um a distribution over
the external state so mu here parametrize a distribution which by construction coincides
with the true distribution p of eta given b and in fact thanks to the conditional independence
between external states and autonomous states you can just drop the condition upon a and you
just have q mu equal p of eta given f and equivalently you can you can write it p of eta given pi
and the idea here is that you can view q mu as a variational distribution if you want you can
write its associated variational free energy so you have this this formula here the free energy
and because q mu is already already coincide with the true posterior distribution if you will
the first term here goes to zero and so that f here reduces if you will to
the surprise over particular states and surprise over particular states they appear here in the
equations of the autonomous states so we can do this identification and we realize that the
autonomous uh mode not only minimize um not only minimize surprise though but free energy in general
and the way uh mu the internal states will be updated when the sensory states will change
will always be so that this divergence here is zero so that mu is always always keeps track or
synchronized with or in fact interfere the external states so that you can interpret that
under a generative model which is here p the next entity the internal states can be viewed as
performing inference over external states and so in fact it's not only this divergence which is
minimized but it's also surprised and it's not only um only the internal states which uh minimize
free energy but also the active states so let me give an example let's say that the actual
instantiated sensory states are likely sensory states or unsurprising sensory states and by
definition in general the instantiated sensory states will be likely sensory states so mu will
will the corresponding mu will be so that this divergence will be zero as we just discussed
and at the same times the corresponding active mode will be so you can see in composition with the
third term here i of a given s and mu a this active mode will just be the one the most consistent
with this in intensiated sensory states and in fact you can view it the other way around and say that
the active mode is the mode which yield unsurprising sensory states so that the particle can be viewed
viewed as uh actively sampling unsurprising or likely sensory states or equivalently you can
say that the particle kind of um accumulate evidence for its own generative model and i'm going to
say something about the generative model in a sec but i just want first to so yeah this sentence
here just sum up what we said mu is updated so that q mu is always the the best distribution of
our external states and we refer to this as perceptual inference and the idea to in addition
trying to minimize surprise for action is called active inference so a brief note
uh we said earlier that in order to have a synchronization map of closed form it could
be necessary to work under a Laplace approximation so that in that case q mu is always is is just
the best Gaussian for instance of the target density so that the divergence here would not
be zero but it still would be minimized so that the identification here between the two gradients
still hold and nothing change um nothing changes with respect to our discussion so here i just want
to say something about this what we are doing here basically we assume that we have our agents
or organisms that survives indeed exist or persist in a given environment let's say at a
given time scale and we end up with the fact that our particle must be equipped with or must be
must embody a generative model which may or may not exactly coincide with the true generative process
and which encodes the causal structure of the world under which it tries to perform inference
and to minimize surprise to perform perceptual and active inference but the interesting thing
as well is that and i think that's something fundamental that people tend to misunderstood
i guess maybe i'm not sure is that the generative model also encodes the preferences of the system
and let me explain why if i tell you that an organism manages to survive to exist to persist
etc and so it means that such an organism manages to stay in its homeostatic life compatible
states you would be of course it almost sounds like a tautology survive equal
staying in it in in its homeostatic states that obvious right and that's exactly what we are doing
here we assume existence survival so that the likely state in which the particle will will persist
are preferred states per se so that for instance if i'm considering the prior of my generative model
over sensory inputs p of s sensory outcomes s associated with high p of s so likely or
unsurprising sensory states are preferred sensory states states hence when i'm saying that the active
states try to sample unsurprising sensory states it means trying to sample preferred sensory states
and so basically the particle appears to kind of actively accumulate evidence
for its own existence in a way it kind of sample life to sample it kind of sample life compatible
data if you will and that's exactly the definition of self-evident thing so i think
we touch here something fundamental about agency is that agents are self-evident thing
creatures in that sense okay anyway so basically i think that's the most interesting things of
the free energy principle we start from existence and we end up that such a particle which is
coupled to the world in that way must embody a generative model which encodes the causal structure
of the world and which encodes the its preferences in terms of what is life compatible if you will
okay so just to sum up what we did here this idea that free energy is minimized
you can write it that way and this is in a way a variational principle for self organization
that's a free energy principle so here i just wrote what we just discussed the agent keeps
tracks and acts on its external milieu through perceptual and active inference
and note that interestingly enough you can write such a principle as a principle of
least or stationary action where the Lagrangian which is constantly minimized along the path
is variational free energy so here are some concluding remarks i'm not going to throw all
of them but the first one is basically what we just discussed this idea that the generative model
encodes preferences if an agent maintains existence its likely states are its preferred ones per se
hence the notion of stealthily dancing and i just also want to point out that this new approach
or chapter of physics let's say consisting in describing physical systems as encoding probabilistic
beliefs is called Bayesian mechanics okay so having said that thank you very much and especially
thanks to all these guys who who helped me so much especially Len and yeah thank you for
for your your attention
I'm back thank you Richard
okay well while we're settling back in and anyone is asking questions in a live stream
what is your phd research and if this is your side project what is your main project that this
kind of relates to yes so well in fact um i kind of read about the free energy principle
in my free time whenever i i had some time and what i'm doing in my phd is so we have
a couple of projects the first project we did was really modeling bacterial evolution through
so basically we model bacterial evolution as a bias random work on genotype space with
successive mutations and and and successful fixations so that's what we are doing it's
not related to the fp at all and the second thing we have been doing is modeling so basically we
had a system where you have bacteria which can kill each other thanks to a system which is called
the t6 secretion system they kind of have needles with which they can go through the membrane of
other bacteria and and liberate toxins and they can also bind to each other so there is like a
prepredator kind of dynamics and we did like a lattice gas modeling of such systems so basically
that's what i'm doing in my phd which is not related to to Bayesian mechanics but i would like to
to transition to to the field afterwards so yeah i will see how it it goes
i remember when i thought my phd wasn't related to active inference
okay cool well the work built to an amazing crescendo that in its simplicity
even though you highlighted it's easy to fly by which is the coincidence of the preferences
and the expectations so could you maybe give a little context how else has that nexus
of preference and expectation been approached and is the fvp only and simply and always that
coincidence is that coincidence upstream or downstream of some other commitment that we make
like what are the commitments that we really make and is that um alignment the commitment or a
resulting commitment yeah so um so first of all i think the notion of um self evidencing
may be a bit refined with the next formulation but anyway it's i think that's a crucial point
about the fvp and usually it's kind of confusing because when you're reading the papers and
people are starting to write that the system um sample evidence for its own existence you're like
what i mean i'm not sure to understand what's going on here um but in fact yeah it's i i think
the way i i um introduced it this idea that by definition um a living thing is a thing which
which managed to sample live compatible uh sensory data is really what allows this
um align alignment story between the that the idea that between surprise and preferences
basically and this idea that actively sampling um unsurprising data is in fact and it's not
like a tricky wording it's in in a way that's really what's happening it is sampling uh live
compatible or preferred in that sense data hence the notion of self evidencing um but um
yeah i think the whole idea here is that we start from existence we start from the uh
from this past coupling architecture where the particle uh managed to maintain its physical
integrity managed to display a mark of blanket which uh allows the the agent to have its nice
it's um its own internal dynamics separated from the external so somehow it managed to
counter dissipation or whatever and so from there likely states are states consistent with the fact
that it is existing existing indeed so i think that's basically the the the idea but yeah in the
beginning this kind of um line of reasoning can be a bit uh confusing but in fact i think that's
very much what the FEP is all about and actually last remark um in a machine learning street talk
interview of Maxwell Ramstead he it was titled the FEP as um a physics of survival if i remember
well and i think that's that's very very much what what it is all about in a way
awesome how would you relate what you just described to reward or to reinforcement type
learning schemes yeah so i i mean i'm not an expert at all i could not uh make the bridge here
but i know that um Lance Dacosta made several uh works and interviews about the the subject
and actually i think there is a very new paper called active inference as a model of agency
you just shared actually today um so i yeah i recommend the viewers to to check them out
and as far as i know but here i'm just i'm just uh seeing what i heard is that um any
reinforcement learning algorithms can be um can be framed in terms of active inference
so i think active inference is a very um uh fundamental scheme but yeah yeah it's all good
like the reason i ask just with how you presented it is what kind of observations do we want to
sample that could be the sensory embodied interface between the agent and the environment
or you can take a more cognitivist approach and sample internal observations but those are just
external some other internal so what do we want to really sample well if you're even in a position
where you're talking about sampling from like a utility or a reward distribution you've already
specified a distribution why not just specify the existence distribution the actual attractors
and stationaries of the measurements and then um it's simpler because there's no proposal of a
secondary intermediate between the temperature and how good different temperatures are by going
and just saying it's not rewarding to be at 37 homeostatic temperature it's just expected and
likely and the ball rims downhill it's actually a lot simpler and more general yeah and i i think that
um it's way more simpler to i mean the idea here is that the agent has a kind of world model which
as you said uh specified what are the the expectation uh with regards to just existing
in a way and as opposed to designing explicitly
objective functions
with the which incorporates the notions of utility and so on so yeah i'm very much agree
um earlier when we were looking at the flows and we had the breakdown of a flow um could you maybe just
um what animal are you thinking about or what scenario can help us understand like what's the
solid black line what's the small red line what's the spiral what's like a physiological setting
that we could associate here to help us understand that kind of complex movement
yeah so uh generally speaking the first thing i could say is that this notion of solenoidal flow
so it's like in the schematic schematic in the first slide where you have you had this
either contour circulation on the next entity or here the the the component of the flow which
creates this sort of spiral here it can so that um it's this sort of um oscillations are i think
the sort of oscillations or cycles that are ubiquitous in living systems um i mean i i'm not
a biologist but you can uh or not really a biologist but you can think of the circadian
cycle or or anything in any sort of systems there is this sort of of of um attractor where
you're circulating along and so here specifically to this to this um a schematic here i think the
idea is that um um you have so you have you basically let's say that uh for a given sensory
states you have a corresponding autonomous mode and the when the sensory states change the
autonomous mode mode changes as well and in fact move on its so-called manifold so basically i guess
here you have the mode moving on its manifold and now if we take the perspective of this
autonomous states here we converge to the the manifold to the mode
um and because of the solenoidal uh component of this flow the way we will um reach it is
with this kind of uh ever-decreasing cycles um so here's the idea and i and um i really recommend
here the free energy principle simple paper you have the the flow on the manifold it's just the
path of the mode itself let's say and you have the flow of the manifold was gradient component
is the flow towards the manifold in fact um so basically so that's basically how autonomous states
kind of um reacts to to to sensory data which change the autonomous mode and i think the whole
an important idea here is to assume that the flow of the manifold is fast as opposed to the flow
on the manifold so that's basically the sensory states are always uh in the vicinity of their
mode and move with their mode and um sorry and um and um and yeah i think that's pretty much the the
idea here okay so let's just say that the black line is um our homeostatic body existence life
compatible ph oxygen blood sugar and yeah we are that light blue dot that's off that manifold
of course if we were far enough off to be dead it would be a moot question but we're off but
within a life um scaffolding a compatible zone and now as time pushes us down into the right
um there are different slices that we can trace um we could take the shortest path the gradient
flow directly towards the manifold so as that plays out through time it would look like a linear line
converging to the thick black line or pure solenoidal flow would just stay equally far away
from the thick black line and continue to spiral so that would look like a cork screw uh through
time and then here when you have the combined character of the linearized convergence towards
the manifold and the cork screw out through time we get this kind of winding spiral so
it reflects on me that the gradient flow is pragmatic value in that it aligns future observations
with preferences and the solenoidal flow has an almost epistemic character in that it circulates
amongst a set of equally valid outcomes yet here we're not looking at the pragmatic plus
epistemic decomposition of the expected free energy policy selection strategy like equation 2.6
in the 2022 textbook so is that just a concordance or where do you see some of those topics connecting
um I am not sure maybe uh but having said that on this on the the meaning of the solenoidal part
here I know that on the on the uh I don't remember if it's in the free energy principle
simpler paper or or someone else but there is an analogy I mean they discuss the the meaning
and the role of the solenoidal flow where they say that it it it kind of help um it kind of helps
mixing system the systems and you can view and they discuss the metaphor with where you want to
dilute your um your um your coffee for instance and you're going to have this sort of uh motion
in order to reach the the as fast as possible the the steady state where everything is diluted but I
I am I'm not sure I didn't think enough myself to provide any sort of interesting insight
oh good just to have composed it it's very insightful um well you made choices assembling
things like what do you feel like would have been background maybe a course or a skill what
background do you feel like you kind of conditioned upon that somebody might want to check out
and then what do you feel like you would have wanted to include in the state-based formalism
um because to to bring it into a under one hour timing is very concise so where do you feel like
somebody could fill in some background to pick up with you at the beginning and then what else
do you think would make a fuller presentation I think I mean there are a few aspects and details
I didn't really uh like fully discuss um well first of all all these um things which
here which involves like center theory a center manifold theory and stuff like that
uh we we kind of played uh qualitatively with it we didn't really go into that
and also if we want to be like full really full formally speaking um let's see maybe um
um uh well there are a couple of things where we that we kind of accept without really checking
all the assumptions and all the derivation derivation and I'm especially thinking of the
of the Helmholtz Hau decomposition of F because of course you need a steady state
net density to exist to in order to have such a decomposition so here I think it's it's there is
a lot of stuff to to check and I mean there is a nice I think it's in the appendix B of the
Bayesian mechanics of stationary process paper by Lenz where it derives the Helmholtz decomposition
uh so yeah there are quite a few things we kind of state we vote um derive so it can
if people are interested in in going further I think that's kind of interesting formal uh directions
um and um um yeah cool I think it'll be a really fun collaborative project to
axiomatize and formalize and modularize using the actin fontology and understand a lot of these um
relationships and then the other piece that that made me think about is like
what work is any of this math doing at all just kind of like the the ultimate existential question
here um and when we condition upon existence we've kind of like off sourced a lot of cognition
we don't need to make the jump or the walk or the miracle from axiom to embodied existence or to
even measured hypothetical existence so that is left unaddressed the margin was not big enough
but it wasn't even addressed and maybe there are even advantages to leaving the um
um what happens before the conditioning
you don't want to take it with you after you condition upon it that's the whole mark off
like a concept like if you're like well I'm conditioning on five years ago in the present
but also I'm carrying five years with me today well then it's like well then it wasn't conditioned
upon so to really condition upon measurements is an extremely radically simplifying maneuver
that may change the scope or the applicability of the framework
relative to a conception in which what the free energy principle does is describe how things come
to be however yeah this rather conditioning upon it opens up that discussion and more circumscribes
this very analytically tractable setting of the agent and the environment across a conditional
interface yeah by the way about about the conditional thing there is now the notion of
you know weak mark of blankets that Dalton introduced which kind of lose the approach let's say
and because indeed there is a question on I mean does it apart from the formal setting we have here
can we really apply it to real systems and stuff like that and also I think it's the physics of
survival in itself at a given survive at a given time scale there is at if I have at a given time
scale we survive indeed in the sense that there is indeed this partition or conditional independence
between the internal and external here is the physics you have to comply with but we didn't
tell you tells you how was the mark of blanket rise or whatever it's it's it's it's just not what it
is designed to to explain but I think generally speaking it's it's really informative because
for instance if you are considering the I mean the sort of approach in general I mean for instance
if you consider the pendulum effect where you put pendulum oscillating on the table and they are
going to synchronize synchronize with each other and I think that Kuiha Isomura did a paper about
that recently in order to understand what is going on and why the pendulum synchronized at some point
you just have to recycle all this line of reasoning with the synchronization map that's very what is
at play and what explains why the pendulum synchronized when they are both on the same table
so I think it really it is really informative to in order to understand what is going on when we
are talking about synchronization phenomena across sparsely coupled systems and also it gives you
I guess the sort of recipe to understand what it what it what it takes to be an agent if you want
to design your an intelligence system and but yeah the question of how much useful it is
beyond the fact that it's just some nice formal framework it's it's an interesting
interesting discussion yeah and I just two things first I would like to go back to your
previous question about what sort of things could be could be discussed further I think
an interesting point we didn't really discuss fully is the notion of synchronization map
because we didn't necessarily discuss the the hypothesis and stuff about the synchronization
map and I in fact I think there is much things that can be said for instance because we assume
injectivity thanks to the rank nullity theorem it's kind of constrain the dimension of the
internal manifold here with respect to the blanket manifold here and it kinds of constrain
in order to have injectivity thanks to the rank nullity theorem and so it kind of constrain the
the in order to say it in a qualitative fashion it kind of constrain the the complexity or
richness of the internal states which speaks nicely to other frameworks like
like HB's laws of requisite variability where you want the regulator system to be as
as sophisticated or as rich to the regulated systems and here you need the internal states to
be enough complex to or to constitute the sufficient statistics let's say to parametrize
to be able to parametrize the density indeed and this and this richness let's say is constrained by
the the cardinality of your sensory channels if you will because basically you need the internal
manifold to be to have the same dimension than the blanket manifold or the sensory manifold to be
to have the same dimensions than the autonomous manifold so I I mean I think there is many things
to discuss about this this aspect here and the last thing I would like to say about your
about your last question about the applicability of the framework and how much it's useful
as opposed to be a simple elegant formal framework I think so you know there is this
these papers about about like the Markov-Blanquet trick and stuff like that
about how much difficulties like to identify what states corresponds to the Markov-Blanquet or whatever
and I'm personally I'm not really convinced by this these critiques because to me it's like
to me it's like saying to Newton yeah I mean it's I'm not sure that I can do anything with your
framework it's it's complicated if not impossible to model systems with clearly identified and
separated rows and masses let's say okay fine but we are talking about Newton mechanics here so
I mean I think it's the same here it's if you have a sparsic coupled random domain because
systems that's the sort of behavior it will display it tells you fundamental things about
the nature of living systems and the idea that when it comes to a specific system it can be
quite tricky to to model it that's another question
and indeed when it comes to the art of modeling complex systems it's it's it's interesting and
and we can discuss about how much complicated it can be to apply the framework yeah awesome I love
that it's like the art of the science and the art of the modeling and the and the craft especially
in the kind of early hand-built largely custom stage like one thing I even wondered looking
through these slides what fraction of these representations and formalisms exist only
analytically and do or is there a code representation of this exact scenario
or you know are some of these areas equations that don't have
code realizations they're just pure existing equations
so I mean I think more or less everything here can be can be simulated even this synchronization
thing here you can perform simulations where you can really literally see within the simulations
the synchronization and I mean the the the whole thing here can be you can simulate such
sparsic coupled random dynamic systems and and kind of interprets the dynamics indeed as
the way we we frame we frame it
but but yeah that's also an interesting aspect it could be cool like in the github repo
in the journal for this transcript or something like that to to curate together
the simulations that demonstrate or a minimal specification for it
you know because it's it's actually there is a great yeah and actually there is a
I mean I think it's in the in in length paper about synchronization map the Bayesian mechanics
of stationary process processes paper there there are some simulations where he shows that
I mean he shows the the synchronization map at play and it shows that
basically you can't go back to I mean if the the the map between the blanket states to the
internet states is not injective uh and you apply the synchronization map to the actual
sensory uh to the actual internet states it gives you like some not relevant things and
there are some nice plots from simulations so that's definitely a paper to to check out
so where do we land and then how do we leap exercise relax to prepare for part two
yeah so I think here's the the world point was I mean the this world formulation is in a way
about the momentary the short day the short term and the momentary response to autonomous states to
sensory stimuli let's say if there is this uh this um I mean the the kind of instantiated active
states are so that it comes whatever but in the next uh video where we will look at the path
based formulation of the framework the world idea would be to ask what about path and what about
future path and what about the long term behavior uh and what about planning what about higher order
cognitive abilities um and we we will kind of extend the scope of what we are doing in in in
that sense um but um yeah I mean I think from a formal point of view uh here I kind of introduced
many things variational variational inference synchronization map etc one after the other
before actually deriving the free energy principle next time I think we will it will
be more straightforward but the the main concepts to to which will be at the core of the of the
framework and which can be confusing if it's the first time you you look at it is the notion of
of generalized coordinates of motion when you relax the white noise assumption and that's
something that can be confusing especially for the physicists because when you're starting
saying yeah the generalized lagrangian he plays a role of an action or whatever they are like
no but lagrangian is not an action what are you talking about etc but when you get acquainted with
the the world construction is very elegant but that definitely something people can
started look at before prior to the the live stream yeah awesome yeah well it was excellent
you you brought a lot together and a lot of trails leading off this trail and the citations and
previous um papers that that also brought things together lance's work and others
and it's going to be awesome to see part two so thank you Richard yeah thank you very much
Daniel thank you all right see ya bye bye
you
