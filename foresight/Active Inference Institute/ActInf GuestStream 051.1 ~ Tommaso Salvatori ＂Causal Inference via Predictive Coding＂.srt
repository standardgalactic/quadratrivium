1
00:00:00,000 --> 00:00:21,480
Hello and welcome.

2
00:00:21,480 --> 00:00:28,420
It's active inference gas stream number 51.1 on July 28th, 2023.

3
00:00:28,420 --> 00:00:34,520
We are here with Tomaso Salvatore and we will be having a presentation and a discussion

4
00:00:34,520 --> 00:00:39,420
on the recent work, causal inference via predictive coding.

5
00:00:39,420 --> 00:00:42,160
So thanks so much for joining.

6
00:00:42,160 --> 00:00:47,220
For those who are watching live, feel free to write questions in the live chat and off

7
00:00:47,220 --> 00:00:48,220
to you.

8
00:00:48,220 --> 00:00:49,220
Thank you.

9
00:00:49,220 --> 00:00:54,700
Thank you very much, Daniel, for inviting me.

10
00:00:54,700 --> 00:00:59,540
It's been a big fan of the channel and I've been watching a lot of videos, so I'm quite

11
00:00:59,540 --> 00:01:04,500
excited to be here and be the one speaking this time.

12
00:01:04,500 --> 00:01:09,540
So I'm going to talk about this recent preprint that I put out, which has been the work of

13
00:01:09,540 --> 00:01:12,620
the last couple of months.

14
00:01:12,620 --> 00:01:22,220
And it's a collaboration with Luca Vincetti, Amin Makarak, Bernmille and Thomas Lukasiiewicz.

15
00:01:22,220 --> 00:01:27,820
It's basically a joint work between Versus, which is the company I work for, the University

16
00:01:27,820 --> 00:01:31,860
of Oxford and Theo Vien.

17
00:01:31,860 --> 00:01:42,040
So during this talk, I will, this is basically the outline of the talk, I will start talking

18
00:01:42,040 --> 00:01:49,340
about what predictive coding is and give an introduction of what it is, a brief historical

19
00:01:49,340 --> 00:01:55,820
introduction, why I think it's important to study predictive coding, even for example

20
00:01:55,820 --> 00:01:58,740
for the machine learning perspective.

21
00:01:58,740 --> 00:02:06,100
I will then provide a small intro to what causal inference is.

22
00:02:06,100 --> 00:02:12,300
And once we have all those informations together, I will then discuss why I wrote this paper,

23
00:02:12,300 --> 00:02:18,820
what was basically the research question that inspired me and the other collaborators.

24
00:02:19,820 --> 00:02:27,380
And present the main results, which are how to perform inference, so intervention and

25
00:02:27,380 --> 00:02:34,980
counterfactual inference, and how to learn the causal structures from a given data set

26
00:02:34,980 --> 00:02:37,060
using predictive coding.

27
00:02:37,060 --> 00:02:43,980
And then I will of course conclude with a small summary and some discussion on why I

28
00:02:44,020 --> 00:02:48,180
believe this work can be impactful in some future directions.

29
00:02:50,780 --> 00:02:53,460
So what is predictive coding?

30
00:02:53,460 --> 00:02:58,780
Predictive coding is in general famous for being a neuroscience inspired learning method,

31
00:02:58,780 --> 00:03:03,300
so a theory of how information processing in the brain works.

32
00:03:04,660 --> 00:03:11,380
And brain formally speaking, the theory of predictive coding can be described as basically

33
00:03:11,380 --> 00:03:18,860
having a hierarchical structure of neurons in the brain and you have two different families

34
00:03:18,860 --> 00:03:20,980
of neurons in the brain.

35
00:03:20,980 --> 00:03:27,540
The first family is the one in charge of sending prediction information, so neurons in a specific

36
00:03:27,540 --> 00:03:36,420
level of the hierarchy send information and predict the activity of the level below.

37
00:03:36,420 --> 00:03:40,100
And the second family of neurons is that of error neurons.

38
00:03:40,100 --> 00:03:45,140
And the error neurons, they send prediction error information up the hierarchy.

39
00:03:45,140 --> 00:03:49,580
So one level predicts the activity of the level below.

40
00:03:49,580 --> 00:03:54,460
This activity has some, this prediction has some mismatch, which we were actually going

41
00:03:54,460 --> 00:03:56,540
on in the level below.

42
00:03:56,540 --> 00:04:01,620
And the information about the prediction error gets sent up the hierarchy.

43
00:04:02,540 --> 00:04:10,540
However, predictive coding was actually not burned as a neuroscience, as a theory from

44
00:04:10,540 --> 00:04:15,740
neurosciences, but it was actually initially developed as a method for signal processing

45
00:04:15,740 --> 00:04:18,500
and compression back in the 50s.

46
00:04:18,500 --> 00:04:26,980
So the work of Oliver, Elias, which are actually contemporary of Shannon, they realized that

47
00:04:26,980 --> 00:04:34,660
once we have a predictor, a model that works that is well in predicting data, sending messages

48
00:04:34,660 --> 00:04:41,100
about the error in those predictions is actually much cheaper than sending the entire message

49
00:04:41,100 --> 00:04:43,460
every time.

50
00:04:43,460 --> 00:04:49,300
And this is how predictive coding was born, so as a signal processing and compression

51
00:04:49,300 --> 00:04:53,900
mechanism in information theory back in the 50s.

52
00:04:53,900 --> 00:05:04,060
He was actually in the 80s, that he became that exactly the same model was used in neuroscience.

53
00:05:04,060 --> 00:05:11,500
And so with the work from Mumford or other works, for example, explain how the rate enough

54
00:05:11,500 --> 00:05:15,980
processing formation, so we get prediction signals from the outside world, and we need

55
00:05:15,980 --> 00:05:23,020
to compress these representation and have this internal representation in our neurons.

56
00:05:23,020 --> 00:05:29,660
And the method is very similar, if not equivalent to the one that was developed by Elias and

57
00:05:29,660 --> 00:05:33,420
Oliver in the 50s.

58
00:05:33,420 --> 00:05:39,900
Maybe what's the biggest paradigm shift, happening in 1999, thanks to the work of Raoul

59
00:05:39,900 --> 00:05:47,100
and Ballard, in which they introduced this concept that I mentioned earlier about hierarchical

60
00:05:47,100 --> 00:05:53,980
structures in the brain, where prediction information is top down and error information

61
00:05:53,980 --> 00:05:55,860
is bottom up.

62
00:05:55,860 --> 00:06:02,340
And something that they did that wasn't done before is that they explain and develop this

63
00:06:02,340 --> 00:06:08,700
theory about not only inference, but also about how learning works in the brain.

64
00:06:08,700 --> 00:06:13,660
So it's also a theory of how our synapses get updated.

65
00:06:13,660 --> 00:06:19,380
And the last big breakthrough that I'm going to talk about in this brief historical introduction

66
00:06:19,380 --> 00:06:28,620
is from 2003, but then it kept going in the years after, thanks to Carfriston, in which

67
00:06:28,620 --> 00:06:36,780
basically he took the theory of Raoul and Ballard, and he extended it and generalized

68
00:06:36,780 --> 00:06:40,740
it to the theory of generative models.

69
00:06:40,740 --> 00:06:47,140
So basically the main claim that Carfriston did is that predictive coding is an evidence-maximization

70
00:06:47,140 --> 00:06:55,340
scheme of a specific kind of generative model, which I'm going to introduce later as well.

71
00:06:55,340 --> 00:07:04,220
So to make a brief summary, the first two kinds of predictive coding that I described,

72
00:07:04,220 --> 00:07:08,980
so signal processing and compression and the information processing in the retina and in

73
00:07:08,980 --> 00:07:12,820
the brain in general, they are inference methods.

74
00:07:12,820 --> 00:07:21,060
And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st

75
00:07:21,060 --> 00:07:25,100
century, is that predictive coding was seen as a learning algorithm.

76
00:07:25,100 --> 00:07:31,180
So we can first compress information and then update all the synapses or all the latent

77
00:07:31,180 --> 00:07:37,540
variables that we have in our generative model to improve our generative model itself.

78
00:07:38,540 --> 00:07:44,660
So let's give some definitions that are a little bit more formal.

79
00:07:44,660 --> 00:07:50,820
So predictive coding can be seen as a hierarchical Gaussian generative model.

80
00:07:50,820 --> 00:07:55,500
So here is a very simple figure in which we have this hierarchical structure, which can

81
00:07:55,500 --> 00:07:58,860
be as deep as we want.

82
00:07:58,860 --> 00:08:05,580
And prediction signals go from one latent variable, Xn, to the following one, and it

83
00:08:05,580 --> 00:08:10,460
gets transformed every time via function gn, or gi.

84
00:08:10,460 --> 00:08:19,540
And this is a generative model, as I said, and what's the marginal probability of this

85
00:08:19,540 --> 00:08:20,540
generative model?

86
00:08:20,540 --> 00:08:26,620
Well, it's simply the probability of the last, can you see my cursor?

87
00:08:26,620 --> 00:08:27,620
Yes, right?

88
00:08:27,620 --> 00:08:28,620
Yes, perfect.

89
00:08:28,620 --> 00:08:35,140
So it's the generative model of the last vertex, is the distribution of the last vertex, times

90
00:08:35,140 --> 00:08:40,740
the probability distribution of every other vertex, conditioned on the activity of the

91
00:08:40,740 --> 00:08:46,060
vertex before, or the latent variable before.

92
00:08:46,060 --> 00:08:50,980
I earlier said that it's a Gaussian generative model, which means that those probabilities

93
00:08:50,980 --> 00:09:02,380
they are in Gaussian form, and those function g, in general, and especially since, for

94
00:09:02,380 --> 00:09:07,060
example, in a round baller paper, and in all the papers that came afterwards, also because

95
00:09:07,060 --> 00:09:14,580
of the deep learning revolution, those functions are simply linear maps, or nonlinear maps with

96
00:09:14,580 --> 00:09:23,580
activation functions, or nonlinear maps with activation function and an additive bias.

97
00:09:23,580 --> 00:09:29,420
So we can give a formal definition of predictive coding, and we can say that predictive coding

98
00:09:29,420 --> 00:09:35,180
is an inversion scheme for such a generative model, where its model evidence is maximized

99
00:09:35,180 --> 00:09:41,180
by minimizing a quantity that is called the variational free energy.

100
00:09:41,180 --> 00:09:46,420
In general, the goal of every generative model is to maximize model evidence, but this quantity

101
00:09:46,420 --> 00:09:55,420
is always intractable, and we have some techniques that allow us to approximate the solution.

102
00:09:55,420 --> 00:10:02,300
And the one that we use in predictive coding is minimizing a variational free energy, which

103
00:10:02,300 --> 00:10:05,580
is a lower bound of the model evidence.

104
00:10:05,580 --> 00:10:12,340
In this work, and actually in a lot of other ones, so is the standard way of doing it,

105
00:10:12,340 --> 00:10:19,860
this minimization is performed via gradient descent, and there are actually other methods

106
00:10:19,860 --> 00:10:24,780
such as expectation maximization, which is often equivalent, or you can use some other

107
00:10:24,780 --> 00:10:31,300
message-passing algorithms such as belief propagation, for example.

108
00:10:31,300 --> 00:10:36,380
And going a little bit back in time, so we're getting a little bit about the statistical

109
00:10:36,380 --> 00:10:44,460
generative models, we can see predictive coding, as I said already a couple of times, as a

110
00:10:44,460 --> 00:10:50,620
hierarchical model with neural activities, so with neurons, latent variables that represent

111
00:10:50,620 --> 00:10:56,780
neural activities, they send their signal down the hierarchy, and with error nodes or

112
00:10:56,780 --> 00:11:02,300
error neurons, they send their signal up the hierarchy, so they send the error information

113
00:11:02,300 --> 00:11:03,300
back.

114
00:11:03,300 --> 00:11:08,020
What's the variational free energy of these class-operated coding models?

115
00:11:08,020 --> 00:11:15,820
It's simply the sum of the mean square error of all the error neurons, so it's the sum

116
00:11:15,820 --> 00:11:22,460
of the total error squared.

117
00:11:22,460 --> 00:11:28,180
And this representation is going to be useful in the later slides, and I'm going to explain

118
00:11:28,180 --> 00:11:31,580
how to use predictive coding to model causal inference, for example.

119
00:11:31,580 --> 00:11:37,100
What do you think predictive coding is important and is a nice algorithm to study?

120
00:11:37,100 --> 00:11:41,260
Well, first of all, as I said earlier, it optimizes the correct objective, which is

121
00:11:41,260 --> 00:11:48,300
the model evidence or marginal likelihood, and then it does so by optimizing a lower bound,

122
00:11:48,300 --> 00:11:52,620
which is called the variational free energy, as I said, and the variational free energy

123
00:11:52,620 --> 00:11:58,900
is interesting because it can be written as a sum of two different terms, which are

124
00:11:58,900 --> 00:12:06,300
and each of those terms optimizing it as important impacts, for example, in machine learning

125
00:12:06,300 --> 00:12:09,300
tasks or in general in learning tasks.

126
00:12:09,300 --> 00:12:12,660
So one of those terms forces memorization.

127
00:12:12,660 --> 00:12:19,860
So the second term basically tells forces the model to fit a specific data set.

128
00:12:19,860 --> 00:12:24,860
And the first term forces the model to minimize the complexity.

129
00:12:24,860 --> 00:12:31,060
And as we know, for example, from the Occam's razor theory, if we have two different models

130
00:12:31,060 --> 00:12:35,980
that perform similarly on a specific training set, the one that we have to get and the one

131
00:12:35,980 --> 00:12:41,340
that is expected to generalize the most is the less complex one.

132
00:12:41,340 --> 00:12:48,860
So updating generative model via variational free energy allows us to basically converge

133
00:12:48,860 --> 00:12:55,900
to the optimal Occam razor model, which both memorizes a data set, but is also able to

134
00:12:55,900 --> 00:13:01,020
generalize very well on unseen data points.

135
00:13:01,020 --> 00:13:09,980
A second reason why predictive coding is important is that it actually doesn't have

136
00:13:09,980 --> 00:13:15,500
to be defined on a hierarchical structure, but it can be modeled on more complex and

137
00:13:15,500 --> 00:13:22,100
flexible architectures such as directed graphical model with any shape or generalized even more

138
00:13:22,100 --> 00:13:25,580
to networks with a lot of cycles that resemble brain region.

139
00:13:25,580 --> 00:13:31,500
And the underlying reason is that you're not learning and predicting with a forward

140
00:13:31,500 --> 00:13:36,900
pass and then back propagating the error, but you're minimizing an energy function.

141
00:13:36,900 --> 00:13:43,780
And this allows basically every kind of hierarchy to be, allows to go behind hierarchies and

142
00:13:43,780 --> 00:13:46,580
allow to learn cycles.

143
00:13:46,580 --> 00:13:50,700
And this is actually quite important because the brain is full of cycles as we have some

144
00:13:50,700 --> 00:13:58,060
information from some recent papers that may manage to map completely the brain of some

145
00:13:58,060 --> 00:14:00,980
animals such as fruit fly.

146
00:14:00,980 --> 00:14:02,380
The brain is full of cycles.

147
00:14:02,380 --> 00:14:11,020
So it makes sense to train our machine learning models or our models in general with an algorithm

148
00:14:11,020 --> 00:14:17,500
that allows us to train using cyclic structures.

149
00:14:17,500 --> 00:14:22,100
The third reason why predictive coding is interesting is that it has been formally proven

150
00:14:22,100 --> 00:14:26,340
that it is more robust than standard neural networks trained with back propagation.

151
00:14:26,340 --> 00:14:31,900
So if you have a neural network and you want to perform classification tasks, you, predictive

152
00:14:31,900 --> 00:14:34,500
coding is more robust.

153
00:14:34,500 --> 00:14:40,340
And this is interesting in tasks such as online learning, training on small datasets

154
00:14:40,340 --> 00:14:42,900
or continuous learning tasks.

155
00:14:42,900 --> 00:14:48,300
And the theory basically comes from the fact that imperative coding has been proved to

156
00:14:48,300 --> 00:14:53,980
approximate implicit gradient descent, which is a different version of the explicit gradient

157
00:14:53,980 --> 00:15:00,100
descent, which is the standard gradient descent used in the, in every single model basically.

158
00:15:00,100 --> 00:15:06,020
And it's a variation that is more robust.

159
00:15:06,020 --> 00:15:08,900
I think, okay, I did a quite a long intro to predictive coding.

160
00:15:08,900 --> 00:15:15,020
I think I'm now moving to the second topic, which is causal inference and what's causal

161
00:15:15,020 --> 00:15:16,020
inference?

162
00:15:16,020 --> 00:15:21,660
Causal inference is a theory, is a very general theory that has been formalized the most by

163
00:15:21,660 --> 00:15:22,660
Judea Perl.

164
00:15:22,660 --> 00:15:26,740
He's definitely the most important person in the field of causal inference.

165
00:15:26,740 --> 00:15:28,780
He wrote some very nice books.

166
00:15:28,780 --> 00:15:35,700
For example, the book of why is highly recommended if you want to learn more about this topic.

167
00:15:35,700 --> 00:15:38,860
And it basically tackles the following problem.

168
00:15:38,860 --> 00:15:42,780
So let's assume we have a joint probability distribution, which is associated with a Bayesian

169
00:15:42,780 --> 00:15:43,780
network.

170
00:15:43,780 --> 00:15:49,940
This is going to be a little bit the running example through all the paper, especially with

171
00:15:49,940 --> 00:15:53,940
your net with Bayesian networks of this shape.

172
00:15:53,940 --> 00:16:00,940
Those Bayesian networks, the variables inside, they can represent different quantities.

173
00:16:00,940 --> 00:16:07,820
So for example, a Bayesian network with this shape can represent the quantities on the

174
00:16:07,820 --> 00:16:08,820
right.

175
00:16:08,820 --> 00:16:13,940
So a socio-economical statue of an individual, its education level, its intelligence, and

176
00:16:13,940 --> 00:16:17,580
its income level.

177
00:16:17,580 --> 00:16:24,900
Something the classical statistics is very good at, and it's a while most used application,

178
00:16:24,900 --> 00:16:27,780
is to model observations or correlations.

179
00:16:27,780 --> 00:16:34,700
A correlation basically answered the question, what is the, if we observe another variable

180
00:16:34,700 --> 00:16:35,820
C?

181
00:16:35,820 --> 00:16:40,140
So for example, in this case, what is, what's the income level, the expected income level

182
00:16:40,140 --> 00:16:44,540
of an individual, if I observe his education level?

183
00:16:44,540 --> 00:16:50,820
And of course, if that person has a higher degree of education, for example, a master

184
00:16:50,820 --> 00:16:56,340
or a PhD, I'm expecting general that person to have a higher income level.

185
00:16:56,340 --> 00:16:57,820
And this is a correlation.

186
00:16:57,820 --> 00:17:03,340
However, sometimes there are things that are very hard to observe, but they play a huge

187
00:17:03,340 --> 00:17:06,300
role in determining those quantities.

188
00:17:06,300 --> 00:17:12,940
So for example, it could be that the income level is much, much more defined by the intelligence

189
00:17:12,940 --> 00:17:15,980
of a specific person.

190
00:17:15,980 --> 00:17:21,700
And maybe that the intelligence, so if a person is intelligent, he's also most likely to have

191
00:17:21,700 --> 00:17:25,020
a higher education level.

192
00:17:25,020 --> 00:17:32,620
But still the real reason why the income is high is because of the IQ.

193
00:17:33,260 --> 00:17:39,260
This cannot be studied by simple correlations and has to be studied by a more advanced technique,

194
00:17:39,260 --> 00:17:40,900
which is called an intervention.

195
00:17:40,900 --> 00:17:46,900
An intervention basically answers the question, what is the, if we change C to a specific

196
00:17:46,900 --> 00:17:48,500
value?

197
00:17:48,500 --> 00:17:55,780
So for example, we can take an individual and check his income level, and then change

198
00:17:55,780 --> 00:18:01,060
its education level, so intervene on this word, and change his education level without

199
00:18:01,100 --> 00:18:06,660
touching his intelligence, and see how much his income changes.

200
00:18:06,660 --> 00:18:12,620
For example, if the income changes a lot, it means that the intelligence doesn't play

201
00:18:12,620 --> 00:18:16,020
a big role in this, but the education level does.

202
00:18:16,020 --> 00:18:20,220
If the income level doesn't change much, it means that maybe there's a hidden variable,

203
00:18:20,220 --> 00:18:26,300
in this case, the intelligence that determines the income level of a person.

204
00:18:26,300 --> 00:18:30,860
The third quantity important in causal inference is that of counterfactuals.

205
00:18:30,860 --> 00:18:36,780
So for example, a counterfactual answers the question, what would be, had we changed C

206
00:18:36,780 --> 00:18:39,420
to a different value in the past?

207
00:18:39,420 --> 00:18:43,220
So for example, we can see that the difference between interventions and counterfactuals is

208
00:18:43,220 --> 00:18:46,460
that interventions act in the future.

209
00:18:46,460 --> 00:18:51,300
So I'm interviewing in the world now to observe a change in the future.

210
00:18:51,300 --> 00:18:57,460
Well, counterfactual allow us to go back in time and change a variable back in time and

211
00:18:57,540 --> 00:19:03,180
see how the change would have influenced the world we live in now.

212
00:19:03,180 --> 00:19:08,300
And those are defined by Judea Perle as the three levels of causal inference.

213
00:19:08,300 --> 00:19:12,140
Correlation is the first level, intervention is the second level, and counterfactual is

214
00:19:12,140 --> 00:19:16,620
the third level.

215
00:19:16,620 --> 00:19:17,620
What are interventions?

216
00:19:17,620 --> 00:19:22,420
I'm going to define them more formally now, how that I gave an intuitive definition.

217
00:19:22,420 --> 00:19:28,220
And I'm using this notation here, which is the same actually throughout all the presentation.

218
00:19:28,220 --> 00:19:33,860
So X is always going to be a latent variable, SI is always going to be a data point or an

219
00:19:33,860 --> 00:19:38,180
observation, and VI is always going to be a vertex.

220
00:19:38,180 --> 00:19:45,580
So every time you see VI, we're only interested in the structure of the graph, for example.

221
00:19:45,580 --> 00:19:51,100
So let's assume we have a Bayesian model, which has the same structure as the Bayesian

222
00:19:51,100 --> 00:19:55,140
model we saw in the previous slide.

223
00:19:55,140 --> 00:20:01,780
Given that X3 is equal to S3, this is the observation we make, statistics allows us to compute the

224
00:20:01,780 --> 00:20:09,620
probability or the expectation of X4, which is the latent variable related to this vertex,

225
00:20:09,620 --> 00:20:12,620
given that X3 is equal to S3.

226
00:20:12,620 --> 00:20:20,620
To perform an intervention, we need a new kind of notation, which is called the do operation.

227
00:20:21,500 --> 00:20:28,820
So in this case, X4, we want to compute the probability of X4, given the fact that we

228
00:20:28,820 --> 00:20:33,540
intervene in the world and change X3 to S3.

229
00:20:33,540 --> 00:20:34,900
And how do we do this?

230
00:20:34,900 --> 00:20:41,520
To perform an intervention, Judea Perl tells us that we have to have an intermediate step

231
00:20:41,520 --> 00:20:47,900
before computing a correlation, is that first we have to remove all the incoming edges to

232
00:20:47,900 --> 00:20:50,460
V3.

233
00:20:50,460 --> 00:20:56,020
So we have to study not this Bayesian network, but this second one.

234
00:20:56,020 --> 00:21:03,580
And then at this point, we are allowed to compute a correlation, as we normally do.

235
00:21:03,580 --> 00:21:07,220
And this is an intervention.

236
00:21:07,220 --> 00:21:12,700
A counterfactual is a generalization of this that, as I said, lived in the past, and they

237
00:21:12,700 --> 00:21:15,580
are computing using structural causal models.

238
00:21:15,580 --> 00:21:22,460
A structural causal model is a tuple, which is conceptually similar to a Bayesian network.

239
00:21:22,460 --> 00:21:27,900
But basically, we have this new class of variables on top, which are the unobservable variables

240
00:21:27,900 --> 00:21:29,420
they use.

241
00:21:29,420 --> 00:21:33,500
So we have the Bayesian network that we had before, X1, X2, X3, S4.

242
00:21:33,500 --> 00:21:40,500
But we also have those unobservable or variables that depend on the environment.

243
00:21:40,500 --> 00:21:47,500
You cannot control them, you can infer them, but they are there.

244
00:21:47,500 --> 00:21:57,500
And F is a set of functions that depends on all the, basically, F of X3 depends on X1,

245
00:21:57,500 --> 00:22:01,820
because you have an arrow, on X2, because you have an arrow, and on the unobservable

246
00:22:01,820 --> 00:22:06,340
variable that also influences X3.

247
00:22:06,340 --> 00:22:14,100
So yes, intuitively, you can think of a structural causal model as a Bayesian network with those

248
00:22:14,100 --> 00:22:22,460
unobservable variables on top, and each unobservable variable only influences its own, its own

249
00:22:22,460 --> 00:22:23,460
related variable X.

250
00:22:23,460 --> 00:22:27,260
So, for example, IU will never touch X1 as well.

251
00:22:27,260 --> 00:22:35,260
U3 will only touch U3, U1 will only influence X1, and so forth, and so on.

252
00:22:35,260 --> 00:22:39,220
So performing counterfactual inference answers the following question.

253
00:22:39,220 --> 00:22:48,220
So what would X4 be at X3 being equal to another variable in a past situation, U?

254
00:22:48,220 --> 00:22:51,620
And computing this counterfactual requires three different steps.

255
00:22:51,620 --> 00:22:58,220
So abduction is the computation of all the background variables.

256
00:22:58,220 --> 00:23:03,340
So in this step, we want to go back in time and understand how the environment, the unobservable

257
00:23:03,340 --> 00:23:08,340
environment, was in that specific moment in time.

258
00:23:08,340 --> 00:23:17,260
And we do this by fixing all the latent variables X to some specific data that we already have,

259
00:23:17,260 --> 00:23:20,340
and performing this inference on the use.

260
00:23:20,340 --> 00:23:28,780
Then we're going to use the U to keep the U that we have learned, and perform an intervention.

261
00:23:28,780 --> 00:23:35,340
So a counterfactual can also be seen as an intervention back in time, in which we know

262
00:23:35,340 --> 00:23:43,620
the environment variables U1, U2, and U4 in that specific moment.

263
00:23:43,620 --> 00:23:46,980
And what's the missing step?

264
00:23:46,980 --> 00:23:54,060
So what would X4 be at X3 being equal to another data point in that specific situation?

265
00:23:54,620 --> 00:23:57,460
Now we can compute a correlation.

266
00:23:57,460 --> 00:24:03,660
And the correlation, we do it on the graph in which we have already performed an intervention

267
00:24:03,660 --> 00:24:10,420
using the environment variables that we have learned in the abduction step.

268
00:24:10,420 --> 00:24:15,780
And this is a counterfactual inference.

269
00:24:15,780 --> 00:24:22,380
This is the last slide of the causal inference introduction, and it's about structure learning.

270
00:24:22,380 --> 00:24:28,740
Basically, everything I've said so far relies on the fact that we know the causal dependencies

271
00:24:28,740 --> 00:24:30,540
among the data points.

272
00:24:30,540 --> 00:24:35,140
So we know the structure of the graph, we know which variable influences which one,

273
00:24:35,140 --> 00:24:37,580
we know the arrows in general.

274
00:24:37,580 --> 00:24:40,660
But in practice, this is actually not always possible.

275
00:24:40,660 --> 00:24:46,620
So we don't have access to the causal graph most of the times.

276
00:24:46,620 --> 00:24:50,940
And actually learning the best causal graph from data is still an open problem.

277
00:24:50,940 --> 00:24:51,940
We are improving in this.

278
00:24:51,940 --> 00:24:53,140
We are getting better.

279
00:24:53,140 --> 00:25:01,340
But how to perform this task exactly is still an open problem.

280
00:25:01,340 --> 00:25:05,860
So as I said, basically, the goal is to infer causal relationships from observational data.

281
00:25:05,860 --> 00:25:11,380
So given a data set, we want to infer the directed acyclic graph that describes the

282
00:25:11,380 --> 00:25:16,140
connectivity between the system and the variables of the data set.

283
00:25:16,140 --> 00:25:22,540
So for example here, we have an example that I guess we are all familiar with thanks because

284
00:25:22,540 --> 00:25:24,260
of the pandemic.

285
00:25:24,260 --> 00:25:31,740
So we have those four variables, age, vaccine, hospitalization, and CT.

286
00:25:31,740 --> 00:25:35,900
And we want to infer the causal dependencies among those variables.

287
00:25:35,900 --> 00:25:40,380
So for example, we want to learn directly from data that the probability of a person

288
00:25:40,380 --> 00:25:46,780
being hospitalized depends on its age and on the fact whether it's vaccinated or not,

289
00:25:46,780 --> 00:25:51,620
and so forth and so on.

290
00:25:51,620 --> 00:25:58,660
So this is the end of the long introduction, but I hope it was clear enough and I hope

291
00:25:58,660 --> 00:26:05,060
that I gave the basics to understand basically the results of the paper.

292
00:26:05,060 --> 00:26:07,940
And now we can go to the research questions.

293
00:26:07,940 --> 00:26:10,860
So the research questions are the following.

294
00:26:10,860 --> 00:26:17,300
First I want to see whether creative coding can be used to perform causal inference.

295
00:26:17,300 --> 00:26:23,700
So creative coding so far has only been used to perform two compute correlations in Bayesian

296
00:26:23,700 --> 00:26:25,380
networks.

297
00:26:25,380 --> 00:26:30,340
And the big question is, can we go beyond correlation and model intervention and counterfactual

298
00:26:30,340 --> 00:26:32,940
in a biological, plausible way?

299
00:26:32,940 --> 00:26:38,940
So in a way that it's, for example, simple, intuitive, and allow us to only play with

300
00:26:38,940 --> 00:26:44,220
the neurons and not touch, for example, the huge structure of the graph.

301
00:26:44,220 --> 00:26:48,820
And more in practice, more specifically, the question becomes, can we define a creative

302
00:26:48,820 --> 00:26:55,580
coding-based structural causal model to perform interventions and counterfactuals?

303
00:26:55,580 --> 00:27:01,320
The second question is, as I said, that having a structural causal model assumes that we

304
00:27:01,320 --> 00:27:04,520
know the structure of the Bayesian network.

305
00:27:04,520 --> 00:27:08,160
So it assumes that we have the arrows.

306
00:27:08,160 --> 00:27:12,000
Can we go beyond this and use creative coding networks to learn the causal structure of

307
00:27:12,000 --> 00:27:14,200
the graph?

308
00:27:14,200 --> 00:27:21,560
Basically, giving positive answers to both those questions would allow us to use creative

309
00:27:21,560 --> 00:27:27,400
coding as an end-to-end causal inference method, which basically takes a data set and allow

310
00:27:27,400 --> 00:27:37,040
us to test interventions and counterfactual predictions directly from this data set.

311
00:27:37,040 --> 00:27:42,040
So let's tackle the first problem, so causal inference via creative coding, which is also

312
00:27:42,040 --> 00:27:47,040
the section that gives the title to the paper, basically.

313
00:27:47,040 --> 00:27:53,040
And here I will show how to perform correlations with creative coding, which is already known,

314
00:27:53,040 --> 00:28:01,440
and how to perform interventional queries, which I think is the real question of the paper.

315
00:28:01,440 --> 00:28:07,560
So here is a causal graph, which is the usual graph that we had.

316
00:28:07,560 --> 00:28:10,520
And here is the corresponding creative coding model.

317
00:28:10,520 --> 00:28:15,960
So the axes are the latent variables and correspond to the neurons in a neural network

318
00:28:15,960 --> 00:28:18,360
model.

319
00:28:18,360 --> 00:28:25,880
And the black arrow passes prediction information from one neuron to the one down the hierarchy.

320
00:28:25,880 --> 00:28:31,880
And every vertex also has this error neuron, which passes information up the hierarchy.

321
00:28:31,880 --> 00:28:38,240
So the information of every error goes to the value node in the up the hierarchy and

322
00:28:38,240 --> 00:28:44,960
basically tells it to correct itself to change the prediction.

323
00:28:44,960 --> 00:28:49,120
So to perform a correlation using creative coding, what you have to do is that you take

324
00:28:49,120 --> 00:28:54,000
an observation and you simply fix the value of a specific neuron.

325
00:28:54,000 --> 00:28:59,840
So if you want to compute the probability of X4 given X3 equal to S3, we simply have

326
00:28:59,840 --> 00:29:06,600
to take X3 and fix it to S3 in a way that it doesn't change anymore and run an energy

327
00:29:06,600 --> 00:29:08,480
minimization.

328
00:29:08,480 --> 00:29:16,320
And this model, by minimizing, by updating the axes via a minimization of the variational

329
00:29:16,320 --> 00:29:21,080
free energy, allows the model to converge to a solution to this question.

330
00:29:21,080 --> 00:29:26,320
So the probability or the expected value of X4 given X3 equals 3.

331
00:29:26,320 --> 00:29:32,560
But how do I perform an intervention now without acting on the structure of the graph?

332
00:29:32,560 --> 00:29:37,200
Well, this is basically the first idea of the paper.

333
00:29:38,160 --> 00:29:40,520
This is still how to perform a correlation.

334
00:29:40,520 --> 00:29:45,240
So fix S3 equal to X3 is the first step in the algorithm.

335
00:29:45,240 --> 00:29:51,320
And the second one is to update the axes by minimizing the variational free energy.

336
00:29:51,320 --> 00:29:57,040
An intervention, which in theory corresponds in removing those arrows and answers to the

337
00:29:57,040 --> 00:30:04,120
question, the probability of X4 by performing an intervention, so do X3 equal S3?

338
00:30:04,120 --> 00:30:07,320
This coding can be performed as follows.

339
00:30:07,320 --> 00:30:10,120
So I'm going to write the algorithm here.

340
00:30:10,120 --> 00:30:19,160
So first, as in a correlation, you fix X3 equal to the observation that you get.

341
00:30:19,160 --> 00:30:21,560
Then this is the important step.

342
00:30:21,560 --> 00:30:27,360
You have to intervene not on the graph anymore, but on the prediction error and fix it equal

343
00:30:27,360 --> 00:30:29,360
to zero.

344
00:30:29,360 --> 00:30:36,840
Assuming a prediction error equal to zero basically makes sense, meaning less information

345
00:30:36,840 --> 00:30:41,360
up the hierarchy or actually sends no information up the hierarchy because it basically tells

346
00:30:41,360 --> 00:30:45,480
you that the prediction is always correct.

347
00:30:45,480 --> 00:30:51,000
And the third step is to, as we did before, to update the axes, the unconstrained axis

348
00:30:51,000 --> 00:30:55,960
or X1, X2, X4 by minimizing the variational free energy.

349
00:30:55,960 --> 00:31:01,680
As I will show now experimentally, by simply doing this little trick of setting a prediction

350
00:31:01,680 --> 00:31:09,080
error to be equal to zero, it prevents us to actually act on the structure of the graph

351
00:31:09,080 --> 00:31:17,920
as the theory of Duke-Alculus does and to infer the variables after an intervention

352
00:31:17,920 --> 00:31:25,000
by simply performing a variational free energy minimization.

353
00:31:25,000 --> 00:31:26,760
What about counterfactual inference?

354
00:31:26,760 --> 00:31:35,040
Counterfactual inference is actually easy once we have defined how to do an intervention.

355
00:31:35,040 --> 00:31:39,040
And this is because, as we saw earlier, performing a counterfactual is similar to performing an

356
00:31:39,040 --> 00:31:48,320
intervention in a past situation after you have inferred the unobservable variables.

357
00:31:48,320 --> 00:31:53,760
So as you can see in the plot I showed earlier about the abduction action and prediction

358
00:31:53,760 --> 00:31:59,920
steps, the action and prediction steps, they did not have those two arrows.

359
00:31:59,920 --> 00:32:01,680
They were removed.

360
00:32:01,680 --> 00:32:11,200
Pretty coding allows us to keep the arrows in the graph and perform counterfactuals by

361
00:32:11,200 --> 00:32:16,240
simply performing an abduction step, as it was done earlier, an action step in which we

362
00:32:16,240 --> 00:32:19,400
simply perform an intervention on the single node.

363
00:32:19,400 --> 00:32:26,280
So we fix the value node and we set the error to zero and run the energy minimization, so

364
00:32:26,280 --> 00:32:32,960
minimizing the variational free energy to compute the prediction.

365
00:32:32,960 --> 00:32:41,560
So I think this is like an easy and elegant method to perform interventions and counterfactuals.

366
00:32:41,560 --> 00:32:47,800
And yeah, so I think the thing we have to show now is whether it works in practice or not.

367
00:32:47,800 --> 00:32:50,400
And we have a couple of experiments.

368
00:32:50,400 --> 00:32:53,560
And I'm going to show you now two different experiments.

369
00:32:53,560 --> 00:33:01,320
The first one is merely proof of concept experiment that shows that the predictive coding is able

370
00:33:01,320 --> 00:33:06,400
to perform intervention and counterfactuals.

371
00:33:06,400 --> 00:33:12,200
And the second one actually shows a simple application in how interventional queries

372
00:33:12,200 --> 00:33:17,520
can be used to improve the performance of classification tasks on a specific kind of

373
00:33:17,520 --> 00:33:22,720
predictive coding networks, which is that of a fully connected model.

374
00:33:22,720 --> 00:33:24,840
Let's start from the first one.

375
00:33:24,840 --> 00:33:26,440
So how do we do this task?

376
00:33:26,440 --> 00:33:34,600
So given a structural causal model, we generate training data and we use it to learn the weights,

377
00:33:34,600 --> 00:33:40,680
so to learn the functions of the structural causal models.

378
00:33:40,680 --> 00:33:46,360
And then we generate test data for both interventional and counterfactual queries.

379
00:33:46,360 --> 00:33:51,540
And we show whether we are able to converge to the correct test data using predictive

380
00:33:51,540 --> 00:33:53,760
coding.

381
00:33:53,760 --> 00:34:00,840
And for example here, those two plots represent the interventional and counterfactual queries

382
00:34:00,840 --> 00:34:06,280
of this specific graph, which is the butterfly bias graph, which is a graph that is often

383
00:34:06,280 --> 00:34:12,000
used in testing whether causal inference, whether interventional and counterfactual

384
00:34:12,000 --> 00:34:15,440
techniques work is as simple as that.

385
00:34:15,440 --> 00:34:19,520
But in the paper, you can find a lot of different graphs.

386
00:34:19,520 --> 00:34:29,480
But in general, those two plots show that the method works, show that the mean absolute

387
00:34:29,480 --> 00:34:36,240
error between the interventional and counterfactual quantities we compute and the interventional

388
00:34:36,720 --> 00:34:43,080
and counterfactual quantities from the original graph are close to each other.

389
00:34:43,080 --> 00:34:46,600
So the error is quite small.

390
00:34:46,600 --> 00:34:51,920
The second experiment is basically an extension of an experiment I proposed in an earlier

391
00:34:51,920 --> 00:34:59,320
paper, which is the learning on arbitrary graph topologies that I wrote last year.

392
00:34:59,320 --> 00:35:05,800
In that paper, I basically proposed this kind of network as a proof of concept, which is

393
00:35:05,800 --> 00:35:14,000
a fully connected network, which is in general the worst neural network you can have to perform

394
00:35:14,000 --> 00:35:25,520
machine learning experiments, because given a fixed set of neurons, basically every pair

395
00:35:25,520 --> 00:35:28,600
of neuron is connected by two different synapses.

396
00:35:28,600 --> 00:35:34,880
So it's the model with the highest complexity possible in general.

397
00:35:34,880 --> 00:35:38,320
The good thing is that since you have a lot of cycles, the model is extremely flexible

398
00:35:38,320 --> 00:35:43,920
in the sense that you can train it, for example, on a minst image and on a data point and on

399
00:35:43,920 --> 00:35:44,920
its label.

400
00:35:44,920 --> 00:35:50,760
But then the way you can query it, thanks to the information going back, is you can query

401
00:35:50,760 --> 00:35:51,760
in a lot of different ways.

402
00:35:51,760 --> 00:35:56,080
So you can form classification tasks in which you provide an image and you run the energy

403
00:35:56,080 --> 00:35:58,120
minimization and get the label.

404
00:35:58,120 --> 00:36:01,760
But you can also, for example, perform generation tasks in which you give the label, run the

405
00:36:01,760 --> 00:36:04,280
energy minimization and get the image.

406
00:36:04,280 --> 00:36:11,160
You can perform, for example, image completion, which should give half the image and let the

407
00:36:11,160 --> 00:36:13,800
model converge to the second half and so forth and so on.

408
00:36:13,800 --> 00:36:20,160
So it's basically a model that learns the statistics of the dataset in its entirety

409
00:36:20,160 --> 00:36:25,400
without being focused on classification or generation in general.

410
00:36:25,400 --> 00:36:28,160
So this flexibility is great.

411
00:36:28,160 --> 00:36:34,120
The problem is that because of this, every single task doesn't work well.

412
00:36:34,120 --> 00:36:39,080
So you can do a lot of different things, but none of them is done well.

413
00:36:39,080 --> 00:36:46,400
And here I want to show how using interventional queries instead of standard correlation queries

414
00:36:46,400 --> 00:36:52,200
or conditional queries slightly improves their results of those classification tasks.

415
00:36:52,200 --> 00:36:59,760
So what are the conjecture reasons of this test accuracy on those tasks not being so

416
00:36:59,760 --> 00:37:00,760
high?

417
00:37:00,760 --> 00:37:07,260
The first, the two reasons are that the model is distracted in correcting every single error.

418
00:37:07,260 --> 00:37:11,200
So basically you present an image and you would like to get a label, but the model is

419
00:37:11,200 --> 00:37:17,040
actually updating itself to also predict the error in the images.

420
00:37:17,040 --> 00:37:21,960
And the second reason, which is the one I said, is that the structure is far too complex.

421
00:37:21,960 --> 00:37:30,520
So again, from an Occam razor argumentation, this is the worst model you can have.

422
00:37:30,520 --> 00:37:34,200
So every time you have a model that fits a dataset, that model is going to be less complex

423
00:37:34,200 --> 00:37:36,640
than this one that is going to be preferred.

424
00:37:36,640 --> 00:37:43,800
But in general, just to start it, the idea is can querying this model be interventions

425
00:37:43,800 --> 00:37:48,160
be used to improve the performance of those fully connected models?

426
00:37:48,160 --> 00:37:51,320
Well, the answer is yes.

427
00:37:51,320 --> 00:37:53,680
So here is how I perform interventional queries.

428
00:37:53,680 --> 00:37:56,800
So I present an image to the network.

429
00:37:56,800 --> 00:38:00,160
I fix the error of the pixels to be equal to zero.

430
00:38:00,160 --> 00:38:03,480
So this error doesn't get propagated in the network.

431
00:38:03,480 --> 00:38:06,000
And then I compute the label.

432
00:38:06,000 --> 00:38:11,400
And as you can see, the accuracy improves, for example, from 89 using the standard query

433
00:38:11,400 --> 00:38:17,680
method of pretty difficult in networks to 92, which is the accuracy after the intervention

434
00:38:17,680 --> 00:38:22,000
and the same happens for fashion means.

435
00:38:22,000 --> 00:38:27,480
And I think that a very legit critic that probably everyone would think when seeing

436
00:38:27,480 --> 00:38:34,400
those plots is that, OK, you improve on means from 89 to 92, it still sucks, basically.

437
00:38:34,400 --> 00:38:36,720
And yeah, it's true.

438
00:38:36,720 --> 00:38:40,920
And I'm actually in the later slides, I'm going to show how to act on the structure

439
00:38:40,920 --> 00:38:47,040
of this fully connected model will improve the results even more until the point they

440
00:38:47,720 --> 00:38:53,000
reach a performance that is not even close to state of the art performance, of course.

441
00:38:53,000 --> 00:39:02,240
But it's still up to a level that becomes basically acceptable and worth investigating.

442
00:39:02,240 --> 00:39:08,680
So yes, so this is the part about causal inference using predictive coding.

443
00:39:08,680 --> 00:39:16,600
And I guess to summarize, I can say that the interesting part of the results I just showed

444
00:39:17,240 --> 00:39:22,520
is that I showed that predictive coding is able to perform interventions in a very easy

445
00:39:22,520 --> 00:39:26,760
and intuitive way because you don't have to act on the structure of the old graph anymore.

446
00:39:26,760 --> 00:39:31,120
Sometimes those functions are not available, so forth and so on.

447
00:39:31,120 --> 00:39:41,880
But you simply have to intervene on a single neuron, set its prediction error to zero and

448
00:39:41,880 --> 00:39:44,680
perform an energy minimization process.

449
00:39:46,760 --> 00:39:53,040
And these extended allowed us to define predictive coding based structural causal models.

450
00:39:53,040 --> 00:39:59,680
Now we move to the second part of the work, which is about structure learning.

451
00:40:02,480 --> 00:40:08,000
So structure learning, as I said, deals with the problem of learning the causal structure

452
00:40:08,000 --> 00:40:12,080
of the model from observational data.

453
00:40:12,080 --> 00:40:21,000
This is actually no problem that has been around for decades and has always been, until

454
00:40:21,000 --> 00:40:25,800
a couple of years ago, tackled using combinatorial search methods.

455
00:40:25,800 --> 00:40:30,800
The problem with those combinatorial search methods is that their complexity grows double

456
00:40:30,800 --> 00:40:33,040
exponentially.

457
00:40:33,040 --> 00:40:40,200
So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn

458
00:40:40,200 --> 00:40:47,000
grows in size, learning it, it's incredibly slow.

459
00:40:47,000 --> 00:40:54,640
The new solution that came out actually a couple of years ago in a newspaper from 2018

460
00:40:54,640 --> 00:40:58,240
showed that it's possible to actually learn this structure, not using a combinatorial

461
00:40:58,240 --> 00:41:01,920
search method, but by using a gradient-based method.

462
00:41:01,920 --> 00:41:09,320
And this was basically this killed the problem in general because now you can simply apply

463
00:41:09,320 --> 00:41:13,440
your on the parameters, which is the prior proposed that I'm going to define a little

464
00:41:13,440 --> 00:41:17,640
bit better in this slide, around gradient descent.

465
00:41:17,640 --> 00:41:23,400
And even if you have a model that is double, triple the size, the algorithm is still incredibly

466
00:41:23,400 --> 00:41:25,800
fast.

467
00:41:25,800 --> 00:41:31,800
And for this reason, this paper is, yeah, I think it's kind of new and I think already

468
00:41:31,800 --> 00:41:35,800
has around 600 citations or things like that.

469
00:41:35,800 --> 00:41:39,560
And every paper that I'm seeing now about causal inference and learning causal structure

470
00:41:39,560 --> 00:41:42,320
of the graph uses their method.

471
00:41:42,320 --> 00:41:48,560
It just changes a little bit, they find faster or slightly better inference methods, but

472
00:41:48,560 --> 00:41:56,760
still they all use the prior, this paper defined, and I do as well, and we do as well.

473
00:41:56,760 --> 00:42:01,080
So here we define a new quantity, which is the agency matrix.

474
00:42:01,080 --> 00:42:05,800
The agency matrix is simply a matrix that encodes the connections of the model.

475
00:42:05,800 --> 00:42:10,560
So it's a binary matrix, and in general, it's a binary matrix.

476
00:42:10,560 --> 00:42:15,440
Then of course, when you do gradient-based optimization, you make it continuous and then

477
00:42:15,440 --> 00:42:21,960
you have some threshold at some point that basically kills an edge or set it to one.

478
00:42:22,280 --> 00:42:34,360
The entry ij is equal to one if the Bayesian graph has an edge from vertex i to vertex j

479
00:42:34,360 --> 00:42:35,760
or zero otherwise.

480
00:42:35,760 --> 00:42:40,400
So for example, this agency matrix here represents the connectivity structure of this Bayesian

481
00:42:40,400 --> 00:42:43,160
network.

482
00:42:43,160 --> 00:42:51,520
And basically this method tackles two problems that we want about learning the structure

483
00:42:51,560 --> 00:42:53,360
of the Bayesian network.

484
00:42:53,360 --> 00:43:00,200
The idea is that we start from a fully connected model, which conceptually is similar, actually

485
00:43:00,200 --> 00:43:04,680
is equivalent to the predictive coding network I defined earlier, which is fully connected.

486
00:43:04,680 --> 00:43:10,480
So you have a lot of vertices and every pair of vertices is connected by two different

487
00:43:10,480 --> 00:43:16,080
edges, and you simply want to prune the ones that are not needed.

488
00:43:16,080 --> 00:43:20,320
So it can be seen as a method that performs model reduction.

489
00:43:20,320 --> 00:43:23,080
You start from a big model and you want to make it small.

490
00:43:23,080 --> 00:43:26,640
So what's the first ingredient to reduce models?

491
00:43:26,640 --> 00:43:29,720
Well, it's of course sparse city.

492
00:43:29,720 --> 00:43:35,400
And what's the prior that everyone uses to make a model more sparse is the Laplace prior,

493
00:43:35,400 --> 00:43:41,240
which in machine learning is simply known as the L1 norm, which is defined here.

494
00:43:41,240 --> 00:43:47,120
The solution that this paper that I mentioned earlier proposed is to add a second prior

495
00:43:47,120 --> 00:43:56,440
on top, which enforces what's probably the biggest characteristic of Bayesian networks

496
00:43:56,440 --> 00:44:00,440
on which you want to perform causal inference, is that you want them to be acyclic.

497
00:44:00,440 --> 00:44:08,440
And basically they show that acyclicity can be imposed on an agency matrix as a prior,

498
00:44:08,440 --> 00:44:10,720
and it has this shape here.

499
00:44:10,720 --> 00:44:19,200
So it's the trace of the matrix that is the exponential of A times A, where A is the

500
00:44:19,200 --> 00:44:21,280
agency matrix again.

501
00:44:21,280 --> 00:44:28,840
And basically this quantity here is equal to zero if and only if the Bayesian network

502
00:44:28,840 --> 00:44:37,960
or whatever graph you're considering is acyclic.

503
00:44:37,960 --> 00:44:46,520
So I'm going to use these in some experiments, so force those two priors on different kinds

504
00:44:46,520 --> 00:44:51,280
of Bayesian networks, and I'm trying to merge them with the techniques we proposed earlier

505
00:44:51,280 --> 00:44:55,360
about performing causal inference via predictive coding.

506
00:44:55,360 --> 00:44:57,240
So I'm going to present two different experiments.

507
00:44:57,240 --> 00:45:03,560
So one is a proof of concept, which is the standard experiments showed in all the structural

508
00:45:03,560 --> 00:45:09,200
learning tasks, which is the inference of the correct Bayesian network from data.

509
00:45:09,200 --> 00:45:16,320
And then I'm going to build on top of the classification experiments I showed earlier,

510
00:45:16,320 --> 00:45:22,200
and show how actually those priors allow us to improve the classification accuracy, the

511
00:45:22,200 --> 00:45:29,720
test accuracy of fully connected predictive coding models.

512
00:45:29,720 --> 00:45:35,400
So let's move to the first experiment, which is to infer the structure of the graph.

513
00:45:35,400 --> 00:45:39,880
And the experiments, they all follow basically the same pipeline in all the papers in the

514
00:45:39,880 --> 00:45:40,880
field.

515
00:45:40,880 --> 00:45:46,320
The first step is to generate a Bayesian network from random graph.

516
00:45:46,320 --> 00:45:51,560
So basically normally the two random graphs that everyone tests are Erdos-Renis graphs

517
00:45:51,560 --> 00:45:53,680
and scale-free graphs.

518
00:45:53,680 --> 00:46:00,560
So you generate those big graphs that normally have 20, 40, 80, 80 different nodes and some

519
00:46:00,560 --> 00:46:04,920
edges that you sample randomly.

520
00:46:04,920 --> 00:46:08,600
And you use this graph to generate a data set.

521
00:46:08,600 --> 00:46:13,840
So you sample, for example, N, big N data points.

522
00:46:13,840 --> 00:46:18,480
And what you do is that you take the graph that you have generated earlier and you throw

523
00:46:18,480 --> 00:46:19,480
it away.

524
00:46:19,480 --> 00:46:21,160
You only keep the data set.

525
00:46:21,160 --> 00:46:27,920
And the task you want to solve now is to have a training algorithm that basically allows

526
00:46:27,920 --> 00:46:34,920
you to retrieve the structure of the graph you have thrown away.

527
00:46:34,920 --> 00:46:39,040
So the way we do it here is that we train a fully connected predictive coding model on

528
00:46:39,040 --> 00:46:46,320
this data set D, using both the sparse and the acyclic priors we have defined earlier.

529
00:46:46,320 --> 00:46:53,920
You can see whether actually the graph that we converge to, after pruning away the entries

530
00:46:53,920 --> 00:46:59,640
of the agency matrix that are smaller than a certain threshold, is similar to that of

531
00:46:59,640 --> 00:47:02,840
the initial graph.

532
00:47:02,840 --> 00:47:05,560
And the results show that this is actually the case.

533
00:47:05,560 --> 00:47:13,600
So this is an example and I show many different parametrizations and dimensions and things

534
00:47:13,600 --> 00:47:15,040
like that in the paper.

535
00:47:15,640 --> 00:47:19,800
But I think those two are the most representative examples with an air nosher in a graph and

536
00:47:19,800 --> 00:47:23,920
a free scale graph with 20 nodes.

537
00:47:23,920 --> 00:47:31,160
And here on the left, you can see the ground truth graph, which is the one sampled randomly.

538
00:47:31,160 --> 00:47:36,240
And on the right, you can see the graph, the predictive coding model as learned from the

539
00:47:36,240 --> 00:47:37,240
data set.

540
00:47:37,240 --> 00:47:40,760
And as you can see, they are quite similar.

541
00:47:40,760 --> 00:47:42,640
It's still not perfect.

542
00:47:42,640 --> 00:47:48,840
So there are some errors, but in general, the structures, they work quite well.

543
00:47:48,840 --> 00:47:54,240
We also have some quantitative experiments that I don't show here, because they're just

544
00:47:54,240 --> 00:47:58,240
huge tables with a lot of numbers and I thought it was maybe a little bit too much for the

545
00:47:58,240 --> 00:47:59,240
presentation.

546
00:47:59,240 --> 00:48:06,400
But there is also that they perform similarly to contemporary methods.

547
00:48:06,400 --> 00:48:12,520
Also because I have to say most of the quality comes from the acyclic priors that was introduced

548
00:48:12,520 --> 00:48:13,520
in 2018.

549
00:48:17,120 --> 00:48:22,760
The second class of experiments are classification experiments, which as I said, are the extensions

550
00:48:22,760 --> 00:48:25,880
of the one I shared earlier.

551
00:48:25,880 --> 00:48:31,000
And the idea is to use structure learning to improve the classification results on the

552
00:48:31,000 --> 00:48:36,960
means and fashion means data set, starting from a fully connected graph.

553
00:48:36,960 --> 00:48:44,000
So what I did is that I divided the fully connected graph in clusters of neurons.

554
00:48:44,000 --> 00:48:49,440
So 1B cluster is the one related to the input.

555
00:48:49,440 --> 00:48:55,680
And then we have some specific number of hidden clusters.

556
00:48:55,680 --> 00:49:02,200
And then we have the label cluster, which is the cluster of neurons that are supposed

557
00:49:02,200 --> 00:49:06,680
to give me the label predictions.

558
00:49:06,680 --> 00:49:10,400
And I've trained them using the first time, the sparse prior only.

559
00:49:10,400 --> 00:49:19,320
So the idea is, what if I prune the connections I don't need from a model and learn a sparser

560
00:49:19,320 --> 00:49:20,320
model?

561
00:49:20,320 --> 00:49:21,800
Does this work?

562
00:49:21,800 --> 00:49:23,480
Well, the answer is no.

563
00:49:23,480 --> 00:49:24,680
It doesn't work.

564
00:49:24,680 --> 00:49:31,840
And the reason why is that at the end, the graph that you converge with is actually degenerate.

565
00:49:31,840 --> 00:49:37,200
So basically, the model learns to predict the label based on the label itself.

566
00:49:37,200 --> 00:49:41,920
So it discards all the information from the input and only keeps the label.

567
00:49:41,920 --> 00:49:44,960
And as you can see here, the label y predicts itself.

568
00:49:44,960 --> 00:49:49,200
Or in other experiments, when you change the parameters, you have that y predicts at zero,

569
00:49:49,200 --> 00:49:53,000
that predicts x1, that predicts y again.

570
00:49:53,000 --> 00:49:56,360
So what's the solution to this problem?

571
00:49:56,360 --> 00:50:03,440
Well, the solution to this problem is that we have to converge to an acyclic graph.

572
00:50:03,440 --> 00:50:07,000
And so we have to add something that prevents acyclicity.

573
00:50:07,000 --> 00:50:08,920
And what is that?

574
00:50:08,920 --> 00:50:10,840
One is, of course, the one I already proposed.

575
00:50:10,840 --> 00:50:14,840
And then I show a second technique.

576
00:50:14,840 --> 00:50:19,600
So the first one uses the acyclic prior defined earlier.

577
00:50:19,600 --> 00:50:24,600
And the second one is a novel technique that actually makes use of negative examples.

578
00:50:24,600 --> 00:50:31,640
So a negative example in this case is simply a data point in which you have an image, but

579
00:50:31,640 --> 00:50:33,440
the label is wrong.

580
00:50:33,440 --> 00:50:37,360
So here, for example, you have an image of a 7, but the label that I'm giving the model

581
00:50:37,360 --> 00:50:41,360
is a 2.

582
00:50:41,360 --> 00:50:48,040
And the idea is very simple and has been used in a lot of works already.

583
00:50:48,040 --> 00:50:54,280
So every time the model sees a positive example, it has to minimize the variational free energy.

584
00:50:54,280 --> 00:50:59,120
And every time it sees a negative example, it has to increase it.

585
00:50:59,120 --> 00:51:05,680
So we will want this quantity to be minimized.

586
00:51:05,680 --> 00:51:10,680
And actually, with a lot of experiments and a lot of experimentations, we saw that the

587
00:51:10,680 --> 00:51:17,440
two techniques basically first lead to the same results and second lead to the same graph

588
00:51:17,440 --> 00:51:18,440
as well.

589
00:51:18,840 --> 00:51:26,480
So here are the new results on means and fashion means using the two techniques that I just

590
00:51:26,480 --> 00:51:28,640
proposed.

591
00:51:28,640 --> 00:51:34,480
And now we move to some which are still not great, but definitely more reasonable test

592
00:51:34,480 --> 00:51:35,560
accuracies.

593
00:51:35,560 --> 00:51:42,560
So here we have a test error of 3.17 for means and a test error of 13.98 for fashion means.

594
00:51:42,680 --> 00:51:49,000
Actually, those results can be much improved by learning the structure of the graph on

595
00:51:49,000 --> 00:51:55,720
means and then fixing the structure of the graph and do some form of fine tuning.

596
00:51:55,720 --> 00:52:00,400
So if you fine tune the model on the correct hierarchical structure, at some point you

597
00:52:00,400 --> 00:52:04,720
reach the test accuracy, which is the one you would expect from a hierarchical model.

598
00:52:04,720 --> 00:52:11,120
But those ones are simply the one, the fully connected model as naturally converged to.

599
00:52:11,120 --> 00:52:17,400
So for example, from a test error of 18.32 of the fully connected model train on fashion

600
00:52:17,400 --> 00:52:23,560
means by simply performing correlations or conditional queries, which is the standard

601
00:52:23,560 --> 00:52:30,400
way of querying operative coding model, adding interventions and the acyclic prior together

602
00:52:30,400 --> 00:52:34,520
makes this test error much lower.

603
00:52:34,520 --> 00:52:38,120
And we can observe it for means as well.

604
00:52:39,120 --> 00:52:46,360
I'm now going a little bit into details on this last experiment and on how the acyclic

605
00:52:46,360 --> 00:52:49,760
prior acts on the structure of the graph.

606
00:52:49,760 --> 00:52:55,920
So I perform an experiment on a new dataset, which is, I mean, calling it a new dataset,

607
00:52:55,920 --> 00:53:01,240
it may be too much, is the, I called it a two means dataset in which you have the input

608
00:53:01,240 --> 00:53:07,960
point is formed of two different images and the label only depends on the second image.

609
00:53:08,680 --> 00:53:09,840
On the first image story.

610
00:53:10,840 --> 00:53:17,040
So the idea here is, is the structure of the model, the acyclic, the acyclicity prior and

611
00:53:17,040 --> 00:53:22,640
things like that able to recognize that the second half of the image is actually meaningless

612
00:53:22,800 --> 00:53:29,720
in, in performing, in learning the in performing classification.

613
00:53:31,280 --> 00:53:32,760
How does training behave in general?

614
00:53:32,760 --> 00:53:38,920
Like, for example, we have this input, input node, output node, and only the nodes are

615
00:53:38,920 --> 00:53:46,080
fully connected and the model converge to a hierarchical structure, which is the one

616
00:53:46,400 --> 00:53:50,040
that we know performs the best on, on classification tasks.

617
00:53:51,080 --> 00:53:55,960
Well, here is a, is an example of a training method of a training run.

618
00:53:56,480 --> 00:53:59,640
So that's C zero, which is the beginning of training.

619
00:54:00,880 --> 00:54:01,960
We have this model here.

620
00:54:02,040 --> 00:54:07,920
So as zero corresponds to the, to the seven, so to the first image as one corresponds to

621
00:54:07,920 --> 00:54:12,640
the second image, again, we have the label Y and all the latent variables X zero X one

622
00:54:12,640 --> 00:54:15,240
X two, and the model is fully connected.

623
00:54:15,280 --> 00:54:18,720
So the agency matrix is, is full of ones.

624
00:54:19,000 --> 00:54:20,160
There are, there are no zeros.

625
00:54:20,320 --> 00:54:22,040
We have self loops and things like that.

626
00:54:23,640 --> 00:54:30,000
We train them at the model for a couple of epochs until, and what we note immediately is

627
00:54:30,040 --> 00:54:34,680
that, for example, the, the model immediately understands that the four is not needed to

628
00:54:34,680 --> 00:54:35,760
perform classification.

629
00:54:36,240 --> 00:54:36,840
So it doesn't.

630
00:54:37,880 --> 00:54:42,880
So every outgoing node from the, from the second input cluster is removed.

631
00:54:44,160 --> 00:54:48,920
And something we didn't understand is that this is, this cluster is the one related to

632
00:54:48,920 --> 00:54:49,480
the output.

633
00:54:50,560 --> 00:54:58,280
So we have a, we have a linear map from S zero to Y directly, which is this part here.

634
00:54:59,240 --> 00:55:05,720
But we know that actually a linear map is not the best map for, for performing classification

635
00:55:05,720 --> 00:55:06,200
on means.

636
00:55:06,440 --> 00:55:07,880
So we, we need some hierarchy.

637
00:55:07,880 --> 00:55:10,520
We need some depth to, to improve the results.

638
00:55:10,920 --> 00:55:17,240
And as you can see, this line here is the, is the accuracy, which up to this point, so

639
00:55:17,240 --> 00:55:23,640
up to C2 is similar to a, so it's 91%, which is slightly, slightly better than linear

640
00:55:23,640 --> 00:55:24,440
classification.

641
00:55:25,400 --> 00:55:30,840
But once you go on with the training, the model understands that it needs some hierarchy

642
00:55:30,840 --> 00:55:32,000
to better fit the data.

643
00:55:32,840 --> 00:55:39,760
So you, you see that this arrow starts getting stronger and stronger over time until it, it

644
00:55:39,760 --> 00:55:44,480
understands that the linear map is not actually really needed and it removes it.

645
00:55:45,400 --> 00:55:50,360
And so the, so the model you converge with is a model that starts from a zero, goes to

646
00:55:50,680 --> 00:55:58,040
a hidden node and then goes to the, to the label with a very weak linear map, which actually

647
00:55:58,040 --> 00:56:03,480
gets removed if you, if you set that threshold of, if you set that threshold of, for example,

648
00:56:03,480 --> 00:56:06,920
0.1, 0.2, at some point, the linear map gets forgotten.

649
00:56:07,240 --> 00:56:11,800
And everything you end up with is with a, is with a hierarchical network.

650
00:56:13,400 --> 00:56:18,120
That is, that is, so it has learned the correct structure to, to perform classification tasks,

651
00:56:18,120 --> 00:56:19,080
which is a hierarchy.

652
00:56:19,480 --> 00:56:25,000
And it has also learned that the second image didn't play any role in defining the,

653
00:56:25,560 --> 00:56:26,840
the test accuracy.

654
00:56:26,840 --> 00:56:29,480
And this is all, this is all performed.

655
00:56:29,480 --> 00:56:35,320
So all those jobs are simply performed by, performed by one free energy minimization

656
00:56:35,320 --> 00:56:36,120
process.

657
00:56:36,120 --> 00:56:40,040
So you initialize the model, you define the free energy, you define the priors.

658
00:56:40,040 --> 00:56:46,120
So the, the sparse and the cyclic prior, you run the, the energy minimization and you converge

659
00:56:46,120 --> 00:56:50,760
to hierarchical, to a hierarchical model, which is well able to perform classification on minced.

660
00:56:51,880 --> 00:56:56,760
And then if you then perform some fine tuning, you reach very competitive results as you do in

661
00:56:56,760 --> 00:56:59,720
feed forward networks with the, with back propagation.

662
00:56:59,720 --> 00:57:01,720
But I think that's not the interesting bit.

663
00:57:01,720 --> 00:57:05,400
The interesting bit is that you, like all this process, this process altogether

664
00:57:06,040 --> 00:57:11,880
of intervention and the acyclicity allows you to take a fully connected network

665
00:57:12,680 --> 00:57:18,040
and converge to a hierarchical one that is, that is able to perform classification with good results.

666
00:57:20,840 --> 00:57:24,600
And yeah, that's basically it.

667
00:57:24,600 --> 00:57:27,160
I'm now, oh yeah, wow, I've talked a lot.

668
00:57:27,160 --> 00:57:34,680
And I'm, this is the conclusion of the talk, which is, I'm basically doing a small summary.

669
00:57:34,680 --> 00:57:39,320
And I think the, the important takeaway if I have to give even one sentence of this paper

670
00:57:39,400 --> 00:57:45,560
is that predictive coding is a belief updating method that is able to perform end to end causal

671
00:57:45,560 --> 00:57:51,560
learning. So it's able to perform interventions to learn a structure from data and then perform

672
00:57:51,560 --> 00:57:59,080
interventions and counterfactuals. So causal inference in natural and efficiency model

673
00:57:59,080 --> 00:58:01,880
interventions by simply setting the prediction error to zero.

674
00:58:01,880 --> 00:58:05,800
So it's a, it's a very easy technique to perform interventions.

675
00:58:05,800 --> 00:58:09,480
And you simply only have to touch one neuron, you don't have to act on the structure of the graph.

676
00:58:10,920 --> 00:58:16,440
You can, you can use it to perform, to, to create structure causal models that are biologically

677
00:58:16,440 --> 00:58:24,440
plausible. It is able to learn the structure for, from data, as I said, maybe a lot of times already.

678
00:58:25,400 --> 00:58:29,480
And, and a couple of sentences about future works is that

679
00:58:30,200 --> 00:58:35,080
something that would be nice to do is to improve the performance of the model we,

680
00:58:35,080 --> 00:58:40,520
we have defined, because I think it performs reasonably well on a lot of tasks.

681
00:58:40,520 --> 00:58:45,880
So it performs reasonably well on structure learning on, for me, intervention and counterfactuals.

682
00:58:47,080 --> 00:58:51,480
But actually, if you look at state of the art model, there's always like a very specific method

683
00:58:51,480 --> 00:58:57,560
that performs better in a, in the single task. So it would be interesting to see if we can

684
00:58:58,040 --> 00:59:04,360
reach those level of performance in, in specific tasks by, by adding some tricks on, or some,

685
00:59:07,080 --> 00:59:12,600
or some new optimization methods, and to generalize it to, to dynamical systems,

686
00:59:12,600 --> 00:59:17,960
which are actually much more interesting, the static systems. So such as dynamical causal models

687
00:59:17,960 --> 00:59:24,600
and, or other techniques that allow you to perform causal inference in systems that move.

688
00:59:24,600 --> 00:59:31,080
So an action taken in a specific time step influences another node in a later time step,

689
00:59:31,080 --> 00:59:38,920
which is basically Granger causality. Yeah, that's it. And thank you very much.

690
00:59:47,560 --> 00:59:52,760
Thank you. Awesome. And very comprehensive presentation. That was really

691
00:59:53,480 --> 01:00:01,480
muted. Sorry, muted on zoom. But yes, thanks for the awesome and very comprehensive

692
01:00:02,360 --> 01:00:07,480
presentation. There was really a lot there. And there was also a lot of great questions

693
01:00:07,480 --> 01:00:13,960
in the live chat. So maybe to warm into the questions, how did you come to study this

694
01:00:14,520 --> 01:00:20,200
topic? Were you studying causality and found predictive coding to be useful or vice versa?

695
01:00:20,200 --> 01:00:25,720
Or how did you come out this intersection? I actually have to say that the first person

696
01:00:25,720 --> 01:00:34,280
that came out with this idea was, was better. So, so like, like, I think a year and a half ago,

697
01:00:34,280 --> 01:00:40,600
even more, he wrote like a page with this idea. And then he got forgotten, and no one picked it up.

698
01:00:41,160 --> 01:00:45,880
And, and last summer, I started getting curious about causality and

699
01:00:46,120 --> 01:00:52,200
I read, for example, the book of why as I listen into podcasts, I don't know the

700
01:00:52,200 --> 01:00:56,520
standard way in which you get interested in a topic. And, and I remember this,

701
01:00:56,520 --> 01:01:02,360
this idea from Baron and proposed it to him. And I was like, why don't we expand it and,

702
01:01:02,360 --> 01:01:07,880
and actually make it a paper. So I, I involve some people to work with experiments and,

703
01:01:08,520 --> 01:01:13,560
and this is the final result at the end. Awesome. Cool. Yeah.

704
01:01:14,120 --> 01:01:19,640
Um, a lot to say. I'm just going to go to the live chat first and address a bunch of different

705
01:01:19,640 --> 01:01:22,680
questions. And if anybody else wants to add more, I'm going to turn the light on first,

706
01:01:22,680 --> 01:01:26,200
because I'm, I think I'm getting in the dark more and more. Yes.

707
01:01:28,360 --> 01:01:33,400
Who said active inference can't solve the dark room issue? Oh, yes, here we are.

708
01:01:34,920 --> 01:01:37,880
So would you say the light switch caused it to be lighter?

709
01:01:38,200 --> 01:01:49,240
Yeah, I think so. No issues here. Um, okay. ML Don wrote since in predictive coding,

710
01:01:49,240 --> 01:01:54,040
all distributions are usually Gaussian, the bottom up messages are precision weighted

711
01:01:54,040 --> 01:01:57,880
prediction errors where precision is the inverse of the Gaussian covariance.

712
01:01:58,520 --> 01:02:01,320
What if non Gaussian distributions are used?

713
01:02:01,640 --> 01:02:10,200
Is, um, basically the general method stays, the different, the main difference is that you,

714
01:02:11,000 --> 01:02:15,720
you don't have prediction errors, which, uh, as was correctly pointed out is the,

715
01:02:15,720 --> 01:02:21,320
basically the derivative of the variational free energy. If you have Gaussian assumptions,

716
01:02:22,920 --> 01:02:25,880
yeah, you don't have that single quantity to set to zero.

717
01:02:26,520 --> 01:02:32,120
And you probably will have to act on the structure of the graph to perform interventions.

718
01:02:34,040 --> 01:02:40,280
And also you, uh, and colleagues had a paper in 2022 predictive coding beyond Gaussian

719
01:02:40,280 --> 01:02:42,680
distributions that, that looked at some of these issues, right?

720
01:02:43,880 --> 01:02:49,160
Yes, yes, exactly. So that paper was a little bit, the idea behind that paper is, uh,

721
01:02:50,520 --> 01:02:55,240
and we model transformers. That's the biggest motivation using predictive coding.

722
01:02:55,320 --> 01:03:00,680
And the answer is, uh, is no, because the, the attention mechanism as a softmax at the end,

723
01:03:00,680 --> 01:03:06,520
and softmax calls to, uh, like not to Gaussian distribution, but to,

724
01:03:08,520 --> 01:03:12,200
yeah, to softmax distribution, the, I don't get the name now, but yes.

725
01:03:13,560 --> 01:03:17,080
And, uh, so yes, that's a generalization. It's a little bit

726
01:03:17,720 --> 01:03:21,000
tricky to call it. Once you remove the Gaston assumption is a little bit

727
01:03:21,000 --> 01:03:24,680
still tricky to call it predictive coding. So it's a,

728
01:03:26,520 --> 01:03:30,040
so for, for example, like talking to, uh, to Carl Freestone,

729
01:03:31,640 --> 01:03:36,120
like predictive coding is only if you, if you have only Gaussian, Gaussian assumptions.

730
01:03:37,880 --> 01:03:40,440
But yes, that's more a philosophical debate than, uh,

731
01:03:42,760 --> 01:03:48,120
Interesting. And another, I think topic that, that's definitely of, of great interest is

732
01:03:48,120 --> 01:03:53,640
similarities and differences between the attention apparatus in transformers

733
01:03:54,360 --> 01:03:59,960
and the way that attention is described from a neurocognitive perspective and from a predictive

734
01:03:59,960 --> 01:04:04,040
processing precision waiting angle. What do you, what do you think about that?

735
01:04:06,440 --> 01:04:13,880
Well, the idea is that, um, yeah, I think about it is that in from a pretty processing

736
01:04:13,880 --> 01:04:18,440
and, uh, and also operational inference perspective, attention can be seen as a,

737
01:04:18,440 --> 01:04:22,280
as a kind of structure learning problem. There's a, I think there's a recent paper from,

738
01:04:23,080 --> 01:04:29,000
from Chris Buckley's group that shows that there should be, there should be a reprint on archive

739
01:04:29,560 --> 01:04:32,520
in which basically they show that the attention mechanism is simply

740
01:04:33,240 --> 01:04:39,720
learning the, the precision on the, on the weight parameters specific to out to a data point.

741
01:04:39,800 --> 01:04:45,160
So this precision is not a, is not a, is not a parameter that is in the structure of the model.

742
01:04:45,160 --> 01:04:49,480
So it's not a model specific parameter. It is a fast changing parameter like the value nodes

743
01:04:50,040 --> 01:04:53,080
that gets updated while minimizing the version of free energy.

744
01:04:53,720 --> 01:04:57,000
And once they, once you've minimized it and compute it, then you throw it away.

745
01:04:57,000 --> 01:04:59,960
And from the next data point, you have to really compute it from scratch.

746
01:05:00,760 --> 01:05:07,160
So yes, I think the, the analogy computation wise is, uh, the attention mechanism can be seen as

747
01:05:07,160 --> 01:05:13,080
a kind of structure learning, but a structure learning that is data point specific and not

748
01:05:13,080 --> 01:05:17,960
model specific. And I think if you want to generalize a little bit and go from,

749
01:05:18,920 --> 01:05:23,000
from the attention mechanism in transformers to the attention mechanism cognitive science,

750
01:05:24,200 --> 01:05:29,720
I feel they're probably too different to, like to draw similarities and, uh,

751
01:05:31,240 --> 01:05:36,840
I think the structure learning analogy and the, how important one connection in is

752
01:05:36,840 --> 01:05:39,800
with respect to another one probably does job much better.

753
01:05:42,040 --> 01:05:49,560
Cool. Great answer. Okay. ML Don asks, in counterfactuals, what is the difference

754
01:05:49,560 --> 01:05:53,640
between hidden variables X and unobserved variables U?

755
01:05:55,560 --> 01:06:02,440
The difference is that you can, uh, I think the main one is that you cannot observe the,

756
01:06:02,520 --> 01:06:07,480
the use. You can use them because you can, you can compute them and fix them,

757
01:06:08,200 --> 01:06:12,440
but you cannot, the idea is that you have no control over them. So the use,

758
01:06:12,440 --> 01:06:17,720
the use should be seen as a environment specific variables that they are there. They,

759
01:06:17,720 --> 01:06:22,840
they influence your process. Okay. Because the, for example, when you go back in time,

760
01:06:22,840 --> 01:06:25,720
the environment is different. So the idea is for example, if you,

761
01:06:26,520 --> 01:06:31,720
like going back to the, to the example before of the, of the expected income of a person with

762
01:06:31,720 --> 01:06:38,760
a specific intelligence of education, uh, uh, education degree, the idea is that if I want to,

763
01:06:39,320 --> 01:06:45,320
to see how much I will learn today with a, with a, with a, I don't know, with a master degree,

764
01:06:45,320 --> 01:06:51,000
is different with respect to how much I would earn 20 years ago with a master degree is different.

765
01:06:51,000 --> 01:06:56,440
For example, here in Italy with respect to other countries and all those variables that are not

766
01:06:56,440 --> 01:07:01,400
under your control, you can not model them using your vision network, but they are there.

767
01:07:01,400 --> 01:07:06,040
Okay. So you, you cannot ignore them when you, when you want to draw conclusions.

768
01:07:06,760 --> 01:07:11,720
So it's, yeah, it's basically everything that you cannot control. You can infer them. So you

769
01:07:11,720 --> 01:07:16,840
can, you can perform a counter counterfactual inference back in time and say, Oh, 20 years

770
01:07:16,840 --> 01:07:23,400
ago, I would have earned this much if I, if I was disintelligent at this degree on average,

771
01:07:23,400 --> 01:07:29,000
of course. And, but it's not that I can change the government policies towards jobs or the,

772
01:07:29,640 --> 01:07:37,240
or things like that. It's a deeper counterfactual. Yes, exactly. So yeah, those are the use.

773
01:07:38,440 --> 01:07:44,200
Awesome. All right. Have you implemented generalized coordinates in predictive coding?

774
01:07:45,960 --> 01:07:52,600
No, I've, no, I've never done it. I've, uh, yeah, I've studied it, but I've, I've never

775
01:07:52,680 --> 01:07:59,880
implemented it. I know they tend to be unstable and, uh, and it's very hard to make them stable.

776
01:07:59,880 --> 01:08:06,200
I think that's the, that's the takeaway that I got from talking to people that have implemented them.

777
01:08:08,440 --> 01:08:13,960
But, but yeah, yeah, I'm aware of some papers that came out actually recently about them that,

778
01:08:13,960 --> 01:08:18,840
that tested on some threshold encoder style. Actually, I think still from Baron,

779
01:08:19,480 --> 01:08:25,400
there's a, there's a paper out there that came out last summer, but no, I've never played them with

780
01:08:25,400 --> 01:08:33,960
them myself. Cool. From Bert, does adding more levels in the hierarchy reduce the distraction

781
01:08:33,960 --> 01:08:42,920
problem of predicting input? Adding more level in, uh, in which sense, because the

782
01:08:42,920 --> 01:08:48,600
destruction problem is given by cycles. So basically you provide an image and the fact that you have

783
01:08:48,840 --> 01:08:56,040
a, so edges going out of the image, going in the, in the neurons, and then other edges going back,

784
01:08:57,640 --> 01:09:03,400
the, this basically creates the fact that you have a, that the error of, that those basically,

785
01:09:03,960 --> 01:09:09,000
these ingoing edges to the pixels of the image, they create some prediction errors. So you have

786
01:09:09,000 --> 01:09:14,200
some prediction errors that get spread inside the model. And that's, yeah, and this problem,

787
01:09:14,200 --> 01:09:19,400
I think is general of cycles. And it's probably not related to hierarchy in general.

788
01:09:21,000 --> 01:09:26,360
So it's, it's, it's the two incoming edges to the pixels. If you don't have incoming edges,

789
01:09:26,360 --> 01:09:33,800
you have no, uh, no distraction problem anymore. Cool. And, and the specification of the acyclic

790
01:09:33,800 --> 01:09:43,240
network through the trace operator, that's a very interesting technique. And when was that

791
01:09:43,320 --> 01:09:44,280
brought into play?

792
01:09:46,840 --> 01:09:53,320
As far as I know, I think it came out with a paper I, I cited in 2018. I, I don't know,

793
01:09:53,320 --> 01:09:59,000
at least in the causal inference literature, I'm, I'm not aware of any previous methods.

794
01:09:59,000 --> 01:10:04,120
I would say no, because that, I mean, that's the highly cited paper. So I would say they came out

795
01:10:04,120 --> 01:10:09,240
with that idea. Wow. Yeah. That's, that's quite nice that you can do gradient descent and learn

796
01:10:09,240 --> 01:10:15,000
the structure. I think that's a, that's a very powerful technique. Yeah. Sometimes it's like

797
01:10:15,000 --> 01:10:21,000
when you look at when different features of Bayesian inference and causal inference became

798
01:10:21,000 --> 01:10:28,680
available, it's really remarkable. Like why, why, why hasn't this been done under a Bayesian causal

799
01:10:28,680 --> 01:10:35,800
modeling framework? It's like, because there's only been like five to 25 years of this happening.

800
01:10:36,680 --> 01:10:42,360
And so that's very, very short. And also it's relatively technical. So there's relatively

801
01:10:42,360 --> 01:10:48,280
few research groups engaging in it. And it's just really cool what it's enabling.

802
01:10:49,800 --> 01:10:54,200
No, yes, yes, exactly. I mean, that's also, I think the exciting part of this field a little bit

803
01:10:54,200 --> 01:10:59,640
that is, I mean, there are definitely break breakthroughs out there that, that still have

804
01:10:59,640 --> 01:11:05,080
to be discovered and probably like, for example, like, or as much as a breakthrough that paper was

805
01:11:05,880 --> 01:11:12,920
they found like, they simply found out the right prior for acyclic structures. Okay, it's a

806
01:11:14,120 --> 01:11:19,640
yeah, I mean, I, I don't know exactly, but it may be an idea that you have in one afternoon.

807
01:11:19,640 --> 01:11:24,520
I don't know about the story of the, how the authors came up with that, but could potentially be

808
01:11:24,520 --> 01:11:29,080
that if they, they are there at the whiteboard, you're like, Oh, that actually works. That's a

809
01:11:29,080 --> 01:11:36,280
huge breakthrough. And I simply defined the prior. And also a lot of these breakthroughs,

810
01:11:37,160 --> 01:11:46,120
they, they don't just stack. It's not like a, a tower of blocks, they layer and they compose.

811
01:11:46,920 --> 01:11:51,000
So then something will be generalized to generalized coordinates or generalized

812
01:11:51,000 --> 01:11:57,800
synchrony or arbitrarily large graphs or sensor fusion with multimodal inputs. And it's like those

813
01:11:57,800 --> 01:12:04,920
all blend in really satisfying and effective ways. So, so even little things that again,

814
01:12:04,920 --> 01:12:13,160
someone can just come up with in a moment can really have impact. Okay, ML Don says,

815
01:12:13,160 --> 01:12:18,200
thanks a lot for asking my questions and thanks a million to Tomaso for the inspiring presentation.

816
01:12:18,200 --> 01:12:22,120
So nice. Thank you very much. And then Bert asks,

817
01:12:23,080 --> 01:12:28,120
how would language models using predictive coding differ from those using transformers?

818
01:12:32,520 --> 01:12:37,320
Okay, I think that actually, if I would have to build today a language model using predictive

819
01:12:37,320 --> 01:12:41,720
coding, I would still use transformers. So the idea is that, for example, if you have a,

820
01:12:42,760 --> 01:12:49,480
let's say this hierarchical graphical model, or this hierarchical Bayesian network,

821
01:12:50,440 --> 01:12:56,040
I've defined in the, in the very first slides, one arrow to encode a function, which is the linear

822
01:12:56,040 --> 01:13:01,720
map. Okay, so one arrow was simply the multiplication of a, of the vector encoded in the latent

823
01:13:01,720 --> 01:13:08,200
variables times the, this weight matrix that you can then make non-linear and things like that.

824
01:13:08,200 --> 01:13:12,200
But that can be actually something much more complex. The, the function encoded in the arrow

825
01:13:12,280 --> 01:13:19,560
can be a convolution, can be an attention mechanism. So, so actually how I would do it,

826
01:13:19,560 --> 01:13:26,680
I will still use the, I mean, which is actually the way we did it in, in, in the Oxford group last

827
01:13:26,680 --> 01:13:32,840
year is that we, we had exactly the structure. Every arrow is a transformer now. So one is

828
01:13:32,840 --> 01:13:37,240
the attention mechanism and the, the next one is the feed forward network as transformers.

829
01:13:38,280 --> 01:13:42,040
And basically the only difference that you have is that those variables you want to compute the

830
01:13:42,040 --> 01:13:47,240
posterior and you make those posterior's independence, independent via, via mean field

831
01:13:47,240 --> 01:13:52,360
approximation. So basically you follow all the steps that allow you to, to converge to the

832
01:13:52,360 --> 01:13:57,400
very, to the variational free energy of creative coding. But the, the way, the way you compute

833
01:13:57,400 --> 01:14:06,120
predictions and the way you, you send signals back is a, is done via transformer. So I will

834
01:14:06,120 --> 01:14:11,720
still use transformers in general. I mean, they work so well that I, I don't think that we can be

835
01:14:12,120 --> 01:14:16,440
arrogant and say, oh no, I'm going to do it better via a purely predictive coding way.

836
01:14:17,640 --> 01:14:21,720
Structure learning is a way to do it, but we'll still approximate transformers anyway.

837
01:14:22,520 --> 01:14:26,680
So you said structure learning would approximate the transformer approach?

838
01:14:27,560 --> 01:14:33,240
Yes. Destruction learning I mentioned earlier in, when, when someone asked the similarities

839
01:14:33,240 --> 01:14:36,040
between predictive coding and the attention mechanism.

840
01:14:38,360 --> 01:14:46,040
Very, yeah, very interesting. One thing I am wondering from MLBong, I could not see the

841
01:14:46,040 --> 01:14:50,600
concept of depth in the predictive coding networks you mentioned. Most likely I missed it. The

842
01:14:50,600 --> 01:14:57,800
definition provided for predictive coding involved the concept of depth. What did you mean by depth?

843
01:14:58,360 --> 01:15:05,560
No, yes, it's true. It's a, because the standard definition, as I said, multiple times is a,

844
01:15:05,560 --> 01:15:09,240
is hierarchical. You have predictions going one directions and prediction error going the

845
01:15:09,240 --> 01:15:15,720
opposite direction. Basically, what, what we did in, in this paper and also in the last one in

846
01:15:16,520 --> 01:15:20,360
which is called the learning on arbitrary graph topologies via predictive coding

847
01:15:21,080 --> 01:15:30,840
is that we can consider depth like as a, as independent, basically pair of latent variable,

848
01:15:30,840 --> 01:15:35,640
latent variable, and arrow. And you have predictions going that direction and prediction

849
01:15:35,640 --> 01:15:41,960
error going the other. But then you can compose these in how many, a lot of ways. So you can,

850
01:15:42,920 --> 01:15:47,960
you can, so basically this composition doesn't have to be hierarchical in the end.

851
01:15:48,680 --> 01:15:51,720
Can have cycles. So then you can, for example, plug in another,

852
01:15:53,000 --> 01:15:57,480
another latent variable to the first one, and then connect the other two. And you can have a

853
01:15:57,480 --> 01:16:03,320
structure that is as entangled as you want. So for example, in the, in the other paper, we train

854
01:16:03,320 --> 01:16:09,000
the, a network that has the shape of a brain structure. So we have a lot of brain regions

855
01:16:09,000 --> 01:16:15,000
that are sparsely connected inside and sparsely connected among each other. And, and there's,

856
01:16:15,000 --> 01:16:18,760
there's nothing hierarchical there at the end, but you can still train it by minimizing

857
01:16:18,760 --> 01:16:23,400
a ratio of free energy and by minimizing the, the total prediction error of the network.

858
01:16:25,800 --> 01:16:35,640
So you could have for a given motif in a entangled graph, you might see three successive layers

859
01:16:35,640 --> 01:16:39,240
that when you looked at them alone, you'd say, Oh, that's a three story building.

860
01:16:39,240 --> 01:16:44,520
That's a three layer model that has a depth of three. But then when you take a bigger picture

861
01:16:45,160 --> 01:16:50,840
there isn't like an explicit top or an explicit bottom to that network.

862
01:16:52,200 --> 01:16:55,880
Yes, exactly. And this is basically given by the, by the fact that every operation in

863
01:16:55,880 --> 01:17:01,640
creative coding networks is strictly local. So, so basically every message passing every

864
01:17:01,640 --> 01:17:06,760
prediction and every prediction error that you send, you only send it to the very nearby neurons.

865
01:17:07,320 --> 01:17:11,160
Okay. And whether the global structure is actually hierarchical or not,

866
01:17:11,880 --> 01:17:14,840
the, the single message passing doesn't even see that.

867
01:17:17,480 --> 01:17:27,000
I guess that's sort of the hope for learning new model architectures is the space of what is

868
01:17:27,880 --> 01:17:34,840
designed top down is very small and a lot of models in use today,

869
01:17:35,480 --> 01:17:42,600
albeit super effective models. Although you could ask effective per unit of compute or not,

870
01:17:42,600 --> 01:17:47,320
that's a second level question. But a lot of effective models today do not have some of these

871
01:17:47,320 --> 01:17:55,240
properties of predictive coding networks, like their capacity to use only local computations,

872
01:17:55,880 --> 01:18:04,760
which gives biological realism or just spatio temporal realism, but also may provide a lot

873
01:18:04,760 --> 01:18:08,920
of advantages in like federated compute or distributed computing settings.

874
01:18:10,600 --> 01:18:15,960
No, yes, exactly. I completely agree. I think the idea in general is that, and I don't know if

875
01:18:15,960 --> 01:18:20,120
that's going to be an advantage. I think it's very promising exactly for the reasons you said.

876
01:18:20,920 --> 01:18:24,920
And the reason is that today's models string with back propagation,

877
01:18:24,920 --> 01:18:32,840
you can basically summarize them as a model string back propagation is a function,

878
01:18:32,840 --> 01:18:37,640
because basically you have a map from input to output, and back propagation basically

879
01:18:38,280 --> 01:18:45,160
spreads information back from its computational graph. So every neural network model used today

880
01:18:46,040 --> 01:18:51,960
is a function. While predictive coding and not only predictive coding, like the whole class of

881
01:18:51,960 --> 01:18:58,360
functions, the class of methods that train in using local computations and actually work by

882
01:18:58,360 --> 01:19:05,000
minimizing a global energy function, they're not limited to model functions from input to output.

883
01:19:05,000 --> 01:19:09,960
They actually model something that kind of resembles physical systems. So you have a physical

884
01:19:09,960 --> 01:19:16,200
system, you fix some values to whatever input you have, and you let the system converge,

885
01:19:16,200 --> 01:19:21,320
and then you read some other value of neurons or variables that are supposed to be output.

886
01:19:21,320 --> 01:19:26,040
But this physical system doesn't have to be a feedforward map. It doesn't have to be a function

887
01:19:26,040 --> 01:19:32,200
that has an input space and an output space, and that's it. So the class of models that you can

888
01:19:32,200 --> 01:19:38,760
learn is also basically you can see like feedforward models and functions, and then a much bigger

889
01:19:38,760 --> 01:19:43,640
class, which is that of physical systems. Whether there's something interesting out here, I don't

890
01:19:43,640 --> 01:19:48,280
know yet, because the functions are working extremely well. We are seeing those days with

891
01:19:48,280 --> 01:19:54,440
back propagation, they work crazy well. So yeah, I don't know if there's anything interesting in

892
01:19:54,440 --> 01:20:00,520
the big part, but the big part is quite big. There are a lot of models that you cannot

893
01:20:01,160 --> 01:20:03,800
train with back propagation, and you can train with predictive coding,

894
01:20:04,680 --> 01:20:11,240
or a background propagation or other methods. That is super interesting. Certainly biological

895
01:20:11,240 --> 01:20:18,200
systems, physical systems solve all kinds of interesting problems. But there's still no free

896
01:20:18,200 --> 01:20:23,640
lunch, and ant species does really well in this environment might not do very well in another

897
01:20:23,640 --> 01:20:31,000
environment. And so out there in the in the hinterlands, there might be some really unique

898
01:20:31,080 --> 01:20:37,080
special algorithms that are not well described by being a function,

899
01:20:38,520 --> 01:20:48,040
yet still provide like a procedural way to to implement heuristics, which might be extremely,

900
01:20:48,040 --> 01:20:54,920
extremely effective. No, yes, yes, exactly. And yeah, and I think this has been most of my

901
01:20:55,720 --> 01:21:01,640
focus of research during my PhD, for example, like finding this application that is like out

902
01:21:01,640 --> 01:21:12,040
here and not inside the the functions. Cool. Well, where does this work go from here? Like,

903
01:21:12,040 --> 01:21:18,600
what directions are you excited about? And how do you see people in the active inference ecosystem

904
01:21:18,600 --> 01:21:25,160
getting involved in this type of work? I think every probably the most promising

905
01:21:25,720 --> 01:21:32,760
direction, which is something maybe I would like to explore a little bit is to, as I said,

906
01:21:32,760 --> 01:21:39,640
there is to go behind statistical models. So everything I've shown so far is about static

907
01:21:39,640 --> 01:21:45,560
data. So the data don't change over time, there's no time inside the definition of

908
01:21:45,560 --> 01:21:50,200
predictive coding as it is as I presented it here. However, you can, for example,

909
01:21:50,200 --> 01:21:55,400
generalize predictive coding to to work with temporal data using generalized coordinates,

910
01:21:55,400 --> 01:22:03,000
as you mentioned earlier, by by presenting it as a as a Kalman Kalman filter generative model.

911
01:22:04,280 --> 01:22:10,440
And and that's where, for example, the causal inference direction could be very useful,

912
01:22:10,440 --> 01:22:16,600
because at that model, in at that point, maybe you can be able to model Granger causality and

913
01:22:17,400 --> 01:22:20,040
and more complex and and useful

914
01:22:21,960 --> 01:22:27,480
dynamical causal models, basically. Because in general, the the due calculus and the

915
01:22:27,480 --> 01:22:35,000
interventional and counterfactual branch of science is mostly developed on on small models.

916
01:22:35,960 --> 01:22:43,400
So it's like you don't do interventions on gigantic models in general. So if you if you

917
01:22:43,400 --> 01:22:50,280
look at medical data, they use relatively small vision networks. And but of course,

918
01:22:50,280 --> 01:22:56,200
if you want to have a dynamical causal model, that models a specific environment or a specific

919
01:22:56,200 --> 01:23:01,080
reality, you have a lot of neurons inside, you have a lot of latent variables, they change over

920
01:23:01,080 --> 01:23:07,000
time and an intervention at some more at some moment creates an effect in a different time step.

921
01:23:07,000 --> 01:23:11,560
So maybe the next time step in 10 different time steps later. And I think that would be

922
01:23:11,560 --> 01:23:16,840
very interesting to develop like a biologically plausible way of passing information

923
01:23:17,960 --> 01:23:21,480
that is also able to model Granger causality, basically.

924
01:23:24,680 --> 01:23:27,400
Where do you see action in these models?

925
01:23:27,400 --> 01:23:39,080
Where do I see action? I didn't think of that. I think I see actions in those models,

926
01:23:39,080 --> 01:23:41,960
maybe in the same way as I as you see in other models, because

927
01:23:43,080 --> 01:23:48,680
creative coding is basically a model of perception. So so an action is you can see

928
01:23:48,680 --> 01:23:54,680
that's a consequence of what you're experiencing. So by changing the way you're you're

929
01:23:55,320 --> 01:24:00,840
experiencing something, then you can compute maybe you can simply perform a smarter action

930
01:24:00,840 --> 01:24:08,280
now that you have more information. But but yeah, I don't think action is very easy. Like,

931
01:24:08,280 --> 01:24:13,080
yeah, I don't see any explicit consequence of actions, besides the fact that this can allow

932
01:24:13,080 --> 01:24:20,200
you to basically maybe to simply draw better conclusions to then perform actions in the future.

933
01:24:20,760 --> 01:24:26,760
I'll add on to that a few ways that people have talked about predictive coding and action.

934
01:24:27,720 --> 01:24:35,560
First off, internal action or covert action is attention. So we can think about perception

935
01:24:35,560 --> 01:24:40,920
as an internal action that that's one approach. Another approach pretty micro is the outputs

936
01:24:40,920 --> 01:24:48,440
of a given node. We can understand that node as a particular thing with its own sensory cognitive

937
01:24:48,440 --> 01:24:55,720
and action states. And so in that sense, the output of a node. And then lastly, which we

938
01:24:55,720 --> 01:25:00,520
explored a little bit in live stream 43, on the theoretical review on predictive coding,

939
01:25:01,080 --> 01:25:05,240
we're reading all the way through. And it was all about perception all about perception. And then

940
01:25:05,240 --> 01:25:15,400
it was like section 5.3. If you have expectations about action, then action is just another variable

941
01:25:15,480 --> 01:25:20,680
in this architecture. And that's really aligned with inactive inference, where instead of having

942
01:25:20,680 --> 01:25:26,440
like a reward or utility function that we maximize, we select action based upon it being the

943
01:25:26,440 --> 01:25:31,240
likeliest course of action, the path of least action, that's Bayesian mechanics. And so it's

944
01:25:31,240 --> 01:25:39,400
actually very natural to bring in an action variable and utilize it essentially as it as if it were

945
01:25:40,120 --> 01:25:46,840
a prediction about something else. Exteroceptively in the world, because we're also expecting action.

946
01:25:48,520 --> 01:25:54,760
No, yes, yes, exactly. No, I like the way of defining actions a lot, actually. And I still

947
01:25:54,760 --> 01:26:01,080
think if it's been like, for example, there are not so many papers that apply this method. I think

948
01:26:01,080 --> 01:26:07,080
there are a couple from from Alexander Orobrie does something similar. But in practice, like

949
01:26:07,880 --> 01:26:12,360
outside of the pure active inference, like applying predictive coding and actions to

950
01:26:13,160 --> 01:26:17,080
solve practical problems hasn't been explored a lot.

951
01:26:19,720 --> 01:26:25,640
Well, thank you for this excellent presentation and discussion. Is there anything else that you

952
01:26:25,640 --> 01:26:33,640
want to say or point people towards? No, just a big thank you for inviting me. And

953
01:26:34,600 --> 01:26:38,760
it was really fun. And I hope to come back at some point for for some future works.

954
01:26:40,280 --> 01:26:50,600
Cool. Anytime, anytime. Thank you, Thomas. So thank you, Daniel. See you. Bye. Bye.

955
01:27:03,640 --> 01:27:05,100
You

