{"text": " Okay, and we're away. Welcome, everybody, to the third episode of Active Inference Insights, brought to you by the Active Inference Institute. I'm your host, Darius Parveezy-Wayne, and today I am absolutely thrilled to be able to speak to John Viveke. John is an award-winning professor of psychology, cognitive science, and Buddhist philosophy at the University of Toronto. He is also the presenter of the renowned YouTube series Awakening from the Meaning Crisis, as well as the newer After Socrates. His work focuses on 4e cognitive science, which holds that cognition is embodied, embedded, enacted, and extended beyond the brain. In particular, John explores relevance realisation, our adaptive ability to zero in on salient information in a world of near-infinite complexity. Last year, he, Mark Miller, and Brett Anderson wrote the paper Predictive Processing and Relevance Realisation, exploring convergent solutions to the frame problem, which proposes that trade-offs in precision weighting, a key second order dynamic in predictive processing hierarchies, are at the heart of our ability to be intelligently ignorant. And that alignment of predictive processing and relevance realisation is exactly what we're going to be talking about today. John, welcome to the show. Thank you so much for joining us. It's an absolute treat. Thank you, there is a great pleasure. I'm hoping that Mark does make it on in December. That'll be great. Yeah, absolutely. But in his absence, because Mark was meant to be here today, but it's unfortunate, he's not unfortunately able to make it. Perhaps, so this podcast is acting as a kind of primer and introduction for our audience to the critical themes in cognitive science and psychology and biology, as well as maths and physics, as they are right now. So I think it would be a worthwhile place to start if you could just unpick what relevance realisation is, and then we'll come to how it aligns with predictive processing. Sure. So relevance realisation is sort of inverts the way common sense works. Common sense is there's a lot that is obvious to us. And you know what it's obvious what you should pay attention to. We're sometimes mistaken. But it's obvious what we should be remembering and what we should be doing. And that's all obvious. And we just run from that. Our job as cognitive scientists is to explain how the brain generates that obviousness, that salience landscapes that makes the right things stand out as relevant to us so that we can do what is astonishing, solve a wide variety of problems and a wide variety of domains. You're a general problem solver, which is just astonishing. I mean, you can learn Albanian history, you could learn about swimming or rock climbing, you can take up the study of dinosaurs in the Jurassic period. Like it's just, you know, it's just amazing and astonishing. And there's good empirical evidence you have something like a general intelligence. There seems to be a general capacity. There's individual variation in talent. Some people are better at learning this domain than other domains. But they're way back from Spearman. We know there's some sort of general ability. I'm going to propose that the general ability are two interlocking things, the anticipation and relevance realisation. What I mean by a general ability is these are kind of meta problems. These are the two big problems you have to solve when you're doing any other specific problem solving. And what makes relevance realisation so hard, generating that obviousness so mysterious, actually. And that's what Scott Atrin says. He says that what science does is it makes what's common sense to take, takes to be obvious, mysterious. Here's a table in front of me, but physics says, well, what is that really? And they're mysterious and it's made out of quarks and all this sort of stuff. It's the same sort of thing here. Because when you think about it outside of common sense, you realise that there's just an overwhelming amount of information that's constantly, as you said, dynamically changing in the environment. There's an overwhelming amount of information in your long-term memory that's constantly changing and being readjusted to the reconstructive nature of memory. There is an overwhelming number of combinations of actions you can perform, sequences of action. And yet, you ignore that overwhelming, almost all of that overwhelming information and zero in on the relevant information so that you are oriented right in the world, finding things obvious, salient, standing out. And it's not just a single thing. There's like a salient's landscape. Some things stand out more than others. Some things are more foreground, background. You have all this happening so that you are capable of solving so many problems in a messy, complex world in which there's constant novelty because of an emergent phenomena. And trying to give this ability to machines has been overwhelmingly difficult and one of the hard, hard problems. We're actually bumping up right into it now as we finally taken up the AGI project, the project of trying to create artificial general intelligence. Excellent. It may be worth digging a little bit deeper into that exposition because people might be thinking, well, don't we just have a module for relevance or salience? Don't we just kind of, we're born and we know, oh, I should not fall off cliffs and I should, but actually what happened yesterday might be quite irrelevant. You have a wonderful explanation in your sort of keynote paper, Explicating Relevance Realization, where you say that leads to an infinite regress. Perhaps you could explain that argument. So the magic module, not to be confused with predictive processing's reported problem of the magic modulators. So let's keep those two distinct from each other. But the magic module is, well, I have a relevance realization thing in my head that just does it. And then that just, and then I can just say, well, how does it do it? Then the mistake people can make is, well, evolution made it. And that's right. Evolution tells me how it got here. That's not telling me how it functions. Evolution tells me how my eye got here and how my ear got here. And that's not telling me how my eye and my ear function. I'm trying to take what's called the design stance. How could, tell me what I need to do to give that module the ability to realize relevance? All you've done is said it has that ability, but it faces the problem of, okay, now it has to know what to pay attention to, and so on and so forth. Now, what you might say is, well, evolution totally prepared us for it. The problem with that is that doesn't work. Evolution can only pick up on things that are long-term invariant and that make an ongoing continuous difference to your reproductive status. And that's not how relevance works. Relevance is really fast and changing. If I say left big toe, it suddenly becomes relevant to you, and it wasn't relevant a minute ago, and what's the Darwinian difference there? And there's nothing that's intrinsically relevant. Well, my own life is intrinsically relevant, is it? You'll sacrifice your life for your kids. Well, my life and my kids, under all circumstances, saving your kid means that 10 million people, oh, well, and so on, and we get into all these philosophical arguments because we realize, no, no, there isn't any hard, fast thing that is always relevant. Relevance is not something for which we can generate a scientific theory. It's not stable. It's not intrinsic to the phenomenon. And there's nothing, other than being relevant, there's nothing that all the events or objects or feelings have in common. What is it that, what do they all have that makes them, other than saying, well, you'll give me synonyms, they're important to me, or they help me solve my problems? Yes, that's exactly it. But how? Exactly. Perfect. And it's so nice to see how the apparently disparate strands of cognitive science, so Gibsonian affordances for a cognitive science, predictive processing and processing and active inference, align at this path, which is that everything is dynamical. Everything is about this mutual unfolding. There is no rarefied static existence. It's what's relevant to you might not be relevant to me, and there are these dynamic relationships which govern that. Yeah, so that's a great place to start. I think let's now go into the solution, if there is such a solution. And let's start with opponent processing. So this is a term that you use frequently. To a layman audience, it's not clear exactly what it means. So what is opponent processing, and how does it help shed light on this complex problem? Sure. And maybe along the way, I can point out how relevance realization really gives some specific teeth to claims of embodiment from 4-Ecox-I. Because I'd like the point you made, and maybe we don't have to do it right now, but I'd like to come back to it, how relevance realization is kind of a glue, they glue together predictive processing and 4-Ecox-I in a powerful way and makes them all, including itself, mutually stronger from that integration. But yeah, the opponent processing, so let's just note something. At many different levels of analysis in your biology, you will find opponent processing at work. I'll take one that's very easy to explain, and people have a ready experience of it. So we are all constantly, mostly unconsciously, although it's affected by conscious factors, we're constantly recalibrating and adjusting, dare I say it, we're even evolving our level of metabolic arousal. I don't mean just sexual arousal, don't be Freud here, I mean arousal. How much, how much, how sort of activated are you, how sort of energized are you? And the problem is there isn't, there isn't some state, homeostatic state you're looking for with that, because if there's a tiger in the room, I need to maximal arousal. And if I'm going to sleep, I need to go to minimal arousal, and I can't just be sort of Canadian and keep sort of average arousal at all times, because then I don't fall asleep and I get killed by tigers. And so it constantly has to be, like we're talking about, it has to constantly be adjusting. So what has evolved in your biology is your autonomic, meaning self-governing, it's a self-organizing system. Like you said, it's a dynamical entity. Your autonomic nervous system, and what it does is it couples together two subsystems that are, have opposite biases from each other. So your sympathetic system is biased to, I'll speak anthropomorphically, just because it speeds things up. Your sympathetic system is biased to seeing as much of it can of the world as threat or opportunity and arousing you as much as it can. And your parasympathetic system is biased the opposite way. It's biased to seeing as much of the world as secure, safe, a place where you can rest and recover. And then these subsystems are not independent from each other. They're locked together, and they're also, they're continually trying to shut each other off. And they're constantly competing that way, but they're cooperatively competing. It's not adversarial, they're cooperatively competing. And what happens is the constant trade-off between those processes, between those processes, constantly, right, in very dynamic manner, constantly recalibrate to your level of arousal to the world. And that's opponent processing. And what you find is you got that, you know, there's other, there's opponent processing between your focal vision and your peripheral vision. There's opponent processing plausibly between your left and right hemispheres. The number of these things, right, it is huge. And I say at many different levels of analysis. And so I think opponent processing is a clue to how relevance realization is actually undertaken by your embodied cognition. Wonderful. And immediately to anyone interested in active inference or its manifestation in continuous state spaces as predictive processing, what will be screaming out is this notion of an attractor set or our homeostatic equilibrium, which Karl has spoken about at length, obviously, and has had many other researchers. But this idea that what this, we are, we suffer, but we also embody in a kind of positive way this itinerancy. We never, we're never stuck in a single mode. We are endowed with the capacity to go beyond our homeostasis and return back to it. But we're always, in a sense, being drawn without ever residing, without ever having stopped at that homeostatic set point. So let's now fold in active inference to the picture. What does it provide relevance realization and the formulation of relevance realization that couldn't be done purely with Fourier cognitive science? So, I mean, I'm going to talk about it in terms of predictive processing, because that's the one, that's the formulation of it, that lines up most cleanly with the theoretical integration. As you said, there's there's clear derivation relations between active inference and predictive processing, and also between them and the Bayesian math, but the Bayesian math, if you were to actually strictly apply it would be computationally intractable. So we're doing some approximation function. So I'm just going to take it that everybody sort of shall knows that I'm playing fair with this, right? I'm not, I'm not trying to be dodgy. And so, relevance realization is grounded in the idea of problem solving. And problem solving actually assumes a fundamental thing, which again is so obvious to us, which is the states you're in and the states you want to be in are not the same state. That's the defining feature of a problem. If I'm in a state I want to be in, I want to be sitting in this chair. I am. I don't have a problem, right? And now, what that means is you're all, you're immediately into something very interesting. The organism is trying to actually predict a possible state in the world and prepare itself, so it's an agent. It doesn't just behave. It alters its behavior to alter the states in the world, right? And so it is trying to, I would say, predictively prepare for the world. It's trying to predict the world, but prepare itself for that world, but also prepare the world so it's more likely to come out in the prediction that it seeks, right? And so I put those two together, and I talk about anticipation. And so whenever you're problem solving, you're anticipating a goal, meaning that prediction and preparation. Now, Fourier coincide doesn't directly talk about that as clear as it needs to. It talks about coupling, and it talks about affordances, and I think there's a deep connection between opponent processing, optimal gripping, and affordances, and maybe we can exploit that at some point. But typically, one of the things where Fourier coincide has some challenges is in more distal relations to the environment, because it tends to rely primarily on coupling. And this is a long-standing critique that one of my colleagues, Brian Cantwell Smith, at the University of Toronto, because that is where all knowledge is flowing from, right, has made. He said, you know, you can be predictably coupled to things that you're in direct causal contact with, but, you know, sorry, dynamically coupled, not predictably coupled, my apologies. But how do you do things that are much more distal? And for me, that is key. And here's why I do this. I think when we evaluate, even intuitively, so I'm not using that as an authority, I'm just showing how readily this works for us. When we evaluate an organism for its intelligence, we tend to do it in two interlocking ways. We do it in terms of, I think, how well does it zero in on relevant information? How well does it pay attention to what it needs? We look at an animal and go, wow, that's really good. Notice how it's noting subtle differences. But we also evaluate the intelligence of an organism, and Michael Levin talks about this with his cognitive light cone idea, is how deeply into the world they can anticipate. And I don't mean just spatio-temporally, I also mean it modally, possibility. Like when I talk to my cat and I say, you know, where's your toy? The cat looks at me and I go, no, well, I say to my puppy, where's your ball? And she goes into the other room, looks for the ball, finds it under the couch and brings it all the way back to me. I go, wow, you're really smart, right? And so we get this because we know how just moment by moment that adaptive ability is built on how distally can we pursue goals? We tend to evaluate people, wow, you pursued a long-term goal and you brought it up. So that's what's missing. And I think that's what's really afforded. Now, I want to make one more point, and then I'll shut up so you can ask me another question, which is these two issues, anticipating more deeply, and remember, I don't just mean spatio-temporally, I mean modally, and relevance realisation are deeply interconnected. The more you anticipate, the more the problem of relevance realisation goes up exponentially. And so these two things, right, they, I would argue, because they're interlocking, they have to be solved together. And I think that's why we use both of them as evaluations of the intelligence of an organism. Okay, I'd like to backtrack to that second argument after this, after what I say here. Yeah, it strikes me that what predictive processing can give, perhaps where 4e Cognitive Science doesn't, but I'm happy to be schooled on this. And I know Varela and Autoporesis in some sense pre-shadowed and prefigured what I'm about to say, is that it kind of gives a ground to our action and to our perception, which is that we have to have a model of the world and a model of ourselves, which is not only descriptively viable, but also normatively viable. In so far as, it's all like, it's very important for me to be able to predict the world. But as you said, we aren't just at the behest of the world's dynamics, we can change the world according to our preferences. Exactly. And I think what predictive processing gives us is, although to be fair, I think Carl might say that this is a bit of an overshoot, is a telos, is a fundamental attractor set to which our perception of action is governed. So I think that's kind of the way that I see the added benefit of this convergence between predictive processing and relevance realization. What I was also going to say there is that it's, fantastically, you're mentioning this kind of deep temporal modeling. Because from my eyes, that's really where a lot of the work is being done right now, which is that selfhood consciousness, even perhaps space itself is downstream on the fact that we can downstream on the degree to which we can model the slower dynamics and the faster dynamics in a generative hierarchy. So that's, I think that's it. I mean, complete agreement with that. I think that's an extension of the argument I made. And so what I wanted to pick up there is you said, if I heard correctly, that the more, the deeper your temporal model, the more you can model slower fluctuations in the environment, the more critical relevance realization becomes, the higher the stakes in some sense. I'm interested in unpicking that argument, because from my eyes, there are plenty of things out there that don't have deep temporal models. So a virus, for example, you're just reacting to very coarse-grained features of the environment. But for them, relevance realization, at least to my eyes, appears as critical as it would for a human. It is. It is. And so I think it's how I'm going to put it, I think it's always the demanding problem. So I'm not denying that even the paramecium has to do salience landscaping. It has to, I'm going to use a word very neutrally here. I'm not connoting anything about consciousness, but it has to recognize this molecule is food and that molecule is poison and swim reliably towards the one and away from the other. So I think the problem is always there, as I was trying to indicate, as soon as you have problems, as soon as your goal states are distinct. And I think the space opens up very quickly exponentially. I just meant that it exponentially gets worse. This is why you see across species, hyperbolic discounting, temporal discounting, because that's a huge relevance realization machine. It's about salience discounting. And the point of that is, is because as you opened into the future, the number of possibilities goes up exponentially. And so you need to have this attenuation function, so you don't get overwhelmed by the possibilities as you extend your cognition into the future. That's why we have hyperbolic discounting. And we pay a huge price for it. It makes us procrastinate. It makes us difficult to give up, to pursue our long-term goals. So there's even a trade-off relationship in there, but I won't get into it. But that's an example. There's a good reason why we must have something like temporal discounting across species. That's what Angelie showed, is because the relevance realization issue just, it gets even worse. It starts out bad and gets even worse. Right. Wonderful. And yeah, I mean, a lot of your work will come to affordances now because a lot of your work is centered on the notion of affordances. So affordances are the possibilities for action afforded, actually circular, possibilities for action granted by the environment. I mean, afforded was a verb and then Gibson made it now. How much has the predictive processing framework supplemented your understanding of affordances? And perhaps the way that what we perceive in the world is not necessarily a feature list of objects, but they are fundamentally affordances. I see my glass of water as grippable rather than being glass this dimension. And I only ask that because, as I said, as conscious, animate cognizers, we are blessed with the capacity to change the world in accordance with our priors, which makes action really the fundamental currency for auto poesis, for self-organization. So does it has predicted processing ramped up the importance that you give to Gibsonian affordances? I don't have it ramped up because I always thought I literally learned this stuff from John Kennedy when it gives his greatest protease. So very early on, I was very much impressed by this. And it occurred to me that relevance realization, the realization of relevance, and I mean realization in both senses, actualizing a possibility and becoming aware of it, or at least detecting it in some fashion, that it's a prototypical kind of affordance. The thing about affordances is they're not found in the object or in the organism. The grippability of the glass is not in the glass, can't be gripped by a peramecium. And it's not in my hand. My hand can't grasp Africa or the sun. It's in a fitted relation between them. And that's exactly what relevance is. It's a fitted relationship between. So I always, before I came into a deep dialogue with predictive processing, I already saw a deep relationship between relevance, realization, and affordances. And I think part of my work has been to really explore the deep ontological significance of that, that we have a category that sort of falls outside. This was Gibson's intent, too. This is what Forde Cogside is really talking about. Falls outside are our belief that we have a complete exhaustive dichotomy between the subjective and the objective, between the inner world and the outer world. And so there's this other important, why call it the transjective, this betweenness, this connectedness, that you see in adeptivity, you see in relevance, but you also see it, like you said. Now, what do I think the predictive processing did? If you'll allow me, I want to talk back and forth symmetrical, not just linear. So I think what the predictive processing does is emphasize the need to try and explain, rather than just... I don't want to be cruel. So I'm being a little bit oversimplistic. I'm doing that just for speed. But generally, it's like, yeah, but how do affordances get actualized in the Ford E-caller? Like, there's a lot of affordances all the time. And what I'm interested is, how do those affordances become, well, salient and obvious to me, so they become binding on my sensory motor behavior. And that was always something of, not that there's Redfield and others who are doing some work on it, but typically they start to invoke predictive processing to try and explain that mechanism of, okay, there's a huge affordance network, but I'm not... Here we go again. How do I select and actualize the ones that actually go into my salience landscaping? And I think predictive processing, especially with the notion of precision waiting, is a good job of that. And what I think relevance realization helps to do to predictive processing, a bunch of other things. But one thing is, predictive processing, basically, there's subtlety to this, and I know there's lots of mouth, but just to keep going, right, that, you know, well, what attention is, is the precision waiting? It's this meta function. And I think that's a very good argument, but there's a conceptual analysis that's been needed, but it's like, but what is salience? And one of the arguments I can make is, well, what the precision waiting is doing is relevance realization. And relevance realization, when relevance realization does a higher order finding relevant of an affordance that has been generated by lower order relevance realization, that's when something becomes salient to us. And so we can actually give a conceptual explanation that goes with the theoretical identity claim in predictive processing, the precision waiting is attention, and what it gives you is salience. So they can, they can unpack each other. I think what another thing that happens is, relevance realization is deeply connected to foreecogside, because relevance is grounded, I would argue, in auto-polisis. Relevance, relevance is always relevant to an organism, right? And so that means, relevance realization is a cold calculation. It's how the organism cares about some more information rather than all the other information, because it is constantly taking care of itself. There are literally, and I mean this physically, things that matter to it, and things that it must import into itself, important things. And some of that's also information. And so why that, if you, if you can, if you can glue foreecogside and predictive processing together, I think you can answer some of the questions, maybe even challenges, that some people in foreecogside are making of predictive processing and saying, well it's not really connected to embodiment. I think, no, no, if you show a deep theoretical integration of predictive processing and relevance realization, and then relevance realization and auto-polisis, then you've really strongly glued predictive processing and foreecogside together. So that's some of the important theoretical work that you've done. Yeah, absolutely. I would love to jump into the critiques that certain foree cognitive research, cognition researchers have leveraged at predictive processing, but let's leave that for a second. This is exactly what I was talking about in terms of telos, which is that exactly this notion of care. And when I spoke to Mara Albaras in an active inference researcher last week, we kind of actually ended up concluding that this is almost the Heideggerian sense of care that we take from our existence. Can I, can I just interrupt there? Yeah. Right. Heideggerian sense of care was exactly what I was invoking. And I, right, and I was deeply influenced by Dreyfus and the frame problem, got this from Heidegger. Yeah. So the connection you just drew is very, I, I, yeah, I explicitly argue for that. Excellent. So yeah, I mean, I personally think that the Heidegger, Heideggerian, Dreyfus, Merleau Ponty lineage needs to be integrated more into active inference. And I'm speaking to Dr. Marilyn Stendera, who has done work on Heidegger and auto-poesis. So that will be really, that will be really fun in the upcoming weeks. This is exactly what I was talking about in terms of telos, which is that taking a stance on your own existence can be computationally modeled as having these kind of high precision priors. So our homeostatic set point will be different from a snake's. And that actually gives a really solid explication of why we act differently in certain contexts than snakes. And so that was my kind. But then that said, I will have to make clear that Carl is in himself, although he has spoken about existential imperative says that there actually is no imperative. All that you're really saying is if a thing like us is to persist over time, this is what it needs to do. And it's just a, it's just cast as a minimization of free energy. So I think there's an interesting question there that maybe you could have a go at, which is to what degree do you see this as a genuine imperative versus just what things do to self-organize over time? Yeah. And so I mean, and Carl's no slope, so he's welcome. I sometimes don't like it when people who, you know, are physicists or something start commenting on philosophical normative questions with an authority they don't properly possess. But I know Carl's a great theorist. And I know he's philosophically educated. I have not yet to talk to him, but I hope I do get to do so. We can do ahead. So yeah, I mean, so I'm going to argue later if I get a chance that what relevance realization doing is strongly analogous, obviously a very different timescale to what evolution does about constantly redesigning the adapted fittedness of the, of organisms to the environment and allowing organisms to fit their environments to them. Because as you keep emphasizing and I keep agreeing with, it's not passive. It's not right. The organism is shaping the environment as the environment is shaping the organism, niche construction and all that sort of thing. And I think that's all right. And then there again, that's a very clear connection to 40 cogside again. But this deeper question, I do want to do it. I want to pause and I want to slow down because it is a really important question. Because it gets us beyond what we might call scientific explanations of how and into properly, but not useless, philosophical reflection of sort of why. And why does this matter to me? Because as you've mentioned, I'm deeply concerned with, you know, issues of meaning in life. This, this, this largely metaphorical, nebulous notion that humans seek meaningful lives, lives that are worth living, even given all of our failures and our faults and our flaws and our foolishness and our frustration, right? And pretty clear empirical evidence that it's not reducible to just subjective well-being or pleasure and pretty good conceptual philosophical argument, because that's where it's relevant, that it's not reducible to just living a moral existence. You could live a very moral existence in which you're sort of experiencing the pleasure of food and other things. And, and, and you've got, you know, certain stability in your environment that you could be very, very lonely. You could be very lonely. So, and what, what that loneliness points to, I'm just making an intuitive gesture. That's not a tight argument. I have tight arguments, but the gesture is people are seeking a kind of connectedness. And now you see where I'm getting to relevance realization and what we're talking about this niche construction, the fittedness, the belonging togetherness, this mutually shaping, like this is this, I argue that that's exactly the connectedness human beings are seeking the meaning in life. Now, you can then ask the question and people, and I'm going to play with two terms of phrase here to play with that. I think we clearly seek meaning in life. I think the evidence for that is growing and it converges with the psychological work on meaning in life and on what Karen Allen calls belongingness. If you don't feel that you belong, you're in trouble and you're in trouble across all these measures, cognitive, emotional, social, financial, blah, you're in trouble, physiologically you're in trouble. That's why solitary confinement is such a punishment, right? So, is there a meaning of life to the meaning in life? The meaning of life is that is there some sort of cosmic destiny, cosmic order to which we are which we are ultimately trying to find. And here's the thing, I don't think I have enough evidence for that. I think that there's lots of evidence for meaning in life and if I'm right that it's relevance realization and if that's something very much like evolution, right? Evolution is constantly redesigning adeptivity, but there isn't a final thing that evolution is aiming towards. There isn't a final form of life. There isn't some like what we're doing is like the sculptor, we're constantly refining and then finally one day we will have the final form of life. And I think this is fundamentally flawed and I think this matters philosophically. I don't think previous forms of life were in some in any kind of moral sense superior to us. So, I don't believe in any nostalgias. Some of the previous things that people found important and relevant and sacred, those were the true ones and we just have to get back there. Nope. And I don't believe the utopia. We are working towards the final ultimate thing that will all agree for all time is the most relevant, most salient. I don't believe that's how this works. So, I reject nostalgias. I reject utopias. The idea that there's a telos can either mean there's a telos to meaning in life, which is it's like Carl says, it's necessary, it's constitutive necessary for living things to have this. But does that mean there's any metaphysical necessity to their being living things? That's a different question. Now, here's my weak answer at that one. Sorry. Long question, but you asked like this is like almost up there with like God. Yeah. Right. Right. I note that I'm in a very difficult situation. I can't see any evidence for and I'm not attacking God here. And as you know, I'm very respectful and even appreciative of religious frameworks and religious lives. But I can't see any evidence for that. However, if you were to ask me, which is a better universe, one with life in it and one without life in it, even if you couldn't exist in either one, I would say the one with life in it. There's some primordial judgment going on there that has some kind of metaphysical import. Now, it's weak. I admitted that ahead of time. And so I'm giving you a very, I'm sorry, very long but wishy-washy answer. I don't really think I believe in anything like the meaning of life, but I do think there's a metaphysical import to being connected to reality. We find reality being connected to reality inherently valuable for its own sake. And we find worlds in which things can realize reality better worlds than ones that don't have that in it. That's my answer. No, it's a wonderful answer. And it, it's really helped me and hopefully our audience clarify exactly what I mean by an existential imperative, which is that the free energy principle doesn't tell you what you're here for. No, it tells you if you're here, what are you doing? Like I've given your phenotype and given your culture and etc, etc. So that's a really useful sort of explanation of exactly what I was going for there. And actually, it points to something I said to Carl right at the end of our podcast, which at the time of recording this came out yesterday. So people should definitely check it out because he's, for sure, for sure, he's on top form, which is that I asked him, I mentioned Thomas Nagel. So Thomas Nagel has this very romantic philosophical notion that life is deaf is bad because life is fundamentally good. And it points to kind of your thought experiment. If we had two worlds, one with life and one without life, I think we would all intuitively say the one with life is better. Whether that has any metaphysical input, as you say, is up for grabs. I was going to ask sort of jumping on the back of the initial thing you said, which was about connectedness and belongingness. So if we take this from a theoretical or computational stance and active inference, what this looks like to me is we embody a model of the world which we can so we can make fundamentally sound predictions about how the world will unfold and how about we will and how we will act to make that world more compatible with our preferences. Now, when that was formalized initially in the active inference literature, immediately a response to this came in the form of what's called the dark room problem. So this is sort of papers in 2011, 2012, and they've been rebutted in multiple ways. But the basic idea is why don't humans just seek out dark rooms where maybe they have access to food and water, but they don't really seek out, but in a state where they don't go and explore and they're not curious. Because what you're saying there is actually a perfect coupling between your predictions of the world and the way the world is unfolding to your eyes. So I was wondering whether you had pondered the dark room problem and where you see exploration and epistemic affordances coming into play here. Yeah, I think that the dark room problem, I'm not, I'm happy with all the other rebuttals. And I don't know, I don't know, I don't know them all, so I may be stepping on somebody else's toes. So if I am, whoever you are, I apologize. I think this is a lock-in individualist model of cognition and how we work. And I think it is therefore the presupposition, well, is that no, that's actually, I don't agree with that presupposition, that our cognition is fundamentally individualistic in that way. I think we are, you know, we are socio-social cultural mammals. And let me point to one thing, you know, yes, measures of G are very robustly predictive, but you know, it's also really a very powerful way of predicting your behavior, your attachment style. This is also very robust. Now, that means, think about what that actually means. And this, you know, and this goes into the heart of religious traditions, like agopic love, right? When you have a child, you have to invert your relevance arrow. It's not how that being is relevant to you. You invert everything around. This is agopic love, and it's how it's different from erotic love or philea love. You invert everything around, right? And so that it's, how am I relevant to this being? How am I relevant to this being? And how can I be relevant in a way that turns it into a person, turns it into an intelligent, rational, self-reflective, relevance-realizer, predictive processor, meaning maker, right? And so, well, first of all, where are my attachment relationships in the dark room? Well, there's other people in the dark room. As soon as there's other people in the dark room, all the problems that you thought you got away from by putting me in a dark room return. Other people have different goals than me. They have different needs. They're going to move around differently, right? That we're going to have to decide about when and where and how we gain access, blah, blah, blah, blah, blah, blah. All of that immediately unfolds. Secondly, and this is part of 4E CogSight, I think the evidence for extended cognition, distributed cognition, that we evolve to work in groups, and the collective intelligence of that is actually our superpower, and other people are making this argument. I think that's clear. The standard thing, take the Waste and Selection Task, put it an individual person, highly educated, highly intelligent, second-year psychology and top-tier university. From the 1960s on, you put them in the Waste and Selection Task, and only 10% get it right, right? You replace that same task with four people who are allowed to talk to each other, and their success rate goes from 10% to 82% reliably. Why? Because we do opponent processing between each other. You have biases different from mine, and if we work in opponent processing, not adversarial, but if I say, you're probably a good source for correcting my bias, and I'm probably a good source for correcting yours, we get the best dynamical self-correction possible. When you add in the fact of the reality, and there's actually evidence, empirical and formal, for the power of collective intelligence, the reality of attachment, well, then the dark room becomes filled with other people who are trying to band together to solve problems that they can't solve individually, and then, to me, everything that the dark room thought it had as an absurdity, like a reductioid absurdum, disappears. That would be my response. Yeah, I think eventually the dark room just becomes the world once you start asking the things you need, eventually it becomes a civilization. Yeah, I love all this stuff on distributed cognition. I spent the first six months of this year working at UCL on finding experimental ground for what's called social baseline theory. Social baseline theory is this idea that humans, like to study a human being in a lab by itself is to study a human being at deficit. Yes. Actually, it's physiological arousal to take one factor is at its baseline when it's with other people, and it will fluctuate according to stuff like attachment style or the relationship that one has with that person, which is really beautiful, but inverts our kind of, well, maybe not common sense, but our institutionalized notion of what it is to be a person, which is kind of alone. Yeah, wonderful. Well, I'm going to take this argument a little bit further if you let me, because this is something that we spoke about a couple of months ago, and I think Mark might have a different opinion, so it'd be cool to unpickle this. I have this slightly fuzzy, wishy-washy idea that the notion that, okay, so I always think about a climber climbing a rock face or a climbing wall. The climbing wall is a kind of beautiful canonical example of an affordance based landscape. It's rich for the affordances. It's got the toe holes, the finger holes. So in many ways, what the rock, in terms of active inference, what it allows the rock climber to do is to self-evidence or find evidence for its own model about itself as a successful rock climber. Now, that's relatively well established in the literature. What isn't established is my kind of, again, slightly wishy-washy idea, which is that the rock climber is offering affordances to the wall. And so this is why, right at the beginning of our conversation, I spoke about mutual coupling. So what dynamical systems theory gives us, what auto-polysis gives us, what active inference gives us, is this idea that to exist is not to be this kind of reified self that takes a objective stance on the world. And actually, what it is, is this really the fundamental unit of analysis should be this dynamic mutual coupling between agent and arena to use your terminology. I've spoken to Mark about this, and he said it's something like an overextension of the term affordances, to say that the human being is offering affordances to the climbing wall. I know Mark isn't here to defend himself, so again, I'm not going to put one... But I will definitely raise this with him again. But just to your own ears, how does that argument sound to you? I like the argument, and yeah, I don't want to speak to you, but Mark, I'll only speak to the propositions that you spoke on his behalf. And I'm very happy to... I mean, Mark and I work close together. Mark is one of the best people I know. Mark's a former student of mine. I'm very proud of Mark, so... But I actually agree with you. Now, that goes to another connection I make in my work that would probably be a little bit stranger to the ears of many of your listeners. They might say, oh, I get why he wants to connect to process inter-relevance realization and the 40-card side. But now what you're doing, and this is where I do a lot of what I call my deeper ontological work, and this is where the Heideggerian stuff really comes to bear, is this is all the work I do on neoplatonism, which may sound like something very arcane. But again, here's the proposal that once we really profoundly accept affordances, and once we accept that they are reciprocally realizing, I agree with you, that affordance not only discloses things about me, it discloses things about the world. And that's a way in which things, like Heidegger's notion of truth is alifea, as an event, as the disclosure, rather than a static property of our propositions. I think that, bang on, now, as Filler argues, and I've been arguing for a while, and John Russen, Filler's book is called Neoplatonism, Heidegger and the History of Being, Relation as Ontological Ground, that relationality is actually the ultimate nature of reality. Heidegger is actually turning back towards this, because what does that give you? If you have that, and I think this is a fair way to put it, if you have that alethetic notion of affordance, then you are getting back to the neoplatonic theory of knowing by conformity, knowing by participation, and the deeper argument goes something like this, that if the fundamental grammar of your cognition, I don't mean the content, your content can go wrong, but if the fundamental grammar, the bottom up, top down, all this stuff we're talking about, if it's not picking up on something fundamental about how reality is structured, right, then you face the kind of profound solipsism, a profound kind of skepticism. And I think that is ultimately leads you into all kinds of performative contradictions that I and I agree with Whitehead, there is devastating as propositional contradiction. There's a longer argument there, and I could point to, please check out some of my talks on YouTube about this longer argument, like Pickstock's Aspects of Truth, where she said, all the things we use to decodemize this, we had the analytic synthetic distinction, that is broken down under philosophical criticism. We had the theory fact distinction, that is broken down. We had the, like, is, ought, that has broken down. Like, think about the relevance, it is or not, well, blah, right, it's sort of both, right? And so her point is all the things that were used to cleave, and then what we tried to do is we tried to make the logical world, the thing that sort of stuck the two worlds together, and that collapsed under Godel and other people. And so we're back to the idea that we either go into the Lockean cabinet, in which we're locked inside of our heads, we're somehow getting postcards sent to us, we think they might be from an outside world that we think might be out there, and we're trying to build it from the postcards, and we're doomed, that's never going to get us there. And so I think if you have an allothetic notion of affordance, you start to make the argument that how reality is realizing, and how we are doing relevance realization, are fundamentally participating in the same principles in a profound way. And I think, yeah, I have no doubt that Mark might not want to do this. I think Mark would be, I would agree with Mark that this is not a direct derivation from sort of classic Christonian presentations of active inference and predicted processing, Bayesian brain, we should settle on a name. But I think if you make the connections through deeper into Gibson, deeper into Foricog side, deeper into Heidegger, you get back to this notion that with the prevalent notion of knowing, that when we know something, what we're doing is our structural functional organization is identifying with the structural functional organization of the thing. And we are both participating in that same form, that same principle, that same grammar. And that has all kinds of metaphysical and ontological consequences, even ethical consequences. It means talking about the truth of good and the beautiful becomes something very relevant again. Yeah, yeah, no, this is exactly where I wanted to go and exactly aligns with the way I'm thinking about the free energy principle at the moment, which is I have a skepticism between the internal and the external distinction. Yes, it's something I spoke about with so a classic problem when one starts reading active inference, as I spoke about with Carl, is whether one would consider the the agent or the internal dynamics is having having a model of the external world or instantiating a model of the external world. Yes, excellent, excellent. And since it's come back to sort of cybernetic formulations of what it is to be a good regulator. So Colin Ashby and all of that stuff from the 50s and 60s. So I have an inherent skepticism because I would I kind of take the stance that actually all that the internal dynamics are doing is instantiating a particularized form of the external dynamics. And this is very easy once you take out the picture of a homunculized ego. Just if you look at it in terms of just the particle physics, we're just a instantiation of some of the physics that defines everything else. But we just we just actually seem to have a kind of complexified Markov blanket or Markov blanket form of that. But there's nothing particularly distinct about that. And we'll come to consciousness because maybe there is. And that I guess is the big elephant in the room here. But it's funny that you mentioned Heidegger because Heidegger to my to my eyes, and in my opinion, and then obviously Dreyfus and Merleau Ponty and the phenomenologists followed Heidegger, really, he's the one who strikes at this established distinction between a subject and an object. So the Cartesian distinction. And then this is where his kind of gripes with Sartre came in. Now, my question is, is that, yes, if you read Heidegger, Dasein, the ground of Dasein is this kind of interrelationality. Alpha is the disclosure of truth in an interrelational relationship, so to speak. But there still seems to be some kind of subject who, you know, the the hammerer who is hammering away at the nail. Now, from the perspective of the hammerer, he's not some reified ego who is objectifying the nail that all that is really being witnessed there from the level of consciousness is the disclosure of activity. But still, we can give a description of a hammerer and a nail. So to what degree in your thinking, do you completely eradicate the subject object distinction? And is there anything that you think predictive processing has to say about that? I think, well, the second question. I'm happy to think about it with you. I have put a lot of thought into the first question. And so, I mean, I think Filler is right in his book that our subjective objective divide is our version of the appearance reality distinction. And it's just our version. In the ancient world, they had a different version. So ours is an in out metaphor. How is the inner and the outer connected? And the ancient world is the problem of the one in the midi. How is the lower and the upper connected? And once you see that, you realize that there's a deeper structural problem that isn't bound to the inner outer or the emergence and the emanation. It's the connectivity issue. And what Filler argues, and there's a lot of other people converging on this, is if you try to, if you go with an Aristotelian metaphysics, or that what's most real are substances in the Aristotelian sense, which are things that can independently exist, then this becomes a big problem for you. And that kind of ontology, I would argue, and I can make the argument, drives you towards a nominalistic epistemology. And then you get, you know, Occam's version of it, or Kant's version of it, where all of the patterns and all the information are just in the mind, which means the mind is radically other, because it is the only place that the information intelligibility actually exists. And the world is profoundly absurd in a deep, deep way. And I think that leads you into, well, first of all, it makes science impossible. And it leads you into all kinds of existential and moral dilemmas. And if you then take a look at the logic of trying to get, you can't actually, and this is what Filler does very carefully, you can't actually get relations out of properties that belong to the Relata. The relation has to precede the Relata in a very important way. So I think there's a deep, at a very deep level, I do want to call it into question. I do want to challenge that. First of all, it's historical. It's cultural. We pretend as if that is the only way in which human beings have related to the world. Like I said, even in the West, that's not their problem in the ancient world. Their problem is the problem of the one in the many. It's not the problem of being in the world. Now, what does this all ground in? I think this grounds in the common problem of the relationship between appearance and reality. And here's where I could now say something that aligns with my previous argument about relationality. See, so, and I'm going to borrow a term from rapport here, the hermeneutics of suspicion. The hermeneutics of suspicion is that a parent, and we got it because of Freud and Nietzsche and blah, blah, blah. There's historical reasons. And those are valuable critiques, by the way. But the hermeneutics of suspicion says that appearances are distorting. They're deceptive. They're destructive, right? They're disruptive. And what we should do is always question whether or not they are leading us into reality. Now, Marlo Ponte has a great argument against that, that he gets from, maybe he doesn't get, but it's in Plato, which is, wait, you're treating real like red. Like, you could just say, if you could look at an isolated thing and say that's real. But real is a comparative term. So, of course, is illusion. To say this is an illusion is to say this is an illusion in comparison, in relation to something that is more real. And that thing is in relation, so on and so forth, right? And what that means is, first of all, you have to see that those judgments are inherently relational. And secondly, you have to call into question the independence of the hermeneutics of suspicion. The hermeneutics of suspicion is actually parasitic on the hermeneutics of beauty. It depends on there being things that we agree on by saying that's where appearance discloses reality rather than distorting it, right? And then as soon as you do that, that undercuts the deep divide, because you ultimately make these divisions between the one and the many, or the subject and the object, by getting a hermeneutics of suspicion into the appearance reality distinction. That's at least the argument I would make. Interesting. Yeah, I like the suspicion of binary-ness inherent in all of this. It speaks to me, and I think it speaks to people who are interested in active inference, and the notion of sort of philosophical vagueness as well. At what point is something real? It's fundamentally a comparative term in the web of things that could be considered real. Yeah. That's right. Yeah. I'm trying to do this without invoking a binary, but you mentioned earlier that four E cognitive scientists do have their critiques of active inference and predictive processing. I will stick with active inference actually for now, and I will explain why later. It's not that interesting. So this is people like Tony Camaro or Ed Bags who are radical inactivists, and what they're critiquing, in a sense, is an interdelist picture of active inference. Again, I'm not putting words into people's mouths, but if you read Jacob Howey's 2016 paper about self-evidencing, some of the language may to some people imply that there is a homunculus that has a representational picture of the world. This, to the ears of an inactivist, is deeply worrying. I would love to, as a four E cognitive scientist with obviously a vested interest in predictive processing as well, I'd love to just hear your take on that debate and, again, without striking up binaries, and whether you think those critiques are legitimate. Well, I mean, it depends what you mean by legitimate. I mean, there's also the ones that Evan Thompson made, and I hold Evan in a very high regard. I mean, they're legitimate in that they are well-made arguments in peer-reviewed journals, and so we have no right to be dismissive of them. Well, if I was to say that let's take as an axiom of active inference. This isn't your case, but let's just say, for sake of argument, that you have some interdelist representations of a statistical model. So you are, in a sense, you're not just embodying a model. You actually have a model. So again, it comes back to that distinction we're talking about. Would an inactivist critique of that internalism be justified? So the reason I'm hesitating is this is landing on the swamp of what do we mean by representation, which, I mean, so does the thermostat represent the temperature in the environment? And everything I'm going to say is controversial because of the swamp. So I want that understood, please. I would say no, because what's needed is there might be some, there might be co-variation, but what I think the critiques of the co-variation model of representation locks ultimately model. So we had the idea that representations have to be similar to what they represent. That's how they represent. And then you have all the problems with similarity and Aristotle even could bring that down. And then lock replaced it with the co-variation model. To have a representation is I have something in my head that reliably co-varies with something in the world, and that's how it represents it. And then the problem with the co-variations is they don't give you the specificity, for example, of thought. So this is co-varying with a bottle, with a tool, with a man-made object. Which is it? Those are not the same things. Those are not the same ideas, but this is co-varying causally with all of that. And you have the problem then of aspectualization. And the Lockian answer, of course, is we get aspects by doing representation. But I agree with Searle that that's the wrong way around, that any representation is inherently aspectual. When I represent this as a bottle, I'm only picking up on some of its properties insofar as they are relevant to each other, insofar as they are relevant to me. So representation depends on aspectualization, which depends on relevance realization. And this is not what you do with relevance realization. You do not represent all the facts, judge them to be irrelevant, and then zero in. So it's ultimately non-representational. So what I would say is, I don't know that some level, there's something like representations. But if I agree with many arguments that representations are more than co-variation, but there's this kind of caretness and aspectualization through them, then they depend on relevance realization, which grounds an autopolicist and is deeply intertwined with predictive processing. That would be my response. Splendid. And I guess what intuitively supports a more representationist picture is consciousness. I think consciousness is going to become a recurring theme in this podcast. Well, it's the Holy Grail, right? It is the Holy Grail. And so I guess my question here is, is that if one was to adopt a radically inactivist view, there's nothing in that picture which needs consciousness. So why couldn't my mutual coupling with the world just happen with the lights off? So this is an argument, I guess, that's rooted in Chalmers 1995 paper, which is, you can give me the function, but you can't give me the why, why are the lights on? And so I guess, in other words, my question too is, yes, that we may have relevance realization, preceding perception or grounding perception. Then why do we have perception in the first place? So again, I'm going to give a gist of an argument that I've spoken about at length elsewhere, and I'm trying to get published on and presented at conferences and so forth. So I don't think you can separate the function and the nature questions, which is what Chalmers Hard Problem relies on. Yeah, you've given me the function, but you've told me nothing about the nature. I don't think function works ontologically like that. I think function has to be plugged into the ontology. And I think as soon as we're talking about the ontology of anything within a living organism, we're talking about something functional. So I think the questions have to be answered interconnected. So let's go back. Let's say that you give me that any cognitive agent has to be doing anticipatory relevance realization, or it's not going to be a general problem solver. And then when it's doing that, it has to be aspectualizing its world. It's not doing all of this, but this as a bottle or the molecule as food, the molecule. And then if you then start to pay attention to, let's look at the continuum of consciousness. Let's look at the possibility, which I have experienced. That's not even the right sentence. Many people have, and this is Foreman's idea, of something like the pure consciousness event. And the pure consciousness event doesn't have, well, actually, I need a distinction here. I'm going to claim it doesn't have one type of qualia, but it has another type of qualia in it. It doesn't have adjectival qualia. There's no red, there's no blue, there's no cap, there's no dot. You're not even conscious of consciousness, you're just conscious. But what it still has, is it still has the adverbial qualia. It still has a sense of here-ness, noun, and the here-ness is profound presence. That's the language with you. The now-ness, eternity, the integration, right, the together-ness, it's everything is one. So all the adverbial qualia are still there, and you still have consciousness. That shows that the adjectival qualia are not necessary to consciousness. Now, I have the other arguments to show they're probably not sufficient, because if I give you sort of atomic blips of blue-ness and green-ness, and they're not bound together, and there's not a here-ness and a noun-ness, so you can anyway orient on them, I don't think you have consciousness either. So I'm not saying that adjectival qualia don't exist. I'm saying that we've held consciousness hostage to them. And what I'm proposing to you is that what consciousness is doing is these adverbial qualia, which are just salience, which is just relevance realization, which is, as far as we can tell, tied to the best evidence we have, it's all controversial, about the function of consciousness. It's tied up with working memory and attention, which are both doing higher order relevance realization. We seem to need consciousness for situations of complexity, novelty, or ill-defined in this, ones that are really demanding on relevance realization. So you can make a pretty, and there's a lot of convergence, actually, on what the function of consciousness is, relevance realization. And I think if you plug in relevance realization, you can at least get all the adverbial qualia. And that, I think, gives you a lot of what consciousness actually is. Cool. Yeah, let's stay on that adverbial qualia, because I'd like to integrate what you've just said with a more active inference account of consciousness, which there are, and then they're also diverse. But one, well, I have to ask a question before, does Foreman say that mind-ness, so feeling that this experience is mine, does he, is that one of his adverbial qualia that persists in the pure consciousness event? So that has another thicket, because the debate about, this is a raging debate. Evan Thompson actually has a good anthology on this, on the self-no-self debate. Right. And I think it's reasonable and complies with most of the evidence, which of course is self-report after the fact, which is problematic, blah, blah, blah, blah. I agree with all of that. I'm not dismissing that. But that's what we basically have to go on right now, that the ego narrative sense of mine and me and I goes away. Whether or not that is a complete loss of the self as where relevance realization is happening or something like that. I'm not convinced that that second thing is the case. I think, so if you would allow me, and this is a torture distinction, a distinction between the ego and the self, I think these experiences very much, the ego goes away. I'm very suspicious of the claim that the self goes away, because people are readily able to recall these experiences. This is what Forman does report, and I would report to you and seamlessly integrate them into their auto-noenic autobiographical memory. There isn't any weird disjarring or where was I during that? Yeah, yeah, yeah, yeah. Okay, excellent. The reason I asked that is because a lot of self-modeling in active inference is based on the work of Thomas Metzinga. So, Thomas Metzinga is such a wonderful addition to this conversation. I had a wonderful conversation with him not that long ago. Oh, really? I love it. But on my show. Amazing. I think he's such an underrated and important philosopher. Oh, I totally agree. Yeah, yeah. So, he's got this notion of, so he makes a distinction between phenomenal self-modeling and an ontological self. So, people intuitively, even everyone listening to this podcast, even the way we act on a day-to-day basis is in lieu of an ontological self. So, we can't help but really think of ourselves as being this soul, this Cartesian soul. But anyway, Metzinga invokes this notion of phenomenal self-modeling, how the system appears to itself. And he's got this minimal phenomenal self, which has been unpicked in other ways, but he normally speaks generally of presentness, minus, and perspectivalness. Exactly what I've been talking about. Yep, exactly. And he gets fleshed out in some of the active inference work. People like Jakob Howie, Carl, and Jakob Limonowski have used Metzinga's work for several papers. And then, on the other end of that, we have what Carl turned an epistemic agent model, which is the system that sees itself as epistemic in the sense that it can retrieve past memories to inform future decisions, and agentive in the fact that it can conduct allostatic action, i.e., action to retain some homeostatic equilibrium, given what it knows about the future. So, me putting on a jumper before I go outside, because I know it's going to be colder outside than it is inside. The reason why I invoke the minimal phenomenal selfhood is because, and Carl actually outlines this in the podcast we did together, that consciousness may be downstream, in a sense, on one, a deep temporal model, and from that, the sense of agency. And he has an argument from referring to dimensions, not in terms of sort of millimeter size, but in terms of the degree to which you have embedded markup blankets, that when we're distant from our actuators, what we end up doing is we have to have a way of distinguishing what's my action from what's your action or from what the world is, how the world has acted on us. And in doing that, we come up with a self other distinction, but also the notion of an agentive self, the idea that I can change the world in accordance to my preferences, as I mentioned beforehand. So, to paraphrase Carl, what that seems to be suggesting is that consciousness is actually rooted in selfhood. And I know there's this whole argument in Metzinger about whether that's the case. That's why I asked whether mindness is also part of these pure consciousness events, or whether it's kind of a post hoc inference. Just wondering whether you had any ideas about that notion, and whether we can truly have consciousness without at least the sense of self. Right. Okay. So, let me try a few things. First of all, I don't think it's inevitable that human beings model themselves as souls. I think that's a Western post Cartesian way of, and that's even, and then the soul as a monadic single substance, I can put it that way. That's not even the case in the ancient world of the West, certainly not the case in other parts of the world, etc. So, I hesitate that that's the claim that's part of how the machinery must unfold. And then secondly, I worry about saying that because it isn't a soul, there isn't a self. This is a weird notion, again, of a substance ontology being just presupposed in an unquestioned manner. I mean, look, we've discovered most things aren't substances. This table, which would be a classic Aristotelian substance, is not a substance. It's a dynamical system of atoms and quarks, and we don't go, oh, well, because tables aren't substances, they aren't real. Like that, we don't do that. And so, I want to just note that I'm worried that there's a substance ontology creeping in here and leading to certain conclusions. Now, that idea about this, I can't remember the name of the people, and I apologize for that. There is a fairly recent theory of consciousness that trying to respond to all the lipid experiments and things like that, arguing that, well, consciousness is sort of after the fact, but consciousness is actually for the future. So, the idea here is, consciousness emerges out of the evolution of episodic memory. And so, the function of consciousness is to allow us to create an episodic memory of something we have already done. And the point about the episodic memory is, as soon as you get into episodic memory, you get into perspectival knowing. That's what an episode is. You have a perspective on a situation, what you found salient and relevant, how your actions and the arena coupled or didn't couple, the affect, all of that, all of that that's so bound up with consciousness. And the point they make, and I totally agree with this, is as soon as you agree that there are multiple kinds of knowing, not just propositional and procedural, but also perspectival and participatory, you get the argument that episodic memory affords perspectival knowing, which allows you to solve problems that you can't solve without perspectival knowing. You can pick up on the world, the world discloses itself in ways that it is not otherwise disposable to you. And then consciousness emerges as an optimization on the formation of episodic memory. So it is optimally transfer appropriate for the future. And then you get, it is a sense of agency, but it's not a billiard ball agency. It's this kind of longitudinal agency. And to tell you the truth, given the human critiques, that's actually the kind of agency the cell has. It has this kind of longitudinal agency around episodes. It doesn't have this, I'm an sort of uncaused cause, a moved mover within myself, which I think is both a ridiculous proposal ontologically and an ethically undesired. Why would I want such a thing? It's completely arbitrary. I think my life is being lived. I'm trying to change myself so that I am, my thoughts are as determined by what is true, my action as determined by what is good, and my perception is determined by what is beautiful as I possibly can. I would like to lose all my freedom in that sense. And so I think if we move to the right level of analysis, and Gallagher makes a convergent argument about this, that when we're talking about agency and selfhood, we're not talking at that limit scale. We're not talking at where's the first movement of the billiard ball chain. We're talking more about, no, no, no, how are we building this long-term virtual engine that enhances our predictive agency in the world? Yeah, absolutely. And Gallagher's work is very convergent with Thomas Metzinger's, of course. And so this, just for our audience, this, these arguments, and this notion of the narrativized self is well fleshed out in active inference literature. So I could point you, yeah, I would probably start with the Fristlin and Limonalski papers. There are two wonderful papers about self-construction under active inference. And then I have a convergent argument coming out of Daniel Huda, who is also in the 4U, the Narrative Practice Hypothesis. And he argues that any mindsight ability, any ability to see in other people's mental states, you know, attributing beliefs and desires to them, requires, well, for example, if I'm going to attribute a belief and desire, I need to know something about your character. Are you lying or not? I need to know something about the context, the setting. What's going on in this situation? Are you tired? Is that a small child? So you're not really lying. And when you tell them at Christmas time that there's a Santa Claus, right? Right. And I need to know what the conflict is, what the problem. Notice what I'm talking about here. I need to know all the elements of narrative. Daniel Huda points out, we practice narrative incessantly. And unlike many of the other things, including language, which we scaffold for children, we scaffold narrative for our kids. I had to sit through the teletubbies twice, in which you have to sit through these really impoverished narratives, because we're scaffolding this up because narrative ability gives us the right, gives us the set of skills, the sets of states of mind with the perspectives taking therein, and the traits of character that allow us to pick up on other people's mental states. And I think that is also a function of the self, the function of selves is to make us agentically predictive to each other. Right. Exactly. Exactly. And there's another, there's another convergent argument in active inference, which is that only in the context of other people like you, would you ever come to the inference that there is something that is like to be you? Yeah. If you were low, that converges with the Vagatsky approach to the development of metacognition and self-awareness, which I think is right. Right. And of course, your colleague at just pointing people in different directions, your colleague at Toronto, former colleague Jordan Peterson, as a whole literature on literature and narrative. And he has a conversation with Carl on his podcast, which is deeply intriguing about these kind of alignments of the free energy principle and narrative. John, I wanted to also speak about flow states. You've written about flow as the sort of locus of implicit learning. As you know, I've just been writing up a paper on flow for an active inference perspective. So I'd love to be able to sort of see maybe where we align, where we don't align and try and sort of unpick that. Just as forewarning my paper, the paper that I've written with my wonderful co-authors is not out yet, but it should be coming out soon. So I want to, I have not yet read your paper because you suggest waiting. Yes. The paper has gone through modifications as papers are want to do. It's in the final stages. So you have access to it. So please feel free to read it. I will read it then. I will read it. I can give a very brief overview. So to not disadvantage you. My, the basic thesis of the basic perspective that we take on the paper is about self-modeling under flow states. So that's the main thing that we're looking at and the attenuation of an epistemic agent model. Because what we're arguing is that certain precision waiting mechanisms are lending, leading the organism to undertake pragmatic actions, seeking out pragmatic affordances, rather than engaging in what Carl or other authors would call epistemic foraging. Now, I think where we might have a point of difference here is that I have a section on flow states and learning. And I think this is a really interesting point because your paper speaks about implicit learning. Whereas I'm, and this, and just to make it transparent, this is there, I've had different perspectives on this, but my current opinion is that in flow states, what you're getting is a reinforcement of the skills that you actually picked up through epistemic work. So I have the example of a violinist. The violinist must undergo a certain period of exploration, of epistemic foraging, which comes with this kind of self-talk, this real prominence of myself as a knowing thing. And then what it does is over time through learning, you get high precision over the beliefs about that action in terms of the technical detail. And then that becomes kind of the foundation on which you can then go and do more epistemic work. And then that expertise development is stepwise, in a sense. I mean, you can picture it either way. But critically, where we differ is that I'm arguing that actually in the moment of flow itself, you're just garnering more evidence for the capacities and the policies that you already have. So I'd like to start there and see what you think of that claim. So we, as you point out, Leo Ferraro in Herobenidae, we argue something different, which is why you're bringing it up. We argue that learning is inevitably occurring even in the flow situation. And so there's improvement. And that's why if the environment doesn't have the capacity to renew its challenges on you, you will very quickly fall out of the flow state. So the argument is, well, why don't you just stay in the flow state? Well, the argument is, because eventually you get a mastery over the environment, which means your skills start to exceed the demands. And so phenomenologically, when I'm in the flow state, like inspiring or lecturing, I'm finding tremendous amount of insight and innovation coming out. Now, this is where it might get tricky, because is procedural innovation a restructuring of, especially, is that a restructuring of your information and therefore have new capacities in it, new emergent abilities? Or is it just a reinforcement? And I don't, we might get into the thesis of ship here, which is problematic. But the one of the defining phenomenological features of the flow state, and I want to be clear, I'm not pinning you down on this, but I do think it's relevant evidence is the ongoing sense of discovery. There's a sense of discovery there. There's a sense of coming to know things you did not know before. And of course, when the flow state is in much more comprehensive expertise, not like the plain tennis or maybe like just your optimal gripping on the world, people come to think they have learned something deeply profound about reality. So I tend to think that there's evidence for transformative learning happening up and beyond just reinforcement learning. Okay. Yeah, I like the return to the phenomena. You know, Dreyfus has this phrase, which I will definitely use over and over again in this podcast, which is when in doubt return to the phenomena. The way that we have cast it is that there's this idea in the literature about a hyper prior that the world is changing. So that we have a prior that the world is changing. And that is adapted for us because we don't get stuck in the same free energy minima. Now, if that prior is always at play, what ends up happening downstream on that is that my inference about my own abilities deteriorates over time. And we experience that, I guess, I mean, like if I play tennis today, and then I play tennis in a week's time, I'm probably going to have less fidelity in my own capacities in the week's time. And I do, if I play tomorrow, having followed today. And so our argument is that the positive affect that's part of the flow state is downstream on the surprise that's generated when you actually violate that hyper prior that the world is changing. Because what you're getting evidence for is that your policy still work in the world that you would have been for, you would have thought didn't lend itself to your policies working. And there's this idea, Casper Hesp's paper and his co-authors in 2021 that affect inactive inference is about basically your model doing better than you thought it would do. And this actually comes a lot into what Mark talks about in terms of aerodynamics. So I'm wondering, I don't know. Well, see, I think that's right. And the problem, of course, is you can get an infinite regress. So well, I'll say, well, the things here, and you'll say, well, there needs to be something a hyper prior behind that. And then we can, that's what I mean about thesis is shit. So again, I would say, well, what are you getting better at? And what you're getting better at, I would argue, is not stand like so expertise is generally built around giving you a well, like a specific domain. We can talk about the possibility of sort of meta expertise if you want. But what I mean by that is, once you've got expertise, you've gotten really good at within a certain domain at formulating the problems you're fronting as for you, well defined problems. That's one way in which an expert is sort of reliably different from a novice. A novice goes in, this is no defined problem, the expert goes, it goes in, it's a, no, it's this problem. And this is the right way to do this and this and this and this. Okay, so we agree with that. But I think what's happening, right? And maybe this is the gray area between your position in mind. I think what's happening is we're discovering new ways in which we can turn well defined problem, sorry, ill defined states into well defined problems. But something's like, I didn't realize I could adapt to that situation, but I can. And that sounds similar. But for me, that's an insight experience. And that's what, like it's an insight flow, because you're getting an extension of your cognitive capacity, because you have restructured what you take to be like your, your problem formulation of the world. That's, that's, yeah, I mean, to supplement that, I actually, I don't disagree on that note, but the way I would formulate that, and this actually isn't in the paper. It's in the paper in some sense, but it's not a fully fleshed out argument, because it wasn't hyper relevant. I'm glad there's another really quality paper on flow. There's really nothing. When we published the paper, we were literally the only paper talking about the cognitive processes that work in flow. It's a bizarre one. There's very little on skillful coping, generally, within cognitive science. But as I said, I think that's why I said there should be more phenomenology active or cognitive science convergence. What we kind of mentioned in brief is that flow, if we take the sort of macro perspective on what flow is, it can also be this kind of humming between the pragmatic and the epistemic boundary. So maybe when you're realizing, oh, I can reframe this kind of perplexing problem and actually something manageable, what you're doing there is maybe just slightly x-ting the flow, doing some epistemic foraging, returning, because something I want to make really clear to everyone listening as well. When we talk about epistemic action and pragmatic action in terms of expected free energy or active inference, it's not like the agent goes, I'm doing pragmatic action right now. It's part of the actual maths. If you look at the maths, firstly, you can do a bit of both at the same time. That's a thing. But it's also like the action policies that we're talking about here, John and I, these are not protracted seven hours of just like pragmatic action. No, these have to be very temporally thin because the world is volatile. The world is constantly changing. And so you have to be able to not just pin all your hopes or pull your eggs in one basket in terms of an action policy. And so actually fundamentally, our divergence might well be a semantic thing, which is that I'm talking very much about the synchronic nature of being in flow right now. And for simplicity, I have that boundary between pragmatic and epistemic, whereas if you take a more macro perspective, as that flow state, as the concerto unfolds, you could be humming at that boundary and doing learning as well. Yeah, I really look forward to your paper because I think that sounds like actually a powerful way in which the theories could be integrated together. I'm also interested in the question that's emerging out of this, well, I've already been interested in the question, but it emerges out of what we're talking about. Because I'm interested because I think there's, there might be an additional thing. Because there's, you can flow in a situation and it doesn't transfer, it doesn't transfer to other domains, video game addiction is the classic model, you can flow in the game, you can't flow in the world. So you get depressed in the world, you can flow in the game and so you want to stay in the game more and more and more. That's very different than how I flow in Tai Chi Chuan. Tai Chi Chuan, in fact, other people pointed this out in me, the flow states that I cultivate in Tai Chi Chuan, I can, they are cultivated in such a way that they transfer broadly and deeply to many other domains. And I'm interested in this sort of ritual framework, because what you get is this ritual and this philosophical framework, Taoism and other, other practices and ecology of practices around so that it broadens, like it transfers in powerful ways. And I'm wondering what the differences are. And also if, how does that show up? Is there a phenomenological difference? I mean, so now really, really, really weakly, anecdotally, right, in terms of phenomenological practice, I do get a sense of a difference between what I'm just flowing and what I'm flowing in a ritual context. So if I enter into a violin hall, and let's say I'm an expert in playing the violin, and I see a crowd and I see my violin and I see the whole stage, what I end up getting is this, so habits are quite well codified within active inference, you get this kind of contextual cue that like in layman's touch, although this isn't, we're saying this isn't happening propositionally, it's time for me to play the violin. And this can look like precision over beliefs about your own action. So what, this is again, not something I've thought about necessarily with any depth, because this is just something that you've brought up. I'm wondering whether that contextual cue, what you're happening when you're doing Tai Chi and then maybe what you're doing when you're lecturing is that your system is seeing similarities in the context and doing similar precision weighting in a separate domain. Yeah, I think that's right. And I think what philosophical frameworks do is they reverse engineer that. They try to find out what might be similar, or maybe even invariant across many contexts, and then build that back into the specific context in which you are doing the practice to exactly afford that transfer appropriate processing. I think that's right. I think there's something else also going on with the cueing, and this goes into Aptur's metamotivational theory. We can be broadly speaking, we frame our arousal in two different ways. When we're in italic mode and we're working towards, where the reward is found in the external goal, then increased effort is experienced as frustration and then it increases too much of getting anxiety. But if I'm doing something where the goal is the behavior itself, like making love or doing poetry, then the increased, not infinitely obviously, but the increased arousal is framed as positive, as excitement. And so I think also, and so Aptur talks about safety framing. Let's call the first italic mode work and the peritolic mode a play. I think ritual has to do with serious play, and I've got a whole argument about that. But I think also what you're doing in the context is you're trying to cue people into the play mode because the play mode allows them to model themselves, their arousal, as positive, rather than as negative. And that allows them, and I think there's a deep connection between this peritolic framing and Csikszentmihais, I think accurate claim that flow is autotelic, doing it for its own sake. Right. Yeah, cool. Yeah, I like the notion of effort here. Recognize that one of the fundamental factors or fundamental characteristics of flow is perceived effortlessness. Yes, even though you can, even though you can have sort of very peripheral cues that you might be expending a lot of metabolic energy. Exactly. And just for the, given this is the Active Inference Institute's podcast, just for those interested in the computational framing of that within Active Inference, Thomas Parr has got a paper out this year on cognitive effort and Active Inference. So that gives a kind of nice computational modeling picture of that, which actually leads me to a much broader question because I know we're, we haven't got too much more time, which is I've noticed in myself to be totally candid that I've started, like I learned about flow and Csikszentmihais through you and through other people, but mainly as a philosophical notion. And now I'm viewing it as a computational notion. Yes. Do you worry about that? Do you worry that if we rely too much on computation, not just computational models, but maths, physics, we in some ways reduce the phenomena in a way that's detrimental, not just because it's not romantic or that it's, but like that we're missing something fundamentally? Well, it depends. I mean, that's a really important question. We could do two hours on just that question. But if you think that a leveled ontology is actually the way ontology is, that the level at which we do science is as ontologically real as the quantum level we discover when we're doing science. And I think you have to come to that conclusion. Right. And I have extended arguments for that elsewhere. Then you can make a clean philosophical distinction between explaining a way in a reductive fashion and explaining that actually enriches your appreciation for the phenomena. Now, if the computational stuff, the computational stuff to my mind, well, let's say what the argument we were just exploring has merit, then notice what we were saying. We're saying, well, what the computational modeling actually does is actually shows why this sometimes very obscure philosophical framework is really important. It's actually doing some really important work. And you can't get rid of it. You can't dispense with it. And that would mean that some of the stuff we're doing, where we're trying to commodify flow and take it out of those frameworks and do the thing we do and sell books. It's actually misrepresenting the phenomena in a way that we can philosophically and scientifically critique. It's like, wait, wait, this phenomena has the power it has, because Chick-Mat St. said it's an evolutionary market for adaptivity. And then, well, making it adaptive is how broadly and deeply is it transferring out of the situation. And then the philosophical framework really matters to what it is and how it functions. And so if you can make a distinction and you need an ontological distinction to make this distinction, but if you can make a distinction between explaining and explaining a way, then I think it's possible to say, no, no, when I do these things. And to be fair to me, there's, I got a lot of people who, you know, from various religious and spiritual backgrounds, and they come in and they say, thank you so much for your work. I now, I much better appreciate this, this experience or that experience or the flow state or this mystical state, because you didn't try and tell me it's nothing. But you tried to say, this is what it's doing. And this is why you like it and value it so much. But do you think your work, to kind of, yeah, to look at that personal vantage point, do you think you've managed to keep, like, because if I, without being sort of embarrassingly lording, people should watch Awakening from the Meaning Crisis, not only for the content, but also just the way you present it, which is just so magical. So thanks. I mean, it's really inspired a lot of the way that I think science should be done and philosophy should be done. What, what is it about, I think, you know, anyone who watches that recognizes there's something kind of special going on there. The ideas are living through you, you're, they're breathing through you, you're breathing through them, there's this really beautiful coupling. Again, we'll come back to that between you and the ideas. What, what is it about that kind of synthesis of the philosophy and the sort of more hard cognitive science? Do you think made that project so successful? I suppose it's a view of the role of cognitive science, what cognitive science is doing. I think what cognitive science does is it's, well, I'll try and do it sort of narratively. And I do this in the series. I think, you know, even the notion of mind that we've been invoking mind and cognition is equivocal, because it means one thing to the neuroscientists who study the brain and looking at neurons and anatomical networks and perhaps functional networks. It means a different thing to the artificial intelligence person who's building algorithms and heuristics and doing reinforcement learning and blah, blah. It means a different thing to the psychologists who study human behavior with experiments and running stats. And they talk about working memory. They don't talk so much right. And it means a different thing to the linguists and a different thing to the cultural anthropologists. By the way, I include them because they were the people that have been studying distributed cognition and collective intelligence. They matter. Yeah. If you think about it on this analogy, it's like they're different countries speaking different languages. And here's the thing that has great value. Specialization has great value. I'm not dismissing that. I am not dismissing that. But each one of these is talking about a different level. And here's what I think I would state as a very plausible claim. I think it's unlikely that these levels in reality, the brain information processing, behavioral, linguistic, sociocultural levels are independent from each other. I think they causal influence and constrain each other. So we are missing something important about the mind by not getting clear about the relationship between these levels. We can be equivocating and we have a fragmented notion if we don't capture the relations of constraint and causation between these different levels of cognition and mind. And so I think the proper function of cognitive science is to use philosophy's skills of creating bridging discourses, creating bridging conceptual vocabulary, theoretical grammar, so that these different disciplines can talk to each other in reciprocally reconstructive ways as a way of converging on the causal and constraint relationships between the levels, rather than trying to compete or say the bottom level is the only real level. So that for me, they all live together. They are doing, and I'm sorry to evoke it again, they're doing opponent processing between each other. And that's what inhabits me when I'm doing cognitive science, that vision. I call it synoptic integration. Yeah, it's, well, it's wonderful to see. So, I mean, like, I think we, I mean, that's the cognitive scientists that I'm aspiring to be. I think you've called it a big picture cognitive scientist as well. Yes. And one of the things, one of the, and I felt very lonely for a while, you know, I was doing relevance realization, which is a big picture. And then, to my great delight, another big picture for ecocide down the road. And then to my even greater happiness, another big picture came down the road, which is, you know, the predictive processing framework. And I think they are more convergent than adversarial. And I, well, you've heard me arguing about how we can integrate them together. Excellent. Yeah, I would say maybe it's just bias. Active inferences or physics processing is a big picture cognitive scientist from the physics and the maths, all the way, well, what we're trying to do to the phenomena, to what it is like to be a, to be a human being, to have conversations like this. John, it's, it was, you know, I can't say it was better than I expected, because I knew it was going to be wonderful, but you're full of just thrilling insights and speaking to you, listening to you is always such a wonderful learning experience. Where that happens in flow or without flow. But I just wanted to thank you so very much for, I know you're extremely busy, but for giving us your time. Where can people find you? What have you got coming up? I'm sure people are curious. So the most immediate thing for this conversation is the talk I gave at Leiden on the predictive processing symposium. It's up on my channel. I think it's the second most recent video on my channel on YouTube. We can put it in the video. Yeah. Where I try to go into the nuts and bolts of how you could integrate predictive processing and relevance realisation theory together. So people might find that. And I've got a lot of good feedback on that for being sort of clear and a good argument. So I would recommend that when people are interested in the broader implications of this awakening for the meeting crisis and then after Socrates also after Socrates is where I take all of this and all of this stuff and how do you turn it into practices in order to overcome self deception and enhance relevance realisation become more wise and virtuous. And so they can take a look there. The arguments around neoplatonism. There are several videos around that. I try to connect neoplatonism to foreecogsci and relevance realisation and predictive processing as we saw in this podcast. But in the neoplatonism I'm working on my third big series. Walking the philosophical Silk Road which will be on Zen neoplatonism. Trying to see if we can bring an integration, an opponent processing, not an adversarial one, an opponent processing between Zen and neoplatonism to give us a rich philosophical framework by which like the philosophical road we can trade ideas and move between worlds without having to descend into well tribalism and other such things. Wonderful, wonderful. Well, again, it was absolutely my pleasure. I apologise. I've got a slight cold. So if I've been a little sniffly or nasal, we can blame it on the London weather. But I'd love to have you back on. At some point your work is truly inspiring. So thank you so much, John, for me and the Institute. Thank you, Doris. I'm happy to come back on. If it turns out that Mark and I come on together, that would be thrilling as well. Great. We will definitely get that sorted. All right, thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.200000000000001, "text": " Okay, and we're away. Welcome, everybody, to the third episode of Active Inference Insights,", "tokens": [50364, 1033, 11, 293, 321, 434, 1314, 13, 4027, 11, 2201, 11, 281, 264, 2636, 3500, 295, 26635, 682, 5158, 9442, 5761, 11, 51024], "temperature": 0.0, "avg_logprob": -0.2429232443532636, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.2290637344121933}, {"id": 1, "seek": 0, "start": 13.200000000000001, "end": 18.84, "text": " brought to you by the Active Inference Institute. I'm your host, Darius Parveezy-Wayne, and", "tokens": [51024, 3038, 281, 291, 538, 264, 26635, 682, 5158, 9446, 13, 286, 478, 428, 3975, 11, 413, 27440, 3457, 303, 68, 1229, 12, 54, 320, 716, 11, 293, 51306], "temperature": 0.0, "avg_logprob": -0.2429232443532636, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.2290637344121933}, {"id": 2, "seek": 0, "start": 18.84, "end": 24.92, "text": " today I am absolutely thrilled to be able to speak to John Viveke. John is an award-winning", "tokens": [51306, 965, 286, 669, 3122, 18744, 281, 312, 1075, 281, 1710, 281, 2619, 44288, 330, 13, 2619, 307, 364, 7130, 12, 32960, 51610], "temperature": 0.0, "avg_logprob": -0.2429232443532636, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.2290637344121933}, {"id": 3, "seek": 0, "start": 24.92, "end": 28.6, "text": " professor of psychology, cognitive science, and Buddhist philosophy at the University", "tokens": [51610, 8304, 295, 15105, 11, 15605, 3497, 11, 293, 22764, 10675, 412, 264, 3535, 51794], "temperature": 0.0, "avg_logprob": -0.2429232443532636, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.2290637344121933}, {"id": 4, "seek": 2860, "start": 28.6, "end": 32.68, "text": " of Toronto. He is also the presenter of the renowned YouTube series Awakening from the", "tokens": [50364, 295, 14140, 13, 634, 307, 611, 264, 35594, 295, 264, 34065, 3088, 2638, 25274, 4559, 490, 264, 50568], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 5, "seek": 2860, "start": 32.68, "end": 37.88, "text": " Meaning Crisis, as well as the newer After Socrates. His work focuses on 4e cognitive", "tokens": [50568, 19948, 42846, 11, 382, 731, 382, 264, 17628, 2381, 407, 50243, 13, 2812, 589, 16109, 322, 1017, 68, 15605, 50828], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 6, "seek": 2860, "start": 37.88, "end": 42.52, "text": " science, which holds that cognition is embodied, embedded, enacted, and extended beyond the", "tokens": [50828, 3497, 11, 597, 9190, 300, 46905, 307, 42046, 11, 16741, 11, 41313, 11, 293, 10913, 4399, 264, 51060], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 7, "seek": 2860, "start": 42.52, "end": 48.0, "text": " brain. In particular, John explores relevance realisation, our adaptive ability to zero", "tokens": [51060, 3567, 13, 682, 1729, 11, 2619, 45473, 32684, 957, 7623, 11, 527, 27912, 3485, 281, 4018, 51334], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 8, "seek": 2860, "start": 48.0, "end": 53.72, "text": " in on salient information in a world of near-infinite complexity. Last year, he, Mark Miller, and", "tokens": [51334, 294, 322, 1845, 1196, 1589, 294, 257, 1002, 295, 2651, 12, 259, 5194, 642, 14024, 13, 5264, 1064, 11, 415, 11, 3934, 16932, 11, 293, 51620], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 9, "seek": 2860, "start": 53.72, "end": 58.0, "text": " Brett Anderson wrote the paper Predictive Processing and Relevance Realisation, exploring", "tokens": [51620, 29447, 18768, 4114, 264, 3035, 430, 24945, 488, 31093, 278, 293, 1300, 28316, 719, 8467, 7623, 11, 12736, 51834], "temperature": 0.0, "avg_logprob": -0.14945201268271793, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.24509581923484802}, {"id": 10, "seek": 5800, "start": 58.0, "end": 62.96, "text": " convergent solutions to the frame problem, which proposes that trade-offs in precision", "tokens": [50364, 9652, 6930, 6547, 281, 264, 3920, 1154, 11, 597, 2365, 4201, 300, 4923, 12, 19231, 294, 18356, 50612], "temperature": 0.0, "avg_logprob": -0.15364861729169133, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.007209013681858778}, {"id": 11, "seek": 5800, "start": 62.96, "end": 67.84, "text": " weighting, a key second order dynamic in predictive processing hierarchies, are at the heart of", "tokens": [50612, 3364, 278, 11, 257, 2141, 1150, 1668, 8546, 294, 35521, 9007, 35250, 530, 11, 366, 412, 264, 1917, 295, 50856], "temperature": 0.0, "avg_logprob": -0.15364861729169133, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.007209013681858778}, {"id": 12, "seek": 5800, "start": 67.84, "end": 72.32, "text": " our ability to be intelligently ignorant. And that alignment of predictive processing", "tokens": [50856, 527, 3485, 281, 312, 5613, 2276, 29374, 13, 400, 300, 18515, 295, 35521, 9007, 51080], "temperature": 0.0, "avg_logprob": -0.15364861729169133, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.007209013681858778}, {"id": 13, "seek": 5800, "start": 72.32, "end": 76.6, "text": " and relevance realisation is exactly what we're going to be talking about today. John,", "tokens": [51080, 293, 32684, 957, 7623, 307, 2293, 437, 321, 434, 516, 281, 312, 1417, 466, 965, 13, 2619, 11, 51294], "temperature": 0.0, "avg_logprob": -0.15364861729169133, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.007209013681858778}, {"id": 14, "seek": 5800, "start": 76.6, "end": 81.06, "text": " welcome to the show. Thank you so much for joining us. It's an absolute treat.", "tokens": [51294, 2928, 281, 264, 855, 13, 1044, 291, 370, 709, 337, 5549, 505, 13, 467, 311, 364, 8236, 2387, 13, 51517], "temperature": 0.0, "avg_logprob": -0.15364861729169133, "compression_ratio": 1.6194029850746268, "no_speech_prob": 0.007209013681858778}, {"id": 15, "seek": 8106, "start": 81.06, "end": 88.18, "text": " Thank you, there is a great pleasure. I'm hoping that Mark does make it on in December.", "tokens": [50364, 1044, 291, 11, 456, 307, 257, 869, 6834, 13, 286, 478, 7159, 300, 3934, 775, 652, 309, 322, 294, 7687, 13, 50720], "temperature": 0.0, "avg_logprob": -0.19374232022267468, "compression_ratio": 1.6066176470588236, "no_speech_prob": 0.5230783820152283}, {"id": 16, "seek": 8106, "start": 88.18, "end": 93.18, "text": " That'll be great. Yeah, absolutely. But in his absence, because Mark was meant to be", "tokens": [50720, 663, 603, 312, 869, 13, 865, 11, 3122, 13, 583, 294, 702, 17145, 11, 570, 3934, 390, 4140, 281, 312, 50970], "temperature": 0.0, "avg_logprob": -0.19374232022267468, "compression_ratio": 1.6066176470588236, "no_speech_prob": 0.5230783820152283}, {"id": 17, "seek": 8106, "start": 93.18, "end": 99.06, "text": " here today, but it's unfortunate, he's not unfortunately able to make it. Perhaps, so", "tokens": [50970, 510, 965, 11, 457, 309, 311, 17843, 11, 415, 311, 406, 7015, 1075, 281, 652, 309, 13, 10517, 11, 370, 51264], "temperature": 0.0, "avg_logprob": -0.19374232022267468, "compression_ratio": 1.6066176470588236, "no_speech_prob": 0.5230783820152283}, {"id": 18, "seek": 8106, "start": 99.06, "end": 105.02000000000001, "text": " this podcast is acting as a kind of primer and introduction for our audience to the critical", "tokens": [51264, 341, 7367, 307, 6577, 382, 257, 733, 295, 12595, 293, 9339, 337, 527, 4034, 281, 264, 4924, 51562], "temperature": 0.0, "avg_logprob": -0.19374232022267468, "compression_ratio": 1.6066176470588236, "no_speech_prob": 0.5230783820152283}, {"id": 19, "seek": 8106, "start": 105.02000000000001, "end": 110.38, "text": " themes in cognitive science and psychology and biology, as well as maths and physics,", "tokens": [51562, 13544, 294, 15605, 3497, 293, 15105, 293, 14956, 11, 382, 731, 382, 36287, 293, 10649, 11, 51830], "temperature": 0.0, "avg_logprob": -0.19374232022267468, "compression_ratio": 1.6066176470588236, "no_speech_prob": 0.5230783820152283}, {"id": 20, "seek": 11038, "start": 110.38, "end": 118.25999999999999, "text": " as they are right now. So I think it would be a worthwhile place to start if you could", "tokens": [50364, 382, 436, 366, 558, 586, 13, 407, 286, 519, 309, 576, 312, 257, 28159, 1081, 281, 722, 498, 291, 727, 50758], "temperature": 0.0, "avg_logprob": -0.15181861290564905, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.08093646913766861}, {"id": 21, "seek": 11038, "start": 118.25999999999999, "end": 123.38, "text": " just unpick what relevance realisation is, and then we'll come to how it aligns with", "tokens": [50758, 445, 20994, 618, 437, 32684, 957, 7623, 307, 11, 293, 550, 321, 603, 808, 281, 577, 309, 7975, 82, 365, 51014], "temperature": 0.0, "avg_logprob": -0.15181861290564905, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.08093646913766861}, {"id": 22, "seek": 11038, "start": 123.38, "end": 133.9, "text": " predictive processing. Sure. So relevance realisation is sort of inverts the way common", "tokens": [51014, 35521, 9007, 13, 4894, 13, 407, 32684, 957, 7623, 307, 1333, 295, 28653, 1373, 264, 636, 2689, 51540], "temperature": 0.0, "avg_logprob": -0.15181861290564905, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.08093646913766861}, {"id": 23, "seek": 13390, "start": 133.9, "end": 140.26000000000002, "text": " sense works. Common sense is there's a lot that is obvious to us. And you know what it's", "tokens": [50364, 2020, 1985, 13, 18235, 2020, 307, 456, 311, 257, 688, 300, 307, 6322, 281, 505, 13, 400, 291, 458, 437, 309, 311, 50682], "temperature": 0.0, "avg_logprob": -0.1622909182593936, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.7408232092857361}, {"id": 24, "seek": 13390, "start": 140.26000000000002, "end": 144.74, "text": " obvious what you should pay attention to. We're sometimes mistaken. But it's obvious", "tokens": [50682, 6322, 437, 291, 820, 1689, 3202, 281, 13, 492, 434, 2171, 21333, 13, 583, 309, 311, 6322, 50906], "temperature": 0.0, "avg_logprob": -0.1622909182593936, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.7408232092857361}, {"id": 25, "seek": 13390, "start": 144.74, "end": 149.58, "text": " what we should be remembering and what we should be doing. And that's all obvious. And", "tokens": [50906, 437, 321, 820, 312, 20719, 293, 437, 321, 820, 312, 884, 13, 400, 300, 311, 439, 6322, 13, 400, 51148], "temperature": 0.0, "avg_logprob": -0.1622909182593936, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.7408232092857361}, {"id": 26, "seek": 13390, "start": 149.58, "end": 155.98000000000002, "text": " we just run from that. Our job as cognitive scientists is to explain how the brain generates", "tokens": [51148, 321, 445, 1190, 490, 300, 13, 2621, 1691, 382, 15605, 7708, 307, 281, 2903, 577, 264, 3567, 23815, 51468], "temperature": 0.0, "avg_logprob": -0.1622909182593936, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.7408232092857361}, {"id": 27, "seek": 13390, "start": 155.98000000000002, "end": 163.5, "text": " that obviousness, that salience landscapes that makes the right things stand out as relevant", "tokens": [51468, 300, 6322, 1287, 11, 300, 1845, 1182, 29822, 300, 1669, 264, 558, 721, 1463, 484, 382, 7340, 51844], "temperature": 0.0, "avg_logprob": -0.1622909182593936, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.7408232092857361}, {"id": 28, "seek": 16350, "start": 163.5, "end": 169.98, "text": " to us so that we can do what is astonishing, solve a wide variety of problems and a wide", "tokens": [50364, 281, 505, 370, 300, 321, 393, 360, 437, 307, 35264, 11, 5039, 257, 4874, 5673, 295, 2740, 293, 257, 4874, 50688], "temperature": 0.0, "avg_logprob": -0.1741200220017206, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.05735033378005028}, {"id": 29, "seek": 16350, "start": 169.98, "end": 176.06, "text": " variety of domains. You're a general problem solver, which is just astonishing. I mean,", "tokens": [50688, 5673, 295, 25514, 13, 509, 434, 257, 2674, 1154, 1404, 331, 11, 597, 307, 445, 35264, 13, 286, 914, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1741200220017206, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.05735033378005028}, {"id": 30, "seek": 16350, "start": 176.06, "end": 180.46, "text": " you can learn Albanian history, you could learn about swimming or rock climbing, you", "tokens": [50992, 291, 393, 1466, 41547, 952, 2503, 11, 291, 727, 1466, 466, 11989, 420, 3727, 14780, 11, 291, 51212], "temperature": 0.0, "avg_logprob": -0.1741200220017206, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.05735033378005028}, {"id": 31, "seek": 16350, "start": 180.46, "end": 186.78, "text": " can take up the study of dinosaurs in the Jurassic period. Like it's just, you know,", "tokens": [51212, 393, 747, 493, 264, 2979, 295, 25851, 294, 264, 44730, 2896, 13, 1743, 309, 311, 445, 11, 291, 458, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1741200220017206, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.05735033378005028}, {"id": 32, "seek": 16350, "start": 186.78, "end": 192.02, "text": " it's just amazing and astonishing. And there's good empirical evidence you have something", "tokens": [51528, 309, 311, 445, 2243, 293, 35264, 13, 400, 456, 311, 665, 31886, 4467, 291, 362, 746, 51790], "temperature": 0.0, "avg_logprob": -0.1741200220017206, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.05735033378005028}, {"id": 33, "seek": 19202, "start": 192.06, "end": 196.02, "text": " like a general intelligence. There seems to be a general capacity. There's individual", "tokens": [50366, 411, 257, 2674, 7599, 13, 821, 2544, 281, 312, 257, 2674, 6042, 13, 821, 311, 2609, 50564], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 34, "seek": 19202, "start": 196.02, "end": 201.14000000000001, "text": " variation in talent. Some people are better at learning this domain than other domains.", "tokens": [50564, 12990, 294, 8301, 13, 2188, 561, 366, 1101, 412, 2539, 341, 9274, 813, 661, 25514, 13, 50820], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 35, "seek": 19202, "start": 201.14000000000001, "end": 206.18, "text": " But they're way back from Spearman. We know there's some sort of general ability. I'm", "tokens": [50820, 583, 436, 434, 636, 646, 490, 3550, 289, 1601, 13, 492, 458, 456, 311, 512, 1333, 295, 2674, 3485, 13, 286, 478, 51072], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 36, "seek": 19202, "start": 206.18, "end": 210.9, "text": " going to propose that the general ability are two interlocking things, the anticipation", "tokens": [51072, 516, 281, 17421, 300, 264, 2674, 3485, 366, 732, 728, 4102, 278, 721, 11, 264, 35979, 51308], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 37, "seek": 19202, "start": 210.9, "end": 215.3, "text": " and relevance realisation. What I mean by a general ability is these are kind of meta", "tokens": [51308, 293, 32684, 957, 7623, 13, 708, 286, 914, 538, 257, 2674, 3485, 307, 613, 366, 733, 295, 19616, 51528], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 38, "seek": 19202, "start": 215.3, "end": 218.84, "text": " problems. These are the two big problems you have to solve when you're doing any other", "tokens": [51528, 2740, 13, 1981, 366, 264, 732, 955, 2740, 291, 362, 281, 5039, 562, 291, 434, 884, 604, 661, 51705], "temperature": 0.0, "avg_logprob": -0.14029658635457357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.12710611522197723}, {"id": 39, "seek": 21884, "start": 218.92000000000002, "end": 225.56, "text": " specific problem solving. And what makes relevance realisation so hard, generating that obviousness", "tokens": [50368, 2685, 1154, 12606, 13, 400, 437, 1669, 32684, 957, 7623, 370, 1152, 11, 17746, 300, 6322, 1287, 50700], "temperature": 0.0, "avg_logprob": -0.20176424059951514, "compression_ratio": 1.7186311787072244, "no_speech_prob": 0.009112230502068996}, {"id": 40, "seek": 21884, "start": 225.56, "end": 231.24, "text": " so mysterious, actually. And that's what Scott Atrin says. He says that what science does", "tokens": [50700, 370, 13831, 11, 767, 13, 400, 300, 311, 437, 6659, 1711, 12629, 1619, 13, 634, 1619, 300, 437, 3497, 775, 50984], "temperature": 0.0, "avg_logprob": -0.20176424059951514, "compression_ratio": 1.7186311787072244, "no_speech_prob": 0.009112230502068996}, {"id": 41, "seek": 21884, "start": 231.24, "end": 236.28, "text": " is it makes what's common sense to take, takes to be obvious, mysterious. Here's a table", "tokens": [50984, 307, 309, 1669, 437, 311, 2689, 2020, 281, 747, 11, 2516, 281, 312, 6322, 11, 13831, 13, 1692, 311, 257, 3199, 51236], "temperature": 0.0, "avg_logprob": -0.20176424059951514, "compression_ratio": 1.7186311787072244, "no_speech_prob": 0.009112230502068996}, {"id": 42, "seek": 21884, "start": 236.28, "end": 240.76, "text": " in front of me, but physics says, well, what is that really? And they're mysterious and", "tokens": [51236, 294, 1868, 295, 385, 11, 457, 10649, 1619, 11, 731, 11, 437, 307, 300, 534, 30, 400, 436, 434, 13831, 293, 51460], "temperature": 0.0, "avg_logprob": -0.20176424059951514, "compression_ratio": 1.7186311787072244, "no_speech_prob": 0.009112230502068996}, {"id": 43, "seek": 21884, "start": 240.76, "end": 244.92000000000002, "text": " it's made out of quarks and all this sort of stuff. It's the same sort of thing here.", "tokens": [51460, 309, 311, 1027, 484, 295, 421, 20851, 293, 439, 341, 1333, 295, 1507, 13, 467, 311, 264, 912, 1333, 295, 551, 510, 13, 51668], "temperature": 0.0, "avg_logprob": -0.20176424059951514, "compression_ratio": 1.7186311787072244, "no_speech_prob": 0.009112230502068996}, {"id": 44, "seek": 24492, "start": 244.92, "end": 250.67999999999998, "text": " Because when you think about it outside of common sense, you realise that there's just", "tokens": [50364, 1436, 562, 291, 519, 466, 309, 2380, 295, 2689, 2020, 11, 291, 18809, 300, 456, 311, 445, 50652], "temperature": 0.0, "avg_logprob": -0.09335768850226152, "compression_ratio": 1.9113924050632911, "no_speech_prob": 0.0014988178154453635}, {"id": 45, "seek": 24492, "start": 250.67999999999998, "end": 255.23999999999998, "text": " an overwhelming amount of information that's constantly, as you said, dynamically changing", "tokens": [50652, 364, 13373, 2372, 295, 1589, 300, 311, 6460, 11, 382, 291, 848, 11, 43492, 4473, 50880], "temperature": 0.0, "avg_logprob": -0.09335768850226152, "compression_ratio": 1.9113924050632911, "no_speech_prob": 0.0014988178154453635}, {"id": 46, "seek": 24492, "start": 255.23999999999998, "end": 259.32, "text": " in the environment. There's an overwhelming amount of information in your long-term memory", "tokens": [50880, 294, 264, 2823, 13, 821, 311, 364, 13373, 2372, 295, 1589, 294, 428, 938, 12, 7039, 4675, 51084], "temperature": 0.0, "avg_logprob": -0.09335768850226152, "compression_ratio": 1.9113924050632911, "no_speech_prob": 0.0014988178154453635}, {"id": 47, "seek": 24492, "start": 259.32, "end": 265.15999999999997, "text": " that's constantly changing and being readjusted to the reconstructive nature of memory. There", "tokens": [51084, 300, 311, 6460, 4473, 293, 885, 1401, 3424, 292, 281, 264, 16891, 21673, 3687, 295, 4675, 13, 821, 51376], "temperature": 0.0, "avg_logprob": -0.09335768850226152, "compression_ratio": 1.9113924050632911, "no_speech_prob": 0.0014988178154453635}, {"id": 48, "seek": 24492, "start": 265.15999999999997, "end": 270.84, "text": " is an overwhelming number of combinations of actions you can perform, sequences of action.", "tokens": [51376, 307, 364, 13373, 1230, 295, 21267, 295, 5909, 291, 393, 2042, 11, 22978, 295, 3069, 13, 51660], "temperature": 0.0, "avg_logprob": -0.09335768850226152, "compression_ratio": 1.9113924050632911, "no_speech_prob": 0.0014988178154453635}, {"id": 49, "seek": 27084, "start": 270.84, "end": 277.08, "text": " And yet, you ignore that overwhelming, almost all of that overwhelming information", "tokens": [50364, 400, 1939, 11, 291, 11200, 300, 13373, 11, 1920, 439, 295, 300, 13373, 1589, 50676], "temperature": 0.0, "avg_logprob": -0.11042460046633325, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0016990568256005645}, {"id": 50, "seek": 27084, "start": 277.08, "end": 282.76, "text": " and zero in on the relevant information so that you are oriented right in the world,", "tokens": [50676, 293, 4018, 294, 322, 264, 7340, 1589, 370, 300, 291, 366, 21841, 558, 294, 264, 1002, 11, 50960], "temperature": 0.0, "avg_logprob": -0.11042460046633325, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0016990568256005645}, {"id": 51, "seek": 27084, "start": 282.76, "end": 289.0, "text": " finding things obvious, salient, standing out. And it's not just a single thing. There's", "tokens": [50960, 5006, 721, 6322, 11, 1845, 1196, 11, 4877, 484, 13, 400, 309, 311, 406, 445, 257, 2167, 551, 13, 821, 311, 51272], "temperature": 0.0, "avg_logprob": -0.11042460046633325, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0016990568256005645}, {"id": 52, "seek": 27084, "start": 289.0, "end": 293.0, "text": " like a salient's landscape. Some things stand out more than others. Some things are more", "tokens": [51272, 411, 257, 1845, 1196, 311, 9661, 13, 2188, 721, 1463, 484, 544, 813, 2357, 13, 2188, 721, 366, 544, 51472], "temperature": 0.0, "avg_logprob": -0.11042460046633325, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0016990568256005645}, {"id": 53, "seek": 27084, "start": 293.0, "end": 298.44, "text": " foreground, background. You have all this happening so that you are capable of solving", "tokens": [51472, 32058, 11, 3678, 13, 509, 362, 439, 341, 2737, 370, 300, 291, 366, 8189, 295, 12606, 51744], "temperature": 0.0, "avg_logprob": -0.11042460046633325, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0016990568256005645}, {"id": 54, "seek": 29844, "start": 298.44, "end": 305.16, "text": " so many problems in a messy, complex world in which there's constant novelty because of an", "tokens": [50364, 370, 867, 2740, 294, 257, 16191, 11, 3997, 1002, 294, 597, 456, 311, 5754, 44805, 570, 295, 364, 50700], "temperature": 0.0, "avg_logprob": -0.09540306791967275, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0016116725746542215}, {"id": 55, "seek": 29844, "start": 305.16, "end": 310.6, "text": " emergent phenomena. And trying to give this ability to machines has been overwhelmingly", "tokens": [50700, 4345, 6930, 22004, 13, 400, 1382, 281, 976, 341, 3485, 281, 8379, 575, 668, 42926, 50972], "temperature": 0.0, "avg_logprob": -0.09540306791967275, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0016116725746542215}, {"id": 56, "seek": 29844, "start": 311.4, "end": 316.6, "text": " difficult and one of the hard, hard problems. We're actually bumping up right into it now", "tokens": [51012, 2252, 293, 472, 295, 264, 1152, 11, 1152, 2740, 13, 492, 434, 767, 9961, 278, 493, 558, 666, 309, 586, 51272], "temperature": 0.0, "avg_logprob": -0.09540306791967275, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0016116725746542215}, {"id": 57, "seek": 29844, "start": 316.6, "end": 321.4, "text": " as we finally taken up the AGI project, the project of trying to create artificial general", "tokens": [51272, 382, 321, 2721, 2726, 493, 264, 316, 26252, 1716, 11, 264, 1716, 295, 1382, 281, 1884, 11677, 2674, 51512], "temperature": 0.0, "avg_logprob": -0.09540306791967275, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0016116725746542215}, {"id": 58, "seek": 29844, "start": 321.4, "end": 327.72, "text": " intelligence. Excellent. It may be worth digging a little bit deeper into that", "tokens": [51512, 7599, 13, 16723, 13, 467, 815, 312, 3163, 17343, 257, 707, 857, 7731, 666, 300, 51828], "temperature": 0.0, "avg_logprob": -0.09540306791967275, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0016116725746542215}, {"id": 59, "seek": 32772, "start": 327.8, "end": 334.84000000000003, "text": " exposition because people might be thinking, well, don't we just have a module for relevance", "tokens": [50368, 1278, 5830, 570, 561, 1062, 312, 1953, 11, 731, 11, 500, 380, 321, 445, 362, 257, 10088, 337, 32684, 50720], "temperature": 0.0, "avg_logprob": -0.13899255835491678, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.002688363427296281}, {"id": 60, "seek": 32772, "start": 334.84000000000003, "end": 341.40000000000003, "text": " or salience? Don't we just kind of, we're born and we know, oh, I should not fall off cliffs and I", "tokens": [50720, 420, 1845, 1182, 30, 1468, 380, 321, 445, 733, 295, 11, 321, 434, 4232, 293, 321, 458, 11, 1954, 11, 286, 820, 406, 2100, 766, 50039, 293, 286, 51048], "temperature": 0.0, "avg_logprob": -0.13899255835491678, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.002688363427296281}, {"id": 61, "seek": 32772, "start": 341.40000000000003, "end": 346.52000000000004, "text": " should, but actually what happened yesterday might be quite irrelevant. You have a wonderful", "tokens": [51048, 820, 11, 457, 767, 437, 2011, 5186, 1062, 312, 1596, 28682, 13, 509, 362, 257, 3715, 51304], "temperature": 0.0, "avg_logprob": -0.13899255835491678, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.002688363427296281}, {"id": 62, "seek": 32772, "start": 348.04, "end": 353.56, "text": " explanation in your sort of keynote paper, Explicating Relevance Realization, where you say", "tokens": [51380, 10835, 294, 428, 1333, 295, 33896, 3035, 11, 12514, 30541, 1300, 28316, 719, 8467, 2144, 11, 689, 291, 584, 51656], "temperature": 0.0, "avg_logprob": -0.13899255835491678, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.002688363427296281}, {"id": 63, "seek": 35356, "start": 354.28000000000003, "end": 361.56, "text": " that leads to an infinite regress. Perhaps you could explain that argument.", "tokens": [50400, 300, 6689, 281, 364, 13785, 1121, 735, 13, 10517, 291, 727, 2903, 300, 6770, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11358879185930083, "compression_ratio": 1.560747663551402, "no_speech_prob": 0.005712064914405346}, {"id": 64, "seek": 35356, "start": 362.44, "end": 366.68, "text": " So the magic module, not to be confused with predictive processing's", "tokens": [50808, 407, 264, 5585, 10088, 11, 406, 281, 312, 9019, 365, 35521, 9007, 311, 51020], "temperature": 0.0, "avg_logprob": -0.11358879185930083, "compression_ratio": 1.560747663551402, "no_speech_prob": 0.005712064914405346}, {"id": 65, "seek": 35356, "start": 367.4, "end": 371.8, "text": " reported problem of the magic modulators. So let's keep those two distinct from each other.", "tokens": [51056, 7055, 1154, 295, 264, 5585, 1072, 39265, 13, 407, 718, 311, 1066, 729, 732, 10644, 490, 1184, 661, 13, 51276], "temperature": 0.0, "avg_logprob": -0.11358879185930083, "compression_ratio": 1.560747663551402, "no_speech_prob": 0.005712064914405346}, {"id": 66, "seek": 35356, "start": 371.8, "end": 377.4, "text": " But the magic module is, well, I have a relevance realization thing in my head that just does it.", "tokens": [51276, 583, 264, 5585, 10088, 307, 11, 731, 11, 286, 362, 257, 32684, 25138, 551, 294, 452, 1378, 300, 445, 775, 309, 13, 51556], "temperature": 0.0, "avg_logprob": -0.11358879185930083, "compression_ratio": 1.560747663551402, "no_speech_prob": 0.005712064914405346}, {"id": 67, "seek": 37740, "start": 377.4, "end": 384.03999999999996, "text": " And then that just, and then I can just say, well, how does it do it? Then the mistake people can", "tokens": [50364, 400, 550, 300, 445, 11, 293, 550, 286, 393, 445, 584, 11, 731, 11, 577, 775, 309, 360, 309, 30, 1396, 264, 6146, 561, 393, 50696], "temperature": 0.0, "avg_logprob": -0.0856783331894293, "compression_ratio": 1.9487179487179487, "no_speech_prob": 0.008314601145684719}, {"id": 68, "seek": 37740, "start": 384.03999999999996, "end": 389.32, "text": " make is, well, evolution made it. And that's right. Evolution tells me how it got here.", "tokens": [50696, 652, 307, 11, 731, 11, 9303, 1027, 309, 13, 400, 300, 311, 558, 13, 40800, 5112, 385, 577, 309, 658, 510, 13, 50960], "temperature": 0.0, "avg_logprob": -0.0856783331894293, "compression_ratio": 1.9487179487179487, "no_speech_prob": 0.008314601145684719}, {"id": 69, "seek": 37740, "start": 389.32, "end": 393.71999999999997, "text": " That's not telling me how it functions. Evolution tells me how my eye got here and", "tokens": [50960, 663, 311, 406, 3585, 385, 577, 309, 6828, 13, 40800, 5112, 385, 577, 452, 3313, 658, 510, 293, 51180], "temperature": 0.0, "avg_logprob": -0.0856783331894293, "compression_ratio": 1.9487179487179487, "no_speech_prob": 0.008314601145684719}, {"id": 70, "seek": 37740, "start": 393.71999999999997, "end": 398.76, "text": " how my ear got here. And that's not telling me how my eye and my ear function. I'm trying to", "tokens": [51180, 577, 452, 1273, 658, 510, 13, 400, 300, 311, 406, 3585, 385, 577, 452, 3313, 293, 452, 1273, 2445, 13, 286, 478, 1382, 281, 51432], "temperature": 0.0, "avg_logprob": -0.0856783331894293, "compression_ratio": 1.9487179487179487, "no_speech_prob": 0.008314601145684719}, {"id": 71, "seek": 37740, "start": 398.76, "end": 403.64, "text": " take what's called the design stance. How could, tell me what I need to do to give that module", "tokens": [51432, 747, 437, 311, 1219, 264, 1715, 21033, 13, 1012, 727, 11, 980, 385, 437, 286, 643, 281, 360, 281, 976, 300, 10088, 51676], "temperature": 0.0, "avg_logprob": -0.0856783331894293, "compression_ratio": 1.9487179487179487, "no_speech_prob": 0.008314601145684719}, {"id": 72, "seek": 40364, "start": 403.64, "end": 408.91999999999996, "text": " the ability to realize relevance? All you've done is said it has that ability,", "tokens": [50364, 264, 3485, 281, 4325, 32684, 30, 1057, 291, 600, 1096, 307, 848, 309, 575, 300, 3485, 11, 50628], "temperature": 0.0, "avg_logprob": -0.09757628970675998, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.00912258867174387}, {"id": 73, "seek": 40364, "start": 408.91999999999996, "end": 413.96, "text": " but it faces the problem of, okay, now it has to know what to pay attention to,", "tokens": [50628, 457, 309, 8475, 264, 1154, 295, 11, 1392, 11, 586, 309, 575, 281, 458, 437, 281, 1689, 3202, 281, 11, 50880], "temperature": 0.0, "avg_logprob": -0.09757628970675998, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.00912258867174387}, {"id": 74, "seek": 40364, "start": 414.84, "end": 419.32, "text": " and so on and so forth. Now, what you might say is, well, evolution totally prepared us for it.", "tokens": [50924, 293, 370, 322, 293, 370, 5220, 13, 823, 11, 437, 291, 1062, 584, 307, 11, 731, 11, 9303, 3879, 4927, 505, 337, 309, 13, 51148], "temperature": 0.0, "avg_logprob": -0.09757628970675998, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.00912258867174387}, {"id": 75, "seek": 40364, "start": 419.32, "end": 425.0, "text": " The problem with that is that doesn't work. Evolution can only pick up on things that are", "tokens": [51148, 440, 1154, 365, 300, 307, 300, 1177, 380, 589, 13, 40800, 393, 787, 1888, 493, 322, 721, 300, 366, 51432], "temperature": 0.0, "avg_logprob": -0.09757628970675998, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.00912258867174387}, {"id": 76, "seek": 42500, "start": 425.0, "end": 433.32, "text": " long-term invariant and that make an ongoing continuous difference to your reproductive", "tokens": [50364, 938, 12, 7039, 33270, 394, 293, 300, 652, 364, 10452, 10957, 2649, 281, 428, 33569, 50780], "temperature": 0.0, "avg_logprob": -0.09801961725408381, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.26881420612335205}, {"id": 77, "seek": 42500, "start": 433.32, "end": 440.6, "text": " status. And that's not how relevance works. Relevance is really fast and changing. If I say", "tokens": [50780, 6558, 13, 400, 300, 311, 406, 577, 32684, 1985, 13, 1300, 28316, 719, 307, 534, 2370, 293, 4473, 13, 759, 286, 584, 51144], "temperature": 0.0, "avg_logprob": -0.09801961725408381, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.26881420612335205}, {"id": 78, "seek": 42500, "start": 440.6, "end": 445.0, "text": " left big toe, it suddenly becomes relevant to you, and it wasn't relevant a minute ago,", "tokens": [51144, 1411, 955, 13976, 11, 309, 5800, 3643, 7340, 281, 291, 11, 293, 309, 2067, 380, 7340, 257, 3456, 2057, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09801961725408381, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.26881420612335205}, {"id": 79, "seek": 42500, "start": 445.0, "end": 449.64, "text": " and what's the Darwinian difference there? And there's nothing that's intrinsically relevant.", "tokens": [51364, 293, 437, 311, 264, 30233, 952, 2649, 456, 30, 400, 456, 311, 1825, 300, 311, 28621, 984, 7340, 13, 51596], "temperature": 0.0, "avg_logprob": -0.09801961725408381, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.26881420612335205}, {"id": 80, "seek": 42500, "start": 449.64, "end": 454.04, "text": " Well, my own life is intrinsically relevant, is it? You'll sacrifice your life for your kids.", "tokens": [51596, 1042, 11, 452, 1065, 993, 307, 28621, 984, 7340, 11, 307, 309, 30, 509, 603, 11521, 428, 993, 337, 428, 2301, 13, 51816], "temperature": 0.0, "avg_logprob": -0.09801961725408381, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.26881420612335205}, {"id": 81, "seek": 45404, "start": 454.04, "end": 459.48, "text": " Well, my life and my kids, under all circumstances, saving your kid", "tokens": [50364, 1042, 11, 452, 993, 293, 452, 2301, 11, 833, 439, 9121, 11, 6816, 428, 1636, 50636], "temperature": 0.0, "avg_logprob": -0.10510209473696622, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0014547165483236313}, {"id": 82, "seek": 45404, "start": 459.48, "end": 463.8, "text": " means that 10 million people, oh, well, and so on, and we get into all these philosophical", "tokens": [50636, 1355, 300, 1266, 2459, 561, 11, 1954, 11, 731, 11, 293, 370, 322, 11, 293, 321, 483, 666, 439, 613, 25066, 50852], "temperature": 0.0, "avg_logprob": -0.10510209473696622, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0014547165483236313}, {"id": 83, "seek": 45404, "start": 463.8, "end": 471.64000000000004, "text": " arguments because we realize, no, no, there isn't any hard, fast thing that is always relevant.", "tokens": [50852, 12869, 570, 321, 4325, 11, 572, 11, 572, 11, 456, 1943, 380, 604, 1152, 11, 2370, 551, 300, 307, 1009, 7340, 13, 51244], "temperature": 0.0, "avg_logprob": -0.10510209473696622, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0014547165483236313}, {"id": 84, "seek": 45404, "start": 471.64000000000004, "end": 477.56, "text": " Relevance is not something for which we can generate a scientific theory. It's not stable.", "tokens": [51244, 1300, 28316, 719, 307, 406, 746, 337, 597, 321, 393, 8460, 257, 8134, 5261, 13, 467, 311, 406, 8351, 13, 51540], "temperature": 0.0, "avg_logprob": -0.10510209473696622, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0014547165483236313}, {"id": 85, "seek": 47756, "start": 477.56, "end": 483.96, "text": " It's not intrinsic to the phenomenon. And there's nothing, other than being relevant,", "tokens": [50364, 467, 311, 406, 35698, 281, 264, 14029, 13, 400, 456, 311, 1825, 11, 661, 813, 885, 7340, 11, 50684], "temperature": 0.0, "avg_logprob": -0.13533463729055303, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0350382961332798}, {"id": 86, "seek": 47756, "start": 483.96, "end": 490.44, "text": " there's nothing that all the events or objects or feelings have in common. What is it that,", "tokens": [50684, 456, 311, 1825, 300, 439, 264, 3931, 420, 6565, 420, 6640, 362, 294, 2689, 13, 708, 307, 309, 300, 11, 51008], "temperature": 0.0, "avg_logprob": -0.13533463729055303, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0350382961332798}, {"id": 87, "seek": 47756, "start": 490.44, "end": 494.44, "text": " what do they all have that makes them, other than saying, well, you'll give me synonyms,", "tokens": [51008, 437, 360, 436, 439, 362, 300, 1669, 552, 11, 661, 813, 1566, 11, 731, 11, 291, 603, 976, 385, 5451, 2526, 2592, 11, 51208], "temperature": 0.0, "avg_logprob": -0.13533463729055303, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0350382961332798}, {"id": 88, "seek": 47756, "start": 494.44, "end": 499.56, "text": " they're important to me, or they help me solve my problems? Yes, that's exactly it. But how?", "tokens": [51208, 436, 434, 1021, 281, 385, 11, 420, 436, 854, 385, 5039, 452, 2740, 30, 1079, 11, 300, 311, 2293, 309, 13, 583, 577, 30, 51464], "temperature": 0.0, "avg_logprob": -0.13533463729055303, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0350382961332798}, {"id": 89, "seek": 49956, "start": 500.04, "end": 510.52, "text": " Exactly. Perfect. And it's so nice to see how the apparently disparate strands of cognitive", "tokens": [50388, 7587, 13, 10246, 13, 400, 309, 311, 370, 1481, 281, 536, 577, 264, 7970, 14548, 473, 29664, 295, 15605, 50912], "temperature": 0.0, "avg_logprob": -0.23471011613544665, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.025450222194194794}, {"id": 90, "seek": 49956, "start": 510.52, "end": 515.56, "text": " science, so Gibsonian affordances for a cognitive science, predictive processing and", "tokens": [50912, 3497, 11, 370, 42250, 952, 6157, 2676, 337, 257, 15605, 3497, 11, 35521, 9007, 293, 51164], "temperature": 0.0, "avg_logprob": -0.23471011613544665, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.025450222194194794}, {"id": 91, "seek": 49956, "start": 515.56, "end": 521.16, "text": " processing and active inference, align at this path, which is that everything is dynamical.", "tokens": [51164, 9007, 293, 4967, 38253, 11, 7975, 412, 341, 3100, 11, 597, 307, 300, 1203, 307, 5999, 804, 13, 51444], "temperature": 0.0, "avg_logprob": -0.23471011613544665, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.025450222194194794}, {"id": 92, "seek": 49956, "start": 521.16, "end": 526.76, "text": " Everything is about this mutual unfolding. There is no rarefied static", "tokens": [51444, 5471, 307, 466, 341, 16917, 44586, 13, 821, 307, 572, 5892, 69, 1091, 13437, 51724], "temperature": 0.0, "avg_logprob": -0.23471011613544665, "compression_ratio": 1.6536585365853658, "no_speech_prob": 0.025450222194194794}, {"id": 93, "seek": 52676, "start": 527.48, "end": 532.84, "text": " existence. It's what's relevant to you might not be relevant to me, and there are these dynamic", "tokens": [50400, 9123, 13, 467, 311, 437, 311, 7340, 281, 291, 1062, 406, 312, 7340, 281, 385, 11, 293, 456, 366, 613, 8546, 50668], "temperature": 0.0, "avg_logprob": -0.12807449427517978, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.01164554338902235}, {"id": 94, "seek": 52676, "start": 532.84, "end": 539.64, "text": " relationships which govern that. Yeah, so that's a great place to start. I think", "tokens": [50668, 6159, 597, 1980, 300, 13, 865, 11, 370, 300, 311, 257, 869, 1081, 281, 722, 13, 286, 519, 51008], "temperature": 0.0, "avg_logprob": -0.12807449427517978, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.01164554338902235}, {"id": 95, "seek": 52676, "start": 540.36, "end": 546.4399999999999, "text": " let's now go into the solution, if there is such a solution. And let's start with", "tokens": [51044, 718, 311, 586, 352, 666, 264, 3827, 11, 498, 456, 307, 1270, 257, 3827, 13, 400, 718, 311, 722, 365, 51348], "temperature": 0.0, "avg_logprob": -0.12807449427517978, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.01164554338902235}, {"id": 96, "seek": 52676, "start": 546.4399999999999, "end": 554.6, "text": " opponent processing. So this is a term that you use frequently. To a layman audience,", "tokens": [51348, 10620, 9007, 13, 407, 341, 307, 257, 1433, 300, 291, 764, 10374, 13, 1407, 257, 2360, 1601, 4034, 11, 51756], "temperature": 0.0, "avg_logprob": -0.12807449427517978, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.01164554338902235}, {"id": 97, "seek": 55460, "start": 554.6800000000001, "end": 559.08, "text": " it's not clear exactly what it means. So what is opponent processing, and how does it help", "tokens": [50368, 309, 311, 406, 1850, 2293, 437, 309, 1355, 13, 407, 437, 307, 10620, 9007, 11, 293, 577, 775, 309, 854, 50588], "temperature": 0.0, "avg_logprob": -0.15365062139730537, "compression_ratio": 1.6044776119402986, "no_speech_prob": 0.002394575858488679}, {"id": 98, "seek": 55460, "start": 559.08, "end": 565.16, "text": " shed light on this complex problem? Sure. And maybe along the way, I can point out how", "tokens": [50588, 14951, 1442, 322, 341, 3997, 1154, 30, 4894, 13, 400, 1310, 2051, 264, 636, 11, 286, 393, 935, 484, 577, 50892], "temperature": 0.0, "avg_logprob": -0.15365062139730537, "compression_ratio": 1.6044776119402986, "no_speech_prob": 0.002394575858488679}, {"id": 99, "seek": 55460, "start": 565.88, "end": 571.8000000000001, "text": " relevance realization really gives some specific teeth to claims of embodiment from 4-Ecox-I.", "tokens": [50928, 32684, 25138, 534, 2709, 512, 2685, 7798, 281, 9441, 295, 28935, 2328, 490, 1017, 12, 36, 1291, 87, 12, 40, 13, 51224], "temperature": 0.0, "avg_logprob": -0.15365062139730537, "compression_ratio": 1.6044776119402986, "no_speech_prob": 0.002394575858488679}, {"id": 100, "seek": 55460, "start": 572.36, "end": 576.0400000000001, "text": " Because I'd like the point you made, and maybe we don't have to do it right now,", "tokens": [51252, 1436, 286, 1116, 411, 264, 935, 291, 1027, 11, 293, 1310, 321, 500, 380, 362, 281, 360, 309, 558, 586, 11, 51436], "temperature": 0.0, "avg_logprob": -0.15365062139730537, "compression_ratio": 1.6044776119402986, "no_speech_prob": 0.002394575858488679}, {"id": 101, "seek": 55460, "start": 576.0400000000001, "end": 579.72, "text": " but I'd like to come back to it, how relevance realization is kind of a glue,", "tokens": [51436, 457, 286, 1116, 411, 281, 808, 646, 281, 309, 11, 577, 32684, 25138, 307, 733, 295, 257, 8998, 11, 51620], "temperature": 0.0, "avg_logprob": -0.15365062139730537, "compression_ratio": 1.6044776119402986, "no_speech_prob": 0.002394575858488679}, {"id": 102, "seek": 57972, "start": 579.72, "end": 586.0400000000001, "text": " they glue together predictive processing and 4-Ecox-I in a powerful way and makes them all,", "tokens": [50364, 436, 8998, 1214, 35521, 9007, 293, 1017, 12, 36, 1291, 87, 12, 40, 294, 257, 4005, 636, 293, 1669, 552, 439, 11, 50680], "temperature": 0.0, "avg_logprob": -0.10266799018496559, "compression_ratio": 1.583629893238434, "no_speech_prob": 0.007342434488236904}, {"id": 103, "seek": 57972, "start": 586.0400000000001, "end": 591.48, "text": " including itself, mutually stronger from that integration. But yeah, the opponent processing,", "tokens": [50680, 3009, 2564, 11, 39144, 7249, 490, 300, 10980, 13, 583, 1338, 11, 264, 10620, 9007, 11, 50952], "temperature": 0.0, "avg_logprob": -0.10266799018496559, "compression_ratio": 1.583629893238434, "no_speech_prob": 0.007342434488236904}, {"id": 104, "seek": 57972, "start": 591.48, "end": 597.24, "text": " so let's just note something. At many different levels of analysis in your biology,", "tokens": [50952, 370, 718, 311, 445, 3637, 746, 13, 1711, 867, 819, 4358, 295, 5215, 294, 428, 14956, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10266799018496559, "compression_ratio": 1.583629893238434, "no_speech_prob": 0.007342434488236904}, {"id": 105, "seek": 57972, "start": 597.24, "end": 602.12, "text": " you will find opponent processing at work. I'll take one that's very easy to explain,", "tokens": [51240, 291, 486, 915, 10620, 9007, 412, 589, 13, 286, 603, 747, 472, 300, 311, 588, 1858, 281, 2903, 11, 51484], "temperature": 0.0, "avg_logprob": -0.10266799018496559, "compression_ratio": 1.583629893238434, "no_speech_prob": 0.007342434488236904}, {"id": 106, "seek": 57972, "start": 602.12, "end": 609.1600000000001, "text": " and people have a ready experience of it. So we are all constantly, mostly unconsciously,", "tokens": [51484, 293, 561, 362, 257, 1919, 1752, 295, 309, 13, 407, 321, 366, 439, 6460, 11, 5240, 18900, 356, 11, 51836], "temperature": 0.0, "avg_logprob": -0.10266799018496559, "compression_ratio": 1.583629893238434, "no_speech_prob": 0.007342434488236904}, {"id": 107, "seek": 60916, "start": 609.24, "end": 613.56, "text": " although it's affected by conscious factors, we're constantly recalibrating and adjusting,", "tokens": [50368, 4878, 309, 311, 8028, 538, 6648, 6771, 11, 321, 434, 6460, 850, 304, 6414, 990, 293, 23559, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10866791432298074, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0015004859305918217}, {"id": 108, "seek": 60916, "start": 614.52, "end": 619.4, "text": " dare I say it, we're even evolving our level of metabolic arousal. I don't mean just sexual", "tokens": [50632, 8955, 286, 584, 309, 11, 321, 434, 754, 21085, 527, 1496, 295, 36464, 594, 563, 304, 13, 286, 500, 380, 914, 445, 6701, 50876], "temperature": 0.0, "avg_logprob": -0.10866791432298074, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0015004859305918217}, {"id": 109, "seek": 60916, "start": 619.4, "end": 624.6, "text": " arousal, don't be Freud here, I mean arousal. How much, how much, how sort of activated are you,", "tokens": [50876, 594, 563, 304, 11, 500, 380, 312, 41590, 510, 11, 286, 914, 594, 563, 304, 13, 1012, 709, 11, 577, 709, 11, 577, 1333, 295, 18157, 366, 291, 11, 51136], "temperature": 0.0, "avg_logprob": -0.10866791432298074, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0015004859305918217}, {"id": 110, "seek": 60916, "start": 624.6, "end": 630.36, "text": " how sort of energized are you? And the problem is there isn't, there isn't some state,", "tokens": [51136, 577, 1333, 295, 49231, 366, 291, 30, 400, 264, 1154, 307, 456, 1943, 380, 11, 456, 1943, 380, 512, 1785, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10866791432298074, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0015004859305918217}, {"id": 111, "seek": 60916, "start": 631.0, "end": 635.8, "text": " homeostatic state you're looking for with that, because if there's a tiger in the room, I need to", "tokens": [51456, 1280, 555, 2399, 1785, 291, 434, 1237, 337, 365, 300, 11, 570, 498, 456, 311, 257, 21432, 294, 264, 1808, 11, 286, 643, 281, 51696], "temperature": 0.0, "avg_logprob": -0.10866791432298074, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.0015004859305918217}, {"id": 112, "seek": 63580, "start": 636.28, "end": 643.9599999999999, "text": " maximal arousal. And if I'm going to sleep, I need to go to minimal arousal, and I can't just be", "tokens": [50388, 49336, 594, 563, 304, 13, 400, 498, 286, 478, 516, 281, 2817, 11, 286, 643, 281, 352, 281, 13206, 594, 563, 304, 11, 293, 286, 393, 380, 445, 312, 50772], "temperature": 0.0, "avg_logprob": -0.11022668985220102, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.01638844609260559}, {"id": 113, "seek": 63580, "start": 643.9599999999999, "end": 648.3599999999999, "text": " sort of Canadian and keep sort of average arousal at all times, because then I don't fall asleep", "tokens": [50772, 1333, 295, 12641, 293, 1066, 1333, 295, 4274, 594, 563, 304, 412, 439, 1413, 11, 570, 550, 286, 500, 380, 2100, 11039, 50992], "temperature": 0.0, "avg_logprob": -0.11022668985220102, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.01638844609260559}, {"id": 114, "seek": 63580, "start": 648.3599999999999, "end": 654.12, "text": " and I get killed by tigers. And so it constantly has to be, like we're talking about, it has to", "tokens": [50992, 293, 286, 483, 4652, 538, 47949, 13, 400, 370, 309, 6460, 575, 281, 312, 11, 411, 321, 434, 1417, 466, 11, 309, 575, 281, 51280], "temperature": 0.0, "avg_logprob": -0.11022668985220102, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.01638844609260559}, {"id": 115, "seek": 63580, "start": 654.12, "end": 660.12, "text": " constantly be adjusting. So what has evolved in your biology is your autonomic, meaning", "tokens": [51280, 6460, 312, 23559, 13, 407, 437, 575, 14178, 294, 428, 14956, 307, 428, 18203, 299, 11, 3620, 51580], "temperature": 0.0, "avg_logprob": -0.11022668985220102, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.01638844609260559}, {"id": 116, "seek": 63580, "start": 660.12, "end": 665.4799999999999, "text": " self-governing, it's a self-organizing system. Like you said, it's a dynamical entity.", "tokens": [51580, 2698, 12, 1571, 331, 773, 11, 309, 311, 257, 2698, 12, 12372, 3319, 1185, 13, 1743, 291, 848, 11, 309, 311, 257, 5999, 804, 13977, 13, 51848], "temperature": 0.0, "avg_logprob": -0.11022668985220102, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.01638844609260559}, {"id": 117, "seek": 66548, "start": 665.48, "end": 672.28, "text": " Your autonomic nervous system, and what it does is it couples together two subsystems that are,", "tokens": [50364, 2260, 18203, 299, 6296, 1185, 11, 293, 437, 309, 775, 307, 309, 20368, 1214, 732, 2090, 9321, 82, 300, 366, 11, 50704], "temperature": 0.0, "avg_logprob": -0.10911260951649059, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.0013244732981547713}, {"id": 118, "seek": 66548, "start": 672.28, "end": 678.9200000000001, "text": " have opposite biases from each other. So your sympathetic system is biased to, I'll speak", "tokens": [50704, 362, 6182, 32152, 490, 1184, 661, 13, 407, 428, 36032, 1185, 307, 28035, 281, 11, 286, 603, 1710, 51036], "temperature": 0.0, "avg_logprob": -0.10911260951649059, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.0013244732981547713}, {"id": 119, "seek": 66548, "start": 678.9200000000001, "end": 683.88, "text": " anthropomorphically, just because it speeds things up. Your sympathetic system is biased to seeing", "tokens": [51036, 22727, 32702, 984, 11, 445, 570, 309, 16411, 721, 493, 13, 2260, 36032, 1185, 307, 28035, 281, 2577, 51284], "temperature": 0.0, "avg_logprob": -0.10911260951649059, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.0013244732981547713}, {"id": 120, "seek": 66548, "start": 683.88, "end": 688.12, "text": " as much of it can of the world as threat or opportunity and arousing you as much as it can.", "tokens": [51284, 382, 709, 295, 309, 393, 295, 264, 1002, 382, 4734, 420, 2650, 293, 594, 24220, 291, 382, 709, 382, 309, 393, 13, 51496], "temperature": 0.0, "avg_logprob": -0.10911260951649059, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.0013244732981547713}, {"id": 121, "seek": 66548, "start": 689.16, "end": 693.24, "text": " And your parasympathetic system is biased the opposite way. It's biased to seeing", "tokens": [51548, 400, 428, 971, 5871, 2455, 998, 3532, 1185, 307, 28035, 264, 6182, 636, 13, 467, 311, 28035, 281, 2577, 51752], "temperature": 0.0, "avg_logprob": -0.10911260951649059, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.0013244732981547713}, {"id": 122, "seek": 69324, "start": 693.24, "end": 698.84, "text": " as much of the world as secure, safe, a place where you can rest and recover. And then these", "tokens": [50364, 382, 709, 295, 264, 1002, 382, 7144, 11, 3273, 11, 257, 1081, 689, 291, 393, 1472, 293, 8114, 13, 400, 550, 613, 50644], "temperature": 0.0, "avg_logprob": -0.12635082668728298, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.005218145903199911}, {"id": 123, "seek": 69324, "start": 698.84, "end": 704.84, "text": " subsystems are not independent from each other. They're locked together, and they're also, they're", "tokens": [50644, 2090, 9321, 82, 366, 406, 6695, 490, 1184, 661, 13, 814, 434, 9376, 1214, 11, 293, 436, 434, 611, 11, 436, 434, 50944], "temperature": 0.0, "avg_logprob": -0.12635082668728298, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.005218145903199911}, {"id": 124, "seek": 69324, "start": 704.84, "end": 710.6, "text": " continually trying to shut each other off. And they're constantly competing that way,", "tokens": [50944, 22277, 1382, 281, 5309, 1184, 661, 766, 13, 400, 436, 434, 6460, 15439, 300, 636, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12635082668728298, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.005218145903199911}, {"id": 125, "seek": 69324, "start": 710.6, "end": 715.5600000000001, "text": " but they're cooperatively competing. It's not adversarial, they're cooperatively competing.", "tokens": [51232, 457, 436, 434, 13414, 19020, 15439, 13, 467, 311, 406, 17641, 44745, 11, 436, 434, 13414, 19020, 15439, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12635082668728298, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.005218145903199911}, {"id": 126, "seek": 69324, "start": 715.5600000000001, "end": 721.88, "text": " And what happens is the constant trade-off between those processes, between those processes,", "tokens": [51480, 400, 437, 2314, 307, 264, 5754, 4923, 12, 4506, 1296, 729, 7555, 11, 1296, 729, 7555, 11, 51796], "temperature": 0.0, "avg_logprob": -0.12635082668728298, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.005218145903199911}, {"id": 127, "seek": 72188, "start": 721.88, "end": 730.4399999999999, "text": " constantly, right, in very dynamic manner, constantly recalibrate to your level of arousal", "tokens": [50364, 6460, 11, 558, 11, 294, 588, 8546, 9060, 11, 6460, 850, 304, 897, 4404, 281, 428, 1496, 295, 594, 563, 304, 50792], "temperature": 0.0, "avg_logprob": -0.1254668396510435, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0009396448731422424}, {"id": 128, "seek": 72188, "start": 730.4399999999999, "end": 734.52, "text": " to the world. And that's opponent processing. And what you find is you got that, you know,", "tokens": [50792, 281, 264, 1002, 13, 400, 300, 311, 10620, 9007, 13, 400, 437, 291, 915, 307, 291, 658, 300, 11, 291, 458, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1254668396510435, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0009396448731422424}, {"id": 129, "seek": 72188, "start": 734.52, "end": 738.4399999999999, "text": " there's other, there's opponent processing between your focal vision and your peripheral vision.", "tokens": [50996, 456, 311, 661, 11, 456, 311, 10620, 9007, 1296, 428, 26592, 5201, 293, 428, 40235, 5201, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1254668396510435, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0009396448731422424}, {"id": 130, "seek": 72188, "start": 738.4399999999999, "end": 744.12, "text": " There's opponent processing plausibly between your left and right hemispheres. The number of these", "tokens": [51192, 821, 311, 10620, 9007, 34946, 3545, 1296, 428, 1411, 293, 558, 8636, 7631, 19464, 13, 440, 1230, 295, 613, 51476], "temperature": 0.0, "avg_logprob": -0.1254668396510435, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.0009396448731422424}, {"id": 131, "seek": 74412, "start": 744.12, "end": 750.92, "text": " things, right, it is huge. And I say at many different levels of analysis. And so", "tokens": [50364, 721, 11, 558, 11, 309, 307, 2603, 13, 400, 286, 584, 412, 867, 819, 4358, 295, 5215, 13, 400, 370, 50704], "temperature": 0.0, "avg_logprob": -0.14572645936693465, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.025809217244386673}, {"id": 132, "seek": 74412, "start": 752.28, "end": 761.0, "text": " I think opponent processing is a clue to how relevance realization is actually undertaken", "tokens": [50772, 286, 519, 10620, 9007, 307, 257, 13602, 281, 577, 32684, 25138, 307, 767, 40313, 51208], "temperature": 0.0, "avg_logprob": -0.14572645936693465, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.025809217244386673}, {"id": 133, "seek": 74412, "start": 761.0, "end": 769.4, "text": " by your embodied cognition. Wonderful. And immediately to anyone interested in active", "tokens": [51208, 538, 428, 42046, 46905, 13, 22768, 13, 400, 4258, 281, 2878, 3102, 294, 4967, 51628], "temperature": 0.0, "avg_logprob": -0.14572645936693465, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.025809217244386673}, {"id": 134, "seek": 76940, "start": 769.4, "end": 775.16, "text": " inference or its manifestation in continuous state spaces as predictive processing,", "tokens": [50364, 38253, 420, 1080, 29550, 294, 10957, 1785, 7673, 382, 35521, 9007, 11, 50652], "temperature": 0.0, "avg_logprob": -0.15629317842680832, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.01168511901050806}, {"id": 135, "seek": 76940, "start": 776.1999999999999, "end": 780.84, "text": " what will be screaming out is this notion of an attractor set or our homeostatic equilibrium,", "tokens": [50704, 437, 486, 312, 12636, 484, 307, 341, 10710, 295, 364, 5049, 284, 992, 420, 527, 1280, 555, 2399, 15625, 11, 50936], "temperature": 0.0, "avg_logprob": -0.15629317842680832, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.01168511901050806}, {"id": 136, "seek": 76940, "start": 780.84, "end": 787.0799999999999, "text": " which Karl has spoken about at length, obviously, and has had many other researchers. But this idea", "tokens": [50936, 597, 20405, 575, 10759, 466, 412, 4641, 11, 2745, 11, 293, 575, 632, 867, 661, 10309, 13, 583, 341, 1558, 51248], "temperature": 0.0, "avg_logprob": -0.15629317842680832, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.01168511901050806}, {"id": 137, "seek": 76940, "start": 787.0799999999999, "end": 796.68, "text": " that what this, we are, we suffer, but we also embody in a kind of positive way this itinerancy.", "tokens": [51248, 300, 437, 341, 11, 321, 366, 11, 321, 9753, 11, 457, 321, 611, 42575, 294, 257, 733, 295, 3353, 636, 341, 309, 4564, 6717, 13, 51728], "temperature": 0.0, "avg_logprob": -0.15629317842680832, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.01168511901050806}, {"id": 138, "seek": 79668, "start": 796.68, "end": 804.3599999999999, "text": " We never, we're never stuck in a single mode. We are endowed with the capacity to", "tokens": [50364, 492, 1128, 11, 321, 434, 1128, 5541, 294, 257, 2167, 4391, 13, 492, 366, 917, 24347, 365, 264, 6042, 281, 50748], "temperature": 0.0, "avg_logprob": -0.13183356415141712, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0006055703270249069}, {"id": 139, "seek": 79668, "start": 804.3599999999999, "end": 809.56, "text": " go beyond our homeostasis and return back to it. But we're always, in a sense, being drawn", "tokens": [50748, 352, 4399, 527, 1280, 555, 26632, 293, 2736, 646, 281, 309, 13, 583, 321, 434, 1009, 11, 294, 257, 2020, 11, 885, 10117, 51008], "temperature": 0.0, "avg_logprob": -0.13183356415141712, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0006055703270249069}, {"id": 140, "seek": 79668, "start": 809.56, "end": 814.92, "text": " without ever residing, without ever having stopped at that homeostatic set point.", "tokens": [51008, 1553, 1562, 725, 2819, 11, 1553, 1562, 1419, 5936, 412, 300, 1280, 555, 2399, 992, 935, 13, 51276], "temperature": 0.0, "avg_logprob": -0.13183356415141712, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0006055703270249069}, {"id": 141, "seek": 79668, "start": 816.04, "end": 823.0, "text": " So let's now fold in active inference to the picture. What does it provide relevance realization", "tokens": [51332, 407, 718, 311, 586, 4860, 294, 4967, 38253, 281, 264, 3036, 13, 708, 775, 309, 2893, 32684, 25138, 51680], "temperature": 0.0, "avg_logprob": -0.13183356415141712, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0006055703270249069}, {"id": 142, "seek": 82300, "start": 823.4, "end": 829.32, "text": " and the formulation of relevance realization that couldn't be done purely with Fourier cognitive", "tokens": [50384, 293, 264, 37642, 295, 32684, 25138, 300, 2809, 380, 312, 1096, 17491, 365, 36810, 15605, 50680], "temperature": 0.0, "avg_logprob": -0.1329007943471273, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0015470072394236922}, {"id": 143, "seek": 82300, "start": 829.32, "end": 835.88, "text": " science? So, I mean, I'm going to talk about it in terms of predictive processing, because that's", "tokens": [50680, 3497, 30, 407, 11, 286, 914, 11, 286, 478, 516, 281, 751, 466, 309, 294, 2115, 295, 35521, 9007, 11, 570, 300, 311, 51008], "temperature": 0.0, "avg_logprob": -0.1329007943471273, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0015470072394236922}, {"id": 144, "seek": 82300, "start": 835.88, "end": 842.92, "text": " the one, that's the formulation of it, that lines up most cleanly with the theoretical", "tokens": [51008, 264, 472, 11, 300, 311, 264, 37642, 295, 309, 11, 300, 3876, 493, 881, 2541, 356, 365, 264, 20864, 51360], "temperature": 0.0, "avg_logprob": -0.1329007943471273, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0015470072394236922}, {"id": 145, "seek": 82300, "start": 842.92, "end": 847.56, "text": " integration. As you said, there's there's clear derivation relations between active inference", "tokens": [51360, 10980, 13, 1018, 291, 848, 11, 456, 311, 456, 311, 1850, 10151, 399, 2299, 1296, 4967, 38253, 51592], "temperature": 0.0, "avg_logprob": -0.1329007943471273, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0015470072394236922}, {"id": 146, "seek": 84756, "start": 847.56, "end": 853.0, "text": " and predictive processing, and also between them and the Bayesian math, but the Bayesian math, if", "tokens": [50364, 293, 35521, 9007, 11, 293, 611, 1296, 552, 293, 264, 7840, 42434, 5221, 11, 457, 264, 7840, 42434, 5221, 11, 498, 50636], "temperature": 0.0, "avg_logprob": -0.131285265872353, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0037058743182569742}, {"id": 147, "seek": 84756, "start": 853.0, "end": 856.76, "text": " you were to actually strictly apply it would be computationally intractable. So we're doing some", "tokens": [50636, 291, 645, 281, 767, 20792, 3079, 309, 576, 312, 24903, 379, 560, 1897, 712, 13, 407, 321, 434, 884, 512, 50824], "temperature": 0.0, "avg_logprob": -0.131285265872353, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0037058743182569742}, {"id": 148, "seek": 84756, "start": 856.76, "end": 861.0799999999999, "text": " approximation function. So I'm just going to take it that everybody sort of shall knows that I'm", "tokens": [50824, 28023, 2445, 13, 407, 286, 478, 445, 516, 281, 747, 309, 300, 2201, 1333, 295, 4393, 3255, 300, 286, 478, 51040], "temperature": 0.0, "avg_logprob": -0.131285265872353, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0037058743182569742}, {"id": 149, "seek": 84756, "start": 861.0799999999999, "end": 869.0799999999999, "text": " playing fair with this, right? I'm not, I'm not trying to be dodgy. And so, relevance realization", "tokens": [51040, 2433, 3143, 365, 341, 11, 558, 30, 286, 478, 406, 11, 286, 478, 406, 1382, 281, 312, 13886, 1480, 13, 400, 370, 11, 32684, 25138, 51440], "temperature": 0.0, "avg_logprob": -0.131285265872353, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0037058743182569742}, {"id": 150, "seek": 84756, "start": 869.0799999999999, "end": 876.5999999999999, "text": " is grounded in the idea of problem solving. And problem solving actually assumes a fundamental", "tokens": [51440, 307, 23535, 294, 264, 1558, 295, 1154, 12606, 13, 400, 1154, 12606, 767, 37808, 257, 8088, 51816], "temperature": 0.0, "avg_logprob": -0.131285265872353, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0037058743182569742}, {"id": 151, "seek": 87660, "start": 876.6, "end": 881.72, "text": " thing, which again is so obvious to us, which is the states you're in and the states you want to", "tokens": [50364, 551, 11, 597, 797, 307, 370, 6322, 281, 505, 11, 597, 307, 264, 4368, 291, 434, 294, 293, 264, 4368, 291, 528, 281, 50620], "temperature": 0.0, "avg_logprob": -0.09706846872965495, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.008979475125670433}, {"id": 152, "seek": 87660, "start": 881.72, "end": 886.0400000000001, "text": " be in are not the same state. That's the defining feature of a problem. If I'm in a state I want", "tokens": [50620, 312, 294, 366, 406, 264, 912, 1785, 13, 663, 311, 264, 17827, 4111, 295, 257, 1154, 13, 759, 286, 478, 294, 257, 1785, 286, 528, 50836], "temperature": 0.0, "avg_logprob": -0.09706846872965495, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.008979475125670433}, {"id": 153, "seek": 87660, "start": 886.0400000000001, "end": 891.8000000000001, "text": " to be in, I want to be sitting in this chair. I am. I don't have a problem, right? And now,", "tokens": [50836, 281, 312, 294, 11, 286, 528, 281, 312, 3798, 294, 341, 6090, 13, 286, 669, 13, 286, 500, 380, 362, 257, 1154, 11, 558, 30, 400, 586, 11, 51124], "temperature": 0.0, "avg_logprob": -0.09706846872965495, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.008979475125670433}, {"id": 154, "seek": 87660, "start": 891.8000000000001, "end": 899.48, "text": " what that means is you're all, you're immediately into something very interesting. The organism is", "tokens": [51124, 437, 300, 1355, 307, 291, 434, 439, 11, 291, 434, 4258, 666, 746, 588, 1880, 13, 440, 24128, 307, 51508], "temperature": 0.0, "avg_logprob": -0.09706846872965495, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.008979475125670433}, {"id": 155, "seek": 89948, "start": 899.48, "end": 908.12, "text": " trying to actually predict a possible state in the world and prepare itself, so it's an agent.", "tokens": [50364, 1382, 281, 767, 6069, 257, 1944, 1785, 294, 264, 1002, 293, 5940, 2564, 11, 370, 309, 311, 364, 9461, 13, 50796], "temperature": 0.0, "avg_logprob": -0.10361205206976996, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.16434702277183533}, {"id": 156, "seek": 89948, "start": 908.12, "end": 915.96, "text": " It doesn't just behave. It alters its behavior to alter the states in the world, right? And so it is", "tokens": [50796, 467, 1177, 380, 445, 15158, 13, 467, 419, 1559, 1080, 5223, 281, 11337, 264, 4368, 294, 264, 1002, 11, 558, 30, 400, 370, 309, 307, 51188], "temperature": 0.0, "avg_logprob": -0.10361205206976996, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.16434702277183533}, {"id": 157, "seek": 89948, "start": 915.96, "end": 922.2, "text": " trying to, I would say, predictively prepare for the world. It's trying to predict the world, but", "tokens": [51188, 1382, 281, 11, 286, 576, 584, 11, 6069, 3413, 5940, 337, 264, 1002, 13, 467, 311, 1382, 281, 6069, 264, 1002, 11, 457, 51500], "temperature": 0.0, "avg_logprob": -0.10361205206976996, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.16434702277183533}, {"id": 158, "seek": 89948, "start": 922.2, "end": 927.48, "text": " prepare itself for that world, but also prepare the world so it's more likely to come out in the", "tokens": [51500, 5940, 2564, 337, 300, 1002, 11, 457, 611, 5940, 264, 1002, 370, 309, 311, 544, 3700, 281, 808, 484, 294, 264, 51764], "temperature": 0.0, "avg_logprob": -0.10361205206976996, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.16434702277183533}, {"id": 159, "seek": 92748, "start": 927.5600000000001, "end": 934.12, "text": " prediction that it seeks, right? And so I put those two together, and I talk about anticipation.", "tokens": [50368, 17630, 300, 309, 28840, 11, 558, 30, 400, 370, 286, 829, 729, 732, 1214, 11, 293, 286, 751, 466, 35979, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14908255887835214, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0013040119083598256}, {"id": 160, "seek": 92748, "start": 935.88, "end": 941.5600000000001, "text": " And so whenever you're problem solving, you're anticipating a goal, meaning that prediction", "tokens": [50784, 400, 370, 5699, 291, 434, 1154, 12606, 11, 291, 434, 40568, 257, 3387, 11, 3620, 300, 17630, 51068], "temperature": 0.0, "avg_logprob": -0.14908255887835214, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0013040119083598256}, {"id": 161, "seek": 92748, "start": 941.5600000000001, "end": 948.12, "text": " and preparation. Now, Fourier coincide doesn't directly talk about that as clear as it needs to.", "tokens": [51068, 293, 13081, 13, 823, 11, 36810, 13001, 482, 1177, 380, 3838, 751, 466, 300, 382, 1850, 382, 309, 2203, 281, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14908255887835214, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0013040119083598256}, {"id": 162, "seek": 92748, "start": 948.12, "end": 954.44, "text": " It talks about coupling, and it talks about affordances, and I think there's a deep connection", "tokens": [51396, 467, 6686, 466, 37447, 11, 293, 309, 6686, 466, 6157, 2676, 11, 293, 286, 519, 456, 311, 257, 2452, 4984, 51712], "temperature": 0.0, "avg_logprob": -0.14908255887835214, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0013040119083598256}, {"id": 163, "seek": 95444, "start": 954.44, "end": 959.48, "text": " between opponent processing, optimal gripping, and affordances, and maybe we can exploit that", "tokens": [50364, 1296, 10620, 9007, 11, 16252, 17865, 3759, 11, 293, 6157, 2676, 11, 293, 1310, 321, 393, 25924, 300, 50616], "temperature": 0.0, "avg_logprob": -0.13893505171233533, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.0038222987204790115}, {"id": 164, "seek": 95444, "start": 959.48, "end": 967.48, "text": " at some point. But typically, one of the things where Fourier coincide has some challenges", "tokens": [50616, 412, 512, 935, 13, 583, 5850, 11, 472, 295, 264, 721, 689, 36810, 13001, 482, 575, 512, 4759, 51016], "temperature": 0.0, "avg_logprob": -0.13893505171233533, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.0038222987204790115}, {"id": 165, "seek": 95444, "start": 967.48, "end": 974.36, "text": " is in more distal relations to the environment, because it tends to rely primarily on coupling.", "tokens": [51016, 307, 294, 544, 1483, 304, 2299, 281, 264, 2823, 11, 570, 309, 12258, 281, 10687, 10029, 322, 37447, 13, 51360], "temperature": 0.0, "avg_logprob": -0.13893505171233533, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.0038222987204790115}, {"id": 166, "seek": 95444, "start": 974.36, "end": 978.0400000000001, "text": " And this is a long-standing critique that one of my colleagues, Brian Cantwell Smith,", "tokens": [51360, 400, 341, 307, 257, 938, 12, 8618, 25673, 300, 472, 295, 452, 7734, 11, 10765, 26697, 6326, 8538, 11, 51544], "temperature": 0.0, "avg_logprob": -0.13893505171233533, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.0038222987204790115}, {"id": 167, "seek": 95444, "start": 978.0400000000001, "end": 981.1600000000001, "text": " at the University of Toronto, because that is where all knowledge is flowing from,", "tokens": [51544, 412, 264, 3535, 295, 14140, 11, 570, 300, 307, 689, 439, 3601, 307, 13974, 490, 11, 51700], "temperature": 0.0, "avg_logprob": -0.13893505171233533, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.0038222987204790115}, {"id": 168, "seek": 98116, "start": 982.12, "end": 987.16, "text": " right, has made. He said, you know, you can be predictably coupled to things that you're in", "tokens": [50412, 558, 11, 575, 1027, 13, 634, 848, 11, 291, 458, 11, 291, 393, 312, 6069, 1188, 29482, 281, 721, 300, 291, 434, 294, 50664], "temperature": 0.0, "avg_logprob": -0.15004980797861137, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00521801458671689}, {"id": 169, "seek": 98116, "start": 987.16, "end": 993.9599999999999, "text": " direct causal contact with, but, you know, sorry, dynamically coupled, not predictably coupled,", "tokens": [50664, 2047, 38755, 3385, 365, 11, 457, 11, 291, 458, 11, 2597, 11, 43492, 29482, 11, 406, 6069, 1188, 29482, 11, 51004], "temperature": 0.0, "avg_logprob": -0.15004980797861137, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00521801458671689}, {"id": 170, "seek": 98116, "start": 993.9599999999999, "end": 1000.28, "text": " my apologies. But how do you do things that are much more distal? And for me, that is key.", "tokens": [51004, 452, 34929, 13, 583, 577, 360, 291, 360, 721, 300, 366, 709, 544, 1483, 304, 30, 400, 337, 385, 11, 300, 307, 2141, 13, 51320], "temperature": 0.0, "avg_logprob": -0.15004980797861137, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00521801458671689}, {"id": 171, "seek": 98116, "start": 1000.28, "end": 1008.8399999999999, "text": " And here's why I do this. I think when we evaluate, even intuitively, so I'm not using that as an", "tokens": [51320, 400, 510, 311, 983, 286, 360, 341, 13, 286, 519, 562, 321, 13059, 11, 754, 46506, 11, 370, 286, 478, 406, 1228, 300, 382, 364, 51748], "temperature": 0.0, "avg_logprob": -0.15004980797861137, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00521801458671689}, {"id": 172, "seek": 100884, "start": 1008.84, "end": 1015.4, "text": " authority, I'm just showing how readily this works for us. When we evaluate an organism for", "tokens": [50364, 8281, 11, 286, 478, 445, 4099, 577, 26336, 341, 1985, 337, 505, 13, 1133, 321, 13059, 364, 24128, 337, 50692], "temperature": 0.0, "avg_logprob": -0.08730220794677734, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.008842453360557556}, {"id": 173, "seek": 100884, "start": 1015.4, "end": 1020.52, "text": " its intelligence, we tend to do it in two interlocking ways. We do it in terms of, I think,", "tokens": [50692, 1080, 7599, 11, 321, 3928, 281, 360, 309, 294, 732, 728, 4102, 278, 2098, 13, 492, 360, 309, 294, 2115, 295, 11, 286, 519, 11, 50948], "temperature": 0.0, "avg_logprob": -0.08730220794677734, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.008842453360557556}, {"id": 174, "seek": 100884, "start": 1021.5600000000001, "end": 1025.08, "text": " how well does it zero in on relevant information? How well does it pay attention to what it needs?", "tokens": [51000, 577, 731, 775, 309, 4018, 294, 322, 7340, 1589, 30, 1012, 731, 775, 309, 1689, 3202, 281, 437, 309, 2203, 30, 51176], "temperature": 0.0, "avg_logprob": -0.08730220794677734, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.008842453360557556}, {"id": 175, "seek": 100884, "start": 1025.08, "end": 1029.88, "text": " We look at an animal and go, wow, that's really good. Notice how it's noting subtle differences.", "tokens": [51176, 492, 574, 412, 364, 5496, 293, 352, 11, 6076, 11, 300, 311, 534, 665, 13, 13428, 577, 309, 311, 26801, 13743, 7300, 13, 51416], "temperature": 0.0, "avg_logprob": -0.08730220794677734, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.008842453360557556}, {"id": 176, "seek": 100884, "start": 1029.88, "end": 1034.52, "text": " But we also evaluate the intelligence of an organism, and Michael Levin talks about this", "tokens": [51416, 583, 321, 611, 13059, 264, 7599, 295, 364, 24128, 11, 293, 5116, 1456, 4796, 6686, 466, 341, 51648], "temperature": 0.0, "avg_logprob": -0.08730220794677734, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.008842453360557556}, {"id": 177, "seek": 103452, "start": 1034.52, "end": 1039.48, "text": " with his cognitive light cone idea, is how deeply into the world they can anticipate. And I don't", "tokens": [50364, 365, 702, 15605, 1442, 19749, 1558, 11, 307, 577, 8760, 666, 264, 1002, 436, 393, 21685, 13, 400, 286, 500, 380, 50612], "temperature": 0.0, "avg_logprob": -0.14823440883470618, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.03307012468576431}, {"id": 178, "seek": 103452, "start": 1039.48, "end": 1045.56, "text": " mean just spatio-temporally, I also mean it modally, possibility. Like when I talk to my cat and I say,", "tokens": [50612, 914, 445, 15000, 1004, 12, 83, 11840, 379, 11, 286, 611, 914, 309, 1072, 379, 11, 7959, 13, 1743, 562, 286, 751, 281, 452, 3857, 293, 286, 584, 11, 50916], "temperature": 0.0, "avg_logprob": -0.14823440883470618, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.03307012468576431}, {"id": 179, "seek": 103452, "start": 1047.24, "end": 1052.28, "text": " you know, where's your toy? The cat looks at me and I go, no, well, I say to my puppy,", "tokens": [51000, 291, 458, 11, 689, 311, 428, 12058, 30, 440, 3857, 1542, 412, 385, 293, 286, 352, 11, 572, 11, 731, 11, 286, 584, 281, 452, 18196, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14823440883470618, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.03307012468576431}, {"id": 180, "seek": 103452, "start": 1052.28, "end": 1057.24, "text": " where's your ball? And she goes into the other room, looks for the ball, finds it under the", "tokens": [51252, 689, 311, 428, 2594, 30, 400, 750, 1709, 666, 264, 661, 1808, 11, 1542, 337, 264, 2594, 11, 10704, 309, 833, 264, 51500], "temperature": 0.0, "avg_logprob": -0.14823440883470618, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.03307012468576431}, {"id": 181, "seek": 103452, "start": 1057.24, "end": 1063.32, "text": " couch and brings it all the way back to me. I go, wow, you're really smart, right? And so we get", "tokens": [51500, 16511, 293, 5607, 309, 439, 264, 636, 646, 281, 385, 13, 286, 352, 11, 6076, 11, 291, 434, 534, 4069, 11, 558, 30, 400, 370, 321, 483, 51804], "temperature": 0.0, "avg_logprob": -0.14823440883470618, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.03307012468576431}, {"id": 182, "seek": 106332, "start": 1063.32, "end": 1070.2, "text": " this because we know how just moment by moment that adaptive ability is built on how distally", "tokens": [50364, 341, 570, 321, 458, 577, 445, 1623, 538, 1623, 300, 27912, 3485, 307, 3094, 322, 577, 1483, 379, 50708], "temperature": 0.0, "avg_logprob": -0.11656570434570312, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.017434140667319298}, {"id": 183, "seek": 106332, "start": 1070.2, "end": 1075.32, "text": " can we pursue goals? We tend to evaluate people, wow, you pursued a long-term goal and you brought", "tokens": [50708, 393, 321, 12392, 5493, 30, 492, 3928, 281, 13059, 561, 11, 6076, 11, 291, 34893, 257, 938, 12, 7039, 3387, 293, 291, 3038, 50964], "temperature": 0.0, "avg_logprob": -0.11656570434570312, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.017434140667319298}, {"id": 184, "seek": 106332, "start": 1075.32, "end": 1081.3999999999999, "text": " it up. So that's what's missing. And I think that's what's really afforded. Now, I want to make one", "tokens": [50964, 309, 493, 13, 407, 300, 311, 437, 311, 5361, 13, 400, 286, 519, 300, 311, 437, 311, 534, 6157, 292, 13, 823, 11, 286, 528, 281, 652, 472, 51268], "temperature": 0.0, "avg_logprob": -0.11656570434570312, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.017434140667319298}, {"id": 185, "seek": 106332, "start": 1081.3999999999999, "end": 1087.08, "text": " more point, and then I'll shut up so you can ask me another question, which is these two issues,", "tokens": [51268, 544, 935, 11, 293, 550, 286, 603, 5309, 493, 370, 291, 393, 1029, 385, 1071, 1168, 11, 597, 307, 613, 732, 2663, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11656570434570312, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.017434140667319298}, {"id": 186, "seek": 106332, "start": 1087.8799999999999, "end": 1092.04, "text": " anticipating more deeply, and remember, I don't just mean spatio-temporally, I mean modally,", "tokens": [51592, 40568, 544, 8760, 11, 293, 1604, 11, 286, 500, 380, 445, 914, 15000, 1004, 12, 83, 11840, 379, 11, 286, 914, 1072, 379, 11, 51800], "temperature": 0.0, "avg_logprob": -0.11656570434570312, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.017434140667319298}, {"id": 187, "seek": 109204, "start": 1092.68, "end": 1099.48, "text": " and relevance realisation are deeply interconnected. The more you anticipate, the more the problem of", "tokens": [50396, 293, 32684, 957, 7623, 366, 8760, 36611, 13, 440, 544, 291, 21685, 11, 264, 544, 264, 1154, 295, 50736], "temperature": 0.0, "avg_logprob": -0.08567060907203031, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.0013630883768200874}, {"id": 188, "seek": 109204, "start": 1099.48, "end": 1106.68, "text": " relevance realisation goes up exponentially. And so these two things, right, they, I would argue,", "tokens": [50736, 32684, 957, 7623, 1709, 493, 37330, 13, 400, 370, 613, 732, 721, 11, 558, 11, 436, 11, 286, 576, 9695, 11, 51096], "temperature": 0.0, "avg_logprob": -0.08567060907203031, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.0013630883768200874}, {"id": 189, "seek": 109204, "start": 1106.68, "end": 1111.3999999999999, "text": " because they're interlocking, they have to be solved together. And I think that's why we use", "tokens": [51096, 570, 436, 434, 728, 4102, 278, 11, 436, 362, 281, 312, 13041, 1214, 13, 400, 286, 519, 300, 311, 983, 321, 764, 51332], "temperature": 0.0, "avg_logprob": -0.08567060907203031, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.0013630883768200874}, {"id": 190, "seek": 109204, "start": 1111.3999999999999, "end": 1115.6399999999999, "text": " both of them as evaluations of the intelligence of an organism.", "tokens": [51332, 1293, 295, 552, 382, 43085, 295, 264, 7599, 295, 364, 24128, 13, 51544], "temperature": 0.0, "avg_logprob": -0.08567060907203031, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.0013630883768200874}, {"id": 191, "seek": 111564, "start": 1116.6000000000001, "end": 1124.0400000000002, "text": " Okay, I'd like to backtrack to that second argument after this, after what I say here.", "tokens": [50412, 1033, 11, 286, 1116, 411, 281, 646, 19466, 281, 300, 1150, 6770, 934, 341, 11, 934, 437, 286, 584, 510, 13, 50784], "temperature": 0.0, "avg_logprob": -0.16646209129920372, "compression_ratio": 1.5186721991701244, "no_speech_prob": 0.005175414495170116}, {"id": 192, "seek": 111564, "start": 1126.3600000000001, "end": 1133.64, "text": " Yeah, it strikes me that what predictive processing can give, perhaps where 4e Cognitive", "tokens": [50900, 865, 11, 309, 16750, 385, 300, 437, 35521, 9007, 393, 976, 11, 4317, 689, 1017, 68, 383, 2912, 2187, 51264], "temperature": 0.0, "avg_logprob": -0.16646209129920372, "compression_ratio": 1.5186721991701244, "no_speech_prob": 0.005175414495170116}, {"id": 193, "seek": 111564, "start": 1133.64, "end": 1139.0, "text": " Science doesn't, but I'm happy to be schooled on this. And I know Varela and Autoporesis in some", "tokens": [51264, 8976, 1177, 380, 11, 457, 286, 478, 2055, 281, 312, 1395, 292, 322, 341, 13, 400, 286, 458, 691, 543, 875, 293, 6049, 404, 2706, 271, 294, 512, 51532], "temperature": 0.0, "avg_logprob": -0.16646209129920372, "compression_ratio": 1.5186721991701244, "no_speech_prob": 0.005175414495170116}, {"id": 194, "seek": 111564, "start": 1139.0, "end": 1145.48, "text": " sense pre-shadowed and prefigured what I'm about to say, is that it kind of gives a ground to", "tokens": [51532, 2020, 659, 12, 2716, 11528, 292, 293, 18417, 328, 3831, 437, 286, 478, 466, 281, 584, 11, 307, 300, 309, 733, 295, 2709, 257, 2727, 281, 51856], "temperature": 0.0, "avg_logprob": -0.16646209129920372, "compression_ratio": 1.5186721991701244, "no_speech_prob": 0.005175414495170116}, {"id": 195, "seek": 114548, "start": 1145.56, "end": 1151.56, "text": " our action and to our perception, which is that we have to have a model of the world and a model", "tokens": [50368, 527, 3069, 293, 281, 527, 12860, 11, 597, 307, 300, 321, 362, 281, 362, 257, 2316, 295, 264, 1002, 293, 257, 2316, 50668], "temperature": 0.0, "avg_logprob": -0.09582395068669723, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0013092758599668741}, {"id": 196, "seek": 114548, "start": 1151.56, "end": 1157.24, "text": " of ourselves, which is not only descriptively viable, but also normatively viable. In so far as,", "tokens": [50668, 295, 4175, 11, 597, 307, 406, 787, 31280, 3413, 22024, 11, 457, 611, 2026, 19020, 22024, 13, 682, 370, 1400, 382, 11, 50952], "temperature": 0.0, "avg_logprob": -0.09582395068669723, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0013092758599668741}, {"id": 197, "seek": 114548, "start": 1157.8, "end": 1163.32, "text": " it's all like, it's very important for me to be able to predict the world. But as you said,", "tokens": [50980, 309, 311, 439, 411, 11, 309, 311, 588, 1021, 337, 385, 281, 312, 1075, 281, 6069, 264, 1002, 13, 583, 382, 291, 848, 11, 51256], "temperature": 0.0, "avg_logprob": -0.09582395068669723, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0013092758599668741}, {"id": 198, "seek": 114548, "start": 1163.32, "end": 1167.56, "text": " we aren't just at the behest of the world's dynamics, we can change the world according", "tokens": [51256, 321, 3212, 380, 445, 412, 264, 1540, 377, 295, 264, 1002, 311, 15679, 11, 321, 393, 1319, 264, 1002, 4650, 51468], "temperature": 0.0, "avg_logprob": -0.09582395068669723, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0013092758599668741}, {"id": 199, "seek": 114548, "start": 1167.56, "end": 1174.84, "text": " to our preferences. Exactly. And I think what predictive processing gives us is, although to", "tokens": [51468, 281, 527, 21910, 13, 7587, 13, 400, 286, 519, 437, 35521, 9007, 2709, 505, 307, 11, 4878, 281, 51832], "temperature": 0.0, "avg_logprob": -0.09582395068669723, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0013092758599668741}, {"id": 200, "seek": 117484, "start": 1174.84, "end": 1181.8799999999999, "text": " be fair, I think Carl might say that this is a bit of an overshoot, is a telos, is a fundamental", "tokens": [50364, 312, 3143, 11, 286, 519, 14256, 1062, 584, 300, 341, 307, 257, 857, 295, 364, 15488, 24467, 11, 307, 257, 15284, 329, 11, 307, 257, 8088, 50716], "temperature": 0.0, "avg_logprob": -0.09968358537425166, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0011806463589891791}, {"id": 201, "seek": 117484, "start": 1181.8799999999999, "end": 1187.08, "text": " attractor set to which our perception of action is governed. So I think that's kind of the way that", "tokens": [50716, 5049, 284, 992, 281, 597, 527, 12860, 295, 3069, 307, 35529, 13, 407, 286, 519, 300, 311, 733, 295, 264, 636, 300, 50976], "temperature": 0.0, "avg_logprob": -0.09968358537425166, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0011806463589891791}, {"id": 202, "seek": 117484, "start": 1187.08, "end": 1195.0, "text": " I see the added benefit of this convergence between predictive processing and relevance", "tokens": [50976, 286, 536, 264, 3869, 5121, 295, 341, 32181, 1296, 35521, 9007, 293, 32684, 51372], "temperature": 0.0, "avg_logprob": -0.09968358537425166, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0011806463589891791}, {"id": 203, "seek": 117484, "start": 1195.0, "end": 1199.8, "text": " realization. What I was also going to say there is that it's, fantastically, you're mentioning", "tokens": [51372, 25138, 13, 708, 286, 390, 611, 516, 281, 584, 456, 307, 300, 309, 311, 11, 4115, 22808, 11, 291, 434, 18315, 51612], "temperature": 0.0, "avg_logprob": -0.09968358537425166, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0011806463589891791}, {"id": 204, "seek": 119980, "start": 1199.8, "end": 1205.0, "text": " this kind of deep temporal modeling. Because from my eyes, that's really where a lot of the work", "tokens": [50364, 341, 733, 295, 2452, 30881, 15983, 13, 1436, 490, 452, 2575, 11, 300, 311, 534, 689, 257, 688, 295, 264, 589, 50624], "temperature": 0.0, "avg_logprob": -0.1290577898968707, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.09736253321170807}, {"id": 205, "seek": 119980, "start": 1205.0, "end": 1212.04, "text": " is being done right now, which is that selfhood consciousness, even perhaps space itself is", "tokens": [50624, 307, 885, 1096, 558, 586, 11, 597, 307, 300, 2698, 3809, 10081, 11, 754, 4317, 1901, 2564, 307, 50976], "temperature": 0.0, "avg_logprob": -0.1290577898968707, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.09736253321170807}, {"id": 206, "seek": 119980, "start": 1212.04, "end": 1220.04, "text": " downstream on the fact that we can downstream on the degree to which we can model the slower", "tokens": [50976, 30621, 322, 264, 1186, 300, 321, 393, 30621, 322, 264, 4314, 281, 597, 321, 393, 2316, 264, 14009, 51376], "temperature": 0.0, "avg_logprob": -0.1290577898968707, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.09736253321170807}, {"id": 207, "seek": 119980, "start": 1220.04, "end": 1227.3999999999999, "text": " dynamics and the faster dynamics in a generative hierarchy. So that's, I think that's it. I mean,", "tokens": [51376, 15679, 293, 264, 4663, 15679, 294, 257, 1337, 1166, 22333, 13, 407, 300, 311, 11, 286, 519, 300, 311, 309, 13, 286, 914, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1290577898968707, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.09736253321170807}, {"id": 208, "seek": 122740, "start": 1227.4, "end": 1230.6000000000001, "text": " complete agreement with that. I think that's an extension of the argument I made.", "tokens": [50364, 3566, 8106, 365, 300, 13, 286, 519, 300, 311, 364, 10320, 295, 264, 6770, 286, 1027, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1035252844933236, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.00764896534383297}, {"id": 209, "seek": 122740, "start": 1232.2800000000002, "end": 1238.0400000000002, "text": " And so what I wanted to pick up there is you said, if I heard correctly, that the more,", "tokens": [50608, 400, 370, 437, 286, 1415, 281, 1888, 493, 456, 307, 291, 848, 11, 498, 286, 2198, 8944, 11, 300, 264, 544, 11, 50896], "temperature": 0.0, "avg_logprob": -0.1035252844933236, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.00764896534383297}, {"id": 210, "seek": 122740, "start": 1238.0400000000002, "end": 1243.64, "text": " the deeper your temporal model, the more you can model slower fluctuations in the environment,", "tokens": [50896, 264, 7731, 428, 30881, 2316, 11, 264, 544, 291, 393, 2316, 14009, 45276, 294, 264, 2823, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1035252844933236, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.00764896534383297}, {"id": 211, "seek": 122740, "start": 1243.64, "end": 1248.68, "text": " the more critical relevance realization becomes, the higher the stakes in some sense.", "tokens": [51176, 264, 544, 4924, 32684, 25138, 3643, 11, 264, 2946, 264, 28429, 294, 512, 2020, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1035252844933236, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.00764896534383297}, {"id": 212, "seek": 122740, "start": 1250.44, "end": 1254.76, "text": " I'm interested in unpicking that argument, because from my eyes, there are plenty of", "tokens": [51516, 286, 478, 3102, 294, 20994, 10401, 300, 6770, 11, 570, 490, 452, 2575, 11, 456, 366, 7140, 295, 51732], "temperature": 0.0, "avg_logprob": -0.1035252844933236, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.00764896534383297}, {"id": 213, "seek": 125476, "start": 1254.76, "end": 1263.16, "text": " things out there that don't have deep temporal models. So a virus, for example, you're just", "tokens": [50364, 721, 484, 456, 300, 500, 380, 362, 2452, 30881, 5245, 13, 407, 257, 5752, 11, 337, 1365, 11, 291, 434, 445, 50784], "temperature": 0.0, "avg_logprob": -0.1504604194475257, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0014748176326975226}, {"id": 214, "seek": 125476, "start": 1263.16, "end": 1269.16, "text": " reacting to very coarse-grained features of the environment. But for them, relevance realization,", "tokens": [50784, 25817, 281, 588, 39312, 12, 20735, 2001, 4122, 295, 264, 2823, 13, 583, 337, 552, 11, 32684, 25138, 11, 51084], "temperature": 0.0, "avg_logprob": -0.1504604194475257, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0014748176326975226}, {"id": 215, "seek": 125476, "start": 1269.16, "end": 1271.8, "text": " at least to my eyes, appears as critical as it would for a human.", "tokens": [51084, 412, 1935, 281, 452, 2575, 11, 7038, 382, 4924, 382, 309, 576, 337, 257, 1952, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1504604194475257, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0014748176326975226}, {"id": 216, "seek": 125476, "start": 1272.68, "end": 1278.44, "text": " It is. It is. And so I think it's how I'm going to put it, I think it's always the", "tokens": [51260, 467, 307, 13, 467, 307, 13, 400, 370, 286, 519, 309, 311, 577, 286, 478, 516, 281, 829, 309, 11, 286, 519, 309, 311, 1009, 264, 51548], "temperature": 0.0, "avg_logprob": -0.1504604194475257, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0014748176326975226}, {"id": 217, "seek": 127844, "start": 1278.44, "end": 1288.3600000000001, "text": " demanding problem. So I'm not denying that even the paramecium has to do salience landscaping.", "tokens": [50364, 19960, 1154, 13, 407, 286, 478, 406, 30363, 300, 754, 264, 971, 529, 19324, 575, 281, 360, 1845, 1182, 23865, 3381, 13, 50860], "temperature": 0.0, "avg_logprob": -0.12353727091913638, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.061797987669706345}, {"id": 218, "seek": 127844, "start": 1288.3600000000001, "end": 1293.3200000000002, "text": " It has to, I'm going to use a word very neutrally here. I'm not connoting anything about consciousness,", "tokens": [50860, 467, 575, 281, 11, 286, 478, 516, 281, 764, 257, 1349, 588, 39913, 379, 510, 13, 286, 478, 406, 46371, 278, 1340, 466, 10081, 11, 51108], "temperature": 0.0, "avg_logprob": -0.12353727091913638, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.061797987669706345}, {"id": 219, "seek": 127844, "start": 1293.3200000000002, "end": 1299.24, "text": " but it has to recognize this molecule is food and that molecule is poison and swim reliably", "tokens": [51108, 457, 309, 575, 281, 5521, 341, 15582, 307, 1755, 293, 300, 15582, 307, 10836, 293, 7110, 49927, 51404], "temperature": 0.0, "avg_logprob": -0.12353727091913638, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.061797987669706345}, {"id": 220, "seek": 127844, "start": 1299.24, "end": 1304.3600000000001, "text": " towards the one and away from the other. So I think the problem is always there,", "tokens": [51404, 3030, 264, 472, 293, 1314, 490, 264, 661, 13, 407, 286, 519, 264, 1154, 307, 1009, 456, 11, 51660], "temperature": 0.0, "avg_logprob": -0.12353727091913638, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.061797987669706345}, {"id": 221, "seek": 130436, "start": 1304.36, "end": 1309.6399999999999, "text": " as I was trying to indicate, as soon as you have problems, as soon as your goal states are", "tokens": [50364, 382, 286, 390, 1382, 281, 13330, 11, 382, 2321, 382, 291, 362, 2740, 11, 382, 2321, 382, 428, 3387, 4368, 366, 50628], "temperature": 0.0, "avg_logprob": -0.08385440538514335, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.029296165332198143}, {"id": 222, "seek": 130436, "start": 1309.6399999999999, "end": 1314.1999999999998, "text": " distinct. And I think the space opens up very quickly exponentially. I just meant that it", "tokens": [50628, 10644, 13, 400, 286, 519, 264, 1901, 9870, 493, 588, 2661, 37330, 13, 286, 445, 4140, 300, 309, 50856], "temperature": 0.0, "avg_logprob": -0.08385440538514335, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.029296165332198143}, {"id": 223, "seek": 130436, "start": 1314.1999999999998, "end": 1322.52, "text": " exponentially gets worse. This is why you see across species, hyperbolic discounting,", "tokens": [50856, 37330, 2170, 5324, 13, 639, 307, 983, 291, 536, 2108, 6172, 11, 9848, 65, 7940, 11635, 278, 11, 51272], "temperature": 0.0, "avg_logprob": -0.08385440538514335, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.029296165332198143}, {"id": 224, "seek": 130436, "start": 1322.52, "end": 1327.32, "text": " temporal discounting, because that's a huge relevance realization machine. It's about", "tokens": [51272, 30881, 11635, 278, 11, 570, 300, 311, 257, 2603, 32684, 25138, 3479, 13, 467, 311, 466, 51512], "temperature": 0.0, "avg_logprob": -0.08385440538514335, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.029296165332198143}, {"id": 225, "seek": 130436, "start": 1327.32, "end": 1332.84, "text": " salience discounting. And the point of that is, is because as you opened into the future,", "tokens": [51512, 1845, 1182, 11635, 278, 13, 400, 264, 935, 295, 300, 307, 11, 307, 570, 382, 291, 5625, 666, 264, 2027, 11, 51788], "temperature": 0.0, "avg_logprob": -0.08385440538514335, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.029296165332198143}, {"id": 226, "seek": 133284, "start": 1332.84, "end": 1337.9599999999998, "text": " the number of possibilities goes up exponentially. And so you need to have this attenuation function,", "tokens": [50364, 264, 1230, 295, 12178, 1709, 493, 37330, 13, 400, 370, 291, 643, 281, 362, 341, 951, 268, 16073, 2445, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 227, "seek": 133284, "start": 1337.9599999999998, "end": 1343.24, "text": " so you don't get overwhelmed by the possibilities as you extend your cognition into the future.", "tokens": [50620, 370, 291, 500, 380, 483, 19042, 538, 264, 12178, 382, 291, 10101, 428, 46905, 666, 264, 2027, 13, 50884], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 228, "seek": 133284, "start": 1343.24, "end": 1348.6, "text": " That's why we have hyperbolic discounting. And we pay a huge price for it. It makes us procrastinate.", "tokens": [50884, 663, 311, 983, 321, 362, 9848, 65, 7940, 11635, 278, 13, 400, 321, 1689, 257, 2603, 3218, 337, 309, 13, 467, 1669, 505, 39306, 13923, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 229, "seek": 133284, "start": 1348.6, "end": 1353.1599999999999, "text": " It makes us difficult to give up, to pursue our long-term goals. So there's even a trade-off", "tokens": [51152, 467, 1669, 505, 2252, 281, 976, 493, 11, 281, 12392, 527, 938, 12, 7039, 5493, 13, 407, 456, 311, 754, 257, 4923, 12, 4506, 51380], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 230, "seek": 133284, "start": 1353.1599999999999, "end": 1357.56, "text": " relationship in there, but I won't get into it. But that's an example. There's a good reason why we", "tokens": [51380, 2480, 294, 456, 11, 457, 286, 1582, 380, 483, 666, 309, 13, 583, 300, 311, 364, 1365, 13, 821, 311, 257, 665, 1778, 983, 321, 51600], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 231, "seek": 133284, "start": 1357.56, "end": 1362.1999999999998, "text": " must have something like temporal discounting across species. That's what Angelie showed,", "tokens": [51600, 1633, 362, 746, 411, 30881, 11635, 278, 2108, 6172, 13, 663, 311, 437, 14902, 414, 4712, 11, 51832], "temperature": 0.0, "avg_logprob": -0.09876751899719238, "compression_ratio": 1.7218934911242603, "no_speech_prob": 0.0019261935958638787}, {"id": 232, "seek": 136220, "start": 1362.2, "end": 1368.1200000000001, "text": " is because the relevance realization issue just, it gets even worse. It starts out bad and gets even", "tokens": [50364, 307, 570, 264, 32684, 25138, 2734, 445, 11, 309, 2170, 754, 5324, 13, 467, 3719, 484, 1578, 293, 2170, 754, 50660], "temperature": 0.0, "avg_logprob": -0.14003894603357905, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0006968055968172848}, {"id": 233, "seek": 136220, "start": 1368.1200000000001, "end": 1374.92, "text": " worse. Right. Wonderful. And yeah, I mean, a lot of your work will come to affordances now because", "tokens": [50660, 5324, 13, 1779, 13, 22768, 13, 400, 1338, 11, 286, 914, 11, 257, 688, 295, 428, 589, 486, 808, 281, 6157, 2676, 586, 570, 51000], "temperature": 0.0, "avg_logprob": -0.14003894603357905, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0006968055968172848}, {"id": 234, "seek": 136220, "start": 1374.92, "end": 1380.04, "text": " a lot of your work is centered on the notion of affordances. So affordances are the possibilities", "tokens": [51000, 257, 688, 295, 428, 589, 307, 18988, 322, 264, 10710, 295, 6157, 2676, 13, 407, 6157, 2676, 366, 264, 12178, 51256], "temperature": 0.0, "avg_logprob": -0.14003894603357905, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0006968055968172848}, {"id": 235, "seek": 136220, "start": 1380.04, "end": 1384.6000000000001, "text": " for action afforded, actually circular, possibilities for action granted by the environment. I mean,", "tokens": [51256, 337, 3069, 6157, 292, 11, 767, 16476, 11, 12178, 337, 3069, 12344, 538, 264, 2823, 13, 286, 914, 11, 51484], "temperature": 0.0, "avg_logprob": -0.14003894603357905, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0006968055968172848}, {"id": 236, "seek": 136220, "start": 1384.6000000000001, "end": 1391.48, "text": " afforded was a verb and then Gibson made it now. How much has the predictive processing", "tokens": [51484, 6157, 292, 390, 257, 9595, 293, 550, 42250, 1027, 309, 586, 13, 1012, 709, 575, 264, 35521, 9007, 51828], "temperature": 0.0, "avg_logprob": -0.14003894603357905, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.0006968055968172848}, {"id": 237, "seek": 139148, "start": 1391.48, "end": 1396.44, "text": " framework supplemented your understanding of affordances? And perhaps the way that", "tokens": [50364, 8388, 15436, 292, 428, 3701, 295, 6157, 2676, 30, 400, 4317, 264, 636, 300, 50612], "temperature": 0.0, "avg_logprob": -0.11653213722761287, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.004107276909053326}, {"id": 238, "seek": 139148, "start": 1397.24, "end": 1404.52, "text": " what we perceive in the world is not necessarily a feature list of objects, but they are fundamentally", "tokens": [50652, 437, 321, 20281, 294, 264, 1002, 307, 406, 4725, 257, 4111, 1329, 295, 6565, 11, 457, 436, 366, 17879, 51016], "temperature": 0.0, "avg_logprob": -0.11653213722761287, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.004107276909053326}, {"id": 239, "seek": 139148, "start": 1404.52, "end": 1411.56, "text": " affordances. I see my glass of water as grippable rather than being glass this dimension. And I", "tokens": [51016, 6157, 2676, 13, 286, 536, 452, 4276, 295, 1281, 382, 17865, 427, 712, 2831, 813, 885, 4276, 341, 10139, 13, 400, 286, 51368], "temperature": 0.0, "avg_logprob": -0.11653213722761287, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.004107276909053326}, {"id": 240, "seek": 139148, "start": 1411.56, "end": 1419.08, "text": " only ask that because, as I said, as conscious, animate cognizers, we are blessed with the", "tokens": [51368, 787, 1029, 300, 570, 11, 382, 286, 848, 11, 382, 6648, 11, 36439, 11786, 22525, 11, 321, 366, 12351, 365, 264, 51744], "temperature": 0.0, "avg_logprob": -0.11653213722761287, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.004107276909053326}, {"id": 241, "seek": 141908, "start": 1419.1599999999999, "end": 1425.3999999999999, "text": " capacity to change the world in accordance with our priors, which makes action really the fundamental", "tokens": [50368, 6042, 281, 1319, 264, 1002, 294, 31110, 365, 527, 1790, 830, 11, 597, 1669, 3069, 534, 264, 8088, 50680], "temperature": 0.0, "avg_logprob": -0.15361488566679113, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006654385942965746}, {"id": 242, "seek": 141908, "start": 1425.3999999999999, "end": 1433.0, "text": " currency for auto poesis, for self-organization. So does it has predicted processing ramped up", "tokens": [50680, 13346, 337, 8399, 714, 9374, 11, 337, 2698, 12, 12372, 2144, 13, 407, 775, 309, 575, 19147, 9007, 12428, 292, 493, 51060], "temperature": 0.0, "avg_logprob": -0.15361488566679113, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006654385942965746}, {"id": 243, "seek": 141908, "start": 1433.56, "end": 1440.04, "text": " the importance that you give to Gibsonian affordances? I don't have it ramped up because I", "tokens": [51088, 264, 7379, 300, 291, 976, 281, 42250, 952, 6157, 2676, 30, 286, 500, 380, 362, 309, 12428, 292, 493, 570, 286, 51412], "temperature": 0.0, "avg_logprob": -0.15361488566679113, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006654385942965746}, {"id": 244, "seek": 141908, "start": 1440.04, "end": 1446.84, "text": " always thought I literally learned this stuff from John Kennedy when it gives his greatest", "tokens": [51412, 1009, 1194, 286, 3736, 3264, 341, 1507, 490, 2619, 16517, 562, 309, 2709, 702, 6636, 51752], "temperature": 0.0, "avg_logprob": -0.15361488566679113, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006654385942965746}, {"id": 245, "seek": 144684, "start": 1446.84, "end": 1454.6799999999998, "text": " protease. So very early on, I was very much impressed by this. And it occurred to me that", "tokens": [50364, 5631, 651, 13, 407, 588, 2440, 322, 11, 286, 390, 588, 709, 11679, 538, 341, 13, 400, 309, 11068, 281, 385, 300, 50756], "temperature": 0.0, "avg_logprob": -0.12470174919475209, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0015476687112823129}, {"id": 246, "seek": 144684, "start": 1455.56, "end": 1462.12, "text": " relevance realization, the realization of relevance, and I mean realization in both senses,", "tokens": [50800, 32684, 25138, 11, 264, 25138, 295, 32684, 11, 293, 286, 914, 25138, 294, 1293, 17057, 11, 51128], "temperature": 0.0, "avg_logprob": -0.12470174919475209, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0015476687112823129}, {"id": 247, "seek": 144684, "start": 1462.12, "end": 1466.84, "text": " actualizing a possibility and becoming aware of it, or at least detecting it in some fashion,", "tokens": [51128, 3539, 3319, 257, 7959, 293, 5617, 3650, 295, 309, 11, 420, 412, 1935, 40237, 309, 294, 512, 6700, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12470174919475209, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0015476687112823129}, {"id": 248, "seek": 144684, "start": 1467.8, "end": 1472.84, "text": " that it's a prototypical kind of affordance. The thing about affordances is they're not found", "tokens": [51412, 300, 309, 311, 257, 46219, 34061, 733, 295, 6157, 719, 13, 440, 551, 466, 6157, 2676, 307, 436, 434, 406, 1352, 51664], "temperature": 0.0, "avg_logprob": -0.12470174919475209, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0015476687112823129}, {"id": 249, "seek": 147284, "start": 1472.84, "end": 1478.6, "text": " in the object or in the organism. The grippability of the glass is not in the glass, can't be gripped", "tokens": [50364, 294, 264, 2657, 420, 294, 264, 24128, 13, 440, 17865, 427, 2310, 295, 264, 4276, 307, 406, 294, 264, 4276, 11, 393, 380, 312, 17865, 3320, 50652], "temperature": 0.0, "avg_logprob": -0.14713766941657433, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.014050575904548168}, {"id": 250, "seek": 147284, "start": 1478.6, "end": 1486.84, "text": " by a peramecium. And it's not in my hand. My hand can't grasp Africa or the sun. It's in a fitted", "tokens": [50652, 538, 257, 680, 529, 19324, 13, 400, 309, 311, 406, 294, 452, 1011, 13, 1222, 1011, 393, 380, 21743, 7349, 420, 264, 3295, 13, 467, 311, 294, 257, 26321, 51064], "temperature": 0.0, "avg_logprob": -0.14713766941657433, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.014050575904548168}, {"id": 251, "seek": 147284, "start": 1486.84, "end": 1492.1999999999998, "text": " relation between them. And that's exactly what relevance is. It's a fitted relationship between.", "tokens": [51064, 9721, 1296, 552, 13, 400, 300, 311, 2293, 437, 32684, 307, 13, 467, 311, 257, 26321, 2480, 1296, 13, 51332], "temperature": 0.0, "avg_logprob": -0.14713766941657433, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.014050575904548168}, {"id": 252, "seek": 147284, "start": 1492.1999999999998, "end": 1499.56, "text": " So I always, before I came into a deep dialogue with predictive processing, I already saw a deep", "tokens": [51332, 407, 286, 1009, 11, 949, 286, 1361, 666, 257, 2452, 10221, 365, 35521, 9007, 11, 286, 1217, 1866, 257, 2452, 51700], "temperature": 0.0, "avg_logprob": -0.14713766941657433, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.014050575904548168}, {"id": 253, "seek": 149956, "start": 1499.56, "end": 1505.32, "text": " relationship between relevance, realization, and affordances. And I think part of my work has been", "tokens": [50364, 2480, 1296, 32684, 11, 25138, 11, 293, 6157, 2676, 13, 400, 286, 519, 644, 295, 452, 589, 575, 668, 50652], "temperature": 0.0, "avg_logprob": -0.14346576791948976, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.006688759196549654}, {"id": 254, "seek": 149956, "start": 1505.32, "end": 1510.76, "text": " to really explore the deep ontological significance of that, that we have a category that sort of", "tokens": [50652, 281, 534, 6839, 264, 2452, 6592, 4383, 17687, 295, 300, 11, 300, 321, 362, 257, 7719, 300, 1333, 295, 50924], "temperature": 0.0, "avg_logprob": -0.14346576791948976, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.006688759196549654}, {"id": 255, "seek": 149956, "start": 1510.76, "end": 1515.8, "text": " falls outside. This was Gibson's intent, too. This is what Forde Cogside is really talking about.", "tokens": [50924, 8804, 2380, 13, 639, 390, 42250, 311, 8446, 11, 886, 13, 639, 307, 437, 11961, 68, 383, 664, 1812, 307, 534, 1417, 466, 13, 51176], "temperature": 0.0, "avg_logprob": -0.14346576791948976, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.006688759196549654}, {"id": 256, "seek": 149956, "start": 1515.8, "end": 1522.76, "text": " Falls outside are our belief that we have a complete exhaustive dichotomy between the subjective", "tokens": [51176, 23245, 2380, 366, 527, 7107, 300, 321, 362, 257, 3566, 14687, 488, 10390, 310, 8488, 1296, 264, 25972, 51524], "temperature": 0.0, "avg_logprob": -0.14346576791948976, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.006688759196549654}, {"id": 257, "seek": 149956, "start": 1522.76, "end": 1527.1599999999999, "text": " and the objective, between the inner world and the outer world. And so there's this other important,", "tokens": [51524, 293, 264, 10024, 11, 1296, 264, 7284, 1002, 293, 264, 10847, 1002, 13, 400, 370, 456, 311, 341, 661, 1021, 11, 51744], "temperature": 0.0, "avg_logprob": -0.14346576791948976, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.006688759196549654}, {"id": 258, "seek": 152716, "start": 1527.16, "end": 1531.48, "text": " why call it the transjective, this betweenness, this connectedness, that you see in", "tokens": [50364, 983, 818, 309, 264, 1145, 1020, 488, 11, 341, 1296, 1287, 11, 341, 4582, 1287, 11, 300, 291, 536, 294, 50580], "temperature": 0.0, "avg_logprob": -0.13923129215035387, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.0023960124235600233}, {"id": 259, "seek": 152716, "start": 1531.48, "end": 1536.92, "text": " adeptivity, you see in relevance, but you also see it, like you said. Now, what do I think", "tokens": [50580, 614, 5250, 4253, 11, 291, 536, 294, 32684, 11, 457, 291, 611, 536, 309, 11, 411, 291, 848, 13, 823, 11, 437, 360, 286, 519, 50852], "temperature": 0.0, "avg_logprob": -0.13923129215035387, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.0023960124235600233}, {"id": 260, "seek": 152716, "start": 1536.92, "end": 1544.3600000000001, "text": " the predictive processing did? If you'll allow me, I want to talk back and forth symmetrical,", "tokens": [50852, 264, 35521, 9007, 630, 30, 759, 291, 603, 2089, 385, 11, 286, 528, 281, 751, 646, 293, 5220, 40360, 11, 51224], "temperature": 0.0, "avg_logprob": -0.13923129215035387, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.0023960124235600233}, {"id": 261, "seek": 152716, "start": 1544.3600000000001, "end": 1551.24, "text": " not just linear. So I think what the predictive processing does is emphasize the need to try", "tokens": [51224, 406, 445, 8213, 13, 407, 286, 519, 437, 264, 35521, 9007, 775, 307, 16078, 264, 643, 281, 853, 51568], "temperature": 0.0, "avg_logprob": -0.13923129215035387, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.0023960124235600233}, {"id": 262, "seek": 155124, "start": 1551.32, "end": 1559.56, "text": " and explain, rather than just... I don't want to be cruel. So I'm being a little bit oversimplistic.", "tokens": [50368, 293, 2903, 11, 2831, 813, 445, 485, 286, 500, 380, 528, 281, 312, 16022, 13, 407, 286, 478, 885, 257, 707, 857, 15488, 332, 564, 3142, 13, 50780], "temperature": 0.0, "avg_logprob": -0.14472665699250106, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.050306763499975204}, {"id": 263, "seek": 155124, "start": 1559.56, "end": 1565.72, "text": " I'm doing that just for speed. But generally, it's like, yeah, but how do affordances get actualized", "tokens": [50780, 286, 478, 884, 300, 445, 337, 3073, 13, 583, 5101, 11, 309, 311, 411, 11, 1338, 11, 457, 577, 360, 6157, 2676, 483, 3539, 1602, 51088], "temperature": 0.0, "avg_logprob": -0.14472665699250106, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.050306763499975204}, {"id": 264, "seek": 155124, "start": 1565.72, "end": 1571.72, "text": " in the Ford E-caller? Like, there's a lot of affordances all the time. And what I'm interested", "tokens": [51088, 294, 264, 11961, 462, 12, 66, 22414, 30, 1743, 11, 456, 311, 257, 688, 295, 6157, 2676, 439, 264, 565, 13, 400, 437, 286, 478, 3102, 51388], "temperature": 0.0, "avg_logprob": -0.14472665699250106, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.050306763499975204}, {"id": 265, "seek": 155124, "start": 1571.72, "end": 1577.48, "text": " is, how do those affordances become, well, salient and obvious to me, so they become", "tokens": [51388, 307, 11, 577, 360, 729, 6157, 2676, 1813, 11, 731, 11, 1845, 1196, 293, 6322, 281, 385, 11, 370, 436, 1813, 51676], "temperature": 0.0, "avg_logprob": -0.14472665699250106, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.050306763499975204}, {"id": 266, "seek": 157748, "start": 1577.48, "end": 1585.32, "text": " binding on my sensory motor behavior. And that was always something of, not that there's Redfield", "tokens": [50364, 17359, 322, 452, 27233, 5932, 5223, 13, 400, 300, 390, 1009, 746, 295, 11, 406, 300, 456, 311, 4477, 7610, 50756], "temperature": 0.0, "avg_logprob": -0.14636161016381305, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.009704860858619213}, {"id": 267, "seek": 157748, "start": 1585.32, "end": 1590.6, "text": " and others who are doing some work on it, but typically they start to invoke predictive processing", "tokens": [50756, 293, 2357, 567, 366, 884, 512, 589, 322, 309, 11, 457, 5850, 436, 722, 281, 41117, 35521, 9007, 51020], "temperature": 0.0, "avg_logprob": -0.14636161016381305, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.009704860858619213}, {"id": 268, "seek": 157748, "start": 1590.6, "end": 1597.08, "text": " to try and explain that mechanism of, okay, there's a huge affordance network, but I'm not...", "tokens": [51020, 281, 853, 293, 2903, 300, 7513, 295, 11, 1392, 11, 456, 311, 257, 2603, 6157, 719, 3209, 11, 457, 286, 478, 406, 485, 51344], "temperature": 0.0, "avg_logprob": -0.14636161016381305, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.009704860858619213}, {"id": 269, "seek": 157748, "start": 1598.76, "end": 1606.44, "text": " Here we go again. How do I select and actualize the ones that actually go into my salience", "tokens": [51428, 1692, 321, 352, 797, 13, 1012, 360, 286, 3048, 293, 3539, 1125, 264, 2306, 300, 767, 352, 666, 452, 1845, 1182, 51812], "temperature": 0.0, "avg_logprob": -0.14636161016381305, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.009704860858619213}, {"id": 270, "seek": 160644, "start": 1606.44, "end": 1611.8, "text": " landscaping? And I think predictive processing, especially with the notion of precision waiting,", "tokens": [50364, 23865, 3381, 30, 400, 286, 519, 35521, 9007, 11, 2318, 365, 264, 10710, 295, 18356, 3806, 11, 50632], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 271, "seek": 160644, "start": 1611.8, "end": 1616.6000000000001, "text": " is a good job of that. And what I think relevance realization helps to do to predictive processing,", "tokens": [50632, 307, 257, 665, 1691, 295, 300, 13, 400, 437, 286, 519, 32684, 25138, 3665, 281, 360, 281, 35521, 9007, 11, 50872], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 272, "seek": 160644, "start": 1616.6000000000001, "end": 1621.48, "text": " a bunch of other things. But one thing is, predictive processing, basically, there's subtlety", "tokens": [50872, 257, 3840, 295, 661, 721, 13, 583, 472, 551, 307, 11, 35521, 9007, 11, 1936, 11, 456, 311, 7257, 75, 2210, 51116], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 273, "seek": 160644, "start": 1621.48, "end": 1624.92, "text": " to this, and I know there's lots of mouth, but just to keep going, right, that, you know, well,", "tokens": [51116, 281, 341, 11, 293, 286, 458, 456, 311, 3195, 295, 4525, 11, 457, 445, 281, 1066, 516, 11, 558, 11, 300, 11, 291, 458, 11, 731, 11, 51288], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 274, "seek": 160644, "start": 1624.92, "end": 1629.24, "text": " what attention is, is the precision waiting? It's this meta function. And I think that's a", "tokens": [51288, 437, 3202, 307, 11, 307, 264, 18356, 3806, 30, 467, 311, 341, 19616, 2445, 13, 400, 286, 519, 300, 311, 257, 51504], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 275, "seek": 160644, "start": 1629.24, "end": 1633.56, "text": " very good argument, but there's a conceptual analysis that's been needed, but it's like,", "tokens": [51504, 588, 665, 6770, 11, 457, 456, 311, 257, 24106, 5215, 300, 311, 668, 2978, 11, 457, 309, 311, 411, 11, 51720], "temperature": 0.0, "avg_logprob": -0.13680686950683593, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.003944404888898134}, {"id": 276, "seek": 163356, "start": 1633.6399999999999, "end": 1638.52, "text": " but what is salience? And one of the arguments I can make is, well, what the precision waiting is", "tokens": [50368, 457, 437, 307, 1845, 1182, 30, 400, 472, 295, 264, 12869, 286, 393, 652, 307, 11, 731, 11, 437, 264, 18356, 3806, 307, 50612], "temperature": 0.0, "avg_logprob": -0.08579797843067917, "compression_ratio": 1.8464566929133859, "no_speech_prob": 0.0023227750789374113}, {"id": 277, "seek": 163356, "start": 1638.52, "end": 1644.52, "text": " doing is relevance realization. And relevance realization, when relevance realization does", "tokens": [50612, 884, 307, 32684, 25138, 13, 400, 32684, 25138, 11, 562, 32684, 25138, 775, 50912], "temperature": 0.0, "avg_logprob": -0.08579797843067917, "compression_ratio": 1.8464566929133859, "no_speech_prob": 0.0023227750789374113}, {"id": 278, "seek": 163356, "start": 1644.52, "end": 1650.44, "text": " a higher order finding relevant of an affordance that has been generated by lower order relevance", "tokens": [50912, 257, 2946, 1668, 5006, 7340, 295, 364, 6157, 719, 300, 575, 668, 10833, 538, 3126, 1668, 32684, 51208], "temperature": 0.0, "avg_logprob": -0.08579797843067917, "compression_ratio": 1.8464566929133859, "no_speech_prob": 0.0023227750789374113}, {"id": 279, "seek": 163356, "start": 1650.44, "end": 1654.6, "text": " realization, that's when something becomes salient to us. And so we can actually give a", "tokens": [51208, 25138, 11, 300, 311, 562, 746, 3643, 1845, 1196, 281, 505, 13, 400, 370, 321, 393, 767, 976, 257, 51416], "temperature": 0.0, "avg_logprob": -0.08579797843067917, "compression_ratio": 1.8464566929133859, "no_speech_prob": 0.0023227750789374113}, {"id": 280, "seek": 163356, "start": 1654.6, "end": 1660.6799999999998, "text": " conceptual explanation that goes with the theoretical identity claim in predictive processing,", "tokens": [51416, 24106, 10835, 300, 1709, 365, 264, 20864, 6575, 3932, 294, 35521, 9007, 11, 51720], "temperature": 0.0, "avg_logprob": -0.08579797843067917, "compression_ratio": 1.8464566929133859, "no_speech_prob": 0.0023227750789374113}, {"id": 281, "seek": 166068, "start": 1660.68, "end": 1665.4, "text": " the precision waiting is attention, and what it gives you is salience. So they can, they can", "tokens": [50364, 264, 18356, 3806, 307, 3202, 11, 293, 437, 309, 2709, 291, 307, 1845, 1182, 13, 407, 436, 393, 11, 436, 393, 50600], "temperature": 0.0, "avg_logprob": -0.2229056676228841, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.002322488697245717}, {"id": 282, "seek": 166068, "start": 1665.4, "end": 1673.5600000000002, "text": " unpack each other. I think what another thing that happens is, relevance realization is deeply", "tokens": [50600, 26699, 1184, 661, 13, 286, 519, 437, 1071, 551, 300, 2314, 307, 11, 32684, 25138, 307, 8760, 51008], "temperature": 0.0, "avg_logprob": -0.2229056676228841, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.002322488697245717}, {"id": 283, "seek": 166068, "start": 1673.5600000000002, "end": 1678.1200000000001, "text": " connected to foreecogside, because relevance is grounded, I would argue, in auto-polisis.", "tokens": [51008, 4582, 281, 2091, 3045, 664, 1812, 11, 570, 32684, 307, 23535, 11, 286, 576, 9695, 11, 294, 8399, 12, 12892, 271, 271, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2229056676228841, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.002322488697245717}, {"id": 284, "seek": 166068, "start": 1679.4, "end": 1686.8400000000001, "text": " Relevance, relevance is always relevant to an organism, right? And so that means,", "tokens": [51300, 1300, 28316, 719, 11, 32684, 307, 1009, 7340, 281, 364, 24128, 11, 558, 30, 400, 370, 300, 1355, 11, 51672], "temperature": 0.0, "avg_logprob": -0.2229056676228841, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.002322488697245717}, {"id": 285, "seek": 168684, "start": 1686.84, "end": 1691.0, "text": " relevance realization is a cold calculation. It's how the organism cares about some more", "tokens": [50364, 32684, 25138, 307, 257, 3554, 17108, 13, 467, 311, 577, 264, 24128, 12310, 466, 512, 544, 50572], "temperature": 0.0, "avg_logprob": -0.1188279187904214, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.002182034542784095}, {"id": 286, "seek": 168684, "start": 1691.0, "end": 1696.04, "text": " information rather than all the other information, because it is constantly taking care of itself.", "tokens": [50572, 1589, 2831, 813, 439, 264, 661, 1589, 11, 570, 309, 307, 6460, 1940, 1127, 295, 2564, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1188279187904214, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.002182034542784095}, {"id": 287, "seek": 168684, "start": 1696.04, "end": 1700.4399999999998, "text": " There are literally, and I mean this physically, things that matter to it, and things that it", "tokens": [50824, 821, 366, 3736, 11, 293, 286, 914, 341, 9762, 11, 721, 300, 1871, 281, 309, 11, 293, 721, 300, 309, 51044], "temperature": 0.0, "avg_logprob": -0.1188279187904214, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.002182034542784095}, {"id": 288, "seek": 168684, "start": 1700.4399999999998, "end": 1706.36, "text": " must import into itself, important things. And some of that's also information. And so", "tokens": [51044, 1633, 974, 666, 2564, 11, 1021, 721, 13, 400, 512, 295, 300, 311, 611, 1589, 13, 400, 370, 51340], "temperature": 0.0, "avg_logprob": -0.1188279187904214, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.002182034542784095}, {"id": 289, "seek": 168684, "start": 1707.3999999999999, "end": 1713.72, "text": " why that, if you, if you can, if you can glue foreecogside and predictive processing together,", "tokens": [51392, 983, 300, 11, 498, 291, 11, 498, 291, 393, 11, 498, 291, 393, 8998, 2091, 3045, 664, 1812, 293, 35521, 9007, 1214, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1188279187904214, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.002182034542784095}, {"id": 290, "seek": 171372, "start": 1713.72, "end": 1718.68, "text": " I think you can answer some of the questions, maybe even challenges, that some people in", "tokens": [50364, 286, 519, 291, 393, 1867, 512, 295, 264, 1651, 11, 1310, 754, 4759, 11, 300, 512, 561, 294, 50612], "temperature": 0.0, "avg_logprob": -0.09435335795084636, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.003322192234918475}, {"id": 291, "seek": 171372, "start": 1718.68, "end": 1723.4, "text": " foreecogside are making of predictive processing and saying, well it's not really connected to", "tokens": [50612, 2091, 3045, 664, 1812, 366, 1455, 295, 35521, 9007, 293, 1566, 11, 731, 309, 311, 406, 534, 4582, 281, 50848], "temperature": 0.0, "avg_logprob": -0.09435335795084636, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.003322192234918475}, {"id": 292, "seek": 171372, "start": 1723.4, "end": 1729.8, "text": " embodiment. I think, no, no, if you show a deep theoretical integration of predictive processing", "tokens": [50848, 28935, 2328, 13, 286, 519, 11, 572, 11, 572, 11, 498, 291, 855, 257, 2452, 20864, 10980, 295, 35521, 9007, 51168], "temperature": 0.0, "avg_logprob": -0.09435335795084636, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.003322192234918475}, {"id": 293, "seek": 171372, "start": 1729.8, "end": 1734.1200000000001, "text": " and relevance realization, and then relevance realization and auto-polisis, then you've really", "tokens": [51168, 293, 32684, 25138, 11, 293, 550, 32684, 25138, 293, 8399, 12, 12892, 271, 271, 11, 550, 291, 600, 534, 51384], "temperature": 0.0, "avg_logprob": -0.09435335795084636, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.003322192234918475}, {"id": 294, "seek": 171372, "start": 1734.1200000000001, "end": 1738.76, "text": " strongly glued predictive processing and foreecogside together. So that's some of the", "tokens": [51384, 10613, 28008, 35521, 9007, 293, 2091, 3045, 664, 1812, 1214, 13, 407, 300, 311, 512, 295, 264, 51616], "temperature": 0.0, "avg_logprob": -0.09435335795084636, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.003322192234918475}, {"id": 295, "seek": 173876, "start": 1739.72, "end": 1745.56, "text": " important theoretical work that you've done. Yeah, absolutely. I would love to jump into", "tokens": [50412, 1021, 20864, 589, 300, 291, 600, 1096, 13, 865, 11, 3122, 13, 286, 576, 959, 281, 3012, 666, 50704], "temperature": 0.0, "avg_logprob": -0.1858033579449321, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0036815947387367487}, {"id": 296, "seek": 173876, "start": 1746.52, "end": 1753.24, "text": " the critiques that certain foree cognitive research, cognition researchers have leveraged", "tokens": [50752, 264, 3113, 4911, 300, 1629, 2091, 68, 15605, 2132, 11, 46905, 10309, 362, 12451, 2980, 51088], "temperature": 0.0, "avg_logprob": -0.1858033579449321, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0036815947387367487}, {"id": 297, "seek": 173876, "start": 1753.24, "end": 1759.24, "text": " at predictive processing, but let's leave that for a second. This is exactly what I was talking", "tokens": [51088, 412, 35521, 9007, 11, 457, 718, 311, 1856, 300, 337, 257, 1150, 13, 639, 307, 2293, 437, 286, 390, 1417, 51388], "temperature": 0.0, "avg_logprob": -0.1858033579449321, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0036815947387367487}, {"id": 298, "seek": 173876, "start": 1759.24, "end": 1765.8, "text": " about in terms of telos, which is that exactly this notion of care. And when I spoke to Mara", "tokens": [51388, 466, 294, 2115, 295, 15284, 329, 11, 597, 307, 300, 2293, 341, 10710, 295, 1127, 13, 400, 562, 286, 7179, 281, 2039, 64, 51716], "temperature": 0.0, "avg_logprob": -0.1858033579449321, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0036815947387367487}, {"id": 299, "seek": 176580, "start": 1765.8, "end": 1772.12, "text": " Albaras in an active inference researcher last week, we kind of actually ended up concluding", "tokens": [50364, 967, 5356, 296, 294, 364, 4967, 38253, 21751, 1036, 1243, 11, 321, 733, 295, 767, 4590, 493, 9312, 278, 50680], "temperature": 0.0, "avg_logprob": -0.25013231462047947, "compression_ratio": 1.6618181818181819, "no_speech_prob": 0.011667679063975811}, {"id": 300, "seek": 176580, "start": 1772.12, "end": 1776.6, "text": " that this is almost the Heideggerian sense of care that we take from our existence.", "tokens": [50680, 300, 341, 307, 1920, 264, 634, 482, 39381, 952, 2020, 295, 1127, 300, 321, 747, 490, 527, 9123, 13, 50904], "temperature": 0.0, "avg_logprob": -0.25013231462047947, "compression_ratio": 1.6618181818181819, "no_speech_prob": 0.011667679063975811}, {"id": 301, "seek": 176580, "start": 1777.32, "end": 1781.8, "text": " Can I, can I just interrupt there? Yeah. Right. Heideggerian sense of care was exactly what I", "tokens": [50940, 1664, 286, 11, 393, 286, 445, 12729, 456, 30, 865, 13, 1779, 13, 634, 482, 39381, 952, 2020, 295, 1127, 390, 2293, 437, 286, 51164], "temperature": 0.0, "avg_logprob": -0.25013231462047947, "compression_ratio": 1.6618181818181819, "no_speech_prob": 0.011667679063975811}, {"id": 302, "seek": 176580, "start": 1781.8, "end": 1788.28, "text": " was invoking. And I, right, and I was deeply influenced by Dreyfus and the frame problem,", "tokens": [51164, 390, 1048, 5953, 13, 400, 286, 11, 558, 11, 293, 286, 390, 8760, 15269, 538, 413, 7950, 69, 301, 293, 264, 3920, 1154, 11, 51488], "temperature": 0.0, "avg_logprob": -0.25013231462047947, "compression_ratio": 1.6618181818181819, "no_speech_prob": 0.011667679063975811}, {"id": 303, "seek": 176580, "start": 1788.28, "end": 1795.1599999999999, "text": " got this from Heidegger. Yeah. So the connection you just drew is very, I, I, yeah, I explicitly", "tokens": [51488, 658, 341, 490, 634, 482, 39381, 13, 865, 13, 407, 264, 4984, 291, 445, 12804, 307, 588, 11, 286, 11, 286, 11, 1338, 11, 286, 20803, 51832], "temperature": 0.0, "avg_logprob": -0.25013231462047947, "compression_ratio": 1.6618181818181819, "no_speech_prob": 0.011667679063975811}, {"id": 304, "seek": 179516, "start": 1795.16, "end": 1800.92, "text": " argue for that. Excellent. So yeah, I mean, I personally think that the Heidegger, Heideggerian,", "tokens": [50364, 9695, 337, 300, 13, 16723, 13, 407, 1338, 11, 286, 914, 11, 286, 5665, 519, 300, 264, 634, 482, 39381, 11, 634, 482, 39381, 952, 11, 50652], "temperature": 0.0, "avg_logprob": -0.1340719759464264, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0011044320417568088}, {"id": 305, "seek": 179516, "start": 1800.92, "end": 1806.3600000000001, "text": " Dreyfus, Merleau Ponty lineage needs to be integrated more into active inference. And I'm", "tokens": [50652, 413, 7950, 69, 301, 11, 6124, 306, 1459, 31756, 874, 38257, 2203, 281, 312, 10919, 544, 666, 4967, 38253, 13, 400, 286, 478, 50924], "temperature": 0.0, "avg_logprob": -0.1340719759464264, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0011044320417568088}, {"id": 306, "seek": 179516, "start": 1806.3600000000001, "end": 1812.1200000000001, "text": " speaking to Dr. Marilyn Stendera, who has done work on Heidegger and auto-poesis. So that will be", "tokens": [50924, 4124, 281, 2491, 13, 48340, 745, 521, 1663, 11, 567, 575, 1096, 589, 322, 634, 482, 39381, 293, 8399, 12, 2259, 9374, 13, 407, 300, 486, 312, 51212], "temperature": 0.0, "avg_logprob": -0.1340719759464264, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0011044320417568088}, {"id": 307, "seek": 179516, "start": 1812.1200000000001, "end": 1815.3200000000002, "text": " really, that will be really fun in the upcoming weeks. This is exactly what I was talking about", "tokens": [51212, 534, 11, 300, 486, 312, 534, 1019, 294, 264, 11500, 3259, 13, 639, 307, 2293, 437, 286, 390, 1417, 466, 51372], "temperature": 0.0, "avg_logprob": -0.1340719759464264, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0011044320417568088}, {"id": 308, "seek": 179516, "start": 1815.3200000000002, "end": 1819.96, "text": " in terms of telos, which is that taking a stance on your own existence can be computationally", "tokens": [51372, 294, 2115, 295, 15284, 329, 11, 597, 307, 300, 1940, 257, 21033, 322, 428, 1065, 9123, 393, 312, 24903, 379, 51604], "temperature": 0.0, "avg_logprob": -0.1340719759464264, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0011044320417568088}, {"id": 309, "seek": 181996, "start": 1819.96, "end": 1826.76, "text": " modeled as having these kind of high precision priors. So our homeostatic set point will be", "tokens": [50364, 37140, 382, 1419, 613, 733, 295, 1090, 18356, 1790, 830, 13, 407, 527, 1280, 555, 2399, 992, 935, 486, 312, 50704], "temperature": 0.0, "avg_logprob": -0.10361662698448251, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.035058535635471344}, {"id": 310, "seek": 181996, "start": 1826.76, "end": 1831.56, "text": " different from a snake's. And that actually gives a really solid explication of why we act", "tokens": [50704, 819, 490, 257, 12650, 311, 13, 400, 300, 767, 2709, 257, 534, 5100, 1490, 8758, 295, 983, 321, 605, 50944], "temperature": 0.0, "avg_logprob": -0.10361662698448251, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.035058535635471344}, {"id": 311, "seek": 181996, "start": 1831.56, "end": 1836.76, "text": " differently in certain contexts than snakes. And so that was my kind. But then that said,", "tokens": [50944, 7614, 294, 1629, 30628, 813, 21817, 13, 400, 370, 300, 390, 452, 733, 13, 583, 550, 300, 848, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10361662698448251, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.035058535635471344}, {"id": 312, "seek": 181996, "start": 1836.76, "end": 1842.8400000000001, "text": " I will have to make clear that Carl is in himself, although he has spoken about existential imperative", "tokens": [51204, 286, 486, 362, 281, 652, 1850, 300, 14256, 307, 294, 3647, 11, 4878, 415, 575, 10759, 466, 37133, 32490, 51508], "temperature": 0.0, "avg_logprob": -0.10361662698448251, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.035058535635471344}, {"id": 313, "seek": 181996, "start": 1842.8400000000001, "end": 1848.2, "text": " says that there actually is no imperative. All that you're really saying is if a thing like us", "tokens": [51508, 1619, 300, 456, 767, 307, 572, 32490, 13, 1057, 300, 291, 434, 534, 1566, 307, 498, 257, 551, 411, 505, 51776], "temperature": 0.0, "avg_logprob": -0.10361662698448251, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.035058535635471344}, {"id": 314, "seek": 184820, "start": 1848.2, "end": 1853.32, "text": " is to persist over time, this is what it needs to do. And it's just a, it's just cast as a", "tokens": [50364, 307, 281, 13233, 670, 565, 11, 341, 307, 437, 309, 2203, 281, 360, 13, 400, 309, 311, 445, 257, 11, 309, 311, 445, 4193, 382, 257, 50620], "temperature": 0.0, "avg_logprob": -0.10365068298025229, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004893423989415169}, {"id": 315, "seek": 184820, "start": 1853.32, "end": 1858.92, "text": " minimization of free energy. So I think there's an interesting question there that maybe you", "tokens": [50620, 4464, 2144, 295, 1737, 2281, 13, 407, 286, 519, 456, 311, 364, 1880, 1168, 456, 300, 1310, 291, 50900], "temperature": 0.0, "avg_logprob": -0.10365068298025229, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004893423989415169}, {"id": 316, "seek": 184820, "start": 1858.92, "end": 1864.6000000000001, "text": " could have a go at, which is to what degree do you see this as a genuine imperative versus", "tokens": [50900, 727, 362, 257, 352, 412, 11, 597, 307, 281, 437, 4314, 360, 291, 536, 341, 382, 257, 16699, 32490, 5717, 51184], "temperature": 0.0, "avg_logprob": -0.10365068298025229, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004893423989415169}, {"id": 317, "seek": 184820, "start": 1864.6000000000001, "end": 1872.2, "text": " just what things do to self-organize over time? Yeah. And so I mean, and Carl's no", "tokens": [51184, 445, 437, 721, 360, 281, 2698, 12, 12372, 1125, 670, 565, 30, 865, 13, 400, 370, 286, 914, 11, 293, 14256, 311, 572, 51564], "temperature": 0.0, "avg_logprob": -0.10365068298025229, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004893423989415169}, {"id": 318, "seek": 187220, "start": 1872.2, "end": 1878.04, "text": " slope, so he's welcome. I sometimes don't like it when people who, you know, are physicists or", "tokens": [50364, 13525, 11, 370, 415, 311, 2928, 13, 286, 2171, 500, 380, 411, 309, 562, 561, 567, 11, 291, 458, 11, 366, 48716, 420, 50656], "temperature": 0.0, "avg_logprob": -0.1313830722462047, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.22503112256526947}, {"id": 319, "seek": 187220, "start": 1878.04, "end": 1883.56, "text": " something start commenting on philosophical normative questions with an authority they", "tokens": [50656, 746, 722, 29590, 322, 25066, 2026, 1166, 1651, 365, 364, 8281, 436, 50932], "temperature": 0.0, "avg_logprob": -0.1313830722462047, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.22503112256526947}, {"id": 320, "seek": 187220, "start": 1883.56, "end": 1890.92, "text": " don't properly possess. But I know Carl's a great theorist. And I know he's philosophically educated.", "tokens": [50932, 500, 380, 6108, 17490, 13, 583, 286, 458, 14256, 311, 257, 869, 27423, 468, 13, 400, 286, 458, 415, 311, 14529, 984, 15872, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1313830722462047, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.22503112256526947}, {"id": 321, "seek": 187220, "start": 1891.64, "end": 1894.52, "text": " I have not yet to talk to him, but I hope I do get to do so.", "tokens": [51336, 286, 362, 406, 1939, 281, 751, 281, 796, 11, 457, 286, 1454, 286, 360, 483, 281, 360, 370, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1313830722462047, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.22503112256526947}, {"id": 322, "seek": 189452, "start": 1894.6, "end": 1904.12, "text": " We can do ahead. So yeah, I mean, so I'm going to argue later if I get a chance that what", "tokens": [50368, 492, 393, 360, 2286, 13, 407, 1338, 11, 286, 914, 11, 370, 286, 478, 516, 281, 9695, 1780, 498, 286, 483, 257, 2931, 300, 437, 50844], "temperature": 0.0, "avg_logprob": -0.17137566066923596, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.02885429933667183}, {"id": 323, "seek": 189452, "start": 1904.12, "end": 1910.44, "text": " relevance realization doing is strongly analogous, obviously a very different timescale to what", "tokens": [50844, 32684, 25138, 884, 307, 10613, 16660, 563, 11, 2745, 257, 588, 819, 1413, 37088, 281, 437, 51160], "temperature": 0.0, "avg_logprob": -0.17137566066923596, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.02885429933667183}, {"id": 324, "seek": 189452, "start": 1910.44, "end": 1917.48, "text": " evolution does about constantly redesigning the adapted fittedness of the, of organisms to the", "tokens": [51160, 9303, 775, 466, 6460, 16762, 9676, 264, 20871, 26321, 1287, 295, 264, 11, 295, 22110, 281, 264, 51512], "temperature": 0.0, "avg_logprob": -0.17137566066923596, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.02885429933667183}, {"id": 325, "seek": 189452, "start": 1917.48, "end": 1922.52, "text": " environment and allowing organisms to fit their environments to them. Because as you keep emphasizing", "tokens": [51512, 2823, 293, 8293, 22110, 281, 3318, 641, 12388, 281, 552, 13, 1436, 382, 291, 1066, 45550, 51764], "temperature": 0.0, "avg_logprob": -0.17137566066923596, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.02885429933667183}, {"id": 326, "seek": 192252, "start": 1922.52, "end": 1927.72, "text": " and I keep agreeing with, it's not passive. It's not right. The organism is shaping the", "tokens": [50364, 293, 286, 1066, 36900, 365, 11, 309, 311, 406, 14975, 13, 467, 311, 406, 558, 13, 440, 24128, 307, 25945, 264, 50624], "temperature": 0.0, "avg_logprob": -0.14184733094840213, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.004751279950141907}, {"id": 327, "seek": 192252, "start": 1927.72, "end": 1931.48, "text": " environment as the environment is shaping the organism, niche construction and all that sort", "tokens": [50624, 2823, 382, 264, 2823, 307, 25945, 264, 24128, 11, 19956, 6435, 293, 439, 300, 1333, 50812], "temperature": 0.0, "avg_logprob": -0.14184733094840213, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.004751279950141907}, {"id": 328, "seek": 192252, "start": 1931.48, "end": 1936.84, "text": " of thing. And I think that's all right. And then there again, that's a very clear connection to", "tokens": [50812, 295, 551, 13, 400, 286, 519, 300, 311, 439, 558, 13, 400, 550, 456, 797, 11, 300, 311, 257, 588, 1850, 4984, 281, 51080], "temperature": 0.0, "avg_logprob": -0.14184733094840213, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.004751279950141907}, {"id": 329, "seek": 192252, "start": 1936.84, "end": 1941.96, "text": " 40 cogside again. But this deeper question, I do want to do it. I want to pause and I want to", "tokens": [51080, 3356, 598, 21559, 482, 797, 13, 583, 341, 7731, 1168, 11, 286, 360, 528, 281, 360, 309, 13, 286, 528, 281, 10465, 293, 286, 528, 281, 51336], "temperature": 0.0, "avg_logprob": -0.14184733094840213, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.004751279950141907}, {"id": 330, "seek": 192252, "start": 1941.96, "end": 1950.28, "text": " slow down because it is a really important question. Because it gets us beyond what we might call", "tokens": [51336, 2964, 760, 570, 309, 307, 257, 534, 1021, 1168, 13, 1436, 309, 2170, 505, 4399, 437, 321, 1062, 818, 51752], "temperature": 0.0, "avg_logprob": -0.14184733094840213, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.004751279950141907}, {"id": 331, "seek": 195028, "start": 1950.28, "end": 1958.44, "text": " scientific explanations of how and into properly, but not useless, philosophical reflection of sort", "tokens": [50364, 8134, 28708, 295, 577, 293, 666, 6108, 11, 457, 406, 14115, 11, 25066, 12914, 295, 1333, 50772], "temperature": 0.0, "avg_logprob": -0.1280528267661294, "compression_ratio": 1.56, "no_speech_prob": 0.0062853069975972176}, {"id": 332, "seek": 195028, "start": 1958.44, "end": 1964.36, "text": " of why. And why does this matter to me? Because as you've mentioned, I'm deeply concerned with,", "tokens": [50772, 295, 983, 13, 400, 983, 775, 341, 1871, 281, 385, 30, 1436, 382, 291, 600, 2835, 11, 286, 478, 8760, 5922, 365, 11, 51068], "temperature": 0.0, "avg_logprob": -0.1280528267661294, "compression_ratio": 1.56, "no_speech_prob": 0.0062853069975972176}, {"id": 333, "seek": 195028, "start": 1965.0, "end": 1972.12, "text": " you know, issues of meaning in life. This, this, this largely metaphorical, nebulous notion that", "tokens": [51100, 291, 458, 11, 2663, 295, 3620, 294, 993, 13, 639, 11, 341, 11, 341, 11611, 19157, 804, 11, 408, 65, 6893, 10710, 300, 51456], "temperature": 0.0, "avg_logprob": -0.1280528267661294, "compression_ratio": 1.56, "no_speech_prob": 0.0062853069975972176}, {"id": 334, "seek": 195028, "start": 1972.12, "end": 1979.16, "text": " humans seek meaningful lives, lives that are worth living, even given all of our failures and our", "tokens": [51456, 6255, 8075, 10995, 2909, 11, 2909, 300, 366, 3163, 2647, 11, 754, 2212, 439, 295, 527, 20774, 293, 527, 51808], "temperature": 0.0, "avg_logprob": -0.1280528267661294, "compression_ratio": 1.56, "no_speech_prob": 0.0062853069975972176}, {"id": 335, "seek": 197916, "start": 1979.16, "end": 1985.16, "text": " faults and our flaws and our foolishness and our frustration, right? And pretty clear empirical", "tokens": [50364, 36090, 293, 527, 27108, 293, 527, 23478, 1287, 293, 527, 20491, 11, 558, 30, 400, 1238, 1850, 31886, 50664], "temperature": 0.0, "avg_logprob": -0.12493023438887163, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0007668676553294063}, {"id": 336, "seek": 197916, "start": 1985.16, "end": 1992.44, "text": " evidence that it's not reducible to just subjective well-being or pleasure and pretty good conceptual", "tokens": [50664, 4467, 300, 309, 311, 406, 2783, 32128, 281, 445, 25972, 731, 12, 13054, 420, 6834, 293, 1238, 665, 24106, 51028], "temperature": 0.0, "avg_logprob": -0.12493023438887163, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0007668676553294063}, {"id": 337, "seek": 197916, "start": 1992.44, "end": 1996.68, "text": " philosophical argument, because that's where it's relevant, that it's not reducible to just living", "tokens": [51028, 25066, 6770, 11, 570, 300, 311, 689, 309, 311, 7340, 11, 300, 309, 311, 406, 2783, 32128, 281, 445, 2647, 51240], "temperature": 0.0, "avg_logprob": -0.12493023438887163, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0007668676553294063}, {"id": 338, "seek": 197916, "start": 1996.68, "end": 2001.0, "text": " a moral existence. You could live a very moral existence in which you're sort of experiencing", "tokens": [51240, 257, 9723, 9123, 13, 509, 727, 1621, 257, 588, 9723, 9123, 294, 597, 291, 434, 1333, 295, 11139, 51456], "temperature": 0.0, "avg_logprob": -0.12493023438887163, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0007668676553294063}, {"id": 339, "seek": 197916, "start": 2001.0, "end": 2007.16, "text": " the pleasure of food and other things. And, and, and you've got, you know, certain stability in", "tokens": [51456, 264, 6834, 295, 1755, 293, 661, 721, 13, 400, 11, 293, 11, 293, 291, 600, 658, 11, 291, 458, 11, 1629, 11826, 294, 51764], "temperature": 0.0, "avg_logprob": -0.12493023438887163, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0007668676553294063}, {"id": 340, "seek": 200716, "start": 2007.16, "end": 2012.2, "text": " your environment that you could be very, very lonely. You could be very lonely. So, and what,", "tokens": [50364, 428, 2823, 300, 291, 727, 312, 588, 11, 588, 14236, 13, 509, 727, 312, 588, 14236, 13, 407, 11, 293, 437, 11, 50616], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 341, "seek": 200716, "start": 2012.2, "end": 2015.72, "text": " what that loneliness points to, I'm just making an intuitive gesture. That's not a tight argument.", "tokens": [50616, 437, 300, 28144, 2793, 281, 11, 286, 478, 445, 1455, 364, 21769, 22252, 13, 663, 311, 406, 257, 4524, 6770, 13, 50792], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 342, "seek": 200716, "start": 2015.72, "end": 2020.92, "text": " I have tight arguments, but the gesture is people are seeking a kind of connectedness. And now you", "tokens": [50792, 286, 362, 4524, 12869, 11, 457, 264, 22252, 307, 561, 366, 11670, 257, 733, 295, 4582, 1287, 13, 400, 586, 291, 51052], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 343, "seek": 200716, "start": 2020.92, "end": 2025.4, "text": " see where I'm getting to relevance realization and what we're talking about this niche construction,", "tokens": [51052, 536, 689, 286, 478, 1242, 281, 32684, 25138, 293, 437, 321, 434, 1417, 466, 341, 19956, 6435, 11, 51276], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 344, "seek": 200716, "start": 2025.4, "end": 2031.0, "text": " the fittedness, the belonging togetherness, this mutually shaping, like this is this,", "tokens": [51276, 264, 26321, 1287, 11, 264, 22957, 1214, 1287, 11, 341, 39144, 25945, 11, 411, 341, 307, 341, 11, 51556], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 345, "seek": 200716, "start": 2031.0, "end": 2036.92, "text": " I argue that that's exactly the connectedness human beings are seeking the meaning in life.", "tokens": [51556, 286, 9695, 300, 300, 311, 2293, 264, 4582, 1287, 1952, 8958, 366, 11670, 264, 3620, 294, 993, 13, 51852], "temperature": 0.0, "avg_logprob": -0.13313392464441198, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0070093427784740925}, {"id": 346, "seek": 203716, "start": 2037.4, "end": 2041.96, "text": " Now, you can then ask the question and people, and I'm going to play with two", "tokens": [50376, 823, 11, 291, 393, 550, 1029, 264, 1168, 293, 561, 11, 293, 286, 478, 516, 281, 862, 365, 732, 50604], "temperature": 0.0, "avg_logprob": -0.10123546992506936, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0006981573533266783}, {"id": 347, "seek": 203716, "start": 2041.96, "end": 2046.44, "text": " terms of phrase here to play with that. I think we clearly seek meaning in life.", "tokens": [50604, 2115, 295, 9535, 510, 281, 862, 365, 300, 13, 286, 519, 321, 4448, 8075, 3620, 294, 993, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10123546992506936, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0006981573533266783}, {"id": 348, "seek": 203716, "start": 2047.88, "end": 2052.52, "text": " I think the evidence for that is growing and it converges with the psychological work on", "tokens": [50900, 286, 519, 264, 4467, 337, 300, 307, 4194, 293, 309, 9652, 2880, 365, 264, 14346, 589, 322, 51132], "temperature": 0.0, "avg_logprob": -0.10123546992506936, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0006981573533266783}, {"id": 349, "seek": 203716, "start": 2052.52, "end": 2058.76, "text": " meaning in life and on what Karen Allen calls belongingness. If you don't feel that you belong,", "tokens": [51132, 3620, 294, 993, 293, 322, 437, 14834, 17160, 5498, 22957, 1287, 13, 759, 291, 500, 380, 841, 300, 291, 5784, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10123546992506936, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0006981573533266783}, {"id": 350, "seek": 203716, "start": 2058.76, "end": 2063.48, "text": " you're in trouble and you're in trouble across all these measures, cognitive, emotional, social,", "tokens": [51444, 291, 434, 294, 5253, 293, 291, 434, 294, 5253, 2108, 439, 613, 8000, 11, 15605, 11, 6863, 11, 2093, 11, 51680], "temperature": 0.0, "avg_logprob": -0.10123546992506936, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0006981573533266783}, {"id": 351, "seek": 206348, "start": 2063.48, "end": 2069.64, "text": " financial, blah, you're in trouble, physiologically you're in trouble. That's why solitary confinement", "tokens": [50364, 4669, 11, 12288, 11, 291, 434, 294, 5253, 11, 21265, 17157, 291, 434, 294, 5253, 13, 663, 311, 983, 44155, 41064, 50672], "temperature": 0.0, "avg_logprob": -0.17398500442504883, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.00113331014290452}, {"id": 352, "seek": 206348, "start": 2069.64, "end": 2077.16, "text": " is such a punishment, right? So, is there a meaning of life to the meaning in life?", "tokens": [50672, 307, 1270, 257, 14133, 11, 558, 30, 407, 11, 307, 456, 257, 3620, 295, 993, 281, 264, 3620, 294, 993, 30, 51048], "temperature": 0.0, "avg_logprob": -0.17398500442504883, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.00113331014290452}, {"id": 353, "seek": 206348, "start": 2078.68, "end": 2087.64, "text": " The meaning of life is that is there some sort of cosmic destiny, cosmic order to which we are", "tokens": [51124, 440, 3620, 295, 993, 307, 300, 307, 456, 512, 1333, 295, 27614, 17893, 11, 27614, 1668, 281, 597, 321, 366, 51572], "temperature": 0.0, "avg_logprob": -0.17398500442504883, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.00113331014290452}, {"id": 354, "seek": 208764, "start": 2088.2799999999997, "end": 2095.0, "text": " which we are ultimately trying to find. And here's the thing, I don't think I have enough evidence", "tokens": [50396, 597, 321, 366, 6284, 1382, 281, 915, 13, 400, 510, 311, 264, 551, 11, 286, 500, 380, 519, 286, 362, 1547, 4467, 50732], "temperature": 0.0, "avg_logprob": -0.11135492220029726, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.02594633214175701}, {"id": 355, "seek": 208764, "start": 2095.0, "end": 2102.04, "text": " for that. I think that there's lots of evidence for meaning in life and if I'm right that it's", "tokens": [50732, 337, 300, 13, 286, 519, 300, 456, 311, 3195, 295, 4467, 337, 3620, 294, 993, 293, 498, 286, 478, 558, 300, 309, 311, 51084], "temperature": 0.0, "avg_logprob": -0.11135492220029726, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.02594633214175701}, {"id": 356, "seek": 208764, "start": 2102.04, "end": 2108.92, "text": " relevance realization and if that's something very much like evolution, right? Evolution is", "tokens": [51084, 32684, 25138, 293, 498, 300, 311, 746, 588, 709, 411, 9303, 11, 558, 30, 40800, 307, 51428], "temperature": 0.0, "avg_logprob": -0.11135492220029726, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.02594633214175701}, {"id": 357, "seek": 208764, "start": 2108.92, "end": 2117.0, "text": " constantly redesigning adeptivity, but there isn't a final thing that evolution is aiming towards.", "tokens": [51428, 6460, 16762, 9676, 614, 5250, 4253, 11, 457, 456, 1943, 380, 257, 2572, 551, 300, 9303, 307, 20253, 3030, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11135492220029726, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.02594633214175701}, {"id": 358, "seek": 211700, "start": 2117.0, "end": 2121.8, "text": " There isn't a final form of life. There isn't some like what we're doing is", "tokens": [50364, 821, 1943, 380, 257, 2572, 1254, 295, 993, 13, 821, 1943, 380, 512, 411, 437, 321, 434, 884, 307, 50604], "temperature": 0.0, "avg_logprob": -0.0938501250877809, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0008555917302146554}, {"id": 359, "seek": 211700, "start": 2122.84, "end": 2128.52, "text": " like the sculptor, we're constantly refining and then finally one day we will have the final", "tokens": [50656, 411, 264, 12613, 284, 11, 321, 434, 6460, 1895, 1760, 293, 550, 2721, 472, 786, 321, 486, 362, 264, 2572, 50940], "temperature": 0.0, "avg_logprob": -0.0938501250877809, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0008555917302146554}, {"id": 360, "seek": 211700, "start": 2128.52, "end": 2135.16, "text": " form of life. And I think this is fundamentally flawed and I think this matters philosophically.", "tokens": [50940, 1254, 295, 993, 13, 400, 286, 519, 341, 307, 17879, 38823, 293, 286, 519, 341, 7001, 14529, 984, 13, 51272], "temperature": 0.0, "avg_logprob": -0.0938501250877809, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0008555917302146554}, {"id": 361, "seek": 211700, "start": 2135.16, "end": 2142.76, "text": " I don't think previous forms of life were in some in any kind of moral sense superior to us.", "tokens": [51272, 286, 500, 380, 519, 3894, 6422, 295, 993, 645, 294, 512, 294, 604, 733, 295, 9723, 2020, 13028, 281, 505, 13, 51652], "temperature": 0.0, "avg_logprob": -0.0938501250877809, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0008555917302146554}, {"id": 362, "seek": 214276, "start": 2142.76, "end": 2148.0400000000004, "text": " So, I don't believe in any nostalgias. Some of the previous things that people found important", "tokens": [50364, 407, 11, 286, 500, 380, 1697, 294, 604, 10397, 20422, 4609, 13, 2188, 295, 264, 3894, 721, 300, 561, 1352, 1021, 50628], "temperature": 0.0, "avg_logprob": -0.0906691501015111, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.017420917749404907}, {"id": 363, "seek": 214276, "start": 2148.0400000000004, "end": 2152.6800000000003, "text": " and relevant and sacred, those were the true ones and we just have to get back there. Nope.", "tokens": [50628, 293, 7340, 293, 15757, 11, 729, 645, 264, 2074, 2306, 293, 321, 445, 362, 281, 483, 646, 456, 13, 12172, 13, 50860], "temperature": 0.0, "avg_logprob": -0.0906691501015111, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.017420917749404907}, {"id": 364, "seek": 214276, "start": 2152.6800000000003, "end": 2158.5200000000004, "text": " And I don't believe the utopia. We are working towards the final ultimate thing that will all", "tokens": [50860, 400, 286, 500, 380, 1697, 264, 2839, 22376, 13, 492, 366, 1364, 3030, 264, 2572, 9705, 551, 300, 486, 439, 51152], "temperature": 0.0, "avg_logprob": -0.0906691501015111, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.017420917749404907}, {"id": 365, "seek": 214276, "start": 2158.5200000000004, "end": 2165.2400000000002, "text": " agree for all time is the most relevant, most salient. I don't believe that's how this works.", "tokens": [51152, 3986, 337, 439, 565, 307, 264, 881, 7340, 11, 881, 1845, 1196, 13, 286, 500, 380, 1697, 300, 311, 577, 341, 1985, 13, 51488], "temperature": 0.0, "avg_logprob": -0.0906691501015111, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.017420917749404907}, {"id": 366, "seek": 216524, "start": 2165.8799999999997, "end": 2173.7999999999997, "text": " So, I reject nostalgias. I reject utopias. The idea that there's a telos can either mean", "tokens": [50396, 407, 11, 286, 8248, 10397, 20422, 4609, 13, 286, 8248, 2839, 404, 4609, 13, 440, 1558, 300, 456, 311, 257, 15284, 329, 393, 2139, 914, 50792], "temperature": 0.0, "avg_logprob": -0.13489354650179544, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.1036757379770279}, {"id": 367, "seek": 216524, "start": 2173.7999999999997, "end": 2179.08, "text": " there's a telos to meaning in life, which is it's like Carl says, it's necessary,", "tokens": [50792, 456, 311, 257, 15284, 329, 281, 3620, 294, 993, 11, 597, 307, 309, 311, 411, 14256, 1619, 11, 309, 311, 4818, 11, 51056], "temperature": 0.0, "avg_logprob": -0.13489354650179544, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.1036757379770279}, {"id": 368, "seek": 216524, "start": 2179.08, "end": 2184.12, "text": " it's constitutive necessary for living things to have this. But does that mean there's any", "tokens": [51056, 309, 311, 23079, 17254, 4818, 337, 2647, 721, 281, 362, 341, 13, 583, 775, 300, 914, 456, 311, 604, 51308], "temperature": 0.0, "avg_logprob": -0.13489354650179544, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.1036757379770279}, {"id": 369, "seek": 216524, "start": 2184.12, "end": 2190.9199999999996, "text": " metaphysical necessity to their being living things? That's a different question. Now, here's my", "tokens": [51308, 30946, 36280, 24217, 281, 641, 885, 2647, 721, 30, 663, 311, 257, 819, 1168, 13, 823, 11, 510, 311, 452, 51648], "temperature": 0.0, "avg_logprob": -0.13489354650179544, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.1036757379770279}, {"id": 370, "seek": 219092, "start": 2190.92, "end": 2196.28, "text": " weak answer at that one. Sorry. Long question, but you asked like this is like almost up there", "tokens": [50364, 5336, 1867, 412, 300, 472, 13, 4919, 13, 8282, 1168, 11, 457, 291, 2351, 411, 341, 307, 411, 1920, 493, 456, 50632], "temperature": 0.0, "avg_logprob": -0.15676952401796976, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.006381368264555931}, {"id": 371, "seek": 219092, "start": 2196.28, "end": 2205.16, "text": " with like God. Yeah. Right. Right. I note that I'm in a very difficult situation. I can't see any", "tokens": [50632, 365, 411, 1265, 13, 865, 13, 1779, 13, 1779, 13, 286, 3637, 300, 286, 478, 294, 257, 588, 2252, 2590, 13, 286, 393, 380, 536, 604, 51076], "temperature": 0.0, "avg_logprob": -0.15676952401796976, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.006381368264555931}, {"id": 372, "seek": 219092, "start": 2205.16, "end": 2211.64, "text": " evidence for and I'm not attacking God here. And as you know, I'm very respectful and even", "tokens": [51076, 4467, 337, 293, 286, 478, 406, 15010, 1265, 510, 13, 400, 382, 291, 458, 11, 286, 478, 588, 26205, 293, 754, 51400], "temperature": 0.0, "avg_logprob": -0.15676952401796976, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.006381368264555931}, {"id": 373, "seek": 219092, "start": 2211.64, "end": 2217.4, "text": " appreciative of religious frameworks and religious lives. But I can't see any evidence for that.", "tokens": [51400, 43239, 295, 7185, 29834, 293, 7185, 2909, 13, 583, 286, 393, 380, 536, 604, 4467, 337, 300, 13, 51688], "temperature": 0.0, "avg_logprob": -0.15676952401796976, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.006381368264555931}, {"id": 374, "seek": 221740, "start": 2217.4, "end": 2224.28, "text": " However, if you were to ask me, which is a better universe, one with life in it and one without", "tokens": [50364, 2908, 11, 498, 291, 645, 281, 1029, 385, 11, 597, 307, 257, 1101, 6445, 11, 472, 365, 993, 294, 309, 293, 472, 1553, 50708], "temperature": 0.0, "avg_logprob": -0.11058700734918768, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0019875026773661375}, {"id": 375, "seek": 221740, "start": 2224.28, "end": 2229.56, "text": " life in it, even if you couldn't exist in either one, I would say the one with life in it. There's", "tokens": [50708, 993, 294, 309, 11, 754, 498, 291, 2809, 380, 2514, 294, 2139, 472, 11, 286, 576, 584, 264, 472, 365, 993, 294, 309, 13, 821, 311, 50972], "temperature": 0.0, "avg_logprob": -0.11058700734918768, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0019875026773661375}, {"id": 376, "seek": 221740, "start": 2229.56, "end": 2236.28, "text": " some primordial judgment going on there that has some kind of metaphysical import. Now, it's weak.", "tokens": [50972, 512, 2886, 765, 831, 12216, 516, 322, 456, 300, 575, 512, 733, 295, 30946, 36280, 974, 13, 823, 11, 309, 311, 5336, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11058700734918768, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0019875026773661375}, {"id": 377, "seek": 221740, "start": 2237.2400000000002, "end": 2243.8, "text": " I admitted that ahead of time. And so I'm giving you a very, I'm sorry, very long but wishy-washy", "tokens": [51356, 286, 14920, 300, 2286, 295, 565, 13, 400, 370, 286, 478, 2902, 291, 257, 588, 11, 286, 478, 2597, 11, 588, 938, 457, 3172, 88, 12, 38558, 88, 51684], "temperature": 0.0, "avg_logprob": -0.11058700734918768, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0019875026773661375}, {"id": 378, "seek": 224380, "start": 2243.8, "end": 2250.04, "text": " answer. I don't really think I believe in anything like the meaning of life, but I do think there's", "tokens": [50364, 1867, 13, 286, 500, 380, 534, 519, 286, 1697, 294, 1340, 411, 264, 3620, 295, 993, 11, 457, 286, 360, 519, 456, 311, 50676], "temperature": 0.0, "avg_logprob": -0.10170596578846806, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.022956807166337967}, {"id": 379, "seek": 224380, "start": 2250.04, "end": 2257.96, "text": " a metaphysical import to being connected to reality. We find reality being connected to reality", "tokens": [50676, 257, 30946, 36280, 974, 281, 885, 4582, 281, 4103, 13, 492, 915, 4103, 885, 4582, 281, 4103, 51072], "temperature": 0.0, "avg_logprob": -0.10170596578846806, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.022956807166337967}, {"id": 380, "seek": 224380, "start": 2257.96, "end": 2262.92, "text": " inherently valuable for its own sake. And we find worlds in which things can realize reality", "tokens": [51072, 27993, 8263, 337, 1080, 1065, 9717, 13, 400, 321, 915, 13401, 294, 597, 721, 393, 4325, 4103, 51320], "temperature": 0.0, "avg_logprob": -0.10170596578846806, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.022956807166337967}, {"id": 381, "seek": 224380, "start": 2263.48, "end": 2269.4, "text": " better worlds than ones that don't have that in it. That's my answer. No, it's a wonderful answer.", "tokens": [51348, 1101, 13401, 813, 2306, 300, 500, 380, 362, 300, 294, 309, 13, 663, 311, 452, 1867, 13, 883, 11, 309, 311, 257, 3715, 1867, 13, 51644], "temperature": 0.0, "avg_logprob": -0.10170596578846806, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.022956807166337967}, {"id": 382, "seek": 226940, "start": 2269.4, "end": 2275.08, "text": " And it, it's really helped me and hopefully our audience clarify exactly what I mean by", "tokens": [50364, 400, 309, 11, 309, 311, 534, 4254, 385, 293, 4696, 527, 4034, 17594, 2293, 437, 286, 914, 538, 50648], "temperature": 0.0, "avg_logprob": -0.1494005396125022, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.023546870797872543}, {"id": 383, "seek": 226940, "start": 2275.08, "end": 2279.32, "text": " an existential imperative, which is that the free energy principle doesn't tell you", "tokens": [50648, 364, 37133, 32490, 11, 597, 307, 300, 264, 1737, 2281, 8665, 1177, 380, 980, 291, 50860], "temperature": 0.0, "avg_logprob": -0.1494005396125022, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.023546870797872543}, {"id": 384, "seek": 226940, "start": 2280.28, "end": 2286.52, "text": " what you're here for. No, it tells you if you're here, what are you doing? Like I've given your", "tokens": [50908, 437, 291, 434, 510, 337, 13, 883, 11, 309, 5112, 291, 498, 291, 434, 510, 11, 437, 366, 291, 884, 30, 1743, 286, 600, 2212, 428, 51220], "temperature": 0.0, "avg_logprob": -0.1494005396125022, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.023546870797872543}, {"id": 385, "seek": 226940, "start": 2286.52, "end": 2295.08, "text": " phenotype and given your culture and etc, etc. So that's a really useful sort of explanation", "tokens": [51220, 7279, 13108, 293, 2212, 428, 3713, 293, 5183, 11, 5183, 13, 407, 300, 311, 257, 534, 4420, 1333, 295, 10835, 51648], "temperature": 0.0, "avg_logprob": -0.1494005396125022, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.023546870797872543}, {"id": 386, "seek": 229508, "start": 2295.16, "end": 2300.12, "text": " of exactly what I was going for there. And actually, it points to something I said to", "tokens": [50368, 295, 2293, 437, 286, 390, 516, 337, 456, 13, 400, 767, 11, 309, 2793, 281, 746, 286, 848, 281, 50616], "temperature": 0.0, "avg_logprob": -0.1416784269469125, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.06728951632976532}, {"id": 387, "seek": 229508, "start": 2300.12, "end": 2305.08, "text": " Carl right at the end of our podcast, which at the time of recording this came out yesterday. So", "tokens": [50616, 14256, 558, 412, 264, 917, 295, 527, 7367, 11, 597, 412, 264, 565, 295, 6613, 341, 1361, 484, 5186, 13, 407, 50864], "temperature": 0.0, "avg_logprob": -0.1416784269469125, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.06728951632976532}, {"id": 388, "seek": 229508, "start": 2305.08, "end": 2309.3199999999997, "text": " people should definitely check it out because he's, for sure, for sure, he's on top form,", "tokens": [50864, 561, 820, 2138, 1520, 309, 484, 570, 415, 311, 11, 337, 988, 11, 337, 988, 11, 415, 311, 322, 1192, 1254, 11, 51076], "temperature": 0.0, "avg_logprob": -0.1416784269469125, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.06728951632976532}, {"id": 389, "seek": 229508, "start": 2309.3199999999997, "end": 2316.6, "text": " which is that I asked him, I mentioned Thomas Nagel. So Thomas Nagel has this very romantic", "tokens": [51076, 597, 307, 300, 286, 2351, 796, 11, 286, 2835, 8500, 18913, 338, 13, 407, 8500, 18913, 338, 575, 341, 588, 13590, 51440], "temperature": 0.0, "avg_logprob": -0.1416784269469125, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.06728951632976532}, {"id": 390, "seek": 229508, "start": 2316.6, "end": 2322.7599999999998, "text": " philosophical notion that life is deaf is bad because life is fundamentally good. And it points", "tokens": [51440, 25066, 10710, 300, 993, 307, 15559, 307, 1578, 570, 993, 307, 17879, 665, 13, 400, 309, 2793, 51748], "temperature": 0.0, "avg_logprob": -0.1416784269469125, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.06728951632976532}, {"id": 391, "seek": 232276, "start": 2322.76, "end": 2327.8, "text": " to kind of your thought experiment. If we had two worlds, one with life and one without life,", "tokens": [50364, 281, 733, 295, 428, 1194, 5120, 13, 759, 321, 632, 732, 13401, 11, 472, 365, 993, 293, 472, 1553, 993, 11, 50616], "temperature": 0.0, "avg_logprob": -0.11821710211890084, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.012276905588805676}, {"id": 392, "seek": 232276, "start": 2327.8, "end": 2332.0400000000004, "text": " I think we would all intuitively say the one with life is better. Whether that has any", "tokens": [50616, 286, 519, 321, 576, 439, 46506, 584, 264, 472, 365, 993, 307, 1101, 13, 8503, 300, 575, 604, 50828], "temperature": 0.0, "avg_logprob": -0.11821710211890084, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.012276905588805676}, {"id": 393, "seek": 232276, "start": 2332.0400000000004, "end": 2340.5200000000004, "text": " metaphysical input, as you say, is up for grabs. I was going to ask sort of jumping on the back", "tokens": [50828, 30946, 36280, 4846, 11, 382, 291, 584, 11, 307, 493, 337, 30028, 13, 286, 390, 516, 281, 1029, 1333, 295, 11233, 322, 264, 646, 51252], "temperature": 0.0, "avg_logprob": -0.11821710211890084, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.012276905588805676}, {"id": 394, "seek": 232276, "start": 2340.5200000000004, "end": 2345.7200000000003, "text": " of the initial thing you said, which was about connectedness and belongingness. So if we take", "tokens": [51252, 295, 264, 5883, 551, 291, 848, 11, 597, 390, 466, 4582, 1287, 293, 22957, 1287, 13, 407, 498, 321, 747, 51512], "temperature": 0.0, "avg_logprob": -0.11821710211890084, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.012276905588805676}, {"id": 395, "seek": 232276, "start": 2345.7200000000003, "end": 2349.6400000000003, "text": " this from a theoretical or computational stance and active inference, what this looks like to me", "tokens": [51512, 341, 490, 257, 20864, 420, 28270, 21033, 293, 4967, 38253, 11, 437, 341, 1542, 411, 281, 385, 51708], "temperature": 0.0, "avg_logprob": -0.11821710211890084, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.012276905588805676}, {"id": 396, "seek": 234964, "start": 2349.64, "end": 2356.12, "text": " is we embody a model of the world which we can so we can make fundamentally sound predictions", "tokens": [50364, 307, 321, 42575, 257, 2316, 295, 264, 1002, 597, 321, 393, 370, 321, 393, 652, 17879, 1626, 21264, 50688], "temperature": 0.0, "avg_logprob": -0.11124772684914726, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.003027411177754402}, {"id": 397, "seek": 234964, "start": 2356.12, "end": 2360.8399999999997, "text": " about how the world will unfold and how about we will and how we will act to make that world more", "tokens": [50688, 466, 577, 264, 1002, 486, 17980, 293, 577, 466, 321, 486, 293, 577, 321, 486, 605, 281, 652, 300, 1002, 544, 50924], "temperature": 0.0, "avg_logprob": -0.11124772684914726, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.003027411177754402}, {"id": 398, "seek": 234964, "start": 2361.48, "end": 2368.68, "text": " compatible with our preferences. Now, when that was formalized initially in the active inference", "tokens": [50956, 18218, 365, 527, 21910, 13, 823, 11, 562, 300, 390, 9860, 1602, 9105, 294, 264, 4967, 38253, 51316], "temperature": 0.0, "avg_logprob": -0.11124772684914726, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.003027411177754402}, {"id": 399, "seek": 234964, "start": 2368.68, "end": 2373.56, "text": " literature, immediately a response to this came in the form of what's called the dark room problem.", "tokens": [51316, 10394, 11, 4258, 257, 4134, 281, 341, 1361, 294, 264, 1254, 295, 437, 311, 1219, 264, 2877, 1808, 1154, 13, 51560], "temperature": 0.0, "avg_logprob": -0.11124772684914726, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.003027411177754402}, {"id": 400, "seek": 234964, "start": 2374.2, "end": 2378.92, "text": " So this is sort of papers in 2011, 2012, and they've been rebutted in multiple ways. But the", "tokens": [51592, 407, 341, 307, 1333, 295, 10577, 294, 10154, 11, 9125, 11, 293, 436, 600, 668, 319, 5955, 14727, 294, 3866, 2098, 13, 583, 264, 51828], "temperature": 0.0, "avg_logprob": -0.11124772684914726, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.003027411177754402}, {"id": 401, "seek": 237892, "start": 2379.0, "end": 2384.76, "text": " basic idea is why don't humans just seek out dark rooms where maybe they have access to food and", "tokens": [50368, 3875, 1558, 307, 983, 500, 380, 6255, 445, 8075, 484, 2877, 9396, 689, 1310, 436, 362, 2105, 281, 1755, 293, 50656], "temperature": 0.0, "avg_logprob": -0.08547322128130042, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.00083693623309955}, {"id": 402, "seek": 237892, "start": 2384.76, "end": 2390.84, "text": " water, but they don't really seek out, but in a state where they don't go and explore and they're", "tokens": [50656, 1281, 11, 457, 436, 500, 380, 534, 8075, 484, 11, 457, 294, 257, 1785, 689, 436, 500, 380, 352, 293, 6839, 293, 436, 434, 50960], "temperature": 0.0, "avg_logprob": -0.08547322128130042, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.00083693623309955}, {"id": 403, "seek": 237892, "start": 2390.84, "end": 2394.76, "text": " not curious. Because what you're saying there is actually a perfect coupling between your predictions", "tokens": [50960, 406, 6369, 13, 1436, 437, 291, 434, 1566, 456, 307, 767, 257, 2176, 37447, 1296, 428, 21264, 51156], "temperature": 0.0, "avg_logprob": -0.08547322128130042, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.00083693623309955}, {"id": 404, "seek": 237892, "start": 2394.76, "end": 2400.76, "text": " of the world and the way the world is unfolding to your eyes. So I was wondering whether you had", "tokens": [51156, 295, 264, 1002, 293, 264, 636, 264, 1002, 307, 44586, 281, 428, 2575, 13, 407, 286, 390, 6359, 1968, 291, 632, 51456], "temperature": 0.0, "avg_logprob": -0.08547322128130042, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.00083693623309955}, {"id": 405, "seek": 240076, "start": 2401.7200000000003, "end": 2411.0800000000004, "text": " pondered the dark room problem and where you see exploration and epistemic affordances coming into", "tokens": [50412, 17384, 4073, 264, 2877, 1808, 1154, 293, 689, 291, 536, 16197, 293, 2388, 468, 3438, 6157, 2676, 1348, 666, 50880], "temperature": 0.0, "avg_logprob": -0.1312365473052602, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.053097765892744064}, {"id": 406, "seek": 240076, "start": 2411.0800000000004, "end": 2417.7200000000003, "text": " play here. Yeah, I think that the dark room problem, I'm not, I'm happy with all the other", "tokens": [50880, 862, 510, 13, 865, 11, 286, 519, 300, 264, 2877, 1808, 1154, 11, 286, 478, 406, 11, 286, 478, 2055, 365, 439, 264, 661, 51212], "temperature": 0.0, "avg_logprob": -0.1312365473052602, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.053097765892744064}, {"id": 407, "seek": 240076, "start": 2417.7200000000003, "end": 2422.92, "text": " rebuttals. And I don't know, I don't know, I don't know them all, so I may be stepping on somebody", "tokens": [51212, 319, 5955, 83, 1124, 13, 400, 286, 500, 380, 458, 11, 286, 500, 380, 458, 11, 286, 500, 380, 458, 552, 439, 11, 370, 286, 815, 312, 16821, 322, 2618, 51472], "temperature": 0.0, "avg_logprob": -0.1312365473052602, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.053097765892744064}, {"id": 408, "seek": 242292, "start": 2422.92, "end": 2432.44, "text": " else's toes. So if I am, whoever you are, I apologize. I think this is a lock-in individualist", "tokens": [50364, 1646, 311, 14681, 13, 407, 498, 286, 669, 11, 11387, 291, 366, 11, 286, 12328, 13, 286, 519, 341, 307, 257, 4017, 12, 259, 2609, 468, 50840], "temperature": 0.0, "avg_logprob": -0.1400278042524289, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.2777960002422333}, {"id": 409, "seek": 242292, "start": 2432.44, "end": 2439.56, "text": " model of cognition and how we work. And I think it is therefore the presupposition, well, is that no,", "tokens": [50840, 2316, 295, 46905, 293, 577, 321, 589, 13, 400, 286, 519, 309, 307, 4412, 264, 1183, 10504, 5830, 11, 731, 11, 307, 300, 572, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1400278042524289, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.2777960002422333}, {"id": 410, "seek": 242292, "start": 2440.36, "end": 2445.56, "text": " that's actually, I don't agree with that presupposition, that our cognition is fundamentally", "tokens": [51236, 300, 311, 767, 11, 286, 500, 380, 3986, 365, 300, 1183, 10504, 5830, 11, 300, 527, 46905, 307, 17879, 51496], "temperature": 0.0, "avg_logprob": -0.1400278042524289, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.2777960002422333}, {"id": 411, "seek": 244556, "start": 2445.56, "end": 2454.2799999999997, "text": " individualistic in that way. I think we are, you know, we are socio-social cultural mammals.", "tokens": [50364, 2609, 3142, 294, 300, 636, 13, 286, 519, 321, 366, 11, 291, 458, 11, 321, 366, 44303, 12, 48600, 6988, 35408, 13, 50800], "temperature": 0.0, "avg_logprob": -0.10749270889785263, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.22780272364616394}, {"id": 412, "seek": 244556, "start": 2454.92, "end": 2461.16, "text": " And let me point to one thing, you know, yes, measures of G are very robustly predictive,", "tokens": [50832, 400, 718, 385, 935, 281, 472, 551, 11, 291, 458, 11, 2086, 11, 8000, 295, 460, 366, 588, 13956, 356, 35521, 11, 51144], "temperature": 0.0, "avg_logprob": -0.10749270889785263, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.22780272364616394}, {"id": 413, "seek": 244556, "start": 2461.16, "end": 2465.48, "text": " but you know, it's also really a very powerful way of predicting your behavior,", "tokens": [51144, 457, 291, 458, 11, 309, 311, 611, 534, 257, 588, 4005, 636, 295, 32884, 428, 5223, 11, 51360], "temperature": 0.0, "avg_logprob": -0.10749270889785263, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.22780272364616394}, {"id": 414, "seek": 244556, "start": 2465.48, "end": 2472.92, "text": " your attachment style. This is also very robust. Now, that means, think about what that actually", "tokens": [51360, 428, 19431, 3758, 13, 639, 307, 611, 588, 13956, 13, 823, 11, 300, 1355, 11, 519, 466, 437, 300, 767, 51732], "temperature": 0.0, "avg_logprob": -0.10749270889785263, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.22780272364616394}, {"id": 415, "seek": 247292, "start": 2472.92, "end": 2476.6800000000003, "text": " means. And this, you know, and this goes into the heart of religious traditions, like agopic love,", "tokens": [50364, 1355, 13, 400, 341, 11, 291, 458, 11, 293, 341, 1709, 666, 264, 1917, 295, 7185, 15643, 11, 411, 623, 40216, 959, 11, 50552], "temperature": 0.0, "avg_logprob": -0.1204194704691569, "compression_ratio": 1.8987341772151898, "no_speech_prob": 0.05744718015193939}, {"id": 416, "seek": 247292, "start": 2477.32, "end": 2482.52, "text": " right? When you have a child, you have to invert your relevance arrow. It's not how", "tokens": [50584, 558, 30, 1133, 291, 362, 257, 1440, 11, 291, 362, 281, 33966, 428, 32684, 11610, 13, 467, 311, 406, 577, 50844], "temperature": 0.0, "avg_logprob": -0.1204194704691569, "compression_ratio": 1.8987341772151898, "no_speech_prob": 0.05744718015193939}, {"id": 417, "seek": 247292, "start": 2483.2400000000002, "end": 2488.44, "text": " that being is relevant to you. You invert everything around. This is agopic love,", "tokens": [50880, 300, 885, 307, 7340, 281, 291, 13, 509, 33966, 1203, 926, 13, 639, 307, 623, 40216, 959, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1204194704691569, "compression_ratio": 1.8987341772151898, "no_speech_prob": 0.05744718015193939}, {"id": 418, "seek": 247292, "start": 2488.44, "end": 2493.48, "text": " and it's how it's different from erotic love or philea love. You invert everything around,", "tokens": [51140, 293, 309, 311, 577, 309, 311, 819, 490, 1189, 9411, 959, 420, 903, 794, 64, 959, 13, 509, 33966, 1203, 926, 11, 51392], "temperature": 0.0, "avg_logprob": -0.1204194704691569, "compression_ratio": 1.8987341772151898, "no_speech_prob": 0.05744718015193939}, {"id": 419, "seek": 247292, "start": 2493.48, "end": 2498.6, "text": " right? And so that it's, how am I relevant to this being? How am I relevant to this being? And", "tokens": [51392, 558, 30, 400, 370, 300, 309, 311, 11, 577, 669, 286, 7340, 281, 341, 885, 30, 1012, 669, 286, 7340, 281, 341, 885, 30, 400, 51648], "temperature": 0.0, "avg_logprob": -0.1204194704691569, "compression_ratio": 1.8987341772151898, "no_speech_prob": 0.05744718015193939}, {"id": 420, "seek": 249860, "start": 2498.6, "end": 2504.68, "text": " how can I be relevant in a way that turns it into a person, turns it into an intelligent,", "tokens": [50364, 577, 393, 286, 312, 7340, 294, 257, 636, 300, 4523, 309, 666, 257, 954, 11, 4523, 309, 666, 364, 13232, 11, 50668], "temperature": 0.0, "avg_logprob": -0.07163778125730336, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.003221632679924369}, {"id": 421, "seek": 249860, "start": 2504.68, "end": 2511.72, "text": " rational, self-reflective, relevance-realizer, predictive processor, meaning maker, right?", "tokens": [50668, 15090, 11, 2698, 12, 33115, 1809, 488, 11, 32684, 12, 9342, 6545, 11, 35521, 15321, 11, 3620, 17127, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.07163778125730336, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.003221632679924369}, {"id": 422, "seek": 249860, "start": 2511.72, "end": 2517.4, "text": " And so, well, first of all, where are my attachment relationships in the dark room? Well,", "tokens": [51020, 400, 370, 11, 731, 11, 700, 295, 439, 11, 689, 366, 452, 19431, 6159, 294, 264, 2877, 1808, 30, 1042, 11, 51304], "temperature": 0.0, "avg_logprob": -0.07163778125730336, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.003221632679924369}, {"id": 423, "seek": 249860, "start": 2517.4, "end": 2520.8399999999997, "text": " there's other people in the dark room. As soon as there's other people in the dark room, all the", "tokens": [51304, 456, 311, 661, 561, 294, 264, 2877, 1808, 13, 1018, 2321, 382, 456, 311, 661, 561, 294, 264, 2877, 1808, 11, 439, 264, 51476], "temperature": 0.0, "avg_logprob": -0.07163778125730336, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.003221632679924369}, {"id": 424, "seek": 249860, "start": 2520.8399999999997, "end": 2525.7999999999997, "text": " problems that you thought you got away from by putting me in a dark room return. Other people", "tokens": [51476, 2740, 300, 291, 1194, 291, 658, 1314, 490, 538, 3372, 385, 294, 257, 2877, 1808, 2736, 13, 5358, 561, 51724], "temperature": 0.0, "avg_logprob": -0.07163778125730336, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.003221632679924369}, {"id": 425, "seek": 252580, "start": 2525.8, "end": 2530.04, "text": " have different goals than me. They have different needs. They're going to move around differently,", "tokens": [50364, 362, 819, 5493, 813, 385, 13, 814, 362, 819, 2203, 13, 814, 434, 516, 281, 1286, 926, 7614, 11, 50576], "temperature": 0.0, "avg_logprob": -0.15636712151604729, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.02516632340848446}, {"id": 426, "seek": 252580, "start": 2530.04, "end": 2533.5600000000004, "text": " right? That we're going to have to decide about when and where and how we gain access,", "tokens": [50576, 558, 30, 663, 321, 434, 516, 281, 362, 281, 4536, 466, 562, 293, 689, 293, 577, 321, 6052, 2105, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15636712151604729, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.02516632340848446}, {"id": 427, "seek": 252580, "start": 2533.5600000000004, "end": 2538.2000000000003, "text": " blah, blah, blah, blah, blah, blah. All of that immediately unfolds. Secondly, and this is part", "tokens": [50752, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 13, 1057, 295, 300, 4258, 17980, 82, 13, 19483, 11, 293, 341, 307, 644, 50984], "temperature": 0.0, "avg_logprob": -0.15636712151604729, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.02516632340848446}, {"id": 428, "seek": 252580, "start": 2538.2000000000003, "end": 2545.0, "text": " of 4E CogSight, I think the evidence for extended cognition, distributed cognition,", "tokens": [50984, 295, 1017, 36, 383, 664, 50, 397, 11, 286, 519, 264, 4467, 337, 10913, 46905, 11, 12631, 46905, 11, 51324], "temperature": 0.0, "avg_logprob": -0.15636712151604729, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.02516632340848446}, {"id": 429, "seek": 252580, "start": 2545.0, "end": 2551.4, "text": " that we evolve to work in groups, and the collective intelligence of that is actually", "tokens": [51324, 300, 321, 16693, 281, 589, 294, 3935, 11, 293, 264, 12590, 7599, 295, 300, 307, 767, 51644], "temperature": 0.0, "avg_logprob": -0.15636712151604729, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.02516632340848446}, {"id": 430, "seek": 255140, "start": 2551.4, "end": 2557.48, "text": " our superpower, and other people are making this argument. I think that's clear. The standard thing,", "tokens": [50364, 527, 45765, 11, 293, 661, 561, 366, 1455, 341, 6770, 13, 286, 519, 300, 311, 1850, 13, 440, 3832, 551, 11, 50668], "temperature": 0.0, "avg_logprob": -0.15724652500475866, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.08265417069196701}, {"id": 431, "seek": 255140, "start": 2557.48, "end": 2562.76, "text": " take the Waste and Selection Task, put it an individual person, highly educated, highly", "tokens": [50668, 747, 264, 343, 9079, 293, 1100, 5450, 30428, 11, 829, 309, 364, 2609, 954, 11, 5405, 15872, 11, 5405, 50932], "temperature": 0.0, "avg_logprob": -0.15724652500475866, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.08265417069196701}, {"id": 432, "seek": 255140, "start": 2562.76, "end": 2569.48, "text": " intelligent, second-year psychology and top-tier university. From the 1960s on, you put them", "tokens": [50932, 13232, 11, 1150, 12, 5294, 15105, 293, 1192, 12, 25402, 5454, 13, 3358, 264, 16157, 82, 322, 11, 291, 829, 552, 51268], "temperature": 0.0, "avg_logprob": -0.15724652500475866, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.08265417069196701}, {"id": 433, "seek": 255140, "start": 2569.48, "end": 2575.0, "text": " in the Waste and Selection Task, and only 10% get it right, right? You replace that same task", "tokens": [51268, 294, 264, 343, 9079, 293, 1100, 5450, 30428, 11, 293, 787, 1266, 4, 483, 309, 558, 11, 558, 30, 509, 7406, 300, 912, 5633, 51544], "temperature": 0.0, "avg_logprob": -0.15724652500475866, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.08265417069196701}, {"id": 434, "seek": 255140, "start": 2575.0, "end": 2580.52, "text": " with four people who are allowed to talk to each other, and their success rate goes from 10% to", "tokens": [51544, 365, 1451, 561, 567, 366, 4350, 281, 751, 281, 1184, 661, 11, 293, 641, 2245, 3314, 1709, 490, 1266, 4, 281, 51820], "temperature": 0.0, "avg_logprob": -0.15724652500475866, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.08265417069196701}, {"id": 435, "seek": 258052, "start": 2580.52, "end": 2587.64, "text": " 82% reliably. Why? Because we do opponent processing between each other. You have biases", "tokens": [50364, 29097, 4, 49927, 13, 1545, 30, 1436, 321, 360, 10620, 9007, 1296, 1184, 661, 13, 509, 362, 32152, 50720], "temperature": 0.0, "avg_logprob": -0.0724958862577166, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.006485519465059042}, {"id": 436, "seek": 258052, "start": 2587.64, "end": 2593.4, "text": " different from mine, and if we work in opponent processing, not adversarial, but if I say,", "tokens": [50720, 819, 490, 3892, 11, 293, 498, 321, 589, 294, 10620, 9007, 11, 406, 17641, 44745, 11, 457, 498, 286, 584, 11, 51008], "temperature": 0.0, "avg_logprob": -0.0724958862577166, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.006485519465059042}, {"id": 437, "seek": 258052, "start": 2593.4, "end": 2597.08, "text": " you're probably a good source for correcting my bias, and I'm probably a good source for correcting", "tokens": [51008, 291, 434, 1391, 257, 665, 4009, 337, 47032, 452, 12577, 11, 293, 286, 478, 1391, 257, 665, 4009, 337, 47032, 51192], "temperature": 0.0, "avg_logprob": -0.0724958862577166, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.006485519465059042}, {"id": 438, "seek": 258052, "start": 2597.08, "end": 2605.56, "text": " yours, we get the best dynamical self-correction possible. When you add in the fact of the reality,", "tokens": [51192, 6342, 11, 321, 483, 264, 1151, 5999, 804, 2698, 12, 19558, 11417, 1944, 13, 1133, 291, 909, 294, 264, 1186, 295, 264, 4103, 11, 51616], "temperature": 0.0, "avg_logprob": -0.0724958862577166, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.006485519465059042}, {"id": 439, "seek": 258052, "start": 2605.56, "end": 2610.12, "text": " and there's actually evidence, empirical and formal, for the power of collective intelligence,", "tokens": [51616, 293, 456, 311, 767, 4467, 11, 31886, 293, 9860, 11, 337, 264, 1347, 295, 12590, 7599, 11, 51844], "temperature": 0.0, "avg_logprob": -0.0724958862577166, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.006485519465059042}, {"id": 440, "seek": 261012, "start": 2610.12, "end": 2615.7999999999997, "text": " the reality of attachment, well, then the dark room becomes filled with other people", "tokens": [50364, 264, 4103, 295, 19431, 11, 731, 11, 550, 264, 2877, 1808, 3643, 6412, 365, 661, 561, 50648], "temperature": 0.0, "avg_logprob": -0.15631438638562353, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0024664567317813635}, {"id": 441, "seek": 261012, "start": 2615.7999999999997, "end": 2621.24, "text": " who are trying to band together to solve problems that they can't solve individually, and then,", "tokens": [50648, 567, 366, 1382, 281, 4116, 1214, 281, 5039, 2740, 300, 436, 393, 380, 5039, 16652, 11, 293, 550, 11, 50920], "temperature": 0.0, "avg_logprob": -0.15631438638562353, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0024664567317813635}, {"id": 442, "seek": 261012, "start": 2622.12, "end": 2628.44, "text": " to me, everything that the dark room thought it had as an absurdity, like a reductioid absurdum,", "tokens": [50964, 281, 385, 11, 1203, 300, 264, 2877, 1808, 1194, 309, 632, 382, 364, 19774, 507, 11, 411, 257, 2783, 349, 1004, 327, 19774, 449, 11, 51280], "temperature": 0.0, "avg_logprob": -0.15631438638562353, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0024664567317813635}, {"id": 443, "seek": 261012, "start": 2628.44, "end": 2632.7599999999998, "text": " disappears. That would be my response. Yeah, I think eventually the dark room just becomes the", "tokens": [51280, 25527, 13, 663, 576, 312, 452, 4134, 13, 865, 11, 286, 519, 4728, 264, 2877, 1808, 445, 3643, 264, 51496], "temperature": 0.0, "avg_logprob": -0.15631438638562353, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0024664567317813635}, {"id": 444, "seek": 261012, "start": 2632.7599999999998, "end": 2638.8399999999997, "text": " world once you start asking the things you need, eventually it becomes a civilization.", "tokens": [51496, 1002, 1564, 291, 722, 3365, 264, 721, 291, 643, 11, 4728, 309, 3643, 257, 18036, 13, 51800], "temperature": 0.0, "avg_logprob": -0.15631438638562353, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0024664567317813635}, {"id": 445, "seek": 263884, "start": 2638.84, "end": 2643.8, "text": " Yeah, I love all this stuff on distributed cognition. I spent the first six months of this", "tokens": [50364, 865, 11, 286, 959, 439, 341, 1507, 322, 12631, 46905, 13, 286, 4418, 264, 700, 2309, 2493, 295, 341, 50612], "temperature": 0.0, "avg_logprob": -0.12720138103038342, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.005040951073169708}, {"id": 446, "seek": 263884, "start": 2643.8, "end": 2649.88, "text": " year working at UCL on finding experimental ground for what's called social baseline theory.", "tokens": [50612, 1064, 1364, 412, 14079, 43, 322, 5006, 17069, 2727, 337, 437, 311, 1219, 2093, 20518, 5261, 13, 50916], "temperature": 0.0, "avg_logprob": -0.12720138103038342, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.005040951073169708}, {"id": 447, "seek": 263884, "start": 2649.88, "end": 2655.2400000000002, "text": " Social baseline theory is this idea that humans, like to study a human being in a lab by itself is", "tokens": [50916, 9909, 20518, 5261, 307, 341, 1558, 300, 6255, 11, 411, 281, 2979, 257, 1952, 885, 294, 257, 2715, 538, 2564, 307, 51184], "temperature": 0.0, "avg_logprob": -0.12720138103038342, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.005040951073169708}, {"id": 448, "seek": 263884, "start": 2655.2400000000002, "end": 2662.52, "text": " to study a human being at deficit. Yes. Actually, it's physiological arousal to take one factor", "tokens": [51184, 281, 2979, 257, 1952, 885, 412, 22132, 13, 1079, 13, 5135, 11, 309, 311, 41234, 594, 563, 304, 281, 747, 472, 5952, 51548], "temperature": 0.0, "avg_logprob": -0.12720138103038342, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.005040951073169708}, {"id": 449, "seek": 263884, "start": 2663.08, "end": 2667.32, "text": " is at its baseline when it's with other people, and it will fluctuate according to stuff like", "tokens": [51576, 307, 412, 1080, 20518, 562, 309, 311, 365, 661, 561, 11, 293, 309, 486, 23448, 10107, 4650, 281, 1507, 411, 51788], "temperature": 0.0, "avg_logprob": -0.12720138103038342, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.005040951073169708}, {"id": 450, "seek": 266732, "start": 2667.32, "end": 2671.96, "text": " attachment style or the relationship that one has with that person, which is really beautiful,", "tokens": [50364, 19431, 3758, 420, 264, 2480, 300, 472, 575, 365, 300, 954, 11, 597, 307, 534, 2238, 11, 50596], "temperature": 0.0, "avg_logprob": -0.11958997975225034, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.004051851108670235}, {"id": 451, "seek": 266732, "start": 2671.96, "end": 2677.0, "text": " but inverts our kind of, well, maybe not common sense, but our institutionalized notion of what", "tokens": [50596, 457, 28653, 1373, 527, 733, 295, 11, 731, 11, 1310, 406, 2689, 2020, 11, 457, 527, 18391, 1602, 10710, 295, 437, 50848], "temperature": 0.0, "avg_logprob": -0.11958997975225034, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.004051851108670235}, {"id": 452, "seek": 266732, "start": 2677.0, "end": 2682.52, "text": " it is to be a person, which is kind of alone. Yeah, wonderful. Well, I'm going to take this", "tokens": [50848, 309, 307, 281, 312, 257, 954, 11, 597, 307, 733, 295, 3312, 13, 865, 11, 3715, 13, 1042, 11, 286, 478, 516, 281, 747, 341, 51124], "temperature": 0.0, "avg_logprob": -0.11958997975225034, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.004051851108670235}, {"id": 453, "seek": 266732, "start": 2682.52, "end": 2687.88, "text": " argument a little bit further if you let me, because this is something that we spoke about a", "tokens": [51124, 6770, 257, 707, 857, 3052, 498, 291, 718, 385, 11, 570, 341, 307, 746, 300, 321, 7179, 466, 257, 51392], "temperature": 0.0, "avg_logprob": -0.11958997975225034, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.004051851108670235}, {"id": 454, "seek": 266732, "start": 2687.88, "end": 2691.6400000000003, "text": " couple of months ago, and I think Mark might have a different opinion, so it'd be cool to", "tokens": [51392, 1916, 295, 2493, 2057, 11, 293, 286, 519, 3934, 1062, 362, 257, 819, 4800, 11, 370, 309, 1116, 312, 1627, 281, 51580], "temperature": 0.0, "avg_logprob": -0.11958997975225034, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.004051851108670235}, {"id": 455, "seek": 269164, "start": 2692.3599999999997, "end": 2699.08, "text": " unpickle this. I have this slightly fuzzy, wishy-washy idea that the notion that, okay,", "tokens": [50400, 20994, 618, 306, 341, 13, 286, 362, 341, 4748, 34710, 11, 3172, 88, 12, 38558, 88, 1558, 300, 264, 10710, 300, 11, 1392, 11, 50736], "temperature": 0.0, "avg_logprob": -0.18277555465698242, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.08960065990686417}, {"id": 456, "seek": 269164, "start": 2699.08, "end": 2705.48, "text": " so I always think about a climber climbing a rock face or a climbing wall. The climbing wall is a", "tokens": [50736, 370, 286, 1009, 519, 466, 257, 5644, 607, 14780, 257, 3727, 1851, 420, 257, 14780, 2929, 13, 440, 14780, 2929, 307, 257, 51056], "temperature": 0.0, "avg_logprob": -0.18277555465698242, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.08960065990686417}, {"id": 457, "seek": 269164, "start": 2705.48, "end": 2711.24, "text": " kind of beautiful canonical example of an affordance based landscape. It's rich for the affordances.", "tokens": [51056, 733, 295, 2238, 46491, 1365, 295, 364, 6157, 719, 2361, 9661, 13, 467, 311, 4593, 337, 264, 6157, 2676, 13, 51344], "temperature": 0.0, "avg_logprob": -0.18277555465698242, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.08960065990686417}, {"id": 458, "seek": 269164, "start": 2711.24, "end": 2717.72, "text": " It's got the toe holes, the finger holes. So in many ways, what the rock, in terms of active", "tokens": [51344, 467, 311, 658, 264, 13976, 8118, 11, 264, 5984, 8118, 13, 407, 294, 867, 2098, 11, 437, 264, 3727, 11, 294, 2115, 295, 4967, 51668], "temperature": 0.0, "avg_logprob": -0.18277555465698242, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.08960065990686417}, {"id": 459, "seek": 271772, "start": 2717.72, "end": 2723.9599999999996, "text": " inference, what it allows the rock climber to do is to self-evidence or find evidence for its own", "tokens": [50364, 38253, 11, 437, 309, 4045, 264, 3727, 5644, 607, 281, 360, 307, 281, 2698, 12, 13379, 2778, 420, 915, 4467, 337, 1080, 1065, 50676], "temperature": 0.0, "avg_logprob": -0.10040165453541036, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0038730758242309093}, {"id": 460, "seek": 271772, "start": 2723.9599999999996, "end": 2730.7599999999998, "text": " model about itself as a successful rock climber. Now, that's relatively well established in the", "tokens": [50676, 2316, 466, 2564, 382, 257, 4406, 3727, 5644, 607, 13, 823, 11, 300, 311, 7226, 731, 7545, 294, 264, 51016], "temperature": 0.0, "avg_logprob": -0.10040165453541036, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0038730758242309093}, {"id": 461, "seek": 271772, "start": 2730.7599999999998, "end": 2736.04, "text": " literature. What isn't established is my kind of, again, slightly wishy-washy idea, which is that", "tokens": [51016, 10394, 13, 708, 1943, 380, 7545, 307, 452, 733, 295, 11, 797, 11, 4748, 3172, 88, 12, 38558, 88, 1558, 11, 597, 307, 300, 51280], "temperature": 0.0, "avg_logprob": -0.10040165453541036, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0038730758242309093}, {"id": 462, "seek": 271772, "start": 2736.04, "end": 2743.48, "text": " the rock climber is offering affordances to the wall. And so this is why, right at the beginning", "tokens": [51280, 264, 3727, 5644, 607, 307, 8745, 6157, 2676, 281, 264, 2929, 13, 400, 370, 341, 307, 983, 11, 558, 412, 264, 2863, 51652], "temperature": 0.0, "avg_logprob": -0.10040165453541036, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0038730758242309093}, {"id": 463, "seek": 274348, "start": 2743.48, "end": 2748.12, "text": " of our conversation, I spoke about mutual coupling. So what dynamical systems theory gives us,", "tokens": [50364, 295, 527, 3761, 11, 286, 7179, 466, 16917, 37447, 13, 407, 437, 5999, 804, 3652, 5261, 2709, 505, 11, 50596], "temperature": 0.0, "avg_logprob": -0.1778295018651464, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03777584061026573}, {"id": 464, "seek": 274348, "start": 2748.12, "end": 2754.84, "text": " what auto-polysis gives us, what active inference gives us, is this idea that to exist is not to", "tokens": [50596, 437, 8399, 12, 2259, 356, 17122, 2709, 505, 11, 437, 4967, 38253, 2709, 505, 11, 307, 341, 1558, 300, 281, 2514, 307, 406, 281, 50932], "temperature": 0.0, "avg_logprob": -0.1778295018651464, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03777584061026573}, {"id": 465, "seek": 274348, "start": 2754.84, "end": 2760.92, "text": " be this kind of reified self that takes a objective stance on the world. And actually, what it is,", "tokens": [50932, 312, 341, 733, 295, 319, 2587, 2698, 300, 2516, 257, 10024, 21033, 322, 264, 1002, 13, 400, 767, 11, 437, 309, 307, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1778295018651464, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03777584061026573}, {"id": 466, "seek": 274348, "start": 2760.92, "end": 2766.2, "text": " is this really the fundamental unit of analysis should be this dynamic mutual coupling between", "tokens": [51236, 307, 341, 534, 264, 8088, 4985, 295, 5215, 820, 312, 341, 8546, 16917, 37447, 1296, 51500], "temperature": 0.0, "avg_logprob": -0.1778295018651464, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03777584061026573}, {"id": 467, "seek": 274348, "start": 2766.2, "end": 2772.12, "text": " agent and arena to use your terminology. I've spoken to Mark about this, and he", "tokens": [51500, 9461, 293, 18451, 281, 764, 428, 27575, 13, 286, 600, 10759, 281, 3934, 466, 341, 11, 293, 415, 51796], "temperature": 0.0, "avg_logprob": -0.1778295018651464, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.03777584061026573}, {"id": 468, "seek": 277212, "start": 2772.7599999999998, "end": 2777.4, "text": " said it's something like an overextension of the term affordances, to say that the human being is", "tokens": [50396, 848, 309, 311, 746, 411, 364, 38657, 734, 3378, 295, 264, 1433, 6157, 2676, 11, 281, 584, 300, 264, 1952, 885, 307, 50628], "temperature": 0.0, "avg_logprob": -0.1576615422964096, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0006452418165281415}, {"id": 469, "seek": 277212, "start": 2777.4, "end": 2781.3199999999997, "text": " offering affordances to the climbing wall. I know Mark isn't here to defend himself, so again, I'm", "tokens": [50628, 8745, 6157, 2676, 281, 264, 14780, 2929, 13, 286, 458, 3934, 1943, 380, 510, 281, 8602, 3647, 11, 370, 797, 11, 286, 478, 50824], "temperature": 0.0, "avg_logprob": -0.1576615422964096, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0006452418165281415}, {"id": 470, "seek": 277212, "start": 2781.3199999999997, "end": 2786.2799999999997, "text": " not going to put one... But I will definitely raise this with him again. But just to your own", "tokens": [50824, 406, 516, 281, 829, 472, 485, 583, 286, 486, 2138, 5300, 341, 365, 796, 797, 13, 583, 445, 281, 428, 1065, 51072], "temperature": 0.0, "avg_logprob": -0.1576615422964096, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0006452418165281415}, {"id": 471, "seek": 277212, "start": 2786.2799999999997, "end": 2792.12, "text": " ears, how does that argument sound to you? I like the argument, and yeah, I don't want to speak", "tokens": [51072, 8798, 11, 577, 775, 300, 6770, 1626, 281, 291, 30, 286, 411, 264, 6770, 11, 293, 1338, 11, 286, 500, 380, 528, 281, 1710, 51364], "temperature": 0.0, "avg_logprob": -0.1576615422964096, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0006452418165281415}, {"id": 472, "seek": 277212, "start": 2792.12, "end": 2797.72, "text": " to you, but Mark, I'll only speak to the propositions that you spoke on his behalf. And I'm very happy", "tokens": [51364, 281, 291, 11, 457, 3934, 11, 286, 603, 787, 1710, 281, 264, 7532, 2451, 300, 291, 7179, 322, 702, 9490, 13, 400, 286, 478, 588, 2055, 51644], "temperature": 0.0, "avg_logprob": -0.1576615422964096, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0006452418165281415}, {"id": 473, "seek": 279772, "start": 2797.7999999999997, "end": 2802.8399999999997, "text": " to... I mean, Mark and I work close together. Mark is one of the best people I know.", "tokens": [50368, 281, 485, 286, 914, 11, 3934, 293, 286, 589, 1998, 1214, 13, 3934, 307, 472, 295, 264, 1151, 561, 286, 458, 13, 50620], "temperature": 0.0, "avg_logprob": -0.20509558818379386, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.02127663604915142}, {"id": 474, "seek": 279772, "start": 2803.7999999999997, "end": 2810.2799999999997, "text": " Mark's a former student of mine. I'm very proud of Mark, so... But I actually agree with you. Now,", "tokens": [50668, 3934, 311, 257, 5819, 3107, 295, 3892, 13, 286, 478, 588, 4570, 295, 3934, 11, 370, 485, 583, 286, 767, 3986, 365, 291, 13, 823, 11, 50992], "temperature": 0.0, "avg_logprob": -0.20509558818379386, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.02127663604915142}, {"id": 475, "seek": 279772, "start": 2810.2799999999997, "end": 2816.68, "text": " that goes to another connection I make in my work that would probably be a little bit stranger", "tokens": [50992, 300, 1709, 281, 1071, 4984, 286, 652, 294, 452, 589, 300, 576, 1391, 312, 257, 707, 857, 18834, 51312], "temperature": 0.0, "avg_logprob": -0.20509558818379386, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.02127663604915142}, {"id": 476, "seek": 279772, "start": 2816.68, "end": 2820.7599999999998, "text": " to the ears of many of your listeners. They might say, oh, I get why he wants to connect", "tokens": [51312, 281, 264, 8798, 295, 867, 295, 428, 23274, 13, 814, 1062, 584, 11, 1954, 11, 286, 483, 983, 415, 2738, 281, 1745, 51516], "temperature": 0.0, "avg_logprob": -0.20509558818379386, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.02127663604915142}, {"id": 477, "seek": 279772, "start": 2821.7999999999997, "end": 2826.7599999999998, "text": " to process inter-relevance realization and the 40-card side. But now what you're doing,", "tokens": [51568, 281, 1399, 728, 12, 265, 28316, 719, 25138, 293, 264, 3356, 12, 22259, 1252, 13, 583, 586, 437, 291, 434, 884, 11, 51816], "temperature": 0.0, "avg_logprob": -0.20509558818379386, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.02127663604915142}, {"id": 478, "seek": 282676, "start": 2826.76, "end": 2830.6800000000003, "text": " and this is where I do a lot of what I call my deeper ontological work, and this is where the", "tokens": [50364, 293, 341, 307, 689, 286, 360, 257, 688, 295, 437, 286, 818, 452, 7731, 6592, 4383, 589, 11, 293, 341, 307, 689, 264, 50560], "temperature": 0.0, "avg_logprob": -0.11173509940122947, "compression_ratio": 1.76953125, "no_speech_prob": 0.002713983180001378}, {"id": 479, "seek": 282676, "start": 2830.6800000000003, "end": 2836.6000000000004, "text": " Heideggerian stuff really comes to bear, is this is all the work I do on neoplatonism,", "tokens": [50560, 634, 482, 39381, 952, 1507, 534, 1487, 281, 6155, 11, 307, 341, 307, 439, 264, 589, 286, 360, 322, 408, 404, 14087, 266, 1434, 11, 50856], "temperature": 0.0, "avg_logprob": -0.11173509940122947, "compression_ratio": 1.76953125, "no_speech_prob": 0.002713983180001378}, {"id": 480, "seek": 282676, "start": 2836.6000000000004, "end": 2844.1200000000003, "text": " which may sound like something very arcane. But again, here's the proposal that once we", "tokens": [50856, 597, 815, 1626, 411, 746, 588, 10346, 1929, 13, 583, 797, 11, 510, 311, 264, 11494, 300, 1564, 321, 51232], "temperature": 0.0, "avg_logprob": -0.11173509940122947, "compression_ratio": 1.76953125, "no_speech_prob": 0.002713983180001378}, {"id": 481, "seek": 282676, "start": 2844.1200000000003, "end": 2850.5200000000004, "text": " really profoundly accept affordances, and once we accept that they are reciprocally realizing,", "tokens": [51232, 534, 39954, 3241, 6157, 2676, 11, 293, 1564, 321, 3241, 300, 436, 366, 28961, 66, 379, 16734, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11173509940122947, "compression_ratio": 1.76953125, "no_speech_prob": 0.002713983180001378}, {"id": 482, "seek": 282676, "start": 2850.5200000000004, "end": 2855.1600000000003, "text": " I agree with you, that affordance not only discloses things about me, it discloses things", "tokens": [51552, 286, 3986, 365, 291, 11, 300, 6157, 719, 406, 787, 17092, 4201, 721, 466, 385, 11, 309, 17092, 4201, 721, 51784], "temperature": 0.0, "avg_logprob": -0.11173509940122947, "compression_ratio": 1.76953125, "no_speech_prob": 0.002713983180001378}, {"id": 483, "seek": 285516, "start": 2855.16, "end": 2860.7599999999998, "text": " about the world. And that's a way in which things, like Heidegger's notion of truth is alifea,", "tokens": [50364, 466, 264, 1002, 13, 400, 300, 311, 257, 636, 294, 597, 721, 11, 411, 634, 482, 39381, 311, 10710, 295, 3494, 307, 419, 863, 64, 11, 50644], "temperature": 0.0, "avg_logprob": -0.19570785098605686, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0024715440813452005}, {"id": 484, "seek": 285516, "start": 2860.7599999999998, "end": 2867.24, "text": " as an event, as the disclosure, rather than a static property of our propositions.", "tokens": [50644, 382, 364, 2280, 11, 382, 264, 30392, 11, 2831, 813, 257, 13437, 4707, 295, 527, 7532, 2451, 13, 50968], "temperature": 0.0, "avg_logprob": -0.19570785098605686, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0024715440813452005}, {"id": 485, "seek": 285516, "start": 2868.12, "end": 2873.3199999999997, "text": " I think that, bang on, now, as Filler argues, and I've been arguing for a while, and John Russen,", "tokens": [51012, 286, 519, 300, 11, 8550, 322, 11, 586, 11, 382, 479, 10497, 38218, 11, 293, 286, 600, 668, 19697, 337, 257, 1339, 11, 293, 2619, 497, 29202, 11, 51272], "temperature": 0.0, "avg_logprob": -0.19570785098605686, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0024715440813452005}, {"id": 486, "seek": 285516, "start": 2873.8799999999997, "end": 2878.68, "text": " Filler's book is called Neoplatonism, Heidegger and the History of Being, Relation as Ontological", "tokens": [51300, 479, 10497, 311, 1446, 307, 1219, 1734, 404, 14087, 266, 1434, 11, 634, 482, 39381, 293, 264, 12486, 295, 8891, 11, 8738, 399, 382, 16980, 4383, 51540], "temperature": 0.0, "avg_logprob": -0.19570785098605686, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0024715440813452005}, {"id": 487, "seek": 287868, "start": 2878.68, "end": 2885.48, "text": " Ground, that relationality is actually the ultimate nature of reality. Heidegger is actually turning", "tokens": [50364, 28371, 11, 300, 38444, 507, 307, 767, 264, 9705, 3687, 295, 4103, 13, 634, 482, 39381, 307, 767, 6246, 50704], "temperature": 0.0, "avg_logprob": -0.10393736097547743, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.15386734902858734}, {"id": 488, "seek": 287868, "start": 2885.48, "end": 2889.8799999999997, "text": " back towards this, because what does that give you? If you have that, and I think this is a fair", "tokens": [50704, 646, 3030, 341, 11, 570, 437, 775, 300, 976, 291, 30, 759, 291, 362, 300, 11, 293, 286, 519, 341, 307, 257, 3143, 50924], "temperature": 0.0, "avg_logprob": -0.10393736097547743, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.15386734902858734}, {"id": 489, "seek": 287868, "start": 2889.8799999999997, "end": 2895.48, "text": " way to put it, if you have that alethetic notion of affordance, then you are getting back to the", "tokens": [50924, 636, 281, 829, 309, 11, 498, 291, 362, 300, 419, 3293, 3532, 10710, 295, 6157, 719, 11, 550, 291, 366, 1242, 646, 281, 264, 51204], "temperature": 0.0, "avg_logprob": -0.10393736097547743, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.15386734902858734}, {"id": 490, "seek": 287868, "start": 2895.48, "end": 2901.3999999999996, "text": " neoplatonic theory of knowing by conformity, knowing by participation, and the deeper argument", "tokens": [51204, 408, 404, 14087, 11630, 5261, 295, 5276, 538, 18975, 507, 11, 5276, 538, 13487, 11, 293, 264, 7731, 6770, 51500], "temperature": 0.0, "avg_logprob": -0.10393736097547743, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.15386734902858734}, {"id": 491, "seek": 287868, "start": 2901.3999999999996, "end": 2908.3599999999997, "text": " goes something like this, that if the fundamental grammar of your cognition, I don't mean the content,", "tokens": [51500, 1709, 746, 411, 341, 11, 300, 498, 264, 8088, 22317, 295, 428, 46905, 11, 286, 500, 380, 914, 264, 2701, 11, 51848], "temperature": 0.0, "avg_logprob": -0.10393736097547743, "compression_ratio": 1.7761732851985559, "no_speech_prob": 0.15386734902858734}, {"id": 492, "seek": 290868, "start": 2908.68, "end": 2914.68, "text": " your content can go wrong, but if the fundamental grammar, the bottom up, top down, all this stuff", "tokens": [50364, 428, 2701, 393, 352, 2085, 11, 457, 498, 264, 8088, 22317, 11, 264, 2767, 493, 11, 1192, 760, 11, 439, 341, 1507, 50664], "temperature": 0.0, "avg_logprob": -0.1483107990688748, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.0002610878145787865}, {"id": 493, "seek": 290868, "start": 2914.68, "end": 2920.2799999999997, "text": " we're talking about, if it's not picking up on something fundamental about how reality is structured,", "tokens": [50664, 321, 434, 1417, 466, 11, 498, 309, 311, 406, 8867, 493, 322, 746, 8088, 466, 577, 4103, 307, 18519, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1483107990688748, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.0002610878145787865}, {"id": 494, "seek": 290868, "start": 2920.9199999999996, "end": 2926.6, "text": " right, then you face the kind of profound solipsism, a profound kind of skepticism.", "tokens": [50976, 558, 11, 550, 291, 1851, 264, 733, 295, 14382, 1404, 2600, 1434, 11, 257, 14382, 733, 295, 19128, 26356, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1483107990688748, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.0002610878145787865}, {"id": 495, "seek": 290868, "start": 2927.96, "end": 2932.9199999999996, "text": " And I think that is ultimately leads you into all kinds of performative contradictions that I", "tokens": [51328, 400, 286, 519, 300, 307, 6284, 6689, 291, 666, 439, 3685, 295, 2042, 1166, 15858, 15607, 300, 286, 51576], "temperature": 0.0, "avg_logprob": -0.1483107990688748, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.0002610878145787865}, {"id": 496, "seek": 293292, "start": 2933.48, "end": 2937.0, "text": " and I agree with Whitehead, there is devastating as propositional contradiction.", "tokens": [50392, 293, 286, 3986, 365, 5552, 1934, 11, 456, 307, 21280, 382, 7532, 2628, 34937, 13, 50568], "temperature": 0.0, "avg_logprob": -0.19965467266007966, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.02840326353907585}, {"id": 497, "seek": 293292, "start": 2937.88, "end": 2941.7200000000003, "text": " There's a longer argument there, and I could point to, please check out some of my talks", "tokens": [50612, 821, 311, 257, 2854, 6770, 456, 11, 293, 286, 727, 935, 281, 11, 1767, 1520, 484, 512, 295, 452, 6686, 50804], "temperature": 0.0, "avg_logprob": -0.19965467266007966, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.02840326353907585}, {"id": 498, "seek": 293292, "start": 2942.6800000000003, "end": 2948.04, "text": " on YouTube about this longer argument, like Pickstock's Aspects of Truth, where she said,", "tokens": [50852, 322, 3088, 466, 341, 2854, 6770, 11, 411, 14129, 23914, 311, 1018, 1043, 82, 295, 20522, 11, 689, 750, 848, 11, 51120], "temperature": 0.0, "avg_logprob": -0.19965467266007966, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.02840326353907585}, {"id": 499, "seek": 293292, "start": 2948.04, "end": 2953.08, "text": " all the things we use to decodemize this, we had the analytic synthetic distinction,", "tokens": [51120, 439, 264, 721, 321, 764, 281, 979, 378, 443, 1125, 341, 11, 321, 632, 264, 40358, 23420, 16844, 11, 51372], "temperature": 0.0, "avg_logprob": -0.19965467266007966, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.02840326353907585}, {"id": 500, "seek": 293292, "start": 2953.08, "end": 2958.6800000000003, "text": " that is broken down under philosophical criticism. We had the theory fact distinction,", "tokens": [51372, 300, 307, 5463, 760, 833, 25066, 15835, 13, 492, 632, 264, 5261, 1186, 16844, 11, 51652], "temperature": 0.0, "avg_logprob": -0.19965467266007966, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.02840326353907585}, {"id": 501, "seek": 295868, "start": 2958.68, "end": 2963.3999999999996, "text": " that is broken down. We had the, like, is, ought, that has broken down.", "tokens": [50364, 300, 307, 5463, 760, 13, 492, 632, 264, 11, 411, 11, 307, 11, 13416, 11, 300, 575, 5463, 760, 13, 50600], "temperature": 0.0, "avg_logprob": -0.22921468390793096, "compression_ratio": 1.8458333333333334, "no_speech_prob": 0.031119896098971367}, {"id": 502, "seek": 295868, "start": 2963.3999999999996, "end": 2968.52, "text": " Like, think about the relevance, it is or not, well, blah, right, it's sort of both, right?", "tokens": [50600, 1743, 11, 519, 466, 264, 32684, 11, 309, 307, 420, 406, 11, 731, 11, 12288, 11, 558, 11, 309, 311, 1333, 295, 1293, 11, 558, 30, 50856], "temperature": 0.0, "avg_logprob": -0.22921468390793096, "compression_ratio": 1.8458333333333334, "no_speech_prob": 0.031119896098971367}, {"id": 503, "seek": 295868, "start": 2968.52, "end": 2973.64, "text": " And so her point is all the things that were used to cleave, and then what we tried to do", "tokens": [50856, 400, 370, 720, 935, 307, 439, 264, 721, 300, 645, 1143, 281, 1233, 946, 11, 293, 550, 437, 321, 3031, 281, 360, 51112], "temperature": 0.0, "avg_logprob": -0.22921468390793096, "compression_ratio": 1.8458333333333334, "no_speech_prob": 0.031119896098971367}, {"id": 504, "seek": 295868, "start": 2973.64, "end": 2978.12, "text": " is we tried to make the logical world, the thing that sort of stuck the two worlds together,", "tokens": [51112, 307, 321, 3031, 281, 652, 264, 14978, 1002, 11, 264, 551, 300, 1333, 295, 5541, 264, 732, 13401, 1214, 11, 51336], "temperature": 0.0, "avg_logprob": -0.22921468390793096, "compression_ratio": 1.8458333333333334, "no_speech_prob": 0.031119896098971367}, {"id": 505, "seek": 295868, "start": 2978.12, "end": 2985.56, "text": " and that collapsed under Godel and other people. And so we're back to the idea that we either go", "tokens": [51336, 293, 300, 24578, 833, 1265, 338, 293, 661, 561, 13, 400, 370, 321, 434, 646, 281, 264, 1558, 300, 321, 2139, 352, 51708], "temperature": 0.0, "avg_logprob": -0.22921468390793096, "compression_ratio": 1.8458333333333334, "no_speech_prob": 0.031119896098971367}, {"id": 506, "seek": 298556, "start": 2985.56, "end": 2991.64, "text": " into the Lockean cabinet, in which we're locked inside of our heads, we're somehow getting postcards", "tokens": [50364, 666, 264, 12859, 330, 282, 15188, 11, 294, 597, 321, 434, 9376, 1854, 295, 527, 8050, 11, 321, 434, 6063, 1242, 2183, 40604, 50668], "temperature": 0.0, "avg_logprob": -0.09743568102518717, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.00805697776377201}, {"id": 507, "seek": 298556, "start": 2991.64, "end": 2996.04, "text": " sent to us, we think they might be from an outside world that we think might be out there,", "tokens": [50668, 2279, 281, 505, 11, 321, 519, 436, 1062, 312, 490, 364, 2380, 1002, 300, 321, 519, 1062, 312, 484, 456, 11, 50888], "temperature": 0.0, "avg_logprob": -0.09743568102518717, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.00805697776377201}, {"id": 508, "seek": 298556, "start": 2996.04, "end": 3001.32, "text": " and we're trying to build it from the postcards, and we're doomed, that's never going to get us there.", "tokens": [50888, 293, 321, 434, 1382, 281, 1322, 309, 490, 264, 2183, 40604, 11, 293, 321, 434, 33847, 11, 300, 311, 1128, 516, 281, 483, 505, 456, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09743568102518717, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.00805697776377201}, {"id": 509, "seek": 298556, "start": 3001.32, "end": 3008.6, "text": " And so I think if you have an allothetic notion of affordance, you start to make the argument", "tokens": [51152, 400, 370, 286, 519, 498, 291, 362, 364, 439, 900, 3532, 10710, 295, 6157, 719, 11, 291, 722, 281, 652, 264, 6770, 51516], "temperature": 0.0, "avg_logprob": -0.09743568102518717, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.00805697776377201}, {"id": 510, "seek": 298556, "start": 3008.6, "end": 3014.7599999999998, "text": " that how reality is realizing, and how we are doing relevance realization, are fundamentally", "tokens": [51516, 300, 577, 4103, 307, 16734, 11, 293, 577, 321, 366, 884, 32684, 25138, 11, 366, 17879, 51824], "temperature": 0.0, "avg_logprob": -0.09743568102518717, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.00805697776377201}, {"id": 511, "seek": 301476, "start": 3014.76, "end": 3024.76, "text": " participating in the same principles in a profound way. And I think, yeah, I have no doubt that Mark", "tokens": [50364, 13950, 294, 264, 912, 9156, 294, 257, 14382, 636, 13, 400, 286, 519, 11, 1338, 11, 286, 362, 572, 6385, 300, 3934, 50864], "temperature": 0.0, "avg_logprob": -0.13500025693108053, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004192550666630268}, {"id": 512, "seek": 301476, "start": 3024.76, "end": 3031.0, "text": " might not want to do this. I think Mark would be, I would agree with Mark that this is not a direct", "tokens": [50864, 1062, 406, 528, 281, 360, 341, 13, 286, 519, 3934, 576, 312, 11, 286, 576, 3986, 365, 3934, 300, 341, 307, 406, 257, 2047, 51176], "temperature": 0.0, "avg_logprob": -0.13500025693108053, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004192550666630268}, {"id": 513, "seek": 301476, "start": 3031.0, "end": 3036.92, "text": " derivation from sort of classic Christonian presentations of active inference and predicted", "tokens": [51176, 10151, 399, 490, 1333, 295, 7230, 2040, 43294, 18964, 295, 4967, 38253, 293, 19147, 51472], "temperature": 0.0, "avg_logprob": -0.13500025693108053, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004192550666630268}, {"id": 514, "seek": 303692, "start": 3036.92, "end": 3045.0, "text": " processing, Bayesian brain, we should settle on a name. But I think if you make the connections", "tokens": [50364, 9007, 11, 7840, 42434, 3567, 11, 321, 820, 11852, 322, 257, 1315, 13, 583, 286, 519, 498, 291, 652, 264, 9271, 50768], "temperature": 0.0, "avg_logprob": -0.14916508538382395, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.2534934878349304}, {"id": 515, "seek": 303692, "start": 3045.88, "end": 3052.6, "text": " through deeper into Gibson, deeper into Foricog side, deeper into Heidegger, you get back to this", "tokens": [50812, 807, 7731, 666, 42250, 11, 7731, 666, 1171, 299, 664, 1252, 11, 7731, 666, 634, 482, 39381, 11, 291, 483, 646, 281, 341, 51148], "temperature": 0.0, "avg_logprob": -0.14916508538382395, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.2534934878349304}, {"id": 516, "seek": 303692, "start": 3052.6, "end": 3057.96, "text": " notion that with the prevalent notion of knowing, that when we know something, what we're doing is", "tokens": [51148, 10710, 300, 365, 264, 30652, 10710, 295, 5276, 11, 300, 562, 321, 458, 746, 11, 437, 321, 434, 884, 307, 51416], "temperature": 0.0, "avg_logprob": -0.14916508538382395, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.2534934878349304}, {"id": 517, "seek": 303692, "start": 3059.16, "end": 3063.8, "text": " our structural functional organization is identifying with the structural functional", "tokens": [51476, 527, 15067, 11745, 4475, 307, 16696, 365, 264, 15067, 11745, 51708], "temperature": 0.0, "avg_logprob": -0.14916508538382395, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.2534934878349304}, {"id": 518, "seek": 306380, "start": 3063.8, "end": 3070.84, "text": " organization of the thing. And we are both participating in that same form, that same", "tokens": [50364, 4475, 295, 264, 551, 13, 400, 321, 366, 1293, 13950, 294, 300, 912, 1254, 11, 300, 912, 50716], "temperature": 0.0, "avg_logprob": -0.11799211128085267, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.001698039355687797}, {"id": 519, "seek": 306380, "start": 3070.84, "end": 3077.96, "text": " principle, that same grammar. And that has all kinds of metaphysical and ontological consequences,", "tokens": [50716, 8665, 11, 300, 912, 22317, 13, 400, 300, 575, 439, 3685, 295, 30946, 36280, 293, 6592, 4383, 10098, 11, 51072], "temperature": 0.0, "avg_logprob": -0.11799211128085267, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.001698039355687797}, {"id": 520, "seek": 306380, "start": 3077.96, "end": 3083.1600000000003, "text": " even ethical consequences. It means talking about the truth of good and the beautiful becomes", "tokens": [51072, 754, 18890, 10098, 13, 467, 1355, 1417, 466, 264, 3494, 295, 665, 293, 264, 2238, 3643, 51332], "temperature": 0.0, "avg_logprob": -0.11799211128085267, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.001698039355687797}, {"id": 521, "seek": 306380, "start": 3083.1600000000003, "end": 3089.1600000000003, "text": " something very relevant again. Yeah, yeah, no, this is exactly where I wanted to go and exactly", "tokens": [51332, 746, 588, 7340, 797, 13, 865, 11, 1338, 11, 572, 11, 341, 307, 2293, 689, 286, 1415, 281, 352, 293, 2293, 51632], "temperature": 0.0, "avg_logprob": -0.11799211128085267, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.001698039355687797}, {"id": 522, "seek": 306380, "start": 3089.1600000000003, "end": 3093.2400000000002, "text": " aligns with the way I'm thinking about the free energy principle at the moment,", "tokens": [51632, 7975, 82, 365, 264, 636, 286, 478, 1953, 466, 264, 1737, 2281, 8665, 412, 264, 1623, 11, 51836], "temperature": 0.0, "avg_logprob": -0.11799211128085267, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.001698039355687797}, {"id": 523, "seek": 309324, "start": 3093.24, "end": 3098.8399999999997, "text": " which is I have a skepticism between the internal and the external distinction.", "tokens": [50364, 597, 307, 286, 362, 257, 19128, 26356, 1296, 264, 6920, 293, 264, 8320, 16844, 13, 50644], "temperature": 0.0, "avg_logprob": -0.16551944686145317, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0038095407653599977}, {"id": 524, "seek": 309324, "start": 3099.3999999999996, "end": 3104.3599999999997, "text": " Yes, it's something I spoke about with so a classic problem when one starts reading active", "tokens": [50672, 1079, 11, 309, 311, 746, 286, 7179, 466, 365, 370, 257, 7230, 1154, 562, 472, 3719, 3760, 4967, 50920], "temperature": 0.0, "avg_logprob": -0.16551944686145317, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0038095407653599977}, {"id": 525, "seek": 309324, "start": 3104.3599999999997, "end": 3112.12, "text": " inference, as I spoke about with Carl, is whether one would consider the the agent or the internal", "tokens": [50920, 38253, 11, 382, 286, 7179, 466, 365, 14256, 11, 307, 1968, 472, 576, 1949, 264, 264, 9461, 420, 264, 6920, 51308], "temperature": 0.0, "avg_logprob": -0.16551944686145317, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0038095407653599977}, {"id": 526, "seek": 309324, "start": 3112.12, "end": 3118.2, "text": " dynamics is having having a model of the external world or instantiating a model of the external", "tokens": [51308, 15679, 307, 1419, 1419, 257, 2316, 295, 264, 8320, 1002, 420, 9836, 72, 990, 257, 2316, 295, 264, 8320, 51612], "temperature": 0.0, "avg_logprob": -0.16551944686145317, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0038095407653599977}, {"id": 527, "seek": 311820, "start": 3118.2, "end": 3124.6, "text": " world. Yes, excellent, excellent. And since it's come back to sort of cybernetic formulations", "tokens": [50364, 1002, 13, 1079, 11, 7103, 11, 7103, 13, 400, 1670, 309, 311, 808, 646, 281, 1333, 295, 13411, 77, 3532, 1254, 4136, 50684], "temperature": 0.0, "avg_logprob": -0.18307772827148439, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.016882799565792084}, {"id": 528, "seek": 311820, "start": 3124.6, "end": 3129.3199999999997, "text": " of what it is to be a good regulator. So Colin Ashby and all of that stuff from the 50s and 60s.", "tokens": [50684, 295, 437, 309, 307, 281, 312, 257, 665, 36250, 13, 407, 29253, 10279, 2322, 293, 439, 295, 300, 1507, 490, 264, 2625, 82, 293, 4060, 82, 13, 50920], "temperature": 0.0, "avg_logprob": -0.18307772827148439, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.016882799565792084}, {"id": 529, "seek": 311820, "start": 3129.96, "end": 3135.0, "text": " So I have an inherent skepticism because I would I kind of take the stance that actually all that", "tokens": [50952, 407, 286, 362, 364, 26387, 19128, 26356, 570, 286, 576, 286, 733, 295, 747, 264, 21033, 300, 767, 439, 300, 51204], "temperature": 0.0, "avg_logprob": -0.18307772827148439, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.016882799565792084}, {"id": 530, "seek": 311820, "start": 3135.0, "end": 3139.72, "text": " the internal dynamics are doing is instantiating a particularized form of the external dynamics.", "tokens": [51204, 264, 6920, 15679, 366, 884, 307, 9836, 72, 990, 257, 1729, 1602, 1254, 295, 264, 8320, 15679, 13, 51440], "temperature": 0.0, "avg_logprob": -0.18307772827148439, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.016882799565792084}, {"id": 531, "seek": 311820, "start": 3139.72, "end": 3145.0, "text": " And this is very easy once you take out the picture of a homunculized ego. Just if you look at it in", "tokens": [51440, 400, 341, 307, 588, 1858, 1564, 291, 747, 484, 264, 3036, 295, 257, 3655, 409, 2444, 1602, 14495, 13, 1449, 498, 291, 574, 412, 309, 294, 51704], "temperature": 0.0, "avg_logprob": -0.18307772827148439, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.016882799565792084}, {"id": 532, "seek": 314500, "start": 3145.0, "end": 3153.88, "text": " terms of just the particle physics, we're just a instantiation of some of the physics that defines", "tokens": [50364, 2115, 295, 445, 264, 12359, 10649, 11, 321, 434, 445, 257, 9836, 6642, 295, 512, 295, 264, 10649, 300, 23122, 50808], "temperature": 0.0, "avg_logprob": -0.10127143021468278, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.009390167891979218}, {"id": 533, "seek": 314500, "start": 3153.88, "end": 3159.4, "text": " everything else. But we just we just actually seem to have a kind of complexified Markov blanket or", "tokens": [50808, 1203, 1646, 13, 583, 321, 445, 321, 445, 767, 1643, 281, 362, 257, 733, 295, 3997, 2587, 3934, 5179, 17907, 420, 51084], "temperature": 0.0, "avg_logprob": -0.10127143021468278, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.009390167891979218}, {"id": 534, "seek": 314500, "start": 3159.4, "end": 3164.6, "text": " Markov blanket form of that. But there's nothing particularly distinct about that. And we'll come", "tokens": [51084, 3934, 5179, 17907, 1254, 295, 300, 13, 583, 456, 311, 1825, 4098, 10644, 466, 300, 13, 400, 321, 603, 808, 51344], "temperature": 0.0, "avg_logprob": -0.10127143021468278, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.009390167891979218}, {"id": 535, "seek": 314500, "start": 3164.6, "end": 3169.88, "text": " to consciousness because maybe there is. And that I guess is the big elephant in the room here.", "tokens": [51344, 281, 10081, 570, 1310, 456, 307, 13, 400, 300, 286, 2041, 307, 264, 955, 19791, 294, 264, 1808, 510, 13, 51608], "temperature": 0.0, "avg_logprob": -0.10127143021468278, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.009390167891979218}, {"id": 536, "seek": 316988, "start": 3170.36, "end": 3176.84, "text": " But it's funny that you mentioned Heidegger because Heidegger to my to my eyes, and in my", "tokens": [50388, 583, 309, 311, 4074, 300, 291, 2835, 634, 482, 39381, 570, 634, 482, 39381, 281, 452, 281, 452, 2575, 11, 293, 294, 452, 50712], "temperature": 0.0, "avg_logprob": -0.19125158136541193, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.004816767293959856}, {"id": 537, "seek": 316988, "start": 3176.84, "end": 3182.84, "text": " opinion, and then obviously Dreyfus and Merleau Ponty and the phenomenologists followed Heidegger,", "tokens": [50712, 4800, 11, 293, 550, 2745, 413, 7950, 69, 301, 293, 6124, 306, 1459, 31756, 874, 293, 264, 9388, 12256, 6263, 634, 482, 39381, 11, 51012], "temperature": 0.0, "avg_logprob": -0.19125158136541193, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.004816767293959856}, {"id": 538, "seek": 316988, "start": 3183.48, "end": 3192.2000000000003, "text": " really, he's the one who strikes at this established distinction between a subject and an object.", "tokens": [51044, 534, 11, 415, 311, 264, 472, 567, 16750, 412, 341, 7545, 16844, 1296, 257, 3983, 293, 364, 2657, 13, 51480], "temperature": 0.0, "avg_logprob": -0.19125158136541193, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.004816767293959856}, {"id": 539, "seek": 316988, "start": 3193.1600000000003, "end": 3197.56, "text": " So the Cartesian distinction. And then this is where his kind of gripes with Sartre came in.", "tokens": [51528, 407, 264, 22478, 42434, 16844, 13, 400, 550, 341, 307, 689, 702, 733, 295, 17865, 5190, 365, 318, 446, 265, 1361, 294, 13, 51748], "temperature": 0.0, "avg_logprob": -0.19125158136541193, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.004816767293959856}, {"id": 540, "seek": 319756, "start": 3198.36, "end": 3207.0, "text": " Now, my question is, is that, yes, if you read Heidegger, Dasein, the ground of Dasein is this", "tokens": [50404, 823, 11, 452, 1168, 307, 11, 307, 300, 11, 2086, 11, 498, 291, 1401, 634, 482, 39381, 11, 413, 651, 259, 11, 264, 2727, 295, 413, 651, 259, 307, 341, 50836], "temperature": 0.0, "avg_logprob": -0.1603791038945036, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0007550366572104394}, {"id": 541, "seek": 319756, "start": 3207.0, "end": 3213.16, "text": " kind of interrelationality. Alpha is the disclosure of truth in an interrelational relationship,", "tokens": [50836, 733, 295, 728, 4419, 1478, 507, 13, 20588, 307, 264, 30392, 295, 3494, 294, 364, 728, 4419, 1478, 2480, 11, 51144], "temperature": 0.0, "avg_logprob": -0.1603791038945036, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0007550366572104394}, {"id": 542, "seek": 319756, "start": 3213.16, "end": 3221.16, "text": " so to speak. But there still seems to be some kind of subject who, you know, the the hammerer who", "tokens": [51144, 370, 281, 1710, 13, 583, 456, 920, 2544, 281, 312, 512, 733, 295, 3983, 567, 11, 291, 458, 11, 264, 264, 13017, 260, 567, 51544], "temperature": 0.0, "avg_logprob": -0.1603791038945036, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0007550366572104394}, {"id": 543, "seek": 319756, "start": 3221.16, "end": 3225.72, "text": " is hammering away at the nail. Now, from the perspective of the hammerer, he's not some", "tokens": [51544, 307, 13017, 278, 1314, 412, 264, 10173, 13, 823, 11, 490, 264, 4585, 295, 264, 13017, 260, 11, 415, 311, 406, 512, 51772], "temperature": 0.0, "avg_logprob": -0.1603791038945036, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0007550366572104394}, {"id": 544, "seek": 322572, "start": 3225.72, "end": 3233.16, "text": " reified ego who is objectifying the nail that all that is really being witnessed there from the", "tokens": [50364, 319, 2587, 14495, 567, 307, 2657, 5489, 264, 10173, 300, 439, 300, 307, 534, 885, 21519, 456, 490, 264, 50736], "temperature": 0.0, "avg_logprob": -0.10602746064635529, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0018638826441019773}, {"id": 545, "seek": 322572, "start": 3233.16, "end": 3240.52, "text": " level of consciousness is the disclosure of activity. But still, we can give a description of a hammerer", "tokens": [50736, 1496, 295, 10081, 307, 264, 30392, 295, 5191, 13, 583, 920, 11, 321, 393, 976, 257, 3855, 295, 257, 13017, 260, 51104], "temperature": 0.0, "avg_logprob": -0.10602746064635529, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0018638826441019773}, {"id": 546, "seek": 322572, "start": 3240.52, "end": 3248.4399999999996, "text": " and a nail. So to what degree in your thinking, do you completely eradicate the subject object", "tokens": [51104, 293, 257, 10173, 13, 407, 281, 437, 4314, 294, 428, 1953, 11, 360, 291, 2584, 33078, 8700, 264, 3983, 2657, 51500], "temperature": 0.0, "avg_logprob": -0.10602746064635529, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0018638826441019773}, {"id": 547, "seek": 322572, "start": 3248.4399999999996, "end": 3253.64, "text": " distinction? And is there anything that you think predictive processing has to say about that?", "tokens": [51500, 16844, 30, 400, 307, 456, 1340, 300, 291, 519, 35521, 9007, 575, 281, 584, 466, 300, 30, 51760], "temperature": 0.0, "avg_logprob": -0.10602746064635529, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0018638826441019773}, {"id": 548, "seek": 325364, "start": 3254.6, "end": 3257.7999999999997, "text": " I think, well, the second question.", "tokens": [50412, 286, 519, 11, 731, 11, 264, 1150, 1168, 13, 50572], "temperature": 0.0, "avg_logprob": -0.18959305926067074, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.0026697199791669846}, {"id": 549, "seek": 325364, "start": 3263.7999999999997, "end": 3267.7999999999997, "text": " I'm happy to think about it with you. I have put a lot of thought into the first question.", "tokens": [50872, 286, 478, 2055, 281, 519, 466, 309, 365, 291, 13, 286, 362, 829, 257, 688, 295, 1194, 666, 264, 700, 1168, 13, 51072], "temperature": 0.0, "avg_logprob": -0.18959305926067074, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.0026697199791669846}, {"id": 550, "seek": 325364, "start": 3269.08, "end": 3274.92, "text": " And so, I mean, I think Filler is right in his book that our subjective objective divide is our", "tokens": [51136, 400, 370, 11, 286, 914, 11, 286, 519, 479, 10497, 307, 558, 294, 702, 1446, 300, 527, 25972, 10024, 9845, 307, 527, 51428], "temperature": 0.0, "avg_logprob": -0.18959305926067074, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.0026697199791669846}, {"id": 551, "seek": 325364, "start": 3274.92, "end": 3280.92, "text": " version of the appearance reality distinction. And it's just our version. In the ancient world,", "tokens": [51428, 3037, 295, 264, 8967, 4103, 16844, 13, 400, 309, 311, 445, 527, 3037, 13, 682, 264, 7832, 1002, 11, 51728], "temperature": 0.0, "avg_logprob": -0.18959305926067074, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.0026697199791669846}, {"id": 552, "seek": 328092, "start": 3281.0, "end": 3285.0, "text": " they had a different version. So ours is an in out metaphor. How is the inner and the outer", "tokens": [50368, 436, 632, 257, 819, 3037, 13, 407, 11896, 307, 364, 294, 484, 19157, 13, 1012, 307, 264, 7284, 293, 264, 10847, 50568], "temperature": 0.0, "avg_logprob": -0.11595273525156874, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.006483637727797031}, {"id": 553, "seek": 328092, "start": 3285.0, "end": 3289.64, "text": " connected? And the ancient world is the problem of the one in the midi. How is the lower and the", "tokens": [50568, 4582, 30, 400, 264, 7832, 1002, 307, 264, 1154, 295, 264, 472, 294, 264, 2062, 72, 13, 1012, 307, 264, 3126, 293, 264, 50800], "temperature": 0.0, "avg_logprob": -0.11595273525156874, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.006483637727797031}, {"id": 554, "seek": 328092, "start": 3289.64, "end": 3296.52, "text": " upper connected? And once you see that, you realize that there's a deeper structural problem that", "tokens": [50800, 6597, 4582, 30, 400, 1564, 291, 536, 300, 11, 291, 4325, 300, 456, 311, 257, 7731, 15067, 1154, 300, 51144], "temperature": 0.0, "avg_logprob": -0.11595273525156874, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.006483637727797031}, {"id": 555, "seek": 328092, "start": 3296.52, "end": 3303.96, "text": " isn't bound to the inner outer or the emergence and the emanation. It's the connectivity issue.", "tokens": [51144, 1943, 380, 5472, 281, 264, 7284, 10847, 420, 264, 36211, 293, 264, 28211, 399, 13, 467, 311, 264, 21095, 2734, 13, 51516], "temperature": 0.0, "avg_logprob": -0.11595273525156874, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.006483637727797031}, {"id": 556, "seek": 330396, "start": 3304.68, "end": 3313.16, "text": " And what Filler argues, and there's a lot of other people converging on this, is if you try to,", "tokens": [50400, 400, 437, 479, 10497, 38218, 11, 293, 456, 311, 257, 688, 295, 661, 561, 9652, 3249, 322, 341, 11, 307, 498, 291, 853, 281, 11, 50824], "temperature": 0.0, "avg_logprob": -0.1355405924271564, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0226169191300869}, {"id": 557, "seek": 330396, "start": 3313.16, "end": 3318.6, "text": " if you go with an Aristotelian metaphysics, or that what's most real are substances in the", "tokens": [50824, 498, 291, 352, 365, 364, 31310, 310, 338, 952, 30946, 41732, 11, 420, 300, 437, 311, 881, 957, 366, 25455, 294, 264, 51096], "temperature": 0.0, "avg_logprob": -0.1355405924271564, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0226169191300869}, {"id": 558, "seek": 330396, "start": 3318.6, "end": 3325.2400000000002, "text": " Aristotelian sense, which are things that can independently exist, then this becomes a big", "tokens": [51096, 31310, 310, 338, 952, 2020, 11, 597, 366, 721, 300, 393, 21761, 2514, 11, 550, 341, 3643, 257, 955, 51428], "temperature": 0.0, "avg_logprob": -0.1355405924271564, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0226169191300869}, {"id": 559, "seek": 330396, "start": 3325.2400000000002, "end": 3330.76, "text": " problem for you. And that kind of ontology, I would argue, and I can make the argument,", "tokens": [51428, 1154, 337, 291, 13, 400, 300, 733, 295, 6592, 1793, 11, 286, 576, 9695, 11, 293, 286, 393, 652, 264, 6770, 11, 51704], "temperature": 0.0, "avg_logprob": -0.1355405924271564, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0226169191300869}, {"id": 560, "seek": 333076, "start": 3330.76, "end": 3337.32, "text": " drives you towards a nominalistic epistemology. And then you get, you know, Occam's version of it,", "tokens": [50364, 11754, 291, 3030, 257, 41641, 3142, 2388, 43958, 1793, 13, 400, 550, 291, 483, 11, 291, 458, 11, 26191, 335, 311, 3037, 295, 309, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1094216285867894, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0169001966714859}, {"id": 561, "seek": 333076, "start": 3337.32, "end": 3342.92, "text": " or Kant's version of it, where all of the patterns and all the information are just in the mind,", "tokens": [50692, 420, 40927, 311, 3037, 295, 309, 11, 689, 439, 295, 264, 8294, 293, 439, 264, 1589, 366, 445, 294, 264, 1575, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1094216285867894, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0169001966714859}, {"id": 562, "seek": 333076, "start": 3342.92, "end": 3348.92, "text": " which means the mind is radically other, because it is the only place that the information", "tokens": [50972, 597, 1355, 264, 1575, 307, 35508, 661, 11, 570, 309, 307, 264, 787, 1081, 300, 264, 1589, 51272], "temperature": 0.0, "avg_logprob": -0.1094216285867894, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0169001966714859}, {"id": 563, "seek": 333076, "start": 3348.92, "end": 3356.0400000000004, "text": " intelligibility actually exists. And the world is profoundly absurd in a deep, deep way. And I", "tokens": [51272, 5613, 2841, 767, 8198, 13, 400, 264, 1002, 307, 39954, 19774, 294, 257, 2452, 11, 2452, 636, 13, 400, 286, 51628], "temperature": 0.0, "avg_logprob": -0.1094216285867894, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0169001966714859}, {"id": 564, "seek": 335604, "start": 3356.04, "end": 3362.52, "text": " think that leads you into, well, first of all, it makes science impossible. And it leads you into", "tokens": [50364, 519, 300, 6689, 291, 666, 11, 731, 11, 700, 295, 439, 11, 309, 1669, 3497, 6243, 13, 400, 309, 6689, 291, 666, 50688], "temperature": 0.0, "avg_logprob": -0.10075176239013672, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00400471780449152}, {"id": 565, "seek": 335604, "start": 3362.52, "end": 3369.48, "text": " all kinds of existential and moral dilemmas. And if you then take a look at the logic of trying to", "tokens": [50688, 439, 3685, 295, 37133, 293, 9723, 25623, 2174, 296, 13, 400, 498, 291, 550, 747, 257, 574, 412, 264, 9952, 295, 1382, 281, 51036], "temperature": 0.0, "avg_logprob": -0.10075176239013672, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00400471780449152}, {"id": 566, "seek": 335604, "start": 3369.48, "end": 3374.36, "text": " get, you can't actually, and this is what Filler does very carefully, you can't actually get relations", "tokens": [51036, 483, 11, 291, 393, 380, 767, 11, 293, 341, 307, 437, 479, 10497, 775, 588, 7500, 11, 291, 393, 380, 767, 483, 2299, 51280], "temperature": 0.0, "avg_logprob": -0.10075176239013672, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00400471780449152}, {"id": 567, "seek": 335604, "start": 3375.0, "end": 3380.92, "text": " out of properties that belong to the Relata. The relation has to precede the Relata in a very", "tokens": [51312, 484, 295, 7221, 300, 5784, 281, 264, 8738, 3274, 13, 440, 9721, 575, 281, 16969, 68, 264, 8738, 3274, 294, 257, 588, 51608], "temperature": 0.0, "avg_logprob": -0.10075176239013672, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00400471780449152}, {"id": 568, "seek": 338092, "start": 3380.92, "end": 3388.36, "text": " important way. So I think there's a deep, at a very deep level, I do want to call it into question.", "tokens": [50364, 1021, 636, 13, 407, 286, 519, 456, 311, 257, 2452, 11, 412, 257, 588, 2452, 1496, 11, 286, 360, 528, 281, 818, 309, 666, 1168, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09596579682593252, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.03356330096721649}, {"id": 569, "seek": 338092, "start": 3388.36, "end": 3398.12, "text": " I do want to challenge that. First of all, it's historical. It's cultural. We pretend as if", "tokens": [50736, 286, 360, 528, 281, 3430, 300, 13, 2386, 295, 439, 11, 309, 311, 8584, 13, 467, 311, 6988, 13, 492, 11865, 382, 498, 51224], "temperature": 0.0, "avg_logprob": -0.09596579682593252, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.03356330096721649}, {"id": 570, "seek": 338092, "start": 3398.12, "end": 3402.92, "text": " that is the only way in which human beings have related to the world. Like I said, even in the", "tokens": [51224, 300, 307, 264, 787, 636, 294, 597, 1952, 8958, 362, 4077, 281, 264, 1002, 13, 1743, 286, 848, 11, 754, 294, 264, 51464], "temperature": 0.0, "avg_logprob": -0.09596579682593252, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.03356330096721649}, {"id": 571, "seek": 338092, "start": 3402.92, "end": 3407.32, "text": " West, that's not their problem in the ancient world. Their problem is the problem of the one in", "tokens": [51464, 4055, 11, 300, 311, 406, 641, 1154, 294, 264, 7832, 1002, 13, 6710, 1154, 307, 264, 1154, 295, 264, 472, 294, 51684], "temperature": 0.0, "avg_logprob": -0.09596579682593252, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.03356330096721649}, {"id": 572, "seek": 340732, "start": 3407.32, "end": 3412.1200000000003, "text": " the many. It's not the problem of being in the world. Now, what does this all ground in? I think", "tokens": [50364, 264, 867, 13, 467, 311, 406, 264, 1154, 295, 885, 294, 264, 1002, 13, 823, 11, 437, 775, 341, 439, 2727, 294, 30, 286, 519, 50604], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 573, "seek": 340732, "start": 3412.1200000000003, "end": 3417.0800000000004, "text": " this grounds in the common problem of the relationship between appearance and reality. And", "tokens": [50604, 341, 19196, 294, 264, 2689, 1154, 295, 264, 2480, 1296, 8967, 293, 4103, 13, 400, 50852], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 574, "seek": 340732, "start": 3417.0800000000004, "end": 3421.32, "text": " here's where I could now say something that aligns with my previous argument about relationality.", "tokens": [50852, 510, 311, 689, 286, 727, 586, 584, 746, 300, 7975, 82, 365, 452, 3894, 6770, 466, 38444, 507, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 575, "seek": 340732, "start": 3422.36, "end": 3427.7200000000003, "text": " See, so, and I'm going to borrow a term from rapport here, the hermeneutics of suspicion. The", "tokens": [51116, 3008, 11, 370, 11, 293, 286, 478, 516, 281, 11172, 257, 1433, 490, 18018, 510, 11, 264, 720, 76, 1450, 325, 1167, 295, 32020, 13, 440, 51384], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 576, "seek": 340732, "start": 3427.7200000000003, "end": 3432.52, "text": " hermeneutics of suspicion is that a parent, and we got it because of Freud and Nietzsche and", "tokens": [51384, 720, 76, 1450, 325, 1167, 295, 32020, 307, 300, 257, 2596, 11, 293, 321, 658, 309, 570, 295, 41590, 293, 36583, 89, 12287, 293, 51624], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 577, "seek": 340732, "start": 3432.52, "end": 3436.36, "text": " blah, blah, blah. There's historical reasons. And those are valuable critiques, by the way.", "tokens": [51624, 12288, 11, 12288, 11, 12288, 13, 821, 311, 8584, 4112, 13, 400, 729, 366, 8263, 3113, 4911, 11, 538, 264, 636, 13, 51816], "temperature": 0.0, "avg_logprob": -0.13269770392056168, "compression_ratio": 1.7461300309597523, "no_speech_prob": 0.01280701719224453}, {"id": 578, "seek": 343636, "start": 3436.36, "end": 3441.08, "text": " But the hermeneutics of suspicion says that appearances are distorting. They're deceptive.", "tokens": [50364, 583, 264, 720, 76, 1450, 325, 1167, 295, 32020, 1619, 300, 29174, 366, 37555, 278, 13, 814, 434, 368, 1336, 488, 13, 50600], "temperature": 0.0, "avg_logprob": -0.15459149360656738, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.000487733690533787}, {"id": 579, "seek": 343636, "start": 3441.08, "end": 3446.84, "text": " They're destructive, right? They're disruptive. And what we should do is always question whether", "tokens": [50600, 814, 434, 26960, 11, 558, 30, 814, 434, 37865, 13, 400, 437, 321, 820, 360, 307, 1009, 1168, 1968, 50888], "temperature": 0.0, "avg_logprob": -0.15459149360656738, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.000487733690533787}, {"id": 580, "seek": 343636, "start": 3446.84, "end": 3454.1200000000003, "text": " or not they are leading us into reality. Now, Marlo Ponte has a great argument against that,", "tokens": [50888, 420, 406, 436, 366, 5775, 505, 666, 4103, 13, 823, 11, 2039, 752, 430, 10219, 575, 257, 869, 6770, 1970, 300, 11, 51252], "temperature": 0.0, "avg_logprob": -0.15459149360656738, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.000487733690533787}, {"id": 581, "seek": 343636, "start": 3454.1200000000003, "end": 3462.6, "text": " that he gets from, maybe he doesn't get, but it's in Plato, which is, wait, you're treating real like", "tokens": [51252, 300, 415, 2170, 490, 11, 1310, 415, 1177, 380, 483, 11, 457, 309, 311, 294, 43027, 11, 597, 307, 11, 1699, 11, 291, 434, 15083, 957, 411, 51676], "temperature": 0.0, "avg_logprob": -0.15459149360656738, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.000487733690533787}, {"id": 582, "seek": 346260, "start": 3462.68, "end": 3467.72, "text": " red. Like, you could just say, if you could look at an isolated thing and say that's real.", "tokens": [50368, 2182, 13, 1743, 11, 291, 727, 445, 584, 11, 498, 291, 727, 574, 412, 364, 14621, 551, 293, 584, 300, 311, 957, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11968690273808498, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06182071194052696}, {"id": 583, "seek": 346260, "start": 3467.72, "end": 3473.56, "text": " But real is a comparative term. So, of course, is illusion. To say this is an illusion is to say", "tokens": [50620, 583, 957, 307, 257, 39292, 1433, 13, 407, 11, 295, 1164, 11, 307, 18854, 13, 1407, 584, 341, 307, 364, 18854, 307, 281, 584, 50912], "temperature": 0.0, "avg_logprob": -0.11968690273808498, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06182071194052696}, {"id": 584, "seek": 346260, "start": 3473.56, "end": 3480.68, "text": " this is an illusion in comparison, in relation to something that is more real. And that thing", "tokens": [50912, 341, 307, 364, 18854, 294, 9660, 11, 294, 9721, 281, 746, 300, 307, 544, 957, 13, 400, 300, 551, 51268], "temperature": 0.0, "avg_logprob": -0.11968690273808498, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06182071194052696}, {"id": 585, "seek": 346260, "start": 3480.68, "end": 3486.52, "text": " is in relation, so on and so forth, right? And what that means is, first of all, you have to see", "tokens": [51268, 307, 294, 9721, 11, 370, 322, 293, 370, 5220, 11, 558, 30, 400, 437, 300, 1355, 307, 11, 700, 295, 439, 11, 291, 362, 281, 536, 51560], "temperature": 0.0, "avg_logprob": -0.11968690273808498, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06182071194052696}, {"id": 586, "seek": 348652, "start": 3486.52, "end": 3492.68, "text": " that those judgments are inherently relational. And secondly, you have to call into question", "tokens": [50364, 300, 729, 40337, 366, 27993, 38444, 13, 400, 26246, 11, 291, 362, 281, 818, 666, 1168, 50672], "temperature": 0.0, "avg_logprob": -0.09444223601242592, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.08143199980258942}, {"id": 587, "seek": 348652, "start": 3495.32, "end": 3500.7599999999998, "text": " the independence of the hermeneutics of suspicion. The hermeneutics of suspicion is actually", "tokens": [50804, 264, 14640, 295, 264, 720, 76, 1450, 325, 1167, 295, 32020, 13, 440, 720, 76, 1450, 325, 1167, 295, 32020, 307, 767, 51076], "temperature": 0.0, "avg_logprob": -0.09444223601242592, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.08143199980258942}, {"id": 588, "seek": 348652, "start": 3500.7599999999998, "end": 3507.72, "text": " parasitic on the hermeneutics of beauty. It depends on there being things that we agree on", "tokens": [51076, 21012, 44592, 322, 264, 720, 76, 1450, 325, 1167, 295, 6643, 13, 467, 5946, 322, 456, 885, 721, 300, 321, 3986, 322, 51424], "temperature": 0.0, "avg_logprob": -0.09444223601242592, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.08143199980258942}, {"id": 589, "seek": 348652, "start": 3507.72, "end": 3513.96, "text": " by saying that's where appearance discloses reality rather than distorting it, right?", "tokens": [51424, 538, 1566, 300, 311, 689, 8967, 17092, 4201, 4103, 2831, 813, 37555, 278, 309, 11, 558, 30, 51736], "temperature": 0.0, "avg_logprob": -0.09444223601242592, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.08143199980258942}, {"id": 590, "seek": 351396, "start": 3513.96, "end": 3520.2, "text": " And then as soon as you do that, that undercuts the deep divide, because you ultimately make these", "tokens": [50364, 400, 550, 382, 2321, 382, 291, 360, 300, 11, 300, 833, 26158, 264, 2452, 9845, 11, 570, 291, 6284, 652, 613, 50676], "temperature": 0.0, "avg_logprob": -0.13627320841739052, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0010152127360925078}, {"id": 591, "seek": 351396, "start": 3520.2, "end": 3525.2400000000002, "text": " divisions between the one and the many, or the subject and the object, by getting a hermeneutics", "tokens": [50676, 24328, 1296, 264, 472, 293, 264, 867, 11, 420, 264, 3983, 293, 264, 2657, 11, 538, 1242, 257, 720, 76, 1450, 325, 1167, 50928], "temperature": 0.0, "avg_logprob": -0.13627320841739052, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0010152127360925078}, {"id": 592, "seek": 351396, "start": 3525.2400000000002, "end": 3529.88, "text": " of suspicion into the appearance reality distinction. That's at least the argument I would make.", "tokens": [50928, 295, 32020, 666, 264, 8967, 4103, 16844, 13, 663, 311, 412, 1935, 264, 6770, 286, 576, 652, 13, 51160], "temperature": 0.0, "avg_logprob": -0.13627320841739052, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0010152127360925078}, {"id": 593, "seek": 351396, "start": 3530.92, "end": 3539.56, "text": " Interesting. Yeah, I like the suspicion of binary-ness inherent in all of this. It speaks", "tokens": [51212, 14711, 13, 865, 11, 286, 411, 264, 32020, 295, 17434, 12, 1287, 26387, 294, 439, 295, 341, 13, 467, 10789, 51644], "temperature": 0.0, "avg_logprob": -0.13627320841739052, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0010152127360925078}, {"id": 594, "seek": 351396, "start": 3539.56, "end": 3542.6, "text": " to me, and I think it speaks to people who are interested in active inference, and the notion", "tokens": [51644, 281, 385, 11, 293, 286, 519, 309, 10789, 281, 561, 567, 366, 3102, 294, 4967, 38253, 11, 293, 264, 10710, 51796], "temperature": 0.0, "avg_logprob": -0.13627320841739052, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0010152127360925078}, {"id": 595, "seek": 354260, "start": 3542.6, "end": 3549.08, "text": " of sort of philosophical vagueness as well. At what point is something real? It's fundamentally", "tokens": [50364, 295, 1333, 295, 25066, 13501, 7801, 442, 382, 731, 13, 1711, 437, 935, 307, 746, 957, 30, 467, 311, 17879, 50688], "temperature": 0.0, "avg_logprob": -0.16542962018181295, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0026192169170826674}, {"id": 596, "seek": 354260, "start": 3549.08, "end": 3556.68, "text": " a comparative term in the web of things that could be considered real. Yeah. That's right.", "tokens": [50688, 257, 39292, 1433, 294, 264, 3670, 295, 721, 300, 727, 312, 4888, 957, 13, 865, 13, 663, 311, 558, 13, 51068], "temperature": 0.0, "avg_logprob": -0.16542962018181295, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0026192169170826674}, {"id": 597, "seek": 354260, "start": 3556.68, "end": 3565.4, "text": " Yeah. I'm trying to do this without invoking a binary, but you mentioned earlier that four", "tokens": [51068, 865, 13, 286, 478, 1382, 281, 360, 341, 1553, 1048, 5953, 257, 17434, 11, 457, 291, 2835, 3071, 300, 1451, 51504], "temperature": 0.0, "avg_logprob": -0.16542962018181295, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0026192169170826674}, {"id": 598, "seek": 354260, "start": 3565.4, "end": 3571.64, "text": " E cognitive scientists do have their critiques of active inference and predictive processing.", "tokens": [51504, 462, 15605, 7708, 360, 362, 641, 3113, 4911, 295, 4967, 38253, 293, 35521, 9007, 13, 51816], "temperature": 0.0, "avg_logprob": -0.16542962018181295, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0026192169170826674}, {"id": 599, "seek": 357164, "start": 3572.04, "end": 3577.3199999999997, "text": " I will stick with active inference actually for now, and I will explain why later. It's not that", "tokens": [50384, 286, 486, 2897, 365, 4967, 38253, 767, 337, 586, 11, 293, 286, 486, 2903, 983, 1780, 13, 467, 311, 406, 300, 50648], "temperature": 0.0, "avg_logprob": -0.15983081475282326, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.0008018890512175858}, {"id": 600, "seek": 357164, "start": 3577.3199999999997, "end": 3587.08, "text": " interesting. So this is people like Tony Camaro or Ed Bags who are radical inactivists, and what", "tokens": [50648, 1880, 13, 407, 341, 307, 561, 411, 10902, 6886, 9708, 420, 3977, 363, 12109, 567, 366, 12001, 294, 23397, 1751, 11, 293, 437, 51136], "temperature": 0.0, "avg_logprob": -0.15983081475282326, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.0008018890512175858}, {"id": 601, "seek": 357164, "start": 3587.08, "end": 3595.0, "text": " they're critiquing, in a sense, is an interdelist picture of active inference. Again, I'm not putting", "tokens": [51136, 436, 434, 3113, 3221, 278, 11, 294, 257, 2020, 11, 307, 364, 728, 18105, 468, 3036, 295, 4967, 38253, 13, 3764, 11, 286, 478, 406, 3372, 51532], "temperature": 0.0, "avg_logprob": -0.15983081475282326, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.0008018890512175858}, {"id": 602, "seek": 359500, "start": 3595.08, "end": 3602.6, "text": " words into people's mouths, but if you read Jacob Howey's 2016 paper about self-evidencing,", "tokens": [50368, 2283, 666, 561, 311, 33171, 11, 457, 498, 291, 1401, 14117, 1012, 2030, 311, 6549, 3035, 466, 2698, 12, 13379, 4380, 2175, 11, 50744], "temperature": 0.0, "avg_logprob": -0.13127933848987927, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.08815328031778336}, {"id": 603, "seek": 359500, "start": 3603.88, "end": 3609.88, "text": " some of the language may to some people imply that there is a homunculus that has a representational", "tokens": [50808, 512, 295, 264, 2856, 815, 281, 512, 561, 33616, 300, 456, 307, 257, 3655, 409, 36002, 300, 575, 257, 2906, 1478, 51108], "temperature": 0.0, "avg_logprob": -0.13127933848987927, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.08815328031778336}, {"id": 604, "seek": 359500, "start": 3609.88, "end": 3619.96, "text": " picture of the world. This, to the ears of an inactivist, is deeply worrying. I would love to,", "tokens": [51108, 3036, 295, 264, 1002, 13, 639, 11, 281, 264, 8798, 295, 364, 294, 23397, 468, 11, 307, 8760, 18788, 13, 286, 576, 959, 281, 11, 51612], "temperature": 0.0, "avg_logprob": -0.13127933848987927, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.08815328031778336}, {"id": 605, "seek": 361996, "start": 3619.96, "end": 3625.2400000000002, "text": " as a four E cognitive scientist with obviously a vested interest in predictive processing as well,", "tokens": [50364, 382, 257, 1451, 462, 15605, 12662, 365, 2745, 257, 49317, 1179, 294, 35521, 9007, 382, 731, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1583576752589299, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.003998189233243465}, {"id": 606, "seek": 361996, "start": 3625.2400000000002, "end": 3631.56, "text": " I'd love to just hear your take on that debate and, again, without striking up binaries,", "tokens": [50628, 286, 1116, 959, 281, 445, 1568, 428, 747, 322, 300, 7958, 293, 11, 797, 11, 1553, 18559, 493, 5171, 4889, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1583576752589299, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.003998189233243465}, {"id": 607, "seek": 361996, "start": 3632.12, "end": 3635.2400000000002, "text": " and whether you think those critiques are legitimate.", "tokens": [50972, 293, 1968, 291, 519, 729, 3113, 4911, 366, 17956, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1583576752589299, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.003998189233243465}, {"id": 608, "seek": 361996, "start": 3637.4, "end": 3640.68, "text": " Well, I mean, it depends what you mean by legitimate. I mean, there's also the ones", "tokens": [51236, 1042, 11, 286, 914, 11, 309, 5946, 437, 291, 914, 538, 17956, 13, 286, 914, 11, 456, 311, 611, 264, 2306, 51400], "temperature": 0.0, "avg_logprob": -0.1583576752589299, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.003998189233243465}, {"id": 609, "seek": 361996, "start": 3640.68, "end": 3645.8, "text": " that Evan Thompson made, and I hold Evan in a very high regard. I mean, they're legitimate in that", "tokens": [51400, 300, 22613, 23460, 1027, 11, 293, 286, 1797, 22613, 294, 257, 588, 1090, 3843, 13, 286, 914, 11, 436, 434, 17956, 294, 300, 51656], "temperature": 0.0, "avg_logprob": -0.1583576752589299, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.003998189233243465}, {"id": 610, "seek": 364580, "start": 3645.8, "end": 3652.6800000000003, "text": " they are well-made arguments in peer-reviewed journals, and so we have no right to be dismissive", "tokens": [50364, 436, 366, 731, 12, 10341, 12869, 294, 15108, 12, 265, 1759, 292, 29621, 11, 293, 370, 321, 362, 572, 558, 281, 312, 16974, 488, 50708], "temperature": 0.0, "avg_logprob": -0.14295125007629395, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.002789141843095422}, {"id": 611, "seek": 364580, "start": 3652.6800000000003, "end": 3661.7200000000003, "text": " of them. Well, if I was to say that let's take as an axiom of active inference. This isn't your case,", "tokens": [50708, 295, 552, 13, 1042, 11, 498, 286, 390, 281, 584, 300, 718, 311, 747, 382, 364, 6360, 72, 298, 295, 4967, 38253, 13, 639, 1943, 380, 428, 1389, 11, 51160], "temperature": 0.0, "avg_logprob": -0.14295125007629395, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.002789141843095422}, {"id": 612, "seek": 364580, "start": 3661.7200000000003, "end": 3667.48, "text": " but let's just say, for sake of argument, that you have some interdelist representations of a", "tokens": [51160, 457, 718, 311, 445, 584, 11, 337, 9717, 295, 6770, 11, 300, 291, 362, 512, 728, 18105, 468, 33358, 295, 257, 51448], "temperature": 0.0, "avg_logprob": -0.14295125007629395, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.002789141843095422}, {"id": 613, "seek": 364580, "start": 3667.48, "end": 3673.1600000000003, "text": " statistical model. So you are, in a sense, you're not just embodying a model. You actually have a", "tokens": [51448, 22820, 2316, 13, 407, 291, 366, 11, 294, 257, 2020, 11, 291, 434, 406, 445, 42575, 278, 257, 2316, 13, 509, 767, 362, 257, 51732], "temperature": 0.0, "avg_logprob": -0.14295125007629395, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.002789141843095422}, {"id": 614, "seek": 367316, "start": 3673.16, "end": 3678.3599999999997, "text": " model. So again, it comes back to that distinction we're talking about. Would an inactivist critique", "tokens": [50364, 2316, 13, 407, 797, 11, 309, 1487, 646, 281, 300, 16844, 321, 434, 1417, 466, 13, 6068, 364, 294, 23397, 468, 25673, 50624], "temperature": 0.0, "avg_logprob": -0.11573442236169593, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.00023771652195136994}, {"id": 615, "seek": 367316, "start": 3678.3599999999997, "end": 3682.52, "text": " of that internalism be justified?", "tokens": [50624, 295, 300, 6920, 1434, 312, 27808, 30, 50832], "temperature": 0.0, "avg_logprob": -0.11573442236169593, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.00023771652195136994}, {"id": 616, "seek": 367316, "start": 3687.24, "end": 3694.2799999999997, "text": " So the reason I'm hesitating is this is landing on the swamp of what do we mean by representation,", "tokens": [51068, 407, 264, 1778, 286, 478, 10453, 16350, 307, 341, 307, 11202, 322, 264, 31724, 295, 437, 360, 321, 914, 538, 10290, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11573442236169593, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.00023771652195136994}, {"id": 617, "seek": 367316, "start": 3695.3999999999996, "end": 3702.7599999999998, "text": " which, I mean, so does the thermostat represent the temperature in the environment?", "tokens": [51476, 597, 11, 286, 914, 11, 370, 775, 264, 8810, 39036, 2906, 264, 4292, 294, 264, 2823, 30, 51844], "temperature": 0.0, "avg_logprob": -0.11573442236169593, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.00023771652195136994}, {"id": 618, "seek": 370316, "start": 3704.12, "end": 3708.7599999999998, "text": " And everything I'm going to say is controversial because of the swamp. So I want that understood,", "tokens": [50412, 400, 1203, 286, 478, 516, 281, 584, 307, 17323, 570, 295, 264, 31724, 13, 407, 286, 528, 300, 7320, 11, 50644], "temperature": 0.0, "avg_logprob": -0.15762882232666015, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.00021650239068549126}, {"id": 619, "seek": 370316, "start": 3708.7599999999998, "end": 3718.52, "text": " please. I would say no, because what's needed is there might be some, there might be co-variation,", "tokens": [50644, 1767, 13, 286, 576, 584, 572, 11, 570, 437, 311, 2978, 307, 456, 1062, 312, 512, 11, 456, 1062, 312, 598, 12, 34033, 399, 11, 51132], "temperature": 0.0, "avg_logprob": -0.15762882232666015, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.00021650239068549126}, {"id": 620, "seek": 370316, "start": 3718.52, "end": 3725.56, "text": " but what I think the critiques of the co-variation model of representation locks ultimately model.", "tokens": [51132, 457, 437, 286, 519, 264, 3113, 4911, 295, 264, 598, 12, 34033, 399, 2316, 295, 10290, 20703, 6284, 2316, 13, 51484], "temperature": 0.0, "avg_logprob": -0.15762882232666015, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.00021650239068549126}, {"id": 621, "seek": 370316, "start": 3726.68, "end": 3731.24, "text": " So we had the idea that representations have to be similar to what they represent. That's how they", "tokens": [51540, 407, 321, 632, 264, 1558, 300, 33358, 362, 281, 312, 2531, 281, 437, 436, 2906, 13, 663, 311, 577, 436, 51768], "temperature": 0.0, "avg_logprob": -0.15762882232666015, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.00021650239068549126}, {"id": 622, "seek": 373124, "start": 3731.24, "end": 3735.9599999999996, "text": " represent. And then you have all the problems with similarity and Aristotle even could bring that down.", "tokens": [50364, 2906, 13, 400, 550, 291, 362, 439, 264, 2740, 365, 32194, 293, 42368, 754, 727, 1565, 300, 760, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13031308352947235, "compression_ratio": 1.837037037037037, "no_speech_prob": 0.0018094269325956702}, {"id": 623, "seek": 373124, "start": 3736.9199999999996, "end": 3741.72, "text": " And then lock replaced it with the co-variation model. To have a representation is I have something", "tokens": [50648, 400, 550, 4017, 10772, 309, 365, 264, 598, 12, 34033, 399, 2316, 13, 1407, 362, 257, 10290, 307, 286, 362, 746, 50888], "temperature": 0.0, "avg_logprob": -0.13031308352947235, "compression_ratio": 1.837037037037037, "no_speech_prob": 0.0018094269325956702}, {"id": 624, "seek": 373124, "start": 3741.72, "end": 3745.9599999999996, "text": " in my head that reliably co-varies with something in the world, and that's how it represents it.", "tokens": [50888, 294, 452, 1378, 300, 49927, 598, 12, 85, 4889, 365, 746, 294, 264, 1002, 11, 293, 300, 311, 577, 309, 8855, 309, 13, 51100], "temperature": 0.0, "avg_logprob": -0.13031308352947235, "compression_ratio": 1.837037037037037, "no_speech_prob": 0.0018094269325956702}, {"id": 625, "seek": 373124, "start": 3745.9599999999996, "end": 3752.2799999999997, "text": " And then the problem with the co-variations is they don't give you the specificity, for example,", "tokens": [51100, 400, 550, 264, 1154, 365, 264, 598, 12, 34033, 763, 307, 436, 500, 380, 976, 291, 264, 2685, 507, 11, 337, 1365, 11, 51416], "temperature": 0.0, "avg_logprob": -0.13031308352947235, "compression_ratio": 1.837037037037037, "no_speech_prob": 0.0018094269325956702}, {"id": 626, "seek": 373124, "start": 3752.2799999999997, "end": 3759.7999999999997, "text": " of thought. So this is co-varying with a bottle, with a tool, with a man-made object. Which is it?", "tokens": [51416, 295, 1194, 13, 407, 341, 307, 598, 12, 85, 822, 278, 365, 257, 7817, 11, 365, 257, 2290, 11, 365, 257, 587, 12, 10341, 2657, 13, 3013, 307, 309, 30, 51792], "temperature": 0.0, "avg_logprob": -0.13031308352947235, "compression_ratio": 1.837037037037037, "no_speech_prob": 0.0018094269325956702}, {"id": 627, "seek": 375980, "start": 3759.8, "end": 3765.7200000000003, "text": " Those are not the same things. Those are not the same ideas, but this is co-varying causally with", "tokens": [50364, 3950, 366, 406, 264, 912, 721, 13, 3950, 366, 406, 264, 912, 3487, 11, 457, 341, 307, 598, 12, 85, 822, 278, 3302, 379, 365, 50660], "temperature": 0.0, "avg_logprob": -0.10825929738054371, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0014321055496111512}, {"id": 628, "seek": 375980, "start": 3765.7200000000003, "end": 3772.6800000000003, "text": " all of that. And you have the problem then of aspectualization. And the Lockian answer, of", "tokens": [50660, 439, 295, 300, 13, 400, 291, 362, 264, 1154, 550, 295, 382, 1043, 901, 2144, 13, 400, 264, 16736, 952, 1867, 11, 295, 51008], "temperature": 0.0, "avg_logprob": -0.10825929738054371, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0014321055496111512}, {"id": 629, "seek": 375980, "start": 3772.6800000000003, "end": 3781.5600000000004, "text": " course, is we get aspects by doing representation. But I agree with Searle that that's the wrong", "tokens": [51008, 1164, 11, 307, 321, 483, 7270, 538, 884, 10290, 13, 583, 286, 3986, 365, 1100, 36153, 300, 300, 311, 264, 2085, 51452], "temperature": 0.0, "avg_logprob": -0.10825929738054371, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0014321055496111512}, {"id": 630, "seek": 375980, "start": 3781.5600000000004, "end": 3789.4, "text": " way around, that any representation is inherently aspectual. When I represent this as a bottle,", "tokens": [51452, 636, 926, 11, 300, 604, 10290, 307, 27993, 382, 1043, 901, 13, 1133, 286, 2906, 341, 382, 257, 7817, 11, 51844], "temperature": 0.0, "avg_logprob": -0.10825929738054371, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0014321055496111512}, {"id": 631, "seek": 378940, "start": 3789.4, "end": 3795.08, "text": " I'm only picking up on some of its properties insofar as they are relevant to each other,", "tokens": [50364, 286, 478, 787, 8867, 493, 322, 512, 295, 1080, 7221, 294, 539, 21196, 382, 436, 366, 7340, 281, 1184, 661, 11, 50648], "temperature": 0.0, "avg_logprob": -0.058773922709237154, "compression_ratio": 1.8429752066115703, "no_speech_prob": 0.0016999845393002033}, {"id": 632, "seek": 378940, "start": 3795.08, "end": 3801.7200000000003, "text": " insofar as they are relevant to me. So representation depends on aspectualization,", "tokens": [50648, 294, 539, 21196, 382, 436, 366, 7340, 281, 385, 13, 407, 10290, 5946, 322, 382, 1043, 901, 2144, 11, 50980], "temperature": 0.0, "avg_logprob": -0.058773922709237154, "compression_ratio": 1.8429752066115703, "no_speech_prob": 0.0016999845393002033}, {"id": 633, "seek": 378940, "start": 3801.7200000000003, "end": 3806.12, "text": " which depends on relevance realization. And this is not what you do with relevance", "tokens": [50980, 597, 5946, 322, 32684, 25138, 13, 400, 341, 307, 406, 437, 291, 360, 365, 32684, 51200], "temperature": 0.0, "avg_logprob": -0.058773922709237154, "compression_ratio": 1.8429752066115703, "no_speech_prob": 0.0016999845393002033}, {"id": 634, "seek": 378940, "start": 3806.12, "end": 3812.12, "text": " realization. You do not represent all the facts, judge them to be irrelevant, and then zero in.", "tokens": [51200, 25138, 13, 509, 360, 406, 2906, 439, 264, 9130, 11, 6995, 552, 281, 312, 28682, 11, 293, 550, 4018, 294, 13, 51500], "temperature": 0.0, "avg_logprob": -0.058773922709237154, "compression_ratio": 1.8429752066115703, "no_speech_prob": 0.0016999845393002033}, {"id": 635, "seek": 378940, "start": 3812.12, "end": 3818.92, "text": " So it's ultimately non-representational. So what I would say is, I don't know that some level,", "tokens": [51500, 407, 309, 311, 6284, 2107, 12, 19919, 11662, 1478, 13, 407, 437, 286, 576, 584, 307, 11, 286, 500, 380, 458, 300, 512, 1496, 11, 51840], "temperature": 0.0, "avg_logprob": -0.058773922709237154, "compression_ratio": 1.8429752066115703, "no_speech_prob": 0.0016999845393002033}, {"id": 636, "seek": 381892, "start": 3818.92, "end": 3826.6, "text": " there's something like representations. But if I agree with many arguments that representations", "tokens": [50364, 456, 311, 746, 411, 33358, 13, 583, 498, 286, 3986, 365, 867, 12869, 300, 33358, 50748], "temperature": 0.0, "avg_logprob": -0.15247512475038186, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.006360212340950966}, {"id": 637, "seek": 381892, "start": 3826.6, "end": 3833.88, "text": " are more than co-variation, but there's this kind of caretness and aspectualization through them,", "tokens": [50748, 366, 544, 813, 598, 12, 34033, 399, 11, 457, 456, 311, 341, 733, 295, 1127, 83, 1287, 293, 382, 1043, 901, 2144, 807, 552, 11, 51112], "temperature": 0.0, "avg_logprob": -0.15247512475038186, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.006360212340950966}, {"id": 638, "seek": 381892, "start": 3833.88, "end": 3839.2400000000002, "text": " then they depend on relevance realization, which grounds an autopolicist and is deeply", "tokens": [51112, 550, 436, 5672, 322, 32684, 25138, 11, 597, 19196, 364, 31090, 7940, 468, 293, 307, 8760, 51380], "temperature": 0.0, "avg_logprob": -0.15247512475038186, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.006360212340950966}, {"id": 639, "seek": 381892, "start": 3839.2400000000002, "end": 3845.08, "text": " intertwined with predictive processing. That would be my response.", "tokens": [51380, 44400, 2001, 365, 35521, 9007, 13, 663, 576, 312, 452, 4134, 13, 51672], "temperature": 0.0, "avg_logprob": -0.15247512475038186, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.006360212340950966}, {"id": 640, "seek": 384508, "start": 3845.16, "end": 3852.36, "text": " Splendid. And I guess what intuitively supports a more representationist picture is consciousness.", "tokens": [50368, 19788, 521, 327, 13, 400, 286, 2041, 437, 46506, 9346, 257, 544, 10290, 468, 3036, 307, 10081, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1470350053575304, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0635296031832695}, {"id": 641, "seek": 384508, "start": 3853.88, "end": 3857.24, "text": " I think consciousness is going to become a recurring theme in this podcast.", "tokens": [50804, 286, 519, 10081, 307, 516, 281, 1813, 257, 32279, 6314, 294, 341, 7367, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1470350053575304, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0635296031832695}, {"id": 642, "seek": 384508, "start": 3858.92, "end": 3866.2799999999997, "text": " Well, it's the Holy Grail, right? It is the Holy Grail. And so I guess my question here is,", "tokens": [51056, 1042, 11, 309, 311, 264, 6295, 8985, 388, 11, 558, 30, 467, 307, 264, 6295, 8985, 388, 13, 400, 370, 286, 2041, 452, 1168, 510, 307, 11, 51424], "temperature": 0.0, "avg_logprob": -0.1470350053575304, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0635296031832695}, {"id": 643, "seek": 384508, "start": 3866.2799999999997, "end": 3873.96, "text": " is that if one was to adopt a radically inactivist view, there's nothing in that picture which", "tokens": [51424, 307, 300, 498, 472, 390, 281, 6878, 257, 35508, 294, 23397, 468, 1910, 11, 456, 311, 1825, 294, 300, 3036, 597, 51808], "temperature": 0.0, "avg_logprob": -0.1470350053575304, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.0635296031832695}, {"id": 644, "seek": 387396, "start": 3873.96, "end": 3879.16, "text": " needs consciousness. So why couldn't my mutual coupling with the world just happen with the", "tokens": [50364, 2203, 10081, 13, 407, 983, 2809, 380, 452, 16917, 37447, 365, 264, 1002, 445, 1051, 365, 264, 50624], "temperature": 0.0, "avg_logprob": -0.1470681023351925, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0025717930402606726}, {"id": 645, "seek": 387396, "start": 3879.16, "end": 3885.0, "text": " lights off? So this is an argument, I guess, that's rooted in Chalmers 1995 paper, which is,", "tokens": [50624, 5811, 766, 30, 407, 341, 307, 364, 6770, 11, 286, 2041, 11, 300, 311, 25277, 294, 761, 304, 18552, 22601, 3035, 11, 597, 307, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1470681023351925, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0025717930402606726}, {"id": 646, "seek": 387396, "start": 3885.0, "end": 3891.4, "text": " you can give me the function, but you can't give me the why, why are the lights on? And so I guess,", "tokens": [50916, 291, 393, 976, 385, 264, 2445, 11, 457, 291, 393, 380, 976, 385, 264, 983, 11, 983, 366, 264, 5811, 322, 30, 400, 370, 286, 2041, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1470681023351925, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0025717930402606726}, {"id": 647, "seek": 387396, "start": 3892.36, "end": 3898.2, "text": " in other words, my question too is, yes, that we may have relevance realization,", "tokens": [51284, 294, 661, 2283, 11, 452, 1168, 886, 307, 11, 2086, 11, 300, 321, 815, 362, 32684, 25138, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1470681023351925, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0025717930402606726}, {"id": 648, "seek": 389820, "start": 3899.0, "end": 3909.8799999999997, "text": " preceding perception or grounding perception. Then why do we have perception in the first place?", "tokens": [50404, 16969, 278, 12860, 420, 2727, 278, 12860, 13, 1396, 983, 360, 321, 362, 12860, 294, 264, 700, 1081, 30, 50948], "temperature": 0.0, "avg_logprob": -0.16933610546055125, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.007322676479816437}, {"id": 649, "seek": 389820, "start": 3911.8799999999997, "end": 3918.9199999999996, "text": " So again, I'm going to give a gist of an argument that I've spoken about at length elsewhere,", "tokens": [51048, 407, 797, 11, 286, 478, 516, 281, 976, 257, 290, 468, 295, 364, 6770, 300, 286, 600, 10759, 466, 412, 4641, 14517, 11, 51400], "temperature": 0.0, "avg_logprob": -0.16933610546055125, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.007322676479816437}, {"id": 650, "seek": 389820, "start": 3918.9199999999996, "end": 3923.08, "text": " and I'm trying to get published on and presented at conferences and so forth. So", "tokens": [51400, 293, 286, 478, 1382, 281, 483, 6572, 322, 293, 8212, 412, 22032, 293, 370, 5220, 13, 407, 51608], "temperature": 0.0, "avg_logprob": -0.16933610546055125, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.007322676479816437}, {"id": 651, "seek": 392308, "start": 3923.7999999999997, "end": 3929.3199999999997, "text": " I don't think you can separate the function and the nature questions, which is what", "tokens": [50400, 286, 500, 380, 519, 291, 393, 4994, 264, 2445, 293, 264, 3687, 1651, 11, 597, 307, 437, 50676], "temperature": 0.0, "avg_logprob": -0.0670323285970602, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012235777452588081}, {"id": 652, "seek": 392308, "start": 3929.3199999999997, "end": 3935.4, "text": " Chalmers Hard Problem relies on. Yeah, you've given me the function, but you've told me nothing", "tokens": [50676, 761, 304, 18552, 11817, 11676, 30910, 322, 13, 865, 11, 291, 600, 2212, 385, 264, 2445, 11, 457, 291, 600, 1907, 385, 1825, 50980], "temperature": 0.0, "avg_logprob": -0.0670323285970602, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012235777452588081}, {"id": 653, "seek": 392308, "start": 3935.4, "end": 3939.48, "text": " about the nature. I don't think function works ontologically like that. I think function has", "tokens": [50980, 466, 264, 3687, 13, 286, 500, 380, 519, 2445, 1985, 6592, 17157, 411, 300, 13, 286, 519, 2445, 575, 51184], "temperature": 0.0, "avg_logprob": -0.0670323285970602, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012235777452588081}, {"id": 654, "seek": 392308, "start": 3939.48, "end": 3943.64, "text": " to be plugged into the ontology. And I think as soon as we're talking about the ontology of", "tokens": [51184, 281, 312, 25679, 666, 264, 6592, 1793, 13, 400, 286, 519, 382, 2321, 382, 321, 434, 1417, 466, 264, 6592, 1793, 295, 51392], "temperature": 0.0, "avg_logprob": -0.0670323285970602, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012235777452588081}, {"id": 655, "seek": 392308, "start": 3943.64, "end": 3948.52, "text": " anything within a living organism, we're talking about something functional. So I think the questions", "tokens": [51392, 1340, 1951, 257, 2647, 24128, 11, 321, 434, 1417, 466, 746, 11745, 13, 407, 286, 519, 264, 1651, 51636], "temperature": 0.0, "avg_logprob": -0.0670323285970602, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012235777452588081}, {"id": 656, "seek": 394852, "start": 3948.52, "end": 3954.68, "text": " have to be answered interconnected. So let's go back. Let's say that you give me", "tokens": [50364, 362, 281, 312, 10103, 36611, 13, 407, 718, 311, 352, 646, 13, 961, 311, 584, 300, 291, 976, 385, 50672], "temperature": 0.0, "avg_logprob": -0.09003771268404447, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.010165557265281677}, {"id": 657, "seek": 394852, "start": 3957.0, "end": 3963.8, "text": " that any cognitive agent has to be doing anticipatory relevance realization, or it's", "tokens": [50788, 300, 604, 15605, 9461, 575, 281, 312, 884, 10416, 4745, 32684, 25138, 11, 420, 309, 311, 51128], "temperature": 0.0, "avg_logprob": -0.09003771268404447, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.010165557265281677}, {"id": 658, "seek": 394852, "start": 3963.8, "end": 3968.44, "text": " not going to be a general problem solver. And then when it's doing that, it has to be", "tokens": [51128, 406, 516, 281, 312, 257, 2674, 1154, 1404, 331, 13, 400, 550, 562, 309, 311, 884, 300, 11, 309, 575, 281, 312, 51360], "temperature": 0.0, "avg_logprob": -0.09003771268404447, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.010165557265281677}, {"id": 659, "seek": 396844, "start": 3968.52, "end": 3979.4, "text": " aspectualizing its world. It's not doing all of this, but this as a bottle or the molecule", "tokens": [50368, 382, 1043, 901, 3319, 1080, 1002, 13, 467, 311, 406, 884, 439, 295, 341, 11, 457, 341, 382, 257, 7817, 420, 264, 15582, 50912], "temperature": 0.0, "avg_logprob": -0.12815145345834586, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.1966046243906021}, {"id": 660, "seek": 396844, "start": 3979.4, "end": 3985.56, "text": " as food, the molecule. And then if you then start to pay attention to,", "tokens": [50912, 382, 1755, 11, 264, 15582, 13, 400, 550, 498, 291, 550, 722, 281, 1689, 3202, 281, 11, 51220], "temperature": 0.0, "avg_logprob": -0.12815145345834586, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.1966046243906021}, {"id": 661, "seek": 396844, "start": 3987.2400000000002, "end": 3994.04, "text": " let's look at the continuum of consciousness. Let's look at the possibility, which I have", "tokens": [51304, 718, 311, 574, 412, 264, 36120, 295, 10081, 13, 961, 311, 574, 412, 264, 7959, 11, 597, 286, 362, 51644], "temperature": 0.0, "avg_logprob": -0.12815145345834586, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.1966046243906021}, {"id": 662, "seek": 396844, "start": 3994.04, "end": 3997.88, "text": " experienced. That's not even the right sentence. Many people have, and this is Foreman's idea,", "tokens": [51644, 6751, 13, 663, 311, 406, 754, 264, 558, 8174, 13, 5126, 561, 362, 11, 293, 341, 307, 9018, 1601, 311, 1558, 11, 51836], "temperature": 0.0, "avg_logprob": -0.12815145345834586, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.1966046243906021}, {"id": 663, "seek": 399788, "start": 3997.88, "end": 4003.7200000000003, "text": " of something like the pure consciousness event. And the pure consciousness event doesn't have,", "tokens": [50364, 295, 746, 411, 264, 6075, 10081, 2280, 13, 400, 264, 6075, 10081, 2280, 1177, 380, 362, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1542971057276572, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.00239565153606236}, {"id": 664, "seek": 399788, "start": 4005.4, "end": 4009.4, "text": " well, actually, I need a distinction here. I'm going to claim it doesn't have one type of", "tokens": [50740, 731, 11, 767, 11, 286, 643, 257, 16844, 510, 13, 286, 478, 516, 281, 3932, 309, 1177, 380, 362, 472, 2010, 295, 50940], "temperature": 0.0, "avg_logprob": -0.1542971057276572, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.00239565153606236}, {"id": 665, "seek": 399788, "start": 4009.4, "end": 4014.52, "text": " qualia, but it has another type of qualia in it. It doesn't have adjectival qualia. There's no red,", "tokens": [50940, 4101, 654, 11, 457, 309, 575, 1071, 2010, 295, 4101, 654, 294, 309, 13, 467, 1177, 380, 362, 29378, 3576, 4101, 654, 13, 821, 311, 572, 2182, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1542971057276572, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.00239565153606236}, {"id": 666, "seek": 399788, "start": 4014.52, "end": 4019.32, "text": " there's no blue, there's no cap, there's no dot. You're not even conscious of consciousness,", "tokens": [51196, 456, 311, 572, 3344, 11, 456, 311, 572, 1410, 11, 456, 311, 572, 5893, 13, 509, 434, 406, 754, 6648, 295, 10081, 11, 51436], "temperature": 0.0, "avg_logprob": -0.1542971057276572, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.00239565153606236}, {"id": 667, "seek": 399788, "start": 4019.32, "end": 4027.32, "text": " you're just conscious. But what it still has, is it still has the adverbial qualia.", "tokens": [51436, 291, 434, 445, 6648, 13, 583, 437, 309, 920, 575, 11, 307, 309, 920, 575, 264, 614, 25809, 831, 4101, 654, 13, 51836], "temperature": 0.0, "avg_logprob": -0.1542971057276572, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.00239565153606236}, {"id": 668, "seek": 402732, "start": 4027.4, "end": 4035.32, "text": " It still has a sense of here-ness, noun, and the here-ness is profound presence. That's the language", "tokens": [50368, 467, 920, 575, 257, 2020, 295, 510, 12, 1287, 11, 23307, 11, 293, 264, 510, 12, 1287, 307, 14382, 6814, 13, 663, 311, 264, 2856, 50764], "temperature": 0.0, "avg_logprob": -0.1441451930999756, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0005030779866501689}, {"id": 669, "seek": 402732, "start": 4035.32, "end": 4043.6400000000003, "text": " with you. The now-ness, eternity, the integration, right, the together-ness, it's everything is one.", "tokens": [50764, 365, 291, 13, 440, 586, 12, 1287, 11, 27162, 11, 264, 10980, 11, 558, 11, 264, 1214, 12, 1287, 11, 309, 311, 1203, 307, 472, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1441451930999756, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0005030779866501689}, {"id": 670, "seek": 402732, "start": 4043.6400000000003, "end": 4049.8, "text": " So all the adverbial qualia are still there, and you still have consciousness. That shows", "tokens": [51180, 407, 439, 264, 614, 25809, 831, 4101, 654, 366, 920, 456, 11, 293, 291, 920, 362, 10081, 13, 663, 3110, 51488], "temperature": 0.0, "avg_logprob": -0.1441451930999756, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0005030779866501689}, {"id": 671, "seek": 402732, "start": 4049.8, "end": 4055.88, "text": " that the adjectival qualia are not necessary to consciousness. Now, I have the other arguments", "tokens": [51488, 300, 264, 29378, 3576, 4101, 654, 366, 406, 4818, 281, 10081, 13, 823, 11, 286, 362, 264, 661, 12869, 51792], "temperature": 0.0, "avg_logprob": -0.1441451930999756, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0005030779866501689}, {"id": 672, "seek": 405588, "start": 4055.88, "end": 4061.08, "text": " to show they're probably not sufficient, because if I give you sort of atomic blips of blue-ness", "tokens": [50364, 281, 855, 436, 434, 1391, 406, 11563, 11, 570, 498, 286, 976, 291, 1333, 295, 22275, 888, 2600, 295, 3344, 12, 1287, 50624], "temperature": 0.0, "avg_logprob": -0.10149685541788737, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0019254973158240318}, {"id": 673, "seek": 405588, "start": 4061.08, "end": 4065.32, "text": " and green-ness, and they're not bound together, and there's not a here-ness and a noun-ness,", "tokens": [50624, 293, 3092, 12, 1287, 11, 293, 436, 434, 406, 5472, 1214, 11, 293, 456, 311, 406, 257, 510, 12, 1287, 293, 257, 23307, 12, 1287, 11, 50836], "temperature": 0.0, "avg_logprob": -0.10149685541788737, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0019254973158240318}, {"id": 674, "seek": 405588, "start": 4065.32, "end": 4073.08, "text": " so you can anyway orient on them, I don't think you have consciousness either. So I'm not saying that", "tokens": [50836, 370, 291, 393, 4033, 8579, 322, 552, 11, 286, 500, 380, 519, 291, 362, 10081, 2139, 13, 407, 286, 478, 406, 1566, 300, 51224], "temperature": 0.0, "avg_logprob": -0.10149685541788737, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0019254973158240318}, {"id": 675, "seek": 405588, "start": 4073.08, "end": 4078.84, "text": " adjectival qualia don't exist. I'm saying that we've held consciousness hostage to them. And", "tokens": [51224, 29378, 3576, 4101, 654, 500, 380, 2514, 13, 286, 478, 1566, 300, 321, 600, 5167, 10081, 38434, 281, 552, 13, 400, 51512], "temperature": 0.0, "avg_logprob": -0.10149685541788737, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0019254973158240318}, {"id": 676, "seek": 407884, "start": 4079.56, "end": 4086.44, "text": " what I'm proposing to you is that what consciousness is doing is these adverbial qualia,", "tokens": [50400, 437, 286, 478, 29939, 281, 291, 307, 300, 437, 10081, 307, 884, 307, 613, 614, 25809, 831, 4101, 654, 11, 50744], "temperature": 0.0, "avg_logprob": -0.06764441728591919, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.014272479340434074}, {"id": 677, "seek": 407884, "start": 4086.44, "end": 4095.0, "text": " which are just salience, which is just relevance realization, which is, as far as we can tell,", "tokens": [50744, 597, 366, 445, 1845, 1182, 11, 597, 307, 445, 32684, 25138, 11, 597, 307, 11, 382, 1400, 382, 321, 393, 980, 11, 51172], "temperature": 0.0, "avg_logprob": -0.06764441728591919, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.014272479340434074}, {"id": 678, "seek": 407884, "start": 4095.0, "end": 4100.04, "text": " tied to the best evidence we have, it's all controversial, about the function of consciousness.", "tokens": [51172, 9601, 281, 264, 1151, 4467, 321, 362, 11, 309, 311, 439, 17323, 11, 466, 264, 2445, 295, 10081, 13, 51424], "temperature": 0.0, "avg_logprob": -0.06764441728591919, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.014272479340434074}, {"id": 679, "seek": 407884, "start": 4100.04, "end": 4104.2, "text": " It's tied up with working memory and attention, which are both doing higher order relevance", "tokens": [51424, 467, 311, 9601, 493, 365, 1364, 4675, 293, 3202, 11, 597, 366, 1293, 884, 2946, 1668, 32684, 51632], "temperature": 0.0, "avg_logprob": -0.06764441728591919, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.014272479340434074}, {"id": 680, "seek": 410420, "start": 4104.2, "end": 4109.88, "text": " realization. We seem to need consciousness for situations of complexity, novelty, or ill-defined", "tokens": [50364, 25138, 13, 492, 1643, 281, 643, 10081, 337, 6851, 295, 14024, 11, 44805, 11, 420, 3171, 12, 37716, 50648], "temperature": 0.0, "avg_logprob": -0.09784910219524978, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.05404557287693024}, {"id": 681, "seek": 410420, "start": 4109.88, "end": 4114.28, "text": " in this, ones that are really demanding on relevance realization. So you can make a pretty,", "tokens": [50648, 294, 341, 11, 2306, 300, 366, 534, 19960, 322, 32684, 25138, 13, 407, 291, 393, 652, 257, 1238, 11, 50868], "temperature": 0.0, "avg_logprob": -0.09784910219524978, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.05404557287693024}, {"id": 682, "seek": 410420, "start": 4114.28, "end": 4119.08, "text": " and there's a lot of convergence, actually, on what the function of consciousness is,", "tokens": [50868, 293, 456, 311, 257, 688, 295, 32181, 11, 767, 11, 322, 437, 264, 2445, 295, 10081, 307, 11, 51108], "temperature": 0.0, "avg_logprob": -0.09784910219524978, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.05404557287693024}, {"id": 683, "seek": 410420, "start": 4119.08, "end": 4123.8, "text": " relevance realization. And I think if you plug in relevance realization, you can at least get", "tokens": [51108, 32684, 25138, 13, 400, 286, 519, 498, 291, 5452, 294, 32684, 25138, 11, 291, 393, 412, 1935, 483, 51344], "temperature": 0.0, "avg_logprob": -0.09784910219524978, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.05404557287693024}, {"id": 684, "seek": 410420, "start": 4123.8, "end": 4130.04, "text": " all the adverbial qualia. And that, I think, gives you a lot of what consciousness actually is.", "tokens": [51344, 439, 264, 614, 25809, 831, 4101, 654, 13, 400, 300, 11, 286, 519, 11, 2709, 291, 257, 688, 295, 437, 10081, 767, 307, 13, 51656], "temperature": 0.0, "avg_logprob": -0.09784910219524978, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.05404557287693024}, {"id": 685, "seek": 413004, "start": 4130.28, "end": 4137.0, "text": " Cool. Yeah, let's stay on that adverbial qualia, because I'd like to integrate what you've just", "tokens": [50376, 8561, 13, 865, 11, 718, 311, 1754, 322, 300, 614, 25809, 831, 4101, 654, 11, 570, 286, 1116, 411, 281, 13365, 437, 291, 600, 445, 50712], "temperature": 0.0, "avg_logprob": -0.139262056350708, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.00815900694578886}, {"id": 686, "seek": 413004, "start": 4137.0, "end": 4142.76, "text": " said with a more active inference account of consciousness, which there are, and then they're", "tokens": [50712, 848, 365, 257, 544, 4967, 38253, 2696, 295, 10081, 11, 597, 456, 366, 11, 293, 550, 436, 434, 51000], "temperature": 0.0, "avg_logprob": -0.139262056350708, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.00815900694578886}, {"id": 687, "seek": 413004, "start": 4142.76, "end": 4151.16, "text": " also diverse. But one, well, I have to ask a question before, does Foreman say that mind-ness,", "tokens": [51000, 611, 9521, 13, 583, 472, 11, 731, 11, 286, 362, 281, 1029, 257, 1168, 949, 11, 775, 9018, 1601, 584, 300, 1575, 12, 1287, 11, 51420], "temperature": 0.0, "avg_logprob": -0.139262056350708, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.00815900694578886}, {"id": 688, "seek": 413004, "start": 4151.16, "end": 4157.56, "text": " so feeling that this experience is mine, does he, is that one of his adverbial qualia that", "tokens": [51420, 370, 2633, 300, 341, 1752, 307, 3892, 11, 775, 415, 11, 307, 300, 472, 295, 702, 614, 25809, 831, 4101, 654, 300, 51740], "temperature": 0.0, "avg_logprob": -0.139262056350708, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.00815900694578886}, {"id": 689, "seek": 415756, "start": 4157.64, "end": 4164.4400000000005, "text": " persists in the pure consciousness event? So that has another thicket, because the debate about,", "tokens": [50368, 868, 1751, 294, 264, 6075, 10081, 2280, 30, 407, 300, 575, 1071, 5060, 302, 11, 570, 264, 7958, 466, 11, 50708], "temperature": 0.0, "avg_logprob": -0.14328993691338432, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.0038164379075169563}, {"id": 690, "seek": 415756, "start": 4164.4400000000005, "end": 4169.0, "text": " this is a raging debate. Evan Thompson actually has a good anthology on this, on the self-no-self", "tokens": [50708, 341, 307, 257, 44173, 7958, 13, 22613, 23460, 767, 575, 257, 665, 25820, 1793, 322, 341, 11, 322, 264, 2698, 12, 1771, 12, 927, 50936], "temperature": 0.0, "avg_logprob": -0.14328993691338432, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.0038164379075169563}, {"id": 691, "seek": 415756, "start": 4169.0, "end": 4181.96, "text": " debate. Right. And I think it's reasonable and complies with most of the evidence, which", "tokens": [50936, 7958, 13, 1779, 13, 400, 286, 519, 309, 311, 10585, 293, 1209, 530, 365, 881, 295, 264, 4467, 11, 597, 51584], "temperature": 0.0, "avg_logprob": -0.14328993691338432, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.0038164379075169563}, {"id": 692, "seek": 418196, "start": 4181.96, "end": 4187.4800000000005, "text": " of course is self-report after the fact, which is problematic, blah, blah, blah, blah. I agree", "tokens": [50364, 295, 1164, 307, 2698, 12, 265, 2707, 934, 264, 1186, 11, 597, 307, 19011, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 13, 286, 3986, 50640], "temperature": 0.0, "avg_logprob": -0.098187255859375, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.05660868436098099}, {"id": 693, "seek": 418196, "start": 4187.4800000000005, "end": 4191.8, "text": " with all of that. I'm not dismissing that. But that's what we basically have to go on right now,", "tokens": [50640, 365, 439, 295, 300, 13, 286, 478, 406, 16974, 278, 300, 13, 583, 300, 311, 437, 321, 1936, 362, 281, 352, 322, 558, 586, 11, 50856], "temperature": 0.0, "avg_logprob": -0.098187255859375, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.05660868436098099}, {"id": 694, "seek": 418196, "start": 4192.52, "end": 4203.24, "text": " that the ego narrative sense of mine and me and I goes away. Whether or not that is a complete loss", "tokens": [50892, 300, 264, 14495, 9977, 2020, 295, 3892, 293, 385, 293, 286, 1709, 1314, 13, 8503, 420, 406, 300, 307, 257, 3566, 4470, 51428], "temperature": 0.0, "avg_logprob": -0.098187255859375, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.05660868436098099}, {"id": 695, "seek": 420324, "start": 4203.24, "end": 4218.36, "text": " of the self as where relevance realization is happening or something like that. I'm not convinced", "tokens": [50364, 295, 264, 2698, 382, 689, 32684, 25138, 307, 2737, 420, 746, 411, 300, 13, 286, 478, 406, 12561, 51120], "temperature": 0.0, "avg_logprob": -0.1291522837396878, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.012236444279551506}, {"id": 696, "seek": 420324, "start": 4218.36, "end": 4224.2, "text": " that that second thing is the case. I think, so if you would allow me, and this is a torture", "tokens": [51120, 300, 300, 1150, 551, 307, 264, 1389, 13, 286, 519, 11, 370, 498, 291, 576, 2089, 385, 11, 293, 341, 307, 257, 20711, 51412], "temperature": 0.0, "avg_logprob": -0.1291522837396878, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.012236444279551506}, {"id": 697, "seek": 420324, "start": 4224.2, "end": 4228.84, "text": " distinction, a distinction between the ego and the self, I think these experiences very much,", "tokens": [51412, 16844, 11, 257, 16844, 1296, 264, 14495, 293, 264, 2698, 11, 286, 519, 613, 5235, 588, 709, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1291522837396878, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.012236444279551506}, {"id": 698, "seek": 422884, "start": 4228.92, "end": 4234.04, "text": " the ego goes away. I'm very suspicious of the claim that the self goes away, because people are", "tokens": [50368, 264, 14495, 1709, 1314, 13, 286, 478, 588, 17931, 295, 264, 3932, 300, 264, 2698, 1709, 1314, 11, 570, 561, 366, 50624], "temperature": 0.0, "avg_logprob": -0.17710468845982705, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.021576538681983948}, {"id": 699, "seek": 422884, "start": 4234.04, "end": 4239.88, "text": " readily able to recall these experiences. This is what Forman does report, and I would report to you", "tokens": [50624, 26336, 1075, 281, 9901, 613, 5235, 13, 639, 307, 437, 10126, 282, 775, 2275, 11, 293, 286, 576, 2275, 281, 291, 50916], "temperature": 0.0, "avg_logprob": -0.17710468845982705, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.021576538681983948}, {"id": 700, "seek": 422884, "start": 4239.88, "end": 4245.56, "text": " and seamlessly integrate them into their auto-noenic autobiographical memory. There isn't any weird", "tokens": [50916, 293, 38083, 13365, 552, 666, 641, 8399, 12, 1771, 268, 299, 45747, 48434, 4675, 13, 821, 1943, 380, 604, 3657, 51200], "temperature": 0.0, "avg_logprob": -0.17710468845982705, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.021576538681983948}, {"id": 701, "seek": 422884, "start": 4245.56, "end": 4250.52, "text": " disjarring or where was I during that? Yeah, yeah, yeah, yeah. Okay, excellent. The reason I asked", "tokens": [51200, 717, 73, 18285, 420, 689, 390, 286, 1830, 300, 30, 865, 11, 1338, 11, 1338, 11, 1338, 13, 1033, 11, 7103, 13, 440, 1778, 286, 2351, 51448], "temperature": 0.0, "avg_logprob": -0.17710468845982705, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.021576538681983948}, {"id": 702, "seek": 422884, "start": 4250.52, "end": 4257.400000000001, "text": " that is because a lot of self-modeling in active inference is based on the work of Thomas Metzinga.", "tokens": [51448, 300, 307, 570, 257, 688, 295, 2698, 12, 8014, 11031, 294, 4967, 38253, 307, 2361, 322, 264, 589, 295, 8500, 6377, 8781, 64, 13, 51792], "temperature": 0.0, "avg_logprob": -0.17710468845982705, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.021576538681983948}, {"id": 703, "seek": 425740, "start": 4257.48, "end": 4261.96, "text": " So, Thomas Metzinga is such a wonderful addition to this conversation. I had a wonderful conversation", "tokens": [50368, 407, 11, 8500, 6377, 8781, 64, 307, 1270, 257, 3715, 4500, 281, 341, 3761, 13, 286, 632, 257, 3715, 3761, 50592], "temperature": 0.0, "avg_logprob": -0.1714484351021903, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.007329944055527449}, {"id": 704, "seek": 425740, "start": 4261.96, "end": 4268.839999999999, "text": " with him not that long ago. Oh, really? I love it. But on my show. Amazing. I think he's such an", "tokens": [50592, 365, 796, 406, 300, 938, 2057, 13, 876, 11, 534, 30, 286, 959, 309, 13, 583, 322, 452, 855, 13, 14165, 13, 286, 519, 415, 311, 1270, 364, 50936], "temperature": 0.0, "avg_logprob": -0.1714484351021903, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.007329944055527449}, {"id": 705, "seek": 425740, "start": 4268.839999999999, "end": 4275.0, "text": " underrated and important philosopher. Oh, I totally agree. Yeah, yeah. So, he's got this notion of,", "tokens": [50936, 833, 5468, 293, 1021, 29805, 13, 876, 11, 286, 3879, 3986, 13, 865, 11, 1338, 13, 407, 11, 415, 311, 658, 341, 10710, 295, 11, 51244], "temperature": 0.0, "avg_logprob": -0.1714484351021903, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.007329944055527449}, {"id": 706, "seek": 425740, "start": 4275.0, "end": 4279.799999999999, "text": " so he makes a distinction between phenomenal self-modeling and an ontological self. So,", "tokens": [51244, 370, 415, 1669, 257, 16844, 1296, 17778, 2698, 12, 8014, 11031, 293, 364, 6592, 4383, 2698, 13, 407, 11, 51484], "temperature": 0.0, "avg_logprob": -0.1714484351021903, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.007329944055527449}, {"id": 707, "seek": 425740, "start": 4279.799999999999, "end": 4284.36, "text": " people intuitively, even everyone listening to this podcast, even the way we act on a day-to-day", "tokens": [51484, 561, 46506, 11, 754, 1518, 4764, 281, 341, 7367, 11, 754, 264, 636, 321, 605, 322, 257, 786, 12, 1353, 12, 810, 51712], "temperature": 0.0, "avg_logprob": -0.1714484351021903, "compression_ratio": 1.6317567567567568, "no_speech_prob": 0.007329944055527449}, {"id": 708, "seek": 428436, "start": 4284.36, "end": 4290.44, "text": " basis is in lieu of an ontological self. So, we can't help but really think of ourselves as being", "tokens": [50364, 5143, 307, 294, 26036, 295, 364, 6592, 4383, 2698, 13, 407, 11, 321, 393, 380, 854, 457, 534, 519, 295, 4175, 382, 885, 50668], "temperature": 0.0, "avg_logprob": -0.11262214594873889, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.006084455642849207}, {"id": 709, "seek": 428436, "start": 4290.44, "end": 4294.599999999999, "text": " this soul, this Cartesian soul. But anyway, Metzinga invokes this notion of phenomenal", "tokens": [50668, 341, 5133, 11, 341, 22478, 42434, 5133, 13, 583, 4033, 11, 6377, 8781, 64, 1048, 8606, 341, 10710, 295, 17778, 50876], "temperature": 0.0, "avg_logprob": -0.11262214594873889, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.006084455642849207}, {"id": 710, "seek": 428436, "start": 4294.599999999999, "end": 4301.08, "text": " self-modeling, how the system appears to itself. And he's got this minimal phenomenal self, which", "tokens": [50876, 2698, 12, 8014, 11031, 11, 577, 264, 1185, 7038, 281, 2564, 13, 400, 415, 311, 658, 341, 13206, 17778, 2698, 11, 597, 51200], "temperature": 0.0, "avg_logprob": -0.11262214594873889, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.006084455642849207}, {"id": 711, "seek": 428436, "start": 4302.04, "end": 4306.44, "text": " has been unpicked in other ways, but he normally speaks generally of presentness,", "tokens": [51248, 575, 668, 20994, 12598, 294, 661, 2098, 11, 457, 415, 5646, 10789, 5101, 295, 1974, 1287, 11, 51468], "temperature": 0.0, "avg_logprob": -0.11262214594873889, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.006084455642849207}, {"id": 712, "seek": 428436, "start": 4306.44, "end": 4311.24, "text": " minus, and perspectivalness. Exactly what I've been talking about. Yep, exactly. And he gets", "tokens": [51468, 3175, 11, 293, 4096, 3576, 1287, 13, 7587, 437, 286, 600, 668, 1417, 466, 13, 7010, 11, 2293, 13, 400, 415, 2170, 51708], "temperature": 0.0, "avg_logprob": -0.11262214594873889, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.006084455642849207}, {"id": 713, "seek": 431124, "start": 4311.24, "end": 4316.679999999999, "text": " fleshed out in some of the active inference work. People like Jakob Howie, Carl, and Jakob", "tokens": [50364, 12497, 292, 484, 294, 512, 295, 264, 4967, 38253, 589, 13, 3432, 411, 15029, 996, 1012, 414, 11, 14256, 11, 293, 15029, 996, 50636], "temperature": 0.0, "avg_logprob": -0.126581346145784, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.006103330757468939}, {"id": 714, "seek": 431124, "start": 4316.679999999999, "end": 4322.84, "text": " Limonowski have used Metzinga's work for several papers. And then, on the other end of that, we", "tokens": [50636, 16406, 266, 21866, 362, 1143, 6377, 8781, 64, 311, 589, 337, 2940, 10577, 13, 400, 550, 11, 322, 264, 661, 917, 295, 300, 11, 321, 50944], "temperature": 0.0, "avg_logprob": -0.126581346145784, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.006103330757468939}, {"id": 715, "seek": 431124, "start": 4322.84, "end": 4334.04, "text": " have what Carl turned an epistemic agent model, which is the system that sees itself as epistemic", "tokens": [50944, 362, 437, 14256, 3574, 364, 2388, 468, 3438, 9461, 2316, 11, 597, 307, 264, 1185, 300, 8194, 2564, 382, 2388, 468, 3438, 51504], "temperature": 0.0, "avg_logprob": -0.126581346145784, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.006103330757468939}, {"id": 716, "seek": 431124, "start": 4334.04, "end": 4339.24, "text": " in the sense that it can retrieve past memories to inform future decisions, and agentive in the", "tokens": [51504, 294, 264, 2020, 300, 309, 393, 30254, 1791, 8495, 281, 1356, 2027, 5327, 11, 293, 9461, 488, 294, 264, 51764], "temperature": 0.0, "avg_logprob": -0.126581346145784, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.006103330757468939}, {"id": 717, "seek": 433924, "start": 4339.24, "end": 4348.28, "text": " fact that it can conduct allostatic action, i.e., action to retain some homeostatic equilibrium,", "tokens": [50364, 1186, 300, 309, 393, 6018, 439, 555, 2399, 3069, 11, 741, 13, 68, 7933, 3069, 281, 18340, 512, 1280, 555, 2399, 15625, 11, 50816], "temperature": 0.0, "avg_logprob": -0.06362152099609375, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.002118883188813925}, {"id": 718, "seek": 433924, "start": 4348.28, "end": 4351.8, "text": " given what it knows about the future. So, me putting on a jumper before I go outside,", "tokens": [50816, 2212, 437, 309, 3255, 466, 264, 2027, 13, 407, 11, 385, 3372, 322, 257, 44061, 949, 286, 352, 2380, 11, 50992], "temperature": 0.0, "avg_logprob": -0.06362152099609375, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.002118883188813925}, {"id": 719, "seek": 433924, "start": 4351.8, "end": 4355.639999999999, "text": " because I know it's going to be colder outside than it is inside. The reason why I invoke the", "tokens": [50992, 570, 286, 458, 309, 311, 516, 281, 312, 31020, 2380, 813, 309, 307, 1854, 13, 440, 1778, 983, 286, 41117, 264, 51184], "temperature": 0.0, "avg_logprob": -0.06362152099609375, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.002118883188813925}, {"id": 720, "seek": 433924, "start": 4355.639999999999, "end": 4360.12, "text": " minimal phenomenal selfhood is because, and Carl actually outlines this in the podcast we did", "tokens": [51184, 13206, 17778, 2698, 3809, 307, 570, 11, 293, 14256, 767, 40125, 341, 294, 264, 7367, 321, 630, 51408], "temperature": 0.0, "avg_logprob": -0.06362152099609375, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.002118883188813925}, {"id": 721, "seek": 436012, "start": 4360.12, "end": 4368.92, "text": " together, that consciousness may be downstream, in a sense, on one, a deep temporal model,", "tokens": [50364, 1214, 11, 300, 10081, 815, 312, 30621, 11, 294, 257, 2020, 11, 322, 472, 11, 257, 2452, 30881, 2316, 11, 50804], "temperature": 0.0, "avg_logprob": -0.13953622182210287, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.47833529114723206}, {"id": 722, "seek": 436012, "start": 4369.88, "end": 4375.88, "text": " and from that, the sense of agency. And he has an argument from referring to dimensions,", "tokens": [50852, 293, 490, 300, 11, 264, 2020, 295, 7934, 13, 400, 415, 575, 364, 6770, 490, 13761, 281, 12819, 11, 51152], "temperature": 0.0, "avg_logprob": -0.13953622182210287, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.47833529114723206}, {"id": 723, "seek": 436012, "start": 4376.44, "end": 4382.68, "text": " not in terms of sort of millimeter size, but in terms of the degree to which you have embedded", "tokens": [51180, 406, 294, 2115, 295, 1333, 295, 17942, 2744, 11, 457, 294, 2115, 295, 264, 4314, 281, 597, 291, 362, 16741, 51492], "temperature": 0.0, "avg_logprob": -0.13953622182210287, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.47833529114723206}, {"id": 724, "seek": 436012, "start": 4382.68, "end": 4389.72, "text": " markup blankets, that when we're distant from our actuators, what we end up doing is we have to have", "tokens": [51492, 1491, 1010, 38710, 11, 300, 562, 321, 434, 17275, 490, 527, 34964, 3391, 11, 437, 321, 917, 493, 884, 307, 321, 362, 281, 362, 51844], "temperature": 0.0, "avg_logprob": -0.13953622182210287, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.47833529114723206}, {"id": 725, "seek": 438972, "start": 4389.8, "end": 4393.88, "text": " a way of distinguishing what's my action from what's your action or from what the world is,", "tokens": [50368, 257, 636, 295, 11365, 3807, 437, 311, 452, 3069, 490, 437, 311, 428, 3069, 420, 490, 437, 264, 1002, 307, 11, 50572], "temperature": 0.0, "avg_logprob": -0.09142407127048659, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.0009365087025798857}, {"id": 726, "seek": 438972, "start": 4393.88, "end": 4398.6, "text": " how the world has acted on us. And in doing that, we come up with a self other distinction, but", "tokens": [50572, 577, 264, 1002, 575, 20359, 322, 505, 13, 400, 294, 884, 300, 11, 321, 808, 493, 365, 257, 2698, 661, 16844, 11, 457, 50808], "temperature": 0.0, "avg_logprob": -0.09142407127048659, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.0009365087025798857}, {"id": 727, "seek": 438972, "start": 4398.6, "end": 4403.88, "text": " also the notion of an agentive self, the idea that I can change the world in accordance to my", "tokens": [50808, 611, 264, 10710, 295, 364, 9461, 488, 2698, 11, 264, 1558, 300, 286, 393, 1319, 264, 1002, 294, 31110, 281, 452, 51072], "temperature": 0.0, "avg_logprob": -0.09142407127048659, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.0009365087025798857}, {"id": 728, "seek": 438972, "start": 4403.88, "end": 4410.76, "text": " preferences, as I mentioned beforehand. So, to paraphrase Carl, what that seems to be", "tokens": [51072, 21910, 11, 382, 286, 2835, 22893, 13, 407, 11, 281, 36992, 1703, 651, 14256, 11, 437, 300, 2544, 281, 312, 51416], "temperature": 0.0, "avg_logprob": -0.09142407127048659, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.0009365087025798857}, {"id": 729, "seek": 438972, "start": 4411.64, "end": 4417.400000000001, "text": " suggesting is that consciousness is actually rooted in selfhood. And I know there's this whole", "tokens": [51460, 18094, 307, 300, 10081, 307, 767, 25277, 294, 2698, 3809, 13, 400, 286, 458, 456, 311, 341, 1379, 51748], "temperature": 0.0, "avg_logprob": -0.09142407127048659, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.0009365087025798857}, {"id": 730, "seek": 441740, "start": 4417.4, "end": 4422.599999999999, "text": " argument in Metzinger about whether that's the case. That's why I asked whether mindness", "tokens": [50364, 6770, 294, 6377, 89, 6911, 466, 1968, 300, 311, 264, 1389, 13, 663, 311, 983, 286, 2351, 1968, 1575, 1287, 50624], "temperature": 0.0, "avg_logprob": -0.15703899104420732, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.0027456965763121843}, {"id": 731, "seek": 441740, "start": 4422.599999999999, "end": 4427.32, "text": " is also part of these pure consciousness events, or whether it's kind of a post hoc inference.", "tokens": [50624, 307, 611, 644, 295, 613, 6075, 10081, 3931, 11, 420, 1968, 309, 311, 733, 295, 257, 2183, 16708, 38253, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15703899104420732, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.0027456965763121843}, {"id": 732, "seek": 441740, "start": 4428.2, "end": 4433.5599999999995, "text": " Just wondering whether you had any ideas about that notion, and whether we can truly have", "tokens": [50904, 1449, 6359, 1968, 291, 632, 604, 3487, 466, 300, 10710, 11, 293, 1968, 321, 393, 4908, 362, 51172], "temperature": 0.0, "avg_logprob": -0.15703899104420732, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.0027456965763121843}, {"id": 733, "seek": 441740, "start": 4434.12, "end": 4439.879999999999, "text": " consciousness without at least the sense of self. Right. Okay. So,", "tokens": [51200, 10081, 1553, 412, 1935, 264, 2020, 295, 2698, 13, 1779, 13, 1033, 13, 407, 11, 51488], "temperature": 0.0, "avg_logprob": -0.15703899104420732, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.0027456965763121843}, {"id": 734, "seek": 443988, "start": 4440.6, "end": 4453.88, "text": " let me try a few things. First of all, I don't think it's inevitable that human beings model", "tokens": [50400, 718, 385, 853, 257, 1326, 721, 13, 2386, 295, 439, 11, 286, 500, 380, 519, 309, 311, 21451, 300, 1952, 8958, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.1262564200621385, "compression_ratio": 1.3941605839416058, "no_speech_prob": 0.0006984661449678242}, {"id": 735, "seek": 443988, "start": 4453.88, "end": 4462.04, "text": " themselves as souls. I think that's a Western post Cartesian way of, and that's even, and then the", "tokens": [51064, 2969, 382, 16588, 13, 286, 519, 300, 311, 257, 8724, 2183, 22478, 42434, 636, 295, 11, 293, 300, 311, 754, 11, 293, 550, 264, 51472], "temperature": 0.0, "avg_logprob": -0.1262564200621385, "compression_ratio": 1.3941605839416058, "no_speech_prob": 0.0006984661449678242}, {"id": 736, "seek": 446204, "start": 4462.04, "end": 4472.28, "text": " soul as a monadic single substance, I can put it that way. That's not even the case in the ancient", "tokens": [50364, 5133, 382, 257, 1108, 43341, 2167, 12961, 11, 286, 393, 829, 309, 300, 636, 13, 663, 311, 406, 754, 264, 1389, 294, 264, 7832, 50876], "temperature": 0.0, "avg_logprob": -0.13590875424836812, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.022962108254432678}, {"id": 737, "seek": 446204, "start": 4472.28, "end": 4479.4, "text": " world of the West, certainly not the case in other parts of the world, etc. So, I hesitate that", "tokens": [50876, 1002, 295, 264, 4055, 11, 3297, 406, 264, 1389, 294, 661, 3166, 295, 264, 1002, 11, 5183, 13, 407, 11, 286, 20842, 300, 51232], "temperature": 0.0, "avg_logprob": -0.13590875424836812, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.022962108254432678}, {"id": 738, "seek": 446204, "start": 4479.4, "end": 4490.84, "text": " that's the claim that's part of how the machinery must unfold. And then secondly, I worry about", "tokens": [51232, 300, 311, 264, 3932, 300, 311, 644, 295, 577, 264, 27302, 1633, 17980, 13, 400, 550, 26246, 11, 286, 3292, 466, 51804], "temperature": 0.0, "avg_logprob": -0.13590875424836812, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.022962108254432678}, {"id": 739, "seek": 449084, "start": 4490.84, "end": 4500.6, "text": " saying that because it isn't a soul, there isn't a self. This is a weird notion, again, of a substance", "tokens": [50364, 1566, 300, 570, 309, 1943, 380, 257, 5133, 11, 456, 1943, 380, 257, 2698, 13, 639, 307, 257, 3657, 10710, 11, 797, 11, 295, 257, 12961, 50852], "temperature": 0.0, "avg_logprob": -0.15074047473592495, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0027132031973451376}, {"id": 740, "seek": 449084, "start": 4500.6, "end": 4507.72, "text": " ontology being just presupposed in an unquestioned manner. I mean, look, we've discovered most things", "tokens": [50852, 6592, 1793, 885, 445, 1183, 10504, 1744, 294, 364, 517, 20343, 37579, 9060, 13, 286, 914, 11, 574, 11, 321, 600, 6941, 881, 721, 51208], "temperature": 0.0, "avg_logprob": -0.15074047473592495, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0027132031973451376}, {"id": 741, "seek": 449084, "start": 4507.72, "end": 4513.08, "text": " aren't substances. This table, which would be a classic Aristotelian substance, is not a substance.", "tokens": [51208, 3212, 380, 25455, 13, 639, 3199, 11, 597, 576, 312, 257, 7230, 31310, 310, 338, 952, 12961, 11, 307, 406, 257, 12961, 13, 51476], "temperature": 0.0, "avg_logprob": -0.15074047473592495, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0027132031973451376}, {"id": 742, "seek": 449084, "start": 4513.08, "end": 4519.8, "text": " It's a dynamical system of atoms and quarks, and we don't go, oh, well, because tables aren't", "tokens": [51476, 467, 311, 257, 5999, 804, 1185, 295, 16871, 293, 421, 20851, 11, 293, 321, 500, 380, 352, 11, 1954, 11, 731, 11, 570, 8020, 3212, 380, 51812], "temperature": 0.0, "avg_logprob": -0.15074047473592495, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0027132031973451376}, {"id": 743, "seek": 451980, "start": 4519.8, "end": 4527.88, "text": " substances, they aren't real. Like that, we don't do that. And so, I want to just", "tokens": [50364, 25455, 11, 436, 3212, 380, 957, 13, 1743, 300, 11, 321, 500, 380, 360, 300, 13, 400, 370, 11, 286, 528, 281, 445, 50768], "temperature": 0.0, "avg_logprob": -0.11719554120844061, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.002146780490875244}, {"id": 744, "seek": 451980, "start": 4527.88, "end": 4533.320000000001, "text": " note that I'm worried that there's a substance ontology creeping in here and leading to certain", "tokens": [50768, 3637, 300, 286, 478, 5804, 300, 456, 311, 257, 12961, 6592, 1793, 47753, 294, 510, 293, 5775, 281, 1629, 51040], "temperature": 0.0, "avg_logprob": -0.11719554120844061, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.002146780490875244}, {"id": 745, "seek": 451980, "start": 4533.320000000001, "end": 4539.4800000000005, "text": " conclusions. Now, that idea about this, I can't remember the name of the people,", "tokens": [51040, 22865, 13, 823, 11, 300, 1558, 466, 341, 11, 286, 393, 380, 1604, 264, 1315, 295, 264, 561, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11719554120844061, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.002146780490875244}, {"id": 746, "seek": 451980, "start": 4540.360000000001, "end": 4547.24, "text": " and I apologize for that. There is a fairly recent theory of consciousness that trying to", "tokens": [51392, 293, 286, 12328, 337, 300, 13, 821, 307, 257, 6457, 5162, 5261, 295, 10081, 300, 1382, 281, 51736], "temperature": 0.0, "avg_logprob": -0.11719554120844061, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.002146780490875244}, {"id": 747, "seek": 454724, "start": 4547.32, "end": 4552.84, "text": " respond to all the lipid experiments and things like that, arguing that, well,", "tokens": [50368, 4196, 281, 439, 264, 8280, 327, 12050, 293, 721, 411, 300, 11, 19697, 300, 11, 731, 11, 50644], "temperature": 0.0, "avg_logprob": -0.10722977940629168, "compression_ratio": 1.8195876288659794, "no_speech_prob": 0.0028425834607332945}, {"id": 748, "seek": 454724, "start": 4552.84, "end": 4559.88, "text": " consciousness is sort of after the fact, but consciousness is actually for the future. So,", "tokens": [50644, 10081, 307, 1333, 295, 934, 264, 1186, 11, 457, 10081, 307, 767, 337, 264, 2027, 13, 407, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10722977940629168, "compression_ratio": 1.8195876288659794, "no_speech_prob": 0.0028425834607332945}, {"id": 749, "seek": 454724, "start": 4559.88, "end": 4567.32, "text": " the idea here is, consciousness emerges out of the evolution of episodic memory. And so,", "tokens": [50996, 264, 1558, 510, 307, 11, 10081, 38965, 484, 295, 264, 9303, 295, 39200, 299, 4675, 13, 400, 370, 11, 51368], "temperature": 0.0, "avg_logprob": -0.10722977940629168, "compression_ratio": 1.8195876288659794, "no_speech_prob": 0.0028425834607332945}, {"id": 750, "seek": 454724, "start": 4567.32, "end": 4572.2, "text": " the function of consciousness is to allow us to create an episodic memory of something we have", "tokens": [51368, 264, 2445, 295, 10081, 307, 281, 2089, 505, 281, 1884, 364, 39200, 299, 4675, 295, 746, 321, 362, 51612], "temperature": 0.0, "avg_logprob": -0.10722977940629168, "compression_ratio": 1.8195876288659794, "no_speech_prob": 0.0028425834607332945}, {"id": 751, "seek": 457220, "start": 4572.2, "end": 4578.28, "text": " already done. And the point about the episodic memory is, as soon as you get into episodic", "tokens": [50364, 1217, 1096, 13, 400, 264, 935, 466, 264, 39200, 299, 4675, 307, 11, 382, 2321, 382, 291, 483, 666, 39200, 299, 50668], "temperature": 0.0, "avg_logprob": -0.12566100505360386, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.13648253679275513}, {"id": 752, "seek": 457220, "start": 4578.28, "end": 4582.92, "text": " memory, you get into perspectival knowing. That's what an episode is. You have a perspective", "tokens": [50668, 4675, 11, 291, 483, 666, 4096, 3576, 5276, 13, 663, 311, 437, 364, 3500, 307, 13, 509, 362, 257, 4585, 50900], "temperature": 0.0, "avg_logprob": -0.12566100505360386, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.13648253679275513}, {"id": 753, "seek": 457220, "start": 4583.8, "end": 4589.88, "text": " on a situation, what you found salient and relevant, how your actions and the arena", "tokens": [50944, 322, 257, 2590, 11, 437, 291, 1352, 1845, 1196, 293, 7340, 11, 577, 428, 5909, 293, 264, 18451, 51248], "temperature": 0.0, "avg_logprob": -0.12566100505360386, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.13648253679275513}, {"id": 754, "seek": 457220, "start": 4589.88, "end": 4595.32, "text": " coupled or didn't couple, the affect, all of that, all of that that's so bound up", "tokens": [51248, 29482, 420, 994, 380, 1916, 11, 264, 3345, 11, 439, 295, 300, 11, 439, 295, 300, 300, 311, 370, 5472, 493, 51520], "temperature": 0.0, "avg_logprob": -0.12566100505360386, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.13648253679275513}, {"id": 755, "seek": 457220, "start": 4595.32, "end": 4600.92, "text": " with consciousness. And the point they make, and I totally agree with this, is as soon as you agree", "tokens": [51520, 365, 10081, 13, 400, 264, 935, 436, 652, 11, 293, 286, 3879, 3986, 365, 341, 11, 307, 382, 2321, 382, 291, 3986, 51800], "temperature": 0.0, "avg_logprob": -0.12566100505360386, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.13648253679275513}, {"id": 756, "seek": 460092, "start": 4600.92, "end": 4605.0, "text": " that there are multiple kinds of knowing, not just propositional and procedural, but also", "tokens": [50364, 300, 456, 366, 3866, 3685, 295, 5276, 11, 406, 445, 7532, 2628, 293, 43951, 11, 457, 611, 50568], "temperature": 0.0, "avg_logprob": -0.06301130318060154, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.0029793959110975266}, {"id": 757, "seek": 460092, "start": 4605.0, "end": 4612.6, "text": " perspectival and participatory, you get the argument that episodic memory affords", "tokens": [50568, 4096, 3576, 293, 3421, 4745, 11, 291, 483, 264, 6770, 300, 39200, 299, 4675, 2096, 5703, 50948], "temperature": 0.0, "avg_logprob": -0.06301130318060154, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.0029793959110975266}, {"id": 758, "seek": 460092, "start": 4613.4, "end": 4617.64, "text": " perspectival knowing, which allows you to solve problems that you can't solve without", "tokens": [50988, 4096, 3576, 5276, 11, 597, 4045, 291, 281, 5039, 2740, 300, 291, 393, 380, 5039, 1553, 51200], "temperature": 0.0, "avg_logprob": -0.06301130318060154, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.0029793959110975266}, {"id": 759, "seek": 460092, "start": 4617.64, "end": 4623.96, "text": " perspectival knowing. You can pick up on the world, the world discloses itself in ways that it is not", "tokens": [51200, 4096, 3576, 5276, 13, 509, 393, 1888, 493, 322, 264, 1002, 11, 264, 1002, 17092, 4201, 2564, 294, 2098, 300, 309, 307, 406, 51516], "temperature": 0.0, "avg_logprob": -0.06301130318060154, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.0029793959110975266}, {"id": 760, "seek": 462396, "start": 4623.96, "end": 4632.92, "text": " otherwise disposable to you. And then consciousness emerges as an optimization on the formation of", "tokens": [50364, 5911, 41578, 281, 291, 13, 400, 550, 10081, 38965, 382, 364, 19618, 322, 264, 11723, 295, 50812], "temperature": 0.0, "avg_logprob": -0.11482945850917271, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.1158466637134552}, {"id": 761, "seek": 462396, "start": 4633.4800000000005, "end": 4639.96, "text": " episodic memory. So it is optimally transfer appropriate for the future. And then you get,", "tokens": [50840, 39200, 299, 4675, 13, 407, 309, 307, 5028, 379, 5003, 6854, 337, 264, 2027, 13, 400, 550, 291, 483, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11482945850917271, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.1158466637134552}, {"id": 762, "seek": 462396, "start": 4641.16, "end": 4649.72, "text": " it is a sense of agency, but it's not a billiard ball agency. It's this kind of longitudinal agency.", "tokens": [51224, 309, 307, 257, 2020, 295, 7934, 11, 457, 309, 311, 406, 257, 2961, 72, 515, 2594, 7934, 13, 467, 311, 341, 733, 295, 48250, 7934, 13, 51652], "temperature": 0.0, "avg_logprob": -0.11482945850917271, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.1158466637134552}, {"id": 763, "seek": 464972, "start": 4649.72, "end": 4655.0, "text": " And to tell you the truth, given the human critiques, that's actually the kind of agency", "tokens": [50364, 400, 281, 980, 291, 264, 3494, 11, 2212, 264, 1952, 3113, 4911, 11, 300, 311, 767, 264, 733, 295, 7934, 50628], "temperature": 0.0, "avg_logprob": -0.180272951230898, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.012417761608958244}, {"id": 764, "seek": 464972, "start": 4655.0, "end": 4665.240000000001, "text": " the cell has. It has this kind of longitudinal agency around episodes. It doesn't have this,", "tokens": [50628, 264, 2815, 575, 13, 467, 575, 341, 733, 295, 48250, 7934, 926, 9313, 13, 467, 1177, 380, 362, 341, 11, 51140], "temperature": 0.0, "avg_logprob": -0.180272951230898, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.012417761608958244}, {"id": 765, "seek": 464972, "start": 4665.240000000001, "end": 4671.320000000001, "text": " I'm an sort of uncaused cause, a moved mover within myself, which I think is", "tokens": [51140, 286, 478, 364, 1333, 295, 517, 496, 4717, 3082, 11, 257, 4259, 39945, 1951, 2059, 11, 597, 286, 519, 307, 51444], "temperature": 0.0, "avg_logprob": -0.180272951230898, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.012417761608958244}, {"id": 766, "seek": 464972, "start": 4671.96, "end": 4678.52, "text": " both a ridiculous proposal ontologically and an ethically undesired. Why would I want such a thing?", "tokens": [51476, 1293, 257, 11083, 11494, 6592, 17157, 293, 364, 6468, 984, 45667, 1824, 13, 1545, 576, 286, 528, 1270, 257, 551, 30, 51804], "temperature": 0.0, "avg_logprob": -0.180272951230898, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.012417761608958244}, {"id": 767, "seek": 467852, "start": 4679.0, "end": 4686.68, "text": " It's completely arbitrary. I think my life is being lived. I'm trying to change myself so that", "tokens": [50388, 467, 311, 2584, 23211, 13, 286, 519, 452, 993, 307, 885, 5152, 13, 286, 478, 1382, 281, 1319, 2059, 370, 300, 50772], "temperature": 0.0, "avg_logprob": -0.1409558561659351, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.003121501300483942}, {"id": 768, "seek": 467852, "start": 4686.68, "end": 4692.040000000001, "text": " I am, my thoughts are as determined by what is true, my action as determined by what is good,", "tokens": [50772, 286, 669, 11, 452, 4598, 366, 382, 9540, 538, 437, 307, 2074, 11, 452, 3069, 382, 9540, 538, 437, 307, 665, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1409558561659351, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.003121501300483942}, {"id": 769, "seek": 467852, "start": 4692.040000000001, "end": 4697.96, "text": " and my perception is determined by what is beautiful as I possibly can. I would like to lose", "tokens": [51040, 293, 452, 12860, 307, 9540, 538, 437, 307, 2238, 382, 286, 6264, 393, 13, 286, 576, 411, 281, 3624, 51336], "temperature": 0.0, "avg_logprob": -0.1409558561659351, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.003121501300483942}, {"id": 770, "seek": 467852, "start": 4697.96, "end": 4704.84, "text": " all my freedom in that sense. And so I think if we move to the right level of analysis, and Gallagher", "tokens": [51336, 439, 452, 5645, 294, 300, 2020, 13, 400, 370, 286, 519, 498, 321, 1286, 281, 264, 558, 1496, 295, 5215, 11, 293, 14588, 559, 511, 51680], "temperature": 0.0, "avg_logprob": -0.1409558561659351, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.003121501300483942}, {"id": 771, "seek": 470484, "start": 4704.84, "end": 4709.0, "text": " makes a convergent argument about this, that when we're talking about agency and selfhood,", "tokens": [50364, 1669, 257, 9652, 6930, 6770, 466, 341, 11, 300, 562, 321, 434, 1417, 466, 7934, 293, 2698, 3809, 11, 50572], "temperature": 0.0, "avg_logprob": -0.1033423940340678, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.010728178545832634}, {"id": 772, "seek": 470484, "start": 4709.0, "end": 4714.12, "text": " we're not talking at that limit scale. We're not talking at where's the first movement of the", "tokens": [50572, 321, 434, 406, 1417, 412, 300, 4948, 4373, 13, 492, 434, 406, 1417, 412, 689, 311, 264, 700, 3963, 295, 264, 50828], "temperature": 0.0, "avg_logprob": -0.1033423940340678, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.010728178545832634}, {"id": 773, "seek": 470484, "start": 4714.12, "end": 4720.52, "text": " billiard ball chain. We're talking more about, no, no, no, how are we building this long-term", "tokens": [50828, 2961, 72, 515, 2594, 5021, 13, 492, 434, 1417, 544, 466, 11, 572, 11, 572, 11, 572, 11, 577, 366, 321, 2390, 341, 938, 12, 7039, 51148], "temperature": 0.0, "avg_logprob": -0.1033423940340678, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.010728178545832634}, {"id": 774, "seek": 470484, "start": 4720.52, "end": 4728.2, "text": " virtual engine that enhances our predictive agency in the world? Yeah, absolutely. And Gallagher's", "tokens": [51148, 6374, 2848, 300, 46628, 527, 35521, 7934, 294, 264, 1002, 30, 865, 11, 3122, 13, 400, 14588, 559, 511, 311, 51532], "temperature": 0.0, "avg_logprob": -0.1033423940340678, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.010728178545832634}, {"id": 775, "seek": 472820, "start": 4728.2, "end": 4735.639999999999, "text": " work is very convergent with Thomas Metzinger's, of course. And so this, just for our audience,", "tokens": [50364, 589, 307, 588, 9652, 6930, 365, 8500, 6377, 8781, 260, 311, 11, 295, 1164, 13, 400, 370, 341, 11, 445, 337, 527, 4034, 11, 50736], "temperature": 0.0, "avg_logprob": -0.2000531832377116, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.05008789896965027}, {"id": 776, "seek": 472820, "start": 4735.639999999999, "end": 4742.36, "text": " this, these arguments, and this notion of the narrativized self is well fleshed out in active", "tokens": [50736, 341, 11, 613, 12869, 11, 293, 341, 10710, 295, 264, 6397, 10662, 1602, 2698, 307, 731, 12497, 292, 484, 294, 4967, 51072], "temperature": 0.0, "avg_logprob": -0.2000531832377116, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.05008789896965027}, {"id": 777, "seek": 472820, "start": 4742.36, "end": 4747.32, "text": " inference literature. So I could point you, yeah, I would probably start with the Fristlin and", "tokens": [51072, 38253, 10394, 13, 407, 286, 727, 935, 291, 11, 1338, 11, 286, 576, 1391, 722, 365, 264, 1526, 468, 5045, 293, 51320], "temperature": 0.0, "avg_logprob": -0.2000531832377116, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.05008789896965027}, {"id": 778, "seek": 472820, "start": 4747.32, "end": 4752.44, "text": " Limonalski papers. There are two wonderful papers about self-construction under active inference.", "tokens": [51320, 16406, 266, 1124, 2984, 10577, 13, 821, 366, 732, 3715, 10577, 466, 2698, 12, 25279, 3826, 833, 4967, 38253, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2000531832377116, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.05008789896965027}, {"id": 779, "seek": 472820, "start": 4753.08, "end": 4757.639999999999, "text": " And then I have a convergent argument coming out of Daniel Huda, who is also in the 4U,", "tokens": [51608, 400, 550, 286, 362, 257, 9652, 6930, 6770, 1348, 484, 295, 8033, 389, 11152, 11, 567, 307, 611, 294, 264, 1017, 52, 11, 51836], "temperature": 0.0, "avg_logprob": -0.2000531832377116, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.05008789896965027}, {"id": 780, "seek": 475820, "start": 4758.28, "end": 4764.28, "text": " the Narrative Practice Hypothesis. And he argues that any mindsight ability, any ability to see", "tokens": [50368, 264, 45658, 1166, 27904, 45649, 4624, 271, 13, 400, 415, 38218, 300, 604, 9634, 397, 3485, 11, 604, 3485, 281, 536, 50668], "temperature": 0.0, "avg_logprob": -0.1064852038000384, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.001500164857134223}, {"id": 781, "seek": 475820, "start": 4764.28, "end": 4769.88, "text": " in other people's mental states, you know, attributing beliefs and desires to them,", "tokens": [50668, 294, 661, 561, 311, 4973, 4368, 11, 291, 458, 11, 9080, 10861, 13585, 293, 18005, 281, 552, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1064852038000384, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.001500164857134223}, {"id": 782, "seek": 475820, "start": 4771.48, "end": 4776.28, "text": " requires, well, for example, if I'm going to attribute a belief and desire, I need to know", "tokens": [51028, 7029, 11, 731, 11, 337, 1365, 11, 498, 286, 478, 516, 281, 19667, 257, 7107, 293, 7516, 11, 286, 643, 281, 458, 51268], "temperature": 0.0, "avg_logprob": -0.1064852038000384, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.001500164857134223}, {"id": 783, "seek": 475820, "start": 4776.28, "end": 4781.8, "text": " something about your character. Are you lying or not? I need to know something about the context,", "tokens": [51268, 746, 466, 428, 2517, 13, 2014, 291, 8493, 420, 406, 30, 286, 643, 281, 458, 746, 466, 264, 4319, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1064852038000384, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.001500164857134223}, {"id": 784, "seek": 475820, "start": 4781.8, "end": 4786.28, "text": " the setting. What's going on in this situation? Are you tired? Is that a small child? So you're", "tokens": [51544, 264, 3287, 13, 708, 311, 516, 322, 294, 341, 2590, 30, 2014, 291, 5868, 30, 1119, 300, 257, 1359, 1440, 30, 407, 291, 434, 51768], "temperature": 0.0, "avg_logprob": -0.1064852038000384, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.001500164857134223}, {"id": 785, "seek": 478628, "start": 4786.28, "end": 4789.5599999999995, "text": " not really lying. And when you tell them at Christmas time that there's a Santa Claus,", "tokens": [50364, 406, 534, 8493, 13, 400, 562, 291, 980, 552, 412, 5272, 565, 300, 456, 311, 257, 9933, 33153, 11, 50528], "temperature": 0.0, "avg_logprob": -0.10753826661543413, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00218180357478559}, {"id": 786, "seek": 478628, "start": 4789.5599999999995, "end": 4794.92, "text": " right? Right. And I need to know what the conflict is, what the problem. Notice what I'm", "tokens": [50528, 558, 30, 1779, 13, 400, 286, 643, 281, 458, 437, 264, 6596, 307, 11, 437, 264, 1154, 13, 13428, 437, 286, 478, 50796], "temperature": 0.0, "avg_logprob": -0.10753826661543413, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00218180357478559}, {"id": 787, "seek": 478628, "start": 4794.92, "end": 4799.5599999999995, "text": " talking about here. I need to know all the elements of narrative. Daniel Huda points out,", "tokens": [50796, 1417, 466, 510, 13, 286, 643, 281, 458, 439, 264, 4959, 295, 9977, 13, 8033, 389, 11152, 2793, 484, 11, 51028], "temperature": 0.0, "avg_logprob": -0.10753826661543413, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00218180357478559}, {"id": 788, "seek": 478628, "start": 4799.5599999999995, "end": 4804.92, "text": " we practice narrative incessantly. And unlike many of the other things, including language,", "tokens": [51028, 321, 3124, 9977, 294, 780, 3627, 13, 400, 8343, 867, 295, 264, 661, 721, 11, 3009, 2856, 11, 51296], "temperature": 0.0, "avg_logprob": -0.10753826661543413, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00218180357478559}, {"id": 789, "seek": 478628, "start": 4804.92, "end": 4810.679999999999, "text": " which we scaffold for children, we scaffold narrative for our kids. I had to sit through the", "tokens": [51296, 597, 321, 44094, 337, 2227, 11, 321, 44094, 9977, 337, 527, 2301, 13, 286, 632, 281, 1394, 807, 264, 51584], "temperature": 0.0, "avg_logprob": -0.10753826661543413, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00218180357478559}, {"id": 790, "seek": 481068, "start": 4810.68, "end": 4816.280000000001, "text": " teletubbies twice, in which you have to sit through these really impoverished narratives,", "tokens": [50364, 15284, 302, 836, 23177, 6091, 11, 294, 597, 291, 362, 281, 1394, 807, 613, 534, 704, 3570, 4729, 28016, 11, 50644], "temperature": 0.0, "avg_logprob": -0.12828158473109338, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.049538761377334595}, {"id": 791, "seek": 481068, "start": 4816.280000000001, "end": 4823.8, "text": " because we're scaffolding this up because narrative ability gives us the right, gives us the set of", "tokens": [50644, 570, 321, 434, 44094, 278, 341, 493, 570, 9977, 3485, 2709, 505, 264, 558, 11, 2709, 505, 264, 992, 295, 51020], "temperature": 0.0, "avg_logprob": -0.12828158473109338, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.049538761377334595}, {"id": 792, "seek": 481068, "start": 4823.8, "end": 4828.92, "text": " skills, the sets of states of mind with the perspectives taking therein, and the traits", "tokens": [51020, 3942, 11, 264, 6352, 295, 4368, 295, 1575, 365, 264, 16766, 1940, 456, 259, 11, 293, 264, 19526, 51276], "temperature": 0.0, "avg_logprob": -0.12828158473109338, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.049538761377334595}, {"id": 793, "seek": 481068, "start": 4828.92, "end": 4834.280000000001, "text": " of character that allow us to pick up on other people's mental states. And I think that is also", "tokens": [51276, 295, 2517, 300, 2089, 505, 281, 1888, 493, 322, 661, 561, 311, 4973, 4368, 13, 400, 286, 519, 300, 307, 611, 51544], "temperature": 0.0, "avg_logprob": -0.12828158473109338, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.049538761377334595}, {"id": 794, "seek": 481068, "start": 4834.280000000001, "end": 4840.200000000001, "text": " a function of the self, the function of selves is to make us agentically predictive to each other.", "tokens": [51544, 257, 2445, 295, 264, 2698, 11, 264, 2445, 295, 41900, 307, 281, 652, 505, 9461, 984, 35521, 281, 1184, 661, 13, 51840], "temperature": 0.0, "avg_logprob": -0.12828158473109338, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.049538761377334595}, {"id": 795, "seek": 484068, "start": 4840.68, "end": 4844.84, "text": " Right. Exactly. Exactly. And there's another, there's another convergent argument in active", "tokens": [50364, 1779, 13, 7587, 13, 7587, 13, 400, 456, 311, 1071, 11, 456, 311, 1071, 9652, 6930, 6770, 294, 4967, 50572], "temperature": 0.0, "avg_logprob": -0.20679012097810445, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0018930472433567047}, {"id": 796, "seek": 484068, "start": 4844.84, "end": 4851.4800000000005, "text": " inference, which is that only in the context of other people like you, would you ever come to", "tokens": [50572, 38253, 11, 597, 307, 300, 787, 294, 264, 4319, 295, 661, 561, 411, 291, 11, 576, 291, 1562, 808, 281, 50904], "temperature": 0.0, "avg_logprob": -0.20679012097810445, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0018930472433567047}, {"id": 797, "seek": 484068, "start": 4851.4800000000005, "end": 4857.240000000001, "text": " the inference that there is something that is like to be you? Yeah. If you were low,", "tokens": [50904, 264, 38253, 300, 456, 307, 746, 300, 307, 411, 281, 312, 291, 30, 865, 13, 759, 291, 645, 2295, 11, 51192], "temperature": 0.0, "avg_logprob": -0.20679012097810445, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0018930472433567047}, {"id": 798, "seek": 484068, "start": 4858.360000000001, "end": 4862.76, "text": " that converges with the Vagatsky approach to the development of metacognition and self-awareness,", "tokens": [51248, 300, 9652, 2880, 365, 264, 691, 559, 1720, 4133, 3109, 281, 264, 3250, 295, 1131, 326, 2912, 849, 293, 2698, 12, 17074, 1287, 11, 51468], "temperature": 0.0, "avg_logprob": -0.20679012097810445, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0018930472433567047}, {"id": 799, "seek": 484068, "start": 4862.76, "end": 4868.76, "text": " which I think is right. Right. And of course, your colleague at just pointing people in", "tokens": [51468, 597, 286, 519, 307, 558, 13, 1779, 13, 400, 295, 1164, 11, 428, 13532, 412, 445, 12166, 561, 294, 51768], "temperature": 0.0, "avg_logprob": -0.20679012097810445, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0018930472433567047}, {"id": 800, "seek": 486876, "start": 4868.76, "end": 4872.04, "text": " different directions, your colleague at Toronto, former colleague Jordan Peterson,", "tokens": [50364, 819, 11095, 11, 428, 13532, 412, 14140, 11, 5819, 13532, 10979, 36943, 11, 50528], "temperature": 0.0, "avg_logprob": -0.14862972110896916, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.004958359990268946}, {"id": 801, "seek": 486876, "start": 4873.320000000001, "end": 4880.280000000001, "text": " as a whole literature on literature and narrative. And he has a conversation with Carl", "tokens": [50592, 382, 257, 1379, 10394, 322, 10394, 293, 9977, 13, 400, 415, 575, 257, 3761, 365, 14256, 50940], "temperature": 0.0, "avg_logprob": -0.14862972110896916, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.004958359990268946}, {"id": 802, "seek": 486876, "start": 4880.280000000001, "end": 4886.6, "text": " on his podcast, which is deeply intriguing about these kind of alignments of the free energy", "tokens": [50940, 322, 702, 7367, 11, 597, 307, 8760, 32503, 466, 613, 733, 295, 7975, 1117, 295, 264, 1737, 2281, 51256], "temperature": 0.0, "avg_logprob": -0.14862972110896916, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.004958359990268946}, {"id": 803, "seek": 486876, "start": 4886.6, "end": 4896.52, "text": " principle and narrative. John, I wanted to also speak about flow states. You've written about flow", "tokens": [51256, 8665, 293, 9977, 13, 2619, 11, 286, 1415, 281, 611, 1710, 466, 3095, 4368, 13, 509, 600, 3720, 466, 3095, 51752], "temperature": 0.0, "avg_logprob": -0.14862972110896916, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.004958359990268946}, {"id": 804, "seek": 489652, "start": 4896.6, "end": 4902.52, "text": " as the sort of locus of implicit learning. As you know, I've just been writing up a paper on flow", "tokens": [50368, 382, 264, 1333, 295, 450, 1149, 295, 26947, 2539, 13, 1018, 291, 458, 11, 286, 600, 445, 668, 3579, 493, 257, 3035, 322, 3095, 50664], "temperature": 0.0, "avg_logprob": -0.13639718918573288, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.005439357832074165}, {"id": 805, "seek": 489652, "start": 4903.240000000001, "end": 4908.76, "text": " for an active inference perspective. So I'd love to be able to sort of see maybe where we align,", "tokens": [50700, 337, 364, 4967, 38253, 4585, 13, 407, 286, 1116, 959, 281, 312, 1075, 281, 1333, 295, 536, 1310, 689, 321, 7975, 11, 50976], "temperature": 0.0, "avg_logprob": -0.13639718918573288, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.005439357832074165}, {"id": 806, "seek": 489652, "start": 4908.76, "end": 4914.200000000001, "text": " where we don't align and try and sort of unpick that. Just as forewarning my paper, the paper that", "tokens": [50976, 689, 321, 500, 380, 7975, 293, 853, 293, 1333, 295, 20994, 618, 300, 13, 1449, 382, 2091, 86, 2341, 452, 3035, 11, 264, 3035, 300, 51248], "temperature": 0.0, "avg_logprob": -0.13639718918573288, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.005439357832074165}, {"id": 807, "seek": 489652, "start": 4914.200000000001, "end": 4920.4400000000005, "text": " I've written with my wonderful co-authors is not out yet, but it should be coming out soon. So", "tokens": [51248, 286, 600, 3720, 365, 452, 3715, 598, 12, 40198, 830, 307, 406, 484, 1939, 11, 457, 309, 820, 312, 1348, 484, 2321, 13, 407, 51560], "temperature": 0.0, "avg_logprob": -0.13639718918573288, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.005439357832074165}, {"id": 808, "seek": 492044, "start": 4921.4, "end": 4925.4, "text": " I want to, I have not yet read your paper because you suggest waiting.", "tokens": [50412, 286, 528, 281, 11, 286, 362, 406, 1939, 1401, 428, 3035, 570, 291, 3402, 3806, 13, 50612], "temperature": 0.0, "avg_logprob": -0.24258338321339, "compression_ratio": 1.625, "no_speech_prob": 0.007337847724556923}, {"id": 809, "seek": 492044, "start": 4926.28, "end": 4931.5599999999995, "text": " Yes. The paper has gone through modifications as papers are want to do.", "tokens": [50656, 1079, 13, 440, 3035, 575, 2780, 807, 26881, 382, 10577, 366, 528, 281, 360, 13, 50920], "temperature": 0.0, "avg_logprob": -0.24258338321339, "compression_ratio": 1.625, "no_speech_prob": 0.007337847724556923}, {"id": 810, "seek": 492044, "start": 4932.919999999999, "end": 4937.879999999999, "text": " It's in the final stages. So you have access to it. So please feel free to read it.", "tokens": [50988, 467, 311, 294, 264, 2572, 10232, 13, 407, 291, 362, 2105, 281, 309, 13, 407, 1767, 841, 1737, 281, 1401, 309, 13, 51236], "temperature": 0.0, "avg_logprob": -0.24258338321339, "compression_ratio": 1.625, "no_speech_prob": 0.007337847724556923}, {"id": 811, "seek": 492044, "start": 4937.879999999999, "end": 4944.679999999999, "text": " I will read it then. I will read it. I can give a very brief overview. So to not disadvantage you.", "tokens": [51236, 286, 486, 1401, 309, 550, 13, 286, 486, 1401, 309, 13, 286, 393, 976, 257, 588, 5353, 12492, 13, 407, 281, 406, 24292, 291, 13, 51576], "temperature": 0.0, "avg_logprob": -0.24258338321339, "compression_ratio": 1.625, "no_speech_prob": 0.007337847724556923}, {"id": 812, "seek": 494468, "start": 4944.92, "end": 4951.16, "text": " My, the basic thesis of the basic perspective that we take on the paper is about self-modeling", "tokens": [50376, 1222, 11, 264, 3875, 22288, 295, 264, 3875, 4585, 300, 321, 747, 322, 264, 3035, 307, 466, 2698, 12, 8014, 11031, 50688], "temperature": 0.0, "avg_logprob": -0.17180813318011404, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.014913861639797688}, {"id": 813, "seek": 494468, "start": 4951.16, "end": 4955.400000000001, "text": " under flow states. So that's the main thing that we're looking at and the attenuation of an epistemic", "tokens": [50688, 833, 3095, 4368, 13, 407, 300, 311, 264, 2135, 551, 300, 321, 434, 1237, 412, 293, 264, 951, 268, 16073, 295, 364, 2388, 468, 3438, 50900], "temperature": 0.0, "avg_logprob": -0.17180813318011404, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.014913861639797688}, {"id": 814, "seek": 494468, "start": 4955.400000000001, "end": 4962.84, "text": " agent model. Because what we're arguing is that certain precision waiting mechanisms are lending,", "tokens": [50900, 9461, 2316, 13, 1436, 437, 321, 434, 19697, 307, 300, 1629, 18356, 3806, 15902, 366, 29823, 11, 51272], "temperature": 0.0, "avg_logprob": -0.17180813318011404, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.014913861639797688}, {"id": 815, "seek": 494468, "start": 4965.56, "end": 4970.6, "text": " leading the organism to undertake pragmatic actions, seeking out pragmatic affordances,", "tokens": [51408, 5775, 264, 24128, 281, 37010, 46904, 5909, 11, 11670, 484, 46904, 6157, 2676, 11, 51660], "temperature": 0.0, "avg_logprob": -0.17180813318011404, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.014913861639797688}, {"id": 816, "seek": 497060, "start": 4971.56, "end": 4975.56, "text": " rather than engaging in what Carl or other authors would call epistemic foraging.", "tokens": [50412, 2831, 813, 11268, 294, 437, 14256, 420, 661, 16552, 576, 818, 2388, 468, 3438, 337, 3568, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14947186229384948, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.006884846370667219}, {"id": 817, "seek": 497060, "start": 4976.68, "end": 4980.360000000001, "text": " Now, I think where we might have a point of difference here is that I have a section on flow", "tokens": [50668, 823, 11, 286, 519, 689, 321, 1062, 362, 257, 935, 295, 2649, 510, 307, 300, 286, 362, 257, 3541, 322, 3095, 50852], "temperature": 0.0, "avg_logprob": -0.14947186229384948, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.006884846370667219}, {"id": 818, "seek": 497060, "start": 4980.360000000001, "end": 4985.4800000000005, "text": " states and learning. And I think this is a really interesting point because your paper speaks about", "tokens": [50852, 4368, 293, 2539, 13, 400, 286, 519, 341, 307, 257, 534, 1880, 935, 570, 428, 3035, 10789, 466, 51108], "temperature": 0.0, "avg_logprob": -0.14947186229384948, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.006884846370667219}, {"id": 819, "seek": 497060, "start": 4985.4800000000005, "end": 4992.6, "text": " implicit learning. Whereas I'm, and this, and just to make it transparent, this is there,", "tokens": [51108, 26947, 2539, 13, 13813, 286, 478, 11, 293, 341, 11, 293, 445, 281, 652, 309, 12737, 11, 341, 307, 456, 11, 51464], "temperature": 0.0, "avg_logprob": -0.14947186229384948, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.006884846370667219}, {"id": 820, "seek": 497060, "start": 4992.6, "end": 4998.76, "text": " I've had different perspectives on this, but my current opinion is that in flow states,", "tokens": [51464, 286, 600, 632, 819, 16766, 322, 341, 11, 457, 452, 2190, 4800, 307, 300, 294, 3095, 4368, 11, 51772], "temperature": 0.0, "avg_logprob": -0.14947186229384948, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.006884846370667219}, {"id": 821, "seek": 499876, "start": 4998.76, "end": 5005.08, "text": " what you're getting is a reinforcement of the skills that you actually picked up through epistemic", "tokens": [50364, 437, 291, 434, 1242, 307, 257, 29280, 295, 264, 3942, 300, 291, 767, 6183, 493, 807, 2388, 468, 3438, 50680], "temperature": 0.0, "avg_logprob": -0.08119291954852165, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0036980018485337496}, {"id": 822, "seek": 499876, "start": 5005.08, "end": 5014.76, "text": " work. So I have the example of a violinist. The violinist must undergo a certain period of exploration,", "tokens": [50680, 589, 13, 407, 286, 362, 264, 1365, 295, 257, 22878, 468, 13, 440, 22878, 468, 1633, 26426, 257, 1629, 2896, 295, 16197, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08119291954852165, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0036980018485337496}, {"id": 823, "seek": 499876, "start": 5014.76, "end": 5020.280000000001, "text": " of epistemic foraging, which comes with this kind of self-talk, this real prominence of", "tokens": [51164, 295, 2388, 468, 3438, 337, 3568, 11, 597, 1487, 365, 341, 733, 295, 2698, 12, 29302, 11, 341, 957, 39225, 655, 295, 51440], "temperature": 0.0, "avg_logprob": -0.08119291954852165, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0036980018485337496}, {"id": 824, "seek": 499876, "start": 5020.280000000001, "end": 5026.92, "text": " myself as a knowing thing. And then what it does is over time through learning, you get high", "tokens": [51440, 2059, 382, 257, 5276, 551, 13, 400, 550, 437, 309, 775, 307, 670, 565, 807, 2539, 11, 291, 483, 1090, 51772], "temperature": 0.0, "avg_logprob": -0.08119291954852165, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0036980018485337496}, {"id": 825, "seek": 502692, "start": 5026.92, "end": 5032.28, "text": " precision over the beliefs about that action in terms of the technical detail. And then that", "tokens": [50364, 18356, 670, 264, 13585, 466, 300, 3069, 294, 2115, 295, 264, 6191, 2607, 13, 400, 550, 300, 50632], "temperature": 0.0, "avg_logprob": -0.10363490428399602, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010950548574328423}, {"id": 826, "seek": 502692, "start": 5032.28, "end": 5037.0, "text": " becomes kind of the foundation on which you can then go and do more epistemic work. And then that", "tokens": [50632, 3643, 733, 295, 264, 7030, 322, 597, 291, 393, 550, 352, 293, 360, 544, 2388, 468, 3438, 589, 13, 400, 550, 300, 50868], "temperature": 0.0, "avg_logprob": -0.10363490428399602, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010950548574328423}, {"id": 827, "seek": 502692, "start": 5038.6, "end": 5045.0, "text": " expertise development is stepwise, in a sense. I mean, you can picture it either way. But critically,", "tokens": [50948, 11769, 3250, 307, 1823, 3711, 11, 294, 257, 2020, 13, 286, 914, 11, 291, 393, 3036, 309, 2139, 636, 13, 583, 22797, 11, 51268], "temperature": 0.0, "avg_logprob": -0.10363490428399602, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010950548574328423}, {"id": 828, "seek": 502692, "start": 5045.0, "end": 5049.64, "text": " where we differ is that I'm arguing that actually in the moment of flow itself,", "tokens": [51268, 689, 321, 743, 307, 300, 286, 478, 19697, 300, 767, 294, 264, 1623, 295, 3095, 2564, 11, 51500], "temperature": 0.0, "avg_logprob": -0.10363490428399602, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010950548574328423}, {"id": 829, "seek": 502692, "start": 5050.2, "end": 5055.0, "text": " you're just garnering more evidence for the capacities and the policies that you already have.", "tokens": [51528, 291, 434, 445, 25067, 1794, 544, 4467, 337, 264, 39396, 293, 264, 7657, 300, 291, 1217, 362, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10363490428399602, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010950548574328423}, {"id": 830, "seek": 505500, "start": 5055.32, "end": 5060.36, "text": " So I'd like to start there and see what you think of that claim.", "tokens": [50380, 407, 286, 1116, 411, 281, 722, 456, 293, 536, 437, 291, 519, 295, 300, 3932, 13, 50632], "temperature": 0.0, "avg_logprob": -0.23683836243369363, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.001175092882476747}, {"id": 831, "seek": 505500, "start": 5063.16, "end": 5070.12, "text": " So we, as you point out, Leo Ferraro in Herobenidae, we argue something different,", "tokens": [50772, 407, 321, 11, 382, 291, 935, 484, 11, 19344, 10728, 5352, 78, 294, 3204, 996, 268, 2887, 68, 11, 321, 9695, 746, 819, 11, 51120], "temperature": 0.0, "avg_logprob": -0.23683836243369363, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.001175092882476747}, {"id": 832, "seek": 505500, "start": 5070.76, "end": 5078.52, "text": " which is why you're bringing it up. We argue that learning is inevitably occurring even in", "tokens": [51152, 597, 307, 983, 291, 434, 5062, 309, 493, 13, 492, 9695, 300, 2539, 307, 28171, 18386, 754, 294, 51540], "temperature": 0.0, "avg_logprob": -0.23683836243369363, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.001175092882476747}, {"id": 833, "seek": 505500, "start": 5078.52, "end": 5084.44, "text": " the flow situation. And so there's improvement. And that's why if the environment doesn't have", "tokens": [51540, 264, 3095, 2590, 13, 400, 370, 456, 311, 10444, 13, 400, 300, 311, 983, 498, 264, 2823, 1177, 380, 362, 51836], "temperature": 0.0, "avg_logprob": -0.23683836243369363, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.001175092882476747}, {"id": 834, "seek": 508444, "start": 5084.5199999999995, "end": 5089.96, "text": " the capacity to renew its challenges on you, you will very quickly fall out of the flow state.", "tokens": [50368, 264, 6042, 281, 10162, 1080, 4759, 322, 291, 11, 291, 486, 588, 2661, 2100, 484, 295, 264, 3095, 1785, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10088962878821031, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00020653319370467216}, {"id": 835, "seek": 508444, "start": 5089.96, "end": 5094.2, "text": " So the argument is, well, why don't you just stay in the flow state? Well, the argument is,", "tokens": [50640, 407, 264, 6770, 307, 11, 731, 11, 983, 500, 380, 291, 445, 1754, 294, 264, 3095, 1785, 30, 1042, 11, 264, 6770, 307, 11, 50852], "temperature": 0.0, "avg_logprob": -0.10088962878821031, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00020653319370467216}, {"id": 836, "seek": 508444, "start": 5094.2, "end": 5099.799999999999, "text": " because eventually you get a mastery over the environment, which means your skills start to", "tokens": [50852, 570, 4728, 291, 483, 257, 37951, 670, 264, 2823, 11, 597, 1355, 428, 3942, 722, 281, 51132], "temperature": 0.0, "avg_logprob": -0.10088962878821031, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00020653319370467216}, {"id": 837, "seek": 508444, "start": 5099.799999999999, "end": 5106.599999999999, "text": " exceed the demands. And so phenomenologically, when I'm in the flow state, like inspiring or", "tokens": [51132, 14048, 264, 15107, 13, 400, 370, 9388, 17157, 11, 562, 286, 478, 294, 264, 3095, 1785, 11, 411, 15883, 420, 51472], "temperature": 0.0, "avg_logprob": -0.10088962878821031, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00020653319370467216}, {"id": 838, "seek": 508444, "start": 5106.599999999999, "end": 5112.759999999999, "text": " lecturing, I'm finding tremendous amount of insight and innovation coming out.", "tokens": [51472, 5899, 1345, 11, 286, 478, 5006, 10048, 2372, 295, 11269, 293, 8504, 1348, 484, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10088962878821031, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00020653319370467216}, {"id": 839, "seek": 511444, "start": 5114.839999999999, "end": 5122.5199999999995, "text": " Now, this is where it might get tricky, because is procedural innovation a restructuring of,", "tokens": [50384, 823, 11, 341, 307, 689, 309, 1062, 483, 12414, 11, 570, 307, 43951, 8504, 257, 1472, 1757, 1345, 295, 11, 50768], "temperature": 0.0, "avg_logprob": -0.13088433882769415, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.00046532959095202386}, {"id": 840, "seek": 511444, "start": 5122.5199999999995, "end": 5129.48, "text": " especially, is that a restructuring of your information and therefore have new capacities", "tokens": [50768, 2318, 11, 307, 300, 257, 1472, 1757, 1345, 295, 428, 1589, 293, 4412, 362, 777, 39396, 51116], "temperature": 0.0, "avg_logprob": -0.13088433882769415, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.00046532959095202386}, {"id": 841, "seek": 511444, "start": 5129.48, "end": 5135.719999999999, "text": " in it, new emergent abilities? Or is it just a reinforcement? And I don't, we might get into", "tokens": [51116, 294, 309, 11, 777, 4345, 6930, 11582, 30, 1610, 307, 309, 445, 257, 29280, 30, 400, 286, 500, 380, 11, 321, 1062, 483, 666, 51428], "temperature": 0.0, "avg_logprob": -0.13088433882769415, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.00046532959095202386}, {"id": 842, "seek": 513572, "start": 5135.72, "end": 5145.400000000001, "text": " the thesis of ship here, which is problematic. But the one of the defining phenomenological", "tokens": [50364, 264, 22288, 295, 5374, 510, 11, 597, 307, 19011, 13, 583, 264, 472, 295, 264, 17827, 9388, 4383, 50848], "temperature": 0.0, "avg_logprob": -0.13878872574016612, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.049542516469955444}, {"id": 843, "seek": 513572, "start": 5145.400000000001, "end": 5152.4400000000005, "text": " features of the flow state, and I want to be clear, I'm not pinning you down on this,", "tokens": [50848, 4122, 295, 264, 3095, 1785, 11, 293, 286, 528, 281, 312, 1850, 11, 286, 478, 406, 5447, 773, 291, 760, 322, 341, 11, 51200], "temperature": 0.0, "avg_logprob": -0.13878872574016612, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.049542516469955444}, {"id": 844, "seek": 513572, "start": 5152.4400000000005, "end": 5157.96, "text": " but I do think it's relevant evidence is the ongoing sense of discovery. There's a sense of", "tokens": [51200, 457, 286, 360, 519, 309, 311, 7340, 4467, 307, 264, 10452, 2020, 295, 12114, 13, 821, 311, 257, 2020, 295, 51476], "temperature": 0.0, "avg_logprob": -0.13878872574016612, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.049542516469955444}, {"id": 845, "seek": 513572, "start": 5157.96, "end": 5163.56, "text": " discovery there. There's a sense of coming to know things you did not know before. And of course,", "tokens": [51476, 12114, 456, 13, 821, 311, 257, 2020, 295, 1348, 281, 458, 721, 291, 630, 406, 458, 949, 13, 400, 295, 1164, 11, 51756], "temperature": 0.0, "avg_logprob": -0.13878872574016612, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.049542516469955444}, {"id": 846, "seek": 516356, "start": 5163.56, "end": 5168.360000000001, "text": " when the flow state is in much more comprehensive expertise, not like the plain tennis or maybe", "tokens": [50364, 562, 264, 3095, 1785, 307, 294, 709, 544, 13914, 11769, 11, 406, 411, 264, 11121, 18118, 420, 1310, 50604], "temperature": 0.0, "avg_logprob": -0.08879974001929873, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0016457934398204088}, {"id": 847, "seek": 516356, "start": 5168.360000000001, "end": 5172.76, "text": " like just your optimal gripping on the world, people come to think they have learned something", "tokens": [50604, 411, 445, 428, 16252, 17865, 3759, 322, 264, 1002, 11, 561, 808, 281, 519, 436, 362, 3264, 746, 50824], "temperature": 0.0, "avg_logprob": -0.08879974001929873, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0016457934398204088}, {"id": 848, "seek": 516356, "start": 5172.76, "end": 5181.400000000001, "text": " deeply profound about reality. So I tend to think that there's evidence for transformative learning", "tokens": [50824, 8760, 14382, 466, 4103, 13, 407, 286, 3928, 281, 519, 300, 456, 311, 4467, 337, 36070, 2539, 51256], "temperature": 0.0, "avg_logprob": -0.08879974001929873, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0016457934398204088}, {"id": 849, "seek": 516356, "start": 5181.400000000001, "end": 5187.64, "text": " happening up and beyond just reinforcement learning. Okay. Yeah, I like the return to the", "tokens": [51256, 2737, 493, 293, 4399, 445, 29280, 2539, 13, 1033, 13, 865, 11, 286, 411, 264, 2736, 281, 264, 51568], "temperature": 0.0, "avg_logprob": -0.08879974001929873, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0016457934398204088}, {"id": 850, "seek": 516356, "start": 5187.64, "end": 5192.04, "text": " phenomena. You know, Dreyfus has this phrase, which I will definitely use over and over again", "tokens": [51568, 22004, 13, 509, 458, 11, 413, 7950, 69, 301, 575, 341, 9535, 11, 597, 286, 486, 2138, 764, 670, 293, 670, 797, 51788], "temperature": 0.0, "avg_logprob": -0.08879974001929873, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0016457934398204088}, {"id": 851, "seek": 519204, "start": 5192.04, "end": 5198.36, "text": " in this podcast, which is when in doubt return to the phenomena. The way that we have cast it", "tokens": [50364, 294, 341, 7367, 11, 597, 307, 562, 294, 6385, 2736, 281, 264, 22004, 13, 440, 636, 300, 321, 362, 4193, 309, 50680], "temperature": 0.0, "avg_logprob": -0.13229302565256754, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.006060396786779165}, {"id": 852, "seek": 519204, "start": 5198.92, "end": 5203.96, "text": " is that there's this idea in the literature about a hyper prior that the world is changing. So that", "tokens": [50708, 307, 300, 456, 311, 341, 1558, 294, 264, 10394, 466, 257, 9848, 4059, 300, 264, 1002, 307, 4473, 13, 407, 300, 50960], "temperature": 0.0, "avg_logprob": -0.13229302565256754, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.006060396786779165}, {"id": 853, "seek": 519204, "start": 5203.96, "end": 5210.12, "text": " we have a prior that the world is changing. And that is adapted for us because we don't get stuck", "tokens": [50960, 321, 362, 257, 4059, 300, 264, 1002, 307, 4473, 13, 400, 300, 307, 20871, 337, 505, 570, 321, 500, 380, 483, 5541, 51268], "temperature": 0.0, "avg_logprob": -0.13229302565256754, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.006060396786779165}, {"id": 854, "seek": 519204, "start": 5210.12, "end": 5215.72, "text": " in the same free energy minima. Now, if that prior is always at play, what ends up happening", "tokens": [51268, 294, 264, 912, 1737, 2281, 4464, 64, 13, 823, 11, 498, 300, 4059, 307, 1009, 412, 862, 11, 437, 5314, 493, 2737, 51548], "temperature": 0.0, "avg_logprob": -0.13229302565256754, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.006060396786779165}, {"id": 855, "seek": 521572, "start": 5215.72, "end": 5222.2, "text": " downstream on that is that my inference about my own abilities deteriorates over time. And we", "tokens": [50364, 30621, 322, 300, 307, 300, 452, 38253, 466, 452, 1065, 11582, 26431, 1024, 670, 565, 13, 400, 321, 50688], "temperature": 0.0, "avg_logprob": -0.1261184194813604, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.1747036576271057}, {"id": 856, "seek": 521572, "start": 5222.2, "end": 5226.52, "text": " experience that, I guess, I mean, like if I play tennis today, and then I play tennis in a week's", "tokens": [50688, 1752, 300, 11, 286, 2041, 11, 286, 914, 11, 411, 498, 286, 862, 18118, 965, 11, 293, 550, 286, 862, 18118, 294, 257, 1243, 311, 50904], "temperature": 0.0, "avg_logprob": -0.1261184194813604, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.1747036576271057}, {"id": 857, "seek": 521572, "start": 5226.52, "end": 5230.6, "text": " time, I'm probably going to have less fidelity in my own capacities in the week's time. And I do,", "tokens": [50904, 565, 11, 286, 478, 1391, 516, 281, 362, 1570, 46404, 294, 452, 1065, 39396, 294, 264, 1243, 311, 565, 13, 400, 286, 360, 11, 51108], "temperature": 0.0, "avg_logprob": -0.1261184194813604, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.1747036576271057}, {"id": 858, "seek": 521572, "start": 5230.6, "end": 5238.280000000001, "text": " if I play tomorrow, having followed today. And so our argument is that the positive affect", "tokens": [51108, 498, 286, 862, 4153, 11, 1419, 6263, 965, 13, 400, 370, 527, 6770, 307, 300, 264, 3353, 3345, 51492], "temperature": 0.0, "avg_logprob": -0.1261184194813604, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.1747036576271057}, {"id": 859, "seek": 521572, "start": 5238.280000000001, "end": 5243.88, "text": " that's part of the flow state is downstream on the surprise that's generated when you actually", "tokens": [51492, 300, 311, 644, 295, 264, 3095, 1785, 307, 30621, 322, 264, 6365, 300, 311, 10833, 562, 291, 767, 51772], "temperature": 0.0, "avg_logprob": -0.1261184194813604, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.1747036576271057}, {"id": 860, "seek": 524388, "start": 5244.4400000000005, "end": 5248.28, "text": " violate that hyper prior that the world is changing. Because what you're getting evidence for is that", "tokens": [50392, 37478, 300, 9848, 4059, 300, 264, 1002, 307, 4473, 13, 1436, 437, 291, 434, 1242, 4467, 337, 307, 300, 50584], "temperature": 0.0, "avg_logprob": -0.13544633367040135, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.009664410725235939}, {"id": 861, "seek": 524388, "start": 5248.28, "end": 5251.72, "text": " your policy still work in the world that you would have been for, you would have thought", "tokens": [50584, 428, 3897, 920, 589, 294, 264, 1002, 300, 291, 576, 362, 668, 337, 11, 291, 576, 362, 1194, 50756], "temperature": 0.0, "avg_logprob": -0.13544633367040135, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.009664410725235939}, {"id": 862, "seek": 524388, "start": 5251.72, "end": 5258.12, "text": " didn't lend itself to your policies working. And there's this idea, Casper Hesp's paper and his", "tokens": [50756, 994, 380, 21774, 2564, 281, 428, 7657, 1364, 13, 400, 456, 311, 341, 1558, 11, 16100, 610, 389, 13361, 311, 3035, 293, 702, 51076], "temperature": 0.0, "avg_logprob": -0.13544633367040135, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.009664410725235939}, {"id": 863, "seek": 524388, "start": 5258.12, "end": 5264.76, "text": " co-authors in 2021 that affect inactive inference is about basically your model doing better than", "tokens": [51076, 598, 12, 40198, 830, 294, 7201, 300, 3345, 294, 12596, 38253, 307, 466, 1936, 428, 2316, 884, 1101, 813, 51408], "temperature": 0.0, "avg_logprob": -0.13544633367040135, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.009664410725235939}, {"id": 864, "seek": 524388, "start": 5264.76, "end": 5270.52, "text": " you thought it would do. And this actually comes a lot into what Mark talks about in terms of", "tokens": [51408, 291, 1194, 309, 576, 360, 13, 400, 341, 767, 1487, 257, 688, 666, 437, 3934, 6686, 466, 294, 2115, 295, 51696], "temperature": 0.0, "avg_logprob": -0.13544633367040135, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.009664410725235939}, {"id": 865, "seek": 527052, "start": 5270.52, "end": 5283.8, "text": " aerodynamics. So I'm wondering, I don't know. Well, see, I think that's right. And the problem,", "tokens": [50364, 11207, 35483, 13, 407, 286, 478, 6359, 11, 286, 500, 380, 458, 13, 1042, 11, 536, 11, 286, 519, 300, 311, 558, 13, 400, 264, 1154, 11, 51028], "temperature": 0.0, "avg_logprob": -0.20330390255008124, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0275578573346138}, {"id": 866, "seek": 527052, "start": 5283.8, "end": 5288.84, "text": " of course, is you can get an infinite regress. So well, I'll say, well, the things here, and you'll", "tokens": [51028, 295, 1164, 11, 307, 291, 393, 483, 364, 13785, 1121, 735, 13, 407, 731, 11, 286, 603, 584, 11, 731, 11, 264, 721, 510, 11, 293, 291, 603, 51280], "temperature": 0.0, "avg_logprob": -0.20330390255008124, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0275578573346138}, {"id": 867, "seek": 527052, "start": 5288.84, "end": 5293.72, "text": " say, well, there needs to be something a hyper prior behind that. And then we can, that's what I", "tokens": [51280, 584, 11, 731, 11, 456, 2203, 281, 312, 746, 257, 9848, 4059, 2261, 300, 13, 400, 550, 321, 393, 11, 300, 311, 437, 286, 51524], "temperature": 0.0, "avg_logprob": -0.20330390255008124, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0275578573346138}, {"id": 868, "seek": 527052, "start": 5293.72, "end": 5299.4800000000005, "text": " mean about thesis is shit. So again, I would say, well, what are you getting better at? And what", "tokens": [51524, 914, 466, 22288, 307, 4611, 13, 407, 797, 11, 286, 576, 584, 11, 731, 11, 437, 366, 291, 1242, 1101, 412, 30, 400, 437, 51812], "temperature": 0.0, "avg_logprob": -0.20330390255008124, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0275578573346138}, {"id": 869, "seek": 529948, "start": 5299.48, "end": 5307.32, "text": " you're getting better at, I would argue, is not stand like so expertise is generally built around", "tokens": [50364, 291, 434, 1242, 1101, 412, 11, 286, 576, 9695, 11, 307, 406, 1463, 411, 370, 11769, 307, 5101, 3094, 926, 50756], "temperature": 0.0, "avg_logprob": -0.12815241103476666, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0006069630035199225}, {"id": 870, "seek": 529948, "start": 5308.679999999999, "end": 5315.32, "text": " giving you a well, like a specific domain. We can talk about the possibility of sort of meta", "tokens": [50824, 2902, 291, 257, 731, 11, 411, 257, 2685, 9274, 13, 492, 393, 751, 466, 264, 7959, 295, 1333, 295, 19616, 51156], "temperature": 0.0, "avg_logprob": -0.12815241103476666, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0006069630035199225}, {"id": 871, "seek": 529948, "start": 5315.32, "end": 5321.48, "text": " expertise if you want. But what I mean by that is, once you've got expertise, you've gotten really", "tokens": [51156, 11769, 498, 291, 528, 13, 583, 437, 286, 914, 538, 300, 307, 11, 1564, 291, 600, 658, 11769, 11, 291, 600, 5768, 534, 51464], "temperature": 0.0, "avg_logprob": -0.12815241103476666, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0006069630035199225}, {"id": 872, "seek": 529948, "start": 5321.48, "end": 5327.959999999999, "text": " good at within a certain domain at formulating the problems you're fronting as for you, well", "tokens": [51464, 665, 412, 1951, 257, 1629, 9274, 412, 1254, 12162, 264, 2740, 291, 434, 431, 266, 783, 382, 337, 291, 11, 731, 51788], "temperature": 0.0, "avg_logprob": -0.12815241103476666, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0006069630035199225}, {"id": 873, "seek": 532796, "start": 5328.04, "end": 5333.72, "text": " defined problems. That's one way in which an expert is sort of reliably different from a novice.", "tokens": [50368, 7642, 2740, 13, 663, 311, 472, 636, 294, 597, 364, 5844, 307, 1333, 295, 49927, 819, 490, 257, 23883, 573, 13, 50652], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 874, "seek": 532796, "start": 5333.72, "end": 5337.4, "text": " A novice goes in, this is no defined problem, the expert goes, it goes in, it's a, no, it's this", "tokens": [50652, 316, 23883, 573, 1709, 294, 11, 341, 307, 572, 7642, 1154, 11, 264, 5844, 1709, 11, 309, 1709, 294, 11, 309, 311, 257, 11, 572, 11, 309, 311, 341, 50836], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 875, "seek": 532796, "start": 5337.4, "end": 5342.2, "text": " problem. And this is the right way to do this and this and this and this. Okay, so we agree with", "tokens": [50836, 1154, 13, 400, 341, 307, 264, 558, 636, 281, 360, 341, 293, 341, 293, 341, 293, 341, 13, 1033, 11, 370, 321, 3986, 365, 51076], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 876, "seek": 532796, "start": 5342.2, "end": 5347.4, "text": " that. But I think what's happening, right? And maybe this is the gray area between your position", "tokens": [51076, 300, 13, 583, 286, 519, 437, 311, 2737, 11, 558, 30, 400, 1310, 341, 307, 264, 10855, 1859, 1296, 428, 2535, 51336], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 877, "seek": 532796, "start": 5347.4, "end": 5351.32, "text": " in mind. I think what's happening is we're discovering new ways in which we can turn", "tokens": [51336, 294, 1575, 13, 286, 519, 437, 311, 2737, 307, 321, 434, 24773, 777, 2098, 294, 597, 321, 393, 1261, 51532], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 878, "seek": 532796, "start": 5351.32, "end": 5356.28, "text": " well defined problem, sorry, ill defined states into well defined problems. But something's like,", "tokens": [51532, 731, 7642, 1154, 11, 2597, 11, 3171, 7642, 4368, 666, 731, 7642, 2740, 13, 583, 746, 311, 411, 11, 51780], "temperature": 0.0, "avg_logprob": -0.17335959194468803, "compression_ratio": 1.9791666666666667, "no_speech_prob": 0.025160839781165123}, {"id": 879, "seek": 535628, "start": 5356.36, "end": 5366.84, "text": " I didn't realize I could adapt to that situation, but I can. And that sounds similar. But for me,", "tokens": [50368, 286, 994, 380, 4325, 286, 727, 6231, 281, 300, 2590, 11, 457, 286, 393, 13, 400, 300, 3263, 2531, 13, 583, 337, 385, 11, 50892], "temperature": 0.0, "avg_logprob": -0.16416205822581975, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0016475971788167953}, {"id": 880, "seek": 535628, "start": 5366.84, "end": 5373.8, "text": " that's an insight experience. And that's what, like it's an insight flow, because you're getting", "tokens": [50892, 300, 311, 364, 11269, 1752, 13, 400, 300, 311, 437, 11, 411, 309, 311, 364, 11269, 3095, 11, 570, 291, 434, 1242, 51240], "temperature": 0.0, "avg_logprob": -0.16416205822581975, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0016475971788167953}, {"id": 881, "seek": 535628, "start": 5373.8, "end": 5379.5599999999995, "text": " an extension of your cognitive capacity, because you have restructured what you take to be", "tokens": [51240, 364, 10320, 295, 428, 15605, 6042, 11, 570, 291, 362, 1472, 46847, 437, 291, 747, 281, 312, 51528], "temperature": 0.0, "avg_logprob": -0.16416205822581975, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0016475971788167953}, {"id": 882, "seek": 537956, "start": 5380.52, "end": 5384.280000000001, "text": " like your, your problem formulation of the world. That's, that's,", "tokens": [50412, 411, 428, 11, 428, 1154, 37642, 295, 264, 1002, 13, 663, 311, 11, 300, 311, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1712195895133762, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.02840994857251644}, {"id": 883, "seek": 537956, "start": 5385.64, "end": 5393.240000000001, "text": " yeah, I mean, to supplement that, I actually, I don't disagree on that note, but the way I would", "tokens": [50668, 1338, 11, 286, 914, 11, 281, 15436, 300, 11, 286, 767, 11, 286, 500, 380, 14091, 322, 300, 3637, 11, 457, 264, 636, 286, 576, 51048], "temperature": 0.0, "avg_logprob": -0.1712195895133762, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.02840994857251644}, {"id": 884, "seek": 537956, "start": 5393.240000000001, "end": 5396.84, "text": " formulate that, and this actually isn't in the paper. It's in the paper in some sense,", "tokens": [51048, 47881, 300, 11, 293, 341, 767, 1943, 380, 294, 264, 3035, 13, 467, 311, 294, 264, 3035, 294, 512, 2020, 11, 51228], "temperature": 0.0, "avg_logprob": -0.1712195895133762, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.02840994857251644}, {"id": 885, "seek": 537956, "start": 5396.84, "end": 5399.8, "text": " but it's not a fully fleshed out argument, because it wasn't hyper relevant.", "tokens": [51228, 457, 309, 311, 406, 257, 4498, 12497, 292, 484, 6770, 11, 570, 309, 2067, 380, 9848, 7340, 13, 51376], "temperature": 0.0, "avg_logprob": -0.1712195895133762, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.02840994857251644}, {"id": 886, "seek": 537956, "start": 5400.4400000000005, "end": 5405.160000000001, "text": " I'm glad there's another really quality paper on flow. There's really nothing.", "tokens": [51408, 286, 478, 5404, 456, 311, 1071, 534, 3125, 3035, 322, 3095, 13, 821, 311, 534, 1825, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1712195895133762, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.02840994857251644}, {"id": 887, "seek": 540516, "start": 5405.72, "end": 5410.84, "text": " When we published the paper, we were literally the only paper talking about the cognitive", "tokens": [50392, 1133, 321, 6572, 264, 3035, 11, 321, 645, 3736, 264, 787, 3035, 1417, 466, 264, 15605, 50648], "temperature": 0.0, "avg_logprob": -0.18342588061378115, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.005382695235311985}, {"id": 888, "seek": 540516, "start": 5410.84, "end": 5417.96, "text": " processes that work in flow. It's a bizarre one. There's very little on skillful coping,", "tokens": [50648, 7555, 300, 589, 294, 3095, 13, 467, 311, 257, 18265, 472, 13, 821, 311, 588, 707, 322, 5389, 906, 32893, 11, 51004], "temperature": 0.0, "avg_logprob": -0.18342588061378115, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.005382695235311985}, {"id": 889, "seek": 540516, "start": 5417.96, "end": 5422.84, "text": " generally, within cognitive science. But as I said, I think that's why I said there should be", "tokens": [51004, 5101, 11, 1951, 15605, 3497, 13, 583, 382, 286, 848, 11, 286, 519, 300, 311, 983, 286, 848, 456, 820, 312, 51248], "temperature": 0.0, "avg_logprob": -0.18342588061378115, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.005382695235311985}, {"id": 890, "seek": 540516, "start": 5422.84, "end": 5430.92, "text": " more phenomenology active or cognitive science convergence. What we kind of mentioned in brief", "tokens": [51248, 544, 9388, 1793, 4967, 420, 15605, 3497, 32181, 13, 708, 321, 733, 295, 2835, 294, 5353, 51652], "temperature": 0.0, "avg_logprob": -0.18342588061378115, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.005382695235311985}, {"id": 891, "seek": 543092, "start": 5430.92, "end": 5435.96, "text": " is that flow, if we take the sort of macro perspective on what flow is, it can also be", "tokens": [50364, 307, 300, 3095, 11, 498, 321, 747, 264, 1333, 295, 18887, 4585, 322, 437, 3095, 307, 11, 309, 393, 611, 312, 50616], "temperature": 0.0, "avg_logprob": -0.11543554013913816, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0015472747618332505}, {"id": 892, "seek": 543092, "start": 5435.96, "end": 5441.24, "text": " this kind of humming between the pragmatic and the epistemic boundary. So maybe when you're", "tokens": [50616, 341, 733, 295, 34965, 1296, 264, 46904, 293, 264, 2388, 468, 3438, 12866, 13, 407, 1310, 562, 291, 434, 50880], "temperature": 0.0, "avg_logprob": -0.11543554013913816, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0015472747618332505}, {"id": 893, "seek": 543092, "start": 5441.24, "end": 5450.2, "text": " realizing, oh, I can reframe this kind of perplexing problem and actually something manageable,", "tokens": [50880, 16734, 11, 1954, 11, 286, 393, 13334, 529, 341, 733, 295, 680, 18945, 278, 1154, 293, 767, 746, 38798, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11543554013913816, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0015472747618332505}, {"id": 894, "seek": 543092, "start": 5450.2, "end": 5455.4, "text": " what you're doing there is maybe just slightly x-ting the flow, doing some epistemic foraging,", "tokens": [51328, 437, 291, 434, 884, 456, 307, 1310, 445, 4748, 2031, 12, 783, 264, 3095, 11, 884, 512, 2388, 468, 3438, 337, 3568, 11, 51588], "temperature": 0.0, "avg_logprob": -0.11543554013913816, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0015472747618332505}, {"id": 895, "seek": 543092, "start": 5455.4, "end": 5458.52, "text": " returning, because something I want to make really clear to everyone listening as well.", "tokens": [51588, 12678, 11, 570, 746, 286, 528, 281, 652, 534, 1850, 281, 1518, 4764, 382, 731, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11543554013913816, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.0015472747618332505}, {"id": 896, "seek": 545852, "start": 5459.240000000001, "end": 5464.360000000001, "text": " When we talk about epistemic action and pragmatic action in terms of expected free energy or active", "tokens": [50400, 1133, 321, 751, 466, 2388, 468, 3438, 3069, 293, 46904, 3069, 294, 2115, 295, 5176, 1737, 2281, 420, 4967, 50656], "temperature": 0.0, "avg_logprob": -0.14239880442619324, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.002471861196681857}, {"id": 897, "seek": 545852, "start": 5464.360000000001, "end": 5471.160000000001, "text": " inference, it's not like the agent goes, I'm doing pragmatic action right now. It's part", "tokens": [50656, 38253, 11, 309, 311, 406, 411, 264, 9461, 1709, 11, 286, 478, 884, 46904, 3069, 558, 586, 13, 467, 311, 644, 50996], "temperature": 0.0, "avg_logprob": -0.14239880442619324, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.002471861196681857}, {"id": 898, "seek": 545852, "start": 5471.160000000001, "end": 5479.160000000001, "text": " of the actual maths. If you look at the maths, firstly, you can do a bit of both at the same", "tokens": [50996, 295, 264, 3539, 36287, 13, 759, 291, 574, 412, 264, 36287, 11, 27376, 11, 291, 393, 360, 257, 857, 295, 1293, 412, 264, 912, 51396], "temperature": 0.0, "avg_logprob": -0.14239880442619324, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.002471861196681857}, {"id": 899, "seek": 545852, "start": 5479.160000000001, "end": 5484.4400000000005, "text": " time. That's a thing. But it's also like the action policies that we're talking about here,", "tokens": [51396, 565, 13, 663, 311, 257, 551, 13, 583, 309, 311, 611, 411, 264, 3069, 7657, 300, 321, 434, 1417, 466, 510, 11, 51660], "temperature": 0.0, "avg_logprob": -0.14239880442619324, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.002471861196681857}, {"id": 900, "seek": 548444, "start": 5484.5199999999995, "end": 5491.5599999999995, "text": " John and I, these are not protracted seven hours of just like pragmatic action. No,", "tokens": [50368, 2619, 293, 286, 11, 613, 366, 406, 1742, 1897, 292, 3407, 2496, 295, 445, 411, 46904, 3069, 13, 883, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11085958991731916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018536461517214775}, {"id": 901, "seek": 548444, "start": 5491.5599999999995, "end": 5495.719999999999, "text": " these have to be very temporally thin because the world is volatile. The world is constantly", "tokens": [50720, 613, 362, 281, 312, 588, 8219, 379, 5862, 570, 264, 1002, 307, 34377, 13, 440, 1002, 307, 6460, 50928], "temperature": 0.0, "avg_logprob": -0.11085958991731916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018536461517214775}, {"id": 902, "seek": 548444, "start": 5495.719999999999, "end": 5500.599999999999, "text": " changing. And so you have to be able to not just pin all your hopes or pull your eggs in one basket", "tokens": [50928, 4473, 13, 400, 370, 291, 362, 281, 312, 1075, 281, 406, 445, 5447, 439, 428, 13681, 420, 2235, 428, 6466, 294, 472, 8390, 51172], "temperature": 0.0, "avg_logprob": -0.11085958991731916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018536461517214775}, {"id": 903, "seek": 548444, "start": 5500.599999999999, "end": 5505.879999999999, "text": " in terms of an action policy. And so actually fundamentally, our divergence might well be", "tokens": [51172, 294, 2115, 295, 364, 3069, 3897, 13, 400, 370, 767, 17879, 11, 527, 47387, 1062, 731, 312, 51436], "temperature": 0.0, "avg_logprob": -0.11085958991731916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018536461517214775}, {"id": 904, "seek": 548444, "start": 5505.879999999999, "end": 5511.08, "text": " a semantic thing, which is that I'm talking very much about the synchronic nature of being in flow", "tokens": [51436, 257, 47982, 551, 11, 597, 307, 300, 286, 478, 1417, 588, 709, 466, 264, 5451, 339, 10011, 3687, 295, 885, 294, 3095, 51696], "temperature": 0.0, "avg_logprob": -0.11085958991731916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018536461517214775}, {"id": 905, "seek": 551108, "start": 5511.08, "end": 5517.4, "text": " right now. And for simplicity, I have that boundary between pragmatic and epistemic,", "tokens": [50364, 558, 586, 13, 400, 337, 25632, 11, 286, 362, 300, 12866, 1296, 46904, 293, 2388, 468, 3438, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1137442244696863, "compression_ratio": 1.6147859922178989, "no_speech_prob": 0.0029783830977976322}, {"id": 906, "seek": 551108, "start": 5517.4, "end": 5523.96, "text": " whereas if you take a more macro perspective, as that flow state, as the concerto unfolds,", "tokens": [50680, 9735, 498, 291, 747, 257, 544, 18887, 4585, 11, 382, 300, 3095, 1785, 11, 382, 264, 8543, 78, 17980, 82, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1137442244696863, "compression_ratio": 1.6147859922178989, "no_speech_prob": 0.0029783830977976322}, {"id": 907, "seek": 551108, "start": 5523.96, "end": 5526.36, "text": " you could be humming at that boundary and doing learning as well.", "tokens": [51008, 291, 727, 312, 34965, 412, 300, 12866, 293, 884, 2539, 382, 731, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1137442244696863, "compression_ratio": 1.6147859922178989, "no_speech_prob": 0.0029783830977976322}, {"id": 908, "seek": 551108, "start": 5527.24, "end": 5532.28, "text": " Yeah, I really look forward to your paper because I think that sounds like actually", "tokens": [51172, 865, 11, 286, 534, 574, 2128, 281, 428, 3035, 570, 286, 519, 300, 3263, 411, 767, 51424], "temperature": 0.0, "avg_logprob": -0.1137442244696863, "compression_ratio": 1.6147859922178989, "no_speech_prob": 0.0029783830977976322}, {"id": 909, "seek": 551108, "start": 5532.84, "end": 5538.84, "text": " a powerful way in which the theories could be integrated together. I'm also interested in", "tokens": [51452, 257, 4005, 636, 294, 597, 264, 13667, 727, 312, 10919, 1214, 13, 286, 478, 611, 3102, 294, 51752], "temperature": 0.0, "avg_logprob": -0.1137442244696863, "compression_ratio": 1.6147859922178989, "no_speech_prob": 0.0029783830977976322}, {"id": 910, "seek": 553884, "start": 5538.84, "end": 5543.24, "text": " the question that's emerging out of this, well, I've already been interested in the question,", "tokens": [50364, 264, 1168, 300, 311, 14989, 484, 295, 341, 11, 731, 11, 286, 600, 1217, 668, 3102, 294, 264, 1168, 11, 50584], "temperature": 0.0, "avg_logprob": -0.13233919558317764, "compression_ratio": 1.9020408163265305, "no_speech_prob": 0.010007740929722786}, {"id": 911, "seek": 553884, "start": 5543.24, "end": 5547.88, "text": " but it emerges out of what we're talking about. Because I'm interested because I think there's,", "tokens": [50584, 457, 309, 38965, 484, 295, 437, 321, 434, 1417, 466, 13, 1436, 286, 478, 3102, 570, 286, 519, 456, 311, 11, 50816], "temperature": 0.0, "avg_logprob": -0.13233919558317764, "compression_ratio": 1.9020408163265305, "no_speech_prob": 0.010007740929722786}, {"id": 912, "seek": 553884, "start": 5547.88, "end": 5556.68, "text": " there might be an additional thing. Because there's, you can flow in a situation and it", "tokens": [50816, 456, 1062, 312, 364, 4497, 551, 13, 1436, 456, 311, 11, 291, 393, 3095, 294, 257, 2590, 293, 309, 51256], "temperature": 0.0, "avg_logprob": -0.13233919558317764, "compression_ratio": 1.9020408163265305, "no_speech_prob": 0.010007740929722786}, {"id": 913, "seek": 553884, "start": 5556.68, "end": 5561.24, "text": " doesn't transfer, it doesn't transfer to other domains, video game addiction is the classic", "tokens": [51256, 1177, 380, 5003, 11, 309, 1177, 380, 5003, 281, 661, 25514, 11, 960, 1216, 16835, 307, 264, 7230, 51484], "temperature": 0.0, "avg_logprob": -0.13233919558317764, "compression_ratio": 1.9020408163265305, "no_speech_prob": 0.010007740929722786}, {"id": 914, "seek": 553884, "start": 5561.24, "end": 5565.08, "text": " model, you can flow in the game, you can't flow in the world. So you get depressed in the world,", "tokens": [51484, 2316, 11, 291, 393, 3095, 294, 264, 1216, 11, 291, 393, 380, 3095, 294, 264, 1002, 13, 407, 291, 483, 18713, 294, 264, 1002, 11, 51676], "temperature": 0.0, "avg_logprob": -0.13233919558317764, "compression_ratio": 1.9020408163265305, "no_speech_prob": 0.010007740929722786}, {"id": 915, "seek": 556508, "start": 5565.08, "end": 5568.12, "text": " you can flow in the game and so you want to stay in the game more and more and more.", "tokens": [50364, 291, 393, 3095, 294, 264, 1216, 293, 370, 291, 528, 281, 1754, 294, 264, 1216, 544, 293, 544, 293, 544, 13, 50516], "temperature": 0.0, "avg_logprob": -0.14279487069729155, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.019998542964458466}, {"id": 916, "seek": 556508, "start": 5569.96, "end": 5574.28, "text": " That's very different than how I flow in Tai Chi Chuan. Tai Chi Chuan, in fact,", "tokens": [50608, 663, 311, 588, 819, 813, 577, 286, 3095, 294, 9623, 17730, 761, 6139, 13, 9623, 17730, 761, 6139, 11, 294, 1186, 11, 50824], "temperature": 0.0, "avg_logprob": -0.14279487069729155, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.019998542964458466}, {"id": 917, "seek": 556508, "start": 5574.28, "end": 5578.68, "text": " other people pointed this out in me, the flow states that I cultivate in Tai Chi Chuan,", "tokens": [50824, 661, 561, 10932, 341, 484, 294, 385, 11, 264, 3095, 4368, 300, 286, 33341, 294, 9623, 17730, 761, 6139, 11, 51044], "temperature": 0.0, "avg_logprob": -0.14279487069729155, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.019998542964458466}, {"id": 918, "seek": 556508, "start": 5578.68, "end": 5583.48, "text": " I can, they are cultivated in such a way that they transfer broadly and deeply to many other", "tokens": [51044, 286, 393, 11, 436, 366, 46770, 294, 1270, 257, 636, 300, 436, 5003, 19511, 293, 8760, 281, 867, 661, 51284], "temperature": 0.0, "avg_logprob": -0.14279487069729155, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.019998542964458466}, {"id": 919, "seek": 556508, "start": 5583.48, "end": 5590.92, "text": " domains. And I'm interested in this sort of ritual framework, because what you get is this", "tokens": [51284, 25514, 13, 400, 286, 478, 3102, 294, 341, 1333, 295, 13792, 8388, 11, 570, 437, 291, 483, 307, 341, 51656], "temperature": 0.0, "avg_logprob": -0.14279487069729155, "compression_ratio": 1.7868852459016393, "no_speech_prob": 0.019998542964458466}, {"id": 920, "seek": 559092, "start": 5590.92, "end": 5597.24, "text": " ritual and this philosophical framework, Taoism and other, other practices and ecology of practices", "tokens": [50364, 13792, 293, 341, 25066, 8388, 11, 26580, 1434, 293, 661, 11, 661, 7525, 293, 39683, 295, 7525, 50680], "temperature": 0.0, "avg_logprob": -0.14467650193434495, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.01442807074636221}, {"id": 921, "seek": 559092, "start": 5597.24, "end": 5606.84, "text": " around so that it broadens, like it transfers in powerful ways. And I'm wondering what the", "tokens": [50680, 926, 370, 300, 309, 4152, 694, 11, 411, 309, 29137, 294, 4005, 2098, 13, 400, 286, 478, 6359, 437, 264, 51160], "temperature": 0.0, "avg_logprob": -0.14467650193434495, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.01442807074636221}, {"id": 922, "seek": 559092, "start": 5606.84, "end": 5612.68, "text": " differences are. And also if, how does that show up? Is there a phenomenological difference?", "tokens": [51160, 7300, 366, 13, 400, 611, 498, 11, 577, 775, 300, 855, 493, 30, 1119, 456, 257, 9388, 4383, 2649, 30, 51452], "temperature": 0.0, "avg_logprob": -0.14467650193434495, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.01442807074636221}, {"id": 923, "seek": 561268, "start": 5613.64, "end": 5620.6, "text": " I mean, so now really, really, really weakly, anecdotally, right, in terms of phenomenological", "tokens": [50412, 286, 914, 11, 370, 586, 534, 11, 534, 11, 534, 5336, 356, 11, 26652, 310, 379, 11, 558, 11, 294, 2115, 295, 9388, 4383, 50760], "temperature": 0.0, "avg_logprob": -0.14272622973005347, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.07546526938676834}, {"id": 924, "seek": 561268, "start": 5620.6, "end": 5626.68, "text": " practice, I do get a sense of a difference between what I'm just flowing and what I'm flowing in a", "tokens": [50760, 3124, 11, 286, 360, 483, 257, 2020, 295, 257, 2649, 1296, 437, 286, 478, 445, 13974, 293, 437, 286, 478, 13974, 294, 257, 51064], "temperature": 0.0, "avg_logprob": -0.14272622973005347, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.07546526938676834}, {"id": 925, "seek": 561268, "start": 5626.68, "end": 5633.0, "text": " ritual context. So if I enter into a violin hall, and let's say I'm an expert in playing the violin,", "tokens": [51064, 13792, 4319, 13, 407, 498, 286, 3242, 666, 257, 22878, 6500, 11, 293, 718, 311, 584, 286, 478, 364, 5844, 294, 2433, 264, 22878, 11, 51380], "temperature": 0.0, "avg_logprob": -0.14272622973005347, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.07546526938676834}, {"id": 926, "seek": 561268, "start": 5633.56, "end": 5638.6, "text": " and I see a crowd and I see my violin and I see the whole stage, what I end up getting is this,", "tokens": [51408, 293, 286, 536, 257, 6919, 293, 286, 536, 452, 22878, 293, 286, 536, 264, 1379, 3233, 11, 437, 286, 917, 493, 1242, 307, 341, 11, 51660], "temperature": 0.0, "avg_logprob": -0.14272622973005347, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.07546526938676834}, {"id": 927, "seek": 563860, "start": 5639.320000000001, "end": 5645.64, "text": " so habits are quite well codified within active inference, you get this kind of contextual cue", "tokens": [50400, 370, 14100, 366, 1596, 731, 17656, 2587, 1951, 4967, 38253, 11, 291, 483, 341, 733, 295, 35526, 22656, 50716], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 928, "seek": 563860, "start": 5645.64, "end": 5649.08, "text": " that like in layman's touch, although this isn't, we're saying this isn't happening propositionally,", "tokens": [50716, 300, 411, 294, 2360, 1601, 311, 2557, 11, 4878, 341, 1943, 380, 11, 321, 434, 1566, 341, 1943, 380, 2737, 24830, 379, 11, 50888], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 929, "seek": 563860, "start": 5649.8, "end": 5655.400000000001, "text": " it's time for me to play the violin. And this can look like precision over beliefs about your own", "tokens": [50924, 309, 311, 565, 337, 385, 281, 862, 264, 22878, 13, 400, 341, 393, 574, 411, 18356, 670, 13585, 466, 428, 1065, 51204], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 930, "seek": 563860, "start": 5655.400000000001, "end": 5661.160000000001, "text": " action. So what, this is again, not something I've thought about necessarily with any depth,", "tokens": [51204, 3069, 13, 407, 437, 11, 341, 307, 797, 11, 406, 746, 286, 600, 1194, 466, 4725, 365, 604, 7161, 11, 51492], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 931, "seek": 563860, "start": 5661.160000000001, "end": 5664.84, "text": " because this is just something that you've brought up. I'm wondering whether that contextual cue,", "tokens": [51492, 570, 341, 307, 445, 746, 300, 291, 600, 3038, 493, 13, 286, 478, 6359, 1968, 300, 35526, 22656, 11, 51676], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 932, "seek": 563860, "start": 5664.84, "end": 5667.8, "text": " what you're happening when you're doing Tai Chi and then maybe what you're doing when you're", "tokens": [51676, 437, 291, 434, 2737, 562, 291, 434, 884, 9623, 17730, 293, 550, 1310, 437, 291, 434, 884, 562, 291, 434, 51824], "temperature": 0.0, "avg_logprob": -0.12625517668547453, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.011199642904102802}, {"id": 933, "seek": 566780, "start": 5667.8, "end": 5678.84, "text": " lecturing is that your system is seeing similarities in the context and doing similar precision", "tokens": [50364, 5899, 1345, 307, 300, 428, 1185, 307, 2577, 24197, 294, 264, 4319, 293, 884, 2531, 18356, 50916], "temperature": 0.0, "avg_logprob": -0.14248360356976908, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.00045045922161079943}, {"id": 934, "seek": 566780, "start": 5678.84, "end": 5685.56, "text": " weighting in a separate domain. Yeah, I think that's right. And I think what philosophical", "tokens": [50916, 3364, 278, 294, 257, 4994, 9274, 13, 865, 11, 286, 519, 300, 311, 558, 13, 400, 286, 519, 437, 25066, 51252], "temperature": 0.0, "avg_logprob": -0.14248360356976908, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.00045045922161079943}, {"id": 935, "seek": 566780, "start": 5685.56, "end": 5694.360000000001, "text": " frameworks do is they reverse engineer that. They try to find out what might be similar,", "tokens": [51252, 29834, 360, 307, 436, 9943, 11403, 300, 13, 814, 853, 281, 915, 484, 437, 1062, 312, 2531, 11, 51692], "temperature": 0.0, "avg_logprob": -0.14248360356976908, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.00045045922161079943}, {"id": 936, "seek": 569436, "start": 5694.44, "end": 5700.5199999999995, "text": " or maybe even invariant across many contexts, and then build that back into the specific context", "tokens": [50368, 420, 1310, 754, 33270, 394, 2108, 867, 30628, 11, 293, 550, 1322, 300, 646, 666, 264, 2685, 4319, 50672], "temperature": 0.0, "avg_logprob": -0.14313223050988239, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.009701007045805454}, {"id": 937, "seek": 569436, "start": 5700.5199999999995, "end": 5706.679999999999, "text": " in which you are doing the practice to exactly afford that transfer appropriate processing.", "tokens": [50672, 294, 597, 291, 366, 884, 264, 3124, 281, 2293, 6157, 300, 5003, 6854, 9007, 13, 50980], "temperature": 0.0, "avg_logprob": -0.14313223050988239, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.009701007045805454}, {"id": 938, "seek": 569436, "start": 5706.679999999999, "end": 5711.24, "text": " I think that's right. I think there's something else also going on with the cueing, and this goes", "tokens": [50980, 286, 519, 300, 311, 558, 13, 286, 519, 456, 311, 746, 1646, 611, 516, 322, 365, 264, 22656, 278, 11, 293, 341, 1709, 51208], "temperature": 0.0, "avg_logprob": -0.14313223050988239, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.009701007045805454}, {"id": 939, "seek": 569436, "start": 5711.24, "end": 5718.599999999999, "text": " into Aptur's metamotivational theory. We can be broadly speaking, we frame our arousal in two", "tokens": [51208, 666, 316, 662, 374, 311, 1131, 335, 310, 592, 1478, 5261, 13, 492, 393, 312, 19511, 4124, 11, 321, 3920, 527, 594, 563, 304, 294, 732, 51576], "temperature": 0.0, "avg_logprob": -0.14313223050988239, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.009701007045805454}, {"id": 940, "seek": 571860, "start": 5718.6, "end": 5725.0, "text": " different ways. When we're in italic mode and we're working towards, where the reward is found", "tokens": [50364, 819, 2098, 13, 1133, 321, 434, 294, 22366, 299, 4391, 293, 321, 434, 1364, 3030, 11, 689, 264, 7782, 307, 1352, 50684], "temperature": 0.0, "avg_logprob": -0.14784354255312965, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.153647780418396}, {"id": 941, "seek": 571860, "start": 5725.0, "end": 5731.160000000001, "text": " in the external goal, then increased effort is experienced as frustration and then it increases", "tokens": [50684, 294, 264, 8320, 3387, 11, 550, 6505, 4630, 307, 6751, 382, 20491, 293, 550, 309, 8637, 50992], "temperature": 0.0, "avg_logprob": -0.14784354255312965, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.153647780418396}, {"id": 942, "seek": 571860, "start": 5731.160000000001, "end": 5736.120000000001, "text": " too much of getting anxiety. But if I'm doing something where the goal is the behavior itself,", "tokens": [50992, 886, 709, 295, 1242, 9119, 13, 583, 498, 286, 478, 884, 746, 689, 264, 3387, 307, 264, 5223, 2564, 11, 51240], "temperature": 0.0, "avg_logprob": -0.14784354255312965, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.153647780418396}, {"id": 943, "seek": 571860, "start": 5736.120000000001, "end": 5740.92, "text": " like making love or doing poetry, then the increased, not infinitely obviously, but the", "tokens": [51240, 411, 1455, 959, 420, 884, 15155, 11, 550, 264, 6505, 11, 406, 36227, 2745, 11, 457, 264, 51480], "temperature": 0.0, "avg_logprob": -0.14784354255312965, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.153647780418396}, {"id": 944, "seek": 574092, "start": 5740.92, "end": 5748.84, "text": " increased arousal is framed as positive, as excitement. And so I think also, and so Aptur", "tokens": [50364, 6505, 594, 563, 304, 307, 30420, 382, 3353, 11, 382, 14755, 13, 400, 370, 286, 519, 611, 11, 293, 370, 316, 662, 374, 50760], "temperature": 0.0, "avg_logprob": -0.13950695310320174, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.07254485040903091}, {"id": 945, "seek": 574092, "start": 5748.84, "end": 5757.24, "text": " talks about safety framing. Let's call the first italic mode work and the peritolic mode", "tokens": [50760, 6686, 466, 4514, 28971, 13, 961, 311, 818, 264, 700, 22366, 299, 4391, 589, 293, 264, 680, 270, 7940, 4391, 51180], "temperature": 0.0, "avg_logprob": -0.13950695310320174, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.07254485040903091}, {"id": 946, "seek": 574092, "start": 5759.56, "end": 5764.36, "text": " a play. I think ritual has to do with serious play, and I've got a whole argument about that.", "tokens": [51296, 257, 862, 13, 286, 519, 13792, 575, 281, 360, 365, 3156, 862, 11, 293, 286, 600, 658, 257, 1379, 6770, 466, 300, 13, 51536], "temperature": 0.0, "avg_logprob": -0.13950695310320174, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.07254485040903091}, {"id": 947, "seek": 574092, "start": 5764.36, "end": 5769.4, "text": " But I think also what you're doing in the context is you're trying to cue people into the play mode", "tokens": [51536, 583, 286, 519, 611, 437, 291, 434, 884, 294, 264, 4319, 307, 291, 434, 1382, 281, 22656, 561, 666, 264, 862, 4391, 51788], "temperature": 0.0, "avg_logprob": -0.13950695310320174, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.07254485040903091}, {"id": 948, "seek": 576940, "start": 5770.12, "end": 5777.16, "text": " because the play mode allows them to model themselves, their arousal, as positive,", "tokens": [50400, 570, 264, 862, 4391, 4045, 552, 281, 2316, 2969, 11, 641, 594, 563, 304, 11, 382, 3353, 11, 50752], "temperature": 0.0, "avg_logprob": -0.2519085804621379, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.007446490693837404}, {"id": 949, "seek": 576940, "start": 5777.799999999999, "end": 5782.599999999999, "text": " rather than as negative. And that allows them, and I think there's a deep connection between", "tokens": [50784, 2831, 813, 382, 3671, 13, 400, 300, 4045, 552, 11, 293, 286, 519, 456, 311, 257, 2452, 4984, 1296, 51024], "temperature": 0.0, "avg_logprob": -0.2519085804621379, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.007446490693837404}, {"id": 950, "seek": 576940, "start": 5782.599999999999, "end": 5788.759999999999, "text": " this peritolic framing and Csikszentmihais, I think accurate claim that flow is autotelic,", "tokens": [51024, 341, 680, 270, 7940, 28971, 293, 383, 82, 23292, 14185, 3057, 71, 1527, 11, 286, 519, 8559, 3932, 300, 3095, 307, 1476, 310, 338, 299, 11, 51332], "temperature": 0.0, "avg_logprob": -0.2519085804621379, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.007446490693837404}, {"id": 951, "seek": 576940, "start": 5788.759999999999, "end": 5794.5199999999995, "text": " doing it for its own sake. Right. Yeah, cool. Yeah, I like the notion of effort here.", "tokens": [51332, 884, 309, 337, 1080, 1065, 9717, 13, 1779, 13, 865, 11, 1627, 13, 865, 11, 286, 411, 264, 10710, 295, 4630, 510, 13, 51620], "temperature": 0.0, "avg_logprob": -0.2519085804621379, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.007446490693837404}, {"id": 952, "seek": 579452, "start": 5795.4800000000005, "end": 5799.56, "text": " Recognize that one of the fundamental factors or fundamental characteristics of flow is", "tokens": [50412, 44682, 1125, 300, 472, 295, 264, 8088, 6771, 420, 8088, 10891, 295, 3095, 307, 50616], "temperature": 0.0, "avg_logprob": -0.14051480293273927, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.04325978457927704}, {"id": 953, "seek": 579452, "start": 5800.360000000001, "end": 5805.56, "text": " perceived effortlessness. Yes, even though you can, even though you can have sort of very", "tokens": [50656, 19049, 4630, 26663, 13, 1079, 11, 754, 1673, 291, 393, 11, 754, 1673, 291, 393, 362, 1333, 295, 588, 50916], "temperature": 0.0, "avg_logprob": -0.14051480293273927, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.04325978457927704}, {"id": 954, "seek": 579452, "start": 5806.280000000001, "end": 5812.76, "text": " peripheral cues that you might be expending a lot of metabolic energy. Exactly. And just for the,", "tokens": [50952, 40235, 32192, 300, 291, 1062, 312, 1278, 2029, 257, 688, 295, 36464, 2281, 13, 7587, 13, 400, 445, 337, 264, 11, 51276], "temperature": 0.0, "avg_logprob": -0.14051480293273927, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.04325978457927704}, {"id": 955, "seek": 579452, "start": 5813.320000000001, "end": 5817.8, "text": " given this is the Active Inference Institute's podcast, just for those interested in the", "tokens": [51304, 2212, 341, 307, 264, 26635, 682, 5158, 9446, 311, 7367, 11, 445, 337, 729, 3102, 294, 264, 51528], "temperature": 0.0, "avg_logprob": -0.14051480293273927, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.04325978457927704}, {"id": 956, "seek": 579452, "start": 5817.8, "end": 5822.76, "text": " computational framing of that within Active Inference, Thomas Parr has got a paper out this", "tokens": [51528, 28270, 28971, 295, 300, 1951, 26635, 682, 5158, 11, 8500, 47890, 575, 658, 257, 3035, 484, 341, 51776], "temperature": 0.0, "avg_logprob": -0.14051480293273927, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.04325978457927704}, {"id": 957, "seek": 582276, "start": 5822.76, "end": 5828.2, "text": " year on cognitive effort and Active Inference. So that gives a kind of nice computational", "tokens": [50364, 1064, 322, 15605, 4630, 293, 26635, 682, 5158, 13, 407, 300, 2709, 257, 733, 295, 1481, 28270, 50636], "temperature": 0.0, "avg_logprob": -0.106606091771807, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.004880188032984734}, {"id": 958, "seek": 582276, "start": 5828.2, "end": 5833.16, "text": " modeling picture of that, which actually leads me to a much broader question because I know we're,", "tokens": [50636, 15983, 3036, 295, 300, 11, 597, 767, 6689, 385, 281, 257, 709, 13227, 1168, 570, 286, 458, 321, 434, 11, 50884], "temperature": 0.0, "avg_logprob": -0.106606091771807, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.004880188032984734}, {"id": 959, "seek": 582276, "start": 5833.16, "end": 5839.24, "text": " we haven't got too much more time, which is I've noticed in myself to be totally candid that I've", "tokens": [50884, 321, 2378, 380, 658, 886, 709, 544, 565, 11, 597, 307, 286, 600, 5694, 294, 2059, 281, 312, 3879, 6268, 300, 286, 600, 51188], "temperature": 0.0, "avg_logprob": -0.106606091771807, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.004880188032984734}, {"id": 960, "seek": 582276, "start": 5839.24, "end": 5845.96, "text": " started, like I learned about flow and Csikszentmihais through you and through other people,", "tokens": [51188, 1409, 11, 411, 286, 3264, 466, 3095, 293, 383, 82, 23292, 14185, 3057, 71, 1527, 807, 291, 293, 807, 661, 561, 11, 51524], "temperature": 0.0, "avg_logprob": -0.106606091771807, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.004880188032984734}, {"id": 961, "seek": 582276, "start": 5845.96, "end": 5852.4400000000005, "text": " but mainly as a philosophical notion. And now I'm viewing it as a computational notion.", "tokens": [51524, 457, 8704, 382, 257, 25066, 10710, 13, 400, 586, 286, 478, 17480, 309, 382, 257, 28270, 10710, 13, 51848], "temperature": 0.0, "avg_logprob": -0.106606091771807, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.004880188032984734}, {"id": 962, "seek": 585244, "start": 5852.44, "end": 5859.799999999999, "text": " Yes. Do you worry about that? Do you worry that if we rely too much on computation, not just", "tokens": [50364, 1079, 13, 1144, 291, 3292, 466, 300, 30, 1144, 291, 3292, 300, 498, 321, 10687, 886, 709, 322, 24903, 11, 406, 445, 50732], "temperature": 0.0, "avg_logprob": -0.11470165252685546, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006614169105887413}, {"id": 963, "seek": 585244, "start": 5859.799999999999, "end": 5866.839999999999, "text": " computational models, but maths, physics, we in some ways reduce the phenomena in a way that's", "tokens": [50732, 28270, 5245, 11, 457, 36287, 11, 10649, 11, 321, 294, 512, 2098, 5407, 264, 22004, 294, 257, 636, 300, 311, 51084], "temperature": 0.0, "avg_logprob": -0.11470165252685546, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006614169105887413}, {"id": 964, "seek": 585244, "start": 5866.839999999999, "end": 5872.44, "text": " detrimental, not just because it's not romantic or that it's, but like that we're missing something", "tokens": [51084, 45694, 11, 406, 445, 570, 309, 311, 406, 13590, 420, 300, 309, 311, 11, 457, 411, 300, 321, 434, 5361, 746, 51364], "temperature": 0.0, "avg_logprob": -0.11470165252685546, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006614169105887413}, {"id": 965, "seek": 585244, "start": 5872.44, "end": 5878.28, "text": " fundamentally? Well, it depends. I mean, that's a really important question. We could do two hours", "tokens": [51364, 17879, 30, 1042, 11, 309, 5946, 13, 286, 914, 11, 300, 311, 257, 534, 1021, 1168, 13, 492, 727, 360, 732, 2496, 51656], "temperature": 0.0, "avg_logprob": -0.11470165252685546, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006614169105887413}, {"id": 966, "seek": 587828, "start": 5878.28, "end": 5885.32, "text": " on just that question. But if you think that a leveled ontology is actually the way ontology is,", "tokens": [50364, 322, 445, 300, 1168, 13, 583, 498, 291, 519, 300, 257, 1496, 292, 6592, 1793, 307, 767, 264, 636, 6592, 1793, 307, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10583107216844281, "compression_ratio": 1.749034749034749, "no_speech_prob": 0.00419372133910656}, {"id": 967, "seek": 587828, "start": 5886.759999999999, "end": 5891.08, "text": " that the level at which we do science is as ontologically real as the quantum level we", "tokens": [50788, 300, 264, 1496, 412, 597, 321, 360, 3497, 307, 382, 6592, 17157, 957, 382, 264, 13018, 1496, 321, 51004], "temperature": 0.0, "avg_logprob": -0.10583107216844281, "compression_ratio": 1.749034749034749, "no_speech_prob": 0.00419372133910656}, {"id": 968, "seek": 587828, "start": 5891.08, "end": 5894.5199999999995, "text": " discover when we're doing science. And I think you have to come to that conclusion.", "tokens": [51004, 4411, 562, 321, 434, 884, 3497, 13, 400, 286, 519, 291, 362, 281, 808, 281, 300, 10063, 13, 51176], "temperature": 0.0, "avg_logprob": -0.10583107216844281, "compression_ratio": 1.749034749034749, "no_speech_prob": 0.00419372133910656}, {"id": 969, "seek": 587828, "start": 5894.5199999999995, "end": 5900.5199999999995, "text": " Right. And I have extended arguments for that elsewhere. Then you can make a clean philosophical", "tokens": [51176, 1779, 13, 400, 286, 362, 10913, 12869, 337, 300, 14517, 13, 1396, 291, 393, 652, 257, 2541, 25066, 51476], "temperature": 0.0, "avg_logprob": -0.10583107216844281, "compression_ratio": 1.749034749034749, "no_speech_prob": 0.00419372133910656}, {"id": 970, "seek": 587828, "start": 5900.5199999999995, "end": 5905.08, "text": " distinction between explaining a way in a reductive fashion and explaining that actually", "tokens": [51476, 16844, 1296, 13468, 257, 636, 294, 257, 2783, 20221, 6700, 293, 13468, 300, 767, 51704], "temperature": 0.0, "avg_logprob": -0.10583107216844281, "compression_ratio": 1.749034749034749, "no_speech_prob": 0.00419372133910656}, {"id": 971, "seek": 590508, "start": 5905.08, "end": 5911.16, "text": " enriches your appreciation for the phenomena. Now, if the computational stuff, the computational", "tokens": [50364, 18849, 279, 428, 18909, 337, 264, 22004, 13, 823, 11, 498, 264, 28270, 1507, 11, 264, 28270, 50668], "temperature": 0.0, "avg_logprob": -0.11020457969521577, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.001324308686889708}, {"id": 972, "seek": 590508, "start": 5911.16, "end": 5915.8, "text": " stuff to my mind, well, let's say what the argument we were just exploring has merit,", "tokens": [50668, 1507, 281, 452, 1575, 11, 731, 11, 718, 311, 584, 437, 264, 6770, 321, 645, 445, 12736, 575, 24527, 11, 50900], "temperature": 0.0, "avg_logprob": -0.11020457969521577, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.001324308686889708}, {"id": 973, "seek": 590508, "start": 5916.44, "end": 5919.96, "text": " then notice what we were saying. We're saying, well, what the computational modeling actually does", "tokens": [50932, 550, 3449, 437, 321, 645, 1566, 13, 492, 434, 1566, 11, 731, 11, 437, 264, 28270, 15983, 767, 775, 51108], "temperature": 0.0, "avg_logprob": -0.11020457969521577, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.001324308686889708}, {"id": 974, "seek": 590508, "start": 5919.96, "end": 5925.8, "text": " is actually shows why this sometimes very obscure philosophical framework is really important.", "tokens": [51108, 307, 767, 3110, 983, 341, 2171, 588, 34443, 25066, 8388, 307, 534, 1021, 13, 51400], "temperature": 0.0, "avg_logprob": -0.11020457969521577, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.001324308686889708}, {"id": 975, "seek": 590508, "start": 5925.8, "end": 5931.8, "text": " It's actually doing some really important work. And you can't get rid of it. You can't dispense with", "tokens": [51400, 467, 311, 767, 884, 512, 534, 1021, 589, 13, 400, 291, 393, 380, 483, 3973, 295, 309, 13, 509, 393, 380, 4920, 1288, 365, 51700], "temperature": 0.0, "avg_logprob": -0.11020457969521577, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.001324308686889708}, {"id": 976, "seek": 593180, "start": 5932.2, "end": 5939.400000000001, "text": " it. And that would mean that some of the stuff we're doing, where we're trying to commodify flow", "tokens": [50384, 309, 13, 400, 300, 576, 914, 300, 512, 295, 264, 1507, 321, 434, 884, 11, 689, 321, 434, 1382, 281, 19931, 2505, 3095, 50744], "temperature": 0.0, "avg_logprob": -0.18484884627321932, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007319772848859429}, {"id": 977, "seek": 593180, "start": 5939.400000000001, "end": 5945.56, "text": " and take it out of those frameworks and do the thing we do and sell books.", "tokens": [50744, 293, 747, 309, 484, 295, 729, 29834, 293, 360, 264, 551, 321, 360, 293, 3607, 3642, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18484884627321932, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007319772848859429}, {"id": 978, "seek": 593180, "start": 5948.2, "end": 5954.28, "text": " It's actually misrepresenting the phenomena in a way that we can philosophically and scientifically", "tokens": [51184, 467, 311, 767, 3346, 19919, 11662, 278, 264, 22004, 294, 257, 636, 300, 321, 393, 14529, 984, 293, 39719, 51488], "temperature": 0.0, "avg_logprob": -0.18484884627321932, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007319772848859429}, {"id": 979, "seek": 593180, "start": 5954.28, "end": 5959.88, "text": " critique. It's like, wait, wait, this phenomena has the power it has, because Chick-Mat St.", "tokens": [51488, 25673, 13, 467, 311, 411, 11, 1699, 11, 1699, 11, 341, 22004, 575, 264, 1347, 309, 575, 11, 570, 38930, 12, 44, 267, 745, 13, 51768], "temperature": 0.0, "avg_logprob": -0.18484884627321932, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007319772848859429}, {"id": 980, "seek": 595988, "start": 5960.2, "end": 5965.96, "text": " said it's an evolutionary market for adaptivity. And then, well, making it adaptive is how broadly", "tokens": [50380, 848, 309, 311, 364, 27567, 2142, 337, 6231, 4253, 13, 400, 550, 11, 731, 11, 1455, 309, 27912, 307, 577, 19511, 50668], "temperature": 0.0, "avg_logprob": -0.11127555595253999, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.002550279488787055}, {"id": 981, "seek": 595988, "start": 5965.96, "end": 5970.6, "text": " and deeply is it transferring out of the situation. And then the philosophical framework really", "tokens": [50668, 293, 8760, 307, 309, 31437, 484, 295, 264, 2590, 13, 400, 550, 264, 25066, 8388, 534, 50900], "temperature": 0.0, "avg_logprob": -0.11127555595253999, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.002550279488787055}, {"id": 982, "seek": 595988, "start": 5970.6, "end": 5976.84, "text": " matters to what it is and how it functions. And so if you can make a distinction and you need", "tokens": [50900, 7001, 281, 437, 309, 307, 293, 577, 309, 6828, 13, 400, 370, 498, 291, 393, 652, 257, 16844, 293, 291, 643, 51212], "temperature": 0.0, "avg_logprob": -0.11127555595253999, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.002550279488787055}, {"id": 983, "seek": 595988, "start": 5976.84, "end": 5980.2, "text": " an ontological distinction to make this distinction, but if you can make a distinction", "tokens": [51212, 364, 6592, 4383, 16844, 281, 652, 341, 16844, 11, 457, 498, 291, 393, 652, 257, 16844, 51380], "temperature": 0.0, "avg_logprob": -0.11127555595253999, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.002550279488787055}, {"id": 984, "seek": 595988, "start": 5980.2, "end": 5986.68, "text": " between explaining and explaining a way, then I think it's possible to say, no, no,", "tokens": [51380, 1296, 13468, 293, 13468, 257, 636, 11, 550, 286, 519, 309, 311, 1944, 281, 584, 11, 572, 11, 572, 11, 51704], "temperature": 0.0, "avg_logprob": -0.11127555595253999, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.002550279488787055}, {"id": 985, "seek": 598668, "start": 5986.68, "end": 5992.6, "text": " when I do these things. And to be fair to me, there's, I got a lot of people who, you know,", "tokens": [50364, 562, 286, 360, 613, 721, 13, 400, 281, 312, 3143, 281, 385, 11, 456, 311, 11, 286, 658, 257, 688, 295, 561, 567, 11, 291, 458, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11740030893465368, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.008310615085065365}, {"id": 986, "seek": 598668, "start": 5992.6, "end": 5996.76, "text": " from various religious and spiritual backgrounds, and they come in and they say, thank you so much", "tokens": [50660, 490, 3683, 7185, 293, 6960, 17336, 11, 293, 436, 808, 294, 293, 436, 584, 11, 1309, 291, 370, 709, 50868], "temperature": 0.0, "avg_logprob": -0.11740030893465368, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.008310615085065365}, {"id": 987, "seek": 598668, "start": 5996.76, "end": 6003.4800000000005, "text": " for your work. I now, I much better appreciate this, this experience or that experience or the", "tokens": [50868, 337, 428, 589, 13, 286, 586, 11, 286, 709, 1101, 4449, 341, 11, 341, 1752, 420, 300, 1752, 420, 264, 51204], "temperature": 0.0, "avg_logprob": -0.11740030893465368, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.008310615085065365}, {"id": 988, "seek": 598668, "start": 6003.4800000000005, "end": 6009.4800000000005, "text": " flow state or this mystical state, because you didn't try and tell me it's nothing. But you", "tokens": [51204, 3095, 1785, 420, 341, 40565, 1785, 11, 570, 291, 994, 380, 853, 293, 980, 385, 309, 311, 1825, 13, 583, 291, 51504], "temperature": 0.0, "avg_logprob": -0.11740030893465368, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.008310615085065365}, {"id": 989, "seek": 598668, "start": 6009.4800000000005, "end": 6014.280000000001, "text": " tried to say, this is what it's doing. And this is why you like it and value it so much.", "tokens": [51504, 3031, 281, 584, 11, 341, 307, 437, 309, 311, 884, 13, 400, 341, 307, 983, 291, 411, 309, 293, 2158, 309, 370, 709, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11740030893465368, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.008310615085065365}, {"id": 990, "seek": 601428, "start": 6014.28, "end": 6020.44, "text": " But do you think your work, to kind of, yeah, to look at that personal vantage point, do you", "tokens": [50364, 583, 360, 291, 519, 428, 589, 11, 281, 733, 295, 11, 1338, 11, 281, 574, 412, 300, 2973, 46206, 935, 11, 360, 291, 50672], "temperature": 0.0, "avg_logprob": -0.13025849274914675, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0014682416804134846}, {"id": 991, "seek": 601428, "start": 6020.44, "end": 6028.5199999999995, "text": " think you've managed to keep, like, because if I, without being sort of embarrassingly lording,", "tokens": [50672, 519, 291, 600, 6453, 281, 1066, 11, 411, 11, 570, 498, 286, 11, 1553, 885, 1333, 295, 9187, 12163, 287, 3357, 11, 51076], "temperature": 0.0, "avg_logprob": -0.13025849274914675, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0014682416804134846}, {"id": 992, "seek": 601428, "start": 6029.5599999999995, "end": 6035.48, "text": " people should watch Awakening from the Meaning Crisis, not only for the content, but also just", "tokens": [51128, 561, 820, 1159, 25274, 4559, 490, 264, 19948, 42846, 11, 406, 787, 337, 264, 2701, 11, 457, 611, 445, 51424], "temperature": 0.0, "avg_logprob": -0.13025849274914675, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0014682416804134846}, {"id": 993, "seek": 601428, "start": 6035.48, "end": 6041.48, "text": " the way you present it, which is just so magical. So thanks. I mean, it's really inspired a lot of", "tokens": [51424, 264, 636, 291, 1974, 309, 11, 597, 307, 445, 370, 12066, 13, 407, 3231, 13, 286, 914, 11, 309, 311, 534, 7547, 257, 688, 295, 51724], "temperature": 0.0, "avg_logprob": -0.13025849274914675, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0014682416804134846}, {"id": 994, "seek": 604148, "start": 6041.48, "end": 6047.639999999999, "text": " the way that I think science should be done and philosophy should be done. What, what is it about,", "tokens": [50364, 264, 636, 300, 286, 519, 3497, 820, 312, 1096, 293, 10675, 820, 312, 1096, 13, 708, 11, 437, 307, 309, 466, 11, 50672], "temperature": 0.0, "avg_logprob": -0.10547761157550643, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.023979173973202705}, {"id": 995, "seek": 604148, "start": 6047.639999999999, "end": 6052.28, "text": " I think, you know, anyone who watches that recognizes there's something kind of special", "tokens": [50672, 286, 519, 11, 291, 458, 11, 2878, 567, 17062, 300, 26564, 456, 311, 746, 733, 295, 2121, 50904], "temperature": 0.0, "avg_logprob": -0.10547761157550643, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.023979173973202705}, {"id": 996, "seek": 604148, "start": 6052.28, "end": 6056.919999999999, "text": " going on there. The ideas are living through you, you're, they're breathing through you,", "tokens": [50904, 516, 322, 456, 13, 440, 3487, 366, 2647, 807, 291, 11, 291, 434, 11, 436, 434, 9570, 807, 291, 11, 51136], "temperature": 0.0, "avg_logprob": -0.10547761157550643, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.023979173973202705}, {"id": 997, "seek": 604148, "start": 6056.919999999999, "end": 6060.679999999999, "text": " you're breathing through them, there's this really beautiful coupling. Again, we'll come back to that", "tokens": [51136, 291, 434, 9570, 807, 552, 11, 456, 311, 341, 534, 2238, 37447, 13, 3764, 11, 321, 603, 808, 646, 281, 300, 51324], "temperature": 0.0, "avg_logprob": -0.10547761157550643, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.023979173973202705}, {"id": 998, "seek": 604148, "start": 6060.679999999999, "end": 6069.959999999999, "text": " between you and the ideas. What, what is it about that kind of synthesis of the philosophy and the", "tokens": [51324, 1296, 291, 293, 264, 3487, 13, 708, 11, 437, 307, 309, 466, 300, 733, 295, 30252, 295, 264, 10675, 293, 264, 51788], "temperature": 0.0, "avg_logprob": -0.10547761157550643, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.023979173973202705}, {"id": 999, "seek": 606996, "start": 6070.04, "end": 6076.6, "text": " sort of more hard cognitive science? Do you think made that project so successful?", "tokens": [50368, 1333, 295, 544, 1152, 15605, 3497, 30, 1144, 291, 519, 1027, 300, 1716, 370, 4406, 30, 50696], "temperature": 0.0, "avg_logprob": -0.09468429471239631, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0004572323814500123}, {"id": 1000, "seek": 606996, "start": 6078.92, "end": 6083.96, "text": " I suppose it's a view of the role of cognitive science, what cognitive science is doing.", "tokens": [50812, 286, 7297, 309, 311, 257, 1910, 295, 264, 3090, 295, 15605, 3497, 11, 437, 15605, 3497, 307, 884, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09468429471239631, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0004572323814500123}, {"id": 1001, "seek": 606996, "start": 6087.24, "end": 6090.84, "text": " I think what cognitive science does is it's,", "tokens": [51228, 286, 519, 437, 15605, 3497, 775, 307, 309, 311, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09468429471239631, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0004572323814500123}, {"id": 1002, "seek": 606996, "start": 6093.72, "end": 6099.4800000000005, "text": " well, I'll try and do it sort of narratively. And I do this in the series. I think, you know,", "tokens": [51552, 731, 11, 286, 603, 853, 293, 360, 309, 1333, 295, 9977, 356, 13, 400, 286, 360, 341, 294, 264, 2638, 13, 286, 519, 11, 291, 458, 11, 51840], "temperature": 0.0, "avg_logprob": -0.09468429471239631, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0004572323814500123}, {"id": 1003, "seek": 609996, "start": 6100.68, "end": 6104.28, "text": " even the notion of mind that we've been invoking mind and cognition is equivocal,", "tokens": [50400, 754, 264, 10710, 295, 1575, 300, 321, 600, 668, 1048, 5953, 1575, 293, 46905, 307, 48726, 36483, 11, 50580], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1004, "seek": 609996, "start": 6105.08, "end": 6112.12, "text": " because it means one thing to the neuroscientists who study the brain and looking at neurons and", "tokens": [50620, 570, 309, 1355, 472, 551, 281, 264, 28813, 5412, 1751, 567, 2979, 264, 3567, 293, 1237, 412, 22027, 293, 50972], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1005, "seek": 609996, "start": 6112.12, "end": 6115.72, "text": " anatomical networks and perhaps functional networks. It means a different thing to the", "tokens": [50972, 21618, 298, 804, 9590, 293, 4317, 11745, 9590, 13, 467, 1355, 257, 819, 551, 281, 264, 51152], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1006, "seek": 609996, "start": 6115.72, "end": 6121.4, "text": " artificial intelligence person who's building algorithms and heuristics and doing reinforcement", "tokens": [51152, 11677, 7599, 954, 567, 311, 2390, 14642, 293, 415, 374, 6006, 293, 884, 29280, 51436], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1007, "seek": 609996, "start": 6121.4, "end": 6125.4800000000005, "text": " learning and blah, blah. It means a different thing to the psychologists who study human behavior", "tokens": [51436, 2539, 293, 12288, 11, 12288, 13, 467, 1355, 257, 819, 551, 281, 264, 41562, 567, 2979, 1952, 5223, 51640], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1008, "seek": 609996, "start": 6125.4800000000005, "end": 6129.72, "text": " with experiments and running stats. And they talk about working memory. They don't talk so much", "tokens": [51640, 365, 12050, 293, 2614, 18152, 13, 400, 436, 751, 466, 1364, 4675, 13, 814, 500, 380, 751, 370, 709, 51852], "temperature": 0.0, "avg_logprob": -0.10353103734679141, "compression_ratio": 1.8561872909698998, "no_speech_prob": 0.0006161683122627437}, {"id": 1009, "seek": 612972, "start": 6130.12, "end": 6133.0, "text": " right. And it means a different thing to the linguists and a different thing to the cultural", "tokens": [50384, 558, 13, 400, 309, 1355, 257, 819, 551, 281, 264, 21766, 1751, 293, 257, 819, 551, 281, 264, 6988, 50528], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1010, "seek": 612972, "start": 6133.0, "end": 6136.52, "text": " anthropologists. By the way, I include them because they were the people that have been", "tokens": [50528, 22727, 12256, 13, 3146, 264, 636, 11, 286, 4090, 552, 570, 436, 645, 264, 561, 300, 362, 668, 50704], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1011, "seek": 612972, "start": 6136.52, "end": 6141.56, "text": " studying distributed cognition and collective intelligence. They matter. Yeah. If you think", "tokens": [50704, 7601, 12631, 46905, 293, 12590, 7599, 13, 814, 1871, 13, 865, 13, 759, 291, 519, 50956], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1012, "seek": 612972, "start": 6141.56, "end": 6145.4800000000005, "text": " about it on this analogy, it's like they're different countries speaking different languages.", "tokens": [50956, 466, 309, 322, 341, 21663, 11, 309, 311, 411, 436, 434, 819, 3517, 4124, 819, 8650, 13, 51152], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1013, "seek": 612972, "start": 6145.4800000000005, "end": 6151.88, "text": " And here's the thing that has great value. Specialization has great value. I'm not dismissing", "tokens": [51152, 400, 510, 311, 264, 551, 300, 575, 869, 2158, 13, 11863, 2144, 575, 869, 2158, 13, 286, 478, 406, 16974, 278, 51472], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1014, "seek": 612972, "start": 6151.88, "end": 6157.88, "text": " that. I am not dismissing that. But each one of these is talking about a different level. And", "tokens": [51472, 300, 13, 286, 669, 406, 16974, 278, 300, 13, 583, 1184, 472, 295, 613, 307, 1417, 466, 257, 819, 1496, 13, 400, 51772], "temperature": 0.0, "avg_logprob": -0.10681438446044922, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.00028677197406068444}, {"id": 1015, "seek": 615788, "start": 6157.88, "end": 6163.64, "text": " here's what I think I would state as a very plausible claim. I think it's unlikely that these", "tokens": [50364, 510, 311, 437, 286, 519, 286, 576, 1785, 382, 257, 588, 39925, 3932, 13, 286, 519, 309, 311, 17518, 300, 613, 50652], "temperature": 0.0, "avg_logprob": -0.10165550158574031, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.0005525241722352803}, {"id": 1016, "seek": 615788, "start": 6163.64, "end": 6169.8, "text": " levels in reality, the brain information processing, behavioral, linguistic, sociocultural levels", "tokens": [50652, 4358, 294, 4103, 11, 264, 3567, 1589, 9007, 11, 19124, 11, 43002, 11, 3075, 905, 11056, 4358, 50960], "temperature": 0.0, "avg_logprob": -0.10165550158574031, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.0005525241722352803}, {"id": 1017, "seek": 615788, "start": 6169.8, "end": 6174.68, "text": " are independent from each other. I think they causal influence and constrain each other.", "tokens": [50960, 366, 6695, 490, 1184, 661, 13, 286, 519, 436, 38755, 6503, 293, 1817, 7146, 1184, 661, 13, 51204], "temperature": 0.0, "avg_logprob": -0.10165550158574031, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.0005525241722352803}, {"id": 1018, "seek": 615788, "start": 6174.68, "end": 6181.8, "text": " So we are missing something important about the mind by not getting clear about the relationship", "tokens": [51204, 407, 321, 366, 5361, 746, 1021, 466, 264, 1575, 538, 406, 1242, 1850, 466, 264, 2480, 51560], "temperature": 0.0, "avg_logprob": -0.10165550158574031, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.0005525241722352803}, {"id": 1019, "seek": 615788, "start": 6181.8, "end": 6187.4800000000005, "text": " between these levels. We can be equivocating and we have a fragmented notion if we don't capture", "tokens": [51560, 1296, 613, 4358, 13, 492, 393, 312, 48726, 905, 990, 293, 321, 362, 257, 9241, 14684, 10710, 498, 321, 500, 380, 7983, 51844], "temperature": 0.0, "avg_logprob": -0.10165550158574031, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.0005525241722352803}, {"id": 1020, "seek": 618788, "start": 6187.88, "end": 6193.4800000000005, "text": " the relations of constraint and causation between these different levels of cognition and mind.", "tokens": [50364, 264, 2299, 295, 25534, 293, 3302, 399, 1296, 613, 819, 4358, 295, 46905, 293, 1575, 13, 50644], "temperature": 0.0, "avg_logprob": -0.13240926106770834, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.0002131064684363082}, {"id": 1021, "seek": 618788, "start": 6194.04, "end": 6201.16, "text": " And so I think the proper function of cognitive science is to use philosophy's skills of", "tokens": [50672, 400, 370, 286, 519, 264, 2296, 2445, 295, 15605, 3497, 307, 281, 764, 10675, 311, 3942, 295, 51028], "temperature": 0.0, "avg_logprob": -0.13240926106770834, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.0002131064684363082}, {"id": 1022, "seek": 618788, "start": 6202.36, "end": 6209.24, "text": " creating bridging discourses, creating bridging conceptual vocabulary, theoretical grammar,", "tokens": [51088, 4084, 16362, 3249, 43609, 279, 11, 4084, 16362, 3249, 24106, 19864, 11, 20864, 22317, 11, 51432], "temperature": 0.0, "avg_logprob": -0.13240926106770834, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.0002131064684363082}, {"id": 1023, "seek": 618788, "start": 6209.24, "end": 6215.4800000000005, "text": " so that these different disciplines can talk to each other in reciprocally reconstructive ways", "tokens": [51432, 370, 300, 613, 819, 21919, 393, 751, 281, 1184, 661, 294, 28961, 66, 379, 16891, 21673, 2098, 51744], "temperature": 0.0, "avg_logprob": -0.13240926106770834, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.0002131064684363082}, {"id": 1024, "seek": 621548, "start": 6215.5599999999995, "end": 6221.639999999999, "text": " as a way of converging on the causal and constraint relationships between the levels,", "tokens": [50368, 382, 257, 636, 295, 9652, 3249, 322, 264, 38755, 293, 25534, 6159, 1296, 264, 4358, 11, 50672], "temperature": 0.0, "avg_logprob": -0.10966038977962801, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002433372661471367}, {"id": 1025, "seek": 621548, "start": 6221.639999999999, "end": 6228.2, "text": " rather than trying to compete or say the bottom level is the only real level. So that for me,", "tokens": [50672, 2831, 813, 1382, 281, 11831, 420, 584, 264, 2767, 1496, 307, 264, 787, 957, 1496, 13, 407, 300, 337, 385, 11, 51000], "temperature": 0.0, "avg_logprob": -0.10966038977962801, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002433372661471367}, {"id": 1026, "seek": 621548, "start": 6228.2, "end": 6233.32, "text": " they all live together. They are doing, and I'm sorry to evoke it again, they're doing", "tokens": [51000, 436, 439, 1621, 1214, 13, 814, 366, 884, 11, 293, 286, 478, 2597, 281, 1073, 2949, 309, 797, 11, 436, 434, 884, 51256], "temperature": 0.0, "avg_logprob": -0.10966038977962801, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002433372661471367}, {"id": 1027, "seek": 621548, "start": 6233.32, "end": 6242.679999999999, "text": " opponent processing between each other. And that's what inhabits me when I'm doing cognitive", "tokens": [51256, 10620, 9007, 1296, 1184, 661, 13, 400, 300, 311, 437, 16934, 1208, 385, 562, 286, 478, 884, 15605, 51724], "temperature": 0.0, "avg_logprob": -0.10966038977962801, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002433372661471367}, {"id": 1028, "seek": 624268, "start": 6242.68, "end": 6247.320000000001, "text": " science, that vision. I call it synoptic integration. Yeah, it's, well, it's wonderful to see.", "tokens": [50364, 3497, 11, 300, 5201, 13, 286, 818, 309, 5451, 5747, 299, 10980, 13, 865, 11, 309, 311, 11, 731, 11, 309, 311, 3715, 281, 536, 13, 50596], "temperature": 0.0, "avg_logprob": -0.18741915207500606, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.05990353599190712}, {"id": 1029, "seek": 624268, "start": 6249.08, "end": 6255.240000000001, "text": " So, I mean, like, I think we, I mean, that's the cognitive scientists that I'm aspiring to be.", "tokens": [50684, 407, 11, 286, 914, 11, 411, 11, 286, 519, 321, 11, 286, 914, 11, 300, 311, 264, 15605, 7708, 300, 286, 478, 45405, 281, 312, 13, 50992], "temperature": 0.0, "avg_logprob": -0.18741915207500606, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.05990353599190712}, {"id": 1030, "seek": 624268, "start": 6255.240000000001, "end": 6259.320000000001, "text": " I think you've called it a big picture cognitive scientist as well. Yes. And one of the things,", "tokens": [50992, 286, 519, 291, 600, 1219, 309, 257, 955, 3036, 15605, 12662, 382, 731, 13, 1079, 13, 400, 472, 295, 264, 721, 11, 51196], "temperature": 0.0, "avg_logprob": -0.18741915207500606, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.05990353599190712}, {"id": 1031, "seek": 624268, "start": 6259.320000000001, "end": 6263.88, "text": " one of the, and I felt very lonely for a while, you know, I was doing relevance realization,", "tokens": [51196, 472, 295, 264, 11, 293, 286, 2762, 588, 14236, 337, 257, 1339, 11, 291, 458, 11, 286, 390, 884, 32684, 25138, 11, 51424], "temperature": 0.0, "avg_logprob": -0.18741915207500606, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.05990353599190712}, {"id": 1032, "seek": 624268, "start": 6263.88, "end": 6270.04, "text": " which is a big picture. And then, to my great delight, another big picture for ecocide down", "tokens": [51424, 597, 307, 257, 955, 3036, 13, 400, 550, 11, 281, 452, 869, 11627, 11, 1071, 955, 3036, 337, 11437, 27791, 760, 51732], "temperature": 0.0, "avg_logprob": -0.18741915207500606, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.05990353599190712}, {"id": 1033, "seek": 627004, "start": 6270.04, "end": 6275.72, "text": " the road. And then to my even greater happiness, another big picture came down the road,", "tokens": [50364, 264, 3060, 13, 400, 550, 281, 452, 754, 5044, 8324, 11, 1071, 955, 3036, 1361, 760, 264, 3060, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1527326185624678, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.0013431576080620289}, {"id": 1034, "seek": 627004, "start": 6275.72, "end": 6281.56, "text": " which is, you know, the predictive processing framework. And I think they are more convergent", "tokens": [50648, 597, 307, 11, 291, 458, 11, 264, 35521, 9007, 8388, 13, 400, 286, 519, 436, 366, 544, 9652, 6930, 50940], "temperature": 0.0, "avg_logprob": -0.1527326185624678, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.0013431576080620289}, {"id": 1035, "seek": 627004, "start": 6281.56, "end": 6287.8, "text": " than adversarial. And I, well, you've heard me arguing about how we can integrate them together.", "tokens": [50940, 813, 17641, 44745, 13, 400, 286, 11, 731, 11, 291, 600, 2198, 385, 19697, 466, 577, 321, 393, 13365, 552, 1214, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1527326185624678, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.0013431576080620289}, {"id": 1036, "seek": 627004, "start": 6287.8, "end": 6293.8, "text": " Excellent. Yeah, I would say maybe it's just bias. Active inferences or physics processing is a", "tokens": [51252, 16723, 13, 865, 11, 286, 576, 584, 1310, 309, 311, 445, 12577, 13, 26635, 13596, 2667, 420, 10649, 9007, 307, 257, 51552], "temperature": 0.0, "avg_logprob": -0.1527326185624678, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.0013431576080620289}, {"id": 1037, "seek": 629380, "start": 6293.8, "end": 6300.2, "text": " big picture cognitive scientist from the physics and the maths, all the way, well, what we're", "tokens": [50364, 955, 3036, 15605, 12662, 490, 264, 10649, 293, 264, 36287, 11, 439, 264, 636, 11, 731, 11, 437, 321, 434, 50684], "temperature": 0.0, "avg_logprob": -0.1368073348341317, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.03823082149028778}, {"id": 1038, "seek": 629380, "start": 6300.2, "end": 6306.12, "text": " trying to do to the phenomena, to what it is like to be a, to be a human being, to have conversations", "tokens": [50684, 1382, 281, 360, 281, 264, 22004, 11, 281, 437, 309, 307, 411, 281, 312, 257, 11, 281, 312, 257, 1952, 885, 11, 281, 362, 7315, 50980], "temperature": 0.0, "avg_logprob": -0.1368073348341317, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.03823082149028778}, {"id": 1039, "seek": 629380, "start": 6306.12, "end": 6311.88, "text": " like this. John, it's, it was, you know, I can't say it was better than I expected,", "tokens": [50980, 411, 341, 13, 2619, 11, 309, 311, 11, 309, 390, 11, 291, 458, 11, 286, 393, 380, 584, 309, 390, 1101, 813, 286, 5176, 11, 51268], "temperature": 0.0, "avg_logprob": -0.1368073348341317, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.03823082149028778}, {"id": 1040, "seek": 629380, "start": 6311.88, "end": 6317.56, "text": " because I knew it was going to be wonderful, but you're full of just thrilling insights and", "tokens": [51268, 570, 286, 2586, 309, 390, 516, 281, 312, 3715, 11, 457, 291, 434, 1577, 295, 445, 39347, 14310, 293, 51552], "temperature": 0.0, "avg_logprob": -0.1368073348341317, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.03823082149028778}, {"id": 1041, "seek": 629380, "start": 6318.2, "end": 6321.64, "text": " speaking to you, listening to you is always such a wonderful learning experience.", "tokens": [51584, 4124, 281, 291, 11, 4764, 281, 291, 307, 1009, 1270, 257, 3715, 2539, 1752, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1368073348341317, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.03823082149028778}, {"id": 1042, "seek": 632164, "start": 6321.64, "end": 6329.400000000001, "text": " Where that happens in flow or without flow. But I just wanted to thank you so very much for,", "tokens": [50364, 2305, 300, 2314, 294, 3095, 420, 1553, 3095, 13, 583, 286, 445, 1415, 281, 1309, 291, 370, 588, 709, 337, 11, 50752], "temperature": 0.0, "avg_logprob": -0.07639179229736329, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003695117775350809}, {"id": 1043, "seek": 632164, "start": 6329.400000000001, "end": 6335.56, "text": " I know you're extremely busy, but for giving us your time. Where can people find you? What have", "tokens": [50752, 286, 458, 291, 434, 4664, 5856, 11, 457, 337, 2902, 505, 428, 565, 13, 2305, 393, 561, 915, 291, 30, 708, 362, 51060], "temperature": 0.0, "avg_logprob": -0.07639179229736329, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003695117775350809}, {"id": 1044, "seek": 632164, "start": 6335.56, "end": 6341.88, "text": " you got coming up? I'm sure people are curious. So the most immediate thing for this conversation", "tokens": [51060, 291, 658, 1348, 493, 30, 286, 478, 988, 561, 366, 6369, 13, 407, 264, 881, 11629, 551, 337, 341, 3761, 51376], "temperature": 0.0, "avg_logprob": -0.07639179229736329, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003695117775350809}, {"id": 1045, "seek": 632164, "start": 6342.76, "end": 6348.200000000001, "text": " is the talk I gave at Leiden on the predictive processing symposium. It's up on my channel.", "tokens": [51420, 307, 264, 751, 286, 2729, 412, 1456, 4380, 322, 264, 35521, 9007, 13240, 42161, 13, 467, 311, 493, 322, 452, 2269, 13, 51692], "temperature": 0.0, "avg_logprob": -0.07639179229736329, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003695117775350809}, {"id": 1046, "seek": 634820, "start": 6348.28, "end": 6351.72, "text": " I think it's the second most recent video on my channel on YouTube.", "tokens": [50368, 286, 519, 309, 311, 264, 1150, 881, 5162, 960, 322, 452, 2269, 322, 3088, 13, 50540], "temperature": 0.0, "avg_logprob": -0.15555818798472582, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.010803801007568836}, {"id": 1047, "seek": 634820, "start": 6351.72, "end": 6356.84, "text": " We can put it in the video. Yeah. Where I try to go into the nuts and bolts of how you could", "tokens": [50540, 492, 393, 829, 309, 294, 264, 960, 13, 865, 13, 2305, 286, 853, 281, 352, 666, 264, 10483, 293, 18127, 295, 577, 291, 727, 50796], "temperature": 0.0, "avg_logprob": -0.15555818798472582, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.010803801007568836}, {"id": 1048, "seek": 634820, "start": 6356.84, "end": 6362.28, "text": " integrate predictive processing and relevance realisation theory together. So people might find", "tokens": [50796, 13365, 35521, 9007, 293, 32684, 957, 7623, 5261, 1214, 13, 407, 561, 1062, 915, 51068], "temperature": 0.0, "avg_logprob": -0.15555818798472582, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.010803801007568836}, {"id": 1049, "seek": 634820, "start": 6362.28, "end": 6368.04, "text": " that. And I've got a lot of good feedback on that for being sort of clear and a good argument.", "tokens": [51068, 300, 13, 400, 286, 600, 658, 257, 688, 295, 665, 5824, 322, 300, 337, 885, 1333, 295, 1850, 293, 257, 665, 6770, 13, 51356], "temperature": 0.0, "avg_logprob": -0.15555818798472582, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.010803801007568836}, {"id": 1050, "seek": 634820, "start": 6368.92, "end": 6376.04, "text": " So I would recommend that when people are interested in the broader implications of this", "tokens": [51400, 407, 286, 576, 2748, 300, 562, 561, 366, 3102, 294, 264, 13227, 16602, 295, 341, 51756], "temperature": 0.0, "avg_logprob": -0.15555818798472582, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.010803801007568836}, {"id": 1051, "seek": 637604, "start": 6376.12, "end": 6383.24, "text": " awakening for the meeting crisis and then after Socrates also after Socrates is where I take all", "tokens": [50368, 31550, 337, 264, 3440, 5869, 293, 550, 934, 407, 50243, 611, 934, 407, 50243, 307, 689, 286, 747, 439, 50724], "temperature": 0.0, "avg_logprob": -0.20516048388534716, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.003822309197857976}, {"id": 1052, "seek": 637604, "start": 6383.24, "end": 6387.72, "text": " of this and all of this stuff and how do you turn it into practices in order to overcome", "tokens": [50724, 295, 341, 293, 439, 295, 341, 1507, 293, 577, 360, 291, 1261, 309, 666, 7525, 294, 1668, 281, 10473, 50948], "temperature": 0.0, "avg_logprob": -0.20516048388534716, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.003822309197857976}, {"id": 1053, "seek": 637604, "start": 6387.72, "end": 6393.64, "text": " self deception and enhance relevance realisation become more wise and virtuous. And so they can", "tokens": [50948, 2698, 40451, 293, 11985, 32684, 957, 7623, 1813, 544, 10829, 293, 48918, 13, 400, 370, 436, 393, 51244], "temperature": 0.0, "avg_logprob": -0.20516048388534716, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.003822309197857976}, {"id": 1054, "seek": 637604, "start": 6393.64, "end": 6401.56, "text": " take a look there. The arguments around neoplatonism. There are several videos around that. I try to", "tokens": [51244, 747, 257, 574, 456, 13, 440, 12869, 926, 408, 404, 14087, 266, 1434, 13, 821, 366, 2940, 2145, 926, 300, 13, 286, 853, 281, 51640], "temperature": 0.0, "avg_logprob": -0.20516048388534716, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.003822309197857976}, {"id": 1055, "seek": 640156, "start": 6401.64, "end": 6407.320000000001, "text": " connect neoplatonism to foreecogsci and relevance realisation and predictive processing as we", "tokens": [50368, 1745, 408, 404, 14087, 266, 1434, 281, 2091, 3045, 664, 82, 537, 293, 32684, 957, 7623, 293, 35521, 9007, 382, 321, 50652], "temperature": 0.0, "avg_logprob": -0.1676743217136549, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.032072246074676514}, {"id": 1056, "seek": 640156, "start": 6407.320000000001, "end": 6413.240000000001, "text": " saw in this podcast. But in the neoplatonism I'm working on my third big series. Walking the", "tokens": [50652, 1866, 294, 341, 7367, 13, 583, 294, 264, 408, 404, 14087, 266, 1434, 286, 478, 1364, 322, 452, 2636, 955, 2638, 13, 26964, 264, 50948], "temperature": 0.0, "avg_logprob": -0.1676743217136549, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.032072246074676514}, {"id": 1057, "seek": 640156, "start": 6413.240000000001, "end": 6418.68, "text": " philosophical Silk Road which will be on Zen neoplatonism. Trying to see if we can bring", "tokens": [50948, 25066, 43853, 11507, 597, 486, 312, 322, 22387, 408, 404, 14087, 266, 1434, 13, 20180, 281, 536, 498, 321, 393, 1565, 51220], "temperature": 0.0, "avg_logprob": -0.1676743217136549, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.032072246074676514}, {"id": 1058, "seek": 640156, "start": 6418.68, "end": 6423.400000000001, "text": " an integration, an opponent processing, not an adversarial one, an opponent processing between", "tokens": [51220, 364, 10980, 11, 364, 10620, 9007, 11, 406, 364, 17641, 44745, 472, 11, 364, 10620, 9007, 1296, 51456], "temperature": 0.0, "avg_logprob": -0.1676743217136549, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.032072246074676514}, {"id": 1059, "seek": 640156, "start": 6423.96, "end": 6430.52, "text": " Zen and neoplatonism to give us a rich philosophical framework by which like the philosophical road", "tokens": [51484, 22387, 293, 408, 404, 14087, 266, 1434, 281, 976, 505, 257, 4593, 25066, 8388, 538, 597, 411, 264, 25066, 3060, 51812], "temperature": 0.0, "avg_logprob": -0.1676743217136549, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.032072246074676514}, {"id": 1060, "seek": 643052, "start": 6430.52, "end": 6439.080000000001, "text": " we can trade ideas and move between worlds without having to descend into well tribalism", "tokens": [50364, 321, 393, 4923, 3487, 293, 1286, 1296, 13401, 1553, 1419, 281, 16333, 666, 731, 20958, 1434, 50792], "temperature": 0.0, "avg_logprob": -0.14821172564217214, "compression_ratio": 1.4891774891774892, "no_speech_prob": 0.004593358375132084}, {"id": 1061, "seek": 643052, "start": 6439.080000000001, "end": 6444.92, "text": " and other such things. Wonderful, wonderful. Well, again, it was absolutely my pleasure.", "tokens": [50792, 293, 661, 1270, 721, 13, 22768, 11, 3715, 13, 1042, 11, 797, 11, 309, 390, 3122, 452, 6834, 13, 51084], "temperature": 0.0, "avg_logprob": -0.14821172564217214, "compression_ratio": 1.4891774891774892, "no_speech_prob": 0.004593358375132084}, {"id": 1062, "seek": 643052, "start": 6444.92, "end": 6448.6, "text": " I apologise. I've got a slight cold. So if I've been a little sniffly or nasal,", "tokens": [51084, 286, 50128, 13, 286, 600, 658, 257, 4036, 3554, 13, 407, 498, 286, 600, 668, 257, 707, 31101, 356, 420, 41575, 11, 51268], "temperature": 0.0, "avg_logprob": -0.14821172564217214, "compression_ratio": 1.4891774891774892, "no_speech_prob": 0.004593358375132084}, {"id": 1063, "seek": 643052, "start": 6451.400000000001, "end": 6456.360000000001, "text": " we can blame it on the London weather. But I'd love to have you back on. At some point", "tokens": [51408, 321, 393, 10127, 309, 322, 264, 7042, 5503, 13, 583, 286, 1116, 959, 281, 362, 291, 646, 322, 13, 1711, 512, 935, 51656], "temperature": 0.0, "avg_logprob": -0.14821172564217214, "compression_ratio": 1.4891774891774892, "no_speech_prob": 0.004593358375132084}, {"id": 1064, "seek": 645636, "start": 6457.24, "end": 6462.44, "text": " your work is truly inspiring. So thank you so much, John, for me and the Institute.", "tokens": [50408, 428, 589, 307, 4908, 15883, 13, 407, 1309, 291, 370, 709, 11, 2619, 11, 337, 385, 293, 264, 9446, 13, 50668], "temperature": 0.0, "avg_logprob": -0.2014091730117798, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.0539059117436409}, {"id": 1065, "seek": 645636, "start": 6463.4, "end": 6470.5199999999995, "text": " Thank you, Doris. I'm happy to come back on. If it turns out that Mark and I come on together,", "tokens": [50716, 1044, 291, 11, 13643, 271, 13, 286, 478, 2055, 281, 808, 646, 322, 13, 759, 309, 4523, 484, 300, 3934, 293, 286, 808, 322, 1214, 11, 51072], "temperature": 0.0, "avg_logprob": -0.2014091730117798, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.0539059117436409}, {"id": 1066, "seek": 645636, "start": 6470.5199999999995, "end": 6474.12, "text": " that would be thrilling as well. Great. We will definitely get that sorted. All right, thank you.", "tokens": [51072, 300, 576, 312, 39347, 382, 731, 13, 3769, 13, 492, 486, 2138, 483, 300, 25462, 13, 1057, 558, 11, 1309, 291, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2014091730117798, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.0539059117436409}, {"id": 1067, "seek": 645636, "start": 6475.32, "end": 6479.32, "text": " Thank you.", "tokens": [51312, 1044, 291, 13, 51512], "temperature": 0.0, "avg_logprob": -0.2014091730117798, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.0539059117436409}], "language": "en"}