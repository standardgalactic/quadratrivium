start	end	text
0	9540	Hello and welcome, everyone, to the Active Inference Institute.
9540	16640	This is Active Gueststream number 41.1 on April 25, 2023.
16640	20460	We're here with Elliot Murphy and Steven Piantadosi.
20460	22280	This is going to be quite a discussion.
22280	25640	We will begin with opening statements from Steven and Elliot.
25640	29880	Elliot will then lead with some questions and we'll have an open discussion at the
29880	30880	end.
30880	36080	So, Steven, please, thank you for joining and to your opening statement.
36080	37080	Cool.
37080	39360	Hi, so I'm Steve Piantadosi.
39360	45320	I'm a professor in psychology and neuroscience at UC Berkeley.
45320	50080	And I guess part of the reason that we're here is that I recently wrote a paper on large
50080	57080	language models in part trying to convey some enthusiasm about what they've kind of accomplished
57080	61000	in terms of learning syntax and semantics.
61000	65600	And in part pointing out, I think that these models really change how we should think about
65600	72240	language, how we should think about theories of linguistic representation and theories
72240	76360	of grammar and likely also theories of learning.
76360	77360	Yeah.
77360	78360	Awesome.
78360	79360	Yeah.
79360	81360	So, I'm Elliot Murphy.
81360	85680	I'm a postdoc in the Department of Neurosurgery at UC Health in Texas.
85680	89600	I read Steven's paper with great interest and there's a lot of people.
89600	93440	There were some areas of convergence, but the things I want to kind of focus on today
93440	98520	in responding to Steven and kind of probing how to do with areas of divergence maybe.
98520	104200	So, you know, Steven's paper is based on the idea that modern machine learning has subverted
104200	108800	and bypassed the entire theoretical framework of Chomsky's approach.
108800	111840	So I wanted to kind of respond to some of these main arguments and some other related
111840	116560	arguments in the literature that some folks listening might have some insight and thoughts
116560	117560	on.
117560	122000	So, it's a very common criticism to say that large language models just predict the next
122000	125800	token, which is obviously a bit of a cliche, right?
125800	127000	It's not quite true.
127000	132560	They don't just predict the next token, they also seem to confabulate, they seem to hallucinate,
132560	138000	they maybe lie, they randomly provide different answers to the same question, and they seem
138000	141120	to stochastically mimic language like structures.
141120	144160	They sometimes correct themselves sometimes when they shouldn't.
144160	147120	If you push them a little, they kind of change their mind sometimes.
147120	150400	In fact, if Fox News is currently looking for a replacement for Tucker Carlson, they
150400	151400	could do less.
151400	156400	They could definitely do worse than using ChachiBT if they're looking for a similar
156400	157400	caliber.
157400	161880	So, these models seem to do all sorts of like wild things, and over the past 10 years, there's
161880	166320	been a sequence of different, you know, systems developed like where to be, where to be, and
166320	169840	each of them is based on a different neural net approach, but ultimately they all seem
169840	174560	to take words and characterize them by lists of hundreds or thousands of numbers.
174560	181800	So the GTP3 network has 175 billion weights, 96 attention heads in its architecture, and
181800	184600	as far as what I know, maybe Stephen can correct me here.
184600	188360	We don't really have a great idea of what these different parts really mean.
188360	190080	It just seems to kind of work that way.
190080	195040	Like attention heads in GTP3 can pay attention to much earlier tokens in the string in order
195040	198840	to help them predict the next token, but the whole architecture from start to finish is
198840	204760	kind of engineering-based motivations, and I always kind of wonder what about all the
204760	209600	models that kind of failed from these LLMs, from the different tech companies.
209600	213360	It's like these companies often seem to, you know, make it seem like they have these models
213360	218840	that really work very well straight out the box, and they all seem to be named after some
218840	220840	kind of famous artists, right?
220840	223120	They have Dali after Salvador Dali.
223120	227240	They have Da Vinci, maybe pretty soon one of these companies will release a large language
227240	231520	model called Jesus or something, I don't know.
231520	234040	But they always say, here's our new foundation model.
234040	235040	It's called Picasso.
235040	236040	It's the first one we tried.
236040	237040	It works just great.
237040	240720	No problems straight out the box, but I always wonder what about all the other black boxes
240720	242960	that have kind of failed every time?
242960	247360	That doesn't seem to be a kind of a very open and clear structure to the kind of scientific
247360	252200	reasoning behind selecting, you know, one model or another, but again, I might be, I'm
252200	255120	open to be corrected about that.
255120	260520	So even basic language models do pretty well on basic word prediction.
260520	264160	So the issue is whether these tools provide any insights into traditional psycholinguistic
264160	266400	notions like grammar and parsing.
266400	270680	So this is really why I kind of prefer the term corpus model rather language model, suggested
270680	273240	by people like Sabra Varys.
273240	278040	So as we pointed out that no one really thinks LLMs tell us anything profound about Python
278040	282040	when they learn Python code just as well as natural language, but Python is a symbolic
282040	286840	language with a phrase structure grammar and nobody says LLMs are unveiling the secrets
286840	287840	of Python.
287840	288840	Right.
288840	292080	So just to quote Varys here, he says, if A and N models can be construed as explanatory
292080	296240	theories for natural language based on their successes on language tasks, then in the absence
296240	299520	of counter arguments, they should be good explanatory theories for computer language
299520	300520	as well.
300520	305400	Therefore, successful A and N models of natural language cannot be used as evidence against
305400	308120	generative phrase structure grammars in language.
308120	312160	So corpus model is really a more appropriate term for other reasons too.
312160	315880	People like Emily Bender and some others have shown that features of the training corpus,
315880	320200	in fact, I think Steven cites this, you cite this in your paper actually as a limitation.
320200	324160	They show that features of the training corpus can heavily influence the learning process.
324160	327400	So it's been shown that the performance of large language models on language tasks is
327400	331800	really heavily influenced by the diversity of the training corpus.
331800	334240	But natural language itself is not biased, right?
334240	336800	It's just the computational system.
336800	340160	Some beings can be biased in what they say and how they act.
340160	342520	But natural language itself isn't biased, right?
342520	347880	So large language models, therefore, it seems difficult for me to agree that they are being
347880	349400	subject to all sorts of biases.
349400	352920	They therefore can't really be models of language, they're models of something else.
352920	358840	So just to kind of wrap up this argument, even though LLMs are clearly exposed to vastly
358840	362560	more linguistic experience in children, again, this is something else that Steven concedes
362560	364500	and talks about in his paper.
364500	369020	And so their learning outcomes may still be relevant in addressing what grammatical generalizations
369020	370660	are learnable in principle.
370660	374220	So I do agree with this statement here, that in principle they can tell us something about
374220	378900	learnability rather than things like broad acquisitionist frameworks.
378900	382500	But that's about as much I think you can maybe say right now.
382500	387140	Showing that some inductive biases are not necessary for learning is not really the same
387140	389840	thing as showing that it isn't present in children.
389840	393620	So there's been a long debate about whether negative evidence and instruction and correction
393740	399300	and feedback during language learning are necessary or even useful for infants and children.
399300	403340	But right now I kind of agree more with Eugene Choi and Gary Marcus and others who have highlighted
403340	406820	how LLMs are currently very expensive to train.
406820	411740	They're clearly an example of concentrated private power in the hands of a few tech companies.
411740	417060	Their environment impact is massive and many of people have been less constrained and conservative
417060	421940	in their assessment here, which is much less so than Gary Marcus and Eugene.
421980	428820	So Bill Gates recently wrote that chatGPT is the biggest tech development since the
428820	430820	graphical user interface, the GUI.
430820	436100	And Henry Kissinger wrote in February in the Wall Street Journal that as chatGPTs capacities
436100	441620	become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality
441620	444180	and reorganize politics and society.
444180	449180	Generative AI is poised to generate new forms of human consciousness, so very radical claims
449180	450180	happening at the moment.
450180	456100	I do wonder if sometimes all of the AI hype may have, you know, see it into certain portions
456100	459940	of academia potentially, a lot of ground claims being made.
459940	463220	But I think, you know, more concretely, just to put it back to Stephen here, I wanted to
463220	468820	maybe raise the issue of there's a critique by Roscoe and Beaumont that I think he's read
468820	471500	on Lingbos.
471500	476140	I think you saw on Twitter that you don't like the response they gave because the objection
476140	479620	that they made is that, you know, science is an example of deductible logic.
479620	483340	Your objection is that science isn't deductive, it's inductive, right?
483340	488380	But I think their general point might be more accurate, namely that you can't use the fact
488380	493620	that language models do well predicting some linguistic behavior in humans and some neuroimaging
493620	494860	responses.
494860	499580	You can't use that alone to claim that they can yield a theory of human language.
499580	503340	So in your paper, Stephen, you know that it seems that certain structures work better
503340	504340	than others.
504340	508460	The right attentional mechanism is important, prediction is important, semantic representations
508460	509460	are important.
509500	512620	And therefore, we can glean currently based on these models, right?
512620	515900	But so far, that's really all I've been able to glean in the literature.
515900	517940	I'm not sure if you have more insights here.
517940	523700	So Roscoe and Beaumont use the example of poor prediction, but strong explanation, right?
523700	527140	Explanatory power and not predictive accuracy forms the basis of modern science.
527140	530780	I don't want to explore this a little bit later, maybe, but modern language models can
530780	534540	accurately model parts of human language, but they can also perform very well on impossible
534540	539420	languages and unnatural structures that humans can't learn and have great difficulty
539500	540140	processing.
540140	542220	And I know you're familiar with these with these criticisms, right?
543180	544940	But you're definitely not alone here at the same time.
544940	552220	So Ilya Tskeva, the chief scientist at OpenAI, he said in an interview recently, what does
552220	554060	it mean to predict the next token well enough?
554460	559020	It means that you understand the underlying reality that led to the creation of that token,
560060	563420	which is quite divergent from a lot of more conservative claims in the literature here.
564620	568940	And also, you know, I would just say in response to that, that different components of science
569020	572220	can be either inductive or deductive, right?
572220	573340	It's not really an either-or.
573340	574700	You have an existing theory.
574700	576300	You formulate a hypothesis.
576300	580460	You collect data, you analyze it, and that's kind of a deductive process.
580460	583340	But there's also cases where you start with a specific observation.
583340	586460	You find some patterns and you induce general conclusions, right?
586460	593020	And then there's abduction, where you magically invent hypotheses and reduce the hypothesis space.
593020	598220	You wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific
598220	600540	or abductive reasoning is unscientific, right?
600540	602860	These are all just different ways of doing stuff.
602860	609260	I mean, in your paper, you give the examples of using models to predict hurricanes and pandemics
609260	612860	as being examples of stuff that is as rigorous as science gets.
612860	617180	And then you employ a reader to conclude that the situation is no different for language models.
618220	622380	But I guess for me, the issue is that models predicting hurricanes are not in the business
622380	625980	of answering the question, what is the hurricane, right?
626060	629420	Models accurately predicting the weather are very accurate, but they're not.
629420	633420	They're aligned with the meteorology department, but they're not a substitute for it.
634380	636140	So I guess I'll just hand it over to you.
637820	640220	Yeah. Okay. Well, there's a lot there.
641660	648860	I guess I could start just by saying that I agree with many of these criticisms, right,
648940	657900	about these models being controlled by one or two companies that being very, very problematic.
660300	664780	They have all kinds of biases that they've acquired because they're trained on text from the
664780	672940	internet. That's hugely problematic. I certainly agree that there's things at least at present
672940	680140	that the models don't do well, right? So I think it's easy to find examples of questions
680140	685900	and problems that will trip them up. I think why I've been excited about them, though,
686940	694060	is not necessarily in those terms, right, but in terms of performance on language,
694780	702140	specifically syntax and semantics. I think they're far beyond kind of any other theory
702140	709900	in any other domain, right? So there's no other theory out of linguistics or computer science
710620	717340	which can generate long, coherent grammatical passages of text.
718460	727260	And so kind of admitting all of their problems as tools or things which are deployed by companies,
728220	735100	there's still this question of, like, how are they at dealing with language? And I think this
735100	740140	is where a lot of the enthusiasm comes from, is there really hasn't been anything even remotely
740140	746220	like them in terms of linguistic ability. And that's the thing that I think is exciting. So,
746220	753100	yes, I agree with a bunch of these things you started with, but nonetheless, like I think in
753100	757980	terms of syntax and semantics, there's just no other theory which is comparable to them.
760220	766460	But so let me push that back then, right? So the main objection from a lot of people I've spoken
766460	771740	to in the departments of linguistics who are like a lot of the general first of your paper
772460	777900	is to really say, well, you're right, they do a wonderful job accurately modeling all aspects
777980	783260	of a lot of aspects of syntax and semantics. However, I don't know if any real just like,
783260	786860	you know, Chomsky talks about facts about language, which is an old fashioned notion.
787580	790780	But I really think that's kind of an important notion too, right? Like,
790780	798620	is there some discovery about language itself that LLMs can uniquely provide? So like, if LLMs
798620	804780	made some prediction about, let's say you have a sentence structure type X being more difficult
804860	809100	to process than sentence type Y. And this is a unique prediction that only they'd
809660	814060	generate it. And no human linguist, Chomsky, Honesty, and Adger, none of these people had
814060	817900	ever predicted that before. But it turns out to be true. You do eye tracking experiments,
817900	821020	you do all sorts of different behavioral experiments. And it turns out, oh, you know,
821020	825020	after all, it turns out to be true. This is the new insight about language processing,
825020	829580	it's a new insight about language, you know, behavior. I just wonder, I'm not saying that
829580	833660	this is not possible in principle, because it might happen in the near future. But that's,
833660	838140	I guess, for me, the crux of why a lot of linguists speaking up, speaking on behalf of
838780	843100	the entire linguistic community here. And, you know, I guess that would be one of the main
843100	851100	objections. Yeah, I mean, I don't know of, I guess, I think of the insights they've provided as
851100	858220	kind of general principles, right? So I think about these things like the power of memorizing
858220	863260	chunks of language, right? So like, they seem to be very good at constructions, for example.
863340	867500	And there's lots of linguistic theories, Chomsky's in particular, right, which are
868380	874140	about trying to find kind of minimal amounts of structure to memorize, right, trying to derive
874140	880780	as much as possible from some small set, some small collection of operations. And I think
880780	887020	that hasn't gone well for those theories, right? Whereas this goes really well, right? So if we
887020	890860	think about something which has the memorization abilities, if we think about theories of grammar,
890860	898460	for example, which build on, you know, humans like really remarkable ability to memorize different
898460	902780	constructions, right, or different words, you know, tens of thousands of words, tens of thousands of
902780	907020	different constructions, sorry, tens of thousands of different idioms, maybe our theory of grammar
907020	912140	should be integrated with that. And there in some sense, a kind of proof of principle that
912140	917740	that kind of approach can work well, right? Can think about making other types of predictions
917740	923500	with them, some of which people are currently doing, but for example, trying to use them to measure
924300	930300	processing difficulty, measure surprise, for example, from these models, their surprise measures,
930300	936620	right, are much better than, say, context-free grammars or other kinds of language models.
936620	941980	And then it's an interesting question how those surprises or predictabilities relate to human
941980	947180	processing, right? And it may capture some of it or might be nonlinear, or it might, you know,
947180	952700	only capture a little bit of it or whatever. That's an interesting kind of other scientific
952700	957580	question. But I think in principle, right, they can make predictions about, for example, the
957580	963500	connections between sentences, right? So in the paper, I gave this example of, you know, converting
963500	969820	a declaration into a question in 10 different ways, right? And presumably when it, when, you know,
969820	975820	GPT or something is doing that, it's finding 10 different questions which are all in some way
975820	982700	related, kind of nearby in the models underlying semantic or syntactic space. And so those kinds
982700	989500	of things are of the type that I think, you know, some linguists might want, right, which is here's
989500	994220	some hidden connection between sentences or their, or their structures. But as far as I know, they
994220	1000460	haven't been evaluated empirically yet. So, yeah, yeah, I mean, these kinds of models are only a
1000460	1005820	few years old. So I think it's, it's reasonable to be excited about them, even though this kind
1005820	1011260	of work hasn't been done yet. No, that's right. No, totally. Totally. I mean, I think that's
1011260	1015820	the right perspective to take. But I think this gets to the issue of the, you mentioned surprise
1015820	1022220	or you mentioned learnability, you know, LMS learn some syntax, but they do so. We've obviously way,
1022220	1028700	way more data than infants do. Such that observations of potential structure in and of itself is not
1028780	1032940	a refutation of the poverty of the stimulus, well, the weaker version, I should say, of the
1032940	1037900	poverty of the stimulus argument. So the mere fact that LMS can do what they do without grammatical
1037900	1042380	prize is very striking, I agree. And in fact, you wouldn't have predicted that maybe five or six or
1042380	1048060	seven years ago. But it doesn't yet invalidate the claim that humans have such a prize and we
1048060	1052380	bring those prizes with us. And so in order to see if computational linguistics can constrain
1052380	1056140	hypotheses and theoretical linguistics, which I think it can do, by the way, this needs to be
1056140	1059980	done with, you know, careful experiments in which different learning parameters are controlled.
1060700	1067180	And gigantic language models like GPT3 are basically, you know, useless here. So this gets
1067180	1072380	to some of Tarlin's complaints that we need something like a baby LM project, which I know
1072380	1076700	you're interested in, where we have more, you know, ecologically valid training sets. You make the
1076700	1080620	prediction in your paper that some structure will be learned from that. I suspect you might be right
1080620	1084940	there. But, you know, even so, even with the baby LM challenge, there's still the kind of
1085020	1090620	non-trivial issue of addressing more traditional issues like when the kids start to generalize
1090620	1095420	based on the amount of current input, based on different factors, cross-linguistically. And that
1095420	1101180	requires just traditional, you know, psycholinguistics and language acquisition. So LMs, you know, do
1101180	1105260	care about things like frequency and surprise will, as you said, but there's a really nice paper by
1105260	1110140	Sophie Slatzen, Andrea Martin, a really beautiful paper that I think you may have seen that shows
1110140	1114860	very nicely that distributional statistics can sometimes be a cue to moments of structure
1114860	1119340	building. But it doesn't replace these notions pertaining to composition. So I'll just read a
1119340	1125820	quote from Chomsky 57, which sounds a lot like what Slatzen Martin said. Despite undeniable
1125820	1131260	interest and importance of semantic and statistical models of language, they appear to have no direct
1131260	1135180	relevance to the problem of determining or characterizing the set of grammatical utterances.
1135180	1139020	I think that we are forced to conclude that grammar is autonomous and independent of meaning,
1139020	1143260	and that probabilistic models give no particular insight into some of the basic problems of
1143260	1149500	syntactic structure. So that second hedge of the second sentence turned out to be incorrect.
1150060	1152460	But it's so it's true that, you know, what Chomsky said of available
1153020	1159020	stat models in 57 is no longer accurate when applied to models today. That can make abstract
1159020	1162300	generalizations about novel strings and distributional categories, as you mentioned,
1162300	1167260	right? But the performance of a single model does not provide direct evidence for or against
1167260	1171260	the landability of a particular structure. Like given the vast distance between any
1171260	1176620	computational model available today and the human brain, model success does not mean that the
1176620	1182620	structure is necessarily land and model failure also doesn't mean that the structure is not landable,
1182620	1190140	right? Yeah, yeah. So I mean, I think it's maybe worth unpacking kind of a couple different versions
1190140	1195500	of learnability arguments that people have made, because there have been very, very strong kind
1195580	1201260	of impossibility claims coming out of kind of Chomsky's tradition, right, that were never
1201260	1206940	claims about the amount of data that was required, right? They were claims about the logical problem
1206940	1212860	of language learning and that it was just impossible, right? It was impossible without having
1214780	1219580	kind of substantial constraints on the class of languages or the class of grammars that you
1220140	1225580	would acquire. And people for a long time have been arguing against that version of things.
1227180	1231180	You know, there's old work by Gold, and then there's whole kind of grammatical
1231180	1237820	theories of acquisition built on that tradition that worry a lot about the kind of order in
1237820	1241820	which you traverse through different hypotheses and consider different options and things.
1242780	1248620	And my favorite reference in this is this paper by Nick Chater and Paul Vitani,
1249420	1255260	called something like Ideal Learning of Natural Language, that basically shows that an unconstrained
1255260	1262700	learner could, with enough data, acquire the kind of generating rules or the generating grammar
1263340	1270300	just from observing strings, right? But that paper was really in response to this huge body of work
1270380	1275980	that was arguing that learning from positive examples, so from just observing strings,
1275980	1284380	was like logically impossible, right? So of course, you know, people in Chomsky's tradition
1284380	1291020	really liked that form of argument because it was one that said you had to have something innately
1291740	1296300	specified in order for language acquisition to work. It was like kind of a mathematical argument,
1296780	1302300	that you had to have some kind of innate grammar, innate ordering of hypotheses or something,
1302300	1309020	and all of that just turned out to be totally wrong. So if you move to slightly more kind of
1309020	1315900	realistic learning settings, which Chater and Vitani do, then it turns out you like an idealized
1315900	1320780	learner can acquire stuff, and there's no statements about the amount of data that's required even
1320780	1328860	there, right? That's the kind of pure logical ability to learn, and that ability is what I think
1328860	1334620	the big versions of large language models also speak to, right? So Chater and Vitani and other
1334620	1340540	work kind of in that spirit is, you know, mathematical and kind of arguing in principle,
1340540	1348220	but never created something which was really a grammar, right, or a real kind of implemented
1348220	1355580	language model. So even, you know, a model which is trained on 100 million or 100 billion or however
1355580	1363500	many tokens, right, even that kind of model I think is relevant to that version of the debate,
1363500	1370620	right, and showing that language learning is not impossible from a very unconstrained space.
1370940	1378380	Okay. And then there's a second version, right, which is can we learn language with the specific
1378380	1383500	data that kids get, right, and that's both amount of data and form of the data,
1384700	1391100	and so for people who don't know, the BabyLM Challenge is this,
1391580	1401580	sorry, we think to call it a competition or a, I guess it is a challenge, trying to get people
1401580	1407740	to train language models on human sized amounts of data. So that's something more like, I think
1407740	1413420	there's two different versions, 10 or 100 million different, 10 or 100 million different words in
1413420	1422060	the training set, which is like, you know, 100th or 1000th or something as big as these big AI
1422060	1429660	companies are using for their language models. And I think actually it's like, that's exactly the
1429660	1433820	right kind of thing and exactly what the field needs, right, because you might find that on
1434620	1440940	a child sized amount of data, you can essentially learn syntax, right, which I think would be the
1440940	1445500	strongest argument against these property of stimulus claims, you could alternatively find that
1446140	1452540	maybe you can't learn very much, maybe you, you know, come up with a much crumbier kind of language
1452540	1459020	model or it's lacking some syntactic or semantic abilities. I actually think that the failures,
1459020	1465500	they are a little bit hard to interpret because kids data, when they're actually learning language,
1465500	1471580	they get a lot more data than just strings of sentences, right, they're interacting in an
1471580	1477500	environment. So there's stuff in the world in front of them. Their utterances are also interactive,
1477500	1481820	right, so you can say something and see whether your parent brings you the thing that you asked for,
1481820	1489420	for example, right, that's long been argued by people as a, you know, important cue in language
1489500	1498460	acquisition. So in the baby LM challenge, there is an ability to train these models
1499180	1503900	with kind of multimodal input, so I think you can give them as much video data as you want to give,
1505500	1510540	but probably it's hard to kind of replicate exactly the type of setup and feedback that kids
1510540	1518540	actually get. So I don't know, you know, I'm excited to see where that goes and how things pan
1518540	1526860	out there. You know, I think that there is an interesting related question for large language
1526860	1534220	models, which is like what, which is understanding exactly what all of the data is doing. So it
1534220	1540060	could be that you need so much data for these models because they're effectively inventing
1540060	1546780	some form of semantics internally, right, so they're both discovering the rules of syntax and they
1546780	1553660	appear to be learning quite a bit about word meanings. And it's not, it's totally unclear,
1553660	1560220	I think, how much of the data in these modern models is needed for syntax versus semantics.
1560220	1567900	My own guess, I think, would be that the syntactic side is probably requires much less data than the
1567900	1573180	semantic side. Actually, a student, a former student of mine, Frank Malica, and I wrote a paper a few
1573180	1578300	years ago trying to estimate the amount of information a learner would necessarily have to
1578300	1584140	acquire for learning the different aspects of language. So you have to learn all the words and
1584140	1587980	you learn their forms, you learn their meanings, you probably know their frequencies, you have to
1587980	1594940	learn syntax. And basically what we found in that analysis, that was, you know, basically just a kind
1594940	1601020	of back of the envelope calculation for each of these domains is that syntax is actually very few
1601020	1608060	bits of information, it doesn't take that much information to learn syntax. Whereas like most
1608060	1615740	of the information you acquire is actually for semantics. So specifying, you know, 30 to 50,000
1615740	1623340	different word meanings, you know, even if each meaning is just a few bits, right, like that requires
1623340	1630220	a lot of information and probably each meaning is more than a few bits, right. So it could be,
1630220	1634540	like, that would make me guess that what's happening with large language models is most
1634540	1639500	of their training data is about word semantics. And you can think about other ways that kids get
1639500	1646140	word semantics, right, that's not just kind of co-current patterns in text. But I agree, all of
1646140	1651180	that is up in the air and really exciting to see what will happen. Yeah, I know that some of the
1651180	1658060	earlier results from Linsen's lab suggest that at least restricted to equitably valid, you know,
1658060	1663420	training set sides, models seem to generalize, you know, linear rules for English, probably
1663420	1667500	has no question of formation, rather than the hierarchical rule, the correct hierarchical rule.
1667500	1673020	So I think there's a real sense in which, you know, the space of the correct syntactic prize and
1673020	1677900	inductive biases really is yet to be really settled on. But it seems, at least to me, pretty
1677900	1682220	obvious that there has to be some. So there's also some evidence that children in English,
1682220	1686700	going back to this frequency issue, that children in English sometimes spell out an intermediate
1686780	1692380	copy of movement in the specified position of the lower complementizer position of a long-distance
1692380	1696940	WH question. So there's a thesis by Thornton at some of the papers about this. So they say,
1696940	1701580	which person do you think who did that, rather than which person do you think did that? So this
1701580	1705820	is an interesting, you know, missetting, because some languages do actually spell out these intermediate
1705820	1710940	copies, but English doesn't. So the kid makes the error in setting their grammar, but the frequency
1710940	1716220	of the input is actually zero. So our mutual friend Gary Marcus also has an argument against
1716220	1721580	frequency-determining a kid's output. In the case of German noun plurals, a more regular form of the
1721580	1725580	setting kind is preferred, not the frequent one. And there's lots of examples like this. So it's
1725580	1730300	sometimes claimed that subject-experiencer passives, where the subject is passively experiencing
1730300	1735100	something, are very delayed in kids in comprehension studies until around eight, because they're not
1735100	1740620	very frequent in the input. But Ken Wexler and colleagues have gone through subject-experiencer
1740620	1745900	WH questions like, who likes Mary? And they discovered that these are as infrequent in the
1745900	1750700	input as subject-experiencer passives. But kids have no problem in comprehension studies of these
1750700	1756460	questions. But they do have problems comprehending subject-experiencer verbal passives. So frequency
1756460	1760780	once again seems to be irrelevant. Or at least it's not explanatory, right? I guess it's not
1760780	1765420	explanatory with respect to theory building. So how can LMS help with these, you know,
1765420	1770300	diverging cases when there's clearly something else going on besides frequency? So LMS, you know,
1770300	1775020	they seem to generalize just, again, going back to this issue of the cases that you have in your
1775020	1780060	paper. You show that they generalize the structure of color screen ideas, which is obviously very
1780060	1785100	cool. But the positive stimulus has never really been about not being able to learn language
1785100	1789500	statistically. I know you made that claim, right? But Chomsky's point in the 50s about statistical
1789500	1795020	models of the day is not true of commercial LMS in 2023. And that's correct. But we can't use that
1795020	1799660	single point to undermine, you know, the entire generator enterprise. Chomsky's basic point was
1799660	1804300	that you could have a grammatical structure wherein every background has zero frequency,
1804300	1808700	and it also fails to provide clearly interpretable instructions to the conceptual interfaces.
1808700	1812700	So interfaces with other systems of the mind. So as you're showing your paper, GPT
1812700	1818780	mimics examples like full screen ideas. But, you know, again, this sentence yields over 150,000
1818780	1823500	results on Google, and it's discussed extensively in the literature. It's able to mimic the fact
1823500	1827340	that it can mimic this doesn't really tell us that much. At least we can't really say anything
1827340	1833260	with much confidence. So, you know, Albeba behind University College Dublin has this quote recently,
1833260	1838860	do not mistake your own vulnerability for an LMS intelligence. In fact, even Yanlacun wrote last
1838860	1844460	year that critics are right to accuse LMS of being engaged in a kind of mimicry. And the example
1844460	1850140	sentence is from chat GPT that you give in the paper. Actually, don't do a good job because,
1850140	1854220	as you say, it's likely that, you know, meaningless language is rare in the training data, but they
1854220	1858540	can either do it or they can't. But there's no middle ground in terms of giving us 10 examples
1858540	1864860	like this. So, you have colourless green ideas, which are very different semantic objects from
1864860	1871260	things like brown shimmering rabbits, white glittery bears, black shiny kangaroos, green
1871260	1876460	glittering monkeys, yellow dazzling lions, red shimmering elephants, right? These are all like
1876460	1881740	semantic, semantically weird and a bit strange, but they're still like legal structures. They're
1881740	1893020	kind of meaningful semantic objects. Right? I just said, yeah. Yeah. I mean, so maybe I can
1893020	1898540	respond to the first point first, right? So, you started off talking about these other
1899340	1905660	kinds of acquisition patterns, which maybe don't map directly onto frequency. And I think it's
1905660	1913500	actually a mistake to think that kind of modern learning models should be just based on frequency,
1913500	1920220	because they're clearly learning like pretty complicated families of rules or constructions
1920220	1927260	or something. And I think it's very likely that when they're learning that, they're in some sense
1927260	1933500	searching for a simple or parsimonious explanation of the data that they've seen, right? And how that
1933500	1940940	caches out in a neural network is maybe complicated and depends on parameters and the specifics of
1940940	1949020	the learning algorithm and those kind of things. But I think it's, I'd suspect maybe that it's
1949020	1960300	likely to be the case that they're learning over a complicated set of things, right? A complicated
1960540	1968620	kind of family of rules and constructions. And that means I think that their generalizations,
1968620	1975820	maybe like the examples of people that you gave, might be kind of discontinuous in the input,
1975820	1981500	right? So, sometimes you could imagine seeing some strings which lead you to a grammar and
1981500	1987100	the simplest grammar of the data that you've seen so far is one which predicts an unseen string,
1987100	1995100	right? And if that happens, then you'll be taking the data, learning a representation
1995100	2001740	which generalizes in some novel unseen way so far, purely because that generalization is
2001740	2005660	sort of the simplest account of the data that you've seen to date, right? I think that's sort of
2005660	2010220	what linguists try to do, right? Try to look at the data and come up with a theory of it,
2010220	2015580	and then sometimes that theory predicts some new phenomenon, right? Or some new type of sentence.
2016460	2020300	And so, if they're learning over as sufficiently rich space of theories,
2021180	2027020	then it wouldn't be unreasonable or unexpected for them to also show those kinds of patterns. Now,
2027020	2033420	whether they do or not I think is still an open empirical question, right? Because we have to
2033420	2038140	train them on small amounts of data and test their generalizations and these kind of things.
2038140	2045100	But I don't think like just the fact that humans do things which are not purely based on
2045100	2049020	frequency is any evidence at all, either way, right? Because once you're learning over rich
2049020	2056140	and interesting classes of theories, then that is the expected behavior. Actually, I had a paper
2056140	2065900	about a year ago that I think you're familiar with, Yang and Pianta dosi, where we were looking at
2067740	2073100	kind of what happens when you give a program learning model strings from different formal
2073100	2081180	languages. So think of like giving a general model just 10 or 20 maybe simple strings that
2081180	2087180	obey some pattern and then asking it to find a program which can explain that data, which often
2087180	2094940	means finding some way of kind of programmatically writing down the pattern in the strings. And
2094940	2101820	in that figure, we have a paper which is really relevant to this point where the generalizations
2101820	2107820	that that kind of model makes are I think kind of qualitatively like the ones you're describing
2107820	2113900	for people, right? Where you can give them a small amount of data and it will predict unseen
2113900	2119340	strings with very high probability, even though there's zero frequency in the training input,
2119340	2124460	right? And the reason it does that is that often the most concise computational description of the
2124460	2132780	data that you've seen is one that predicts some particular new unseen output. So that model is
2132780	2139020	essentially an implementation of the kind of Chater and Vitani program learning idea that I
2139020	2144060	brought up earlier. But it's one that I think, you know, if you think about in the context of
2144060	2149820	these arguments of kids saying unusual or unexpected things, like that is predicted by all of these
2149820	2155180	kinds of accounts, right? Because as long as these things are effectively comparing an interesting
2155180	2163180	space of grammars, then they'll show that kind of behavior, I think. Yeah. So, okay. So I guess,
2163180	2171260	you know, the argument would be that, at least from the gender perspective, syntax is functioning
2171260	2176780	separately, but it still maps to semantics, it informs pragmatics, right? So in the minimalist
2176780	2181740	program, syntax is obviously minimalist, it's very small, it's just a linearization and labeling,
2181740	2186700	they're the two only operations, you have a linearization algorithm to central motor systems
2186700	2193180	and some kind of categorization algorithm at the conceptual systems. So Chomsky's architecture
2193180	2197500	is kind of reliant on the process of mapping syntax to semantics, right? It's form meaning
2197500	2203020	regulation, it's not just structure, and it's not just meaning. So LMS don't really have this mapping
2203020	2207420	process, right? Like, where's the mapping to semantics? And if there is a mapping, what does
2207420	2212460	the mapping process look like? What are the properties of its semantics? What are the properties of
2212460	2216460	the semantics placed on their own sets of constraints on the mapping process? Like,
2216460	2222220	they do for natural language? Do these kind of constraints inform each other? Is they kind of
2222220	2228220	a back and forth process? Like, LMS don't really seem to describe this form meaning pairing,
2228940	2236700	which means which strings, for example, right? Sorry, are you saying that they don't have semantics
2236700	2242380	at all? Or are you saying that there's just not a clear delineation between how the structures
2242380	2246860	get mapped onto the semantics? Yeah, the latter, right? So they clearly have some, potentially
2246860	2250620	some kind of semantics. I know you've argued for conceptual role theory being relevant here, right?
2250620	2254140	The rest of it is maybe a little bit more mysterious, but the actual, so in linguistics
2254460	2259020	there's a theory of the mapping process itself, it's explicit, and you can see it in action,
2259020	2262540	and you can test different theories of it in psycholinguistic models and what have you.
2262540	2268220	The actual regulation, the kind of constrained ambiguity, ambiguity in the sense of one word,
2268220	2271980	multiple meanings, or one structure, multiple interpretations, etc, right?
2273340	2278700	Yeah, I mean, if you think they have semantics, then I think they have to have a mapping from
2278700	2284620	the syntax to the semantics. I agree, it's not as like, nobody really understands how they're
2284620	2291820	working on any deep level, right? So I agree, it's not as clear as, say, in generative syntax and
2291820	2299180	semantics, right, where you kind of write down the rules of composition and can derive a compositional
2299180	2304620	meaning from a sentence from the component parts or something, right? Like, that's not how they're
2304620	2310940	working, right? But I just, I wouldn't take for granted that it has to be like that. Like,
2312940	2318460	it could be that how they're working is actually how we work, right? That everything is represented
2318460	2325100	in some high-dimensional vector space, and there's some complicated way in which that vector semantics
2325100	2333500	gets updated with each additional word or whatever in a linguistic stream. But like, I think it's
2333500	2338540	clear that they have some kind of representation of the semantics of a sentence, right? Like,
2338540	2343420	they can answer questions, for example, at least approximately. I mean, it's not perfect, but
2344380	2351260	it's not like a n-gram model or something, right? Which really doesn't have semantics. So I think
2351260	2360620	that they're definitely representing semantics and, you know, updating that as they process
2360620	2366140	language, it just happens not to look like these other formal theories. And I guess, I don't see
2366140	2369900	why that's a problem, right? Like, those other formal theories could just be, you know, poor
2369900	2375420	approximations or just totally wrong, right? Yeah, yeah, no, no, totally, totally. I mean,
2375420	2379580	there's also ways in which some of the formal formal theories in semantics are already
2379580	2383180	potentially compatible. We've got some of these things are doing, right? So another way to think
2383180	2388940	about this is, you know, LMS are, well, LMS are compression algorithms, but natural language
2388940	2394460	understanding is kind of more about decompression. It's disambiguating, meaning x, out of meanings,
2394460	2398860	x, y, z. It's all about making inferences about, you know, meta relations between concepts that
2398860	2404060	are not in the training data. So some examples that Millie Mitchell gives are things like on top of,
2404060	2409500	you know, she's on top of a game, it's on top of the box, all of these kind of vary with context.
2409500	2413180	So there's a lot of other things that are going on, right? And I think you discussed some of the
2413260	2419500	examples on your paper. So, you know, but the fact that the language is still not, at least,
2419500	2424860	again, under this theory of language, it's not about string generation. It's about this form,
2424860	2429260	meaning, pairing machine. So some semantics in the generative tradition, even think all the
2429260	2434620	rest of semantics is just and, right? So both Rasky's conjunctivist theories semantics is that
2434620	2440780	human semantics is just and that's it. Which, again, is very simple, elegant. It's, it's,
2440780	2446300	it's interpretable. It's compatible with other things that, you know, are maybe going on in your
2446300	2450300	neck of the woods, right? But regardless, it's still, you know, natural language is still more
2450300	2455660	compositional than things like, you know, formal languages just to make a clear distinction that's
2455660	2460460	been made. They have a much richer compositional structure. There's more stuff going on, maybe.
2460460	2463980	So it's important that before that, you know, things like attention-based machine mechanisms
2463980	2469740	and transformers allow for combinations of discrete token bindings, which is more
2469740	2473660	approximate to a merge-like operator than simple recurrent matrix multiplication.
2474860	2478220	But, you know, the issue of binary branching, binary branching of merge, just to choose,
2478220	2482940	for example, here to talk about the four meaning regulation, one principle. Binary branching in
2482940	2487580	merge is an interesting question, but Gem2 grammar has always been open to different origins and
2487580	2492220	locations of this apparent constraint in syntactic computation. Like, where does it come from?
2492220	2496780	Maybe it's a condition on merge. Maybe it's imposed by a smooth system. Maybe it's a kind of prior,
2496780	2501900	you know, who knows. And in fact, some more recent work in Gem2 grammar has tried to ground
2502860	2507500	do away with a lot of the set theoretic assumptions of merge, right? Maybe set theory isn't the best
2507500	2511980	way to model the Gem2 grammar. Maybe more logical accounts are more appropriate. There's lots of
2511980	2517500	other recent ideas there, which are all compatible with the, with Chomsky's approach, right? In fact,
2517500	2520860	one of the things that Chomsky likes the most is when he's, when he's proven wrong, right? A lot of
2520860	2526620	these theories are going against the core mainstream minimalist architecture. But yeah,
2526620	2532940	I think it's a very diverse, like, vibrant field. The people who are Adjah, Hornstein, you know,
2532940	2539340	Petrosky, Haji Borre, they disagree in fundamental ways with a lot of what the mainstream of
2539340	2544220	Gem2 grammar would say, but there's still more scope for disagreement. But it's still compatible
2544220	2548460	with setting core assumptions, right? So a lot of David Adjah's work, for example, kind of deviates
2548460	2553580	in this core respect, but it's still trying to ground these intuitions in different formal systems.
2554620	2561500	So, you know, it's kind of, I want to get your thoughts again on, I mentioned Mitchell, right?
2561500	2567340	So Mitchell and Bowers 2020, they have this paper, priorless recurrent networks laying curiously,
2567340	2570780	but I think you might be aware of, right? So this is a really good example just to kind of get to
2570780	2574940	the heart of the issue. So recurrent neural networks have been shown to accurately model,
2574940	2578860	you know, non-veb number agreement, but Mitchell and Bowers show that these networks will also
2578860	2583420	learn a number agreement with unnatural sentence structures. So structures that are not found
2583420	2587740	in natural language, and which humans have a hard time processing, right? So the mode of learning
2587740	2594220	for RNNs is, at least for RNNs, qualitatively distinct from infant, you know, infant homo sapiens,
2594220	2599180	right? So the story is Mitchell and Bowers show that while their LSTM model has a good representation
2599180	2604060	of singular variances, plural for individual sentences, there's no generalization going on,
2604060	2607820	right? They can represent at the individual level. So the model doesn't have a representation of
2607820	2612940	number as an abstraction. What number is? Only concrete instances of singular versus plural.
2613900	2620060	So successfully predicting language behavior via LM, or successfully predicting neural responses
2620060	2623340	in a similar way is obviously great. And maybe we can get into that issue later,
2623340	2627100	but there's only one side of the coin here, right? The other side of the coin is explaining why this
2627100	2631100	type of behavior and not some other behavior, why this structure and not some other, and that's
2631100	2638620	maybe Chomsky's most important point, really, why this and not some other system. So linguistic
2638620	2642620	theory kind of gives you that other side of the coin, right? Whereas LM's really don't. So the
2642620	2649100	Mitchell and Bowers paper does something that- He does it! Well, yeah, so like, take Yael LaCrette's
2649100	2653900	and Stanislas de Haines' work from 2019, right? They looked at number agreement in an LSTM and
2653900	2658700	found two specialized units that encoded number agreement, but the overall contribution to performance
2658700	2664540	was low. And then in 2021, Yael LaCrette's have this paper where they show that in the neural
2664540	2670060	language model, it did not achieve genuine recursive processing of nested long range agreement,
2670060	2675100	gender marking in Italian, I think, even if some hierarchical processing, you know, was achieved,
2675100	2679660	as you've argued before, right? Some hierarchy was there, it was there. But the question is,
2679660	2684140	is it the right mapping? Is it the right kind of hierarchy? They found that LSTM based models could
2684140	2688780	learn subject-verb agreement over short spans, one degree of embedding, but they failed at some
2688780	2695660	longer dependencies. And in their most recent paper, LaCrette set out with De Haines showed that
2695660	2701580	they evaluated modern transformer LM's, including GPT2 XL, on the same task. And the transformers
2701580	2706220	performed more similarly to humans than LSTMs did and performed above transfer overall, but they
2706220	2710540	still performed below chance in one key condition, which is the, as I mentioned, the multiple embedding
2710540	2715020	one, the difficult structures. So the reason why I mentioned these studies is because, you know,
2715820	2720700	it's not just to explore the limits of LM's, which is an interesting question. But consider work by
2720700	2727020	people like Neil Smith at UCL, right? He did work in the 90s with a polyglot, savant, and neurotypical
2727020	2731900	controls comparing them. So he investigated second language learning of an artificial language
2731900	2735740	containing both natural and unnatural ground structures, like the Michelin Bowers paper,
2735740	2739500	right? The whole framework is natural versus unnatural. And they found that while both the
2739500	2744860	savant, Christopher, the savant, and the controls could master the linguistically natural aspects,
2744860	2748780	only the controls could eventually handle the structure dependent unnatural phenomena,
2748780	2753100	and neither of them could master the structure independent aspects. So some weird rules where
2753100	2756540	it's like, you know, you mark the emphasis on the third word of the sentence, things like that.
2756540	2761660	So they argue that Christopher's abilities are entirely due to his intact linguistic faculties,
2761660	2766860	but the controls could employ more domain general kind of cognitive resources, like, you know,
2766940	2770940	tension control, etc., which is why they could deal with those difficult processes.
2771580	2777340	But I just mentioned, you know, a minute ago, that the LSTM in the Michelin Bowers paper approaches
2777340	2782140	natural and unnatural structures in pretty much the same way. So it's not, you know, it's not a
2782140	2787420	psychologically plausible model, I would argue, for whatever humans are doing. And similar observations
2787420	2791900	can apply to the limits of transformer models in Le Creta's work. And all of these themes are like,
2791900	2795980	right up there, they're saying that there's all the way to the present. So another one of
2795980	2800140	Tal Linsen's recent papers that he posted a few weeks ago, looking at child directed speech,
2800140	2805420	showed that LSTMs and transformers limited to ecologically plausible amounts of data
2805420	2809180	generalize, as I mentioned, the linear rules for English, right, rather than the abstract rules.
2809740	2815260	And in fact, more recent work from Linsen's lab last week, looking at, well, last year, I should say,
2815260	2820700	shows that looking at garden paths, surprise does not explain syntactic disambiguation
2820700	2825180	difficulty, right? Surprise will underpredicts the size of the garden path effect across all
2825180	2828540	constructions. And this gets to this issue that you mentioned before, you know, maybe surprise
2828540	2832380	all this related to some aspects of syntax, but maybe not other ones, it's kind of a,
2832380	2836220	it's a very nontrivial issue that is very much, it's open to discussion. It's not,
2836220	2840620	it hasn't settled yet. But so Linsen showed that garden path effects are just way more
2840620	2844780	difficult than you would expect from mere unpredictability. So another way of phrasing
2844780	2850860	this argument is to quote a recent argument with Chomsky's to get at this natural basis,
2850860	2854780	unnatural issue. He says, suppose we have an expanded periodic table that includes
2854780	2859980	all the elements that do exist, all the elements that can possibly exist, and all the elements
2859980	2865020	that cannot possibly exist. And let's say you have some model, some artificial model that fails
2865020	2869500	to distinguish between these three categories, whatever this model is doing, it's not helping
2869500	2873660	those understand chemistry, right? It's doing something else. It's doing something for sure,
2873660	2877260	but whether or not it's helping those understand chemistry is something separate. And I know that
2877260	2880700	you've said in response to some of these studies, I think you've said that, you know,
2881660	2885180	in order to show that something is likely to be impossible, somewhere in your paper, I think you
2885180	2891820	say, in order to show that something is impossible with normal bounds and false positives, you'd
2891820	2895980	need to show, you need to look at something like 500 independently sampled languages. So you cite
2895980	2900220	this in your paper, right? Which you probably can't do, that's just not, it's not a feasible thing to
2900220	2906220	do. So, you know, I'm not too sure that this really refutes the principle argument that I'm
2906220	2910700	making here, right? Because people like Mitchell and Bowers are making an argument about impossibility
2910700	2914700	in principle, not in some kind of extensional sense, you know, just like searching across the
2914700	2919420	world languages to see, to prove across every single language that it is impossible, right?
2919420	2923660	That's kind of, it's a different argument, whether it's impossible in some random language in the
2923660	2928060	Amazon, compared to actually impossible, based on the principles of what the language system is
2928060	2931820	actually doing, like what it can do. So I would just say that, you know, all of these kind of
2932060	2938140	I think that that that point is, is that you don't actually know what is typologically not
2938140	2943020	possible, right? So people like to say things like, you know, there's no language that does X,
2943020	2949020	therefore we have to build that restriction into our statistical models. But if it's not
2949020	2953900	statistically justified that there is no language that does X, right? If you've only looked at
2953900	2957740	20, 20 European languages or something, right? I mean, it's, it's not
2958300	2965980	like that shouldn't motivate doing anything to the models, right? If it's, if it's not a
2965980	2973420	statistically justified universal, I think. Well, you know, I think, you're totally right,
2973420	2976940	but that just applies more generally to the social sciences and psychological sciences,
2976940	2980940	right? Like typologically, it's very difficult to establish these things, right? So I guess
2981020	2985900	you're, you're, I guess you're just kind of steelman you're a bit, you're saying that the strong
2985900	2991500	claim is very difficult to prove, right? Like the reason the language that has X.
2992220	2996860	The strong claim that something is not allowed in, in natural languages, I think very, very
2996860	3006540	difficult to prove. And, you know, I think that there have been lots of, you know, strong attempts,
3006620	3013020	there's been lots of strong claims from, often from, from generative syntax, right,
3013740	3020700	about what all languages do. And I think that, you know, people have been very good at finding
3020700	3025500	kind of counter examples to a lot of those things. I cite this paper by Evans and Levinson,
3026540	3031420	which actually, you know, I had heard for years about how no language does X and that's what
3031420	3035660	we're using to construct our theories. And that Evans and Levins paper, Evans and Levinson paper
3035660	3041340	really kind of changed my mind about this, right? That like language is actually much more
3042140	3048940	diverse than, than I think most, most syntacticians will, you know, try to construct theories for
3048940	3054940	something. So, you know, I think we, going back to kind of the beginning of what you said, I think
3055180	3061020	we, we'd agree that, that you need language architectures which learn the things that kids
3061020	3066620	learn and learn it from data that they learn. And those architectures might, might be unlikely to
3066620	3072780	be things like LSTMs or, you know, simple recurrent networks or, or whatever, right? Like, I think
3072780	3077580	all of that work is, is very useful in, in kind of honing in on the right architecture.
3078300	3085820	So, I'm just trying to, to remember all of, all of the points you were making. Oh, yeah. So,
3086620	3093420	but I think this, that there, there's a kind of flip side to this, which is that I think that
3093420	3099420	the space of things people can learn is actually kind of underestimated, right? Like, there's this
3099420	3106300	bias to, to, to say, you know, people can't learn X, Y and Z. But people, at least outside of language
3106300	3110940	have this, this really remarkable ability to learn different kinds of patterns, right? Like,
3110940	3117980	the patterns you find in, in music or mathematics, for example, we can learn sophisticated types of,
3117980	3124220	of algorithms, right? We can learn to, you know, fly a space shuttle or to, you know, tie knots in,
3124220	3129420	for rock climbing or whatever, right? Like, there's all kinds of kind of procedural and
3129420	3134860	algorithmic knowledge, which is structural that, that people are able to acquire. And I think that,
3134860	3141740	that that notion very rightly kind of motivates looking for learning systems, which can work
3141740	3149340	over pretty unrestricted spaces, right? So, you know, you, you, you might say that, okay, well,
3149340	3154780	language is different because language is a restricted space. And it might be true that,
3154780	3158380	that language is restricted, but it also might be true that the things we see in language come
3158380	3164140	from other sources, right? It could be that languages, especially pragmatic, for example,
3164140	3169180	compared to music or mathematics, right? And those kinds of pragmatic constraints
3169980	3173900	are the things that constrain the, the form of language, right? Or language is communicative,
3173900	3178540	it's probably more communicative than, than music, for example. And that might constrain the, the,
3178540	3183500	the form of things. So, I mean, as, as you know, this is very old debate in, in linguistics about
3183500	3190620	kind of where the, where the properties of, of natural language come from. And I guess what I'm
3190620	3195820	trying to say is that there's one kind of perspective where you look at all of the things humans can do
3195820	3201260	even outside of language, all of the rich structures and algorithms and processes we're able to learn
3201260	3206460	about and internalize. And you say, okay, maybe language is like that. And then yes, language
3206460	3211020	also has some of these other funny little properties. But, you know, maybe those come
3211020	3217020	from some other, other pieces of, of where language comes from, right? It's, you know,
3217020	3223260	we have pretty sophisticated pragmatic reasoning. We're using it to achieve certain communicative
3223260	3229580	ends. You can find all kinds of kind of communicative features within the, the language system itself.
3229580	3234700	And so, so maybe some of these other properties are, are properties that have some other origin.
3235580	3241340	And that, that view, I think could be wrong, but it's, it's one that I think needs to be looked at
3241340	3251100	to see if it's wrong, right? Like, I think it's been kind of dismissed by large chunks of, of
3251100	3255180	linguists, right? Just, you know, I've heard people say stuff like, oh, well, communication
3255180	3259980	doesn't really explain anything about language, right? And what they mean often is it doesn't
3259980	3264780	explain like the particular island constraints or something that they're, that they're working on,
3264780	3268620	right? But there's all kinds of other things in language that communicative pressures probably
3268620	3276220	do explain. So I guess my, my pitch is always for, for kind of breadth in term, breadth in
3276220	3282460	consideration of the forces that, that can shape language and not needing to put it all into some
3282460	3285020	form of, of innate constraints or something like that.
3285020	3288460	No, no, totally. And I think, I think a lot of that stuff is, is, is compatible with, with, with
3288460	3292860	the minimalist program, because the minimalist program wants syntax to be minimal. It doesn't
3292860	3296060	want it to be complicated. It doesn't want it to be, you know, any more complicated than it has to
3296060	3299740	be. So there were some, you mentioned the curious properties, right? So there were some of the
3299740	3303980	properties that need to be counted for in any model of language that are, I'll give you one
3303980	3309020	example, right? The setting of Pearson features. And these Pearson features exhibit very non,
3309020	3313180	non trivial generalizations that do not seem to be counted for via domain general learning
3313260	3317500	mechanisms. So I'm citing here the work of Daniel Harbour at Queen Mary. So for example,
3317500	3322220	the morphological composition of Pearson, its interaction of number, its connection to space,
3322860	3327100	properties of its semantics and its linearization, they all appear to be strong candidates for our
3327100	3330460	knowledge of language, right? What we mean by knowledge of language. But on the other hand,
3330460	3335340	we have things like case and agreement and head movement. And these are all structural phenomena.
3335900	3343180	However, they seem to resist a purely meaning based explanation in theoretical linguistics,
3343180	3346940	right? It would be great if syntax were nothing but a computational engine
3346940	3351260	that builds structured meaning. And that's the minimalist program, the goal. But that's not
3351260	3355820	what we actually find. That's not in any actual minimalist, like concrete model, any concrete
3355820	3361100	minimalist theory. The goal is just like, the program is language is perfect. Okay, that's the
3361100	3365820	program. Is that what we find? No, obviously not. Okay, no, no linguist actually believes that.
3366860	3371740	So it'd be great if syntax was like that. But I think, you know, the program is to look for
3371740	3376860	perfection, but not always find it. So case and agreement and head movement are morphological,
3376860	3380780	morphophonological phenomena, the properties of the performance systems, what's called
3380780	3384380	performance systems. And so the minimalist program itself is really compatible with a lot
3384380	3388220	of what you're saying about, you know, language, language, there are aspects of language that
3388220	3394380	can be perfected and optimized for communicative efficiency. Absolutely. Totally. No doubt about
3394380	3399820	it. But where is that locus of efficiency? Is it in the syntax itself? Or is it some kind of
3399820	3404220	extra linguistic system? Is it in pragmatics? You know, is it in century motor? Is it in the
3404220	3409980	speech? And property of speech and phonology? Probably, you know, I mean, who knows. But
3409980	3416860	I think all of these things demand much more, you know, serious consideration into old fashioned
3416860	3420620	notions like structure dependence, compositionality and what have you, things like that, which
3420620	3425820	you can maybe find somewhere in the literature, but even just basic topics like, you know,
3426780	3433100	quantifier raising, extended projections, adverbial hierarchies, all of these things
3433100	3438220	in the minimalist program can be extra linguistic, right? They can actually be outside of syntax and
3439340	3444140	very queer properties of the semantic conceptual systems, which are in themselves kind of domain
3444140	3449180	general, weird leftovers from ancient primate cognition, right? The features of the way we
3449180	3453260	pass events, the way we pass, you know, agents and patients, things like that. That's definitely not,
3453260	3458380	that's not human specific. But, you know, the way that syntax provides instructions to these
3458380	3464460	systems, you know, probably seems to be. So, you know, generative linguists have different theories of
3464460	3468140	also language production too. I'll just talk about language production based on whether we
3468140	3472220	store lemmas or whether we build words in the exact same way we will phrase and sentence this.
3472220	3475660	So, I know that you make distinction between construction grammar and kind of generative
3475660	3480220	grammar and, you know, the weight they place on memorizing constructions versus just building
3480220	3485180	things from the bottom up, from the ground up, right? And so, you know, in some generative inspired
3485180	3489900	models, mechanisms which generate syntactic structure make no distinctions between processes
3489900	3495420	that apply above or below the word level. There's no point at which meaning syntax and form are
3495420	3500060	all stored together as single atomic representations. Each stage in lexical access is a transition
3500060	3504540	between different kinds of data structures, right? There's meaning, there's form and there's
3504540	3508860	syntax. These three features kind of come in together and they don't always overlap. Different
3508860	3515420	languages realize them in different ways. And so, you know, a word, the basic definition of a word
3515420	3521340	is just this weird multi-system definition where lots of things, lots of different cognitive systems
3521340	3526380	enrich the basis of every lexical item, right? You have, there's nothing like this really,
3526380	3531100	this enrichment process anywhere else in linguistic theory, right? Or at least in
3531100	3538060	what LLMs are doing. Like, so I guess, what, I guess I would ask you, what is your definition
3538060	3543740	of a word, right? And what can LLMs really provide insights into weirdhood, right? Because if you
3543740	3547500	can't, if you don't have a definition of what a word is, then you're really in trouble, right?
3547500	3552860	Like, we have to at least use LLMs or artificial systems to inform what we mean by a word. Or
3552860	3557740	maybe we don't need that anymore. I'm not sure what you think. I'm not sure what you mean. I mean,
3560700	3566620	I don't have a... What is a word? Why does that matter? I mean, that's just a convention about
3566620	3572300	how we use the term word, right? What, like, I mean, you could use, you know, lemmas or word
3572300	3577580	firms or whatever. Like, that just feels like a conventional choice. I'm not sure what's at,
3577580	3583500	what's at stake there. So how would you, I guess I would say, I agree, word is a conventionalization,
3583500	3589740	you know. Our intuitive concept of word is often biased by orthography, the way we put spaces between
3589740	3594300	things, right? So I agree with that criticism. You know, word in the intuitive sense is not really
3594300	3599340	a scientific construct. However, I guess, let me rephrase my question. How would you, you know,
3599340	3603500	decompose the intuitive concept of word into something that is more kind of, you know,
3603580	3607420	scientifically amenable or psychologically plausible, which is exactly what genitive
3607420	3611340	grammar tries to do by decomposing words into, you know, distinctive features,
3611900	3616860	morphological categories, conceptual roots being matched with categorical features,
3616860	3622380	you know, you get a concept, you know, and you match it with a noun or a verb category to get a noun
3622380	3627740	or a verb. These different models make different predictions, right? Yeah, I mean, I think that
3627740	3632540	general idea is likely to be right for large language models. Like, I think they kind of
3632540	3637660	must have things that are kind of like part of speech categories, for example. And I think that
3637660	3645500	they kind of must be able to update those, their categories based on the language that they've seen
3645500	3652300	so far, right? So like, like, you know, GPT puts nouns and verbs in the right places. And to do
3652300	3657580	that, you kind of need some representation of the nouns versus the verbs, and you need some ability to
3658380	3663260	locate yourself in a string of other words and figure out if there's likely to be a noun or a
3663260	3670620	verb next. So I think that on that level, those kinds of properties of words are very likely to
3670620	3677420	be right. And there are also things which are very likely to be found kind of in the internal
3677420	3682460	representations of these models. I don't see how it could be any other way other than that.
3683100	3692140	But like, as far as I know, that's not where the main debates or disagreement, I think, is,
3692140	3699580	right? Like, I think all theories of language have to have to say that there's different kinds of
3699580	3704780	words that can show up in different places or something like that. Yeah. Okay, so how about
3704780	3710620	the issue? You mentioned communication, right? So, you know, and you're totally right, when Trump
3711580	3715580	says things like language is a thought system or, you know, language didn't evolve,
3716380	3719660	he's kind of being a little bit cheeky. He doesn't really mean that. He kind of means it in a very
3719660	3724620	specific sense, right? But, you know, when we say language is a thought system, what we mean is
3725740	3729420	we're trying to get it an architectural claim. So if you look at the architecture of the minimalist
3729420	3735180	program, the syntactic derivation and the conceptual systems are literally different systems, right?
3735180	3739340	The conceptual systems take stuff from syntax and then does its own business with it and the
3739340	3744140	CI systems have their own peculiar rules and principles, which is why thought and language
3744140	3749500	are both similar symbolic compositional systems, but in different ways. Only a subset of thought
3749500	3755580	is properly called the CI interface system, since the CI systems are by definition, you know,
3755580	3760460	whatever conceptual systems you would have that can access and read out instructions from syntax.
3760460	3763740	And we don't know what they are fully. They seem to have something to do with events and
3763740	3767580	grammatical reference and definiteness. They seem to be the main categories that language,
3767580	3771980	you know, cares about conceptually, but we don't really know. That's kind of just a hypothesis,
3771980	3778060	right? But what we do know is that they don't seem to make use of color all that much. So no
3778060	3785580	language morphologically marks shades of color. All the conceptual features like worry or concern,
3785580	3789180	like no language morphologically marks a degree of worry or concern about an issue,
3789180	3795180	but we do make use of epistemological notions like evidentiality and things like that.
3795180	3799740	So, you know, I guess what I'm saying is the minimalist program does a good job of
3800460	3804540	trying to figure out which aspects of thought language is intimately tied to,
3804540	3809020	and which aspects of thought it's not tied to. So the minimalist program allows us to kind of carve
3809020	3813580	that up quite neatly. And this is a much more nuanced framework than, you know, when Chomsky
3813580	3818060	says language is thought, again, he doesn't, maybe he means it, maybe he doesn't, but that's not what
3818060	3823100	the actual architecture of his theory says. It's a rhetorical device that is very, you know, useful
3823100	3828860	and interesting to attract undergraduate audiences. But if you look at actual theories that are coming
3828860	3833020	out of the minimalist program, no one really believes language equals thought, right? The language
3833020	3837900	system seems to, it tries its best to access and reformat and manipulate various conceptual
3837900	3842220	systems, but it has its limits, right? We know what systems, spell keys, core knowledge systems are
3842220	3848860	hooked up to with respect to the syntax engine, and which ones are not. So, you know, this kind of
3848860	3853820	gets back to the idea that lexicalization of a concept seems to maybe alter it in some way.
3853820	3858300	It kind of imbues it with elements that are not there in the concept itself. So if you lexicalize
3858300	3862220	the concept, you suddenly transform it a little bit, you give it a little extra, you sprinkle
3862220	3867100	something else on top of it, and that seems to vary across different noun types. But these are all
3867100	3874700	like very clear architectural claims within gem diagram that make very clear empirical predictions.
3874700	3878860	So in other words, I guess what I'm saying is all these neuropsychology studies that are
3878860	3884540	coincided, you know, in a lot of work in this fame, what does it really show? I think it shows
3884540	3889900	that, you know, when language is damaged in the brain, it loses its particular sway or mode of
3889900	3894860	influencing those systems. But there's no real prediction from within the gem to grammar enterprise
3894860	3899020	that those non-linguistic systems should be impaired or should suddenly, you know, shut down
3899020	3903420	if the core language system is compromised, right? In fact, if anything, that just
3904140	3909580	emphasizes the principal divorce between the syntactic system and non-linguistic systems,
3909580	3913420	right? So I think the, a lot of predictions here from the language and communication,
3914140	3917340	you know, literature are kind of missing the point of the architectural claims.
3919340	3921660	I can just give, or Daniel, do you want to go?
3922220	3922860	Yeah, go ahead.
3922860	3927740	Give a little bit of background there. So there's these papers from
3928780	3938220	Ev Fedorenko and Rosemary Varley that are examining in part of them aphasic patients. So
3938220	3945260	people who have impaired linguistic abilities, basically showing that with impaired linguistic
3945260	3953020	abilities, you can still have preserved kind of reasoning abilities. So people like chess masters,
3953020	3960700	chess grandmasters, for example, who are obviously very good at reasoning might not have kind of
3960700	3966220	intact linguistic abilities. And then complimenting that kind of patient work, there's also work from
3966220	3975660	Ed's lab showing that the parts of the brain that care about language are separable from
3975660	3980060	the parts of the brain that care about other domains, even ones that seem kind of language-like.
3980060	3986460	So things like music and mathematics tend not to happen in the language areas.
3987180	3994220	So Ev and others have argued that this is basically evidence against the Chomsky
3994220	4000940	inclaim that language is the medium for thinking, because there's thinking that can happen
4000940	4005100	in the absence of language and the brain areas that care about language seem not to be the brain
4005100	4012060	areas that care about thinking. I guess, Elliot, you're saying that people don't really believe that.
4014940	4017020	They don't believe that distinction, I mean.
4017740	4025340	And also, there's a lot of self-contradiction even within these arguments, right? So in your paper,
4025340	4029180	you sometimes say that Chomsky thinks that language is a thought system, but then a few
4029180	4033660	pages later, you'll say Chomsky also believes that syntax is some totally separate system from
4033660	4038780	anything else, right? Your autonomy of syntax, etc. So does Chomsky think-
4038780	4041340	That's not my contradiction. I mean, he said both of those things.
4041980	4046860	Right, exactly. So therefore, you may want to ask yourself, does he really believe these things?
4047500	4051260	Or what is the case if it arises from the architecture, right?
4051260	4055660	So just saying language is a thought system, what does that mean? That doesn't mean anything.
4055660	4060700	It's just a very vague statement. The question is how exactly is language contributing to thought
4060700	4061900	and how is it not contributing?
4063980	4070220	Yeah, I mean, I think his claim is mainly evolutionary or something, right, that this is the
4070220	4076620	origins of the system, which I think is sort of equally hard to square with the kind of
4076620	4085500	patient and neuroimaging data. But if he doesn't think that, then he shouldn't say it,
4086540	4088460	or people will respond to what he said, I think.
4090220	4094620	The argument is that language is a kind of thought system. It regulates some aspects of
4094620	4098700	thought and it yields some aspects of thought that are clearly unique to humans,
4098700	4103980	but it's not intrinsically or causally tied to it. The architecture of the system is very different
4103980	4108460	from the kind of generalizations you can rhetorically evince from the architecture.
4108460	4112620	So for instance, when you cite work from a phasic patient showing no deficits in complex
4112620	4116460	reasoning, as you just mentioned, playing chess and so on, we would actually expect this under a
4116460	4121740	kind of non-lexicalist framework of generative syntax, where meaning, as I said, meaning syntax
4121740	4126700	and form, form just meaning anything that you can externalize language in, all these things
4126700	4131100	are separate features and separate systems. The autonomy of syntax doesn't mean,
4132700	4136380	what a lot of people think it means, it just means that there are certain syntactic operations
4136380	4140540	that are not semantic. There are certain things you can do with syntax that you can only do with
4140540	4144060	syntax and you can't do with semantics. So this gets back to the difference between
4145100	4151180	Petrowski's theory that semantics is just and versus a lot of syntacticians' belief that
4151180	4155340	there are certain peculiar weird things you can do with syntax that are just syntactic.
4155340	4161340	So there is a divorce even within the kind of architectural framework. So it's not too surprising
4161340	4165020	that you also find that divorce at the neuropsychological level, I would say.
4166060	4173260	Well, I think I would want a prediction of the language's thought evolutionary idea then.
4176380	4183980	If you're saying that doesn't predict that thought relies on language, then I think whoever
4183980	4190380	likes that theory should come up with some predictions about what that theory actually
4190380	4195500	means. I feel like those kinds of predictions are often really necessary for understanding the
4195500	4201180	content of a prediction. So sorry, Daniel, your hand's been up for a while.
4202140	4214300	No, it's all good. Just kind of wanted to bring a breath in and an opportunity for anyone to ask
4214300	4223180	any other questions. But wow, thank you both for the many topics we've covered. We'll have,
4223180	4227660	in the last minutes, a kind of conclusion in next steps. But Dave, would you like to
4228220	4230860	ask a question or just give a short reflection?
4237020	4244700	Okay, no. There are many comments in the chat, so I hope that both of you can read them
4244700	4251980	on your own time to see what everyone added. Where do we go from here? As we
4252940	4261740	roar into May 2023 and beyond, what can linguists, large language model developers
4261740	4266940	and users, cognitive scientists, what do you each think are some of the most fruitful pathways
4266940	4276140	forward? Well, I would say the most fruitful pathway forward is to really take cognitive
4276140	4282620	psychology seriously. There's a lot of nice work recently trying to align things like chat
4282620	4287180	GPT, Wolfram Alpha plugins, the way that chat GPT can interface with different kind of modules.
4287980	4294300	The way of building a legitimate kind of AGI system doesn't necessarily have to be psychologically
4294300	4297980	reliant on the kind of modules that human beings have, but I think it will benefit from it. So
4297980	4305100	there have been some claims that large language models can maybe do all sorts of things. But I
4305100	4308620	think in the long run, it's most likely going to be the case that LLMs can do something very
4308620	4312780	important and very interesting, but it's only going to be one piece of the puzzle. So in fact,
4312780	4318780	even OpenAI CEO Sam Altman said last week that what we can do with LLMs has really kind of been
4318780	4325340	exhausted. We need new directions, new avenues and so on. I guess he was probably speaking to
4325340	4331260	investors more than linguistic students here, but I think he's also right. LLMs can do something
4331260	4336860	spectacular, but they're probably going to form a small part of the general AGI architecture,
4336860	4343100	right? If you want to think about AGI as a potential, potential goal here. So, you know,
4343900	4351340	I think a lot of the, so let me give me an example here. So Anna Ivanova, who's a very good
4351340	4355980	cognitive scientist, she has a paper recently arguing for a kind of modular architecture for
4355980	4360700	LLMs, which is a very nice framework, right? It's very cognitively plausible. It's exactly the
4360700	4364380	kind of thing that we should be pushing for. It's compatible with Howard Gardner's, you know,
4364380	4368620	notion of multiple intelligences and so on. But I think at the same time, just to finish this
4368620	4375180	comment, there was a tech talk last week, I think, or maybe a few days ago, where a lot of this stuff
4375180	4381740	can be conflated with AI hype in an unproductive way. So Greg Brockman from OpenAI, he gave one of
4381740	4386700	his, one of these big TED talks where he showed different plugins that chat GPD can do. I mentioned
4386700	4391580	Wolfram Alpert, right? But there's also things like image generation, Instacart shopping, where you
4391580	4397180	can get chat GPD to buy you things and what have you. And again, this takes you back to the idea
4397180	4401660	that multiple subsystems can do different sub-functions. So Brockman also showed an example
4401660	4408540	of giving chat GPD an Excel file, a CSV file from an archive database of academic papers,
4408540	4413900	where it just listed a bunch of papers and then titles and what have you, right? And he said that,
4414140	4420060	using chat GPD, it uses world knowledge to infer what the titles of the columns mean. So we understood
4420060	4425100	that title means the title of the paper. It understood that authors mean the number of authors
4425100	4430220	per paper. It understood that created means the date the paper was submitted, right? And because
4430220	4435500	it's a TED talk, the audience gave us a standing elevation, right? But the ability to describe
4435500	4442780	labels on an Excel file is, I guess, nice. But I'm not sure you'd really call it world knowledge.
4442780	4447980	So I guess, I would just say there's a lot of progress needs to be made alongside reducing
4447980	4452300	anthropomorphism. You have to have the right balance of it. So like I said, you have to have
4452300	4457260	the right balance of psychologically plausible kind of modular architecture, but you can't have too
4457260	4461820	much anthropomorphism because then you'll get carried away. You have to find, we have to find
4461820	4467100	the right balance between modeling kind of human-like modular systems, but not doing it
4467100	4471580	to a degree that is a bit implausible or scientifically unhelpful.
4475580	4478940	I mean, I think I agree with all of that. I'm really excited about these
4479660	4486380	ways of kind of connecting language models to other forms of information processing,
4486380	4491500	which does seem like what people have. I think I've been very surprised at the
4492460	4499980	the things they are able to do just as language modeling, right? So different kinds of reasoning
4499980	4507180	puzzles and things that they can solve, I think, is really fascinating and maybe will require us
4507180	4513740	to rethink the relationships between language and thought and try to figure out a way of being
4513740	4519500	specific about what it means for something to have a representation or to reason over that
4519500	4527180	representation. But ultimately, I think I agree that people have different modes of thinking about
4527180	4536060	things and that seems important for intelligence. I'm also super excited about the BabyLM challenge.
4536060	4543660	So I think on the kind of linguistic side, that's exactly the right thing of seeing how far we can
4543660	4553660	get with smaller data sets and maybe eventually after that trying to understand some more about
4553660	4559820	the kinds of semantics that kids acquire and where they get it from and how kind of external semantics
4559820	4568060	can inform language learning or specifically maybe grammar and syntax learning. I guess my other
4568940	4577180	path forward point would be that there's... I feel like these kinds of models have
4577180	4584940	have really gone far beyond people's expectations for this kind of class of model, right? Kind of
4584940	4593100	ground up statistical learning, discovering patterns in text seems to give really pretty
4593100	4599260	remarkable results. And that for me going forward, I think has just introduced a huge wave of
4599260	4605740	uncertainty over theories. So I think that our theories of basically everything in language for
4605740	4612460	sure, but cognition, probably neuroscience, like all of those things I think are going to be reworked
4612460	4619660	when we really come to kind of understand the ability of really general kinds of learning
4619660	4628380	systems like these. So that makes it on the one hand kind of a bummer for past theories,
4628380	4636620	especially theories which relied on learning not being able to work well. But on the upside,
4636620	4643260	I think it makes it a very exciting time both for AI and cognitive science and linguistics,
4643980	4649500	where now there's these really, really powerful tools that seem like a qualitatively
4650540	4656940	different size step towards human abilities. And I think kind of integrating them and taking
4657820	4663820	both the kind of engineering lessons and the kind of philosophical lessons about how they're made
4663820	4669260	and what kinds of principles go into designing intelligent systems. I think that those things
4669260	4676780	will really shape the field over the next five or 10 years. And also, I would just say in the
4676780	4680940	context of broader themes here, right, like you're totally right, like I remember when I was reading
4680940	4687820	about when Deep Blue, the Kasparov, was it, the chess thing, right? And there were some commentators
4687820	4693260	who said, you know, chess is over. If an AI can beat a human, then it's game over. What's the
4693260	4698860	point in studying chess? You know, there's no need of boring anymore. And I guess if AI has achieved
4698860	4702380	seemingly everything that humans need to do to play chess, what's the point of playing it?
4703180	4706620	But I think, you know, if anything, it turned out to increase the popularity of chess, right?
4706620	4711260	There are now many chess celebrities as well, worldwide tournaments. And I would predict that
4711260	4714860	the same is probably going to happen with language too. You know, LLMs do not mean
4714860	4718460	it's the end of language, no more language, no more linguistics. I would actually push back
4718460	4723340	and say maybe it would be the opposite. You know, the success of LLMs will increase general
4723340	4727900	interest in linguistic theory, due to their apparent, you know, weird constraints and apparent
4727900	4733100	limitations, right? Because I would also say, you know, scale, at this point, the chess issue,
4733740	4739420	scale is kind of definitely far from all that's needed. What is lacking is an ability of LLMs to,
4739420	4743580	you know, really abstract their knowledge and experiences in order to make robust predictions
4743580	4747340	and generalizations and so on. I gave some examples, but there's some others in the literature
4747340	4751100	where it doesn't seem to really be good at generalizing. It can kind of mimic particular
4751100	4756300	token types. But I would, you know, I would guess my final, my final thing would be that,
4756300	4761820	you know, the language acquisition literature doesn't necessarily need LLMs though. You know,
4761820	4766460	cognitive scientists don't really need LLMs. We could potentially, you know,
4766460	4770460	me and Stephen obviously disagree here, but I would say big tech companies
4770460	4774620	profiting off LLMs need LLMs, right? They're the only ones that really do. It may be the case
4774620	4779180	that the mind is a very, I will say, you know, the mind is a very diverse space. It may be that
4779180	4783260	there are certain forms of behavior and learning that might be captured by processes similar to
4783260	4786540	what LLMs are doing. So Stephen has given some interesting examples in his papers about
4786540	4791260	magnetism and we're kind of rules of learning that are very, very general and very quick and
4791260	4795900	very mysterious. So, you know, maybe for those sorts of things, that kind of learning will be
4795900	4800300	relevant. But I still think it's unlikely that one of the candidates will be natural language,
4801420	4805100	at least the way natural language works in its full glory in terms of the four meaning
4805100	4809740	regulation and what have you. So I guess I would, you know, it kind of reminds me of where you,
4810540	4814540	you know, you have this image of, I saw John with chapter four recently, right? And he has this,
4814540	4817580	there's this scene where he's walking in the desert and he's not sure if he's seen
4817580	4821740	this guy that he wants to assassinate. It's kind of like when you walk in the desert
4822700	4827100	and you have an illusion of seeing an oasis because it turns out you're hallucinating.
4827100	4830860	But then you realize that, you know, sometimes before it's too late that you actually are
4830860	4834460	hallucinating. It's you're not seeing an oasis. You're still in the desert. And I think that's
4834460	4839580	kind of maybe the situation we're in right now with linguistic competence of laws of language
4839580	4845500	models. We have the illusion of linguistic competence. But, you know, you always see the
4845500	4849660	illusion before you find the oasis, right? So I think, I think right now we're in the
4849660	4854540	hallucinating stage of the desert where we're seeing potential sparks of linguistic competence,
4854540	4859660	but it's still not very clear and I'm robust. And we haven't actually reached the oasis yet.
4860620	4868700	Um, just a rapid fire question. So see if you can give a short response. So
4869660	4875580	Sphinode, you know, writes question, is it correct to say that large language models have no priors?
4878620	4882700	Do large language models have priors? I'd say yes, they definitely do.
4883100	4890540	Um, and there, I think the difference to how people, you know, are used to thinking about
4890540	4895420	priors in Bayesian inference, for example, if you like write down a Bayesian statistical model,
4895420	4899740	you say like, you know, here's the parameters and here's what the priors are on the parameters.
4900780	4905020	Large language models, I think the priors are and maybe neural nets in general, I think that the
4905020	4909980	that the priors are much more implicit, right? So there's some functions which they find easier
4909980	4915500	to learn than other functions. And there's even some work trying to discover, you know,
4915500	4921020	some statement of what those kind of implicit priors are. But that's actually how I think about,
4923100	4926460	you know, comparison of different neural network architectures, right,
4927580	4932060	which is maybe something Elliot and I might agree on, right? Like you have to find priors which
4932060	4938220	allow them to learn the things that kids learn, right? And not all architectures will do that.
4938860	4943900	Even among architectures which are turn complete or capable of learning any kind of function,
4943900	4951500	not all of them will do it, even on kind of huge data set sizes. So I think of this sort of search
4951500	4958060	over neural net architectures as really one of a search over priors. But it's not priors or,
4958060	4961740	I mean, you could think of it as a search over universal grammar or something, right? But it's,
4961740	4967660	it's, it's not priors or universal grammar in the sense that people have talked about it as like
4967660	4971980	an explicit statement about what kinds of rules are allowed or an explicit statement about what
4971980	4976540	kinds of functions or high probability or something like that. It's all implicitly coded there.
4977260	4981660	Yeah, totally. I think, I think that's right. I mean, you know, the real question is reducing
4981660	4986860	the space of what those priors are like. And if it's anything remotely like what human beings
4986860	4991660	are doing, so LLMs like, I would, I would at least say that things like GPT-3 are an existence
4991660	4997260	proof of, you know, that building fully functioning syntactic categories from surface
4997260	5003500	distributional analysis alone is possible. That's, yes, that is correct. But, you know,
5003500	5009900	even so, I would say most syntacticians don't really believe that syntactic categories are innate.
5009900	5014780	So the prior issue is slightly less relevant here. It's the operations that are said to be innate.
5014780	5019900	So the, in the syntax domain, it's particular linguistic computations that are said to be innate
5019900	5023980	and categories themselves. In fact, even Charles Young has admitted in the last couple of years
5023980	5030060	that they are maybe innate, but maybe not. So people have given, I know of a relevant prize,
5030060	5034460	they are things like, you know, me and Gary Marcus have talked about compositionality.
5034460	5039340	That seems to be a big problem. So people have given chat GPT BBC news articles asking it to
5039340	5045740	compress it and then re-explain it. So one example I saw was Peter Smith 58 is being arrested on
5045740	5051580	charges of manslaughter and you get it to compress it and re-explain it. And it comes out as 58 people
5051580	5054860	are being charged with manslaughter. All right. That's a pretty clear example of a lack of
5054860	5059100	compositionality being built into whatever compression it's doing. And there's no example
5059100	5063580	where there's been, there's some examples of potential analogical reasoning. So in Bing chat,
5063580	5068700	you know, Bing has this, this chat function. The question is, is it just finding meta relations
5068700	5073180	that have already been documented by humans or is it genuinely creating new relations that the new
5073180	5080380	stuff that is being built. So, you know, someone asked through me a table comparing Jesus Christ
5081020	5087340	with the Nokia 9910, right, the cell phone Nokia 9910. And it said, you know, it compared the
5087340	5094540	release dates. It compared the size, the weight. It compared the CPU with Jesus's all-powerful
5094540	5099580	knowledge. It compared the memory of the phone with the all-knowing nature of God, right.
5099900	5104940	And it also, I think it said that they were both resurrected because the Nokia was re-released a
5104940	5109020	couple of times, right. So the Nokia. That sounds like a great answer. What's wrong with that?
5109020	5114620	Okay. It may be. It sounds a lot like analogical reasoning, but then it also had some quite weird
5114620	5119100	ones where it was like, you know, for the camera, it said, no, it just gave Jesus's description,
5119100	5123980	but it's not really what a camera is. There's some kind of things that look like analogical
5123980	5132060	reasoning, maybe, but it's unclear. Yeah. I think that sounds like an awesome answer to me.
5134620	5139100	I was going to say, like, you said large-language models learn they're an existence proof of part
5139100	5143580	of speech categories, but like, they don't just output part of speech categories, right. Like,
5143580	5151500	they have a lot of grammatical syntactic knowledge. And moreover, like, they have a lot of semantic
5151500	5157260	knowledge and probably some pragmatic knowledge. And, you know, they're not bad at translation.
5157260	5163020	And like, it's way more that they have discovered than just part of speech categories.
5164860	5168300	Well, sorry, I said syntactic. I'm sorry. It's like syntactic categories.
5169100	5172380	Right. Well, sorry. Yeah. Yeah. But they've discovered way more than that.
5172940	5181980	Yeah. I'm going to, as a teaser slash motivator for hopefully both of you to join again in the
5181980	5187660	future with or without other guests, a few of the exciting questions just for us to include in this
5187660	5191260	transcript. And then thank you both, Elliott and Stephen, for joining. So just a few of the last
5191260	5195900	questions that were asked, Juan asked, how do small transformers, Zhang et al. 2020,
5195900	5201260	compared with children learning language? 96 asked, what are your thoughts on implicit
5201260	5207420	priors versus animal instinct? Rojda asked, what constraints that space in LLMs, don't
5207420	5212300	they get there by training? So are they discovering it? That's not what they implement at the start
5212300	5219180	maybe. And there's many more questions. So I hope that we can all review and reread each other's
5219180	5225500	works and come together for 41.2 in some future time. Thank you, Elliott and Stephen,
5225500	5229980	for this excellent stream. Thank you, Dave. Thank you both. Yeah. Thank you so much.
5229980	5236860	Very well. Bye. See you.
