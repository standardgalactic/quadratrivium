1
00:00:00,000 --> 00:00:14,200
Hello and welcome, everyone. This is Active Inference Mathstream 9.1 on March 5, 2024.

2
00:00:14,200 --> 00:00:20,640
We're here with Jonathan Gord and we'll be discussing a variety of topics yet to be determined

3
00:00:20,640 --> 00:00:25,960
or are they? So thank you for joining and to you for any introduction and we'll really

4
00:00:25,960 --> 00:00:29,680
look forward to everyone's comments and questions. So thanks again for joining to you.

5
00:00:30,320 --> 00:00:35,040
Okay, well, yeah, thanks very much, Daniel, for the introduction and for inviting me to be here

6
00:00:35,040 --> 00:00:44,080
on Active Inference. I'm looking forward to a very, very fun discussion. So I don't have anything

7
00:00:44,080 --> 00:00:47,280
especially prepared to talk about, which is probably a good thing because it means we'll be

8
00:00:47,280 --> 00:00:51,840
able to extend the kind of the unstructured part of this for as long as possible. But I think just

9
00:00:51,840 --> 00:00:57,120
to give a little bit of context, I want to talk about an area where I think some things that I've

10
00:00:57,200 --> 00:01:02,400
been working on, some collaborators that might have been working on, that might have some kind of

11
00:01:02,400 --> 00:01:07,360
intersection of interest with things that, you know, Active Inference type people might care about,

12
00:01:07,360 --> 00:01:13,280
right? So, and in particular, that concerns the relationship between kind of computation,

13
00:01:13,280 --> 00:01:18,640
observation, and cognition, and specifically using methods that come from category theory and

14
00:01:18,640 --> 00:01:22,960
topos theory and some other kind of branches of mathematics and theoretical computer science

15
00:01:23,040 --> 00:01:28,320
to understand the relationship between system, specifically the computational and algorithmic

16
00:01:28,320 --> 00:01:33,440
complexity of systems versus the computational algorithmic complexity of observers of those

17
00:01:33,440 --> 00:01:38,480
systems and how those things trade off between each other. So, so just to give a little bit of

18
00:01:38,480 --> 00:01:43,920
context to that, I want to show these are just some visuals from a paper that I put out about a

19
00:01:43,920 --> 00:01:48,800
year ago now, and that this kind of really defines this research program that I've been working on

20
00:01:48,800 --> 00:01:53,360
for the last year and a half in some form or another, which is looking at exactly this trade-off

21
00:01:53,920 --> 00:01:58,560
using category theoretic machinery. So, here's a specification of a Turing machine. This is

22
00:01:58,560 --> 00:02:03,280
just a simple deterministic computation. It's saying, you know, you have a Turing machine that

23
00:02:03,280 --> 00:02:07,680
has this head state and this tape state, and on the next step, you're going to replace the tape

24
00:02:07,680 --> 00:02:10,720
state with something that looks like this, the head state with something that looks like that,

25
00:02:10,720 --> 00:02:14,560
and you're going to scroll the Turing machine head left, or in this case, scroll it right,

26
00:02:14,560 --> 00:02:19,600
etc. So, this is just a, you know, specification of a very simple computation. I think this is a

27
00:02:19,600 --> 00:02:23,920
two-state, two-color Turing machine on a simple, you know, one-dimensional tape. It's about as

28
00:02:23,920 --> 00:02:28,320
simple a computation as you could define. So, if you run that thing for some initial condition,

29
00:02:28,320 --> 00:02:34,400
you'll get an evolution that looks like this. And so, right now, this is just a purely deterministic,

30
00:02:34,400 --> 00:02:39,760
you know, single-path evolution. But from this, we can construct, we can build a mathematical

31
00:02:39,760 --> 00:02:45,520
structure. Namely, we can build a category. So, and the rules for how we build that category are

32
00:02:45,520 --> 00:02:50,400
very simple. So, you know, each arrow here is some simple computation, some application of

33
00:02:50,400 --> 00:02:54,640
the Turing machine transition function. And then what we can do is we can say, well, any time we

34
00:02:54,640 --> 00:02:58,800
have two arrows that are laid end-to-end like this, we can compose them together to create a

35
00:02:58,800 --> 00:03:03,520
third arrow that goes like that. I may even have a picture, yes, like this. So, you know, we have

36
00:03:03,520 --> 00:03:08,800
a computation f that takes us from x to y, a computation g that takes us from y to z, and we

37
00:03:09,040 --> 00:03:13,920
then we obtain a composite computation g compose f that takes us directly from x to z.

38
00:03:13,920 --> 00:03:19,680
And we also add some additional edges, some additional arrows on each state itself,

39
00:03:19,680 --> 00:03:24,640
a sort of identity, an identity operation that maps the computational state directly to itself.

40
00:03:25,360 --> 00:03:31,520
And so, this combined with some axioms of associativity and identity forms a category

41
00:03:31,520 --> 00:03:35,840
of elementary computations. So, this is a very, very simple example. But what I want to try and

42
00:03:35,840 --> 00:03:42,240
build up towards and kind of pump your intuition for is a category which I call comp, which is a

43
00:03:42,240 --> 00:03:48,400
category whose objects are all essentially the class of all data structures and whose arrows or

44
00:03:48,400 --> 00:03:53,120
morphisms are the class of all elementary computations. So, you start by just applying, you

45
00:03:53,120 --> 00:03:56,720
know, all possible computations or, you know, in this case, for the case of Turing machines,

46
00:03:56,720 --> 00:04:00,640
all possible, you know, Turing machine transition functions. And then you do this

47
00:04:00,640 --> 00:04:05,280
closure operation where you, you know, where you essentially do what I'm doing here, but you,

48
00:04:05,280 --> 00:04:10,080
you know, you allow those elementary computations to be composed together in arbitrary ways.

49
00:04:10,080 --> 00:04:15,600
And so, that gives you effectively a class of all possible programs. So, this category contains

50
00:04:15,600 --> 00:04:20,240
not only all possible data structures as objects, but all possible programs as morphisms. And this

51
00:04:20,240 --> 00:04:25,840
is a very rich category with some very interesting algebraic structure that we'll kind of, again,

52
00:04:25,920 --> 00:04:32,160
I'm sure we'll allude to in our subsequent discussion. But in a sense, when we do this,

53
00:04:32,160 --> 00:04:36,080
when we do this operation of taking what mathematically we call a transitive closure,

54
00:04:36,080 --> 00:04:41,280
right, where we allow two elementary computations to be composed together to produce a third,

55
00:04:42,320 --> 00:04:46,160
we are essentially kind of neglecting considerations of computational complexity,

56
00:04:46,160 --> 00:04:52,160
right, because, you know, this arrow here might correspond to one application of the Turing machine

57
00:04:52,160 --> 00:04:56,320
transition function, this arrow might correspond to another application of the transition function,

58
00:04:56,320 --> 00:05:00,480
but this composite arrow correspond might correspond to two applications of the transition

59
00:05:00,480 --> 00:05:06,480
function. And so, somehow, when we allow arrows or morphisms to be composed in this way,

60
00:05:06,480 --> 00:05:11,280
we're neglecting considerations of the complexity of operations. So, the question then is, you know,

61
00:05:11,280 --> 00:05:16,560
could we imagine constructing a generalization of category theory, which takes into account

62
00:05:16,560 --> 00:05:21,200
computational complexity. So, here's an example of how that would look, right. So, here you can see

63
00:05:21,200 --> 00:05:25,440
every edge, every morphism has been tagged with certain computational complexity information,

64
00:05:25,440 --> 00:05:30,480
in particular, it's been tagged with a number specifying what is the minimum number of applications

65
00:05:30,480 --> 00:05:34,640
of my transition function, what's the minimum number of elementary computations that I need in

66
00:05:34,640 --> 00:05:38,400
order to evolve from this data structure to this data structure. So, here, to go from here to here,

67
00:05:38,400 --> 00:05:42,800
it's just one, to go from here to here, it's one, to go from here to here, it's one, etc.

68
00:05:43,600 --> 00:05:48,480
But to go from here to here directly, it would be three, to go from here to here directly,

69
00:05:48,560 --> 00:05:52,880
it would be two. And, you know, just for convention, we say that the identity,

70
00:05:53,600 --> 00:05:57,440
the identity computation, the trivial computation always has complexity zero.

71
00:05:58,640 --> 00:06:03,200
And then this, so this is, again, a fairly simple mathematical structure, and you can

72
00:06:04,400 --> 00:06:10,080
construct this, again, using purely category theoretic technology by building a particular

73
00:06:10,080 --> 00:06:15,520
functor from the category of computations, and from the category of data structures and computations

74
00:06:15,600 --> 00:06:20,160
to a what's called a discrete co-borderism category. And again, we might discuss that later on if

75
00:06:20,160 --> 00:06:23,680
people are interested, but let me not get too bogged down into the technical details of how we

76
00:06:23,680 --> 00:06:29,040
do that. But once you've got this, it gives us immediately a very nice way of characterizing

77
00:06:29,040 --> 00:06:35,760
phenomena like computational irreducibility. So, there is this idea that has existed in some form

78
00:06:35,760 --> 00:06:39,120
or another since the very early days of theoretical computer science, since the days of, you know,

79
00:06:39,120 --> 00:06:44,720
girdle and turing and post and church and so on, but was given this term computational irreducibility

80
00:06:44,720 --> 00:06:52,160
by Stephen Wolfram, where the idea is essentially that you just, you know, intuitively, you describe

81
00:06:52,160 --> 00:06:56,240
a computation as being irreducible, or, you know, the result of the computation as being

82
00:06:56,240 --> 00:07:02,400
irreducibly complex, if it's not possible to shortcut it in any way, right? So, where, you

83
00:07:02,400 --> 00:07:06,400
know, it takes that computation takes a certain number of steps, and there does not exist a

84
00:07:06,400 --> 00:07:10,720
shorter computation that would give you the same answer in less time. And one of the nice

85
00:07:10,720 --> 00:07:14,800
features of thinking about computations and their complexity algebraically like this is that it

86
00:07:14,800 --> 00:07:19,040
gives you a purely algebraic characterization of irreducibility. In particular, what it says

87
00:07:19,040 --> 00:07:24,720
is that irreducible computations are ones for which the computational complexity acts

88
00:07:24,720 --> 00:07:31,840
additive, purely additively under composition. So, if it's the case that if we compose, say, two

89
00:07:33,680 --> 00:07:40,400
computations of complexity one together, if the resulting composite takes, you know, has complexity

90
00:07:40,960 --> 00:07:44,960
two, then it's an irreducible computation. If it has complexity less than two, like one,

91
00:07:44,960 --> 00:07:48,720
then that means that we could have jumped directly from the input to the output

92
00:07:48,720 --> 00:07:51,840
without having to pass through the two elementary computations that made it up. So, that would be

93
00:07:51,840 --> 00:07:56,240
an example of a reducible computation. So, reducible computations are ones whose complexities

94
00:07:56,240 --> 00:08:02,560
compose sub-additively in this category theoretic sense. And, okay, so here's an

95
00:08:04,000 --> 00:08:09,600
illustration of showing what intermediate computational states you had to go through

96
00:08:09,600 --> 00:08:12,800
in order to get from one data structure to another data structure. So, to go from here

97
00:08:12,800 --> 00:08:16,720
to here, you had to go through steps one to two. To go from here to here, you had to go through

98
00:08:16,720 --> 00:08:22,720
steps one, two, and one, two, three, and four, et cetera. So, you can build up a kind of complete

99
00:08:22,720 --> 00:08:27,200
algebra of complexity this way, which has some nice properties, which, again, I can talk about,

100
00:08:27,200 --> 00:08:32,480
but let me not get too bogged down in mathematical details right now. But here's the thing I really

101
00:08:32,480 --> 00:08:37,440
want to talk about, which is what happens when you go to multi-way systems. What happens when

102
00:08:37,520 --> 00:08:42,400
you go to non-deterministic computations? So, now, imagine having, instead of just a

103
00:08:42,400 --> 00:08:47,440
Turing machine with a single rule, a single transition function that just evolves deterministically

104
00:08:47,440 --> 00:08:50,800
with a single thread of time, now imagine having a Turing machine that has, say, two

105
00:08:50,800 --> 00:08:54,880
transition functions, like this one and this one. And so, at any given point, it can apply one of

106
00:08:54,880 --> 00:09:00,960
the two. And so, now, evolution, instead of just being a single path, becomes this kind of branching

107
00:09:00,960 --> 00:09:05,520
structure, which, if we didn't have any merging, would be a tree, but because we are merging

108
00:09:05,600 --> 00:09:11,120
equivalent states, it's actually just a kind of more general directed graph. And so, it looks

109
00:09:11,120 --> 00:09:16,240
like this, and this we call a multi-way system. And so, we can build a category out of these

110
00:09:16,240 --> 00:09:22,240
multi-way systems as well. We can build a category using exactly the same rules. So, again, we do

111
00:09:22,240 --> 00:09:28,240
this transitive closure operation. So, we add an edge for every possible composition of these

112
00:09:28,240 --> 00:09:33,760
elementary computations and an identity edge that maps every data structure to itself. But it turns

113
00:09:33,760 --> 00:09:39,440
out this category has even more structure than the single-way system that we showed previously,

114
00:09:39,440 --> 00:09:45,360
because now, it's possible to compose computations not just sequentially in time using ordinary

115
00:09:45,360 --> 00:09:50,240
composition, but it's possible to compose them in parallel across what is sometimes referred

116
00:09:50,240 --> 00:09:57,040
to as branchial space. So, essentially, you're saying instead of, you know, the ordinary morphism

117
00:09:57,040 --> 00:10:01,520
composition that I showed previously is essentially saying, you know, I apply this

118
00:10:01,600 --> 00:10:06,880
elementary computation, then this elementary computation sequentially. Whereas this parallel

119
00:10:06,880 --> 00:10:11,200
composition is saying, I apply this computation and this computation in parallel to the same

120
00:10:11,200 --> 00:10:16,240
data structure. And so, that parallel operation is what causes these branches, right? Effectively,

121
00:10:16,240 --> 00:10:21,040
when you have two threads of time that are branching from the same state, like here,

122
00:10:21,840 --> 00:10:27,120
that's arising because we have chosen to apply this elementary computation and this elementary

123
00:10:27,200 --> 00:10:34,320
computation together in parallel rather than sequentialized in time. And so, here you can see

124
00:10:35,040 --> 00:10:40,160
this parallelization indicated using what's referred to as a branchial decomposition,

125
00:10:40,160 --> 00:10:44,400
which is just a kind of a visual way of decomposing what's going on between these different threads

126
00:10:44,400 --> 00:10:49,200
of time. And again, there's a purely algebraic characterization of what's going on here,

127
00:10:49,200 --> 00:10:54,000
which is that what we've done is we've taken our simple category that we started with,

128
00:10:54,000 --> 00:10:57,840
and we've equipped it with a tensor product structure. And so, it's become what we fancily

129
00:10:57,840 --> 00:11:02,400
call a monoidal category or actually a symmetric monoidal category. So, the tensor, so we now

130
00:11:02,400 --> 00:11:07,200
have these two operations. We have sequential composition in time, and we have this tensor

131
00:11:07,200 --> 00:11:14,080
product operation, which is a parallel composition in branchial space. And just like we can have,

132
00:11:14,080 --> 00:11:19,120
just like before, where we equipped our edges, our morphisms with certain computational

133
00:11:19,440 --> 00:11:24,640
complexity information, we can do the same thing, and we described how those complexities composed

134
00:11:24,640 --> 00:11:28,880
sequentially in time. We can do the same thing and describe how the complexities compose in

135
00:11:28,880 --> 00:11:34,560
parallel as one composes morphisms in branchial space. And so, this allows one by exactly the

136
00:11:34,560 --> 00:11:39,120
same token to quantify multi-computational irreducibility rather than just computational

137
00:11:39,120 --> 00:11:44,000
irreducibility. So, now, multi-computational irreducibility becomes a measure of how additive

138
00:11:44,000 --> 00:11:48,640
or sub-additive your time complexities are when you compose them in parallel through the tensor

139
00:11:48,640 --> 00:11:54,640
products rather than just in sequence through standard morphism composition. Okay, but I promise

140
00:11:54,640 --> 00:11:58,560
I am cut, and so here's an analogous diagram to the one I showed before showing all the kind of

141
00:11:58,560 --> 00:12:08,960
intermediate steps that are being applied when one constructs computations or indeed multi-computations

142
00:12:08,960 --> 00:12:13,920
by composing elementary computations both sequentially in time and in parallel in branchial

143
00:12:13,920 --> 00:12:19,840
space. Now, but I promise I am the point that I'm trying to get to is that it turns out that in

144
00:12:19,840 --> 00:12:23,840
addition to just being a useful way to think about computational complexity theory and to formulate

145
00:12:23,840 --> 00:12:28,240
complexity classes like, you know, polynomial time on non-deterministic polynomial time, etc.,

146
00:12:28,240 --> 00:12:31,680
it turns out this is also an interesting way to think about the role of observation

147
00:12:32,560 --> 00:12:39,760
in sort of computational models of reality. Because so here's where I'm going to get a

148
00:12:39,760 --> 00:12:44,400
little bit philosophical, and I don't immediately have a slide or a graphic that I can show to

149
00:12:44,400 --> 00:12:49,040
illustrate this point. But so when we think about modeling a system computationally,

150
00:12:50,320 --> 00:12:53,600
one has to bear in mind that there are really two computations going on, right? There's the

151
00:12:53,600 --> 00:12:58,080
computation that the system is itself performing, and then there's the computation that the observer,

152
00:12:58,080 --> 00:13:02,720
the person who is measuring that system and concluding things from it, there's the computation

153
00:13:02,720 --> 00:13:08,720
that they are performing. And somehow, you know, so when we construct models of reality or when we

154
00:13:08,720 --> 00:13:14,000
construct models of systems, you know, and we want to describe kind of at a meta level what we're

155
00:13:14,000 --> 00:13:18,960
doing in computational terms, there's our own computation that, you know, that's going on

156
00:13:18,960 --> 00:13:22,720
inside our own internal representation of the world. And then there's presumably some external

157
00:13:22,720 --> 00:13:27,120
computation that's going on outside. And then when we make observations and when we make measurements,

158
00:13:27,120 --> 00:13:31,440
when we construct theoretical models, what we're doing is we're somehow constructing some kind

159
00:13:31,440 --> 00:13:36,640
of encoding function that allows us to take a concrete physical state of the system we're

160
00:13:36,640 --> 00:13:41,760
observing and encode it as some abstract state of the internal model that we have of what's going on.

161
00:13:42,640 --> 00:13:48,720
And that's all very well. But then one, but then now we don't just have one computation to care

162
00:13:48,720 --> 00:13:52,480
about, we have three, right? We've got the computation of the system, computation of the

163
00:13:52,480 --> 00:13:56,960
observer, and the computation of this encoding function computation that's responsible for their,

164
00:13:56,960 --> 00:14:02,400
for their, you know, the interface between their internal model of the world and the external

165
00:14:02,400 --> 00:14:07,200
reality. And the computational complexities of these computations into play in an extremely

166
00:14:07,200 --> 00:14:11,520
interesting way. And so the, you know, part of the reason for trying to develop this algebraic

167
00:14:11,520 --> 00:14:15,360
semantics for thinking about computational complexity and multi computational complexity

168
00:14:15,360 --> 00:14:21,120
was to try to give one a systematic way to reason about exactly this three-way interplay between

169
00:14:21,120 --> 00:14:27,840
systems, observers, and encoding functions. And so in particular, when we make when an

170
00:14:27,840 --> 00:14:31,920
observer makes a model of the world, one thing that they're doing is that they are, you know,

171
00:14:32,160 --> 00:14:36,160
for any, for any model, isn't just, you know, a complete description of reality,

172
00:14:36,160 --> 00:14:38,960
there's a certain amount of coarse-graining, right? There's a certain amount of

173
00:14:40,960 --> 00:14:44,640
taking a bunch of states that in the system itself are distinguished,

174
00:14:44,640 --> 00:14:49,120
but in the internal model are treated as the same, they're kind of, you know, they're cast in the

175
00:14:49,120 --> 00:14:53,840
same bucket. So in some sense, you know, how coarse a model is, is a measure of how much the

176
00:14:53,840 --> 00:14:58,400
encoding function fails to be subjective, right? And so again, there's a kind of algebraic or

177
00:14:58,400 --> 00:15:04,720
category theoretic characterization of what's going on, that, you know, the fewer of your

178
00:15:04,720 --> 00:15:10,320
morphisms are epimorphisms, the more coarse your model is, the more abstract or idealized

179
00:15:10,320 --> 00:15:16,480
your model of reality is. And so then the interesting thing is that this characterization

180
00:15:16,480 --> 00:15:21,520
of multi computational irreducibility, this measure of how additive or sub-additive your

181
00:15:21,520 --> 00:15:26,320
complexities are, as you compose them together in parallel, gives you a measure of the relative

182
00:15:26,320 --> 00:15:30,720
complexity of the evolution function, that is the function that evolves your computation

183
00:15:30,720 --> 00:15:35,440
forwards in time, versus the equivalence function, that is the function that declares that two

184
00:15:35,440 --> 00:15:40,160
computational states, two data structures, are to be treated as equivalent. And that interplay,

185
00:15:40,720 --> 00:15:45,680
I claim, is a kind of abstract meta way of thinking about the interplay between the

186
00:15:45,680 --> 00:15:49,760
computation of systems versus the computation of observers, because, you know, so in a sense,

187
00:15:49,760 --> 00:15:54,960
the role of the system is to evolve forwards in time, whereas the role of the observer is to take

188
00:15:55,760 --> 00:16:02,480
states in the system that are distinguished in reality and say, you know, subject to my

189
00:16:02,480 --> 00:16:07,200
idealized model, I'm going to treat these as the same. So the system is defining the evolution

190
00:16:07,200 --> 00:16:12,240
function, but the observer is defining this equivalence function. And so then the tradeoff

191
00:16:12,240 --> 00:16:20,320
in their complexities becomes exactly a tradeoff between what are the algebraic rules that describe

192
00:16:20,320 --> 00:16:24,080
the complexities as they compose sequentially, versus the algebraic rules that describe the

193
00:16:24,080 --> 00:16:29,840
complexities as they compose under this tensor product operation. And so I've shown this in

194
00:16:29,840 --> 00:16:35,040
particular for Turing machine systems, but this is a very general kind of algebraic semantics,

195
00:16:35,040 --> 00:16:38,320
you can apply it to hypergraphs, you can apply it to combinators, lambda calculus,

196
00:16:38,320 --> 00:16:45,040
doesn't matter. In a sense, there is just one category up to isomorphism of data structures

197
00:16:45,040 --> 00:16:48,880
and computations, and there are simply many different ways of parameterizing what that

198
00:16:48,880 --> 00:16:53,280
category is doing through things like Turing machines or hypergraphs or whatever. The

199
00:16:53,280 --> 00:16:57,760
algebraic formalism transcends the particular details of the computations that one's dealing with.

200
00:16:58,560 --> 00:17:03,360
And so yeah, as I say, what one ends up with is, I think, a fairly general formalism for

201
00:17:03,360 --> 00:17:09,440
thinking about the interplay between observers and the systems that they observe. And that gives

202
00:17:09,440 --> 00:17:14,720
one a, I promise I'll stop monologuing in a moment and we'll try and pick apart what I'm

203
00:17:14,720 --> 00:17:20,240
really talking about here. But so I'll just conclude with, you know, once one has that

204
00:17:20,240 --> 00:17:25,760
algebraic semantics, a whole bunch of things which I think previously would have been, at least to

205
00:17:25,760 --> 00:17:30,640
me, previously seemed like kind of fundamental confusions about, you know, how scientific

206
00:17:30,640 --> 00:17:34,720
observation works and how it interplays with computational models, those confusions kind of

207
00:17:34,720 --> 00:17:38,960
become much easier to clarify once you think about it in this kind of more compositional way. So

208
00:17:40,080 --> 00:17:45,520
to give a very simple example, or kind of very degenerate example,

209
00:17:45,920 --> 00:17:54,000
you can, you know, within this algebraic semantics, you can effectively trade off the

210
00:17:54,800 --> 00:17:58,640
computational complexity of the system for the computational complexity of the observer, right?

211
00:17:58,640 --> 00:18:06,160
So you can have, you can have kind of, in effect, two degenerate cases. You can have the case where

212
00:18:06,160 --> 00:18:11,280
the system itself has a completely trivial evolution function. The system itself has, you

213
00:18:11,280 --> 00:18:16,640
know, is doing something completely elementary in its, in how it evolves. But then the observer

214
00:18:17,280 --> 00:18:20,880
has some incredibly complicated equivalence function that makes the system look like it's

215
00:18:20,880 --> 00:18:24,080
doing something really complicated, even though what it's actually doing is something very simple.

216
00:18:25,360 --> 00:18:29,440
And so then you, so you have the phenomenon where actually kind of all of the complexity is in the

217
00:18:29,440 --> 00:18:33,680
eye of the, is in the eye of the observer. You can also have the other degenerate case where the

218
00:18:33,680 --> 00:18:37,840
observer is doing something absolutely trivial, where the, you know, the encoding function or the

219
00:18:37,920 --> 00:18:41,600
observer's own internal representation is just an identity function or something. So there's no

220
00:18:41,600 --> 00:18:45,440
complexity there, but the system is doing something incredibly complex. It's doing some,

221
00:18:45,440 --> 00:18:49,920
some totally, some really sophisticated universal computation. And so that will also appear very

222
00:18:49,920 --> 00:18:56,000
complex to that observer. And so, and you can also have any kind of, any intermediate, you know,

223
00:18:56,000 --> 00:19:02,560
there's this vast interstitial space between these two extremes. And so one thing that's kind of

224
00:19:02,560 --> 00:19:08,320
always, one sort of philosophical problem that I've always kind of been interested in ever since

225
00:19:08,320 --> 00:19:14,720
I was a kid, which is this, this sort of, this tension between empiricism versus rationalism,

226
00:19:14,720 --> 00:19:19,600
right? You know, the question of, you know, on the white, if you look back at the history of,

227
00:19:19,600 --> 00:19:23,440
where, you know, early European philosophy, or, you know, it's certainly Western, you know,

228
00:19:23,440 --> 00:19:28,160
Western post enlightenment philosophy, you had people like, you know, Descartes and Leibniz and,

229
00:19:28,240 --> 00:19:32,960
and so on, who were, you know, in a more sophisticated way, Bishop Barkley with subjective

230
00:19:32,960 --> 00:19:36,320
immaterialism, who were trying to push for this idea that, oh, you know, all the, all the

231
00:19:36,320 --> 00:19:39,920
sophistication is what's going on inside the observer's head. And, you know, what goes on in

232
00:19:39,920 --> 00:19:43,520
reality is somehow secondary. And then you had people like, you know, Locke and Hume and the

233
00:19:43,520 --> 00:19:47,040
empiricists, who were saying, no, no, we should try and get the observer as much out of the picture

234
00:19:47,040 --> 00:19:51,040
as possible. And we should say all the sophistication is going on kind of in the external world.

235
00:19:51,040 --> 00:19:55,040
And this in one, you know, one nice consequence of this is it gives one, one nice consequence of

236
00:19:55,040 --> 00:19:59,520
this formalism is it gives one actually an algebraic way of kind of parameterizing this

237
00:19:59,520 --> 00:20:04,880
spectrum from rationalism to empiricism, right, that, that you can, you can choose the rationalist

238
00:20:04,880 --> 00:20:10,720
extreme where, you know, you know, you just have some, some space of all possible computations and,

239
00:20:10,720 --> 00:20:14,960
and the observer is doing all of the work to try to narrow down to a particular one,

240
00:20:14,960 --> 00:20:19,840
or you can have the kind of empiricist extreme where, you know, the observer is a completely

241
00:20:19,840 --> 00:20:24,800
elementary system. And, you know, and everything and everything they observe is just being built

242
00:20:24,800 --> 00:20:30,240
up from a kind of bottom up construction, or you can have anything in between. And in a sense,

243
00:20:30,240 --> 00:20:34,880
we now have, I think, the beginnings of a mathematical theory that explain, that's able

244
00:20:34,880 --> 00:20:40,320
to explain how those complexities trade off in a very direct way. So I think there's potentially

245
00:20:41,040 --> 00:20:46,400
places of mutual interest there in kind of the, in thinking about, yeah, as I say, cognition,

246
00:20:46,400 --> 00:20:50,080
observation, measurement, scientific modeling, and so on, in fundamentally computational terms.

247
00:20:50,080 --> 00:20:54,720
So I think that hopefully that will provide some, some context for, for a discussion.

248
00:20:58,720 --> 00:21:05,680
Thank you. Great opening. There's so many places to spin in and jump through.

249
00:21:07,200 --> 00:21:11,600
I guess I'll start with the two things I wrote down were unity is plural and at

250
00:21:11,600 --> 00:21:16,480
minimum two and beauty is in the eye of the beholder and the way that these kinds of pieces

251
00:21:16,480 --> 00:21:23,280
of timeless wisdom that describe that fundamentally relational component to observers in systems,

252
00:21:23,280 --> 00:21:27,760
which are not all kinds of systems per se, but those kinds of systems, those things are true for,

253
00:21:28,400 --> 00:21:35,200
and then the way in which along the formalism that you described with the system observer encoding

254
00:21:36,000 --> 00:21:42,320
freeway partition, and then the way that in free energy principle and the particular physics,

255
00:21:42,320 --> 00:21:48,640
that interface gets broken out from the agent's perspective into the incoming sensory and the

256
00:21:48,640 --> 00:21:56,480
outgoing action. So then that results in the fourfold particular partition. So maybe just to kind of,

257
00:21:57,440 --> 00:22:04,160
well, there, how do we partition the, from a category theory perspective or however,

258
00:22:04,160 --> 00:22:11,680
the action perception loop or the engagement loop? Like, how do we make a topology or

259
00:22:11,680 --> 00:22:19,120
compare a contrast different topologies and flows over this kind of seemingly pervasive or universal

260
00:22:19,120 --> 00:22:25,600
interface like concept? That's a fascinating question. So I don't have the answer and this is

261
00:22:25,600 --> 00:22:31,280
maybe a place where, where both you Daniel and perhaps David may have, you know, useful perspectives

262
00:22:31,280 --> 00:22:37,760
on this because, you know, I'm, I'm, you know, I read Carl's work a few years back and so I have

263
00:22:37,760 --> 00:22:42,080
some familiarity with the terms, but I'm by no means a kind of, you know, an expert on free

264
00:22:42,080 --> 00:22:46,160
energy principle or, you know, active inference and those kinds of things. But I think it's a very

265
00:22:46,160 --> 00:22:50,560
good point that you raise. And so, you know, I should begin by just being honest and say that,

266
00:22:50,560 --> 00:22:55,200
you know, everything I'm doing, you know, all that I just described is of course an idealization

267
00:22:55,200 --> 00:22:59,200
and that, you know, in reality, you know, in particular, it's an idealization, which I think

268
00:22:59,280 --> 00:23:03,680
you were very right, Daniel, to kind of pick up on. It's an idealization in which we say the

269
00:23:03,680 --> 00:23:07,360
observer is completely kind of non-interacting with the world somehow, right? That, you know,

270
00:23:07,360 --> 00:23:12,080
in a sense that there's just input coming in and nothing, nothing coming out. But of course,

271
00:23:12,080 --> 00:23:15,600
we know that's not really how observation works. Observation is necessarily a kind of two-way

272
00:23:15,600 --> 00:23:20,480
process. And so what's needed is not just this kind of very clean algebraic semantics that I've

273
00:23:20,480 --> 00:23:23,760
described here, which assumes that there's a essentially a one-way function from the world

274
00:23:23,760 --> 00:23:28,640
to the observer, but actually something more like a kind of second-order cybernetics description

275
00:23:29,440 --> 00:23:34,240
of, you know, what's really going on where you have, you know, first-order and second-order

276
00:23:34,240 --> 00:23:37,760
interactions, whereas exactly as you say, you can get these sort of feedback loops from

277
00:23:37,760 --> 00:23:42,160
observation to action and back again, which are probably, which is, I mean, still an idealization,

278
00:23:42,160 --> 00:23:46,880
but probably a more realistic idealization for how real observers and real measurement apparatus

279
00:23:46,880 --> 00:23:52,160
work. So I just want to begin by saying that I don't know, right? And, you know, the question of

280
00:23:52,160 --> 00:23:56,160
how this formalism into plays with things like second-order cybernetics and other areas where

281
00:23:56,160 --> 00:24:00,720
I know these kinds of questions have been explored, that's something I'm very interested to find out

282
00:24:00,720 --> 00:24:10,640
about, you know, going forward. But I think, hmm, okay, so, yeah, so at some point, maybe you could

283
00:24:11,440 --> 00:24:15,440
help me understand, you know, potentially where things might fit in with, you know,

284
00:24:15,440 --> 00:24:23,920
with the kind of broader active inference framework. I'm not sure I necessarily have that

285
00:24:23,920 --> 00:24:33,120
much more to comment on than that. Yeah, other than to say that, you know, in a sense, okay,

286
00:24:33,120 --> 00:24:41,680
so maybe, you know, one further comment is that, because, I mean, you asked specifically about

287
00:24:41,680 --> 00:24:46,800
how the kind of compositional category theoretic perspective might be useful. So I don't think

288
00:24:46,800 --> 00:24:49,600
category theory in itself is going to be the complete answer. I think it will be category

289
00:24:49,600 --> 00:24:53,120
theory augmented with some other things, computational complexity, probably second-order

290
00:24:53,120 --> 00:24:58,640
cybernetics, and some other things that I may not be aware of. But one place where I think that

291
00:24:58,640 --> 00:25:05,280
viewpoint is useful, at least on a philosophical level, is the idea that comes about, that you

292
00:25:05,280 --> 00:25:09,360
really obtain by studying mathematical structures in a category theoretic way, which is that the

293
00:25:09,360 --> 00:25:16,400
identity of something, you can define it both in terms of its intrinsic properties, or you can

294
00:25:16,400 --> 00:25:20,320
define it in terms of, you know, the stuff that you can do to it, right? So, you know, this was

295
00:25:20,320 --> 00:25:24,080
really the transition that happened in the foundations of mathematics as a result of people

296
00:25:24,080 --> 00:25:29,680
like Samuel Alenberg and Saunders-McLean. So, you know, category theory has its origins in this

297
00:25:29,680 --> 00:25:33,840
sort of slightly obstruous branch of algebraic topology. It was, you know, initially developed

298
00:25:33,840 --> 00:25:38,880
by people like Alexander Gordon-Deek and John P. S. Sear for doing homological algebra for,

299
00:25:38,880 --> 00:25:42,880
you know, for reasoning about sort of the algebraic structure of topological spaces. But then later,

300
00:25:42,880 --> 00:25:46,800
in the, I think, 1960s, 1970s, these two American mathematicians, Alenberg and McLean,

301
00:25:46,800 --> 00:25:50,640
realized that it was useful not just for thinking about topology, but for thinking about kind of

302
00:25:50,640 --> 00:25:54,720
mathematical structure in general. And then later on applied category theorists started saying,

303
00:25:54,720 --> 00:25:59,200
well, maybe it's useful for just thinking about structure in general. But, you know, the key kind

304
00:25:59,200 --> 00:26:05,520
of conceptual or philosophical shift that it imposes is, you know, historically, thanks to the work

305
00:26:05,520 --> 00:26:09,920
of people like Kantor and Frege and Russell and so on, people have thought about mathematical

306
00:26:09,920 --> 00:26:14,000
structures in the foundations of mathematics as, you know, in terms of set theory. And the idea

307
00:26:14,000 --> 00:26:18,240
in set theory is you have this, you know, things like the axiom of extension that effectively say

308
00:26:18,240 --> 00:26:23,520
set is defined by what's inside it, right? So in other words, you know, you, a mathematical

309
00:26:23,520 --> 00:26:27,680
structure obtains its identity by, you know, you break it apart and you look at what's inside.

310
00:26:29,040 --> 00:26:34,320
In category theory, it's a completely different view. The view instead is you say, well, no,

311
00:26:34,320 --> 00:26:38,080
you can't look inside, you know, it's a fundamental rule of category theory that you can't look inside

312
00:26:38,080 --> 00:26:43,680
an object. You know, it's internal structure, if it has any sort of outer bounds to you.

313
00:26:43,680 --> 00:26:49,040
And instead, you give that object identity in terms of how it relates to other objects

314
00:26:49,040 --> 00:26:53,040
of the same type, right? So in other words, you know, you can ask, you know, what can I do to

315
00:26:53,040 --> 00:26:57,680
this? What functions can I apply to it? What functions can I apply to something else that

316
00:26:57,680 --> 00:27:02,320
map into this? So, you know, if I want to define, I don't know, the real numbers or the integers

317
00:27:02,320 --> 00:27:06,320
or something in the set theory, you know, from a set theory perspective, you would say that the,

318
00:27:06,400 --> 00:27:10,720
you know, the essence of the real numbers are all the numbers that are inside that set or all

319
00:27:10,720 --> 00:27:15,440
the numbers that are inside are. Whereas the category theory perspective is no, the essence

320
00:27:15,440 --> 00:27:19,440
of the real numbers are all the functions that you can define that take real numbers to some

321
00:27:19,440 --> 00:27:23,680
other number system or real numbers to themselves or that take some other number system into the

322
00:27:23,680 --> 00:27:28,640
real numbers or et cetera. And, you know, some of the deepest results in category theory, like the

323
00:27:28,640 --> 00:27:34,880
Oneida Lemma and other things, are telling one in some very precise sense that these two perspectives

324
00:27:34,880 --> 00:27:40,000
are really the same at some fundamental level, but that, you know, identifying an object based

325
00:27:40,000 --> 00:27:44,000
on its internal structure, based on breaking it apart and asking what's inside and identifying

326
00:27:44,000 --> 00:27:49,280
an object by asking, what can I do to it? And what, you know, what can this object be transformed

327
00:27:49,280 --> 00:27:53,680
into and what things can be transformed into this object? Those give you exactly the same

328
00:27:53,680 --> 00:27:57,440
information. It's far from obvious that that's true, you know, the Oneida Lemma is a very kind of,

329
00:27:58,560 --> 00:28:02,240
one of those results where you can never quite work out of its obvious or if it's incredibly,

330
00:28:02,240 --> 00:28:06,720
you know, mysterious. But I tend to fall on the side that it's incredibly mysterious. It's far

331
00:28:06,720 --> 00:28:11,440
from evidence that those two perspectives would really be the same. And yet, the point you're

332
00:28:11,440 --> 00:28:18,800
making Daniel, I think, is that in a sense, historical ways of thinking about scientific

333
00:28:18,800 --> 00:28:22,960
observation have tended towards the set theoretic viewpoint, tended towards the viewpoint that we

334
00:28:22,960 --> 00:28:26,160
understand systems based on kind of breaking them apart into constituent components.

335
00:28:27,200 --> 00:28:31,440
But perhaps a more realistic view is something more like the category theory perspective,

336
00:28:31,440 --> 00:28:36,560
where we say, you know, I understand a system by interacting with it, right? By asking,

337
00:28:36,560 --> 00:28:40,880
what can I do to it? And how does it behave as I, when I perform certain operations to it?

338
00:28:40,880 --> 00:28:44,880
And that's a fundamentally, you know, that's a fundamentally two-way process that involves not

339
00:28:44,880 --> 00:28:49,520
just passive observation, but also kind of active participation. And somehow we need to develop

340
00:28:49,520 --> 00:28:53,440
a formalism that kind of incorporates those two elements. And maybe, you know, maybe it already

341
00:28:53,440 --> 00:28:58,160
exists. And it exists in this large literature tree of which I'm largely unaware. That's partly

342
00:28:58,160 --> 00:29:01,920
why I want to be here to try and find out, you know, what things I missed, so to speak.

343
00:29:03,520 --> 00:29:11,440
Well, all right, a few points. Self-evident is far from evident. Also, I tend to the mysterious,

344
00:29:11,440 --> 00:29:16,720
which is to say, saying more with less, especially for these frameworks, because they're less

345
00:29:16,720 --> 00:29:24,240
opinionated, so that their space of internal semantics can be larger. And then that description

346
00:29:24,240 --> 00:29:29,040
that you provided with the relationship between the set and the category theory. So I kind of

347
00:29:29,040 --> 00:29:36,160
summarized it as set is to essential inclusion as category is to relational function. Now,

348
00:29:36,160 --> 00:29:44,720
if our concept of organismality or of action in the niche is constructive compositional material,

349
00:29:45,840 --> 00:29:51,520
then we are looking for, like, what is in or out? Is the microbiome in? Is the

350
00:29:52,160 --> 00:29:56,960
pheromone in the ant colony in or out of that thing? Because it's looking for, like, a static

351
00:29:56,960 --> 00:30:05,200
material answer. And then in contrast, the other side of that coin highlights the dynamic,

352
00:30:05,200 --> 00:30:10,480
like, whatever it is that self-organizing of the tornado is the tornado, whatever it is that

353
00:30:10,480 --> 00:30:17,200
self-organizing for the ant is the ant. And then also this, like, hint slash

354
00:30:18,160 --> 00:30:24,400
mobius strip or something that those two in the moment are indistinguishable.

355
00:30:25,280 --> 00:30:29,680
And yet systems that we choose to define one way or another, or keeping both open,

356
00:30:30,480 --> 00:30:36,240
those design decisions do make all the difference, even if for real systems, as they're observed,

357
00:30:37,040 --> 00:30:44,960
there's indistinguishability. Right, right. I think that's a very important point. And one which,

358
00:30:45,920 --> 00:30:52,000
I mean, this is a, it's always a kind of concern I have whenever I start thinking about, you know,

359
00:30:52,000 --> 00:30:58,400
embodied cognition or, you know, extended phenotype type ideas, right? But in a sense, you know,

360
00:30:59,280 --> 00:31:03,760
if what one is trying to do here is construct a kind of formalistic model of observation or

361
00:31:03,760 --> 00:31:08,960
of cognition or something, then as a kind of first order approximation, one has to start by

362
00:31:08,960 --> 00:31:12,880
somehow decomposing the world into observers and systems. But of course, that, you know,

363
00:31:12,960 --> 00:31:17,200
we know that that decomposition is somehow arbitrarily imposed, right? And that, you know,

364
00:31:19,040 --> 00:31:22,720
if you take these things to their extremes, and you say, you know, you allow essentially

365
00:31:22,720 --> 00:31:26,320
everything that the agent is interacting with to be considered, you know, like,

366
00:31:26,320 --> 00:31:29,760
not just the microbiome, as you say, but also, you know, tools that they construct or

367
00:31:29,760 --> 00:31:33,840
environments in which they exist and so on. If you, as you start to consider all of that to be

368
00:31:33,840 --> 00:31:37,840
a component of that organism, you know, of that agent's phenotype, which is a completely reasonable

369
00:31:37,840 --> 00:31:45,120
thing to do, then, and you start to, you know, you start to say, okay, well, their cognitive

370
00:31:45,120 --> 00:31:49,120
processes are not just localized to their brain or their spinal column, but are kind of somehow

371
00:31:49,120 --> 00:31:53,280
extended to, you know, the computers they use, the paper they write on, the books they read,

372
00:31:53,280 --> 00:31:59,920
et cetera. Again, perfectly reasonable thing to do and sort of somehow more descriptive of what's

373
00:31:59,920 --> 00:32:04,640
really going on. But my fear is always, if you take that too far, then, you know, you end up

374
00:32:04,640 --> 00:32:08,960
destroying the whole assumption that the idealization was based on, which is that you can neatly

375
00:32:08,960 --> 00:32:14,400
decompose, you know, the world into observers and systems. And so, I always get a bit nervous

376
00:32:14,400 --> 00:32:20,320
when thinking about that, that it's like, yes, you know, in a sense, you know, we know this is an

377
00:32:20,320 --> 00:32:27,040
approximation and we know that that approximation is not really true, but how, you know, how much can

378
00:32:27,040 --> 00:32:31,600
you afford to sort of loosen your grip on that approximation before the whole thing just kind

379
00:32:31,600 --> 00:32:35,760
of falls apart. I don't really know the answer to that question. But I think it's an interesting one.

380
00:32:35,760 --> 00:32:41,760
Yeah. How about ask some questions from chat and then give your first thoughts and then we'll see

381
00:32:42,560 --> 00:32:47,360
maybe where that kind of lands with further questions or how it connects to active inference.

382
00:32:47,360 --> 00:32:51,280
So that sounds good. Oh, by the way, should I keep my screen share on or should I?

383
00:32:51,280 --> 00:32:57,840
Yeah, we might want to go to a figure. So it's fine. Okay. Yeah. All right. Quantum Bell wrote,

384
00:32:58,720 --> 00:33:01,680
how does this help us reason about causality?

385
00:33:03,440 --> 00:33:09,040
That's a fascinating question. Okay. So that's that's another major aspect of, you know, why I

386
00:33:09,040 --> 00:33:15,440
think this research program is exciting, because so again, this is something where I'm interested

387
00:33:15,440 --> 00:33:18,880
to get the kind of active inference perspective, because I know this again, it's a topic in which

388
00:33:18,880 --> 00:33:26,480
much has been written and I'm largely ignorant. But yes, so one question you could ask is,

389
00:33:26,960 --> 00:33:29,920
yeah, if you have a description of a computation like this, like,

390
00:33:30,720 --> 00:33:34,720
let's go back up to the Turing machine case, the single way Turing machine case that's

391
00:33:34,720 --> 00:33:37,760
relatively easy to analyze, although still far from obvious what's going on.

392
00:33:38,480 --> 00:33:42,800
So suppose you have a computation of this kind, and you want to ask, what is its causal structure?

393
00:33:42,800 --> 00:33:46,960
In other words, you know, for each edge, each time I'm applying this Turing machine transition

394
00:33:46,960 --> 00:33:52,000
function, can I construct some kind of graph, you know, some some directed graph representation that

395
00:33:52,000 --> 00:33:57,600
tells me how these events are linked together. So in the, within the original research program,

396
00:33:58,560 --> 00:34:02,240
the so-called Wolfram model research program that kind of started a lot of these investigations,

397
00:34:02,240 --> 00:34:05,200
we were looking at this all the time, right? We were looking at kind of, you know, taking

398
00:34:05,200 --> 00:34:09,520
computations and looking at that causal structure and trying to, you know, infer things about what

399
00:34:09,520 --> 00:34:14,320
was going on about the, you know, the semantics of the computation based on causal relationships.

400
00:34:14,320 --> 00:34:17,600
And at a certain point, I started to realize, and I think other people had realized this before

401
00:34:17,600 --> 00:34:22,960
I did, but I'm often slow to pick these things up. I, I, I and other people started to realize that

402
00:34:22,960 --> 00:34:29,520
the notion of causality we were using was kind of nonsense. I mean, it was not completely hopeless,

403
00:34:29,520 --> 00:34:33,680
but it wasn't really causality, or it couldn't really be called causality in any, in any definite

404
00:34:33,680 --> 00:34:40,160
sense. So what do I mean by that? So first of all, it was a very technical problem. So if you're

405
00:34:40,160 --> 00:34:44,160
looking at something like a Turing machine evolution or a hypergraphic writing system as we were,

406
00:34:45,120 --> 00:34:49,920
then there's a very tempting and apparently obvious natural definition of causality that

407
00:34:49,920 --> 00:34:54,480
you can use, which is to ask, you know, when you split the world up into events that take,

408
00:34:54,480 --> 00:34:58,480
you know, some part of your data structure as input and output, you know, some other part of a

409
00:34:58,480 --> 00:35:04,480
data structure as output, then you can very easily ask, well, does the output of one event

410
00:35:04,480 --> 00:35:09,600
intersect with the input of another event? So if I show the hypergraph example, it's perhaps easier

411
00:35:09,600 --> 00:35:15,040
to see. So you have a hypergraphy writing rule that looks like this, right? So you know, you have,

412
00:35:15,040 --> 00:35:18,240
you say, if I have a piece of hypergraph that looks like that, I replace it with another piece of

413
00:35:18,240 --> 00:35:22,880
hypergraph that looks like this. So at each, each time you apply an event, you can think of that

414
00:35:22,880 --> 00:35:28,320
event as, you know, ingesting certain hyper edges and kind of, you know, replacing them with others.

415
00:35:28,320 --> 00:35:32,560
So you can divide it up into a sort of the, the, the input hyper edges that are being ingested

416
00:35:32,560 --> 00:35:37,200
versus the output hyper edges that are being produced. And so then you can ask, well, do the

417
00:35:37,200 --> 00:35:44,640
output, did I use, did I subsequently ingest in some future event hyper edges that were output

418
00:35:44,640 --> 00:35:48,320
in some previous event? Well, if the answer is yes, then pretty obviously that future event

419
00:35:48,320 --> 00:35:52,080
couldn't have occurred unless the previous event had already occurred. So then you could say, well,

420
00:35:52,080 --> 00:35:56,480
then one of those events causes the other. So in general, you could say that two, you know,

421
00:35:56,480 --> 00:36:03,120
an event A causes event B, if it's the case that the output, that the collection of tokens that

422
00:36:03,120 --> 00:36:07,200
was produced in the output of event A has a non-zero intersection with the collection tokens

423
00:36:07,200 --> 00:36:11,360
that were ingested as part of the input of event B. And that's a very tempting, very natural

424
00:36:11,360 --> 00:36:15,920
definition of kind of causality in these systems. Turns out it doesn't really work. I mean, it works

425
00:36:16,720 --> 00:36:20,800
pretty well, but there are cases in which it fails and it fails pretty spectacularly. And

426
00:36:20,800 --> 00:36:24,960
so the kind of canonical case where it fails spectacularly is that you can have events that

427
00:36:24,960 --> 00:36:29,040
don't actually do anything, right? You can have events that just kind of touch an edge, touch a

428
00:36:29,040 --> 00:36:34,720
token and output it again unchanged, but maybe, you know, it modifies the name, it modifies the

429
00:36:34,720 --> 00:36:38,880
identifier, but it doesn't actually change anything about the structure of the hypergraph or the

430
00:36:38,880 --> 00:36:43,440
Turing machine state or whatever. So pretty obviously that event doesn't matter. It shouldn't

431
00:36:43,440 --> 00:36:48,720
be causally related to anything in the future. But because it ingested the edge and then didn't do

432
00:36:48,720 --> 00:36:52,880
anything, you know, did some identity operation, but then, you know, produced it in the output again,

433
00:36:52,880 --> 00:36:56,880
it will kind of register as being causally related to any future event that used that

434
00:36:56,880 --> 00:37:01,200
edge, even though it didn't make any difference. That's just one very obvious example. There are

435
00:37:01,200 --> 00:37:05,440
other cases where it became clear that whatever this thing was, whatever this algorithm was

436
00:37:05,440 --> 00:37:10,080
detecting, it wasn't really causality. So I tried to think about, you know, what's a more sensible

437
00:37:10,080 --> 00:37:14,320
definition of causality. And I started working on things to do with, you know, a slightly kind of

438
00:37:14,320 --> 00:37:19,440
blockchain inspired ideas where you say, okay, well, rather than just arbitrarily assigning,

439
00:37:19,440 --> 00:37:24,720
you know, identifiers to these tokens every time they're created, what if I recursively

440
00:37:25,600 --> 00:37:29,920
construct the identifier of the token based on its causal history. So in other words, each token,

441
00:37:29,920 --> 00:37:35,520
like each hyper edge or each state in my each, each tape square on my Turing machine tape,

442
00:37:36,400 --> 00:37:41,360
I, the identifier is not just some random number that gets generated by my algorithm,

443
00:37:41,360 --> 00:37:47,200
but instead its identifier is a directed graph representation of its complete causal history.

444
00:37:47,200 --> 00:37:52,720
Well, then kind of recursively, it's identified can only change if the causal history was updated.

445
00:37:52,800 --> 00:37:56,400
And so you don't end up with these kind of spurious causal relations that I described before.

446
00:37:56,400 --> 00:38:01,600
So that seemed like one tempting way of resolving this problem. But then I realized,

447
00:38:01,600 --> 00:38:04,880
actually, there's a much more fundamental problem. There's a problem, there's a philosophical problem

448
00:38:04,880 --> 00:38:09,600
with the way that we're thinking about causality, which is that it's not really, you know, so,

449
00:38:10,960 --> 00:38:17,440
okay, this is a long tangent, which I'll talk about a little bit, but I won't get into the

450
00:38:17,440 --> 00:38:21,120
complete details unless people are interested. But I ended up T I ended up talking to a bunch of

451
00:38:21,120 --> 00:38:24,800
philosophers who, you know, who worked on causality and people who worked on parallel

452
00:38:24,800 --> 00:38:28,560
programming and quantum information theory and other places where causality was,

453
00:38:28,560 --> 00:38:32,480
was, was studied and asked them kind of basically what, what is, what, what do you mean by causality?

454
00:38:32,480 --> 00:38:39,120
What is causality? What is this thing we're trying to define? And in some form or another,

455
00:38:39,120 --> 00:38:44,000
all of the definitions boiled down to, you know, event A causes event B if

456
00:38:44,960 --> 00:38:49,200
had event A not occurred, then event B would not have occurred. So in other words,

457
00:38:49,200 --> 00:38:53,440
you need a counterfactual, you need some possible history, some possible world in which event A

458
00:38:53,440 --> 00:38:57,520
didn't happen. But if you're reasoning about a purely deterministic event system like a Turing

459
00:38:57,520 --> 00:39:01,520
machine, that doesn't make any sense. Because if you're, you know, if you, if you have a single

460
00:39:01,520 --> 00:39:06,240
Turing machine transition function, there is no possible world in which that transition function

461
00:39:06,240 --> 00:39:09,360
didn't fire in that particular way. Because if it didn't fire in that particular way,

462
00:39:09,360 --> 00:39:12,480
you would not be reasoning about that Turing machine anymore, you'd be reasoning about

463
00:39:12,480 --> 00:39:18,160
different Turing machine. So suddenly this, you know, to make sense of these notions of causality,

464
00:39:18,160 --> 00:39:23,040
you need a kind of Leibnizian, you know, modality is view of reality that just doesn't exist for

465
00:39:23,040 --> 00:39:27,760
these deterministic computational systems. So either you need to define computation,

466
00:39:28,400 --> 00:39:33,840
you need to define causality only at the multiway level, only at the level where you have many

467
00:39:33,840 --> 00:39:37,440
computations or possibly all computations happening in parallel, and then you can define

468
00:39:37,440 --> 00:39:41,760
causality relative to all of the, you know, relative between them, or you were kind of posed, right?

469
00:39:41,760 --> 00:39:45,680
There wasn't really, you know, that seemed like the only kind of the only get out, or you'd need

470
00:39:45,680 --> 00:39:50,320
some fundamentally new philosophical theory of causality that I was not qualified to produce.

471
00:39:51,120 --> 00:39:56,240
And so that's, again, part of the reason, part of what motivated this general

472
00:39:56,240 --> 00:40:01,360
research program, which is trying to think about this category of not just a single computation,

473
00:40:01,920 --> 00:40:05,440
and with a single sequence of data structures, because it's clear that you can't, you know,

474
00:40:05,440 --> 00:40:10,880
philosophically meaningful way assign causality in that case, but rather, you know, looking at the

475
00:40:10,880 --> 00:40:14,640
algebraic structure of the category of all possible computations and all possible data

476
00:40:14,640 --> 00:40:18,960
structures. And in that situation, there is a notion of causality you can equip that with,

477
00:40:18,960 --> 00:40:23,520
and there's a nice, again, a nice mathematical description in terms of, in terms of weak two

478
00:40:23,520 --> 00:40:28,560
categories and so on, which again, I can talk about if people are interested. But yeah, so it's

479
00:40:28,560 --> 00:40:32,880
clear that these things are very deeply related that this sort of theory of the category of

480
00:40:32,880 --> 00:40:36,720
computations and data structures, and the theory of how you assign causality in a meaningful way,

481
00:40:37,760 --> 00:40:42,960
are very deeply related. And I'll just mention one other thing on that topic, which is again,

482
00:40:42,960 --> 00:40:48,640
just the area which I find quite exciting, because it's an unexpected spin out of this program,

483
00:40:48,640 --> 00:40:53,760
which is that, so once you have a way of consistently applying causality at a per token

484
00:40:53,760 --> 00:40:58,800
level in these systems, it gives you a way of vastly generalizing what computation is.

485
00:41:00,400 --> 00:41:03,920
And you can, in particular, you can get, you can derive something which I call,

486
00:41:05,280 --> 00:41:08,800
well, which I'm provisionally calling covariant computation, although it should probably have

487
00:41:08,800 --> 00:41:16,320
a better name than that, which is, so in our traditional kind of Turing Church type models

488
00:41:16,320 --> 00:41:21,200
of computation, computation is a purely forwards in time operation. So at every point, you know,

489
00:41:21,200 --> 00:41:25,280
you have a complete data structure, and computation is about deriving what is the next state of that

490
00:41:25,280 --> 00:41:28,960
data structure. So in a sense, it's only a forwards in time thing. You might be able to

491
00:41:28,960 --> 00:41:32,000
kind of reconstruct the initial conditions based on some subsequent data structures,

492
00:41:32,000 --> 00:41:34,800
you might be able to go backwards in time, but that's essentially what you're doing.

493
00:41:35,760 --> 00:41:42,000
But then you could imagine, okay, suppose I don't know the complete state of my data structure,

494
00:41:42,000 --> 00:41:47,360
I know instead, I know one part of my data structure, but I know it, I know its history

495
00:41:47,360 --> 00:41:51,680
throughout all of time. So you could imagine, say an elementary cellular automaton or a Turing

496
00:41:51,680 --> 00:41:56,240
machine tape, where you'd know nothing about the tape, but you know the state of one cell,

497
00:41:56,240 --> 00:42:00,080
and you know it, you know, throughout all of time. And then the question is, what can you infer

498
00:42:00,080 --> 00:42:03,280
about the rest of the computation? And it turns out that for those kind of structured array type

499
00:42:03,280 --> 00:42:07,760
systems, you can infer a lot, you can actually evolve the system, not forwards in time, but

500
00:42:07,760 --> 00:42:13,920
sideways in space, and obtain a kind of causal diamond that so okay, the top left, top right,

501
00:42:13,920 --> 00:42:17,920
bottom left, bottom right corners are undetermined. But everything inside that diamond can be

502
00:42:17,920 --> 00:42:24,320
determined just from that one row, or that one column that you know, you know, sort of extended

503
00:42:24,320 --> 00:42:29,520
throughout time. And so, you know, and that's a fundamentally different notion of computation.

504
00:42:29,520 --> 00:42:33,520
So it's a version of computation, which is not forwards in time, but sideways in space.

505
00:42:33,520 --> 00:42:37,520
But you can also have version of computation that is sideways and branch your space where you

506
00:42:37,520 --> 00:42:42,560
know, you know, one complete state, you know, you know, one branch of the multiway system extended

507
00:42:42,560 --> 00:42:45,680
throughout time. And then the question is, what else can you infer about the, you know, but the

508
00:42:45,680 --> 00:42:49,040
rest of the multiway system just from that one branch? And again, the answer turns out to be,

509
00:42:49,040 --> 00:42:55,200
you can infer a lot, but not everything. And so, just like in the reason I call this covariant

510
00:42:55,200 --> 00:42:59,040
computation is because it's very analogous to what happens in relativity. So in relativity,

511
00:42:59,120 --> 00:43:03,200
once you buy into this notion of general covariance and the notion that space and time are kind of

512
00:43:03,200 --> 00:43:07,760
fundamentally the same thing, then you have to somehow relax your traditional view of what

513
00:43:07,760 --> 00:43:12,000
dynamical systems do, which is, you know, we typically think of systems as evolving, you know,

514
00:43:12,000 --> 00:43:16,880
you have a snapshot of your initial of your data at, you know, localized on a, you know,

515
00:43:16,880 --> 00:43:20,480
for a particular state of space, you know, on a particular space like hypersurface.

516
00:43:20,480 --> 00:43:23,760
And then your laws of physics tell you how that space like hypersurface evolves forwards or

517
00:43:23,760 --> 00:43:31,440
backwards in time. But in a covariant picture of physics, then you must also allow for, you know,

518
00:43:31,440 --> 00:43:36,160
your initial data to be defined on a time like hypersurface, and you for you to be able to evolve

519
00:43:36,160 --> 00:43:40,560
that time like hypersurface sideways in space or mixtures of the two and so on. And so it's clear

520
00:43:40,560 --> 00:43:45,040
that there's a very general, a vast generalization of ordinary computation theory that you can,

521
00:43:45,040 --> 00:43:49,520
that you can construct that's kind of physics inspired in that sense, in which you can have

522
00:43:49,600 --> 00:43:55,040
mixing of space time and kind of multiway directions in a completely consistent way.

523
00:43:55,040 --> 00:44:00,160
But to make those things consistent, you need to have a definite way of assigning causality. You

524
00:44:00,160 --> 00:44:04,320
need to because, you know, any computation that you do, even if it permutes the directions of

525
00:44:04,320 --> 00:44:08,720
space and time and branching space and so on, must always somehow preserve the causal structure,

526
00:44:08,720 --> 00:44:13,200
has to respect the causal structure of what's going on, or else it's inconsistent. And so

527
00:44:13,920 --> 00:44:17,920
this question of how you construct a covariant theory of computation is, it turns out,

528
00:44:17,920 --> 00:44:22,640
intimately related to the question of how you take this category of computations and data structures

529
00:44:22,640 --> 00:44:26,640
and equip it with a consistent notion of causality. So very interesting question,

530
00:44:26,640 --> 00:44:33,840
we could talk about that at great length. Okay, to follow with a few pieces, it's very related to

531
00:44:33,840 --> 00:44:40,000
Professor Mike Levin's notion of poly computing, and about the necessity for a causality concept to

532
00:44:40,000 --> 00:44:46,000
be created or deployed when the question arises, was that me? Was that action or change due to me?

533
00:44:46,960 --> 00:44:53,280
Also, connectivities, even just in the neuroimaging setting, which is kind of the cradle

534
00:44:53,280 --> 00:44:56,800
from which active inference and free energy principle arise from,

535
00:44:56,800 --> 00:45:01,920
it's really important to distinguish the functional, effective, and anatomical connectivities.

536
00:45:01,920 --> 00:45:06,960
And that was one of the points that Toby St. Clair Smith made in his dissertation,

537
00:45:06,960 --> 00:45:13,280
which is that a lot of times the Bayesian graphs don't convey all of the necessary and sufficient

538
00:45:13,280 --> 00:45:18,800
information to make the reproducible computation, which is one of those kind of what's missing from

539
00:45:18,800 --> 00:45:23,520
the graph is what motivated a lot of the category theory developments in active inference,

540
00:45:23,520 --> 00:45:29,760
as well as some of the formal ontological works with Sumo and Dave here and Adam Pease, because

541
00:45:30,800 --> 00:45:35,840
implementing modal and higher order logics is really important if it's a possible situation

542
00:45:35,840 --> 00:45:39,200
where a mind can have a perspective on a mind and all these things like that.

543
00:45:40,160 --> 00:45:48,880
Then the ant-turing tape, the tape is the pheromone, and then the decision space is the nest

544
00:45:48,880 --> 00:45:57,120
mate's scrolling. So when you had a deterministic turing tape, that was like a movie because the

545
00:45:57,120 --> 00:46:02,560
nest mate couldn't make any choices, except for internal action, which is kind of side topic,

546
00:46:02,560 --> 00:46:08,160
but it couldn't make any choices on the tape. Whereas when there's a multi-way,

547
00:46:08,160 --> 00:46:13,440
which is basically in active inference, what we talk about in terms of affordances and the policy

548
00:46:13,440 --> 00:46:19,120
space and the temporal depth of planning and counterfactuals on action and action-conditioned

549
00:46:19,120 --> 00:46:24,000
world transition states like the B matrix, all those kinds of topics come into play,

550
00:46:24,000 --> 00:46:29,760
because if you want to have a causal buffer or grasp on what is it that something that could do

551
00:46:29,760 --> 00:46:35,680
otherwise does, what does it cause to do when it does or doesn't do otherwise, you need something

552
00:46:35,680 --> 00:46:42,640
like a deterministic handle around what could be a probabilistic or deterministic,

553
00:46:42,640 --> 00:46:47,280
but at least multi-way map of some kind of cognitive territory.

554
00:46:51,760 --> 00:46:58,320
That's a very, very interesting perspective. Again, I'm betraying my ignorance of active

555
00:46:58,320 --> 00:47:06,720
inference theory here, but it sounds almost like, when you have this kind of interplay between

556
00:47:08,080 --> 00:47:17,280
sort of epistemic versus pragmatic, there's two aspects of how this kind of speculative

557
00:47:17,280 --> 00:47:25,680
part of cognition works. This is something which I thought about in a completely different context

558
00:47:25,680 --> 00:47:30,240
in relation to things like quantum information theory, but I wonder if there's a potential

559
00:47:30,240 --> 00:47:37,600
overlap there. There are certain situations when thinking about these kinds of systems,

560
00:47:38,320 --> 00:47:43,600
purely abstractly, where you kind of need two different notions of causality. You need a kind

561
00:47:43,600 --> 00:47:50,480
of speculative notion that's dynamic, that can be rewritten, and then you need a kind of definite

562
00:47:50,480 --> 00:47:56,560
notion that's immutable. A classic example of this is for something like quantum information

563
00:47:56,560 --> 00:48:01,280
theory. You can have superpositions of causal orders, you can have quantum switches, you can have

564
00:48:03,120 --> 00:48:08,320
causal structure that exists in superpositions of different kind of directed graph states,

565
00:48:08,320 --> 00:48:12,400
but then once you apply Hermitian operator, once you apply a measurement, the causal structure is

566
00:48:12,400 --> 00:48:16,480
definite because then everything is relativistic and you have covariance. You have similar things,

567
00:48:16,560 --> 00:48:23,760
as I understand, with distributed computing, with parallel computing, where you potentially allow

568
00:48:23,760 --> 00:48:27,520
for speculative execution for a certain number of steps where you're kind of treeing out this

569
00:48:27,520 --> 00:48:32,640
multi-way system and you have a superposition or at least a collection of possible causal histories,

570
00:48:32,640 --> 00:48:36,880
but then eventually you have to choose an actual operation to do and then the causal history,

571
00:48:36,880 --> 00:48:42,240
you have this big block that gets laid down and then the causal history is somehow definite.

572
00:48:42,480 --> 00:48:52,480
I wonder if there's a way of thinking about speculative execution of agents

573
00:48:52,480 --> 00:48:57,600
and the interrelation between that speculative execution and agent actions in terms of, again,

574
00:48:57,600 --> 00:49:01,600
this interplay between two different causal structures, between a dynamic one versus an

575
00:49:01,600 --> 00:49:08,800
immutable one. Yeah. Well, one funny way to think about that is a single agent that has this

576
00:49:08,800 --> 00:49:15,760
counterfactual contemplative ability could be in the center place foraging arena and then imagining

577
00:49:15,760 --> 00:49:22,000
with discrete branching paths like a chess algorithm or like a probability distribution

578
00:49:22,000 --> 00:49:27,200
could be like imagining where it could go, but not all cognitive things or the kind of things that

579
00:49:27,200 --> 00:49:34,240
make plans of their own actions, whereas like an ant colony has nest mates on the ground. So

580
00:49:34,240 --> 00:49:40,560
they're actually realizing in these finite trajectories, the real exoskeleton on the ground

581
00:49:41,440 --> 00:49:48,400
that plays out, ending up with those simulated trajectories could have been simulated or could

582
00:49:48,400 --> 00:49:52,960
have been probabilistically blurred, but that's kind of the difference between like the embodiment

583
00:49:52,960 --> 00:49:58,480
and like the body moving there for a mammal or for an animal and then like the mind simulating it

584
00:49:58,560 --> 00:50:06,480
and then just to the epistemic and pragmatic tradeoff in decision making. So let's just say that

585
00:50:06,480 --> 00:50:13,840
we're in that multi-way moment. We'll just have two options, two different slices of the B variable

586
00:50:13,840 --> 00:50:19,040
and the policy selection question is about which way are you going to go? Which affordance in the

587
00:50:19,040 --> 00:50:24,800
moment? Policy is basically the affordances for the time horizon of planning, but if it's only one

588
00:50:24,800 --> 00:50:29,040
time step or just the next one, then the affordance space is just the actions that can be taken.

589
00:50:30,960 --> 00:50:36,480
One way to make it so that what happens is the likeliest thing, path of least action, which is

590
00:50:36,480 --> 00:50:41,360
kind of what opens up the whole physics of cognitive systems angle in contrast to like a

591
00:50:41,360 --> 00:50:47,120
reinforcement or reward learning perspective, what makes it the likeliest thing is starting with

592
00:50:47,120 --> 00:50:53,520
habit. So it could just be drawn from a fixed habitual distribution. However, for adaptive action,

593
00:50:54,080 --> 00:50:59,120
habit gets up-weighted with expected free energy, which is a functional that takes in

594
00:50:59,920 --> 00:51:04,960
the policy space, which is summing up to one because there's a probability over actions,

595
00:51:04,960 --> 00:51:12,240
and then up-weighting policies according to their score on expected free energy, which is

596
00:51:12,800 --> 00:51:18,400
consisting of epistemic plus pragmatic value. So how much is it going to align the observations to

597
00:51:18,400 --> 00:51:24,480
be what I like to see? That's pragmatic value with a preference. What is my expected information

598
00:51:24,480 --> 00:51:31,680
gain? That's the epistemic value. So how those are parameterized make the agent that always seeks

599
00:51:31,680 --> 00:51:37,520
out new information or always goes with habit or there's so much policy space because the knobs

600
00:51:38,240 --> 00:51:44,560
are not just simple sliders or there are multiple knobs, even though they are seemingly

601
00:51:45,360 --> 00:51:53,120
quite conciliant and minimal, like it's hard to imagine less, yet especially when there's

602
00:51:53,120 --> 00:51:59,120
richness in the environment, even simple systems can have like enormously complex or

603
00:51:59,120 --> 00:52:06,800
adaptive behaviors. I'll just leave it there. No, I never really thought of the, so okay,

604
00:52:07,840 --> 00:52:13,280
yeah, two things, right? So first of all, the perspective of, you know, thinking of an ant

605
00:52:13,280 --> 00:52:17,600
colony or a termite colony or something as being akin to a mind, that's, you know,

606
00:52:18,640 --> 00:52:23,760
I was familiar with that perspective from people like Dan Dennett and so on. But the idea that the

607
00:52:23,760 --> 00:52:29,680
individual ants in that colony are in a sense enact, they're kind of the hardware enacting the

608
00:52:29,680 --> 00:52:35,120
speculative execution, that's a very interesting idea. It's not speculative for them. Well,

609
00:52:35,120 --> 00:52:38,960
yeah, no, exactly. But it's sort of from the mind's perspective, I guess it's almost like

610
00:52:38,960 --> 00:52:42,880
speculative execution, but it's speculative execution that's being actuated in the physical

611
00:52:42,880 --> 00:52:49,760
world, which is very interesting. I not really thought about that before. But then, yeah, okay,

612
00:52:49,760 --> 00:52:55,680
so then the point you're making about connection to free energy and sort of habit formation and so

613
00:52:55,680 --> 00:53:05,520
on, okay, so I wonder if, you know, if we're thinking about a model of cognition in which

614
00:53:05,520 --> 00:53:10,960
there are these two distinct causality notions, the immutable versus the dynamic one,

615
00:53:13,200 --> 00:53:19,520
I wonder if the, so you gave a very, very nice account of how habit formation

616
00:53:20,080 --> 00:53:26,240
sort of works in these kinds of formalisms based on, you know, prior experience of expected

617
00:53:26,240 --> 00:53:33,760
free energy. So I wonder if there's a way of describing that abstractly in terms of something

618
00:53:33,760 --> 00:53:39,040
like, you know, you perform the speculative execution step where you're, you know, you're

619
00:53:39,040 --> 00:53:44,080
treeing out several multiway possibilities. And initially, you kind of, you know, you know nothing

620
00:53:44,080 --> 00:53:47,680
or you have no habits, you're just kind of, you're treeing everything out with kind of equal waiting.

621
00:53:48,240 --> 00:53:53,280
But then, you know, for each possible path, you're calculating either an actual or an expected

622
00:53:53,280 --> 00:53:59,200
free energy. And then somehow, you know, in future speculative executions, you wait those

623
00:53:59,200 --> 00:54:03,920
paths which you previously had found to have higher free energy as higher. And so, you know,

624
00:54:03,920 --> 00:54:08,240
you're more likely to explore those and less likely explore ones which are, you know, which

625
00:54:08,240 --> 00:54:14,080
have that lower expected value. It feels like something, something like that should fit very,

626
00:54:14,080 --> 00:54:18,800
very nicely into an algebraic semantics like this, which would be interesting.

627
00:54:18,800 --> 00:54:24,240
Oh, how about more questions from the chat? Okay, upcycle club writes,

628
00:54:25,520 --> 00:54:30,080
acknowledging the limitations of traditional entropy in multi computations

629
00:54:30,720 --> 00:54:36,480
motivates us to develop context specific entropy metrics. Can you share some insights

630
00:54:36,480 --> 00:54:43,120
towards such efforts? Yeah, I can certainly try. So, so yes, I mean,

631
00:54:44,560 --> 00:54:49,840
the first point is that, you know, it's, I think it was, there's that famous conversation between

632
00:54:49,840 --> 00:54:53,680
John von Neumann and Claude Shannon, where I think von Neumann famously said that like,

633
00:54:53,680 --> 00:54:58,560
Shannon should call his measure entropy because no one knows what it means, right? And I submit that

634
00:54:58,560 --> 00:55:03,200
the reason that no one knows what entropy means is because it's dependent. I mean, okay, one of

635
00:55:03,200 --> 00:55:07,120
the reasons no one knows what entropy means is because it's dependent on exactly what we've

636
00:55:07,120 --> 00:55:13,200
been talking about is dependent on the equivalence function of the observer. So it's one of these

637
00:55:13,200 --> 00:55:22,320
things like, I don't know, maybe this is a stupid analogy to use, but it's, sorry, I'm going to go

638
00:55:22,320 --> 00:55:26,640
off on a tangent, but I promise it's sort of relevant. But so one thing that, okay, one thing

639
00:55:26,640 --> 00:55:31,280
that always breaks my brain is when I try and think about like actuaries and life insurance

640
00:55:31,280 --> 00:55:40,640
policies, because it's one of those areas where those models only make sense if they're not

641
00:55:40,640 --> 00:55:45,760
perfect in a sense, right? Like, so if you had an actuary who knew exactly how long everyone was

642
00:55:45,760 --> 00:55:51,040
going to live, and somehow that information was kind of openly available, there would be no,

643
00:55:51,040 --> 00:55:56,720
like life insurance policies would be pointless. It's, but you know, whereas also if you had a

644
00:55:56,720 --> 00:56:00,160
model that was completely hopeless of predicting how long people would live, they would also be

645
00:56:01,040 --> 00:56:07,920
life insurance policies would also be pointless. The very existence of actuarial science

646
00:56:07,920 --> 00:56:13,600
relies on your model neither being perfect nor being awful. It somehow has to exist somewhere in

647
00:56:13,600 --> 00:56:19,040
between. And as I said, that's something which I, it's one of those topics where if I think about

648
00:56:19,040 --> 00:56:23,600
it for too long, it all just stops making sense. And entropy has very much that same character,

649
00:56:23,600 --> 00:56:28,960
right? Because if you were Laplace's demon, if you had perfect information about the system that

650
00:56:28,960 --> 00:56:33,760
you were observing, there's no notion of entropy, right? It's just every, you know, you know every

651
00:56:33,760 --> 00:56:38,640
micro-estate. So, you know, the Boltzmann formula gives you an entropy value of zero,

652
00:56:39,760 --> 00:56:45,440
where, you know, the notion of entropy only exists once you take that perfect knowledge

653
00:56:45,440 --> 00:56:49,680
of a system and you coarse-grain it, you define, as I was describing earlier, you introduce an

654
00:56:49,680 --> 00:56:55,680
encoding function that is not 100% subjective, so that now you are mapping certain distinct

655
00:56:55,680 --> 00:57:00,080
micro-estates onto the same coarse-grained macro-estates. And then now you can ask, okay,

656
00:57:00,080 --> 00:57:03,360
what's the number of micro-estates that, you know, certainly how coarse is my coarse-graining?

657
00:57:03,360 --> 00:57:07,360
What's the number of micro-estates consistent with this macro-estate? What's the number of different

658
00:57:07,360 --> 00:57:11,520
values of my domain that gets mapped to a single point in my co-domain of my encoding function?

659
00:57:11,520 --> 00:57:17,360
And that's what entropy is. And so it's, it's very closely, I mean, it is effectively a measure of

660
00:57:17,360 --> 00:57:21,920
how good is my coarse-graining. So if you had perfect knowledge, there's no entropy. If you have no

661
00:57:21,920 --> 00:57:28,240
knowledge, there's no entropy. It relies upon you having a not completely trivial, but also not

662
00:57:28,240 --> 00:57:33,600
100%, you know, a slightly subjective, but not 100% subjective encoding function,

663
00:57:34,880 --> 00:57:41,280
just like with life insurance policies. But so the reason, the reason I'm stating that is because,

664
00:57:41,280 --> 00:57:46,000
so now it kind of, I think from that perspective, becomes a little bit clearer why there are all

665
00:57:46,000 --> 00:57:49,840
these different notions of entropy and why, as the questioner was alluding to, why entropy seems

666
00:57:49,840 --> 00:57:56,960
to be so, as a concept seems to be so domain and system specific, because every different system,

667
00:57:56,960 --> 00:58:01,120
every different observer will, in principle, have a different set of encoding functions,

668
00:58:01,120 --> 00:58:04,960
a different set of equivalence functions, and each one will give rise to a different calculation

669
00:58:04,960 --> 00:58:10,000
of entropy. And so one way that you can think about this program, this program to try to

670
00:58:11,280 --> 00:58:15,040
understand the algebraic interplay between time complexity versus kind of equivalency

671
00:58:15,040 --> 00:58:20,880
complexity or computational irreducibility versus multi-computational irreducibility.

672
00:58:20,880 --> 00:58:26,400
In some sense, that is a program to try to understand how different definitions of entropy

673
00:58:26,400 --> 00:58:32,080
relate to each other in these kinds of systems. How, you know, if I take one idealized observer

674
00:58:32,080 --> 00:58:37,920
that has this equivalence function, and I ask, okay, suppose now they communicate with this

675
00:58:37,920 --> 00:58:41,600
different observer with a different equivalency function, they come to different understandings

676
00:58:41,680 --> 00:58:45,280
of what the entropy of the system is, but what is the relationship between their measured entropy

677
00:58:45,280 --> 00:58:50,320
values? They clearly is one that depends algebraically on some details of the distinction

678
00:58:50,320 --> 00:58:54,320
between their respective equivalence functions, but there doesn't seem to be yet any general

679
00:58:54,320 --> 00:59:00,640
theory for how those things are related, and that's part of the kind of raison d'être of this

680
00:59:01,360 --> 00:59:09,840
of this research program. But yeah, I mean, okay, so one thing that I will comment on,

681
00:59:11,360 --> 00:59:15,200
although this is a little bit more speculative, it can sort of quasi philosophical comment, but

682
00:59:15,200 --> 00:59:21,600
so one place where these notions of entropy become one place where the fact that you have

683
00:59:21,600 --> 00:59:25,040
all these different notions of entropy becomes kind of interesting is in fundamental physics.

684
00:59:26,000 --> 00:59:30,560
So when you start to think about, if you try to model physics and the universe in these

685
00:59:30,560 --> 00:59:36,320
fundamentally computational terms, then one fairly generic sort of conclusion that you can reach

686
00:59:36,320 --> 00:59:40,880
is that gravitation, general relativity is essentially an entropic phenomenon. I mean,

687
00:59:41,520 --> 00:59:45,600
Valinde and people have kind of talked about this in non computational contexts too,

688
00:59:45,600 --> 00:59:48,880
but it's very, very natural if you start to think about space like hypersurfaces being

689
00:59:48,880 --> 00:59:53,520
a sort of hypergraphs, then, you know, in order to obtain a continuum geometry that's compatible

690
00:59:53,520 --> 00:59:58,480
with the Einstein equations, you need to have certain ergodicity, you know, you need to be

691
00:59:58,480 --> 01:00:03,920
able to make certain ergodicity assumptions on the rewriting, which in turn, sort of implies

692
01:00:03,920 --> 01:00:08,640
certain lower bounds on the entropy of the system. So somehow gravitation, general relativity,

693
01:00:08,640 --> 01:00:13,280
is a coarse grained theory that you obtain in the limit as the entropy goes to infinity.

694
01:00:13,840 --> 01:00:19,920
But something that's interesting is that quantum mechanics, on the other hand, is an idealization

695
01:00:19,920 --> 01:00:24,880
that you obtain in the limit as entropy goes to zero, because in kind of one of these purely

696
01:00:24,880 --> 01:00:29,120
computational models of physics, the quantum mechanical state of a system is described in

697
01:00:29,120 --> 01:00:34,640
terms of its multi-way structure. It's described in terms of, you know, when I have a kind of a

698
01:00:34,640 --> 01:00:40,320
branching program like this, let me find one of these like this, then, you know, I can divide it

699
01:00:40,320 --> 01:00:45,600
up into these sort of, into these simultaneity surfaces. And if I associate each state of the

700
01:00:45,600 --> 01:00:49,360
program as being like the analog of a quantum eigen state, and the kind of path weightings

701
01:00:49,360 --> 01:00:53,840
as being an analogous to the amplitudes associated with the eigen states, I can quickly build up

702
01:00:53,840 --> 01:00:57,360
a description of this multi-way system in terms of the evolution of some discrete analog of the

703
01:00:57,360 --> 01:01:01,520
Schrodinger equation. And it turns out you get a theory that is kind of equivalent of the mathematically

704
01:01:01,520 --> 01:01:06,880
isomorphic to standard quantum mechanics out of it. So quantum mechanics is sort of, you know,

705
01:01:06,880 --> 01:01:12,880
inextricably bound up with the phenomenon of the multi-way system. But if you take the entropy

706
01:01:12,880 --> 01:01:18,800
to infinity here, then you're effectively, then the sophistication of your equivalence function

707
01:01:18,800 --> 01:01:22,960
becomes arbitrarily large, which means that you can describe any, essentially any pair of

708
01:01:22,960 --> 01:01:29,280
states as being equivalent. And so it turns out that the, that actually the quantum mechanical

709
01:01:29,280 --> 01:01:32,720
case corresponds to the zero entropy limit, whereas the kind of general relativistic case

710
01:01:32,720 --> 01:01:37,040
corresponds to the infinite entropy limit. But they're kind of two different, two fundamentally

711
01:01:37,040 --> 01:01:41,040
different notions of entropy, one of which exists at the single-way level, one of which exists at

712
01:01:41,040 --> 01:01:46,320
the multi-way level. And again, the question of how these things into play is partly why we're,

713
01:01:46,320 --> 01:01:50,480
you know, why we're investigating this. And it's clear that that question has links to these

714
01:01:50,480 --> 01:01:57,360
quite foundational questions in fundamental physics. The ideal point mass and the ideal

715
01:01:57,360 --> 01:02:04,480
distribution with its center of gravity and all this, Dave, question for you, from you.

716
01:02:05,760 --> 01:02:16,160
Yes. Okay, I was, can you hear? Yeah, go for it. Okay, good. Yeah, I'd like to hear down to the

717
01:02:16,160 --> 01:02:22,560
low road during this discussion, maybe show business. What people are trying to explain,

718
01:02:22,560 --> 01:02:28,320
what is computational reducibility or irreducibility? Often you'll see a graph that says, well,

719
01:02:28,320 --> 01:02:33,760
now here's the, the computation, our target running along, the fox is running along,

720
01:02:33,760 --> 01:02:40,320
and behind it, there's a team of algorithms that would like to catch it. And yeah, it either does

721
01:02:40,400 --> 01:02:46,640
or it doesn't outrun all of them, but some can come, can seem to come pretty close to

722
01:02:47,600 --> 01:02:52,960
catching it. Now, there's something else people have been looking at for a few years, the Mandelbrot

723
01:02:52,960 --> 01:02:59,440
set. The Mandelbrot set really is a determined, not only a deterministic computation, it's a crisp

724
01:02:59,440 --> 01:03:08,000
computation. Every set, every point, either is inside the safe zone, it's going to sit there,

725
01:03:08,000 --> 01:03:13,840
and it's happy and quiet, and you color it black, or it flies off to infinity.

726
01:03:15,520 --> 01:03:20,480
So you could, it could just be a bunch of yes or no, but that's not the way people

727
01:03:21,040 --> 01:03:26,320
trying to calm down after a day of work want to look at it. They want to say, hey, I want to see

728
01:03:26,320 --> 01:03:33,280
the 32 million deep Mandelbrot set. And I want to see it in colors. So when you get the colors,

729
01:03:33,280 --> 01:03:39,360
you ask, how hard did I have to work? How long did I have to grind along before I made that decision?

730
01:03:39,360 --> 01:03:44,000
Oh, this is the point that's going to settle down and be quiet and uninteresting, or it's going to

731
01:03:44,000 --> 01:03:51,840
fly off to infinity. So you put colors in. Now, I just wonder, would it be interesting to anyone

732
01:03:51,840 --> 01:04:00,640
to fuzzify these gorgeous causal and multi-causal graphs and just show this is the portion where

733
01:04:00,640 --> 01:04:08,000
we had to work really hard, rule 35 or whatever. We had to go 15,000 generations before we gave up

734
01:04:08,000 --> 01:04:13,840
and said, we're not going to follow this anymore. The other one, oh, after 30 steps, I see.

735
01:04:16,080 --> 01:04:21,920
That's a really interesting question. And I mean, yes. So a couple of comments on that. I mean,

736
01:04:21,920 --> 01:04:27,120
so one is, I mean, you're right in the sense that, yes, the Mandelbrot set is a very clean

737
01:04:27,120 --> 01:04:32,480
thing to describe. I would say it's not, I mean, I would argue it's actually not a crisp

738
01:04:33,360 --> 01:04:37,520
computation in that sense, precisely for reasons of computational irreducibility,

739
01:04:37,520 --> 01:04:41,920
because as you go arbitrarily close to the boundary of the set, you can have complex

740
01:04:41,920 --> 01:04:46,000
numbers that stay, that have a kind of indefinite period of transient, right? There's no

741
01:04:46,000 --> 01:04:53,200
upper bound on how long the ZN squared plus C orbit can last before it either diverges or

742
01:04:53,200 --> 01:05:01,120
converges to zero. And that statement that there will be points that can remain,

743
01:05:01,120 --> 01:05:05,360
that can get tied up in these orbits indefinitely is really a computational irreducibility statement.

744
01:05:06,720 --> 01:05:14,000
But yeah, I mean, your question about, could you construct, I mean, that way of coloring

745
01:05:14,000 --> 01:05:19,440
points that are on the boundary of the Mandelbrot set in that way, the so-called escape time algorithm,

746
01:05:19,520 --> 01:05:23,360
right? Where you kind of, where you color them based on how many steps that I need to

747
01:05:23,360 --> 01:05:28,800
do before it either converged or escaped off above some, or the complex numbers modulus

748
01:05:29,440 --> 01:05:34,080
exceeded some value. Yeah, it's an interesting idea that you could try and kind of construct

749
01:05:34,080 --> 01:05:38,320
geometrical representations of the space of possible computations based on a kind of escape

750
01:05:38,320 --> 01:05:43,120
time algorithm for computational irreducibility. Yeah, so I mean, there are the possible ways

751
01:05:43,120 --> 01:05:48,320
that you could do that, right? That you said, we know, we've known since the days of Turing,

752
01:05:48,320 --> 01:05:52,240
that the halting problem for Turing machines is undecidable, right? That there's no finite

753
01:05:52,240 --> 01:05:55,840
computation that you can do that will determine over an arbitrary computer program will terminate

754
01:05:55,840 --> 01:06:00,080
in finite time. So you could do it, you could, if you once you have a way of kind of geometrizing

755
01:06:00,080 --> 01:06:04,960
the space of possible computations, which we, which we have now, as you say. Yeah, you could easily

756
01:06:04,960 --> 01:06:08,240
construct a kind of escape time algorithm where you know, you have your analog of the Mandelbrot

757
01:06:08,240 --> 01:06:11,600
set, which is, you know, here's all the halting computations, here are all the definitely non

758
01:06:11,600 --> 01:06:15,840
halting computations. And then there's some boundary of very fuzzy stuff where we kind of don't

759
01:06:15,840 --> 01:06:20,080
really know, we have to do a lot of work. And even then it's only heuristic, which is, which

760
01:06:20,080 --> 01:06:24,800
is so, yeah, I mean, a very directly analogous. And then, yeah, and then the question becomes,

761
01:06:25,600 --> 01:06:31,520
you know, so in the case of the M set, you know, there is some underlying theory mostly,

762
01:06:31,520 --> 01:06:35,760
I think mostly due to like Dwadi and Hubbard and people that allows you to kind of predict,

763
01:06:35,760 --> 01:06:40,560
like, you know, if you have a finite filament, you can, like, of the Mandelbrot set, you can

764
01:06:40,560 --> 01:06:45,440
kind of predict where that filament is going to be based on some complicated complex analysis

765
01:06:45,440 --> 01:06:49,840
argument. And, you know, I mean, the question is once you, if you once you have a geometrization

766
01:06:49,840 --> 01:06:55,040
of the space of possible computations, can you construct some general theory like the theory,

767
01:06:55,040 --> 01:06:58,480
you know, developed by Dwadi and Hubbard that tells you things about, you know, the topology

768
01:06:58,480 --> 01:07:03,120
of the computations, whether certain regions of the space are connected, whether they're compact,

769
01:07:03,120 --> 01:07:06,400
whether you can make, you know, whether you can do a similar thing, or you can make predictions

770
01:07:06,400 --> 01:07:10,480
based on the geometry of one part of the set, you know, where you can extrapolate to things about

771
01:07:10,480 --> 01:07:15,200
the geometry of another part of the set. That's a really interesting question. And yeah, again,

772
01:07:15,760 --> 01:07:19,680
as with so many of these things, it's one of these, like, I don't know the answer, but it's a good

773
01:07:19,680 --> 01:07:25,840
reason for investigating, you know, for pursuing the program, right? Yeah, from the Wolfram side,

774
01:07:25,840 --> 01:07:32,720
as well as from the active inference side, there's both the information topology and the

775
01:07:32,720 --> 01:07:38,480
information geometry side, and not just in the kind of topological deep learning, but rather

776
01:07:38,480 --> 01:07:44,880
looking at the topology of information flows, which has been heavily developed in the category

777
01:07:45,120 --> 01:07:50,800
related to quantum information sciences. And then the information geometries that allow us to do,

778
01:07:50,800 --> 01:07:57,120
like, machine learning type, accelerated optimization ingredients, all these kinds of concepts that come

779
01:07:57,120 --> 01:08:04,640
into play with geometry, and we just don't get them from topology. So topology kind of sketches

780
01:08:04,640 --> 01:08:10,560
the skeleton, and then with the computers that we have, the information geometry is at least on

781
01:08:10,560 --> 01:08:15,840
the data sets and the ways that we have computation today, that's kind of like the quantitative

782
01:08:15,840 --> 01:08:23,920
numerical versus the formal. One question is, are these all discrete state space, discrete time

783
01:08:23,920 --> 01:08:30,800
formalisms? Because in active inference, we often deal with hybrid models that have discrete and

784
01:08:30,800 --> 01:08:35,440
continuous state spaces, and the same generative model or the same system of interest could be

785
01:08:35,440 --> 01:08:41,360
modeled with like a discrete time chapter seven, or a continuous time chapter eight model. So how

786
01:08:41,360 --> 01:08:46,400
does this deal with that? That's a really good question. I mean, yeah, so most of what I've been

787
01:08:47,440 --> 01:08:54,000
looking at so far has consisted of discrete time, discrete space models, for no particularly

788
01:08:54,000 --> 01:08:59,120
principled reason other than they're easier to analyze, right? For the most part, because you

789
01:08:59,120 --> 01:09:03,040
can do explicit computations because they're kind of more amenable to constructive analysis.

790
01:09:03,760 --> 01:09:08,560
It's easier to do, but the beauty of a lot of this, that's one of the beauties of using kind of

791
01:09:08,560 --> 01:09:13,440
general mathematical formalism is that once you develop it, it's often quite easy to extend even

792
01:09:13,440 --> 01:09:18,880
to cases that, or to extrapolate to the cases that you didn't explicitly analyze. So in principle,

793
01:09:18,880 --> 01:09:23,760
this formalism works for continuum space and continuum time systems as well,

794
01:09:23,760 --> 01:09:28,320
just with some slight modifications. So rather than having say, branching and merging, you instead

795
01:09:28,320 --> 01:09:32,800
have, if you think about this thing as now being a dynamical system, described on the

796
01:09:32,800 --> 01:09:36,640
on some symplectic manifold, then these kind of branching, merging operations of the multi-way

797
01:09:36,640 --> 01:09:42,800
system become effectively divergence and convergence, differential operators are defined on the

798
01:09:42,800 --> 01:09:48,640
symplectic manifold. And so one place where we can start to analyze that explicitly, and which

799
01:09:48,640 --> 01:09:52,640
I've done a little bit of work on, but it's one of these things which I want to go back to very

800
01:09:52,640 --> 01:09:57,440
soon, is looking at PetriNets. So PetriNets are interesting because they are a discrete time,

801
01:09:57,440 --> 01:10:02,640
discrete space system, but they admit a continuum space, continuum time, description

802
01:10:02,640 --> 01:10:06,960
in terms of ordinary differential equations and so on. So they're a nice example of a hybrid

803
01:10:06,960 --> 01:10:11,200
kind of discrete event versus continuum event system, where it's clear that this formalism

804
01:10:11,200 --> 01:10:17,040
can be used and is somehow agnostic as to whether the underlying system is discrete or continuous.

805
01:10:18,080 --> 01:10:21,040
Again, there's a broader philosophical point to make here, which is that

806
01:10:24,480 --> 01:10:28,320
in a way, one of the reasons I don't feel embarrassed to be working primarily with discrete

807
01:10:28,320 --> 01:10:33,280
systems is because, again, once you start to think about things in terms of, okay,

808
01:10:33,280 --> 01:10:36,560
you have to not just care about nature, you also have to care about the computations the observer

809
01:10:36,560 --> 01:10:41,920
can perform and what it's able to infer, then you quickly realize that in a sense, just like,

810
01:10:41,920 --> 01:10:45,520
whatever, as you were saying earlier, Danny, beauty is in the eye of the beholder. I think

811
01:10:45,520 --> 01:10:52,240
discreteness and continuity are also in the eye of the beholder, right? So if you have

812
01:10:52,880 --> 01:11:02,400
a universe that is fundamentally continuous, that's described by a continuum, the Renzi and

813
01:11:02,400 --> 01:11:06,800
Manifold or something, but your constraints are that the only experiments you perform have

814
01:11:06,800 --> 01:11:14,400
computable outcomes, have discrete outcomes, where the possible number of observables is

815
01:11:14,400 --> 01:11:18,400
always countable, then in a sense, it doesn't matter, right? It's irrelevant to you as an

816
01:11:18,400 --> 01:11:23,600
observer whether the system is discrete or continuous, because the only parts of it that

817
01:11:23,600 --> 01:11:27,520
you can interface with and interact with are discrete, and so you could have replaced the

818
01:11:27,520 --> 01:11:30,320
underlying substrate with a purely discrete mathematical structure and you wouldn't be able

819
01:11:30,320 --> 01:11:38,000
to tell. So in some sense, I don't feel too embarrassed dealing with discrete event systems

820
01:11:38,000 --> 01:11:41,840
because even if I don't necessarily believe that nature is discrete, because I don't think that's,

821
01:11:41,840 --> 01:11:46,880
I'm not even sure how we would be able to answer that, I'm reasonably convinced that

822
01:11:47,840 --> 01:11:53,200
the experiments that we can perform and the observations that we're able to perform are

823
01:11:53,200 --> 01:11:57,440
ultimately computable, and therefore, the underlying substrate might as well be discrete,

824
01:11:57,440 --> 01:12:03,120
even if it's not in reality, so to speak. Yeah, that's a great comment and definitely

825
01:12:03,120 --> 01:12:08,640
calls back to your earlier points about discretization being in the eye of the beholder,

826
01:12:08,640 --> 01:12:14,160
like in the active inference models, observations, raw data may already be discretized depending

827
01:12:14,160 --> 01:12:19,520
on the situation, but even if it weren't, like it were a continuous sensory perception or modeled

828
01:12:19,520 --> 01:12:26,800
as such analytically, still commonly models discretizing categorize as they move up cognitive

829
01:12:26,800 --> 01:12:33,360
hierarchies, and that was like initially explored to get more of this discrete either or decision

830
01:12:33,360 --> 01:12:41,120
making, planning, all those kinds of properties. Well, there's many interesting angles like

831
01:12:41,360 --> 01:12:48,720
I'm sure also it could be a multiplexed language model prompt, but like what are you working on

832
01:12:48,720 --> 01:12:57,440
or excited about for 2024? That's a good question. So I've kind of already given some hints about,

833
01:12:57,440 --> 01:13:04,400
you know, like this general research program of trying to understand computational complexity

834
01:13:04,400 --> 01:13:08,240
and algorithmic complexity and interplays between observers and systems through this

835
01:13:08,240 --> 01:13:14,560
category theoretic lens. That's a major thing which I started on, say, about maybe a couple

836
01:13:14,560 --> 01:13:18,240
of years ago. I mean, in some form, I've been working on it for a long time, but this more

837
01:13:18,240 --> 01:13:22,480
recent perspective on it is maybe a couple of years old. But for the various reasons over the

838
01:13:22,480 --> 01:13:28,640
last year or so, I've kind of put that to rest and I've been focused on these much more physics

839
01:13:28,640 --> 01:13:32,560
oriented questions about discrete space time and understanding things like, you know, how do

840
01:13:32,560 --> 01:13:36,400
black holes work and how does accretion work in discrete space time, which is also very important

841
01:13:36,400 --> 01:13:43,040
and very exciting. But I've sort of slightly been missing these more abstract directions. And so

842
01:13:43,040 --> 01:13:48,400
I have maybe one or two major physics related things that I need to finish off and then I

843
01:13:48,400 --> 01:13:53,200
really want to go back to this to the greatest extent possible. And yeah, I mean, so one thing

844
01:13:53,200 --> 01:14:01,440
is that's quite clear is that there's great interplay between this formalism and existing

845
01:14:01,440 --> 01:14:05,120
theories of computational and algorithmic complexity. So in particular, you know, so

846
01:14:05,760 --> 01:14:10,960
one very basic example is I mentioned before that, you know, you have this kind of coherence

847
01:14:10,960 --> 01:14:14,560
between these two different algebraic structures between your the operation of

848
01:14:14,560 --> 01:14:19,200
time like composition versus the operation of kind of parallel composition. And these two

849
01:14:19,200 --> 01:14:24,000
algebraic structures are in general related, although the precise conditions that relate

850
01:14:24,000 --> 01:14:27,440
them are not clear. And that's partly what we're trying to what we're trying to understand.

851
01:14:28,400 --> 01:14:34,800
But it turns out that degenerate cases of that of that question corresponds to unsolved problems

852
01:14:34,800 --> 01:14:39,840
in computational complexity theory. So, for instance, the P bus NP problem can essentially

853
01:14:39,840 --> 01:14:43,600
be recast in these terms that you can recast the P bus NP problem is the question about

854
01:14:44,720 --> 01:14:49,840
is the coherence between the time like composition of computational complexity and

855
01:14:49,840 --> 01:14:53,680
the parallel composition of computational complexity, which are what P and NP respectively

856
01:14:53,760 --> 01:14:58,560
are really about, it are those coherence conditions, the strictest they can be,

857
01:14:59,360 --> 01:15:04,080
which would be the case that the P equals NP, or are they somehow more lax, which would be the

858
01:15:04,080 --> 01:15:09,280
case that P does not equal NP. And so there's, you know, that's that's one thing that we kind

859
01:15:09,280 --> 01:15:12,800
of were already investigated, but it's clear that a whole bunch of questions about, you know,

860
01:15:13,760 --> 01:15:18,000
how the time complexity and space complexity trade off or how to come over of complexity and

861
01:15:18,000 --> 01:15:22,080
time complexity trade off, these, these are questions which can be recast in this kind of

862
01:15:22,080 --> 01:15:28,240
more algebraic category, theoretic lens, and, and, and will hopefully give insight into this

863
01:15:28,240 --> 01:15:32,480
general program of trying to understand observers and their relationship to the world. And those

864
01:15:32,480 --> 01:15:37,280
are kind of major, well, with any luck, those are major theorems that I hope we'll be able to

865
01:15:37,280 --> 01:15:44,640
prove at some point in 2024. That's that's that's that's awesome. And it makes me think about

866
01:15:44,640 --> 01:15:51,760
parallel, more nest mates, more CPU threads, deeper in times, more sequential, more planning,

867
01:15:51,760 --> 01:15:57,360
and more cognitive single monolithic agent. And then the kind of question is like, can anything

868
01:15:57,360 --> 01:16:05,200
that a single agent, mega matrix could do, cannot be decomposable at space advantage,

869
01:16:05,200 --> 01:16:11,440
or even at space disadvantage in decomposed it into a single time step operation?

870
01:16:12,320 --> 01:16:17,280
Yeah, that's a super important question. And one that, you know, with the possible exception

871
01:16:17,280 --> 01:16:22,880
of this community, not many people have asked, right? I mean, so that's something which comes

872
01:16:22,880 --> 01:16:28,560
up in quantum computing, right? So a lot of the hype around quantum computation comes from these

873
01:16:28,560 --> 01:16:32,960
theoretical speedups that derive from the fact that you're able to, you know, you're able to

874
01:16:32,960 --> 01:16:37,680
support these super positions of different, you know, where, you know, each, that each state of

875
01:16:37,680 --> 01:16:41,120
your data structure corresponds to a different eigenstate, and you're able to evolve some super

876
01:16:41,360 --> 01:16:44,720
superposition of those eigenstates. But then at the end, you have to actually come to a

877
01:16:44,720 --> 01:16:48,320
definite conclusion about what the answer is, you have to perform some measurement operation.

878
01:16:48,320 --> 01:16:52,400
And that measurement operation is lossy, it's often non deterministic, you, you know, you often

879
01:16:52,400 --> 01:16:58,000
have to repeat it multiple times. And, you know, it's becoming increasingly clear that for a large

880
01:16:58,000 --> 01:17:02,640
class of operations that were previously thought to have quantum advantage, the additional complexity

881
01:17:02,640 --> 01:17:06,560
of the measurement step really kills any quantum advantage that you may have had that you know,

882
01:17:06,560 --> 01:17:11,040
you get some advantage by doing unitary evolution. But then you lose all of it by having to do

883
01:17:11,040 --> 01:17:16,160
the submission projection at the end. And that's really a story of, again, this interplay between

884
01:17:16,160 --> 01:17:21,040
the time complexity saving of doing a multi doing a computation of doing a multi computation in

885
01:17:21,040 --> 01:17:25,920
parallel, versus the loss that comes from the complexity of the equivalence function that

886
01:17:25,920 --> 01:17:30,640
you need to apply in order to get to some definite conclusion about what the, you know, about what

887
01:17:30,640 --> 01:17:35,040
happened because, you know, ultimately, you need to somehow collapse that that that directed graph

888
01:17:35,040 --> 01:17:39,360
into a single thread of time in order to be able to have some coherent representation of what happened.

889
01:17:40,160 --> 01:17:43,840
And so again, understand that, you know, that's a place where understanding these, you know,

890
01:17:43,840 --> 01:17:48,640
the these tradeoffs will become very important. And as I say, in some limiting case that that gives

891
01:17:49,280 --> 01:17:53,200
some perspective on your question, Daniel, which is, which I agree is a very interesting question

892
01:17:53,200 --> 01:17:59,600
about, you know, in principle, we know that anything that a deterministic Turing machine

893
01:17:59,600 --> 01:18:04,160
can do a non deterministic Turing machine can do and vice versa with some speed up or slow down.

894
01:18:05,120 --> 01:18:10,480
But that statement, which is a classic result in, you know, in computability theory, neglects

895
01:18:10,480 --> 01:18:13,840
all consideration of the equivalence function. So there may be cases where the equivalence

896
01:18:13,840 --> 01:18:19,040
function is so complex, that essentially, you know, that to do state equivalence becomes

897
01:18:19,040 --> 01:18:23,200
undecidable. And so in that case, you have a scenario where actually, you know, you've got a

898
01:18:23,200 --> 01:18:28,720
multi computational system, but to collapse it to one that's equivalent to a single way system

899
01:18:28,720 --> 01:18:32,000
requires unbounded amounts of computational effort. And so that's so actually they become

900
01:18:32,080 --> 01:18:38,000
inequivalent, even though, you know, computability theory says they should be the same. So it's

901
01:18:38,000 --> 01:18:43,280
clear that there's a there's a more rich, more subtle theory that's underlying here that we're

902
01:18:43,280 --> 01:18:46,720
just beginning to kind of glimpse, and that I hope will, you know, we'll be able to kind of

903
01:18:46,720 --> 01:18:50,000
to prove some new limited results about soon, once we understand it a bit better.

904
01:18:51,360 --> 01:18:57,520
Awesome. And I think definitely a special shout out to all of our colleagues on either like the

905
01:18:57,520 --> 01:19:04,640
Wolfram and or active inference side, because we've seen few if any active inference models

906
01:19:05,520 --> 01:19:12,640
phrased analytically or computationally with the Wolfram technology from studying complexity

907
01:19:12,640 --> 01:19:19,200
in other areas. It's really clear to see how productive and powerful the software and the

908
01:19:19,200 --> 01:19:25,600
tools can be and changing and growing every day. So it's really interesting, maybe someone can

909
01:19:26,560 --> 01:19:33,200
if they're listening this far in, like, go from one side to the other and back or make a Wolfram

910
01:19:33,200 --> 01:19:41,040
active inference model, or do some other kind of combination, because it's very fruitful territory.

911
01:19:41,920 --> 01:19:51,440
And we know that our elders have already spoken, they've okayed it. No, but really, it's so

912
01:19:52,240 --> 01:19:59,280
rich with connections here between the areas that we're all studying and feeling like converging

913
01:20:00,000 --> 01:20:04,880
on many common places to scaffold and jump off from together.

914
01:20:07,040 --> 01:20:13,440
Yeah, I agree. And I mean, at least the kinds of things that we were discussing here about,

915
01:20:13,440 --> 01:20:17,360
you know, speculative execution and behavior formation through free energy principle and

916
01:20:17,360 --> 01:20:21,680
so on, those things should be relatively easy to implement in the framework that's already been

917
01:20:21,680 --> 01:20:27,360
developed here. I mean, so that's just a question of just implementing some kind of computation of

918
01:20:27,360 --> 01:20:32,720
expected free energy and using that to weight multiway paths in the speculative execution model.

919
01:20:33,840 --> 01:20:40,480
So at least the beginnings of that implementation, I think the path is pretty clear. And we will

920
01:20:40,480 --> 01:20:44,720
probably end up doing at some point in the future anyway, as part of other research.

921
01:20:45,680 --> 01:20:50,480
So yeah, so I agree. It's a very exciting kind of point of interface.

922
01:20:51,040 --> 01:20:56,960
Yeah, well, I hope that we can stay in touch if you ever want to come back for a

923
01:20:58,160 --> 01:21:04,480
009.2, or if we want to even facilitate some kind of working group or some connections

924
01:21:05,040 --> 01:21:10,400
to really strengthen and like include the participation of more people in this super

925
01:21:10,400 --> 01:21:15,360
exciting area, that would be amazing. Yeah, that sounds fun. Let's let's try and set something up.

926
01:21:16,160 --> 01:21:22,880
Well, Dave, first penultimate comments, then Jonathan, you can have the kind of last comments.

927
01:21:23,680 --> 01:21:31,120
Yes, I hope you do get to continue on the Wolfram physics side to think about a more general notion

928
01:21:31,120 --> 01:21:37,840
of what these ultimate things are. Are they observers or does that already prejudice the case

929
01:21:37,840 --> 01:21:45,360
of what you might find if you call them workers or actors or, you know, go back and ask Stuart

930
01:21:45,360 --> 01:21:53,520
Kaufman, what must a mind do to earn its way in the world? Let's make this keep happening. Thank you.

931
01:21:54,400 --> 01:21:59,920
Thank you, Dave, for suggesting it also. It was a great suggestion. Jonathan?

932
01:22:00,880 --> 01:22:06,720
No, I think that's a fantastic note to end on. I mean, in a sense, this idea that we should

933
01:22:06,720 --> 01:22:10,240
start to move, I mean, so it's okay, big picture for a moment, like

934
01:22:11,840 --> 01:22:16,240
where, you know, this formalism is being developed, the formalism I've described in this

935
01:22:16,960 --> 01:22:20,960
in this discussion is being developed, you know, assuming a kind of purely passive observer

936
01:22:20,960 --> 01:22:24,560
idealization. And that's already been incredibly difficult, right? This is clear, there's a lot

937
01:22:24,560 --> 01:22:28,960
we don't understand at that. But of course, David is right that in a sense, you know, what, you know,

938
01:22:28,960 --> 01:22:33,760
ultimately, we want to start transitioning to a participatory observer model, where you allow

939
01:22:33,760 --> 01:22:37,280
for two-way interactions or, you know, higher order interactions between observers and systems

940
01:22:37,280 --> 01:22:41,840
and things that don't just go in one direction. And yeah, you know, in a sense, I view a lot of

941
01:22:41,840 --> 01:22:45,600
what we're trying to do with, you know, trying to nail down these notions of causality, trying to

942
01:22:45,600 --> 01:22:49,760
understand these interplays between different complexity and entropy measures as, you know,

943
01:22:49,760 --> 01:22:54,400
the necessary groundwork for developing that subsequent theory, right? That does, you know,

944
01:22:54,400 --> 01:22:59,600
it's clear that if we want to have a version of the, you know, of this kind of compositional

945
01:22:59,600 --> 01:23:04,000
multi-way formalism that is also compatible with things like second-order cybernetics,

946
01:23:04,000 --> 01:23:08,400
then, you know, at the very least, we need to have a very coherent notion for what causality is

947
01:23:08,400 --> 01:23:13,760
and, you know, and a robust algebraic description of that that's not going to break or change.

948
01:23:13,760 --> 01:23:22,720
And so I think the nonparticipatory observer model is a useful starting point because it's one that

949
01:23:22,720 --> 01:23:28,160
is just within our grasp of, you know, of being kind of mathematically tractable. And then the

950
01:23:28,160 --> 01:23:32,320
hope is that the technology and the ideas and the conceptual structure that we develop for

951
01:23:32,320 --> 01:23:37,360
understanding that will then, as I say, lay the groundwork for developing something that's more

952
01:23:37,360 --> 01:23:44,480
like what real, you know, active participatory observers do. And, and yeah, I mean, I think I

953
01:23:44,480 --> 01:23:49,760
don't, on the, on the scientific side, I'm not really sure I have any final comments apart from

954
01:23:49,760 --> 01:23:53,360
just, you know, as obvious, this is a, you know, this is still a story that's being, that's being

955
01:23:53,440 --> 01:23:59,200
developed. And, and yeah, as Daniel alluded to, I hope that we can continue to interact and collaborate

956
01:23:59,200 --> 01:24:04,320
where, where that makes sense. And, and yeah, at the very least, I think, in cooperation of kind

957
01:24:04,320 --> 01:24:09,600
of these, you know, these active inference models within these discrete time, in the first instance,

958
01:24:09,600 --> 01:24:14,400
discrete time, you know, computational frameworks, and allowing things like speculative execution

959
01:24:14,400 --> 01:24:19,760
and multi-way path waiting based on free energy estimates. I think that's, you know, that's a,

960
01:24:20,000 --> 01:24:25,840
that's a project that's of obvious mutual interest and, and something that I, that I hope will happen

961
01:24:25,840 --> 01:24:32,160
in the coming months. Thank you. We just speculatively executed active Wolfram inference.

962
01:24:33,520 --> 01:24:38,320
Basically. Sounds very good. All right. Thank you, Jonathan. Thank you, Dave. Thank you, everyone.

963
01:24:39,040 --> 01:24:42,160
See y'all next time.

964
01:24:49,760 --> 01:24:51,140
you

965
01:25:19,760 --> 01:25:21,140
you

