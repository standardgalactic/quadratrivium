WEBVTT

00:00.000 --> 00:18.840
Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024

00:18.840 --> 00:24.800
on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre

00:24.800 --> 00:32.040
and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining, to you

00:32.040 --> 00:35.680
for introductions and to take us through the paper.

00:35.680 --> 00:36.200
Oh, thank you.

00:36.200 --> 00:37.200
And welcome everyone.

00:37.200 --> 00:45.000
This is Active Inference Gas Stream 84.1 on July 22nd, 2024.

00:45.000 --> 00:45.800
Thank you.

00:45.800 --> 00:49.760
Okay, thank you guys. Go for it.

00:49.760 --> 00:51.920
Hi, thanks for having us.

00:51.920 --> 01:00.040
So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney, and Charles?

01:00.040 --> 01:16.640
I'm Charles Rathkoff. I'm a prominent research associate at the Ulich Research Center in Ulich, Germany, in a big neuroscience institute.

01:16.680 --> 01:19.840
So, should we go through the paper briefly?

01:19.840 --> 01:23.880
So, yeah, we wrote this paper together.

01:23.880 --> 01:29.920
Actually, we started by writing a general audience piece that was published in the box online.

01:29.920 --> 01:31.680
Tell us when was that?

01:31.680 --> 01:33.640
A few months ago, I guess.

01:33.640 --> 01:37.520
Yeah, I think it was close to a year ago, maybe.

01:37.520 --> 01:42.000
I don't think it was published a year ago. I think it was published in 2024.

01:42.000 --> 01:45.240
But yeah, we worked on it for a while.

01:45.240 --> 01:53.560
Yeah, so this piece was doing, I guess, two things, that piece that we published in the box.

01:53.560 --> 01:57.640
It was pushing back against what we call the all-or-nothing principle,

01:57.640 --> 02:03.200
which we defined as the idea that either something has a mind or it doesn't.

02:03.200 --> 02:11.840
So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.

02:11.880 --> 02:16.840
And we argued that this was not the best framing to think of,

02:16.840 --> 02:22.960
especially, and family, our systems that seem to have sophisticated behavioral capacities,

02:22.960 --> 02:26.280
like large language models or AIC systems in general,

02:26.280 --> 02:30.400
where we don't want to take various cognitive capacities of the package

02:30.400 --> 02:34.160
and package them into this idea of a mind, where either you have the mind or you have them,

02:34.160 --> 02:39.920
then if you have the mind, you have all of these things as a package, consciousness, reasoning, etc.

02:39.920 --> 02:43.840
planning, memory, theory of minds.

02:43.840 --> 02:49.280
So, we thought, as a remedy to this kind of all-or-nothing approach,

02:49.280 --> 02:53.840
we argued for what we call the divide and conquer strategy

02:53.840 --> 02:57.040
when studying the cognitive capacities of these systems,

02:57.040 --> 03:02.480
which involved looking at these capacities on a piecemeal basis, case-by-case,

03:02.480 --> 03:06.080
with an open-minded empirical approach.

03:07.040 --> 03:13.040
Yeah, Charles, I don't know if you have anything else that's down in the books, piece or the backgrounds.

03:13.040 --> 03:23.760
Yeah, I mean, we made one point in there about why it is that people feel so torn about

03:23.760 --> 03:31.600
reactions to large language models, and we said a little bit about the psychology of essentialism,

03:31.680 --> 03:38.960
which is the idea that we naturally categorize especially living things

03:39.920 --> 03:46.320
with respect to a presumed essence. So, we gave an example of an oak tree, I think,

03:47.920 --> 03:54.880
and we said that what people tend to think as they grow up and learn about the natural world is that

03:55.600 --> 04:05.280
an oak tree remains an oak tree regardless of changes to its observable properties,

04:06.080 --> 04:15.200
and what makes an oak tree is this unobservable essence of oakness, or whatever it presumably has,

04:15.200 --> 04:20.960
and there's some experimental psychology and developmental psychology showing that we

04:21.920 --> 04:31.360
have a similar attitude towards mindedness or having a mind, and that is a somewhat speculative

04:31.360 --> 04:36.640
explanation for why the literature on large language models is so torn, and some people

04:36.640 --> 04:45.760
are quite dismissive, and other people think that it's a step away from AGI. It's that if you

04:46.720 --> 04:53.840
feel like you've got to put large language models into one of two boxes,

04:54.960 --> 05:03.600
the box that has the essence of mindedness for the box that lacks it, then you will be forced

05:03.600 --> 05:10.240
either to say it doesn't have what it takes to do any of the things that we associate with

05:10.240 --> 05:18.640
having a mind such as reasoning, or it has the essential characteristics of mindedness,

05:18.640 --> 05:23.360
and therefore we should expect it to have all of the other properties we associate with

05:23.360 --> 05:29.920
mindedness as well such as consciousness or understanding or whatever.

05:32.480 --> 05:39.360
Right, I think it's worth emphasizing as you did that the background motivation for starting to

05:39.360 --> 05:46.080
write on this general piece in the first place is indeed that the general discourse on AI systems

05:46.080 --> 05:54.080
and LLMs in particular is extremely polarized in a way that is very dichotomous and stark, so

05:54.080 --> 06:00.160
you have one ahead people and one ahead people arguing that these systems are no more than

06:01.120 --> 06:06.640
so-called stochastic parrots that are haphazardly stitching together samples from the training data

06:06.640 --> 06:12.880
and regurgitating them, or that they are no smarter than toaster or that they only do next

06:12.880 --> 06:20.560
stochastic prediction on exploration and therefore it is a non-starter to ascribe to them any form

06:20.560 --> 06:25.840
of cognitive capacity or maybe even a category mistake, and on the other end of the spectrum,

06:25.840 --> 06:30.080
on the other end of the spectrum you have people arguing that the systems are haphazardly

06:30.080 --> 06:35.120
jurors of superhuman intelligence, that they exhibit sparks of artificial general intelligence to

06:36.160 --> 06:42.480
parrots, it's literally the title of a paper by Microsoft on 24,

06:44.960 --> 06:50.800
and many people hyping up the capacity of the systems in a way that might seem very speculative

06:50.800 --> 06:56.960
and untethered from actual empirical results, so there is this huge gap between these two positions

06:56.960 --> 07:02.000
and there's going to be a very rich and complex and nuanced middle ground that is underexplored,

07:04.080 --> 07:10.640
or perhaps I think we did make that point, if not explicitly in the published piece

07:10.640 --> 07:15.920
and in some draft, that there's something reassuring about being able to make definitive

07:15.920 --> 07:22.240
claims about what these systems are and what they do, so either they're re-unsophisticated or they're

07:22.960 --> 07:31.760
very much like us and either of these claims kind of meet somewhere in an way in saying that we

07:31.760 --> 07:36.960
have a clear idea of what the systems do and what they are, and I think it's a little more

07:36.960 --> 07:46.320
epistemic and comfortable to say we have to study them empirically and find out what they

07:46.320 --> 07:52.080
can or cannot do and why and what are the underlying mechanisms, and we simply don't know

07:52.800 --> 07:58.800
a priori just by looking at the architecture, the learning objective, the training data,

07:59.440 --> 08:04.480
these sources of evidence are insufficient to make the definitive claims about what these systems

08:04.480 --> 08:11.280
are capable of, so I think that's part of the big part of the motivation and that fits into that

08:12.080 --> 08:20.320
more academic paper as well. Yeah, one other small side note which we don't make

08:20.320 --> 08:26.640
in the paper but I think might be relevant, especially for people working in philosophy,

08:28.080 --> 08:33.760
LOMs are epistemically uncomfortable, I think that was the phrase you just used, Raph, which

08:33.760 --> 08:41.840
is fitting, not only because they're so new and different but also because they are artifacts,

08:41.840 --> 08:46.080
right, they're things that humans have constructed and engineered and

08:48.880 --> 08:54.800
we don't have a thorough understanding of how they work, I mean mechanistic interpretability and

08:54.800 --> 09:03.120
various behavioral research is helping us improve our understanding but on the whole

09:03.120 --> 09:06.560
our understanding is not nearly as deep as hopefully it one day will be

09:08.240 --> 09:13.520
and this is by itself a really strange situation to be in that we've constructed an artifact that

09:13.520 --> 09:24.960
we only partially understand in the in the past artificial intelligence was seen as a way of

09:26.800 --> 09:30.160
constructing something like an epistemic assistant, right, something that will

09:31.120 --> 09:37.600
help us but not something that will kind of alienate us from the process of coming to know

09:37.600 --> 09:44.400
about the world so I think there's an extra layer of discomfort built into thinking about

09:44.400 --> 09:54.640
large language models and that may also play into the the divisiveness of debates about what they

09:54.640 --> 10:03.200
can do and just to add to that I guess we should be clear that this does not entail in any way that

10:03.200 --> 10:10.800
we think the systems are so completely alien and beyond the reach of our current understanding that

10:10.800 --> 10:17.680
anything goes and that they could very well be you know have like human-like intelligence or

10:17.680 --> 10:21.840
superhuman intelligence and we simply cannot say whether or not they do or because that's

10:21.920 --> 10:28.080
sometimes what you see in some outfits where people frame these systems as noble alien forms of

10:28.080 --> 10:32.960
intelligence that we have created but do not understand our control and that is you know

10:32.960 --> 10:38.320
as a slippery slope that leads some people to then claim that they have all these quite magical

10:39.040 --> 10:43.120
abilities and that's not all what we want to say here and in fact we want to resist

10:44.080 --> 10:48.800
yeah yeah yeah so if we we think that that's just as much of a cup out as

10:50.720 --> 10:56.640
completely dismissing a priori what these systems might be capable of without doing the

10:56.640 --> 11:00.880
work of looking into the capacities with behavioral and mechanistic studies so

11:00.880 --> 11:04.640
we very much want to resist both extremes of the spectrum if that makes sense

11:06.720 --> 11:10.240
okay so now should we move towards the content of the current paper

11:10.240 --> 11:17.040
sounds good yeah okay maybe Raphael I'll just start with the distinction between anthropomorphism

11:17.040 --> 11:27.280
and anthropocentrism and then you can take the next step so everyone is aware of the problem of

11:27.920 --> 11:33.440
anthropomorphic bias in some form I mean anthropomorphism is just the idea of projecting

11:33.440 --> 11:42.720
human qualities onto something non-human and it's quite easy to especially when you're having a

11:42.720 --> 11:48.880
productive successful exchange with a large language model it's easy to slip into this

11:49.920 --> 11:58.160
interpretive mode where you reason about the responses of the large language model

11:58.800 --> 12:07.680
as if they were coming from an agent just like you and maybe that's a useful thing to do in some

12:07.680 --> 12:13.840
circumstances but from a theoretical perspective it's certainly a mistake because large language

12:13.840 --> 12:23.120
model is radically unlike you know a human agent in all sorts of ways but that's only one form of

12:23.120 --> 12:29.520
sort of human-centric bias the other one is anthropocentrism or what we call in that box

12:29.520 --> 12:37.520
article anthropocentric chauvinism and that idea is pretty straightforward it's the idea that

12:38.960 --> 12:46.160
the human way of solving problems is the gold standard of solving problems generally so that

12:47.040 --> 12:53.840
to the extent that a system solves a problem in a way that diverges from the human strategy

12:54.800 --> 13:07.120
it's only using a trick or a faux solution it's it's not using a deep general rational strategy

13:07.440 --> 13:21.280
and in the debate about what large language models can do we think that the anthropomorphic

13:21.280 --> 13:26.240
bias is pretty well recognized and the anthropocentric bias is not so well recognized

13:26.240 --> 13:34.080
and so part of this paper is is or the main idea behind the paper is to present a systematic

13:34.080 --> 13:39.600
analysis of anthropocentric bias how it comes about and how to push back against it

13:41.600 --> 13:46.000
right and we we want to be very clear and hopefully we're playing the paper that

13:46.560 --> 13:51.840
the reason why we focus on anthropocentric bias here is just because it is I think as

13:51.840 --> 13:59.840
Charles mentioned less discussed and less recognized or some forms of it are less

13:59.920 --> 14:04.960
recognized and we make we propose this new taxonomy but it's not at all suggest that it's

14:06.880 --> 14:13.280
more problematic or more important than the anthropomorphic bias that's well discussed in

14:13.280 --> 14:23.040
the literature of the anthropomorphic biases so in other words this is not you know to frame things

14:23.040 --> 14:27.920
in in this slightly problematic dichotomous way of thinking that's common in the discourse

14:28.000 --> 14:37.360
on LLM this is not a paper that is pioneering to the LLM booster or LLM hype camp even though it

14:37.360 --> 14:43.040
is pushing back against a certain form of dismissal of anthropocentric biases that

14:43.040 --> 14:48.960
only exclusively emphasizes the anthropomorphic biases but perhaps we should flesh this out a

14:48.960 --> 14:53.600
little bit already with the first one we make here in the paper about a performance competence

14:53.600 --> 14:58.080
distinction which is a nice way to bring about both anthropomorphism and anthropocentricism

14:58.080 --> 15:06.720
regarding LLM so this distinction is a very classic distinction in linguistics and cognitive

15:06.720 --> 15:13.360
science and it has already been applied to AI systems in the neural networks productively

15:13.360 --> 15:19.360
like Charles Farson so there's nothing really new here but the distinction comes from

15:19.360 --> 15:25.200
Noam Chomsky originally and the idea is that performance pertains to the external behavior

15:25.200 --> 15:33.120
of a system in a particular domain and competence is the kind of setup underlying knowledge and

15:33.760 --> 15:40.560
computations or mechanisms that enable the system to achieve that behavior

15:41.680 --> 15:46.160
and a familiar observation in linguistics and cognitive science is that there is a double

15:46.160 --> 15:56.880
dissolution between performance and competence so if you take for example language I might during

15:56.880 --> 16:01.600
this very podcast make some grammatical mistakes or some other mistakes in fact I've already

16:02.640 --> 16:08.320
misspoke in a few times I think and repeated myself so I made performance errors but this

16:08.320 --> 16:13.200
does not entail necessarily hopefully that I'm an incompetent language user and that I don't have

16:13.280 --> 16:22.080
in the language the competence so that's a well recognized dissociation you can be competent

16:22.080 --> 16:29.840
and yet make some errors and the reason for that is that there might be some additional factors

16:29.840 --> 16:35.840
that are unrelated to the underlying competence that might impede on my performance so for example

16:35.840 --> 16:43.520
I might be distracted when I speak or there might be other effects on my speaking performance that

16:43.520 --> 16:52.720
don't actually originate from a lack of competence but just impede on the idealized expression

16:52.720 --> 16:57.920
external manifestation of my competence and this is why I misspeak but it's also why recognize

16:57.920 --> 17:04.400
that you can have good performance without competence so we give here the example of a

17:04.400 --> 17:12.000
student cheating on a test or memorizing test answers by brute forcing the test to slightly more

17:13.520 --> 17:18.880
I guess gray area but at least in the cheating case a student can ace a test without being

17:18.880 --> 17:27.360
competent at what the test is actually testing for and it's also well acknowledged in cognitive

17:27.360 --> 17:38.960
science that there can be instances like this throughout you know like that can be manifest

17:38.960 --> 17:44.800
certain experimental settings where the test subject is right for the wrong reasons as it were

17:45.440 --> 17:49.840
namely it's doing well it's exhibiting good performance never realize the underlying reason

17:49.840 --> 17:55.600
for the performance is that there was some perhaps some curiosity that the experimenters

17:55.600 --> 18:00.320
all the scientists haven't thought about that could account for his good performance but

18:00.320 --> 18:05.760
but doesn't actually amount to whatever competence they were setting out to test

18:07.760 --> 18:11.680
so we we start by saying well this is what we could nice because the mistakes this is the

18:11.680 --> 18:15.840
association that is like supplied to humans across the board you can have performance

18:15.840 --> 18:21.120
without competence good performance without competence and you can have that performance despite

18:21.120 --> 18:26.880
competence now when it comes to other lamps the point we make just going to scroll as we go

18:28.240 --> 18:35.760
we have some figures to show later but the point we make is that generally people stress the

18:35.760 --> 18:40.320
dissociation apply the distinction to other lamps the stress decision only in one direction

18:41.120 --> 18:46.800
unlike in the case of human where it's bi-directional and so what people do generally is to say well

18:46.800 --> 18:52.160
other lamps famously you know if you look at the gpt4 technical report and and vice other

18:53.520 --> 18:59.440
any any any report about a new state of the art alone they are getting really really good at a

18:59.440 --> 19:06.480
number of tests and about benchmarks and even human exam examinations human exams the bar exam

19:07.120 --> 19:14.480
medical exams etc so they can get a really good performance test that we tend to think are really

19:14.480 --> 19:22.080
difficult tests that one can only pass at least a human could only pass if they have

19:22.640 --> 19:26.000
a really significant nonchalant amount of competency in particular the main

19:27.760 --> 19:32.960
and the point that is often made when it comes to other lamps is be careful slow it on and try to

19:32.960 --> 19:38.480
find out why the model is doing well on that test because there are various reasons why it could do

19:38.480 --> 19:45.520
well that do not actually indicate that the model has the underlying competence that the test was

19:45.520 --> 19:51.120
designed for when it comes to humans so one big concern for example is data contamination

19:51.120 --> 19:59.440
where very large language models train on internet scale data can easily be trained on some test

19:59.440 --> 20:03.680
items from common benchmarks that leak into the training data such that they can then do really

20:03.680 --> 20:11.280
well on the on the on the benchmark just because they've essentially memorized test items and there

20:11.280 --> 20:18.400
are other more subtle more subtle reasons why performance could be very good for the wrong

20:18.400 --> 20:25.440
reasons so that's very well recognized and a lot of the people who push back against anthropomorphic

20:25.440 --> 20:32.160
bias when it gets to other lamps make that point be careful do not take on another anthropomorphic

20:32.160 --> 20:37.920
attitude to the systems the reason why they do well is not because they have human-like intelligence

20:37.920 --> 20:45.040
or human-like current capacities but it's for this trivial contingent or you know otherwise irrelevant

20:45.040 --> 20:50.480
reasons that account for the good performance now when it comes to the other dissociation of the

20:50.480 --> 20:56.480
other dissociation of the other direction there are people are very reluctant to apply to a lamps

20:57.280 --> 21:02.160
and we think it's because essentially people think in a human case you can make sense of the

21:02.160 --> 21:07.680
idea that the human could do badly on a test or corporate could perform badly on a task

21:08.640 --> 21:14.080
and yet have the competence that you're trying to test but there might be some auxiliary factors

21:14.720 --> 21:23.440
such as working memory limitations attention deficits etc that could impede on the performance

21:23.440 --> 21:34.720
but in the language model I think what we argue in the paper is that

21:36.480 --> 21:40.240
people don't a lot of people don't think that there is an analogous

21:42.400 --> 21:46.560
mechanism at play where there could be some kind of auxiliary factor that impede on performance

21:46.560 --> 21:52.080
performance is what you get what you see is what you get and so the performance is a direct

21:52.800 --> 22:02.320
manifestation of what the system is computing and if you have performance errors

22:03.040 --> 22:06.640
that can only be explained by the lack of competence because there is no additional

22:09.840 --> 22:14.960
independent factor or module that could impede on the performance

22:15.680 --> 22:22.160
sources of interference you might say yeah so yeah I mean I'll

22:23.680 --> 22:30.320
pass it over to you Charles I just wanted to set up this distinction yeah yeah

22:32.320 --> 22:38.640
right so I mean if you think about a traditional computer program

22:39.520 --> 22:46.960
well at least if you think about a simple computer program it's odd to think of it as

22:47.520 --> 22:52.240
some sort of complex systems where it's a complex system where one part of it could sort of

22:52.240 --> 22:57.040
interfere with the workings of another part of it but one of the points we want to make is that

22:57.600 --> 23:04.000
something like that is a realistic possibility with large language models

23:04.560 --> 23:11.120
um okay but I suppose the next part of the paper goes into a taxonomy of

23:11.920 --> 23:17.840
anthropocentric bias and the first sort of overarching point is the distinction between

23:17.840 --> 23:25.840
type one and type two so the type one anthropocentrism is the tendency to assume an LLM's performance

23:26.480 --> 23:33.680
failures designed to measure competence always indicates that it lacks

23:35.520 --> 23:43.280
that competence and so we before we so we'll say something about three different kinds of

23:43.280 --> 23:50.160
type one anthropocentric bias but first a background point which is that

23:50.400 --> 24:00.000
um whenever we think it's possible to give a mechanistic explanation of some

24:00.720 --> 24:09.360
complicated phenomenon we always have to foreground some factors some variables

24:09.760 --> 24:22.320
uh and background others and um the properties that we push into the background

24:26.320 --> 24:31.200
nevertheless matter we're still making assumptions about the nature of those properties

24:31.200 --> 24:36.080
when we try to articulate what's going on with the other properties that we're paying more attention

24:36.080 --> 24:42.160
to and if assumptions about those properties in the background turn out to be wrong

24:43.120 --> 24:52.400
then those mistakes will corrupt our explanation that attends only to the foregrounded factors

24:52.400 --> 24:59.600
so that's a little bit abstract let me just give you a simple example um in comparative cognition

25:00.560 --> 25:08.160
one famous uh behavioral experiment is the mirror test for self-recognition

25:09.040 --> 25:18.640
so the question is roughly do non-human animals have something like a concept of self

25:19.840 --> 25:29.520
and the strategy is to put some sort of mark on their body in original experiments it was

25:29.520 --> 25:36.800
a red dot on the forehead of maybe a monkey and or a bird or whatever and then

25:38.960 --> 25:46.640
you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark

25:47.760 --> 25:51.920
and if it does make an attempt to get rid of the mark that shows that it recognizes that the

25:51.920 --> 26:00.320
image in the mirror is an image of itself and otherwise not um so that that's a cool way to

26:00.320 --> 26:09.120
get a really difficult and abstract question about the mind of a non-human animal but it presumes

26:09.120 --> 26:16.080
or it assumes that animals will care about the fact that they have a red dot on their

26:16.080 --> 26:20.720
forehead that they will be bothered by that and be motivated to get rid of it and if that

26:20.720 --> 26:26.480
assumption is wrong then they might fail the mirror test for self-recognition for reasons that

26:26.480 --> 26:34.800
have little to do with the presence or absence of a capacity for self-recognition until something

26:34.800 --> 26:45.760
similar to that is going on we say in large language models so the first example that we give is to

26:45.760 --> 27:00.240
do with task demands so uh you can it's a pretty natural uh idea that whenever you set up a behavioral

27:00.240 --> 27:08.800
task um there will be demands associated with that task that are not directly related to the

27:08.800 --> 27:14.240
capacity that you're trying to test so to take the most obvious example that I can think of

27:14.240 --> 27:18.720
if you give someone a written test they have to be able to write they have to you know have a hand

27:18.720 --> 27:26.880
and a pen and whatever uh and um if their you know hand uh was injured or whatever and they couldn't

27:26.880 --> 27:33.040
write then um their failure to fill out the test wouldn't tell you anything about their uh you know

27:33.760 --> 27:45.920
academic knowledge um so we suggest that there are auxiliary task demands uh in behavioral

27:45.920 --> 27:52.960
tests of a large language model and they're subtle um you wouldn't you wouldn't think of this right

27:52.960 --> 27:59.760
away but um we talk about a paper from uh who and frank uh they have a couple of papers on this topic

27:59.760 --> 28:08.320
but what they do is they give a large language model um the following sort of question uh this

28:08.320 --> 28:14.080
is a question it's for a grammaticality judgment so you can see on the image there which sentence

28:14.080 --> 28:19.600
is better in english number one every child is studied number two every child have studied

28:20.400 --> 28:27.280
answer with one or two and it gives the wrong output but then

28:29.600 --> 28:36.320
you can also simply look at the probabilities assigned to each of those

28:37.520 --> 28:47.040
sentences within the model and uh figure out directly whether the model thinks that input

28:47.040 --> 28:54.240
A is more likely than input B and it turns out that on a wide variety of questions of this kind

28:55.280 --> 29:02.640
the direct comparison uh does or the large language models perform better with the direct

29:02.640 --> 29:12.400
comparison than they do with the more complex demand for uh metalinguistic judgment so the

29:12.400 --> 29:21.280
fact that the model has to process the numbering of the uh options and then answer in terms of a

29:21.280 --> 29:31.280
number uh is a subtle but nevertheless um important additional variable in the experiment

29:31.840 --> 29:36.320
and that can influence the model's capacity to get the answer right

29:36.560 --> 29:38.960
Rafael do you want to add to that?

29:40.640 --> 29:45.360
No I think that well maybe we can mention briefly the other example that we discuss which is from

29:46.480 --> 29:54.480
this paper by Andrew Lampinen which which kind of has a little extra ingredient that makes

29:55.680 --> 30:01.120
the example interesting and even more problematic in terms of comparative psychology which is

30:01.840 --> 30:12.720
um one way in which auxiliary task demands can be ignored or disregarded or overlooked

30:13.520 --> 30:20.400
is when uh you are doing a direct comparison between humans and other epsilon tasks and

30:20.400 --> 30:26.640
the experimental conditions are mismatched in such a way that the task as you set up

30:27.600 --> 30:35.040
impose the stronger demands stronger task and auxiliary demands on the LOM than those on human

30:35.040 --> 30:40.240
subjects and that's something that that can happen quite often and so there is this this

30:40.240 --> 30:46.800
interesting example from a couple of papers originally published by black reds and colleagues

30:46.800 --> 30:53.040
from standard hands group where they looked at um the ability of language models to handle their

30:53.040 --> 31:03.680
recursion um looking at center embedded closes uh how such closes might um

31:05.680 --> 31:10.160
for example when you had a prepositional phrase within the subject of the sentence and the verb

31:10.160 --> 31:17.680
might throw up either humans or LOMs into agreeing the the verb in the wrong way

31:18.480 --> 31:27.600
so giving the wrong number to the verb for example the keys that the man put on the table

31:29.280 --> 31:34.400
here it should be R because keys is plural but because you have close in the middle

31:35.200 --> 31:39.680
and if you add more closes like this that are embedded in the middle people and LOMs can get

31:39.680 --> 31:47.520
confused and and predict that the verb should be is for example so um they tested this on humans

31:47.520 --> 31:54.000
and LOMs and found that on the more complex examples involving complex more complex constructions or

31:54.000 --> 32:01.280
recursion humans were doing decently well but LOMs performance was collapsing compared to the

32:01.280 --> 32:08.400
single examples and I've heard an opinion from DeepMind um looked into that and realized that

32:08.400 --> 32:13.840
the experimental conditions were mismatched so the humans at this very common in cognitive

32:13.840 --> 32:18.960
science experiments were getting some training before they completed the test items to just get

32:18.960 --> 32:25.600
familiarized with the task so they were given some examples of the task um harsh condition

32:26.400 --> 32:36.160
and the LLabs were just prompted zero shots as um people usually put it so just um without any

32:36.240 --> 32:44.160
example just point blank and Andrew found out that if you he he he replicated the experiments but

32:44.160 --> 32:51.360
I did some proper matched testing conditions for the LL so adding some examples of the task in the

32:51.360 --> 32:57.680
prompt when it's known as future counting and with that he found that performance was equivalent

32:57.680 --> 33:02.320
in fact the LLM that he tested was slightly better on the more complex constructions than humans

33:03.040 --> 33:06.560
so when you match the test conditions here you actually match also

33:07.520 --> 33:13.360
at least you it's it's not it's not automatic but you you you can match the test demands I mean

33:13.360 --> 33:17.680
it could be that there are reasons why the various experimental conditions would result in different

33:17.680 --> 33:24.320
demands for humans and others but you're still in this case even on the playing ground playing field

33:25.360 --> 33:30.480
in such a way that you don't find the behavioral discrepancy that you found initially anymore

33:33.040 --> 33:41.840
um yeah um good so shall we continue to the next

33:42.800 --> 33:53.840
section um so another uh another way that auxiliary or another type of auxiliary task

33:53.840 --> 33:59.600
demand is input independent computational limitations um and here we're thinking of a

33:59.600 --> 34:05.600
few papers that show that the number of forward passes that the transformer can make

34:06.320 --> 34:13.680
influences its ability to find the right spot and parameter space so neural networks are

34:16.800 --> 34:20.080
function approximators but their

34:20.080 --> 34:31.040
um their ability to approximate a function can be eliminated uh can be limited by the

34:33.200 --> 34:40.480
the the number of computations it's allowed to perform and um the

34:40.960 --> 34:48.640
uh sort of crucial feature of uh transformers in this example is that

34:49.680 --> 34:57.920
um the number of operations that determines the next token is limited by the number of

34:57.920 --> 35:08.000
tokens that it's seen so far and it turns out that if you train a transformer with

35:09.680 --> 35:23.200
additional meaningless tokens like pause tokens like the word pause you can increase its accuracy

35:23.920 --> 35:32.640
across a range of of question types um and yeah this is

35:35.040 --> 35:42.960
this counts as an auxiliary task demand in our view because um it's doing something roughly

35:42.960 --> 35:50.240
analogous to sort of giving the model that's the necessary factor right but not yeah yeah sorry

35:50.800 --> 35:58.640
yeah um it's it's doing something like giving the model uh time to think and um

36:00.400 --> 36:10.080
yeah so so you might think that the absence of that additional inference time is a factor

36:11.280 --> 36:17.840
that is not directly not conceptually related to its capacity to answer

36:18.080 --> 36:24.720
uh a question like the one on the screen um you know a simple

36:24.720 --> 36:32.240
earth medical question uh graph do you want to fill in more yes yeah no so I think that analogy

36:32.240 --> 36:38.560
is is a nice one time to think because if you if you tested a human on even a simple mathematical

36:38.560 --> 36:45.200
questions or any any task really and just ask them you know tell them they have like one second

36:45.200 --> 36:52.800
to just blow it out answer performance would probably be pretty bad um and you can think of

36:52.800 --> 36:59.360
asking an LLN to answer a question point blank as very very loosely analogous to that and obviously

37:00.240 --> 37:05.600
this is an analogy and there are very important differences here but I think it's a helpful

37:05.600 --> 37:14.080
heuristic to think about what is um what is going on roughly here and then we uh in in both cases

37:14.880 --> 37:22.000
the system the human or the LLN does not get the chance to perform the necessary computations to

37:22.720 --> 37:28.000
derive the correct answer and so yeah what we talk about in the paper is that you have these

37:28.000 --> 37:35.680
experimental works during that if you ask a question to a language model um the amount of

37:35.760 --> 37:41.120
tokens it's a it's it's that allows to generate before providing the answer

37:43.040 --> 37:48.560
makes a difference to how accurate it is so if it uh it just generates a few tokens then have to

37:48.560 --> 37:52.400
give an answer or even if you just have to give the answer point blank with the very first token

37:53.200 --> 37:57.280
it's going to be less accurate that if you give it a chance to generate a number of tokens before

37:57.280 --> 38:04.320
giving the answer so the usual way in which this is understood is that when you ask when you

38:04.560 --> 38:08.400
you you allow the LLN to generate a number of tokens before giving the answer or you even

38:08.400 --> 38:13.600
prompted to do so you say things step by step for example um that's not as chain of thought

38:13.600 --> 38:19.200
prompting and essentially what you're doing is you're forcing the LLN to generate a reasoning trace

38:19.760 --> 38:25.680
or what looks outwardly externally like a reasoning trace in the output before giving an answer

38:26.720 --> 38:30.000
and we know that chain of thought prompting increases performance accuracy

38:30.720 --> 38:42.560
um but what was found by a couple of papers um is that the mechanistic influence of this process

38:42.560 --> 38:47.360
is not entirely due to the nature of the tokens that are generated in this reasoning trace

38:47.920 --> 38:54.960
in other words it's not just that the LLN has to generate the right tokens corresponding to

38:54.960 --> 39:01.120
different steps of reasoning before giving an answer in fact the very back that you allow the

39:01.120 --> 39:07.440
LLN to just generate tokens any token before giving an answer from a mechanistic perspective

39:07.440 --> 39:13.920
affords the system to perform additional computations that can complete the computational

39:13.920 --> 39:19.520
circuit that otherwise would get a chance to be completed and to derive the correct answer

39:19.520 --> 39:24.400
so as Charles mentioned you can have you can set up an experiment when you have the LLN just

39:24.400 --> 39:31.600
generate meaningless tokens like usual just the dots a bunch of dots dot tokens before giving the

39:31.600 --> 39:38.560
answer and the the more dots you allow before the token gives the answer the greater the

39:39.200 --> 39:45.200
expressivity of the system and the more um the more kinds of programs problems you can answer

39:46.400 --> 39:50.720
and so as Charles mentioned every time an LLN is generated in a token the LLN is performing

39:50.720 --> 39:56.720
one forward pass and so the more tokens it's generating the more forward passes it's doing

39:57.280 --> 40:02.160
and one way to think about what's going on here as well is that having these additional forward

40:02.160 --> 40:07.360
passes where you you feedback the whole input sequence plus the previously generated tokens

40:07.360 --> 40:12.560
to do the system to generate the next is also a way to introduce a form of recurrence in

40:12.560 --> 40:18.080
transformers that are not in terms of the architecture of recurrent networks so that

40:18.080 --> 40:26.320
increases the expressivity and you know in complexity of your unique terms and yeah there is

40:26.320 --> 40:33.680
there is pretty compelling evidence that if you don't allow for that then you're imposing a limitation

40:34.480 --> 40:39.520
that again we think is very very loosely analogous to prompting a human to answer a

40:39.520 --> 40:45.200
point on the question without thinking so that's the next sense an auxiliary factor because

40:45.200 --> 40:51.200
if you give the LLN the opportunity to generate enough tokens it might have the competence

40:52.480 --> 40:58.320
to solve a task but you might not see that otherwise and you might get performance errors

40:58.320 --> 40:59.920
but you do think it's incompetent

41:03.040 --> 41:10.720
all right yeah okay um so the the third type of type one anthropocentric

41:11.600 --> 41:17.520
bias that we talk about is mechanistic interference and so this comes from

41:18.480 --> 41:22.160
the mechanistic interpretability work and the basic idea is that because

41:24.080 --> 41:29.280
large language models are capable of in-context learning they can learn different strategies

41:30.160 --> 41:34.480
for solving a different particular type of problem and the strategy that they

41:35.200 --> 41:41.680
implement at a given time can be different so you can talk about this in terms of

41:42.400 --> 41:49.280
virtual circuits that are formed inside the language model and there's some interesting work from

41:50.240 --> 41:56.400
nil nanda and others showing that in some circumstances these two circuits can compete

41:56.400 --> 42:05.680
with one another so at a certain level of uh or after a certain amount of training

42:06.640 --> 42:13.840
you get one circuit operative after a bit more training you have two different circuits

42:14.880 --> 42:22.480
but they're uh the first circuit is still sort of dominant and then after additional

42:23.440 --> 42:25.040
training the model

42:28.000 --> 42:35.520
converges on on the second circuit and the first one slowly gets sort of

42:38.160 --> 42:44.240
it sort of ceases to influence the internal operations of the model and it's only once

42:44.320 --> 42:49.600
you reach that third phase at which the

42:52.480 --> 42:59.200
the benefits of the second circuit with respect to the first become visible

43:00.240 --> 43:05.600
so you can you can show using decoding work that the second circuit is there

43:06.560 --> 43:12.400
uh before you can show behaviorally that the second circuit

43:14.720 --> 43:23.440
yields better performance accuracy on on the task so um I suppose there's a combination

43:23.440 --> 43:29.840
of two ideas here one is that um there are different strategies a model can

43:30.320 --> 43:41.600
implement for solving a problem we can detect those strategies internally using decoding methods

43:42.400 --> 43:46.640
um so three ideas and then the third is uh

43:50.400 --> 43:53.280
a good strategy can be

43:54.240 --> 44:01.120
uh present in some sense in the model um before it has had the chance to influence

44:01.840 --> 44:09.440
behavior um and and so this is just another way that the link between

44:10.000 --> 44:15.440
performance and competence is shown to be more complicated than I might seem at first

44:16.080 --> 44:17.440
graph

44:19.760 --> 44:25.120
and um yeah just to to clarify one thing so the circuits are just um

44:26.480 --> 44:30.240
you know ways to think about the causal structure of a neural network and

44:30.240 --> 44:35.360
there's essentially computational subgraphs of the network that have a specific function

44:35.360 --> 44:39.280
you can think of a circuit as implementing a particular algorithm or set of computations

44:40.000 --> 44:44.000
um it's a part of what people are interested in in this mechanistic interpretability literature

44:44.000 --> 44:50.320
that we build on people like Neal Mandatrisola and others is reverse engineering the circuit

44:50.320 --> 44:57.440
steps in deep neural networks and large language models um peer to implement certain well-defined

44:57.440 --> 45:03.680
algorithms in some cases at least um and the emerging picture that we build on here is that

45:05.120 --> 45:11.520
there is a lot of redundancy built into neural networks as they learn to perform a task

45:11.600 --> 45:18.240
optimized as a function that in many cases translates into redundant circuits that relate

45:18.240 --> 45:27.040
to the same tasks the same kinds of um the same kinds that we put out with my things and uh for

45:27.040 --> 45:31.840
these circuits might be somewhat identical circuits that are just redundant or they might be

45:32.480 --> 45:37.280
different algorithms just to do to do a similar thing and different strategies to solve some

45:37.280 --> 45:41.280
problem I swear maybe one will be a bit more approximative and the other one a bit more exact

45:41.280 --> 45:49.760
more computationally intensive so that's where you can have some interference um where one uh

45:50.720 --> 45:58.160
or at least some competition where once your kid takes over another and um such that the other

45:58.160 --> 46:03.200
becomes kind of you know it's there it's latent in the system but you don't get a chance to influence

46:03.200 --> 46:08.800
behavior on a specific input so you can get a performance error for that reason and these can

46:08.800 --> 46:14.960
combine with the other things we mentioned here so things like task demands the first thing we

46:14.960 --> 46:21.280
discussed as well as the number of tokens you generate both of these things could cause a

46:21.280 --> 46:27.120
particular circuit to take over another um so it's it's we can think of this holistically as

46:27.920 --> 46:32.320
perhaps if you ask a question point blank to a model without letting it generate

46:32.400 --> 46:37.120
bunch of tokens before giving an answer then one particular approximate circuit might take over

46:37.120 --> 46:43.120
that gives the wrong answer if you let it generate more tokens then another more exact circuit might

46:43.120 --> 46:50.800
be given a chance to um it could influence the output using the right answer and similarly with

46:50.800 --> 47:02.320
task demands uh strong task demands might uh in some cases um impede on the uh triggering of a

47:02.320 --> 47:08.080
certain circuits that would otherwise have given the right answer um so that could be the case

47:08.080 --> 47:13.520
perhaps in the lacrets and lumpy an example where giving examples of the task in the prompts

47:14.080 --> 47:20.000
might actually prime the word circuits to solve the task about complex recursive cases in the right

47:20.000 --> 47:28.240
way um so yeah these are the three main auxiliary factors that relates to what we call type one

47:28.240 --> 47:33.680
anthropomorphism anthropocentrism sorry i guess we should we've been a bit long ways we should

47:33.680 --> 47:40.880
be quick on type two do you want to uh pick fix things after child yeah so type one uh deals

47:40.880 --> 47:49.920
with cases where performance of the model is um weak compared to humans so the model doesn't do

47:49.920 --> 47:57.760
so well um and then type two is when the model does do well but nevertheless is different in some

47:57.760 --> 48:06.000
respect from the uh performance profile of the human or we have some evidence to think that the

48:06.000 --> 48:14.560
model uses a different strategy than humans typically use and um the idea is that um even

48:14.640 --> 48:20.800
once you hold performance equivalent or average performance equivalent um you know making a different

48:20.800 --> 48:27.440
pattern of errors or adopting a different strategy as evidenced by uh you know some interpretability

48:27.440 --> 48:37.840
work any deviance from the human strategy is evidence of fragility or only a trick solution

48:38.320 --> 48:49.680
um and uh this point is a bit more philosophical i suppose but the um idea is that

48:51.760 --> 49:01.280
the human strategy for solving a problem um isn't necessarily the most general strategy

49:02.000 --> 49:12.880
for solving a problem and uh what matters is whether the strategy that is pursued by the model

49:12.880 --> 49:20.240
is general whether it's robust whether it's accurate uh and not merely whether it mirrors the human

49:20.240 --> 49:33.280
strategy um yeah and we end the we end the paper by considering an objection um which is um

49:35.760 --> 49:46.000
why um like given that um in humans we study cognition largely through language

49:46.960 --> 49:53.760
um and given that elements are trained on language or um linguistic outputs from

49:53.760 --> 50:01.120
humans um isn't it appropriate after all to treat um human cognition as the correct or

50:01.120 --> 50:07.760
appropriate the obstacle to study elements and we to that we answer that it depends how we think

50:08.320 --> 50:15.040
of that dialectic um so we acknowledge that there is there is no

50:16.480 --> 50:20.720
really other option than to start or investigation of cognitive abilities in algorithms

50:21.280 --> 50:28.080
but with reference to human cognitive abilities using human cognitive abilities as some kind of

50:28.080 --> 50:33.360
realistic or reference points things like theory of mind memory metacognition

50:34.320 --> 50:39.520
various forms of reasoning etc that are familiar to us because we humans have them

50:40.880 --> 50:44.960
and this is the same thing by the way in animal cognition for example or in developmental psychology

50:44.960 --> 50:51.840
where in any comparative psychology setup um the reference point for what concepts

50:51.840 --> 51:01.360
psychological capacities initially at least um is necessarily tied up with our conception of what

51:01.360 --> 51:07.120
we human we humans have in our repertoire of cognitive capacities but we emphasize that this

51:07.120 --> 51:11.760
is only the starting point so here we've over written from uh the philosopher Ali Boyle who

51:11.760 --> 51:16.720
calls this investigative kinds investigative cognitive kinds we start with a cognitive

51:16.720 --> 51:23.280
kind like memory or metacognition episodic memory metacognition theory of mind um as

51:23.280 --> 51:27.200
as an investigative starting point the starting point of the investigation and then we have

51:27.200 --> 51:34.160
that we we can try to start operationalizing operationalizing this concept this kind this

51:34.160 --> 51:40.480
cognitive capacity in an experiments testing the algorithms on it with an open

51:41.280 --> 51:47.280
mandated empirical approach and then based on the results from that each relatively refine

51:48.560 --> 51:53.920
the capacities that we are the capacity that we're targeting or the definition of the capacity

51:54.080 --> 52:01.120
targeting in a way that could gradually lead us to share the the initial anthropocentric assumptions

52:01.120 --> 52:10.960
that we have such that as the experimental um project runs a course or as as we make as we

52:10.960 --> 52:19.360
we get more results and refine our concepts we may end up with um something that no longer

52:19.360 --> 52:28.320
looks like looking trying to find human like episodic memory in ravens or uh in um or in LMS

52:29.120 --> 52:37.040
but ends up looking like looking for something that some capacity that is that shares some similarity

52:37.040 --> 52:42.560
with human like human episodic memory but is different in other respects um and so we can

52:42.560 --> 52:48.880
gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric

52:50.000 --> 52:54.480
so we emphasize that it is kind of due to feedback look here that's that's that's premised on open

52:54.480 --> 52:59.920
minded empirical investigation that doesn't settle this question a priori but has to start

53:00.800 --> 53:04.240
as a necessary starting point with the the reference to human cognition

53:05.120 --> 53:06.800
i don't know if you want to add to that joss

53:09.600 --> 53:14.400
um no i think that's pretty good maybe we should uh move on to questions

53:14.640 --> 53:25.840
and yeah some good awesome wow you can stop sharing or you could leave it up but i couldn't

53:25.840 --> 53:29.280
move it awesome okay

53:33.520 --> 53:40.640
yeah a lot of interesting pieces there so thank you i'll read some questions from the live chat

53:40.720 --> 53:47.760
but first i just wanted to read a short quote from the 2022 active inference textbook they wrote

53:49.280 --> 53:57.200
um on page 195 some decades ago the philosopher denet lamented that cognitive scientists devote

53:57.200 --> 54:02.880
too much effort to modeling isolated subsystems e.g perception language understanding whose

54:02.880 --> 54:10.000
boundaries are often arbitrary he suggested to try modeling the whole iguana a complete cognitive

54:10.000 --> 54:14.880
creature perhaps a simple one and an environmental niche for it to cope with

54:16.080 --> 54:23.440
so it's interesting about the approach that you're taking this is kind of a simple synthetic

54:23.440 --> 54:31.440
iguana but that's leading to to the the bringing to bear of a lot of these empirical phenomena

54:32.160 --> 54:37.840
because there is something and and so i saw in the presentation paper kind of this call for like

54:38.400 --> 54:45.680
deliberate investigation rather than just chopping up the iguana a priori with a framework that

54:45.680 --> 54:54.000
that applies to humans or that centers humans or or that just uh soothes the epistemic challenge

54:54.000 --> 55:05.360
that's presented okay okay first question from dave he wrote

55:08.480 --> 55:14.400
have you looked at daniel denitz's distinction between competence without awareness and

55:14.400 --> 55:21.120
competence with awareness he expands on this in the 2023 from bacteria to Bach and back

55:21.920 --> 55:27.920
i find this much more valuable than chomsky's highly problematic performance without competence

55:27.920 --> 55:34.960
a situation that chomsky posits but doesn't look at deeply where do you put awareness in all of this

55:34.960 --> 55:46.160
competency uh well maybe i'll let you think uh that one is i can trust because you're

55:46.160 --> 55:53.760
you're maybe more within it than i am but i'll just say um awareness is a very polysemous

55:53.760 --> 56:00.080
term like many terms in philosophy of minds but partially this one more than many others i think

56:00.800 --> 56:08.400
so um it can mean a lot of different things in all of context here we don't focus on things

56:08.400 --> 56:14.800
like consciousness because i think we probably both agree that it's a less tractable uh maybe

56:14.800 --> 56:23.120
empirical problem to try to assess the presence or absence of consciousness in language models

56:23.120 --> 56:27.840
even though many people are interested in that we think that we have more hope of making progress

56:27.840 --> 56:35.360
in the near term with more well-defined cognitive capacities or cognitive functions and things that

56:35.360 --> 56:44.480
relate to forms of certain forms of reasoning and viable binding etc um so we our framework and

56:44.480 --> 56:49.760
principle would apply to things like consciousness or as you put awareness generally speaking but

56:50.720 --> 56:55.040
we don't really focus on that for examples the other quick thing i'll just mention is that i

56:55.120 --> 57:01.600
seem to remember that the phrase from then but again i'm not a then scholar was competence

57:01.600 --> 57:08.640
without comprehension um which seems a little different from competence without awareness

57:08.640 --> 57:15.360
perhaps depending on how you think of comprehension um and yeah i think that does i think it is a

57:15.360 --> 57:22.160
very interesting phrase that it does um in fact i had this project that's unpublished with

57:22.160 --> 57:29.600
chris dolega who i think you had on the podcast as well um on semantic competence in language models

57:29.600 --> 57:37.040
where we use that phrase um to kind of avoid taking its stance on the kind of messy

57:38.160 --> 57:43.680
muddy question of whether hella let's understand language which builds in all sorts of assumptions

57:43.680 --> 57:48.400
including about consciousness actually for some people like chancel um and we focus on

57:48.400 --> 57:53.920
the more restricted notion of competence and i think our paper here also has that property that

57:53.920 --> 57:58.880
would if we have originalized competence we end up operationalizing competence in terms of

57:59.440 --> 58:04.640
the sets of knowledge of the mechanism and mechanisms that enable a system to generalize

58:04.640 --> 58:09.920
well in a given domain basically and in a way that's a supposed evolutionary compared to

58:10.880 --> 58:14.480
some more expensive understandings of competence that

58:15.840 --> 58:22.400
we need to comprehension or understanding more well but i'll let you take that one charles

58:24.880 --> 58:31.200
no yeah that was that was good um the phrase you know the distinction that denadra's is between

58:32.160 --> 58:39.200
competence with comprehension and without and i think um competence with comprehension is the

58:39.200 --> 58:45.120
ability not just to pursue a strategy that's successful for solving a problem but to um

58:45.120 --> 58:52.640
articulate the strategy in such a way that you could teach it for example and um humans only

58:52.640 --> 59:00.960
sometimes have competence with comprehension we have many competences that lack comprehension

59:00.960 --> 59:08.720
right um you know when we learn to walk for example um we have an amazing competence that we

59:09.520 --> 59:15.440
still can't quite translate into robotics because we don't fully understand how it works

59:17.600 --> 59:24.800
and when it comes to our language models i think we should

59:25.440 --> 59:39.440
not expect uh comprehension i mean they have a an amazing suite of competences if you thought that

59:39.440 --> 59:44.160
they also had comprehension then i suppose you would think like well if you want to understand

59:44.160 --> 59:50.720
how a large language model works you can just ask it but that's that's a bad strategy nobody nobody

59:51.280 --> 59:57.840
about um how a large language model works so so they're on the um competence without

59:57.840 --> 01:00:08.960
comprehension side of things um and in order to figure out what in order to figure out what

01:00:08.960 --> 01:00:13.360
the mechanisms are that enable its competencies we have to pursue strategies that are broadly

01:00:13.360 --> 01:00:18.720
similar to the strategies we use in you know cognitive psychology or cognitive linguistics

01:00:19.440 --> 01:00:23.440
um and you know we have to run experiments so i think that that's all very compatible with

01:00:23.440 --> 01:00:31.920
Dan's way of looking at things um one other thing i'll mention um Dan's so Dan was

01:00:35.200 --> 01:00:42.560
quite influential to me and we actually wrote a commentary together which pushes back a little

01:00:42.560 --> 01:00:49.040
bit on a simple understanding of this distinction so we were looking at um the evolution of

01:00:49.040 --> 01:00:58.320
metacognition and basically what we argue is that um given the gradualism of evolution there

01:00:58.320 --> 01:01:03.200
must have been something in between base level cognition and metacognition so we shouldn't

01:01:03.200 --> 01:01:12.800
see that distinction as black and white and um you know i think that if you want to contrast the

01:01:12.800 --> 01:01:28.400
sort of uh cognitive prowess of um a human adult with lots of linguistic and scientific

01:01:28.400 --> 01:01:37.360
uh concepts at her disposal with you know a non-human animal then this strong distinction

01:01:37.360 --> 01:01:44.960
between competence with and without comprehension is reasonable um but in the space of all possible

01:01:44.960 --> 01:01:53.280
minds we should be open to the view that there can be you know semi-competent um forms of cognition

01:01:53.840 --> 01:02:00.240
and just to put it on this uh it occurred to me while I was listening to you as well that um

01:02:00.240 --> 01:02:04.400
the first example of auxiliary taxas we gave auxiliary tax demands in specifically Hugh and

01:02:04.400 --> 01:02:11.120
Frank example is a nice is a nice example where in order to give the metacognistic judgments

01:02:11.120 --> 01:02:15.120
correctly so see that that you would need competence with comprehension because you need to

01:02:15.920 --> 01:02:21.760
understand not only be able to to come to to agree the verb with the subject but know the rule and

01:02:21.760 --> 01:02:30.800
know how to formulate it perhaps realize it uh for example to teach someone right and so uh and so

01:02:30.800 --> 01:02:35.280
when you find that the L.M. can do well at the low task the member of the task and at the high

01:02:35.280 --> 01:02:41.280
task development explicit metacognistic judgments in some way that's an example of the L.M. having

01:02:41.280 --> 01:02:53.600
competence with comprehension yeah yeah nice awesome okay upcycle club wrote question given

01:02:53.600 --> 01:03:01.840
that LLMs inherently reflect anthropocentric biases due to their training on human data and goals

01:03:01.840 --> 01:03:07.040
how can we ensure that their inter model discourse aligns with humanity's values

01:03:11.760 --> 01:03:12.400
um

01:03:15.280 --> 01:03:21.440
so the inter white discourse right be inter model discourse perhaps amongst the models

01:03:22.560 --> 01:03:31.520
I see I see like in that farmville paper yeah the small the yeah both generative agents get

01:03:31.520 --> 01:03:39.520
the small ones uh yeah I think that's beyond the scope of this paper to be honest but um

01:03:39.760 --> 01:03:46.560
I mean we could use about it yeah but I don't know that we have I don't know this project

01:03:46.560 --> 01:03:52.400
has much I mean I I think we both have an interest in the alignment problem uh independently of this

01:03:52.400 --> 01:03:58.320
project but I don't think this project has much to say really about this I'm not sure what you think

01:03:59.280 --> 01:04:00.880
uh yeah yeah I don't

01:04:01.360 --> 01:04:12.240
yeah I don't have anything super concrete from that okay Dave asks an example of inserting noise

01:04:12.240 --> 01:04:19.840
into LLM training that was the section about the extra tokens do you see any analog to

01:04:19.840 --> 01:04:23.600
intermittent reinforcement to uncertainty tolerance

01:04:24.480 --> 01:04:32.000
because you mentioned the extra tokens in the chain of thought and how that could also be replaced

01:04:32.000 --> 01:04:41.280
by by dot dot dot dot dot and so like what is that telling us about model training when um

01:04:42.000 --> 01:04:46.960
it seems like there's some situations where adding superfluous tokens would diminish signal

01:04:46.960 --> 01:04:51.280
in data sets but then here are other situations where it seems to actually help

01:04:54.480 --> 01:05:03.440
um um yeah so in that particular paper I think it's called thinking dot by dot and there is a

01:05:03.440 --> 01:05:13.040
subtitle um it's by Will Merrill and Jacob Sparrow I think um in that paper um if I recall correctly

01:05:13.040 --> 01:05:22.480
what they did is that they they introduced just this one field of token swan meaning this token

01:05:22.560 --> 01:05:29.040
just to hold but just a dot and they trained the model to give an answer after producing

01:05:29.040 --> 01:05:35.280
a certain number of dots that's not just like introducing Rambam and Gibberish in your training

01:05:35.280 --> 01:05:42.480
data it's actually quite a specific intervention that forces the model to um learn to perform

01:05:42.480 --> 01:05:50.800
certain computations before giving an answer um so so it makes sense to me that this couldn't

01:05:50.800 --> 01:05:55.200
diminish performance and like you you could do that from that it's not quite the same as just

01:05:55.200 --> 01:06:00.960
having that training data right um just because the token seems meaningless it's a field of token

01:06:00.960 --> 01:06:07.760
to dot um it's not just random gibberish it's going to throw off the model and and impede its

01:06:07.760 --> 01:06:14.480
its uh the optimization of its learning function or at least good downstream performance um but what

01:06:14.480 --> 01:06:18.320
it's going to do is going to force the model to learn that when there is a dot token it can

01:06:18.320 --> 01:06:22.560
allocate computation with its attention heads and other parts of the architecture in such a way

01:06:22.560 --> 01:06:30.640
that it's um getting towards deriving the correct token when it's finally producing

01:06:30.640 --> 01:06:36.240
the token that matters and that's meaningful after the series of dots um yeah I don't know

01:06:36.240 --> 01:06:41.600
Charles if you have another answer yeah I mean I think it's an important question because

01:06:41.600 --> 01:06:49.840
a priori uh if someone said look we're going to upend and prepend a whole bunch of meaningless

01:06:49.840 --> 01:06:57.840
symbols to an LLM input you might very well think that this will just weaken the signal to

01:06:57.840 --> 01:07:08.800
noise ratio and degrade model performance so it's against that background that the empirical result

01:07:09.040 --> 01:07:16.000
doesn't degrade model performance um ought to be regarded as an important clue about how the model

01:07:16.000 --> 01:07:22.320
works so I think that the the intuition behind this question is indeed part of the interpretation

01:07:22.320 --> 01:07:29.200
of the empirical results right it's surprising for exactly this reason and then the theory that's

01:07:29.200 --> 01:07:34.160
supposed to you know well this is an active inference podcast right so the theory that's

01:07:34.160 --> 01:07:40.480
supposed to help rid some of the surprise here is um the idea that

01:07:44.160 --> 01:07:50.720
given the uh architecture of a transformer where it's it has to go through all the tokens

01:07:51.360 --> 01:07:59.440
in every cycle um having these extra tokens gives it uh sort of more computational bandwidth

01:07:59.440 --> 01:08:06.400
and therefore more expressivity or more capacity to uh you know locate uh the right

01:08:06.400 --> 01:08:14.640
spot and parameter space and and even that in a way reminds me of so it's not just that dots

01:08:14.640 --> 01:08:18.800
improve performance it's not it's that it was like you mentioned it was trained to have that

01:08:18.800 --> 01:08:26.320
and similarly it could have been trained maybe hypothetically to just output Shakespeare quotes

01:08:26.320 --> 01:08:32.880
verbatim while you're processing so that's kind of like a filler or more of like a sort of

01:08:33.680 --> 01:08:42.160
that was a great question it's like these are linguistic paddings that that do create time

01:08:42.880 --> 01:08:51.440
to to get to the meet so not only does it signal and signpost if it's being trained to have that

01:08:51.440 --> 01:08:56.640
meaning which then questions like so that it wasn't a meaningless dot if it if it had a um

01:08:56.640 --> 01:09:06.000
a cognitive or even like a a semantic um aspect I had a question how do you feel like in this

01:09:08.080 --> 01:09:15.520
era Cambrian explosion of diverse intelligences how can we understand capacities

01:09:16.320 --> 01:09:26.880
when they seem so conditional upon the setting and how the system of interest is interacted with

01:09:27.600 --> 01:09:34.160
like what are the practical implications for people who are studying LLMs and other

01:09:34.160 --> 01:09:39.920
synthetic intelligences from like a safety or reliability or performance perspective

01:09:39.920 --> 01:09:45.680
that was we're gonna

01:09:47.280 --> 01:09:48.320
no I was drawing it to you

01:09:50.400 --> 01:09:54.560
so so so just want to chat to some of the questions so the question is

01:09:58.560 --> 01:10:02.080
how the how is the notion of a capacity

01:10:03.040 --> 01:10:10.480
um changing when we have such different systems that seem to have intelligent behavior

01:10:10.480 --> 01:10:16.640
yeah and it's so dependent upon potentially initially unintuitive

01:10:17.840 --> 01:10:24.800
ways of interacting so how can we understand the reliability and the performance and the

01:10:24.800 --> 01:10:32.640
capacity of of a model other than for example by exhaustively inputting prompts

01:10:32.640 --> 01:10:39.920
which can't really happen what what could we really say or no and or just how do you feel

01:10:39.920 --> 01:10:45.120
that this work re-enters into the ways that people practically are using the models

01:10:46.400 --> 01:10:51.200
right okay yeah uh it's the interesting question so the first one the question is I think

01:10:52.000 --> 01:10:57.760
you know part of the background assumption from for this paper that I've explicitly

01:10:57.760 --> 01:11:04.640
defended in other work is that behavioral evidence is simply not sufficient in most cases

01:11:04.640 --> 01:11:11.760
to arbitrate disputes about capacities of LLMs when it comes to human cognition

01:11:13.760 --> 01:11:19.280
we do have to rely a lot on on psychological experiments that are ultimately behavioral

01:11:19.360 --> 01:11:25.520
and we do also rely on self-reports in a little more than we can when it comes to LLMs because we

01:11:27.280 --> 01:11:33.200
despite the move away from relying on intuition and introspection in the history of psychology

01:11:33.200 --> 01:11:39.440
it still has a role to play but we've by and large contributed to us by behavioral experiments

01:11:39.440 --> 01:11:45.760
that get increasingly sophisticated to try to reverse engineer what's going on inside the black

01:11:45.760 --> 01:11:55.600
box when it comes to LLMs partly because that's so different from us um relying exclusively on

01:11:55.600 --> 01:12:02.160
behaviorism is even more difficult because we have even less of an idea of what might be going on

01:12:02.160 --> 01:12:06.240
inside the black box and whether it's anything like what's going on inside all black box and we

01:12:06.240 --> 01:12:12.880
have reasons to think it might be very different so I think I think we both agree that we have to

01:12:12.880 --> 01:12:21.760
supplement this with mechanistic work that's essentially involve performing causal interventions

01:12:21.760 --> 01:12:27.520
on the inner mechanisms on the inner workings of the systems so decoding representation and

01:12:27.520 --> 01:12:32.160
computations that the systems that are in principle available to the systems and then

01:12:32.160 --> 01:12:36.560
intervening on them to confirm hypotheses about the causal role of these representations and

01:12:36.560 --> 01:12:43.200
computations and we have methods to do that and partly what we can be a little optimistic about

01:12:43.200 --> 01:12:47.280
this project even though it's it's it's extremely challenging especially to be scaled up to large

01:12:47.280 --> 01:12:53.120
models is because unlike what's happening in your sense with the brain where the range of decoding

01:12:53.120 --> 01:12:59.280
methods and intervention methods we have is extremely limited but for ethical and for simply

01:12:59.520 --> 01:13:06.720
um practical reasons that we don't just don't have ground truth access to activations in neurons

01:13:06.720 --> 01:13:13.440
at least that easily and we also are generally unable to make specific interventions on

01:13:13.440 --> 01:13:17.840
activation in the brain uh when it comes to algorithms we have full ground truth knowledge

01:13:17.840 --> 01:13:24.400
of all activations of every single part of the network and we also have full access to all of

01:13:24.400 --> 01:13:30.240
it for interventions at inference time so that that opens up a whole new range of things we can do

01:13:30.960 --> 01:13:37.760
and that enables us to go beyond behavioral studies and actually decode these features and

01:13:37.760 --> 01:13:44.080
circuits or as researchers put it in the literature or as philosophers would generally put it

01:13:45.280 --> 01:13:49.360
representations and computations that the system is actually making use of and try to reverse

01:13:49.440 --> 01:13:53.920
engineer what kind of what kind of algorithms it's it's making use of so part of the broader problem

01:13:55.040 --> 01:14:01.520
projects that we have with Charles is to um suppose that we start with these

01:14:01.520 --> 01:14:06.720
investigative kinds as we put as as Alibol calls them these human subject capacities

01:14:08.720 --> 01:14:14.000
we can operationalize them and do behavioral experiments in the top down and then from the

01:14:14.000 --> 01:14:19.040
bottom up we can also try to reverse engineer the mechanism building blocks of the computations

01:14:19.760 --> 01:14:23.040
and representations that evidence may use up to solve the task

01:14:23.600 --> 01:14:28.160
related to that particular capacity and then we can meet somewhere in the middle and try to

01:14:29.280 --> 01:14:36.640
from that line of work that are purchased things from above and bring to the fore some kinds of

01:14:36.640 --> 01:14:42.080
mid-level abstractions as we call it or computational building blocks that might be key to

01:14:43.440 --> 01:14:49.120
the performance of the system in that domain so for example if you're interested in the capacity

01:14:49.120 --> 01:14:56.400
for reasoning you can start with this very broad human-centric notion of reasoning then try to

01:14:56.400 --> 01:15:01.760
operationalize it in a reasoning task then do some behavioral testing and then mechanistic

01:15:01.760 --> 01:15:06.320
interpretability of that reasoning task find out how the system is solving it find out how the

01:15:06.320 --> 01:15:12.480
algorithm is doing well and why reverse engineer building blocks that might for example have to

01:15:12.480 --> 01:15:19.040
be viable manipulation viable binding and then from there you might be able to either actually

01:15:19.040 --> 01:15:25.040
refine the notion of reasoning you started with to have a more specific and that less human-centric

01:15:25.040 --> 01:15:35.840
notion that is now operationalized in more low-level terms like you know that both manipulation

01:15:35.840 --> 01:15:42.400
of variables in certain ways and the binding of variables to theories etc so yeah so that's I

01:15:42.400 --> 01:15:47.040
think the general approach we take now how does that does any of that feedback into

01:15:47.760 --> 01:15:51.600
interactions how we humans interact with algorithms I think that's one way in which

01:15:51.600 --> 01:15:59.760
you could feedback is simply in terms of challenging or our spontaneous anthropomorphic

01:15:59.760 --> 01:16:04.960
attitudes to algorithms to some extent the same way to read a lot of animal cognition perhaps you

01:16:04.960 --> 01:16:11.120
will interact with your cat in a slightly different way that you might maybe not rush the conclusion

01:16:11.120 --> 01:16:18.720
that when your cat performs a certain behavior it has understood what you think and it's modeling

01:16:18.720 --> 01:16:25.680
what you're thinking about what it's thinking or something perhaps you might adopt a more

01:16:25.680 --> 01:16:31.920
deflationary attitude to explain the behavior of your cat doesn't mean you have to love them

01:16:31.920 --> 01:16:39.520
any less or it doesn't mean you have to you know if that is the other thing like if you want at the

01:16:39.520 --> 01:16:43.600
end of the day to speak to your cat like a human because you really are a gentleman for that then

01:16:43.600 --> 01:16:48.000
that's you know all the more part of you in the same way if you find it useful to treat LLMs in

01:16:48.000 --> 01:16:55.280
the way you interact with them to to have fluid interactions with them to treat them as if they

01:16:55.280 --> 01:17:00.560
had beliefs these are as etc of human-like capacities then that's fine if that's for

01:17:00.640 --> 01:17:05.840
actual purposes but at least if that line of work that we are kind of sketching here

01:17:06.640 --> 01:17:14.000
ends up maturing enough the hope is that we can interact with LLMs perhaps in a way that's

01:17:15.840 --> 01:17:20.240
well even if we if we have that kind of intentional sense and may believe about

01:17:20.240 --> 01:17:26.720
who the kinds of besties they have at the background we will know that what their limitations are and

01:17:26.720 --> 01:17:31.520
what their actual besties are i'd make sure to go with that Charles because we're going to use

01:17:31.520 --> 01:17:37.600
this much yeah yeah no i agree with all that i just had a slightly different first reaction to the

01:17:37.600 --> 01:17:43.360
question i took the question to be in part about how to deal with the sort of prompt sensitivity

01:17:43.360 --> 01:17:48.720
of models the fact that sometimes we you know write something that seems natural to us but

01:17:48.720 --> 01:17:54.480
provokes an unexpected response from a large language model and how do we think about that

01:17:55.440 --> 01:17:59.760
and the first thought that occurred to me was just that we should distinguish between

01:18:00.880 --> 01:18:09.520
different kinds of large language models you know we have this sort of huge large language models

01:18:10.160 --> 01:18:17.840
which are fine-tuned to interact with us in a particular way and our and here's the central

01:18:17.920 --> 01:18:26.320
point they're trained on a sort of unthinkably large database whereas there are other sorts

01:18:26.320 --> 01:18:30.960
of large language models where the training data is more circumscribed and where we know

01:18:32.000 --> 01:18:39.440
in more detail you know what where you can survey what the training data says and i think

01:18:39.440 --> 01:18:43.200
if you're interested in you know what the mechanisms are underlying the responses

01:18:44.160 --> 01:18:51.680
it's certainly very helpful to look at smaller but nevertheless large language models where the

01:18:51.680 --> 01:18:56.960
training data is known to us because you know when you train a model on the entire internet

01:18:56.960 --> 01:19:03.760
there are going to be all kinds of you know subtle signals in there that we don't have much hope of

01:19:03.760 --> 01:19:07.760
tracing back to their source but which will influence the model behavior in all sorts of

01:19:08.240 --> 01:19:17.520
ways but working with these somewhat more conscripted models gets rid of that problem at least in part

01:19:21.360 --> 01:19:30.240
cool well where where do you see the work going or where do you plan to continue this direction

01:19:30.560 --> 01:19:38.080
yeah so actually so we wrote this paper this short paper for the ICML

01:19:38.080 --> 01:19:43.120
machine learning conference in the national conference machine learning that's happening

01:19:43.120 --> 01:19:49.600
this week in the data and will be getting to Vienna at the end of the week for the popular

01:19:49.600 --> 01:19:53.920
workshop that we're representing this paper which is a workshop on language models and

01:19:54.000 --> 01:19:59.920
cognitive science so there will be a very strict page limit for these ICML

01:20:01.200 --> 01:20:08.400
contribution which is four pages but what we want to do next is to expand this into a more

01:20:09.760 --> 01:20:14.880
philosophically substantive paper that's going to be a bit longer and that's going to expand on the

01:20:14.880 --> 01:20:19.840
more philosophically meaty parts of that of that project because everything is still a bit compressed

01:20:19.840 --> 01:20:24.080
in that version that we're presenting at ICML so yeah that's the next step for us this is a

01:20:24.080 --> 01:20:28.320
really useful way for us to force ourselves to write things down after running the box piece we

01:20:28.320 --> 01:20:34.080
wanted to write an academic piece now we've written kind of a condense skeleton of the piece that

01:20:34.080 --> 01:20:39.120
focuses more that caters more to an analogians and now the next step is to write the full

01:20:39.120 --> 01:20:44.400
philosophy paper or at least that part of our project to be complete and then I don't have to

01:20:44.400 --> 01:20:49.840
that maybe we'll have all that ideas but yeah yeah

01:20:53.360 --> 01:21:00.240
I got nothing to add to that cool yes well it's very interesting work I think it it brings a

01:21:00.240 --> 01:21:09.520
lot of pieces together and it's some philosophy and cognitive science jumping in jumping into the

01:21:09.600 --> 01:21:16.640
the heat and into the the spotlight and the relevance and so it's going to be an exciting

01:21:16.640 --> 01:21:23.920
learning journey thanks for having us yes yeah thank you very much cool enjoy the conversation

01:21:24.560 --> 01:21:31.760
till next time thank you bye

01:21:39.520 --> 01:21:40.900
you

01:22:09.520 --> 01:22:10.900
you

