{"text": " Hello and welcome, everyone, to the Active Inference Institute. This is Active Gueststream number 41.1 on April 25, 2023. We're here with Elliot Murphy and Steven Piantadosi. This is going to be quite a discussion. We will begin with opening statements from Steven and Elliot. Elliot will then lead with some questions and we'll have an open discussion at the end. So, Steven, please, thank you for joining and to your opening statement. Cool. Hi, so I'm Steve Piantadosi. I'm a professor in psychology and neuroscience at UC Berkeley. And I guess part of the reason that we're here is that I recently wrote a paper on large language models in part trying to convey some enthusiasm about what they've kind of accomplished in terms of learning syntax and semantics. And in part pointing out, I think that these models really change how we should think about language, how we should think about theories of linguistic representation and theories of grammar and likely also theories of learning. Yeah. Awesome. Yeah. So, I'm Elliot Murphy. I'm a postdoc in the Department of Neurosurgery at UC Health in Texas. I read Steven's paper with great interest and there's a lot of people. There were some areas of convergence, but the things I want to kind of focus on today in responding to Steven and kind of probing how to do with areas of divergence maybe. So, you know, Steven's paper is based on the idea that modern machine learning has subverted and bypassed the entire theoretical framework of Chomsky's approach. So I wanted to kind of respond to some of these main arguments and some other related arguments in the literature that some folks listening might have some insight and thoughts on. So, it's a very common criticism to say that large language models just predict the next token, which is obviously a bit of a cliche, right? It's not quite true. They don't just predict the next token, they also seem to confabulate, they seem to hallucinate, they maybe lie, they randomly provide different answers to the same question, and they seem to stochastically mimic language like structures. They sometimes correct themselves sometimes when they shouldn't. If you push them a little, they kind of change their mind sometimes. In fact, if Fox News is currently looking for a replacement for Tucker Carlson, they could do less. They could definitely do worse than using ChachiBT if they're looking for a similar caliber. So, these models seem to do all sorts of like wild things, and over the past 10 years, there's been a sequence of different, you know, systems developed like where to be, where to be, and each of them is based on a different neural net approach, but ultimately they all seem to take words and characterize them by lists of hundreds or thousands of numbers. So the GTP3 network has 175 billion weights, 96 attention heads in its architecture, and as far as what I know, maybe Stephen can correct me here. We don't really have a great idea of what these different parts really mean. It just seems to kind of work that way. Like attention heads in GTP3 can pay attention to much earlier tokens in the string in order to help them predict the next token, but the whole architecture from start to finish is kind of engineering-based motivations, and I always kind of wonder what about all the models that kind of failed from these LLMs, from the different tech companies. It's like these companies often seem to, you know, make it seem like they have these models that really work very well straight out the box, and they all seem to be named after some kind of famous artists, right? They have Dali after Salvador Dali. They have Da Vinci, maybe pretty soon one of these companies will release a large language model called Jesus or something, I don't know. But they always say, here's our new foundation model. It's called Picasso. It's the first one we tried. It works just great. No problems straight out the box, but I always wonder what about all the other black boxes that have kind of failed every time? That doesn't seem to be a kind of a very open and clear structure to the kind of scientific reasoning behind selecting, you know, one model or another, but again, I might be, I'm open to be corrected about that. So even basic language models do pretty well on basic word prediction. So the issue is whether these tools provide any insights into traditional psycholinguistic notions like grammar and parsing. So this is really why I kind of prefer the term corpus model rather language model, suggested by people like Sabra Varys. So as we pointed out that no one really thinks LLMs tell us anything profound about Python when they learn Python code just as well as natural language, but Python is a symbolic language with a phrase structure grammar and nobody says LLMs are unveiling the secrets of Python. Right. So just to quote Varys here, he says, if A and N models can be construed as explanatory theories for natural language based on their successes on language tasks, then in the absence of counter arguments, they should be good explanatory theories for computer language as well. Therefore, successful A and N models of natural language cannot be used as evidence against generative phrase structure grammars in language. So corpus model is really a more appropriate term for other reasons too. People like Emily Bender and some others have shown that features of the training corpus, in fact, I think Steven cites this, you cite this in your paper actually as a limitation. They show that features of the training corpus can heavily influence the learning process. So it's been shown that the performance of large language models on language tasks is really heavily influenced by the diversity of the training corpus. But natural language itself is not biased, right? It's just the computational system. Some beings can be biased in what they say and how they act. But natural language itself isn't biased, right? So large language models, therefore, it seems difficult for me to agree that they are being subject to all sorts of biases. They therefore can't really be models of language, they're models of something else. So just to kind of wrap up this argument, even though LLMs are clearly exposed to vastly more linguistic experience in children, again, this is something else that Steven concedes and talks about in his paper. And so their learning outcomes may still be relevant in addressing what grammatical generalizations are learnable in principle. So I do agree with this statement here, that in principle they can tell us something about learnability rather than things like broad acquisitionist frameworks. But that's about as much I think you can maybe say right now. Showing that some inductive biases are not necessary for learning is not really the same thing as showing that it isn't present in children. So there's been a long debate about whether negative evidence and instruction and correction and feedback during language learning are necessary or even useful for infants and children. But right now I kind of agree more with Eugene Choi and Gary Marcus and others who have highlighted how LLMs are currently very expensive to train. They're clearly an example of concentrated private power in the hands of a few tech companies. Their environment impact is massive and many of people have been less constrained and conservative in their assessment here, which is much less so than Gary Marcus and Eugene. So Bill Gates recently wrote that chatGPT is the biggest tech development since the graphical user interface, the GUI. And Henry Kissinger wrote in February in the Wall Street Journal that as chatGPTs capacities become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality and reorganize politics and society. Generative AI is poised to generate new forms of human consciousness, so very radical claims happening at the moment. I do wonder if sometimes all of the AI hype may have, you know, see it into certain portions of academia potentially, a lot of ground claims being made. But I think, you know, more concretely, just to put it back to Stephen here, I wanted to maybe raise the issue of there's a critique by Roscoe and Beaumont that I think he's read on Lingbos. I think you saw on Twitter that you don't like the response they gave because the objection that they made is that, you know, science is an example of deductible logic. Your objection is that science isn't deductive, it's inductive, right? But I think their general point might be more accurate, namely that you can't use the fact that language models do well predicting some linguistic behavior in humans and some neuroimaging responses. You can't use that alone to claim that they can yield a theory of human language. So in your paper, Stephen, you know that it seems that certain structures work better than others. The right attentional mechanism is important, prediction is important, semantic representations are important. And therefore, we can glean currently based on these models, right? But so far, that's really all I've been able to glean in the literature. I'm not sure if you have more insights here. So Roscoe and Beaumont use the example of poor prediction, but strong explanation, right? Explanatory power and not predictive accuracy forms the basis of modern science. I don't want to explore this a little bit later, maybe, but modern language models can accurately model parts of human language, but they can also perform very well on impossible languages and unnatural structures that humans can't learn and have great difficulty processing. And I know you're familiar with these with these criticisms, right? But you're definitely not alone here at the same time. So Ilya Tskeva, the chief scientist at OpenAI, he said in an interview recently, what does it mean to predict the next token well enough? It means that you understand the underlying reality that led to the creation of that token, which is quite divergent from a lot of more conservative claims in the literature here. And also, you know, I would just say in response to that, that different components of science can be either inductive or deductive, right? It's not really an either-or. You have an existing theory. You formulate a hypothesis. You collect data, you analyze it, and that's kind of a deductive process. But there's also cases where you start with a specific observation. You find some patterns and you induce general conclusions, right? And then there's abduction, where you magically invent hypotheses and reduce the hypothesis space. You wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific or abductive reasoning is unscientific, right? These are all just different ways of doing stuff. I mean, in your paper, you give the examples of using models to predict hurricanes and pandemics as being examples of stuff that is as rigorous as science gets. And then you employ a reader to conclude that the situation is no different for language models. But I guess for me, the issue is that models predicting hurricanes are not in the business of answering the question, what is the hurricane, right? Models accurately predicting the weather are very accurate, but they're not. They're aligned with the meteorology department, but they're not a substitute for it. So I guess I'll just hand it over to you. Yeah. Okay. Well, there's a lot there. I guess I could start just by saying that I agree with many of these criticisms, right, about these models being controlled by one or two companies that being very, very problematic. They have all kinds of biases that they've acquired because they're trained on text from the internet. That's hugely problematic. I certainly agree that there's things at least at present that the models don't do well, right? So I think it's easy to find examples of questions and problems that will trip them up. I think why I've been excited about them, though, is not necessarily in those terms, right, but in terms of performance on language, specifically syntax and semantics. I think they're far beyond kind of any other theory in any other domain, right? So there's no other theory out of linguistics or computer science which can generate long, coherent grammatical passages of text. And so kind of admitting all of their problems as tools or things which are deployed by companies, there's still this question of, like, how are they at dealing with language? And I think this is where a lot of the enthusiasm comes from, is there really hasn't been anything even remotely like them in terms of linguistic ability. And that's the thing that I think is exciting. So, yes, I agree with a bunch of these things you started with, but nonetheless, like I think in terms of syntax and semantics, there's just no other theory which is comparable to them. But so let me push that back then, right? So the main objection from a lot of people I've spoken to in the departments of linguistics who are like a lot of the general first of your paper is to really say, well, you're right, they do a wonderful job accurately modeling all aspects of a lot of aspects of syntax and semantics. However, I don't know if any real just like, you know, Chomsky talks about facts about language, which is an old fashioned notion. But I really think that's kind of an important notion too, right? Like, is there some discovery about language itself that LLMs can uniquely provide? So like, if LLMs made some prediction about, let's say you have a sentence structure type X being more difficult to process than sentence type Y. And this is a unique prediction that only they'd generate it. And no human linguist, Chomsky, Honesty, and Adger, none of these people had ever predicted that before. But it turns out to be true. You do eye tracking experiments, you do all sorts of different behavioral experiments. And it turns out, oh, you know, after all, it turns out to be true. This is the new insight about language processing, it's a new insight about language, you know, behavior. I just wonder, I'm not saying that this is not possible in principle, because it might happen in the near future. But that's, I guess, for me, the crux of why a lot of linguists speaking up, speaking on behalf of the entire linguistic community here. And, you know, I guess that would be one of the main objections. Yeah, I mean, I don't know of, I guess, I think of the insights they've provided as kind of general principles, right? So I think about these things like the power of memorizing chunks of language, right? So like, they seem to be very good at constructions, for example. And there's lots of linguistic theories, Chomsky's in particular, right, which are about trying to find kind of minimal amounts of structure to memorize, right, trying to derive as much as possible from some small set, some small collection of operations. And I think that hasn't gone well for those theories, right? Whereas this goes really well, right? So if we think about something which has the memorization abilities, if we think about theories of grammar, for example, which build on, you know, humans like really remarkable ability to memorize different constructions, right, or different words, you know, tens of thousands of words, tens of thousands of different constructions, sorry, tens of thousands of different idioms, maybe our theory of grammar should be integrated with that. And there in some sense, a kind of proof of principle that that kind of approach can work well, right? Can think about making other types of predictions with them, some of which people are currently doing, but for example, trying to use them to measure processing difficulty, measure surprise, for example, from these models, their surprise measures, right, are much better than, say, context-free grammars or other kinds of language models. And then it's an interesting question how those surprises or predictabilities relate to human processing, right? And it may capture some of it or might be nonlinear, or it might, you know, only capture a little bit of it or whatever. That's an interesting kind of other scientific question. But I think in principle, right, they can make predictions about, for example, the connections between sentences, right? So in the paper, I gave this example of, you know, converting a declaration into a question in 10 different ways, right? And presumably when it, when, you know, GPT or something is doing that, it's finding 10 different questions which are all in some way related, kind of nearby in the models underlying semantic or syntactic space. And so those kinds of things are of the type that I think, you know, some linguists might want, right, which is here's some hidden connection between sentences or their, or their structures. But as far as I know, they haven't been evaluated empirically yet. So, yeah, yeah, I mean, these kinds of models are only a few years old. So I think it's, it's reasonable to be excited about them, even though this kind of work hasn't been done yet. No, that's right. No, totally. Totally. I mean, I think that's the right perspective to take. But I think this gets to the issue of the, you mentioned surprise or you mentioned learnability, you know, LMS learn some syntax, but they do so. We've obviously way, way more data than infants do. Such that observations of potential structure in and of itself is not a refutation of the poverty of the stimulus, well, the weaker version, I should say, of the poverty of the stimulus argument. So the mere fact that LMS can do what they do without grammatical prize is very striking, I agree. And in fact, you wouldn't have predicted that maybe five or six or seven years ago. But it doesn't yet invalidate the claim that humans have such a prize and we bring those prizes with us. And so in order to see if computational linguistics can constrain hypotheses and theoretical linguistics, which I think it can do, by the way, this needs to be done with, you know, careful experiments in which different learning parameters are controlled. And gigantic language models like GPT3 are basically, you know, useless here. So this gets to some of Tarlin's complaints that we need something like a baby LM project, which I know you're interested in, where we have more, you know, ecologically valid training sets. You make the prediction in your paper that some structure will be learned from that. I suspect you might be right there. But, you know, even so, even with the baby LM challenge, there's still the kind of non-trivial issue of addressing more traditional issues like when the kids start to generalize based on the amount of current input, based on different factors, cross-linguistically. And that requires just traditional, you know, psycholinguistics and language acquisition. So LMs, you know, do care about things like frequency and surprise will, as you said, but there's a really nice paper by Sophie Slatzen, Andrea Martin, a really beautiful paper that I think you may have seen that shows very nicely that distributional statistics can sometimes be a cue to moments of structure building. But it doesn't replace these notions pertaining to composition. So I'll just read a quote from Chomsky 57, which sounds a lot like what Slatzen Martin said. Despite undeniable interest and importance of semantic and statistical models of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances. I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular insight into some of the basic problems of syntactic structure. So that second hedge of the second sentence turned out to be incorrect. But it's so it's true that, you know, what Chomsky said of available stat models in 57 is no longer accurate when applied to models today. That can make abstract generalizations about novel strings and distributional categories, as you mentioned, right? But the performance of a single model does not provide direct evidence for or against the landability of a particular structure. Like given the vast distance between any computational model available today and the human brain, model success does not mean that the structure is necessarily land and model failure also doesn't mean that the structure is not landable, right? Yeah, yeah. So I mean, I think it's maybe worth unpacking kind of a couple different versions of learnability arguments that people have made, because there have been very, very strong kind of impossibility claims coming out of kind of Chomsky's tradition, right, that were never claims about the amount of data that was required, right? They were claims about the logical problem of language learning and that it was just impossible, right? It was impossible without having kind of substantial constraints on the class of languages or the class of grammars that you would acquire. And people for a long time have been arguing against that version of things. You know, there's old work by Gold, and then there's whole kind of grammatical theories of acquisition built on that tradition that worry a lot about the kind of order in which you traverse through different hypotheses and consider different options and things. And my favorite reference in this is this paper by Nick Chater and Paul Vitani, called something like Ideal Learning of Natural Language, that basically shows that an unconstrained learner could, with enough data, acquire the kind of generating rules or the generating grammar just from observing strings, right? But that paper was really in response to this huge body of work that was arguing that learning from positive examples, so from just observing strings, was like logically impossible, right? So of course, you know, people in Chomsky's tradition really liked that form of argument because it was one that said you had to have something innately specified in order for language acquisition to work. It was like kind of a mathematical argument, that you had to have some kind of innate grammar, innate ordering of hypotheses or something, and all of that just turned out to be totally wrong. So if you move to slightly more kind of realistic learning settings, which Chater and Vitani do, then it turns out you like an idealized learner can acquire stuff, and there's no statements about the amount of data that's required even there, right? That's the kind of pure logical ability to learn, and that ability is what I think the big versions of large language models also speak to, right? So Chater and Vitani and other work kind of in that spirit is, you know, mathematical and kind of arguing in principle, but never created something which was really a grammar, right, or a real kind of implemented language model. So even, you know, a model which is trained on 100 million or 100 billion or however many tokens, right, even that kind of model I think is relevant to that version of the debate, right, and showing that language learning is not impossible from a very unconstrained space. Okay. And then there's a second version, right, which is can we learn language with the specific data that kids get, right, and that's both amount of data and form of the data, and so for people who don't know, the BabyLM Challenge is this, sorry, we think to call it a competition or a, I guess it is a challenge, trying to get people to train language models on human sized amounts of data. So that's something more like, I think there's two different versions, 10 or 100 million different, 10 or 100 million different words in the training set, which is like, you know, 100th or 1000th or something as big as these big AI companies are using for their language models. And I think actually it's like, that's exactly the right kind of thing and exactly what the field needs, right, because you might find that on a child sized amount of data, you can essentially learn syntax, right, which I think would be the strongest argument against these property of stimulus claims, you could alternatively find that maybe you can't learn very much, maybe you, you know, come up with a much crumbier kind of language model or it's lacking some syntactic or semantic abilities. I actually think that the failures, they are a little bit hard to interpret because kids data, when they're actually learning language, they get a lot more data than just strings of sentences, right, they're interacting in an environment. So there's stuff in the world in front of them. Their utterances are also interactive, right, so you can say something and see whether your parent brings you the thing that you asked for, for example, right, that's long been argued by people as a, you know, important cue in language acquisition. So in the baby LM challenge, there is an ability to train these models with kind of multimodal input, so I think you can give them as much video data as you want to give, but probably it's hard to kind of replicate exactly the type of setup and feedback that kids actually get. So I don't know, you know, I'm excited to see where that goes and how things pan out there. You know, I think that there is an interesting related question for large language models, which is like what, which is understanding exactly what all of the data is doing. So it could be that you need so much data for these models because they're effectively inventing some form of semantics internally, right, so they're both discovering the rules of syntax and they appear to be learning quite a bit about word meanings. And it's not, it's totally unclear, I think, how much of the data in these modern models is needed for syntax versus semantics. My own guess, I think, would be that the syntactic side is probably requires much less data than the semantic side. Actually, a student, a former student of mine, Frank Malica, and I wrote a paper a few years ago trying to estimate the amount of information a learner would necessarily have to acquire for learning the different aspects of language. So you have to learn all the words and you learn their forms, you learn their meanings, you probably know their frequencies, you have to learn syntax. And basically what we found in that analysis, that was, you know, basically just a kind of back of the envelope calculation for each of these domains is that syntax is actually very few bits of information, it doesn't take that much information to learn syntax. Whereas like most of the information you acquire is actually for semantics. So specifying, you know, 30 to 50,000 different word meanings, you know, even if each meaning is just a few bits, right, like that requires a lot of information and probably each meaning is more than a few bits, right. So it could be, like, that would make me guess that what's happening with large language models is most of their training data is about word semantics. And you can think about other ways that kids get word semantics, right, that's not just kind of co-current patterns in text. But I agree, all of that is up in the air and really exciting to see what will happen. Yeah, I know that some of the earlier results from Linsen's lab suggest that at least restricted to equitably valid, you know, training set sides, models seem to generalize, you know, linear rules for English, probably has no question of formation, rather than the hierarchical rule, the correct hierarchical rule. So I think there's a real sense in which, you know, the space of the correct syntactic prize and inductive biases really is yet to be really settled on. But it seems, at least to me, pretty obvious that there has to be some. So there's also some evidence that children in English, going back to this frequency issue, that children in English sometimes spell out an intermediate copy of movement in the specified position of the lower complementizer position of a long-distance WH question. So there's a thesis by Thornton at some of the papers about this. So they say, which person do you think who did that, rather than which person do you think did that? So this is an interesting, you know, missetting, because some languages do actually spell out these intermediate copies, but English doesn't. So the kid makes the error in setting their grammar, but the frequency of the input is actually zero. So our mutual friend Gary Marcus also has an argument against frequency-determining a kid's output. In the case of German noun plurals, a more regular form of the setting kind is preferred, not the frequent one. And there's lots of examples like this. So it's sometimes claimed that subject-experiencer passives, where the subject is passively experiencing something, are very delayed in kids in comprehension studies until around eight, because they're not very frequent in the input. But Ken Wexler and colleagues have gone through subject-experiencer WH questions like, who likes Mary? And they discovered that these are as infrequent in the input as subject-experiencer passives. But kids have no problem in comprehension studies of these questions. But they do have problems comprehending subject-experiencer verbal passives. So frequency once again seems to be irrelevant. Or at least it's not explanatory, right? I guess it's not explanatory with respect to theory building. So how can LMS help with these, you know, diverging cases when there's clearly something else going on besides frequency? So LMS, you know, they seem to generalize just, again, going back to this issue of the cases that you have in your paper. You show that they generalize the structure of color screen ideas, which is obviously very cool. But the positive stimulus has never really been about not being able to learn language statistically. I know you made that claim, right? But Chomsky's point in the 50s about statistical models of the day is not true of commercial LMS in 2023. And that's correct. But we can't use that single point to undermine, you know, the entire generator enterprise. Chomsky's basic point was that you could have a grammatical structure wherein every background has zero frequency, and it also fails to provide clearly interpretable instructions to the conceptual interfaces. So interfaces with other systems of the mind. So as you're showing your paper, GPT mimics examples like full screen ideas. But, you know, again, this sentence yields over 150,000 results on Google, and it's discussed extensively in the literature. It's able to mimic the fact that it can mimic this doesn't really tell us that much. At least we can't really say anything with much confidence. So, you know, Albeba behind University College Dublin has this quote recently, do not mistake your own vulnerability for an LMS intelligence. In fact, even Yanlacun wrote last year that critics are right to accuse LMS of being engaged in a kind of mimicry. And the example sentence is from chat GPT that you give in the paper. Actually, don't do a good job because, as you say, it's likely that, you know, meaningless language is rare in the training data, but they can either do it or they can't. But there's no middle ground in terms of giving us 10 examples like this. So, you have colourless green ideas, which are very different semantic objects from things like brown shimmering rabbits, white glittery bears, black shiny kangaroos, green glittering monkeys, yellow dazzling lions, red shimmering elephants, right? These are all like semantic, semantically weird and a bit strange, but they're still like legal structures. They're kind of meaningful semantic objects. Right? I just said, yeah. Yeah. I mean, so maybe I can respond to the first point first, right? So, you started off talking about these other kinds of acquisition patterns, which maybe don't map directly onto frequency. And I think it's actually a mistake to think that kind of modern learning models should be just based on frequency, because they're clearly learning like pretty complicated families of rules or constructions or something. And I think it's very likely that when they're learning that, they're in some sense searching for a simple or parsimonious explanation of the data that they've seen, right? And how that caches out in a neural network is maybe complicated and depends on parameters and the specifics of the learning algorithm and those kind of things. But I think it's, I'd suspect maybe that it's likely to be the case that they're learning over a complicated set of things, right? A complicated kind of family of rules and constructions. And that means I think that their generalizations, maybe like the examples of people that you gave, might be kind of discontinuous in the input, right? So, sometimes you could imagine seeing some strings which lead you to a grammar and the simplest grammar of the data that you've seen so far is one which predicts an unseen string, right? And if that happens, then you'll be taking the data, learning a representation which generalizes in some novel unseen way so far, purely because that generalization is sort of the simplest account of the data that you've seen to date, right? I think that's sort of what linguists try to do, right? Try to look at the data and come up with a theory of it, and then sometimes that theory predicts some new phenomenon, right? Or some new type of sentence. And so, if they're learning over as sufficiently rich space of theories, then it wouldn't be unreasonable or unexpected for them to also show those kinds of patterns. Now, whether they do or not I think is still an open empirical question, right? Because we have to train them on small amounts of data and test their generalizations and these kind of things. But I don't think like just the fact that humans do things which are not purely based on frequency is any evidence at all, either way, right? Because once you're learning over rich and interesting classes of theories, then that is the expected behavior. Actually, I had a paper about a year ago that I think you're familiar with, Yang and Pianta dosi, where we were looking at kind of what happens when you give a program learning model strings from different formal languages. So think of like giving a general model just 10 or 20 maybe simple strings that obey some pattern and then asking it to find a program which can explain that data, which often means finding some way of kind of programmatically writing down the pattern in the strings. And in that figure, we have a paper which is really relevant to this point where the generalizations that that kind of model makes are I think kind of qualitatively like the ones you're describing for people, right? Where you can give them a small amount of data and it will predict unseen strings with very high probability, even though there's zero frequency in the training input, right? And the reason it does that is that often the most concise computational description of the data that you've seen is one that predicts some particular new unseen output. So that model is essentially an implementation of the kind of Chater and Vitani program learning idea that I brought up earlier. But it's one that I think, you know, if you think about in the context of these arguments of kids saying unusual or unexpected things, like that is predicted by all of these kinds of accounts, right? Because as long as these things are effectively comparing an interesting space of grammars, then they'll show that kind of behavior, I think. Yeah. So, okay. So I guess, you know, the argument would be that, at least from the gender perspective, syntax is functioning separately, but it still maps to semantics, it informs pragmatics, right? So in the minimalist program, syntax is obviously minimalist, it's very small, it's just a linearization and labeling, they're the two only operations, you have a linearization algorithm to central motor systems and some kind of categorization algorithm at the conceptual systems. So Chomsky's architecture is kind of reliant on the process of mapping syntax to semantics, right? It's form meaning regulation, it's not just structure, and it's not just meaning. So LMS don't really have this mapping process, right? Like, where's the mapping to semantics? And if there is a mapping, what does the mapping process look like? What are the properties of its semantics? What are the properties of the semantics placed on their own sets of constraints on the mapping process? Like, they do for natural language? Do these kind of constraints inform each other? Is they kind of a back and forth process? Like, LMS don't really seem to describe this form meaning pairing, which means which strings, for example, right? Sorry, are you saying that they don't have semantics at all? Or are you saying that there's just not a clear delineation between how the structures get mapped onto the semantics? Yeah, the latter, right? So they clearly have some, potentially some kind of semantics. I know you've argued for conceptual role theory being relevant here, right? The rest of it is maybe a little bit more mysterious, but the actual, so in linguistics there's a theory of the mapping process itself, it's explicit, and you can see it in action, and you can test different theories of it in psycholinguistic models and what have you. The actual regulation, the kind of constrained ambiguity, ambiguity in the sense of one word, multiple meanings, or one structure, multiple interpretations, etc, right? Yeah, I mean, if you think they have semantics, then I think they have to have a mapping from the syntax to the semantics. I agree, it's not as like, nobody really understands how they're working on any deep level, right? So I agree, it's not as clear as, say, in generative syntax and semantics, right, where you kind of write down the rules of composition and can derive a compositional meaning from a sentence from the component parts or something, right? Like, that's not how they're working, right? But I just, I wouldn't take for granted that it has to be like that. Like, it could be that how they're working is actually how we work, right? That everything is represented in some high-dimensional vector space, and there's some complicated way in which that vector semantics gets updated with each additional word or whatever in a linguistic stream. But like, I think it's clear that they have some kind of representation of the semantics of a sentence, right? Like, they can answer questions, for example, at least approximately. I mean, it's not perfect, but it's not like a n-gram model or something, right? Which really doesn't have semantics. So I think that they're definitely representing semantics and, you know, updating that as they process language, it just happens not to look like these other formal theories. And I guess, I don't see why that's a problem, right? Like, those other formal theories could just be, you know, poor approximations or just totally wrong, right? Yeah, yeah, no, no, totally, totally. I mean, there's also ways in which some of the formal formal theories in semantics are already potentially compatible. We've got some of these things are doing, right? So another way to think about this is, you know, LMS are, well, LMS are compression algorithms, but natural language understanding is kind of more about decompression. It's disambiguating, meaning x, out of meanings, x, y, z. It's all about making inferences about, you know, meta relations between concepts that are not in the training data. So some examples that Millie Mitchell gives are things like on top of, you know, she's on top of a game, it's on top of the box, all of these kind of vary with context. So there's a lot of other things that are going on, right? And I think you discussed some of the examples on your paper. So, you know, but the fact that the language is still not, at least, again, under this theory of language, it's not about string generation. It's about this form, meaning, pairing machine. So some semantics in the generative tradition, even think all the rest of semantics is just and, right? So both Rasky's conjunctivist theories semantics is that human semantics is just and that's it. Which, again, is very simple, elegant. It's, it's, it's interpretable. It's compatible with other things that, you know, are maybe going on in your neck of the woods, right? But regardless, it's still, you know, natural language is still more compositional than things like, you know, formal languages just to make a clear distinction that's been made. They have a much richer compositional structure. There's more stuff going on, maybe. So it's important that before that, you know, things like attention-based machine mechanisms and transformers allow for combinations of discrete token bindings, which is more approximate to a merge-like operator than simple recurrent matrix multiplication. But, you know, the issue of binary branching, binary branching of merge, just to choose, for example, here to talk about the four meaning regulation, one principle. Binary branching in merge is an interesting question, but Gem2 grammar has always been open to different origins and locations of this apparent constraint in syntactic computation. Like, where does it come from? Maybe it's a condition on merge. Maybe it's imposed by a smooth system. Maybe it's a kind of prior, you know, who knows. And in fact, some more recent work in Gem2 grammar has tried to ground do away with a lot of the set theoretic assumptions of merge, right? Maybe set theory isn't the best way to model the Gem2 grammar. Maybe more logical accounts are more appropriate. There's lots of other recent ideas there, which are all compatible with the, with Chomsky's approach, right? In fact, one of the things that Chomsky likes the most is when he's, when he's proven wrong, right? A lot of these theories are going against the core mainstream minimalist architecture. But yeah, I think it's a very diverse, like, vibrant field. The people who are Adjah, Hornstein, you know, Petrosky, Haji Borre, they disagree in fundamental ways with a lot of what the mainstream of Gem2 grammar would say, but there's still more scope for disagreement. But it's still compatible with setting core assumptions, right? So a lot of David Adjah's work, for example, kind of deviates in this core respect, but it's still trying to ground these intuitions in different formal systems. So, you know, it's kind of, I want to get your thoughts again on, I mentioned Mitchell, right? So Mitchell and Bowers 2020, they have this paper, priorless recurrent networks laying curiously, but I think you might be aware of, right? So this is a really good example just to kind of get to the heart of the issue. So recurrent neural networks have been shown to accurately model, you know, non-veb number agreement, but Mitchell and Bowers show that these networks will also learn a number agreement with unnatural sentence structures. So structures that are not found in natural language, and which humans have a hard time processing, right? So the mode of learning for RNNs is, at least for RNNs, qualitatively distinct from infant, you know, infant homo sapiens, right? So the story is Mitchell and Bowers show that while their LSTM model has a good representation of singular variances, plural for individual sentences, there's no generalization going on, right? They can represent at the individual level. So the model doesn't have a representation of number as an abstraction. What number is? Only concrete instances of singular versus plural. So successfully predicting language behavior via LM, or successfully predicting neural responses in a similar way is obviously great. And maybe we can get into that issue later, but there's only one side of the coin here, right? The other side of the coin is explaining why this type of behavior and not some other behavior, why this structure and not some other, and that's maybe Chomsky's most important point, really, why this and not some other system. So linguistic theory kind of gives you that other side of the coin, right? Whereas LM's really don't. So the Mitchell and Bowers paper does something that- He does it! Well, yeah, so like, take Yael LaCrette's and Stanislas de Haines' work from 2019, right? They looked at number agreement in an LSTM and found two specialized units that encoded number agreement, but the overall contribution to performance was low. And then in 2021, Yael LaCrette's have this paper where they show that in the neural language model, it did not achieve genuine recursive processing of nested long range agreement, gender marking in Italian, I think, even if some hierarchical processing, you know, was achieved, as you've argued before, right? Some hierarchy was there, it was there. But the question is, is it the right mapping? Is it the right kind of hierarchy? They found that LSTM based models could learn subject-verb agreement over short spans, one degree of embedding, but they failed at some longer dependencies. And in their most recent paper, LaCrette set out with De Haines showed that they evaluated modern transformer LM's, including GPT2 XL, on the same task. And the transformers performed more similarly to humans than LSTMs did and performed above transfer overall, but they still performed below chance in one key condition, which is the, as I mentioned, the multiple embedding one, the difficult structures. So the reason why I mentioned these studies is because, you know, it's not just to explore the limits of LM's, which is an interesting question. But consider work by people like Neil Smith at UCL, right? He did work in the 90s with a polyglot, savant, and neurotypical controls comparing them. So he investigated second language learning of an artificial language containing both natural and unnatural ground structures, like the Michelin Bowers paper, right? The whole framework is natural versus unnatural. And they found that while both the savant, Christopher, the savant, and the controls could master the linguistically natural aspects, only the controls could eventually handle the structure dependent unnatural phenomena, and neither of them could master the structure independent aspects. So some weird rules where it's like, you know, you mark the emphasis on the third word of the sentence, things like that. So they argue that Christopher's abilities are entirely due to his intact linguistic faculties, but the controls could employ more domain general kind of cognitive resources, like, you know, tension control, etc., which is why they could deal with those difficult processes. But I just mentioned, you know, a minute ago, that the LSTM in the Michelin Bowers paper approaches natural and unnatural structures in pretty much the same way. So it's not, you know, it's not a psychologically plausible model, I would argue, for whatever humans are doing. And similar observations can apply to the limits of transformer models in Le Creta's work. And all of these themes are like, right up there, they're saying that there's all the way to the present. So another one of Tal Linsen's recent papers that he posted a few weeks ago, looking at child directed speech, showed that LSTMs and transformers limited to ecologically plausible amounts of data generalize, as I mentioned, the linear rules for English, right, rather than the abstract rules. And in fact, more recent work from Linsen's lab last week, looking at, well, last year, I should say, shows that looking at garden paths, surprise does not explain syntactic disambiguation difficulty, right? Surprise will underpredicts the size of the garden path effect across all constructions. And this gets to this issue that you mentioned before, you know, maybe surprise all this related to some aspects of syntax, but maybe not other ones, it's kind of a, it's a very nontrivial issue that is very much, it's open to discussion. It's not, it hasn't settled yet. But so Linsen showed that garden path effects are just way more difficult than you would expect from mere unpredictability. So another way of phrasing this argument is to quote a recent argument with Chomsky's to get at this natural basis, unnatural issue. He says, suppose we have an expanded periodic table that includes all the elements that do exist, all the elements that can possibly exist, and all the elements that cannot possibly exist. And let's say you have some model, some artificial model that fails to distinguish between these three categories, whatever this model is doing, it's not helping those understand chemistry, right? It's doing something else. It's doing something for sure, but whether or not it's helping those understand chemistry is something separate. And I know that you've said in response to some of these studies, I think you've said that, you know, in order to show that something is likely to be impossible, somewhere in your paper, I think you say, in order to show that something is impossible with normal bounds and false positives, you'd need to show, you need to look at something like 500 independently sampled languages. So you cite this in your paper, right? Which you probably can't do, that's just not, it's not a feasible thing to do. So, you know, I'm not too sure that this really refutes the principle argument that I'm making here, right? Because people like Mitchell and Bowers are making an argument about impossibility in principle, not in some kind of extensional sense, you know, just like searching across the world languages to see, to prove across every single language that it is impossible, right? That's kind of, it's a different argument, whether it's impossible in some random language in the Amazon, compared to actually impossible, based on the principles of what the language system is actually doing, like what it can do. So I would just say that, you know, all of these kind of I think that that that point is, is that you don't actually know what is typologically not possible, right? So people like to say things like, you know, there's no language that does X, therefore we have to build that restriction into our statistical models. But if it's not statistically justified that there is no language that does X, right? If you've only looked at 20, 20 European languages or something, right? I mean, it's, it's not like that shouldn't motivate doing anything to the models, right? If it's, if it's not a statistically justified universal, I think. Well, you know, I think, you're totally right, but that just applies more generally to the social sciences and psychological sciences, right? Like typologically, it's very difficult to establish these things, right? So I guess you're, you're, I guess you're just kind of steelman you're a bit, you're saying that the strong claim is very difficult to prove, right? Like the reason the language that has X. The strong claim that something is not allowed in, in natural languages, I think very, very difficult to prove. And, you know, I think that there have been lots of, you know, strong attempts, there's been lots of strong claims from, often from, from generative syntax, right, about what all languages do. And I think that, you know, people have been very good at finding kind of counter examples to a lot of those things. I cite this paper by Evans and Levinson, which actually, you know, I had heard for years about how no language does X and that's what we're using to construct our theories. And that Evans and Levins paper, Evans and Levinson paper really kind of changed my mind about this, right? That like language is actually much more diverse than, than I think most, most syntacticians will, you know, try to construct theories for something. So, you know, I think we, going back to kind of the beginning of what you said, I think we, we'd agree that, that you need language architectures which learn the things that kids learn and learn it from data that they learn. And those architectures might, might be unlikely to be things like LSTMs or, you know, simple recurrent networks or, or whatever, right? Like, I think all of that work is, is very useful in, in kind of honing in on the right architecture. So, I'm just trying to, to remember all of, all of the points you were making. Oh, yeah. So, but I think this, that there, there's a kind of flip side to this, which is that I think that the space of things people can learn is actually kind of underestimated, right? Like, there's this bias to, to, to say, you know, people can't learn X, Y and Z. But people, at least outside of language have this, this really remarkable ability to learn different kinds of patterns, right? Like, the patterns you find in, in music or mathematics, for example, we can learn sophisticated types of, of algorithms, right? We can learn to, you know, fly a space shuttle or to, you know, tie knots in, for rock climbing or whatever, right? Like, there's all kinds of kind of procedural and algorithmic knowledge, which is structural that, that people are able to acquire. And I think that, that that notion very rightly kind of motivates looking for learning systems, which can work over pretty unrestricted spaces, right? So, you know, you, you, you might say that, okay, well, language is different because language is a restricted space. And it might be true that, that language is restricted, but it also might be true that the things we see in language come from other sources, right? It could be that languages, especially pragmatic, for example, compared to music or mathematics, right? And those kinds of pragmatic constraints are the things that constrain the, the form of language, right? Or language is communicative, it's probably more communicative than, than music, for example. And that might constrain the, the, the form of things. So, I mean, as, as you know, this is very old debate in, in linguistics about kind of where the, where the properties of, of natural language come from. And I guess what I'm trying to say is that there's one kind of perspective where you look at all of the things humans can do even outside of language, all of the rich structures and algorithms and processes we're able to learn about and internalize. And you say, okay, maybe language is like that. And then yes, language also has some of these other funny little properties. But, you know, maybe those come from some other, other pieces of, of where language comes from, right? It's, you know, we have pretty sophisticated pragmatic reasoning. We're using it to achieve certain communicative ends. You can find all kinds of kind of communicative features within the, the language system itself. And so, so maybe some of these other properties are, are properties that have some other origin. And that, that view, I think could be wrong, but it's, it's one that I think needs to be looked at to see if it's wrong, right? Like, I think it's been kind of dismissed by large chunks of, of linguists, right? Just, you know, I've heard people say stuff like, oh, well, communication doesn't really explain anything about language, right? And what they mean often is it doesn't explain like the particular island constraints or something that they're, that they're working on, right? But there's all kinds of other things in language that communicative pressures probably do explain. So I guess my, my pitch is always for, for kind of breadth in term, breadth in consideration of the forces that, that can shape language and not needing to put it all into some form of, of innate constraints or something like that. No, no, totally. And I think, I think a lot of that stuff is, is, is compatible with, with, with the minimalist program, because the minimalist program wants syntax to be minimal. It doesn't want it to be complicated. It doesn't want it to be, you know, any more complicated than it has to be. So there were some, you mentioned the curious properties, right? So there were some of the properties that need to be counted for in any model of language that are, I'll give you one example, right? The setting of Pearson features. And these Pearson features exhibit very non, non trivial generalizations that do not seem to be counted for via domain general learning mechanisms. So I'm citing here the work of Daniel Harbour at Queen Mary. So for example, the morphological composition of Pearson, its interaction of number, its connection to space, properties of its semantics and its linearization, they all appear to be strong candidates for our knowledge of language, right? What we mean by knowledge of language. But on the other hand, we have things like case and agreement and head movement. And these are all structural phenomena. However, they seem to resist a purely meaning based explanation in theoretical linguistics, right? It would be great if syntax were nothing but a computational engine that builds structured meaning. And that's the minimalist program, the goal. But that's not what we actually find. That's not in any actual minimalist, like concrete model, any concrete minimalist theory. The goal is just like, the program is language is perfect. Okay, that's the program. Is that what we find? No, obviously not. Okay, no, no linguist actually believes that. So it'd be great if syntax was like that. But I think, you know, the program is to look for perfection, but not always find it. So case and agreement and head movement are morphological, morphophonological phenomena, the properties of the performance systems, what's called performance systems. And so the minimalist program itself is really compatible with a lot of what you're saying about, you know, language, language, there are aspects of language that can be perfected and optimized for communicative efficiency. Absolutely. Totally. No doubt about it. But where is that locus of efficiency? Is it in the syntax itself? Or is it some kind of extra linguistic system? Is it in pragmatics? You know, is it in century motor? Is it in the speech? And property of speech and phonology? Probably, you know, I mean, who knows. But I think all of these things demand much more, you know, serious consideration into old fashioned notions like structure dependence, compositionality and what have you, things like that, which you can maybe find somewhere in the literature, but even just basic topics like, you know, quantifier raising, extended projections, adverbial hierarchies, all of these things in the minimalist program can be extra linguistic, right? They can actually be outside of syntax and very queer properties of the semantic conceptual systems, which are in themselves kind of domain general, weird leftovers from ancient primate cognition, right? The features of the way we pass events, the way we pass, you know, agents and patients, things like that. That's definitely not, that's not human specific. But, you know, the way that syntax provides instructions to these systems, you know, probably seems to be. So, you know, generative linguists have different theories of also language production too. I'll just talk about language production based on whether we store lemmas or whether we build words in the exact same way we will phrase and sentence this. So, I know that you make distinction between construction grammar and kind of generative grammar and, you know, the weight they place on memorizing constructions versus just building things from the bottom up, from the ground up, right? And so, you know, in some generative inspired models, mechanisms which generate syntactic structure make no distinctions between processes that apply above or below the word level. There's no point at which meaning syntax and form are all stored together as single atomic representations. Each stage in lexical access is a transition between different kinds of data structures, right? There's meaning, there's form and there's syntax. These three features kind of come in together and they don't always overlap. Different languages realize them in different ways. And so, you know, a word, the basic definition of a word is just this weird multi-system definition where lots of things, lots of different cognitive systems enrich the basis of every lexical item, right? You have, there's nothing like this really, this enrichment process anywhere else in linguistic theory, right? Or at least in what LLMs are doing. Like, so I guess, what, I guess I would ask you, what is your definition of a word, right? And what can LLMs really provide insights into weirdhood, right? Because if you can't, if you don't have a definition of what a word is, then you're really in trouble, right? Like, we have to at least use LLMs or artificial systems to inform what we mean by a word. Or maybe we don't need that anymore. I'm not sure what you think. I'm not sure what you mean. I mean, I don't have a... What is a word? Why does that matter? I mean, that's just a convention about how we use the term word, right? What, like, I mean, you could use, you know, lemmas or word firms or whatever. Like, that just feels like a conventional choice. I'm not sure what's at, what's at stake there. So how would you, I guess I would say, I agree, word is a conventionalization, you know. Our intuitive concept of word is often biased by orthography, the way we put spaces between things, right? So I agree with that criticism. You know, word in the intuitive sense is not really a scientific construct. However, I guess, let me rephrase my question. How would you, you know, decompose the intuitive concept of word into something that is more kind of, you know, scientifically amenable or psychologically plausible, which is exactly what genitive grammar tries to do by decomposing words into, you know, distinctive features, morphological categories, conceptual roots being matched with categorical features, you know, you get a concept, you know, and you match it with a noun or a verb category to get a noun or a verb. These different models make different predictions, right? Yeah, I mean, I think that general idea is likely to be right for large language models. Like, I think they kind of must have things that are kind of like part of speech categories, for example. And I think that they kind of must be able to update those, their categories based on the language that they've seen so far, right? So like, like, you know, GPT puts nouns and verbs in the right places. And to do that, you kind of need some representation of the nouns versus the verbs, and you need some ability to locate yourself in a string of other words and figure out if there's likely to be a noun or a verb next. So I think that on that level, those kinds of properties of words are very likely to be right. And there are also things which are very likely to be found kind of in the internal representations of these models. I don't see how it could be any other way other than that. But like, as far as I know, that's not where the main debates or disagreement, I think, is, right? Like, I think all theories of language have to have to say that there's different kinds of words that can show up in different places or something like that. Yeah. Okay, so how about the issue? You mentioned communication, right? So, you know, and you're totally right, when Trump says things like language is a thought system or, you know, language didn't evolve, he's kind of being a little bit cheeky. He doesn't really mean that. He kind of means it in a very specific sense, right? But, you know, when we say language is a thought system, what we mean is we're trying to get it an architectural claim. So if you look at the architecture of the minimalist program, the syntactic derivation and the conceptual systems are literally different systems, right? The conceptual systems take stuff from syntax and then does its own business with it and the CI systems have their own peculiar rules and principles, which is why thought and language are both similar symbolic compositional systems, but in different ways. Only a subset of thought is properly called the CI interface system, since the CI systems are by definition, you know, whatever conceptual systems you would have that can access and read out instructions from syntax. And we don't know what they are fully. They seem to have something to do with events and grammatical reference and definiteness. They seem to be the main categories that language, you know, cares about conceptually, but we don't really know. That's kind of just a hypothesis, right? But what we do know is that they don't seem to make use of color all that much. So no language morphologically marks shades of color. All the conceptual features like worry or concern, like no language morphologically marks a degree of worry or concern about an issue, but we do make use of epistemological notions like evidentiality and things like that. So, you know, I guess what I'm saying is the minimalist program does a good job of trying to figure out which aspects of thought language is intimately tied to, and which aspects of thought it's not tied to. So the minimalist program allows us to kind of carve that up quite neatly. And this is a much more nuanced framework than, you know, when Chomsky says language is thought, again, he doesn't, maybe he means it, maybe he doesn't, but that's not what the actual architecture of his theory says. It's a rhetorical device that is very, you know, useful and interesting to attract undergraduate audiences. But if you look at actual theories that are coming out of the minimalist program, no one really believes language equals thought, right? The language system seems to, it tries its best to access and reformat and manipulate various conceptual systems, but it has its limits, right? We know what systems, spell keys, core knowledge systems are hooked up to with respect to the syntax engine, and which ones are not. So, you know, this kind of gets back to the idea that lexicalization of a concept seems to maybe alter it in some way. It kind of imbues it with elements that are not there in the concept itself. So if you lexicalize the concept, you suddenly transform it a little bit, you give it a little extra, you sprinkle something else on top of it, and that seems to vary across different noun types. But these are all like very clear architectural claims within gem diagram that make very clear empirical predictions. So in other words, I guess what I'm saying is all these neuropsychology studies that are coincided, you know, in a lot of work in this fame, what does it really show? I think it shows that, you know, when language is damaged in the brain, it loses its particular sway or mode of influencing those systems. But there's no real prediction from within the gem to grammar enterprise that those non-linguistic systems should be impaired or should suddenly, you know, shut down if the core language system is compromised, right? In fact, if anything, that just emphasizes the principal divorce between the syntactic system and non-linguistic systems, right? So I think the, a lot of predictions here from the language and communication, you know, literature are kind of missing the point of the architectural claims. I can just give, or Daniel, do you want to go? Yeah, go ahead. Give a little bit of background there. So there's these papers from Ev Fedorenko and Rosemary Varley that are examining in part of them aphasic patients. So people who have impaired linguistic abilities, basically showing that with impaired linguistic abilities, you can still have preserved kind of reasoning abilities. So people like chess masters, chess grandmasters, for example, who are obviously very good at reasoning might not have kind of intact linguistic abilities. And then complimenting that kind of patient work, there's also work from Ed's lab showing that the parts of the brain that care about language are separable from the parts of the brain that care about other domains, even ones that seem kind of language-like. So things like music and mathematics tend not to happen in the language areas. So Ev and others have argued that this is basically evidence against the Chomsky inclaim that language is the medium for thinking, because there's thinking that can happen in the absence of language and the brain areas that care about language seem not to be the brain areas that care about thinking. I guess, Elliot, you're saying that people don't really believe that. They don't believe that distinction, I mean. And also, there's a lot of self-contradiction even within these arguments, right? So in your paper, you sometimes say that Chomsky thinks that language is a thought system, but then a few pages later, you'll say Chomsky also believes that syntax is some totally separate system from anything else, right? Your autonomy of syntax, etc. So does Chomsky think- That's not my contradiction. I mean, he said both of those things. Right, exactly. So therefore, you may want to ask yourself, does he really believe these things? Or what is the case if it arises from the architecture, right? So just saying language is a thought system, what does that mean? That doesn't mean anything. It's just a very vague statement. The question is how exactly is language contributing to thought and how is it not contributing? Yeah, I mean, I think his claim is mainly evolutionary or something, right, that this is the origins of the system, which I think is sort of equally hard to square with the kind of patient and neuroimaging data. But if he doesn't think that, then he shouldn't say it, or people will respond to what he said, I think. The argument is that language is a kind of thought system. It regulates some aspects of thought and it yields some aspects of thought that are clearly unique to humans, but it's not intrinsically or causally tied to it. The architecture of the system is very different from the kind of generalizations you can rhetorically evince from the architecture. So for instance, when you cite work from a phasic patient showing no deficits in complex reasoning, as you just mentioned, playing chess and so on, we would actually expect this under a kind of non-lexicalist framework of generative syntax, where meaning, as I said, meaning syntax and form, form just meaning anything that you can externalize language in, all these things are separate features and separate systems. The autonomy of syntax doesn't mean, what a lot of people think it means, it just means that there are certain syntactic operations that are not semantic. There are certain things you can do with syntax that you can only do with syntax and you can't do with semantics. So this gets back to the difference between Petrowski's theory that semantics is just and versus a lot of syntacticians' belief that there are certain peculiar weird things you can do with syntax that are just syntactic. So there is a divorce even within the kind of architectural framework. So it's not too surprising that you also find that divorce at the neuropsychological level, I would say. Well, I think I would want a prediction of the language's thought evolutionary idea then. If you're saying that doesn't predict that thought relies on language, then I think whoever likes that theory should come up with some predictions about what that theory actually means. I feel like those kinds of predictions are often really necessary for understanding the content of a prediction. So sorry, Daniel, your hand's been up for a while. No, it's all good. Just kind of wanted to bring a breath in and an opportunity for anyone to ask any other questions. But wow, thank you both for the many topics we've covered. We'll have, in the last minutes, a kind of conclusion in next steps. But Dave, would you like to ask a question or just give a short reflection? Okay, no. There are many comments in the chat, so I hope that both of you can read them on your own time to see what everyone added. Where do we go from here? As we roar into May 2023 and beyond, what can linguists, large language model developers and users, cognitive scientists, what do you each think are some of the most fruitful pathways forward? Well, I would say the most fruitful pathway forward is to really take cognitive psychology seriously. There's a lot of nice work recently trying to align things like chat GPT, Wolfram Alpha plugins, the way that chat GPT can interface with different kind of modules. The way of building a legitimate kind of AGI system doesn't necessarily have to be psychologically reliant on the kind of modules that human beings have, but I think it will benefit from it. So there have been some claims that large language models can maybe do all sorts of things. But I think in the long run, it's most likely going to be the case that LLMs can do something very important and very interesting, but it's only going to be one piece of the puzzle. So in fact, even OpenAI CEO Sam Altman said last week that what we can do with LLMs has really kind of been exhausted. We need new directions, new avenues and so on. I guess he was probably speaking to investors more than linguistic students here, but I think he's also right. LLMs can do something spectacular, but they're probably going to form a small part of the general AGI architecture, right? If you want to think about AGI as a potential, potential goal here. So, you know, I think a lot of the, so let me give me an example here. So Anna Ivanova, who's a very good cognitive scientist, she has a paper recently arguing for a kind of modular architecture for LLMs, which is a very nice framework, right? It's very cognitively plausible. It's exactly the kind of thing that we should be pushing for. It's compatible with Howard Gardner's, you know, notion of multiple intelligences and so on. But I think at the same time, just to finish this comment, there was a tech talk last week, I think, or maybe a few days ago, where a lot of this stuff can be conflated with AI hype in an unproductive way. So Greg Brockman from OpenAI, he gave one of his, one of these big TED talks where he showed different plugins that chat GPD can do. I mentioned Wolfram Alpert, right? But there's also things like image generation, Instacart shopping, where you can get chat GPD to buy you things and what have you. And again, this takes you back to the idea that multiple subsystems can do different sub-functions. So Brockman also showed an example of giving chat GPD an Excel file, a CSV file from an archive database of academic papers, where it just listed a bunch of papers and then titles and what have you, right? And he said that, using chat GPD, it uses world knowledge to infer what the titles of the columns mean. So we understood that title means the title of the paper. It understood that authors mean the number of authors per paper. It understood that created means the date the paper was submitted, right? And because it's a TED talk, the audience gave us a standing elevation, right? But the ability to describe labels on an Excel file is, I guess, nice. But I'm not sure you'd really call it world knowledge. So I guess, I would just say there's a lot of progress needs to be made alongside reducing anthropomorphism. You have to have the right balance of it. So like I said, you have to have the right balance of psychologically plausible kind of modular architecture, but you can't have too much anthropomorphism because then you'll get carried away. You have to find, we have to find the right balance between modeling kind of human-like modular systems, but not doing it to a degree that is a bit implausible or scientifically unhelpful. I mean, I think I agree with all of that. I'm really excited about these ways of kind of connecting language models to other forms of information processing, which does seem like what people have. I think I've been very surprised at the the things they are able to do just as language modeling, right? So different kinds of reasoning puzzles and things that they can solve, I think, is really fascinating and maybe will require us to rethink the relationships between language and thought and try to figure out a way of being specific about what it means for something to have a representation or to reason over that representation. But ultimately, I think I agree that people have different modes of thinking about things and that seems important for intelligence. I'm also super excited about the BabyLM challenge. So I think on the kind of linguistic side, that's exactly the right thing of seeing how far we can get with smaller data sets and maybe eventually after that trying to understand some more about the kinds of semantics that kids acquire and where they get it from and how kind of external semantics can inform language learning or specifically maybe grammar and syntax learning. I guess my other path forward point would be that there's... I feel like these kinds of models have have really gone far beyond people's expectations for this kind of class of model, right? Kind of ground up statistical learning, discovering patterns in text seems to give really pretty remarkable results. And that for me going forward, I think has just introduced a huge wave of uncertainty over theories. So I think that our theories of basically everything in language for sure, but cognition, probably neuroscience, like all of those things I think are going to be reworked when we really come to kind of understand the ability of really general kinds of learning systems like these. So that makes it on the one hand kind of a bummer for past theories, especially theories which relied on learning not being able to work well. But on the upside, I think it makes it a very exciting time both for AI and cognitive science and linguistics, where now there's these really, really powerful tools that seem like a qualitatively different size step towards human abilities. And I think kind of integrating them and taking both the kind of engineering lessons and the kind of philosophical lessons about how they're made and what kinds of principles go into designing intelligent systems. I think that those things will really shape the field over the next five or 10 years. And also, I would just say in the context of broader themes here, right, like you're totally right, like I remember when I was reading about when Deep Blue, the Kasparov, was it, the chess thing, right? And there were some commentators who said, you know, chess is over. If an AI can beat a human, then it's game over. What's the point in studying chess? You know, there's no need of boring anymore. And I guess if AI has achieved seemingly everything that humans need to do to play chess, what's the point of playing it? But I think, you know, if anything, it turned out to increase the popularity of chess, right? There are now many chess celebrities as well, worldwide tournaments. And I would predict that the same is probably going to happen with language too. You know, LLMs do not mean it's the end of language, no more language, no more linguistics. I would actually push back and say maybe it would be the opposite. You know, the success of LLMs will increase general interest in linguistic theory, due to their apparent, you know, weird constraints and apparent limitations, right? Because I would also say, you know, scale, at this point, the chess issue, scale is kind of definitely far from all that's needed. What is lacking is an ability of LLMs to, you know, really abstract their knowledge and experiences in order to make robust predictions and generalizations and so on. I gave some examples, but there's some others in the literature where it doesn't seem to really be good at generalizing. It can kind of mimic particular token types. But I would, you know, I would guess my final, my final thing would be that, you know, the language acquisition literature doesn't necessarily need LLMs though. You know, cognitive scientists don't really need LLMs. We could potentially, you know, me and Stephen obviously disagree here, but I would say big tech companies profiting off LLMs need LLMs, right? They're the only ones that really do. It may be the case that the mind is a very, I will say, you know, the mind is a very diverse space. It may be that there are certain forms of behavior and learning that might be captured by processes similar to what LLMs are doing. So Stephen has given some interesting examples in his papers about magnetism and we're kind of rules of learning that are very, very general and very quick and very mysterious. So, you know, maybe for those sorts of things, that kind of learning will be relevant. But I still think it's unlikely that one of the candidates will be natural language, at least the way natural language works in its full glory in terms of the four meaning regulation and what have you. So I guess I would, you know, it kind of reminds me of where you, you know, you have this image of, I saw John with chapter four recently, right? And he has this, there's this scene where he's walking in the desert and he's not sure if he's seen this guy that he wants to assassinate. It's kind of like when you walk in the desert and you have an illusion of seeing an oasis because it turns out you're hallucinating. But then you realize that, you know, sometimes before it's too late that you actually are hallucinating. It's you're not seeing an oasis. You're still in the desert. And I think that's kind of maybe the situation we're in right now with linguistic competence of laws of language models. We have the illusion of linguistic competence. But, you know, you always see the illusion before you find the oasis, right? So I think, I think right now we're in the hallucinating stage of the desert where we're seeing potential sparks of linguistic competence, but it's still not very clear and I'm robust. And we haven't actually reached the oasis yet. Um, just a rapid fire question. So see if you can give a short response. So Sphinode, you know, writes question, is it correct to say that large language models have no priors? Do large language models have priors? I'd say yes, they definitely do. Um, and there, I think the difference to how people, you know, are used to thinking about priors in Bayesian inference, for example, if you like write down a Bayesian statistical model, you say like, you know, here's the parameters and here's what the priors are on the parameters. Large language models, I think the priors are and maybe neural nets in general, I think that the that the priors are much more implicit, right? So there's some functions which they find easier to learn than other functions. And there's even some work trying to discover, you know, some statement of what those kind of implicit priors are. But that's actually how I think about, you know, comparison of different neural network architectures, right, which is maybe something Elliot and I might agree on, right? Like you have to find priors which allow them to learn the things that kids learn, right? And not all architectures will do that. Even among architectures which are turn complete or capable of learning any kind of function, not all of them will do it, even on kind of huge data set sizes. So I think of this sort of search over neural net architectures as really one of a search over priors. But it's not priors or, I mean, you could think of it as a search over universal grammar or something, right? But it's, it's, it's not priors or universal grammar in the sense that people have talked about it as like an explicit statement about what kinds of rules are allowed or an explicit statement about what kinds of functions or high probability or something like that. It's all implicitly coded there. Yeah, totally. I think, I think that's right. I mean, you know, the real question is reducing the space of what those priors are like. And if it's anything remotely like what human beings are doing, so LLMs like, I would, I would at least say that things like GPT-3 are an existence proof of, you know, that building fully functioning syntactic categories from surface distributional analysis alone is possible. That's, yes, that is correct. But, you know, even so, I would say most syntacticians don't really believe that syntactic categories are innate. So the prior issue is slightly less relevant here. It's the operations that are said to be innate. So the, in the syntax domain, it's particular linguistic computations that are said to be innate and categories themselves. In fact, even Charles Young has admitted in the last couple of years that they are maybe innate, but maybe not. So people have given, I know of a relevant prize, they are things like, you know, me and Gary Marcus have talked about compositionality. That seems to be a big problem. So people have given chat GPT BBC news articles asking it to compress it and then re-explain it. So one example I saw was Peter Smith 58 is being arrested on charges of manslaughter and you get it to compress it and re-explain it. And it comes out as 58 people are being charged with manslaughter. All right. That's a pretty clear example of a lack of compositionality being built into whatever compression it's doing. And there's no example where there's been, there's some examples of potential analogical reasoning. So in Bing chat, you know, Bing has this, this chat function. The question is, is it just finding meta relations that have already been documented by humans or is it genuinely creating new relations that the new stuff that is being built. So, you know, someone asked through me a table comparing Jesus Christ with the Nokia 9910, right, the cell phone Nokia 9910. And it said, you know, it compared the release dates. It compared the size, the weight. It compared the CPU with Jesus's all-powerful knowledge. It compared the memory of the phone with the all-knowing nature of God, right. And it also, I think it said that they were both resurrected because the Nokia was re-released a couple of times, right. So the Nokia. That sounds like a great answer. What's wrong with that? Okay. It may be. It sounds a lot like analogical reasoning, but then it also had some quite weird ones where it was like, you know, for the camera, it said, no, it just gave Jesus's description, but it's not really what a camera is. There's some kind of things that look like analogical reasoning, maybe, but it's unclear. Yeah. I think that sounds like an awesome answer to me. I was going to say, like, you said large-language models learn they're an existence proof of part of speech categories, but like, they don't just output part of speech categories, right. Like, they have a lot of grammatical syntactic knowledge. And moreover, like, they have a lot of semantic knowledge and probably some pragmatic knowledge. And, you know, they're not bad at translation. And like, it's way more that they have discovered than just part of speech categories. Well, sorry, I said syntactic. I'm sorry. It's like syntactic categories. Right. Well, sorry. Yeah. Yeah. But they've discovered way more than that. Yeah. I'm going to, as a teaser slash motivator for hopefully both of you to join again in the future with or without other guests, a few of the exciting questions just for us to include in this transcript. And then thank you both, Elliott and Stephen, for joining. So just a few of the last questions that were asked, Juan asked, how do small transformers, Zhang et al. 2020, compared with children learning language? 96 asked, what are your thoughts on implicit priors versus animal instinct? Rojda asked, what constraints that space in LLMs, don't they get there by training? So are they discovering it? That's not what they implement at the start maybe. And there's many more questions. So I hope that we can all review and reread each other's works and come together for 41.2 in some future time. Thank you, Elliott and Stephen, for this excellent stream. Thank you, Dave. Thank you both. Yeah. Thank you so much. Very well. Bye. See you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.540000000000001, "text": " Hello and welcome, everyone, to the Active Inference Institute.", "tokens": [50364, 2425, 293, 2928, 11, 1518, 11, 281, 264, 26635, 682, 5158, 9446, 13, 50841], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 1, "seek": 0, "start": 9.540000000000001, "end": 16.64, "text": " This is Active Gueststream number 41.1 on April 25, 2023.", "tokens": [50841, 639, 307, 26635, 2694, 377, 9291, 1230, 18173, 13, 16, 322, 6929, 3552, 11, 44377, 13, 51196], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 2, "seek": 0, "start": 16.64, "end": 20.46, "text": " We're here with Elliot Murphy and Steven Piantadosi.", "tokens": [51196, 492, 434, 510, 365, 38986, 28549, 293, 12754, 430, 5798, 4181, 72, 13, 51387], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 3, "seek": 0, "start": 20.46, "end": 22.28, "text": " This is going to be quite a discussion.", "tokens": [51387, 639, 307, 516, 281, 312, 1596, 257, 5017, 13, 51478], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 4, "seek": 0, "start": 22.28, "end": 25.64, "text": " We will begin with opening statements from Steven and Elliot.", "tokens": [51478, 492, 486, 1841, 365, 5193, 12363, 490, 12754, 293, 38986, 13, 51646], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 5, "seek": 0, "start": 25.64, "end": 29.88, "text": " Elliot will then lead with some questions and we'll have an open discussion at the", "tokens": [51646, 38986, 486, 550, 1477, 365, 512, 1651, 293, 321, 603, 362, 364, 1269, 5017, 412, 264, 51858], "temperature": 0.0, "avg_logprob": -0.21802106110945993, "compression_ratio": 1.5276595744680852, "no_speech_prob": 0.3471088111400604}, {"id": 6, "seek": 2988, "start": 29.88, "end": 30.88, "text": " end.", "tokens": [50364, 917, 13, 50414], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 7, "seek": 2988, "start": 30.88, "end": 36.08, "text": " So, Steven, please, thank you for joining and to your opening statement.", "tokens": [50414, 407, 11, 12754, 11, 1767, 11, 1309, 291, 337, 5549, 293, 281, 428, 5193, 5629, 13, 50674], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 8, "seek": 2988, "start": 36.08, "end": 37.08, "text": " Cool.", "tokens": [50674, 8561, 13, 50724], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 9, "seek": 2988, "start": 37.08, "end": 39.36, "text": " Hi, so I'm Steve Piantadosi.", "tokens": [50724, 2421, 11, 370, 286, 478, 7466, 430, 5798, 4181, 72, 13, 50838], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 10, "seek": 2988, "start": 39.36, "end": 45.32, "text": " I'm a professor in psychology and neuroscience at UC Berkeley.", "tokens": [50838, 286, 478, 257, 8304, 294, 15105, 293, 42762, 412, 14079, 23684, 13, 51136], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 11, "seek": 2988, "start": 45.32, "end": 50.08, "text": " And I guess part of the reason that we're here is that I recently wrote a paper on large", "tokens": [51136, 400, 286, 2041, 644, 295, 264, 1778, 300, 321, 434, 510, 307, 300, 286, 3938, 4114, 257, 3035, 322, 2416, 51374], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 12, "seek": 2988, "start": 50.08, "end": 57.08, "text": " language models in part trying to convey some enthusiasm about what they've kind of accomplished", "tokens": [51374, 2856, 5245, 294, 644, 1382, 281, 16965, 512, 23417, 466, 437, 436, 600, 733, 295, 15419, 51724], "temperature": 0.0, "avg_logprob": -0.22143470362613077, "compression_ratio": 1.4917355371900827, "no_speech_prob": 0.004455093294382095}, {"id": 13, "seek": 5708, "start": 57.08, "end": 61.0, "text": " in terms of learning syntax and semantics.", "tokens": [50364, 294, 2115, 295, 2539, 28431, 293, 4361, 45298, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 14, "seek": 5708, "start": 61.0, "end": 65.6, "text": " And in part pointing out, I think that these models really change how we should think about", "tokens": [50560, 400, 294, 644, 12166, 484, 11, 286, 519, 300, 613, 5245, 534, 1319, 577, 321, 820, 519, 466, 50790], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 15, "seek": 5708, "start": 65.6, "end": 72.24, "text": " language, how we should think about theories of linguistic representation and theories", "tokens": [50790, 2856, 11, 577, 321, 820, 519, 466, 13667, 295, 43002, 10290, 293, 13667, 51122], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 16, "seek": 5708, "start": 72.24, "end": 76.36, "text": " of grammar and likely also theories of learning.", "tokens": [51122, 295, 22317, 293, 3700, 611, 13667, 295, 2539, 13, 51328], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 17, "seek": 5708, "start": 76.36, "end": 77.36, "text": " Yeah.", "tokens": [51328, 865, 13, 51378], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 18, "seek": 5708, "start": 77.36, "end": 78.36, "text": " Awesome.", "tokens": [51378, 10391, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 19, "seek": 5708, "start": 78.36, "end": 79.36, "text": " Yeah.", "tokens": [51428, 865, 13, 51478], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 20, "seek": 5708, "start": 79.36, "end": 81.36, "text": " So, I'm Elliot Murphy.", "tokens": [51478, 407, 11, 286, 478, 38986, 28549, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 21, "seek": 5708, "start": 81.36, "end": 85.68, "text": " I'm a postdoc in the Department of Neurosurgery at UC Health in Texas.", "tokens": [51578, 286, 478, 257, 2183, 39966, 294, 264, 5982, 295, 1734, 8977, 374, 7337, 412, 14079, 5912, 294, 7885, 13, 51794], "temperature": 0.0, "avg_logprob": -0.2193838062852916, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.006669767200946808}, {"id": 22, "seek": 8568, "start": 85.68, "end": 89.60000000000001, "text": " I read Steven's paper with great interest and there's a lot of people.", "tokens": [50364, 286, 1401, 12754, 311, 3035, 365, 869, 1179, 293, 456, 311, 257, 688, 295, 561, 13, 50560], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 23, "seek": 8568, "start": 89.60000000000001, "end": 93.44000000000001, "text": " There were some areas of convergence, but the things I want to kind of focus on today", "tokens": [50560, 821, 645, 512, 3179, 295, 32181, 11, 457, 264, 721, 286, 528, 281, 733, 295, 1879, 322, 965, 50752], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 24, "seek": 8568, "start": 93.44000000000001, "end": 98.52000000000001, "text": " in responding to Steven and kind of probing how to do with areas of divergence maybe.", "tokens": [50752, 294, 16670, 281, 12754, 293, 733, 295, 1239, 278, 577, 281, 360, 365, 3179, 295, 47387, 1310, 13, 51006], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 25, "seek": 8568, "start": 98.52000000000001, "end": 104.2, "text": " So, you know, Steven's paper is based on the idea that modern machine learning has subverted", "tokens": [51006, 407, 11, 291, 458, 11, 12754, 311, 3035, 307, 2361, 322, 264, 1558, 300, 4363, 3479, 2539, 575, 1422, 18537, 51290], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 26, "seek": 8568, "start": 104.2, "end": 108.80000000000001, "text": " and bypassed the entire theoretical framework of Chomsky's approach.", "tokens": [51290, 293, 24996, 292, 264, 2302, 20864, 8388, 295, 761, 4785, 4133, 311, 3109, 13, 51520], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 27, "seek": 8568, "start": 108.80000000000001, "end": 111.84, "text": " So I wanted to kind of respond to some of these main arguments and some other related", "tokens": [51520, 407, 286, 1415, 281, 733, 295, 4196, 281, 512, 295, 613, 2135, 12869, 293, 512, 661, 4077, 51672], "temperature": 0.0, "avg_logprob": -0.18957692333775708, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.02610117197036743}, {"id": 28, "seek": 11184, "start": 111.84, "end": 116.56, "text": " arguments in the literature that some folks listening might have some insight and thoughts", "tokens": [50364, 12869, 294, 264, 10394, 300, 512, 4024, 4764, 1062, 362, 512, 11269, 293, 4598, 50600], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 29, "seek": 11184, "start": 116.56, "end": 117.56, "text": " on.", "tokens": [50600, 322, 13, 50650], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 30, "seek": 11184, "start": 117.56, "end": 122.0, "text": " So, it's a very common criticism to say that large language models just predict the next", "tokens": [50650, 407, 11, 309, 311, 257, 588, 2689, 15835, 281, 584, 300, 2416, 2856, 5245, 445, 6069, 264, 958, 50872], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 31, "seek": 11184, "start": 122.0, "end": 125.80000000000001, "text": " token, which is obviously a bit of a cliche, right?", "tokens": [50872, 14862, 11, 597, 307, 2745, 257, 857, 295, 257, 46705, 11, 558, 30, 51062], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 32, "seek": 11184, "start": 125.80000000000001, "end": 127.0, "text": " It's not quite true.", "tokens": [51062, 467, 311, 406, 1596, 2074, 13, 51122], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 33, "seek": 11184, "start": 127.0, "end": 132.56, "text": " They don't just predict the next token, they also seem to confabulate, they seem to hallucinate,", "tokens": [51122, 814, 500, 380, 445, 6069, 264, 958, 14862, 11, 436, 611, 1643, 281, 1497, 455, 5256, 11, 436, 1643, 281, 35212, 13923, 11, 51400], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 34, "seek": 11184, "start": 132.56, "end": 138.0, "text": " they maybe lie, they randomly provide different answers to the same question, and they seem", "tokens": [51400, 436, 1310, 4544, 11, 436, 16979, 2893, 819, 6338, 281, 264, 912, 1168, 11, 293, 436, 1643, 51672], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 35, "seek": 11184, "start": 138.0, "end": 141.12, "text": " to stochastically mimic language like structures.", "tokens": [51672, 281, 342, 8997, 22808, 31075, 2856, 411, 9227, 13, 51828], "temperature": 0.0, "avg_logprob": -0.15612119038899738, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.07346874475479126}, {"id": 36, "seek": 14112, "start": 141.12, "end": 144.16, "text": " They sometimes correct themselves sometimes when they shouldn't.", "tokens": [50364, 814, 2171, 3006, 2969, 2171, 562, 436, 4659, 380, 13, 50516], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 37, "seek": 14112, "start": 144.16, "end": 147.12, "text": " If you push them a little, they kind of change their mind sometimes.", "tokens": [50516, 759, 291, 2944, 552, 257, 707, 11, 436, 733, 295, 1319, 641, 1575, 2171, 13, 50664], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 38, "seek": 14112, "start": 147.12, "end": 150.4, "text": " In fact, if Fox News is currently looking for a replacement for Tucker Carlson, they", "tokens": [50664, 682, 1186, 11, 498, 11388, 7987, 307, 4362, 1237, 337, 257, 14419, 337, 35581, 14256, 3015, 11, 436, 50828], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 39, "seek": 14112, "start": 150.4, "end": 151.4, "text": " could do less.", "tokens": [50828, 727, 360, 1570, 13, 50878], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 40, "seek": 14112, "start": 151.4, "end": 156.4, "text": " They could definitely do worse than using ChachiBT if they're looking for a similar", "tokens": [50878, 814, 727, 2138, 360, 5324, 813, 1228, 761, 21791, 33, 51, 498, 436, 434, 1237, 337, 257, 2531, 51128], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 41, "seek": 14112, "start": 156.4, "end": 157.4, "text": " caliber.", "tokens": [51128, 41946, 13, 51178], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 42, "seek": 14112, "start": 157.4, "end": 161.88, "text": " So, these models seem to do all sorts of like wild things, and over the past 10 years, there's", "tokens": [51178, 407, 11, 613, 5245, 1643, 281, 360, 439, 7527, 295, 411, 4868, 721, 11, 293, 670, 264, 1791, 1266, 924, 11, 456, 311, 51402], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 43, "seek": 14112, "start": 161.88, "end": 166.32, "text": " been a sequence of different, you know, systems developed like where to be, where to be, and", "tokens": [51402, 668, 257, 8310, 295, 819, 11, 291, 458, 11, 3652, 4743, 411, 689, 281, 312, 11, 689, 281, 312, 11, 293, 51624], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 44, "seek": 14112, "start": 166.32, "end": 169.84, "text": " each of them is based on a different neural net approach, but ultimately they all seem", "tokens": [51624, 1184, 295, 552, 307, 2361, 322, 257, 819, 18161, 2533, 3109, 11, 457, 6284, 436, 439, 1643, 51800], "temperature": 0.0, "avg_logprob": -0.252685005600388, "compression_ratio": 1.717142857142857, "no_speech_prob": 0.0025252525229007006}, {"id": 45, "seek": 16984, "start": 169.84, "end": 174.56, "text": " to take words and characterize them by lists of hundreds or thousands of numbers.", "tokens": [50364, 281, 747, 2283, 293, 38463, 552, 538, 14511, 295, 6779, 420, 5383, 295, 3547, 13, 50600], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 46, "seek": 16984, "start": 174.56, "end": 181.8, "text": " So the GTP3 network has 175 billion weights, 96 attention heads in its architecture, and", "tokens": [50600, 407, 264, 460, 16804, 18, 3209, 575, 41165, 5218, 17443, 11, 24124, 3202, 8050, 294, 1080, 9482, 11, 293, 50962], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 47, "seek": 16984, "start": 181.8, "end": 184.6, "text": " as far as what I know, maybe Stephen can correct me here.", "tokens": [50962, 382, 1400, 382, 437, 286, 458, 11, 1310, 13391, 393, 3006, 385, 510, 13, 51102], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 48, "seek": 16984, "start": 184.6, "end": 188.36, "text": " We don't really have a great idea of what these different parts really mean.", "tokens": [51102, 492, 500, 380, 534, 362, 257, 869, 1558, 295, 437, 613, 819, 3166, 534, 914, 13, 51290], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 49, "seek": 16984, "start": 188.36, "end": 190.08, "text": " It just seems to kind of work that way.", "tokens": [51290, 467, 445, 2544, 281, 733, 295, 589, 300, 636, 13, 51376], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 50, "seek": 16984, "start": 190.08, "end": 195.04, "text": " Like attention heads in GTP3 can pay attention to much earlier tokens in the string in order", "tokens": [51376, 1743, 3202, 8050, 294, 460, 16804, 18, 393, 1689, 3202, 281, 709, 3071, 22667, 294, 264, 6798, 294, 1668, 51624], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 51, "seek": 16984, "start": 195.04, "end": 198.84, "text": " to help them predict the next token, but the whole architecture from start to finish is", "tokens": [51624, 281, 854, 552, 6069, 264, 958, 14862, 11, 457, 264, 1379, 9482, 490, 722, 281, 2413, 307, 51814], "temperature": 0.0, "avg_logprob": -0.14812661731053914, "compression_ratio": 1.64375, "no_speech_prob": 0.07058243453502655}, {"id": 52, "seek": 19884, "start": 198.84, "end": 204.76, "text": " kind of engineering-based motivations, and I always kind of wonder what about all the", "tokens": [50364, 733, 295, 7043, 12, 6032, 39034, 11, 293, 286, 1009, 733, 295, 2441, 437, 466, 439, 264, 50660], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 53, "seek": 19884, "start": 204.76, "end": 209.6, "text": " models that kind of failed from these LLMs, from the different tech companies.", "tokens": [50660, 5245, 300, 733, 295, 7612, 490, 613, 441, 43, 26386, 11, 490, 264, 819, 7553, 3431, 13, 50902], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 54, "seek": 19884, "start": 209.6, "end": 213.36, "text": " It's like these companies often seem to, you know, make it seem like they have these models", "tokens": [50902, 467, 311, 411, 613, 3431, 2049, 1643, 281, 11, 291, 458, 11, 652, 309, 1643, 411, 436, 362, 613, 5245, 51090], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 55, "seek": 19884, "start": 213.36, "end": 218.84, "text": " that really work very well straight out the box, and they all seem to be named after some", "tokens": [51090, 300, 534, 589, 588, 731, 2997, 484, 264, 2424, 11, 293, 436, 439, 1643, 281, 312, 4926, 934, 512, 51364], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 56, "seek": 19884, "start": 218.84, "end": 220.84, "text": " kind of famous artists, right?", "tokens": [51364, 733, 295, 4618, 6910, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 57, "seek": 19884, "start": 220.84, "end": 223.12, "text": " They have Dali after Salvador Dali.", "tokens": [51464, 814, 362, 413, 5103, 934, 32586, 413, 5103, 13, 51578], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 58, "seek": 19884, "start": 223.12, "end": 227.24, "text": " They have Da Vinci, maybe pretty soon one of these companies will release a large language", "tokens": [51578, 814, 362, 3933, 15011, 537, 11, 1310, 1238, 2321, 472, 295, 613, 3431, 486, 4374, 257, 2416, 2856, 51784], "temperature": 0.0, "avg_logprob": -0.203645814725054, "compression_ratio": 1.7809187279151943, "no_speech_prob": 0.06077929586172104}, {"id": 59, "seek": 22724, "start": 227.24, "end": 231.52, "text": " model called Jesus or something, I don't know.", "tokens": [50364, 2316, 1219, 2705, 420, 746, 11, 286, 500, 380, 458, 13, 50578], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 60, "seek": 22724, "start": 231.52, "end": 234.04000000000002, "text": " But they always say, here's our new foundation model.", "tokens": [50578, 583, 436, 1009, 584, 11, 510, 311, 527, 777, 7030, 2316, 13, 50704], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 61, "seek": 22724, "start": 234.04000000000002, "end": 235.04000000000002, "text": " It's called Picasso.", "tokens": [50704, 467, 311, 1219, 49708, 13, 50754], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 62, "seek": 22724, "start": 235.04000000000002, "end": 236.04000000000002, "text": " It's the first one we tried.", "tokens": [50754, 467, 311, 264, 700, 472, 321, 3031, 13, 50804], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 63, "seek": 22724, "start": 236.04000000000002, "end": 237.04000000000002, "text": " It works just great.", "tokens": [50804, 467, 1985, 445, 869, 13, 50854], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 64, "seek": 22724, "start": 237.04000000000002, "end": 240.72, "text": " No problems straight out the box, but I always wonder what about all the other black boxes", "tokens": [50854, 883, 2740, 2997, 484, 264, 2424, 11, 457, 286, 1009, 2441, 437, 466, 439, 264, 661, 2211, 9002, 51038], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 65, "seek": 22724, "start": 240.72, "end": 242.96, "text": " that have kind of failed every time?", "tokens": [51038, 300, 362, 733, 295, 7612, 633, 565, 30, 51150], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 66, "seek": 22724, "start": 242.96, "end": 247.36, "text": " That doesn't seem to be a kind of a very open and clear structure to the kind of scientific", "tokens": [51150, 663, 1177, 380, 1643, 281, 312, 257, 733, 295, 257, 588, 1269, 293, 1850, 3877, 281, 264, 733, 295, 8134, 51370], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 67, "seek": 22724, "start": 247.36, "end": 252.20000000000002, "text": " reasoning behind selecting, you know, one model or another, but again, I might be, I'm", "tokens": [51370, 21577, 2261, 18182, 11, 291, 458, 11, 472, 2316, 420, 1071, 11, 457, 797, 11, 286, 1062, 312, 11, 286, 478, 51612], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 68, "seek": 22724, "start": 252.20000000000002, "end": 255.12, "text": " open to be corrected about that.", "tokens": [51612, 1269, 281, 312, 31687, 466, 300, 13, 51758], "temperature": 0.0, "avg_logprob": -0.16684198031460282, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.006032865960150957}, {"id": 69, "seek": 25512, "start": 255.12, "end": 260.52, "text": " So even basic language models do pretty well on basic word prediction.", "tokens": [50364, 407, 754, 3875, 2856, 5245, 360, 1238, 731, 322, 3875, 1349, 17630, 13, 50634], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 70, "seek": 25512, "start": 260.52, "end": 264.16, "text": " So the issue is whether these tools provide any insights into traditional psycholinguistic", "tokens": [50634, 407, 264, 2734, 307, 1968, 613, 3873, 2893, 604, 14310, 666, 5164, 4681, 401, 7050, 3142, 50816], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 71, "seek": 25512, "start": 264.16, "end": 266.4, "text": " notions like grammar and parsing.", "tokens": [50816, 35799, 411, 22317, 293, 21156, 278, 13, 50928], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 72, "seek": 25512, "start": 266.4, "end": 270.68, "text": " So this is really why I kind of prefer the term corpus model rather language model, suggested", "tokens": [50928, 407, 341, 307, 534, 983, 286, 733, 295, 4382, 264, 1433, 1181, 31624, 2316, 2831, 2856, 2316, 11, 10945, 51142], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 73, "seek": 25512, "start": 270.68, "end": 273.24, "text": " by people like Sabra Varys.", "tokens": [51142, 538, 561, 411, 13915, 424, 14662, 749, 13, 51270], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 74, "seek": 25512, "start": 273.24, "end": 278.04, "text": " So as we pointed out that no one really thinks LLMs tell us anything profound about Python", "tokens": [51270, 407, 382, 321, 10932, 484, 300, 572, 472, 534, 7309, 441, 43, 26386, 980, 505, 1340, 14382, 466, 15329, 51510], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 75, "seek": 25512, "start": 278.04, "end": 282.04, "text": " when they learn Python code just as well as natural language, but Python is a symbolic", "tokens": [51510, 562, 436, 1466, 15329, 3089, 445, 382, 731, 382, 3303, 2856, 11, 457, 15329, 307, 257, 25755, 51710], "temperature": 0.0, "avg_logprob": -0.22784654368524965, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.015583505854010582}, {"id": 76, "seek": 28204, "start": 282.04, "end": 286.84000000000003, "text": " language with a phrase structure grammar and nobody says LLMs are unveiling the secrets", "tokens": [50364, 2856, 365, 257, 9535, 3877, 22317, 293, 5079, 1619, 441, 43, 26386, 366, 31009, 4883, 264, 14093, 50604], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 77, "seek": 28204, "start": 286.84000000000003, "end": 287.84000000000003, "text": " of Python.", "tokens": [50604, 295, 15329, 13, 50654], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 78, "seek": 28204, "start": 287.84000000000003, "end": 288.84000000000003, "text": " Right.", "tokens": [50654, 1779, 13, 50704], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 79, "seek": 28204, "start": 288.84000000000003, "end": 292.08000000000004, "text": " So just to quote Varys here, he says, if A and N models can be construed as explanatory", "tokens": [50704, 407, 445, 281, 6513, 14662, 749, 510, 11, 415, 1619, 11, 498, 316, 293, 426, 5245, 393, 312, 12946, 292, 382, 9045, 4745, 50866], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 80, "seek": 28204, "start": 292.08000000000004, "end": 296.24, "text": " theories for natural language based on their successes on language tasks, then in the absence", "tokens": [50866, 13667, 337, 3303, 2856, 2361, 322, 641, 26101, 322, 2856, 9608, 11, 550, 294, 264, 17145, 51074], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 81, "seek": 28204, "start": 296.24, "end": 299.52000000000004, "text": " of counter arguments, they should be good explanatory theories for computer language", "tokens": [51074, 295, 5682, 12869, 11, 436, 820, 312, 665, 9045, 4745, 13667, 337, 3820, 2856, 51238], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 82, "seek": 28204, "start": 299.52000000000004, "end": 300.52000000000004, "text": " as well.", "tokens": [51238, 382, 731, 13, 51288], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 83, "seek": 28204, "start": 300.52000000000004, "end": 305.40000000000003, "text": " Therefore, successful A and N models of natural language cannot be used as evidence against", "tokens": [51288, 7504, 11, 4406, 316, 293, 426, 5245, 295, 3303, 2856, 2644, 312, 1143, 382, 4467, 1970, 51532], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 84, "seek": 28204, "start": 305.40000000000003, "end": 308.12, "text": " generative phrase structure grammars in language.", "tokens": [51532, 1337, 1166, 9535, 3877, 17570, 685, 294, 2856, 13, 51668], "temperature": 0.0, "avg_logprob": -0.16018465863979928, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0114238066598773}, {"id": 85, "seek": 30812, "start": 308.12, "end": 312.16, "text": " So corpus model is really a more appropriate term for other reasons too.", "tokens": [50364, 407, 1181, 31624, 2316, 307, 534, 257, 544, 6854, 1433, 337, 661, 4112, 886, 13, 50566], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 86, "seek": 30812, "start": 312.16, "end": 315.88, "text": " People like Emily Bender and some others have shown that features of the training corpus,", "tokens": [50566, 3432, 411, 15034, 363, 3216, 293, 512, 2357, 362, 4898, 300, 4122, 295, 264, 3097, 1181, 31624, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 87, "seek": 30812, "start": 315.88, "end": 320.2, "text": " in fact, I think Steven cites this, you cite this in your paper actually as a limitation.", "tokens": [50752, 294, 1186, 11, 286, 519, 12754, 269, 3324, 341, 11, 291, 37771, 341, 294, 428, 3035, 767, 382, 257, 27432, 13, 50968], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 88, "seek": 30812, "start": 320.2, "end": 324.16, "text": " They show that features of the training corpus can heavily influence the learning process.", "tokens": [50968, 814, 855, 300, 4122, 295, 264, 3097, 1181, 31624, 393, 10950, 6503, 264, 2539, 1399, 13, 51166], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 89, "seek": 30812, "start": 324.16, "end": 327.4, "text": " So it's been shown that the performance of large language models on language tasks is", "tokens": [51166, 407, 309, 311, 668, 4898, 300, 264, 3389, 295, 2416, 2856, 5245, 322, 2856, 9608, 307, 51328], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 90, "seek": 30812, "start": 327.4, "end": 331.8, "text": " really heavily influenced by the diversity of the training corpus.", "tokens": [51328, 534, 10950, 15269, 538, 264, 8811, 295, 264, 3097, 1181, 31624, 13, 51548], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 91, "seek": 30812, "start": 331.8, "end": 334.24, "text": " But natural language itself is not biased, right?", "tokens": [51548, 583, 3303, 2856, 2564, 307, 406, 28035, 11, 558, 30, 51670], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 92, "seek": 30812, "start": 334.24, "end": 336.8, "text": " It's just the computational system.", "tokens": [51670, 467, 311, 445, 264, 28270, 1185, 13, 51798], "temperature": 0.0, "avg_logprob": -0.15938503580882138, "compression_ratio": 1.8476190476190477, "no_speech_prob": 0.02299538441002369}, {"id": 93, "seek": 33680, "start": 336.8, "end": 340.16, "text": " Some beings can be biased in what they say and how they act.", "tokens": [50364, 2188, 8958, 393, 312, 28035, 294, 437, 436, 584, 293, 577, 436, 605, 13, 50532], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 94, "seek": 33680, "start": 340.16, "end": 342.52000000000004, "text": " But natural language itself isn't biased, right?", "tokens": [50532, 583, 3303, 2856, 2564, 1943, 380, 28035, 11, 558, 30, 50650], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 95, "seek": 33680, "start": 342.52000000000004, "end": 347.88, "text": " So large language models, therefore, it seems difficult for me to agree that they are being", "tokens": [50650, 407, 2416, 2856, 5245, 11, 4412, 11, 309, 2544, 2252, 337, 385, 281, 3986, 300, 436, 366, 885, 50918], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 96, "seek": 33680, "start": 347.88, "end": 349.40000000000003, "text": " subject to all sorts of biases.", "tokens": [50918, 3983, 281, 439, 7527, 295, 32152, 13, 50994], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 97, "seek": 33680, "start": 349.40000000000003, "end": 352.92, "text": " They therefore can't really be models of language, they're models of something else.", "tokens": [50994, 814, 4412, 393, 380, 534, 312, 5245, 295, 2856, 11, 436, 434, 5245, 295, 746, 1646, 13, 51170], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 98, "seek": 33680, "start": 352.92, "end": 358.84000000000003, "text": " So just to kind of wrap up this argument, even though LLMs are clearly exposed to vastly", "tokens": [51170, 407, 445, 281, 733, 295, 7019, 493, 341, 6770, 11, 754, 1673, 441, 43, 26386, 366, 4448, 9495, 281, 41426, 51466], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 99, "seek": 33680, "start": 358.84000000000003, "end": 362.56, "text": " more linguistic experience in children, again, this is something else that Steven concedes", "tokens": [51466, 544, 43002, 1752, 294, 2227, 11, 797, 11, 341, 307, 746, 1646, 300, 12754, 416, 21526, 51652], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 100, "seek": 33680, "start": 362.56, "end": 364.5, "text": " and talks about in his paper.", "tokens": [51652, 293, 6686, 466, 294, 702, 3035, 13, 51749], "temperature": 0.0, "avg_logprob": -0.16851283997062624, "compression_ratio": 1.7311475409836066, "no_speech_prob": 0.013439333066344261}, {"id": 101, "seek": 36450, "start": 364.5, "end": 369.02, "text": " And so their learning outcomes may still be relevant in addressing what grammatical generalizations", "tokens": [50364, 400, 370, 641, 2539, 10070, 815, 920, 312, 7340, 294, 14329, 437, 17570, 267, 804, 2674, 14455, 50590], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 102, "seek": 36450, "start": 369.02, "end": 370.66, "text": " are learnable in principle.", "tokens": [50590, 366, 1466, 712, 294, 8665, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 103, "seek": 36450, "start": 370.66, "end": 374.22, "text": " So I do agree with this statement here, that in principle they can tell us something about", "tokens": [50672, 407, 286, 360, 3986, 365, 341, 5629, 510, 11, 300, 294, 8665, 436, 393, 980, 505, 746, 466, 50850], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 104, "seek": 36450, "start": 374.22, "end": 378.9, "text": " learnability rather than things like broad acquisitionist frameworks.", "tokens": [50850, 1466, 2310, 2831, 813, 721, 411, 4152, 21668, 468, 29834, 13, 51084], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 105, "seek": 36450, "start": 378.9, "end": 382.5, "text": " But that's about as much I think you can maybe say right now.", "tokens": [51084, 583, 300, 311, 466, 382, 709, 286, 519, 291, 393, 1310, 584, 558, 586, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 106, "seek": 36450, "start": 382.5, "end": 387.14, "text": " Showing that some inductive biases are not necessary for learning is not really the same", "tokens": [51264, 6895, 278, 300, 512, 31612, 488, 32152, 366, 406, 4818, 337, 2539, 307, 406, 534, 264, 912, 51496], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 107, "seek": 36450, "start": 387.14, "end": 389.84, "text": " thing as showing that it isn't present in children.", "tokens": [51496, 551, 382, 4099, 300, 309, 1943, 380, 1974, 294, 2227, 13, 51631], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 108, "seek": 36450, "start": 389.84, "end": 393.62, "text": " So there's been a long debate about whether negative evidence and instruction and correction", "tokens": [51631, 407, 456, 311, 668, 257, 938, 7958, 466, 1968, 3671, 4467, 293, 10951, 293, 19984, 51820], "temperature": 0.0, "avg_logprob": -0.15965288877487183, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0008794603636488318}, {"id": 109, "seek": 39362, "start": 393.74, "end": 399.3, "text": " and feedback during language learning are necessary or even useful for infants and children.", "tokens": [50370, 293, 5824, 1830, 2856, 2539, 366, 4818, 420, 754, 4420, 337, 38829, 293, 2227, 13, 50648], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 110, "seek": 39362, "start": 399.3, "end": 403.34000000000003, "text": " But right now I kind of agree more with Eugene Choi and Gary Marcus and others who have highlighted", "tokens": [50648, 583, 558, 586, 286, 733, 295, 3986, 544, 365, 37059, 33479, 293, 13788, 26574, 293, 2357, 567, 362, 17173, 50850], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 111, "seek": 39362, "start": 403.34000000000003, "end": 406.82, "text": " how LLMs are currently very expensive to train.", "tokens": [50850, 577, 441, 43, 26386, 366, 4362, 588, 5124, 281, 3847, 13, 51024], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 112, "seek": 39362, "start": 406.82, "end": 411.74, "text": " They're clearly an example of concentrated private power in the hands of a few tech companies.", "tokens": [51024, 814, 434, 4448, 364, 1365, 295, 21321, 4551, 1347, 294, 264, 2377, 295, 257, 1326, 7553, 3431, 13, 51270], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 113, "seek": 39362, "start": 411.74, "end": 417.06, "text": " Their environment impact is massive and many of people have been less constrained and conservative", "tokens": [51270, 6710, 2823, 2712, 307, 5994, 293, 867, 295, 561, 362, 668, 1570, 38901, 293, 13780, 51536], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 114, "seek": 39362, "start": 417.06, "end": 421.94, "text": " in their assessment here, which is much less so than Gary Marcus and Eugene.", "tokens": [51536, 294, 641, 9687, 510, 11, 597, 307, 709, 1570, 370, 813, 13788, 26574, 293, 37059, 13, 51780], "temperature": 0.0, "avg_logprob": -0.19901821348402235, "compression_ratio": 1.6699346405228759, "no_speech_prob": 0.004869978409260511}, {"id": 115, "seek": 42194, "start": 421.98, "end": 428.82, "text": " So Bill Gates recently wrote that chatGPT is the biggest tech development since the", "tokens": [50366, 407, 5477, 26494, 3938, 4114, 300, 5081, 38, 47, 51, 307, 264, 3880, 7553, 3250, 1670, 264, 50708], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 116, "seek": 42194, "start": 428.82, "end": 430.82, "text": " graphical user interface, the GUI.", "tokens": [50708, 35942, 4195, 9226, 11, 264, 17917, 40, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 117, "seek": 42194, "start": 430.82, "end": 436.1, "text": " And Henry Kissinger wrote in February in the Wall Street Journal that as chatGPTs capacities", "tokens": [50808, 400, 11085, 24297, 6911, 4114, 294, 8711, 294, 264, 9551, 7638, 16936, 300, 382, 5081, 38, 47, 33424, 39396, 51072], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 118, "seek": 42194, "start": 436.1, "end": 441.62, "text": " become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality", "tokens": [51072, 1813, 13227, 11, 436, 486, 38818, 533, 1952, 3601, 11, 21341, 2962, 294, 264, 7253, 295, 527, 4103, 51348], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 119, "seek": 42194, "start": 441.62, "end": 444.18, "text": " and reorganize politics and society.", "tokens": [51348, 293, 41203, 1125, 7341, 293, 4086, 13, 51476], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 120, "seek": 42194, "start": 444.18, "end": 449.18, "text": " Generative AI is poised to generate new forms of human consciousness, so very radical claims", "tokens": [51476, 15409, 1166, 7318, 307, 714, 2640, 281, 8460, 777, 6422, 295, 1952, 10081, 11, 370, 588, 12001, 9441, 51726], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 121, "seek": 42194, "start": 449.18, "end": 450.18, "text": " happening at the moment.", "tokens": [51726, 2737, 412, 264, 1623, 13, 51776], "temperature": 0.0, "avg_logprob": -0.2299994186118797, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.004598577506840229}, {"id": 122, "seek": 45018, "start": 450.18, "end": 456.1, "text": " I do wonder if sometimes all of the AI hype may have, you know, see it into certain portions", "tokens": [50364, 286, 360, 2441, 498, 2171, 439, 295, 264, 7318, 24144, 815, 362, 11, 291, 458, 11, 536, 309, 666, 1629, 25070, 50660], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 123, "seek": 45018, "start": 456.1, "end": 459.94, "text": " of academia potentially, a lot of ground claims being made.", "tokens": [50660, 295, 28937, 7263, 11, 257, 688, 295, 2727, 9441, 885, 1027, 13, 50852], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 124, "seek": 45018, "start": 459.94, "end": 463.22, "text": " But I think, you know, more concretely, just to put it back to Stephen here, I wanted to", "tokens": [50852, 583, 286, 519, 11, 291, 458, 11, 544, 39481, 736, 11, 445, 281, 829, 309, 646, 281, 13391, 510, 11, 286, 1415, 281, 51016], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 125, "seek": 45018, "start": 463.22, "end": 468.82, "text": " maybe raise the issue of there's a critique by Roscoe and Beaumont that I think he's read", "tokens": [51016, 1310, 5300, 264, 2734, 295, 456, 311, 257, 25673, 538, 11144, 1291, 68, 293, 45786, 449, 896, 300, 286, 519, 415, 311, 1401, 51296], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 126, "seek": 45018, "start": 468.82, "end": 471.5, "text": " on Lingbos.", "tokens": [51296, 322, 20977, 65, 329, 13, 51430], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 127, "seek": 45018, "start": 471.5, "end": 476.14, "text": " I think you saw on Twitter that you don't like the response they gave because the objection", "tokens": [51430, 286, 519, 291, 1866, 322, 5794, 300, 291, 500, 380, 411, 264, 4134, 436, 2729, 570, 264, 35756, 51662], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 128, "seek": 45018, "start": 476.14, "end": 479.62, "text": " that they made is that, you know, science is an example of deductible logic.", "tokens": [51662, 300, 436, 1027, 307, 300, 11, 291, 458, 11, 3497, 307, 364, 1365, 295, 31513, 964, 9952, 13, 51836], "temperature": 0.0, "avg_logprob": -0.25035395341760974, "compression_ratio": 1.678688524590164, "no_speech_prob": 0.005713257938623428}, {"id": 129, "seek": 47962, "start": 479.62, "end": 483.34000000000003, "text": " Your objection is that science isn't deductive, it's inductive, right?", "tokens": [50364, 2260, 35756, 307, 300, 3497, 1943, 380, 31513, 488, 11, 309, 311, 31612, 488, 11, 558, 30, 50550], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 130, "seek": 47962, "start": 483.34000000000003, "end": 488.38, "text": " But I think their general point might be more accurate, namely that you can't use the fact", "tokens": [50550, 583, 286, 519, 641, 2674, 935, 1062, 312, 544, 8559, 11, 20926, 300, 291, 393, 380, 764, 264, 1186, 50802], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 131, "seek": 47962, "start": 488.38, "end": 493.62, "text": " that language models do well predicting some linguistic behavior in humans and some neuroimaging", "tokens": [50802, 300, 2856, 5245, 360, 731, 32884, 512, 43002, 5223, 294, 6255, 293, 512, 16499, 332, 3568, 51064], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 132, "seek": 47962, "start": 493.62, "end": 494.86, "text": " responses.", "tokens": [51064, 13019, 13, 51126], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 133, "seek": 47962, "start": 494.86, "end": 499.58, "text": " You can't use that alone to claim that they can yield a theory of human language.", "tokens": [51126, 509, 393, 380, 764, 300, 3312, 281, 3932, 300, 436, 393, 11257, 257, 5261, 295, 1952, 2856, 13, 51362], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 134, "seek": 47962, "start": 499.58, "end": 503.34000000000003, "text": " So in your paper, Stephen, you know that it seems that certain structures work better", "tokens": [51362, 407, 294, 428, 3035, 11, 13391, 11, 291, 458, 300, 309, 2544, 300, 1629, 9227, 589, 1101, 51550], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 135, "seek": 47962, "start": 503.34000000000003, "end": 504.34000000000003, "text": " than others.", "tokens": [51550, 813, 2357, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 136, "seek": 47962, "start": 504.34000000000003, "end": 508.46, "text": " The right attentional mechanism is important, prediction is important, semantic representations", "tokens": [51600, 440, 558, 3202, 304, 7513, 307, 1021, 11, 17630, 307, 1021, 11, 47982, 33358, 51806], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 137, "seek": 47962, "start": 508.46, "end": 509.46, "text": " are important.", "tokens": [51806, 366, 1021, 13, 51856], "temperature": 0.0, "avg_logprob": -0.13545014137445494, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.003199902595952153}, {"id": 138, "seek": 50946, "start": 509.5, "end": 512.62, "text": " And therefore, we can glean currently based on these models, right?", "tokens": [50366, 400, 4412, 11, 321, 393, 290, 28499, 4362, 2361, 322, 613, 5245, 11, 558, 30, 50522], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 139, "seek": 50946, "start": 512.62, "end": 515.9, "text": " But so far, that's really all I've been able to glean in the literature.", "tokens": [50522, 583, 370, 1400, 11, 300, 311, 534, 439, 286, 600, 668, 1075, 281, 290, 28499, 294, 264, 10394, 13, 50686], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 140, "seek": 50946, "start": 515.9, "end": 517.9399999999999, "text": " I'm not sure if you have more insights here.", "tokens": [50686, 286, 478, 406, 988, 498, 291, 362, 544, 14310, 510, 13, 50788], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 141, "seek": 50946, "start": 517.9399999999999, "end": 523.6999999999999, "text": " So Roscoe and Beaumont use the example of poor prediction, but strong explanation, right?", "tokens": [50788, 407, 11144, 1291, 68, 293, 45786, 449, 896, 764, 264, 1365, 295, 4716, 17630, 11, 457, 2068, 10835, 11, 558, 30, 51076], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 142, "seek": 50946, "start": 523.6999999999999, "end": 527.14, "text": " Explanatory power and not predictive accuracy forms the basis of modern science.", "tokens": [51076, 12514, 282, 4745, 1347, 293, 406, 35521, 14170, 6422, 264, 5143, 295, 4363, 3497, 13, 51248], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 143, "seek": 50946, "start": 527.14, "end": 530.78, "text": " I don't want to explore this a little bit later, maybe, but modern language models can", "tokens": [51248, 286, 500, 380, 528, 281, 6839, 341, 257, 707, 857, 1780, 11, 1310, 11, 457, 4363, 2856, 5245, 393, 51430], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 144, "seek": 50946, "start": 530.78, "end": 534.54, "text": " accurately model parts of human language, but they can also perform very well on impossible", "tokens": [51430, 20095, 2316, 3166, 295, 1952, 2856, 11, 457, 436, 393, 611, 2042, 588, 731, 322, 6243, 51618], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 145, "seek": 50946, "start": 534.54, "end": 539.42, "text": " languages and unnatural structures that humans can't learn and have great difficulty", "tokens": [51618, 8650, 293, 43470, 9227, 300, 6255, 393, 380, 1466, 293, 362, 869, 10360, 51862], "temperature": 0.0, "avg_logprob": -0.195669952703982, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.011442434042692184}, {"id": 146, "seek": 53942, "start": 539.5, "end": 540.14, "text": " processing.", "tokens": [50368, 9007, 13, 50400], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 147, "seek": 53942, "start": 540.14, "end": 542.2199999999999, "text": " And I know you're familiar with these with these criticisms, right?", "tokens": [50400, 400, 286, 458, 291, 434, 4963, 365, 613, 365, 613, 48519, 11, 558, 30, 50504], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 148, "seek": 53942, "start": 543.18, "end": 544.9399999999999, "text": " But you're definitely not alone here at the same time.", "tokens": [50552, 583, 291, 434, 2138, 406, 3312, 510, 412, 264, 912, 565, 13, 50640], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 149, "seek": 53942, "start": 544.9399999999999, "end": 552.2199999999999, "text": " So Ilya Tskeva, the chief scientist at OpenAI, he said in an interview recently, what does", "tokens": [50640, 407, 286, 45106, 16518, 330, 2757, 11, 264, 9588, 12662, 412, 7238, 48698, 11, 415, 848, 294, 364, 4049, 3938, 11, 437, 775, 51004], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 150, "seek": 53942, "start": 552.2199999999999, "end": 554.06, "text": " it mean to predict the next token well enough?", "tokens": [51004, 309, 914, 281, 6069, 264, 958, 14862, 731, 1547, 30, 51096], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 151, "seek": 53942, "start": 554.4599999999999, "end": 559.02, "text": " It means that you understand the underlying reality that led to the creation of that token,", "tokens": [51116, 467, 1355, 300, 291, 1223, 264, 14217, 4103, 300, 4684, 281, 264, 8016, 295, 300, 14862, 11, 51344], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 152, "seek": 53942, "start": 560.06, "end": 563.42, "text": " which is quite divergent from a lot of more conservative claims in the literature here.", "tokens": [51396, 597, 307, 1596, 18558, 6930, 490, 257, 688, 295, 544, 13780, 9441, 294, 264, 10394, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 153, "seek": 53942, "start": 564.62, "end": 568.9399999999999, "text": " And also, you know, I would just say in response to that, that different components of science", "tokens": [51624, 400, 611, 11, 291, 458, 11, 286, 576, 445, 584, 294, 4134, 281, 300, 11, 300, 819, 6677, 295, 3497, 51840], "temperature": 0.0, "avg_logprob": -0.17142796337156369, "compression_ratio": 1.672782874617737, "no_speech_prob": 0.006481391843408346}, {"id": 154, "seek": 56894, "start": 569.0200000000001, "end": 572.22, "text": " can be either inductive or deductive, right?", "tokens": [50368, 393, 312, 2139, 31612, 488, 420, 31513, 488, 11, 558, 30, 50528], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 155, "seek": 56894, "start": 572.22, "end": 573.34, "text": " It's not really an either-or.", "tokens": [50528, 467, 311, 406, 534, 364, 2139, 12, 284, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 156, "seek": 56894, "start": 573.34, "end": 574.7, "text": " You have an existing theory.", "tokens": [50584, 509, 362, 364, 6741, 5261, 13, 50652], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 157, "seek": 56894, "start": 574.7, "end": 576.3000000000001, "text": " You formulate a hypothesis.", "tokens": [50652, 509, 47881, 257, 17291, 13, 50732], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 158, "seek": 56894, "start": 576.3000000000001, "end": 580.46, "text": " You collect data, you analyze it, and that's kind of a deductive process.", "tokens": [50732, 509, 2500, 1412, 11, 291, 12477, 309, 11, 293, 300, 311, 733, 295, 257, 31513, 488, 1399, 13, 50940], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 159, "seek": 56894, "start": 580.46, "end": 583.34, "text": " But there's also cases where you start with a specific observation.", "tokens": [50940, 583, 456, 311, 611, 3331, 689, 291, 722, 365, 257, 2685, 14816, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 160, "seek": 56894, "start": 583.34, "end": 586.46, "text": " You find some patterns and you induce general conclusions, right?", "tokens": [51084, 509, 915, 512, 8294, 293, 291, 41263, 2674, 22865, 11, 558, 30, 51240], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 161, "seek": 56894, "start": 586.46, "end": 593.0200000000001, "text": " And then there's abduction, where you magically invent hypotheses and reduce the hypothesis space.", "tokens": [51240, 400, 550, 456, 311, 410, 40335, 11, 689, 291, 39763, 7962, 49969, 293, 5407, 264, 17291, 1901, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 162, "seek": 56894, "start": 593.0200000000001, "end": 598.22, "text": " You wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific", "tokens": [51568, 509, 2759, 380, 534, 584, 300, 31513, 488, 21577, 307, 2693, 5412, 1089, 420, 31612, 488, 21577, 307, 2693, 5412, 1089, 51828], "temperature": 0.0, "avg_logprob": -0.10766878343166265, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.005432056728750467}, {"id": 163, "seek": 59822, "start": 598.22, "end": 600.5400000000001, "text": " or abductive reasoning is unscientific, right?", "tokens": [50364, 420, 46465, 488, 21577, 307, 2693, 5412, 1089, 11, 558, 30, 50480], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 164, "seek": 59822, "start": 600.5400000000001, "end": 602.86, "text": " These are all just different ways of doing stuff.", "tokens": [50480, 1981, 366, 439, 445, 819, 2098, 295, 884, 1507, 13, 50596], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 165, "seek": 59822, "start": 602.86, "end": 609.26, "text": " I mean, in your paper, you give the examples of using models to predict hurricanes and pandemics", "tokens": [50596, 286, 914, 11, 294, 428, 3035, 11, 291, 976, 264, 5110, 295, 1228, 5245, 281, 6069, 48026, 293, 4565, 38014, 50916], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 166, "seek": 59822, "start": 609.26, "end": 612.86, "text": " as being examples of stuff that is as rigorous as science gets.", "tokens": [50916, 382, 885, 5110, 295, 1507, 300, 307, 382, 29882, 382, 3497, 2170, 13, 51096], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 167, "seek": 59822, "start": 612.86, "end": 617.1800000000001, "text": " And then you employ a reader to conclude that the situation is no different for language models.", "tokens": [51096, 400, 550, 291, 3188, 257, 15149, 281, 16886, 300, 264, 2590, 307, 572, 819, 337, 2856, 5245, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 168, "seek": 59822, "start": 618.22, "end": 622.38, "text": " But I guess for me, the issue is that models predicting hurricanes are not in the business", "tokens": [51364, 583, 286, 2041, 337, 385, 11, 264, 2734, 307, 300, 5245, 32884, 48026, 366, 406, 294, 264, 1606, 51572], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 169, "seek": 59822, "start": 622.38, "end": 625.98, "text": " of answering the question, what is the hurricane, right?", "tokens": [51572, 295, 13430, 264, 1168, 11, 437, 307, 264, 27136, 11, 558, 30, 51752], "temperature": 0.0, "avg_logprob": -0.10621783692957991, "compression_ratio": 1.773851590106007, "no_speech_prob": 0.0010525267571210861}, {"id": 170, "seek": 62598, "start": 626.0600000000001, "end": 629.4200000000001, "text": " Models accurately predicting the weather are very accurate, but they're not.", "tokens": [50368, 6583, 1625, 20095, 32884, 264, 5503, 366, 588, 8559, 11, 457, 436, 434, 406, 13, 50536], "temperature": 0.0, "avg_logprob": -0.16125659509138626, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.0006065811612643301}, {"id": 171, "seek": 62598, "start": 629.4200000000001, "end": 633.4200000000001, "text": " They're aligned with the meteorology department, but they're not a substitute for it.", "tokens": [50536, 814, 434, 17962, 365, 264, 25313, 1793, 5882, 11, 457, 436, 434, 406, 257, 15802, 337, 309, 13, 50736], "temperature": 0.0, "avg_logprob": -0.16125659509138626, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.0006065811612643301}, {"id": 172, "seek": 62598, "start": 634.38, "end": 636.14, "text": " So I guess I'll just hand it over to you.", "tokens": [50784, 407, 286, 2041, 286, 603, 445, 1011, 309, 670, 281, 291, 13, 50872], "temperature": 0.0, "avg_logprob": -0.16125659509138626, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.0006065811612643301}, {"id": 173, "seek": 62598, "start": 637.82, "end": 640.22, "text": " Yeah. Okay. Well, there's a lot there.", "tokens": [50956, 865, 13, 1033, 13, 1042, 11, 456, 311, 257, 688, 456, 13, 51076], "temperature": 0.0, "avg_logprob": -0.16125659509138626, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.0006065811612643301}, {"id": 174, "seek": 62598, "start": 641.66, "end": 648.86, "text": " I guess I could start just by saying that I agree with many of these criticisms, right,", "tokens": [51148, 286, 2041, 286, 727, 722, 445, 538, 1566, 300, 286, 3986, 365, 867, 295, 613, 48519, 11, 558, 11, 51508], "temperature": 0.0, "avg_logprob": -0.16125659509138626, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.0006065811612643301}, {"id": 175, "seek": 64886, "start": 648.94, "end": 657.9, "text": " about these models being controlled by one or two companies that being very, very problematic.", "tokens": [50368, 466, 613, 5245, 885, 10164, 538, 472, 420, 732, 3431, 300, 885, 588, 11, 588, 19011, 13, 50816], "temperature": 0.0, "avg_logprob": -0.10623079049782674, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.07691745460033417}, {"id": 176, "seek": 64886, "start": 660.3000000000001, "end": 664.78, "text": " They have all kinds of biases that they've acquired because they're trained on text from the", "tokens": [50936, 814, 362, 439, 3685, 295, 32152, 300, 436, 600, 17554, 570, 436, 434, 8895, 322, 2487, 490, 264, 51160], "temperature": 0.0, "avg_logprob": -0.10623079049782674, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.07691745460033417}, {"id": 177, "seek": 64886, "start": 664.78, "end": 672.94, "text": " internet. That's hugely problematic. I certainly agree that there's things at least at present", "tokens": [51160, 4705, 13, 663, 311, 27417, 19011, 13, 286, 3297, 3986, 300, 456, 311, 721, 412, 1935, 412, 1974, 51568], "temperature": 0.0, "avg_logprob": -0.10623079049782674, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.07691745460033417}, {"id": 178, "seek": 67294, "start": 672.94, "end": 680.1400000000001, "text": " that the models don't do well, right? So I think it's easy to find examples of questions", "tokens": [50364, 300, 264, 5245, 500, 380, 360, 731, 11, 558, 30, 407, 286, 519, 309, 311, 1858, 281, 915, 5110, 295, 1651, 50724], "temperature": 0.0, "avg_logprob": -0.09315409379846909, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0757828950881958}, {"id": 179, "seek": 67294, "start": 680.1400000000001, "end": 685.9000000000001, "text": " and problems that will trip them up. I think why I've been excited about them, though,", "tokens": [50724, 293, 2740, 300, 486, 4931, 552, 493, 13, 286, 519, 983, 286, 600, 668, 2919, 466, 552, 11, 1673, 11, 51012], "temperature": 0.0, "avg_logprob": -0.09315409379846909, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0757828950881958}, {"id": 180, "seek": 67294, "start": 686.94, "end": 694.0600000000001, "text": " is not necessarily in those terms, right, but in terms of performance on language,", "tokens": [51064, 307, 406, 4725, 294, 729, 2115, 11, 558, 11, 457, 294, 2115, 295, 3389, 322, 2856, 11, 51420], "temperature": 0.0, "avg_logprob": -0.09315409379846909, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0757828950881958}, {"id": 181, "seek": 67294, "start": 694.7800000000001, "end": 702.1400000000001, "text": " specifically syntax and semantics. I think they're far beyond kind of any other theory", "tokens": [51456, 4682, 28431, 293, 4361, 45298, 13, 286, 519, 436, 434, 1400, 4399, 733, 295, 604, 661, 5261, 51824], "temperature": 0.0, "avg_logprob": -0.09315409379846909, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0757828950881958}, {"id": 182, "seek": 70214, "start": 702.14, "end": 709.9, "text": " in any other domain, right? So there's no other theory out of linguistics or computer science", "tokens": [50364, 294, 604, 661, 9274, 11, 558, 30, 407, 456, 311, 572, 661, 5261, 484, 295, 21766, 6006, 420, 3820, 3497, 50752], "temperature": 0.0, "avg_logprob": -0.08642462889353435, "compression_ratio": 1.4628571428571429, "no_speech_prob": 0.0004653089854400605}, {"id": 183, "seek": 70214, "start": 710.62, "end": 717.34, "text": " which can generate long, coherent grammatical passages of text.", "tokens": [50788, 597, 393, 8460, 938, 11, 36239, 17570, 267, 804, 31589, 295, 2487, 13, 51124], "temperature": 0.0, "avg_logprob": -0.08642462889353435, "compression_ratio": 1.4628571428571429, "no_speech_prob": 0.0004653089854400605}, {"id": 184, "seek": 70214, "start": 718.46, "end": 727.26, "text": " And so kind of admitting all of their problems as tools or things which are deployed by companies,", "tokens": [51180, 400, 370, 733, 295, 44056, 439, 295, 641, 2740, 382, 3873, 420, 721, 597, 366, 17826, 538, 3431, 11, 51620], "temperature": 0.0, "avg_logprob": -0.08642462889353435, "compression_ratio": 1.4628571428571429, "no_speech_prob": 0.0004653089854400605}, {"id": 185, "seek": 72726, "start": 728.22, "end": 735.1, "text": " there's still this question of, like, how are they at dealing with language? And I think this", "tokens": [50412, 456, 311, 920, 341, 1168, 295, 11, 411, 11, 577, 366, 436, 412, 6260, 365, 2856, 30, 400, 286, 519, 341, 50756], "temperature": 0.0, "avg_logprob": -0.11320212612981381, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0035915502812713385}, {"id": 186, "seek": 72726, "start": 735.1, "end": 740.14, "text": " is where a lot of the enthusiasm comes from, is there really hasn't been anything even remotely", "tokens": [50756, 307, 689, 257, 688, 295, 264, 23417, 1487, 490, 11, 307, 456, 534, 6132, 380, 668, 1340, 754, 20824, 51008], "temperature": 0.0, "avg_logprob": -0.11320212612981381, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0035915502812713385}, {"id": 187, "seek": 72726, "start": 740.14, "end": 746.22, "text": " like them in terms of linguistic ability. And that's the thing that I think is exciting. So,", "tokens": [51008, 411, 552, 294, 2115, 295, 43002, 3485, 13, 400, 300, 311, 264, 551, 300, 286, 519, 307, 4670, 13, 407, 11, 51312], "temperature": 0.0, "avg_logprob": -0.11320212612981381, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0035915502812713385}, {"id": 188, "seek": 72726, "start": 746.22, "end": 753.1, "text": " yes, I agree with a bunch of these things you started with, but nonetheless, like I think in", "tokens": [51312, 2086, 11, 286, 3986, 365, 257, 3840, 295, 613, 721, 291, 1409, 365, 11, 457, 26756, 11, 411, 286, 519, 294, 51656], "temperature": 0.0, "avg_logprob": -0.11320212612981381, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0035915502812713385}, {"id": 189, "seek": 75310, "start": 753.1, "end": 757.98, "text": " terms of syntax and semantics, there's just no other theory which is comparable to them.", "tokens": [50364, 2115, 295, 28431, 293, 4361, 45298, 11, 456, 311, 445, 572, 661, 5261, 597, 307, 25323, 281, 552, 13, 50608], "temperature": 0.0, "avg_logprob": -0.15039004860343513, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0019758748821914196}, {"id": 190, "seek": 75310, "start": 760.22, "end": 766.46, "text": " But so let me push that back then, right? So the main objection from a lot of people I've spoken", "tokens": [50720, 583, 370, 718, 385, 2944, 300, 646, 550, 11, 558, 30, 407, 264, 2135, 35756, 490, 257, 688, 295, 561, 286, 600, 10759, 51032], "temperature": 0.0, "avg_logprob": -0.15039004860343513, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0019758748821914196}, {"id": 191, "seek": 75310, "start": 766.46, "end": 771.74, "text": " to in the departments of linguistics who are like a lot of the general first of your paper", "tokens": [51032, 281, 294, 264, 15326, 295, 21766, 6006, 567, 366, 411, 257, 688, 295, 264, 2674, 700, 295, 428, 3035, 51296], "temperature": 0.0, "avg_logprob": -0.15039004860343513, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0019758748821914196}, {"id": 192, "seek": 75310, "start": 772.46, "end": 777.9, "text": " is to really say, well, you're right, they do a wonderful job accurately modeling all aspects", "tokens": [51332, 307, 281, 534, 584, 11, 731, 11, 291, 434, 558, 11, 436, 360, 257, 3715, 1691, 20095, 15983, 439, 7270, 51604], "temperature": 0.0, "avg_logprob": -0.15039004860343513, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0019758748821914196}, {"id": 193, "seek": 77790, "start": 777.98, "end": 783.26, "text": " of a lot of aspects of syntax and semantics. However, I don't know if any real just like,", "tokens": [50368, 295, 257, 688, 295, 7270, 295, 28431, 293, 4361, 45298, 13, 2908, 11, 286, 500, 380, 458, 498, 604, 957, 445, 411, 11, 50632], "temperature": 0.0, "avg_logprob": -0.1268355824925878, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.03702905401587486}, {"id": 194, "seek": 77790, "start": 783.26, "end": 786.86, "text": " you know, Chomsky talks about facts about language, which is an old fashioned notion.", "tokens": [50632, 291, 458, 11, 761, 4785, 4133, 6686, 466, 9130, 466, 2856, 11, 597, 307, 364, 1331, 40646, 10710, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1268355824925878, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.03702905401587486}, {"id": 195, "seek": 77790, "start": 787.5799999999999, "end": 790.78, "text": " But I really think that's kind of an important notion too, right? Like,", "tokens": [50848, 583, 286, 534, 519, 300, 311, 733, 295, 364, 1021, 10710, 886, 11, 558, 30, 1743, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1268355824925878, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.03702905401587486}, {"id": 196, "seek": 77790, "start": 790.78, "end": 798.62, "text": " is there some discovery about language itself that LLMs can uniquely provide? So like, if LLMs", "tokens": [51008, 307, 456, 512, 12114, 466, 2856, 2564, 300, 441, 43, 26386, 393, 31474, 2893, 30, 407, 411, 11, 498, 441, 43, 26386, 51400], "temperature": 0.0, "avg_logprob": -0.1268355824925878, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.03702905401587486}, {"id": 197, "seek": 77790, "start": 798.62, "end": 804.78, "text": " made some prediction about, let's say you have a sentence structure type X being more difficult", "tokens": [51400, 1027, 512, 17630, 466, 11, 718, 311, 584, 291, 362, 257, 8174, 3877, 2010, 1783, 885, 544, 2252, 51708], "temperature": 0.0, "avg_logprob": -0.1268355824925878, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.03702905401587486}, {"id": 198, "seek": 80478, "start": 804.86, "end": 809.1, "text": " to process than sentence type Y. And this is a unique prediction that only they'd", "tokens": [50368, 281, 1399, 813, 8174, 2010, 398, 13, 400, 341, 307, 257, 3845, 17630, 300, 787, 436, 1116, 50580], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 199, "seek": 80478, "start": 809.66, "end": 814.06, "text": " generate it. And no human linguist, Chomsky, Honesty, and Adger, none of these people had", "tokens": [50608, 8460, 309, 13, 400, 572, 1952, 21766, 468, 11, 761, 4785, 4133, 11, 6625, 7819, 11, 293, 1999, 1321, 11, 6022, 295, 613, 561, 632, 50828], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 200, "seek": 80478, "start": 814.06, "end": 817.9, "text": " ever predicted that before. But it turns out to be true. You do eye tracking experiments,", "tokens": [50828, 1562, 19147, 300, 949, 13, 583, 309, 4523, 484, 281, 312, 2074, 13, 509, 360, 3313, 11603, 12050, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 201, "seek": 80478, "start": 817.9, "end": 821.02, "text": " you do all sorts of different behavioral experiments. And it turns out, oh, you know,", "tokens": [51020, 291, 360, 439, 7527, 295, 819, 19124, 12050, 13, 400, 309, 4523, 484, 11, 1954, 11, 291, 458, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 202, "seek": 80478, "start": 821.02, "end": 825.02, "text": " after all, it turns out to be true. This is the new insight about language processing,", "tokens": [51176, 934, 439, 11, 309, 4523, 484, 281, 312, 2074, 13, 639, 307, 264, 777, 11269, 466, 2856, 9007, 11, 51376], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 203, "seek": 80478, "start": 825.02, "end": 829.5799999999999, "text": " it's a new insight about language, you know, behavior. I just wonder, I'm not saying that", "tokens": [51376, 309, 311, 257, 777, 11269, 466, 2856, 11, 291, 458, 11, 5223, 13, 286, 445, 2441, 11, 286, 478, 406, 1566, 300, 51604], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 204, "seek": 80478, "start": 829.5799999999999, "end": 833.66, "text": " this is not possible in principle, because it might happen in the near future. But that's,", "tokens": [51604, 341, 307, 406, 1944, 294, 8665, 11, 570, 309, 1062, 1051, 294, 264, 2651, 2027, 13, 583, 300, 311, 11, 51808], "temperature": 0.0, "avg_logprob": -0.1510277644843812, "compression_ratio": 1.8580060422960725, "no_speech_prob": 0.01977079175412655}, {"id": 205, "seek": 83366, "start": 833.66, "end": 838.14, "text": " I guess, for me, the crux of why a lot of linguists speaking up, speaking on behalf of", "tokens": [50364, 286, 2041, 11, 337, 385, 11, 264, 5140, 87, 295, 983, 257, 688, 295, 21766, 1751, 4124, 493, 11, 4124, 322, 9490, 295, 50588], "temperature": 0.0, "avg_logprob": -0.11132416055222188, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00113069883082062}, {"id": 206, "seek": 83366, "start": 838.78, "end": 843.1, "text": " the entire linguistic community here. And, you know, I guess that would be one of the main", "tokens": [50620, 264, 2302, 43002, 1768, 510, 13, 400, 11, 291, 458, 11, 286, 2041, 300, 576, 312, 472, 295, 264, 2135, 50836], "temperature": 0.0, "avg_logprob": -0.11132416055222188, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00113069883082062}, {"id": 207, "seek": 83366, "start": 843.1, "end": 851.1, "text": " objections. Yeah, I mean, I don't know of, I guess, I think of the insights they've provided as", "tokens": [50836, 44649, 13, 865, 11, 286, 914, 11, 286, 500, 380, 458, 295, 11, 286, 2041, 11, 286, 519, 295, 264, 14310, 436, 600, 5649, 382, 51236], "temperature": 0.0, "avg_logprob": -0.11132416055222188, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00113069883082062}, {"id": 208, "seek": 83366, "start": 851.1, "end": 858.22, "text": " kind of general principles, right? So I think about these things like the power of memorizing", "tokens": [51236, 733, 295, 2674, 9156, 11, 558, 30, 407, 286, 519, 466, 613, 721, 411, 264, 1347, 295, 10560, 3319, 51592], "temperature": 0.0, "avg_logprob": -0.11132416055222188, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00113069883082062}, {"id": 209, "seek": 83366, "start": 858.22, "end": 863.26, "text": " chunks of language, right? So like, they seem to be very good at constructions, for example.", "tokens": [51592, 24004, 295, 2856, 11, 558, 30, 407, 411, 11, 436, 1643, 281, 312, 588, 665, 412, 7690, 626, 11, 337, 1365, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11132416055222188, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00113069883082062}, {"id": 210, "seek": 86326, "start": 863.34, "end": 867.5, "text": " And there's lots of linguistic theories, Chomsky's in particular, right, which are", "tokens": [50368, 400, 456, 311, 3195, 295, 43002, 13667, 11, 761, 4785, 4133, 311, 294, 1729, 11, 558, 11, 597, 366, 50576], "temperature": 0.0, "avg_logprob": -0.08635789211665358, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.0006875032559037209}, {"id": 211, "seek": 86326, "start": 868.38, "end": 874.14, "text": " about trying to find kind of minimal amounts of structure to memorize, right, trying to derive", "tokens": [50620, 466, 1382, 281, 915, 733, 295, 13206, 11663, 295, 3877, 281, 27478, 11, 558, 11, 1382, 281, 28446, 50908], "temperature": 0.0, "avg_logprob": -0.08635789211665358, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.0006875032559037209}, {"id": 212, "seek": 86326, "start": 874.14, "end": 880.78, "text": " as much as possible from some small set, some small collection of operations. And I think", "tokens": [50908, 382, 709, 382, 1944, 490, 512, 1359, 992, 11, 512, 1359, 5765, 295, 7705, 13, 400, 286, 519, 51240], "temperature": 0.0, "avg_logprob": -0.08635789211665358, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.0006875032559037209}, {"id": 213, "seek": 86326, "start": 880.78, "end": 887.02, "text": " that hasn't gone well for those theories, right? Whereas this goes really well, right? So if we", "tokens": [51240, 300, 6132, 380, 2780, 731, 337, 729, 13667, 11, 558, 30, 13813, 341, 1709, 534, 731, 11, 558, 30, 407, 498, 321, 51552], "temperature": 0.0, "avg_logprob": -0.08635789211665358, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.0006875032559037209}, {"id": 214, "seek": 86326, "start": 887.02, "end": 890.86, "text": " think about something which has the memorization abilities, if we think about theories of grammar,", "tokens": [51552, 519, 466, 746, 597, 575, 264, 10560, 2144, 11582, 11, 498, 321, 519, 466, 13667, 295, 22317, 11, 51744], "temperature": 0.0, "avg_logprob": -0.08635789211665358, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.0006875032559037209}, {"id": 215, "seek": 89086, "start": 890.86, "end": 898.46, "text": " for example, which build on, you know, humans like really remarkable ability to memorize different", "tokens": [50364, 337, 1365, 11, 597, 1322, 322, 11, 291, 458, 11, 6255, 411, 534, 12802, 3485, 281, 27478, 819, 50744], "temperature": 0.0, "avg_logprob": -0.08139689763387044, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.001244231010787189}, {"id": 216, "seek": 89086, "start": 898.46, "end": 902.78, "text": " constructions, right, or different words, you know, tens of thousands of words, tens of thousands of", "tokens": [50744, 7690, 626, 11, 558, 11, 420, 819, 2283, 11, 291, 458, 11, 10688, 295, 5383, 295, 2283, 11, 10688, 295, 5383, 295, 50960], "temperature": 0.0, "avg_logprob": -0.08139689763387044, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.001244231010787189}, {"id": 217, "seek": 89086, "start": 902.78, "end": 907.02, "text": " different constructions, sorry, tens of thousands of different idioms, maybe our theory of grammar", "tokens": [50960, 819, 7690, 626, 11, 2597, 11, 10688, 295, 5383, 295, 819, 18014, 4785, 11, 1310, 527, 5261, 295, 22317, 51172], "temperature": 0.0, "avg_logprob": -0.08139689763387044, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.001244231010787189}, {"id": 218, "seek": 89086, "start": 907.02, "end": 912.14, "text": " should be integrated with that. And there in some sense, a kind of proof of principle that", "tokens": [51172, 820, 312, 10919, 365, 300, 13, 400, 456, 294, 512, 2020, 11, 257, 733, 295, 8177, 295, 8665, 300, 51428], "temperature": 0.0, "avg_logprob": -0.08139689763387044, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.001244231010787189}, {"id": 219, "seek": 89086, "start": 912.14, "end": 917.74, "text": " that kind of approach can work well, right? Can think about making other types of predictions", "tokens": [51428, 300, 733, 295, 3109, 393, 589, 731, 11, 558, 30, 1664, 519, 466, 1455, 661, 3467, 295, 21264, 51708], "temperature": 0.0, "avg_logprob": -0.08139689763387044, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.001244231010787189}, {"id": 220, "seek": 91774, "start": 917.74, "end": 923.5, "text": " with them, some of which people are currently doing, but for example, trying to use them to measure", "tokens": [50364, 365, 552, 11, 512, 295, 597, 561, 366, 4362, 884, 11, 457, 337, 1365, 11, 1382, 281, 764, 552, 281, 3481, 50652], "temperature": 0.0, "avg_logprob": -0.14770157272751266, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.004328850656747818}, {"id": 221, "seek": 91774, "start": 924.3, "end": 930.3, "text": " processing difficulty, measure surprise, for example, from these models, their surprise measures,", "tokens": [50692, 9007, 10360, 11, 3481, 6365, 11, 337, 1365, 11, 490, 613, 5245, 11, 641, 6365, 8000, 11, 50992], "temperature": 0.0, "avg_logprob": -0.14770157272751266, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.004328850656747818}, {"id": 222, "seek": 91774, "start": 930.3, "end": 936.62, "text": " right, are much better than, say, context-free grammars or other kinds of language models.", "tokens": [50992, 558, 11, 366, 709, 1101, 813, 11, 584, 11, 4319, 12, 10792, 17570, 685, 420, 661, 3685, 295, 2856, 5245, 13, 51308], "temperature": 0.0, "avg_logprob": -0.14770157272751266, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.004328850656747818}, {"id": 223, "seek": 91774, "start": 936.62, "end": 941.98, "text": " And then it's an interesting question how those surprises or predictabilities relate to human", "tokens": [51308, 400, 550, 309, 311, 364, 1880, 1168, 577, 729, 22655, 420, 6069, 6167, 10961, 281, 1952, 51576], "temperature": 0.0, "avg_logprob": -0.14770157272751266, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.004328850656747818}, {"id": 224, "seek": 91774, "start": 941.98, "end": 947.1800000000001, "text": " processing, right? And it may capture some of it or might be nonlinear, or it might, you know,", "tokens": [51576, 9007, 11, 558, 30, 400, 309, 815, 7983, 512, 295, 309, 420, 1062, 312, 2107, 28263, 11, 420, 309, 1062, 11, 291, 458, 11, 51836], "temperature": 0.0, "avg_logprob": -0.14770157272751266, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.004328850656747818}, {"id": 225, "seek": 94718, "start": 947.18, "end": 952.6999999999999, "text": " only capture a little bit of it or whatever. That's an interesting kind of other scientific", "tokens": [50364, 787, 7983, 257, 707, 857, 295, 309, 420, 2035, 13, 663, 311, 364, 1880, 733, 295, 661, 8134, 50640], "temperature": 0.0, "avg_logprob": -0.09276972407788302, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0012833088403567672}, {"id": 226, "seek": 94718, "start": 952.6999999999999, "end": 957.5799999999999, "text": " question. But I think in principle, right, they can make predictions about, for example, the", "tokens": [50640, 1168, 13, 583, 286, 519, 294, 8665, 11, 558, 11, 436, 393, 652, 21264, 466, 11, 337, 1365, 11, 264, 50884], "temperature": 0.0, "avg_logprob": -0.09276972407788302, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0012833088403567672}, {"id": 227, "seek": 94718, "start": 957.5799999999999, "end": 963.5, "text": " connections between sentences, right? So in the paper, I gave this example of, you know, converting", "tokens": [50884, 9271, 1296, 16579, 11, 558, 30, 407, 294, 264, 3035, 11, 286, 2729, 341, 1365, 295, 11, 291, 458, 11, 29942, 51180], "temperature": 0.0, "avg_logprob": -0.09276972407788302, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0012833088403567672}, {"id": 228, "seek": 94718, "start": 963.5, "end": 969.8199999999999, "text": " a declaration into a question in 10 different ways, right? And presumably when it, when, you know,", "tokens": [51180, 257, 27606, 666, 257, 1168, 294, 1266, 819, 2098, 11, 558, 30, 400, 26742, 562, 309, 11, 562, 11, 291, 458, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09276972407788302, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0012833088403567672}, {"id": 229, "seek": 94718, "start": 969.8199999999999, "end": 975.8199999999999, "text": " GPT or something is doing that, it's finding 10 different questions which are all in some way", "tokens": [51496, 26039, 51, 420, 746, 307, 884, 300, 11, 309, 311, 5006, 1266, 819, 1651, 597, 366, 439, 294, 512, 636, 51796], "temperature": 0.0, "avg_logprob": -0.09276972407788302, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0012833088403567672}, {"id": 230, "seek": 97582, "start": 975.82, "end": 982.7, "text": " related, kind of nearby in the models underlying semantic or syntactic space. And so those kinds", "tokens": [50364, 4077, 11, 733, 295, 11184, 294, 264, 5245, 14217, 47982, 420, 23980, 19892, 1901, 13, 400, 370, 729, 3685, 50708], "temperature": 0.0, "avg_logprob": -0.0894088550489776, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.002713691908866167}, {"id": 231, "seek": 97582, "start": 982.7, "end": 989.5, "text": " of things are of the type that I think, you know, some linguists might want, right, which is here's", "tokens": [50708, 295, 721, 366, 295, 264, 2010, 300, 286, 519, 11, 291, 458, 11, 512, 21766, 1751, 1062, 528, 11, 558, 11, 597, 307, 510, 311, 51048], "temperature": 0.0, "avg_logprob": -0.0894088550489776, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.002713691908866167}, {"id": 232, "seek": 97582, "start": 989.5, "end": 994.22, "text": " some hidden connection between sentences or their, or their structures. But as far as I know, they", "tokens": [51048, 512, 7633, 4984, 1296, 16579, 420, 641, 11, 420, 641, 9227, 13, 583, 382, 1400, 382, 286, 458, 11, 436, 51284], "temperature": 0.0, "avg_logprob": -0.0894088550489776, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.002713691908866167}, {"id": 233, "seek": 97582, "start": 994.22, "end": 1000.46, "text": " haven't been evaluated empirically yet. So, yeah, yeah, I mean, these kinds of models are only a", "tokens": [51284, 2378, 380, 668, 25509, 25790, 984, 1939, 13, 407, 11, 1338, 11, 1338, 11, 286, 914, 11, 613, 3685, 295, 5245, 366, 787, 257, 51596], "temperature": 0.0, "avg_logprob": -0.0894088550489776, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.002713691908866167}, {"id": 234, "seek": 100046, "start": 1000.46, "end": 1005.82, "text": " few years old. So I think it's, it's reasonable to be excited about them, even though this kind", "tokens": [50364, 1326, 924, 1331, 13, 407, 286, 519, 309, 311, 11, 309, 311, 10585, 281, 312, 2919, 466, 552, 11, 754, 1673, 341, 733, 50632], "temperature": 0.0, "avg_logprob": -0.1402955585055881, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.012100324966013432}, {"id": 235, "seek": 100046, "start": 1005.82, "end": 1011.26, "text": " of work hasn't been done yet. No, that's right. No, totally. Totally. I mean, I think that's", "tokens": [50632, 295, 589, 6132, 380, 668, 1096, 1939, 13, 883, 11, 300, 311, 558, 13, 883, 11, 3879, 13, 22837, 13, 286, 914, 11, 286, 519, 300, 311, 50904], "temperature": 0.0, "avg_logprob": -0.1402955585055881, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.012100324966013432}, {"id": 236, "seek": 100046, "start": 1011.26, "end": 1015.82, "text": " the right perspective to take. But I think this gets to the issue of the, you mentioned surprise", "tokens": [50904, 264, 558, 4585, 281, 747, 13, 583, 286, 519, 341, 2170, 281, 264, 2734, 295, 264, 11, 291, 2835, 6365, 51132], "temperature": 0.0, "avg_logprob": -0.1402955585055881, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.012100324966013432}, {"id": 237, "seek": 100046, "start": 1015.82, "end": 1022.22, "text": " or you mentioned learnability, you know, LMS learn some syntax, but they do so. We've obviously way,", "tokens": [51132, 420, 291, 2835, 1466, 2310, 11, 291, 458, 11, 441, 10288, 1466, 512, 28431, 11, 457, 436, 360, 370, 13, 492, 600, 2745, 636, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1402955585055881, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.012100324966013432}, {"id": 238, "seek": 100046, "start": 1022.22, "end": 1028.7, "text": " way more data than infants do. Such that observations of potential structure in and of itself is not", "tokens": [51452, 636, 544, 1412, 813, 38829, 360, 13, 9653, 300, 18163, 295, 3995, 3877, 294, 293, 295, 2564, 307, 406, 51776], "temperature": 0.0, "avg_logprob": -0.1402955585055881, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.012100324966013432}, {"id": 239, "seek": 102870, "start": 1028.78, "end": 1032.94, "text": " a refutation of the poverty of the stimulus, well, the weaker version, I should say, of the", "tokens": [50368, 257, 1895, 11380, 295, 264, 10958, 295, 264, 21366, 11, 731, 11, 264, 24286, 3037, 11, 286, 820, 584, 11, 295, 264, 50576], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 240, "seek": 102870, "start": 1032.94, "end": 1037.9, "text": " poverty of the stimulus argument. So the mere fact that LMS can do what they do without grammatical", "tokens": [50576, 10958, 295, 264, 21366, 6770, 13, 407, 264, 8401, 1186, 300, 441, 10288, 393, 360, 437, 436, 360, 1553, 17570, 267, 804, 50824], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 241, "seek": 102870, "start": 1037.9, "end": 1042.38, "text": " prize is very striking, I agree. And in fact, you wouldn't have predicted that maybe five or six or", "tokens": [50824, 12818, 307, 588, 18559, 11, 286, 3986, 13, 400, 294, 1186, 11, 291, 2759, 380, 362, 19147, 300, 1310, 1732, 420, 2309, 420, 51048], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 242, "seek": 102870, "start": 1042.38, "end": 1048.06, "text": " seven years ago. But it doesn't yet invalidate the claim that humans have such a prize and we", "tokens": [51048, 3407, 924, 2057, 13, 583, 309, 1177, 380, 1939, 34702, 473, 264, 3932, 300, 6255, 362, 1270, 257, 12818, 293, 321, 51332], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 243, "seek": 102870, "start": 1048.06, "end": 1052.38, "text": " bring those prizes with us. And so in order to see if computational linguistics can constrain", "tokens": [51332, 1565, 729, 27350, 365, 505, 13, 400, 370, 294, 1668, 281, 536, 498, 28270, 21766, 6006, 393, 1817, 7146, 51548], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 244, "seek": 102870, "start": 1052.38, "end": 1056.14, "text": " hypotheses and theoretical linguistics, which I think it can do, by the way, this needs to be", "tokens": [51548, 49969, 293, 20864, 21766, 6006, 11, 597, 286, 519, 309, 393, 360, 11, 538, 264, 636, 11, 341, 2203, 281, 312, 51736], "temperature": 0.0, "avg_logprob": -0.12325793252864355, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.005766807124018669}, {"id": 245, "seek": 105614, "start": 1056.14, "end": 1059.98, "text": " done with, you know, careful experiments in which different learning parameters are controlled.", "tokens": [50364, 1096, 365, 11, 291, 458, 11, 5026, 12050, 294, 597, 819, 2539, 9834, 366, 10164, 13, 50556], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 246, "seek": 105614, "start": 1060.7, "end": 1067.18, "text": " And gigantic language models like GPT3 are basically, you know, useless here. So this gets", "tokens": [50592, 400, 26800, 2856, 5245, 411, 26039, 51, 18, 366, 1936, 11, 291, 458, 11, 14115, 510, 13, 407, 341, 2170, 50916], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 247, "seek": 105614, "start": 1067.18, "end": 1072.38, "text": " to some of Tarlin's complaints that we need something like a baby LM project, which I know", "tokens": [50916, 281, 512, 295, 10537, 5045, 311, 19585, 300, 321, 643, 746, 411, 257, 3186, 46529, 1716, 11, 597, 286, 458, 51176], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 248, "seek": 105614, "start": 1072.38, "end": 1076.7, "text": " you're interested in, where we have more, you know, ecologically valid training sets. You make the", "tokens": [51176, 291, 434, 3102, 294, 11, 689, 321, 362, 544, 11, 291, 458, 11, 11437, 17157, 7363, 3097, 6352, 13, 509, 652, 264, 51392], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 249, "seek": 105614, "start": 1076.7, "end": 1080.6200000000001, "text": " prediction in your paper that some structure will be learned from that. I suspect you might be right", "tokens": [51392, 17630, 294, 428, 3035, 300, 512, 3877, 486, 312, 3264, 490, 300, 13, 286, 9091, 291, 1062, 312, 558, 51588], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 250, "seek": 105614, "start": 1080.6200000000001, "end": 1084.94, "text": " there. But, you know, even so, even with the baby LM challenge, there's still the kind of", "tokens": [51588, 456, 13, 583, 11, 291, 458, 11, 754, 370, 11, 754, 365, 264, 3186, 46529, 3430, 11, 456, 311, 920, 264, 733, 295, 51804], "temperature": 0.0, "avg_logprob": -0.15422408260516265, "compression_ratio": 1.6875, "no_speech_prob": 0.0006248958525247872}, {"id": 251, "seek": 108494, "start": 1085.02, "end": 1090.6200000000001, "text": " non-trivial issue of addressing more traditional issues like when the kids start to generalize", "tokens": [50368, 2107, 12, 83, 470, 22640, 2734, 295, 14329, 544, 5164, 2663, 411, 562, 264, 2301, 722, 281, 2674, 1125, 50648], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 252, "seek": 108494, "start": 1090.6200000000001, "end": 1095.42, "text": " based on the amount of current input, based on different factors, cross-linguistically. And that", "tokens": [50648, 2361, 322, 264, 2372, 295, 2190, 4846, 11, 2361, 322, 819, 6771, 11, 3278, 12, 1688, 84, 20458, 13, 400, 300, 50888], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 253, "seek": 108494, "start": 1095.42, "end": 1101.18, "text": " requires just traditional, you know, psycholinguistics and language acquisition. So LMs, you know, do", "tokens": [50888, 7029, 445, 5164, 11, 291, 458, 11, 4681, 401, 7050, 6006, 293, 2856, 21668, 13, 407, 46529, 82, 11, 291, 458, 11, 360, 51176], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 254, "seek": 108494, "start": 1101.18, "end": 1105.26, "text": " care about things like frequency and surprise will, as you said, but there's a really nice paper by", "tokens": [51176, 1127, 466, 721, 411, 7893, 293, 6365, 486, 11, 382, 291, 848, 11, 457, 456, 311, 257, 534, 1481, 3035, 538, 51380], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 255, "seek": 108494, "start": 1105.26, "end": 1110.14, "text": " Sophie Slatzen, Andrea Martin, a really beautiful paper that I think you may have seen that shows", "tokens": [51380, 29645, 6187, 267, 2904, 11, 24215, 9184, 11, 257, 534, 2238, 3035, 300, 286, 519, 291, 815, 362, 1612, 300, 3110, 51624], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 256, "seek": 108494, "start": 1110.14, "end": 1114.8600000000001, "text": " very nicely that distributional statistics can sometimes be a cue to moments of structure", "tokens": [51624, 588, 9594, 300, 7316, 304, 12523, 393, 2171, 312, 257, 22656, 281, 6065, 295, 3877, 51860], "temperature": 0.0, "avg_logprob": -0.1543257556744476, "compression_ratio": 1.7395209580838322, "no_speech_prob": 0.0008711859700269997}, {"id": 257, "seek": 111486, "start": 1114.86, "end": 1119.34, "text": " building. But it doesn't replace these notions pertaining to composition. So I'll just read a", "tokens": [50364, 2390, 13, 583, 309, 1177, 380, 7406, 613, 35799, 49582, 281, 12686, 13, 407, 286, 603, 445, 1401, 257, 50588], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 258, "seek": 111486, "start": 1119.34, "end": 1125.82, "text": " quote from Chomsky 57, which sounds a lot like what Slatzen Martin said. Despite undeniable", "tokens": [50588, 6513, 490, 761, 4785, 4133, 21423, 11, 597, 3263, 257, 688, 411, 437, 6187, 267, 2904, 9184, 848, 13, 11334, 674, 268, 9364, 50912], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 259, "seek": 111486, "start": 1125.82, "end": 1131.26, "text": " interest and importance of semantic and statistical models of language, they appear to have no direct", "tokens": [50912, 1179, 293, 7379, 295, 47982, 293, 22820, 5245, 295, 2856, 11, 436, 4204, 281, 362, 572, 2047, 51184], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 260, "seek": 111486, "start": 1131.26, "end": 1135.1799999999998, "text": " relevance to the problem of determining or characterizing the set of grammatical utterances.", "tokens": [51184, 32684, 281, 264, 1154, 295, 23751, 420, 2517, 3319, 264, 992, 295, 17570, 267, 804, 17567, 2676, 13, 51380], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 261, "seek": 111486, "start": 1135.1799999999998, "end": 1139.02, "text": " I think that we are forced to conclude that grammar is autonomous and independent of meaning,", "tokens": [51380, 286, 519, 300, 321, 366, 7579, 281, 16886, 300, 22317, 307, 23797, 293, 6695, 295, 3620, 11, 51572], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 262, "seek": 111486, "start": 1139.02, "end": 1143.26, "text": " and that probabilistic models give no particular insight into some of the basic problems of", "tokens": [51572, 293, 300, 31959, 3142, 5245, 976, 572, 1729, 11269, 666, 512, 295, 264, 3875, 2740, 295, 51784], "temperature": 0.0, "avg_logprob": -0.0657697569939398, "compression_ratio": 1.6598240469208212, "no_speech_prob": 0.0009971421677619219}, {"id": 263, "seek": 114326, "start": 1143.26, "end": 1149.5, "text": " syntactic structure. So that second hedge of the second sentence turned out to be incorrect.", "tokens": [50364, 23980, 19892, 3877, 13, 407, 300, 1150, 25304, 295, 264, 1150, 8174, 3574, 484, 281, 312, 18424, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 264, "seek": 114326, "start": 1150.06, "end": 1152.46, "text": " But it's so it's true that, you know, what Chomsky said of available", "tokens": [50704, 583, 309, 311, 370, 309, 311, 2074, 300, 11, 291, 458, 11, 437, 761, 4785, 4133, 848, 295, 2435, 50824], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 265, "seek": 114326, "start": 1153.02, "end": 1159.02, "text": " stat models in 57 is no longer accurate when applied to models today. That can make abstract", "tokens": [50852, 2219, 5245, 294, 21423, 307, 572, 2854, 8559, 562, 6456, 281, 5245, 965, 13, 663, 393, 652, 12649, 51152], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 266, "seek": 114326, "start": 1159.02, "end": 1162.3, "text": " generalizations about novel strings and distributional categories, as you mentioned,", "tokens": [51152, 2674, 14455, 466, 7613, 13985, 293, 7316, 304, 10479, 11, 382, 291, 2835, 11, 51316], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 267, "seek": 114326, "start": 1162.3, "end": 1167.26, "text": " right? But the performance of a single model does not provide direct evidence for or against", "tokens": [51316, 558, 30, 583, 264, 3389, 295, 257, 2167, 2316, 775, 406, 2893, 2047, 4467, 337, 420, 1970, 51564], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 268, "seek": 114326, "start": 1167.26, "end": 1171.26, "text": " the landability of a particular structure. Like given the vast distance between any", "tokens": [51564, 264, 2117, 2310, 295, 257, 1729, 3877, 13, 1743, 2212, 264, 8369, 4560, 1296, 604, 51764], "temperature": 0.0, "avg_logprob": -0.1482406118641729, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.0018521256279200315}, {"id": 269, "seek": 117126, "start": 1171.26, "end": 1176.62, "text": " computational model available today and the human brain, model success does not mean that the", "tokens": [50364, 28270, 2316, 2435, 965, 293, 264, 1952, 3567, 11, 2316, 2245, 775, 406, 914, 300, 264, 50632], "temperature": 0.0, "avg_logprob": -0.12541570334598937, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0004649789188988507}, {"id": 270, "seek": 117126, "start": 1176.62, "end": 1182.62, "text": " structure is necessarily land and model failure also doesn't mean that the structure is not landable,", "tokens": [50632, 3877, 307, 4725, 2117, 293, 2316, 7763, 611, 1177, 380, 914, 300, 264, 3877, 307, 406, 2117, 712, 11, 50932], "temperature": 0.0, "avg_logprob": -0.12541570334598937, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0004649789188988507}, {"id": 271, "seek": 117126, "start": 1182.62, "end": 1190.14, "text": " right? Yeah, yeah. So I mean, I think it's maybe worth unpacking kind of a couple different versions", "tokens": [50932, 558, 30, 865, 11, 1338, 13, 407, 286, 914, 11, 286, 519, 309, 311, 1310, 3163, 26699, 278, 733, 295, 257, 1916, 819, 9606, 51308], "temperature": 0.0, "avg_logprob": -0.12541570334598937, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0004649789188988507}, {"id": 272, "seek": 117126, "start": 1190.14, "end": 1195.5, "text": " of learnability arguments that people have made, because there have been very, very strong kind", "tokens": [51308, 295, 1466, 2310, 12869, 300, 561, 362, 1027, 11, 570, 456, 362, 668, 588, 11, 588, 2068, 733, 51576], "temperature": 0.0, "avg_logprob": -0.12541570334598937, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0004649789188988507}, {"id": 273, "seek": 119550, "start": 1195.58, "end": 1201.26, "text": " of impossibility claims coming out of kind of Chomsky's tradition, right, that were never", "tokens": [50368, 295, 38802, 2841, 9441, 1348, 484, 295, 733, 295, 761, 4785, 4133, 311, 6994, 11, 558, 11, 300, 645, 1128, 50652], "temperature": 0.0, "avg_logprob": -0.10152253759912698, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.053371720016002655}, {"id": 274, "seek": 119550, "start": 1201.26, "end": 1206.94, "text": " claims about the amount of data that was required, right? They were claims about the logical problem", "tokens": [50652, 9441, 466, 264, 2372, 295, 1412, 300, 390, 4739, 11, 558, 30, 814, 645, 9441, 466, 264, 14978, 1154, 50936], "temperature": 0.0, "avg_logprob": -0.10152253759912698, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.053371720016002655}, {"id": 275, "seek": 119550, "start": 1206.94, "end": 1212.86, "text": " of language learning and that it was just impossible, right? It was impossible without having", "tokens": [50936, 295, 2856, 2539, 293, 300, 309, 390, 445, 6243, 11, 558, 30, 467, 390, 6243, 1553, 1419, 51232], "temperature": 0.0, "avg_logprob": -0.10152253759912698, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.053371720016002655}, {"id": 276, "seek": 119550, "start": 1214.78, "end": 1219.58, "text": " kind of substantial constraints on the class of languages or the class of grammars that you", "tokens": [51328, 733, 295, 16726, 18491, 322, 264, 1508, 295, 8650, 420, 264, 1508, 295, 17570, 685, 300, 291, 51568], "temperature": 0.0, "avg_logprob": -0.10152253759912698, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.053371720016002655}, {"id": 277, "seek": 121958, "start": 1220.1399999999999, "end": 1225.58, "text": " would acquire. And people for a long time have been arguing against that version of things.", "tokens": [50392, 576, 20001, 13, 400, 561, 337, 257, 938, 565, 362, 668, 19697, 1970, 300, 3037, 295, 721, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1030112413259653, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.020014125853776932}, {"id": 278, "seek": 121958, "start": 1227.1799999999998, "end": 1231.1799999999998, "text": " You know, there's old work by Gold, and then there's whole kind of grammatical", "tokens": [50744, 509, 458, 11, 456, 311, 1331, 589, 538, 6731, 11, 293, 550, 456, 311, 1379, 733, 295, 17570, 267, 804, 50944], "temperature": 0.0, "avg_logprob": -0.1030112413259653, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.020014125853776932}, {"id": 279, "seek": 121958, "start": 1231.1799999999998, "end": 1237.82, "text": " theories of acquisition built on that tradition that worry a lot about the kind of order in", "tokens": [50944, 13667, 295, 21668, 3094, 322, 300, 6994, 300, 3292, 257, 688, 466, 264, 733, 295, 1668, 294, 51276], "temperature": 0.0, "avg_logprob": -0.1030112413259653, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.020014125853776932}, {"id": 280, "seek": 121958, "start": 1237.82, "end": 1241.82, "text": " which you traverse through different hypotheses and consider different options and things.", "tokens": [51276, 597, 291, 45674, 807, 819, 49969, 293, 1949, 819, 3956, 293, 721, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1030112413259653, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.020014125853776932}, {"id": 281, "seek": 124182, "start": 1242.78, "end": 1248.62, "text": " And my favorite reference in this is this paper by Nick Chater and Paul Vitani,", "tokens": [50412, 400, 452, 2954, 6408, 294, 341, 307, 341, 3035, 538, 9449, 761, 771, 293, 4552, 22463, 3782, 11, 50704], "temperature": 0.0, "avg_logprob": -0.17189563613340078, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0060965497978031635}, {"id": 282, "seek": 124182, "start": 1249.4199999999998, "end": 1255.26, "text": " called something like Ideal Learning of Natural Language, that basically shows that an unconstrained", "tokens": [50744, 1219, 746, 411, 13090, 304, 15205, 295, 20137, 24445, 11, 300, 1936, 3110, 300, 364, 35847, 19639, 2001, 51036], "temperature": 0.0, "avg_logprob": -0.17189563613340078, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0060965497978031635}, {"id": 283, "seek": 124182, "start": 1255.26, "end": 1262.7, "text": " learner could, with enough data, acquire the kind of generating rules or the generating grammar", "tokens": [51036, 33347, 727, 11, 365, 1547, 1412, 11, 20001, 264, 733, 295, 17746, 4474, 420, 264, 17746, 22317, 51408], "temperature": 0.0, "avg_logprob": -0.17189563613340078, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0060965497978031635}, {"id": 284, "seek": 124182, "start": 1263.34, "end": 1270.3, "text": " just from observing strings, right? But that paper was really in response to this huge body of work", "tokens": [51440, 445, 490, 22107, 13985, 11, 558, 30, 583, 300, 3035, 390, 534, 294, 4134, 281, 341, 2603, 1772, 295, 589, 51788], "temperature": 0.0, "avg_logprob": -0.17189563613340078, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0060965497978031635}, {"id": 285, "seek": 127030, "start": 1270.3799999999999, "end": 1275.98, "text": " that was arguing that learning from positive examples, so from just observing strings,", "tokens": [50368, 300, 390, 19697, 300, 2539, 490, 3353, 5110, 11, 370, 490, 445, 22107, 13985, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11933865604630436, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.0005032336921431124}, {"id": 286, "seek": 127030, "start": 1275.98, "end": 1284.3799999999999, "text": " was like logically impossible, right? So of course, you know, people in Chomsky's tradition", "tokens": [50648, 390, 411, 38887, 6243, 11, 558, 30, 407, 295, 1164, 11, 291, 458, 11, 561, 294, 761, 4785, 4133, 311, 6994, 51068], "temperature": 0.0, "avg_logprob": -0.11933865604630436, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.0005032336921431124}, {"id": 287, "seek": 127030, "start": 1284.3799999999999, "end": 1291.02, "text": " really liked that form of argument because it was one that said you had to have something innately", "tokens": [51068, 534, 4501, 300, 1254, 295, 6770, 570, 309, 390, 472, 300, 848, 291, 632, 281, 362, 746, 7714, 1592, 51400], "temperature": 0.0, "avg_logprob": -0.11933865604630436, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.0005032336921431124}, {"id": 288, "seek": 127030, "start": 1291.74, "end": 1296.3, "text": " specified in order for language acquisition to work. It was like kind of a mathematical argument,", "tokens": [51436, 22206, 294, 1668, 337, 2856, 21668, 281, 589, 13, 467, 390, 411, 733, 295, 257, 18894, 6770, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11933865604630436, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.0005032336921431124}, {"id": 289, "seek": 129630, "start": 1296.78, "end": 1302.3, "text": " that you had to have some kind of innate grammar, innate ordering of hypotheses or something,", "tokens": [50388, 300, 291, 632, 281, 362, 512, 733, 295, 41766, 22317, 11, 41766, 21739, 295, 49969, 420, 746, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10148863845996642, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0009398054098710418}, {"id": 290, "seek": 129630, "start": 1302.3, "end": 1309.02, "text": " and all of that just turned out to be totally wrong. So if you move to slightly more kind of", "tokens": [50664, 293, 439, 295, 300, 445, 3574, 484, 281, 312, 3879, 2085, 13, 407, 498, 291, 1286, 281, 4748, 544, 733, 295, 51000], "temperature": 0.0, "avg_logprob": -0.10148863845996642, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0009398054098710418}, {"id": 291, "seek": 129630, "start": 1309.02, "end": 1315.8999999999999, "text": " realistic learning settings, which Chater and Vitani do, then it turns out you like an idealized", "tokens": [51000, 12465, 2539, 6257, 11, 597, 761, 771, 293, 22463, 3782, 360, 11, 550, 309, 4523, 484, 291, 411, 364, 7157, 1602, 51344], "temperature": 0.0, "avg_logprob": -0.10148863845996642, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0009398054098710418}, {"id": 292, "seek": 129630, "start": 1315.8999999999999, "end": 1320.78, "text": " learner can acquire stuff, and there's no statements about the amount of data that's required even", "tokens": [51344, 33347, 393, 20001, 1507, 11, 293, 456, 311, 572, 12363, 466, 264, 2372, 295, 1412, 300, 311, 4739, 754, 51588], "temperature": 0.0, "avg_logprob": -0.10148863845996642, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0009398054098710418}, {"id": 293, "seek": 132078, "start": 1320.78, "end": 1328.86, "text": " there, right? That's the kind of pure logical ability to learn, and that ability is what I think", "tokens": [50364, 456, 11, 558, 30, 663, 311, 264, 733, 295, 6075, 14978, 3485, 281, 1466, 11, 293, 300, 3485, 307, 437, 286, 519, 50768], "temperature": 0.0, "avg_logprob": -0.08659299744500054, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.02095518261194229}, {"id": 294, "seek": 132078, "start": 1328.86, "end": 1334.62, "text": " the big versions of large language models also speak to, right? So Chater and Vitani and other", "tokens": [50768, 264, 955, 9606, 295, 2416, 2856, 5245, 611, 1710, 281, 11, 558, 30, 407, 761, 771, 293, 22463, 3782, 293, 661, 51056], "temperature": 0.0, "avg_logprob": -0.08659299744500054, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.02095518261194229}, {"id": 295, "seek": 132078, "start": 1334.62, "end": 1340.54, "text": " work kind of in that spirit is, you know, mathematical and kind of arguing in principle,", "tokens": [51056, 589, 733, 295, 294, 300, 3797, 307, 11, 291, 458, 11, 18894, 293, 733, 295, 19697, 294, 8665, 11, 51352], "temperature": 0.0, "avg_logprob": -0.08659299744500054, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.02095518261194229}, {"id": 296, "seek": 132078, "start": 1340.54, "end": 1348.22, "text": " but never created something which was really a grammar, right, or a real kind of implemented", "tokens": [51352, 457, 1128, 2942, 746, 597, 390, 534, 257, 22317, 11, 558, 11, 420, 257, 957, 733, 295, 12270, 51736], "temperature": 0.0, "avg_logprob": -0.08659299744500054, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.02095518261194229}, {"id": 297, "seek": 134822, "start": 1348.22, "end": 1355.58, "text": " language model. So even, you know, a model which is trained on 100 million or 100 billion or however", "tokens": [50364, 2856, 2316, 13, 407, 754, 11, 291, 458, 11, 257, 2316, 597, 307, 8895, 322, 2319, 2459, 420, 2319, 5218, 420, 4461, 50732], "temperature": 0.0, "avg_logprob": -0.12448415894439255, "compression_ratio": 1.6, "no_speech_prob": 0.0032214168459177017}, {"id": 298, "seek": 134822, "start": 1355.58, "end": 1363.5, "text": " many tokens, right, even that kind of model I think is relevant to that version of the debate,", "tokens": [50732, 867, 22667, 11, 558, 11, 754, 300, 733, 295, 2316, 286, 519, 307, 7340, 281, 300, 3037, 295, 264, 7958, 11, 51128], "temperature": 0.0, "avg_logprob": -0.12448415894439255, "compression_ratio": 1.6, "no_speech_prob": 0.0032214168459177017}, {"id": 299, "seek": 134822, "start": 1363.5, "end": 1370.6200000000001, "text": " right, and showing that language learning is not impossible from a very unconstrained space.", "tokens": [51128, 558, 11, 293, 4099, 300, 2856, 2539, 307, 406, 6243, 490, 257, 588, 35847, 19639, 2001, 1901, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12448415894439255, "compression_ratio": 1.6, "no_speech_prob": 0.0032214168459177017}, {"id": 300, "seek": 137062, "start": 1370.9399999999998, "end": 1378.3799999999999, "text": " Okay. And then there's a second version, right, which is can we learn language with the specific", "tokens": [50380, 1033, 13, 400, 550, 456, 311, 257, 1150, 3037, 11, 558, 11, 597, 307, 393, 321, 1466, 2856, 365, 264, 2685, 50752], "temperature": 0.0, "avg_logprob": -0.17955525716145834, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.02973363734781742}, {"id": 301, "seek": 137062, "start": 1378.3799999999999, "end": 1383.5, "text": " data that kids get, right, and that's both amount of data and form of the data,", "tokens": [50752, 1412, 300, 2301, 483, 11, 558, 11, 293, 300, 311, 1293, 2372, 295, 1412, 293, 1254, 295, 264, 1412, 11, 51008], "temperature": 0.0, "avg_logprob": -0.17955525716145834, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.02973363734781742}, {"id": 302, "seek": 137062, "start": 1384.6999999999998, "end": 1391.1, "text": " and so for people who don't know, the BabyLM Challenge is this,", "tokens": [51068, 293, 370, 337, 561, 567, 500, 380, 458, 11, 264, 9425, 43, 44, 17517, 307, 341, 11, 51388], "temperature": 0.0, "avg_logprob": -0.17955525716145834, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.02973363734781742}, {"id": 303, "seek": 139110, "start": 1391.58, "end": 1401.58, "text": " sorry, we think to call it a competition or a, I guess it is a challenge, trying to get people", "tokens": [50388, 2597, 11, 321, 519, 281, 818, 309, 257, 6211, 420, 257, 11, 286, 2041, 309, 307, 257, 3430, 11, 1382, 281, 483, 561, 50888], "temperature": 0.0, "avg_logprob": -0.1917665890284947, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.03357440233230591}, {"id": 304, "seek": 139110, "start": 1401.58, "end": 1407.74, "text": " to train language models on human sized amounts of data. So that's something more like, I think", "tokens": [50888, 281, 3847, 2856, 5245, 322, 1952, 20004, 11663, 295, 1412, 13, 407, 300, 311, 746, 544, 411, 11, 286, 519, 51196], "temperature": 0.0, "avg_logprob": -0.1917665890284947, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.03357440233230591}, {"id": 305, "seek": 139110, "start": 1407.74, "end": 1413.4199999999998, "text": " there's two different versions, 10 or 100 million different, 10 or 100 million different words in", "tokens": [51196, 456, 311, 732, 819, 9606, 11, 1266, 420, 2319, 2459, 819, 11, 1266, 420, 2319, 2459, 819, 2283, 294, 51480], "temperature": 0.0, "avg_logprob": -0.1917665890284947, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.03357440233230591}, {"id": 306, "seek": 141342, "start": 1413.42, "end": 1422.0600000000002, "text": " the training set, which is like, you know, 100th or 1000th or something as big as these big AI", "tokens": [50364, 264, 3097, 992, 11, 597, 307, 411, 11, 291, 458, 11, 2319, 392, 420, 9714, 392, 420, 746, 382, 955, 382, 613, 955, 7318, 50796], "temperature": 0.0, "avg_logprob": -0.09971409655631856, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.028425758704543114}, {"id": 307, "seek": 141342, "start": 1422.0600000000002, "end": 1429.66, "text": " companies are using for their language models. And I think actually it's like, that's exactly the", "tokens": [50796, 3431, 366, 1228, 337, 641, 2856, 5245, 13, 400, 286, 519, 767, 309, 311, 411, 11, 300, 311, 2293, 264, 51176], "temperature": 0.0, "avg_logprob": -0.09971409655631856, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.028425758704543114}, {"id": 308, "seek": 141342, "start": 1429.66, "end": 1433.8200000000002, "text": " right kind of thing and exactly what the field needs, right, because you might find that on", "tokens": [51176, 558, 733, 295, 551, 293, 2293, 437, 264, 2519, 2203, 11, 558, 11, 570, 291, 1062, 915, 300, 322, 51384], "temperature": 0.0, "avg_logprob": -0.09971409655631856, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.028425758704543114}, {"id": 309, "seek": 141342, "start": 1434.6200000000001, "end": 1440.94, "text": " a child sized amount of data, you can essentially learn syntax, right, which I think would be the", "tokens": [51424, 257, 1440, 20004, 2372, 295, 1412, 11, 291, 393, 4476, 1466, 28431, 11, 558, 11, 597, 286, 519, 576, 312, 264, 51740], "temperature": 0.0, "avg_logprob": -0.09971409655631856, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.028425758704543114}, {"id": 310, "seek": 144094, "start": 1440.94, "end": 1445.5, "text": " strongest argument against these property of stimulus claims, you could alternatively find that", "tokens": [50364, 16595, 6770, 1970, 613, 4707, 295, 21366, 9441, 11, 291, 727, 8535, 356, 915, 300, 50592], "temperature": 0.0, "avg_logprob": -0.11326343362981622, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.0037062556948512793}, {"id": 311, "seek": 144094, "start": 1446.14, "end": 1452.54, "text": " maybe you can't learn very much, maybe you, you know, come up with a much crumbier kind of language", "tokens": [50624, 1310, 291, 393, 380, 1466, 588, 709, 11, 1310, 291, 11, 291, 458, 11, 808, 493, 365, 257, 709, 941, 2860, 811, 733, 295, 2856, 50944], "temperature": 0.0, "avg_logprob": -0.11326343362981622, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.0037062556948512793}, {"id": 312, "seek": 144094, "start": 1452.54, "end": 1459.02, "text": " model or it's lacking some syntactic or semantic abilities. I actually think that the failures,", "tokens": [50944, 2316, 420, 309, 311, 20889, 512, 23980, 19892, 420, 47982, 11582, 13, 286, 767, 519, 300, 264, 20774, 11, 51268], "temperature": 0.0, "avg_logprob": -0.11326343362981622, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.0037062556948512793}, {"id": 313, "seek": 144094, "start": 1459.02, "end": 1465.5, "text": " they are a little bit hard to interpret because kids data, when they're actually learning language,", "tokens": [51268, 436, 366, 257, 707, 857, 1152, 281, 7302, 570, 2301, 1412, 11, 562, 436, 434, 767, 2539, 2856, 11, 51592], "temperature": 0.0, "avg_logprob": -0.11326343362981622, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.0037062556948512793}, {"id": 314, "seek": 146550, "start": 1465.5, "end": 1471.58, "text": " they get a lot more data than just strings of sentences, right, they're interacting in an", "tokens": [50364, 436, 483, 257, 688, 544, 1412, 813, 445, 13985, 295, 16579, 11, 558, 11, 436, 434, 18017, 294, 364, 50668], "temperature": 0.0, "avg_logprob": -0.09015346855245611, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.017973478883504868}, {"id": 315, "seek": 146550, "start": 1471.58, "end": 1477.5, "text": " environment. So there's stuff in the world in front of them. Their utterances are also interactive,", "tokens": [50668, 2823, 13, 407, 456, 311, 1507, 294, 264, 1002, 294, 1868, 295, 552, 13, 6710, 17567, 2676, 366, 611, 15141, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09015346855245611, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.017973478883504868}, {"id": 316, "seek": 146550, "start": 1477.5, "end": 1481.82, "text": " right, so you can say something and see whether your parent brings you the thing that you asked for,", "tokens": [50964, 558, 11, 370, 291, 393, 584, 746, 293, 536, 1968, 428, 2596, 5607, 291, 264, 551, 300, 291, 2351, 337, 11, 51180], "temperature": 0.0, "avg_logprob": -0.09015346855245611, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.017973478883504868}, {"id": 317, "seek": 146550, "start": 1481.82, "end": 1489.42, "text": " for example, right, that's long been argued by people as a, you know, important cue in language", "tokens": [51180, 337, 1365, 11, 558, 11, 300, 311, 938, 668, 20219, 538, 561, 382, 257, 11, 291, 458, 11, 1021, 22656, 294, 2856, 51560], "temperature": 0.0, "avg_logprob": -0.09015346855245611, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.017973478883504868}, {"id": 318, "seek": 148942, "start": 1489.5, "end": 1498.46, "text": " acquisition. So in the baby LM challenge, there is an ability to train these models", "tokens": [50368, 21668, 13, 407, 294, 264, 3186, 46529, 3430, 11, 456, 307, 364, 3485, 281, 3847, 613, 5245, 50816], "temperature": 0.0, "avg_logprob": -0.10239161836340073, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.04398634657263756}, {"id": 319, "seek": 148942, "start": 1499.18, "end": 1503.9, "text": " with kind of multimodal input, so I think you can give them as much video data as you want to give,", "tokens": [50852, 365, 733, 295, 32972, 378, 304, 4846, 11, 370, 286, 519, 291, 393, 976, 552, 382, 709, 960, 1412, 382, 291, 528, 281, 976, 11, 51088], "temperature": 0.0, "avg_logprob": -0.10239161836340073, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.04398634657263756}, {"id": 320, "seek": 148942, "start": 1505.5, "end": 1510.54, "text": " but probably it's hard to kind of replicate exactly the type of setup and feedback that kids", "tokens": [51168, 457, 1391, 309, 311, 1152, 281, 733, 295, 25356, 2293, 264, 2010, 295, 8657, 293, 5824, 300, 2301, 51420], "temperature": 0.0, "avg_logprob": -0.10239161836340073, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.04398634657263756}, {"id": 321, "seek": 148942, "start": 1510.54, "end": 1518.54, "text": " actually get. So I don't know, you know, I'm excited to see where that goes and how things pan", "tokens": [51420, 767, 483, 13, 407, 286, 500, 380, 458, 11, 291, 458, 11, 286, 478, 2919, 281, 536, 689, 300, 1709, 293, 577, 721, 2462, 51820], "temperature": 0.0, "avg_logprob": -0.10239161836340073, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.04398634657263756}, {"id": 322, "seek": 151854, "start": 1518.54, "end": 1526.86, "text": " out there. You know, I think that there is an interesting related question for large language", "tokens": [50364, 484, 456, 13, 509, 458, 11, 286, 519, 300, 456, 307, 364, 1880, 4077, 1168, 337, 2416, 2856, 50780], "temperature": 0.0, "avg_logprob": -0.0948229057844295, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.0005882344557903707}, {"id": 323, "seek": 151854, "start": 1526.86, "end": 1534.22, "text": " models, which is like what, which is understanding exactly what all of the data is doing. So it", "tokens": [50780, 5245, 11, 597, 307, 411, 437, 11, 597, 307, 3701, 2293, 437, 439, 295, 264, 1412, 307, 884, 13, 407, 309, 51148], "temperature": 0.0, "avg_logprob": -0.0948229057844295, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.0005882344557903707}, {"id": 324, "seek": 151854, "start": 1534.22, "end": 1540.06, "text": " could be that you need so much data for these models because they're effectively inventing", "tokens": [51148, 727, 312, 300, 291, 643, 370, 709, 1412, 337, 613, 5245, 570, 436, 434, 8659, 7962, 278, 51440], "temperature": 0.0, "avg_logprob": -0.0948229057844295, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.0005882344557903707}, {"id": 325, "seek": 151854, "start": 1540.06, "end": 1546.78, "text": " some form of semantics internally, right, so they're both discovering the rules of syntax and they", "tokens": [51440, 512, 1254, 295, 4361, 45298, 19501, 11, 558, 11, 370, 436, 434, 1293, 24773, 264, 4474, 295, 28431, 293, 436, 51776], "temperature": 0.0, "avg_logprob": -0.0948229057844295, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.0005882344557903707}, {"id": 326, "seek": 154678, "start": 1546.78, "end": 1553.66, "text": " appear to be learning quite a bit about word meanings. And it's not, it's totally unclear,", "tokens": [50364, 4204, 281, 312, 2539, 1596, 257, 857, 466, 1349, 28138, 13, 400, 309, 311, 406, 11, 309, 311, 3879, 25636, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1103360215011908, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.002979838289320469}, {"id": 327, "seek": 154678, "start": 1553.66, "end": 1560.22, "text": " I think, how much of the data in these modern models is needed for syntax versus semantics.", "tokens": [50708, 286, 519, 11, 577, 709, 295, 264, 1412, 294, 613, 4363, 5245, 307, 2978, 337, 28431, 5717, 4361, 45298, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1103360215011908, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.002979838289320469}, {"id": 328, "seek": 154678, "start": 1560.22, "end": 1567.8999999999999, "text": " My own guess, I think, would be that the syntactic side is probably requires much less data than the", "tokens": [51036, 1222, 1065, 2041, 11, 286, 519, 11, 576, 312, 300, 264, 23980, 19892, 1252, 307, 1391, 7029, 709, 1570, 1412, 813, 264, 51420], "temperature": 0.0, "avg_logprob": -0.1103360215011908, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.002979838289320469}, {"id": 329, "seek": 154678, "start": 1567.8999999999999, "end": 1573.18, "text": " semantic side. Actually, a student, a former student of mine, Frank Malica, and I wrote a paper a few", "tokens": [51420, 47982, 1252, 13, 5135, 11, 257, 3107, 11, 257, 5819, 3107, 295, 3892, 11, 6823, 5746, 2262, 11, 293, 286, 4114, 257, 3035, 257, 1326, 51684], "temperature": 0.0, "avg_logprob": -0.1103360215011908, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.002979838289320469}, {"id": 330, "seek": 157318, "start": 1573.18, "end": 1578.3, "text": " years ago trying to estimate the amount of information a learner would necessarily have to", "tokens": [50364, 924, 2057, 1382, 281, 12539, 264, 2372, 295, 1589, 257, 33347, 576, 4725, 362, 281, 50620], "temperature": 0.0, "avg_logprob": -0.0941314880664532, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.005552595015615225}, {"id": 331, "seek": 157318, "start": 1578.3, "end": 1584.14, "text": " acquire for learning the different aspects of language. So you have to learn all the words and", "tokens": [50620, 20001, 337, 2539, 264, 819, 7270, 295, 2856, 13, 407, 291, 362, 281, 1466, 439, 264, 2283, 293, 50912], "temperature": 0.0, "avg_logprob": -0.0941314880664532, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.005552595015615225}, {"id": 332, "seek": 157318, "start": 1584.14, "end": 1587.98, "text": " you learn their forms, you learn their meanings, you probably know their frequencies, you have to", "tokens": [50912, 291, 1466, 641, 6422, 11, 291, 1466, 641, 28138, 11, 291, 1391, 458, 641, 20250, 11, 291, 362, 281, 51104], "temperature": 0.0, "avg_logprob": -0.0941314880664532, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.005552595015615225}, {"id": 333, "seek": 157318, "start": 1587.98, "end": 1594.94, "text": " learn syntax. And basically what we found in that analysis, that was, you know, basically just a kind", "tokens": [51104, 1466, 28431, 13, 400, 1936, 437, 321, 1352, 294, 300, 5215, 11, 300, 390, 11, 291, 458, 11, 1936, 445, 257, 733, 51452], "temperature": 0.0, "avg_logprob": -0.0941314880664532, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.005552595015615225}, {"id": 334, "seek": 157318, "start": 1594.94, "end": 1601.02, "text": " of back of the envelope calculation for each of these domains is that syntax is actually very few", "tokens": [51452, 295, 646, 295, 264, 19989, 17108, 337, 1184, 295, 613, 25514, 307, 300, 28431, 307, 767, 588, 1326, 51756], "temperature": 0.0, "avg_logprob": -0.0941314880664532, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.005552595015615225}, {"id": 335, "seek": 160102, "start": 1601.02, "end": 1608.06, "text": " bits of information, it doesn't take that much information to learn syntax. Whereas like most", "tokens": [50364, 9239, 295, 1589, 11, 309, 1177, 380, 747, 300, 709, 1589, 281, 1466, 28431, 13, 13813, 411, 881, 50716], "temperature": 0.0, "avg_logprob": -0.11102961490028783, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.007116838824003935}, {"id": 336, "seek": 160102, "start": 1608.06, "end": 1615.74, "text": " of the information you acquire is actually for semantics. So specifying, you know, 30 to 50,000", "tokens": [50716, 295, 264, 1589, 291, 20001, 307, 767, 337, 4361, 45298, 13, 407, 1608, 5489, 11, 291, 458, 11, 2217, 281, 2625, 11, 1360, 51100], "temperature": 0.0, "avg_logprob": -0.11102961490028783, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.007116838824003935}, {"id": 337, "seek": 160102, "start": 1615.74, "end": 1623.34, "text": " different word meanings, you know, even if each meaning is just a few bits, right, like that requires", "tokens": [51100, 819, 1349, 28138, 11, 291, 458, 11, 754, 498, 1184, 3620, 307, 445, 257, 1326, 9239, 11, 558, 11, 411, 300, 7029, 51480], "temperature": 0.0, "avg_logprob": -0.11102961490028783, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.007116838824003935}, {"id": 338, "seek": 160102, "start": 1623.34, "end": 1630.22, "text": " a lot of information and probably each meaning is more than a few bits, right. So it could be,", "tokens": [51480, 257, 688, 295, 1589, 293, 1391, 1184, 3620, 307, 544, 813, 257, 1326, 9239, 11, 558, 13, 407, 309, 727, 312, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11102961490028783, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.007116838824003935}, {"id": 339, "seek": 163022, "start": 1630.22, "end": 1634.54, "text": " like, that would make me guess that what's happening with large language models is most", "tokens": [50364, 411, 11, 300, 576, 652, 385, 2041, 300, 437, 311, 2737, 365, 2416, 2856, 5245, 307, 881, 50580], "temperature": 0.0, "avg_logprob": -0.12536843884892823, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0009826814057305455}, {"id": 340, "seek": 163022, "start": 1634.54, "end": 1639.5, "text": " of their training data is about word semantics. And you can think about other ways that kids get", "tokens": [50580, 295, 641, 3097, 1412, 307, 466, 1349, 4361, 45298, 13, 400, 291, 393, 519, 466, 661, 2098, 300, 2301, 483, 50828], "temperature": 0.0, "avg_logprob": -0.12536843884892823, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0009826814057305455}, {"id": 341, "seek": 163022, "start": 1639.5, "end": 1646.14, "text": " word semantics, right, that's not just kind of co-current patterns in text. But I agree, all of", "tokens": [50828, 1349, 4361, 45298, 11, 558, 11, 300, 311, 406, 445, 733, 295, 598, 12, 49827, 8294, 294, 2487, 13, 583, 286, 3986, 11, 439, 295, 51160], "temperature": 0.0, "avg_logprob": -0.12536843884892823, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0009826814057305455}, {"id": 342, "seek": 163022, "start": 1646.14, "end": 1651.18, "text": " that is up in the air and really exciting to see what will happen. Yeah, I know that some of the", "tokens": [51160, 300, 307, 493, 294, 264, 1988, 293, 534, 4670, 281, 536, 437, 486, 1051, 13, 865, 11, 286, 458, 300, 512, 295, 264, 51412], "temperature": 0.0, "avg_logprob": -0.12536843884892823, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0009826814057305455}, {"id": 343, "seek": 163022, "start": 1651.18, "end": 1658.06, "text": " earlier results from Linsen's lab suggest that at least restricted to equitably valid, you know,", "tokens": [51412, 3071, 3542, 490, 441, 1292, 268, 311, 2715, 3402, 300, 412, 1935, 20608, 281, 1267, 270, 1188, 7363, 11, 291, 458, 11, 51756], "temperature": 0.0, "avg_logprob": -0.12536843884892823, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0009826814057305455}, {"id": 344, "seek": 165806, "start": 1658.06, "end": 1663.4199999999998, "text": " training set sides, models seem to generalize, you know, linear rules for English, probably", "tokens": [50364, 3097, 992, 4881, 11, 5245, 1643, 281, 2674, 1125, 11, 291, 458, 11, 8213, 4474, 337, 3669, 11, 1391, 50632], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 345, "seek": 165806, "start": 1663.4199999999998, "end": 1667.5, "text": " has no question of formation, rather than the hierarchical rule, the correct hierarchical rule.", "tokens": [50632, 575, 572, 1168, 295, 11723, 11, 2831, 813, 264, 35250, 804, 4978, 11, 264, 3006, 35250, 804, 4978, 13, 50836], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 346, "seek": 165806, "start": 1667.5, "end": 1673.02, "text": " So I think there's a real sense in which, you know, the space of the correct syntactic prize and", "tokens": [50836, 407, 286, 519, 456, 311, 257, 957, 2020, 294, 597, 11, 291, 458, 11, 264, 1901, 295, 264, 3006, 23980, 19892, 12818, 293, 51112], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 347, "seek": 165806, "start": 1673.02, "end": 1677.8999999999999, "text": " inductive biases really is yet to be really settled on. But it seems, at least to me, pretty", "tokens": [51112, 31612, 488, 32152, 534, 307, 1939, 281, 312, 534, 14819, 322, 13, 583, 309, 2544, 11, 412, 1935, 281, 385, 11, 1238, 51356], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 348, "seek": 165806, "start": 1677.8999999999999, "end": 1682.22, "text": " obvious that there has to be some. So there's also some evidence that children in English,", "tokens": [51356, 6322, 300, 456, 575, 281, 312, 512, 13, 407, 456, 311, 611, 512, 4467, 300, 2227, 294, 3669, 11, 51572], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 349, "seek": 165806, "start": 1682.22, "end": 1686.7, "text": " going back to this frequency issue, that children in English sometimes spell out an intermediate", "tokens": [51572, 516, 646, 281, 341, 7893, 2734, 11, 300, 2227, 294, 3669, 2171, 9827, 484, 364, 19376, 51796], "temperature": 0.0, "avg_logprob": -0.15697188810868698, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.007719626184552908}, {"id": 350, "seek": 168670, "start": 1686.78, "end": 1692.38, "text": " copy of movement in the specified position of the lower complementizer position of a long-distance", "tokens": [50368, 5055, 295, 3963, 294, 264, 22206, 2535, 295, 264, 3126, 17103, 6545, 2535, 295, 257, 938, 12, 67, 20829, 50648], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 351, "seek": 168670, "start": 1692.38, "end": 1696.94, "text": " WH question. So there's a thesis by Thornton at some of the papers about this. So they say,", "tokens": [50648, 8183, 1168, 13, 407, 456, 311, 257, 22288, 538, 17777, 580, 266, 412, 512, 295, 264, 10577, 466, 341, 13, 407, 436, 584, 11, 50876], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 352, "seek": 168670, "start": 1696.94, "end": 1701.5800000000002, "text": " which person do you think who did that, rather than which person do you think did that? So this", "tokens": [50876, 597, 954, 360, 291, 519, 567, 630, 300, 11, 2831, 813, 597, 954, 360, 291, 519, 630, 300, 30, 407, 341, 51108], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 353, "seek": 168670, "start": 1701.5800000000002, "end": 1705.82, "text": " is an interesting, you know, missetting, because some languages do actually spell out these intermediate", "tokens": [51108, 307, 364, 1880, 11, 291, 458, 11, 1713, 302, 783, 11, 570, 512, 8650, 360, 767, 9827, 484, 613, 19376, 51320], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 354, "seek": 168670, "start": 1705.82, "end": 1710.94, "text": " copies, but English doesn't. So the kid makes the error in setting their grammar, but the frequency", "tokens": [51320, 14341, 11, 457, 3669, 1177, 380, 13, 407, 264, 1636, 1669, 264, 6713, 294, 3287, 641, 22317, 11, 457, 264, 7893, 51576], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 355, "seek": 168670, "start": 1710.94, "end": 1716.22, "text": " of the input is actually zero. So our mutual friend Gary Marcus also has an argument against", "tokens": [51576, 295, 264, 4846, 307, 767, 4018, 13, 407, 527, 16917, 1277, 13788, 26574, 611, 575, 364, 6770, 1970, 51840], "temperature": 0.0, "avg_logprob": -0.12628173828125, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.0032572681084275246}, {"id": 356, "seek": 171622, "start": 1716.22, "end": 1721.58, "text": " frequency-determining a kid's output. In the case of German noun plurals, a more regular form of the", "tokens": [50364, 7893, 12, 49136, 1760, 257, 1636, 311, 5598, 13, 682, 264, 1389, 295, 6521, 23307, 499, 374, 1124, 11, 257, 544, 3890, 1254, 295, 264, 50632], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 357, "seek": 171622, "start": 1721.58, "end": 1725.58, "text": " setting kind is preferred, not the frequent one. And there's lots of examples like this. So it's", "tokens": [50632, 3287, 733, 307, 16494, 11, 406, 264, 18004, 472, 13, 400, 456, 311, 3195, 295, 5110, 411, 341, 13, 407, 309, 311, 50832], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 358, "seek": 171622, "start": 1725.58, "end": 1730.3, "text": " sometimes claimed that subject-experiencer passives, where the subject is passively experiencing", "tokens": [50832, 2171, 12941, 300, 3983, 12, 3121, 610, 1053, 1776, 1320, 1539, 11, 689, 264, 3983, 307, 1320, 3413, 11139, 51068], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 359, "seek": 171622, "start": 1730.3, "end": 1735.1000000000001, "text": " something, are very delayed in kids in comprehension studies until around eight, because they're not", "tokens": [51068, 746, 11, 366, 588, 20268, 294, 2301, 294, 44991, 5313, 1826, 926, 3180, 11, 570, 436, 434, 406, 51308], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 360, "seek": 171622, "start": 1735.1000000000001, "end": 1740.6200000000001, "text": " very frequent in the input. But Ken Wexler and colleagues have gone through subject-experiencer", "tokens": [51308, 588, 18004, 294, 264, 4846, 13, 583, 8273, 492, 87, 1918, 293, 7734, 362, 2780, 807, 3983, 12, 3121, 610, 1053, 1776, 51584], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 361, "seek": 171622, "start": 1740.6200000000001, "end": 1745.9, "text": " WH questions like, who likes Mary? And they discovered that these are as infrequent in the", "tokens": [51584, 8183, 1651, 411, 11, 567, 5902, 6059, 30, 400, 436, 6941, 300, 613, 366, 382, 1536, 265, 28842, 294, 264, 51848], "temperature": 0.0, "avg_logprob": -0.13087504250662668, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.002296457765623927}, {"id": 362, "seek": 174590, "start": 1745.9, "end": 1750.7, "text": " input as subject-experiencer passives. But kids have no problem in comprehension studies of these", "tokens": [50364, 4846, 382, 3983, 12, 3121, 610, 1053, 1776, 1320, 1539, 13, 583, 2301, 362, 572, 1154, 294, 44991, 5313, 295, 613, 50604], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 363, "seek": 174590, "start": 1750.7, "end": 1756.46, "text": " questions. But they do have problems comprehending subject-experiencer verbal passives. So frequency", "tokens": [50604, 1651, 13, 583, 436, 360, 362, 2740, 10753, 2029, 3983, 12, 3121, 610, 1053, 1776, 24781, 1320, 1539, 13, 407, 7893, 50892], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 364, "seek": 174590, "start": 1756.46, "end": 1760.7800000000002, "text": " once again seems to be irrelevant. Or at least it's not explanatory, right? I guess it's not", "tokens": [50892, 1564, 797, 2544, 281, 312, 28682, 13, 1610, 412, 1935, 309, 311, 406, 9045, 4745, 11, 558, 30, 286, 2041, 309, 311, 406, 51108], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 365, "seek": 174590, "start": 1760.7800000000002, "end": 1765.42, "text": " explanatory with respect to theory building. So how can LMS help with these, you know,", "tokens": [51108, 9045, 4745, 365, 3104, 281, 5261, 2390, 13, 407, 577, 393, 441, 10288, 854, 365, 613, 11, 291, 458, 11, 51340], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 366, "seek": 174590, "start": 1765.42, "end": 1770.3000000000002, "text": " diverging cases when there's clearly something else going on besides frequency? So LMS, you know,", "tokens": [51340, 18558, 3249, 3331, 562, 456, 311, 4448, 746, 1646, 516, 322, 11868, 7893, 30, 407, 441, 10288, 11, 291, 458, 11, 51584], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 367, "seek": 174590, "start": 1770.3000000000002, "end": 1775.02, "text": " they seem to generalize just, again, going back to this issue of the cases that you have in your", "tokens": [51584, 436, 1643, 281, 2674, 1125, 445, 11, 797, 11, 516, 646, 281, 341, 2734, 295, 264, 3331, 300, 291, 362, 294, 428, 51820], "temperature": 0.0, "avg_logprob": -0.08467306217677156, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0006958423764444888}, {"id": 368, "seek": 177502, "start": 1775.02, "end": 1780.06, "text": " paper. You show that they generalize the structure of color screen ideas, which is obviously very", "tokens": [50364, 3035, 13, 509, 855, 300, 436, 2674, 1125, 264, 3877, 295, 2017, 2568, 3487, 11, 597, 307, 2745, 588, 50616], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 369, "seek": 177502, "start": 1780.06, "end": 1785.1, "text": " cool. But the positive stimulus has never really been about not being able to learn language", "tokens": [50616, 1627, 13, 583, 264, 3353, 21366, 575, 1128, 534, 668, 466, 406, 885, 1075, 281, 1466, 2856, 50868], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 370, "seek": 177502, "start": 1785.1, "end": 1789.5, "text": " statistically. I know you made that claim, right? But Chomsky's point in the 50s about statistical", "tokens": [50868, 36478, 13, 286, 458, 291, 1027, 300, 3932, 11, 558, 30, 583, 761, 4785, 4133, 311, 935, 294, 264, 2625, 82, 466, 22820, 51088], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 371, "seek": 177502, "start": 1789.5, "end": 1795.02, "text": " models of the day is not true of commercial LMS in 2023. And that's correct. But we can't use that", "tokens": [51088, 5245, 295, 264, 786, 307, 406, 2074, 295, 6841, 441, 10288, 294, 44377, 13, 400, 300, 311, 3006, 13, 583, 321, 393, 380, 764, 300, 51364], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 372, "seek": 177502, "start": 1795.02, "end": 1799.66, "text": " single point to undermine, you know, the entire generator enterprise. Chomsky's basic point was", "tokens": [51364, 2167, 935, 281, 39257, 11, 291, 458, 11, 264, 2302, 19265, 14132, 13, 761, 4785, 4133, 311, 3875, 935, 390, 51596], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 373, "seek": 177502, "start": 1799.66, "end": 1804.3, "text": " that you could have a grammatical structure wherein every background has zero frequency,", "tokens": [51596, 300, 291, 727, 362, 257, 17570, 267, 804, 3877, 43531, 633, 3678, 575, 4018, 7893, 11, 51828], "temperature": 0.0, "avg_logprob": -0.10357955676406178, "compression_ratio": 1.6656976744186047, "no_speech_prob": 0.0028774954844266176}, {"id": 374, "seek": 180430, "start": 1804.3, "end": 1808.7, "text": " and it also fails to provide clearly interpretable instructions to the conceptual interfaces.", "tokens": [50364, 293, 309, 611, 18199, 281, 2893, 4448, 7302, 712, 9415, 281, 264, 24106, 28416, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 375, "seek": 180430, "start": 1808.7, "end": 1812.7, "text": " So interfaces with other systems of the mind. So as you're showing your paper, GPT", "tokens": [50584, 407, 28416, 365, 661, 3652, 295, 264, 1575, 13, 407, 382, 291, 434, 4099, 428, 3035, 11, 26039, 51, 50784], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 376, "seek": 180430, "start": 1812.7, "end": 1818.78, "text": " mimics examples like full screen ideas. But, you know, again, this sentence yields over 150,000", "tokens": [50784, 12247, 1167, 5110, 411, 1577, 2568, 3487, 13, 583, 11, 291, 458, 11, 797, 11, 341, 8174, 32168, 670, 8451, 11, 1360, 51088], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 377, "seek": 180430, "start": 1818.78, "end": 1823.5, "text": " results on Google, and it's discussed extensively in the literature. It's able to mimic the fact", "tokens": [51088, 3542, 322, 3329, 11, 293, 309, 311, 7152, 32636, 294, 264, 10394, 13, 467, 311, 1075, 281, 31075, 264, 1186, 51324], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 378, "seek": 180430, "start": 1823.5, "end": 1827.34, "text": " that it can mimic this doesn't really tell us that much. At least we can't really say anything", "tokens": [51324, 300, 309, 393, 31075, 341, 1177, 380, 534, 980, 505, 300, 709, 13, 1711, 1935, 321, 393, 380, 534, 584, 1340, 51516], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 379, "seek": 180430, "start": 1827.34, "end": 1833.26, "text": " with much confidence. So, you know, Albeba behind University College Dublin has this quote recently,", "tokens": [51516, 365, 709, 6687, 13, 407, 11, 291, 458, 11, 967, 650, 4231, 2261, 3535, 6745, 42323, 575, 341, 6513, 3938, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1592159993720777, "compression_ratio": 1.6282420749279538, "no_speech_prob": 0.0013495167950168252}, {"id": 380, "seek": 183326, "start": 1833.26, "end": 1838.86, "text": " do not mistake your own vulnerability for an LMS intelligence. In fact, even Yanlacun wrote last", "tokens": [50364, 360, 406, 6146, 428, 1065, 24210, 337, 364, 441, 10288, 7599, 13, 682, 1186, 11, 754, 13633, 75, 326, 409, 4114, 1036, 50644], "temperature": 0.0, "avg_logprob": -0.18191502171178017, "compression_ratio": 1.5770491803278688, "no_speech_prob": 0.004761541727930307}, {"id": 381, "seek": 183326, "start": 1838.86, "end": 1844.46, "text": " year that critics are right to accuse LMS of being engaged in a kind of mimicry. And the example", "tokens": [50644, 1064, 300, 22503, 366, 558, 281, 43610, 441, 10288, 295, 885, 8237, 294, 257, 733, 295, 31075, 627, 13, 400, 264, 1365, 50924], "temperature": 0.0, "avg_logprob": -0.18191502171178017, "compression_ratio": 1.5770491803278688, "no_speech_prob": 0.004761541727930307}, {"id": 382, "seek": 183326, "start": 1844.46, "end": 1850.14, "text": " sentence is from chat GPT that you give in the paper. Actually, don't do a good job because,", "tokens": [50924, 8174, 307, 490, 5081, 26039, 51, 300, 291, 976, 294, 264, 3035, 13, 5135, 11, 500, 380, 360, 257, 665, 1691, 570, 11, 51208], "temperature": 0.0, "avg_logprob": -0.18191502171178017, "compression_ratio": 1.5770491803278688, "no_speech_prob": 0.004761541727930307}, {"id": 383, "seek": 183326, "start": 1850.14, "end": 1854.22, "text": " as you say, it's likely that, you know, meaningless language is rare in the training data, but they", "tokens": [51208, 382, 291, 584, 11, 309, 311, 3700, 300, 11, 291, 458, 11, 33232, 2856, 307, 5892, 294, 264, 3097, 1412, 11, 457, 436, 51412], "temperature": 0.0, "avg_logprob": -0.18191502171178017, "compression_ratio": 1.5770491803278688, "no_speech_prob": 0.004761541727930307}, {"id": 384, "seek": 183326, "start": 1854.22, "end": 1858.54, "text": " can either do it or they can't. But there's no middle ground in terms of giving us 10 examples", "tokens": [51412, 393, 2139, 360, 309, 420, 436, 393, 380, 13, 583, 456, 311, 572, 2808, 2727, 294, 2115, 295, 2902, 505, 1266, 5110, 51628], "temperature": 0.0, "avg_logprob": -0.18191502171178017, "compression_ratio": 1.5770491803278688, "no_speech_prob": 0.004761541727930307}, {"id": 385, "seek": 185854, "start": 1858.54, "end": 1864.86, "text": " like this. So, you have colourless green ideas, which are very different semantic objects from", "tokens": [50364, 411, 341, 13, 407, 11, 291, 362, 8267, 1832, 3092, 3487, 11, 597, 366, 588, 819, 47982, 6565, 490, 50680], "temperature": 0.0, "avg_logprob": -0.12257966128262607, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.05777447298169136}, {"id": 386, "seek": 185854, "start": 1864.86, "end": 1871.26, "text": " things like brown shimmering rabbits, white glittery bears, black shiny kangaroos, green", "tokens": [50680, 721, 411, 6292, 35088, 278, 38752, 11, 2418, 18620, 88, 17276, 11, 2211, 16997, 47898, 9708, 329, 11, 3092, 51000], "temperature": 0.0, "avg_logprob": -0.12257966128262607, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.05777447298169136}, {"id": 387, "seek": 185854, "start": 1871.26, "end": 1876.46, "text": " glittering monkeys, yellow dazzling lions, red shimmering elephants, right? These are all like", "tokens": [51000, 18620, 278, 29534, 11, 5566, 44078, 1688, 32564, 11, 2182, 35088, 278, 33015, 11, 558, 30, 1981, 366, 439, 411, 51260], "temperature": 0.0, "avg_logprob": -0.12257966128262607, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.05777447298169136}, {"id": 388, "seek": 185854, "start": 1876.46, "end": 1881.74, "text": " semantic, semantically weird and a bit strange, but they're still like legal structures. They're", "tokens": [51260, 47982, 11, 4361, 49505, 3657, 293, 257, 857, 5861, 11, 457, 436, 434, 920, 411, 5089, 9227, 13, 814, 434, 51524], "temperature": 0.0, "avg_logprob": -0.12257966128262607, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.05777447298169136}, {"id": 389, "seek": 188174, "start": 1881.74, "end": 1893.02, "text": " kind of meaningful semantic objects. Right? I just said, yeah. Yeah. I mean, so maybe I can", "tokens": [50364, 733, 295, 10995, 47982, 6565, 13, 1779, 30, 286, 445, 848, 11, 1338, 13, 865, 13, 286, 914, 11, 370, 1310, 286, 393, 50928], "temperature": 0.0, "avg_logprob": -0.17880654335021973, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.004822691436856985}, {"id": 390, "seek": 188174, "start": 1893.02, "end": 1898.54, "text": " respond to the first point first, right? So, you started off talking about these other", "tokens": [50928, 4196, 281, 264, 700, 935, 700, 11, 558, 30, 407, 11, 291, 1409, 766, 1417, 466, 613, 661, 51204], "temperature": 0.0, "avg_logprob": -0.17880654335021973, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.004822691436856985}, {"id": 391, "seek": 188174, "start": 1899.34, "end": 1905.66, "text": " kinds of acquisition patterns, which maybe don't map directly onto frequency. And I think it's", "tokens": [51244, 3685, 295, 21668, 8294, 11, 597, 1310, 500, 380, 4471, 3838, 3911, 7893, 13, 400, 286, 519, 309, 311, 51560], "temperature": 0.0, "avg_logprob": -0.17880654335021973, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.004822691436856985}, {"id": 392, "seek": 190566, "start": 1905.66, "end": 1913.5, "text": " actually a mistake to think that kind of modern learning models should be just based on frequency,", "tokens": [50364, 767, 257, 6146, 281, 519, 300, 733, 295, 4363, 2539, 5245, 820, 312, 445, 2361, 322, 7893, 11, 50756], "temperature": 0.0, "avg_logprob": -0.07187581616778706, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0034821867011487484}, {"id": 393, "seek": 190566, "start": 1913.5, "end": 1920.22, "text": " because they're clearly learning like pretty complicated families of rules or constructions", "tokens": [50756, 570, 436, 434, 4448, 2539, 411, 1238, 6179, 4466, 295, 4474, 420, 7690, 626, 51092], "temperature": 0.0, "avg_logprob": -0.07187581616778706, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0034821867011487484}, {"id": 394, "seek": 190566, "start": 1920.22, "end": 1927.26, "text": " or something. And I think it's very likely that when they're learning that, they're in some sense", "tokens": [51092, 420, 746, 13, 400, 286, 519, 309, 311, 588, 3700, 300, 562, 436, 434, 2539, 300, 11, 436, 434, 294, 512, 2020, 51444], "temperature": 0.0, "avg_logprob": -0.07187581616778706, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0034821867011487484}, {"id": 395, "seek": 190566, "start": 1927.26, "end": 1933.5, "text": " searching for a simple or parsimonious explanation of the data that they've seen, right? And how that", "tokens": [51444, 10808, 337, 257, 2199, 420, 21156, 25098, 851, 10835, 295, 264, 1412, 300, 436, 600, 1612, 11, 558, 30, 400, 577, 300, 51756], "temperature": 0.0, "avg_logprob": -0.07187581616778706, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0034821867011487484}, {"id": 396, "seek": 193350, "start": 1933.5, "end": 1940.94, "text": " caches out in a neural network is maybe complicated and depends on parameters and the specifics of", "tokens": [50364, 269, 13272, 484, 294, 257, 18161, 3209, 307, 1310, 6179, 293, 5946, 322, 9834, 293, 264, 28454, 295, 50736], "temperature": 0.0, "avg_logprob": -0.11938069848453298, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0010002566268667579}, {"id": 397, "seek": 193350, "start": 1940.94, "end": 1949.02, "text": " the learning algorithm and those kind of things. But I think it's, I'd suspect maybe that it's", "tokens": [50736, 264, 2539, 9284, 293, 729, 733, 295, 721, 13, 583, 286, 519, 309, 311, 11, 286, 1116, 9091, 1310, 300, 309, 311, 51140], "temperature": 0.0, "avg_logprob": -0.11938069848453298, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0010002566268667579}, {"id": 398, "seek": 193350, "start": 1949.02, "end": 1960.3, "text": " likely to be the case that they're learning over a complicated set of things, right? A complicated", "tokens": [51140, 3700, 281, 312, 264, 1389, 300, 436, 434, 2539, 670, 257, 6179, 992, 295, 721, 11, 558, 30, 316, 6179, 51704], "temperature": 0.0, "avg_logprob": -0.11938069848453298, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0010002566268667579}, {"id": 399, "seek": 196030, "start": 1960.54, "end": 1968.62, "text": " kind of family of rules and constructions. And that means I think that their generalizations,", "tokens": [50376, 733, 295, 1605, 295, 4474, 293, 7690, 626, 13, 400, 300, 1355, 286, 519, 300, 641, 2674, 14455, 11, 50780], "temperature": 0.0, "avg_logprob": -0.09052924134514549, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.006287813186645508}, {"id": 400, "seek": 196030, "start": 1968.62, "end": 1975.82, "text": " maybe like the examples of people that you gave, might be kind of discontinuous in the input,", "tokens": [50780, 1310, 411, 264, 5110, 295, 561, 300, 291, 2729, 11, 1062, 312, 733, 295, 31420, 12549, 294, 264, 4846, 11, 51140], "temperature": 0.0, "avg_logprob": -0.09052924134514549, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.006287813186645508}, {"id": 401, "seek": 196030, "start": 1975.82, "end": 1981.5, "text": " right? So, sometimes you could imagine seeing some strings which lead you to a grammar and", "tokens": [51140, 558, 30, 407, 11, 2171, 291, 727, 3811, 2577, 512, 13985, 597, 1477, 291, 281, 257, 22317, 293, 51424], "temperature": 0.0, "avg_logprob": -0.09052924134514549, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.006287813186645508}, {"id": 402, "seek": 196030, "start": 1981.5, "end": 1987.1, "text": " the simplest grammar of the data that you've seen so far is one which predicts an unseen string,", "tokens": [51424, 264, 22811, 22317, 295, 264, 1412, 300, 291, 600, 1612, 370, 1400, 307, 472, 597, 6069, 82, 364, 40608, 6798, 11, 51704], "temperature": 0.0, "avg_logprob": -0.09052924134514549, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.006287813186645508}, {"id": 403, "seek": 198710, "start": 1987.1, "end": 1995.1, "text": " right? And if that happens, then you'll be taking the data, learning a representation", "tokens": [50364, 558, 30, 400, 498, 300, 2314, 11, 550, 291, 603, 312, 1940, 264, 1412, 11, 2539, 257, 10290, 50764], "temperature": 0.0, "avg_logprob": -0.07514386428029914, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00043046707287430763}, {"id": 404, "seek": 198710, "start": 1995.1, "end": 2001.74, "text": " which generalizes in some novel unseen way so far, purely because that generalization is", "tokens": [50764, 597, 2674, 5660, 294, 512, 7613, 40608, 636, 370, 1400, 11, 17491, 570, 300, 2674, 2144, 307, 51096], "temperature": 0.0, "avg_logprob": -0.07514386428029914, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00043046707287430763}, {"id": 405, "seek": 198710, "start": 2001.74, "end": 2005.6599999999999, "text": " sort of the simplest account of the data that you've seen to date, right? I think that's sort of", "tokens": [51096, 1333, 295, 264, 22811, 2696, 295, 264, 1412, 300, 291, 600, 1612, 281, 4002, 11, 558, 30, 286, 519, 300, 311, 1333, 295, 51292], "temperature": 0.0, "avg_logprob": -0.07514386428029914, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00043046707287430763}, {"id": 406, "seek": 198710, "start": 2005.6599999999999, "end": 2010.2199999999998, "text": " what linguists try to do, right? Try to look at the data and come up with a theory of it,", "tokens": [51292, 437, 21766, 1751, 853, 281, 360, 11, 558, 30, 6526, 281, 574, 412, 264, 1412, 293, 808, 493, 365, 257, 5261, 295, 309, 11, 51520], "temperature": 0.0, "avg_logprob": -0.07514386428029914, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00043046707287430763}, {"id": 407, "seek": 198710, "start": 2010.2199999999998, "end": 2015.58, "text": " and then sometimes that theory predicts some new phenomenon, right? Or some new type of sentence.", "tokens": [51520, 293, 550, 2171, 300, 5261, 6069, 82, 512, 777, 14029, 11, 558, 30, 1610, 512, 777, 2010, 295, 8174, 13, 51788], "temperature": 0.0, "avg_logprob": -0.07514386428029914, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00043046707287430763}, {"id": 408, "seek": 201558, "start": 2016.46, "end": 2020.3, "text": " And so, if they're learning over as sufficiently rich space of theories,", "tokens": [50408, 400, 370, 11, 498, 436, 434, 2539, 670, 382, 31868, 4593, 1901, 295, 13667, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09538321311657245, "compression_ratio": 1.6079136690647482, "no_speech_prob": 0.0010646835435181856}, {"id": 409, "seek": 201558, "start": 2021.1799999999998, "end": 2027.02, "text": " then it wouldn't be unreasonable or unexpected for them to also show those kinds of patterns. Now,", "tokens": [50644, 550, 309, 2759, 380, 312, 41730, 420, 13106, 337, 552, 281, 611, 855, 729, 3685, 295, 8294, 13, 823, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09538321311657245, "compression_ratio": 1.6079136690647482, "no_speech_prob": 0.0010646835435181856}, {"id": 410, "seek": 201558, "start": 2027.02, "end": 2033.4199999999998, "text": " whether they do or not I think is still an open empirical question, right? Because we have to", "tokens": [50936, 1968, 436, 360, 420, 406, 286, 519, 307, 920, 364, 1269, 31886, 1168, 11, 558, 30, 1436, 321, 362, 281, 51256], "temperature": 0.0, "avg_logprob": -0.09538321311657245, "compression_ratio": 1.6079136690647482, "no_speech_prob": 0.0010646835435181856}, {"id": 411, "seek": 201558, "start": 2033.4199999999998, "end": 2038.1399999999999, "text": " train them on small amounts of data and test their generalizations and these kind of things.", "tokens": [51256, 3847, 552, 322, 1359, 11663, 295, 1412, 293, 1500, 641, 2674, 14455, 293, 613, 733, 295, 721, 13, 51492], "temperature": 0.0, "avg_logprob": -0.09538321311657245, "compression_ratio": 1.6079136690647482, "no_speech_prob": 0.0010646835435181856}, {"id": 412, "seek": 201558, "start": 2038.1399999999999, "end": 2045.1, "text": " But I don't think like just the fact that humans do things which are not purely based on", "tokens": [51492, 583, 286, 500, 380, 519, 411, 445, 264, 1186, 300, 6255, 360, 721, 597, 366, 406, 17491, 2361, 322, 51840], "temperature": 0.0, "avg_logprob": -0.09538321311657245, "compression_ratio": 1.6079136690647482, "no_speech_prob": 0.0010646835435181856}, {"id": 413, "seek": 204510, "start": 2045.1, "end": 2049.02, "text": " frequency is any evidence at all, either way, right? Because once you're learning over rich", "tokens": [50364, 7893, 307, 604, 4467, 412, 439, 11, 2139, 636, 11, 558, 30, 1436, 1564, 291, 434, 2539, 670, 4593, 50560], "temperature": 0.0, "avg_logprob": -0.1113955866206776, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.001409308984875679}, {"id": 414, "seek": 204510, "start": 2049.02, "end": 2056.14, "text": " and interesting classes of theories, then that is the expected behavior. Actually, I had a paper", "tokens": [50560, 293, 1880, 5359, 295, 13667, 11, 550, 300, 307, 264, 5176, 5223, 13, 5135, 11, 286, 632, 257, 3035, 50916], "temperature": 0.0, "avg_logprob": -0.1113955866206776, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.001409308984875679}, {"id": 415, "seek": 204510, "start": 2056.14, "end": 2065.9, "text": " about a year ago that I think you're familiar with, Yang and Pianta dosi, where we were looking at", "tokens": [50916, 466, 257, 1064, 2057, 300, 286, 519, 291, 434, 4963, 365, 11, 11978, 293, 430, 5798, 64, 4491, 72, 11, 689, 321, 645, 1237, 412, 51404], "temperature": 0.0, "avg_logprob": -0.1113955866206776, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.001409308984875679}, {"id": 416, "seek": 204510, "start": 2067.74, "end": 2073.1, "text": " kind of what happens when you give a program learning model strings from different formal", "tokens": [51496, 733, 295, 437, 2314, 562, 291, 976, 257, 1461, 2539, 2316, 13985, 490, 819, 9860, 51764], "temperature": 0.0, "avg_logprob": -0.1113955866206776, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.001409308984875679}, {"id": 417, "seek": 207310, "start": 2073.1, "end": 2081.18, "text": " languages. So think of like giving a general model just 10 or 20 maybe simple strings that", "tokens": [50364, 8650, 13, 407, 519, 295, 411, 2902, 257, 2674, 2316, 445, 1266, 420, 945, 1310, 2199, 13985, 300, 50768], "temperature": 0.0, "avg_logprob": -0.08303360711960565, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0012839073315262794}, {"id": 418, "seek": 207310, "start": 2081.18, "end": 2087.18, "text": " obey some pattern and then asking it to find a program which can explain that data, which often", "tokens": [50768, 19297, 512, 5102, 293, 550, 3365, 309, 281, 915, 257, 1461, 597, 393, 2903, 300, 1412, 11, 597, 2049, 51068], "temperature": 0.0, "avg_logprob": -0.08303360711960565, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0012839073315262794}, {"id": 419, "seek": 207310, "start": 2087.18, "end": 2094.94, "text": " means finding some way of kind of programmatically writing down the pattern in the strings. And", "tokens": [51068, 1355, 5006, 512, 636, 295, 733, 295, 37648, 5030, 3579, 760, 264, 5102, 294, 264, 13985, 13, 400, 51456], "temperature": 0.0, "avg_logprob": -0.08303360711960565, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0012839073315262794}, {"id": 420, "seek": 207310, "start": 2094.94, "end": 2101.8199999999997, "text": " in that figure, we have a paper which is really relevant to this point where the generalizations", "tokens": [51456, 294, 300, 2573, 11, 321, 362, 257, 3035, 597, 307, 534, 7340, 281, 341, 935, 689, 264, 2674, 14455, 51800], "temperature": 0.0, "avg_logprob": -0.08303360711960565, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0012839073315262794}, {"id": 421, "seek": 210182, "start": 2101.82, "end": 2107.82, "text": " that that kind of model makes are I think kind of qualitatively like the ones you're describing", "tokens": [50364, 300, 300, 733, 295, 2316, 1669, 366, 286, 519, 733, 295, 31312, 356, 411, 264, 2306, 291, 434, 16141, 50664], "temperature": 0.0, "avg_logprob": -0.06501828488849458, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004582144902087748}, {"id": 422, "seek": 210182, "start": 2107.82, "end": 2113.9, "text": " for people, right? Where you can give them a small amount of data and it will predict unseen", "tokens": [50664, 337, 561, 11, 558, 30, 2305, 291, 393, 976, 552, 257, 1359, 2372, 295, 1412, 293, 309, 486, 6069, 40608, 50968], "temperature": 0.0, "avg_logprob": -0.06501828488849458, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004582144902087748}, {"id": 423, "seek": 210182, "start": 2113.9, "end": 2119.34, "text": " strings with very high probability, even though there's zero frequency in the training input,", "tokens": [50968, 13985, 365, 588, 1090, 8482, 11, 754, 1673, 456, 311, 4018, 7893, 294, 264, 3097, 4846, 11, 51240], "temperature": 0.0, "avg_logprob": -0.06501828488849458, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004582144902087748}, {"id": 424, "seek": 210182, "start": 2119.34, "end": 2124.46, "text": " right? And the reason it does that is that often the most concise computational description of the", "tokens": [51240, 558, 30, 400, 264, 1778, 309, 775, 300, 307, 300, 2049, 264, 881, 44882, 28270, 3855, 295, 264, 51496], "temperature": 0.0, "avg_logprob": -0.06501828488849458, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004582144902087748}, {"id": 425, "seek": 212446, "start": 2124.46, "end": 2132.78, "text": " data that you've seen is one that predicts some particular new unseen output. So that model is", "tokens": [50364, 1412, 300, 291, 600, 1612, 307, 472, 300, 6069, 82, 512, 1729, 777, 40608, 5598, 13, 407, 300, 2316, 307, 50780], "temperature": 0.0, "avg_logprob": -0.08825958316976373, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.010984028689563274}, {"id": 426, "seek": 212446, "start": 2132.78, "end": 2139.02, "text": " essentially an implementation of the kind of Chater and Vitani program learning idea that I", "tokens": [50780, 4476, 364, 11420, 295, 264, 733, 295, 761, 771, 293, 22463, 3782, 1461, 2539, 1558, 300, 286, 51092], "temperature": 0.0, "avg_logprob": -0.08825958316976373, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.010984028689563274}, {"id": 427, "seek": 212446, "start": 2139.02, "end": 2144.06, "text": " brought up earlier. But it's one that I think, you know, if you think about in the context of", "tokens": [51092, 3038, 493, 3071, 13, 583, 309, 311, 472, 300, 286, 519, 11, 291, 458, 11, 498, 291, 519, 466, 294, 264, 4319, 295, 51344], "temperature": 0.0, "avg_logprob": -0.08825958316976373, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.010984028689563274}, {"id": 428, "seek": 212446, "start": 2144.06, "end": 2149.82, "text": " these arguments of kids saying unusual or unexpected things, like that is predicted by all of these", "tokens": [51344, 613, 12869, 295, 2301, 1566, 10901, 420, 13106, 721, 11, 411, 300, 307, 19147, 538, 439, 295, 613, 51632], "temperature": 0.0, "avg_logprob": -0.08825958316976373, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.010984028689563274}, {"id": 429, "seek": 214982, "start": 2149.82, "end": 2155.1800000000003, "text": " kinds of accounts, right? Because as long as these things are effectively comparing an interesting", "tokens": [50364, 3685, 295, 9402, 11, 558, 30, 1436, 382, 938, 382, 613, 721, 366, 8659, 15763, 364, 1880, 50632], "temperature": 0.0, "avg_logprob": -0.13408135865864002, "compression_ratio": 1.552, "no_speech_prob": 0.0024670003913342953}, {"id": 430, "seek": 214982, "start": 2155.1800000000003, "end": 2163.1800000000003, "text": " space of grammars, then they'll show that kind of behavior, I think. Yeah. So, okay. So I guess,", "tokens": [50632, 1901, 295, 17570, 685, 11, 550, 436, 603, 855, 300, 733, 295, 5223, 11, 286, 519, 13, 865, 13, 407, 11, 1392, 13, 407, 286, 2041, 11, 51032], "temperature": 0.0, "avg_logprob": -0.13408135865864002, "compression_ratio": 1.552, "no_speech_prob": 0.0024670003913342953}, {"id": 431, "seek": 214982, "start": 2163.1800000000003, "end": 2171.26, "text": " you know, the argument would be that, at least from the gender perspective, syntax is functioning", "tokens": [51032, 291, 458, 11, 264, 6770, 576, 312, 300, 11, 412, 1935, 490, 264, 7898, 4585, 11, 28431, 307, 18483, 51436], "temperature": 0.0, "avg_logprob": -0.13408135865864002, "compression_ratio": 1.552, "no_speech_prob": 0.0024670003913342953}, {"id": 432, "seek": 214982, "start": 2171.26, "end": 2176.78, "text": " separately, but it still maps to semantics, it informs pragmatics, right? So in the minimalist", "tokens": [51436, 14759, 11, 457, 309, 920, 11317, 281, 4361, 45298, 11, 309, 45320, 33394, 15677, 1167, 11, 558, 30, 407, 294, 264, 50192, 51712], "temperature": 0.0, "avg_logprob": -0.13408135865864002, "compression_ratio": 1.552, "no_speech_prob": 0.0024670003913342953}, {"id": 433, "seek": 217678, "start": 2176.78, "end": 2181.7400000000002, "text": " program, syntax is obviously minimalist, it's very small, it's just a linearization and labeling,", "tokens": [50364, 1461, 11, 28431, 307, 2745, 50192, 11, 309, 311, 588, 1359, 11, 309, 311, 445, 257, 8213, 2144, 293, 40244, 11, 50612], "temperature": 0.0, "avg_logprob": -0.14924037562007397, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.0028485918883234262}, {"id": 434, "seek": 217678, "start": 2181.7400000000002, "end": 2186.7000000000003, "text": " they're the two only operations, you have a linearization algorithm to central motor systems", "tokens": [50612, 436, 434, 264, 732, 787, 7705, 11, 291, 362, 257, 8213, 2144, 9284, 281, 5777, 5932, 3652, 50860], "temperature": 0.0, "avg_logprob": -0.14924037562007397, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.0028485918883234262}, {"id": 435, "seek": 217678, "start": 2186.7000000000003, "end": 2193.1800000000003, "text": " and some kind of categorization algorithm at the conceptual systems. So Chomsky's architecture", "tokens": [50860, 293, 512, 733, 295, 19250, 2144, 9284, 412, 264, 24106, 3652, 13, 407, 761, 4785, 4133, 311, 9482, 51184], "temperature": 0.0, "avg_logprob": -0.14924037562007397, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.0028485918883234262}, {"id": 436, "seek": 217678, "start": 2193.1800000000003, "end": 2197.5, "text": " is kind of reliant on the process of mapping syntax to semantics, right? It's form meaning", "tokens": [51184, 307, 733, 295, 1039, 5798, 322, 264, 1399, 295, 18350, 28431, 281, 4361, 45298, 11, 558, 30, 467, 311, 1254, 3620, 51400], "temperature": 0.0, "avg_logprob": -0.14924037562007397, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.0028485918883234262}, {"id": 437, "seek": 217678, "start": 2197.5, "end": 2203.02, "text": " regulation, it's not just structure, and it's not just meaning. So LMS don't really have this mapping", "tokens": [51400, 15062, 11, 309, 311, 406, 445, 3877, 11, 293, 309, 311, 406, 445, 3620, 13, 407, 441, 10288, 500, 380, 534, 362, 341, 18350, 51676], "temperature": 0.0, "avg_logprob": -0.14924037562007397, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.0028485918883234262}, {"id": 438, "seek": 220302, "start": 2203.02, "end": 2207.42, "text": " process, right? Like, where's the mapping to semantics? And if there is a mapping, what does", "tokens": [50364, 1399, 11, 558, 30, 1743, 11, 689, 311, 264, 18350, 281, 4361, 45298, 30, 400, 498, 456, 307, 257, 18350, 11, 437, 775, 50584], "temperature": 0.0, "avg_logprob": -0.14109679630824498, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.02384537272155285}, {"id": 439, "seek": 220302, "start": 2207.42, "end": 2212.46, "text": " the mapping process look like? What are the properties of its semantics? What are the properties of", "tokens": [50584, 264, 18350, 1399, 574, 411, 30, 708, 366, 264, 7221, 295, 1080, 4361, 45298, 30, 708, 366, 264, 7221, 295, 50836], "temperature": 0.0, "avg_logprob": -0.14109679630824498, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.02384537272155285}, {"id": 440, "seek": 220302, "start": 2212.46, "end": 2216.46, "text": " the semantics placed on their own sets of constraints on the mapping process? Like,", "tokens": [50836, 264, 4361, 45298, 7074, 322, 641, 1065, 6352, 295, 18491, 322, 264, 18350, 1399, 30, 1743, 11, 51036], "temperature": 0.0, "avg_logprob": -0.14109679630824498, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.02384537272155285}, {"id": 441, "seek": 220302, "start": 2216.46, "end": 2222.22, "text": " they do for natural language? Do these kind of constraints inform each other? Is they kind of", "tokens": [51036, 436, 360, 337, 3303, 2856, 30, 1144, 613, 733, 295, 18491, 1356, 1184, 661, 30, 1119, 436, 733, 295, 51324], "temperature": 0.0, "avg_logprob": -0.14109679630824498, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.02384537272155285}, {"id": 442, "seek": 220302, "start": 2222.22, "end": 2228.22, "text": " a back and forth process? Like, LMS don't really seem to describe this form meaning pairing,", "tokens": [51324, 257, 646, 293, 5220, 1399, 30, 1743, 11, 441, 10288, 500, 380, 534, 1643, 281, 6786, 341, 1254, 3620, 32735, 11, 51624], "temperature": 0.0, "avg_logprob": -0.14109679630824498, "compression_ratio": 1.9132231404958677, "no_speech_prob": 0.02384537272155285}, {"id": 443, "seek": 222822, "start": 2228.9399999999996, "end": 2236.7, "text": " which means which strings, for example, right? Sorry, are you saying that they don't have semantics", "tokens": [50400, 597, 1355, 597, 13985, 11, 337, 1365, 11, 558, 30, 4919, 11, 366, 291, 1566, 300, 436, 500, 380, 362, 4361, 45298, 50788], "temperature": 0.0, "avg_logprob": -0.13502696436694545, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0038792395498603582}, {"id": 444, "seek": 222822, "start": 2236.7, "end": 2242.3799999999997, "text": " at all? Or are you saying that there's just not a clear delineation between how the structures", "tokens": [50788, 412, 439, 30, 1610, 366, 291, 1566, 300, 456, 311, 445, 406, 257, 1850, 1103, 533, 399, 1296, 577, 264, 9227, 51072], "temperature": 0.0, "avg_logprob": -0.13502696436694545, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0038792395498603582}, {"id": 445, "seek": 222822, "start": 2242.3799999999997, "end": 2246.8599999999997, "text": " get mapped onto the semantics? Yeah, the latter, right? So they clearly have some, potentially", "tokens": [51072, 483, 33318, 3911, 264, 4361, 45298, 30, 865, 11, 264, 18481, 11, 558, 30, 407, 436, 4448, 362, 512, 11, 7263, 51296], "temperature": 0.0, "avg_logprob": -0.13502696436694545, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0038792395498603582}, {"id": 446, "seek": 222822, "start": 2246.8599999999997, "end": 2250.62, "text": " some kind of semantics. I know you've argued for conceptual role theory being relevant here, right?", "tokens": [51296, 512, 733, 295, 4361, 45298, 13, 286, 458, 291, 600, 20219, 337, 24106, 3090, 5261, 885, 7340, 510, 11, 558, 30, 51484], "temperature": 0.0, "avg_logprob": -0.13502696436694545, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0038792395498603582}, {"id": 447, "seek": 222822, "start": 2250.62, "end": 2254.14, "text": " The rest of it is maybe a little bit more mysterious, but the actual, so in linguistics", "tokens": [51484, 440, 1472, 295, 309, 307, 1310, 257, 707, 857, 544, 13831, 11, 457, 264, 3539, 11, 370, 294, 21766, 6006, 51660], "temperature": 0.0, "avg_logprob": -0.13502696436694545, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0038792395498603582}, {"id": 448, "seek": 225414, "start": 2254.46, "end": 2259.02, "text": " there's a theory of the mapping process itself, it's explicit, and you can see it in action,", "tokens": [50380, 456, 311, 257, 5261, 295, 264, 18350, 1399, 2564, 11, 309, 311, 13691, 11, 293, 291, 393, 536, 309, 294, 3069, 11, 50608], "temperature": 0.0, "avg_logprob": -0.14264712510285554, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.0009993808344006538}, {"id": 449, "seek": 225414, "start": 2259.02, "end": 2262.54, "text": " and you can test different theories of it in psycholinguistic models and what have you.", "tokens": [50608, 293, 291, 393, 1500, 819, 13667, 295, 309, 294, 4681, 401, 7050, 3142, 5245, 293, 437, 362, 291, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14264712510285554, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.0009993808344006538}, {"id": 450, "seek": 225414, "start": 2262.54, "end": 2268.22, "text": " The actual regulation, the kind of constrained ambiguity, ambiguity in the sense of one word,", "tokens": [50784, 440, 3539, 15062, 11, 264, 733, 295, 38901, 46519, 11, 46519, 294, 264, 2020, 295, 472, 1349, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14264712510285554, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.0009993808344006538}, {"id": 451, "seek": 225414, "start": 2268.22, "end": 2271.98, "text": " multiple meanings, or one structure, multiple interpretations, etc, right?", "tokens": [51068, 3866, 28138, 11, 420, 472, 3877, 11, 3866, 37547, 11, 5183, 11, 558, 30, 51256], "temperature": 0.0, "avg_logprob": -0.14264712510285554, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.0009993808344006538}, {"id": 452, "seek": 225414, "start": 2273.3399999999997, "end": 2278.7, "text": " Yeah, I mean, if you think they have semantics, then I think they have to have a mapping from", "tokens": [51324, 865, 11, 286, 914, 11, 498, 291, 519, 436, 362, 4361, 45298, 11, 550, 286, 519, 436, 362, 281, 362, 257, 18350, 490, 51592], "temperature": 0.0, "avg_logprob": -0.14264712510285554, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.0009993808344006538}, {"id": 453, "seek": 227870, "start": 2278.7, "end": 2284.62, "text": " the syntax to the semantics. I agree, it's not as like, nobody really understands how they're", "tokens": [50364, 264, 28431, 281, 264, 4361, 45298, 13, 286, 3986, 11, 309, 311, 406, 382, 411, 11, 5079, 534, 15146, 577, 436, 434, 50660], "temperature": 0.0, "avg_logprob": -0.11784767150878907, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.03728465363383293}, {"id": 454, "seek": 227870, "start": 2284.62, "end": 2291.8199999999997, "text": " working on any deep level, right? So I agree, it's not as clear as, say, in generative syntax and", "tokens": [50660, 1364, 322, 604, 2452, 1496, 11, 558, 30, 407, 286, 3986, 11, 309, 311, 406, 382, 1850, 382, 11, 584, 11, 294, 1337, 1166, 28431, 293, 51020], "temperature": 0.0, "avg_logprob": -0.11784767150878907, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.03728465363383293}, {"id": 455, "seek": 227870, "start": 2291.8199999999997, "end": 2299.18, "text": " semantics, right, where you kind of write down the rules of composition and can derive a compositional", "tokens": [51020, 4361, 45298, 11, 558, 11, 689, 291, 733, 295, 2464, 760, 264, 4474, 295, 12686, 293, 393, 28446, 257, 10199, 2628, 51388], "temperature": 0.0, "avg_logprob": -0.11784767150878907, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.03728465363383293}, {"id": 456, "seek": 227870, "start": 2299.18, "end": 2304.62, "text": " meaning from a sentence from the component parts or something, right? Like, that's not how they're", "tokens": [51388, 3620, 490, 257, 8174, 490, 264, 6542, 3166, 420, 746, 11, 558, 30, 1743, 11, 300, 311, 406, 577, 436, 434, 51660], "temperature": 0.0, "avg_logprob": -0.11784767150878907, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.03728465363383293}, {"id": 457, "seek": 230462, "start": 2304.62, "end": 2310.94, "text": " working, right? But I just, I wouldn't take for granted that it has to be like that. Like,", "tokens": [50364, 1364, 11, 558, 30, 583, 286, 445, 11, 286, 2759, 380, 747, 337, 12344, 300, 309, 575, 281, 312, 411, 300, 13, 1743, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09478153066432222, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.003272012108936906}, {"id": 458, "seek": 230462, "start": 2312.94, "end": 2318.46, "text": " it could be that how they're working is actually how we work, right? That everything is represented", "tokens": [50780, 309, 727, 312, 300, 577, 436, 434, 1364, 307, 767, 577, 321, 589, 11, 558, 30, 663, 1203, 307, 10379, 51056], "temperature": 0.0, "avg_logprob": -0.09478153066432222, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.003272012108936906}, {"id": 459, "seek": 230462, "start": 2318.46, "end": 2325.1, "text": " in some high-dimensional vector space, and there's some complicated way in which that vector semantics", "tokens": [51056, 294, 512, 1090, 12, 18759, 8062, 1901, 11, 293, 456, 311, 512, 6179, 636, 294, 597, 300, 8062, 4361, 45298, 51388], "temperature": 0.0, "avg_logprob": -0.09478153066432222, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.003272012108936906}, {"id": 460, "seek": 230462, "start": 2325.1, "end": 2333.5, "text": " gets updated with each additional word or whatever in a linguistic stream. But like, I think it's", "tokens": [51388, 2170, 10588, 365, 1184, 4497, 1349, 420, 2035, 294, 257, 43002, 4309, 13, 583, 411, 11, 286, 519, 309, 311, 51808], "temperature": 0.0, "avg_logprob": -0.09478153066432222, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.003272012108936906}, {"id": 461, "seek": 233350, "start": 2333.5, "end": 2338.54, "text": " clear that they have some kind of representation of the semantics of a sentence, right? Like,", "tokens": [50364, 1850, 300, 436, 362, 512, 733, 295, 10290, 295, 264, 4361, 45298, 295, 257, 8174, 11, 558, 30, 1743, 11, 50616], "temperature": 0.0, "avg_logprob": -0.10472721181890016, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0033752599265426397}, {"id": 462, "seek": 233350, "start": 2338.54, "end": 2343.42, "text": " they can answer questions, for example, at least approximately. I mean, it's not perfect, but", "tokens": [50616, 436, 393, 1867, 1651, 11, 337, 1365, 11, 412, 1935, 10447, 13, 286, 914, 11, 309, 311, 406, 2176, 11, 457, 50860], "temperature": 0.0, "avg_logprob": -0.10472721181890016, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0033752599265426397}, {"id": 463, "seek": 233350, "start": 2344.38, "end": 2351.26, "text": " it's not like a n-gram model or something, right? Which really doesn't have semantics. So I think", "tokens": [50908, 309, 311, 406, 411, 257, 297, 12, 1342, 2316, 420, 746, 11, 558, 30, 3013, 534, 1177, 380, 362, 4361, 45298, 13, 407, 286, 519, 51252], "temperature": 0.0, "avg_logprob": -0.10472721181890016, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0033752599265426397}, {"id": 464, "seek": 233350, "start": 2351.26, "end": 2360.62, "text": " that they're definitely representing semantics and, you know, updating that as they process", "tokens": [51252, 300, 436, 434, 2138, 13460, 4361, 45298, 293, 11, 291, 458, 11, 25113, 300, 382, 436, 1399, 51720], "temperature": 0.0, "avg_logprob": -0.10472721181890016, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0033752599265426397}, {"id": 465, "seek": 236062, "start": 2360.62, "end": 2366.14, "text": " language, it just happens not to look like these other formal theories. And I guess, I don't see", "tokens": [50364, 2856, 11, 309, 445, 2314, 406, 281, 574, 411, 613, 661, 9860, 13667, 13, 400, 286, 2041, 11, 286, 500, 380, 536, 50640], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 466, "seek": 236062, "start": 2366.14, "end": 2369.9, "text": " why that's a problem, right? Like, those other formal theories could just be, you know, poor", "tokens": [50640, 983, 300, 311, 257, 1154, 11, 558, 30, 1743, 11, 729, 661, 9860, 13667, 727, 445, 312, 11, 291, 458, 11, 4716, 50828], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 467, "seek": 236062, "start": 2369.9, "end": 2375.42, "text": " approximations or just totally wrong, right? Yeah, yeah, no, no, totally, totally. I mean,", "tokens": [50828, 8542, 763, 420, 445, 3879, 2085, 11, 558, 30, 865, 11, 1338, 11, 572, 11, 572, 11, 3879, 11, 3879, 13, 286, 914, 11, 51104], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 468, "seek": 236062, "start": 2375.42, "end": 2379.58, "text": " there's also ways in which some of the formal formal theories in semantics are already", "tokens": [51104, 456, 311, 611, 2098, 294, 597, 512, 295, 264, 9860, 9860, 13667, 294, 4361, 45298, 366, 1217, 51312], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 469, "seek": 236062, "start": 2379.58, "end": 2383.18, "text": " potentially compatible. We've got some of these things are doing, right? So another way to think", "tokens": [51312, 7263, 18218, 13, 492, 600, 658, 512, 295, 613, 721, 366, 884, 11, 558, 30, 407, 1071, 636, 281, 519, 51492], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 470, "seek": 236062, "start": 2383.18, "end": 2388.94, "text": " about this is, you know, LMS are, well, LMS are compression algorithms, but natural language", "tokens": [51492, 466, 341, 307, 11, 291, 458, 11, 441, 10288, 366, 11, 731, 11, 441, 10288, 366, 19355, 14642, 11, 457, 3303, 2856, 51780], "temperature": 0.0, "avg_logprob": -0.10933925412225386, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.004748223815113306}, {"id": 471, "seek": 238894, "start": 2388.94, "end": 2394.46, "text": " understanding is kind of more about decompression. It's disambiguating, meaning x, out of meanings,", "tokens": [50364, 3701, 307, 733, 295, 544, 466, 22867, 2775, 13, 467, 311, 717, 2173, 16397, 990, 11, 3620, 2031, 11, 484, 295, 28138, 11, 50640], "temperature": 0.0, "avg_logprob": -0.12239150418580033, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.0018397130770608783}, {"id": 472, "seek": 238894, "start": 2394.46, "end": 2398.86, "text": " x, y, z. It's all about making inferences about, you know, meta relations between concepts that", "tokens": [50640, 2031, 11, 288, 11, 710, 13, 467, 311, 439, 466, 1455, 13596, 2667, 466, 11, 291, 458, 11, 19616, 2299, 1296, 10392, 300, 50860], "temperature": 0.0, "avg_logprob": -0.12239150418580033, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.0018397130770608783}, {"id": 473, "seek": 238894, "start": 2398.86, "end": 2404.06, "text": " are not in the training data. So some examples that Millie Mitchell gives are things like on top of,", "tokens": [50860, 366, 406, 294, 264, 3097, 1412, 13, 407, 512, 5110, 300, 7190, 414, 27582, 2709, 366, 721, 411, 322, 1192, 295, 11, 51120], "temperature": 0.0, "avg_logprob": -0.12239150418580033, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.0018397130770608783}, {"id": 474, "seek": 238894, "start": 2404.06, "end": 2409.5, "text": " you know, she's on top of a game, it's on top of the box, all of these kind of vary with context.", "tokens": [51120, 291, 458, 11, 750, 311, 322, 1192, 295, 257, 1216, 11, 309, 311, 322, 1192, 295, 264, 2424, 11, 439, 295, 613, 733, 295, 10559, 365, 4319, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12239150418580033, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.0018397130770608783}, {"id": 475, "seek": 238894, "start": 2409.5, "end": 2413.18, "text": " So there's a lot of other things that are going on, right? And I think you discussed some of the", "tokens": [51392, 407, 456, 311, 257, 688, 295, 661, 721, 300, 366, 516, 322, 11, 558, 30, 400, 286, 519, 291, 7152, 512, 295, 264, 51576], "temperature": 0.0, "avg_logprob": -0.12239150418580033, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.0018397130770608783}, {"id": 476, "seek": 241318, "start": 2413.2599999999998, "end": 2419.5, "text": " examples on your paper. So, you know, but the fact that the language is still not, at least,", "tokens": [50368, 5110, 322, 428, 3035, 13, 407, 11, 291, 458, 11, 457, 264, 1186, 300, 264, 2856, 307, 920, 406, 11, 412, 1935, 11, 50680], "temperature": 0.0, "avg_logprob": -0.2592759510827443, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.021334433928132057}, {"id": 477, "seek": 241318, "start": 2419.5, "end": 2424.8599999999997, "text": " again, under this theory of language, it's not about string generation. It's about this form,", "tokens": [50680, 797, 11, 833, 341, 5261, 295, 2856, 11, 309, 311, 406, 466, 6798, 5125, 13, 467, 311, 466, 341, 1254, 11, 50948], "temperature": 0.0, "avg_logprob": -0.2592759510827443, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.021334433928132057}, {"id": 478, "seek": 241318, "start": 2424.8599999999997, "end": 2429.2599999999998, "text": " meaning, pairing machine. So some semantics in the generative tradition, even think all the", "tokens": [50948, 3620, 11, 32735, 3479, 13, 407, 512, 4361, 45298, 294, 264, 1337, 1166, 6994, 11, 754, 519, 439, 264, 51168], "temperature": 0.0, "avg_logprob": -0.2592759510827443, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.021334433928132057}, {"id": 479, "seek": 241318, "start": 2429.2599999999998, "end": 2434.62, "text": " rest of semantics is just and, right? So both Rasky's conjunctivist theories semantics is that", "tokens": [51168, 1472, 295, 4361, 45298, 307, 445, 293, 11, 558, 30, 407, 1293, 497, 3863, 88, 311, 18244, 349, 592, 468, 13667, 4361, 45298, 307, 300, 51436], "temperature": 0.0, "avg_logprob": -0.2592759510827443, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.021334433928132057}, {"id": 480, "seek": 241318, "start": 2434.62, "end": 2440.7799999999997, "text": " human semantics is just and that's it. Which, again, is very simple, elegant. It's, it's,", "tokens": [51436, 1952, 4361, 45298, 307, 445, 293, 300, 311, 309, 13, 3013, 11, 797, 11, 307, 588, 2199, 11, 21117, 13, 467, 311, 11, 309, 311, 11, 51744], "temperature": 0.0, "avg_logprob": -0.2592759510827443, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.021334433928132057}, {"id": 481, "seek": 244078, "start": 2440.78, "end": 2446.3, "text": " it's interpretable. It's compatible with other things that, you know, are maybe going on in your", "tokens": [50364, 309, 311, 7302, 712, 13, 467, 311, 18218, 365, 661, 721, 300, 11, 291, 458, 11, 366, 1310, 516, 322, 294, 428, 50640], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 482, "seek": 244078, "start": 2446.3, "end": 2450.3, "text": " neck of the woods, right? But regardless, it's still, you know, natural language is still more", "tokens": [50640, 6189, 295, 264, 15296, 11, 558, 30, 583, 10060, 11, 309, 311, 920, 11, 291, 458, 11, 3303, 2856, 307, 920, 544, 50840], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 483, "seek": 244078, "start": 2450.3, "end": 2455.6600000000003, "text": " compositional than things like, you know, formal languages just to make a clear distinction that's", "tokens": [50840, 10199, 2628, 813, 721, 411, 11, 291, 458, 11, 9860, 8650, 445, 281, 652, 257, 1850, 16844, 300, 311, 51108], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 484, "seek": 244078, "start": 2455.6600000000003, "end": 2460.46, "text": " been made. They have a much richer compositional structure. There's more stuff going on, maybe.", "tokens": [51108, 668, 1027, 13, 814, 362, 257, 709, 29021, 10199, 2628, 3877, 13, 821, 311, 544, 1507, 516, 322, 11, 1310, 13, 51348], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 485, "seek": 244078, "start": 2460.46, "end": 2463.98, "text": " So it's important that before that, you know, things like attention-based machine mechanisms", "tokens": [51348, 407, 309, 311, 1021, 300, 949, 300, 11, 291, 458, 11, 721, 411, 3202, 12, 6032, 3479, 15902, 51524], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 486, "seek": 244078, "start": 2463.98, "end": 2469.7400000000002, "text": " and transformers allow for combinations of discrete token bindings, which is more", "tokens": [51524, 293, 4088, 433, 2089, 337, 21267, 295, 27706, 14862, 14786, 1109, 11, 597, 307, 544, 51812], "temperature": 0.0, "avg_logprob": -0.12489599853981542, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.003072440391406417}, {"id": 487, "seek": 246974, "start": 2469.74, "end": 2473.66, "text": " approximate to a merge-like operator than simple recurrent matrix multiplication.", "tokens": [50364, 30874, 281, 257, 22183, 12, 4092, 12973, 813, 2199, 18680, 1753, 8141, 27290, 13, 50560], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 488, "seek": 246974, "start": 2474.8599999999997, "end": 2478.22, "text": " But, you know, the issue of binary branching, binary branching of merge, just to choose,", "tokens": [50620, 583, 11, 291, 458, 11, 264, 2734, 295, 17434, 9819, 278, 11, 17434, 9819, 278, 295, 22183, 11, 445, 281, 2826, 11, 50788], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 489, "seek": 246974, "start": 2478.22, "end": 2482.9399999999996, "text": " for example, here to talk about the four meaning regulation, one principle. Binary branching in", "tokens": [50788, 337, 1365, 11, 510, 281, 751, 466, 264, 1451, 3620, 15062, 11, 472, 8665, 13, 363, 4066, 9819, 278, 294, 51024], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 490, "seek": 246974, "start": 2482.9399999999996, "end": 2487.58, "text": " merge is an interesting question, but Gem2 grammar has always been open to different origins and", "tokens": [51024, 22183, 307, 364, 1880, 1168, 11, 457, 22894, 17, 22317, 575, 1009, 668, 1269, 281, 819, 22721, 293, 51256], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 491, "seek": 246974, "start": 2487.58, "end": 2492.22, "text": " locations of this apparent constraint in syntactic computation. Like, where does it come from?", "tokens": [51256, 9253, 295, 341, 18335, 25534, 294, 23980, 19892, 24903, 13, 1743, 11, 689, 775, 309, 808, 490, 30, 51488], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 492, "seek": 246974, "start": 2492.22, "end": 2496.7799999999997, "text": " Maybe it's a condition on merge. Maybe it's imposed by a smooth system. Maybe it's a kind of prior,", "tokens": [51488, 2704, 309, 311, 257, 4188, 322, 22183, 13, 2704, 309, 311, 26491, 538, 257, 5508, 1185, 13, 2704, 309, 311, 257, 733, 295, 4059, 11, 51716], "temperature": 0.0, "avg_logprob": -0.18146028591476324, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.002445581601932645}, {"id": 493, "seek": 249678, "start": 2496.78, "end": 2501.9, "text": " you know, who knows. And in fact, some more recent work in Gem2 grammar has tried to ground", "tokens": [50364, 291, 458, 11, 567, 3255, 13, 400, 294, 1186, 11, 512, 544, 5162, 589, 294, 22894, 17, 22317, 575, 3031, 281, 2727, 50620], "temperature": 0.0, "avg_logprob": -0.17024706304073334, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.005657259374856949}, {"id": 494, "seek": 249678, "start": 2502.86, "end": 2507.5, "text": " do away with a lot of the set theoretic assumptions of merge, right? Maybe set theory isn't the best", "tokens": [50668, 360, 1314, 365, 257, 688, 295, 264, 992, 14308, 299, 17695, 295, 22183, 11, 558, 30, 2704, 992, 5261, 1943, 380, 264, 1151, 50900], "temperature": 0.0, "avg_logprob": -0.17024706304073334, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.005657259374856949}, {"id": 495, "seek": 249678, "start": 2507.5, "end": 2511.98, "text": " way to model the Gem2 grammar. Maybe more logical accounts are more appropriate. There's lots of", "tokens": [50900, 636, 281, 2316, 264, 22894, 17, 22317, 13, 2704, 544, 14978, 9402, 366, 544, 6854, 13, 821, 311, 3195, 295, 51124], "temperature": 0.0, "avg_logprob": -0.17024706304073334, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.005657259374856949}, {"id": 496, "seek": 249678, "start": 2511.98, "end": 2517.5, "text": " other recent ideas there, which are all compatible with the, with Chomsky's approach, right? In fact,", "tokens": [51124, 661, 5162, 3487, 456, 11, 597, 366, 439, 18218, 365, 264, 11, 365, 761, 4785, 4133, 311, 3109, 11, 558, 30, 682, 1186, 11, 51400], "temperature": 0.0, "avg_logprob": -0.17024706304073334, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.005657259374856949}, {"id": 497, "seek": 249678, "start": 2517.5, "end": 2520.86, "text": " one of the things that Chomsky likes the most is when he's, when he's proven wrong, right? A lot of", "tokens": [51400, 472, 295, 264, 721, 300, 761, 4785, 4133, 5902, 264, 881, 307, 562, 415, 311, 11, 562, 415, 311, 12785, 2085, 11, 558, 30, 316, 688, 295, 51568], "temperature": 0.0, "avg_logprob": -0.17024706304073334, "compression_ratio": 1.7598566308243728, "no_speech_prob": 0.005657259374856949}, {"id": 498, "seek": 252086, "start": 2520.86, "end": 2526.6200000000003, "text": " these theories are going against the core mainstream minimalist architecture. But yeah,", "tokens": [50364, 613, 13667, 366, 516, 1970, 264, 4965, 15960, 50192, 9482, 13, 583, 1338, 11, 50652], "temperature": 0.0, "avg_logprob": -0.20625475247701008, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.06419697403907776}, {"id": 499, "seek": 252086, "start": 2526.6200000000003, "end": 2532.94, "text": " I think it's a very diverse, like, vibrant field. The people who are Adjah, Hornstein, you know,", "tokens": [50652, 286, 519, 309, 311, 257, 588, 9521, 11, 411, 11, 21571, 2519, 13, 440, 561, 567, 366, 1999, 73, 545, 11, 31792, 9089, 11, 291, 458, 11, 50968], "temperature": 0.0, "avg_logprob": -0.20625475247701008, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.06419697403907776}, {"id": 500, "seek": 252086, "start": 2532.94, "end": 2539.34, "text": " Petrosky, Haji Borre, they disagree in fundamental ways with a lot of what the mainstream of", "tokens": [50968, 10472, 2635, 4133, 11, 389, 29319, 13739, 265, 11, 436, 14091, 294, 8088, 2098, 365, 257, 688, 295, 437, 264, 15960, 295, 51288], "temperature": 0.0, "avg_logprob": -0.20625475247701008, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.06419697403907776}, {"id": 501, "seek": 252086, "start": 2539.34, "end": 2544.2200000000003, "text": " Gem2 grammar would say, but there's still more scope for disagreement. But it's still compatible", "tokens": [51288, 22894, 17, 22317, 576, 584, 11, 457, 456, 311, 920, 544, 11923, 337, 38947, 13, 583, 309, 311, 920, 18218, 51532], "temperature": 0.0, "avg_logprob": -0.20625475247701008, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.06419697403907776}, {"id": 502, "seek": 252086, "start": 2544.2200000000003, "end": 2548.46, "text": " with setting core assumptions, right? So a lot of David Adjah's work, for example, kind of deviates", "tokens": [51532, 365, 3287, 4965, 17695, 11, 558, 30, 407, 257, 688, 295, 4389, 1999, 73, 545, 311, 589, 11, 337, 1365, 11, 733, 295, 31219, 1024, 51744], "temperature": 0.0, "avg_logprob": -0.20625475247701008, "compression_ratio": 1.585284280936455, "no_speech_prob": 0.06419697403907776}, {"id": 503, "seek": 254846, "start": 2548.46, "end": 2553.58, "text": " in this core respect, but it's still trying to ground these intuitions in different formal systems.", "tokens": [50364, 294, 341, 4965, 3104, 11, 457, 309, 311, 920, 1382, 281, 2727, 613, 16224, 626, 294, 819, 9860, 3652, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1273872931798299, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004231636878103018}, {"id": 504, "seek": 254846, "start": 2554.62, "end": 2561.5, "text": " So, you know, it's kind of, I want to get your thoughts again on, I mentioned Mitchell, right?", "tokens": [50672, 407, 11, 291, 458, 11, 309, 311, 733, 295, 11, 286, 528, 281, 483, 428, 4598, 797, 322, 11, 286, 2835, 27582, 11, 558, 30, 51016], "temperature": 0.0, "avg_logprob": -0.1273872931798299, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004231636878103018}, {"id": 505, "seek": 254846, "start": 2561.5, "end": 2567.34, "text": " So Mitchell and Bowers 2020, they have this paper, priorless recurrent networks laying curiously,", "tokens": [51016, 407, 27582, 293, 12903, 433, 4808, 11, 436, 362, 341, 3035, 11, 4059, 1832, 18680, 1753, 9590, 14903, 6369, 356, 11, 51308], "temperature": 0.0, "avg_logprob": -0.1273872931798299, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004231636878103018}, {"id": 506, "seek": 254846, "start": 2567.34, "end": 2570.78, "text": " but I think you might be aware of, right? So this is a really good example just to kind of get to", "tokens": [51308, 457, 286, 519, 291, 1062, 312, 3650, 295, 11, 558, 30, 407, 341, 307, 257, 534, 665, 1365, 445, 281, 733, 295, 483, 281, 51480], "temperature": 0.0, "avg_logprob": -0.1273872931798299, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004231636878103018}, {"id": 507, "seek": 254846, "start": 2570.78, "end": 2574.94, "text": " the heart of the issue. So recurrent neural networks have been shown to accurately model,", "tokens": [51480, 264, 1917, 295, 264, 2734, 13, 407, 18680, 1753, 18161, 9590, 362, 668, 4898, 281, 20095, 2316, 11, 51688], "temperature": 0.0, "avg_logprob": -0.1273872931798299, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004231636878103018}, {"id": 508, "seek": 257494, "start": 2574.94, "end": 2578.86, "text": " you know, non-veb number agreement, but Mitchell and Bowers show that these networks will also", "tokens": [50364, 291, 458, 11, 2107, 12, 303, 65, 1230, 8106, 11, 457, 27582, 293, 12903, 433, 855, 300, 613, 9590, 486, 611, 50560], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 509, "seek": 257494, "start": 2578.86, "end": 2583.42, "text": " learn a number agreement with unnatural sentence structures. So structures that are not found", "tokens": [50560, 1466, 257, 1230, 8106, 365, 43470, 8174, 9227, 13, 407, 9227, 300, 366, 406, 1352, 50788], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 510, "seek": 257494, "start": 2583.42, "end": 2587.7400000000002, "text": " in natural language, and which humans have a hard time processing, right? So the mode of learning", "tokens": [50788, 294, 3303, 2856, 11, 293, 597, 6255, 362, 257, 1152, 565, 9007, 11, 558, 30, 407, 264, 4391, 295, 2539, 51004], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 511, "seek": 257494, "start": 2587.7400000000002, "end": 2594.2200000000003, "text": " for RNNs is, at least for RNNs, qualitatively distinct from infant, you know, infant homo sapiens,", "tokens": [51004, 337, 45702, 45, 82, 307, 11, 412, 1935, 337, 45702, 45, 82, 11, 31312, 356, 10644, 490, 16757, 11, 291, 458, 11, 16757, 3655, 78, 18985, 24594, 11, 51328], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 512, "seek": 257494, "start": 2594.2200000000003, "end": 2599.18, "text": " right? So the story is Mitchell and Bowers show that while their LSTM model has a good representation", "tokens": [51328, 558, 30, 407, 264, 1657, 307, 27582, 293, 12903, 433, 855, 300, 1339, 641, 441, 6840, 44, 2316, 575, 257, 665, 10290, 51576], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 513, "seek": 257494, "start": 2599.18, "end": 2604.06, "text": " of singular variances, plural for individual sentences, there's no generalization going on,", "tokens": [51576, 295, 20010, 1374, 21518, 11, 25377, 337, 2609, 16579, 11, 456, 311, 572, 2674, 2144, 516, 322, 11, 51820], "temperature": 0.0, "avg_logprob": -0.1242326929949332, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0052552297711372375}, {"id": 514, "seek": 260406, "start": 2604.06, "end": 2607.82, "text": " right? They can represent at the individual level. So the model doesn't have a representation of", "tokens": [50364, 558, 30, 814, 393, 2906, 412, 264, 2609, 1496, 13, 407, 264, 2316, 1177, 380, 362, 257, 10290, 295, 50552], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 515, "seek": 260406, "start": 2607.82, "end": 2612.94, "text": " number as an abstraction. What number is? Only concrete instances of singular versus plural.", "tokens": [50552, 1230, 382, 364, 37765, 13, 708, 1230, 307, 30, 5686, 9859, 14519, 295, 20010, 5717, 25377, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 516, "seek": 260406, "start": 2613.9, "end": 2620.06, "text": " So successfully predicting language behavior via LM, or successfully predicting neural responses", "tokens": [50856, 407, 10727, 32884, 2856, 5223, 5766, 46529, 11, 420, 10727, 32884, 18161, 13019, 51164], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 517, "seek": 260406, "start": 2620.06, "end": 2623.34, "text": " in a similar way is obviously great. And maybe we can get into that issue later,", "tokens": [51164, 294, 257, 2531, 636, 307, 2745, 869, 13, 400, 1310, 321, 393, 483, 666, 300, 2734, 1780, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 518, "seek": 260406, "start": 2623.34, "end": 2627.1, "text": " but there's only one side of the coin here, right? The other side of the coin is explaining why this", "tokens": [51328, 457, 456, 311, 787, 472, 1252, 295, 264, 11464, 510, 11, 558, 30, 440, 661, 1252, 295, 264, 11464, 307, 13468, 983, 341, 51516], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 519, "seek": 260406, "start": 2627.1, "end": 2631.1, "text": " type of behavior and not some other behavior, why this structure and not some other, and that's", "tokens": [51516, 2010, 295, 5223, 293, 406, 512, 661, 5223, 11, 983, 341, 3877, 293, 406, 512, 661, 11, 293, 300, 311, 51716], "temperature": 0.0, "avg_logprob": -0.10392092889355074, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0010892846621572971}, {"id": 520, "seek": 263110, "start": 2631.1, "end": 2638.62, "text": " maybe Chomsky's most important point, really, why this and not some other system. So linguistic", "tokens": [50364, 1310, 761, 4785, 4133, 311, 881, 1021, 935, 11, 534, 11, 983, 341, 293, 406, 512, 661, 1185, 13, 407, 43002, 50740], "temperature": 0.0, "avg_logprob": -0.20731474304199218, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.001951694255694747}, {"id": 521, "seek": 263110, "start": 2638.62, "end": 2642.62, "text": " theory kind of gives you that other side of the coin, right? Whereas LM's really don't. So the", "tokens": [50740, 5261, 733, 295, 2709, 291, 300, 661, 1252, 295, 264, 11464, 11, 558, 30, 13813, 46529, 311, 534, 500, 380, 13, 407, 264, 50940], "temperature": 0.0, "avg_logprob": -0.20731474304199218, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.001951694255694747}, {"id": 522, "seek": 263110, "start": 2642.62, "end": 2649.1, "text": " Mitchell and Bowers paper does something that- He does it! Well, yeah, so like, take Yael LaCrette's", "tokens": [50940, 27582, 293, 12903, 433, 3035, 775, 746, 300, 12, 634, 775, 309, 0, 1042, 11, 1338, 11, 370, 411, 11, 747, 398, 4300, 2369, 34, 1505, 975, 311, 51264], "temperature": 0.0, "avg_logprob": -0.20731474304199218, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.001951694255694747}, {"id": 523, "seek": 263110, "start": 2649.1, "end": 2653.9, "text": " and Stanislas de Haines' work from 2019, right? They looked at number agreement in an LSTM and", "tokens": [51264, 293, 10061, 5788, 296, 368, 389, 19295, 6, 589, 490, 6071, 11, 558, 30, 814, 2956, 412, 1230, 8106, 294, 364, 441, 6840, 44, 293, 51504], "temperature": 0.0, "avg_logprob": -0.20731474304199218, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.001951694255694747}, {"id": 524, "seek": 263110, "start": 2653.9, "end": 2658.7, "text": " found two specialized units that encoded number agreement, but the overall contribution to performance", "tokens": [51504, 1352, 732, 19813, 6815, 300, 2058, 12340, 1230, 8106, 11, 457, 264, 4787, 13150, 281, 3389, 51744], "temperature": 0.0, "avg_logprob": -0.20731474304199218, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.001951694255694747}, {"id": 525, "seek": 265870, "start": 2658.7, "end": 2664.54, "text": " was low. And then in 2021, Yael LaCrette's have this paper where they show that in the neural", "tokens": [50364, 390, 2295, 13, 400, 550, 294, 7201, 11, 398, 4300, 2369, 34, 1505, 975, 311, 362, 341, 3035, 689, 436, 855, 300, 294, 264, 18161, 50656], "temperature": 0.0, "avg_logprob": -0.13934531093628938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.005286233965307474}, {"id": 526, "seek": 265870, "start": 2664.54, "end": 2670.06, "text": " language model, it did not achieve genuine recursive processing of nested long range agreement,", "tokens": [50656, 2856, 2316, 11, 309, 630, 406, 4584, 16699, 20560, 488, 9007, 295, 15646, 292, 938, 3613, 8106, 11, 50932], "temperature": 0.0, "avg_logprob": -0.13934531093628938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.005286233965307474}, {"id": 527, "seek": 265870, "start": 2670.06, "end": 2675.1, "text": " gender marking in Italian, I think, even if some hierarchical processing, you know, was achieved,", "tokens": [50932, 7898, 25482, 294, 10003, 11, 286, 519, 11, 754, 498, 512, 35250, 804, 9007, 11, 291, 458, 11, 390, 11042, 11, 51184], "temperature": 0.0, "avg_logprob": -0.13934531093628938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.005286233965307474}, {"id": 528, "seek": 265870, "start": 2675.1, "end": 2679.66, "text": " as you've argued before, right? Some hierarchy was there, it was there. But the question is,", "tokens": [51184, 382, 291, 600, 20219, 949, 11, 558, 30, 2188, 22333, 390, 456, 11, 309, 390, 456, 13, 583, 264, 1168, 307, 11, 51412], "temperature": 0.0, "avg_logprob": -0.13934531093628938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.005286233965307474}, {"id": 529, "seek": 265870, "start": 2679.66, "end": 2684.14, "text": " is it the right mapping? Is it the right kind of hierarchy? They found that LSTM based models could", "tokens": [51412, 307, 309, 264, 558, 18350, 30, 1119, 309, 264, 558, 733, 295, 22333, 30, 814, 1352, 300, 441, 6840, 44, 2361, 5245, 727, 51636], "temperature": 0.0, "avg_logprob": -0.13934531093628938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.005286233965307474}, {"id": 530, "seek": 268414, "start": 2684.14, "end": 2688.7799999999997, "text": " learn subject-verb agreement over short spans, one degree of embedding, but they failed at some", "tokens": [50364, 1466, 3983, 12, 25809, 8106, 670, 2099, 44086, 11, 472, 4314, 295, 12240, 3584, 11, 457, 436, 7612, 412, 512, 50596], "temperature": 0.0, "avg_logprob": -0.1771174886952276, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.033746570348739624}, {"id": 531, "seek": 268414, "start": 2688.7799999999997, "end": 2695.66, "text": " longer dependencies. And in their most recent paper, LaCrette set out with De Haines showed that", "tokens": [50596, 2854, 36606, 13, 400, 294, 641, 881, 5162, 3035, 11, 2369, 34, 1505, 975, 992, 484, 365, 1346, 389, 19295, 4712, 300, 50940], "temperature": 0.0, "avg_logprob": -0.1771174886952276, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.033746570348739624}, {"id": 532, "seek": 268414, "start": 2695.66, "end": 2701.58, "text": " they evaluated modern transformer LM's, including GPT2 XL, on the same task. And the transformers", "tokens": [50940, 436, 25509, 4363, 31782, 46529, 311, 11, 3009, 26039, 51, 17, 37210, 11, 322, 264, 912, 5633, 13, 400, 264, 4088, 433, 51236], "temperature": 0.0, "avg_logprob": -0.1771174886952276, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.033746570348739624}, {"id": 533, "seek": 268414, "start": 2701.58, "end": 2706.22, "text": " performed more similarly to humans than LSTMs did and performed above transfer overall, but they", "tokens": [51236, 10332, 544, 14138, 281, 6255, 813, 441, 6840, 26386, 630, 293, 10332, 3673, 5003, 4787, 11, 457, 436, 51468], "temperature": 0.0, "avg_logprob": -0.1771174886952276, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.033746570348739624}, {"id": 534, "seek": 268414, "start": 2706.22, "end": 2710.54, "text": " still performed below chance in one key condition, which is the, as I mentioned, the multiple embedding", "tokens": [51468, 920, 10332, 2507, 2931, 294, 472, 2141, 4188, 11, 597, 307, 264, 11, 382, 286, 2835, 11, 264, 3866, 12240, 3584, 51684], "temperature": 0.0, "avg_logprob": -0.1771174886952276, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.033746570348739624}, {"id": 535, "seek": 271054, "start": 2710.54, "end": 2715.02, "text": " one, the difficult structures. So the reason why I mentioned these studies is because, you know,", "tokens": [50364, 472, 11, 264, 2252, 9227, 13, 407, 264, 1778, 983, 286, 2835, 613, 5313, 307, 570, 11, 291, 458, 11, 50588], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 536, "seek": 271054, "start": 2715.82, "end": 2720.7, "text": " it's not just to explore the limits of LM's, which is an interesting question. But consider work by", "tokens": [50628, 309, 311, 406, 445, 281, 6839, 264, 10406, 295, 46529, 311, 11, 597, 307, 364, 1880, 1168, 13, 583, 1949, 589, 538, 50872], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 537, "seek": 271054, "start": 2720.7, "end": 2727.02, "text": " people like Neil Smith at UCL, right? He did work in the 90s with a polyglot, savant, and neurotypical", "tokens": [50872, 561, 411, 18615, 8538, 412, 14079, 43, 11, 558, 30, 634, 630, 589, 294, 264, 4289, 82, 365, 257, 6754, 7191, 310, 11, 601, 5219, 11, 293, 12087, 6737, 34061, 51188], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 538, "seek": 271054, "start": 2727.02, "end": 2731.9, "text": " controls comparing them. So he investigated second language learning of an artificial language", "tokens": [51188, 9003, 15763, 552, 13, 407, 415, 30070, 1150, 2856, 2539, 295, 364, 11677, 2856, 51432], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 539, "seek": 271054, "start": 2731.9, "end": 2735.74, "text": " containing both natural and unnatural ground structures, like the Michelin Bowers paper,", "tokens": [51432, 19273, 1293, 3303, 293, 43470, 2727, 9227, 11, 411, 264, 23709, 259, 12903, 433, 3035, 11, 51624], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 540, "seek": 271054, "start": 2735.74, "end": 2739.5, "text": " right? The whole framework is natural versus unnatural. And they found that while both the", "tokens": [51624, 558, 30, 440, 1379, 8388, 307, 3303, 5717, 43470, 13, 400, 436, 1352, 300, 1339, 1293, 264, 51812], "temperature": 0.0, "avg_logprob": -0.14422162851892917, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.005481827072799206}, {"id": 541, "seek": 273950, "start": 2739.5, "end": 2744.86, "text": " savant, Christopher, the savant, and the controls could master the linguistically natural aspects,", "tokens": [50364, 601, 5219, 11, 20649, 11, 264, 601, 5219, 11, 293, 264, 9003, 727, 4505, 264, 21766, 20458, 3303, 7270, 11, 50632], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 542, "seek": 273950, "start": 2744.86, "end": 2748.78, "text": " only the controls could eventually handle the structure dependent unnatural phenomena,", "tokens": [50632, 787, 264, 9003, 727, 4728, 4813, 264, 3877, 12334, 43470, 22004, 11, 50828], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 543, "seek": 273950, "start": 2748.78, "end": 2753.1, "text": " and neither of them could master the structure independent aspects. So some weird rules where", "tokens": [50828, 293, 9662, 295, 552, 727, 4505, 264, 3877, 6695, 7270, 13, 407, 512, 3657, 4474, 689, 51044], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 544, "seek": 273950, "start": 2753.1, "end": 2756.54, "text": " it's like, you know, you mark the emphasis on the third word of the sentence, things like that.", "tokens": [51044, 309, 311, 411, 11, 291, 458, 11, 291, 1491, 264, 16271, 322, 264, 2636, 1349, 295, 264, 8174, 11, 721, 411, 300, 13, 51216], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 545, "seek": 273950, "start": 2756.54, "end": 2761.66, "text": " So they argue that Christopher's abilities are entirely due to his intact linguistic faculties,", "tokens": [51216, 407, 436, 9695, 300, 20649, 311, 11582, 366, 7696, 3462, 281, 702, 23493, 43002, 44137, 530, 11, 51472], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 546, "seek": 273950, "start": 2761.66, "end": 2766.86, "text": " but the controls could employ more domain general kind of cognitive resources, like, you know,", "tokens": [51472, 457, 264, 9003, 727, 3188, 544, 9274, 2674, 733, 295, 15605, 3593, 11, 411, 11, 291, 458, 11, 51732], "temperature": 0.0, "avg_logprob": -0.10676550070444743, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.0004703787271864712}, {"id": 547, "seek": 276686, "start": 2766.94, "end": 2770.94, "text": " tension control, etc., which is why they could deal with those difficult processes.", "tokens": [50368, 8980, 1969, 11, 5183, 7933, 597, 307, 983, 436, 727, 2028, 365, 729, 2252, 7555, 13, 50568], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 548, "seek": 276686, "start": 2771.58, "end": 2777.34, "text": " But I just mentioned, you know, a minute ago, that the LSTM in the Michelin Bowers paper approaches", "tokens": [50600, 583, 286, 445, 2835, 11, 291, 458, 11, 257, 3456, 2057, 11, 300, 264, 441, 6840, 44, 294, 264, 23709, 259, 12903, 433, 3035, 11587, 50888], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 549, "seek": 276686, "start": 2777.34, "end": 2782.1400000000003, "text": " natural and unnatural structures in pretty much the same way. So it's not, you know, it's not a", "tokens": [50888, 3303, 293, 43470, 9227, 294, 1238, 709, 264, 912, 636, 13, 407, 309, 311, 406, 11, 291, 458, 11, 309, 311, 406, 257, 51128], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 550, "seek": 276686, "start": 2782.1400000000003, "end": 2787.42, "text": " psychologically plausible model, I would argue, for whatever humans are doing. And similar observations", "tokens": [51128, 41387, 39925, 2316, 11, 286, 576, 9695, 11, 337, 2035, 6255, 366, 884, 13, 400, 2531, 18163, 51392], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 551, "seek": 276686, "start": 2787.42, "end": 2791.9, "text": " can apply to the limits of transformer models in Le Creta's work. And all of these themes are like,", "tokens": [51392, 393, 3079, 281, 264, 10406, 295, 31782, 5245, 294, 1456, 9549, 1328, 311, 589, 13, 400, 439, 295, 613, 13544, 366, 411, 11, 51616], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 552, "seek": 276686, "start": 2791.9, "end": 2795.98, "text": " right up there, they're saying that there's all the way to the present. So another one of", "tokens": [51616, 558, 493, 456, 11, 436, 434, 1566, 300, 456, 311, 439, 264, 636, 281, 264, 1974, 13, 407, 1071, 472, 295, 51820], "temperature": 0.0, "avg_logprob": -0.14577506085951553, "compression_ratio": 1.6560693641618498, "no_speech_prob": 0.005027843173593283}, {"id": 553, "seek": 279598, "start": 2795.98, "end": 2800.14, "text": " Tal Linsen's recent papers that he posted a few weeks ago, looking at child directed speech,", "tokens": [50364, 10516, 441, 1292, 268, 311, 5162, 10577, 300, 415, 9437, 257, 1326, 3259, 2057, 11, 1237, 412, 1440, 12898, 6218, 11, 50572], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 554, "seek": 279598, "start": 2800.14, "end": 2805.42, "text": " showed that LSTMs and transformers limited to ecologically plausible amounts of data", "tokens": [50572, 4712, 300, 441, 6840, 26386, 293, 4088, 433, 5567, 281, 11437, 17157, 39925, 11663, 295, 1412, 50836], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 555, "seek": 279598, "start": 2805.42, "end": 2809.18, "text": " generalize, as I mentioned, the linear rules for English, right, rather than the abstract rules.", "tokens": [50836, 2674, 1125, 11, 382, 286, 2835, 11, 264, 8213, 4474, 337, 3669, 11, 558, 11, 2831, 813, 264, 12649, 4474, 13, 51024], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 556, "seek": 279598, "start": 2809.7400000000002, "end": 2815.26, "text": " And in fact, more recent work from Linsen's lab last week, looking at, well, last year, I should say,", "tokens": [51052, 400, 294, 1186, 11, 544, 5162, 589, 490, 441, 1292, 268, 311, 2715, 1036, 1243, 11, 1237, 412, 11, 731, 11, 1036, 1064, 11, 286, 820, 584, 11, 51328], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 557, "seek": 279598, "start": 2815.26, "end": 2820.7, "text": " shows that looking at garden paths, surprise does not explain syntactic disambiguation", "tokens": [51328, 3110, 300, 1237, 412, 7431, 14518, 11, 6365, 775, 406, 2903, 23980, 19892, 717, 2173, 328, 16073, 51600], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 558, "seek": 279598, "start": 2820.7, "end": 2825.18, "text": " difficulty, right? Surprise will underpredicts the size of the garden path effect across all", "tokens": [51600, 10360, 11, 558, 30, 36701, 486, 833, 79, 24945, 82, 264, 2744, 295, 264, 7431, 3100, 1802, 2108, 439, 51824], "temperature": 0.0, "avg_logprob": -0.162091802148258, "compression_ratio": 1.6696696696696696, "no_speech_prob": 0.0034879695158451796}, {"id": 559, "seek": 282518, "start": 2825.18, "end": 2828.54, "text": " constructions. And this gets to this issue that you mentioned before, you know, maybe surprise", "tokens": [50364, 7690, 626, 13, 400, 341, 2170, 281, 341, 2734, 300, 291, 2835, 949, 11, 291, 458, 11, 1310, 6365, 50532], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 560, "seek": 282518, "start": 2828.54, "end": 2832.3799999999997, "text": " all this related to some aspects of syntax, but maybe not other ones, it's kind of a,", "tokens": [50532, 439, 341, 4077, 281, 512, 7270, 295, 28431, 11, 457, 1310, 406, 661, 2306, 11, 309, 311, 733, 295, 257, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 561, "seek": 282518, "start": 2832.3799999999997, "end": 2836.22, "text": " it's a very nontrivial issue that is very much, it's open to discussion. It's not,", "tokens": [50724, 309, 311, 257, 588, 297, 896, 470, 22640, 2734, 300, 307, 588, 709, 11, 309, 311, 1269, 281, 5017, 13, 467, 311, 406, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 562, "seek": 282518, "start": 2836.22, "end": 2840.62, "text": " it hasn't settled yet. But so Linsen showed that garden path effects are just way more", "tokens": [50916, 309, 6132, 380, 14819, 1939, 13, 583, 370, 441, 1292, 268, 4712, 300, 7431, 3100, 5065, 366, 445, 636, 544, 51136], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 563, "seek": 282518, "start": 2840.62, "end": 2844.7799999999997, "text": " difficult than you would expect from mere unpredictability. So another way of phrasing", "tokens": [51136, 2252, 813, 291, 576, 2066, 490, 8401, 28341, 2310, 13, 407, 1071, 636, 295, 7636, 3349, 51344], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 564, "seek": 282518, "start": 2844.7799999999997, "end": 2850.8599999999997, "text": " this argument is to quote a recent argument with Chomsky's to get at this natural basis,", "tokens": [51344, 341, 6770, 307, 281, 6513, 257, 5162, 6770, 365, 761, 4785, 4133, 311, 281, 483, 412, 341, 3303, 5143, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 565, "seek": 282518, "start": 2850.8599999999997, "end": 2854.7799999999997, "text": " unnatural issue. He says, suppose we have an expanded periodic table that includes", "tokens": [51648, 43470, 2734, 13, 634, 1619, 11, 7297, 321, 362, 364, 14342, 27790, 3199, 300, 5974, 51844], "temperature": 0.0, "avg_logprob": -0.1279385295135296, "compression_ratio": 1.6963788300835654, "no_speech_prob": 0.001973810838535428}, {"id": 566, "seek": 285478, "start": 2854.78, "end": 2859.98, "text": " all the elements that do exist, all the elements that can possibly exist, and all the elements", "tokens": [50364, 439, 264, 4959, 300, 360, 2514, 11, 439, 264, 4959, 300, 393, 6264, 2514, 11, 293, 439, 264, 4959, 50624], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 567, "seek": 285478, "start": 2859.98, "end": 2865.02, "text": " that cannot possibly exist. And let's say you have some model, some artificial model that fails", "tokens": [50624, 300, 2644, 6264, 2514, 13, 400, 718, 311, 584, 291, 362, 512, 2316, 11, 512, 11677, 2316, 300, 18199, 50876], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 568, "seek": 285478, "start": 2865.02, "end": 2869.5, "text": " to distinguish between these three categories, whatever this model is doing, it's not helping", "tokens": [50876, 281, 20206, 1296, 613, 1045, 10479, 11, 2035, 341, 2316, 307, 884, 11, 309, 311, 406, 4315, 51100], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 569, "seek": 285478, "start": 2869.5, "end": 2873.6600000000003, "text": " those understand chemistry, right? It's doing something else. It's doing something for sure,", "tokens": [51100, 729, 1223, 12558, 11, 558, 30, 467, 311, 884, 746, 1646, 13, 467, 311, 884, 746, 337, 988, 11, 51308], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 570, "seek": 285478, "start": 2873.6600000000003, "end": 2877.26, "text": " but whether or not it's helping those understand chemistry is something separate. And I know that", "tokens": [51308, 457, 1968, 420, 406, 309, 311, 4315, 729, 1223, 12558, 307, 746, 4994, 13, 400, 286, 458, 300, 51488], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 571, "seek": 285478, "start": 2877.26, "end": 2880.7000000000003, "text": " you've said in response to some of these studies, I think you've said that, you know,", "tokens": [51488, 291, 600, 848, 294, 4134, 281, 512, 295, 613, 5313, 11, 286, 519, 291, 600, 848, 300, 11, 291, 458, 11, 51660], "temperature": 0.0, "avg_logprob": -0.08941450644665816, "compression_ratio": 2.0474452554744524, "no_speech_prob": 0.0031215092167258263}, {"id": 572, "seek": 288070, "start": 2881.66, "end": 2885.18, "text": " in order to show that something is likely to be impossible, somewhere in your paper, I think you", "tokens": [50412, 294, 1668, 281, 855, 300, 746, 307, 3700, 281, 312, 6243, 11, 4079, 294, 428, 3035, 11, 286, 519, 291, 50588], "temperature": 0.0, "avg_logprob": -0.1256438414255778, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005651441402733326}, {"id": 573, "seek": 288070, "start": 2885.18, "end": 2891.8199999999997, "text": " say, in order to show that something is impossible with normal bounds and false positives, you'd", "tokens": [50588, 584, 11, 294, 1668, 281, 855, 300, 746, 307, 6243, 365, 2710, 29905, 293, 7908, 35127, 11, 291, 1116, 50920], "temperature": 0.0, "avg_logprob": -0.1256438414255778, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005651441402733326}, {"id": 574, "seek": 288070, "start": 2891.8199999999997, "end": 2895.98, "text": " need to show, you need to look at something like 500 independently sampled languages. So you cite", "tokens": [50920, 643, 281, 855, 11, 291, 643, 281, 574, 412, 746, 411, 5923, 21761, 3247, 15551, 8650, 13, 407, 291, 37771, 51128], "temperature": 0.0, "avg_logprob": -0.1256438414255778, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005651441402733326}, {"id": 575, "seek": 288070, "start": 2895.98, "end": 2900.22, "text": " this in your paper, right? Which you probably can't do, that's just not, it's not a feasible thing to", "tokens": [51128, 341, 294, 428, 3035, 11, 558, 30, 3013, 291, 1391, 393, 380, 360, 11, 300, 311, 445, 406, 11, 309, 311, 406, 257, 26648, 551, 281, 51340], "temperature": 0.0, "avg_logprob": -0.1256438414255778, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005651441402733326}, {"id": 576, "seek": 288070, "start": 2900.22, "end": 2906.22, "text": " do. So, you know, I'm not too sure that this really refutes the principle argument that I'm", "tokens": [51340, 360, 13, 407, 11, 291, 458, 11, 286, 478, 406, 886, 988, 300, 341, 534, 1895, 1819, 264, 8665, 6770, 300, 286, 478, 51640], "temperature": 0.0, "avg_logprob": -0.1256438414255778, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005651441402733326}, {"id": 577, "seek": 290622, "start": 2906.22, "end": 2910.7, "text": " making here, right? Because people like Mitchell and Bowers are making an argument about impossibility", "tokens": [50364, 1455, 510, 11, 558, 30, 1436, 561, 411, 27582, 293, 12903, 433, 366, 1455, 364, 6770, 466, 38802, 2841, 50588], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 578, "seek": 290622, "start": 2910.7, "end": 2914.7, "text": " in principle, not in some kind of extensional sense, you know, just like searching across the", "tokens": [50588, 294, 8665, 11, 406, 294, 512, 733, 295, 1279, 11075, 2020, 11, 291, 458, 11, 445, 411, 10808, 2108, 264, 50788], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 579, "seek": 290622, "start": 2914.7, "end": 2919.4199999999996, "text": " world languages to see, to prove across every single language that it is impossible, right?", "tokens": [50788, 1002, 8650, 281, 536, 11, 281, 7081, 2108, 633, 2167, 2856, 300, 309, 307, 6243, 11, 558, 30, 51024], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 580, "seek": 290622, "start": 2919.4199999999996, "end": 2923.66, "text": " That's kind of, it's a different argument, whether it's impossible in some random language in the", "tokens": [51024, 663, 311, 733, 295, 11, 309, 311, 257, 819, 6770, 11, 1968, 309, 311, 6243, 294, 512, 4974, 2856, 294, 264, 51236], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 581, "seek": 290622, "start": 2923.66, "end": 2928.06, "text": " Amazon, compared to actually impossible, based on the principles of what the language system is", "tokens": [51236, 6795, 11, 5347, 281, 767, 6243, 11, 2361, 322, 264, 9156, 295, 437, 264, 2856, 1185, 307, 51456], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 582, "seek": 290622, "start": 2928.06, "end": 2931.8199999999997, "text": " actually doing, like what it can do. So I would just say that, you know, all of these kind of", "tokens": [51456, 767, 884, 11, 411, 437, 309, 393, 360, 13, 407, 286, 576, 445, 584, 300, 11, 291, 458, 11, 439, 295, 613, 733, 295, 51644], "temperature": 0.0, "avg_logprob": -0.12403852419745653, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.020192783325910568}, {"id": 583, "seek": 293182, "start": 2932.06, "end": 2938.1400000000003, "text": " I think that that that point is, is that you don't actually know what is typologically not", "tokens": [50376, 286, 519, 300, 300, 300, 935, 307, 11, 307, 300, 291, 500, 380, 767, 458, 437, 307, 2125, 17157, 406, 50680], "temperature": 0.0, "avg_logprob": -0.13636010776866567, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.005382484290748835}, {"id": 584, "seek": 293182, "start": 2938.1400000000003, "end": 2943.02, "text": " possible, right? So people like to say things like, you know, there's no language that does X,", "tokens": [50680, 1944, 11, 558, 30, 407, 561, 411, 281, 584, 721, 411, 11, 291, 458, 11, 456, 311, 572, 2856, 300, 775, 1783, 11, 50924], "temperature": 0.0, "avg_logprob": -0.13636010776866567, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.005382484290748835}, {"id": 585, "seek": 293182, "start": 2943.02, "end": 2949.02, "text": " therefore we have to build that restriction into our statistical models. But if it's not", "tokens": [50924, 4412, 321, 362, 281, 1322, 300, 29529, 666, 527, 22820, 5245, 13, 583, 498, 309, 311, 406, 51224], "temperature": 0.0, "avg_logprob": -0.13636010776866567, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.005382484290748835}, {"id": 586, "seek": 293182, "start": 2949.02, "end": 2953.9, "text": " statistically justified that there is no language that does X, right? If you've only looked at", "tokens": [51224, 36478, 27808, 300, 456, 307, 572, 2856, 300, 775, 1783, 11, 558, 30, 759, 291, 600, 787, 2956, 412, 51468], "temperature": 0.0, "avg_logprob": -0.13636010776866567, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.005382484290748835}, {"id": 587, "seek": 293182, "start": 2953.9, "end": 2957.7400000000002, "text": " 20, 20 European languages or something, right? I mean, it's, it's not", "tokens": [51468, 945, 11, 945, 6473, 8650, 420, 746, 11, 558, 30, 286, 914, 11, 309, 311, 11, 309, 311, 406, 51660], "temperature": 0.0, "avg_logprob": -0.13636010776866567, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.005382484290748835}, {"id": 588, "seek": 295774, "start": 2958.2999999999997, "end": 2965.9799999999996, "text": " like that shouldn't motivate doing anything to the models, right? If it's, if it's not a", "tokens": [50392, 411, 300, 4659, 380, 28497, 884, 1340, 281, 264, 5245, 11, 558, 30, 759, 309, 311, 11, 498, 309, 311, 406, 257, 50776], "temperature": 0.0, "avg_logprob": -0.1446084643519202, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0018936049891635776}, {"id": 589, "seek": 295774, "start": 2965.9799999999996, "end": 2973.4199999999996, "text": " statistically justified universal, I think. Well, you know, I think, you're totally right,", "tokens": [50776, 36478, 27808, 11455, 11, 286, 519, 13, 1042, 11, 291, 458, 11, 286, 519, 11, 291, 434, 3879, 558, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1446084643519202, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0018936049891635776}, {"id": 590, "seek": 295774, "start": 2973.4199999999996, "end": 2976.9399999999996, "text": " but that just applies more generally to the social sciences and psychological sciences,", "tokens": [51148, 457, 300, 445, 13165, 544, 5101, 281, 264, 2093, 17677, 293, 14346, 17677, 11, 51324], "temperature": 0.0, "avg_logprob": -0.1446084643519202, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0018936049891635776}, {"id": 591, "seek": 295774, "start": 2976.9399999999996, "end": 2980.9399999999996, "text": " right? Like typologically, it's very difficult to establish these things, right? So I guess", "tokens": [51324, 558, 30, 1743, 2125, 17157, 11, 309, 311, 588, 2252, 281, 8327, 613, 721, 11, 558, 30, 407, 286, 2041, 51524], "temperature": 0.0, "avg_logprob": -0.1446084643519202, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0018936049891635776}, {"id": 592, "seek": 298094, "start": 2981.02, "end": 2985.9, "text": " you're, you're, I guess you're just kind of steelman you're a bit, you're saying that the strong", "tokens": [50368, 291, 434, 11, 291, 434, 11, 286, 2041, 291, 434, 445, 733, 295, 8269, 1601, 291, 434, 257, 857, 11, 291, 434, 1566, 300, 264, 2068, 50612], "temperature": 0.0, "avg_logprob": -0.22802061887131525, "compression_ratio": 1.770334928229665, "no_speech_prob": 0.008421474136412144}, {"id": 593, "seek": 298094, "start": 2985.9, "end": 2991.5, "text": " claim is very difficult to prove, right? Like the reason the language that has X.", "tokens": [50612, 3932, 307, 588, 2252, 281, 7081, 11, 558, 30, 1743, 264, 1778, 264, 2856, 300, 575, 1783, 13, 50892], "temperature": 0.0, "avg_logprob": -0.22802061887131525, "compression_ratio": 1.770334928229665, "no_speech_prob": 0.008421474136412144}, {"id": 594, "seek": 298094, "start": 2992.2200000000003, "end": 2996.86, "text": " The strong claim that something is not allowed in, in natural languages, I think very, very", "tokens": [50928, 440, 2068, 3932, 300, 746, 307, 406, 4350, 294, 11, 294, 3303, 8650, 11, 286, 519, 588, 11, 588, 51160], "temperature": 0.0, "avg_logprob": -0.22802061887131525, "compression_ratio": 1.770334928229665, "no_speech_prob": 0.008421474136412144}, {"id": 595, "seek": 298094, "start": 2996.86, "end": 3006.54, "text": " difficult to prove. And, you know, I think that there have been lots of, you know, strong attempts,", "tokens": [51160, 2252, 281, 7081, 13, 400, 11, 291, 458, 11, 286, 519, 300, 456, 362, 668, 3195, 295, 11, 291, 458, 11, 2068, 15257, 11, 51644], "temperature": 0.0, "avg_logprob": -0.22802061887131525, "compression_ratio": 1.770334928229665, "no_speech_prob": 0.008421474136412144}, {"id": 596, "seek": 300654, "start": 3006.62, "end": 3013.02, "text": " there's been lots of strong claims from, often from, from generative syntax, right,", "tokens": [50368, 456, 311, 668, 3195, 295, 2068, 9441, 490, 11, 2049, 490, 11, 490, 1337, 1166, 28431, 11, 558, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12926564009293265, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0018091717502102256}, {"id": 597, "seek": 300654, "start": 3013.74, "end": 3020.7, "text": " about what all languages do. And I think that, you know, people have been very good at finding", "tokens": [50724, 466, 437, 439, 8650, 360, 13, 400, 286, 519, 300, 11, 291, 458, 11, 561, 362, 668, 588, 665, 412, 5006, 51072], "temperature": 0.0, "avg_logprob": -0.12926564009293265, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0018091717502102256}, {"id": 598, "seek": 300654, "start": 3020.7, "end": 3025.5, "text": " kind of counter examples to a lot of those things. I cite this paper by Evans and Levinson,", "tokens": [51072, 733, 295, 5682, 5110, 281, 257, 688, 295, 729, 721, 13, 286, 37771, 341, 3035, 538, 30055, 293, 28471, 14321, 11, 51312], "temperature": 0.0, "avg_logprob": -0.12926564009293265, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0018091717502102256}, {"id": 599, "seek": 300654, "start": 3026.54, "end": 3031.42, "text": " which actually, you know, I had heard for years about how no language does X and that's what", "tokens": [51364, 597, 767, 11, 291, 458, 11, 286, 632, 2198, 337, 924, 466, 577, 572, 2856, 775, 1783, 293, 300, 311, 437, 51608], "temperature": 0.0, "avg_logprob": -0.12926564009293265, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0018091717502102256}, {"id": 600, "seek": 303142, "start": 3031.42, "end": 3035.66, "text": " we're using to construct our theories. And that Evans and Levins paper, Evans and Levinson paper", "tokens": [50364, 321, 434, 1228, 281, 7690, 527, 13667, 13, 400, 300, 30055, 293, 28471, 1292, 3035, 11, 30055, 293, 28471, 14321, 3035, 50576], "temperature": 0.0, "avg_logprob": -0.11144426465034485, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.005639911629259586}, {"id": 601, "seek": 303142, "start": 3035.66, "end": 3041.34, "text": " really kind of changed my mind about this, right? That like language is actually much more", "tokens": [50576, 534, 733, 295, 3105, 452, 1575, 466, 341, 11, 558, 30, 663, 411, 2856, 307, 767, 709, 544, 50860], "temperature": 0.0, "avg_logprob": -0.11144426465034485, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.005639911629259586}, {"id": 602, "seek": 303142, "start": 3042.14, "end": 3048.94, "text": " diverse than, than I think most, most syntacticians will, you know, try to construct theories for", "tokens": [50900, 9521, 813, 11, 813, 286, 519, 881, 11, 881, 23980, 578, 8455, 486, 11, 291, 458, 11, 853, 281, 7690, 13667, 337, 51240], "temperature": 0.0, "avg_logprob": -0.11144426465034485, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.005639911629259586}, {"id": 603, "seek": 303142, "start": 3048.94, "end": 3054.94, "text": " something. So, you know, I think we, going back to kind of the beginning of what you said, I think", "tokens": [51240, 746, 13, 407, 11, 291, 458, 11, 286, 519, 321, 11, 516, 646, 281, 733, 295, 264, 2863, 295, 437, 291, 848, 11, 286, 519, 51540], "temperature": 0.0, "avg_logprob": -0.11144426465034485, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.005639911629259586}, {"id": 604, "seek": 305494, "start": 3055.18, "end": 3061.02, "text": " we, we'd agree that, that you need language architectures which learn the things that kids", "tokens": [50376, 321, 11, 321, 1116, 3986, 300, 11, 300, 291, 643, 2856, 6331, 1303, 597, 1466, 264, 721, 300, 2301, 50668], "temperature": 0.0, "avg_logprob": -0.11482505208438205, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.014055013656616211}, {"id": 605, "seek": 305494, "start": 3061.02, "end": 3066.62, "text": " learn and learn it from data that they learn. And those architectures might, might be unlikely to", "tokens": [50668, 1466, 293, 1466, 309, 490, 1412, 300, 436, 1466, 13, 400, 729, 6331, 1303, 1062, 11, 1062, 312, 17518, 281, 50948], "temperature": 0.0, "avg_logprob": -0.11482505208438205, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.014055013656616211}, {"id": 606, "seek": 305494, "start": 3066.62, "end": 3072.78, "text": " be things like LSTMs or, you know, simple recurrent networks or, or whatever, right? Like, I think", "tokens": [50948, 312, 721, 411, 441, 6840, 26386, 420, 11, 291, 458, 11, 2199, 18680, 1753, 9590, 420, 11, 420, 2035, 11, 558, 30, 1743, 11, 286, 519, 51256], "temperature": 0.0, "avg_logprob": -0.11482505208438205, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.014055013656616211}, {"id": 607, "seek": 305494, "start": 3072.78, "end": 3077.58, "text": " all of that work is, is very useful in, in kind of honing in on the right architecture.", "tokens": [51256, 439, 295, 300, 589, 307, 11, 307, 588, 4420, 294, 11, 294, 733, 295, 2157, 278, 294, 322, 264, 558, 9482, 13, 51496], "temperature": 0.0, "avg_logprob": -0.11482505208438205, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.014055013656616211}, {"id": 608, "seek": 307758, "start": 3078.2999999999997, "end": 3085.8199999999997, "text": " So, I'm just trying to, to remember all of, all of the points you were making. Oh, yeah. So,", "tokens": [50400, 407, 11, 286, 478, 445, 1382, 281, 11, 281, 1604, 439, 295, 11, 439, 295, 264, 2793, 291, 645, 1455, 13, 876, 11, 1338, 13, 407, 11, 50776], "temperature": 0.0, "avg_logprob": -0.12656696422680005, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.012051764875650406}, {"id": 609, "seek": 307758, "start": 3086.62, "end": 3093.42, "text": " but I think this, that there, there's a kind of flip side to this, which is that I think that", "tokens": [50816, 457, 286, 519, 341, 11, 300, 456, 11, 456, 311, 257, 733, 295, 7929, 1252, 281, 341, 11, 597, 307, 300, 286, 519, 300, 51156], "temperature": 0.0, "avg_logprob": -0.12656696422680005, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.012051764875650406}, {"id": 610, "seek": 307758, "start": 3093.42, "end": 3099.42, "text": " the space of things people can learn is actually kind of underestimated, right? Like, there's this", "tokens": [51156, 264, 1901, 295, 721, 561, 393, 1466, 307, 767, 733, 295, 24612, 33008, 11, 558, 30, 1743, 11, 456, 311, 341, 51456], "temperature": 0.0, "avg_logprob": -0.12656696422680005, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.012051764875650406}, {"id": 611, "seek": 307758, "start": 3099.42, "end": 3106.2999999999997, "text": " bias to, to, to say, you know, people can't learn X, Y and Z. But people, at least outside of language", "tokens": [51456, 12577, 281, 11, 281, 11, 281, 584, 11, 291, 458, 11, 561, 393, 380, 1466, 1783, 11, 398, 293, 1176, 13, 583, 561, 11, 412, 1935, 2380, 295, 2856, 51800], "temperature": 0.0, "avg_logprob": -0.12656696422680005, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.012051764875650406}, {"id": 612, "seek": 310630, "start": 3106.3, "end": 3110.94, "text": " have this, this really remarkable ability to learn different kinds of patterns, right? Like,", "tokens": [50364, 362, 341, 11, 341, 534, 12802, 3485, 281, 1466, 819, 3685, 295, 8294, 11, 558, 30, 1743, 11, 50596], "temperature": 0.0, "avg_logprob": -0.09263414447590457, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0011327839456498623}, {"id": 613, "seek": 310630, "start": 3110.94, "end": 3117.98, "text": " the patterns you find in, in music or mathematics, for example, we can learn sophisticated types of,", "tokens": [50596, 264, 8294, 291, 915, 294, 11, 294, 1318, 420, 18666, 11, 337, 1365, 11, 321, 393, 1466, 16950, 3467, 295, 11, 50948], "temperature": 0.0, "avg_logprob": -0.09263414447590457, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0011327839456498623}, {"id": 614, "seek": 310630, "start": 3117.98, "end": 3124.2200000000003, "text": " of algorithms, right? We can learn to, you know, fly a space shuttle or to, you know, tie knots in,", "tokens": [50948, 295, 14642, 11, 558, 30, 492, 393, 1466, 281, 11, 291, 458, 11, 3603, 257, 1901, 26728, 420, 281, 11, 291, 458, 11, 7582, 27426, 294, 11, 51260], "temperature": 0.0, "avg_logprob": -0.09263414447590457, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0011327839456498623}, {"id": 615, "seek": 310630, "start": 3124.2200000000003, "end": 3129.42, "text": " for rock climbing or whatever, right? Like, there's all kinds of kind of procedural and", "tokens": [51260, 337, 3727, 14780, 420, 2035, 11, 558, 30, 1743, 11, 456, 311, 439, 3685, 295, 733, 295, 43951, 293, 51520], "temperature": 0.0, "avg_logprob": -0.09263414447590457, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0011327839456498623}, {"id": 616, "seek": 310630, "start": 3129.42, "end": 3134.86, "text": " algorithmic knowledge, which is structural that, that people are able to acquire. And I think that,", "tokens": [51520, 9284, 299, 3601, 11, 597, 307, 15067, 300, 11, 300, 561, 366, 1075, 281, 20001, 13, 400, 286, 519, 300, 11, 51792], "temperature": 0.0, "avg_logprob": -0.09263414447590457, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0011327839456498623}, {"id": 617, "seek": 313486, "start": 3134.86, "end": 3141.7400000000002, "text": " that that notion very rightly kind of motivates looking for learning systems, which can work", "tokens": [50364, 300, 300, 10710, 588, 32879, 733, 295, 42569, 1237, 337, 2539, 3652, 11, 597, 393, 589, 50708], "temperature": 0.0, "avg_logprob": -0.08178749513090326, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008554898668080568}, {"id": 618, "seek": 313486, "start": 3141.7400000000002, "end": 3149.34, "text": " over pretty unrestricted spaces, right? So, you know, you, you, you might say that, okay, well,", "tokens": [50708, 670, 1238, 35103, 3740, 292, 7673, 11, 558, 30, 407, 11, 291, 458, 11, 291, 11, 291, 11, 291, 1062, 584, 300, 11, 1392, 11, 731, 11, 51088], "temperature": 0.0, "avg_logprob": -0.08178749513090326, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008554898668080568}, {"id": 619, "seek": 313486, "start": 3149.34, "end": 3154.78, "text": " language is different because language is a restricted space. And it might be true that,", "tokens": [51088, 2856, 307, 819, 570, 2856, 307, 257, 20608, 1901, 13, 400, 309, 1062, 312, 2074, 300, 11, 51360], "temperature": 0.0, "avg_logprob": -0.08178749513090326, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008554898668080568}, {"id": 620, "seek": 313486, "start": 3154.78, "end": 3158.38, "text": " that language is restricted, but it also might be true that the things we see in language come", "tokens": [51360, 300, 2856, 307, 20608, 11, 457, 309, 611, 1062, 312, 2074, 300, 264, 721, 321, 536, 294, 2856, 808, 51540], "temperature": 0.0, "avg_logprob": -0.08178749513090326, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.0008554898668080568}, {"id": 621, "seek": 315838, "start": 3158.38, "end": 3164.1400000000003, "text": " from other sources, right? It could be that languages, especially pragmatic, for example,", "tokens": [50364, 490, 661, 7139, 11, 558, 30, 467, 727, 312, 300, 8650, 11, 2318, 46904, 11, 337, 1365, 11, 50652], "temperature": 0.0, "avg_logprob": -0.06458420960799507, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.03900511935353279}, {"id": 622, "seek": 315838, "start": 3164.1400000000003, "end": 3169.1800000000003, "text": " compared to music or mathematics, right? And those kinds of pragmatic constraints", "tokens": [50652, 5347, 281, 1318, 420, 18666, 11, 558, 30, 400, 729, 3685, 295, 46904, 18491, 50904], "temperature": 0.0, "avg_logprob": -0.06458420960799507, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.03900511935353279}, {"id": 623, "seek": 315838, "start": 3169.98, "end": 3173.9, "text": " are the things that constrain the, the form of language, right? Or language is communicative,", "tokens": [50944, 366, 264, 721, 300, 1817, 7146, 264, 11, 264, 1254, 295, 2856, 11, 558, 30, 1610, 2856, 307, 3363, 1166, 11, 51140], "temperature": 0.0, "avg_logprob": -0.06458420960799507, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.03900511935353279}, {"id": 624, "seek": 315838, "start": 3173.9, "end": 3178.54, "text": " it's probably more communicative than, than music, for example. And that might constrain the, the,", "tokens": [51140, 309, 311, 1391, 544, 3363, 1166, 813, 11, 813, 1318, 11, 337, 1365, 13, 400, 300, 1062, 1817, 7146, 264, 11, 264, 11, 51372], "temperature": 0.0, "avg_logprob": -0.06458420960799507, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.03900511935353279}, {"id": 625, "seek": 315838, "start": 3178.54, "end": 3183.5, "text": " the form of things. So, I mean, as, as you know, this is very old debate in, in linguistics about", "tokens": [51372, 264, 1254, 295, 721, 13, 407, 11, 286, 914, 11, 382, 11, 382, 291, 458, 11, 341, 307, 588, 1331, 7958, 294, 11, 294, 21766, 6006, 466, 51620], "temperature": 0.0, "avg_logprob": -0.06458420960799507, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.03900511935353279}, {"id": 626, "seek": 318350, "start": 3183.5, "end": 3190.62, "text": " kind of where the, where the properties of, of natural language come from. And I guess what I'm", "tokens": [50364, 733, 295, 689, 264, 11, 689, 264, 7221, 295, 11, 295, 3303, 2856, 808, 490, 13, 400, 286, 2041, 437, 286, 478, 50720], "temperature": 0.0, "avg_logprob": -0.07194830853006114, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.020951295271515846}, {"id": 627, "seek": 318350, "start": 3190.62, "end": 3195.82, "text": " trying to say is that there's one kind of perspective where you look at all of the things humans can do", "tokens": [50720, 1382, 281, 584, 307, 300, 456, 311, 472, 733, 295, 4585, 689, 291, 574, 412, 439, 295, 264, 721, 6255, 393, 360, 50980], "temperature": 0.0, "avg_logprob": -0.07194830853006114, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.020951295271515846}, {"id": 628, "seek": 318350, "start": 3195.82, "end": 3201.26, "text": " even outside of language, all of the rich structures and algorithms and processes we're able to learn", "tokens": [50980, 754, 2380, 295, 2856, 11, 439, 295, 264, 4593, 9227, 293, 14642, 293, 7555, 321, 434, 1075, 281, 1466, 51252], "temperature": 0.0, "avg_logprob": -0.07194830853006114, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.020951295271515846}, {"id": 629, "seek": 318350, "start": 3201.26, "end": 3206.46, "text": " about and internalize. And you say, okay, maybe language is like that. And then yes, language", "tokens": [51252, 466, 293, 6920, 1125, 13, 400, 291, 584, 11, 1392, 11, 1310, 2856, 307, 411, 300, 13, 400, 550, 2086, 11, 2856, 51512], "temperature": 0.0, "avg_logprob": -0.07194830853006114, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.020951295271515846}, {"id": 630, "seek": 318350, "start": 3206.46, "end": 3211.02, "text": " also has some of these other funny little properties. But, you know, maybe those come", "tokens": [51512, 611, 575, 512, 295, 613, 661, 4074, 707, 7221, 13, 583, 11, 291, 458, 11, 1310, 729, 808, 51740], "temperature": 0.0, "avg_logprob": -0.07194830853006114, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.020951295271515846}, {"id": 631, "seek": 321102, "start": 3211.02, "end": 3217.02, "text": " from some other, other pieces of, of where language comes from, right? It's, you know,", "tokens": [50364, 490, 512, 661, 11, 661, 3755, 295, 11, 295, 689, 2856, 1487, 490, 11, 558, 30, 467, 311, 11, 291, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0689002690690287, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0019865192007273436}, {"id": 632, "seek": 321102, "start": 3217.02, "end": 3223.2599999999998, "text": " we have pretty sophisticated pragmatic reasoning. We're using it to achieve certain communicative", "tokens": [50664, 321, 362, 1238, 16950, 46904, 21577, 13, 492, 434, 1228, 309, 281, 4584, 1629, 3363, 1166, 50976], "temperature": 0.0, "avg_logprob": -0.0689002690690287, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0019865192007273436}, {"id": 633, "seek": 321102, "start": 3223.2599999999998, "end": 3229.58, "text": " ends. You can find all kinds of kind of communicative features within the, the language system itself.", "tokens": [50976, 5314, 13, 509, 393, 915, 439, 3685, 295, 733, 295, 3363, 1166, 4122, 1951, 264, 11, 264, 2856, 1185, 2564, 13, 51292], "temperature": 0.0, "avg_logprob": -0.0689002690690287, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0019865192007273436}, {"id": 634, "seek": 321102, "start": 3229.58, "end": 3234.7, "text": " And so, so maybe some of these other properties are, are properties that have some other origin.", "tokens": [51292, 400, 370, 11, 370, 1310, 512, 295, 613, 661, 7221, 366, 11, 366, 7221, 300, 362, 512, 661, 4957, 13, 51548], "temperature": 0.0, "avg_logprob": -0.0689002690690287, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0019865192007273436}, {"id": 635, "seek": 323470, "start": 3235.58, "end": 3241.3399999999997, "text": " And that, that view, I think could be wrong, but it's, it's one that I think needs to be looked at", "tokens": [50408, 400, 300, 11, 300, 1910, 11, 286, 519, 727, 312, 2085, 11, 457, 309, 311, 11, 309, 311, 472, 300, 286, 519, 2203, 281, 312, 2956, 412, 50696], "temperature": 0.0, "avg_logprob": -0.08731276255387527, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.05030256137251854}, {"id": 636, "seek": 323470, "start": 3241.3399999999997, "end": 3251.1, "text": " to see if it's wrong, right? Like, I think it's been kind of dismissed by large chunks of, of", "tokens": [50696, 281, 536, 498, 309, 311, 2085, 11, 558, 30, 1743, 11, 286, 519, 309, 311, 668, 733, 295, 29970, 538, 2416, 24004, 295, 11, 295, 51184], "temperature": 0.0, "avg_logprob": -0.08731276255387527, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.05030256137251854}, {"id": 637, "seek": 323470, "start": 3251.1, "end": 3255.18, "text": " linguists, right? Just, you know, I've heard people say stuff like, oh, well, communication", "tokens": [51184, 21766, 1751, 11, 558, 30, 1449, 11, 291, 458, 11, 286, 600, 2198, 561, 584, 1507, 411, 11, 1954, 11, 731, 11, 6101, 51388], "temperature": 0.0, "avg_logprob": -0.08731276255387527, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.05030256137251854}, {"id": 638, "seek": 323470, "start": 3255.18, "end": 3259.98, "text": " doesn't really explain anything about language, right? And what they mean often is it doesn't", "tokens": [51388, 1177, 380, 534, 2903, 1340, 466, 2856, 11, 558, 30, 400, 437, 436, 914, 2049, 307, 309, 1177, 380, 51628], "temperature": 0.0, "avg_logprob": -0.08731276255387527, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.05030256137251854}, {"id": 639, "seek": 325998, "start": 3259.98, "end": 3264.78, "text": " explain like the particular island constraints or something that they're, that they're working on,", "tokens": [50364, 2903, 411, 264, 1729, 6077, 18491, 420, 746, 300, 436, 434, 11, 300, 436, 434, 1364, 322, 11, 50604], "temperature": 0.0, "avg_logprob": -0.09924839973449708, "compression_ratio": 1.748, "no_speech_prob": 0.003940106835216284}, {"id": 640, "seek": 325998, "start": 3264.78, "end": 3268.62, "text": " right? But there's all kinds of other things in language that communicative pressures probably", "tokens": [50604, 558, 30, 583, 456, 311, 439, 3685, 295, 661, 721, 294, 2856, 300, 3363, 1166, 23573, 1391, 50796], "temperature": 0.0, "avg_logprob": -0.09924839973449708, "compression_ratio": 1.748, "no_speech_prob": 0.003940106835216284}, {"id": 641, "seek": 325998, "start": 3268.62, "end": 3276.22, "text": " do explain. So I guess my, my pitch is always for, for kind of breadth in term, breadth in", "tokens": [50796, 360, 2903, 13, 407, 286, 2041, 452, 11, 452, 7293, 307, 1009, 337, 11, 337, 733, 295, 35862, 294, 1433, 11, 35862, 294, 51176], "temperature": 0.0, "avg_logprob": -0.09924839973449708, "compression_ratio": 1.748, "no_speech_prob": 0.003940106835216284}, {"id": 642, "seek": 325998, "start": 3276.22, "end": 3282.46, "text": " consideration of the forces that, that can shape language and not needing to put it all into some", "tokens": [51176, 12381, 295, 264, 5874, 300, 11, 300, 393, 3909, 2856, 293, 406, 18006, 281, 829, 309, 439, 666, 512, 51488], "temperature": 0.0, "avg_logprob": -0.09924839973449708, "compression_ratio": 1.748, "no_speech_prob": 0.003940106835216284}, {"id": 643, "seek": 325998, "start": 3282.46, "end": 3285.02, "text": " form of, of innate constraints or something like that.", "tokens": [51488, 1254, 295, 11, 295, 41766, 18491, 420, 746, 411, 300, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09924839973449708, "compression_ratio": 1.748, "no_speech_prob": 0.003940106835216284}, {"id": 644, "seek": 328502, "start": 3285.02, "end": 3288.46, "text": " No, no, totally. And I think, I think a lot of that stuff is, is, is compatible with, with, with", "tokens": [50364, 883, 11, 572, 11, 3879, 13, 400, 286, 519, 11, 286, 519, 257, 688, 295, 300, 1507, 307, 11, 307, 11, 307, 18218, 365, 11, 365, 11, 365, 50536], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 645, "seek": 328502, "start": 3288.46, "end": 3292.86, "text": " the minimalist program, because the minimalist program wants syntax to be minimal. It doesn't", "tokens": [50536, 264, 50192, 1461, 11, 570, 264, 50192, 1461, 2738, 28431, 281, 312, 13206, 13, 467, 1177, 380, 50756], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 646, "seek": 328502, "start": 3292.86, "end": 3296.06, "text": " want it to be complicated. It doesn't want it to be, you know, any more complicated than it has to", "tokens": [50756, 528, 309, 281, 312, 6179, 13, 467, 1177, 380, 528, 309, 281, 312, 11, 291, 458, 11, 604, 544, 6179, 813, 309, 575, 281, 50916], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 647, "seek": 328502, "start": 3296.06, "end": 3299.74, "text": " be. So there were some, you mentioned the curious properties, right? So there were some of the", "tokens": [50916, 312, 13, 407, 456, 645, 512, 11, 291, 2835, 264, 6369, 7221, 11, 558, 30, 407, 456, 645, 512, 295, 264, 51100], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 648, "seek": 328502, "start": 3299.74, "end": 3303.98, "text": " properties that need to be counted for in any model of language that are, I'll give you one", "tokens": [51100, 7221, 300, 643, 281, 312, 20150, 337, 294, 604, 2316, 295, 2856, 300, 366, 11, 286, 603, 976, 291, 472, 51312], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 649, "seek": 328502, "start": 3303.98, "end": 3309.02, "text": " example, right? The setting of Pearson features. And these Pearson features exhibit very non,", "tokens": [51312, 1365, 11, 558, 30, 440, 3287, 295, 39041, 4122, 13, 400, 613, 39041, 4122, 20487, 588, 2107, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 650, "seek": 328502, "start": 3309.02, "end": 3313.18, "text": " non trivial generalizations that do not seem to be counted for via domain general learning", "tokens": [51564, 2107, 26703, 2674, 14455, 300, 360, 406, 1643, 281, 312, 20150, 337, 5766, 9274, 2674, 2539, 51772], "temperature": 0.0, "avg_logprob": -0.1363547682762146, "compression_ratio": 2.003030303030303, "no_speech_prob": 0.0024716893676668406}, {"id": 651, "seek": 331318, "start": 3313.2599999999998, "end": 3317.5, "text": " mechanisms. So I'm citing here the work of Daniel Harbour at Queen Mary. So for example,", "tokens": [50368, 15902, 13, 407, 286, 478, 48749, 510, 264, 589, 295, 8033, 3653, 15533, 412, 10077, 6059, 13, 407, 337, 1365, 11, 50580], "temperature": 0.0, "avg_logprob": -0.1388704442532263, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.003650318132713437}, {"id": 652, "seek": 331318, "start": 3317.5, "end": 3322.22, "text": " the morphological composition of Pearson, its interaction of number, its connection to space,", "tokens": [50580, 264, 25778, 4383, 12686, 295, 39041, 11, 1080, 9285, 295, 1230, 11, 1080, 4984, 281, 1901, 11, 50816], "temperature": 0.0, "avg_logprob": -0.1388704442532263, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.003650318132713437}, {"id": 653, "seek": 331318, "start": 3322.8599999999997, "end": 3327.1, "text": " properties of its semantics and its linearization, they all appear to be strong candidates for our", "tokens": [50848, 7221, 295, 1080, 4361, 45298, 293, 1080, 8213, 2144, 11, 436, 439, 4204, 281, 312, 2068, 11255, 337, 527, 51060], "temperature": 0.0, "avg_logprob": -0.1388704442532263, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.003650318132713437}, {"id": 654, "seek": 331318, "start": 3327.1, "end": 3330.46, "text": " knowledge of language, right? What we mean by knowledge of language. But on the other hand,", "tokens": [51060, 3601, 295, 2856, 11, 558, 30, 708, 321, 914, 538, 3601, 295, 2856, 13, 583, 322, 264, 661, 1011, 11, 51228], "temperature": 0.0, "avg_logprob": -0.1388704442532263, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.003650318132713437}, {"id": 655, "seek": 331318, "start": 3330.46, "end": 3335.3399999999997, "text": " we have things like case and agreement and head movement. And these are all structural phenomena.", "tokens": [51228, 321, 362, 721, 411, 1389, 293, 8106, 293, 1378, 3963, 13, 400, 613, 366, 439, 15067, 22004, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1388704442532263, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.003650318132713437}, {"id": 656, "seek": 333534, "start": 3335.9, "end": 3343.1800000000003, "text": " However, they seem to resist a purely meaning based explanation in theoretical linguistics,", "tokens": [50392, 2908, 11, 436, 1643, 281, 4597, 257, 17491, 3620, 2361, 10835, 294, 20864, 21766, 6006, 11, 50756], "temperature": 0.0, "avg_logprob": -0.12394764399764562, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.130074143409729}, {"id": 657, "seek": 333534, "start": 3343.1800000000003, "end": 3346.94, "text": " right? It would be great if syntax were nothing but a computational engine", "tokens": [50756, 558, 30, 467, 576, 312, 869, 498, 28431, 645, 1825, 457, 257, 28270, 2848, 50944], "temperature": 0.0, "avg_logprob": -0.12394764399764562, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.130074143409729}, {"id": 658, "seek": 333534, "start": 3346.94, "end": 3351.26, "text": " that builds structured meaning. And that's the minimalist program, the goal. But that's not", "tokens": [50944, 300, 15182, 18519, 3620, 13, 400, 300, 311, 264, 50192, 1461, 11, 264, 3387, 13, 583, 300, 311, 406, 51160], "temperature": 0.0, "avg_logprob": -0.12394764399764562, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.130074143409729}, {"id": 659, "seek": 333534, "start": 3351.26, "end": 3355.82, "text": " what we actually find. That's not in any actual minimalist, like concrete model, any concrete", "tokens": [51160, 437, 321, 767, 915, 13, 663, 311, 406, 294, 604, 3539, 50192, 11, 411, 9859, 2316, 11, 604, 9859, 51388], "temperature": 0.0, "avg_logprob": -0.12394764399764562, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.130074143409729}, {"id": 660, "seek": 333534, "start": 3355.82, "end": 3361.1000000000004, "text": " minimalist theory. The goal is just like, the program is language is perfect. Okay, that's the", "tokens": [51388, 50192, 5261, 13, 440, 3387, 307, 445, 411, 11, 264, 1461, 307, 2856, 307, 2176, 13, 1033, 11, 300, 311, 264, 51652], "temperature": 0.0, "avg_logprob": -0.12394764399764562, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.130074143409729}, {"id": 661, "seek": 336110, "start": 3361.1, "end": 3365.8199999999997, "text": " program. Is that what we find? No, obviously not. Okay, no, no linguist actually believes that.", "tokens": [50364, 1461, 13, 1119, 300, 437, 321, 915, 30, 883, 11, 2745, 406, 13, 1033, 11, 572, 11, 572, 21766, 468, 767, 12307, 300, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 662, "seek": 336110, "start": 3366.86, "end": 3371.74, "text": " So it'd be great if syntax was like that. But I think, you know, the program is to look for", "tokens": [50652, 407, 309, 1116, 312, 869, 498, 28431, 390, 411, 300, 13, 583, 286, 519, 11, 291, 458, 11, 264, 1461, 307, 281, 574, 337, 50896], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 663, "seek": 336110, "start": 3371.74, "end": 3376.86, "text": " perfection, but not always find it. So case and agreement and head movement are morphological,", "tokens": [50896, 19708, 11, 457, 406, 1009, 915, 309, 13, 407, 1389, 293, 8106, 293, 1378, 3963, 366, 25778, 4383, 11, 51152], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 664, "seek": 336110, "start": 3376.86, "end": 3380.7799999999997, "text": " morphophonological phenomena, the properties of the performance systems, what's called", "tokens": [51152, 25778, 5317, 266, 4383, 22004, 11, 264, 7221, 295, 264, 3389, 3652, 11, 437, 311, 1219, 51348], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 665, "seek": 336110, "start": 3380.7799999999997, "end": 3384.38, "text": " performance systems. And so the minimalist program itself is really compatible with a lot", "tokens": [51348, 3389, 3652, 13, 400, 370, 264, 50192, 1461, 2564, 307, 534, 18218, 365, 257, 688, 51528], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 666, "seek": 336110, "start": 3384.38, "end": 3388.22, "text": " of what you're saying about, you know, language, language, there are aspects of language that", "tokens": [51528, 295, 437, 291, 434, 1566, 466, 11, 291, 458, 11, 2856, 11, 2856, 11, 456, 366, 7270, 295, 2856, 300, 51720], "temperature": 0.0, "avg_logprob": -0.13565309119947028, "compression_ratio": 1.7611464968152866, "no_speech_prob": 0.008714518509805202}, {"id": 667, "seek": 338822, "start": 3388.22, "end": 3394.3799999999997, "text": " can be perfected and optimized for communicative efficiency. Absolutely. Totally. No doubt about", "tokens": [50364, 393, 312, 2176, 292, 293, 26941, 337, 3363, 1166, 10493, 13, 7021, 13, 22837, 13, 883, 6385, 466, 50672], "temperature": 0.0, "avg_logprob": -0.14062870154946538, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.00341378478333354}, {"id": 668, "seek": 338822, "start": 3394.3799999999997, "end": 3399.8199999999997, "text": " it. But where is that locus of efficiency? Is it in the syntax itself? Or is it some kind of", "tokens": [50672, 309, 13, 583, 689, 307, 300, 450, 1149, 295, 10493, 30, 1119, 309, 294, 264, 28431, 2564, 30, 1610, 307, 309, 512, 733, 295, 50944], "temperature": 0.0, "avg_logprob": -0.14062870154946538, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.00341378478333354}, {"id": 669, "seek": 338822, "start": 3399.8199999999997, "end": 3404.22, "text": " extra linguistic system? Is it in pragmatics? You know, is it in century motor? Is it in the", "tokens": [50944, 2857, 43002, 1185, 30, 1119, 309, 294, 33394, 15677, 1167, 30, 509, 458, 11, 307, 309, 294, 4901, 5932, 30, 1119, 309, 294, 264, 51164], "temperature": 0.0, "avg_logprob": -0.14062870154946538, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.00341378478333354}, {"id": 670, "seek": 338822, "start": 3404.22, "end": 3409.98, "text": " speech? And property of speech and phonology? Probably, you know, I mean, who knows. But", "tokens": [51164, 6218, 30, 400, 4707, 295, 6218, 293, 30754, 1793, 30, 9210, 11, 291, 458, 11, 286, 914, 11, 567, 3255, 13, 583, 51452], "temperature": 0.0, "avg_logprob": -0.14062870154946538, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.00341378478333354}, {"id": 671, "seek": 338822, "start": 3409.98, "end": 3416.8599999999997, "text": " I think all of these things demand much more, you know, serious consideration into old fashioned", "tokens": [51452, 286, 519, 439, 295, 613, 721, 4733, 709, 544, 11, 291, 458, 11, 3156, 12381, 666, 1331, 40646, 51796], "temperature": 0.0, "avg_logprob": -0.14062870154946538, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.00341378478333354}, {"id": 672, "seek": 341686, "start": 3416.86, "end": 3420.6200000000003, "text": " notions like structure dependence, compositionality and what have you, things like that, which", "tokens": [50364, 35799, 411, 3877, 31704, 11, 12686, 1860, 293, 437, 362, 291, 11, 721, 411, 300, 11, 597, 50552], "temperature": 0.0, "avg_logprob": -0.1381680679321289, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00393605837598443}, {"id": 673, "seek": 341686, "start": 3420.6200000000003, "end": 3425.82, "text": " you can maybe find somewhere in the literature, but even just basic topics like, you know,", "tokens": [50552, 291, 393, 1310, 915, 4079, 294, 264, 10394, 11, 457, 754, 445, 3875, 8378, 411, 11, 291, 458, 11, 50812], "temperature": 0.0, "avg_logprob": -0.1381680679321289, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00393605837598443}, {"id": 674, "seek": 341686, "start": 3426.78, "end": 3433.1, "text": " quantifier raising, extended projections, adverbial hierarchies, all of these things", "tokens": [50860, 4426, 9902, 11225, 11, 10913, 32371, 11, 614, 25809, 831, 35250, 530, 11, 439, 295, 613, 721, 51176], "temperature": 0.0, "avg_logprob": -0.1381680679321289, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00393605837598443}, {"id": 675, "seek": 341686, "start": 3433.1, "end": 3438.2200000000003, "text": " in the minimalist program can be extra linguistic, right? They can actually be outside of syntax and", "tokens": [51176, 294, 264, 50192, 1461, 393, 312, 2857, 43002, 11, 558, 30, 814, 393, 767, 312, 2380, 295, 28431, 293, 51432], "temperature": 0.0, "avg_logprob": -0.1381680679321289, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00393605837598443}, {"id": 676, "seek": 341686, "start": 3439.34, "end": 3444.1400000000003, "text": " very queer properties of the semantic conceptual systems, which are in themselves kind of domain", "tokens": [51488, 588, 20323, 7221, 295, 264, 47982, 24106, 3652, 11, 597, 366, 294, 2969, 733, 295, 9274, 51728], "temperature": 0.0, "avg_logprob": -0.1381680679321289, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00393605837598443}, {"id": 677, "seek": 344414, "start": 3444.14, "end": 3449.18, "text": " general, weird leftovers from ancient primate cognition, right? The features of the way we", "tokens": [50364, 2674, 11, 3657, 43011, 490, 7832, 2886, 473, 46905, 11, 558, 30, 440, 4122, 295, 264, 636, 321, 50616], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 678, "seek": 344414, "start": 3449.18, "end": 3453.2599999999998, "text": " pass events, the way we pass, you know, agents and patients, things like that. That's definitely not,", "tokens": [50616, 1320, 3931, 11, 264, 636, 321, 1320, 11, 291, 458, 11, 12554, 293, 4209, 11, 721, 411, 300, 13, 663, 311, 2138, 406, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 679, "seek": 344414, "start": 3453.2599999999998, "end": 3458.3799999999997, "text": " that's not human specific. But, you know, the way that syntax provides instructions to these", "tokens": [50820, 300, 311, 406, 1952, 2685, 13, 583, 11, 291, 458, 11, 264, 636, 300, 28431, 6417, 9415, 281, 613, 51076], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 680, "seek": 344414, "start": 3458.3799999999997, "end": 3464.46, "text": " systems, you know, probably seems to be. So, you know, generative linguists have different theories of", "tokens": [51076, 3652, 11, 291, 458, 11, 1391, 2544, 281, 312, 13, 407, 11, 291, 458, 11, 1337, 1166, 21766, 1751, 362, 819, 13667, 295, 51380], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 681, "seek": 344414, "start": 3464.46, "end": 3468.14, "text": " also language production too. I'll just talk about language production based on whether we", "tokens": [51380, 611, 2856, 4265, 886, 13, 286, 603, 445, 751, 466, 2856, 4265, 2361, 322, 1968, 321, 51564], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 682, "seek": 344414, "start": 3468.14, "end": 3472.22, "text": " store lemmas or whether we build words in the exact same way we will phrase and sentence this.", "tokens": [51564, 3531, 7495, 3799, 420, 1968, 321, 1322, 2283, 294, 264, 1900, 912, 636, 321, 486, 9535, 293, 8174, 341, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1347007893804294, "compression_ratio": 1.79375, "no_speech_prob": 0.0010444620857015252}, {"id": 683, "seek": 347222, "start": 3472.22, "end": 3475.66, "text": " So, I know that you make distinction between construction grammar and kind of generative", "tokens": [50364, 407, 11, 286, 458, 300, 291, 652, 16844, 1296, 6435, 22317, 293, 733, 295, 1337, 1166, 50536], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 684, "seek": 347222, "start": 3475.66, "end": 3480.22, "text": " grammar and, you know, the weight they place on memorizing constructions versus just building", "tokens": [50536, 22317, 293, 11, 291, 458, 11, 264, 3364, 436, 1081, 322, 10560, 3319, 7690, 626, 5717, 445, 2390, 50764], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 685, "seek": 347222, "start": 3480.22, "end": 3485.18, "text": " things from the bottom up, from the ground up, right? And so, you know, in some generative inspired", "tokens": [50764, 721, 490, 264, 2767, 493, 11, 490, 264, 2727, 493, 11, 558, 30, 400, 370, 11, 291, 458, 11, 294, 512, 1337, 1166, 7547, 51012], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 686, "seek": 347222, "start": 3485.18, "end": 3489.8999999999996, "text": " models, mechanisms which generate syntactic structure make no distinctions between processes", "tokens": [51012, 5245, 11, 15902, 597, 8460, 23980, 19892, 3877, 652, 572, 1483, 49798, 1296, 7555, 51248], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 687, "seek": 347222, "start": 3489.8999999999996, "end": 3495.4199999999996, "text": " that apply above or below the word level. There's no point at which meaning syntax and form are", "tokens": [51248, 300, 3079, 3673, 420, 2507, 264, 1349, 1496, 13, 821, 311, 572, 935, 412, 597, 3620, 28431, 293, 1254, 366, 51524], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 688, "seek": 347222, "start": 3495.4199999999996, "end": 3500.06, "text": " all stored together as single atomic representations. Each stage in lexical access is a transition", "tokens": [51524, 439, 12187, 1214, 382, 2167, 22275, 33358, 13, 6947, 3233, 294, 476, 87, 804, 2105, 307, 257, 6034, 51756], "temperature": 0.0, "avg_logprob": -0.0974190850411692, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0009992151753976941}, {"id": 689, "seek": 350006, "start": 3500.06, "end": 3504.54, "text": " between different kinds of data structures, right? There's meaning, there's form and there's", "tokens": [50364, 1296, 819, 3685, 295, 1412, 9227, 11, 558, 30, 821, 311, 3620, 11, 456, 311, 1254, 293, 456, 311, 50588], "temperature": 0.0, "avg_logprob": -0.10662139858211483, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.0030060317367315292}, {"id": 690, "seek": 350006, "start": 3504.54, "end": 3508.86, "text": " syntax. These three features kind of come in together and they don't always overlap. Different", "tokens": [50588, 28431, 13, 1981, 1045, 4122, 733, 295, 808, 294, 1214, 293, 436, 500, 380, 1009, 19959, 13, 20825, 50804], "temperature": 0.0, "avg_logprob": -0.10662139858211483, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.0030060317367315292}, {"id": 691, "seek": 350006, "start": 3508.86, "end": 3515.42, "text": " languages realize them in different ways. And so, you know, a word, the basic definition of a word", "tokens": [50804, 8650, 4325, 552, 294, 819, 2098, 13, 400, 370, 11, 291, 458, 11, 257, 1349, 11, 264, 3875, 7123, 295, 257, 1349, 51132], "temperature": 0.0, "avg_logprob": -0.10662139858211483, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.0030060317367315292}, {"id": 692, "seek": 350006, "start": 3515.42, "end": 3521.34, "text": " is just this weird multi-system definition where lots of things, lots of different cognitive systems", "tokens": [51132, 307, 445, 341, 3657, 4825, 12, 28215, 7123, 689, 3195, 295, 721, 11, 3195, 295, 819, 15605, 3652, 51428], "temperature": 0.0, "avg_logprob": -0.10662139858211483, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.0030060317367315292}, {"id": 693, "seek": 350006, "start": 3521.34, "end": 3526.38, "text": " enrich the basis of every lexical item, right? You have, there's nothing like this really,", "tokens": [51428, 18849, 264, 5143, 295, 633, 476, 87, 804, 3174, 11, 558, 30, 509, 362, 11, 456, 311, 1825, 411, 341, 534, 11, 51680], "temperature": 0.0, "avg_logprob": -0.10662139858211483, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.0030060317367315292}, {"id": 694, "seek": 352638, "start": 3526.38, "end": 3531.1, "text": " this enrichment process anywhere else in linguistic theory, right? Or at least in", "tokens": [50364, 341, 49900, 1399, 4992, 1646, 294, 43002, 5261, 11, 558, 30, 1610, 412, 1935, 294, 50600], "temperature": 0.0, "avg_logprob": -0.11822725477672759, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.004151447676122189}, {"id": 695, "seek": 352638, "start": 3531.1, "end": 3538.06, "text": " what LLMs are doing. Like, so I guess, what, I guess I would ask you, what is your definition", "tokens": [50600, 437, 441, 43, 26386, 366, 884, 13, 1743, 11, 370, 286, 2041, 11, 437, 11, 286, 2041, 286, 576, 1029, 291, 11, 437, 307, 428, 7123, 50948], "temperature": 0.0, "avg_logprob": -0.11822725477672759, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.004151447676122189}, {"id": 696, "seek": 352638, "start": 3538.06, "end": 3543.7400000000002, "text": " of a word, right? And what can LLMs really provide insights into weirdhood, right? Because if you", "tokens": [50948, 295, 257, 1349, 11, 558, 30, 400, 437, 393, 441, 43, 26386, 534, 2893, 14310, 666, 3657, 3809, 11, 558, 30, 1436, 498, 291, 51232], "temperature": 0.0, "avg_logprob": -0.11822725477672759, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.004151447676122189}, {"id": 697, "seek": 352638, "start": 3543.7400000000002, "end": 3547.5, "text": " can't, if you don't have a definition of what a word is, then you're really in trouble, right?", "tokens": [51232, 393, 380, 11, 498, 291, 500, 380, 362, 257, 7123, 295, 437, 257, 1349, 307, 11, 550, 291, 434, 534, 294, 5253, 11, 558, 30, 51420], "temperature": 0.0, "avg_logprob": -0.11822725477672759, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.004151447676122189}, {"id": 698, "seek": 352638, "start": 3547.5, "end": 3552.86, "text": " Like, we have to at least use LLMs or artificial systems to inform what we mean by a word. Or", "tokens": [51420, 1743, 11, 321, 362, 281, 412, 1935, 764, 441, 43, 26386, 420, 11677, 3652, 281, 1356, 437, 321, 914, 538, 257, 1349, 13, 1610, 51688], "temperature": 0.0, "avg_logprob": -0.11822725477672759, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.004151447676122189}, {"id": 699, "seek": 355286, "start": 3552.86, "end": 3557.7400000000002, "text": " maybe we don't need that anymore. I'm not sure what you think. I'm not sure what you mean. I mean,", "tokens": [50364, 1310, 321, 500, 380, 643, 300, 3602, 13, 286, 478, 406, 988, 437, 291, 519, 13, 286, 478, 406, 988, 437, 291, 914, 13, 286, 914, 11, 50608], "temperature": 0.0, "avg_logprob": -0.13971288354547173, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0028839486185461283}, {"id": 700, "seek": 355286, "start": 3560.7000000000003, "end": 3566.6200000000003, "text": " I don't have a... What is a word? Why does that matter? I mean, that's just a convention about", "tokens": [50756, 286, 500, 380, 362, 257, 485, 708, 307, 257, 1349, 30, 1545, 775, 300, 1871, 30, 286, 914, 11, 300, 311, 445, 257, 10286, 466, 51052], "temperature": 0.0, "avg_logprob": -0.13971288354547173, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0028839486185461283}, {"id": 701, "seek": 355286, "start": 3566.6200000000003, "end": 3572.3, "text": " how we use the term word, right? What, like, I mean, you could use, you know, lemmas or word", "tokens": [51052, 577, 321, 764, 264, 1433, 1349, 11, 558, 30, 708, 11, 411, 11, 286, 914, 11, 291, 727, 764, 11, 291, 458, 11, 7495, 3799, 420, 1349, 51336], "temperature": 0.0, "avg_logprob": -0.13971288354547173, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0028839486185461283}, {"id": 702, "seek": 355286, "start": 3572.3, "end": 3577.58, "text": " firms or whatever. Like, that just feels like a conventional choice. I'm not sure what's at,", "tokens": [51336, 18055, 420, 2035, 13, 1743, 11, 300, 445, 3417, 411, 257, 16011, 3922, 13, 286, 478, 406, 988, 437, 311, 412, 11, 51600], "temperature": 0.0, "avg_logprob": -0.13971288354547173, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0028839486185461283}, {"id": 703, "seek": 357758, "start": 3577.58, "end": 3583.5, "text": " what's at stake there. So how would you, I guess I would say, I agree, word is a conventionalization,", "tokens": [50364, 437, 311, 412, 10407, 456, 13, 407, 577, 576, 291, 11, 286, 2041, 286, 576, 584, 11, 286, 3986, 11, 1349, 307, 257, 16011, 2144, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1172660734595322, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.012195919640362263}, {"id": 704, "seek": 357758, "start": 3583.5, "end": 3589.74, "text": " you know. Our intuitive concept of word is often biased by orthography, the way we put spaces between", "tokens": [50660, 291, 458, 13, 2621, 21769, 3410, 295, 1349, 307, 2049, 28035, 538, 19052, 5820, 11, 264, 636, 321, 829, 7673, 1296, 50972], "temperature": 0.0, "avg_logprob": -0.1172660734595322, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.012195919640362263}, {"id": 705, "seek": 357758, "start": 3589.74, "end": 3594.2999999999997, "text": " things, right? So I agree with that criticism. You know, word in the intuitive sense is not really", "tokens": [50972, 721, 11, 558, 30, 407, 286, 3986, 365, 300, 15835, 13, 509, 458, 11, 1349, 294, 264, 21769, 2020, 307, 406, 534, 51200], "temperature": 0.0, "avg_logprob": -0.1172660734595322, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.012195919640362263}, {"id": 706, "seek": 357758, "start": 3594.2999999999997, "end": 3599.34, "text": " a scientific construct. However, I guess, let me rephrase my question. How would you, you know,", "tokens": [51200, 257, 8134, 7690, 13, 2908, 11, 286, 2041, 11, 718, 385, 319, 44598, 651, 452, 1168, 13, 1012, 576, 291, 11, 291, 458, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1172660734595322, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.012195919640362263}, {"id": 707, "seek": 357758, "start": 3599.34, "end": 3603.5, "text": " decompose the intuitive concept of word into something that is more kind of, you know,", "tokens": [51452, 22867, 541, 264, 21769, 3410, 295, 1349, 666, 746, 300, 307, 544, 733, 295, 11, 291, 458, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1172660734595322, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.012195919640362263}, {"id": 708, "seek": 360350, "start": 3603.58, "end": 3607.42, "text": " scientifically amenable or psychologically plausible, which is exactly what genitive", "tokens": [50368, 39719, 18497, 712, 420, 41387, 39925, 11, 597, 307, 2293, 437, 1049, 2187, 50560], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 709, "seek": 360350, "start": 3607.42, "end": 3611.34, "text": " grammar tries to do by decomposing words into, you know, distinctive features,", "tokens": [50560, 22317, 9898, 281, 360, 538, 22867, 6110, 2283, 666, 11, 291, 458, 11, 27766, 4122, 11, 50756], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 710, "seek": 360350, "start": 3611.9, "end": 3616.86, "text": " morphological categories, conceptual roots being matched with categorical features,", "tokens": [50784, 25778, 4383, 10479, 11, 24106, 10669, 885, 21447, 365, 19250, 804, 4122, 11, 51032], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 711, "seek": 360350, "start": 3616.86, "end": 3622.38, "text": " you know, you get a concept, you know, and you match it with a noun or a verb category to get a noun", "tokens": [51032, 291, 458, 11, 291, 483, 257, 3410, 11, 291, 458, 11, 293, 291, 2995, 309, 365, 257, 23307, 420, 257, 9595, 7719, 281, 483, 257, 23307, 51308], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 712, "seek": 360350, "start": 3622.38, "end": 3627.74, "text": " or a verb. These different models make different predictions, right? Yeah, I mean, I think that", "tokens": [51308, 420, 257, 9595, 13, 1981, 819, 5245, 652, 819, 21264, 11, 558, 30, 865, 11, 286, 914, 11, 286, 519, 300, 51576], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 713, "seek": 360350, "start": 3627.74, "end": 3632.54, "text": " general idea is likely to be right for large language models. Like, I think they kind of", "tokens": [51576, 2674, 1558, 307, 3700, 281, 312, 558, 337, 2416, 2856, 5245, 13, 1743, 11, 286, 519, 436, 733, 295, 51816], "temperature": 0.0, "avg_logprob": -0.15729674354928438, "compression_ratio": 1.8129251700680271, "no_speech_prob": 0.002881552092730999}, {"id": 714, "seek": 363254, "start": 3632.54, "end": 3637.66, "text": " must have things that are kind of like part of speech categories, for example. And I think that", "tokens": [50364, 1633, 362, 721, 300, 366, 733, 295, 411, 644, 295, 6218, 10479, 11, 337, 1365, 13, 400, 286, 519, 300, 50620], "temperature": 0.0, "avg_logprob": -0.07412863750847018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0003919716691598296}, {"id": 715, "seek": 363254, "start": 3637.66, "end": 3645.5, "text": " they kind of must be able to update those, their categories based on the language that they've seen", "tokens": [50620, 436, 733, 295, 1633, 312, 1075, 281, 5623, 729, 11, 641, 10479, 2361, 322, 264, 2856, 300, 436, 600, 1612, 51012], "temperature": 0.0, "avg_logprob": -0.07412863750847018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0003919716691598296}, {"id": 716, "seek": 363254, "start": 3645.5, "end": 3652.3, "text": " so far, right? So like, like, you know, GPT puts nouns and verbs in the right places. And to do", "tokens": [51012, 370, 1400, 11, 558, 30, 407, 411, 11, 411, 11, 291, 458, 11, 26039, 51, 8137, 48184, 293, 30051, 294, 264, 558, 3190, 13, 400, 281, 360, 51352], "temperature": 0.0, "avg_logprob": -0.07412863750847018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0003919716691598296}, {"id": 717, "seek": 363254, "start": 3652.3, "end": 3657.58, "text": " that, you kind of need some representation of the nouns versus the verbs, and you need some ability to", "tokens": [51352, 300, 11, 291, 733, 295, 643, 512, 10290, 295, 264, 48184, 5717, 264, 30051, 11, 293, 291, 643, 512, 3485, 281, 51616], "temperature": 0.0, "avg_logprob": -0.07412863750847018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0003919716691598296}, {"id": 718, "seek": 365758, "start": 3658.38, "end": 3663.2599999999998, "text": " locate yourself in a string of other words and figure out if there's likely to be a noun or a", "tokens": [50404, 22370, 1803, 294, 257, 6798, 295, 661, 2283, 293, 2573, 484, 498, 456, 311, 3700, 281, 312, 257, 23307, 420, 257, 50648], "temperature": 0.0, "avg_logprob": -0.15437682815220044, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034829440992325544}, {"id": 719, "seek": 365758, "start": 3663.2599999999998, "end": 3670.62, "text": " verb next. So I think that on that level, those kinds of properties of words are very likely to", "tokens": [50648, 9595, 958, 13, 407, 286, 519, 300, 322, 300, 1496, 11, 729, 3685, 295, 7221, 295, 2283, 366, 588, 3700, 281, 51016], "temperature": 0.0, "avg_logprob": -0.15437682815220044, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034829440992325544}, {"id": 720, "seek": 365758, "start": 3670.62, "end": 3677.42, "text": " be right. And there are also things which are very likely to be found kind of in the internal", "tokens": [51016, 312, 558, 13, 400, 456, 366, 611, 721, 597, 366, 588, 3700, 281, 312, 1352, 733, 295, 294, 264, 6920, 51356], "temperature": 0.0, "avg_logprob": -0.15437682815220044, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034829440992325544}, {"id": 721, "seek": 365758, "start": 3677.42, "end": 3682.46, "text": " representations of these models. I don't see how it could be any other way other than that.", "tokens": [51356, 33358, 295, 613, 5245, 13, 286, 500, 380, 536, 577, 309, 727, 312, 604, 661, 636, 661, 813, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15437682815220044, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034829440992325544}, {"id": 722, "seek": 368246, "start": 3683.1, "end": 3692.14, "text": " But like, as far as I know, that's not where the main debates or disagreement, I think, is,", "tokens": [50396, 583, 411, 11, 382, 1400, 382, 286, 458, 11, 300, 311, 406, 689, 264, 2135, 24203, 420, 38947, 11, 286, 519, 11, 307, 11, 50848], "temperature": 0.0, "avg_logprob": -0.24954664092702963, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00817795004695654}, {"id": 723, "seek": 368246, "start": 3692.14, "end": 3699.58, "text": " right? Like, I think all theories of language have to have to say that there's different kinds of", "tokens": [50848, 558, 30, 1743, 11, 286, 519, 439, 13667, 295, 2856, 362, 281, 362, 281, 584, 300, 456, 311, 819, 3685, 295, 51220], "temperature": 0.0, "avg_logprob": -0.24954664092702963, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00817795004695654}, {"id": 724, "seek": 368246, "start": 3699.58, "end": 3704.78, "text": " words that can show up in different places or something like that. Yeah. Okay, so how about", "tokens": [51220, 2283, 300, 393, 855, 493, 294, 819, 3190, 420, 746, 411, 300, 13, 865, 13, 1033, 11, 370, 577, 466, 51480], "temperature": 0.0, "avg_logprob": -0.24954664092702963, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00817795004695654}, {"id": 725, "seek": 368246, "start": 3704.78, "end": 3710.62, "text": " the issue? You mentioned communication, right? So, you know, and you're totally right, when Trump", "tokens": [51480, 264, 2734, 30, 509, 2835, 6101, 11, 558, 30, 407, 11, 291, 458, 11, 293, 291, 434, 3879, 558, 11, 562, 3899, 51772], "temperature": 0.0, "avg_logprob": -0.24954664092702963, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00817795004695654}, {"id": 726, "seek": 371062, "start": 3711.58, "end": 3715.58, "text": " says things like language is a thought system or, you know, language didn't evolve,", "tokens": [50412, 1619, 721, 411, 2856, 307, 257, 1194, 1185, 420, 11, 291, 458, 11, 2856, 994, 380, 16693, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 727, "seek": 371062, "start": 3716.38, "end": 3719.66, "text": " he's kind of being a little bit cheeky. He doesn't really mean that. He kind of means it in a very", "tokens": [50652, 415, 311, 733, 295, 885, 257, 707, 857, 12839, 88, 13, 634, 1177, 380, 534, 914, 300, 13, 634, 733, 295, 1355, 309, 294, 257, 588, 50816], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 728, "seek": 371062, "start": 3719.66, "end": 3724.62, "text": " specific sense, right? But, you know, when we say language is a thought system, what we mean is", "tokens": [50816, 2685, 2020, 11, 558, 30, 583, 11, 291, 458, 11, 562, 321, 584, 2856, 307, 257, 1194, 1185, 11, 437, 321, 914, 307, 51064], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 729, "seek": 371062, "start": 3725.74, "end": 3729.42, "text": " we're trying to get it an architectural claim. So if you look at the architecture of the minimalist", "tokens": [51120, 321, 434, 1382, 281, 483, 309, 364, 26621, 3932, 13, 407, 498, 291, 574, 412, 264, 9482, 295, 264, 50192, 51304], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 730, "seek": 371062, "start": 3729.42, "end": 3735.18, "text": " program, the syntactic derivation and the conceptual systems are literally different systems, right?", "tokens": [51304, 1461, 11, 264, 23980, 19892, 10151, 399, 293, 264, 24106, 3652, 366, 3736, 819, 3652, 11, 558, 30, 51592], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 731, "seek": 371062, "start": 3735.18, "end": 3739.3399999999997, "text": " The conceptual systems take stuff from syntax and then does its own business with it and the", "tokens": [51592, 440, 24106, 3652, 747, 1507, 490, 28431, 293, 550, 775, 1080, 1065, 1606, 365, 309, 293, 264, 51800], "temperature": 0.0, "avg_logprob": -0.12021353665520162, "compression_ratio": 1.8511326860841424, "no_speech_prob": 0.002775870030745864}, {"id": 732, "seek": 373934, "start": 3739.34, "end": 3744.1400000000003, "text": " CI systems have their own peculiar rules and principles, which is why thought and language", "tokens": [50364, 37777, 3652, 362, 641, 1065, 27149, 4474, 293, 9156, 11, 597, 307, 983, 1194, 293, 2856, 50604], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 733, "seek": 373934, "start": 3744.1400000000003, "end": 3749.5, "text": " are both similar symbolic compositional systems, but in different ways. Only a subset of thought", "tokens": [50604, 366, 1293, 2531, 25755, 10199, 2628, 3652, 11, 457, 294, 819, 2098, 13, 5686, 257, 25993, 295, 1194, 50872], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 734, "seek": 373934, "start": 3749.5, "end": 3755.58, "text": " is properly called the CI interface system, since the CI systems are by definition, you know,", "tokens": [50872, 307, 6108, 1219, 264, 37777, 9226, 1185, 11, 1670, 264, 37777, 3652, 366, 538, 7123, 11, 291, 458, 11, 51176], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 735, "seek": 373934, "start": 3755.58, "end": 3760.46, "text": " whatever conceptual systems you would have that can access and read out instructions from syntax.", "tokens": [51176, 2035, 24106, 3652, 291, 576, 362, 300, 393, 2105, 293, 1401, 484, 9415, 490, 28431, 13, 51420], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 736, "seek": 373934, "start": 3760.46, "end": 3763.7400000000002, "text": " And we don't know what they are fully. They seem to have something to do with events and", "tokens": [51420, 400, 321, 500, 380, 458, 437, 436, 366, 4498, 13, 814, 1643, 281, 362, 746, 281, 360, 365, 3931, 293, 51584], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 737, "seek": 373934, "start": 3763.7400000000002, "end": 3767.58, "text": " grammatical reference and definiteness. They seem to be the main categories that language,", "tokens": [51584, 17570, 267, 804, 6408, 293, 1561, 6009, 442, 13, 814, 1643, 281, 312, 264, 2135, 10479, 300, 2856, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09509099116090869, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.0004709115601144731}, {"id": 738, "seek": 376758, "start": 3767.58, "end": 3771.98, "text": " you know, cares about conceptually, but we don't really know. That's kind of just a hypothesis,", "tokens": [50364, 291, 458, 11, 12310, 466, 3410, 671, 11, 457, 321, 500, 380, 534, 458, 13, 663, 311, 733, 295, 445, 257, 17291, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1417545799736504, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.011609652079641819}, {"id": 739, "seek": 376758, "start": 3771.98, "end": 3778.06, "text": " right? But what we do know is that they don't seem to make use of color all that much. So no", "tokens": [50584, 558, 30, 583, 437, 321, 360, 458, 307, 300, 436, 500, 380, 1643, 281, 652, 764, 295, 2017, 439, 300, 709, 13, 407, 572, 50888], "temperature": 0.0, "avg_logprob": -0.1417545799736504, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.011609652079641819}, {"id": 740, "seek": 376758, "start": 3778.06, "end": 3785.58, "text": " language morphologically marks shades of color. All the conceptual features like worry or concern,", "tokens": [50888, 2856, 25778, 17157, 10640, 20639, 295, 2017, 13, 1057, 264, 24106, 4122, 411, 3292, 420, 3136, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1417545799736504, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.011609652079641819}, {"id": 741, "seek": 376758, "start": 3785.58, "end": 3789.18, "text": " like no language morphologically marks a degree of worry or concern about an issue,", "tokens": [51264, 411, 572, 2856, 25778, 17157, 10640, 257, 4314, 295, 3292, 420, 3136, 466, 364, 2734, 11, 51444], "temperature": 0.0, "avg_logprob": -0.1417545799736504, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.011609652079641819}, {"id": 742, "seek": 376758, "start": 3789.18, "end": 3795.18, "text": " but we do make use of epistemological notions like evidentiality and things like that.", "tokens": [51444, 457, 321, 360, 652, 764, 295, 2388, 43958, 4383, 35799, 411, 16371, 831, 507, 293, 721, 411, 300, 13, 51744], "temperature": 0.0, "avg_logprob": -0.1417545799736504, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.011609652079641819}, {"id": 743, "seek": 379518, "start": 3795.18, "end": 3799.74, "text": " So, you know, I guess what I'm saying is the minimalist program does a good job of", "tokens": [50364, 407, 11, 291, 458, 11, 286, 2041, 437, 286, 478, 1566, 307, 264, 50192, 1461, 775, 257, 665, 1691, 295, 50592], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 744, "seek": 379518, "start": 3800.46, "end": 3804.54, "text": " trying to figure out which aspects of thought language is intimately tied to,", "tokens": [50628, 1382, 281, 2573, 484, 597, 7270, 295, 1194, 2856, 307, 560, 5401, 9601, 281, 11, 50832], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 745, "seek": 379518, "start": 3804.54, "end": 3809.02, "text": " and which aspects of thought it's not tied to. So the minimalist program allows us to kind of carve", "tokens": [50832, 293, 597, 7270, 295, 1194, 309, 311, 406, 9601, 281, 13, 407, 264, 50192, 1461, 4045, 505, 281, 733, 295, 33832, 51056], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 746, "seek": 379518, "start": 3809.02, "end": 3813.58, "text": " that up quite neatly. And this is a much more nuanced framework than, you know, when Chomsky", "tokens": [51056, 300, 493, 1596, 36634, 13, 400, 341, 307, 257, 709, 544, 45115, 8388, 813, 11, 291, 458, 11, 562, 761, 4785, 4133, 51284], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 747, "seek": 379518, "start": 3813.58, "end": 3818.06, "text": " says language is thought, again, he doesn't, maybe he means it, maybe he doesn't, but that's not what", "tokens": [51284, 1619, 2856, 307, 1194, 11, 797, 11, 415, 1177, 380, 11, 1310, 415, 1355, 309, 11, 1310, 415, 1177, 380, 11, 457, 300, 311, 406, 437, 51508], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 748, "seek": 379518, "start": 3818.06, "end": 3823.1, "text": " the actual architecture of his theory says. It's a rhetorical device that is very, you know, useful", "tokens": [51508, 264, 3539, 9482, 295, 702, 5261, 1619, 13, 467, 311, 257, 24182, 284, 804, 4302, 300, 307, 588, 11, 291, 458, 11, 4420, 51760], "temperature": 0.0, "avg_logprob": -0.1045744537461734, "compression_ratio": 1.85, "no_speech_prob": 0.00047657685354352}, {"id": 749, "seek": 382310, "start": 3823.1, "end": 3828.86, "text": " and interesting to attract undergraduate audiences. But if you look at actual theories that are coming", "tokens": [50364, 293, 1880, 281, 5049, 19113, 15479, 13, 583, 498, 291, 574, 412, 3539, 13667, 300, 366, 1348, 50652], "temperature": 0.0, "avg_logprob": -0.10205236781727184, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.0019397832220420241}, {"id": 750, "seek": 382310, "start": 3828.86, "end": 3833.02, "text": " out of the minimalist program, no one really believes language equals thought, right? The language", "tokens": [50652, 484, 295, 264, 50192, 1461, 11, 572, 472, 534, 12307, 2856, 6915, 1194, 11, 558, 30, 440, 2856, 50860], "temperature": 0.0, "avg_logprob": -0.10205236781727184, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.0019397832220420241}, {"id": 751, "seek": 382310, "start": 3833.02, "end": 3837.9, "text": " system seems to, it tries its best to access and reformat and manipulate various conceptual", "tokens": [50860, 1185, 2544, 281, 11, 309, 9898, 1080, 1151, 281, 2105, 293, 8290, 267, 293, 20459, 3683, 24106, 51104], "temperature": 0.0, "avg_logprob": -0.10205236781727184, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.0019397832220420241}, {"id": 752, "seek": 382310, "start": 3837.9, "end": 3842.22, "text": " systems, but it has its limits, right? We know what systems, spell keys, core knowledge systems are", "tokens": [51104, 3652, 11, 457, 309, 575, 1080, 10406, 11, 558, 30, 492, 458, 437, 3652, 11, 9827, 9317, 11, 4965, 3601, 3652, 366, 51320], "temperature": 0.0, "avg_logprob": -0.10205236781727184, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.0019397832220420241}, {"id": 753, "seek": 382310, "start": 3842.22, "end": 3848.86, "text": " hooked up to with respect to the syntax engine, and which ones are not. So, you know, this kind of", "tokens": [51320, 20410, 493, 281, 365, 3104, 281, 264, 28431, 2848, 11, 293, 597, 2306, 366, 406, 13, 407, 11, 291, 458, 11, 341, 733, 295, 51652], "temperature": 0.0, "avg_logprob": -0.10205236781727184, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.0019397832220420241}, {"id": 754, "seek": 384886, "start": 3848.86, "end": 3853.82, "text": " gets back to the idea that lexicalization of a concept seems to maybe alter it in some way.", "tokens": [50364, 2170, 646, 281, 264, 1558, 300, 476, 87, 804, 2144, 295, 257, 3410, 2544, 281, 1310, 11337, 309, 294, 512, 636, 13, 50612], "temperature": 0.0, "avg_logprob": -0.12946746660315472, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.05075974762439728}, {"id": 755, "seek": 384886, "start": 3853.82, "end": 3858.3, "text": " It kind of imbues it with elements that are not there in the concept itself. So if you lexicalize", "tokens": [50612, 467, 733, 295, 566, 65, 1247, 309, 365, 4959, 300, 366, 406, 456, 294, 264, 3410, 2564, 13, 407, 498, 291, 476, 87, 804, 1125, 50836], "temperature": 0.0, "avg_logprob": -0.12946746660315472, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.05075974762439728}, {"id": 756, "seek": 384886, "start": 3858.3, "end": 3862.2200000000003, "text": " the concept, you suddenly transform it a little bit, you give it a little extra, you sprinkle", "tokens": [50836, 264, 3410, 11, 291, 5800, 4088, 309, 257, 707, 857, 11, 291, 976, 309, 257, 707, 2857, 11, 291, 24745, 51032], "temperature": 0.0, "avg_logprob": -0.12946746660315472, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.05075974762439728}, {"id": 757, "seek": 384886, "start": 3862.2200000000003, "end": 3867.1, "text": " something else on top of it, and that seems to vary across different noun types. But these are all", "tokens": [51032, 746, 1646, 322, 1192, 295, 309, 11, 293, 300, 2544, 281, 10559, 2108, 819, 23307, 3467, 13, 583, 613, 366, 439, 51276], "temperature": 0.0, "avg_logprob": -0.12946746660315472, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.05075974762439728}, {"id": 758, "seek": 384886, "start": 3867.1, "end": 3874.7000000000003, "text": " like very clear architectural claims within gem diagram that make very clear empirical predictions.", "tokens": [51276, 411, 588, 1850, 26621, 9441, 1951, 7173, 10686, 300, 652, 588, 1850, 31886, 21264, 13, 51656], "temperature": 0.0, "avg_logprob": -0.12946746660315472, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.05075974762439728}, {"id": 759, "seek": 387470, "start": 3874.7, "end": 3878.8599999999997, "text": " So in other words, I guess what I'm saying is all these neuropsychology studies that are", "tokens": [50364, 407, 294, 661, 2283, 11, 286, 2041, 437, 286, 478, 1566, 307, 439, 613, 22510, 1513, 4202, 1793, 5313, 300, 366, 50572], "temperature": 0.0, "avg_logprob": -0.1354266857278758, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.004590438678860664}, {"id": 760, "seek": 387470, "start": 3878.8599999999997, "end": 3884.54, "text": " coincided, you know, in a lot of work in this fame, what does it really show? I think it shows", "tokens": [50572, 13001, 2112, 11, 291, 458, 11, 294, 257, 688, 295, 589, 294, 341, 16874, 11, 437, 775, 309, 534, 855, 30, 286, 519, 309, 3110, 50856], "temperature": 0.0, "avg_logprob": -0.1354266857278758, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.004590438678860664}, {"id": 761, "seek": 387470, "start": 3884.54, "end": 3889.8999999999996, "text": " that, you know, when language is damaged in the brain, it loses its particular sway or mode of", "tokens": [50856, 300, 11, 291, 458, 11, 562, 2856, 307, 14080, 294, 264, 3567, 11, 309, 18293, 1080, 1729, 27555, 420, 4391, 295, 51124], "temperature": 0.0, "avg_logprob": -0.1354266857278758, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.004590438678860664}, {"id": 762, "seek": 387470, "start": 3889.8999999999996, "end": 3894.8599999999997, "text": " influencing those systems. But there's no real prediction from within the gem to grammar enterprise", "tokens": [51124, 40396, 729, 3652, 13, 583, 456, 311, 572, 957, 17630, 490, 1951, 264, 7173, 281, 22317, 14132, 51372], "temperature": 0.0, "avg_logprob": -0.1354266857278758, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.004590438678860664}, {"id": 763, "seek": 387470, "start": 3894.8599999999997, "end": 3899.02, "text": " that those non-linguistic systems should be impaired or should suddenly, you know, shut down", "tokens": [51372, 300, 729, 2107, 12, 1688, 84, 3142, 3652, 820, 312, 36762, 420, 820, 5800, 11, 291, 458, 11, 5309, 760, 51580], "temperature": 0.0, "avg_logprob": -0.1354266857278758, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.004590438678860664}, {"id": 764, "seek": 389902, "start": 3899.02, "end": 3903.42, "text": " if the core language system is compromised, right? In fact, if anything, that just", "tokens": [50364, 498, 264, 4965, 2856, 1185, 307, 32463, 11, 558, 30, 682, 1186, 11, 498, 1340, 11, 300, 445, 50584], "temperature": 0.0, "avg_logprob": -0.13732427231808927, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004740073811262846}, {"id": 765, "seek": 389902, "start": 3904.14, "end": 3909.58, "text": " emphasizes the principal divorce between the syntactic system and non-linguistic systems,", "tokens": [50620, 48856, 264, 9716, 16052, 1296, 264, 23980, 19892, 1185, 293, 2107, 12, 1688, 84, 3142, 3652, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13732427231808927, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004740073811262846}, {"id": 766, "seek": 389902, "start": 3909.58, "end": 3913.42, "text": " right? So I think the, a lot of predictions here from the language and communication,", "tokens": [50892, 558, 30, 407, 286, 519, 264, 11, 257, 688, 295, 21264, 510, 490, 264, 2856, 293, 6101, 11, 51084], "temperature": 0.0, "avg_logprob": -0.13732427231808927, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004740073811262846}, {"id": 767, "seek": 389902, "start": 3914.14, "end": 3917.34, "text": " you know, literature are kind of missing the point of the architectural claims.", "tokens": [51120, 291, 458, 11, 10394, 366, 733, 295, 5361, 264, 935, 295, 264, 26621, 9441, 13, 51280], "temperature": 0.0, "avg_logprob": -0.13732427231808927, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004740073811262846}, {"id": 768, "seek": 389902, "start": 3919.34, "end": 3921.66, "text": " I can just give, or Daniel, do you want to go?", "tokens": [51380, 286, 393, 445, 976, 11, 420, 8033, 11, 360, 291, 528, 281, 352, 30, 51496], "temperature": 0.0, "avg_logprob": -0.13732427231808927, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004740073811262846}, {"id": 769, "seek": 392166, "start": 3922.22, "end": 3922.8599999999997, "text": " Yeah, go ahead.", "tokens": [50392, 865, 11, 352, 2286, 13, 50424], "temperature": 0.0, "avg_logprob": -0.21473735570907593, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.008058436214923859}, {"id": 770, "seek": 392166, "start": 3922.8599999999997, "end": 3927.74, "text": " Give a little bit of background there. So there's these papers from", "tokens": [50424, 5303, 257, 707, 857, 295, 3678, 456, 13, 407, 456, 311, 613, 10577, 490, 50668], "temperature": 0.0, "avg_logprob": -0.21473735570907593, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.008058436214923859}, {"id": 771, "seek": 392166, "start": 3928.7799999999997, "end": 3938.22, "text": " Ev Fedorenko and Rosemary Varley that are examining in part of them aphasic patients. So", "tokens": [50720, 5689, 7772, 10948, 4093, 293, 11144, 37529, 14662, 3420, 300, 366, 34662, 294, 644, 295, 552, 257, 7485, 299, 4209, 13, 407, 51192], "temperature": 0.0, "avg_logprob": -0.21473735570907593, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.008058436214923859}, {"id": 772, "seek": 392166, "start": 3938.22, "end": 3945.2599999999998, "text": " people who have impaired linguistic abilities, basically showing that with impaired linguistic", "tokens": [51192, 561, 567, 362, 36762, 43002, 11582, 11, 1936, 4099, 300, 365, 36762, 43002, 51544], "temperature": 0.0, "avg_logprob": -0.21473735570907593, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.008058436214923859}, {"id": 773, "seek": 394526, "start": 3945.26, "end": 3953.0200000000004, "text": " abilities, you can still have preserved kind of reasoning abilities. So people like chess masters,", "tokens": [50364, 11582, 11, 291, 393, 920, 362, 22242, 733, 295, 21577, 11582, 13, 407, 561, 411, 24122, 19294, 11, 50752], "temperature": 0.0, "avg_logprob": -0.10400351010836088, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.060931023210287094}, {"id": 774, "seek": 394526, "start": 3953.0200000000004, "end": 3960.7000000000003, "text": " chess grandmasters, for example, who are obviously very good at reasoning might not have kind of", "tokens": [50752, 24122, 2697, 3799, 1559, 11, 337, 1365, 11, 567, 366, 2745, 588, 665, 412, 21577, 1062, 406, 362, 733, 295, 51136], "temperature": 0.0, "avg_logprob": -0.10400351010836088, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.060931023210287094}, {"id": 775, "seek": 394526, "start": 3960.7000000000003, "end": 3966.2200000000003, "text": " intact linguistic abilities. And then complimenting that kind of patient work, there's also work from", "tokens": [51136, 23493, 43002, 11582, 13, 400, 550, 16250, 278, 300, 733, 295, 4537, 589, 11, 456, 311, 611, 589, 490, 51412], "temperature": 0.0, "avg_logprob": -0.10400351010836088, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.060931023210287094}, {"id": 776, "seek": 396622, "start": 3966.22, "end": 3975.66, "text": " Ed's lab showing that the parts of the brain that care about language are separable from", "tokens": [50364, 3977, 311, 2715, 4099, 300, 264, 3166, 295, 264, 3567, 300, 1127, 466, 2856, 366, 3128, 712, 490, 50836], "temperature": 0.0, "avg_logprob": -0.11911946535110474, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.11272372305393219}, {"id": 777, "seek": 396622, "start": 3975.66, "end": 3980.06, "text": " the parts of the brain that care about other domains, even ones that seem kind of language-like.", "tokens": [50836, 264, 3166, 295, 264, 3567, 300, 1127, 466, 661, 25514, 11, 754, 2306, 300, 1643, 733, 295, 2856, 12, 4092, 13, 51056], "temperature": 0.0, "avg_logprob": -0.11911946535110474, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.11272372305393219}, {"id": 778, "seek": 396622, "start": 3980.06, "end": 3986.4599999999996, "text": " So things like music and mathematics tend not to happen in the language areas.", "tokens": [51056, 407, 721, 411, 1318, 293, 18666, 3928, 406, 281, 1051, 294, 264, 2856, 3179, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11911946535110474, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.11272372305393219}, {"id": 779, "seek": 396622, "start": 3987.18, "end": 3994.22, "text": " So Ev and others have argued that this is basically evidence against the Chomsky", "tokens": [51412, 407, 5689, 293, 2357, 362, 20219, 300, 341, 307, 1936, 4467, 1970, 264, 761, 4785, 4133, 51764], "temperature": 0.0, "avg_logprob": -0.11911946535110474, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.11272372305393219}, {"id": 780, "seek": 399422, "start": 3994.22, "end": 4000.9399999999996, "text": " inclaim that language is the medium for thinking, because there's thinking that can happen", "tokens": [50364, 834, 10970, 300, 2856, 307, 264, 6399, 337, 1953, 11, 570, 456, 311, 1953, 300, 393, 1051, 50700], "temperature": 0.0, "avg_logprob": -0.1572758356730143, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.005378134548664093}, {"id": 781, "seek": 399422, "start": 4000.9399999999996, "end": 4005.1, "text": " in the absence of language and the brain areas that care about language seem not to be the brain", "tokens": [50700, 294, 264, 17145, 295, 2856, 293, 264, 3567, 3179, 300, 1127, 466, 2856, 1643, 406, 281, 312, 264, 3567, 50908], "temperature": 0.0, "avg_logprob": -0.1572758356730143, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.005378134548664093}, {"id": 782, "seek": 399422, "start": 4005.1, "end": 4012.06, "text": " areas that care about thinking. I guess, Elliot, you're saying that people don't really believe that.", "tokens": [50908, 3179, 300, 1127, 466, 1953, 13, 286, 2041, 11, 38986, 11, 291, 434, 1566, 300, 561, 500, 380, 534, 1697, 300, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1572758356730143, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.005378134548664093}, {"id": 783, "seek": 399422, "start": 4014.9399999999996, "end": 4017.02, "text": " They don't believe that distinction, I mean.", "tokens": [51400, 814, 500, 380, 1697, 300, 16844, 11, 286, 914, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1572758356730143, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.005378134548664093}, {"id": 784, "seek": 401702, "start": 4017.74, "end": 4025.34, "text": " And also, there's a lot of self-contradiction even within these arguments, right? So in your paper,", "tokens": [50400, 400, 611, 11, 456, 311, 257, 688, 295, 2698, 12, 9000, 6206, 4105, 754, 1951, 613, 12869, 11, 558, 30, 407, 294, 428, 3035, 11, 50780], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 785, "seek": 401702, "start": 4025.34, "end": 4029.18, "text": " you sometimes say that Chomsky thinks that language is a thought system, but then a few", "tokens": [50780, 291, 2171, 584, 300, 761, 4785, 4133, 7309, 300, 2856, 307, 257, 1194, 1185, 11, 457, 550, 257, 1326, 50972], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 786, "seek": 401702, "start": 4029.18, "end": 4033.66, "text": " pages later, you'll say Chomsky also believes that syntax is some totally separate system from", "tokens": [50972, 7183, 1780, 11, 291, 603, 584, 761, 4785, 4133, 611, 12307, 300, 28431, 307, 512, 3879, 4994, 1185, 490, 51196], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 787, "seek": 401702, "start": 4033.66, "end": 4038.78, "text": " anything else, right? Your autonomy of syntax, etc. So does Chomsky think-", "tokens": [51196, 1340, 1646, 11, 558, 30, 2260, 27278, 295, 28431, 11, 5183, 13, 407, 775, 761, 4785, 4133, 519, 12, 51452], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 788, "seek": 401702, "start": 4038.78, "end": 4041.34, "text": " That's not my contradiction. I mean, he said both of those things.", "tokens": [51452, 663, 311, 406, 452, 34937, 13, 286, 914, 11, 415, 848, 1293, 295, 729, 721, 13, 51580], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 789, "seek": 401702, "start": 4041.98, "end": 4046.86, "text": " Right, exactly. So therefore, you may want to ask yourself, does he really believe these things?", "tokens": [51612, 1779, 11, 2293, 13, 407, 4412, 11, 291, 815, 528, 281, 1029, 1803, 11, 775, 415, 534, 1697, 613, 721, 30, 51856], "temperature": 0.0, "avg_logprob": -0.16136391957600912, "compression_ratio": 1.7308970099667773, "no_speech_prob": 0.017613446339964867}, {"id": 790, "seek": 404702, "start": 4047.5, "end": 4051.2599999999998, "text": " Or what is the case if it arises from the architecture, right?", "tokens": [50388, 1610, 437, 307, 264, 1389, 498, 309, 27388, 490, 264, 9482, 11, 558, 30, 50576], "temperature": 0.0, "avg_logprob": -0.15865019093389096, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.00025270378682762384}, {"id": 791, "seek": 404702, "start": 4051.2599999999998, "end": 4055.66, "text": " So just saying language is a thought system, what does that mean? That doesn't mean anything.", "tokens": [50576, 407, 445, 1566, 2856, 307, 257, 1194, 1185, 11, 437, 775, 300, 914, 30, 663, 1177, 380, 914, 1340, 13, 50796], "temperature": 0.0, "avg_logprob": -0.15865019093389096, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.00025270378682762384}, {"id": 792, "seek": 404702, "start": 4055.66, "end": 4060.7, "text": " It's just a very vague statement. The question is how exactly is language contributing to thought", "tokens": [50796, 467, 311, 445, 257, 588, 24247, 5629, 13, 440, 1168, 307, 577, 2293, 307, 2856, 19270, 281, 1194, 51048], "temperature": 0.0, "avg_logprob": -0.15865019093389096, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.00025270378682762384}, {"id": 793, "seek": 404702, "start": 4060.7, "end": 4061.9, "text": " and how is it not contributing?", "tokens": [51048, 293, 577, 307, 309, 406, 19270, 30, 51108], "temperature": 0.0, "avg_logprob": -0.15865019093389096, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.00025270378682762384}, {"id": 794, "seek": 404702, "start": 4063.98, "end": 4070.22, "text": " Yeah, I mean, I think his claim is mainly evolutionary or something, right, that this is the", "tokens": [51212, 865, 11, 286, 914, 11, 286, 519, 702, 3932, 307, 8704, 27567, 420, 746, 11, 558, 11, 300, 341, 307, 264, 51524], "temperature": 0.0, "avg_logprob": -0.15865019093389096, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.00025270378682762384}, {"id": 795, "seek": 407022, "start": 4070.22, "end": 4076.62, "text": " origins of the system, which I think is sort of equally hard to square with the kind of", "tokens": [50364, 22721, 295, 264, 1185, 11, 597, 286, 519, 307, 1333, 295, 12309, 1152, 281, 3732, 365, 264, 733, 295, 50684], "temperature": 0.0, "avg_logprob": -0.102418665983239, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.006890036165714264}, {"id": 796, "seek": 407022, "start": 4076.62, "end": 4085.5, "text": " patient and neuroimaging data. But if he doesn't think that, then he shouldn't say it,", "tokens": [50684, 4537, 293, 16499, 332, 3568, 1412, 13, 583, 498, 415, 1177, 380, 519, 300, 11, 550, 415, 4659, 380, 584, 309, 11, 51128], "temperature": 0.0, "avg_logprob": -0.102418665983239, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.006890036165714264}, {"id": 797, "seek": 407022, "start": 4086.54, "end": 4088.4599999999996, "text": " or people will respond to what he said, I think.", "tokens": [51180, 420, 561, 486, 4196, 281, 437, 415, 848, 11, 286, 519, 13, 51276], "temperature": 0.0, "avg_logprob": -0.102418665983239, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.006890036165714264}, {"id": 798, "seek": 407022, "start": 4090.22, "end": 4094.62, "text": " The argument is that language is a kind of thought system. It regulates some aspects of", "tokens": [51364, 440, 6770, 307, 300, 2856, 307, 257, 733, 295, 1194, 1185, 13, 467, 9837, 1024, 512, 7270, 295, 51584], "temperature": 0.0, "avg_logprob": -0.102418665983239, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.006890036165714264}, {"id": 799, "seek": 407022, "start": 4094.62, "end": 4098.7, "text": " thought and it yields some aspects of thought that are clearly unique to humans,", "tokens": [51584, 1194, 293, 309, 32168, 512, 7270, 295, 1194, 300, 366, 4448, 3845, 281, 6255, 11, 51788], "temperature": 0.0, "avg_logprob": -0.102418665983239, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.006890036165714264}, {"id": 800, "seek": 409870, "start": 4098.7, "end": 4103.98, "text": " but it's not intrinsically or causally tied to it. The architecture of the system is very different", "tokens": [50364, 457, 309, 311, 406, 28621, 984, 420, 3302, 379, 9601, 281, 309, 13, 440, 9482, 295, 264, 1185, 307, 588, 819, 50628], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 801, "seek": 409870, "start": 4103.98, "end": 4108.46, "text": " from the kind of generalizations you can rhetorically evince from the architecture.", "tokens": [50628, 490, 264, 733, 295, 2674, 14455, 291, 393, 24182, 284, 984, 1073, 1236, 490, 264, 9482, 13, 50852], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 802, "seek": 409870, "start": 4108.46, "end": 4112.62, "text": " So for instance, when you cite work from a phasic patient showing no deficits in complex", "tokens": [50852, 407, 337, 5197, 11, 562, 291, 37771, 589, 490, 257, 903, 296, 299, 4537, 4099, 572, 49616, 294, 3997, 51060], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 803, "seek": 409870, "start": 4112.62, "end": 4116.46, "text": " reasoning, as you just mentioned, playing chess and so on, we would actually expect this under a", "tokens": [51060, 21577, 11, 382, 291, 445, 2835, 11, 2433, 24122, 293, 370, 322, 11, 321, 576, 767, 2066, 341, 833, 257, 51252], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 804, "seek": 409870, "start": 4116.46, "end": 4121.74, "text": " kind of non-lexicalist framework of generative syntax, where meaning, as I said, meaning syntax", "tokens": [51252, 733, 295, 2107, 12, 2021, 804, 468, 8388, 295, 1337, 1166, 28431, 11, 689, 3620, 11, 382, 286, 848, 11, 3620, 28431, 51516], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 805, "seek": 409870, "start": 4121.74, "end": 4126.7, "text": " and form, form just meaning anything that you can externalize language in, all these things", "tokens": [51516, 293, 1254, 11, 1254, 445, 3620, 1340, 300, 291, 393, 8320, 1125, 2856, 294, 11, 439, 613, 721, 51764], "temperature": 0.0, "avg_logprob": -0.11997442755080362, "compression_ratio": 1.779552715654952, "no_speech_prob": 0.0020954818464815617}, {"id": 806, "seek": 412670, "start": 4126.7, "end": 4131.099999999999, "text": " are separate features and separate systems. The autonomy of syntax doesn't mean,", "tokens": [50364, 366, 4994, 4122, 293, 4994, 3652, 13, 440, 27278, 295, 28431, 1177, 380, 914, 11, 50584], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 807, "seek": 412670, "start": 4132.7, "end": 4136.38, "text": " what a lot of people think it means, it just means that there are certain syntactic operations", "tokens": [50664, 437, 257, 688, 295, 561, 519, 309, 1355, 11, 309, 445, 1355, 300, 456, 366, 1629, 23980, 19892, 7705, 50848], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 808, "seek": 412670, "start": 4136.38, "end": 4140.54, "text": " that are not semantic. There are certain things you can do with syntax that you can only do with", "tokens": [50848, 300, 366, 406, 47982, 13, 821, 366, 1629, 721, 291, 393, 360, 365, 28431, 300, 291, 393, 787, 360, 365, 51056], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 809, "seek": 412670, "start": 4140.54, "end": 4144.0599999999995, "text": " syntax and you can't do with semantics. So this gets back to the difference between", "tokens": [51056, 28431, 293, 291, 393, 380, 360, 365, 4361, 45298, 13, 407, 341, 2170, 646, 281, 264, 2649, 1296, 51232], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 810, "seek": 412670, "start": 4145.099999999999, "end": 4151.179999999999, "text": " Petrowski's theory that semantics is just and versus a lot of syntacticians' belief that", "tokens": [51284, 10472, 81, 21866, 311, 5261, 300, 4361, 45298, 307, 445, 293, 5717, 257, 688, 295, 23980, 19892, 2567, 6, 7107, 300, 51588], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 811, "seek": 412670, "start": 4151.179999999999, "end": 4155.34, "text": " there are certain peculiar weird things you can do with syntax that are just syntactic.", "tokens": [51588, 456, 366, 1629, 27149, 3657, 721, 291, 393, 360, 365, 28431, 300, 366, 445, 23980, 19892, 13, 51796], "temperature": 0.0, "avg_logprob": -0.11256830153926727, "compression_ratio": 2.065891472868217, "no_speech_prob": 0.00620335154235363}, {"id": 812, "seek": 415534, "start": 4155.34, "end": 4161.34, "text": " So there is a divorce even within the kind of architectural framework. So it's not too surprising", "tokens": [50364, 407, 456, 307, 257, 16052, 754, 1951, 264, 733, 295, 26621, 8388, 13, 407, 309, 311, 406, 886, 8830, 50664], "temperature": 0.0, "avg_logprob": -0.14631602275802427, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0007310670334845781}, {"id": 813, "seek": 415534, "start": 4161.34, "end": 4165.02, "text": " that you also find that divorce at the neuropsychological level, I would say.", "tokens": [50664, 300, 291, 611, 915, 300, 16052, 412, 264, 22510, 1513, 4202, 4383, 1496, 11, 286, 576, 584, 13, 50848], "temperature": 0.0, "avg_logprob": -0.14631602275802427, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0007310670334845781}, {"id": 814, "seek": 415534, "start": 4166.06, "end": 4173.26, "text": " Well, I think I would want a prediction of the language's thought evolutionary idea then.", "tokens": [50900, 1042, 11, 286, 519, 286, 576, 528, 257, 17630, 295, 264, 2856, 311, 1194, 27567, 1558, 550, 13, 51260], "temperature": 0.0, "avg_logprob": -0.14631602275802427, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0007310670334845781}, {"id": 815, "seek": 415534, "start": 4176.38, "end": 4183.9800000000005, "text": " If you're saying that doesn't predict that thought relies on language, then I think whoever", "tokens": [51416, 759, 291, 434, 1566, 300, 1177, 380, 6069, 300, 1194, 30910, 322, 2856, 11, 550, 286, 519, 11387, 51796], "temperature": 0.0, "avg_logprob": -0.14631602275802427, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0007310670334845781}, {"id": 816, "seek": 418398, "start": 4183.98, "end": 4190.379999999999, "text": " likes that theory should come up with some predictions about what that theory actually", "tokens": [50364, 5902, 300, 5261, 820, 808, 493, 365, 512, 21264, 466, 437, 300, 5261, 767, 50684], "temperature": 0.0, "avg_logprob": -0.10572686948274311, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0009108623489737511}, {"id": 817, "seek": 418398, "start": 4190.379999999999, "end": 4195.5, "text": " means. I feel like those kinds of predictions are often really necessary for understanding the", "tokens": [50684, 1355, 13, 286, 841, 411, 729, 3685, 295, 21264, 366, 2049, 534, 4818, 337, 3701, 264, 50940], "temperature": 0.0, "avg_logprob": -0.10572686948274311, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0009108623489737511}, {"id": 818, "seek": 418398, "start": 4195.5, "end": 4201.179999999999, "text": " content of a prediction. So sorry, Daniel, your hand's been up for a while.", "tokens": [50940, 2701, 295, 257, 17630, 13, 407, 2597, 11, 8033, 11, 428, 1011, 311, 668, 493, 337, 257, 1339, 13, 51224], "temperature": 0.0, "avg_logprob": -0.10572686948274311, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0009108623489737511}, {"id": 819, "seek": 420118, "start": 4202.14, "end": 4214.3, "text": " No, it's all good. Just kind of wanted to bring a breath in and an opportunity for anyone to ask", "tokens": [50412, 883, 11, 309, 311, 439, 665, 13, 1449, 733, 295, 1415, 281, 1565, 257, 6045, 294, 293, 364, 2650, 337, 2878, 281, 1029, 51020], "temperature": 0.0, "avg_logprob": -0.12743786589740075, "compression_ratio": 1.452127659574468, "no_speech_prob": 0.041449859738349915}, {"id": 820, "seek": 420118, "start": 4214.3, "end": 4223.18, "text": " any other questions. But wow, thank you both for the many topics we've covered. We'll have,", "tokens": [51020, 604, 661, 1651, 13, 583, 6076, 11, 1309, 291, 1293, 337, 264, 867, 8378, 321, 600, 5343, 13, 492, 603, 362, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12743786589740075, "compression_ratio": 1.452127659574468, "no_speech_prob": 0.041449859738349915}, {"id": 821, "seek": 420118, "start": 4223.18, "end": 4227.66, "text": " in the last minutes, a kind of conclusion in next steps. But Dave, would you like to", "tokens": [51464, 294, 264, 1036, 2077, 11, 257, 733, 295, 10063, 294, 958, 4439, 13, 583, 11017, 11, 576, 291, 411, 281, 51688], "temperature": 0.0, "avg_logprob": -0.12743786589740075, "compression_ratio": 1.452127659574468, "no_speech_prob": 0.041449859738349915}, {"id": 822, "seek": 422766, "start": 4228.22, "end": 4230.86, "text": " ask a question or just give a short reflection?", "tokens": [50392, 1029, 257, 1168, 420, 445, 976, 257, 2099, 12914, 30, 50524], "temperature": 0.0, "avg_logprob": -0.12105488373061359, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.003272242611274123}, {"id": 823, "seek": 422766, "start": 4237.0199999999995, "end": 4244.7, "text": " Okay, no. There are many comments in the chat, so I hope that both of you can read them", "tokens": [50832, 1033, 11, 572, 13, 821, 366, 867, 3053, 294, 264, 5081, 11, 370, 286, 1454, 300, 1293, 295, 291, 393, 1401, 552, 51216], "temperature": 0.0, "avg_logprob": -0.12105488373061359, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.003272242611274123}, {"id": 824, "seek": 422766, "start": 4244.7, "end": 4251.98, "text": " on your own time to see what everyone added. Where do we go from here? As we", "tokens": [51216, 322, 428, 1065, 565, 281, 536, 437, 1518, 3869, 13, 2305, 360, 321, 352, 490, 510, 30, 1018, 321, 51580], "temperature": 0.0, "avg_logprob": -0.12105488373061359, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.003272242611274123}, {"id": 825, "seek": 425198, "start": 4252.94, "end": 4261.74, "text": " roar into May 2023 and beyond, what can linguists, large language model developers", "tokens": [50412, 40347, 666, 1891, 44377, 293, 4399, 11, 437, 393, 21766, 1751, 11, 2416, 2856, 2316, 8849, 50852], "temperature": 0.0, "avg_logprob": -0.1292455236790544, "compression_ratio": 1.52, "no_speech_prob": 0.011317531578242779}, {"id": 826, "seek": 425198, "start": 4261.74, "end": 4266.94, "text": " and users, cognitive scientists, what do you each think are some of the most fruitful pathways", "tokens": [50852, 293, 5022, 11, 15605, 7708, 11, 437, 360, 291, 1184, 519, 366, 512, 295, 264, 881, 49795, 22988, 51112], "temperature": 0.0, "avg_logprob": -0.1292455236790544, "compression_ratio": 1.52, "no_speech_prob": 0.011317531578242779}, {"id": 827, "seek": 425198, "start": 4266.94, "end": 4276.139999999999, "text": " forward? Well, I would say the most fruitful pathway forward is to really take cognitive", "tokens": [51112, 2128, 30, 1042, 11, 286, 576, 584, 264, 881, 49795, 18590, 2128, 307, 281, 534, 747, 15605, 51572], "temperature": 0.0, "avg_logprob": -0.1292455236790544, "compression_ratio": 1.52, "no_speech_prob": 0.011317531578242779}, {"id": 828, "seek": 427614, "start": 4276.14, "end": 4282.62, "text": " psychology seriously. There's a lot of nice work recently trying to align things like chat", "tokens": [50364, 15105, 6638, 13, 821, 311, 257, 688, 295, 1481, 589, 3938, 1382, 281, 7975, 721, 411, 5081, 50688], "temperature": 0.0, "avg_logprob": -0.1446751681241122, "compression_ratio": 1.6267123287671232, "no_speech_prob": 0.033491529524326324}, {"id": 829, "seek": 427614, "start": 4282.62, "end": 4287.18, "text": " GPT, Wolfram Alpha plugins, the way that chat GPT can interface with different kind of modules.", "tokens": [50688, 26039, 51, 11, 16634, 2356, 20588, 33759, 11, 264, 636, 300, 5081, 26039, 51, 393, 9226, 365, 819, 733, 295, 16679, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1446751681241122, "compression_ratio": 1.6267123287671232, "no_speech_prob": 0.033491529524326324}, {"id": 830, "seek": 427614, "start": 4287.9800000000005, "end": 4294.3, "text": " The way of building a legitimate kind of AGI system doesn't necessarily have to be psychologically", "tokens": [50956, 440, 636, 295, 2390, 257, 17956, 733, 295, 316, 26252, 1185, 1177, 380, 4725, 362, 281, 312, 41387, 51272], "temperature": 0.0, "avg_logprob": -0.1446751681241122, "compression_ratio": 1.6267123287671232, "no_speech_prob": 0.033491529524326324}, {"id": 831, "seek": 427614, "start": 4294.3, "end": 4297.9800000000005, "text": " reliant on the kind of modules that human beings have, but I think it will benefit from it. So", "tokens": [51272, 1039, 5798, 322, 264, 733, 295, 16679, 300, 1952, 8958, 362, 11, 457, 286, 519, 309, 486, 5121, 490, 309, 13, 407, 51456], "temperature": 0.0, "avg_logprob": -0.1446751681241122, "compression_ratio": 1.6267123287671232, "no_speech_prob": 0.033491529524326324}, {"id": 832, "seek": 427614, "start": 4297.9800000000005, "end": 4305.1, "text": " there have been some claims that large language models can maybe do all sorts of things. But I", "tokens": [51456, 456, 362, 668, 512, 9441, 300, 2416, 2856, 5245, 393, 1310, 360, 439, 7527, 295, 721, 13, 583, 286, 51812], "temperature": 0.0, "avg_logprob": -0.1446751681241122, "compression_ratio": 1.6267123287671232, "no_speech_prob": 0.033491529524326324}, {"id": 833, "seek": 430510, "start": 4305.1, "end": 4308.620000000001, "text": " think in the long run, it's most likely going to be the case that LLMs can do something very", "tokens": [50364, 519, 294, 264, 938, 1190, 11, 309, 311, 881, 3700, 516, 281, 312, 264, 1389, 300, 441, 43, 26386, 393, 360, 746, 588, 50540], "temperature": 0.0, "avg_logprob": -0.1087076937566038, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0037484513595700264}, {"id": 834, "seek": 430510, "start": 4308.620000000001, "end": 4312.780000000001, "text": " important and very interesting, but it's only going to be one piece of the puzzle. So in fact,", "tokens": [50540, 1021, 293, 588, 1880, 11, 457, 309, 311, 787, 516, 281, 312, 472, 2522, 295, 264, 12805, 13, 407, 294, 1186, 11, 50748], "temperature": 0.0, "avg_logprob": -0.1087076937566038, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0037484513595700264}, {"id": 835, "seek": 430510, "start": 4312.780000000001, "end": 4318.780000000001, "text": " even OpenAI CEO Sam Altman said last week that what we can do with LLMs has really kind of been", "tokens": [50748, 754, 7238, 48698, 9282, 4832, 15992, 1601, 848, 1036, 1243, 300, 437, 321, 393, 360, 365, 441, 43, 26386, 575, 534, 733, 295, 668, 51048], "temperature": 0.0, "avg_logprob": -0.1087076937566038, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0037484513595700264}, {"id": 836, "seek": 430510, "start": 4318.780000000001, "end": 4325.34, "text": " exhausted. We need new directions, new avenues and so on. I guess he was probably speaking to", "tokens": [51048, 17992, 13, 492, 643, 777, 11095, 11, 777, 43039, 293, 370, 322, 13, 286, 2041, 415, 390, 1391, 4124, 281, 51376], "temperature": 0.0, "avg_logprob": -0.1087076937566038, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0037484513595700264}, {"id": 837, "seek": 430510, "start": 4325.34, "end": 4331.26, "text": " investors more than linguistic students here, but I think he's also right. LLMs can do something", "tokens": [51376, 11519, 544, 813, 43002, 1731, 510, 11, 457, 286, 519, 415, 311, 611, 558, 13, 441, 43, 26386, 393, 360, 746, 51672], "temperature": 0.0, "avg_logprob": -0.1087076937566038, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0037484513595700264}, {"id": 838, "seek": 433126, "start": 4331.26, "end": 4336.860000000001, "text": " spectacular, but they're probably going to form a small part of the general AGI architecture,", "tokens": [50364, 18149, 11, 457, 436, 434, 1391, 516, 281, 1254, 257, 1359, 644, 295, 264, 2674, 316, 26252, 9482, 11, 50644], "temperature": 0.0, "avg_logprob": -0.14884839964307045, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01730410009622574}, {"id": 839, "seek": 433126, "start": 4336.860000000001, "end": 4343.1, "text": " right? If you want to think about AGI as a potential, potential goal here. So, you know,", "tokens": [50644, 558, 30, 759, 291, 528, 281, 519, 466, 316, 26252, 382, 257, 3995, 11, 3995, 3387, 510, 13, 407, 11, 291, 458, 11, 50956], "temperature": 0.0, "avg_logprob": -0.14884839964307045, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01730410009622574}, {"id": 840, "seek": 433126, "start": 4343.900000000001, "end": 4351.34, "text": " I think a lot of the, so let me give me an example here. So Anna Ivanova, who's a very good", "tokens": [50996, 286, 519, 257, 688, 295, 264, 11, 370, 718, 385, 976, 385, 364, 1365, 510, 13, 407, 12899, 26546, 3730, 2757, 11, 567, 311, 257, 588, 665, 51368], "temperature": 0.0, "avg_logprob": -0.14884839964307045, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01730410009622574}, {"id": 841, "seek": 433126, "start": 4351.34, "end": 4355.9800000000005, "text": " cognitive scientist, she has a paper recently arguing for a kind of modular architecture for", "tokens": [51368, 15605, 12662, 11, 750, 575, 257, 3035, 3938, 19697, 337, 257, 733, 295, 31111, 9482, 337, 51600], "temperature": 0.0, "avg_logprob": -0.14884839964307045, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01730410009622574}, {"id": 842, "seek": 433126, "start": 4355.9800000000005, "end": 4360.7, "text": " LLMs, which is a very nice framework, right? It's very cognitively plausible. It's exactly the", "tokens": [51600, 441, 43, 26386, 11, 597, 307, 257, 588, 1481, 8388, 11, 558, 30, 467, 311, 588, 15605, 356, 39925, 13, 467, 311, 2293, 264, 51836], "temperature": 0.0, "avg_logprob": -0.14884839964307045, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01730410009622574}, {"id": 843, "seek": 436070, "start": 4360.7, "end": 4364.38, "text": " kind of thing that we should be pushing for. It's compatible with Howard Gardner's, you know,", "tokens": [50364, 733, 295, 551, 300, 321, 820, 312, 7380, 337, 13, 467, 311, 18218, 365, 17626, 12882, 1193, 311, 11, 291, 458, 11, 50548], "temperature": 0.0, "avg_logprob": -0.10988514922385992, "compression_ratio": 1.5691318327974277, "no_speech_prob": 0.002267984440550208}, {"id": 844, "seek": 436070, "start": 4364.38, "end": 4368.62, "text": " notion of multiple intelligences and so on. But I think at the same time, just to finish this", "tokens": [50548, 10710, 295, 3866, 5613, 2667, 293, 370, 322, 13, 583, 286, 519, 412, 264, 912, 565, 11, 445, 281, 2413, 341, 50760], "temperature": 0.0, "avg_logprob": -0.10988514922385992, "compression_ratio": 1.5691318327974277, "no_speech_prob": 0.002267984440550208}, {"id": 845, "seek": 436070, "start": 4368.62, "end": 4375.179999999999, "text": " comment, there was a tech talk last week, I think, or maybe a few days ago, where a lot of this stuff", "tokens": [50760, 2871, 11, 456, 390, 257, 7553, 751, 1036, 1243, 11, 286, 519, 11, 420, 1310, 257, 1326, 1708, 2057, 11, 689, 257, 688, 295, 341, 1507, 51088], "temperature": 0.0, "avg_logprob": -0.10988514922385992, "compression_ratio": 1.5691318327974277, "no_speech_prob": 0.002267984440550208}, {"id": 846, "seek": 436070, "start": 4375.179999999999, "end": 4381.74, "text": " can be conflated with AI hype in an unproductive way. So Greg Brockman from OpenAI, he gave one of", "tokens": [51088, 393, 312, 1497, 38539, 365, 7318, 24144, 294, 364, 517, 14314, 20221, 636, 13, 407, 11490, 32093, 1601, 490, 7238, 48698, 11, 415, 2729, 472, 295, 51416], "temperature": 0.0, "avg_logprob": -0.10988514922385992, "compression_ratio": 1.5691318327974277, "no_speech_prob": 0.002267984440550208}, {"id": 847, "seek": 436070, "start": 4381.74, "end": 4386.7, "text": " his, one of these big TED talks where he showed different plugins that chat GPD can do. I mentioned", "tokens": [51416, 702, 11, 472, 295, 613, 955, 43036, 6686, 689, 415, 4712, 819, 33759, 300, 5081, 460, 17349, 393, 360, 13, 286, 2835, 51664], "temperature": 0.0, "avg_logprob": -0.10988514922385992, "compression_ratio": 1.5691318327974277, "no_speech_prob": 0.002267984440550208}, {"id": 848, "seek": 438670, "start": 4386.7, "end": 4391.58, "text": " Wolfram Alpert, right? But there's also things like image generation, Instacart shopping, where you", "tokens": [50364, 16634, 2356, 967, 15346, 11, 558, 30, 583, 456, 311, 611, 721, 411, 3256, 5125, 11, 2730, 326, 446, 8688, 11, 689, 291, 50608], "temperature": 0.0, "avg_logprob": -0.12643089450773645, "compression_ratio": 1.6335616438356164, "no_speech_prob": 0.021590469405055046}, {"id": 849, "seek": 438670, "start": 4391.58, "end": 4397.179999999999, "text": " can get chat GPD to buy you things and what have you. And again, this takes you back to the idea", "tokens": [50608, 393, 483, 5081, 460, 17349, 281, 2256, 291, 721, 293, 437, 362, 291, 13, 400, 797, 11, 341, 2516, 291, 646, 281, 264, 1558, 50888], "temperature": 0.0, "avg_logprob": -0.12643089450773645, "compression_ratio": 1.6335616438356164, "no_speech_prob": 0.021590469405055046}, {"id": 850, "seek": 438670, "start": 4397.179999999999, "end": 4401.66, "text": " that multiple subsystems can do different sub-functions. So Brockman also showed an example", "tokens": [50888, 300, 3866, 2090, 9321, 82, 393, 360, 819, 1422, 12, 15930, 3916, 13, 407, 32093, 1601, 611, 4712, 364, 1365, 51112], "temperature": 0.0, "avg_logprob": -0.12643089450773645, "compression_ratio": 1.6335616438356164, "no_speech_prob": 0.021590469405055046}, {"id": 851, "seek": 438670, "start": 4401.66, "end": 4408.54, "text": " of giving chat GPD an Excel file, a CSV file from an archive database of academic papers,", "tokens": [51112, 295, 2902, 5081, 460, 17349, 364, 19060, 3991, 11, 257, 48814, 3991, 490, 364, 23507, 8149, 295, 7778, 10577, 11, 51456], "temperature": 0.0, "avg_logprob": -0.12643089450773645, "compression_ratio": 1.6335616438356164, "no_speech_prob": 0.021590469405055046}, {"id": 852, "seek": 438670, "start": 4408.54, "end": 4413.9, "text": " where it just listed a bunch of papers and then titles and what have you, right? And he said that,", "tokens": [51456, 689, 309, 445, 10052, 257, 3840, 295, 10577, 293, 550, 12992, 293, 437, 362, 291, 11, 558, 30, 400, 415, 848, 300, 11, 51724], "temperature": 0.0, "avg_logprob": -0.12643089450773645, "compression_ratio": 1.6335616438356164, "no_speech_prob": 0.021590469405055046}, {"id": 853, "seek": 441390, "start": 4414.139999999999, "end": 4420.0599999999995, "text": " using chat GPD, it uses world knowledge to infer what the titles of the columns mean. So we understood", "tokens": [50376, 1228, 5081, 460, 17349, 11, 309, 4960, 1002, 3601, 281, 13596, 437, 264, 12992, 295, 264, 13766, 914, 13, 407, 321, 7320, 50672], "temperature": 0.0, "avg_logprob": -0.11564870981069711, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.003949542064219713}, {"id": 854, "seek": 441390, "start": 4420.0599999999995, "end": 4425.099999999999, "text": " that title means the title of the paper. It understood that authors mean the number of authors", "tokens": [50672, 300, 4876, 1355, 264, 4876, 295, 264, 3035, 13, 467, 7320, 300, 16552, 914, 264, 1230, 295, 16552, 50924], "temperature": 0.0, "avg_logprob": -0.11564870981069711, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.003949542064219713}, {"id": 855, "seek": 441390, "start": 4425.099999999999, "end": 4430.219999999999, "text": " per paper. It understood that created means the date the paper was submitted, right? And because", "tokens": [50924, 680, 3035, 13, 467, 7320, 300, 2942, 1355, 264, 4002, 264, 3035, 390, 14405, 11, 558, 30, 400, 570, 51180], "temperature": 0.0, "avg_logprob": -0.11564870981069711, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.003949542064219713}, {"id": 856, "seek": 441390, "start": 4430.219999999999, "end": 4435.5, "text": " it's a TED talk, the audience gave us a standing elevation, right? But the ability to describe", "tokens": [51180, 309, 311, 257, 43036, 751, 11, 264, 4034, 2729, 505, 257, 4877, 25827, 11, 558, 30, 583, 264, 3485, 281, 6786, 51444], "temperature": 0.0, "avg_logprob": -0.11564870981069711, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.003949542064219713}, {"id": 857, "seek": 441390, "start": 4435.5, "end": 4442.78, "text": " labels on an Excel file is, I guess, nice. But I'm not sure you'd really call it world knowledge.", "tokens": [51444, 16949, 322, 364, 19060, 3991, 307, 11, 286, 2041, 11, 1481, 13, 583, 286, 478, 406, 988, 291, 1116, 534, 818, 309, 1002, 3601, 13, 51808], "temperature": 0.0, "avg_logprob": -0.11564870981069711, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.003949542064219713}, {"id": 858, "seek": 444278, "start": 4442.78, "end": 4447.98, "text": " So I guess, I would just say there's a lot of progress needs to be made alongside reducing", "tokens": [50364, 407, 286, 2041, 11, 286, 576, 445, 584, 456, 311, 257, 688, 295, 4205, 2203, 281, 312, 1027, 12385, 12245, 50624], "temperature": 0.0, "avg_logprob": -0.10680485638705167, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.0015043478924781084}, {"id": 859, "seek": 444278, "start": 4447.98, "end": 4452.3, "text": " anthropomorphism. You have to have the right balance of it. So like I said, you have to have", "tokens": [50624, 22727, 32702, 1434, 13, 509, 362, 281, 362, 264, 558, 4772, 295, 309, 13, 407, 411, 286, 848, 11, 291, 362, 281, 362, 50840], "temperature": 0.0, "avg_logprob": -0.10680485638705167, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.0015043478924781084}, {"id": 860, "seek": 444278, "start": 4452.3, "end": 4457.259999999999, "text": " the right balance of psychologically plausible kind of modular architecture, but you can't have too", "tokens": [50840, 264, 558, 4772, 295, 41387, 39925, 733, 295, 31111, 9482, 11, 457, 291, 393, 380, 362, 886, 51088], "temperature": 0.0, "avg_logprob": -0.10680485638705167, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.0015043478924781084}, {"id": 861, "seek": 444278, "start": 4457.259999999999, "end": 4461.82, "text": " much anthropomorphism because then you'll get carried away. You have to find, we have to find", "tokens": [51088, 709, 22727, 32702, 1434, 570, 550, 291, 603, 483, 9094, 1314, 13, 509, 362, 281, 915, 11, 321, 362, 281, 915, 51316], "temperature": 0.0, "avg_logprob": -0.10680485638705167, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.0015043478924781084}, {"id": 862, "seek": 444278, "start": 4461.82, "end": 4467.099999999999, "text": " the right balance between modeling kind of human-like modular systems, but not doing it", "tokens": [51316, 264, 558, 4772, 1296, 15983, 733, 295, 1952, 12, 4092, 31111, 3652, 11, 457, 406, 884, 309, 51580], "temperature": 0.0, "avg_logprob": -0.10680485638705167, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.0015043478924781084}, {"id": 863, "seek": 446710, "start": 4467.1, "end": 4471.58, "text": " to a degree that is a bit implausible or scientifically unhelpful.", "tokens": [50364, 281, 257, 4314, 300, 307, 257, 857, 8484, 8463, 964, 420, 39719, 517, 37451, 906, 13, 50588], "temperature": 0.0, "avg_logprob": -0.08096506118774414, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.002975892974063754}, {"id": 864, "seek": 446710, "start": 4475.58, "end": 4478.9400000000005, "text": " I mean, I think I agree with all of that. I'm really excited about these", "tokens": [50788, 286, 914, 11, 286, 519, 286, 3986, 365, 439, 295, 300, 13, 286, 478, 534, 2919, 466, 613, 50956], "temperature": 0.0, "avg_logprob": -0.08096506118774414, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.002975892974063754}, {"id": 865, "seek": 446710, "start": 4479.660000000001, "end": 4486.38, "text": " ways of kind of connecting language models to other forms of information processing,", "tokens": [50992, 2098, 295, 733, 295, 11015, 2856, 5245, 281, 661, 6422, 295, 1589, 9007, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08096506118774414, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.002975892974063754}, {"id": 866, "seek": 446710, "start": 4486.38, "end": 4491.5, "text": " which does seem like what people have. I think I've been very surprised at the", "tokens": [51328, 597, 775, 1643, 411, 437, 561, 362, 13, 286, 519, 286, 600, 668, 588, 6100, 412, 264, 51584], "temperature": 0.0, "avg_logprob": -0.08096506118774414, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.002975892974063754}, {"id": 867, "seek": 449150, "start": 4492.46, "end": 4499.98, "text": " the things they are able to do just as language modeling, right? So different kinds of reasoning", "tokens": [50412, 264, 721, 436, 366, 1075, 281, 360, 445, 382, 2856, 15983, 11, 558, 30, 407, 819, 3685, 295, 21577, 50788], "temperature": 0.0, "avg_logprob": -0.11550156864119165, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.008571835234761238}, {"id": 868, "seek": 449150, "start": 4499.98, "end": 4507.18, "text": " puzzles and things that they can solve, I think, is really fascinating and maybe will require us", "tokens": [50788, 24138, 293, 721, 300, 436, 393, 5039, 11, 286, 519, 11, 307, 534, 10343, 293, 1310, 486, 3651, 505, 51148], "temperature": 0.0, "avg_logprob": -0.11550156864119165, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.008571835234761238}, {"id": 869, "seek": 449150, "start": 4507.18, "end": 4513.74, "text": " to rethink the relationships between language and thought and try to figure out a way of being", "tokens": [51148, 281, 34595, 264, 6159, 1296, 2856, 293, 1194, 293, 853, 281, 2573, 484, 257, 636, 295, 885, 51476], "temperature": 0.0, "avg_logprob": -0.11550156864119165, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.008571835234761238}, {"id": 870, "seek": 449150, "start": 4513.74, "end": 4519.5, "text": " specific about what it means for something to have a representation or to reason over that", "tokens": [51476, 2685, 466, 437, 309, 1355, 337, 746, 281, 362, 257, 10290, 420, 281, 1778, 670, 300, 51764], "temperature": 0.0, "avg_logprob": -0.11550156864119165, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.008571835234761238}, {"id": 871, "seek": 451950, "start": 4519.5, "end": 4527.18, "text": " representation. But ultimately, I think I agree that people have different modes of thinking about", "tokens": [50364, 10290, 13, 583, 6284, 11, 286, 519, 286, 3986, 300, 561, 362, 819, 14068, 295, 1953, 466, 50748], "temperature": 0.0, "avg_logprob": -0.12693536103661382, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.002049664268270135}, {"id": 872, "seek": 451950, "start": 4527.18, "end": 4536.06, "text": " things and that seems important for intelligence. I'm also super excited about the BabyLM challenge.", "tokens": [50748, 721, 293, 300, 2544, 1021, 337, 7599, 13, 286, 478, 611, 1687, 2919, 466, 264, 9425, 43, 44, 3430, 13, 51192], "temperature": 0.0, "avg_logprob": -0.12693536103661382, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.002049664268270135}, {"id": 873, "seek": 451950, "start": 4536.06, "end": 4543.66, "text": " So I think on the kind of linguistic side, that's exactly the right thing of seeing how far we can", "tokens": [51192, 407, 286, 519, 322, 264, 733, 295, 43002, 1252, 11, 300, 311, 2293, 264, 558, 551, 295, 2577, 577, 1400, 321, 393, 51572], "temperature": 0.0, "avg_logprob": -0.12693536103661382, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.002049664268270135}, {"id": 874, "seek": 454366, "start": 4543.66, "end": 4553.66, "text": " get with smaller data sets and maybe eventually after that trying to understand some more about", "tokens": [50364, 483, 365, 4356, 1412, 6352, 293, 1310, 4728, 934, 300, 1382, 281, 1223, 512, 544, 466, 50864], "temperature": 0.0, "avg_logprob": -0.11378563427534259, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.0566130168735981}, {"id": 875, "seek": 454366, "start": 4553.66, "end": 4559.82, "text": " the kinds of semantics that kids acquire and where they get it from and how kind of external semantics", "tokens": [50864, 264, 3685, 295, 4361, 45298, 300, 2301, 20001, 293, 689, 436, 483, 309, 490, 293, 577, 733, 295, 8320, 4361, 45298, 51172], "temperature": 0.0, "avg_logprob": -0.11378563427534259, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.0566130168735981}, {"id": 876, "seek": 454366, "start": 4559.82, "end": 4568.0599999999995, "text": " can inform language learning or specifically maybe grammar and syntax learning. I guess my other", "tokens": [51172, 393, 1356, 2856, 2539, 420, 4682, 1310, 22317, 293, 28431, 2539, 13, 286, 2041, 452, 661, 51584], "temperature": 0.0, "avg_logprob": -0.11378563427534259, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.0566130168735981}, {"id": 877, "seek": 456806, "start": 4568.9400000000005, "end": 4577.18, "text": " path forward point would be that there's... I feel like these kinds of models have", "tokens": [50408, 3100, 2128, 935, 576, 312, 300, 456, 311, 485, 286, 841, 411, 613, 3685, 295, 5245, 362, 50820], "temperature": 0.0, "avg_logprob": -0.1765999389907061, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.004903825931251049}, {"id": 878, "seek": 456806, "start": 4577.18, "end": 4584.9400000000005, "text": " have really gone far beyond people's expectations for this kind of class of model, right? Kind of", "tokens": [50820, 362, 534, 2780, 1400, 4399, 561, 311, 9843, 337, 341, 733, 295, 1508, 295, 2316, 11, 558, 30, 9242, 295, 51208], "temperature": 0.0, "avg_logprob": -0.1765999389907061, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.004903825931251049}, {"id": 879, "seek": 456806, "start": 4584.9400000000005, "end": 4593.1, "text": " ground up statistical learning, discovering patterns in text seems to give really pretty", "tokens": [51208, 2727, 493, 22820, 2539, 11, 24773, 8294, 294, 2487, 2544, 281, 976, 534, 1238, 51616], "temperature": 0.0, "avg_logprob": -0.1765999389907061, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.004903825931251049}, {"id": 880, "seek": 459310, "start": 4593.1, "end": 4599.26, "text": " remarkable results. And that for me going forward, I think has just introduced a huge wave of", "tokens": [50364, 12802, 3542, 13, 400, 300, 337, 385, 516, 2128, 11, 286, 519, 575, 445, 7268, 257, 2603, 5772, 295, 50672], "temperature": 0.0, "avg_logprob": -0.12022139386432927, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.017162783071398735}, {"id": 881, "seek": 459310, "start": 4599.26, "end": 4605.740000000001, "text": " uncertainty over theories. So I think that our theories of basically everything in language for", "tokens": [50672, 15697, 670, 13667, 13, 407, 286, 519, 300, 527, 13667, 295, 1936, 1203, 294, 2856, 337, 50996], "temperature": 0.0, "avg_logprob": -0.12022139386432927, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.017162783071398735}, {"id": 882, "seek": 459310, "start": 4605.740000000001, "end": 4612.46, "text": " sure, but cognition, probably neuroscience, like all of those things I think are going to be reworked", "tokens": [50996, 988, 11, 457, 46905, 11, 1391, 42762, 11, 411, 439, 295, 729, 721, 286, 519, 366, 516, 281, 312, 48376, 292, 51332], "temperature": 0.0, "avg_logprob": -0.12022139386432927, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.017162783071398735}, {"id": 883, "seek": 459310, "start": 4612.46, "end": 4619.660000000001, "text": " when we really come to kind of understand the ability of really general kinds of learning", "tokens": [51332, 562, 321, 534, 808, 281, 733, 295, 1223, 264, 3485, 295, 534, 2674, 3685, 295, 2539, 51692], "temperature": 0.0, "avg_logprob": -0.12022139386432927, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.017162783071398735}, {"id": 884, "seek": 461966, "start": 4619.66, "end": 4628.38, "text": " systems like these. So that makes it on the one hand kind of a bummer for past theories,", "tokens": [50364, 3652, 411, 613, 13, 407, 300, 1669, 309, 322, 264, 472, 1011, 733, 295, 257, 13309, 936, 337, 1791, 13667, 11, 50800], "temperature": 0.0, "avg_logprob": -0.10366275816252737, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0015006347093731165}, {"id": 885, "seek": 461966, "start": 4628.38, "end": 4636.62, "text": " especially theories which relied on learning not being able to work well. But on the upside,", "tokens": [50800, 2318, 13667, 597, 35463, 322, 2539, 406, 885, 1075, 281, 589, 731, 13, 583, 322, 264, 14119, 11, 51212], "temperature": 0.0, "avg_logprob": -0.10366275816252737, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0015006347093731165}, {"id": 886, "seek": 461966, "start": 4636.62, "end": 4643.26, "text": " I think it makes it a very exciting time both for AI and cognitive science and linguistics,", "tokens": [51212, 286, 519, 309, 1669, 309, 257, 588, 4670, 565, 1293, 337, 7318, 293, 15605, 3497, 293, 21766, 6006, 11, 51544], "temperature": 0.0, "avg_logprob": -0.10366275816252737, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0015006347093731165}, {"id": 887, "seek": 464326, "start": 4643.9800000000005, "end": 4649.5, "text": " where now there's these really, really powerful tools that seem like a qualitatively", "tokens": [50400, 689, 586, 456, 311, 613, 534, 11, 534, 4005, 3873, 300, 1643, 411, 257, 31312, 356, 50676], "temperature": 0.0, "avg_logprob": -0.08668396631876628, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.03159652650356293}, {"id": 888, "seek": 464326, "start": 4650.54, "end": 4656.9400000000005, "text": " different size step towards human abilities. And I think kind of integrating them and taking", "tokens": [50728, 819, 2744, 1823, 3030, 1952, 11582, 13, 400, 286, 519, 733, 295, 26889, 552, 293, 1940, 51048], "temperature": 0.0, "avg_logprob": -0.08668396631876628, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.03159652650356293}, {"id": 889, "seek": 464326, "start": 4657.820000000001, "end": 4663.820000000001, "text": " both the kind of engineering lessons and the kind of philosophical lessons about how they're made", "tokens": [51092, 1293, 264, 733, 295, 7043, 8820, 293, 264, 733, 295, 25066, 8820, 466, 577, 436, 434, 1027, 51392], "temperature": 0.0, "avg_logprob": -0.08668396631876628, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.03159652650356293}, {"id": 890, "seek": 464326, "start": 4663.820000000001, "end": 4669.26, "text": " and what kinds of principles go into designing intelligent systems. I think that those things", "tokens": [51392, 293, 437, 3685, 295, 9156, 352, 666, 14685, 13232, 3652, 13, 286, 519, 300, 729, 721, 51664], "temperature": 0.0, "avg_logprob": -0.08668396631876628, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.03159652650356293}, {"id": 891, "seek": 466926, "start": 4669.26, "end": 4676.780000000001, "text": " will really shape the field over the next five or 10 years. And also, I would just say in the", "tokens": [50364, 486, 534, 3909, 264, 2519, 670, 264, 958, 1732, 420, 1266, 924, 13, 400, 611, 11, 286, 576, 445, 584, 294, 264, 50740], "temperature": 0.0, "avg_logprob": -0.20660768655630257, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.004585757851600647}, {"id": 892, "seek": 466926, "start": 4676.780000000001, "end": 4680.9400000000005, "text": " context of broader themes here, right, like you're totally right, like I remember when I was reading", "tokens": [50740, 4319, 295, 13227, 13544, 510, 11, 558, 11, 411, 291, 434, 3879, 558, 11, 411, 286, 1604, 562, 286, 390, 3760, 50948], "temperature": 0.0, "avg_logprob": -0.20660768655630257, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.004585757851600647}, {"id": 893, "seek": 466926, "start": 4680.9400000000005, "end": 4687.820000000001, "text": " about when Deep Blue, the Kasparov, was it, the chess thing, right? And there were some commentators", "tokens": [50948, 466, 562, 14895, 8510, 11, 264, 28059, 2181, 5179, 11, 390, 309, 11, 264, 24122, 551, 11, 558, 30, 400, 456, 645, 512, 2871, 3391, 51292], "temperature": 0.0, "avg_logprob": -0.20660768655630257, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.004585757851600647}, {"id": 894, "seek": 466926, "start": 4687.820000000001, "end": 4693.26, "text": " who said, you know, chess is over. If an AI can beat a human, then it's game over. What's the", "tokens": [51292, 567, 848, 11, 291, 458, 11, 24122, 307, 670, 13, 759, 364, 7318, 393, 4224, 257, 1952, 11, 550, 309, 311, 1216, 670, 13, 708, 311, 264, 51564], "temperature": 0.0, "avg_logprob": -0.20660768655630257, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.004585757851600647}, {"id": 895, "seek": 466926, "start": 4693.26, "end": 4698.860000000001, "text": " point in studying chess? You know, there's no need of boring anymore. And I guess if AI has achieved", "tokens": [51564, 935, 294, 7601, 24122, 30, 509, 458, 11, 456, 311, 572, 643, 295, 9989, 3602, 13, 400, 286, 2041, 498, 7318, 575, 11042, 51844], "temperature": 0.0, "avg_logprob": -0.20660768655630257, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.004585757851600647}, {"id": 896, "seek": 469886, "start": 4698.86, "end": 4702.38, "text": " seemingly everything that humans need to do to play chess, what's the point of playing it?", "tokens": [50364, 18709, 1203, 300, 6255, 643, 281, 360, 281, 862, 24122, 11, 437, 311, 264, 935, 295, 2433, 309, 30, 50540], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 897, "seek": 469886, "start": 4703.179999999999, "end": 4706.62, "text": " But I think, you know, if anything, it turned out to increase the popularity of chess, right?", "tokens": [50580, 583, 286, 519, 11, 291, 458, 11, 498, 1340, 11, 309, 3574, 484, 281, 3488, 264, 19301, 295, 24122, 11, 558, 30, 50752], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 898, "seek": 469886, "start": 4706.62, "end": 4711.259999999999, "text": " There are now many chess celebrities as well, worldwide tournaments. And I would predict that", "tokens": [50752, 821, 366, 586, 867, 24122, 23200, 382, 731, 11, 13485, 32004, 13, 400, 286, 576, 6069, 300, 50984], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 899, "seek": 469886, "start": 4711.259999999999, "end": 4714.86, "text": " the same is probably going to happen with language too. You know, LLMs do not mean", "tokens": [50984, 264, 912, 307, 1391, 516, 281, 1051, 365, 2856, 886, 13, 509, 458, 11, 441, 43, 26386, 360, 406, 914, 51164], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 900, "seek": 469886, "start": 4714.86, "end": 4718.46, "text": " it's the end of language, no more language, no more linguistics. I would actually push back", "tokens": [51164, 309, 311, 264, 917, 295, 2856, 11, 572, 544, 2856, 11, 572, 544, 21766, 6006, 13, 286, 576, 767, 2944, 646, 51344], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 901, "seek": 469886, "start": 4718.46, "end": 4723.339999999999, "text": " and say maybe it would be the opposite. You know, the success of LLMs will increase general", "tokens": [51344, 293, 584, 1310, 309, 576, 312, 264, 6182, 13, 509, 458, 11, 264, 2245, 295, 441, 43, 26386, 486, 3488, 2674, 51588], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 902, "seek": 469886, "start": 4723.339999999999, "end": 4727.9, "text": " interest in linguistic theory, due to their apparent, you know, weird constraints and apparent", "tokens": [51588, 1179, 294, 43002, 5261, 11, 3462, 281, 641, 18335, 11, 291, 458, 11, 3657, 18491, 293, 18335, 51816], "temperature": 0.0, "avg_logprob": -0.09661754284029693, "compression_ratio": 1.807909604519774, "no_speech_prob": 0.0009173268335871398}, {"id": 903, "seek": 472790, "start": 4727.9, "end": 4733.099999999999, "text": " limitations, right? Because I would also say, you know, scale, at this point, the chess issue,", "tokens": [50364, 15705, 11, 558, 30, 1436, 286, 576, 611, 584, 11, 291, 458, 11, 4373, 11, 412, 341, 935, 11, 264, 24122, 2734, 11, 50624], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 904, "seek": 472790, "start": 4733.74, "end": 4739.42, "text": " scale is kind of definitely far from all that's needed. What is lacking is an ability of LLMs to,", "tokens": [50656, 4373, 307, 733, 295, 2138, 1400, 490, 439, 300, 311, 2978, 13, 708, 307, 20889, 307, 364, 3485, 295, 441, 43, 26386, 281, 11, 50940], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 905, "seek": 472790, "start": 4739.42, "end": 4743.58, "text": " you know, really abstract their knowledge and experiences in order to make robust predictions", "tokens": [50940, 291, 458, 11, 534, 12649, 641, 3601, 293, 5235, 294, 1668, 281, 652, 13956, 21264, 51148], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 906, "seek": 472790, "start": 4743.58, "end": 4747.339999999999, "text": " and generalizations and so on. I gave some examples, but there's some others in the literature", "tokens": [51148, 293, 2674, 14455, 293, 370, 322, 13, 286, 2729, 512, 5110, 11, 457, 456, 311, 512, 2357, 294, 264, 10394, 51336], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 907, "seek": 472790, "start": 4747.339999999999, "end": 4751.099999999999, "text": " where it doesn't seem to really be good at generalizing. It can kind of mimic particular", "tokens": [51336, 689, 309, 1177, 380, 1643, 281, 534, 312, 665, 412, 2674, 3319, 13, 467, 393, 733, 295, 31075, 1729, 51524], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 908, "seek": 472790, "start": 4751.099999999999, "end": 4756.299999999999, "text": " token types. But I would, you know, I would guess my final, my final thing would be that,", "tokens": [51524, 14862, 3467, 13, 583, 286, 576, 11, 291, 458, 11, 286, 576, 2041, 452, 2572, 11, 452, 2572, 551, 576, 312, 300, 11, 51784], "temperature": 0.0, "avg_logprob": -0.12103348884029665, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0023083833511918783}, {"id": 909, "seek": 475630, "start": 4756.3, "end": 4761.820000000001, "text": " you know, the language acquisition literature doesn't necessarily need LLMs though. You know,", "tokens": [50364, 291, 458, 11, 264, 2856, 21668, 10394, 1177, 380, 4725, 643, 441, 43, 26386, 1673, 13, 509, 458, 11, 50640], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 910, "seek": 475630, "start": 4761.820000000001, "end": 4766.46, "text": " cognitive scientists don't really need LLMs. We could potentially, you know,", "tokens": [50640, 15605, 7708, 500, 380, 534, 643, 441, 43, 26386, 13, 492, 727, 7263, 11, 291, 458, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 911, "seek": 475630, "start": 4766.46, "end": 4770.46, "text": " me and Stephen obviously disagree here, but I would say big tech companies", "tokens": [50872, 385, 293, 13391, 2745, 14091, 510, 11, 457, 286, 576, 584, 955, 7553, 3431, 51072], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 912, "seek": 475630, "start": 4770.46, "end": 4774.62, "text": " profiting off LLMs need LLMs, right? They're the only ones that really do. It may be the case", "tokens": [51072, 1740, 1748, 766, 441, 43, 26386, 643, 441, 43, 26386, 11, 558, 30, 814, 434, 264, 787, 2306, 300, 534, 360, 13, 467, 815, 312, 264, 1389, 51280], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 913, "seek": 475630, "start": 4774.62, "end": 4779.18, "text": " that the mind is a very, I will say, you know, the mind is a very diverse space. It may be that", "tokens": [51280, 300, 264, 1575, 307, 257, 588, 11, 286, 486, 584, 11, 291, 458, 11, 264, 1575, 307, 257, 588, 9521, 1901, 13, 467, 815, 312, 300, 51508], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 914, "seek": 475630, "start": 4779.18, "end": 4783.26, "text": " there are certain forms of behavior and learning that might be captured by processes similar to", "tokens": [51508, 456, 366, 1629, 6422, 295, 5223, 293, 2539, 300, 1062, 312, 11828, 538, 7555, 2531, 281, 51712], "temperature": 0.0, "avg_logprob": -0.09311021360239588, "compression_ratio": 1.7467105263157894, "no_speech_prob": 0.0007932067383080721}, {"id": 915, "seek": 478326, "start": 4783.26, "end": 4786.54, "text": " what LLMs are doing. So Stephen has given some interesting examples in his papers about", "tokens": [50364, 437, 441, 43, 26386, 366, 884, 13, 407, 13391, 575, 2212, 512, 1880, 5110, 294, 702, 10577, 466, 50528], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 916, "seek": 478326, "start": 4786.54, "end": 4791.26, "text": " magnetism and we're kind of rules of learning that are very, very general and very quick and", "tokens": [50528, 15211, 1434, 293, 321, 434, 733, 295, 4474, 295, 2539, 300, 366, 588, 11, 588, 2674, 293, 588, 1702, 293, 50764], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 917, "seek": 478326, "start": 4791.26, "end": 4795.900000000001, "text": " very mysterious. So, you know, maybe for those sorts of things, that kind of learning will be", "tokens": [50764, 588, 13831, 13, 407, 11, 291, 458, 11, 1310, 337, 729, 7527, 295, 721, 11, 300, 733, 295, 2539, 486, 312, 50996], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 918, "seek": 478326, "start": 4795.900000000001, "end": 4800.3, "text": " relevant. But I still think it's unlikely that one of the candidates will be natural language,", "tokens": [50996, 7340, 13, 583, 286, 920, 519, 309, 311, 17518, 300, 472, 295, 264, 11255, 486, 312, 3303, 2856, 11, 51216], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 919, "seek": 478326, "start": 4801.42, "end": 4805.1, "text": " at least the way natural language works in its full glory in terms of the four meaning", "tokens": [51272, 412, 1935, 264, 636, 3303, 2856, 1985, 294, 1080, 1577, 11924, 294, 2115, 295, 264, 1451, 3620, 51456], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 920, "seek": 478326, "start": 4805.1, "end": 4809.74, "text": " regulation and what have you. So I guess I would, you know, it kind of reminds me of where you,", "tokens": [51456, 15062, 293, 437, 362, 291, 13, 407, 286, 2041, 286, 576, 11, 291, 458, 11, 309, 733, 295, 12025, 385, 295, 689, 291, 11, 51688], "temperature": 0.0, "avg_logprob": -0.161365236554827, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.009245551191270351}, {"id": 921, "seek": 480974, "start": 4810.54, "end": 4814.54, "text": " you know, you have this image of, I saw John with chapter four recently, right? And he has this,", "tokens": [50404, 291, 458, 11, 291, 362, 341, 3256, 295, 11, 286, 1866, 2619, 365, 7187, 1451, 3938, 11, 558, 30, 400, 415, 575, 341, 11, 50604], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 922, "seek": 480974, "start": 4814.54, "end": 4817.58, "text": " there's this scene where he's walking in the desert and he's not sure if he's seen", "tokens": [50604, 456, 311, 341, 4145, 689, 415, 311, 4494, 294, 264, 11029, 293, 415, 311, 406, 988, 498, 415, 311, 1612, 50756], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 923, "seek": 480974, "start": 4817.58, "end": 4821.74, "text": " this guy that he wants to assassinate. It's kind of like when you walk in the desert", "tokens": [50756, 341, 2146, 300, 415, 2738, 281, 16475, 13923, 13, 467, 311, 733, 295, 411, 562, 291, 1792, 294, 264, 11029, 50964], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 924, "seek": 480974, "start": 4822.7, "end": 4827.099999999999, "text": " and you have an illusion of seeing an oasis because it turns out you're hallucinating.", "tokens": [51012, 293, 291, 362, 364, 18854, 295, 2577, 364, 277, 26632, 570, 309, 4523, 484, 291, 434, 35212, 8205, 13, 51232], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 925, "seek": 480974, "start": 4827.099999999999, "end": 4830.86, "text": " But then you realize that, you know, sometimes before it's too late that you actually are", "tokens": [51232, 583, 550, 291, 4325, 300, 11, 291, 458, 11, 2171, 949, 309, 311, 886, 3469, 300, 291, 767, 366, 51420], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 926, "seek": 480974, "start": 4830.86, "end": 4834.46, "text": " hallucinating. It's you're not seeing an oasis. You're still in the desert. And I think that's", "tokens": [51420, 35212, 8205, 13, 467, 311, 291, 434, 406, 2577, 364, 277, 26632, 13, 509, 434, 920, 294, 264, 11029, 13, 400, 286, 519, 300, 311, 51600], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 927, "seek": 480974, "start": 4834.46, "end": 4839.58, "text": " kind of maybe the situation we're in right now with linguistic competence of laws of language", "tokens": [51600, 733, 295, 1310, 264, 2590, 321, 434, 294, 558, 586, 365, 43002, 39965, 295, 6064, 295, 2856, 51856], "temperature": 0.0, "avg_logprob": -0.0952806293964386, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0031935246661305428}, {"id": 928, "seek": 483958, "start": 4839.58, "end": 4845.5, "text": " models. We have the illusion of linguistic competence. But, you know, you always see the", "tokens": [50364, 5245, 13, 492, 362, 264, 18854, 295, 43002, 39965, 13, 583, 11, 291, 458, 11, 291, 1009, 536, 264, 50660], "temperature": 0.0, "avg_logprob": -0.11780359480116102, "compression_ratio": 1.65, "no_speech_prob": 0.000967180822044611}, {"id": 929, "seek": 483958, "start": 4845.5, "end": 4849.66, "text": " illusion before you find the oasis, right? So I think, I think right now we're in the", "tokens": [50660, 18854, 949, 291, 915, 264, 277, 26632, 11, 558, 30, 407, 286, 519, 11, 286, 519, 558, 586, 321, 434, 294, 264, 50868], "temperature": 0.0, "avg_logprob": -0.11780359480116102, "compression_ratio": 1.65, "no_speech_prob": 0.000967180822044611}, {"id": 930, "seek": 483958, "start": 4849.66, "end": 4854.54, "text": " hallucinating stage of the desert where we're seeing potential sparks of linguistic competence,", "tokens": [50868, 35212, 8205, 3233, 295, 264, 11029, 689, 321, 434, 2577, 3995, 44102, 295, 43002, 39965, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11780359480116102, "compression_ratio": 1.65, "no_speech_prob": 0.000967180822044611}, {"id": 931, "seek": 483958, "start": 4854.54, "end": 4859.66, "text": " but it's still not very clear and I'm robust. And we haven't actually reached the oasis yet.", "tokens": [51112, 457, 309, 311, 920, 406, 588, 1850, 293, 286, 478, 13956, 13, 400, 321, 2378, 380, 767, 6488, 264, 277, 26632, 1939, 13, 51368], "temperature": 0.0, "avg_logprob": -0.11780359480116102, "compression_ratio": 1.65, "no_speech_prob": 0.000967180822044611}, {"id": 932, "seek": 485966, "start": 4860.62, "end": 4868.7, "text": " Um, just a rapid fire question. So see if you can give a short response. So", "tokens": [50412, 3301, 11, 445, 257, 7558, 2610, 1168, 13, 407, 536, 498, 291, 393, 976, 257, 2099, 4134, 13, 407, 50816], "temperature": 0.0, "avg_logprob": -0.2676827206331141, "compression_ratio": 1.5341614906832297, "no_speech_prob": 0.06271393597126007}, {"id": 933, "seek": 485966, "start": 4869.66, "end": 4875.58, "text": " Sphinode, you know, writes question, is it correct to say that large language models have no priors?", "tokens": [50864, 318, 48522, 1429, 11, 291, 458, 11, 13657, 1168, 11, 307, 309, 3006, 281, 584, 300, 2416, 2856, 5245, 362, 572, 1790, 830, 30, 51160], "temperature": 0.0, "avg_logprob": -0.2676827206331141, "compression_ratio": 1.5341614906832297, "no_speech_prob": 0.06271393597126007}, {"id": 934, "seek": 485966, "start": 4878.62, "end": 4882.7, "text": " Do large language models have priors? I'd say yes, they definitely do.", "tokens": [51312, 1144, 2416, 2856, 5245, 362, 1790, 830, 30, 286, 1116, 584, 2086, 11, 436, 2138, 360, 13, 51516], "temperature": 0.0, "avg_logprob": -0.2676827206331141, "compression_ratio": 1.5341614906832297, "no_speech_prob": 0.06271393597126007}, {"id": 935, "seek": 488270, "start": 4883.099999999999, "end": 4890.54, "text": " Um, and there, I think the difference to how people, you know, are used to thinking about", "tokens": [50384, 3301, 11, 293, 456, 11, 286, 519, 264, 2649, 281, 577, 561, 11, 291, 458, 11, 366, 1143, 281, 1953, 466, 50756], "temperature": 0.0, "avg_logprob": -0.13197308071589064, "compression_ratio": 1.8588235294117648, "no_speech_prob": 0.0069018155336380005}, {"id": 936, "seek": 488270, "start": 4890.54, "end": 4895.42, "text": " priors in Bayesian inference, for example, if you like write down a Bayesian statistical model,", "tokens": [50756, 1790, 830, 294, 7840, 42434, 38253, 11, 337, 1365, 11, 498, 291, 411, 2464, 760, 257, 7840, 42434, 22820, 2316, 11, 51000], "temperature": 0.0, "avg_logprob": -0.13197308071589064, "compression_ratio": 1.8588235294117648, "no_speech_prob": 0.0069018155336380005}, {"id": 937, "seek": 488270, "start": 4895.42, "end": 4899.74, "text": " you say like, you know, here's the parameters and here's what the priors are on the parameters.", "tokens": [51000, 291, 584, 411, 11, 291, 458, 11, 510, 311, 264, 9834, 293, 510, 311, 437, 264, 1790, 830, 366, 322, 264, 9834, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13197308071589064, "compression_ratio": 1.8588235294117648, "no_speech_prob": 0.0069018155336380005}, {"id": 938, "seek": 488270, "start": 4900.78, "end": 4905.0199999999995, "text": " Large language models, I think the priors are and maybe neural nets in general, I think that the", "tokens": [51268, 33092, 2856, 5245, 11, 286, 519, 264, 1790, 830, 366, 293, 1310, 18161, 36170, 294, 2674, 11, 286, 519, 300, 264, 51480], "temperature": 0.0, "avg_logprob": -0.13197308071589064, "compression_ratio": 1.8588235294117648, "no_speech_prob": 0.0069018155336380005}, {"id": 939, "seek": 488270, "start": 4905.0199999999995, "end": 4909.98, "text": " that the priors are much more implicit, right? So there's some functions which they find easier", "tokens": [51480, 300, 264, 1790, 830, 366, 709, 544, 26947, 11, 558, 30, 407, 456, 311, 512, 6828, 597, 436, 915, 3571, 51728], "temperature": 0.0, "avg_logprob": -0.13197308071589064, "compression_ratio": 1.8588235294117648, "no_speech_prob": 0.0069018155336380005}, {"id": 940, "seek": 490998, "start": 4909.98, "end": 4915.5, "text": " to learn than other functions. And there's even some work trying to discover, you know,", "tokens": [50364, 281, 1466, 813, 661, 6828, 13, 400, 456, 311, 754, 512, 589, 1382, 281, 4411, 11, 291, 458, 11, 50640], "temperature": 0.0, "avg_logprob": -0.11326305888523565, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0028433690313249826}, {"id": 941, "seek": 490998, "start": 4915.5, "end": 4921.0199999999995, "text": " some statement of what those kind of implicit priors are. But that's actually how I think about,", "tokens": [50640, 512, 5629, 295, 437, 729, 733, 295, 26947, 1790, 830, 366, 13, 583, 300, 311, 767, 577, 286, 519, 466, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11326305888523565, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0028433690313249826}, {"id": 942, "seek": 490998, "start": 4923.099999999999, "end": 4926.459999999999, "text": " you know, comparison of different neural network architectures, right,", "tokens": [51020, 291, 458, 11, 9660, 295, 819, 18161, 3209, 6331, 1303, 11, 558, 11, 51188], "temperature": 0.0, "avg_logprob": -0.11326305888523565, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0028433690313249826}, {"id": 943, "seek": 490998, "start": 4927.58, "end": 4932.0599999999995, "text": " which is maybe something Elliot and I might agree on, right? Like you have to find priors which", "tokens": [51244, 597, 307, 1310, 746, 38986, 293, 286, 1062, 3986, 322, 11, 558, 30, 1743, 291, 362, 281, 915, 1790, 830, 597, 51468], "temperature": 0.0, "avg_logprob": -0.11326305888523565, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0028433690313249826}, {"id": 944, "seek": 490998, "start": 4932.0599999999995, "end": 4938.219999999999, "text": " allow them to learn the things that kids learn, right? And not all architectures will do that.", "tokens": [51468, 2089, 552, 281, 1466, 264, 721, 300, 2301, 1466, 11, 558, 30, 400, 406, 439, 6331, 1303, 486, 360, 300, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11326305888523565, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0028433690313249826}, {"id": 945, "seek": 493822, "start": 4938.860000000001, "end": 4943.900000000001, "text": " Even among architectures which are turn complete or capable of learning any kind of function,", "tokens": [50396, 2754, 3654, 6331, 1303, 597, 366, 1261, 3566, 420, 8189, 295, 2539, 604, 733, 295, 2445, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11527007525084448, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.0006261775852181017}, {"id": 946, "seek": 493822, "start": 4943.900000000001, "end": 4951.5, "text": " not all of them will do it, even on kind of huge data set sizes. So I think of this sort of search", "tokens": [50648, 406, 439, 295, 552, 486, 360, 309, 11, 754, 322, 733, 295, 2603, 1412, 992, 11602, 13, 407, 286, 519, 295, 341, 1333, 295, 3164, 51028], "temperature": 0.0, "avg_logprob": -0.11527007525084448, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.0006261775852181017}, {"id": 947, "seek": 493822, "start": 4951.5, "end": 4958.06, "text": " over neural net architectures as really one of a search over priors. But it's not priors or,", "tokens": [51028, 670, 18161, 2533, 6331, 1303, 382, 534, 472, 295, 257, 3164, 670, 1790, 830, 13, 583, 309, 311, 406, 1790, 830, 420, 11, 51356], "temperature": 0.0, "avg_logprob": -0.11527007525084448, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.0006261775852181017}, {"id": 948, "seek": 493822, "start": 4958.06, "end": 4961.740000000001, "text": " I mean, you could think of it as a search over universal grammar or something, right? But it's,", "tokens": [51356, 286, 914, 11, 291, 727, 519, 295, 309, 382, 257, 3164, 670, 11455, 22317, 420, 746, 11, 558, 30, 583, 309, 311, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11527007525084448, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.0006261775852181017}, {"id": 949, "seek": 493822, "start": 4961.740000000001, "end": 4967.66, "text": " it's, it's not priors or universal grammar in the sense that people have talked about it as like", "tokens": [51540, 309, 311, 11, 309, 311, 406, 1790, 830, 420, 11455, 22317, 294, 264, 2020, 300, 561, 362, 2825, 466, 309, 382, 411, 51836], "temperature": 0.0, "avg_logprob": -0.11527007525084448, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.0006261775852181017}, {"id": 950, "seek": 496766, "start": 4967.66, "end": 4971.98, "text": " an explicit statement about what kinds of rules are allowed or an explicit statement about what", "tokens": [50364, 364, 13691, 5629, 466, 437, 3685, 295, 4474, 366, 4350, 420, 364, 13691, 5629, 466, 437, 50580], "temperature": 0.0, "avg_logprob": -0.1050559868246822, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0006759074749425054}, {"id": 951, "seek": 496766, "start": 4971.98, "end": 4976.54, "text": " kinds of functions or high probability or something like that. It's all implicitly coded there.", "tokens": [50580, 3685, 295, 6828, 420, 1090, 8482, 420, 746, 411, 300, 13, 467, 311, 439, 26947, 356, 34874, 456, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1050559868246822, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0006759074749425054}, {"id": 952, "seek": 496766, "start": 4977.26, "end": 4981.66, "text": " Yeah, totally. I think, I think that's right. I mean, you know, the real question is reducing", "tokens": [50844, 865, 11, 3879, 13, 286, 519, 11, 286, 519, 300, 311, 558, 13, 286, 914, 11, 291, 458, 11, 264, 957, 1168, 307, 12245, 51064], "temperature": 0.0, "avg_logprob": -0.1050559868246822, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0006759074749425054}, {"id": 953, "seek": 496766, "start": 4981.66, "end": 4986.86, "text": " the space of what those priors are like. And if it's anything remotely like what human beings", "tokens": [51064, 264, 1901, 295, 437, 729, 1790, 830, 366, 411, 13, 400, 498, 309, 311, 1340, 20824, 411, 437, 1952, 8958, 51324], "temperature": 0.0, "avg_logprob": -0.1050559868246822, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0006759074749425054}, {"id": 954, "seek": 496766, "start": 4986.86, "end": 4991.66, "text": " are doing, so LLMs like, I would, I would at least say that things like GPT-3 are an existence", "tokens": [51324, 366, 884, 11, 370, 441, 43, 26386, 411, 11, 286, 576, 11, 286, 576, 412, 1935, 584, 300, 721, 411, 26039, 51, 12, 18, 366, 364, 9123, 51564], "temperature": 0.0, "avg_logprob": -0.1050559868246822, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0006759074749425054}, {"id": 955, "seek": 499166, "start": 4991.66, "end": 4997.26, "text": " proof of, you know, that building fully functioning syntactic categories from surface", "tokens": [50364, 8177, 295, 11, 291, 458, 11, 300, 2390, 4498, 18483, 23980, 19892, 10479, 490, 3753, 50644], "temperature": 0.0, "avg_logprob": -0.08198760639537464, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.019428959116339684}, {"id": 956, "seek": 499166, "start": 4997.26, "end": 5003.5, "text": " distributional analysis alone is possible. That's, yes, that is correct. But, you know,", "tokens": [50644, 7316, 304, 5215, 3312, 307, 1944, 13, 663, 311, 11, 2086, 11, 300, 307, 3006, 13, 583, 11, 291, 458, 11, 50956], "temperature": 0.0, "avg_logprob": -0.08198760639537464, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.019428959116339684}, {"id": 957, "seek": 499166, "start": 5003.5, "end": 5009.9, "text": " even so, I would say most syntacticians don't really believe that syntactic categories are innate.", "tokens": [50956, 754, 370, 11, 286, 576, 584, 881, 23980, 19892, 2567, 500, 380, 534, 1697, 300, 23980, 19892, 10479, 366, 41766, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08198760639537464, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.019428959116339684}, {"id": 958, "seek": 499166, "start": 5009.9, "end": 5014.78, "text": " So the prior issue is slightly less relevant here. It's the operations that are said to be innate.", "tokens": [51276, 407, 264, 4059, 2734, 307, 4748, 1570, 7340, 510, 13, 467, 311, 264, 7705, 300, 366, 848, 281, 312, 41766, 13, 51520], "temperature": 0.0, "avg_logprob": -0.08198760639537464, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.019428959116339684}, {"id": 959, "seek": 499166, "start": 5014.78, "end": 5019.9, "text": " So the, in the syntax domain, it's particular linguistic computations that are said to be innate", "tokens": [51520, 407, 264, 11, 294, 264, 28431, 9274, 11, 309, 311, 1729, 43002, 2807, 763, 300, 366, 848, 281, 312, 41766, 51776], "temperature": 0.0, "avg_logprob": -0.08198760639537464, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.019428959116339684}, {"id": 960, "seek": 501990, "start": 5019.9, "end": 5023.98, "text": " and categories themselves. In fact, even Charles Young has admitted in the last couple of years", "tokens": [50364, 293, 10479, 2969, 13, 682, 1186, 11, 754, 10523, 8160, 575, 14920, 294, 264, 1036, 1916, 295, 924, 50568], "temperature": 0.0, "avg_logprob": -0.13878364730299564, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.005655788350850344}, {"id": 961, "seek": 501990, "start": 5023.98, "end": 5030.0599999999995, "text": " that they are maybe innate, but maybe not. So people have given, I know of a relevant prize,", "tokens": [50568, 300, 436, 366, 1310, 41766, 11, 457, 1310, 406, 13, 407, 561, 362, 2212, 11, 286, 458, 295, 257, 7340, 12818, 11, 50872], "temperature": 0.0, "avg_logprob": -0.13878364730299564, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.005655788350850344}, {"id": 962, "seek": 501990, "start": 5030.0599999999995, "end": 5034.46, "text": " they are things like, you know, me and Gary Marcus have talked about compositionality.", "tokens": [50872, 436, 366, 721, 411, 11, 291, 458, 11, 385, 293, 13788, 26574, 362, 2825, 466, 12686, 1860, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13878364730299564, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.005655788350850344}, {"id": 963, "seek": 501990, "start": 5034.46, "end": 5039.339999999999, "text": " That seems to be a big problem. So people have given chat GPT BBC news articles asking it to", "tokens": [51092, 663, 2544, 281, 312, 257, 955, 1154, 13, 407, 561, 362, 2212, 5081, 26039, 51, 22669, 2583, 11290, 3365, 309, 281, 51336], "temperature": 0.0, "avg_logprob": -0.13878364730299564, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.005655788350850344}, {"id": 964, "seek": 501990, "start": 5039.339999999999, "end": 5045.74, "text": " compress it and then re-explain it. So one example I saw was Peter Smith 58 is being arrested on", "tokens": [51336, 14778, 309, 293, 550, 319, 12, 23040, 491, 309, 13, 407, 472, 1365, 286, 1866, 390, 6508, 8538, 21786, 307, 885, 12469, 322, 51656], "temperature": 0.0, "avg_logprob": -0.13878364730299564, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.005655788350850344}, {"id": 965, "seek": 504574, "start": 5045.74, "end": 5051.58, "text": " charges of manslaughter and you get it to compress it and re-explain it. And it comes out as 58 people", "tokens": [50364, 12235, 295, 18868, 5330, 293, 291, 483, 309, 281, 14778, 309, 293, 319, 12, 23040, 491, 309, 13, 400, 309, 1487, 484, 382, 21786, 561, 50656], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 966, "seek": 504574, "start": 5051.58, "end": 5054.86, "text": " are being charged with manslaughter. All right. That's a pretty clear example of a lack of", "tokens": [50656, 366, 885, 11109, 365, 18868, 5330, 13, 1057, 558, 13, 663, 311, 257, 1238, 1850, 1365, 295, 257, 5011, 295, 50820], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 967, "seek": 504574, "start": 5054.86, "end": 5059.099999999999, "text": " compositionality being built into whatever compression it's doing. And there's no example", "tokens": [50820, 12686, 1860, 885, 3094, 666, 2035, 19355, 309, 311, 884, 13, 400, 456, 311, 572, 1365, 51032], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 968, "seek": 504574, "start": 5059.099999999999, "end": 5063.58, "text": " where there's been, there's some examples of potential analogical reasoning. So in Bing chat,", "tokens": [51032, 689, 456, 311, 668, 11, 456, 311, 512, 5110, 295, 3995, 16660, 804, 21577, 13, 407, 294, 30755, 5081, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 969, "seek": 504574, "start": 5063.58, "end": 5068.7, "text": " you know, Bing has this, this chat function. The question is, is it just finding meta relations", "tokens": [51256, 291, 458, 11, 30755, 575, 341, 11, 341, 5081, 2445, 13, 440, 1168, 307, 11, 307, 309, 445, 5006, 19616, 2299, 51512], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 970, "seek": 504574, "start": 5068.7, "end": 5073.179999999999, "text": " that have already been documented by humans or is it genuinely creating new relations that the new", "tokens": [51512, 300, 362, 1217, 668, 23007, 538, 6255, 420, 307, 309, 17839, 4084, 777, 2299, 300, 264, 777, 51736], "temperature": 0.0, "avg_logprob": -0.11995221618422888, "compression_ratio": 1.7875, "no_speech_prob": 0.007468287367373705}, {"id": 971, "seek": 507318, "start": 5073.18, "end": 5080.38, "text": " stuff that is being built. So, you know, someone asked through me a table comparing Jesus Christ", "tokens": [50364, 1507, 300, 307, 885, 3094, 13, 407, 11, 291, 458, 11, 1580, 2351, 807, 385, 257, 3199, 15763, 2705, 2040, 50724], "temperature": 0.0, "avg_logprob": -0.18635066674680126, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00560505036264658}, {"id": 972, "seek": 507318, "start": 5081.02, "end": 5087.34, "text": " with the Nokia 9910, right, the cell phone Nokia 9910. And it said, you know, it compared the", "tokens": [50756, 365, 264, 43980, 11803, 3279, 11, 558, 11, 264, 2815, 2593, 43980, 11803, 3279, 13, 400, 309, 848, 11, 291, 458, 11, 309, 5347, 264, 51072], "temperature": 0.0, "avg_logprob": -0.18635066674680126, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00560505036264658}, {"id": 973, "seek": 507318, "start": 5087.34, "end": 5094.54, "text": " release dates. It compared the size, the weight. It compared the CPU with Jesus's all-powerful", "tokens": [51072, 4374, 11691, 13, 467, 5347, 264, 2744, 11, 264, 3364, 13, 467, 5347, 264, 13199, 365, 2705, 311, 439, 12, 9513, 906, 51432], "temperature": 0.0, "avg_logprob": -0.18635066674680126, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00560505036264658}, {"id": 974, "seek": 507318, "start": 5094.54, "end": 5099.58, "text": " knowledge. It compared the memory of the phone with the all-knowing nature of God, right.", "tokens": [51432, 3601, 13, 467, 5347, 264, 4675, 295, 264, 2593, 365, 264, 439, 12, 15869, 278, 3687, 295, 1265, 11, 558, 13, 51684], "temperature": 0.0, "avg_logprob": -0.18635066674680126, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00560505036264658}, {"id": 975, "seek": 509958, "start": 5099.9, "end": 5104.94, "text": " And it also, I think it said that they were both resurrected because the Nokia was re-released a", "tokens": [50380, 400, 309, 611, 11, 286, 519, 309, 848, 300, 436, 645, 1293, 48825, 570, 264, 43980, 390, 319, 12, 265, 41087, 257, 50632], "temperature": 0.0, "avg_logprob": -0.15704139830574157, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.007688733283430338}, {"id": 976, "seek": 509958, "start": 5104.94, "end": 5109.0199999999995, "text": " couple of times, right. So the Nokia. That sounds like a great answer. What's wrong with that?", "tokens": [50632, 1916, 295, 1413, 11, 558, 13, 407, 264, 43980, 13, 663, 3263, 411, 257, 869, 1867, 13, 708, 311, 2085, 365, 300, 30, 50836], "temperature": 0.0, "avg_logprob": -0.15704139830574157, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.007688733283430338}, {"id": 977, "seek": 509958, "start": 5109.0199999999995, "end": 5114.62, "text": " Okay. It may be. It sounds a lot like analogical reasoning, but then it also had some quite weird", "tokens": [50836, 1033, 13, 467, 815, 312, 13, 467, 3263, 257, 688, 411, 16660, 804, 21577, 11, 457, 550, 309, 611, 632, 512, 1596, 3657, 51116], "temperature": 0.0, "avg_logprob": -0.15704139830574157, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.007688733283430338}, {"id": 978, "seek": 509958, "start": 5114.62, "end": 5119.1, "text": " ones where it was like, you know, for the camera, it said, no, it just gave Jesus's description,", "tokens": [51116, 2306, 689, 309, 390, 411, 11, 291, 458, 11, 337, 264, 2799, 11, 309, 848, 11, 572, 11, 309, 445, 2729, 2705, 311, 3855, 11, 51340], "temperature": 0.0, "avg_logprob": -0.15704139830574157, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.007688733283430338}, {"id": 979, "seek": 509958, "start": 5119.1, "end": 5123.98, "text": " but it's not really what a camera is. There's some kind of things that look like analogical", "tokens": [51340, 457, 309, 311, 406, 534, 437, 257, 2799, 307, 13, 821, 311, 512, 733, 295, 721, 300, 574, 411, 16660, 804, 51584], "temperature": 0.0, "avg_logprob": -0.15704139830574157, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.007688733283430338}, {"id": 980, "seek": 512398, "start": 5123.98, "end": 5132.0599999999995, "text": " reasoning, maybe, but it's unclear. Yeah. I think that sounds like an awesome answer to me.", "tokens": [50364, 21577, 11, 1310, 11, 457, 309, 311, 25636, 13, 865, 13, 286, 519, 300, 3263, 411, 364, 3476, 1867, 281, 385, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1895650291442871, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.004465392790734768}, {"id": 981, "seek": 512398, "start": 5134.62, "end": 5139.099999999999, "text": " I was going to say, like, you said large-language models learn they're an existence proof of part", "tokens": [50896, 286, 390, 516, 281, 584, 11, 411, 11, 291, 848, 2416, 12, 25241, 20473, 5245, 1466, 436, 434, 364, 9123, 8177, 295, 644, 51120], "temperature": 0.0, "avg_logprob": -0.1895650291442871, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.004465392790734768}, {"id": 982, "seek": 512398, "start": 5139.099999999999, "end": 5143.58, "text": " of speech categories, but like, they don't just output part of speech categories, right. Like,", "tokens": [51120, 295, 6218, 10479, 11, 457, 411, 11, 436, 500, 380, 445, 5598, 644, 295, 6218, 10479, 11, 558, 13, 1743, 11, 51344], "temperature": 0.0, "avg_logprob": -0.1895650291442871, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.004465392790734768}, {"id": 983, "seek": 512398, "start": 5143.58, "end": 5151.5, "text": " they have a lot of grammatical syntactic knowledge. And moreover, like, they have a lot of semantic", "tokens": [51344, 436, 362, 257, 688, 295, 17570, 267, 804, 23980, 19892, 3601, 13, 400, 544, 3570, 11, 411, 11, 436, 362, 257, 688, 295, 47982, 51740], "temperature": 0.0, "avg_logprob": -0.1895650291442871, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.004465392790734768}, {"id": 984, "seek": 515150, "start": 5151.5, "end": 5157.26, "text": " knowledge and probably some pragmatic knowledge. And, you know, they're not bad at translation.", "tokens": [50364, 3601, 293, 1391, 512, 46904, 3601, 13, 400, 11, 291, 458, 11, 436, 434, 406, 1578, 412, 12853, 13, 50652], "temperature": 0.0, "avg_logprob": -0.16271462551383084, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0071203759871423244}, {"id": 985, "seek": 515150, "start": 5157.26, "end": 5163.02, "text": " And like, it's way more that they have discovered than just part of speech categories.", "tokens": [50652, 400, 411, 11, 309, 311, 636, 544, 300, 436, 362, 6941, 813, 445, 644, 295, 6218, 10479, 13, 50940], "temperature": 0.0, "avg_logprob": -0.16271462551383084, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0071203759871423244}, {"id": 986, "seek": 515150, "start": 5164.86, "end": 5168.3, "text": " Well, sorry, I said syntactic. I'm sorry. It's like syntactic categories.", "tokens": [51032, 1042, 11, 2597, 11, 286, 848, 23980, 19892, 13, 286, 478, 2597, 13, 467, 311, 411, 23980, 19892, 10479, 13, 51204], "temperature": 0.0, "avg_logprob": -0.16271462551383084, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0071203759871423244}, {"id": 987, "seek": 515150, "start": 5169.1, "end": 5172.38, "text": " Right. Well, sorry. Yeah. Yeah. But they've discovered way more than that.", "tokens": [51244, 1779, 13, 1042, 11, 2597, 13, 865, 13, 865, 13, 583, 436, 600, 6941, 636, 544, 813, 300, 13, 51408], "temperature": 0.0, "avg_logprob": -0.16271462551383084, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0071203759871423244}, {"id": 988, "seek": 517238, "start": 5172.9400000000005, "end": 5181.9800000000005, "text": " Yeah. I'm going to, as a teaser slash motivator for hopefully both of you to join again in the", "tokens": [50392, 865, 13, 286, 478, 516, 281, 11, 382, 257, 35326, 17330, 5426, 1639, 337, 4696, 1293, 295, 291, 281, 3917, 797, 294, 264, 50844], "temperature": 0.0, "avg_logprob": -0.16635017735617502, "compression_ratio": 1.6076388888888888, "no_speech_prob": 0.03845371678471565}, {"id": 989, "seek": 517238, "start": 5181.9800000000005, "end": 5187.66, "text": " future with or without other guests, a few of the exciting questions just for us to include in this", "tokens": [50844, 2027, 365, 420, 1553, 661, 9804, 11, 257, 1326, 295, 264, 4670, 1651, 445, 337, 505, 281, 4090, 294, 341, 51128], "temperature": 0.0, "avg_logprob": -0.16635017735617502, "compression_ratio": 1.6076388888888888, "no_speech_prob": 0.03845371678471565}, {"id": 990, "seek": 517238, "start": 5187.66, "end": 5191.26, "text": " transcript. And then thank you both, Elliott and Stephen, for joining. So just a few of the last", "tokens": [51128, 24444, 13, 400, 550, 1309, 291, 1293, 11, 46170, 293, 13391, 11, 337, 5549, 13, 407, 445, 257, 1326, 295, 264, 1036, 51308], "temperature": 0.0, "avg_logprob": -0.16635017735617502, "compression_ratio": 1.6076388888888888, "no_speech_prob": 0.03845371678471565}, {"id": 991, "seek": 517238, "start": 5191.26, "end": 5195.900000000001, "text": " questions that were asked, Juan asked, how do small transformers, Zhang et al. 2020,", "tokens": [51308, 1651, 300, 645, 2351, 11, 17064, 2351, 11, 577, 360, 1359, 4088, 433, 11, 17729, 1030, 419, 13, 4808, 11, 51540], "temperature": 0.0, "avg_logprob": -0.16635017735617502, "compression_ratio": 1.6076388888888888, "no_speech_prob": 0.03845371678471565}, {"id": 992, "seek": 517238, "start": 5195.900000000001, "end": 5201.26, "text": " compared with children learning language? 96 asked, what are your thoughts on implicit", "tokens": [51540, 5347, 365, 2227, 2539, 2856, 30, 24124, 2351, 11, 437, 366, 428, 4598, 322, 26947, 51808], "temperature": 0.0, "avg_logprob": -0.16635017735617502, "compression_ratio": 1.6076388888888888, "no_speech_prob": 0.03845371678471565}, {"id": 993, "seek": 520126, "start": 5201.26, "end": 5207.42, "text": " priors versus animal instinct? Rojda asked, what constraints that space in LLMs, don't", "tokens": [50364, 1790, 830, 5717, 5496, 16556, 30, 3101, 73, 2675, 2351, 11, 437, 18491, 300, 1901, 294, 441, 43, 26386, 11, 500, 380, 50672], "temperature": 0.0, "avg_logprob": -0.11161454785771731, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.004535910673439503}, {"id": 994, "seek": 520126, "start": 5207.42, "end": 5212.3, "text": " they get there by training? So are they discovering it? That's not what they implement at the start", "tokens": [50672, 436, 483, 456, 538, 3097, 30, 407, 366, 436, 24773, 309, 30, 663, 311, 406, 437, 436, 4445, 412, 264, 722, 50916], "temperature": 0.0, "avg_logprob": -0.11161454785771731, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.004535910673439503}, {"id": 995, "seek": 520126, "start": 5212.3, "end": 5219.18, "text": " maybe. And there's many more questions. So I hope that we can all review and reread each other's", "tokens": [50916, 1310, 13, 400, 456, 311, 867, 544, 1651, 13, 407, 286, 1454, 300, 321, 393, 439, 3131, 293, 46453, 345, 1184, 661, 311, 51260], "temperature": 0.0, "avg_logprob": -0.11161454785771731, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.004535910673439503}, {"id": 996, "seek": 520126, "start": 5219.18, "end": 5225.5, "text": " works and come together for 41.2 in some future time. Thank you, Elliott and Stephen,", "tokens": [51260, 1985, 293, 808, 1214, 337, 18173, 13, 17, 294, 512, 2027, 565, 13, 1044, 291, 11, 46170, 293, 13391, 11, 51576], "temperature": 0.0, "avg_logprob": -0.11161454785771731, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.004535910673439503}, {"id": 997, "seek": 520126, "start": 5225.5, "end": 5229.9800000000005, "text": " for this excellent stream. Thank you, Dave. Thank you both. Yeah. Thank you so much.", "tokens": [51576, 337, 341, 7103, 4309, 13, 1044, 291, 11, 11017, 13, 1044, 291, 1293, 13, 865, 13, 1044, 291, 370, 709, 13, 51800], "temperature": 0.0, "avg_logprob": -0.11161454785771731, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.004535910673439503}, {"id": 998, "seek": 522998, "start": 5229.98, "end": 5236.86, "text": " Very well. Bye. See you.", "tokens": [50404, 4372, 731, 13, 4621, 13, 3008, 291, 13, 50708], "temperature": 0.0, "avg_logprob": -0.5310137055136941, "compression_ratio": 0.75, "no_speech_prob": 0.04855011776089668}], "language": "en"}