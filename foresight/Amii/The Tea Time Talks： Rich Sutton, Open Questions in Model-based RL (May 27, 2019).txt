Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just
have a welcome you to tea.
We do this each year and it's very informal and I welcome you all to sign up and participate
and to ask questions and you know we're here just to talk about science and ideas and exchange
points of view.
So think of it as a community, enjoy the tea, have a cookie, it's all good.
The other thing is Thursday, Thursday I think we still have an open slot and we could have
Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that
will work out, I'm pretty sure.
Okay, so the topics are really intelligence and how do we understand how the mind might
work and could work and today my topic, really the topic is model-based reinforcement learning.
Model-based reinforcement learning is really it's sort of the whole enchilada.
We try to get all the pieces present, model-based, model-free, make understanding of the world,
be able to reason or plan with it and how many people here have just raised your hand
if you've heard of the phrase model-based reinforcement learning.
How many haven't, how many actually, let me say it a little differently.
How many of you just think it's the model-based reinforcement like the most important thing
in AI, like I do, or should I get a little bit of an argument from authority for it?
Okay, I spent that long talking about it, I might as well give my, I just threw in a
slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon
talking about predictive learning, understanding the world, Yashio Benjo talks about the most
important step in AI is to make predictive causal model of the world.
This is a thing that's coming around at last and this is our topic, our topic is model-based
reinforcement learning and it means making a model of the world, I can use this word
model to mean model of the world, how the world works, I'll only use the word model
as a world model, model of the MDP, the Markov decision process, the transition dynamics of
the world and model-based reinforcement learning, we learn the model, we learn the model and
we also then use the model to plan or reason about what to do.
So this is, we call this planning and planning proceeds by, you imagine states, you look
ahead from the individual states to see what might happen and you back up to improve your
policy or your value with those states that you're imagining might happen and then this
is how you figure out, decide what to do.
Okay so it's decided this idea has been around for a while, I'm going to talk briefly about
the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first
proposed or first explicitly proposed that planning and learning would be radically similar.
So there's some reinforcement learning algorithm that's interacting with the world and then
it's also learning the world model which is another box that you can plug in place of
the world and instead interact with the model for a while, in that sense, and so you're
interacting with the model and you're saying what if I do this, the model says that would
happen, you get this reward and you learn from that just as if it had really happened
and so then this way the planning or the logic and the reasoning, certainly the planning
is radically similar to just acting in the world, just acting in imagination.
So that's the first half of the idea, the second half of the idea is that these things
are all done simultaneously, so planning and learning and executing, they're all been going
on all the time, so you're always interacting with the world, getting new experience, you
can do a model for your reinforcement learning with that bit, but you also use that experience
to make the model and the model through planning and that also affects your value function
and your policy and makes you do better.
Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm
that's kind of useful to complement those diagrams, those visual things with a little
bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from
the book, so basically your Q is your action value function and model SA is your prediction
about what would happen if you were in state S and did action A and you initialize that
and then as you go through life, you're just doing this loop over and over again, you're
looking at the state you're in, you're picking an action in it and set B somehow, then you
take that action, send it to the world, the world gives you back a result, R, the reward
and the next state S prime and so then you do model free learning with that little four
tuple of experience, state action, reward, next state, so here we're doing Q learning
in step D to update the action values and then we update the model because we've seen
what will happen for that state and that action, that this will happen and this is where we're
assuming a deterministic world because really many things might happen if you did that action
in this state and we only saw one thing and now we're going to just overwrite the model
so that I'll predict always that that would happen.
So the thing about DynA, particularly the first instance of it in 1990, it's a very
simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully
express all the possibilities.
We're going to talk about today's, how you can go and consider more possibilities.
Anyway, after you do, now we've got as far as learning the model and then step F is where
you do the plan.
So by the plan, you just imagine some state, maybe randomly from something you've seen
before, imagine an action, you ask the model what would happen if you were there and then
you do that same step, same, it's the same as D, it's Q-learning, but here now we're
doing Q-learning on these imagined things and there we're doing Q-learning on the real thing.
So that's the basic idea of DynA, I'm sure many of you have seen that before.
Now, maybe you've seen the demos of it before, so here's the standard demo where we have
a grid world, we have a start state down in that corner and we have a goal state where
you get a plus one reward and the actions are to move up, down, right and left, so four
actions from each state, the states are all represented tabularly, so like this is state
34 and this is state 92 and the model says, oh, when I go action four in state 34, I end
up in state 92, that's what the model is learning and because it has the model, it is very quickly
learning how to get from start to goal efficiently and it's actually able to learn about states
that it's not in and this becomes most apparent if we pick up the goal and move it to a different
place, so of course once we've moved it, the agent will go back to, it'll go here again
where it used to be and be disappointed so to speak and then it has no idea where it
is, it's built a model but the model says that there's no goal up there, as the last
time it was there, there was no goal, okay, but eventually it will stumble upon the goal
again and then it will be very quickly able to plot a path, so watch when it first finds
it now, here it will eventually see it's learning the right way to go and it's learning a good
path very quickly because it has a model, okay, so that's like tabular dyna, okay, and
today we want to talk about the extensions of this, open questions in it, yeah I guess
I didn't even say it, it's open questions and planning, I'm not going to tell you the answers,
I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic
dynamics, what you saw was just deterministic dynamics, we assume that the world always
went the same way but you could instead of overwriting what the model says in the state
in action pair, you could start to collect a list of all the things that might happen
in their probabilities and then you could sample that and you could do exactly the same thing,
you could add function approximation, now function approximation, I'm going to talk about it,
but it's really a spectrum, a range of degrees of function approximation, so what we saw
was tabular, I call it tabular and that means every state action pair is treated totally
different from every other state action pair, there's no similarity between them, there's
no generalization and so there's just a big table and I store things in that state action
pair and really in real life certainly in computer go and in Atari games and in any
robotics application you have to generalize from one state to another and that's, you
know, you never see the same state twice, okay, but we start with the tabular and you
think you're used to your deep learning, that'd be a nonlinear system, you could also have
linear things that turn out to be quite important and even the aggregate case, state aggregate
means you still have a table but there could be many states fall into the same table entry,
okay, so you're aggregating states and treating them all the same, this is a nice case actually,
we can get theoretical results for it that we can't get for the other cases, okay, so
there's function approximation, we want to do that in some sense that's our bread and
butter, we just generalize the table to a function approximator like supervised learning
system, but let's go on, I want to extend it quite far, so let's list the things and
the next big thing is partial observability because really the world doesn't even give
us states, it gives us observations, it gives us things that happened, things that are senses,
it doesn't tell us, we don't know the full state of the world, we just get an observation and now,
we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest,
the rest of it is basically the kind of thing we've talked about so far, we have the world,
we have our policy and our value function and we're interacting with the world,
we're getting rewards and we're getting some observations and then that red box is turning
into a state and so once we get past the red box, it's just like before, we had a state and we can
make the, send that state up to the model to be learned and we can send that state up to the
planner to propose things and the planner will do some adjustments to the policy and value
function just like the reward does, but it will come from the planner and this will be the common
path between model free learning and model based learning. Okay, so the thing in the red box,
this is the state update function which just says that the agent has to take responsibility for
learning some mapping from the observation, the last state and its action to what it's going to use
as its state, it stays as a summary of the past, it's good for making decisions and predicting
the future and so the state update function is called U, it's exactly this thing and it's
got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U
box is given and I'm going to mostly assume. Anyway, when you talk about changing the state
feature vector or the state representation, that will be the state update function. Okay,
that's a major extension and at the same time it's almost done because I've got some kind of a
box and so I've got some kind of a box, produces some kind of a state representation and my methods
always, at least once I did the second step, a function approximation, they always were able
to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect
it is, I will be able to do a certain well with it just as I would be able to do certain well
with a certain feature vector representing the state. Okay, another big step is that if we do it
right, it doesn't we can separate it from all the other issues, just like we have here, which is to
do temple abstraction. Really if you take your model of the world, your model of the world is not
if I'm in this state, I do this action one step later, I'll be in this other state, it's really
more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a
sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big
multi-step events and we're actually the kind of learning and kind of reasoning and planning we
want to include, should include all those sorts of things. So there is a theory of options which
enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases.
Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in
a little bit. So rushing along, I'm talking about open questions in model based reinforcement,
so I have to say a little bit what's closed, what I'm not going to consider open. So these are my
settings, these are my presumptions, and I say closed-ish because like lots of people will
disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning
should be online, incremental, like asynchronous dynamic programming and like the dynasy system
you've just seen. I think that models and planning, they should be state to state. So
many people in the literature make models and do planning where they include the observations in
the plan. You're like, if I did this then I would see that and then I would, no, no, it should just
be state to state. And if you think about it just a little bit longer, really it's obvious you've
got to be state to state. You don't want to have your observations which are tied to the single
time step and tied to state update. You want all those to be separated. Okay, now of course it's
not really state to state, the state feature to state feature, state feature vector to state
feature vector, and that will be where the feature vectors are coming from the learn state
update function that we mentioned earlier. Okay, closed models, planning, they should be
temporarily abstract, there should not be one step, they should be used based on options. Also,
search control. Search control is how you decide which states to think about in imagination, and
that's essential for your plan to be efficient. If you think about stupid states, you'll just learn
stupid things, but if you can just select the key states to think about to form your plan, then you
can be efficient and effective in your planning. And the last thing is that, so these are sort of
like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it
directly. And similarly, we need some problems to in order to structure the learning of the
options and the option models. Okay, let's go on to the open questions. The open questions. Number
one, should the model, what is this model, should it generate sample states, which I suggested,
or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go
through it in detail. But how should planning be done with average war? This is the other big thing
that I hope to cover today, average war. And then all the other things I won't, I won't probably won't
get to. But let's look at, so let's let's go to how we put function approximation in here. And
what is the content of the model? So just a little bit of terminology. Of course, planning proceeds
by using the model to look ahead, imagining something that might happen. Each one of these
imaginings of the future from a state action pair is called a projection. I'm going to use this word
projection. This is where we imagine a future. Okay, and then after one or more projections,
we compute something. And then we back it up. That's called a backup. And this goes on forever.
Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm
looking at these state action pairs and imagining might happen. So what would be the projections
in this picture? Uh, Schmach, where's the projections in this picture?
At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then
everyone can can just do it. Where are the projections? The projections are where you're
imagining the future from a state action pair. This is my test to see if you're actually following
my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state.
This is a state action pair. Because you can tell because it has an action on it and it comes from
a state. So it's a state action pair. And then you imagine the future, the projections are here.
There are three projections. We're looking ahead, all the actions I might make, and I project what
would happen. And I figure out how good they would be. I take the max and I back it up. Okay,
so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick.
Well, it's, it's from, from the state action pair to where it goes. This, this part is the
projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here?
Say that again.
You should have, you should be sure by now. So there are the projections. So this is,
this is a long skinny sequence. This is a skinny backup. So we're probably sampling,
instead of doing all possible actions, we're sampling an action, we're sampling a next state,
we're sampling an action after that, and we're sampling an x state after that. But these two
are the, are the projection parts. The other parts are parts that the agent is doing. The agent
says, suppose I do this action, and then ask the model, what would happen, the projection? Okay.
And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves,
always goes from the leaves to the top.
No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part,
anyway, is definitely your choice. It's not an imagination about what the world might do.
Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one,
the projections are here. Now these two states, they might be the same state. Maybe I imagined
this one, and I said, huh, now what, what if I was there and I did this one? So that,
so that if they were the same state, you might imagine doing the same thing. But in fact,
by definition, as a backup, they are separate backups, and you'd get these two, two together.
And if you did this one first, and then you did that one, then it might be a similar effect,
because you would change the value, estimated value of this state, and then you change, use
that to change the estimate of that one, that one. Okay. Okay. I have to keep going.
Good. So,
this is the biggest question. What is the output of a projection?
Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide,
what is it really? Because we're using a function approximation, and our states are probably real
valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in
this, I'm imagining being in the state, doing an action, but the world is stochastic, many things
could happen. So one thing I could do is I could represent the whole distribution of all the things
that could happen. Okay. This isn't totally crazy. People are doing this. This system called
Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are,
are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated,
they're, they're going to, we want methods that are general and scalable and proximal. And so that
we, can we do this without committing to a very specific form for the function approximator?
I am skeptical that we can do this. Okay. This is the first question, the first open question.
I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution.
But if, if you did this, then how could you roll it out? How could you iterate it? How could you
go to another step? Because you'd go from a state action period to a distribution. Here's a messy
distribution thing. And then how could you go from there? How could you roll on to the next
projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course,
you can always sample that distribution. And then you have a sample model. So you get the state action
pair and you get a sample of the next state. And then you can roll it out. You say, okay,
there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there?
And you can go on. But you really have many of the same problems because you have to learn
the distribution because you have to, you have to generate a sample from that distribution and
you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility.
But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from
which you sample. And you notice that now planning has become stochastic, because there, you would
have to do many samples like in Monte Carlo tree search of that next state in order to average
over them and get an expected expectation. Whereas up here, it was deterministic. I get you
the whole deterministic distribution. Okay. And then there's the third case, which I like the best,
which is that you learn the output of a projection is an expectation, an expected feature vector.
Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out
because you get this exp, you know, this average of feature vectors for the next state. And it's
straightforward to learn this expectation models, because that's what all of our Algorb zoo, they
learn expectations. And, but in general, we've lost information. If you only have the expected
next feature vector, instead of the whole distribution, you lose special things. But,
but there's this important fact, mathematical fact that in the special case of linear value
functions, you actually don't lose anything. So I, I guess I don't have time to do this equation.
So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically,
we can show that if you do, you're doing the update with the distribution model, and you can
write up mathematically, this is what it is. And then just through a few steps, you can prove that
you get exactly the same thing if you, if you use an expectation model. So here, this is, this is
the probability distribution of the next states. Here we have the expected next feature vector for
the state, and the, the action or option O. And you can show that these are equal in the special
case, where the value functions are linear. Okay. So this is open questions, open questions. So
this is just a proposed strategy, is that with linear value functions and expectation models.
And so, you know, I just want to talk a little bit about this question, should the value function
be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question
of moving the work around, whatever you do, you have to learn the nonlinear relationship.
And the strategy of, of an expectation model is that the nonlinear work is done in the state
update function. So it puts the burden on the state update function. And so here I want to talk
about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this
picture from the book of the shape of all the backups. Now, these are the shapes of the backup
really. This side is planning, that side was seen as not planning. You know, just TD and Monte
Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning
could could could involve a short, not just the wide backups of dynamic programming and and tree
search or exhaustive search. But you can do the skinny backups. And so my my long short start is
that I'm going to those are the those are the projections is that I'm going to argue that
really everything can be done with the smallest, the smallest backup, just looking ahead from
sample one action and sample one expectation outcome. And that's that's that's I think is a cool
way to do planning. And you can do that without losing anything. Because if you want to assemble a
bunch of these tiny backups in the right order, or in just over and over again, you can learn a
long plan. Okay. So I have one more slide, just going to briefly talk about the average award
setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we
use function approximation, we have to go to the average word setting, we have to give up discounting.
And I just want to make the observation in front of you all that that this planning with
average award, it's still a totally open question. I thought it was easy. But I was thinking about
the other day with Zach. And it's really an open open question. It's even open for the tabular
case. You just take one step dine and try to make an average reward version. That would be a paper
because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for
your to do this summer to get your master's done just in time for September, that would be a good
one. If you don't have one already. Okay. And there's also questions whether the model should
should give us the the the expected reward or the expected difference between the reward and the
average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not
gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these
are these are the questions that we started with the open questions. Should the model generate
sample states or expectations? And if it's going to give us expected states, should the value function
be linear? We've seen how those fit together nice. And the question is a further question is can state
update support that can it learn a good enough state features so that the value function can be
linear without losing something important? And then there are other questions about whether the
model whether this suggests the model should be linear as well, or whether it can be semi linear,
which means like a squashing function applied to a linear function. We talked about how planning
should you know, once we combine average reward with planning, unsolved problem, we should work
on that. We should also worry about how should planning affect the actual actions. And what
sub problems should direct the construction of the option models. And I can't I shouldn't try to
explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is
that we want maybe we want to expect the states, maybe one of the value functions be linear,
maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition,
that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so
I'm done. Thank you for your attention.
Okay, we do have a little bit of time for questions. I ended abruptly there, but
that's the story, open questions and model based reinforcement. Please.
Okay, so that's probably should have been one of my my closed questions, because we definitely need
off policy learning in order to learn the models, in order to be do it efficiently. And so the part
of the premise is that we're doing off policy learning. And we have, we have a suite of a few
methods that will work on that nowadays. Yeah, off policy learning is essential.
So I would assume that you would want to learn lots of value functions and not just one.
If you want all of them to be linear in your representation, then that's a lot of burden
on your representation. Yes. So if all the complexity is in the state representation,
then what is the model really giving you? Well, it's giving you the dynamics, which is it's not
in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state
update function. And but more importantly, I'm just realizing that I forgot to thank my new
collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this,
and they should be up here. And, and we have we submitted a paper on expectation models to
to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out.
Yeah. So so a lot of work is going into state update.
That's that's the strategy. I think I think it's might be appropriate. Good.
I have a couple of observations. I don't know, even whether I'm on the
page to you, but I am interested in applied intelligence. One observation is Wall Street,
they seem to have a numerical model of the world. You know, so I mean, that's one world or one model
that you can look at. It seems to me they're far more numerical than other types of domains.
The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's
not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know,
so it's each duck in the clock can radiate any number of different directions. So I'm not sure
whether your model could cover the eagle catching the ducks or you want to give that to us.
Thank you. That's good. So this linearity thing,
it's a very important that it's linear in the in the features. Okay, and and this is the this is
the trick. It's sort of already known, it's well known that anything can be linear if you arrange
the right features. So you could do the duck, you could you could presumably capture the higher
order, the interactions between the, so what do you lose when you get linearity? You know,
let's say you have two features a and b. Well, if the right choice or the right value
depends upon both of them being present at the same time. And then then then then you can't do
that linearly. The linear function, you can only say, oh, this has a certain effect, this has a
certain effect. And there can't be a special thing that if they're on together. Okay. And so what
you do with that none, and when there's an interaction between variables, you know, maybe it's
it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or
that's the kind of thing that you can't do. But but we've known since the beginning of neural
networks, that that if you then add a third variable for the conjunction of the two original
features, then you can learn the nonlinear function in the original in the first two. Okay, and so
the same same is true. So it's not a principle limitation in any way. So think again about
a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function,
many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way
of finding the features in the last layer, then you could be linear. So in some sense, what we're
just saying, take the that thing that you worked on in the last layer and make that your state.
And so as your state, then you would have to your models would have to produce it.
You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating?
I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear
map, then that nonlinear map would have to figure out that that a that these two variables are,
are, need to be treated especially, and they would have to create the conjunction term inside
that that nonlinear mapping. So it's like we're doing the same work, we're pushing it
into a different place. We're pushing it into the state update. Yeah, other questions?
Okay, thank you very much.
