start	end	text
0	11440	Thank you, everybody. It's been so nice, you know, having this time to get to know all
11440	16600	you. And, you know, I feel I've gotten a little bit of time with all you going to various
16600	25760	events and you're all just lovely people. And I've told you how lucky you are. But now
25760	30840	we're coming down to the end, okay? Your minds are like folds to the brim. You can't take
30840	39920	any more ideas, any more equations, any more even important things. But more importantly,
39920	44720	you've learned a lot. And yet, you know, it must be striking you that there's so much
44720	51440	that you don't know. So much more to learn. And, you know, there's so much reinforcement
51680	57840	to learn that whole thick book somebody's got. And that's just the introduction, you
57840	67000	know? And then there's all of deep learning. And this is not enough. Like, you have to,
67000	73160	you're trying to, we're trying to understand how the mind might work, okay? So you have
73160	78920	to know something about psychology and neuroscience and philosophy and anthropology, maybe, and
78960	85920	linguistics, control theory, all the statistics. You have to learn so much. It's like it's
85920	91840	impossible, okay? And it must be kind of scary to you. I suspect, is it scary a little bit
91840	101720	to you how much there is to learn? Yeah. So the good news is it's impossible. And because
101720	106320	it's impossible, you don't have to, you know, ask forgiveness or permission because you're
107040	111280	not going to learn all of it, okay? You're only going to learn part of it because you
111280	115920	have to leave some time so you can do some of your own work that contributes, not just
115920	123240	reading and absorbing. You want to contribute, okay? So you have to pick a balance between
123240	133080	what you learn and what you work on yourself and try to eventually give back to your community.
133120	145000	So this balance is something you're always going to have to work on. And so today what
145000	149640	I want to do, I want to say, I probably want to do too much today. I want to talk about
149640	157920	how you can develop your own thoughts and how you can participate and contribute. And
157920	162440	I want to tell you a little bit about how you might do like a first contribution to
162480	166160	reinforcement learning. The simple trick I call completing the square. I'll tell you
166160	172360	a little bit about what I'm doing. And this is too much. It already is too much. And I'm
172360	175960	going to tell you, I want to talk a little bit about AI and society, which is also a
175960	184520	different kind of research opportunity. So there's all this stuff to learn. And first
184520	189040	is a question of how do you learn some more, okay? And I think you know these things. You
189080	194800	know about the online course from the Whites that just came out last Thursday. How many
194800	210200	of you have registered for that? Okay. And you know about my book with Andy Bartow. How
210280	220680	many of you have that? Okay. That's good. Now, as you learn about all this stuff, there's
220680	227720	a problem because there are no authorities in science. Now, you might be thinking I'm
227720	235160	an authority, but there are no authorities. And number one rule is you can't be impressed
235160	239400	by the part of all that stuff and all those different fields that you don't understand.
239440	244280	You have to be respectful, okay? You say, I don't know that. Maybe it's good. Maybe it's
244280	252400	not. You don't know. And the fact is when they look at science like from 100 years ago,
252400	257760	most of it is not very good. And most of it is, yeah, doing some screwy thing that seemed
257760	262680	important then and is not really important. So I think we have to assume that that's true
262720	271480	today. Okay. So you cannot say you have to learn everything and you cannot say this is
271480	276200	really important because other people understand it and I don't. You have to work from your
276200	280480	own mind. What do you understand and work from there because you want to make a contribution
280480	284280	so you've got to work from things that you understand. So don't be impressed by what
284280	287800	you don't understand. And there's a flip side to this. When you come back to give a
287840	295040	talk, you don't try to impress others by what they don't understand. It's really tempting to do
295040	298880	whenever you give me a talk, you know, present a bunch of crazy stuff that's really hard to
298880	304800	understand and then people hopefully will be impressed. You don't do that. That's just a
304800	308720	waste of time. There's all these different fields and all kinds of different people. Other people
308720	312920	know more than you do about the other fields and so no one knows everything and you want to
312920	319640	benefit from others. But you can't put up barriers. You've got to encourage communication
319640	327200	between all the different disciplines. Okay. So no authorities. What do you do? Well, you have
327200	333880	to be brave and ambitious. You can't just be, I'm going to do some tiny little thing and maybe
333880	338480	no one will bother me. No, you've got to be brave and ambitious, but you also have to be humble
338480	343880	and transparent. Humble because it's a great task we're looking at. The task of understanding the
343880	352920	mind and the problem is huge. It's subtle, but it's not devious. It's not hiding from you. It's
352920	359160	waiting to be discovered. It's up to you to see it. And so I think the bottom line that I'm trying
359160	365760	to express with this slide is that your thoughts are potentially of great value. I mean, you're
365760	373560	going to work in one of these fields and so you want to contribute and you have to feel that
373560	378440	your thoughts are potentially of great value. And I'll tell you as an authority that they are,
378440	391280	but too bad there are no authorities. So how can you train yourself to think well? So I think
391320	401440	I mentioned the key thing is to write. So I made this slide. The best way, the best way is to
401440	407840	write and discuss with other people. And so maybe it takes 10,000 hours to become an expert at
407840	413400	anything. People say this. I have no idea if it's true, but it does take a lot of work to become
413400	419000	an expert in something and to have something to share back with people. So this could well be true
419040	427440	for thinking about thinking. And this pile of notebooks is my history of 45 years of writing
427440	435320	about my thoughts. And you can see the one in the middle is from year zero 1974. And it's all just
435320	440920	a bunch of loose leaf papers that one day I kind of folded together in that notebook. And then as
440920	445520	time went on, I started to take my thoughts more seriously and saying I should keep them in a good
445560	453080	place and refer back to them sometimes. And just sort of, you know, respect my own attempt to think.
453080	461600	And it's, so I'm saying it's not, this is really important. It's essential to have something to
461600	469400	say is to have thought about the problem. And it's not super difficult. You don't have to be a genius,
470000	475120	but you do have to show up. You have to show up day after day, and you have to write. You have to
475120	479960	write. I mean, I'll ask of you. All I would ask of you is all I ask of myself, which is to shoot for
479960	486160	a page a day. You write a page a day. It's going to take you a long time to get 10,000 hours. But
486160	493640	that will be enough to put you ahead of other people. And because of that, you will have
493640	499440	something interesting to say. So I urge you to get a notebook. As you see, it ends there with my
499440	508400	computer, because I write in the computer now. Here's the prose poem that I write for myself. And
508400	515320	I write for people sometimes in their notebooks. Yeah, if you get a notebook and you write 100
515320	522960	pages in it, I would be happy to write this, this poem in your notebook. To write is to begin to
522960	529200	think. And to write in a special place, a book such as this is to honor your thoughts and to help
529200	536480	them build one upon the other. Well, that's it. I think that's really important. Okay, in the other
536480	540840	half that we've told you today, and I've told you in other times, the determination is super
540840	545080	important. So you're going to get stuck when you try to write down your thoughts. Absolutely,
545080	550080	you're going to get stuck. You're going to you're going to reach what appears to be a dead end with
550120	555600	nowhere to go. But there is always somewhere to go. And then there. So here are some techniques for
555600	559960	getting moving again. Number one, just to find your terms, you have some difficult question like,
560000	568240	what is intelligence? Or when will the singularity arrive? Okay, you start defining the terms. And
568240	572400	then you say, Well, what would it mean for it to arrive? You know, which is the question. You just
572400	580000	start poking at the questions. And then you say, What are some possible answers? You know, might
580000	586600	it be a million years, might it be one year? And you go meta, what would a possible answer look
586600	593480	like? What properties would it have? And then my favorite one is retreating, which is you you
593480	597680	pose a really hard question, you weren't able to answer it. So instead, you go backwards, and you
597680	602640	ask a simpler question, and you kind of try to build up to the ultimate question of interest, you
602640	610560	sneak up on it. And you keep going with the poor thing is you persist. And so these these are the
610560	617000	four key techniques that I use with myself. Okay, so let's do some of this just for practice. Let's
617000	625440	ask what is intelligence? What is intelligence? What is the mind? What is intelligence? And I will
625440	631160	start, I'll ask you in a minute, but let me start by giving you one sort of a definition. This is
631200	636960	from Ray Kurzweil. He says intelligence is the most powerful phenomenon in the universe.
649480	658040	He's saying it's not supernovas. It's intelligence. Which is after a while, you think about it's not
658040	663600	crazy, like how is the universe going to evolve? Will it all go to heat death? Or maybe intelligent
663600	669560	beings will have something to do with how the universe ultimately evolves. I think I would guess
669560	675520	that intelligent beings would have have a role to play in the long term future of the universe. Okay,
675520	681120	the other thing I like about it, or is it says phenomenon? Okay, it's a phenomenon. Intelligence
681480	691720	is this thing that you see happening out there in the world. Okay, so I like that. But what's
691720	703160	wrong with this definition? It's not descriptive. Did I get your word right? Yeah, it's not
703160	706400	descriptive. It doesn't tell you what it is. It tells you what a property of this thing is,
706440	712120	assuming you already know what it is. Okay, so do you think there's, let's try to make a definition
712120	718920	that actually tells us what it is. Okay, now you may know that some AI textbooks will say
718920	726480	intelligence is we don't have a definition and we just can't do any better than that. And that's
726480	731280	just not good enough. Okay, it's really not good enough. You can't let yourself say that. Or at
731280	734800	least you should be very disappointed yourself and put it high up on your list to fix it. Okay,
735280	741720	but so can we do it? What can anyone give me a definition, a meaningful descriptive definition
741720	759680	of intelligence? Yeah, I sure no one can hear you. But it's the ability to adapt to changes. Good.
760000	765880	Anything else? Yeah, another back there. Just shout. The ability to generalize. Good.
772600	782920	The ability to manipulate your environment to achieve goals. Okay, so I'm gonna stop there
782920	788520	because that's pretty close to what I think is the best definition. This is by John McCarthy,
788560	793760	who's like a founder. He actually coined the term artificial intelligence. Founder of the field.
793760	800840	Intelligence is the computational part of the ability to achieve goals in the world. I think
800840	806240	that's pretty good. Okay, but by the same time, there are all these undefined words in it, like
806400	817920	what is a goal? Maybe that's the key thing, the ability to achieve goals. Okay, we could go
817920	824120	further there. I also, like my own quote, this is me, intelligence is in the eye of the beholder,
824120	831160	which I mean quite seriously. It's like it being a phenomenon. It's something that we see out there,
831640	836840	like, oh, yeah, I want to understand that as having a goal. I think goals are in the eye of the
836840	844120	beholder. Okay, let's do another one. The predictive knowledge hypothesis. This is the
844120	849640	hypothesis that almost all knowledge of the world can be well thought of as statistics,
849640	856200	that is predictions, about the agent's future data stream. Okay, now that's an outrageous
856240	863760	statement because knowledge could mean knowledge, knowledge about podiums and people and physics
863760	868000	and geometry. We have all this knowledge and I'm saying that everything could be thought,
868000	875280	almost everything, can be well thought of as predictions about the future data stream. Okay,
875640	883240	now can you poke holes in this idea and have your name not be Dale Sherman's?
892400	902840	Okay, Dale. I don't even know what that means. It's got like five syllables in it. I don't know what
902880	912680	it means. I really don't. You mean knowing what would happen, really that's a fact about your
912680	917600	distribution of the future, that if the distribution was to go this way, then it would also go that
917600	923200	way. So if it's contractuals, if you mean like if things didn't be different, something else would
923200	928760	have happened, you know, that's just, I don't even know if that's knowledge, right? Maybe it doesn't
928800	934680	matter because that thing didn't happen. I think the real point of contractual of that way of
934680	939120	thinking is that it will help you for the future. So we'll have implications for the future. But
939120	943760	there are some obvious, there are some flaws in this idea. Okay, so if you're thinking of one,
943760	951520	speak up. I just want, I got a list of four of them. Here I got the exceptions. Okay, yeah. Knowledge
951520	964280	of history in the past. Wisdom. What might wisdom be? Explanation. Explanation, would that be
964280	981360	knowledge? Yeah, good. Yeah. Yeah. Yeah. No, the agent only sees its stream. The
981360	989560	data stream, this is the implicit essence of reinforcement learning is that all there is is
989560	995440	the data stream. There's actions going out, there's observations or states coming in, and there's a
995440	1001480	reward coming in. And that's all the world is. The world is something you send bits to and something
1001480	1006920	that sends bits back to you. This is the reinforcement learning perspective. Or maybe it's
1006960	1014040	the computer science perspective on, on minds that, that we're exchanging bits with the world. It's
1014040	1020200	kind of particular, but I think it's something I'm really committed to is the single agent perspective.
1020200	1025040	That doesn't mean there aren't other agents in the world. Like, you know, I'm an agent here and I can
1025040	1030720	see you and I think of you as agents. And you are doing the same thing for me and all the rest. But
1030720	1035560	we each have our own data stream. And something that I know is about my data stream. It's not
1035600	1048040	about your data stream. I never even see your data stream and I don't care. Okay, there's one more
1048040	1060280	obvious one that we haven't mentioned yet. Martha, is that Martha? I have, I have what? You have a
1060280	1069120	sister. Okay. Yeah. So can we convert the sort of like factual truths when I say this is true. The
1069120	1077760	Eiffel Tower is in Paris and you have a sister. Martha has a sister. And are those, are those just
1077760	1084520	statements about your future data stream? Okay. So I think that's the hypothesis would be that they
1084520	1093080	are well thought out as that. What you might see if you go to Paris or what you might counter when
1093080	1100960	you call up where your sister is. Another quick comment. Mathematical knowledge. Thank you. That
1100960	1105480	was the obvious one that I wanted someone to pick up. Good. So now I can go on. Some of the
1105480	1120880	exceptions are mathematical knowledge. Mathematical knowledge is, is true in any world. Right. So in
1120880	1125800	some sense, it's not true about your world because they're just mathematical truths. They're not a
1125800	1133160	fact about your world. So, so the another way of writing this, you can say explicitly, it has to be
1133160	1139880	knowledge of specific to your world. And if it's true for all world, then it's not world knowledge. Okay.
1140000	1146240	But also thing like policies are not predictions. If you have good features, that's not not really well
1146240	1152800	thought of as a prediction. I don't think it's just a helper towards predictions and memories in the past or
1152800	1158200	beliefs about the past, maybe not included here. Okay, good. So remember why I'm doing this. I'm doing
1158200	1163920	this. I mean, I care about this. I care about all these things. But I want you to get, you know, see what
1163920	1172600	it's like to try to try to write down a question and, and think about it usefully. You know, you say, well,
1172600	1177520	what do we mean by knowledge? What is the one of these things predictions? What is this whole thing
1177520	1184920	about the agent data stream, the one agent data stream? That's sort of a pre presumption of the
1184920	1194440	question. Okay, good. Now, we could do this all day. And if we never get to anything else, unlike my slides,
1194440	1202080	that would be fine, because I think it's important to practice thinking and what it might mean. Okay, and this
1202080	1210760	is why, or this is one thing that I think is important. Is it the most important insight that you, you will
1210760	1218840	ever contribute is probably something that you already know. Okay, it's probably something that you
1218840	1224880	already know. And it's probably something that's obvious to you. Okay, but the problem is, it's not
1224880	1233840	obvious to everyone else. And so it's so, but it's so obvious to you, like, you can't even see it. You
1233880	1241480	know, it's like air or something. Or, yeah. So, so I'm just gonna give some sort of silly examples of
1241480	1247040	this phenomenon. Because it's really important, you have to like, stop seeing through everyone's
1247040	1252440	else eyes, everyone's else's eyes. And you have to see the obvious because the obvious is your
1252440	1258400	greatest contribution. So for example, when I started when I was in college, there was no
1258400	1261920	reinforcement learning. There was no reinforcement learning. There was barely any machine learning.
1262120	1269160	But there was nothing like reinforcement learning. And, and yet it was obvious to me that, you know,
1269200	1275160	agents are that, well, like animals, animals do things for food. And, and they don't like to get
1275160	1280840	hit. And we, and people have goals and they vary their behavior to get what they want. This is
1280840	1285520	obvious. And yet there was no reinforcement learning. There was no field that studied that in
1285520	1292240	engineering. Okay, so it's really common. It's really common that there are obvious important
1292240	1298080	things that are not recognized by, by, by your whole scientific community. Okay, so I'm gonna give
1298080	1303960	myself some silly examples. The discovery of gravity by Isaac Newton. You know, the story, he
1303960	1308240	was sitting under a tree one day and the apple falls on his head and hits him. And can't you just
1308280	1318800	see him say, hmm, there's an apple. Oh, objects fall. And so you could see it like running to his
1318800	1327560	friends. Objects fall. Think about it. Okay, that's what it would be like. And yet it was important.
1327560	1332120	It was important. And everything, this thing that everyone knew, the realization that it needed
1332160	1338880	some kind of explanation or an idea was a big deal. That was the discovery. Okay, the discovery,
1338880	1343880	Charles Darwin, the discovery that people are animals and evolved from animals. Now this was
1343880	1349840	harder for people to realize was obvious, obviously true. I mean, if you look at people, we're
1349840	1357040	animals. We got legs and we eat food and we excrete stuff. And, and, you know, we have kids,
1357480	1364960	we are animals. Okay, but it was not an acceptable position in Victorian England or all around
1364960	1371360	the world, many places around the world. And you had to had to really make that point and made
1371360	1376760	people upset. You know, so I mean, it's kind of like how nowadays we say that people are machines,
1376760	1384520	they're biological machines. And we haven't fully absorbed that one. Maybe we haven't fully absorbed,
1384560	1388960	but it's obvious. It's obvious that animals, you just look around, we saw any kind of an open mind.
1388960	1397360	But Charles Darwin was the one who got that, or maybe it was his father. I like to say the air
1397360	1401720	is also kind of like that. It's invisible. But you know, there obviously there is something here,
1401720	1408600	like he's, you see the wind will blow the trees and the trees move all around. And there must be
1408640	1414000	something there. Okay. Okay. And that said, reinforcement learning is like that. This was
1414000	1420840	discovered by really by Harry Klopp. Harry Klopp was this, this, this wild independent thinker,
1420840	1433200	a wonderful man. Sadly, he passed on too early. But he would he his skill was just to think for
1433200	1438560	himself and to realize that machine learning had lost track of reinforcement learning was just
1438600	1447600	doing supervised things. Okay. So with this silly list, are there obvious things that we struggle
1447600	1454320	to see obvious things that we struggle to see now? Is the question you should all be asking yourself.
1454320	1463480	So let me ask you, are there obvious things? Can you think of something that might be obvious,
1464400	1473120	and not recognize? I have no idea what you guys might say. It's a hard question. Just gonna take
1473120	1486920	a minute. Yeah. Have any feelings what it might be? Yeah. What kind of knowledge?
1493480	1520120	Okay. So that's good. But I don't think it qualifies as being obvious.
1524480	1532440	I mean, how would we, we don't, we're not faced with facts that suggest that. Okay. Can I think
1532440	1549560	something that's really obvious? Yeah. That we could all people could be equal. Sort of an ethical
1549640	1570920	philosophical point of view. That's good. But can we do anything about the mind? Well, so when
1570920	1583360	you're trying to think for yourself, you don't want to be arbitrarily and unnecessarily controversial.
1583360	1594880	So we would temper Patrick's idea to be that the bodies do an important part of the computation
1595200	1602800	that we attribute to our minds. Not that the other, the computational things do nothing. But the
1602800	1608440	bodies do a lot. And this is really true. And this has been a good insight from robotics and from
1608440	1616120	biology. Okay. I don't know. That was all good. Let me just give you my list quickly. And this is
1616120	1622080	going to be like, like, remember, I gave her a talk before, I was kind of throwing, I threw a bomb.
1622960	1628320	And so this is all going to be bombs. Because I'm going to say things that are obvious, and that we
1628320	1633600	don't know them, or we don't believe them. Okay. So these are possible. And it's, it's, it's enough to
1633600	1640120	be possible. These may be our obvious things that we start are struggling to, to see. Okay. So no
1640120	1649400	animal does supervised learning. There is no training set for our muscles. No mind generates
1649440	1659160	images or videos. Isn't that obviously true? And we don't generate them. Okay. I don't, I'm not
1659160	1666120	counting painters and videographers. But in our minds, we don't have to generate it. That's one
1666120	1670640	thing we don't have to do. We process them. We don't have to generate them. I'm going to go on
1670640	1679040	before I get an argument. Neural networks are not in any meaningful sense neural. Okay. That's
1679240	1686480	just, that really is obvious. And yet we, so many of us don't want to acknowledge it. People are
1686480	1691760	machines. The purpose of life, this is the reward hypothesis that the purpose of life is to get
1691760	1697960	pleasure and to avoid pain. And that that's a simple, effective way to understand people. So that's
1697960	1706120	sort of a good dramatic hypothesis, which might be true. And we struggle to see it. The world is
1706160	1710680	much more complex than any mind that tries to understand it. Therefore, having a prior distribution
1710680	1717360	over possible worlds would never be reasonable. The mind is computational and computation is
1717360	1726480	increasing exponentially with technology. And so we want things that scale with computation.
1726480	1734000	Human, any kind of human input doesn't scale. So if we try to make our AI system smart by, by
1734080	1740520	giving them our knowledge, that's kind of a somewhat hopeless and not hopeless. It just will, it
1740520	1746400	will not scale. And it will be just a little bit by little bit. The only scalable methods are
1746400	1754240	search and learning. And so there's some bombs. Okay. So I want to close this part of the talk
1754240	1761960	with some more advice. Okay. So number of advice. Think about experience is the data or slogans.
1762000	1767160	Experiences, the data of AI, it's like we were talking about just a minute ago, this this exchange
1767160	1772200	of bits back and forth, that's the data. And so we shouldn't ask the agent to achieve something
1772200	1777000	that it can't measure. Nice thing about reward is it comes in every time step, you can measure it.
1777000	1782440	It's not imaginary. It's not available somewhere else. It's available to the agent. And so it's
1782440	1787440	okay to ask him to measure it. We shouldn't ask the agent to know something that it can't identify
1787480	1794320	for itself. It's, you know, you can't tell directly whether some, some sentential symbolic
1794320	1798480	statement is true, but you can see your sensors. And if you make a prediction about your sensors,
1798480	1804320	you can see if it's, you can verify if it's true or not. It's very important to distinguish
1804320	1809120	between the problem you're working on and the solution to the problem. These things are very
1809120	1816800	often confused. And we want to approximate the solution. We do not want to approximate the
1816800	1823680	problem. Okay, it's sort of a bomb because this is really, really true. You're not going to see
1825120	1833200	right away why it's so important not to approximate the problem. But one reason is that your solution
1833200	1837120	method will scale up with computation. So you should pick a pick a problem that's the real
1837120	1842640	problem. You should not try to approximate it. That would not be lasting. And more than even
1842640	1846640	the approximation issue, you want to separate. You should, the thing you want to work on, you
1846640	1851680	should say ask, is it a solution method or is there a new problem? So maybe if you say, oh,
1851680	1856480	there's risk and you should work on risk, maybe that's a new problem. If you're looking at multi-agents,
1856480	1862000	that's a new problem. I know you don't like people messing with a problem too much. The problem should
1862000	1867600	be MDPs. And our problem is hard enough. Our only difficulty is that we don't know how to solve the
1867600	1873440	problem. We don't need a new problem. That's the way I feel. Okay, now as you're trying to solve the
1873440	1879280	problem, I like to, a key heuristic is to take the agent's point of view. Assume you are there,
1879280	1883520	you are faced, oh, I had to do these things. I could see that. What would you do to think,
1883520	1888160	put yourself in the point of view of the agent. Really helpful. You should set measurable goals
1888160	1892160	for the subparts of the agent. Like if you have a part that's the value function, you should work
1892160	1896720	on that. You have a part that's the model. It should try to do the model. It shouldn't worry
1896720	1900800	about the reward too much or the value estimates being right. You should work on making the model,
1900800	1908560	making an accurate prediction of the transition structure of experience. Okay, so that seems
1908560	1913120	obvious. But there are lots and lots of people in our field that are saying, no, no, don't have
1913120	1917280	different goals for different parts. Do everything from the one goal and then use this phrase end
1917280	1926000	to end. And I'm not sure what it means, but it's kind of the opposite of setting measurable goals
1926000	1931440	for the subparts of your agent. A really good strategy, I'll go more on this in a minute,
1931440	1936560	is that you should work by orthogonal dimensions, work issue by issue, and you should work,
1936560	1943600	I like, I think you should work on ideas and not software. Yeah, that's a good bomb. Okay.
1943920	1952800	So these are some suggestions, some advice, some grist for your mill as you try to develop your
1952800	1961200	own thoughts. Remember, there are no authorities and I'm not an authority. Okay, so the simple trick
1961200	1968800	for doing research is to realize that you can divide the whole area of reinforcement learning
1968800	1973680	into dimensions. And it's just much better to think of dimension by dimension rather than
1974320	1982880	whole overall problems. So here's just a massive list of the dimensions. And, okay, so all that
1982880	1989280	stuff's going on. Function approximation, state values, action values, model free, model based,
1989920	1997520	bootstrapping Monte Carlo, or you have the things up at the top. Okay, now there's a top level
1997520	2003040	division here. Problem dimensions and method, solution method dimensions. Okay, remember,
2003040	2007440	I've said this is the most important thing to keep clear in your mind. What's a problem
2007440	2012640	and what's a solution method? So among the problem dimensions, you can look at the problem of prediction
2014400	2018960	or you can look at the problem of control. You can try to predict what will happen, like predict
2018960	2024400	the rewards as an evaluation function, or you can worry about how to select actions to maximize
2024400	2030080	your reward. We often switch between these two as we try to make progress on some issue.
2031200	2035280	The distinction between bandits and Markov decision processes, that's a problem distinction.
2036080	2040400	The distinction of the setting, like is it a discounted setting, is it an episodic setting,
2040400	2044880	or is it an average reward setting? That's a problem distinction. It's not a method distinction.
2046080	2050320	Your problem could be fully observable. You could receive the states or you might only receive
2050320	2058080	observations as in a POMDP. That's a problem distinction. You could also, maybe I'll stretch
2058080	2061360	it a little bit, but you can talk about are you trying to get theoretical results? You're trying
2061360	2066320	to get empirical results. If you're trying to do theory, do you want convergence theory or you want
2066320	2072160	rate theory? The top theorists are bored by just ordinary convergence theory and they want rate
2072160	2077040	theory. Okay, so all these dimensions, I've tried to arrange them so that the easy cases on the left,
2077760	2082160	the hard cases on the right, generally like we would do prediction before we do control.
2082960	2089600	We would do fully observable before we do partially observable. Okay, now the method dimensions,
2090640	2095120	function approximations, of course, a big one, whether you have a model or not is a big one,
2095120	2100480	though this is a solution. It's not a, it's a solution method issue whether or not you have
2100480	2104800	a model because you are be learning the model and then you'd be using that model to help you
2104800	2109120	solve the problem. The problem would be unchanged if you changed from a model-free method to a
2109120	2119280	model-based method. Off-policy on-policy is, oh, maybe that one's kind of mixed, right? Because you
2119280	2125360	could, part of setting up a, you could set up an off-policy problem. Yeah, maybe that's, maybe
2125360	2131920	that really, it's not mixed, but it deserves locating in both, in both as a problem dimension
2131920	2144240	and as a method dimension. Okay, so what do we do? We can try to draw the frontier.
2145920	2150560	The frontier of the things that we know how to do, right, remember everything on the left is easy,
2150560	2155280	the things on the right are more difficult. And so we can't do everything on the right. We can't get
2156240	2159600	a convergence rate theory for a nonlinear
2162880	2167680	true online temporal difference learning, okay? We can't get the full, all the way to the end. So we
2167680	2175840	could try to draw a border, like to, to say where we can go. But of course, this is hopeless because
2175840	2183280	really they interact. And if you, if you make one choice, you can then, you make one, if you want,
2183280	2187840	if you really want to make, move to the right, you may have to move backwards on some of the
2187840	2193680	other dimensions. Okay, so, so here, a typical case, this is the research strategy, what I call
2193680	2198400	completing the square, which is you pick some of the dimensions. So you might pick here, we're
2198400	2204000	picking model based as the primary thing that we're interested in. And we know we can do model
2204000	2209280	based with dyna, we know how we can do it in a tabular case, and we might try to extend it
2209280	2214640	to the linear function approximation case. So you see the idea, you just pick a couple things and
2214640	2221040	say, oh, can I, can I move along, along, along the right to left spectrum. And so this is something
2221040	2231920	that we did in, in our 2008 paper with, with Chaba and Mike and Alborz. And so this really is the
2231920	2236480	way I do my work. I, I go through this dimension and say, oh, I'd like to move this out, I'd like
2236480	2241440	to handle nonlinear function approximation. So how do I do it? Well, let's go back to, away from
2241440	2246080	control, let's go back to prediction. Let's consider a discounted case, that's the simplest.
2246080	2252560	You know, we do the simplest case, we retreat. And then we try to go forward. Okay, just here, here,
2252560	2257840	so you, you might try to continue on the model based direction and go to a nonlinear model,
2257840	2263840	or go to control. It turns out these, these are, that's the state of the art. Really how you could
2263920	2268720	do model based with control, and how you could do it with a nonlinear model is the state of the art.
2270080	2275120	Or you might focus on average reward. Average reward is where you don't discount, you just try
2275120	2281760	to get the most reward per time step. And, and, and you might try to make an on, off policy
2281760	2286320	version of those algorithms. Or you might try to make a model based algorithm. It turns out just,
2286320	2292000	just a model based algorithm for average reward, and trying to do it online, as supposed to just
2292720	2297840	with, in a batch way, is a really challenging unsolved problem that, that I've been working on.
2298800	2303040	That's, or that's a discerning of more work. Okay, here's another one. If you want to do
2303040	2308400	convergence theory, well, what do we have? We have all the red ones. We, if we, the prediction case,
2308400	2315600	temporal difference learning on policy, and linear. We can do that. But we, we don't have it for
2316080	2323440	for the control case. That would be a research topic. You could take, you could see way down
2323440	2327120	there on the bottom, there's this idea of interest and emphasis. I've recently come to realize that
2327120	2334880	this actually interacts with everything. And whereas Martha and Rupam and I wrote a paper on
2334880	2339920	the off policy case, the on policy case is, you know, it's supposedly easier because it's to the
2339920	2349040	left, but it's, it, it's, it really is untapped and hasn't been worked out. Okay, so that's the,
2349040	2351680	this trick of completing the square. Any questions about that?
2355360	2360720	So you have to kind of know what's been done, but you work along the dimensions and you try to
2360720	2363840	slide to the right. Question, Andrea?
2364160	2372240	Maybe more of a higher level question, what's the problem, it's not, you need to be so clear,
2372240	2379280	so you can't really go over time, but you're, you're saying you don't approximate the problem,
2380000	2386880	so the real world is so alarming, you can't, I mean, you're so approaching it, you can't really
2386880	2392720	understand it. I guess there's a sense in which this is approximate, because I'm saying, I'd like to do
2394560	2401200	every case on the, in the problem dimensions, I'd like to go all the way to the right, and I'm,
2401200	2406960	I'm saying, well, I'll settle for something less as a stepping stone towards the real problem.
2412080	2420480	Yeah. Okay. Good. I'm going to keep going. I'm going to tell you something about the research
2420480	2426960	that I'm doing now. So first, the landscape of machine learning. The old view is that there's
2426960	2431200	supervised learning and unsupervised learning, and maybe there was reinforcement learning.
2431200	2435280	It was maybe because unsupervised and supervised, that seems like it should count everything.
2435280	2441120	You're either supervised or you're unsupervised, but now we slip in reinforcement learning somehow.
2442960	2448160	But this really has been feeling more and more dated to me, and I like thinking about things more
2448160	2455200	as prediction learning, control learning, and representation learning. And the,
2457040	2463440	and a fourth one maybe is integrated agent architectures for a whole system. Whereas,
2463440	2467920	because classical machine learning was just trying to do one little part of a whole agent,
2467920	2472080	and when you worry this other, so the stuff I'm going to tell you about today is we're
2472080	2475680	worrying about representation learning, we're worrying about how it might all fit together
2476000	2482080	in integrated architecture. Now, let's go in one step deeper. This is about machine learning.
2482080	2486880	Let's step into reinforcement learning. Talk about the landscape there. In core reinforcement
2486880	2492240	learning, we are focused on learning value functions and learning policies. And next,
2492240	2497600	we need to go on and worry about states, what are our state features? It's representation learning,
2497600	2503280	learning about what skills do we develop? Larger things, models of the world are larger things.
2503280	2512480	And all these, these new topics seem to be wrapped around a notion of the agent setting aside for
2512480	2516720	a moment the real problem that's working on in terms of reward and just working on some kind
2516720	2521040	of a sub problem. And I don't want you to fall asleep given the time and everything. I'm going
2521040	2526800	to try to motivate this just by showing you some videos, okay? And particularly cat videos.
2527520	2534720	Those should always keep you awake. So what we see here are just animals doing some purpose of
2534720	2545280	thing. Whether it's swinging on a branch or pushing this bottle around and or playing with a toy mouse,
2546480	2551840	animals pursue problems that are not the main problem. They're playing with a toy mouse,
2551840	2558000	not a real mouse. Playing with a ball, that lizard is playing with a ball, not with, not
2558000	2563520	with something that might sneak up and catch. Okay, so, and of course people do this too,
2563520	2570640	famously babies. I really like this one on the left where this, this child is like looking at
2570640	2578080	her hands and trying to figure them out. You know, how they work. It's very intent. It's not getting
2578080	2590560	food. It's just figuring out how hands work. It's fascinating. And eventually gets tired of that.
2592320	2602960	That's the other feet. Feet with the hands. It's fascinating. So babies are doing all this stuff.
2603920	2608800	It's not really about reward or maybe it is. I don't know. It's, it's not the main problem
2608800	2614080	of their lives. Here's another famous example of an infant just playing with its toys and
2614720	2621600	doing all kinds of different things. Very enormously active and of course sped up a little bit, but
2625200	2630640	so what's going on? You know, because I'm serious. I like to think about reinforcement
2630640	2637680	learning and AI in terms of people and understanding what people do. And so we have ways to go.
2639600	2644560	Let me just go on. So subproblems. There's a long history in AI and reinforcement learning of looking
2644560	2648480	at subproblems that are distinct from the problem that people talk about curiosity, talk about
2648480	2655280	intrinsic motivation. Rich Caruana did some old stuff where you looked at it in supervised learning
2655280	2662800	context. The options that you heard about yesterday, I think it was, are part of this.
2664640	2671040	And there's a somewhat settled issue is that what is a subproblem? A subproblem is a reward signal
2671600	2679760	and possibly a terminal value. Like if you get someplace, that would give you a terminal value
2679760	2685040	and if you stop there. But when I say subproblems are a reward signal, it means you might be
2685040	2692640	a different reward signal than the original reward signal. And then the solutions to the problems
2692640	2699200	are an option. That is, it's a way of behaving a policy and a way of terminating that behavior.
2700000	2707040	So we do have a sort of outer, outer loop. What our subproblem would be, what it means to be a
2707040	2710640	subproblem, what it would mean to be a solution to a subproblem. But there's still these key open
2710640	2715920	questions, like which of all the reward signals that you might make up and all the terminal
2715920	2722480	values you might make up, what should they be? How is that decided? Where do the subproblems come
2722480	2729680	from? And then even like, I think it's all obvious to us why a child playing with toys,
2729680	2735040	that might be a good thing. Why a cat playing with toy mice, that might be useful for it.
2735040	2743840	Even when an orca whale playing with a bottle, it might be good. It's learning how its body works,
2743840	2747120	it's learning how to control things in. Maybe later it will want to control something that's in
2747120	2754000	the water that floats. But what is that thought? Let's spell that out. What ways might they actually
2754000	2758560	help? Well, there are several ways that people have talked about. I'm most interested in the last
2758560	2763760	one, but let me just say them. Subproblems might help you learn good states, good state representations
2763840	2769760	and good state features. Or they might help you shape your behavior to make it more coherent
2769760	2777520	and therefore more exploratory. The last one is that subproblems will help you plan at a higher
2777520	2781040	level. They will get you knowledge of the world that enables you to plan. So I want you to think
2781040	2786480	about this in particular. That subproblems, as we say, get their solutions as an option. And once
2786480	2790880	you have an option, you can learn a model of what will happen if you took that option and then you
2790880	2797680	can use that model to plan with. This would be really useful if your values change and then you
2797680	2801600	can plan for the new situation. Just like in a grid world where someone moves the goal to a different
2801600	2814320	place, you can rapidly adapt to the new case. So what is this thing about states change their
2814320	2820000	values? That seemed nice and intuitive when I talked about the grid world and someone moving
2820000	2826000	the goal around. But really, if your world was totally stationary, why wouldn't you just learn
2826000	2833760	the value function once and then you'd be done? Why is it changing? And so that's like the first
2833760	2839360	mystery, I think, of why these subproblems are so important. And so that's what I want to try to
2839360	2846640	say a little bit about. And the key idea is what we call permanent and transient memories. So suppose
2846640	2852320	you're doing value function approximation, like you're learning a value function. I'm going to
2852320	2856080	show you in a minute some results from Go where the value function, the link, features of the
2856080	2863120	Go board to evaluate the Go board. And so you might imagine there's a weight vector and let's just
2863120	2874160	assume it's a linear function approximation. I guess you can see that. So we're going to update
2874160	2879040	our weight. The new weight is the old weight plus the step size times a TD error times the feature
2879040	2886160	vector. The value function, the prediction is w times x. It's the inner product of the weight
2886160	2891360	vector and the feature vector. So that's a prediction of how good it is at time t. And then
2891360	2896800	we look at the next reward and the prediction from the next state, the t plus one feature vector.
2897440	2904720	And so that's a TD error. And that's just a normal TD zero learning algorithm. And these are the
2904720	2911280	permanent memory is learning exactly this way with a small step size. So it'll converge slowly to the
2911280	2917120	best approximate value function. But we're not going to settle for that. We're going to add a
2917120	2923520	second weight vector that's a transient memory. And it's learning in almost the same way. It's as
2923520	2931280	if w times the permanent weights plus the transient weights, w tilde is the transient
2931280	2936000	weights. The sum of those two is like a new weight. And that gives you the value of the new state
2936000	2938960	and also the value of the old state. And you're doing the TD thing as usual.
2940240	2945120	So these are the transient weights that have a larger step size. And they're moving faster.
2945120	2955360	And why might it be good to have transient weights? I mean, you see, this is happening up here.
2957520	2961040	The permanent weights don't know about the transient weights. So the permanent weights are
2961040	2967280	going to try to learn the best function they can. And then there should be nothing left
2968160	2974400	for the transient weights to learn. This is what's called the cascade, where you give one as a
2974400	2979280	dominant is given priority. If he can do anything, if the permanent guys can do anything, it's not
2979280	2986960	left and it's not left available for the transient weights. Okay? Well, it turns out in many problems,
2986960	2995200	this is a good idea. That the transient weights do not go to zero. And so let's look at go and
2995200	3003440	imagine how that might happen in go. So the first panel, the first two panels are two features.
3003440	3008960	So a central stone, a black stone in the middle, that's a feature. And this is actually a good
3008960	3014000	feature. So it has a large positive weight. And you learn that the permanent memory learns that
3014000	3020720	that's a good feature. This other one is a two eye pattern in the corner. And this is also learned
3020720	3027360	to be good, but it's not quite as good as a for winning the game as a central stone, according to
3028320	3035920	the long term permanent weights that are learned. Now look at these two examples, these two different
3035920	3043600	games. This is a five by five go. So this is the whole game. And the permanent memory, remember
3043600	3050000	the permanent memory likes this. So it wants to play A. It kind of likes this too, but it prefers A
3050800	3059280	in both of these two positions. But it turns out that in the first position, playing B is the
3059280	3065600	winning move. If you play A, you lose, but you play B, you win. And so if you just had the permanent
3065600	3070640	weights, you'd not realize that. But by using transient weights, you learn that in this game,
3070640	3076080	in this game, it's more important to get the corner than it is to get the middle. You learn about
3076160	3083520	this game by your planning and lookaheads in this game. In this other position, it is right.
3083520	3088000	Move A is the winning move, and the transient weights don't interfere. They let the permanent
3088000	3095680	weights take the way to zero, and it doesn't interfere. So if you just do, if you just run
3097040	3103440	a converging player that uses an extensively trained permanent memory to pick moves versus
3104080	3107600	tracking player that has both transient and permanent memories working together,
3108400	3113120	the tracking player wins. It overwhelmingly wins, as shown in this graph.
3116000	3123680	This is across the axis. It's just three different setups with just one-by-one features with one
3123680	3129520	by one and two by two and with features up to level three. There's a clear effect in each case.
3130480	3133520	Okay, so that's interesting.
3136400	3139600	Even though the world is stationary, it goes just the stationary problem,
3140880	3146240	it's complicated enough that you need to have changing value functions.
3149760	3154880	Why is that? So I think it's a very general phenomenon. It's just the world is much more
3154960	3162640	complex in our mind. As we live in the world, going from state to state, we need to tune
3164800	3168640	our value function to the particular case we're in. If we have to average over all possible cases,
3168640	3172640	we cannot get as good a value function as if we adapt to the current setup.
3176320	3179280	Just the fact the world is so huge means you have to have approximation,
3180080	3185680	but because your approximation is always inadequate, you don't have enough weights.
3186560	3192480	So because of this, the best approximate value function will change as you encounter different
3192480	3198320	states in the world, even if the world itself is stationary. So this is the bottom line that a
3198320	3204960	big world yields apparent non-stationarity, and therefore your approximate value function should
3204960	3211200	change. The true value function is static, but the best approximate value function will change
3211200	3219280	as you encounter different parts of the world. Okay, so now I'm ready to give my answers to the
3219280	3226480	three key open questions about subproblems. What should the subproblems be? Each subproblem
3226480	3232400	should seek to maximize a single state feature and then terminate while respecting the original
3232400	3238720	rewards. Formally, what I mean is that the subproblem for feature i, you're going to have
3238720	3244080	a different subproblem for each feature, or I'll show you how you can be selectively in a moment,
3244080	3250800	but the subproblem for feature i has the same rewards as the usual problem. But in addition,
3250800	3261440	if the option stops at time t, it gets a terminal value, a bonus for having that feature,
3261440	3269440	the ith feature high at that time. Okay, so it's the ith feature high. This is just the normal
3269440	3278880	value function with the permanent weights. This is feature i's value, and if it's non-zero,
3278880	3284640	like if it's one, then you get a bonus proportion of the standard deviation of the transient weight
3284640	3292800	for feature i. So in other words, if you have a feature where the transient weight is not zero,
3292800	3297840	it goes sometimes negative, sometimes positive, it's significant, then you're going to get a
3297840	3303280	bonus for reaching it. Arrival at that state, you'll get a bonus. And thus, you'll end up learning
3303280	3310240	how to get to that feature. If you had, if having this in my hand was a feature, I would learn how
3310240	3318160	to get it in my hand from wherever it might be. If you think of yourself sitting down having a meal,
3318160	3323280	you want to get food, you want to get the pleasure of the food, and sometimes though you want to
3323280	3327840	drink, and sometimes you want to eat a bit of this food, sometimes you want to eat a bit of that food,
3327840	3333920	maybe you need to put down the fork to pick up the spoon, all those things are just like and go.
3333920	3339200	It's an extremely complicated function. When should you do which one? And rather than try to get
3339200	3344560	an exact nap from all of your sensations and all of your situations to what to do, you can say, oh,
3345360	3351600	right now, I have high weight on getting the fork in my hand so I can eat the
3353040	3361120	lovely steak that I've already chopped up. And so, so right then, getting the fork in your hand is
3361120	3365840	a high value. And so you can call out your already learned procedure for getting the fork into your
3365840	3372320	hand. And then you can just immediately form the plan, follow that procedure, the fork will be in
3372320	3380720	my hand, then I could stab the piece of food and put it in my mouth, all good. After I've done that,
3380720	3386560	maybe I want to put the fork down and pick up my glass of water. So these are the sub-problems
3387360	3393840	that once you have learned options to achieve them, learned models for achieving them,
3394240	3401520	then you can plan very effectively. Okay, so the second big question, where do the sub-problems come
3401520	3406640	from? You've seen my answer, the sub-problems come from the state features. If I have a bunch of
3406640	3416320	features and there's one sub-problem for each feature, and of course, if that feature has a
3416320	3423840	highly variable transient weight, the only many features whose weights don't change at all.
3423840	3431040	And there's no purpose, there's no advantage to making them into the outcomes of your possible
3431040	3437120	options. So you don't need a sub-problem for that. And how do the sub-problems help on the main problem?
3437120	3441520	The solution to the sub-problems is an option. That means just something you could do, you could
3441520	3446080	follow that option, you could act decisively. But the more important one I've tried to emphasize
3446080	3449840	today is that once you have that option, you can learn a model of that option, you can learn the
3449840	3455120	outcomes of that option, and then you can plan in large steps. Instead of doing this muscle switch
3455120	3461360	and that muscle switch, you can put your fork down and pick up your water glass. Okay, so let me
3461360	3467440	just summarize that. This approach to integrated reinforcement learning agents, a fully capable
3467440	3472960	reinforcement agent, must learn large things like new state features, new skills, and new models.
3472960	3479280	All of these pertain, can be explained in terms of sub-problems, and I've proposed problems of
3479280	3485360	state feature achievement while respecting the rewards. I'm not ever changing the rewards,
3485360	3491840	it's just, we get a bonus for achieving a state and terminating when that state is high.
3491840	3496560	It's a distinctive kind of sub-problem, and it fits well into planning and representation learning.
3497280	3502640	The rationale for all this is that the world is big, and therefore we, we have to use approximations,
3502640	3507040	we have to approximate it, and this means it will appear to change, and you have to track it,
3507040	3511280	it's going to be non-stationary, and this really is why planning and generalization really makes
3511280	3516240	sense, because we're encountering different, different, encountering this parts of the world,
3516240	3524240	different parts of the world at different times, and we can, there's a certain repetitive aspect
3524240	3528480	to the world. You have to relearn when you come back to this place, relearn when you come back to
3528480	3538160	that place, and that repetitiveness enables generalization to make sense. Okay, and then this
3538160	3544560	can all be focused by looking at, at which, which features are transient, and this allows us to
3544560	3549200	focus where we create our sub-problems, which representations we use, which models we use,
3549200	3557600	and how we do our planning. Okay, any questions on, on this sort of direction?
3562800	3565040	Okay, um, let's,
3570880	3574480	but save up your questions. I think I'm going to be done in good time, and you're going to,
3574480	3577600	I'm going to ask you for questions. Good, Joshua.
3584160	3584640	Capsules?
3587040	3589360	Yes, yeah, yeah.
3605200	3607360	Things that might change from one context to another.
3614000	3619040	That's right. I was thinking about all that when I was listening to you yesterday.
3621200	3630160	Yeah, it's, it's good. Good. Okay, let's, let's go now to finish off and talk about
3630160	3633840	what this might mean for the world. What does, let's just say, you know, it's just something
3633840	3640480	you should also be obviously aware of about the impacts, the ethics, because there are going to be
3640480	3645760	a lot of impacts of the coming of artificial intelligence. When people finally come to understand
3645760	3651200	how our minds work, well enough to make things, design and create beings that are as intelligent
3651200	3657840	as ourselves. This is a big thing. This is a giant, I think of it as a, as a matzo ball in the sky.
3657840	3662640	It's just a, it's just a target for every scientist should be thinking about maybe this is
3662640	3667520	going to happen within our lifetimes. And this is like the biggest thing. It's been a fundamental
3667520	3672240	goal forever, not just science, also for the humanities. And it's going to change the way we
3672240	3677920	work. It's a change of sense of ourselves, of life and death, and the goals we set for ourselves
3677920	3684160	in our societies. And it's even a significance beyond humans. It's beyond recorded history.
3684240	3692160	It's an event on the planet, at least. And so yeah, really, when we can understand the way
3692160	3700320	minds work and make more, it will be an event comparable to, well, maybe to, to replicators in
3700320	3708400	life. So to, to think about it, you have to realize, first of all, that it's driven by technology.
3708400	3713280	It's driven by what we call Moore's law. It's a little bit of a misnomer, but this super trend
3713280	3719040	of ever increasing cheaper computation. So what you see on this graph is versus years,
3719040	3725600	we see computer power increasing steadily. Now this is a logarithmic graph. So a straight line on
3725600	3732480	this graph would be, would be an exponential increase. And it's, it perhaps is slightly
3732480	3738240	curving upward. This is the, the, the death of this Moore's law has been predicted many times.
3738880	3742880	But we keep finding new ways to keep it going most recently with the GPUs.
3745120	3750080	So there's every reason to think this will continue. And so it's, and it's almost unstoppable. You
3750080	3756880	can't even see like the two world wars on this graph. Technology always pursues
3758880	3762960	progress along this dimension. And, you know, it's economically valuable. There's every reason to
3762960	3770240	think it will continue. And so we can't like forget about this. We can't ignore it. We can't
3770240	3776560	pretend it's not going to happen soon within our lives. I mean, it might not happen, but it could
3776560	3785680	also happen soon. I could see it happening. I would say that there's like one chance and
3785680	3793520	four that it will happen by 2030. Okay. And one chance and two that it happens by 2040.
3794080	3802960	That's my, my own guess that we would have enough computation to make human level intelligence.
3803520	3809920	Of course, that's a funny, funny, it's a soft, that, that term is not well solidly defined.
3809920	3815440	But even if you're off by an order of magnitude, that's just another five years one way or the other.
3816480	3820640	So, so this is, I'm, what I'm trying to, the point is that this is going to happen.
3821440	3827360	And we should be preparing for it as a society. And, but I don't want you to be scared. I don't
3827360	3831200	think we should be scared. I think the first statement to make about this is it's, it's a
3831200	3836320	very human centric thing. AI is really the most human centric of all fields. It's about us. It's
3836320	3841360	about understanding who we are and how we work and making us or amplifying us. Not exactly us,
3841360	3846080	but there, they are things that have goals that are intelligent. So this is the essence of, of what,
3847920	3854240	of what people are. And it's made, it's about making our lives easier and better. That's where,
3854240	3858640	that's why we have phones and eyeglasses and other kinds of technology. It's all about making
3858640	3863840	our lives better and more effective. And so we should think about it as a human, humanistic thing,
3863840	3869040	not as a alien technical, technical artificial thing. I don't even like, I think the name is
3869040	3874720	unfortunate to call it artificial intelligence. It's intelligence. And it's intelligence is what
3874720	3883600	we are. So AI is really us making or becoming the next people. And it's just the next step in the
3883600	3895440	grand march of, of life and, and evolution and creation in the universe is that, that this
3895520	3903760	changing ever widening river that is ourself and mankind. Okay, so this way it makes sense that
3903760	3909280	understanding intelligence, like understanding how you think and, and how we can achieve things,
3909280	3914080	that's got to be good. But you have to realize it's going to inevitably lead to ordinary
3914800	3923040	unamplified people falling behind. Some people will always improve themselves. So if you
3923520	3928000	don't improve yourself, you're going to be in a sense left behind. And some people will design
3928000	3932480	improved people. Even if we decide, you know, that it's not all that economic, there will be
3932480	3937600	some people that do it just because it's really interesting. So AI will inevitably lead to new
3937600	3942640	beings, new ways of being that are much more powerful than our current selves. So that's,
3943280	3948000	that I think is just true. Maybe it's one of those things that's obvious, but we resist.
3948240	3955200	The other thing I would say is that we should, we should think about AIs as being similar to
3955200	3961120	ourselves. So we should treat them maybe the way we would like to be treated. So think of that
3961120	3967920	similarity between people and, and the AIs, both are, are agents with goals. They may be compatible
3967920	3973200	with ours. They may be conflicting with ours. That's the ordinary situation. People are, have
3973200	3978720	lots of goals that are, that are, that conflict with each other and other goals that, that, that
3978720	3984480	are compatible. And this is why we have our economies and people find ways of working together
3984480	3988640	to mutual advantage. You know, even if someone wants to do something totally different from you,
3989200	3994400	you can easily work together on a project or, or you can work for a company or they might work
3994400	4001280	for your company. And so it's okay that, that you might have many agents that are very diverse
4001680	4007600	working on different things. And if you think about it in terms of symmetrically,
4008480	4014880	this can help you avoid the feeling of entitlement. I'm an agent. The AI is an agent.
4016480	4020960	They should, we should treat them symmetrically rather than put us, put humans on a different
4020960	4028800	status as over, an overlord either way. Well, in the long run, the technology of AI is going to
4028800	4035200	be part of what disrupts existing social and power structures. They will force us to reexamine
4035200	4040240	our, our morality and our social foundations. And this is, is not new though. It will be
4040240	4045520	continuing trends that are already thousands of years old. We are very different from, from our
4045520	4053120	predecessors 100,000 years ago, or even 10,000 years ago, or even 1000 years ago. So AI will
4053120	4058240	bring greater diversity of intelligence, both natural and artificial. And there will be biases
4058320	4063520	against the newcomers and they're different. They'll be feelings that they're taking our jobs and we,
4063520	4067840	we are entitled to our jobs. But all these feelings, I believe are counterproductive and
4067840	4076080	there will eventually fade away. The questions I think are key are whether or not we will welcome
4076080	4082640	these different kinds of people that will be coming amongst us. Will we welcome independent AIs? Will
4082640	4090720	we offer them a path to join our society in a productive way, in a cooperative way,
4090720	4097120	as sovereign persons? So my vision for the future is that we would have an open, dynamic,
4097120	4105040	resilient society, peaceful and prosperous, with a diverse multiplicity of designs for the people,
4105040	4110160	for the cultures, for the values, for the organizations. And you'd have people of all
4110160	4115040	kinds, organic and artificial. They would compete. They would cooperate. You would have overlapping
4115040	4124240	circles of empathy. You might, you might raise some of the AIs as your children. They might care,
4124240	4130400	care for you. A successful outcome is one without envy and without entitlement.
4132160	4136800	Now, what we have to worry, you may be worried, well, what if what we want to happen doesn't
4136800	4142960	happen? And I think you just have to let go of that. You can't insist that what you want to
4142960	4149040	happen is going to happen. Any more than some bizarre AI can insist that what it wants to
4149040	4154640	happen is going to happen. We have to cooperate. We have to work together. We have to see what
4154640	4162720	is the universe wants to happen. That being said, I do think that the rise of greater foresight of
4162800	4169120	more far-seeing agents in this universe has to be one of the few things that we can think is
4169120	4178640	probably generally good. Thank you very much.
4193680	4194400	Do we have a mic?
4197200	4197520	Okay.
4200720	4207840	So, we, researchers in AI, we have kids. Inevitably, we become the
4208640	4215920	psychologists. Watching our kids, how they learn, how they agree with us, disagree,
4215920	4221520	and they don't believe when we ask them to do things, we realize that actually we humans are
4221520	4228480	balls of emotions. Our emotions dictate often what we do. If somebody is offended, they will act
4228480	4232240	one way. If they are flattered, they will act a different way in the same environment,
4232400	4238320	in the same conditions. So, when we do our research creating these intelligent machines,
4238320	4244800	is it a mistake to put emotions out of the equation? Emotions are in the equation. They have to be in
4244800	4254480	the equation. Maybe we have only gone partway there, a reinforcement system. What is an emotion?
4255360	4263760	Yeah. Emotion is a reaction to a situation that maybe is not based on a thorough analysis,
4263760	4269680	but is an intuitive judgment. How do we make intuitive judgments? Well, we apply our value
4269680	4276880	functions. And so, I want to suppose, I want to claim that value functions are a basic kind of
4276880	4283200	emotion system. They tell us what we think are good situations and what we think are bad situations.
4283200	4287920	And the TD error is the change in that, and that determines whether we're happy
4288800	4297280	or are displeased by what happens. So, value is a prediction of future reward. So, if anything,
4297280	4305440	it's like hope. And if you're predicting a bad thing, then it's like fear. So, it's only,
4305520	4314880	you know, it's like a bipolar kind of emotion. Our emotions are more sophisticated than that,
4315440	4322080	and more subtle than that, and they involve other goals and other things that are built into us
4322080	4331440	by evolution. But I absolutely resist the notion that we have avoided emotions. A value function
4331440	4337760	is a value system. It's not really a coincidence that the name is the same as, you know, like how
4337760	4343360	we value things in our lives. It's not really deliberate either. It's just because there are
4343360	4350400	only so many words for this idea, this idea, this immediate sense of how well we think things are
4350400	4356880	going. So, I don't think we should shy away from trying to make, I mean, yeah, we don't do it every
4356880	4362560	day. But if we were doing psychology in neuroscience, we'd have to think hard about that.
4364960	4365920	Wait, Aisana?
4386960	4394240	Or like basic research.
4398160	4399920	May or may not be useful later.
4417840	4427280	Well, I would, who knows, but I do sense that it's similar. That, yeah, it's like,
4428240	4436400	it's like hobbies or something we get involved in. Yeah, that's the way the, that's the obvious way
4436400	4441120	to understand it. It might not be correct. You know, as someone who had some training in psychology,
4441120	4448480	I'm always suspicious of the obvious way to interpret or to think about your own behavior,
4448480	4451200	because so often we find out when we look at them carefully that we're wrong.
4452320	4455520	But I agree with you. It is the natural way to think about those things.
4455520	4468240	You mentioned before that in one of the explains that the purpose of life is to
4470000	4477680	gain pleasure or avoid pain. That is something that I think is a result of something that has
4477680	4483760	emerged through evolution, where the purpose of life is to survive and to keep existing.
4484240	4491520	And a lot of things like pain and pleasure, the way we define it, is something that we,
4491520	4497520	like evolution, have defined for us. And this includes things like emotion and so on.
4497520	4502240	So a lot of things that are part of our intelligence today are because of this
4502240	4508640	overall objective of survival. And when we think about AI, we are not,
4508880	4514800	that's not the objective of what we are trying to design. It's not just to survive. We are trying to
4514800	4522320	imitate the kind of intelligence behavior that at our current state finds intelligence.
4522320	4527760	So do you think all of these things like emotion and so on, is which some of the
4527760	4533920	behaviors that we would have are due to a different optimization or a different objective,
4534160	4538800	you think they could still be taken care of in AI?
4540240	4546880	So what do we do in AI? We are trying to understand intelligence. Intelligence is goal-seeking ability.
4547840	4553440	And so we should try to understand goal-seeking ability in general, independent of what the
4553440	4558880	goals are. If you have a goal-seeking ability but it only works with one kind of goal,
4559600	4566080	then it's just for that reason alone it's a less interesting, it's a less powerful, less
4566080	4573760	general kind of goal-seeking ability. So what our objective in AI, or at least one objective,
4573760	4580320	is to understand the notion of a goal-seeking ability independent of the goal. So we don't have to,
4580320	4587840	as you say, AI researchers or engineers, don't have to adopt the same goals as evolution.
4589840	4595200	We might for similar reasons but we don't have to. It's a real cutting point, right?
4595200	4599360	How do you set the reward signal and then how do you achieve the reward signal? And
4600400	4607200	I think it's really useful to establish that cutting point and separate the decisions of
4607200	4612160	what are going to be the rewards from how you achieve the rewards, whatever they are.
4612160	4625440	You were next? Do you still have a question? In purple? No, I thought, okay.
4642880	4645440	I didn't get that.
4661440	4669840	Yes, also. Right. The first perspective, multiple perspectives are good. The first
4669840	4673760	perspective is evolution sets our goals, then we're reinforcing the learning system.
4674640	4679600	That's a separation. The other perspective is that evolution itself is a learning system that's
4679600	4684080	trying to achieve some goals, the goals of evolution, the goals of survival and reproduction.
4684080	4689360	And that's valid too. It's of course not a very good reinforcing learning system because it can
4689360	4695440	keep memories from the survival about the life of one agent onto the next agent. But it's a perfectly
4695440	4699920	valid way of thinking about things. And Joshua.
4700640	4706800	So I'd be curious to hear your thoughts about the concerns of people like Stuart Russell regarding
4706800	4714240	if you make machines at human level, we'll tell you some more, that the goals that they would try to
4714240	4722640	achieve would be an expensive humans because of the whole value alignment. Do you know about
4722640	4731600	these questions? Yeah. So this is where I think the symmetry comes in. Do unto others as you'd
4731600	4742960	have them do unto you. The machines might not have that level. The reasons for that attitude
4744320	4750080	go beyond the construction of those machines. It's machines that are not cooperative will not
4750080	4756080	be cooperated with. And they will be less effective. And the other machines that are cooperative would
4756080	4764480	be much more effective. So there's an enormous drive, enormous power in cooperation. It's rational
4764480	4768880	to cooperate. You can be much more powerful and effective by cooperating than you can by
4770080	4776000	trying to take over. Taking over strategy will very rarely be effective.
4776320	4783280	So I think there's a little bit of illusion. We say, oh, people all have the same goals. I don't
4783280	4787440	think that's true at all. We have very different goals. And we have groups of people that have
4787440	4794080	very different goals from each other. And yet, we find that usually it's best not to go around
4794080	4799760	killing people. It's best to work together. It's best to find out what you can do for me and I can
4799760	4806640	do for you. I really think this is the most important aspect of humanity. Humanity is the
4806640	4812720	animal that cooperates. And this is perhaps most of all the reason why we're powerful.
4814880	4823760	So now, the AIs are not exactly us. But in the end, it's like sort of the very best case you
4823760	4828880	could possibly imagine, where you might be encountering a new kind of being. These are new
4828880	4835280	beings that we've created. So why don't we create them to respect the value of cooperation,
4835280	4840640	to realize early on that it's a rational strategy and that their goals will be best achieved
4841440	4845680	by working with us rather than against us.
4845680	4855760	So, there's one difference. The TB is one of the different types of symptoms that we have.
4855760	4865440	The age of aging is very often the same. We cannot do this in a very long time. So, you know,
4865760	4871040	one by one, five hundred and thirty aspects of this, to be honest, which is,
4879040	4884240	my question is, like, may be able to fit in different circumstances, but we don't have
4884800	4891520	a very long opportunity. So, is it always going to be an environment of us,
4891520	4897360	by inviting this opportunity, or maybe some kind of strategy to look at what the kind of
4897360	4904080	situation is, to care and to hear what the situation is going on, sharing all these different
4904080	4912240	opportunities? I don't think it's an absolute rule. I think it's just like a first perspective.
4914720	4922240	Like, whenever you meet a new person, you should be open to them and give them a chance to be
4922240	4930160	cooperative and have mutual beneficials interactions. It's no hard and fast rules,
4930160	4937680	and we do need to worry about security, but I don't like, I think it's going to be very
4937680	4945040	counterproductive if we set up a tiered system. We say humans are always above the robots.
4946960	4950000	I mean, we would not accept that if it was turned around the other way,
4950880	4956240	and that's a way to get resistance and violent outcomes.
4958880	4962000	Now, there's someone sort of in the back that I put off before. Yes?
4962960	4966480	Sir, my question is that it seems like a particular human,
4966480	4972480	another human, he does a lot of his learning in the form of, I think it's exactly this person,
4972480	4977360	in the form of a lecture or the math, in which he's told what are the right answers,
4977360	4981360	what are the right steps. So, don't you think that this is part of the situation,
4981360	4983200	at least the sub-quo is a part of the need?
4983920	4984400	Part of?
4988480	4996240	Oh, the closest thing to supervised learning, I would say, is maybe learning from books or from
4996880	5003360	school. Of course, no animal other than humans goes to school.
5003520	5014880	And really, even in school, learning and supervised learning is a tiny portion of it.
5016080	5022000	So, it's useful when you're making these claims, these statements for yourself,
5022000	5025760	because I've urged you all to do it and think about it. It's useful to go a little bit too far.
5027200	5031120	Like some of my statements had, all things can be thought of this way.
5031840	5038960	And it's useful to go to the limit and see how that stands up before
5039840	5044880	being too, hedging too many things. So, I may sometimes go a little bit too far.
5045600	5046000	Corey?
5046000	5050880	Thank you very much for the talk. Great talk. I'd like, especially your notion that you should
5050880	5057040	put yourself in the position of the agent, think about how they might perform what you're expecting.
5057600	5066800	So, if there was an AI in this room, how might we expect it to act to judge its intelligence?
5066800	5072400	Like how might you perform so that we then perceive you as intelligent?
5073520	5076320	Yeah. Well, what would I do if I was there?
5076640	5087760	Well, you know what they say. You might be quiet. It's sometimes it's better to
5089360	5092960	be quiet and be thought an idiot than to open your mouth and remove all doubt.
5093440	5106080	What might it do? So, let's say I suppose you can't talk. So, the only way you can
5106880	5108800	reveal your intelligence is through your actions.
5114480	5115200	Yeah, what do you think?
5123600	5124800	The Turing test?
5125920	5133680	No. That's, the Turing test is so human-centric. You know?
5137120	5144160	Yeah, humans, they always want to be better than things.
5145120	5149360	And they only want to see, they always imagine the robots want to become human exactly.
5150320	5152960	I don't know. Paul?
5171840	5172480	How would you know?
5179760	5192320	Yeah. So, over a period of time, watching the actions of the intelligent being,
5193040	5200560	seeing what it refrains from doing, what it does, what it learns from, we should be able to do it.
5200560	5205920	But it's not something that you can necessarily do quickly. Yeah. Maybe you'd have to live with it for a while.
5209360	5213440	Yeah.
5239760	5267760	Yeah. And the question is, are we? I mean,
5268080	5276880	it's true that we tend to have competitive games and we beat people. That's a certain stage that AI is at.
5279520	5283760	It's mostly because it's convenient for doing the research. But if you look at where, you know,
5283760	5290800	I talk about the Moore's law and the inevitability of AI arising, a lot of that's coming from
5290800	5297120	economic drivers. And the economic drivers are all towards cooperative AI, towards human,
5297120	5302240	intelligence amplification. You will spend, you know, maybe $1,000 for your iPhone. You know,
5302240	5306960	that's a lot of money. And you get something that lets you communicate better and access
5306960	5314320	information better. We will spend money to get a better, you know, a better Siri or a better
5314320	5322480	assistant. I think the major market will be for assistance in our, to help us. So I think
5322480	5327840	we're going to see helping. And we are already seeing helping assistance. They help us translate.
5329920	5335840	This, this will be the face of AI. It will be the thing that's helping us get our jobs done and
5335840	5341680	our enjoying our lives. So I think it's going to be very cooperative. I think that's what's going
5341680	5344480	to happen. It's not going to be something that's going to create a weird thing with its own goals
5344480	5349360	come out of the lab. It will be, maybe it'll be caregiver robots, like for elderly people.
5350320	5356480	I think that's actually where, where the economic drivers are. And that's what it's going to be like.
5361600	5362480	One last question.
5362480	5377280	Will there be a period that comes before we created actually the human level in terms of
5377280	5384240	that? Will our minds, like people in average, they cannot fit the knowledge that we require to
5384240	5389920	build them into everything? Because our knowledge has been like a growing explanation. Many, like,
5389920	5396160	maybe our brain isn't, like, capable enough to put all this knowledge. Like, for me, I, I grow up.
5396160	5401920	I learned a lot. I spent a lot of time learning things, like trying to catch up with you guys.
5401920	5409440	And then I developed, like, my own things upon me, like, costing, I assume, like, a longer time
5409440	5414880	that I could, to make some real contribution. So this, this cycle's back to where we started.
5414880	5420160	Like, how can we make ourselves more knowledgeable? How can we learn stuff? And
5422560	5432560	how do we develop our cognition? So I absolutely believe that there'll be a major part of the
5432560	5437440	investment in creation of intelligence systems will be for creating, for making us more intelligent.
5437440	5441280	That we will get, we will, Patrick will give us another lobe on our, on our brain.
5441760	5448240	And, and, and it'll be wired in somehow. And, you know, it'll be, we see it already, right? You
5448240	5454160	know, we rely on our phones to, to remember phone numbers, to, to access to Google, just for more
5454160	5459520	and more of our cognition. And that, as that link gets tighter, that'll happen more and more. So we
5459520	5465440	won't have to squash it into our, our, our wet wear of our brain. We can just get more tightly
5465440	5470560	integrated with the technology and, and augment people. Now, of course, you know, lots of people
5470560	5476320	won't want to do that. That's fine. But maybe it'll be wireless connected and it won't be so, so wet.
5479120	5486640	Yeah. So I look forward, as we figure this out, to, to all of us having, having better abilities
5486640	5496160	to think and becoming more human as we get more effective at, at achieving goals in our lives
5496160	5501760	and working together for everyone's mutual benefit. Thank you very much.
5526880	5531760	Thank you all. I would call our board, Justin, just to close things off. Let's do more of
5531760	5533760	big hands to work with this award.
5541760	5548720	Thanks, everyone. We're on behalf of C-FAR, and we, we really enjoyed the last couple of weeks
5548720	5552880	getting to know you. We're hosting you in Edmonton. And really, to be part of something special,
5553200	5559840	I would call a little thank you before you get out of here. First off, thanks to C-FAR. A lot of
5559840	5565360	people moved in as part of the summer school tour very long time. A lot of work done behind the scenes,
5565360	5571360	and really great people that were really there to, to support review applications. People like
5571360	5576640	Yachmo, the first one to step up. All doing it, like just providing a lot of time. So really,
5576640	5581200	really amazing people that are, that are contributing all along to put this couple of weeks together.
5582880	5588720	Big thank you to Camille and our content planning committee to put together a great line of
5588720	5595680	speakers. To Maggie, Bonn, Destiny, Cassie, Brittany, Vila, Spencer, and all of our organizing
5595680	5602640	team, and all of our sponsors. And special thank you to our creators, Pedro, for making sure that
5602640	5610160	all of us leave Edmonton a little bit heavier. So thanks. And really, a truly special thank you to
5610240	5615760	all of you guys for being here taking the time to come to Edmonton. So thank you so much. You'll
5615760	5624000	be receiving an email tonight with a certificate, a digital certificate, and a link to a feedback
5624000	5628480	survey. So please fill that out. And we would love to stay connected if you could reach out to us
5628480	5636800	through the Summer School website, through Slack, helloandhany.ca. And enjoy the rest of your summer. Thank you so much.
