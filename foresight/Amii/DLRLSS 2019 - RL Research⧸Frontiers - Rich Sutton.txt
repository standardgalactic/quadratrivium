Thank you, everybody. It's been so nice, you know, having this time to get to know all
you. And, you know, I feel I've gotten a little bit of time with all you going to various
events and you're all just lovely people. And I've told you how lucky you are. But now
we're coming down to the end, okay? Your minds are like folds to the brim. You can't take
any more ideas, any more equations, any more even important things. But more importantly,
you've learned a lot. And yet, you know, it must be striking you that there's so much
that you don't know. So much more to learn. And, you know, there's so much reinforcement
to learn that whole thick book somebody's got. And that's just the introduction, you
know? And then there's all of deep learning. And this is not enough. Like, you have to,
you're trying to, we're trying to understand how the mind might work, okay? So you have
to know something about psychology and neuroscience and philosophy and anthropology, maybe, and
linguistics, control theory, all the statistics. You have to learn so much. It's like it's
impossible, okay? And it must be kind of scary to you. I suspect, is it scary a little bit
to you how much there is to learn? Yeah. So the good news is it's impossible. And because
it's impossible, you don't have to, you know, ask forgiveness or permission because you're
not going to learn all of it, okay? You're only going to learn part of it because you
have to leave some time so you can do some of your own work that contributes, not just
reading and absorbing. You want to contribute, okay? So you have to pick a balance between
what you learn and what you work on yourself and try to eventually give back to your community.
So this balance is something you're always going to have to work on. And so today what
I want to do, I want to say, I probably want to do too much today. I want to talk about
how you can develop your own thoughts and how you can participate and contribute. And
I want to tell you a little bit about how you might do like a first contribution to
reinforcement learning. The simple trick I call completing the square. I'll tell you
a little bit about what I'm doing. And this is too much. It already is too much. And I'm
going to tell you, I want to talk a little bit about AI and society, which is also a
different kind of research opportunity. So there's all this stuff to learn. And first
is a question of how do you learn some more, okay? And I think you know these things. You
know about the online course from the Whites that just came out last Thursday. How many
of you have registered for that? Okay. And you know about my book with Andy Bartow. How
many of you have that? Okay. That's good. Now, as you learn about all this stuff, there's
a problem because there are no authorities in science. Now, you might be thinking I'm
an authority, but there are no authorities. And number one rule is you can't be impressed
by the part of all that stuff and all those different fields that you don't understand.
You have to be respectful, okay? You say, I don't know that. Maybe it's good. Maybe it's
not. You don't know. And the fact is when they look at science like from 100 years ago,
most of it is not very good. And most of it is, yeah, doing some screwy thing that seemed
important then and is not really important. So I think we have to assume that that's true
today. Okay. So you cannot say you have to learn everything and you cannot say this is
really important because other people understand it and I don't. You have to work from your
own mind. What do you understand and work from there because you want to make a contribution
so you've got to work from things that you understand. So don't be impressed by what
you don't understand. And there's a flip side to this. When you come back to give a
talk, you don't try to impress others by what they don't understand. It's really tempting to do
whenever you give me a talk, you know, present a bunch of crazy stuff that's really hard to
understand and then people hopefully will be impressed. You don't do that. That's just a
waste of time. There's all these different fields and all kinds of different people. Other people
know more than you do about the other fields and so no one knows everything and you want to
benefit from others. But you can't put up barriers. You've got to encourage communication
between all the different disciplines. Okay. So no authorities. What do you do? Well, you have
to be brave and ambitious. You can't just be, I'm going to do some tiny little thing and maybe
no one will bother me. No, you've got to be brave and ambitious, but you also have to be humble
and transparent. Humble because it's a great task we're looking at. The task of understanding the
mind and the problem is huge. It's subtle, but it's not devious. It's not hiding from you. It's
waiting to be discovered. It's up to you to see it. And so I think the bottom line that I'm trying
to express with this slide is that your thoughts are potentially of great value. I mean, you're
going to work in one of these fields and so you want to contribute and you have to feel that
your thoughts are potentially of great value. And I'll tell you as an authority that they are,
but too bad there are no authorities. So how can you train yourself to think well? So I think
I mentioned the key thing is to write. So I made this slide. The best way, the best way is to
write and discuss with other people. And so maybe it takes 10,000 hours to become an expert at
anything. People say this. I have no idea if it's true, but it does take a lot of work to become
an expert in something and to have something to share back with people. So this could well be true
for thinking about thinking. And this pile of notebooks is my history of 45 years of writing
about my thoughts. And you can see the one in the middle is from year zero 1974. And it's all just
a bunch of loose leaf papers that one day I kind of folded together in that notebook. And then as
time went on, I started to take my thoughts more seriously and saying I should keep them in a good
place and refer back to them sometimes. And just sort of, you know, respect my own attempt to think.
And it's, so I'm saying it's not, this is really important. It's essential to have something to
say is to have thought about the problem. And it's not super difficult. You don't have to be a genius,
but you do have to show up. You have to show up day after day, and you have to write. You have to
write. I mean, I'll ask of you. All I would ask of you is all I ask of myself, which is to shoot for
a page a day. You write a page a day. It's going to take you a long time to get 10,000 hours. But
that will be enough to put you ahead of other people. And because of that, you will have
something interesting to say. So I urge you to get a notebook. As you see, it ends there with my
computer, because I write in the computer now. Here's the prose poem that I write for myself. And
I write for people sometimes in their notebooks. Yeah, if you get a notebook and you write 100
pages in it, I would be happy to write this, this poem in your notebook. To write is to begin to
think. And to write in a special place, a book such as this is to honor your thoughts and to help
them build one upon the other. Well, that's it. I think that's really important. Okay, in the other
half that we've told you today, and I've told you in other times, the determination is super
important. So you're going to get stuck when you try to write down your thoughts. Absolutely,
you're going to get stuck. You're going to you're going to reach what appears to be a dead end with
nowhere to go. But there is always somewhere to go. And then there. So here are some techniques for
getting moving again. Number one, just to find your terms, you have some difficult question like,
what is intelligence? Or when will the singularity arrive? Okay, you start defining the terms. And
then you say, Well, what would it mean for it to arrive? You know, which is the question. You just
start poking at the questions. And then you say, What are some possible answers? You know, might
it be a million years, might it be one year? And you go meta, what would a possible answer look
like? What properties would it have? And then my favorite one is retreating, which is you you
pose a really hard question, you weren't able to answer it. So instead, you go backwards, and you
ask a simpler question, and you kind of try to build up to the ultimate question of interest, you
sneak up on it. And you keep going with the poor thing is you persist. And so these these are the
four key techniques that I use with myself. Okay, so let's do some of this just for practice. Let's
ask what is intelligence? What is intelligence? What is the mind? What is intelligence? And I will
start, I'll ask you in a minute, but let me start by giving you one sort of a definition. This is
from Ray Kurzweil. He says intelligence is the most powerful phenomenon in the universe.
He's saying it's not supernovas. It's intelligence. Which is after a while, you think about it's not
crazy, like how is the universe going to evolve? Will it all go to heat death? Or maybe intelligent
beings will have something to do with how the universe ultimately evolves. I think I would guess
that intelligent beings would have have a role to play in the long term future of the universe. Okay,
the other thing I like about it, or is it says phenomenon? Okay, it's a phenomenon. Intelligence
is this thing that you see happening out there in the world. Okay, so I like that. But what's
wrong with this definition? It's not descriptive. Did I get your word right? Yeah, it's not
descriptive. It doesn't tell you what it is. It tells you what a property of this thing is,
assuming you already know what it is. Okay, so do you think there's, let's try to make a definition
that actually tells us what it is. Okay, now you may know that some AI textbooks will say
intelligence is we don't have a definition and we just can't do any better than that. And that's
just not good enough. Okay, it's really not good enough. You can't let yourself say that. Or at
least you should be very disappointed yourself and put it high up on your list to fix it. Okay,
but so can we do it? What can anyone give me a definition, a meaningful descriptive definition
of intelligence? Yeah, I sure no one can hear you. But it's the ability to adapt to changes. Good.
Anything else? Yeah, another back there. Just shout. The ability to generalize. Good.
The ability to manipulate your environment to achieve goals. Okay, so I'm gonna stop there
because that's pretty close to what I think is the best definition. This is by John McCarthy,
who's like a founder. He actually coined the term artificial intelligence. Founder of the field.
Intelligence is the computational part of the ability to achieve goals in the world. I think
that's pretty good. Okay, but by the same time, there are all these undefined words in it, like
what is a goal? Maybe that's the key thing, the ability to achieve goals. Okay, we could go
further there. I also, like my own quote, this is me, intelligence is in the eye of the beholder,
which I mean quite seriously. It's like it being a phenomenon. It's something that we see out there,
like, oh, yeah, I want to understand that as having a goal. I think goals are in the eye of the
beholder. Okay, let's do another one. The predictive knowledge hypothesis. This is the
hypothesis that almost all knowledge of the world can be well thought of as statistics,
that is predictions, about the agent's future data stream. Okay, now that's an outrageous
statement because knowledge could mean knowledge, knowledge about podiums and people and physics
and geometry. We have all this knowledge and I'm saying that everything could be thought,
almost everything, can be well thought of as predictions about the future data stream. Okay,
now can you poke holes in this idea and have your name not be Dale Sherman's?
Okay, Dale. I don't even know what that means. It's got like five syllables in it. I don't know what
it means. I really don't. You mean knowing what would happen, really that's a fact about your
distribution of the future, that if the distribution was to go this way, then it would also go that
way. So if it's contractuals, if you mean like if things didn't be different, something else would
have happened, you know, that's just, I don't even know if that's knowledge, right? Maybe it doesn't
matter because that thing didn't happen. I think the real point of contractual of that way of
thinking is that it will help you for the future. So we'll have implications for the future. But
there are some obvious, there are some flaws in this idea. Okay, so if you're thinking of one,
speak up. I just want, I got a list of four of them. Here I got the exceptions. Okay, yeah. Knowledge
of history in the past. Wisdom. What might wisdom be? Explanation. Explanation, would that be
knowledge? Yeah, good. Yeah. Yeah. Yeah. No, the agent only sees its stream. The
data stream, this is the implicit essence of reinforcement learning is that all there is is
the data stream. There's actions going out, there's observations or states coming in, and there's a
reward coming in. And that's all the world is. The world is something you send bits to and something
that sends bits back to you. This is the reinforcement learning perspective. Or maybe it's
the computer science perspective on, on minds that, that we're exchanging bits with the world. It's
kind of particular, but I think it's something I'm really committed to is the single agent perspective.
That doesn't mean there aren't other agents in the world. Like, you know, I'm an agent here and I can
see you and I think of you as agents. And you are doing the same thing for me and all the rest. But
we each have our own data stream. And something that I know is about my data stream. It's not
about your data stream. I never even see your data stream and I don't care. Okay, there's one more
obvious one that we haven't mentioned yet. Martha, is that Martha? I have, I have what? You have a
sister. Okay. Yeah. So can we convert the sort of like factual truths when I say this is true. The
Eiffel Tower is in Paris and you have a sister. Martha has a sister. And are those, are those just
statements about your future data stream? Okay. So I think that's the hypothesis would be that they
are well thought out as that. What you might see if you go to Paris or what you might counter when
you call up where your sister is. Another quick comment. Mathematical knowledge. Thank you. That
was the obvious one that I wanted someone to pick up. Good. So now I can go on. Some of the
exceptions are mathematical knowledge. Mathematical knowledge is, is true in any world. Right. So in
some sense, it's not true about your world because they're just mathematical truths. They're not a
fact about your world. So, so the another way of writing this, you can say explicitly, it has to be
knowledge of specific to your world. And if it's true for all world, then it's not world knowledge. Okay.
But also thing like policies are not predictions. If you have good features, that's not not really well
thought of as a prediction. I don't think it's just a helper towards predictions and memories in the past or
beliefs about the past, maybe not included here. Okay, good. So remember why I'm doing this. I'm doing
this. I mean, I care about this. I care about all these things. But I want you to get, you know, see what
it's like to try to try to write down a question and, and think about it usefully. You know, you say, well,
what do we mean by knowledge? What is the one of these things predictions? What is this whole thing
about the agent data stream, the one agent data stream? That's sort of a pre presumption of the
question. Okay, good. Now, we could do this all day. And if we never get to anything else, unlike my slides,
that would be fine, because I think it's important to practice thinking and what it might mean. Okay, and this
is why, or this is one thing that I think is important. Is it the most important insight that you, you will
ever contribute is probably something that you already know. Okay, it's probably something that you
already know. And it's probably something that's obvious to you. Okay, but the problem is, it's not
obvious to everyone else. And so it's so, but it's so obvious to you, like, you can't even see it. You
know, it's like air or something. Or, yeah. So, so I'm just gonna give some sort of silly examples of
this phenomenon. Because it's really important, you have to like, stop seeing through everyone's
else eyes, everyone's else's eyes. And you have to see the obvious because the obvious is your
greatest contribution. So for example, when I started when I was in college, there was no
reinforcement learning. There was no reinforcement learning. There was barely any machine learning.
But there was nothing like reinforcement learning. And, and yet it was obvious to me that, you know,
agents are that, well, like animals, animals do things for food. And, and they don't like to get
hit. And we, and people have goals and they vary their behavior to get what they want. This is
obvious. And yet there was no reinforcement learning. There was no field that studied that in
engineering. Okay, so it's really common. It's really common that there are obvious important
things that are not recognized by, by, by your whole scientific community. Okay, so I'm gonna give
myself some silly examples. The discovery of gravity by Isaac Newton. You know, the story, he
was sitting under a tree one day and the apple falls on his head and hits him. And can't you just
see him say, hmm, there's an apple. Oh, objects fall. And so you could see it like running to his
friends. Objects fall. Think about it. Okay, that's what it would be like. And yet it was important.
It was important. And everything, this thing that everyone knew, the realization that it needed
some kind of explanation or an idea was a big deal. That was the discovery. Okay, the discovery,
Charles Darwin, the discovery that people are animals and evolved from animals. Now this was
harder for people to realize was obvious, obviously true. I mean, if you look at people, we're
animals. We got legs and we eat food and we excrete stuff. And, and, you know, we have kids,
we are animals. Okay, but it was not an acceptable position in Victorian England or all around
the world, many places around the world. And you had to had to really make that point and made
people upset. You know, so I mean, it's kind of like how nowadays we say that people are machines,
they're biological machines. And we haven't fully absorbed that one. Maybe we haven't fully absorbed,
but it's obvious. It's obvious that animals, you just look around, we saw any kind of an open mind.
But Charles Darwin was the one who got that, or maybe it was his father. I like to say the air
is also kind of like that. It's invisible. But you know, there obviously there is something here,
like he's, you see the wind will blow the trees and the trees move all around. And there must be
something there. Okay. Okay. And that said, reinforcement learning is like that. This was
discovered by really by Harry Klopp. Harry Klopp was this, this, this wild independent thinker,
a wonderful man. Sadly, he passed on too early. But he would he his skill was just to think for
himself and to realize that machine learning had lost track of reinforcement learning was just
doing supervised things. Okay. So with this silly list, are there obvious things that we struggle
to see obvious things that we struggle to see now? Is the question you should all be asking yourself.
So let me ask you, are there obvious things? Can you think of something that might be obvious,
and not recognize? I have no idea what you guys might say. It's a hard question. Just gonna take
a minute. Yeah. Have any feelings what it might be? Yeah. What kind of knowledge?
Okay. So that's good. But I don't think it qualifies as being obvious.
I mean, how would we, we don't, we're not faced with facts that suggest that. Okay. Can I think
something that's really obvious? Yeah. That we could all people could be equal. Sort of an ethical
philosophical point of view. That's good. But can we do anything about the mind? Well, so when
you're trying to think for yourself, you don't want to be arbitrarily and unnecessarily controversial.
So we would temper Patrick's idea to be that the bodies do an important part of the computation
that we attribute to our minds. Not that the other, the computational things do nothing. But the
bodies do a lot. And this is really true. And this has been a good insight from robotics and from
biology. Okay. I don't know. That was all good. Let me just give you my list quickly. And this is
going to be like, like, remember, I gave her a talk before, I was kind of throwing, I threw a bomb.
And so this is all going to be bombs. Because I'm going to say things that are obvious, and that we
don't know them, or we don't believe them. Okay. So these are possible. And it's, it's, it's enough to
be possible. These may be our obvious things that we start are struggling to, to see. Okay. So no
animal does supervised learning. There is no training set for our muscles. No mind generates
images or videos. Isn't that obviously true? And we don't generate them. Okay. I don't, I'm not
counting painters and videographers. But in our minds, we don't have to generate it. That's one
thing we don't have to do. We process them. We don't have to generate them. I'm going to go on
before I get an argument. Neural networks are not in any meaningful sense neural. Okay. That's
just, that really is obvious. And yet we, so many of us don't want to acknowledge it. People are
machines. The purpose of life, this is the reward hypothesis that the purpose of life is to get
pleasure and to avoid pain. And that that's a simple, effective way to understand people. So that's
sort of a good dramatic hypothesis, which might be true. And we struggle to see it. The world is
much more complex than any mind that tries to understand it. Therefore, having a prior distribution
over possible worlds would never be reasonable. The mind is computational and computation is
increasing exponentially with technology. And so we want things that scale with computation.
Human, any kind of human input doesn't scale. So if we try to make our AI system smart by, by
giving them our knowledge, that's kind of a somewhat hopeless and not hopeless. It just will, it
will not scale. And it will be just a little bit by little bit. The only scalable methods are
search and learning. And so there's some bombs. Okay. So I want to close this part of the talk
with some more advice. Okay. So number of advice. Think about experience is the data or slogans.
Experiences, the data of AI, it's like we were talking about just a minute ago, this this exchange
of bits back and forth, that's the data. And so we shouldn't ask the agent to achieve something
that it can't measure. Nice thing about reward is it comes in every time step, you can measure it.
It's not imaginary. It's not available somewhere else. It's available to the agent. And so it's
okay to ask him to measure it. We shouldn't ask the agent to know something that it can't identify
for itself. It's, you know, you can't tell directly whether some, some sentential symbolic
statement is true, but you can see your sensors. And if you make a prediction about your sensors,
you can see if it's, you can verify if it's true or not. It's very important to distinguish
between the problem you're working on and the solution to the problem. These things are very
often confused. And we want to approximate the solution. We do not want to approximate the
problem. Okay, it's sort of a bomb because this is really, really true. You're not going to see
right away why it's so important not to approximate the problem. But one reason is that your solution
method will scale up with computation. So you should pick a pick a problem that's the real
problem. You should not try to approximate it. That would not be lasting. And more than even
the approximation issue, you want to separate. You should, the thing you want to work on, you
should say ask, is it a solution method or is there a new problem? So maybe if you say, oh,
there's risk and you should work on risk, maybe that's a new problem. If you're looking at multi-agents,
that's a new problem. I know you don't like people messing with a problem too much. The problem should
be MDPs. And our problem is hard enough. Our only difficulty is that we don't know how to solve the
problem. We don't need a new problem. That's the way I feel. Okay, now as you're trying to solve the
problem, I like to, a key heuristic is to take the agent's point of view. Assume you are there,
you are faced, oh, I had to do these things. I could see that. What would you do to think,
put yourself in the point of view of the agent. Really helpful. You should set measurable goals
for the subparts of the agent. Like if you have a part that's the value function, you should work
on that. You have a part that's the model. It should try to do the model. It shouldn't worry
about the reward too much or the value estimates being right. You should work on making the model,
making an accurate prediction of the transition structure of experience. Okay, so that seems
obvious. But there are lots and lots of people in our field that are saying, no, no, don't have
different goals for different parts. Do everything from the one goal and then use this phrase end
to end. And I'm not sure what it means, but it's kind of the opposite of setting measurable goals
for the subparts of your agent. A really good strategy, I'll go more on this in a minute,
is that you should work by orthogonal dimensions, work issue by issue, and you should work,
I like, I think you should work on ideas and not software. Yeah, that's a good bomb. Okay.
So these are some suggestions, some advice, some grist for your mill as you try to develop your
own thoughts. Remember, there are no authorities and I'm not an authority. Okay, so the simple trick
for doing research is to realize that you can divide the whole area of reinforcement learning
into dimensions. And it's just much better to think of dimension by dimension rather than
whole overall problems. So here's just a massive list of the dimensions. And, okay, so all that
stuff's going on. Function approximation, state values, action values, model free, model based,
bootstrapping Monte Carlo, or you have the things up at the top. Okay, now there's a top level
division here. Problem dimensions and method, solution method dimensions. Okay, remember,
I've said this is the most important thing to keep clear in your mind. What's a problem
and what's a solution method? So among the problem dimensions, you can look at the problem of prediction
or you can look at the problem of control. You can try to predict what will happen, like predict
the rewards as an evaluation function, or you can worry about how to select actions to maximize
your reward. We often switch between these two as we try to make progress on some issue.
The distinction between bandits and Markov decision processes, that's a problem distinction.
The distinction of the setting, like is it a discounted setting, is it an episodic setting,
or is it an average reward setting? That's a problem distinction. It's not a method distinction.
Your problem could be fully observable. You could receive the states or you might only receive
observations as in a POMDP. That's a problem distinction. You could also, maybe I'll stretch
it a little bit, but you can talk about are you trying to get theoretical results? You're trying
to get empirical results. If you're trying to do theory, do you want convergence theory or you want
rate theory? The top theorists are bored by just ordinary convergence theory and they want rate
theory. Okay, so all these dimensions, I've tried to arrange them so that the easy cases on the left,
the hard cases on the right, generally like we would do prediction before we do control.
We would do fully observable before we do partially observable. Okay, now the method dimensions,
function approximations, of course, a big one, whether you have a model or not is a big one,
though this is a solution. It's not a, it's a solution method issue whether or not you have
a model because you are be learning the model and then you'd be using that model to help you
solve the problem. The problem would be unchanged if you changed from a model-free method to a
model-based method. Off-policy on-policy is, oh, maybe that one's kind of mixed, right? Because you
could, part of setting up a, you could set up an off-policy problem. Yeah, maybe that's, maybe
that really, it's not mixed, but it deserves locating in both, in both as a problem dimension
and as a method dimension. Okay, so what do we do? We can try to draw the frontier.
The frontier of the things that we know how to do, right, remember everything on the left is easy,
the things on the right are more difficult. And so we can't do everything on the right. We can't get
a convergence rate theory for a nonlinear
true online temporal difference learning, okay? We can't get the full, all the way to the end. So we
could try to draw a border, like to, to say where we can go. But of course, this is hopeless because
really they interact. And if you, if you make one choice, you can then, you make one, if you want,
if you really want to make, move to the right, you may have to move backwards on some of the
other dimensions. Okay, so, so here, a typical case, this is the research strategy, what I call
completing the square, which is you pick some of the dimensions. So you might pick here, we're
picking model based as the primary thing that we're interested in. And we know we can do model
based with dyna, we know how we can do it in a tabular case, and we might try to extend it
to the linear function approximation case. So you see the idea, you just pick a couple things and
say, oh, can I, can I move along, along, along the right to left spectrum. And so this is something
that we did in, in our 2008 paper with, with Chaba and Mike and Alborz. And so this really is the
way I do my work. I, I go through this dimension and say, oh, I'd like to move this out, I'd like
to handle nonlinear function approximation. So how do I do it? Well, let's go back to, away from
control, let's go back to prediction. Let's consider a discounted case, that's the simplest.
You know, we do the simplest case, we retreat. And then we try to go forward. Okay, just here, here,
so you, you might try to continue on the model based direction and go to a nonlinear model,
or go to control. It turns out these, these are, that's the state of the art. Really how you could
do model based with control, and how you could do it with a nonlinear model is the state of the art.
Or you might focus on average reward. Average reward is where you don't discount, you just try
to get the most reward per time step. And, and, and you might try to make an on, off policy
version of those algorithms. Or you might try to make a model based algorithm. It turns out just,
just a model based algorithm for average reward, and trying to do it online, as supposed to just
with, in a batch way, is a really challenging unsolved problem that, that I've been working on.
That's, or that's a discerning of more work. Okay, here's another one. If you want to do
convergence theory, well, what do we have? We have all the red ones. We, if we, the prediction case,
temporal difference learning on policy, and linear. We can do that. But we, we don't have it for
for the control case. That would be a research topic. You could take, you could see way down
there on the bottom, there's this idea of interest and emphasis. I've recently come to realize that
this actually interacts with everything. And whereas Martha and Rupam and I wrote a paper on
the off policy case, the on policy case is, you know, it's supposedly easier because it's to the
left, but it's, it, it's, it really is untapped and hasn't been worked out. Okay, so that's the,
this trick of completing the square. Any questions about that?
So you have to kind of know what's been done, but you work along the dimensions and you try to
slide to the right. Question, Andrea?
Maybe more of a higher level question, what's the problem, it's not, you need to be so clear,
so you can't really go over time, but you're, you're saying you don't approximate the problem,
so the real world is so alarming, you can't, I mean, you're so approaching it, you can't really
understand it. I guess there's a sense in which this is approximate, because I'm saying, I'd like to do
every case on the, in the problem dimensions, I'd like to go all the way to the right, and I'm,
I'm saying, well, I'll settle for something less as a stepping stone towards the real problem.
Yeah. Okay. Good. I'm going to keep going. I'm going to tell you something about the research
that I'm doing now. So first, the landscape of machine learning. The old view is that there's
supervised learning and unsupervised learning, and maybe there was reinforcement learning.
It was maybe because unsupervised and supervised, that seems like it should count everything.
You're either supervised or you're unsupervised, but now we slip in reinforcement learning somehow.
But this really has been feeling more and more dated to me, and I like thinking about things more
as prediction learning, control learning, and representation learning. And the,
and a fourth one maybe is integrated agent architectures for a whole system. Whereas,
because classical machine learning was just trying to do one little part of a whole agent,
and when you worry this other, so the stuff I'm going to tell you about today is we're
worrying about representation learning, we're worrying about how it might all fit together
in integrated architecture. Now, let's go in one step deeper. This is about machine learning.
Let's step into reinforcement learning. Talk about the landscape there. In core reinforcement
learning, we are focused on learning value functions and learning policies. And next,
we need to go on and worry about states, what are our state features? It's representation learning,
learning about what skills do we develop? Larger things, models of the world are larger things.
And all these, these new topics seem to be wrapped around a notion of the agent setting aside for
a moment the real problem that's working on in terms of reward and just working on some kind
of a sub problem. And I don't want you to fall asleep given the time and everything. I'm going
to try to motivate this just by showing you some videos, okay? And particularly cat videos.
Those should always keep you awake. So what we see here are just animals doing some purpose of
thing. Whether it's swinging on a branch or pushing this bottle around and or playing with a toy mouse,
animals pursue problems that are not the main problem. They're playing with a toy mouse,
not a real mouse. Playing with a ball, that lizard is playing with a ball, not with, not
with something that might sneak up and catch. Okay, so, and of course people do this too,
famously babies. I really like this one on the left where this, this child is like looking at
her hands and trying to figure them out. You know, how they work. It's very intent. It's not getting
food. It's just figuring out how hands work. It's fascinating. And eventually gets tired of that.
That's the other feet. Feet with the hands. It's fascinating. So babies are doing all this stuff.
It's not really about reward or maybe it is. I don't know. It's, it's not the main problem
of their lives. Here's another famous example of an infant just playing with its toys and
doing all kinds of different things. Very enormously active and of course sped up a little bit, but
so what's going on? You know, because I'm serious. I like to think about reinforcement
learning and AI in terms of people and understanding what people do. And so we have ways to go.
Let me just go on. So subproblems. There's a long history in AI and reinforcement learning of looking
at subproblems that are distinct from the problem that people talk about curiosity, talk about
intrinsic motivation. Rich Caruana did some old stuff where you looked at it in supervised learning
context. The options that you heard about yesterday, I think it was, are part of this.
And there's a somewhat settled issue is that what is a subproblem? A subproblem is a reward signal
and possibly a terminal value. Like if you get someplace, that would give you a terminal value
and if you stop there. But when I say subproblems are a reward signal, it means you might be
a different reward signal than the original reward signal. And then the solutions to the problems
are an option. That is, it's a way of behaving a policy and a way of terminating that behavior.
So we do have a sort of outer, outer loop. What our subproblem would be, what it means to be a
subproblem, what it would mean to be a solution to a subproblem. But there's still these key open
questions, like which of all the reward signals that you might make up and all the terminal
values you might make up, what should they be? How is that decided? Where do the subproblems come
from? And then even like, I think it's all obvious to us why a child playing with toys,
that might be a good thing. Why a cat playing with toy mice, that might be useful for it.
Even when an orca whale playing with a bottle, it might be good. It's learning how its body works,
it's learning how to control things in. Maybe later it will want to control something that's in
the water that floats. But what is that thought? Let's spell that out. What ways might they actually
help? Well, there are several ways that people have talked about. I'm most interested in the last
one, but let me just say them. Subproblems might help you learn good states, good state representations
and good state features. Or they might help you shape your behavior to make it more coherent
and therefore more exploratory. The last one is that subproblems will help you plan at a higher
level. They will get you knowledge of the world that enables you to plan. So I want you to think
about this in particular. That subproblems, as we say, get their solutions as an option. And once
you have an option, you can learn a model of what will happen if you took that option and then you
can use that model to plan with. This would be really useful if your values change and then you
can plan for the new situation. Just like in a grid world where someone moves the goal to a different
place, you can rapidly adapt to the new case. So what is this thing about states change their
values? That seemed nice and intuitive when I talked about the grid world and someone moving
the goal around. But really, if your world was totally stationary, why wouldn't you just learn
the value function once and then you'd be done? Why is it changing? And so that's like the first
mystery, I think, of why these subproblems are so important. And so that's what I want to try to
say a little bit about. And the key idea is what we call permanent and transient memories. So suppose
you're doing value function approximation, like you're learning a value function. I'm going to
show you in a minute some results from Go where the value function, the link, features of the
Go board to evaluate the Go board. And so you might imagine there's a weight vector and let's just
assume it's a linear function approximation. I guess you can see that. So we're going to update
our weight. The new weight is the old weight plus the step size times a TD error times the feature
vector. The value function, the prediction is w times x. It's the inner product of the weight
vector and the feature vector. So that's a prediction of how good it is at time t. And then
we look at the next reward and the prediction from the next state, the t plus one feature vector.
And so that's a TD error. And that's just a normal TD zero learning algorithm. And these are the
permanent memory is learning exactly this way with a small step size. So it'll converge slowly to the
best approximate value function. But we're not going to settle for that. We're going to add a
second weight vector that's a transient memory. And it's learning in almost the same way. It's as
if w times the permanent weights plus the transient weights, w tilde is the transient
weights. The sum of those two is like a new weight. And that gives you the value of the new state
and also the value of the old state. And you're doing the TD thing as usual.
So these are the transient weights that have a larger step size. And they're moving faster.
And why might it be good to have transient weights? I mean, you see, this is happening up here.
The permanent weights don't know about the transient weights. So the permanent weights are
going to try to learn the best function they can. And then there should be nothing left
for the transient weights to learn. This is what's called the cascade, where you give one as a
dominant is given priority. If he can do anything, if the permanent guys can do anything, it's not
left and it's not left available for the transient weights. Okay? Well, it turns out in many problems,
this is a good idea. That the transient weights do not go to zero. And so let's look at go and
imagine how that might happen in go. So the first panel, the first two panels are two features.
So a central stone, a black stone in the middle, that's a feature. And this is actually a good
feature. So it has a large positive weight. And you learn that the permanent memory learns that
that's a good feature. This other one is a two eye pattern in the corner. And this is also learned
to be good, but it's not quite as good as a for winning the game as a central stone, according to
the long term permanent weights that are learned. Now look at these two examples, these two different
games. This is a five by five go. So this is the whole game. And the permanent memory, remember
the permanent memory likes this. So it wants to play A. It kind of likes this too, but it prefers A
in both of these two positions. But it turns out that in the first position, playing B is the
winning move. If you play A, you lose, but you play B, you win. And so if you just had the permanent
weights, you'd not realize that. But by using transient weights, you learn that in this game,
in this game, it's more important to get the corner than it is to get the middle. You learn about
this game by your planning and lookaheads in this game. In this other position, it is right.
Move A is the winning move, and the transient weights don't interfere. They let the permanent
weights take the way to zero, and it doesn't interfere. So if you just do, if you just run
a converging player that uses an extensively trained permanent memory to pick moves versus
tracking player that has both transient and permanent memories working together,
the tracking player wins. It overwhelmingly wins, as shown in this graph.
This is across the axis. It's just three different setups with just one-by-one features with one
by one and two by two and with features up to level three. There's a clear effect in each case.
Okay, so that's interesting.
Even though the world is stationary, it goes just the stationary problem,
it's complicated enough that you need to have changing value functions.
Why is that? So I think it's a very general phenomenon. It's just the world is much more
complex in our mind. As we live in the world, going from state to state, we need to tune
our value function to the particular case we're in. If we have to average over all possible cases,
we cannot get as good a value function as if we adapt to the current setup.
Just the fact the world is so huge means you have to have approximation,
but because your approximation is always inadequate, you don't have enough weights.
So because of this, the best approximate value function will change as you encounter different
states in the world, even if the world itself is stationary. So this is the bottom line that a
big world yields apparent non-stationarity, and therefore your approximate value function should
change. The true value function is static, but the best approximate value function will change
as you encounter different parts of the world. Okay, so now I'm ready to give my answers to the
three key open questions about subproblems. What should the subproblems be? Each subproblem
should seek to maximize a single state feature and then terminate while respecting the original
rewards. Formally, what I mean is that the subproblem for feature i, you're going to have
a different subproblem for each feature, or I'll show you how you can be selectively in a moment,
but the subproblem for feature i has the same rewards as the usual problem. But in addition,
if the option stops at time t, it gets a terminal value, a bonus for having that feature,
the ith feature high at that time. Okay, so it's the ith feature high. This is just the normal
value function with the permanent weights. This is feature i's value, and if it's non-zero,
like if it's one, then you get a bonus proportion of the standard deviation of the transient weight
for feature i. So in other words, if you have a feature where the transient weight is not zero,
it goes sometimes negative, sometimes positive, it's significant, then you're going to get a
bonus for reaching it. Arrival at that state, you'll get a bonus. And thus, you'll end up learning
how to get to that feature. If you had, if having this in my hand was a feature, I would learn how
to get it in my hand from wherever it might be. If you think of yourself sitting down having a meal,
you want to get food, you want to get the pleasure of the food, and sometimes though you want to
drink, and sometimes you want to eat a bit of this food, sometimes you want to eat a bit of that food,
maybe you need to put down the fork to pick up the spoon, all those things are just like and go.
It's an extremely complicated function. When should you do which one? And rather than try to get
an exact nap from all of your sensations and all of your situations to what to do, you can say, oh,
right now, I have high weight on getting the fork in my hand so I can eat the
lovely steak that I've already chopped up. And so, so right then, getting the fork in your hand is
a high value. And so you can call out your already learned procedure for getting the fork into your
hand. And then you can just immediately form the plan, follow that procedure, the fork will be in
my hand, then I could stab the piece of food and put it in my mouth, all good. After I've done that,
maybe I want to put the fork down and pick up my glass of water. So these are the sub-problems
that once you have learned options to achieve them, learned models for achieving them,
then you can plan very effectively. Okay, so the second big question, where do the sub-problems come
from? You've seen my answer, the sub-problems come from the state features. If I have a bunch of
features and there's one sub-problem for each feature, and of course, if that feature has a
highly variable transient weight, the only many features whose weights don't change at all.
And there's no purpose, there's no advantage to making them into the outcomes of your possible
options. So you don't need a sub-problem for that. And how do the sub-problems help on the main problem?
The solution to the sub-problems is an option. That means just something you could do, you could
follow that option, you could act decisively. But the more important one I've tried to emphasize
today is that once you have that option, you can learn a model of that option, you can learn the
outcomes of that option, and then you can plan in large steps. Instead of doing this muscle switch
and that muscle switch, you can put your fork down and pick up your water glass. Okay, so let me
just summarize that. This approach to integrated reinforcement learning agents, a fully capable
reinforcement agent, must learn large things like new state features, new skills, and new models.
All of these pertain, can be explained in terms of sub-problems, and I've proposed problems of
state feature achievement while respecting the rewards. I'm not ever changing the rewards,
it's just, we get a bonus for achieving a state and terminating when that state is high.
It's a distinctive kind of sub-problem, and it fits well into planning and representation learning.
The rationale for all this is that the world is big, and therefore we, we have to use approximations,
we have to approximate it, and this means it will appear to change, and you have to track it,
it's going to be non-stationary, and this really is why planning and generalization really makes
sense, because we're encountering different, different, encountering this parts of the world,
different parts of the world at different times, and we can, there's a certain repetitive aspect
to the world. You have to relearn when you come back to this place, relearn when you come back to
that place, and that repetitiveness enables generalization to make sense. Okay, and then this
can all be focused by looking at, at which, which features are transient, and this allows us to
focus where we create our sub-problems, which representations we use, which models we use,
and how we do our planning. Okay, any questions on, on this sort of direction?
Okay, um, let's,
but save up your questions. I think I'm going to be done in good time, and you're going to,
I'm going to ask you for questions. Good, Joshua.
Capsules?
Yes, yeah, yeah.
Things that might change from one context to another.
That's right. I was thinking about all that when I was listening to you yesterday.
Yeah, it's, it's good. Good. Okay, let's, let's go now to finish off and talk about
what this might mean for the world. What does, let's just say, you know, it's just something
you should also be obviously aware of about the impacts, the ethics, because there are going to be
a lot of impacts of the coming of artificial intelligence. When people finally come to understand
how our minds work, well enough to make things, design and create beings that are as intelligent
as ourselves. This is a big thing. This is a giant, I think of it as a, as a matzo ball in the sky.
It's just a, it's just a target for every scientist should be thinking about maybe this is
going to happen within our lifetimes. And this is like the biggest thing. It's been a fundamental
goal forever, not just science, also for the humanities. And it's going to change the way we
work. It's a change of sense of ourselves, of life and death, and the goals we set for ourselves
in our societies. And it's even a significance beyond humans. It's beyond recorded history.
It's an event on the planet, at least. And so yeah, really, when we can understand the way
minds work and make more, it will be an event comparable to, well, maybe to, to replicators in
life. So to, to think about it, you have to realize, first of all, that it's driven by technology.
It's driven by what we call Moore's law. It's a little bit of a misnomer, but this super trend
of ever increasing cheaper computation. So what you see on this graph is versus years,
we see computer power increasing steadily. Now this is a logarithmic graph. So a straight line on
this graph would be, would be an exponential increase. And it's, it perhaps is slightly
curving upward. This is the, the, the death of this Moore's law has been predicted many times.
But we keep finding new ways to keep it going most recently with the GPUs.
So there's every reason to think this will continue. And so it's, and it's almost unstoppable. You
can't even see like the two world wars on this graph. Technology always pursues
progress along this dimension. And, you know, it's economically valuable. There's every reason to
think it will continue. And so we can't like forget about this. We can't ignore it. We can't
pretend it's not going to happen soon within our lives. I mean, it might not happen, but it could
also happen soon. I could see it happening. I would say that there's like one chance and
four that it will happen by 2030. Okay. And one chance and two that it happens by 2040.
That's my, my own guess that we would have enough computation to make human level intelligence.
Of course, that's a funny, funny, it's a soft, that, that term is not well solidly defined.
But even if you're off by an order of magnitude, that's just another five years one way or the other.
So, so this is, I'm, what I'm trying to, the point is that this is going to happen.
And we should be preparing for it as a society. And, but I don't want you to be scared. I don't
think we should be scared. I think the first statement to make about this is it's, it's a
very human centric thing. AI is really the most human centric of all fields. It's about us. It's
about understanding who we are and how we work and making us or amplifying us. Not exactly us,
but there, they are things that have goals that are intelligent. So this is the essence of, of what,
of what people are. And it's made, it's about making our lives easier and better. That's where,
that's why we have phones and eyeglasses and other kinds of technology. It's all about making
our lives better and more effective. And so we should think about it as a human, humanistic thing,
not as a alien technical, technical artificial thing. I don't even like, I think the name is
unfortunate to call it artificial intelligence. It's intelligence. And it's intelligence is what
we are. So AI is really us making or becoming the next people. And it's just the next step in the
grand march of, of life and, and evolution and creation in the universe is that, that this
changing ever widening river that is ourself and mankind. Okay, so this way it makes sense that
understanding intelligence, like understanding how you think and, and how we can achieve things,
that's got to be good. But you have to realize it's going to inevitably lead to ordinary
unamplified people falling behind. Some people will always improve themselves. So if you
don't improve yourself, you're going to be in a sense left behind. And some people will design
improved people. Even if we decide, you know, that it's not all that economic, there will be
some people that do it just because it's really interesting. So AI will inevitably lead to new
beings, new ways of being that are much more powerful than our current selves. So that's,
that I think is just true. Maybe it's one of those things that's obvious, but we resist.
The other thing I would say is that we should, we should think about AIs as being similar to
ourselves. So we should treat them maybe the way we would like to be treated. So think of that
similarity between people and, and the AIs, both are, are agents with goals. They may be compatible
with ours. They may be conflicting with ours. That's the ordinary situation. People are, have
lots of goals that are, that are, that conflict with each other and other goals that, that, that
are compatible. And this is why we have our economies and people find ways of working together
to mutual advantage. You know, even if someone wants to do something totally different from you,
you can easily work together on a project or, or you can work for a company or they might work
for your company. And so it's okay that, that you might have many agents that are very diverse
working on different things. And if you think about it in terms of symmetrically,
this can help you avoid the feeling of entitlement. I'm an agent. The AI is an agent.
They should, we should treat them symmetrically rather than put us, put humans on a different
status as over, an overlord either way. Well, in the long run, the technology of AI is going to
be part of what disrupts existing social and power structures. They will force us to reexamine
our, our morality and our social foundations. And this is, is not new though. It will be
continuing trends that are already thousands of years old. We are very different from, from our
predecessors 100,000 years ago, or even 10,000 years ago, or even 1000 years ago. So AI will
bring greater diversity of intelligence, both natural and artificial. And there will be biases
against the newcomers and they're different. They'll be feelings that they're taking our jobs and we,
we are entitled to our jobs. But all these feelings, I believe are counterproductive and
there will eventually fade away. The questions I think are key are whether or not we will welcome
these different kinds of people that will be coming amongst us. Will we welcome independent AIs? Will
we offer them a path to join our society in a productive way, in a cooperative way,
as sovereign persons? So my vision for the future is that we would have an open, dynamic,
resilient society, peaceful and prosperous, with a diverse multiplicity of designs for the people,
for the cultures, for the values, for the organizations. And you'd have people of all
kinds, organic and artificial. They would compete. They would cooperate. You would have overlapping
circles of empathy. You might, you might raise some of the AIs as your children. They might care,
care for you. A successful outcome is one without envy and without entitlement.
Now, what we have to worry, you may be worried, well, what if what we want to happen doesn't
happen? And I think you just have to let go of that. You can't insist that what you want to
happen is going to happen. Any more than some bizarre AI can insist that what it wants to
happen is going to happen. We have to cooperate. We have to work together. We have to see what
is the universe wants to happen. That being said, I do think that the rise of greater foresight of
more far-seeing agents in this universe has to be one of the few things that we can think is
probably generally good. Thank you very much.
Do we have a mic?
Okay.
So, we, researchers in AI, we have kids. Inevitably, we become the
psychologists. Watching our kids, how they learn, how they agree with us, disagree,
and they don't believe when we ask them to do things, we realize that actually we humans are
balls of emotions. Our emotions dictate often what we do. If somebody is offended, they will act
one way. If they are flattered, they will act a different way in the same environment,
in the same conditions. So, when we do our research creating these intelligent machines,
is it a mistake to put emotions out of the equation? Emotions are in the equation. They have to be in
the equation. Maybe we have only gone partway there, a reinforcement system. What is an emotion?
Yeah. Emotion is a reaction to a situation that maybe is not based on a thorough analysis,
but is an intuitive judgment. How do we make intuitive judgments? Well, we apply our value
functions. And so, I want to suppose, I want to claim that value functions are a basic kind of
emotion system. They tell us what we think are good situations and what we think are bad situations.
And the TD error is the change in that, and that determines whether we're happy
or are displeased by what happens. So, value is a prediction of future reward. So, if anything,
it's like hope. And if you're predicting a bad thing, then it's like fear. So, it's only,
you know, it's like a bipolar kind of emotion. Our emotions are more sophisticated than that,
and more subtle than that, and they involve other goals and other things that are built into us
by evolution. But I absolutely resist the notion that we have avoided emotions. A value function
is a value system. It's not really a coincidence that the name is the same as, you know, like how
we value things in our lives. It's not really deliberate either. It's just because there are
only so many words for this idea, this idea, this immediate sense of how well we think things are
going. So, I don't think we should shy away from trying to make, I mean, yeah, we don't do it every
day. But if we were doing psychology in neuroscience, we'd have to think hard about that.
Wait, Aisana?
Or like basic research.
May or may not be useful later.
Well, I would, who knows, but I do sense that it's similar. That, yeah, it's like,
it's like hobbies or something we get involved in. Yeah, that's the way the, that's the obvious way
to understand it. It might not be correct. You know, as someone who had some training in psychology,
I'm always suspicious of the obvious way to interpret or to think about your own behavior,
because so often we find out when we look at them carefully that we're wrong.
But I agree with you. It is the natural way to think about those things.
You mentioned before that in one of the explains that the purpose of life is to
gain pleasure or avoid pain. That is something that I think is a result of something that has
emerged through evolution, where the purpose of life is to survive and to keep existing.
And a lot of things like pain and pleasure, the way we define it, is something that we,
like evolution, have defined for us. And this includes things like emotion and so on.
So a lot of things that are part of our intelligence today are because of this
overall objective of survival. And when we think about AI, we are not,
that's not the objective of what we are trying to design. It's not just to survive. We are trying to
imitate the kind of intelligence behavior that at our current state finds intelligence.
So do you think all of these things like emotion and so on, is which some of the
behaviors that we would have are due to a different optimization or a different objective,
you think they could still be taken care of in AI?
So what do we do in AI? We are trying to understand intelligence. Intelligence is goal-seeking ability.
And so we should try to understand goal-seeking ability in general, independent of what the
goals are. If you have a goal-seeking ability but it only works with one kind of goal,
then it's just for that reason alone it's a less interesting, it's a less powerful, less
general kind of goal-seeking ability. So what our objective in AI, or at least one objective,
is to understand the notion of a goal-seeking ability independent of the goal. So we don't have to,
as you say, AI researchers or engineers, don't have to adopt the same goals as evolution.
We might for similar reasons but we don't have to. It's a real cutting point, right?
How do you set the reward signal and then how do you achieve the reward signal? And
I think it's really useful to establish that cutting point and separate the decisions of
what are going to be the rewards from how you achieve the rewards, whatever they are.
You were next? Do you still have a question? In purple? No, I thought, okay.
I didn't get that.
Yes, also. Right. The first perspective, multiple perspectives are good. The first
perspective is evolution sets our goals, then we're reinforcing the learning system.
That's a separation. The other perspective is that evolution itself is a learning system that's
trying to achieve some goals, the goals of evolution, the goals of survival and reproduction.
And that's valid too. It's of course not a very good reinforcing learning system because it can
keep memories from the survival about the life of one agent onto the next agent. But it's a perfectly
valid way of thinking about things. And Joshua.
So I'd be curious to hear your thoughts about the concerns of people like Stuart Russell regarding
if you make machines at human level, we'll tell you some more, that the goals that they would try to
achieve would be an expensive humans because of the whole value alignment. Do you know about
these questions? Yeah. So this is where I think the symmetry comes in. Do unto others as you'd
have them do unto you. The machines might not have that level. The reasons for that attitude
go beyond the construction of those machines. It's machines that are not cooperative will not
be cooperated with. And they will be less effective. And the other machines that are cooperative would
be much more effective. So there's an enormous drive, enormous power in cooperation. It's rational
to cooperate. You can be much more powerful and effective by cooperating than you can by
trying to take over. Taking over strategy will very rarely be effective.
So I think there's a little bit of illusion. We say, oh, people all have the same goals. I don't
think that's true at all. We have very different goals. And we have groups of people that have
very different goals from each other. And yet, we find that usually it's best not to go around
killing people. It's best to work together. It's best to find out what you can do for me and I can
do for you. I really think this is the most important aspect of humanity. Humanity is the
animal that cooperates. And this is perhaps most of all the reason why we're powerful.
So now, the AIs are not exactly us. But in the end, it's like sort of the very best case you
could possibly imagine, where you might be encountering a new kind of being. These are new
beings that we've created. So why don't we create them to respect the value of cooperation,
to realize early on that it's a rational strategy and that their goals will be best achieved
by working with us rather than against us.
So, there's one difference. The TB is one of the different types of symptoms that we have.
The age of aging is very often the same. We cannot do this in a very long time. So, you know,
one by one, five hundred and thirty aspects of this, to be honest, which is,
my question is, like, may be able to fit in different circumstances, but we don't have
a very long opportunity. So, is it always going to be an environment of us,
by inviting this opportunity, or maybe some kind of strategy to look at what the kind of
situation is, to care and to hear what the situation is going on, sharing all these different
opportunities? I don't think it's an absolute rule. I think it's just like a first perspective.
Like, whenever you meet a new person, you should be open to them and give them a chance to be
cooperative and have mutual beneficials interactions. It's no hard and fast rules,
and we do need to worry about security, but I don't like, I think it's going to be very
counterproductive if we set up a tiered system. We say humans are always above the robots.
I mean, we would not accept that if it was turned around the other way,
and that's a way to get resistance and violent outcomes.
Now, there's someone sort of in the back that I put off before. Yes?
Sir, my question is that it seems like a particular human,
another human, he does a lot of his learning in the form of, I think it's exactly this person,
in the form of a lecture or the math, in which he's told what are the right answers,
what are the right steps. So, don't you think that this is part of the situation,
at least the sub-quo is a part of the need?
Part of?
Oh, the closest thing to supervised learning, I would say, is maybe learning from books or from
school. Of course, no animal other than humans goes to school.
And really, even in school, learning and supervised learning is a tiny portion of it.
So, it's useful when you're making these claims, these statements for yourself,
because I've urged you all to do it and think about it. It's useful to go a little bit too far.
Like some of my statements had, all things can be thought of this way.
And it's useful to go to the limit and see how that stands up before
being too, hedging too many things. So, I may sometimes go a little bit too far.
Corey?
Thank you very much for the talk. Great talk. I'd like, especially your notion that you should
put yourself in the position of the agent, think about how they might perform what you're expecting.
So, if there was an AI in this room, how might we expect it to act to judge its intelligence?
Like how might you perform so that we then perceive you as intelligent?
Yeah. Well, what would I do if I was there?
Well, you know what they say. You might be quiet. It's sometimes it's better to
be quiet and be thought an idiot than to open your mouth and remove all doubt.
What might it do? So, let's say I suppose you can't talk. So, the only way you can
reveal your intelligence is through your actions.
Yeah, what do you think?
The Turing test?
No. That's, the Turing test is so human-centric. You know?
Yeah, humans, they always want to be better than things.
And they only want to see, they always imagine the robots want to become human exactly.
I don't know. Paul?
How would you know?
Yeah. So, over a period of time, watching the actions of the intelligent being,
seeing what it refrains from doing, what it does, what it learns from, we should be able to do it.
But it's not something that you can necessarily do quickly. Yeah. Maybe you'd have to live with it for a while.
Yeah.
Yeah. And the question is, are we? I mean,
it's true that we tend to have competitive games and we beat people. That's a certain stage that AI is at.
It's mostly because it's convenient for doing the research. But if you look at where, you know,
I talk about the Moore's law and the inevitability of AI arising, a lot of that's coming from
economic drivers. And the economic drivers are all towards cooperative AI, towards human,
intelligence amplification. You will spend, you know, maybe $1,000 for your iPhone. You know,
that's a lot of money. And you get something that lets you communicate better and access
information better. We will spend money to get a better, you know, a better Siri or a better
assistant. I think the major market will be for assistance in our, to help us. So I think
we're going to see helping. And we are already seeing helping assistance. They help us translate.
This, this will be the face of AI. It will be the thing that's helping us get our jobs done and
our enjoying our lives. So I think it's going to be very cooperative. I think that's what's going
to happen. It's not going to be something that's going to create a weird thing with its own goals
come out of the lab. It will be, maybe it'll be caregiver robots, like for elderly people.
I think that's actually where, where the economic drivers are. And that's what it's going to be like.
One last question.
Will there be a period that comes before we created actually the human level in terms of
that? Will our minds, like people in average, they cannot fit the knowledge that we require to
build them into everything? Because our knowledge has been like a growing explanation. Many, like,
maybe our brain isn't, like, capable enough to put all this knowledge. Like, for me, I, I grow up.
I learned a lot. I spent a lot of time learning things, like trying to catch up with you guys.
And then I developed, like, my own things upon me, like, costing, I assume, like, a longer time
that I could, to make some real contribution. So this, this cycle's back to where we started.
Like, how can we make ourselves more knowledgeable? How can we learn stuff? And
how do we develop our cognition? So I absolutely believe that there'll be a major part of the
investment in creation of intelligence systems will be for creating, for making us more intelligent.
That we will get, we will, Patrick will give us another lobe on our, on our brain.
And, and, and it'll be wired in somehow. And, you know, it'll be, we see it already, right? You
know, we rely on our phones to, to remember phone numbers, to, to access to Google, just for more
and more of our cognition. And that, as that link gets tighter, that'll happen more and more. So we
won't have to squash it into our, our, our wet wear of our brain. We can just get more tightly
integrated with the technology and, and augment people. Now, of course, you know, lots of people
won't want to do that. That's fine. But maybe it'll be wireless connected and it won't be so, so wet.
Yeah. So I look forward, as we figure this out, to, to all of us having, having better abilities
to think and becoming more human as we get more effective at, at achieving goals in our lives
and working together for everyone's mutual benefit. Thank you very much.
Thank you all. I would call our board, Justin, just to close things off. Let's do more of
big hands to work with this award.
Thanks, everyone. We're on behalf of C-FAR, and we, we really enjoyed the last couple of weeks
getting to know you. We're hosting you in Edmonton. And really, to be part of something special,
I would call a little thank you before you get out of here. First off, thanks to C-FAR. A lot of
people moved in as part of the summer school tour very long time. A lot of work done behind the scenes,
and really great people that were really there to, to support review applications. People like
Yachmo, the first one to step up. All doing it, like just providing a lot of time. So really,
really amazing people that are, that are contributing all along to put this couple of weeks together.
Big thank you to Camille and our content planning committee to put together a great line of
speakers. To Maggie, Bonn, Destiny, Cassie, Brittany, Vila, Spencer, and all of our organizing
team, and all of our sponsors. And special thank you to our creators, Pedro, for making sure that
all of us leave Edmonton a little bit heavier. So thanks. And really, a truly special thank you to
all of you guys for being here taking the time to come to Edmonton. So thank you so much. You'll
be receiving an email tonight with a certificate, a digital certificate, and a link to a feedback
survey. So please fill that out. And we would love to stay connected if you could reach out to us
through the Summer School website, through Slack, helloandhany.ca. And enjoy the rest of your summer. Thank you so much.
