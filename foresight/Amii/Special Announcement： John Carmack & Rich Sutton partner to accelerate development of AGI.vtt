WEBVTT

00:00.000 --> 00:08.120
All right. Hello everyone. Welcome to AME HQ. For those who I haven't met, my name is

00:08.120 --> 00:12.480
Cam Linky. I'm the CEO here at AME and we're pleased to welcome everybody today,

00:12.480 --> 00:17.680
both to our office here and to our friends who've tuned in online on the

00:17.680 --> 00:22.680
live stream. Many of you probably know about AME, but for those who don't, AME is one

00:22.680 --> 00:26.800
of Canada's three national standards of artificial intelligence. We're tasked

00:27.120 --> 00:31.600
with advancing Canada's AI potential and we're proud to collaborate with the

00:31.600 --> 00:38.160
University of Alberta on driving AME's AI research excellence forward. We're

00:38.160 --> 00:41.840
really, really excited to have everyone here today for this special

00:41.840 --> 00:46.200
announcement from our Chief Scientific Advisor, Rich Sutton. Before we get

00:46.200 --> 00:50.480
started, we'd like to respectfully acknowledge that we're on Treaty Six

00:50.480 --> 00:54.640
Territory, a traditional gathering place for the diverse Indigenous peoples,

00:55.040 --> 01:02.320
including the Cree, Blackfoot, MÃ©tis, Nakoda Sioux, Haudenosaunee, Dene, Ojibwe,

01:02.320 --> 01:08.560
Sotu, Anishinaabe, Inuit, and many other peoples whose histories, languages, and

01:08.560 --> 01:14.480
cultures continue to influence our vibrant community. And it's in the spirit of

01:14.480 --> 01:18.880
that exchange of knowledge and of that gathering that we're excited to welcome

01:18.880 --> 01:23.840
everyone here today. AME, we're really excited to support our friend and mentor,

01:23.840 --> 01:27.680
Rich Sutton. There's so much that could be said about Rich and the pioneering

01:27.680 --> 01:32.000
work that he's done in reinforcement learning and AI. The span of Rich's

01:32.000 --> 01:36.560
impact is almost hard to enumerate from his dedication to fundamental research

01:36.560 --> 01:41.680
and the advancement of the science of AI to his efforts to train the next

01:41.680 --> 01:46.720
generation of AI researchers. His book, Reinforcement Learning and Introduction,

01:46.720 --> 01:52.000
has both educated and inspired scores of graduate students and beyond, and is one

01:52.000 --> 01:58.160
of the most approachable people in the field, always having time for researchers,

01:58.160 --> 02:04.240
for curious observers, and for everyone in between to sit down, have a discussion,

02:04.240 --> 02:08.320
discuss reinforcement learning, discuss artificial intelligence. He's one of the

02:08.320 --> 02:13.600
people that, while many people have already left either the party or the conference,

02:13.600 --> 02:16.800
Rich will be in the corner somewhere having a conversation with someone about

02:17.520 --> 02:23.280
reinforcement learning, about some nuance of the field, and one of the things we talk

02:23.280 --> 02:29.040
about here a lot at AME is approachability, and that approachability is something that we've

02:29.040 --> 02:35.280
really been inspired by Rich from. Speaking of pioneers, the guest I'd like to invite up with

02:35.280 --> 02:40.080
Rich has trailblazed multiple fields. Among his list of pioneering accomplishments are

02:40.080 --> 02:46.480
co-founding id software, and his leading work there in computer graphics and computer gaming created

02:46.480 --> 02:51.600
an entire genre of gaming. Since then, John's also known for his work in rocketry,

02:51.600 --> 02:57.600
and ushering in a modern era of virtual reality with Oculus Rift. So we're so excited to have

02:57.600 --> 03:03.600
them both here at AME HQ, and everyone please join me in welcoming John Carmack and Rich Sutton to the

03:03.600 --> 03:25.760
stage. Thank you so much, Cam. It's a delight to speak to you all this afternoon. Today I am

03:25.760 --> 03:31.840
pleased to announce the formation of a partnership between John Carmack and myself to work directly

03:31.840 --> 03:36.000
towards the challenge of understanding and creating an artificial general intelligence

03:37.040 --> 03:41.760
based on reinforcement learning and neural networks. The partnership will be embodied

03:41.760 --> 03:48.320
within Keen Technologies, which is a startup that John created about a year ago. And as of today,

03:48.320 --> 03:53.680
I am an employee of that company. Of course, I will continue as Professor at the University of Alberta,

03:53.680 --> 04:00.080
and as the Chief Scientific Advisor at AME, the Alberta Machine Intelligence Institute that you're

04:01.040 --> 04:08.160
all here at. So I'm very excited to be partnering with John. John is the world's preeminent developer

04:08.160 --> 04:13.680
of complex high-performance real-time systems. His skill and brilliance was first demonstrated

04:14.320 --> 04:20.880
in technical innovations in 2D and 3D graphics in computer games such as Doom and Quake. His

04:20.880 --> 04:27.920
engineering skills were honed on rockets at Armadillo Aerospace and by his work on immersive

04:27.920 --> 04:36.240
virtual reality at Oculus. Roughly five years ago, John became, you know, he turned to the

04:36.240 --> 04:43.120
challenge of artificial general intelligence and that eventually led to Keen. Now, when I first

04:43.120 --> 04:49.840
learned about John and Keen earlier this year, I was struck by how aligned we were, I was with him,

04:50.480 --> 04:56.960
despite, you know, my having done this all my life and John just turning to it a few years ago,

04:57.920 --> 05:02.960
there are many aspects to the way we are so strongly aligned, but let me identify just four of them.

05:03.600 --> 05:08.960
We both strongly felt that the field of artificial general intelligence was dominated by

05:08.960 --> 05:14.880
too narrow a set of ideas. This was groupthink and this groupthink was to be avoided. We both

05:15.520 --> 05:22.400
strongly felt that trying to make money too soon was like an off-ramp on the road to AGI,

05:22.400 --> 05:27.760
and we need to keep our eyes on the long-term prize of full AGI, eyes on the prize.

05:29.360 --> 05:35.440
And number three, artificial general intelligence is not too complex for one person to understand

05:35.440 --> 05:43.120
the principles of it or even to write the code for it. That's that as an important part of our

05:43.120 --> 05:52.320
alignment. And finally, 2030 is a good target to have a success, to have a prototype AGI,

05:52.960 --> 06:00.480
to show signs of life. We're a toddler, as John likes to say. So John and I are really well aligned,

06:01.360 --> 06:10.960
and, but there's a sort of an elephant in the room and it's become clear. Keen is a small team,

06:10.960 --> 06:21.360
the entire technical team of Keen Technologies is here today, and it's John and me and Gloria and

06:21.360 --> 06:27.920
Lucas, if you could stand up, and also Joseph, who's we expect to join us by the end of the year,

06:28.880 --> 06:37.840
and now they're all these Keen people are now part of our community. And could you all give

06:37.840 --> 06:56.400
them a good Alberta applause? Thank you for that. So the elephant in the room is it this is a small

06:56.400 --> 07:05.680
team and other companies have thousands of technical staff and are spending billions of dollars on

07:05.680 --> 07:11.040
AGI. It's truly audacious of us to think that we can make a contribution.

07:13.920 --> 07:21.440
But there are actually it's audacious of us to think we can compete with them. And we think we

07:21.440 --> 07:27.120
can or at least we can make a contribution. There's much to say about why that might be

07:27.120 --> 07:32.160
reasonable to think and I'm going to refrain from trying to explain it. But I will only remind you

07:32.160 --> 07:38.960
of what Margaret Mead said. She said, never doubt that a small group of thoughtful committed people

07:38.960 --> 07:43.200
can change the world. Indeed, it is the only thing that ever has.

07:45.440 --> 07:49.760
Now I'd like to turn the floor over to John to say a few words. The legendary John Karnack.

07:49.760 --> 08:04.320
So a couple months ago, I got an email from Richard Sutton. I'm like, well, this is cool.

08:04.320 --> 08:10.480
So I am relatively new to the artificial intelligence field. And when I started kind of my larval

08:10.480 --> 08:15.440
phases, I like to call it where I just inhale all of the relevant information in kind of the first

08:15.440 --> 08:21.200
year. One of the very important references for me was Richard's book on reinforcement learning.

08:21.200 --> 08:26.160
And in fact, I later went through the first half of it again with my son doing exercises

08:26.160 --> 08:31.840
just a couple years ago. And of course, I reference the bitter lesson all the time as one of the

08:31.840 --> 08:36.240
deep fundamental insights to the kind of broader effort of everything that goes on here.

08:36.800 --> 08:42.800
So I'm Richard was trying to figure out I'm kind of like how he should be pursuing his efforts

08:42.800 --> 08:47.360
towards research across different options in commercial, academic and nonprofit.

08:47.360 --> 08:52.880
And I wanted to be super helpful. I hooked him up with a few other people and I tried to kind of

08:52.880 --> 08:58.480
give him whatever help that I could. But I did very tentatively kind of broach the subject that,

08:58.480 --> 09:02.800
all right, you're the godfather of reinforcement learning, you can write your own ticket anywhere

09:02.800 --> 09:08.480
you want to go work. But there are some downsides to especially larger established organizations.

09:09.040 --> 09:14.960
And you may get access to lots of resources and still have the freedom to do whatever you want.

09:14.960 --> 09:22.000
But there are problems with culture and direction, kind of strategic direction in other areas.

09:22.000 --> 09:28.240
It's why as fired as I was when OpenAI tried to recruit me, I respect all that they do,

09:28.240 --> 09:32.640
but they've got their plan, they've got their directions, it's not really what I'm doing.

09:33.360 --> 09:39.200
And there began a little process of kind of feeling out, well, exactly how do you feel about

09:39.200 --> 09:43.760
certain of these directions? Like, what are the odds of technologies going this way? How

09:43.760 --> 09:50.240
important is this? How important is that? And it turned out that we really did have a remarkable

09:50.240 --> 09:55.280
amount of overlap in what we think is possible, what we think the remaining challenges are.

09:56.000 --> 10:03.280
And to be clear, nobody has line of sight on the solution to this today, but we feel,

10:03.280 --> 10:10.000
and I am a number of other people, that it's not that much left. There are things we don't know,

10:10.000 --> 10:14.800
we don't know how to get there, but they all feel like the same scope of things that mattered in

10:14.800 --> 10:19.920
this last decade. They're relatively simple things in the way that you set up your architectures,

10:19.920 --> 10:25.200
the way you do your training, the way you query it in different ways. And I certainly think that

10:25.280 --> 10:30.080
in 30 years, when the textbook of artificial general intelligence is written, it's going

10:30.080 --> 10:35.040
to get digested down to a chapter that people are going to be able to understand. We just need

10:35.040 --> 10:42.480
to figure out what those sort of core ideas are. So it was really still to my great surprise,

10:42.480 --> 10:47.840
but immense pleasure, that we have decided to go ahead and work on this together. So

10:47.840 --> 10:53.920
Richard is working at Keen Technologies now, he's in our workplace, and we spent all day today

10:53.920 --> 10:58.640
basically talking about the plan of research, how we want to start figuring out what we're going

10:58.640 --> 11:04.640
to be doing. So this is incredibly exciting for me. And would you like to say anything else Richard?

11:15.600 --> 11:19.600
Thank you, John. I think now we'd like to take questions from the media.

11:24.400 --> 11:31.120
So we're going to open up the floor for any media questions. For the media in the room,

11:31.120 --> 11:37.360
if you have a question, please come over here. And for media on the line. So operator, please open

11:37.360 --> 11:53.040
up the line for the first question. Oh, there's no media on the phone.

11:53.680 --> 12:03.040
Any media questions from the room? Oh, hi, I'm Renali Unchin with CBC Edmonton.

12:03.040 --> 12:07.680
So following this partnership, I'm wondering if you could talk about some of the tangible steps

12:07.680 --> 12:15.120
that are going to be taken to kind of develop AGI more. So one of the big points that we do have

12:15.120 --> 12:20.480
alignment on is that there's not a near term offer answer where there's not going to be a chat

12:20.480 --> 12:26.320
gbt like deliverable that goes out and allows the world where there are fundamental research

12:26.320 --> 12:32.240
questions that need to be answered. And this is me learning how to be a researcher and hopefully

12:32.240 --> 12:39.120
learning from Richard a lot about that process. But we have internal projects and angles of attack

12:39.120 --> 12:45.040
on things, but they probably will not have a lot of publicly visible effects for likely years.

12:45.360 --> 12:51.520
We are fairly aligned on this sort of we are seven ish six seven eight years out from something

12:51.520 --> 12:59.360
really big and important being publicly visible. Anything? That's perfect. Next question.

13:00.880 --> 13:04.560
Yeah, I have a follow up. So for the average person, obviously, there was a lot of terminology

13:04.560 --> 13:09.760
thrown around. Why should the average person care about this if you can describe in the most simple

13:09.760 --> 13:14.560
terms? Well, I would say the average person probably shouldn't care a whole lot about this,

13:14.640 --> 13:19.200
you know, I mean, this is something this is inside baseball work for people that are

13:19.200 --> 13:24.000
in the field. I am, you know, a lot of people are going to be geeking out about this where,

13:24.000 --> 13:29.520
you know, rich is a big, big name, big percentage, and I'm a big name in a very different field.

13:29.520 --> 13:33.840
And we're kind of coming together. And I do think there's synergistic benefits for that.

13:33.840 --> 13:38.800
And it's kind of exciting just as a, you know, two great taste, taste great together sort of

13:38.800 --> 13:44.800
mix things up. I am between the, you know, different backgrounds, but I know, I mean,

13:44.800 --> 13:49.040
I don't want us to try to make this out to be something that the man in the street should

13:49.040 --> 13:54.880
actually care about. It may yet lead to one of the most important things in the world or in history,

13:54.880 --> 14:00.480
but I there's very little guarantees about any of that. And that's the benefit of the way we're

14:00.480 --> 14:05.200
financed in the way we're structured right now. We don't have to have a rush to a product. We don't

14:05.200 --> 14:10.160
have to have a rush to make sure that there's an investor return in a very short amount of time.

14:10.160 --> 14:14.000
We can concentrate on just trying to answer these critical important questions.

14:15.120 --> 14:19.280
Okay. Cool. Great. Any other questions?

14:22.480 --> 14:30.080
Fantastic. That concludes the Q&A portion of the program. Oh, yes, Rich. If we're done,

14:30.080 --> 14:38.400
I have a few things I want to say. Please. Before we close, I want to thank Amy and the

14:38.400 --> 14:42.800
University of Alberta for their help today, particularly Stephanie Enders and Laura Carter

14:42.800 --> 14:50.560
and Cam and Linda Vang, express my appreciation for the entire Edmonton and Alberta community

14:50.560 --> 14:56.000
for all they do in creating the rich intellectual environment in which I and this part of Keen

14:56.000 --> 15:02.160
can thrive. The community has supported fundamental research in AI for 30 years

15:03.360 --> 15:09.760
before I came here. And this is bearing fruit now in Amy and in industry and startup companies and

15:09.760 --> 15:15.200
more fundamental research results. Please join me in a huge round of applause for Amy and for

15:15.200 --> 15:30.800
yourselves, the entire AI community. And with that, I want to turn the floor over to someone else.

15:32.960 --> 15:38.640
Cam, there you are. All right. Well, thanks everyone. How about one big round of applause

15:38.720 --> 15:51.040
for Rich and John again. So thank you everyone for joining us today. That concludes the program

15:51.040 --> 15:56.160
for the announcement. We're going to reset the room for the fireside chat. So if everyone can do

15:56.160 --> 16:01.440
us a favor, I know this is a pain, but we need you to all leave the room, continue the conversation.

16:01.440 --> 16:07.440
Everybody can have their favorite conversation about what took place here today. Head over to

16:07.440 --> 16:11.920
the co-working space in the cafe. We have food and drinks and caffeine to really amp you up for

16:11.920 --> 16:17.600
this next portion. If you've RSVP'd for the fireside chat, we'll we'll readmit you shortly.

16:17.600 --> 16:22.320
So please do us a favor, move over there to our beautiful space and we'll see you back here shortly.

16:22.320 --> 16:35.600
Thanks. All right. Okay, we're going. Welcome to the second half of the show. If you can do us a

16:35.600 --> 16:41.120
favor, if you're on the edge of the roll, please squeeze into the middle. That will help as stragglers

16:41.120 --> 16:46.080
come in, not doing this, excuse me, pardon me, excuse me, pardon me, and distracting the speakers,

16:46.080 --> 16:51.280
so or just me. So do me a favor, squeeze into the middle, help us out. Thank you very much.

16:52.000 --> 16:58.720
Get close, get to know your neighbor. Welcome back for those who missed the earlier part of this.

16:58.720 --> 17:04.880
I'm Cam Linky, I'm the CEO here at Amy. We're excited to welcome back to the second part of our

17:04.880 --> 17:12.000
doubleheader with a special fireside chat. So our host for the night is well known for his hate of

17:12.000 --> 17:19.200
love of games, love of games. Yes, loves games. An Amy fellow, Canada CIFAR AI chair and full

17:19.200 --> 17:24.880
professor at the University of Alberta. Mike is best known for his work in poker, most notably

17:24.880 --> 17:32.080
solving the game of heads up, no limit, Texas hold them in 2015. And for deep stack in 2016,

17:32.080 --> 17:37.280
the first AI to be human professionals at heads up, no limit, Texas hold them. I think I made an

17:37.280 --> 17:44.320
error in there that Mike's going to correct. He's glaring at me. We're so excited for Mike to be our

17:44.320 --> 17:49.280
host and to host our special guests tonight. And so I don't make any more errors. Please welcome

17:49.280 --> 17:57.200
Mike Bowling to the stage. Thank you very much, Cam. I'm sorry, I do have to correct you. So we

17:57.280 --> 18:02.960
did essentially solve heads up limit. And we beat the first to be professional players,

18:02.960 --> 18:08.480
heads up no limit. Okay, but you're not here for that. I get to have the great honor of introducing

18:08.480 --> 18:13.200
John Carmack. Many people have already said wonderful words about him, but maybe you weren't

18:13.200 --> 18:20.480
here for that. I'll just say that he's a pretty big deal, maybe a BFD, if you will. But I'll just say

18:20.480 --> 18:28.560
my first connection to John was probably my first memory of seeing something really innovative

18:28.560 --> 18:32.560
was thanks to John. Because if you're really young, you can't see innovation. Everything is new.

18:32.560 --> 18:37.680
So you didn't know what came before. So you don't know this change things. And when I was 17, I went

18:37.680 --> 18:41.920
to a gaming convention and they're sitting was a rows of computers. This wasn't even a video game

18:41.920 --> 18:46.320
convention. It was board games and real playing games. And they're sitting was a rows of computers

18:46.320 --> 18:52.320
with Wolfenstein 3D running on them in the summer of 1992. And my mind was blown. So thank you,

18:52.320 --> 19:03.840
John Carmack, and welcome to stage. Welcome, John. Thank you.

19:03.840 --> 19:13.360
Oh, I'm supposed to push the button.

19:15.280 --> 19:21.760
Okay. Yeah, when I was asked to do this, I was trying to think through. I felt like I had a lot

19:21.760 --> 19:26.880
of pressure on me to ask you really insightful questions for the audience. But I decided I'm

19:26.880 --> 19:30.640
going to kind of almost ignore the audience. I'm just going to ask you questions. I feel like I

19:30.640 --> 19:35.360
want to ask you totally selfishly. And I'm going to hope because I have a love of games,

19:36.480 --> 19:40.960
you know, I have a love of innovation, and I have a love for AI that maybe these are the

19:40.960 --> 19:46.240
questions that the audience wants to hear too. But so I want to walk you back to the beginnings

19:46.240 --> 19:53.520
of id and Wolfenstein 3D. And maybe you can say some words about how did you even end up

19:54.080 --> 19:57.280
at that place? And then I want to turn around and talk a bit more about innovation from there.

19:58.240 --> 20:03.840
Yeah, so it did. Games were super important for me in my childhood. You know, I love the

20:03.840 --> 20:09.840
arcade games. I love the early 8-bit video games and the tabletop games, all of that. And so it

20:09.840 --> 20:15.280
did seem obvious to me that this love of computers that I had that the best way to express it was

20:15.280 --> 20:20.320
through games. Now, it is important to kind of differentiate a little bit there where there

20:20.320 --> 20:25.200
are a lot of people that go into gaming and they learn about computers so they can make games.

20:25.280 --> 20:31.120
And it was a little more fortuitous for me because I had this intrinsic love of computers as well.

20:31.120 --> 20:36.560
And I probably could have been happy and found interest in doing more mundane things. But the

20:36.560 --> 20:41.520
games were there. They were the obvious place and it turned out to work, you know, really well for

20:41.520 --> 20:45.840
me. And so I had been, you know, for the first time I could do anything on a computer. I was

20:45.840 --> 20:50.080
trying to write games. I was trying to, you know, at first mimic some of the other games that I

20:50.080 --> 20:55.440
would see. The first, the early text adventure games, writing my dungeon crawlers, you know,

20:55.440 --> 21:01.040
kind of inspired by wizardry and Ultima and the early things like that. But as I got to be a teenager

21:01.040 --> 21:06.000
and I built the assembly language skills and started figuring out how to do the sort of the

21:06.000 --> 21:12.400
real world things and I had a talent for it. I got good at it and I had decided that I wanted to

21:12.400 --> 21:17.280
actually try to make a living making games. And there was, you know, there was a lean year there

21:17.280 --> 21:22.880
of barely scraping by doing contract programming work before I accepted the position to come down

21:22.880 --> 21:29.120
and work at soft disk publishing where I met the other founders of id software. And we started out

21:29.120 --> 21:34.320
making our initial side scroller games, which was Commander Keen, which I have kind of harkened

21:34.320 --> 21:40.720
back to with the name of my latest company with Keen Technologies. But then we, you know, we really

21:40.720 --> 21:47.200
broke out and kind of made our name with the 3D games. And I always did kind of look at that as

21:47.520 --> 21:52.800
it wasn't the games were the same things that you could do in 2D, but the change in perspective

21:52.800 --> 22:00.000
of going to 3D, it made a qualitative difference, even if sort of symbolically it was the same game,

22:00.000 --> 22:04.880
but it made a great difference in the impact it had on people. Tell me more about that that jump

22:04.880 --> 22:08.720
to 3D. Was that was that intentional where you like I could think of so many ways this could

22:08.720 --> 22:12.080
have gone where you could have just been like that seems like a hard challenging computer's

22:12.080 --> 22:16.880
problem. And so I'm going to solve that versus feeling like this is the future of games. So we

22:16.880 --> 22:20.800
need to innovate in that front versus not even realizing maybe that you were being innovative.

22:20.800 --> 22:26.240
Like how did that work? No, I knew I was heading for that from the early 80s. I am, you know, as a

22:26.240 --> 22:31.360
you know, even as a young teenager, you would see the representations of 3D graphics or in the movie

22:31.360 --> 22:37.520
and you think about you see Tron or you see 3D animated logos. I can remember making a little

22:37.520 --> 22:43.360
wireframe MTV logo spin around on my Apple to kind of figuring out these basics and looking back,

22:43.360 --> 22:46.800
like I didn't know how to do clipping at the time so nothing can get close to the edge of the

22:46.880 --> 22:52.560
screen. It just has to stay in the center and rotate around. But that idea of wanting to be

22:52.560 --> 22:57.760
inside the video game, I mean, everybody that's a gamer kind of got that sense of you want,

22:57.760 --> 23:02.000
you look at the game, you play the game, you appreciate it, but how amazing would it be to

23:02.000 --> 23:06.960
put yourself inside it and then, you know, see the enemies coming at you rather than looking

23:06.960 --> 23:13.600
down at it as that 16 by 16 block of pixels. And that was magical. And you could take exactly the

23:13.600 --> 23:19.760
same game and put the user inside it and that really followed on even more with virtual reality

23:19.760 --> 23:24.320
later on where getting the sense of presence where you kind of believe that you're in this

23:24.320 --> 23:30.400
other environment. So that was always a big issue for me, but I do, you know, it is important to

23:30.400 --> 23:35.520
say that that's kind of the obvious visible thing that stands out that there was a time period where

23:35.520 --> 23:41.280
there was nothing else like that, but there are a thousand good decisions that go into making a game

23:41.280 --> 23:47.920
and it was never about just the technology. It was about doing those thousand things right.

23:47.920 --> 23:53.600
All the subtleties about how you feel, how you move, how the guns react, what the sound effect is,

23:53.600 --> 23:58.800
there's 10 things you layer on top of every action that happens in a game to wind up giving

23:58.800 --> 24:03.440
it this really good feel. And most of them will be things that people don't even notice. I mean,

24:03.440 --> 24:09.040
everybody points to Oh my God, this 3d black magic that was going on that that left people,

24:09.600 --> 24:13.040
you know, just wondering how they could compete with something like that until

24:13.040 --> 24:18.480
you know, engine technologies got out there. But there were plenty of other now forgotten games

24:18.480 --> 24:23.600
that wound up having flashy 3d graphics, but people don't remember them because they didn't

24:23.600 --> 24:28.160
do all the other things, right? I mean, I definitely remember the just smoothness of the

24:28.160 --> 24:32.000
original it games were just, I think just blew people away. Like it felt like you could play them

24:32.000 --> 24:36.640
as opposed to being pulled out of the immersion from the clunkiness. But that's also innovative

24:36.640 --> 24:41.520
technology too, right? That's some of the things that were actually subtle that were really important

24:41.520 --> 24:46.960
to me. And nobody actually called these out. But in terms of the graphics, the way they were

24:46.960 --> 24:53.360
rendered, there were a lot of 3d games that had this kind of non solid feel to it, where there

24:53.360 --> 24:58.720
are these subtle things that happen to do with pixel centers and avoiding cracks between polygons.

24:58.720 --> 25:03.360
And you can still make great games for that. Like the entire PlayStation one, I am, you know,

25:03.360 --> 25:09.440
set of titles was all done with this integer snapped affine interpolated rendering technology.

25:09.440 --> 25:14.640
You can do good things with that. But I always took pride in the solidity, the kind of sense that

25:14.640 --> 25:19.760
everything was really rock solid there, where some games felt fragile, like, you know, you bump the

25:19.760 --> 25:23.840
wrong way and you're going to slide through the wall, get caught and see a flickering mess of

25:23.840 --> 25:29.520
stuff. And for the most part, I, we took great pains to make that not happen in ours.

25:29.520 --> 25:32.960
That's awesome. Is there anything else that like you think like this was the innovative part,

25:33.040 --> 25:37.280
but in the, you know, decades later, no one remembers that being the innovation?

25:37.280 --> 25:43.120
Well, what was great is we had this period of like this five year period during the development

25:43.120 --> 25:49.440
going from Wolfenstein 3d to doom to quake. And there were so many things that wound up setting

25:49.440 --> 25:56.080
the tone for gaming for the following 30 something years. And things like the multiplayer gaming,

25:56.080 --> 26:01.600
the modding, these were not the flashy 3d graphics technology. But they, you know, they were things

26:01.600 --> 26:06.800
that were super valuable. And it was happy to see people, you know, follow up on it, even little

26:06.800 --> 26:11.600
things like having a console, you know, having the, you know, the override ability for the different

26:11.600 --> 26:16.720
data sets. There were a lot of decisions like that, that, you know, that worked out pretty well.

26:16.720 --> 26:22.000
And I could imagine a world where 3d graphics was inevitable. It was being done on higher end

26:22.000 --> 26:28.880
systems, offline rendering. But it is possible to imagine a contingent set of history where

26:28.880 --> 26:34.400
you didn't wind up with this action oriented things because the dominant vision was you do sims,

26:34.400 --> 26:40.880
you do flight sims, driving sims, tank sims, you know, destroyer sims. And this idea of this run

26:40.880 --> 26:46.240
and gun really fast paced action, twitch reaction that, you know, whipping around the mouse directly

26:46.240 --> 26:51.920
at inhuman speeds and all of these things, that might not have happened without id software in

26:51.920 --> 26:57.680
the early days. I wonder just like how much that affected the development even get to where we are

26:57.680 --> 27:03.440
today in terms of would, you know, we've seen gaming drive technology a lot, right? So would we

27:03.440 --> 27:08.160
have GPUs today? Would we be sitting on top of AI on top of Dean learning sitting on top of GPUs

27:08.160 --> 27:12.400
today? If we didn't have those initial games saying we could have action oriented, broader,

27:12.400 --> 27:17.120
open games. Do you think that's a possibility? So the GPU side of laddering you, but yeah,

27:17.120 --> 27:23.120
I mean, I do, you know, it's one of those things where I smile and I'll talk about it a little bit.

27:24.080 --> 27:30.800
I do take some satisfaction in the fact that the GPUs were largely built to play the Quake series

27:30.800 --> 27:37.040
of games at the beginning. And then you had this great insight from like Jensen at Nvidia saying

27:37.040 --> 27:41.840
it's like, well, all these pixel processing things that we're doing, we can do other things with them.

27:41.840 --> 27:47.600
And they had a lot of foresight to do the long game investment in CUDA and give us the kind of

27:47.680 --> 27:53.680
generalized processing. Because people were doing general processing before that in this horrible

27:53.680 --> 27:58.800
way. You'd paint, you'd encode your values into pixels, you draw a giant triangle that covers

27:58.800 --> 28:04.400
the screen to do an array processing action on there. And it was, you know, it was effective a

28:04.400 --> 28:10.960
little bit, but it was awful. But the evolution of GPUs to where they are today as this quite

28:10.960 --> 28:16.080
general purpose device that does underpin all of the modern era of artificial intelligence.

28:16.720 --> 28:22.000
It's nice to, even if I wound up not working in artificial intelligence, it would be something

28:22.000 --> 28:26.880
that I'm proud of that I at least contributed at some point to that evolution. I have one more

28:26.880 --> 28:31.360
question of curiosity from this time that relates to Wolfenstein itself. So I played

28:31.360 --> 28:37.520
the original Wolfenstein games and Beyond Wolfenstein and love them. How did it, how did this,

28:37.520 --> 28:42.080
like, did you have that IP in mind and then we're designing a game for it? Did you, did you,

28:42.720 --> 28:47.120
you know, have the game first? Like, what was the order of events and, and how did that come

28:47.120 --> 28:52.320
back? So all of us at Id, I am, you know, John, Tom, Jay and I, I, we were all Apple II background

28:52.320 --> 28:58.240
people. So we all had this background with the original Escape from Castle Wolfenstein. And it

28:58.240 --> 29:02.480
was a game that had these, it was more what you today would think of as a stealth game. You would

29:02.480 --> 29:08.000
sneak around, you'd wear guards uniforms, you'd drag the bodies out of the way. But it still did

29:08.000 --> 29:14.640
have that sense of shooting Nazis. And I, we had originally thought we were going to do some alien

29:14.640 --> 29:19.520
based game, you're calling it it's green and pissed, just kind of a generic alien shooter. I, this

29:19.520 --> 29:25.520
was following up off of our kind of fantasy themed Catacombs 3D. But when the idea came up that's

29:25.520 --> 29:30.880
like, well, what if we did Wolfenstein 3D? I am, you know, all the good memories for us. It was

29:30.880 --> 29:36.640
playing the nostalgia card even for us at that time, some 15 years after the that was initially

29:36.640 --> 29:44.560
released. And back at that time, I look back and kind of cringe at, like our business practices at

29:44.560 --> 29:50.160
the time, we did not sort this out very well. We kind of just charged ahead and looked around a

29:50.160 --> 29:55.920
little bit thinking, well, maybe the rights are clear, these companies seem bankrupt. We eventually

29:55.920 --> 30:02.080
ran into Silas Warner, the original author at an Apple II convention. And he was, you know, he

30:02.080 --> 30:06.160
was delighted to see Wolfenstein something. Of course, he didn't own the rights. His blessing

30:06.160 --> 30:11.760
didn't actually mean anything from a legal term. But to us, it felt good to have the original creator

30:11.760 --> 30:17.120
kind of bless our effort and what we were doing there. And in the end, we got out of it unscathed.

30:17.120 --> 30:24.480
But that was luck involved, I think. Awesome. Also, in the early times, I feel like it formed

30:24.480 --> 30:28.400
another aspect of your career, which is your proponent for open source software,

30:29.200 --> 30:32.800
both in distribution models of how software is distributed. We're pretty original in those

30:32.800 --> 30:37.760
early games. I'm wondering what, well, yeah, what's your trajectory through that? What's your

30:37.760 --> 30:43.840
thoughts about open source, say, then and now, and what connections are between them?

30:43.840 --> 30:48.640
So probably the most formative book of my teenage years was Stephen Levy's book,

30:48.640 --> 30:54.880
Hackers, Heroes of the Computer Revolution. And, you know, I read it dog-eared and it had

30:54.880 --> 31:00.160
these major themes about the hacker ethic and the kind of sharing of information and

31:00.160 --> 31:05.680
communal use of code in different ways. And this was before, you know, open source became what it

31:05.680 --> 31:10.960
is today or even before the Free Software Foundation kind of had their mission. And I, you know,

31:10.960 --> 31:17.760
coined the term. So that was, you know, that was pretty deeply in me early on. And it was fortunate

31:17.760 --> 31:24.400
that John Romero, my kind of co-programmer, had similar feelings about it, that this idea that

31:24.400 --> 31:29.520
it's just amazingly cool to be able to share the program to make it possible, where we had these

31:29.520 --> 31:33.600
memories of hacking the games, like you'd get Ultima, you'd get your Sector Editor out, you'd

31:33.600 --> 31:41.520
find out, oh, that's my gold, I want 9999 in there. And I always, I mean, I remember fervently wishing

31:41.520 --> 31:45.840
that I could look at the source code for these things to be able to go, these games that I

31:45.840 --> 31:52.080
adored that I spent a lot of time on, I wanted to see exactly how they were made. And to get into

31:52.080 --> 31:58.320
a position as a small private company, you know, where we owned all of our own IP, the ability to

31:58.320 --> 32:03.680
kind of make that earlier childhood wish that I had had come true for a whole new generation of

32:03.680 --> 32:10.160
programmers was, you know, it was very motivating for me. And it was a drawn out process inside the

32:10.160 --> 32:14.720
company, because there's a huge divide generally between the technical and the creative people

32:14.720 --> 32:20.720
here, where, I mean, it's not a judgmental side of things, but just the technical people tend to

32:20.720 --> 32:27.120
get this sharing more than most of the artists and designers do, where there's a lot more worry

32:27.120 --> 32:33.920
about chain of credit and, you know, where the work builds upon other people's work. So it was

32:33.920 --> 32:40.480
not fully understood by everyone in the company, but, you know, a little bit of it was, I was able

32:40.480 --> 32:44.560
to throw a little bit of my weight around in my position. And I know there was a little bit of

32:44.560 --> 32:48.960
hard feelings for a while that I did something that there was thinking was bad for the business.

32:48.960 --> 32:52.720
Is this related to the leak of the Quake source code? Or do you think you have a different,

32:52.880 --> 32:58.160
no, just the open source in general, where there was this point early on where we had released

32:58.160 --> 33:04.320
the tools for Doom and the ability to do all of this. And a product came out called Dezone,

33:04.320 --> 33:10.080
which was a CD-ROM just full of hundreds and hundreds of maps. And the people that did that

33:10.080 --> 33:16.880
actually made more money than we made on Doom 2, because we did not have a great royalty deal at

33:16.880 --> 33:22.400
the time. And, you know, they shoveled that out. And there was some genuine bitterness that that

33:22.400 --> 33:28.080
was possible and that to some point, degree, I had enabled that by sharing the tools. But I did

33:28.080 --> 33:33.760
feel very good that a decade or more goes on, a couple decades now. And the people,

33:34.800 --> 33:40.400
like Kevin Cloud, who is one of the artists on there, had later told me that, no, that really

33:40.400 --> 33:45.920
was all for the best, looking back on it. And we are in this almost unique position where Doom

33:45.920 --> 33:49.360
will never die. As long as there are processors, Doom will run on them.

33:50.320 --> 33:55.040
Right. Do you think there's lessons to take now? And then, like, I feel like the open source

33:55.040 --> 34:00.160
community has played a role in the last 10-year development of AI. What do you see the lessons

34:00.160 --> 34:04.080
that we should learn in thinking about how that connects to the current innovation?

34:04.080 --> 34:09.280
Yeah, I'm still surprised that it doesn't play more of a role in the game industry, where I

34:11.680 --> 34:18.080
I'm genuinely surprised that there is not more like full open source development. You see bits

34:18.080 --> 34:23.600
of it in the Minecraft mod scene where you have projects up on GitHub. But people that are generally

34:23.600 --> 34:29.680
making games, they still feel very protective of their source. And then you had the commercial

34:29.680 --> 34:37.440
companies with Unity coming in and making a very powerful product that gets the job done that was

34:37.440 --> 34:43.200
better than open source alternatives in many cases. I think Epic does a really grand thing by

34:43.200 --> 34:47.040
they have strict licensing terms, but the fact that the source code is all available,

34:47.040 --> 34:52.960
that's half the battle. I am pragmatic. I'm not a purist. I'm not going to license snipe somebody

34:52.960 --> 34:59.200
about what they're using. Just having the source code available for view is a large chunk of the

34:59.200 --> 35:05.360
value. So there's good stuff there. But in the AI space, it's much more open. And I think in the

35:05.360 --> 35:12.320
broader sense of where academia has gone, where comparing, I go back and I do read a lot of papers

35:12.320 --> 35:18.880
from the 90s and stuff. And it's papers without code, without data, unreproducible. There's a whole

35:18.880 --> 35:25.520
lot of things that are just probably are not right. And the path to having the norm, it's still not

35:25.520 --> 35:31.520
fully established and there's still pushback about it. But the world is a far, far better place now

35:31.520 --> 35:37.120
for the openness that we do have on the pace of artificial intelligence and most other science

35:37.200 --> 35:42.880
that I don't even have windows into the fact that code available on GitHub. I ideally data there.

35:42.880 --> 35:47.520
We still have too many cases of data available upon request and there's still norms that need

35:47.520 --> 35:52.080
to be pushed there because you do still have sensitive people. It's like, oh, if I release this,

35:52.080 --> 35:56.400
people will find mistakes. They'll critique my code. They'll, you know, they'll have all of this.

35:56.400 --> 36:01.440
And that's still a problem being worked through. But I have no complaints about the state that we're

36:01.440 --> 36:05.120
at now and the trajectory that we're on. I think it really is one of the great things for the world

36:05.120 --> 36:10.640
today. Cool. I want to have an AI, but before that, let me take your detour through VR for a minute.

36:12.160 --> 36:15.600
Tell me what would, did that just feel like a natural extension for you for games or were

36:15.600 --> 36:20.240
you thinking something bigger? What was, what was motivating you to move? Yeah, so I had actually

36:20.240 --> 36:26.240
tried some VR stuff back in the nineties. We had one of the really old headsets that cost $10,000

36:26.240 --> 36:32.480
and was like 320 by 240 resolution screens and pixels the size of small footballs in your peripheral

36:32.480 --> 36:39.120
vision. And it was clearly not the right time for something there. And it was interesting where

36:39.120 --> 36:43.440
I went and I said, I'm just going to go look at the state of VR because it's been two decades.

36:43.440 --> 36:50.880
Surely it's all fixed by now. And I was really surprised to find that even though technically

36:50.880 --> 36:57.520
we had good screens, good accelerometers, we had all the things that were seem to be necessary to

36:57.520 --> 37:02.640
make this work. But there were still just this handful of government contractors making headsets.

37:02.640 --> 37:08.080
They, there were $50,000 headsets in some cases and they still weren't even all that great.

37:08.720 --> 37:14.320
And that was one of those points where, you know, you can say that opportunity is the difference

37:14.320 --> 37:18.720
between what's possible and what people are actually doing. And it did seem there. It's like

37:18.720 --> 37:24.400
from a technical level, these things were now possible. And the early experiments that I did

37:24.400 --> 37:30.560
there showed that it can also be really quite compelling. Now there's, there's the obvious

37:30.560 --> 37:34.560
play there about, well, you, you make games, it's more immersive. There's the step from

37:34.560 --> 37:39.760
looking at a 2D game to looking at a 3D game to being inside a 3D game. And that is, you know,

37:39.760 --> 37:46.400
absolutely true. But I do think there's the even more powerful case about once you have a virtual

37:46.400 --> 37:53.360
interface, you know, everything that you do on screens can at least in theory be done better

37:53.360 --> 38:00.080
in a more flexible way with a virtual in virtual reality headset. So I do think there is that

38:00.640 --> 38:06.400
many billion dollar value there. You know, I spent eight years involved with it. And,

38:07.040 --> 38:11.120
you know, there's things that I'm very proud of the quest to the quest three being announced

38:11.120 --> 38:17.520
officially and coming out real soon now are great pieces of hardware. And there's an interview that

38:17.520 --> 38:25.040
I did 10 years ago in 2012 or 2013 where I'm saying this is the way I want things to go. I want

38:25.040 --> 38:30.800
this self contained device that has inside out position tracking not cabled anything no external

38:30.800 --> 38:37.760
tracking aids that can run lightweight applications internally all by itself and can connect wirelessly

38:37.760 --> 38:43.840
to PCs to go ahead and have higher performance things. And that turned out just the way I wanted.

38:43.840 --> 38:49.280
But there was a lot of things that I had friction at meta with trying to get the rest of the way.

38:49.280 --> 38:54.240
Right. I was going to I was going to ask you. I started thinking about the transition into AI.

38:54.240 --> 38:58.160
I don't know how much of that was somewhat being disillusioned by the impact that you want to see

38:58.160 --> 39:03.840
in VR. No, it really wasn't. I was still fighting the good fight as much as I could from my position

39:03.840 --> 39:11.680
in VR. But it is funny how my origin story for the AI really does go back to Sam Altman and open AI

39:11.680 --> 39:16.720
trying to recruit me when I knew nothing about the state of AI. And I was very flattered by that,

39:16.720 --> 39:22.240
that they just thought that I you know that my skill set in background could play a useful part

39:22.240 --> 39:26.560
in the company that they were building and putting together. Now, we were talking about that just

39:26.560 --> 39:32.000
before we got on before we got on stage. But had you already started to look at that point that

39:32.000 --> 39:37.680
raised your profile? No, no, just like they went after you as purely you are a brilliant engineer.

39:37.680 --> 39:43.200
You will help get us the rest of the way there. Yeah, the only I had some vague relationship to

39:43.200 --> 39:48.080
it for like there were hand tracking models and eye tracking and face tracking neural nets were used

39:48.080 --> 39:55.600
a little bit in the VR side of things. But I hadn't I I had barely touched it. I had done sort of my

39:55.600 --> 40:00.640
neural nets in C. I just spent a week just kind of writing that by myself. But that was about it.

40:01.280 --> 40:06.080
Yeah. So then so that was so they reached out to you. And then that made you think I should

40:06.080 --> 40:12.160
look into this AI stuff. Yeah, exactly. And then I looked into it. And, you know, and I was very

40:12.160 --> 40:16.480
happy they they helped me kind of get my feet under me. And like, here's roughly what you need to

40:16.480 --> 40:22.560
learn. And when I started looking into it, it, I carefully thought about it. And I reached this

40:22.560 --> 40:27.760
conclusion that this is probably the highest leverage time for someone like me, you know,

40:27.760 --> 40:33.440
an individual engineer in the history of ever, you know, that there are so many important things

40:33.440 --> 40:39.280
that require huge teams of people that require lots of management. But the gap between where we

40:39.280 --> 40:45.040
are now standing on the shoulders of all those giants, and this really transformative thing with

40:45.040 --> 40:50.160
general intelligences, applying to almost everything that we do intellectually in the world,

40:50.960 --> 40:55.760
that feels like this small modest number of things. And when I look back at what

40:55.760 --> 41:00.320
mattered in the last decade, they all feel like things that I would come up with. So

41:00.960 --> 41:07.040
it was exciting. Yeah. So I think it's kind of funny that if you look at what Edmonton's

41:07.040 --> 41:11.760
innovative scenes are, there's one on the side of games, and there's one on the side of AI.

41:11.760 --> 41:18.080
Do you think that that's you've you're traveling a similar set of spaces? Do you think that that's

41:18.080 --> 41:21.120
coincidence? Or do you think that they're related? It's interesting, because of course,

41:21.120 --> 41:26.240
famously, like Demis Hussabis at Deep Mind was also a gaming background person. I'm

41:27.200 --> 41:33.600
Yeah, it's hard to say there are certainly some aspects of being all about virtual worlds and

41:33.600 --> 41:39.200
simulations that plays a little bit into it, but it's a little bit of a stretch. I think it might

41:39.200 --> 41:45.680
be more fundamental that the people that are overjoyed at the technology of games, it's this

41:45.680 --> 41:51.040
general sort of optimism about what you can do with technology. And you've had you've gone through

41:51.040 --> 41:55.920
the feedback cycle of getting this huge reward and joy from solving a wonderful technical

41:55.920 --> 42:01.280
problem, and it causes you to delve deeper into it and push harder on it. And it does, you know,

42:01.280 --> 42:05.200
in some degree give you confidence or maybe even hubris to think that you can make a difference

42:05.200 --> 42:14.240
in some of the other areas. Cool. So this this partnership with Rich, I'm kind of curious.

42:14.240 --> 42:19.040
All disclosures. I was part of, I was also a co-author on the paper of the Alberta Plan with

42:19.040 --> 42:24.000
Rich and Patrick. But I want to ask you, have you read the Alberta Plan? So I hadn't before I

42:24.000 --> 42:28.880
started talking with Rich. So I was familiar with, you know, I had gone through his textbook,

42:28.880 --> 42:35.600
I'm, you know, twice actually, and I loved his bitter lesson paper and I had caught like a couple

42:35.600 --> 42:41.840
of his presentations. But I was several years behind on sort of the research work that he was doing.

42:41.840 --> 42:48.160
And I had it, you know, I had my, my mental model of Richard Sutton had a few things that I had

42:48.160 --> 42:53.040
some concerns about from my view of reinforcement learning from several, you know, several years

42:53.040 --> 42:57.920
back. And when I started talking with him and started going on, like all of his more recent

42:57.920 --> 43:03.520
work, I got most of my concerns there were pretty much elate. And it was kind of surprising how

43:03.520 --> 43:09.040
aligned we were on a lot of things. Okay, if you could pick one thing that you think the Alberta

43:09.040 --> 43:16.160
Plan gets right and one thing that you think the Alberta Plan gets totally wrong. So yeah,

43:17.120 --> 43:24.720
I'm setting you guys up for success. So I have concerns that the monolithic nature of the models

43:24.720 --> 43:30.720
that are talked about there may not capture some important aspects of the way human brains work

43:30.720 --> 43:37.680
as a much more distributed semi-consensus finding system. I mean, we draw these neat boxes about

43:37.680 --> 43:43.200
your state, your policy and implemented with monolithic models. You know, these are,

43:43.200 --> 43:47.840
we have Turing equivalents on a lot of these things. I won't say anything can't get across

43:47.840 --> 43:53.680
the finish line, but I do like to at least gesture in the direction of our one existence

43:53.680 --> 43:59.520
proof of in general intelligence with biology and say that there, there is much less of a central

43:59.520 --> 44:05.440
driving force behind these things that I believe they operate more at these somewhat lower levels.

44:05.440 --> 44:10.800
And I, you know, I don't have this resolved, but that's always a concern for me when something,

44:10.800 --> 44:15.360
I mean, we've, we've all stepped back away from anything smelling of symbolism, but

44:15.360 --> 44:21.440
there's still monolithic tendencies that I think may yet prove to be a little bit of a retarded

44:21.440 --> 44:27.840
for it. But the, you know, the important things about what an AI does is digest its experience

44:27.840 --> 44:32.560
into a state representing, you know, its view of the world and how it predicts things going forward

44:32.560 --> 44:37.440
and how it needs to have motivations both internally and externally imposed. These are

44:37.440 --> 44:42.880
core that, that are completely unaddressed by the, you know, the current, the current sensation

44:42.880 --> 44:47.600
with the large language models, you know, these take a very, very wide context and they throw

44:47.600 --> 44:51.760
something at it and you get an answer out the end. And there's enormous value. I don't want to

44:51.760 --> 44:56.560
take anything away from everything that's being done with that, but that's not how our brains are

44:56.560 --> 45:01.920
working. And I think that there are important things that, that it doesn't, that it doesn't

45:02.000 --> 45:07.040
encompass. And yet every lab in the world is throwing so much of the dominant share of

45:07.040 --> 45:12.640
their resources at that. So being a little bit contrarian, both of us, I think is, is a positive

45:12.640 --> 45:17.760
thing. Yeah, I'm glad you used that word. That seems like a word when I first thought of, of, of

45:17.760 --> 45:21.360
you and Rich, I was like, well, those two things I know you're aligned on being contrarian, but I

45:21.360 --> 45:27.360
don't know if you can be aligned on being contrarian. I'm going to ask you one more question, but I'll,

45:27.360 --> 45:31.200
I'll say that I'm going to open this up to the audience for questions after this last question

45:31.200 --> 45:38.800
and the microphone I believe is over here. So if you want to rush to get in line to ask a question,

45:38.800 --> 45:42.720
I'll let you do that while I ask my last question to John, which is, you know, what's,

45:42.720 --> 45:48.800
what's your future? You talked a little bit about 2030 as being a goal of, we might see some AGI,

45:48.800 --> 45:53.840
but maybe you could describe what, when you say there's a 60, 50 to 60% chance that it happens

45:53.840 --> 46:00.240
by 2030, paint me a picture of what happens, looks like. So it does appear that I, you know,

46:00.240 --> 46:06.240
I'm a person of decade-long efforts where I, there's a spectrum of useful there. There's a lot

46:06.240 --> 46:10.960
of people that have 18-month kind of passions and cycle through and you get more breadth with that,

46:10.960 --> 46:15.680
but I've been a fairly in-depth person, you know, overlapping with, you know, gaming and

46:15.680 --> 46:21.680
rocketry and virtual reality and now AI, and I fully expect this to consume a decade of my life

46:21.680 --> 46:27.920
or more. So I certainly wouldn't give up in less than a decade, and if things are going well,

46:27.920 --> 46:35.040
it'll carry on longer than that. I do intend us to stay intentionally away from anything

46:35.040 --> 46:41.200
smacking a commercial product. I think that it is important. So this is one of the things I do

46:41.200 --> 46:45.200
struggle with a lot though. I mean, there's, there are all of these things. I'm far from having this,

46:45.200 --> 46:51.520
I, you know, surety of direction or anything. One of the problems that I had a lot at META

46:51.520 --> 46:56.480
was a lot of the research I thought was actually not particularly valuable, that it was building

46:56.480 --> 47:00.960
for things, and I wanted everyone to concentrate just on product. You know, with, you have product,

47:00.960 --> 47:05.280
there's a million things you can do, just do those million things, don't look too far ahead,

47:05.840 --> 47:11.440
and yet here I am doing exactly the opposite, and I, you know, I have the arguments with myself,

47:11.440 --> 47:15.760
do I have a legitimate reason why this is different, or should I just be doing some data

47:15.760 --> 47:19.840
generation startup, you know, the, the 10 companies that want me to help them do game

47:19.840 --> 47:25.120
generation with generative AI, there's, you know, there's tens of millions of dollars just to be

47:25.120 --> 47:30.960
had at the drop of a hat to go do something like that, and I can't say with surety that

47:30.960 --> 47:34.960
that's not true, that you don't learn as much along the way building those skills there,

47:35.760 --> 47:41.680
but I, my hunch, my bet that I'm making with this company and putting my effort into it is

47:41.680 --> 47:46.560
that there are a number of things that are very important, that are not immediately commercially

47:46.560 --> 47:53.600
valuable, that contribute to the big brass ring of the big deal there, so we have,

47:54.640 --> 47:59.360
just today we spent most of the day at white rooms, at conference rooms, kind of talking about

48:00.400 --> 48:04.480
finding out what areas we're aligned on, what areas we think need to be explored,

48:04.480 --> 48:09.360
I, hashing out a little bit of differences in understanding on some of the air, the questions,

48:09.360 --> 48:14.640
but I have a, you know, a series of things that I'm building up that I hope will demonstrate to me

48:14.640 --> 48:19.600
that we're at least on the right track, but in the end, you want to get something that winds up

48:19.600 --> 48:26.160
being sort of a virtual being that you can delegate tasks to, that's not just a chat bot,

48:26.160 --> 48:32.240
it's not just a tool that you use, but it starts winding up being options in remote

48:32.240 --> 48:37.840
employees for, for companies where you have the option of like, oh, I liked working with AI Tom

48:37.840 --> 48:42.640
before I'll take five more, you know, of him, put them on the different tasks here, and that's,

48:42.720 --> 48:48.560
I, you know, at the end result, so many of the things that humans do now are mediated by

48:48.560 --> 48:53.040
computer interactions, and the pandemic really did put quite a point on that where

48:53.040 --> 48:57.360
more things than people would have thought possible five years ago really are capable of

48:57.360 --> 49:02.720
being done just through the computer intermediation. And I think that, you know, that is the opportunity

49:02.720 --> 49:07.440
to have this golden age of creative power and ability with magnifying all of that.

49:08.400 --> 49:14.320
Awesome, thanks. I didn't see anyone walk over to the microphone, which I told you,

49:14.320 --> 49:19.680
there's no hand raising just to be clear. There's no hand raising, you have to walk to the microphone.

49:25.120 --> 49:31.600
Take it away. Hello. Thank you so much. So I have a question about kind of this path

49:31.600 --> 49:37.120
towards artificial general intelligence, and I'll kind of frame it in terms of quake. So

49:37.200 --> 49:43.040
in quake, you have fast inverse square root, which was a method that no one would have thought of

49:43.040 --> 49:47.120
to do this process much more efficiently. So I'm wondering when you think towards the path of

49:47.120 --> 49:52.320
artificial general intelligence, how much of it do you think is these ideas that no one was thinking

49:52.320 --> 49:56.480
of that are just a little bit more efficient than before? And how much of it is kind of a combination

49:56.480 --> 50:00.320
of ideas that we already have and that are floating around our circles? Yeah, so there's

50:00.320 --> 50:05.200
some interesting aspects to the question of efficiency, where I have to guard my time to

50:05.200 --> 50:10.720
not spend too much time on optimization, because I love that work. It's like a vacation for me to

50:10.720 --> 50:18.320
have something so clear, just make this number go down. That's fun for me, but it's not the most

50:18.320 --> 50:24.080
important thing, because in so many ways, making something two times faster, four times faster,

50:24.080 --> 50:29.920
even 10 times faster, if it's not on the critical path, that's probably not the right thing to do

50:29.920 --> 50:35.520
now. Now, where it does matter, you get into factors of 100 and 1000, and there are architectural

50:35.520 --> 50:41.520
choices that you make that have these three plus orders of magnitude importance. And those are

50:41.520 --> 50:46.160
important that you not mess those up, that you not do something. And there are architectures that

50:46.160 --> 50:50.720
you could make that are just going to be like involve pointer chasing or something that are just

50:50.720 --> 50:56.320
not going to be good enough, because three orders of magnitude matters. You can't run your experiments

50:56.320 --> 51:01.920
in time, you can't deploy it and all that. But on the other hand, there are some architectures that

51:02.960 --> 51:08.720
may be important that just because they're not easily expressed as tensors in PyTorch or Jax

51:08.720 --> 51:14.560
or whatever, that people shy away from, because you only build the things that the tools you're

51:14.560 --> 51:20.720
familiar with are capable of building. And I do think there is potentially some value there for

51:20.720 --> 51:26.800
architectures that as a low-level programmer that's comfortable writing just raw CUDA and managing

51:26.800 --> 51:35.040
my own network communications for things, there are things that I may do that others wouldn't

51:35.040 --> 51:40.720
consider. And one of the aspects is the boundaries that we put on the things that we're going to do,

51:40.720 --> 51:46.000
one of my boundaries is it has to be able to run in real time. Maybe not from my very first experiment,

51:46.080 --> 51:53.200
but if I can't manage a 30 hertz, 33 millisecond sort of training update for a continuous online

51:53.200 --> 52:00.400
learned algorithm, then I probably won't consider it, because I do consider that necessary for

52:00.960 --> 52:05.280
even while you might grow in a simulated world, at some point people aren't going to really buy it

52:05.280 --> 52:11.440
until they're having a Zoom call with the AI and poking it in various ways and having conversations.

52:11.520 --> 52:17.760
That ability to run in real time goes against the current grain, because things are moving up to

52:17.760 --> 52:22.240
you use a warehouse full of computers, but your step time might be a second and a half or something.

52:22.960 --> 52:27.680
There are alternate ways of structuring your systems such that you use a warehouse full of

52:27.680 --> 52:33.040
computers and you get a 30 millisecond time, but you can't do that with the tools that most people

52:33.040 --> 52:38.080
are using today. So I hope that it does have some benefit, but that's one of those speculative things.

52:38.080 --> 52:42.320
My background may let me do something that might be important, but I can't say with any

52:42.320 --> 52:46.320
confidence that it actually is critical. That's awesome. Thank you very much.

52:49.360 --> 52:54.880
All right, next question. Hey there, John. So I'm a huge fan boy. I just wanted to say thank

52:54.880 --> 53:00.960
you for everything and you've done for this industry and specifically thank you for building Quake.

53:01.280 --> 53:03.280
Excellent.

53:06.480 --> 53:11.200
Quake was a very important game for me for both my social and professional life. Thank you.

53:12.320 --> 53:18.160
But my question is about who in this world has the ability to build the next great thing?

53:18.960 --> 53:25.600
In my mind, throughout history of computers, there's always been like a sole person or a small team

53:26.080 --> 53:32.720
and not some giant corporation that's really pushed things forward. Every programming language

53:32.720 --> 53:39.600
was created by like one or two people. There's Linux. Google was famously started by two dudes in

53:39.600 --> 53:46.640
a garage. Your former employer, Facebook, was created by one guy or one set of twins, depending

53:46.640 --> 53:52.000
on who you ask. And I basically believe you created a lot of these technologies on your own.

53:52.960 --> 53:59.280
But when we look towards AI and VR, there seems to be like these huge requirements for enormous

53:59.280 --> 54:06.880
amounts of data, big training sets, multi-million dollar compute time. So I ask like can a small

54:06.880 --> 54:14.160
team still build the next great thing? So there are, I would say most projects in the world of

54:14.160 --> 54:21.600
importance actually do need larger teams and resources. Building commercial infrastructure,

54:21.600 --> 54:25.680
building highways, these take large teams. And even in the software side of things, building

54:25.680 --> 54:30.960
Chrome takes a large team, building Android takes a large team. And these are super important things

54:30.960 --> 54:37.120
that the world runs on to some degree. So the default state is it probably needs a lot of people

54:37.120 --> 54:44.560
for these big things. But my point about AI or AGI right now being this potentially unique

54:44.560 --> 54:52.400
point of high leverage comes from my belief that I have this whole set of spiel about how

54:52.400 --> 54:56.960
the data is not that large, the compute is probably not that large in the larger scheme of

54:56.960 --> 55:02.720
things. Like a point I make, a year of life is a billion frames at 30 frames per second.

55:02.720 --> 55:08.080
And that fits on a thumb drive. And you can tell that a one-year-old is a conscious intelligent

55:08.080 --> 55:13.520
being. So it does not require all the data on the internet to demonstrate artificial general

55:13.520 --> 55:19.440
intelligence if your algorithm is correct. The arguments about how much compute you may need

55:19.440 --> 55:25.360
are, I don't have as ironclad positions about that, but I have plenty of reason to believe

55:25.360 --> 55:30.000
based on the capabilities of what the networks are doing today and the size of the brain and the

55:30.000 --> 55:35.760
parts that we think we understand what's going on, that it is not a warehouse full of computers.

55:35.760 --> 55:42.240
I mean, I don't think it's one node or even one rack, but in that scale over the course of the

55:42.240 --> 55:48.640
coming decade, we will factor that by, it will be another order of magnitude less. And I do think

55:48.640 --> 55:56.320
it's inevitable, but there is this period where well-healed individuals right now can potentially

55:56.320 --> 56:00.400
take a crack at this. If none of us make it, then eventually it's going to get to the point where

56:00.400 --> 56:06.320
every grad student has the resources to take a stab at these problems and the problem will fall

56:06.320 --> 56:10.960
shortly after that point if it hasn't fallen before that. But I think we're in this magical time

56:10.960 --> 56:16.640
right now where it is a golden opportunity. And I'm honestly surprised that there aren't

56:16.640 --> 56:21.840
more people. I mean, there are hundreds of people like me that were technical people that

56:23.040 --> 56:28.400
succeeded, sold companies, have the resources to go and apply this. I'm kind of surprised that

56:28.400 --> 56:33.280
there aren't more of them taking small stabs at it because even if you think you've got one tenth

56:33.280 --> 56:39.200
of one percent chance of kind of making it and getting all the way, it's kind of a Pascal's

56:39.200 --> 56:45.440
wager sort of thing where the expected value from that, if you have no fear of ruin, is still quite

56:45.440 --> 56:54.960
significant. Great answer. Thank you so much. Hello, John. So I understand that the AGI Gold,

56:54.960 --> 57:00.960
my team, a little bit abstract. So my question goes towards, if you're taking steps towards this

57:00.960 --> 57:06.640
goal, how are you making sure your, how are you measuring your progress? Are you decomposing it

57:06.640 --> 57:12.800
into some problems? Like what are the proofs for the steps? Or maybe abstractly, are you sure

57:12.800 --> 57:18.800
you're going to be making progress towards this 2030 goal? Yeah, so I'm not at all sure about it.

57:18.800 --> 57:25.920
And it is almost one of the key questions. How do you gauge whether your child is growing properly?

57:25.920 --> 57:30.640
Do you have things that are crude like the basic responsiveness tests about things? You track

57:30.720 --> 57:36.960
pupil movement, but how do you tell at the earliest level before a billion steps have gone by what

57:36.960 --> 57:42.960
level of cognitive processing is going on there? And I have like my angles on this involve, you

57:42.960 --> 57:47.920
know, like moving foveas and centers of attention. And there's low level things that I can look at

57:47.920 --> 57:52.960
and say, I believe in intelligence should be following these patterns. And I have these indirect

57:52.960 --> 57:58.640
measures that I can make of it. But I dearly wish that I had better objective measures because

57:59.600 --> 58:05.680
it's undervalued how critical benchmarks were to the pace of this last decade of terrific AI

58:05.680 --> 58:12.080
progress where turning things from a discussion section into numeric values. And yes, there's

58:12.080 --> 58:16.720
downsides and people talk about grad student descent of the problems with leader boards and

58:16.720 --> 58:22.400
different things like that. But overall, it's been enormously valuable. And I, you know, I do worry

58:22.400 --> 58:27.520
a little bit even with the LLMs that I don't really buy a lot of the measures of how they're

58:27.520 --> 58:32.960
benchmarked against each other. And it's an even harder problem for AGI. So I wish I had

58:32.960 --> 58:37.600
a better answer to that because I think it's important. But I think that there's at least

58:37.600 --> 58:43.040
directions that we could be following that we feel pretty good about. But it's entirely possible.

58:43.040 --> 58:47.600
Years could go by and it turns out it was the wrong direction, but it's still worth the try.

58:48.400 --> 58:49.040
Okay, thank you.

58:52.240 --> 58:57.120
Yeah, I had a question about the kind of AGI you visualize building at Keen. So when I think

58:57.120 --> 59:01.200
about AGI, I think there's an embodiment, there's an actual robot functioning in the real world.

59:01.200 --> 59:04.880
So are you thinking about something which is animal like or human like, or is it come

59:04.880 --> 59:10.560
completely virtual? Alright, so I'm not a fan of robots, which puts me at odds with Joseph here.

59:11.920 --> 59:17.280
And, you know, it comes from, of course, I'm the virtual reality guy. So I believe in simulation

59:17.280 --> 59:23.680
and the general ability to, you know, to do valuable things in simulation. While robotics puts

59:24.080 --> 59:29.360
much of the value of working with robots is this discipline of saying you're going to be real time,

59:29.360 --> 59:34.160
you're working with reality, you can't just get slower and slower and make your model better by,

59:34.160 --> 59:39.440
you know, by scaling it in a way contrary to time. So I think that that discipline can still

59:39.440 --> 59:44.160
be maintained in a virtual environment. I am. And I think that's the primary benefit. And there's a

59:44.160 --> 59:50.880
lot of downsides to robots where I did spend over a decade building rocket ships and that drives home

59:50.880 --> 59:56.320
sort of that, you know, the cussedness of physical things. And the less you can be forced to work

59:56.320 --> 01:00:06.000
with physical objects, the better in most cases. So I think in general, I don't expect us to be

01:00:06.000 --> 01:00:11.200
doing anything robot based. Now the question of exactly what simulated environment you have,

01:00:11.200 --> 01:00:16.800
there's a broad range there where like right now I'm working with sort of this 2d infinite movie

01:00:16.800 --> 01:00:20.960
wall of like moving around looking at different things. And there's a degree of agency there.

01:00:20.960 --> 01:00:26.000
And you could learn about 3d environments and you can put 3d games inside there. But to me,

01:00:26.000 --> 01:00:30.400
like it's an open question, should it be a virtual reality environment of instead of a

01:00:30.400 --> 01:00:35.200
virtual 2d wall? Should it be a physical space where they have to like walk around essentially

01:00:35.200 --> 01:00:39.600
to look at different things? And I don't know, I'm leaning towards the simpler salute, the simpler

01:00:39.600 --> 01:00:44.720
possibilities right now. But if it turns out that I wind up making a full fledged, I am, you know,

01:00:44.720 --> 01:00:49.280
game engine rendered virtual world, I wouldn't be at all surprised if that's where we wind

01:00:49.280 --> 01:00:59.920
up in a year or two. Thank you. Thanks John. So it's very special to have you joining our community

01:00:59.920 --> 01:01:07.360
here in Edmonton. And when Rich emailed you, you must have thought to yourself whether you wanted

01:01:07.360 --> 01:01:15.440
to start this partnership. And obviously you did. So I was wondering about what were the behaviors

01:01:15.440 --> 01:01:21.920
that you saw in this community that wanted to make you join our community? And more broadly,

01:01:21.920 --> 01:01:26.640
like what should we hold on to? Because we might have taken it for granted because we've been

01:01:26.640 --> 01:01:32.160
fostering for so long. Yeah. So the, you know, initially I did not get my hopes up. I was just

01:01:32.160 --> 01:01:37.120
like, Hey, it's cool to be having a communication with Richard Sutton. And I was going to try to

01:01:37.520 --> 01:01:43.680
my helpful self as much as I could be. And the fact that we did wind up hitting it off well enough

01:01:44.320 --> 01:01:50.880
that it's almost remarkable how many areas of overlap that we've got in terms of the way we

01:01:50.880 --> 01:02:00.320
look at this. And, you know, in some ways, both me and Dallas and Rich up here in Alberta,

01:02:00.320 --> 01:02:05.520
it is, this is not the center of gravity of artificial intelligence research. I, you know,

01:02:05.520 --> 01:02:11.200
we are in our own way in the wilderness. And I think there's some commonality that came from that

01:02:11.840 --> 01:02:19.600
where there is, you know, there is this kind of fashion in research and especially in the

01:02:19.600 --> 01:02:26.800
commercial side of it. And the current looking at things like the reinforcement learning and the

01:02:26.800 --> 01:02:32.160
real time online continuous learning, these are not the current fashion for things. I, you know,

01:02:32.160 --> 01:02:38.240
great strides are being made with large language models, large batch training, slow steps, inference

01:02:38.240 --> 01:02:44.240
only. And, you know, and that's all great. But everybody should be aware that this is not the

01:02:44.240 --> 01:02:49.680
be all end all, it doesn't solve all of the problems. So somebody's still got to be looking for

01:02:49.680 --> 01:02:55.360
the remaining answers, you know, we don't have them all. So being willing to go against the grain

01:02:55.360 --> 01:03:01.280
and to, to step a little bit outside and have a lot of people say, Why are you working on that?

01:03:01.280 --> 01:03:06.960
You know, that's not the, you know, the mainstream. That's not where you can go impress the VCs or

01:03:06.960 --> 01:03:13.200
whatever. But at some point, somebody has to solve some of these problems. And, you know, the willingness

01:03:13.200 --> 01:03:18.960
to go ahead and do that, because you think that it's the right long term solution. It's I, you

01:03:18.960 --> 01:03:24.080
know, I do tip my hat to the kind of the academic virtues of you are trying to find truth, you're

01:03:24.080 --> 01:03:28.160
trying to find, you know, the knowledge and the way to solve these problems in the hopes that

01:03:28.160 --> 01:03:32.560
then they will be applied into the world to produce great value across the different areas.

01:03:34.560 --> 01:03:38.800
But yeah, the biggest thing right now is we're both outside the mainstream, but we both have

01:03:39.360 --> 01:03:45.520
conviction and we have, you know, reason to believe that it's a profitable direction to be

01:03:45.520 --> 01:03:50.880
pursuing. And this is far from the only direction. Again, I'm surprised that there aren't more efforts

01:03:50.880 --> 01:03:55.760
like this going on, rather than having the 10th company training their own large language model

01:03:55.760 --> 01:04:00.560
to compete with something. There are other interesting problems that may wind up being

01:04:00.560 --> 01:04:07.360
even more important. And being excited about the general technology while kind of picking a path

01:04:07.360 --> 01:04:14.080
that's not just following somebody else's trail in the specifics, I think is important. You know,

01:04:14.080 --> 01:04:22.560
the world needs more people like that. Thanks. Hello. I'm very excited to hear about this

01:04:22.560 --> 01:04:28.720
partnership that you have here. And so one of the things that I like about it is that there's a

01:04:28.720 --> 01:04:34.400
lot of blue sky research that you sort of have in mind, right, that for the next seven, 10 years,

01:04:34.400 --> 01:04:41.760
whatever, there's going to be a lot of focus on these fundamental things, some maybe online,

01:04:42.320 --> 01:04:50.000
real time algorithms to do certain things. But you also emphasize that there is no need for a

01:04:50.000 --> 01:04:55.280
tangible product in, let's say, in this timeframe, right? So all this sounds very much like an

01:04:55.280 --> 01:05:02.240
academic lab. And while this is certainly not one, right? And perhaps, so my question is out of

01:05:02.240 --> 01:05:07.040
curiosity that, is this something you are specifically guarding against? Because like deep

01:05:07.040 --> 01:05:12.640
mind and companies like this also started with a similar perhaps intention, right, that lots of

01:05:12.640 --> 01:05:18.640
blue sky research, solve intelligence, use that to solve everything else. But now, maybe it's not

01:05:18.640 --> 01:05:23.120
exactly the same anymore, right? So are you specifically guarding against that? Or is that

01:05:23.120 --> 01:05:28.960
something you don't really think about? So I am unashamedly a capitalist. I, you know, I do,

01:05:28.960 --> 01:05:33.840
I think that this could make me a trillionaire if everything goes well, it's worth, I, you know,

01:05:33.840 --> 01:05:38.320
it's worth kind of aiming for some things that I, you know, producing value, I think it's good for

01:05:38.320 --> 01:05:43.760
the world. I mean, I deeply believe that building commercial enterprises is most of what has made

01:05:43.760 --> 01:05:49.760
the world what it is today in a very positive sense. So, you know, and I don't have the academic

01:05:49.760 --> 01:05:55.360
background, I am, you know, I respect, I recognize I'm standing on the shoulders of all of the academic

01:05:55.360 --> 01:05:59.760
work that's been done before. And it was funny because I talked to, I'm, you know, ahead of a

01:05:59.760 --> 01:06:03.520
university one time, and I was kind of mentioning how I feel a little guilty being a commercial

01:06:03.520 --> 01:06:08.240
company that is built on lots of this research. And he said, you know, your tax dollars have paid

01:06:08.240 --> 01:06:12.720
for a whole lot of this research, don't feel bad at all. You know, this is the point of all of this

01:06:12.720 --> 01:06:19.600
is to let people try to build on it. So, yeah, it's, it is a commercial effort. And part of that

01:06:19.600 --> 01:06:25.760
was a focusing tool for myself, where I spent kind of the prior few years in what I call Victorian

01:06:25.760 --> 01:06:30.720
gentleman scientist mode. I was kind of styled like Darwin or Babbage, where I'm a rich guy that

01:06:30.720 --> 01:06:35.840
can buy all the scientific tools that he needs and can kind of have my backyard laboratory for

01:06:35.840 --> 01:06:41.520
things. And, you know, and I learned a whole lot. I spent the time kind of going through this

01:06:41.520 --> 01:06:45.920
extended larval stage about finding out what everybody is up to and getting myself

01:06:45.920 --> 01:06:51.120
up to the modern standards there. But it always gave me an out, you know, just at that level,

01:06:51.120 --> 01:06:55.520
I could think of it as like, well, it's almost a hobby. I could just quit at any time. I could

01:06:55.520 --> 01:07:01.280
divert my time any way that I want. And I had had several, several different organizations

01:07:01.280 --> 01:07:06.640
pestering me about form a company, take our investment money. And I didn't need it for the

01:07:06.720 --> 01:07:12.640
money. But I looked at it as a focusing tool, where I have in many ways an overactive sense

01:07:12.640 --> 01:07:19.280
of responsibility to, you know, to investors, you know, it, it killed me at, at meta, just seeing

01:07:19.280 --> 01:07:24.240
all that money going out, I went and like, no, turn it into profitable businesses earlier.

01:07:24.240 --> 01:07:29.600
So that is an aspect of it. And then hiring employees means it's like, okay, now I've got

01:07:29.600 --> 01:07:35.520
people's paychecks that I am responsible for. So it makes me focus better on the, you know,

01:07:35.520 --> 01:07:41.440
on the tasks at hand as well as actually bringing the resources to bear. So yeah, in some ways,

01:07:41.440 --> 01:07:48.000
it's a pure research play. And, but at some point it turns into if it works products that

01:07:48.000 --> 01:07:57.440
really literally reshape the world. Thank you. Yeah, so I think you touched on this a bit earlier,

01:07:57.440 --> 01:08:05.200
but I'm sure many people have tried to hire Dr. Sutton before and failed. And so, yeah,

01:08:05.200 --> 01:08:10.080
like, could you talk a bit more about like, why you succeeded where no one else could and was it

01:08:10.080 --> 01:08:14.800
like fully attributed to being aligned or? Yeah, so I still, you know, I still wind up

01:08:14.800 --> 01:08:19.840
being happily surprised that I'm here today with this because I, you know, a couple of times I

01:08:19.840 --> 01:08:24.000
just said, I know you can write your own ticket anywhere, you can go to any of these companies

01:08:24.000 --> 01:08:27.680
and everybody would be happy to have the father of modern reinforcement learning just

01:08:28.560 --> 01:08:32.400
in their collection. And for some big companies, it is almost like a Pokemon collection,

01:08:32.400 --> 01:08:36.880
you know, collect your favorite researchers and, you know, make sure they're on your team and your

01:08:36.880 --> 01:08:43.120
deck. But I, you know, I think, you know, part of it is that we want to actually do these things.

01:08:43.120 --> 01:08:48.560
It's not about prestige and being out there and kind of staking a claim. We want it to exist.

01:08:48.560 --> 01:08:54.880
We want to build it. I am, and I think that there is a sense that a smaller team, there's, I mean,

01:08:54.880 --> 01:09:00.160
there's all sorts of drags and friction that you get in big teams. It is wonderful to have a 10,000

01:09:00.160 --> 01:09:05.360
GPU cluster that you could go ahead and just kind of get time on whenever you feel like it.

01:09:05.360 --> 01:09:10.560
But I am, again, I've got enough options for, you know, for doing things like that,

01:09:11.440 --> 01:09:16.880
that I don't think that's going to hold us back. And the ability to have a purity of focus, I think

01:09:16.880 --> 01:09:23.680
is important. But I'm, yeah, I'm, I'm still, you know, really very happy that it all worked out this

01:09:23.680 --> 01:09:33.200
way. Thank you. Hello. Thank you so much for the talk, John. It was fantastic. So my question

01:09:33.200 --> 01:09:41.520
goes back to the discussion about game development and open source. And I would like just to hear

01:09:41.520 --> 01:09:48.400
your opinion about what is, what do you see as the future of open source software in the video game

01:09:48.400 --> 01:09:56.480
industry, especially since it's a world full of full of IPs and proprietary platforms?

01:09:57.440 --> 01:10:03.040
So there is, you know, right now is an interesting inflection point because Unity had their whole

01:10:03.040 --> 01:10:08.240
change their terms of services. And this is making a lot of people reevaluate what they're doing.

01:10:08.800 --> 01:10:14.000
And it's been a long time since I was really close to the work being done on the open source engines.

01:10:14.560 --> 01:10:20.560
But, you know, I believe it's fair to say that Unity with all their employees, they do a lot to

01:10:20.560 --> 01:10:26.000
make it a comfortable, cozy development experience. They do provide a lot of value. And you do get a

01:10:26.000 --> 01:10:31.120
lot of open source crusaders that just don't acknowledge the value that the commercial companies

01:10:31.120 --> 01:10:35.600
bring to the table when they build their software. Because there are a lot of blind spots that the

01:10:35.600 --> 01:10:40.240
open source teams, it's, it's weird how the DNA of the people that are willing to work on open

01:10:40.240 --> 01:10:44.880
source wind up with certain important blind spots in the kind of applicability of their

01:10:44.880 --> 01:10:50.000
software and usability. You make super powerful tools, but, you know, they're not very comfortable

01:10:50.000 --> 01:10:56.080
and it winds up making them not the right call for a lot of people. But like I said, I very much

01:10:56.080 --> 01:11:00.720
respect EPICS in between position where even if you don't pay them a cent, you can read every line

01:11:00.720 --> 01:11:06.560
of code and learn from it and understand how things work. You can pay reasonable license fees.

01:11:06.560 --> 01:11:11.280
Having competition there is great, you know, letting people choose. And I always thought that

01:11:11.280 --> 01:11:16.720
the trade-offs between Unity and Unreal was a nice market balance to have. And like in virtual

01:11:16.720 --> 01:11:22.400
reality, 95% of the developers wound up with Unity because of just the crowd that was attracted.

01:11:22.400 --> 01:11:27.520
But it was always expected that if you need serious control, you'd then go with Unreal and

01:11:28.080 --> 01:11:33.360
you could write things at a lower level. So there's an opportunity now, I think there will

01:11:33.360 --> 01:11:38.800
inevitably be some larger move after Unity stumble here to open source projects.

01:11:39.840 --> 01:11:46.240
I don't expect it to be a tsunami that like accelerates and takes over everything because

01:11:46.240 --> 01:11:51.120
there are just so many of these grubby business things that are just not fun to do on the open

01:11:51.120 --> 01:11:58.240
source projects that I, you know, I could imagine a scenario where somebody like, you know, Meta

01:11:58.240 --> 01:12:03.680
gets behind open source things and winds up helping do some of the unpopular, unfun things

01:12:04.880 --> 01:12:11.840
for their own kind of self-interested reasons that could be positive. But I think I'm resigned to

01:12:11.840 --> 01:12:19.360
a slow pace of migration towards more open source tools. Again, at this point, 30 years back,

01:12:19.360 --> 01:12:24.400
I'm disappointed that we aren't further along there relative to like compiler tool chains and

01:12:24.400 --> 01:12:29.440
things where the world is a better place for having those be almost universally open source.

01:12:29.440 --> 01:12:34.000
And we probably could have been there on some game engine type things, but I don't think the

01:12:34.720 --> 01:12:39.520
fundamental reasons why we're not have changed. So I think it's going to be modest changes going

01:12:39.520 --> 01:12:45.280
forward still. Thank you so much. I'm being told we're almost out of time and we have a line of

01:12:45.280 --> 01:12:49.360
six people. So I'm going to go in a lightning round mode and you get 10 seconds to ask your

01:12:49.360 --> 01:12:54.320
question. I can actually hang around a little bit after the actual official end.

01:12:55.840 --> 01:12:59.840
Okay. Well, so I worked with Mike and Rich at DeepMind and actually some friends and I left

01:12:59.840 --> 01:13:04.720
DeepMind to create a startup here in town using AI to make AI in video games. So first of all,

01:13:04.720 --> 01:13:08.640
thank you for not shooting to make a commercial product in the next couple years in the AI and

01:13:08.640 --> 01:13:13.920
game space. But my question is more like, I know Rich has a really strong commitment to open research

01:13:13.920 --> 01:13:16.960
and it sounds like you embraced that as well with the hacker mentality. So

01:13:17.760 --> 01:13:21.440
what are the plans for Keen to like share your insights, your breakthroughs, maybe even your

01:13:21.440 --> 01:13:28.800
code? So this was one of the significant conversations that we had. And Rich does

01:13:28.800 --> 01:13:33.600
understand the importance of commercial businesses and that there's reasons why these are

01:13:33.600 --> 01:13:40.320
fundamentally important and the incentives matter. And there's a talk about like, yes,

01:13:40.320 --> 01:13:45.920
I did champion a lot of releases of different things and I would, there's certain technologies

01:13:45.920 --> 01:13:50.960
that I'm happy to have us publish. There's other things more like around experiment design and

01:13:50.960 --> 01:13:56.960
architectures that I probably don't want us talking about. And it's going to be in a situation like

01:13:56.960 --> 01:14:02.320
if, if somebody wants to write a paper about something, we'll have a conversation about it.

01:14:02.320 --> 01:14:06.640
And if it's, you know, if it's something at the very tactical level, like, hey, this is an

01:14:06.640 --> 01:14:11.440
interesting way to do an optimizer. This is, you know, another way to calculate your, your,

01:14:11.440 --> 01:14:17.200
your TD assignments, whatever, that's, that's probably fine. But here is the architecture

01:14:17.200 --> 01:14:23.520
of what our proto AI, AGI looks like that's probably not going to be talked about. Because

01:14:24.320 --> 01:14:28.560
all of these are going to be things that we see it today where all of the major laboratories,

01:14:28.560 --> 01:14:34.480
they can reproduce anybody else's work with a couple sentences of direction and a couple weeks of

01:14:34.480 --> 01:14:39.600
time. And I think that's going to be the case for AGI as well. When, you know, when it does all

01:14:39.600 --> 01:14:44.320
get solved, the textbook from the future is going to have a relatively thin chapter about this is

01:14:44.320 --> 01:14:51.600
actually all that's really required built on the baseline to make it happen. So yeah, there's some

01:14:51.600 --> 01:14:55.200
sense of secrecy. And then there's real concerns about any real company is going to have employee

01:14:55.200 --> 01:14:59.120
turnover, they're going to walk out the door, you know, NDA or not, they're going to know what the

01:14:59.120 --> 01:15:05.760
things are. So there is a large part of this gamble on even if all of this value gets developed,

01:15:05.760 --> 01:15:10.720
much of it may dissipate into the broader world, which is great for the world, not so great for

01:15:10.720 --> 01:15:16.800
my investors, but we're all still collectively willing to take that risk. Thank you. Okay,

01:15:16.800 --> 01:15:22.080
it looked like lightning round didn't work. So I'm going to call it here because we're out of time,

01:15:22.080 --> 01:15:27.040
but John has very generously offered to answer some questions. If you have some, we'll make

01:15:27.040 --> 01:15:32.320
sure we start with this line first. Yeah. But let's first thank John for telling us so much

01:15:32.320 --> 01:15:47.680
insight about his past, present and future. All right, thanks. Do I have to vacate the stage?

01:15:47.680 --> 01:15:59.040
Yeah, I don't know what we do next. Thank you. Bye.

