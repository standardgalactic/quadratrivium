{"text": " Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just have a welcome you to tea. We do this each year and it's very informal and I welcome you all to sign up and participate and to ask questions and you know we're here just to talk about science and ideas and exchange points of view. So think of it as a community, enjoy the tea, have a cookie, it's all good. The other thing is Thursday, Thursday I think we still have an open slot and we could have Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that will work out, I'm pretty sure. Okay, so the topics are really intelligence and how do we understand how the mind might work and could work and today my topic, really the topic is model-based reinforcement learning. Model-based reinforcement learning is really it's sort of the whole enchilada. We try to get all the pieces present, model-based, model-free, make understanding of the world, be able to reason or plan with it and how many people here have just raised your hand if you've heard of the phrase model-based reinforcement learning. How many haven't, how many actually, let me say it a little differently. How many of you just think it's the model-based reinforcement like the most important thing in AI, like I do, or should I get a little bit of an argument from authority for it? Okay, I spent that long talking about it, I might as well give my, I just threw in a slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon talking about predictive learning, understanding the world, Yashio Benjo talks about the most important step in AI is to make predictive causal model of the world. This is a thing that's coming around at last and this is our topic, our topic is model-based reinforcement learning and it means making a model of the world, I can use this word model to mean model of the world, how the world works, I'll only use the word model as a world model, model of the MDP, the Markov decision process, the transition dynamics of the world and model-based reinforcement learning, we learn the model, we learn the model and we also then use the model to plan or reason about what to do. So this is, we call this planning and planning proceeds by, you imagine states, you look ahead from the individual states to see what might happen and you back up to improve your policy or your value with those states that you're imagining might happen and then this is how you figure out, decide what to do. Okay so it's decided this idea has been around for a while, I'm going to talk briefly about the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first proposed or first explicitly proposed that planning and learning would be radically similar. So there's some reinforcement learning algorithm that's interacting with the world and then it's also learning the world model which is another box that you can plug in place of the world and instead interact with the model for a while, in that sense, and so you're interacting with the model and you're saying what if I do this, the model says that would happen, you get this reward and you learn from that just as if it had really happened and so then this way the planning or the logic and the reasoning, certainly the planning is radically similar to just acting in the world, just acting in imagination. So that's the first half of the idea, the second half of the idea is that these things are all done simultaneously, so planning and learning and executing, they're all been going on all the time, so you're always interacting with the world, getting new experience, you can do a model for your reinforcement learning with that bit, but you also use that experience to make the model and the model through planning and that also affects your value function and your policy and makes you do better. Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm that's kind of useful to complement those diagrams, those visual things with a little bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from the book, so basically your Q is your action value function and model SA is your prediction about what would happen if you were in state S and did action A and you initialize that and then as you go through life, you're just doing this loop over and over again, you're looking at the state you're in, you're picking an action in it and set B somehow, then you take that action, send it to the world, the world gives you back a result, R, the reward and the next state S prime and so then you do model free learning with that little four tuple of experience, state action, reward, next state, so here we're doing Q learning in step D to update the action values and then we update the model because we've seen what will happen for that state and that action, that this will happen and this is where we're assuming a deterministic world because really many things might happen if you did that action in this state and we only saw one thing and now we're going to just overwrite the model so that I'll predict always that that would happen. So the thing about DynA, particularly the first instance of it in 1990, it's a very simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully express all the possibilities. We're going to talk about today's, how you can go and consider more possibilities. Anyway, after you do, now we've got as far as learning the model and then step F is where you do the plan. So by the plan, you just imagine some state, maybe randomly from something you've seen before, imagine an action, you ask the model what would happen if you were there and then you do that same step, same, it's the same as D, it's Q-learning, but here now we're doing Q-learning on these imagined things and there we're doing Q-learning on the real thing. So that's the basic idea of DynA, I'm sure many of you have seen that before. Now, maybe you've seen the demos of it before, so here's the standard demo where we have a grid world, we have a start state down in that corner and we have a goal state where you get a plus one reward and the actions are to move up, down, right and left, so four actions from each state, the states are all represented tabularly, so like this is state 34 and this is state 92 and the model says, oh, when I go action four in state 34, I end up in state 92, that's what the model is learning and because it has the model, it is very quickly learning how to get from start to goal efficiently and it's actually able to learn about states that it's not in and this becomes most apparent if we pick up the goal and move it to a different place, so of course once we've moved it, the agent will go back to, it'll go here again where it used to be and be disappointed so to speak and then it has no idea where it is, it's built a model but the model says that there's no goal up there, as the last time it was there, there was no goal, okay, but eventually it will stumble upon the goal again and then it will be very quickly able to plot a path, so watch when it first finds it now, here it will eventually see it's learning the right way to go and it's learning a good path very quickly because it has a model, okay, so that's like tabular dyna, okay, and today we want to talk about the extensions of this, open questions in it, yeah I guess I didn't even say it, it's open questions and planning, I'm not going to tell you the answers, I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic dynamics, what you saw was just deterministic dynamics, we assume that the world always went the same way but you could instead of overwriting what the model says in the state in action pair, you could start to collect a list of all the things that might happen in their probabilities and then you could sample that and you could do exactly the same thing, you could add function approximation, now function approximation, I'm going to talk about it, but it's really a spectrum, a range of degrees of function approximation, so what we saw was tabular, I call it tabular and that means every state action pair is treated totally different from every other state action pair, there's no similarity between them, there's no generalization and so there's just a big table and I store things in that state action pair and really in real life certainly in computer go and in Atari games and in any robotics application you have to generalize from one state to another and that's, you know, you never see the same state twice, okay, but we start with the tabular and you think you're used to your deep learning, that'd be a nonlinear system, you could also have linear things that turn out to be quite important and even the aggregate case, state aggregate means you still have a table but there could be many states fall into the same table entry, okay, so you're aggregating states and treating them all the same, this is a nice case actually, we can get theoretical results for it that we can't get for the other cases, okay, so there's function approximation, we want to do that in some sense that's our bread and butter, we just generalize the table to a function approximator like supervised learning system, but let's go on, I want to extend it quite far, so let's list the things and the next big thing is partial observability because really the world doesn't even give us states, it gives us observations, it gives us things that happened, things that are senses, it doesn't tell us, we don't know the full state of the world, we just get an observation and now, we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest, the rest of it is basically the kind of thing we've talked about so far, we have the world, we have our policy and our value function and we're interacting with the world, we're getting rewards and we're getting some observations and then that red box is turning into a state and so once we get past the red box, it's just like before, we had a state and we can make the, send that state up to the model to be learned and we can send that state up to the planner to propose things and the planner will do some adjustments to the policy and value function just like the reward does, but it will come from the planner and this will be the common path between model free learning and model based learning. Okay, so the thing in the red box, this is the state update function which just says that the agent has to take responsibility for learning some mapping from the observation, the last state and its action to what it's going to use as its state, it stays as a summary of the past, it's good for making decisions and predicting the future and so the state update function is called U, it's exactly this thing and it's got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U box is given and I'm going to mostly assume. Anyway, when you talk about changing the state feature vector or the state representation, that will be the state update function. Okay, that's a major extension and at the same time it's almost done because I've got some kind of a box and so I've got some kind of a box, produces some kind of a state representation and my methods always, at least once I did the second step, a function approximation, they always were able to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect it is, I will be able to do a certain well with it just as I would be able to do certain well with a certain feature vector representing the state. Okay, another big step is that if we do it right, it doesn't we can separate it from all the other issues, just like we have here, which is to do temple abstraction. Really if you take your model of the world, your model of the world is not if I'm in this state, I do this action one step later, I'll be in this other state, it's really more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big multi-step events and we're actually the kind of learning and kind of reasoning and planning we want to include, should include all those sorts of things. So there is a theory of options which enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases. Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in a little bit. So rushing along, I'm talking about open questions in model based reinforcement, so I have to say a little bit what's closed, what I'm not going to consider open. So these are my settings, these are my presumptions, and I say closed-ish because like lots of people will disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning should be online, incremental, like asynchronous dynamic programming and like the dynasy system you've just seen. I think that models and planning, they should be state to state. So many people in the literature make models and do planning where they include the observations in the plan. You're like, if I did this then I would see that and then I would, no, no, it should just be state to state. And if you think about it just a little bit longer, really it's obvious you've got to be state to state. You don't want to have your observations which are tied to the single time step and tied to state update. You want all those to be separated. Okay, now of course it's not really state to state, the state feature to state feature, state feature vector to state feature vector, and that will be where the feature vectors are coming from the learn state update function that we mentioned earlier. Okay, closed models, planning, they should be temporarily abstract, there should not be one step, they should be used based on options. Also, search control. Search control is how you decide which states to think about in imagination, and that's essential for your plan to be efficient. If you think about stupid states, you'll just learn stupid things, but if you can just select the key states to think about to form your plan, then you can be efficient and effective in your planning. And the last thing is that, so these are sort of like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it directly. And similarly, we need some problems to in order to structure the learning of the options and the option models. Okay, let's go on to the open questions. The open questions. Number one, should the model, what is this model, should it generate sample states, which I suggested, or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go through it in detail. But how should planning be done with average war? This is the other big thing that I hope to cover today, average war. And then all the other things I won't, I won't probably won't get to. But let's look at, so let's let's go to how we put function approximation in here. And what is the content of the model? So just a little bit of terminology. Of course, planning proceeds by using the model to look ahead, imagining something that might happen. Each one of these imaginings of the future from a state action pair is called a projection. I'm going to use this word projection. This is where we imagine a future. Okay, and then after one or more projections, we compute something. And then we back it up. That's called a backup. And this goes on forever. Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm looking at these state action pairs and imagining might happen. So what would be the projections in this picture? Uh, Schmach, where's the projections in this picture? At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then everyone can can just do it. Where are the projections? The projections are where you're imagining the future from a state action pair. This is my test to see if you're actually following my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state. This is a state action pair. Because you can tell because it has an action on it and it comes from a state. So it's a state action pair. And then you imagine the future, the projections are here. There are three projections. We're looking ahead, all the actions I might make, and I project what would happen. And I figure out how good they would be. I take the max and I back it up. Okay, so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick. Well, it's, it's from, from the state action pair to where it goes. This, this part is the projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here? Say that again. You should have, you should be sure by now. So there are the projections. So this is, this is a long skinny sequence. This is a skinny backup. So we're probably sampling, instead of doing all possible actions, we're sampling an action, we're sampling a next state, we're sampling an action after that, and we're sampling an x state after that. But these two are the, are the projection parts. The other parts are parts that the agent is doing. The agent says, suppose I do this action, and then ask the model, what would happen, the projection? Okay. And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves, always goes from the leaves to the top. No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part, anyway, is definitely your choice. It's not an imagination about what the world might do. Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one, the projections are here. Now these two states, they might be the same state. Maybe I imagined this one, and I said, huh, now what, what if I was there and I did this one? So that, so that if they were the same state, you might imagine doing the same thing. But in fact, by definition, as a backup, they are separate backups, and you'd get these two, two together. And if you did this one first, and then you did that one, then it might be a similar effect, because you would change the value, estimated value of this state, and then you change, use that to change the estimate of that one, that one. Okay. Okay. I have to keep going. Good. So, this is the biggest question. What is the output of a projection? Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide, what is it really? Because we're using a function approximation, and our states are probably real valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in this, I'm imagining being in the state, doing an action, but the world is stochastic, many things could happen. So one thing I could do is I could represent the whole distribution of all the things that could happen. Okay. This isn't totally crazy. People are doing this. This system called Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are, are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated, they're, they're going to, we want methods that are general and scalable and proximal. And so that we, can we do this without committing to a very specific form for the function approximator? I am skeptical that we can do this. Okay. This is the first question, the first open question. I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution. But if, if you did this, then how could you roll it out? How could you iterate it? How could you go to another step? Because you'd go from a state action period to a distribution. Here's a messy distribution thing. And then how could you go from there? How could you roll on to the next projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course, you can always sample that distribution. And then you have a sample model. So you get the state action pair and you get a sample of the next state. And then you can roll it out. You say, okay, there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there? And you can go on. But you really have many of the same problems because you have to learn the distribution because you have to, you have to generate a sample from that distribution and you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility. But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from which you sample. And you notice that now planning has become stochastic, because there, you would have to do many samples like in Monte Carlo tree search of that next state in order to average over them and get an expected expectation. Whereas up here, it was deterministic. I get you the whole deterministic distribution. Okay. And then there's the third case, which I like the best, which is that you learn the output of a projection is an expectation, an expected feature vector. Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out because you get this exp, you know, this average of feature vectors for the next state. And it's straightforward to learn this expectation models, because that's what all of our Algorb zoo, they learn expectations. And, but in general, we've lost information. If you only have the expected next feature vector, instead of the whole distribution, you lose special things. But, but there's this important fact, mathematical fact that in the special case of linear value functions, you actually don't lose anything. So I, I guess I don't have time to do this equation. So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically, we can show that if you do, you're doing the update with the distribution model, and you can write up mathematically, this is what it is. And then just through a few steps, you can prove that you get exactly the same thing if you, if you use an expectation model. So here, this is, this is the probability distribution of the next states. Here we have the expected next feature vector for the state, and the, the action or option O. And you can show that these are equal in the special case, where the value functions are linear. Okay. So this is open questions, open questions. So this is just a proposed strategy, is that with linear value functions and expectation models. And so, you know, I just want to talk a little bit about this question, should the value function be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question of moving the work around, whatever you do, you have to learn the nonlinear relationship. And the strategy of, of an expectation model is that the nonlinear work is done in the state update function. So it puts the burden on the state update function. And so here I want to talk about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this picture from the book of the shape of all the backups. Now, these are the shapes of the backup really. This side is planning, that side was seen as not planning. You know, just TD and Monte Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning could could could involve a short, not just the wide backups of dynamic programming and and tree search or exhaustive search. But you can do the skinny backups. And so my my long short start is that I'm going to those are the those are the projections is that I'm going to argue that really everything can be done with the smallest, the smallest backup, just looking ahead from sample one action and sample one expectation outcome. And that's that's that's I think is a cool way to do planning. And you can do that without losing anything. Because if you want to assemble a bunch of these tiny backups in the right order, or in just over and over again, you can learn a long plan. Okay. So I have one more slide, just going to briefly talk about the average award setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we use function approximation, we have to go to the average word setting, we have to give up discounting. And I just want to make the observation in front of you all that that this planning with average award, it's still a totally open question. I thought it was easy. But I was thinking about the other day with Zach. And it's really an open open question. It's even open for the tabular case. You just take one step dine and try to make an average reward version. That would be a paper because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for your to do this summer to get your master's done just in time for September, that would be a good one. If you don't have one already. Okay. And there's also questions whether the model should should give us the the the expected reward or the expected difference between the reward and the average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these are these are the questions that we started with the open questions. Should the model generate sample states or expectations? And if it's going to give us expected states, should the value function be linear? We've seen how those fit together nice. And the question is a further question is can state update support that can it learn a good enough state features so that the value function can be linear without losing something important? And then there are other questions about whether the model whether this suggests the model should be linear as well, or whether it can be semi linear, which means like a squashing function applied to a linear function. We talked about how planning should you know, once we combine average reward with planning, unsolved problem, we should work on that. We should also worry about how should planning affect the actual actions. And what sub problems should direct the construction of the option models. And I can't I shouldn't try to explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is that we want maybe we want to expect the states, maybe one of the value functions be linear, maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition, that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so I'm done. Thank you for your attention. Okay, we do have a little bit of time for questions. I ended abruptly there, but that's the story, open questions and model based reinforcement. Please. Okay, so that's probably should have been one of my my closed questions, because we definitely need off policy learning in order to learn the models, in order to be do it efficiently. And so the part of the premise is that we're doing off policy learning. And we have, we have a suite of a few methods that will work on that nowadays. Yeah, off policy learning is essential. So I would assume that you would want to learn lots of value functions and not just one. If you want all of them to be linear in your representation, then that's a lot of burden on your representation. Yes. So if all the complexity is in the state representation, then what is the model really giving you? Well, it's giving you the dynamics, which is it's not in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state update function. And but more importantly, I'm just realizing that I forgot to thank my new collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this, and they should be up here. And, and we have we submitted a paper on expectation models to to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out. Yeah. So so a lot of work is going into state update. That's that's the strategy. I think I think it's might be appropriate. Good. I have a couple of observations. I don't know, even whether I'm on the page to you, but I am interested in applied intelligence. One observation is Wall Street, they seem to have a numerical model of the world. You know, so I mean, that's one world or one model that you can look at. It seems to me they're far more numerical than other types of domains. The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know, so it's each duck in the clock can radiate any number of different directions. So I'm not sure whether your model could cover the eagle catching the ducks or you want to give that to us. Thank you. That's good. So this linearity thing, it's a very important that it's linear in the in the features. Okay, and and this is the this is the trick. It's sort of already known, it's well known that anything can be linear if you arrange the right features. So you could do the duck, you could you could presumably capture the higher order, the interactions between the, so what do you lose when you get linearity? You know, let's say you have two features a and b. Well, if the right choice or the right value depends upon both of them being present at the same time. And then then then then you can't do that linearly. The linear function, you can only say, oh, this has a certain effect, this has a certain effect. And there can't be a special thing that if they're on together. Okay. And so what you do with that none, and when there's an interaction between variables, you know, maybe it's it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or that's the kind of thing that you can't do. But but we've known since the beginning of neural networks, that that if you then add a third variable for the conjunction of the two original features, then you can learn the nonlinear function in the original in the first two. Okay, and so the same same is true. So it's not a principle limitation in any way. So think again about a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function, many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way of finding the features in the last layer, then you could be linear. So in some sense, what we're just saying, take the that thing that you worked on in the last layer and make that your state. And so as your state, then you would have to your models would have to produce it. You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating? I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear map, then that nonlinear map would have to figure out that that a that these two variables are, are, need to be treated especially, and they would have to create the conjunction term inside that that nonlinear mapping. So it's like we're doing the same work, we're pushing it into a different place. We're pushing it into the state update. Yeah, other questions? Okay, thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.4, "text": " Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just", "tokens": [50364, 1033, 11, 309, 311, 406, 534, 1596, 1017, 13, 5211, 1939, 11, 291, 434, 732, 2077, 11, 457, 718, 385, 445, 722, 365, 445, 50984], "temperature": 0.0, "avg_logprob": -0.23414747105088346, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.22120244801044464}, {"id": 1, "seek": 0, "start": 12.4, "end": 15.24, "text": " have a welcome you to tea.", "tokens": [50984, 362, 257, 2928, 291, 281, 5817, 13, 51126], "temperature": 0.0, "avg_logprob": -0.23414747105088346, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.22120244801044464}, {"id": 2, "seek": 0, "start": 15.24, "end": 22.52, "text": " We do this each year and it's very informal and I welcome you all to sign up and participate", "tokens": [51126, 492, 360, 341, 1184, 1064, 293, 309, 311, 588, 24342, 293, 286, 2928, 291, 439, 281, 1465, 493, 293, 8197, 51490], "temperature": 0.0, "avg_logprob": -0.23414747105088346, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.22120244801044464}, {"id": 3, "seek": 0, "start": 22.52, "end": 28.44, "text": " and to ask questions and you know we're here just to talk about science and ideas and exchange", "tokens": [51490, 293, 281, 1029, 1651, 293, 291, 458, 321, 434, 510, 445, 281, 751, 466, 3497, 293, 3487, 293, 7742, 51786], "temperature": 0.0, "avg_logprob": -0.23414747105088346, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.22120244801044464}, {"id": 4, "seek": 0, "start": 28.44, "end": 29.76, "text": " points of view.", "tokens": [51786, 2793, 295, 1910, 13, 51852], "temperature": 0.0, "avg_logprob": -0.23414747105088346, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.22120244801044464}, {"id": 5, "seek": 2976, "start": 29.76, "end": 37.52, "text": " So think of it as a community, enjoy the tea, have a cookie, it's all good.", "tokens": [50364, 407, 519, 295, 309, 382, 257, 1768, 11, 2103, 264, 5817, 11, 362, 257, 14417, 11, 309, 311, 439, 665, 13, 50752], "temperature": 0.0, "avg_logprob": -0.24970133645193918, "compression_ratio": 1.5508982035928143, "no_speech_prob": 0.0019545170944184065}, {"id": 6, "seek": 2976, "start": 37.52, "end": 46.44, "text": " The other thing is Thursday, Thursday I think we still have an open slot and we could have", "tokens": [50752, 440, 661, 551, 307, 10383, 11, 10383, 286, 519, 321, 920, 362, 364, 1269, 14747, 293, 321, 727, 362, 51198], "temperature": 0.0, "avg_logprob": -0.24970133645193918, "compression_ratio": 1.5508982035928143, "no_speech_prob": 0.0019545170944184065}, {"id": 7, "seek": 2976, "start": 46.44, "end": 56.6, "text": " Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that", "tokens": [51198, 27787, 11, 27787, 815, 747, 309, 11, 286, 390, 7159, 11170, 576, 312, 510, 293, 415, 576, 747, 309, 11, 457, 300, 51706], "temperature": 0.0, "avg_logprob": -0.24970133645193918, "compression_ratio": 1.5508982035928143, "no_speech_prob": 0.0019545170944184065}, {"id": 8, "seek": 5660, "start": 57.56, "end": 60.4, "text": " will work out, I'm pretty sure.", "tokens": [50412, 486, 589, 484, 11, 286, 478, 1238, 988, 13, 50554], "temperature": 0.0, "avg_logprob": -0.23926156820710173, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.04390418902039528}, {"id": 9, "seek": 5660, "start": 60.4, "end": 68.04, "text": " Okay, so the topics are really intelligence and how do we understand how the mind might", "tokens": [50554, 1033, 11, 370, 264, 8378, 366, 534, 7599, 293, 577, 360, 321, 1223, 577, 264, 1575, 1062, 50936], "temperature": 0.0, "avg_logprob": -0.23926156820710173, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.04390418902039528}, {"id": 10, "seek": 5660, "start": 68.04, "end": 74.4, "text": " work and could work and today my topic, really the topic is model-based reinforcement learning.", "tokens": [50936, 589, 293, 727, 589, 293, 965, 452, 4829, 11, 534, 264, 4829, 307, 2316, 12, 6032, 29280, 2539, 13, 51254], "temperature": 0.0, "avg_logprob": -0.23926156820710173, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.04390418902039528}, {"id": 11, "seek": 5660, "start": 74.4, "end": 78.36, "text": " Model-based reinforcement learning is really it's sort of the whole enchilada.", "tokens": [51254, 17105, 12, 6032, 29280, 2539, 307, 534, 309, 311, 1333, 295, 264, 1379, 35213, 388, 1538, 13, 51452], "temperature": 0.0, "avg_logprob": -0.23926156820710173, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.04390418902039528}, {"id": 12, "seek": 5660, "start": 78.36, "end": 84.56, "text": " We try to get all the pieces present, model-based, model-free, make understanding of the world,", "tokens": [51452, 492, 853, 281, 483, 439, 264, 3755, 1974, 11, 2316, 12, 6032, 11, 2316, 12, 10792, 11, 652, 3701, 295, 264, 1002, 11, 51762], "temperature": 0.0, "avg_logprob": -0.23926156820710173, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.04390418902039528}, {"id": 13, "seek": 8456, "start": 84.56, "end": 91.44, "text": " be able to reason or plan with it and how many people here have just raised your hand", "tokens": [50364, 312, 1075, 281, 1778, 420, 1393, 365, 309, 293, 577, 867, 561, 510, 362, 445, 6005, 428, 1011, 50708], "temperature": 0.0, "avg_logprob": -0.25788056223016037, "compression_ratio": 1.6898395721925135, "no_speech_prob": 0.12234813719987869}, {"id": 14, "seek": 8456, "start": 91.44, "end": 96.56, "text": " if you've heard of the phrase model-based reinforcement learning.", "tokens": [50708, 498, 291, 600, 2198, 295, 264, 9535, 2316, 12, 6032, 29280, 2539, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25788056223016037, "compression_ratio": 1.6898395721925135, "no_speech_prob": 0.12234813719987869}, {"id": 15, "seek": 8456, "start": 96.56, "end": 103.2, "text": " How many haven't, how many actually, let me say it a little differently.", "tokens": [50964, 1012, 867, 2378, 380, 11, 577, 867, 767, 11, 718, 385, 584, 309, 257, 707, 7614, 13, 51296], "temperature": 0.0, "avg_logprob": -0.25788056223016037, "compression_ratio": 1.6898395721925135, "no_speech_prob": 0.12234813719987869}, {"id": 16, "seek": 8456, "start": 103.2, "end": 107.92, "text": " How many of you just think it's the model-based reinforcement like the most important thing", "tokens": [51296, 1012, 867, 295, 291, 445, 519, 309, 311, 264, 2316, 12, 6032, 29280, 411, 264, 881, 1021, 551, 51532], "temperature": 0.0, "avg_logprob": -0.25788056223016037, "compression_ratio": 1.6898395721925135, "no_speech_prob": 0.12234813719987869}, {"id": 17, "seek": 10792, "start": 107.92, "end": 117.6, "text": " in AI, like I do, or should I get a little bit of an argument from authority for it?", "tokens": [50364, 294, 7318, 11, 411, 286, 360, 11, 420, 820, 286, 483, 257, 707, 857, 295, 364, 6770, 490, 8281, 337, 309, 30, 50848], "temperature": 0.0, "avg_logprob": -0.29377281919438786, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.5646858215332031}, {"id": 18, "seek": 10792, "start": 117.6, "end": 123.88, "text": " Okay, I spent that long talking about it, I might as well give my, I just threw in a", "tokens": [50848, 1033, 11, 286, 4418, 300, 938, 1417, 466, 309, 11, 286, 1062, 382, 731, 976, 452, 11, 286, 445, 11918, 294, 257, 51162], "temperature": 0.0, "avg_logprob": -0.29377281919438786, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.5646858215332031}, {"id": 19, "seek": 10792, "start": 123.88, "end": 129.52, "text": " slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon", "tokens": [51162, 4137, 689, 286, 584, 3195, 295, 561, 366, 1417, 466, 341, 11, 309, 311, 1333, 295, 264, 551, 11, 4956, 40113, 4106, 51444], "temperature": 0.0, "avg_logprob": -0.29377281919438786, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.5646858215332031}, {"id": 20, "seek": 10792, "start": 129.52, "end": 135.44, "text": " talking about predictive learning, understanding the world, Yashio Benjo talks about the most", "tokens": [51444, 1417, 466, 35521, 2539, 11, 3701, 264, 1002, 11, 398, 1299, 1004, 3964, 5134, 6686, 466, 264, 881, 51740], "temperature": 0.0, "avg_logprob": -0.29377281919438786, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.5646858215332031}, {"id": 21, "seek": 13544, "start": 135.44, "end": 139.52, "text": " important step in AI is to make predictive causal model of the world.", "tokens": [50364, 1021, 1823, 294, 7318, 307, 281, 652, 35521, 38755, 2316, 295, 264, 1002, 13, 50568], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 22, "seek": 13544, "start": 139.52, "end": 145.68, "text": " This is a thing that's coming around at last and this is our topic, our topic is model-based", "tokens": [50568, 639, 307, 257, 551, 300, 311, 1348, 926, 412, 1036, 293, 341, 307, 527, 4829, 11, 527, 4829, 307, 2316, 12, 6032, 50876], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 23, "seek": 13544, "start": 145.68, "end": 149.07999999999998, "text": " reinforcement learning and it means making a model of the world, I can use this word", "tokens": [50876, 29280, 2539, 293, 309, 1355, 1455, 257, 2316, 295, 264, 1002, 11, 286, 393, 764, 341, 1349, 51046], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 24, "seek": 13544, "start": 149.07999999999998, "end": 153.52, "text": " model to mean model of the world, how the world works, I'll only use the word model", "tokens": [51046, 2316, 281, 914, 2316, 295, 264, 1002, 11, 577, 264, 1002, 1985, 11, 286, 603, 787, 764, 264, 1349, 2316, 51268], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 25, "seek": 13544, "start": 153.52, "end": 159.04, "text": " as a world model, model of the MDP, the Markov decision process, the transition dynamics of", "tokens": [51268, 382, 257, 1002, 2316, 11, 2316, 295, 264, 376, 11373, 11, 264, 3934, 5179, 3537, 1399, 11, 264, 6034, 15679, 295, 51544], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 26, "seek": 13544, "start": 159.04, "end": 164.8, "text": " the world and model-based reinforcement learning, we learn the model, we learn the model and", "tokens": [51544, 264, 1002, 293, 2316, 12, 6032, 29280, 2539, 11, 321, 1466, 264, 2316, 11, 321, 1466, 264, 2316, 293, 51832], "temperature": 0.0, "avg_logprob": -0.17973145912951372, "compression_ratio": 2.072289156626506, "no_speech_prob": 0.30673032999038696}, {"id": 27, "seek": 16480, "start": 164.8, "end": 170.28, "text": " we also then use the model to plan or reason about what to do.", "tokens": [50364, 321, 611, 550, 764, 264, 2316, 281, 1393, 420, 1778, 466, 437, 281, 360, 13, 50638], "temperature": 0.0, "avg_logprob": -0.1653400739034017, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.008054189383983612}, {"id": 28, "seek": 16480, "start": 170.28, "end": 176.64000000000001, "text": " So this is, we call this planning and planning proceeds by, you imagine states, you look", "tokens": [50638, 407, 341, 307, 11, 321, 818, 341, 5038, 293, 5038, 32280, 538, 11, 291, 3811, 4368, 11, 291, 574, 50956], "temperature": 0.0, "avg_logprob": -0.1653400739034017, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.008054189383983612}, {"id": 29, "seek": 16480, "start": 176.64000000000001, "end": 180.32000000000002, "text": " ahead from the individual states to see what might happen and you back up to improve your", "tokens": [50956, 2286, 490, 264, 2609, 4368, 281, 536, 437, 1062, 1051, 293, 291, 646, 493, 281, 3470, 428, 51140], "temperature": 0.0, "avg_logprob": -0.1653400739034017, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.008054189383983612}, {"id": 30, "seek": 16480, "start": 180.32000000000002, "end": 185.12, "text": " policy or your value with those states that you're imagining might happen and then this", "tokens": [51140, 3897, 420, 428, 2158, 365, 729, 4368, 300, 291, 434, 27798, 1062, 1051, 293, 550, 341, 51380], "temperature": 0.0, "avg_logprob": -0.1653400739034017, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.008054189383983612}, {"id": 31, "seek": 16480, "start": 185.12, "end": 189.64000000000001, "text": " is how you figure out, decide what to do.", "tokens": [51380, 307, 577, 291, 2573, 484, 11, 4536, 437, 281, 360, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1653400739034017, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.008054189383983612}, {"id": 32, "seek": 18964, "start": 189.64, "end": 196.48, "text": " Okay so it's decided this idea has been around for a while, I'm going to talk briefly about", "tokens": [50364, 1033, 370, 309, 311, 3047, 341, 1558, 575, 668, 926, 337, 257, 1339, 11, 286, 478, 516, 281, 751, 10515, 466, 50706], "temperature": 0.0, "avg_logprob": -0.23020121130612817, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.04593750834465027}, {"id": 33, "seek": 18964, "start": 196.48, "end": 202.48, "text": " the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first", "tokens": [50706, 264, 413, 2534, 289, 9482, 11, 746, 286, 630, 294, 13384, 11, 1920, 2217, 924, 2057, 586, 11, 689, 286, 700, 51006], "temperature": 0.0, "avg_logprob": -0.23020121130612817, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.04593750834465027}, {"id": 34, "seek": 18964, "start": 202.48, "end": 207.2, "text": " proposed or first explicitly proposed that planning and learning would be radically similar.", "tokens": [51006, 10348, 420, 700, 20803, 10348, 300, 5038, 293, 2539, 576, 312, 35508, 2531, 13, 51242], "temperature": 0.0, "avg_logprob": -0.23020121130612817, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.04593750834465027}, {"id": 35, "seek": 18964, "start": 207.2, "end": 211.88, "text": " So there's some reinforcement learning algorithm that's interacting with the world and then", "tokens": [51242, 407, 456, 311, 512, 29280, 2539, 9284, 300, 311, 18017, 365, 264, 1002, 293, 550, 51476], "temperature": 0.0, "avg_logprob": -0.23020121130612817, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.04593750834465027}, {"id": 36, "seek": 18964, "start": 211.88, "end": 216.35999999999999, "text": " it's also learning the world model which is another box that you can plug in place of", "tokens": [51476, 309, 311, 611, 2539, 264, 1002, 2316, 597, 307, 1071, 2424, 300, 291, 393, 5452, 294, 1081, 295, 51700], "temperature": 0.0, "avg_logprob": -0.23020121130612817, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.04593750834465027}, {"id": 37, "seek": 21636, "start": 216.36, "end": 221.46, "text": " the world and instead interact with the model for a while, in that sense, and so you're", "tokens": [50364, 264, 1002, 293, 2602, 4648, 365, 264, 2316, 337, 257, 1339, 11, 294, 300, 2020, 11, 293, 370, 291, 434, 50619], "temperature": 0.0, "avg_logprob": -0.1902598012792002, "compression_ratio": 1.9457013574660633, "no_speech_prob": 0.18671205639839172}, {"id": 38, "seek": 21636, "start": 221.46, "end": 225.52, "text": " interacting with the model and you're saying what if I do this, the model says that would", "tokens": [50619, 18017, 365, 264, 2316, 293, 291, 434, 1566, 437, 498, 286, 360, 341, 11, 264, 2316, 1619, 300, 576, 50822], "temperature": 0.0, "avg_logprob": -0.1902598012792002, "compression_ratio": 1.9457013574660633, "no_speech_prob": 0.18671205639839172}, {"id": 39, "seek": 21636, "start": 225.52, "end": 229.20000000000002, "text": " happen, you get this reward and you learn from that just as if it had really happened", "tokens": [50822, 1051, 11, 291, 483, 341, 7782, 293, 291, 1466, 490, 300, 445, 382, 498, 309, 632, 534, 2011, 51006], "temperature": 0.0, "avg_logprob": -0.1902598012792002, "compression_ratio": 1.9457013574660633, "no_speech_prob": 0.18671205639839172}, {"id": 40, "seek": 21636, "start": 229.20000000000002, "end": 235.96, "text": " and so then this way the planning or the logic and the reasoning, certainly the planning", "tokens": [51006, 293, 370, 550, 341, 636, 264, 5038, 420, 264, 9952, 293, 264, 21577, 11, 3297, 264, 5038, 51344], "temperature": 0.0, "avg_logprob": -0.1902598012792002, "compression_ratio": 1.9457013574660633, "no_speech_prob": 0.18671205639839172}, {"id": 41, "seek": 21636, "start": 235.96, "end": 242.28000000000003, "text": " is radically similar to just acting in the world, just acting in imagination.", "tokens": [51344, 307, 35508, 2531, 281, 445, 6577, 294, 264, 1002, 11, 445, 6577, 294, 12938, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1902598012792002, "compression_ratio": 1.9457013574660633, "no_speech_prob": 0.18671205639839172}, {"id": 42, "seek": 24228, "start": 242.28, "end": 246.92, "text": " So that's the first half of the idea, the second half of the idea is that these things", "tokens": [50364, 407, 300, 311, 264, 700, 1922, 295, 264, 1558, 11, 264, 1150, 1922, 295, 264, 1558, 307, 300, 613, 721, 50596], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 43, "seek": 24228, "start": 246.92, "end": 252.12, "text": " are all done simultaneously, so planning and learning and executing, they're all been going", "tokens": [50596, 366, 439, 1096, 16561, 11, 370, 5038, 293, 2539, 293, 32368, 11, 436, 434, 439, 668, 516, 50856], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 44, "seek": 24228, "start": 252.12, "end": 257.04, "text": " on all the time, so you're always interacting with the world, getting new experience, you", "tokens": [50856, 322, 439, 264, 565, 11, 370, 291, 434, 1009, 18017, 365, 264, 1002, 11, 1242, 777, 1752, 11, 291, 51102], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 45, "seek": 24228, "start": 257.04, "end": 260.8, "text": " can do a model for your reinforcement learning with that bit, but you also use that experience", "tokens": [51102, 393, 360, 257, 2316, 337, 428, 29280, 2539, 365, 300, 857, 11, 457, 291, 611, 764, 300, 1752, 51290], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 46, "seek": 24228, "start": 260.8, "end": 265.44, "text": " to make the model and the model through planning and that also affects your value function", "tokens": [51290, 281, 652, 264, 2316, 293, 264, 2316, 807, 5038, 293, 300, 611, 11807, 428, 2158, 2445, 51522], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 47, "seek": 24228, "start": 265.44, "end": 267.4, "text": " and your policy and makes you do better.", "tokens": [51522, 293, 428, 3897, 293, 1669, 291, 360, 1101, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1747668105944068, "compression_ratio": 1.8893129770992367, "no_speech_prob": 0.2534659504890442}, {"id": 48, "seek": 26740, "start": 267.52, "end": 273.84, "text": " Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm", "tokens": [50370, 3950, 366, 264, 732, 3487, 295, 413, 2534, 32, 11, 309, 311, 733, 295, 588, 2199, 293, 321, 652, 309, 666, 364, 9284, 50686], "temperature": 0.0, "avg_logprob": -0.2041166288043381, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.016388442367315292}, {"id": 49, "seek": 26740, "start": 273.84, "end": 278.91999999999996, "text": " that's kind of useful to complement those diagrams, those visual things with a little", "tokens": [50686, 300, 311, 733, 295, 4420, 281, 17103, 729, 36709, 11, 729, 5056, 721, 365, 257, 707, 50940], "temperature": 0.0, "avg_logprob": -0.2041166288043381, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.016388442367315292}, {"id": 50, "seek": 26740, "start": 278.91999999999996, "end": 283.52, "text": " bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from", "tokens": [50940, 857, 295, 257, 10686, 11, 652, 309, 588, 9859, 11, 370, 341, 307, 257, 10686, 11, 341, 307, 364, 9284, 490, 51170], "temperature": 0.0, "avg_logprob": -0.2041166288043381, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.016388442367315292}, {"id": 51, "seek": 26740, "start": 283.52, "end": 289.52, "text": " the book, so basically your Q is your action value function and model SA is your prediction", "tokens": [51170, 264, 1446, 11, 370, 1936, 428, 1249, 307, 428, 3069, 2158, 2445, 293, 2316, 16482, 307, 428, 17630, 51470], "temperature": 0.0, "avg_logprob": -0.2041166288043381, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.016388442367315292}, {"id": 52, "seek": 26740, "start": 289.52, "end": 293.44, "text": " about what would happen if you were in state S and did action A and you initialize that", "tokens": [51470, 466, 437, 576, 1051, 498, 291, 645, 294, 1785, 318, 293, 630, 3069, 316, 293, 291, 5883, 1125, 300, 51666], "temperature": 0.0, "avg_logprob": -0.2041166288043381, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.016388442367315292}, {"id": 53, "seek": 29344, "start": 293.56, "end": 297.28, "text": " and then as you go through life, you're just doing this loop over and over again, you're", "tokens": [50370, 293, 550, 382, 291, 352, 807, 993, 11, 291, 434, 445, 884, 341, 6367, 670, 293, 670, 797, 11, 291, 434, 50556], "temperature": 0.0, "avg_logprob": -0.16926029039465862, "compression_ratio": 1.8649789029535866, "no_speech_prob": 0.12067558616399765}, {"id": 54, "seek": 29344, "start": 297.28, "end": 302.48, "text": " looking at the state you're in, you're picking an action in it and set B somehow, then you", "tokens": [50556, 1237, 412, 264, 1785, 291, 434, 294, 11, 291, 434, 8867, 364, 3069, 294, 309, 293, 992, 363, 6063, 11, 550, 291, 50816], "temperature": 0.0, "avg_logprob": -0.16926029039465862, "compression_ratio": 1.8649789029535866, "no_speech_prob": 0.12067558616399765}, {"id": 55, "seek": 29344, "start": 302.48, "end": 308.4, "text": " take that action, send it to the world, the world gives you back a result, R, the reward", "tokens": [50816, 747, 300, 3069, 11, 2845, 309, 281, 264, 1002, 11, 264, 1002, 2709, 291, 646, 257, 1874, 11, 497, 11, 264, 7782, 51112], "temperature": 0.0, "avg_logprob": -0.16926029039465862, "compression_ratio": 1.8649789029535866, "no_speech_prob": 0.12067558616399765}, {"id": 56, "seek": 29344, "start": 308.4, "end": 314.2, "text": " and the next state S prime and so then you do model free learning with that little four", "tokens": [51112, 293, 264, 958, 1785, 318, 5835, 293, 370, 550, 291, 360, 2316, 1737, 2539, 365, 300, 707, 1451, 51402], "temperature": 0.0, "avg_logprob": -0.16926029039465862, "compression_ratio": 1.8649789029535866, "no_speech_prob": 0.12067558616399765}, {"id": 57, "seek": 29344, "start": 314.2, "end": 320.0, "text": " tuple of experience, state action, reward, next state, so here we're doing Q learning", "tokens": [51402, 2604, 781, 295, 1752, 11, 1785, 3069, 11, 7782, 11, 958, 1785, 11, 370, 510, 321, 434, 884, 1249, 2539, 51692], "temperature": 0.0, "avg_logprob": -0.16926029039465862, "compression_ratio": 1.8649789029535866, "no_speech_prob": 0.12067558616399765}, {"id": 58, "seek": 32000, "start": 320.08, "end": 325.52, "text": " in step D to update the action values and then we update the model because we've seen", "tokens": [50368, 294, 1823, 413, 281, 5623, 264, 3069, 4190, 293, 550, 321, 5623, 264, 2316, 570, 321, 600, 1612, 50640], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 59, "seek": 32000, "start": 325.52, "end": 329.76, "text": " what will happen for that state and that action, that this will happen and this is where we're", "tokens": [50640, 437, 486, 1051, 337, 300, 1785, 293, 300, 3069, 11, 300, 341, 486, 1051, 293, 341, 307, 689, 321, 434, 50852], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 60, "seek": 32000, "start": 329.76, "end": 334.64, "text": " assuming a deterministic world because really many things might happen if you did that action", "tokens": [50852, 11926, 257, 15957, 3142, 1002, 570, 534, 867, 721, 1062, 1051, 498, 291, 630, 300, 3069, 51096], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 61, "seek": 32000, "start": 334.64, "end": 338.72, "text": " in this state and we only saw one thing and now we're going to just overwrite the model", "tokens": [51096, 294, 341, 1785, 293, 321, 787, 1866, 472, 551, 293, 586, 321, 434, 516, 281, 445, 670, 21561, 264, 2316, 51300], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 62, "seek": 32000, "start": 338.72, "end": 342.36, "text": " so that I'll predict always that that would happen.", "tokens": [51300, 370, 300, 286, 603, 6069, 1009, 300, 300, 576, 1051, 13, 51482], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 63, "seek": 32000, "start": 342.36, "end": 346.2, "text": " So the thing about DynA, particularly the first instance of it in 1990, it's a very", "tokens": [51482, 407, 264, 551, 466, 413, 2534, 32, 11, 4098, 264, 700, 5197, 295, 309, 294, 13384, 11, 309, 311, 257, 588, 51674], "temperature": 0.0, "avg_logprob": -0.15503069559733074, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.014489753171801567}, {"id": 64, "seek": 34620, "start": 346.2, "end": 353.84, "text": " simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully", "tokens": [50364, 2199, 293, 1920, 257, 12058, 1389, 11, 309, 311, 588, 1850, 457, 309, 311, 787, 12675, 11, 309, 1177, 380, 4498, 50746], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 65, "seek": 34620, "start": 353.84, "end": 354.84, "text": " express all the possibilities.", "tokens": [50746, 5109, 439, 264, 12178, 13, 50796], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 66, "seek": 34620, "start": 354.84, "end": 358.84, "text": " We're going to talk about today's, how you can go and consider more possibilities.", "tokens": [50796, 492, 434, 516, 281, 751, 466, 965, 311, 11, 577, 291, 393, 352, 293, 1949, 544, 12178, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 67, "seek": 34620, "start": 358.84, "end": 364.0, "text": " Anyway, after you do, now we've got as far as learning the model and then step F is where", "tokens": [50996, 5684, 11, 934, 291, 360, 11, 586, 321, 600, 658, 382, 1400, 382, 2539, 264, 2316, 293, 550, 1823, 479, 307, 689, 51254], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 68, "seek": 34620, "start": 364.0, "end": 365.8, "text": " you do the plan.", "tokens": [51254, 291, 360, 264, 1393, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 69, "seek": 34620, "start": 365.8, "end": 369.91999999999996, "text": " So by the plan, you just imagine some state, maybe randomly from something you've seen", "tokens": [51344, 407, 538, 264, 1393, 11, 291, 445, 3811, 512, 1785, 11, 1310, 16979, 490, 746, 291, 600, 1612, 51550], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 70, "seek": 34620, "start": 369.91999999999996, "end": 375.36, "text": " before, imagine an action, you ask the model what would happen if you were there and then", "tokens": [51550, 949, 11, 3811, 364, 3069, 11, 291, 1029, 264, 2316, 437, 576, 1051, 498, 291, 645, 456, 293, 550, 51822], "temperature": 0.0, "avg_logprob": -0.1989592429130308, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.01664784364402294}, {"id": 71, "seek": 37536, "start": 375.48, "end": 379.52000000000004, "text": " you do that same step, same, it's the same as D, it's Q-learning, but here now we're", "tokens": [50370, 291, 360, 300, 912, 1823, 11, 912, 11, 309, 311, 264, 912, 382, 413, 11, 309, 311, 1249, 12, 47204, 11, 457, 510, 586, 321, 434, 50572], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 72, "seek": 37536, "start": 379.52000000000004, "end": 383.56, "text": " doing Q-learning on these imagined things and there we're doing Q-learning on the real thing.", "tokens": [50572, 884, 1249, 12, 47204, 322, 613, 16590, 721, 293, 456, 321, 434, 884, 1249, 12, 47204, 322, 264, 957, 551, 13, 50774], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 73, "seek": 37536, "start": 383.56, "end": 388.0, "text": " So that's the basic idea of DynA, I'm sure many of you have seen that before.", "tokens": [50774, 407, 300, 311, 264, 3875, 1558, 295, 413, 2534, 32, 11, 286, 478, 988, 867, 295, 291, 362, 1612, 300, 949, 13, 50996], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 74, "seek": 37536, "start": 388.0, "end": 392.32, "text": " Now, maybe you've seen the demos of it before, so here's the standard demo where we have", "tokens": [50996, 823, 11, 1310, 291, 600, 1612, 264, 33788, 295, 309, 949, 11, 370, 510, 311, 264, 3832, 10723, 689, 321, 362, 51212], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 75, "seek": 37536, "start": 392.32, "end": 397.84000000000003, "text": " a grid world, we have a start state down in that corner and we have a goal state where", "tokens": [51212, 257, 10748, 1002, 11, 321, 362, 257, 722, 1785, 760, 294, 300, 4538, 293, 321, 362, 257, 3387, 1785, 689, 51488], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 76, "seek": 37536, "start": 397.84000000000003, "end": 403.48, "text": " you get a plus one reward and the actions are to move up, down, right and left, so four", "tokens": [51488, 291, 483, 257, 1804, 472, 7782, 293, 264, 5909, 366, 281, 1286, 493, 11, 760, 11, 558, 293, 1411, 11, 370, 1451, 51770], "temperature": 0.0, "avg_logprob": -0.16923495514752113, "compression_ratio": 1.8909090909090909, "no_speech_prob": 0.08256662636995316}, {"id": 77, "seek": 40348, "start": 403.52000000000004, "end": 408.04, "text": " actions from each state, the states are all represented tabularly, so like this is state", "tokens": [50366, 5909, 490, 1184, 1785, 11, 264, 4368, 366, 439, 10379, 4421, 1040, 356, 11, 370, 411, 341, 307, 1785, 50592], "temperature": 0.0, "avg_logprob": -0.1595039044396352, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010009253397583961}, {"id": 78, "seek": 40348, "start": 408.04, "end": 415.04, "text": " 34 and this is state 92 and the model says, oh, when I go action four in state 34, I end", "tokens": [50592, 12790, 293, 341, 307, 1785, 28225, 293, 264, 2316, 1619, 11, 1954, 11, 562, 286, 352, 3069, 1451, 294, 1785, 12790, 11, 286, 917, 50942], "temperature": 0.0, "avg_logprob": -0.1595039044396352, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010009253397583961}, {"id": 79, "seek": 40348, "start": 416.20000000000005, "end": 421.72, "text": " up in state 92, that's what the model is learning and because it has the model, it is very quickly", "tokens": [51000, 493, 294, 1785, 28225, 11, 300, 311, 437, 264, 2316, 307, 2539, 293, 570, 309, 575, 264, 2316, 11, 309, 307, 588, 2661, 51276], "temperature": 0.0, "avg_logprob": -0.1595039044396352, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010009253397583961}, {"id": 80, "seek": 40348, "start": 421.72, "end": 428.72, "text": " learning how to get from start to goal efficiently and it's actually able to learn about states", "tokens": [51276, 2539, 577, 281, 483, 490, 722, 281, 3387, 19621, 293, 309, 311, 767, 1075, 281, 1466, 466, 4368, 51626], "temperature": 0.0, "avg_logprob": -0.1595039044396352, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010009253397583961}, {"id": 81, "seek": 40348, "start": 428.88, "end": 433.04, "text": " that it's not in and this becomes most apparent if we pick up the goal and move it to a different", "tokens": [51634, 300, 309, 311, 406, 294, 293, 341, 3643, 881, 18335, 498, 321, 1888, 493, 264, 3387, 293, 1286, 309, 281, 257, 819, 51842], "temperature": 0.0, "avg_logprob": -0.1595039044396352, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010009253397583961}, {"id": 82, "seek": 43304, "start": 433.04, "end": 438.24, "text": " place, so of course once we've moved it, the agent will go back to, it'll go here again", "tokens": [50364, 1081, 11, 370, 295, 1164, 1564, 321, 600, 4259, 309, 11, 264, 9461, 486, 352, 646, 281, 11, 309, 603, 352, 510, 797, 50624], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 83, "seek": 43304, "start": 438.24, "end": 443.52000000000004, "text": " where it used to be and be disappointed so to speak and then it has no idea where it", "tokens": [50624, 689, 309, 1143, 281, 312, 293, 312, 13856, 370, 281, 1710, 293, 550, 309, 575, 572, 1558, 689, 309, 50888], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 84, "seek": 43304, "start": 443.52000000000004, "end": 447.04, "text": " is, it's built a model but the model says that there's no goal up there, as the last", "tokens": [50888, 307, 11, 309, 311, 3094, 257, 2316, 457, 264, 2316, 1619, 300, 456, 311, 572, 3387, 493, 456, 11, 382, 264, 1036, 51064], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 85, "seek": 43304, "start": 447.04, "end": 450.88, "text": " time it was there, there was no goal, okay, but eventually it will stumble upon the goal", "tokens": [51064, 565, 309, 390, 456, 11, 456, 390, 572, 3387, 11, 1392, 11, 457, 4728, 309, 486, 41302, 3564, 264, 3387, 51256], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 86, "seek": 43304, "start": 450.88, "end": 456.36, "text": " again and then it will be very quickly able to plot a path, so watch when it first finds", "tokens": [51256, 797, 293, 550, 309, 486, 312, 588, 2661, 1075, 281, 7542, 257, 3100, 11, 370, 1159, 562, 309, 700, 10704, 51530], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 87, "seek": 43304, "start": 456.36, "end": 462.32000000000005, "text": " it now, here it will eventually see it's learning the right way to go and it's learning a good", "tokens": [51530, 309, 586, 11, 510, 309, 486, 4728, 536, 309, 311, 2539, 264, 558, 636, 281, 352, 293, 309, 311, 2539, 257, 665, 51828], "temperature": 0.0, "avg_logprob": -0.17422685623168946, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0033747716806828976}, {"id": 88, "seek": 46232, "start": 462.32, "end": 468.84, "text": " path very quickly because it has a model, okay, so that's like tabular dyna, okay, and", "tokens": [50364, 3100, 588, 2661, 570, 309, 575, 257, 2316, 11, 1392, 11, 370, 300, 311, 411, 4421, 1040, 14584, 629, 11, 1392, 11, 293, 50690], "temperature": 0.0, "avg_logprob": -0.21245178554369057, "compression_ratio": 1.75, "no_speech_prob": 0.02225709706544876}, {"id": 89, "seek": 46232, "start": 468.84, "end": 473.76, "text": " today we want to talk about the extensions of this, open questions in it, yeah I guess", "tokens": [50690, 965, 321, 528, 281, 751, 466, 264, 25129, 295, 341, 11, 1269, 1651, 294, 309, 11, 1338, 286, 2041, 50936], "temperature": 0.0, "avg_logprob": -0.21245178554369057, "compression_ratio": 1.75, "no_speech_prob": 0.02225709706544876}, {"id": 90, "seek": 46232, "start": 473.76, "end": 478.68, "text": " I didn't even say it, it's open questions and planning, I'm not going to tell you the answers,", "tokens": [50936, 286, 994, 380, 754, 584, 309, 11, 309, 311, 1269, 1651, 293, 5038, 11, 286, 478, 406, 516, 281, 980, 291, 264, 6338, 11, 51182], "temperature": 0.0, "avg_logprob": -0.21245178554369057, "compression_ratio": 1.75, "no_speech_prob": 0.02225709706544876}, {"id": 91, "seek": 46232, "start": 478.68, "end": 484.96, "text": " I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic", "tokens": [51182, 286, 478, 516, 281, 853, 281, 992, 264, 1651, 11, 370, 31193, 629, 9482, 26448, 8195, 281, 342, 8997, 2750, 51496], "temperature": 0.0, "avg_logprob": -0.21245178554369057, "compression_ratio": 1.75, "no_speech_prob": 0.02225709706544876}, {"id": 92, "seek": 46232, "start": 484.96, "end": 489.84, "text": " dynamics, what you saw was just deterministic dynamics, we assume that the world always", "tokens": [51496, 15679, 11, 437, 291, 1866, 390, 445, 15957, 3142, 15679, 11, 321, 6552, 300, 264, 1002, 1009, 51740], "temperature": 0.0, "avg_logprob": -0.21245178554369057, "compression_ratio": 1.75, "no_speech_prob": 0.02225709706544876}, {"id": 93, "seek": 48984, "start": 489.84, "end": 494.84, "text": " went the same way but you could instead of overwriting what the model says in the state", "tokens": [50364, 1437, 264, 912, 636, 457, 291, 727, 2602, 295, 670, 19868, 437, 264, 2316, 1619, 294, 264, 1785, 50614], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 94, "seek": 48984, "start": 494.84, "end": 498.64, "text": " in action pair, you could start to collect a list of all the things that might happen", "tokens": [50614, 294, 3069, 6119, 11, 291, 727, 722, 281, 2500, 257, 1329, 295, 439, 264, 721, 300, 1062, 1051, 50804], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 95, "seek": 48984, "start": 498.64, "end": 503.96, "text": " in their probabilities and then you could sample that and you could do exactly the same thing,", "tokens": [50804, 294, 641, 33783, 293, 550, 291, 727, 6889, 300, 293, 291, 727, 360, 2293, 264, 912, 551, 11, 51070], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 96, "seek": 48984, "start": 503.96, "end": 508.84, "text": " you could add function approximation, now function approximation, I'm going to talk about it,", "tokens": [51070, 291, 727, 909, 2445, 28023, 11, 586, 2445, 28023, 11, 286, 478, 516, 281, 751, 466, 309, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 97, "seek": 48984, "start": 508.84, "end": 514.8399999999999, "text": " but it's really a spectrum, a range of degrees of function approximation, so what we saw", "tokens": [51314, 457, 309, 311, 534, 257, 11143, 11, 257, 3613, 295, 5310, 295, 2445, 28023, 11, 370, 437, 321, 1866, 51614], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 98, "seek": 48984, "start": 514.8399999999999, "end": 519.72, "text": " was tabular, I call it tabular and that means every state action pair is treated totally", "tokens": [51614, 390, 4421, 1040, 11, 286, 818, 309, 4421, 1040, 293, 300, 1355, 633, 1785, 3069, 6119, 307, 8668, 3879, 51858], "temperature": 0.0, "avg_logprob": -0.1917643085602791, "compression_ratio": 1.9708029197080292, "no_speech_prob": 0.30666038393974304}, {"id": 99, "seek": 51972, "start": 519.72, "end": 523.32, "text": " different from every other state action pair, there's no similarity between them, there's", "tokens": [50364, 819, 490, 633, 661, 1785, 3069, 6119, 11, 456, 311, 572, 32194, 1296, 552, 11, 456, 311, 50544], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 100, "seek": 51972, "start": 523.32, "end": 527.88, "text": " no generalization and so there's just a big table and I store things in that state action", "tokens": [50544, 572, 2674, 2144, 293, 370, 456, 311, 445, 257, 955, 3199, 293, 286, 3531, 721, 294, 300, 1785, 3069, 50772], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 101, "seek": 51972, "start": 527.88, "end": 533.9200000000001, "text": " pair and really in real life certainly in computer go and in Atari games and in any", "tokens": [50772, 6119, 293, 534, 294, 957, 993, 3297, 294, 3820, 352, 293, 294, 41381, 2813, 293, 294, 604, 51074], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 102, "seek": 51972, "start": 533.9200000000001, "end": 539.08, "text": " robotics application you have to generalize from one state to another and that's, you", "tokens": [51074, 34145, 3861, 291, 362, 281, 2674, 1125, 490, 472, 1785, 281, 1071, 293, 300, 311, 11, 291, 51332], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 103, "seek": 51972, "start": 539.08, "end": 543.84, "text": " know, you never see the same state twice, okay, but we start with the tabular and you", "tokens": [51332, 458, 11, 291, 1128, 536, 264, 912, 1785, 6091, 11, 1392, 11, 457, 321, 722, 365, 264, 4421, 1040, 293, 291, 51570], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 104, "seek": 51972, "start": 543.84, "end": 548.24, "text": " think you're used to your deep learning, that'd be a nonlinear system, you could also have", "tokens": [51570, 519, 291, 434, 1143, 281, 428, 2452, 2539, 11, 300, 1116, 312, 257, 2107, 28263, 1185, 11, 291, 727, 611, 362, 51790], "temperature": 0.0, "avg_logprob": -0.1673789100041465, "compression_ratio": 1.8327526132404182, "no_speech_prob": 0.03108992427587509}, {"id": 105, "seek": 54824, "start": 548.24, "end": 553.72, "text": " linear things that turn out to be quite important and even the aggregate case, state aggregate", "tokens": [50364, 8213, 721, 300, 1261, 484, 281, 312, 1596, 1021, 293, 754, 264, 26118, 1389, 11, 1785, 26118, 50638], "temperature": 0.0, "avg_logprob": -0.19574457948858087, "compression_ratio": 1.8, "no_speech_prob": 0.030202927067875862}, {"id": 106, "seek": 54824, "start": 553.72, "end": 560.2, "text": " means you still have a table but there could be many states fall into the same table entry,", "tokens": [50638, 1355, 291, 920, 362, 257, 3199, 457, 456, 727, 312, 867, 4368, 2100, 666, 264, 912, 3199, 8729, 11, 50962], "temperature": 0.0, "avg_logprob": -0.19574457948858087, "compression_ratio": 1.8, "no_speech_prob": 0.030202927067875862}, {"id": 107, "seek": 54824, "start": 560.2, "end": 569.04, "text": " okay, so you're aggregating states and treating them all the same, this is a nice case actually,", "tokens": [50962, 1392, 11, 370, 291, 434, 16743, 990, 4368, 293, 15083, 552, 439, 264, 912, 11, 341, 307, 257, 1481, 1389, 767, 11, 51404], "temperature": 0.0, "avg_logprob": -0.19574457948858087, "compression_ratio": 1.8, "no_speech_prob": 0.030202927067875862}, {"id": 108, "seek": 54824, "start": 569.04, "end": 576.28, "text": " we can get theoretical results for it that we can't get for the other cases, okay, so", "tokens": [51404, 321, 393, 483, 20864, 3542, 337, 309, 300, 321, 393, 380, 483, 337, 264, 661, 3331, 11, 1392, 11, 370, 51766], "temperature": 0.0, "avg_logprob": -0.19574457948858087, "compression_ratio": 1.8, "no_speech_prob": 0.030202927067875862}, {"id": 109, "seek": 57628, "start": 576.3199999999999, "end": 580.92, "text": " there's function approximation, we want to do that in some sense that's our bread and", "tokens": [50366, 456, 311, 2445, 28023, 11, 321, 528, 281, 360, 300, 294, 512, 2020, 300, 311, 527, 5961, 293, 50596], "temperature": 0.0, "avg_logprob": -0.1693516811692571, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.10654696822166443}, {"id": 110, "seek": 57628, "start": 580.92, "end": 587.4399999999999, "text": " butter, we just generalize the table to a function approximator like supervised learning", "tokens": [50596, 5517, 11, 321, 445, 2674, 1125, 264, 3199, 281, 257, 2445, 8542, 1639, 411, 46533, 2539, 50922], "temperature": 0.0, "avg_logprob": -0.1693516811692571, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.10654696822166443}, {"id": 111, "seek": 57628, "start": 587.4399999999999, "end": 595.68, "text": " system, but let's go on, I want to extend it quite far, so let's list the things and", "tokens": [50922, 1185, 11, 457, 718, 311, 352, 322, 11, 286, 528, 281, 10101, 309, 1596, 1400, 11, 370, 718, 311, 1329, 264, 721, 293, 51334], "temperature": 0.0, "avg_logprob": -0.1693516811692571, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.10654696822166443}, {"id": 112, "seek": 57628, "start": 595.68, "end": 600.64, "text": " the next big thing is partial observability because really the world doesn't even give", "tokens": [51334, 264, 958, 955, 551, 307, 14641, 9951, 2310, 570, 534, 264, 1002, 1177, 380, 754, 976, 51582], "temperature": 0.0, "avg_logprob": -0.1693516811692571, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.10654696822166443}, {"id": 113, "seek": 60064, "start": 600.64, "end": 606.36, "text": " us states, it gives us observations, it gives us things that happened, things that are senses,", "tokens": [50364, 505, 4368, 11, 309, 2709, 505, 18163, 11, 309, 2709, 505, 721, 300, 2011, 11, 721, 300, 366, 17057, 11, 50650], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 114, "seek": 60064, "start": 606.36, "end": 612.04, "text": " it doesn't tell us, we don't know the full state of the world, we just get an observation and now,", "tokens": [50650, 309, 1177, 380, 980, 505, 11, 321, 500, 380, 458, 264, 1577, 1785, 295, 264, 1002, 11, 321, 445, 483, 364, 14816, 293, 586, 11, 50934], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 115, "seek": 60064, "start": 612.04, "end": 617.4399999999999, "text": " we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest,", "tokens": [50934, 321, 362, 257, 707, 4282, 11, 1392, 11, 586, 11200, 264, 4282, 382, 264, 2182, 2424, 11, 457, 498, 291, 574, 412, 264, 1472, 11, 51204], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 116, "seek": 60064, "start": 617.4399999999999, "end": 620.72, "text": " the rest of it is basically the kind of thing we've talked about so far, we have the world,", "tokens": [51204, 264, 1472, 295, 309, 307, 1936, 264, 733, 295, 551, 321, 600, 2825, 466, 370, 1400, 11, 321, 362, 264, 1002, 11, 51368], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 117, "seek": 60064, "start": 620.72, "end": 625.56, "text": " we have our policy and our value function and we're interacting with the world,", "tokens": [51368, 321, 362, 527, 3897, 293, 527, 2158, 2445, 293, 321, 434, 18017, 365, 264, 1002, 11, 51610], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 118, "seek": 60064, "start": 625.56, "end": 630.4399999999999, "text": " we're getting rewards and we're getting some observations and then that red box is turning", "tokens": [51610, 321, 434, 1242, 17203, 293, 321, 434, 1242, 512, 18163, 293, 550, 300, 2182, 2424, 307, 6246, 51854], "temperature": 0.0, "avg_logprob": -0.16613618589991289, "compression_ratio": 2.021978021978022, "no_speech_prob": 0.2534234821796417}, {"id": 119, "seek": 63044, "start": 630.44, "end": 637.6, "text": " into a state and so once we get past the red box, it's just like before, we had a state and we can", "tokens": [50364, 666, 257, 1785, 293, 370, 1564, 321, 483, 1791, 264, 2182, 2424, 11, 309, 311, 445, 411, 949, 11, 321, 632, 257, 1785, 293, 321, 393, 50722], "temperature": 0.0, "avg_logprob": -0.1658172810331304, "compression_ratio": 1.9191919191919191, "no_speech_prob": 0.0018083684844896197}, {"id": 120, "seek": 63044, "start": 637.6, "end": 642.5600000000001, "text": " make the, send that state up to the model to be learned and we can send that state up to the", "tokens": [50722, 652, 264, 11, 2845, 300, 1785, 493, 281, 264, 2316, 281, 312, 3264, 293, 321, 393, 2845, 300, 1785, 493, 281, 264, 50970], "temperature": 0.0, "avg_logprob": -0.1658172810331304, "compression_ratio": 1.9191919191919191, "no_speech_prob": 0.0018083684844896197}, {"id": 121, "seek": 63044, "start": 642.5600000000001, "end": 651.1600000000001, "text": " planner to propose things and the planner will do some adjustments to the policy and value", "tokens": [50970, 31268, 281, 17421, 721, 293, 264, 31268, 486, 360, 512, 18624, 281, 264, 3897, 293, 2158, 51400], "temperature": 0.0, "avg_logprob": -0.1658172810331304, "compression_ratio": 1.9191919191919191, "no_speech_prob": 0.0018083684844896197}, {"id": 122, "seek": 63044, "start": 651.1600000000001, "end": 655.5200000000001, "text": " function just like the reward does, but it will come from the planner and this will be the common", "tokens": [51400, 2445, 445, 411, 264, 7782, 775, 11, 457, 309, 486, 808, 490, 264, 31268, 293, 341, 486, 312, 264, 2689, 51618], "temperature": 0.0, "avg_logprob": -0.1658172810331304, "compression_ratio": 1.9191919191919191, "no_speech_prob": 0.0018083684844896197}, {"id": 123, "seek": 65552, "start": 656.12, "end": 662.1999999999999, "text": " path between model free learning and model based learning. Okay, so the thing in the red box,", "tokens": [50394, 3100, 1296, 2316, 1737, 2539, 293, 2316, 2361, 2539, 13, 1033, 11, 370, 264, 551, 294, 264, 2182, 2424, 11, 50698], "temperature": 0.0, "avg_logprob": -0.20177403363314542, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.15179051458835602}, {"id": 124, "seek": 65552, "start": 662.1999999999999, "end": 668.56, "text": " this is the state update function which just says that the agent has to take responsibility for", "tokens": [50698, 341, 307, 264, 1785, 5623, 2445, 597, 445, 1619, 300, 264, 9461, 575, 281, 747, 6357, 337, 51016], "temperature": 0.0, "avg_logprob": -0.20177403363314542, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.15179051458835602}, {"id": 125, "seek": 65552, "start": 668.56, "end": 673.12, "text": " learning some mapping from the observation, the last state and its action to what it's going to use", "tokens": [51016, 2539, 512, 18350, 490, 264, 14816, 11, 264, 1036, 1785, 293, 1080, 3069, 281, 437, 309, 311, 516, 281, 764, 51244], "temperature": 0.0, "avg_logprob": -0.20177403363314542, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.15179051458835602}, {"id": 126, "seek": 65552, "start": 673.12, "end": 677.84, "text": " as its state, it stays as a summary of the past, it's good for making decisions and predicting", "tokens": [51244, 382, 1080, 1785, 11, 309, 10834, 382, 257, 12691, 295, 264, 1791, 11, 309, 311, 665, 337, 1455, 5327, 293, 32884, 51480], "temperature": 0.0, "avg_logprob": -0.20177403363314542, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.15179051458835602}, {"id": 127, "seek": 67784, "start": 677.84, "end": 686.52, "text": " the future and so the state update function is called U, it's exactly this thing and it's", "tokens": [50364, 264, 2027, 293, 370, 264, 1785, 5623, 2445, 307, 1219, 624, 11, 309, 311, 2293, 341, 551, 293, 309, 311, 50798], "temperature": 0.0, "avg_logprob": -0.1819619398850661, "compression_ratio": 1.7815533980582525, "no_speech_prob": 0.2067779153585434}, {"id": 128, "seek": 67784, "start": 686.52, "end": 692.64, "text": " got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U", "tokens": [50798, 658, 281, 312, 3264, 13, 1033, 11, 457, 294, 341, 751, 286, 478, 516, 281, 6552, 300, 264, 1785, 307, 2212, 293, 264, 624, 51104], "temperature": 0.0, "avg_logprob": -0.1819619398850661, "compression_ratio": 1.7815533980582525, "no_speech_prob": 0.2067779153585434}, {"id": 129, "seek": 67784, "start": 692.64, "end": 698.8000000000001, "text": " box is given and I'm going to mostly assume. Anyway, when you talk about changing the state", "tokens": [51104, 2424, 307, 2212, 293, 286, 478, 516, 281, 5240, 6552, 13, 5684, 11, 562, 291, 751, 466, 4473, 264, 1785, 51412], "temperature": 0.0, "avg_logprob": -0.1819619398850661, "compression_ratio": 1.7815533980582525, "no_speech_prob": 0.2067779153585434}, {"id": 130, "seek": 67784, "start": 698.8000000000001, "end": 705.5600000000001, "text": " feature vector or the state representation, that will be the state update function. Okay,", "tokens": [51412, 4111, 8062, 420, 264, 1785, 10290, 11, 300, 486, 312, 264, 1785, 5623, 2445, 13, 1033, 11, 51750], "temperature": 0.0, "avg_logprob": -0.1819619398850661, "compression_ratio": 1.7815533980582525, "no_speech_prob": 0.2067779153585434}, {"id": 131, "seek": 70556, "start": 705.5999999999999, "end": 713.4799999999999, "text": " that's a major extension and at the same time it's almost done because I've got some kind of a", "tokens": [50366, 300, 311, 257, 2563, 10320, 293, 412, 264, 912, 565, 309, 311, 1920, 1096, 570, 286, 600, 658, 512, 733, 295, 257, 50760], "temperature": 0.0, "avg_logprob": -0.1639955308702257, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.007343238685280085}, {"id": 132, "seek": 70556, "start": 713.4799999999999, "end": 718.4, "text": " box and so I've got some kind of a box, produces some kind of a state representation and my methods", "tokens": [50760, 2424, 293, 370, 286, 600, 658, 512, 733, 295, 257, 2424, 11, 14725, 512, 733, 295, 257, 1785, 10290, 293, 452, 7150, 51006], "temperature": 0.0, "avg_logprob": -0.1639955308702257, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.007343238685280085}, {"id": 133, "seek": 70556, "start": 718.4, "end": 723.8, "text": " always, at least once I did the second step, a function approximation, they always were able", "tokens": [51006, 1009, 11, 412, 1935, 1564, 286, 630, 264, 1150, 1823, 11, 257, 2445, 28023, 11, 436, 1009, 645, 1075, 51276], "temperature": 0.0, "avg_logprob": -0.1639955308702257, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.007343238685280085}, {"id": 134, "seek": 70556, "start": 723.8, "end": 729.68, "text": " to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect", "tokens": [51276, 281, 3241, 257, 10290, 300, 2067, 380, 4725, 2176, 293, 370, 2035, 624, 2709, 385, 11, 4461, 26714, 51570], "temperature": 0.0, "avg_logprob": -0.1639955308702257, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.007343238685280085}, {"id": 135, "seek": 70556, "start": 729.68, "end": 733.92, "text": " it is, I will be able to do a certain well with it just as I would be able to do certain well", "tokens": [51570, 309, 307, 11, 286, 486, 312, 1075, 281, 360, 257, 1629, 731, 365, 309, 445, 382, 286, 576, 312, 1075, 281, 360, 1629, 731, 51782], "temperature": 0.0, "avg_logprob": -0.1639955308702257, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.007343238685280085}, {"id": 136, "seek": 73392, "start": 733.9599999999999, "end": 742.7199999999999, "text": " with a certain feature vector representing the state. Okay, another big step is that if we do it", "tokens": [50366, 365, 257, 1629, 4111, 8062, 13460, 264, 1785, 13, 1033, 11, 1071, 955, 1823, 307, 300, 498, 321, 360, 309, 50804], "temperature": 0.0, "avg_logprob": -0.18283111270111385, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0021148542873561382}, {"id": 137, "seek": 73392, "start": 742.7199999999999, "end": 748.64, "text": " right, it doesn't we can separate it from all the other issues, just like we have here, which is to", "tokens": [50804, 558, 11, 309, 1177, 380, 321, 393, 4994, 309, 490, 439, 264, 661, 2663, 11, 445, 411, 321, 362, 510, 11, 597, 307, 281, 51100], "temperature": 0.0, "avg_logprob": -0.18283111270111385, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0021148542873561382}, {"id": 138, "seek": 73392, "start": 748.64, "end": 752.28, "text": " do temple abstraction. Really if you take your model of the world, your model of the world is not", "tokens": [51100, 360, 10184, 37765, 13, 4083, 498, 291, 747, 428, 2316, 295, 264, 1002, 11, 428, 2316, 295, 264, 1002, 307, 406, 51282], "temperature": 0.0, "avg_logprob": -0.18283111270111385, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0021148542873561382}, {"id": 139, "seek": 73392, "start": 752.28, "end": 756.8399999999999, "text": " if I'm in this state, I do this action one step later, I'll be in this other state, it's really", "tokens": [51282, 498, 286, 478, 294, 341, 1785, 11, 286, 360, 341, 3069, 472, 1823, 1780, 11, 286, 603, 312, 294, 341, 661, 1785, 11, 309, 311, 534, 51510], "temperature": 0.0, "avg_logprob": -0.18283111270111385, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0021148542873561382}, {"id": 140, "seek": 75684, "start": 756.84, "end": 763.84, "text": " more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a", "tokens": [50364, 544, 411, 11, 1954, 11, 498, 286, 352, 281, 264, 751, 11, 286, 603, 1466, 746, 11, 420, 498, 286, 1190, 1280, 11, 286, 727, 1862, 257, 50714], "temperature": 0.0, "avg_logprob": -0.18371138280751753, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.31354930996894836}, {"id": 141, "seek": 75684, "start": 763.84, "end": 770.5600000000001, "text": " sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big", "tokens": [50714, 11141, 11, 420, 286, 393, 747, 257, 5720, 293, 3147, 281, 6732, 19845, 13, 1033, 11, 370, 729, 366, 2745, 439, 955, 51050], "temperature": 0.0, "avg_logprob": -0.18371138280751753, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.31354930996894836}, {"id": 142, "seek": 75684, "start": 770.5600000000001, "end": 776.08, "text": " multi-step events and we're actually the kind of learning and kind of reasoning and planning we", "tokens": [51050, 4825, 12, 16792, 3931, 293, 321, 434, 767, 264, 733, 295, 2539, 293, 733, 295, 21577, 293, 5038, 321, 51326], "temperature": 0.0, "avg_logprob": -0.18371138280751753, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.31354930996894836}, {"id": 143, "seek": 75684, "start": 776.08, "end": 781.0400000000001, "text": " want to include, should include all those sorts of things. So there is a theory of options which", "tokens": [51326, 528, 281, 4090, 11, 820, 4090, 439, 729, 7527, 295, 721, 13, 407, 456, 307, 257, 5261, 295, 3956, 597, 51574], "temperature": 0.0, "avg_logprob": -0.18371138280751753, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.31354930996894836}, {"id": 144, "seek": 78104, "start": 781.0799999999999, "end": 788.68, "text": " enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases.", "tokens": [50366, 17077, 505, 281, 2387, 729, 17600, 370, 11, 457, 321, 393, 2387, 439, 729, 382, 2293, 294, 264, 912, 3331, 13, 50746], "temperature": 0.0, "avg_logprob": -0.22696642531562097, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.09798613935709}, {"id": 145, "seek": 78104, "start": 788.68, "end": 794.1999999999999, "text": " Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in", "tokens": [50746, 1033, 11, 293, 1036, 437, 30, 440, 4274, 7782, 3287, 11, 264, 4274, 7782, 3287, 11, 286, 603, 751, 466, 300, 294, 51022], "temperature": 0.0, "avg_logprob": -0.22696642531562097, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.09798613935709}, {"id": 146, "seek": 78104, "start": 794.1999999999999, "end": 803.24, "text": " a little bit. So rushing along, I'm talking about open questions in model based reinforcement,", "tokens": [51022, 257, 707, 857, 13, 407, 25876, 2051, 11, 286, 478, 1417, 466, 1269, 1651, 294, 2316, 2361, 29280, 11, 51474], "temperature": 0.0, "avg_logprob": -0.22696642531562097, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.09798613935709}, {"id": 147, "seek": 78104, "start": 803.24, "end": 809.48, "text": " so I have to say a little bit what's closed, what I'm not going to consider open. So these are my", "tokens": [51474, 370, 286, 362, 281, 584, 257, 707, 857, 437, 311, 5395, 11, 437, 286, 478, 406, 516, 281, 1949, 1269, 13, 407, 613, 366, 452, 51786], "temperature": 0.0, "avg_logprob": -0.22696642531562097, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.09798613935709}, {"id": 148, "seek": 80948, "start": 810.32, "end": 815.84, "text": " settings, these are my presumptions, and I say closed-ish because like lots of people will", "tokens": [50406, 6257, 11, 613, 366, 452, 18028, 9799, 11, 293, 286, 584, 5395, 12, 742, 570, 411, 3195, 295, 561, 486, 50682], "temperature": 0.0, "avg_logprob": -0.2253938524910573, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0008293487480841577}, {"id": 149, "seek": 80948, "start": 815.84, "end": 824.08, "text": " disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning", "tokens": [50682, 14091, 365, 385, 11, 420, 436, 576, 14091, 365, 385, 498, 286, 2729, 552, 257, 2931, 11, 1392, 30, 286, 519, 5038, 51094], "temperature": 0.0, "avg_logprob": -0.2253938524910573, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0008293487480841577}, {"id": 150, "seek": 80948, "start": 824.08, "end": 829.32, "text": " should be online, incremental, like asynchronous dynamic programming and like the dynasy system", "tokens": [51094, 820, 312, 2950, 11, 35759, 11, 411, 49174, 8546, 9410, 293, 411, 264, 274, 2534, 5871, 1185, 51356], "temperature": 0.0, "avg_logprob": -0.2253938524910573, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0008293487480841577}, {"id": 151, "seek": 80948, "start": 829.32, "end": 835.16, "text": " you've just seen. I think that models and planning, they should be state to state. So", "tokens": [51356, 291, 600, 445, 1612, 13, 286, 519, 300, 5245, 293, 5038, 11, 436, 820, 312, 1785, 281, 1785, 13, 407, 51648], "temperature": 0.0, "avg_logprob": -0.2253938524910573, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0008293487480841577}, {"id": 152, "seek": 83516, "start": 835.1999999999999, "end": 841.36, "text": " many people in the literature make models and do planning where they include the observations in", "tokens": [50366, 867, 561, 294, 264, 10394, 652, 5245, 293, 360, 5038, 689, 436, 4090, 264, 18163, 294, 50674], "temperature": 0.0, "avg_logprob": -0.17615534334766622, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0037066189106553793}, {"id": 153, "seek": 83516, "start": 841.36, "end": 846.7199999999999, "text": " the plan. You're like, if I did this then I would see that and then I would, no, no, it should just", "tokens": [50674, 264, 1393, 13, 509, 434, 411, 11, 498, 286, 630, 341, 550, 286, 576, 536, 300, 293, 550, 286, 576, 11, 572, 11, 572, 11, 309, 820, 445, 50942], "temperature": 0.0, "avg_logprob": -0.17615534334766622, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0037066189106553793}, {"id": 154, "seek": 83516, "start": 846.7199999999999, "end": 856.64, "text": " be state to state. And if you think about it just a little bit longer, really it's obvious you've", "tokens": [50942, 312, 1785, 281, 1785, 13, 400, 498, 291, 519, 466, 309, 445, 257, 707, 857, 2854, 11, 534, 309, 311, 6322, 291, 600, 51438], "temperature": 0.0, "avg_logprob": -0.17615534334766622, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0037066189106553793}, {"id": 155, "seek": 83516, "start": 856.64, "end": 861.92, "text": " got to be state to state. You don't want to have your observations which are tied to the single", "tokens": [51438, 658, 281, 312, 1785, 281, 1785, 13, 509, 500, 380, 528, 281, 362, 428, 18163, 597, 366, 9601, 281, 264, 2167, 51702], "temperature": 0.0, "avg_logprob": -0.17615534334766622, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0037066189106553793}, {"id": 156, "seek": 86192, "start": 861.92, "end": 867.68, "text": " time step and tied to state update. You want all those to be separated. Okay, now of course it's", "tokens": [50364, 565, 1823, 293, 9601, 281, 1785, 5623, 13, 509, 528, 439, 729, 281, 312, 12005, 13, 1033, 11, 586, 295, 1164, 309, 311, 50652], "temperature": 0.0, "avg_logprob": -0.2121344236569984, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.004904637113213539}, {"id": 157, "seek": 86192, "start": 867.68, "end": 871.68, "text": " not really state to state, the state feature to state feature, state feature vector to state", "tokens": [50652, 406, 534, 1785, 281, 1785, 11, 264, 1785, 4111, 281, 1785, 4111, 11, 1785, 4111, 8062, 281, 1785, 50852], "temperature": 0.0, "avg_logprob": -0.2121344236569984, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.004904637113213539}, {"id": 158, "seek": 86192, "start": 871.68, "end": 877.28, "text": " feature vector, and that will be where the feature vectors are coming from the learn state", "tokens": [50852, 4111, 8062, 11, 293, 300, 486, 312, 689, 264, 4111, 18875, 366, 1348, 490, 264, 1466, 1785, 51132], "temperature": 0.0, "avg_logprob": -0.2121344236569984, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.004904637113213539}, {"id": 159, "seek": 86192, "start": 877.28, "end": 884.64, "text": " update function that we mentioned earlier. Okay, closed models, planning, they should be", "tokens": [51132, 5623, 2445, 300, 321, 2835, 3071, 13, 1033, 11, 5395, 5245, 11, 5038, 11, 436, 820, 312, 51500], "temperature": 0.0, "avg_logprob": -0.2121344236569984, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.004904637113213539}, {"id": 160, "seek": 86192, "start": 884.64, "end": 890.8, "text": " temporarily abstract, there should not be one step, they should be used based on options. Also,", "tokens": [51500, 23750, 12649, 11, 456, 820, 406, 312, 472, 1823, 11, 436, 820, 312, 1143, 2361, 322, 3956, 13, 2743, 11, 51808], "temperature": 0.0, "avg_logprob": -0.2121344236569984, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.004904637113213539}, {"id": 161, "seek": 89080, "start": 891.4399999999999, "end": 896.3199999999999, "text": " search control. Search control is how you decide which states to think about in imagination, and", "tokens": [50396, 3164, 1969, 13, 17180, 1969, 307, 577, 291, 4536, 597, 4368, 281, 519, 466, 294, 12938, 11, 293, 50640], "temperature": 0.0, "avg_logprob": -0.1315941443810096, "compression_ratio": 1.8807692307692307, "no_speech_prob": 0.0006666168919764459}, {"id": 162, "seek": 89080, "start": 896.3199999999999, "end": 901.52, "text": " that's essential for your plan to be efficient. If you think about stupid states, you'll just learn", "tokens": [50640, 300, 311, 7115, 337, 428, 1393, 281, 312, 7148, 13, 759, 291, 519, 466, 6631, 4368, 11, 291, 603, 445, 1466, 50900], "temperature": 0.0, "avg_logprob": -0.1315941443810096, "compression_ratio": 1.8807692307692307, "no_speech_prob": 0.0006666168919764459}, {"id": 163, "seek": 89080, "start": 901.52, "end": 907.12, "text": " stupid things, but if you can just select the key states to think about to form your plan, then you", "tokens": [50900, 6631, 721, 11, 457, 498, 291, 393, 445, 3048, 264, 2141, 4368, 281, 519, 466, 281, 1254, 428, 1393, 11, 550, 291, 51180], "temperature": 0.0, "avg_logprob": -0.1315941443810096, "compression_ratio": 1.8807692307692307, "no_speech_prob": 0.0006666168919764459}, {"id": 164, "seek": 89080, "start": 907.12, "end": 913.4399999999999, "text": " can be efficient and effective in your planning. And the last thing is that, so these are sort of", "tokens": [51180, 393, 312, 7148, 293, 4942, 294, 428, 5038, 13, 400, 264, 1036, 551, 307, 300, 11, 370, 613, 366, 1333, 295, 51496], "temperature": 0.0, "avg_logprob": -0.1315941443810096, "compression_ratio": 1.8807692307692307, "no_speech_prob": 0.0006666168919764459}, {"id": 165, "seek": 89080, "start": 913.4399999999999, "end": 917.68, "text": " like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it", "tokens": [51496, 411, 1566, 11, 286, 643, 341, 11, 286, 10692, 286, 643, 341, 11, 754, 1673, 286, 478, 406, 516, 281, 2028, 365, 309, 51708], "temperature": 0.0, "avg_logprob": -0.1315941443810096, "compression_ratio": 1.8807692307692307, "no_speech_prob": 0.0006666168919764459}, {"id": 166, "seek": 91768, "start": 917.68, "end": 922.88, "text": " directly. And similarly, we need some problems to in order to structure the learning of the", "tokens": [50364, 3838, 13, 400, 14138, 11, 321, 643, 512, 2740, 281, 294, 1668, 281, 3877, 264, 2539, 295, 264, 50624], "temperature": 0.0, "avg_logprob": -0.13882601159250635, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.004069717600941658}, {"id": 167, "seek": 91768, "start": 922.88, "end": 927.76, "text": " options and the option models. Okay, let's go on to the open questions. The open questions. Number", "tokens": [50624, 3956, 293, 264, 3614, 5245, 13, 1033, 11, 718, 311, 352, 322, 281, 264, 1269, 1651, 13, 440, 1269, 1651, 13, 5118, 50868], "temperature": 0.0, "avg_logprob": -0.13882601159250635, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.004069717600941658}, {"id": 168, "seek": 91768, "start": 927.76, "end": 934.0, "text": " one, should the model, what is this model, should it generate sample states, which I suggested,", "tokens": [50868, 472, 11, 820, 264, 2316, 11, 437, 307, 341, 2316, 11, 820, 309, 8460, 6889, 4368, 11, 597, 286, 10945, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13882601159250635, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.004069717600941658}, {"id": 169, "seek": 91768, "start": 934.0, "end": 940.3199999999999, "text": " or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go", "tokens": [51180, 420, 820, 8460, 5176, 4368, 30, 1033, 11, 456, 311, 257, 3840, 295, 721, 833, 300, 13, 400, 286, 478, 516, 281, 352, 51496], "temperature": 0.0, "avg_logprob": -0.13882601159250635, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.004069717600941658}, {"id": 170, "seek": 91768, "start": 940.3199999999999, "end": 944.0799999999999, "text": " through it in detail. But how should planning be done with average war? This is the other big thing", "tokens": [51496, 807, 309, 294, 2607, 13, 583, 577, 820, 5038, 312, 1096, 365, 4274, 1516, 30, 639, 307, 264, 661, 955, 551, 51684], "temperature": 0.0, "avg_logprob": -0.13882601159250635, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.004069717600941658}, {"id": 171, "seek": 94408, "start": 944.08, "end": 950.08, "text": " that I hope to cover today, average war. And then all the other things I won't, I won't probably won't", "tokens": [50364, 300, 286, 1454, 281, 2060, 965, 11, 4274, 1516, 13, 400, 550, 439, 264, 661, 721, 286, 1582, 380, 11, 286, 1582, 380, 1391, 1582, 380, 50664], "temperature": 0.0, "avg_logprob": -0.1113348776294339, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.007344153709709644}, {"id": 172, "seek": 94408, "start": 950.08, "end": 956.1600000000001, "text": " get to. But let's look at, so let's let's go to how we put function approximation in here. And", "tokens": [50664, 483, 281, 13, 583, 718, 311, 574, 412, 11, 370, 718, 311, 718, 311, 352, 281, 577, 321, 829, 2445, 28023, 294, 510, 13, 400, 50968], "temperature": 0.0, "avg_logprob": -0.1113348776294339, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.007344153709709644}, {"id": 173, "seek": 94408, "start": 956.1600000000001, "end": 962.48, "text": " what is the content of the model? So just a little bit of terminology. Of course, planning proceeds", "tokens": [50968, 437, 307, 264, 2701, 295, 264, 2316, 30, 407, 445, 257, 707, 857, 295, 27575, 13, 2720, 1164, 11, 5038, 32280, 51284], "temperature": 0.0, "avg_logprob": -0.1113348776294339, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.007344153709709644}, {"id": 174, "seek": 94408, "start": 962.48, "end": 967.0400000000001, "text": " by using the model to look ahead, imagining something that might happen. Each one of these", "tokens": [51284, 538, 1228, 264, 2316, 281, 574, 2286, 11, 27798, 746, 300, 1062, 1051, 13, 6947, 472, 295, 613, 51512], "temperature": 0.0, "avg_logprob": -0.1113348776294339, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.007344153709709644}, {"id": 175, "seek": 94408, "start": 967.0400000000001, "end": 972.48, "text": " imaginings of the future from a state action pair is called a projection. I'm going to use this word", "tokens": [51512, 23427, 1109, 295, 264, 2027, 490, 257, 1785, 3069, 6119, 307, 1219, 257, 22743, 13, 286, 478, 516, 281, 764, 341, 1349, 51784], "temperature": 0.0, "avg_logprob": -0.1113348776294339, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.007344153709709644}, {"id": 176, "seek": 97248, "start": 972.48, "end": 978.32, "text": " projection. This is where we imagine a future. Okay, and then after one or more projections,", "tokens": [50364, 22743, 13, 639, 307, 689, 321, 3811, 257, 2027, 13, 1033, 11, 293, 550, 934, 472, 420, 544, 32371, 11, 50656], "temperature": 0.0, "avg_logprob": -0.12182115198491694, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0033219780307263136}, {"id": 177, "seek": 97248, "start": 978.32, "end": 983.9200000000001, "text": " we compute something. And then we back it up. That's called a backup. And this goes on forever.", "tokens": [50656, 321, 14722, 746, 13, 400, 550, 321, 646, 309, 493, 13, 663, 311, 1219, 257, 14807, 13, 400, 341, 1709, 322, 5680, 13, 50936], "temperature": 0.0, "avg_logprob": -0.12182115198491694, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0033219780307263136}, {"id": 178, "seek": 97248, "start": 983.9200000000001, "end": 989.6800000000001, "text": " Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm", "tokens": [50936, 1033, 11, 370, 586, 490, 341, 10686, 307, 257, 7476, 14807, 13, 286, 478, 1953, 466, 341, 1785, 293, 286, 478, 51224], "temperature": 0.0, "avg_logprob": -0.12182115198491694, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0033219780307263136}, {"id": 179, "seek": 97248, "start": 989.6800000000001, "end": 993.2, "text": " looking at these state action pairs and imagining might happen. So what would be the projections", "tokens": [51224, 1237, 412, 613, 1785, 3069, 15494, 293, 27798, 1062, 1051, 13, 407, 437, 576, 312, 264, 32371, 51400], "temperature": 0.0, "avg_logprob": -0.12182115198491694, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0033219780307263136}, {"id": 180, "seek": 99320, "start": 993.2, "end": 1000.1600000000001, "text": " in this picture? Uh, Schmach, where's the projections in this picture?", "tokens": [50364, 294, 341, 3036, 30, 4019, 11, 2065, 76, 608, 11, 689, 311, 264, 32371, 294, 341, 3036, 30, 50712], "temperature": 0.0, "avg_logprob": -0.20070067511664497, "compression_ratio": 1.7156862745098038, "no_speech_prob": 0.08874748647212982}, {"id": 181, "seek": 99320, "start": 1006.6400000000001, "end": 1011.6, "text": " At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then", "tokens": [51036, 1711, 264, 1192, 13, 2205, 13, 663, 311, 3879, 2085, 13, 400, 1670, 415, 311, 11, 1670, 415, 658, 309, 3879, 2085, 11, 550, 51284], "temperature": 0.0, "avg_logprob": -0.20070067511664497, "compression_ratio": 1.7156862745098038, "no_speech_prob": 0.08874748647212982}, {"id": 182, "seek": 99320, "start": 1011.6, "end": 1017.36, "text": " everyone can can just do it. Where are the projections? The projections are where you're", "tokens": [51284, 1518, 393, 393, 445, 360, 309, 13, 2305, 366, 264, 32371, 30, 440, 32371, 366, 689, 291, 434, 51572], "temperature": 0.0, "avg_logprob": -0.20070067511664497, "compression_ratio": 1.7156862745098038, "no_speech_prob": 0.08874748647212982}, {"id": 183, "seek": 99320, "start": 1017.36, "end": 1022.72, "text": " imagining the future from a state action pair. This is my test to see if you're actually following", "tokens": [51572, 27798, 264, 2027, 490, 257, 1785, 3069, 6119, 13, 639, 307, 452, 1500, 281, 536, 498, 291, 434, 767, 3480, 51840], "temperature": 0.0, "avg_logprob": -0.20070067511664497, "compression_ratio": 1.7156862745098038, "no_speech_prob": 0.08874748647212982}, {"id": 184, "seek": 102272, "start": 1022.72, "end": 1029.68, "text": " my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state.", "tokens": [50364, 452, 21988, 13, 16217, 490, 1785, 3069, 6119, 11, 291, 3811, 264, 2027, 13, 1033, 11, 613, 11, 341, 307, 257, 1785, 13, 50712], "temperature": 0.0, "avg_logprob": -0.12988815610370938, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0015007854672148824}, {"id": 185, "seek": 102272, "start": 1030.56, "end": 1034.24, "text": " This is a state action pair. Because you can tell because it has an action on it and it comes from", "tokens": [50756, 639, 307, 257, 1785, 3069, 6119, 13, 1436, 291, 393, 980, 570, 309, 575, 364, 3069, 322, 309, 293, 309, 1487, 490, 50940], "temperature": 0.0, "avg_logprob": -0.12988815610370938, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0015007854672148824}, {"id": 186, "seek": 102272, "start": 1034.24, "end": 1038.72, "text": " a state. So it's a state action pair. And then you imagine the future, the projections are here.", "tokens": [50940, 257, 1785, 13, 407, 309, 311, 257, 1785, 3069, 6119, 13, 400, 550, 291, 3811, 264, 2027, 11, 264, 32371, 366, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12988815610370938, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0015007854672148824}, {"id": 187, "seek": 102272, "start": 1038.72, "end": 1044.24, "text": " There are three projections. We're looking ahead, all the actions I might make, and I project what", "tokens": [51164, 821, 366, 1045, 32371, 13, 492, 434, 1237, 2286, 11, 439, 264, 5909, 286, 1062, 652, 11, 293, 286, 1716, 437, 51440], "temperature": 0.0, "avg_logprob": -0.12988815610370938, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0015007854672148824}, {"id": 188, "seek": 102272, "start": 1044.24, "end": 1048.8, "text": " would happen. And I figure out how good they would be. I take the max and I back it up. Okay,", "tokens": [51440, 576, 1051, 13, 400, 286, 2573, 484, 577, 665, 436, 576, 312, 13, 286, 747, 264, 11469, 293, 286, 646, 309, 493, 13, 1033, 11, 51668], "temperature": 0.0, "avg_logprob": -0.12988815610370938, "compression_ratio": 1.8957528957528957, "no_speech_prob": 0.0015007854672148824}, {"id": 189, "seek": 104880, "start": 1048.8799999999999, "end": 1055.52, "text": " so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick.", "tokens": [50368, 370, 264, 14807, 550, 1709, 490, 264, 5510, 281, 264, 1192, 295, 264, 11, 295, 264, 1399, 13, 1033, 13, 1033, 11, 28160, 11, 1702, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1754467248916626, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006562399794347584}, {"id": 190, "seek": 104880, "start": 1061.44, "end": 1065.84, "text": " Well, it's, it's from, from the state action pair to where it goes. This, this part is the", "tokens": [50996, 1042, 11, 309, 311, 11, 309, 311, 490, 11, 490, 264, 1785, 3069, 6119, 281, 689, 309, 1709, 13, 639, 11, 341, 644, 307, 264, 51216], "temperature": 0.0, "avg_logprob": -0.1754467248916626, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006562399794347584}, {"id": 191, "seek": 104880, "start": 1065.84, "end": 1073.28, "text": " projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here?", "tokens": [51216, 22743, 13, 1779, 13, 1033, 11, 665, 13, 400, 437, 466, 341, 3036, 11, 28160, 30, 2305, 366, 264, 32371, 510, 30, 51588], "temperature": 0.0, "avg_logprob": -0.1754467248916626, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006562399794347584}, {"id": 192, "seek": 107880, "start": 1079.28, "end": 1079.9199999999998, "text": " Say that again.", "tokens": [50388, 6463, 300, 797, 13, 50420], "temperature": 0.0, "avg_logprob": -0.19571573709704213, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0047545116394758224}, {"id": 193, "seek": 107880, "start": 1085.44, "end": 1092.08, "text": " You should have, you should be sure by now. So there are the projections. So this is,", "tokens": [50696, 509, 820, 362, 11, 291, 820, 312, 988, 538, 586, 13, 407, 456, 366, 264, 32371, 13, 407, 341, 307, 11, 51028], "temperature": 0.0, "avg_logprob": -0.19571573709704213, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0047545116394758224}, {"id": 194, "seek": 107880, "start": 1092.08, "end": 1098.72, "text": " this is a long skinny sequence. This is a skinny backup. So we're probably sampling,", "tokens": [51028, 341, 307, 257, 938, 25193, 8310, 13, 639, 307, 257, 25193, 14807, 13, 407, 321, 434, 1391, 21179, 11, 51360], "temperature": 0.0, "avg_logprob": -0.19571573709704213, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0047545116394758224}, {"id": 195, "seek": 107880, "start": 1098.72, "end": 1102.32, "text": " instead of doing all possible actions, we're sampling an action, we're sampling a next state,", "tokens": [51360, 2602, 295, 884, 439, 1944, 5909, 11, 321, 434, 21179, 364, 3069, 11, 321, 434, 21179, 257, 958, 1785, 11, 51540], "temperature": 0.0, "avg_logprob": -0.19571573709704213, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0047545116394758224}, {"id": 196, "seek": 107880, "start": 1102.32, "end": 1105.6, "text": " we're sampling an action after that, and we're sampling an x state after that. But these two", "tokens": [51540, 321, 434, 21179, 364, 3069, 934, 300, 11, 293, 321, 434, 21179, 364, 2031, 1785, 934, 300, 13, 583, 613, 732, 51704], "temperature": 0.0, "avg_logprob": -0.19571573709704213, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0047545116394758224}, {"id": 197, "seek": 110560, "start": 1105.6, "end": 1109.6799999999998, "text": " are the, are the projection parts. The other parts are parts that the agent is doing. The agent", "tokens": [50364, 366, 264, 11, 366, 264, 22743, 3166, 13, 440, 661, 3166, 366, 3166, 300, 264, 9461, 307, 884, 13, 440, 9461, 50568], "temperature": 0.0, "avg_logprob": -0.14711322303579635, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.004828797187656164}, {"id": 198, "seek": 110560, "start": 1109.6799999999998, "end": 1115.12, "text": " says, suppose I do this action, and then ask the model, what would happen, the projection? Okay.", "tokens": [50568, 1619, 11, 7297, 286, 360, 341, 3069, 11, 293, 550, 1029, 264, 2316, 11, 437, 576, 1051, 11, 264, 22743, 30, 1033, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14711322303579635, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.004828797187656164}, {"id": 199, "seek": 110560, "start": 1115.12, "end": 1122.0, "text": " And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves,", "tokens": [50840, 400, 370, 437, 466, 264, 14807, 510, 30, 1033, 13, 407, 264, 14807, 510, 1709, 490, 11, 490, 264, 11, 490, 264, 5510, 11, 51184], "temperature": 0.0, "avg_logprob": -0.14711322303579635, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.004828797187656164}, {"id": 200, "seek": 110560, "start": 1122.0, "end": 1123.4399999999998, "text": " always goes from the leaves to the top.", "tokens": [51184, 1009, 1709, 490, 264, 5510, 281, 264, 1192, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14711322303579635, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.004828797187656164}, {"id": 201, "seek": 110560, "start": 1128.7199999999998, "end": 1135.1999999999998, "text": " No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part,", "tokens": [51520, 883, 11, 572, 11, 572, 11, 572, 13, 1726, 264, 636, 286, 478, 516, 281, 764, 264, 1349, 13, 1033, 13, 400, 341, 11, 341, 307, 11, 341, 644, 11, 51844], "temperature": 0.0, "avg_logprob": -0.14711322303579635, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.004828797187656164}, {"id": 202, "seek": 113520, "start": 1135.2, "end": 1139.28, "text": " anyway, is definitely your choice. It's not an imagination about what the world might do.", "tokens": [50364, 4033, 11, 307, 2138, 428, 3922, 13, 467, 311, 406, 364, 12938, 466, 437, 264, 1002, 1062, 360, 13, 50568], "temperature": 0.0, "avg_logprob": -0.10189809337739021, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.0006262042443268001}, {"id": 203, "seek": 113520, "start": 1140.0, "end": 1145.04, "text": " Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one,", "tokens": [50604, 1033, 13, 286, 478, 516, 281, 764, 264, 1002, 13, 286, 478, 445, 516, 281, 11, 1392, 13, 407, 11, 293, 550, 264, 1036, 472, 11, 50856], "temperature": 0.0, "avg_logprob": -0.10189809337739021, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.0006262042443268001}, {"id": 204, "seek": 113520, "start": 1145.76, "end": 1152.64, "text": " the projections are here. Now these two states, they might be the same state. Maybe I imagined", "tokens": [50892, 264, 32371, 366, 510, 13, 823, 613, 732, 4368, 11, 436, 1062, 312, 264, 912, 1785, 13, 2704, 286, 16590, 51236], "temperature": 0.0, "avg_logprob": -0.10189809337739021, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.0006262042443268001}, {"id": 205, "seek": 113520, "start": 1152.64, "end": 1158.48, "text": " this one, and I said, huh, now what, what if I was there and I did this one? So that,", "tokens": [51236, 341, 472, 11, 293, 286, 848, 11, 7020, 11, 586, 437, 11, 437, 498, 286, 390, 456, 293, 286, 630, 341, 472, 30, 407, 300, 11, 51528], "temperature": 0.0, "avg_logprob": -0.10189809337739021, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.0006262042443268001}, {"id": 206, "seek": 113520, "start": 1160.48, "end": 1164.88, "text": " so that if they were the same state, you might imagine doing the same thing. But in fact,", "tokens": [51628, 370, 300, 498, 436, 645, 264, 912, 1785, 11, 291, 1062, 3811, 884, 264, 912, 551, 13, 583, 294, 1186, 11, 51848], "temperature": 0.0, "avg_logprob": -0.10189809337739021, "compression_ratio": 1.7768924302788844, "no_speech_prob": 0.0006262042443268001}, {"id": 207, "seek": 116488, "start": 1164.88, "end": 1169.2, "text": " by definition, as a backup, they are separate backups, and you'd get these two, two together.", "tokens": [50364, 538, 7123, 11, 382, 257, 14807, 11, 436, 366, 4994, 50160, 11, 293, 291, 1116, 483, 613, 732, 11, 732, 1214, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 208, "seek": 116488, "start": 1169.2, "end": 1173.44, "text": " And if you did this one first, and then you did that one, then it might be a similar effect,", "tokens": [50580, 400, 498, 291, 630, 341, 472, 700, 11, 293, 550, 291, 630, 300, 472, 11, 550, 309, 1062, 312, 257, 2531, 1802, 11, 50792], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 209, "seek": 116488, "start": 1173.44, "end": 1177.1200000000001, "text": " because you would change the value, estimated value of this state, and then you change, use", "tokens": [50792, 570, 291, 576, 1319, 264, 2158, 11, 14109, 2158, 295, 341, 1785, 11, 293, 550, 291, 1319, 11, 764, 50976], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 210, "seek": 116488, "start": 1177.1200000000001, "end": 1180.72, "text": " that to change the estimate of that one, that one. Okay. Okay. I have to keep going.", "tokens": [50976, 300, 281, 1319, 264, 12539, 295, 300, 472, 11, 300, 472, 13, 1033, 13, 1033, 13, 286, 362, 281, 1066, 516, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 211, "seek": 116488, "start": 1184.64, "end": 1185.5200000000002, "text": " Good. So,", "tokens": [51352, 2205, 13, 407, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 212, "seek": 116488, "start": 1190.48, "end": 1193.3600000000001, "text": " this is the biggest question. What is the output of a projection?", "tokens": [51644, 341, 307, 264, 3880, 1168, 13, 708, 307, 264, 5598, 295, 257, 22743, 30, 51788], "temperature": 0.0, "avg_logprob": -0.1377706527709961, "compression_ratio": 1.756, "no_speech_prob": 0.0005882639088667929}, {"id": 213, "seek": 119336, "start": 1194.24, "end": 1199.84, "text": " Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide,", "tokens": [50408, 1033, 13, 5681, 1983, 3413, 11, 309, 311, 11, 309, 311, 1850, 1547, 13, 583, 1564, 321, 483, 3156, 11, 321, 362, 281, 4536, 11, 50688], "temperature": 0.0, "avg_logprob": -0.11835108028622124, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.000472803832963109}, {"id": 214, "seek": 119336, "start": 1199.84, "end": 1205.52, "text": " what is it really? Because we're using a function approximation, and our states are probably real", "tokens": [50688, 437, 307, 309, 534, 30, 1436, 321, 434, 1228, 257, 2445, 28023, 11, 293, 527, 4368, 366, 1391, 957, 50972], "temperature": 0.0, "avg_logprob": -0.11835108028622124, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.000472803832963109}, {"id": 215, "seek": 119336, "start": 1205.52, "end": 1213.52, "text": " valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in", "tokens": [50972, 22608, 4111, 18875, 13, 400, 370, 11, 437, 360, 286, 643, 30, 708, 11, 437, 307, 264, 5598, 295, 257, 2316, 30, 1743, 11, 286, 478, 294, 51372], "temperature": 0.0, "avg_logprob": -0.11835108028622124, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.000472803832963109}, {"id": 216, "seek": 119336, "start": 1213.52, "end": 1218.24, "text": " this, I'm imagining being in the state, doing an action, but the world is stochastic, many things", "tokens": [51372, 341, 11, 286, 478, 27798, 885, 294, 264, 1785, 11, 884, 364, 3069, 11, 457, 264, 1002, 307, 342, 8997, 2750, 11, 867, 721, 51608], "temperature": 0.0, "avg_logprob": -0.11835108028622124, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.000472803832963109}, {"id": 217, "seek": 119336, "start": 1218.24, "end": 1222.1599999999999, "text": " could happen. So one thing I could do is I could represent the whole distribution of all the things", "tokens": [51608, 727, 1051, 13, 407, 472, 551, 286, 727, 360, 307, 286, 727, 2906, 264, 1379, 7316, 295, 439, 264, 721, 51804], "temperature": 0.0, "avg_logprob": -0.11835108028622124, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.000472803832963109}, {"id": 218, "seek": 122216, "start": 1222.16, "end": 1227.44, "text": " that could happen. Okay. This isn't totally crazy. People are doing this. This system called", "tokens": [50364, 300, 727, 1051, 13, 1033, 13, 639, 1943, 380, 3879, 3219, 13, 3432, 366, 884, 341, 13, 639, 1185, 1219, 50628], "temperature": 0.0, "avg_logprob": -0.15890904558383354, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015484531177207828}, {"id": 219, "seek": 122216, "start": 1227.44, "end": 1235.76, "text": " Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are,", "tokens": [50628, 18026, 4093, 538, 3934, 413, 10987, 81, 900, 11, 293, 415, 311, 884, 300, 13, 583, 309, 311, 19011, 570, 37870, 366, 11, 51044], "temperature": 0.0, "avg_logprob": -0.15890904558383354, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015484531177207828}, {"id": 220, "seek": 122216, "start": 1235.76, "end": 1241.76, "text": " are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated,", "tokens": [51044, 366, 2416, 295, 957, 2158, 4111, 18875, 13, 467, 311, 257, 11, 309, 311, 11, 309, 311, 11, 436, 434, 2416, 11, 436, 434, 6179, 11, 51344], "temperature": 0.0, "avg_logprob": -0.15890904558383354, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015484531177207828}, {"id": 221, "seek": 122216, "start": 1241.76, "end": 1246.24, "text": " they're, they're going to, we want methods that are general and scalable and proximal. And so that", "tokens": [51344, 436, 434, 11, 436, 434, 516, 281, 11, 321, 528, 7150, 300, 366, 2674, 293, 38481, 293, 21932, 304, 13, 400, 370, 300, 51568], "temperature": 0.0, "avg_logprob": -0.15890904558383354, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015484531177207828}, {"id": 222, "seek": 122216, "start": 1246.24, "end": 1251.52, "text": " we, can we do this without committing to a very specific form for the function approximator?", "tokens": [51568, 321, 11, 393, 321, 360, 341, 1553, 26659, 281, 257, 588, 2685, 1254, 337, 264, 2445, 8542, 1639, 30, 51832], "temperature": 0.0, "avg_logprob": -0.15890904558383354, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015484531177207828}, {"id": 223, "seek": 125152, "start": 1251.52, "end": 1256.4, "text": " I am skeptical that we can do this. Okay. This is the first question, the first open question.", "tokens": [50364, 286, 669, 28601, 300, 321, 393, 360, 341, 13, 1033, 13, 639, 307, 264, 700, 1168, 11, 264, 700, 1269, 1168, 13, 50608], "temperature": 0.0, "avg_logprob": -0.07816830277442932, "compression_ratio": 1.9019607843137254, "no_speech_prob": 0.0014545063022524118}, {"id": 224, "seek": 125152, "start": 1256.4, "end": 1261.12, "text": " I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution.", "tokens": [50608, 286, 478, 516, 281, 584, 286, 478, 28601, 11, 457, 286, 478, 534, 1566, 309, 311, 1269, 13, 2704, 291, 393, 360, 309, 382, 257, 7316, 13, 50844], "temperature": 0.0, "avg_logprob": -0.07816830277442932, "compression_ratio": 1.9019607843137254, "no_speech_prob": 0.0014545063022524118}, {"id": 225, "seek": 125152, "start": 1262.32, "end": 1266.56, "text": " But if, if you did this, then how could you roll it out? How could you iterate it? How could you", "tokens": [50904, 583, 498, 11, 498, 291, 630, 341, 11, 550, 577, 727, 291, 3373, 309, 484, 30, 1012, 727, 291, 44497, 309, 30, 1012, 727, 291, 51116], "temperature": 0.0, "avg_logprob": -0.07816830277442932, "compression_ratio": 1.9019607843137254, "no_speech_prob": 0.0014545063022524118}, {"id": 226, "seek": 125152, "start": 1266.56, "end": 1270.24, "text": " go to another step? Because you'd go from a state action period to a distribution. Here's a messy", "tokens": [51116, 352, 281, 1071, 1823, 30, 1436, 291, 1116, 352, 490, 257, 1785, 3069, 2896, 281, 257, 7316, 13, 1692, 311, 257, 16191, 51300], "temperature": 0.0, "avg_logprob": -0.07816830277442932, "compression_ratio": 1.9019607843137254, "no_speech_prob": 0.0014545063022524118}, {"id": 227, "seek": 125152, "start": 1270.8, "end": 1275.2, "text": " distribution thing. And then how could you go from there? How could you roll on to the next", "tokens": [51328, 7316, 551, 13, 400, 550, 577, 727, 291, 352, 490, 456, 30, 1012, 727, 291, 3373, 322, 281, 264, 958, 51548], "temperature": 0.0, "avg_logprob": -0.07816830277442932, "compression_ratio": 1.9019607843137254, "no_speech_prob": 0.0014545063022524118}, {"id": 228, "seek": 127520, "start": 1275.28, "end": 1283.52, "text": " projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course,", "tokens": [50368, 22743, 30, 509, 576, 312, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 257, 707, 857, 19011, 13, 823, 11, 295, 1164, 11, 50780], "temperature": 0.0, "avg_logprob": -0.14622757651589133, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.020327743142843246}, {"id": 229, "seek": 127520, "start": 1283.52, "end": 1288.24, "text": " you can always sample that distribution. And then you have a sample model. So you get the state action", "tokens": [50780, 291, 393, 1009, 6889, 300, 7316, 13, 400, 550, 291, 362, 257, 6889, 2316, 13, 407, 291, 483, 264, 1785, 3069, 51016], "temperature": 0.0, "avg_logprob": -0.14622757651589133, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.020327743142843246}, {"id": 230, "seek": 127520, "start": 1288.24, "end": 1291.8400000000001, "text": " pair and you get a sample of the next state. And then you can roll it out. You say, okay,", "tokens": [51016, 6119, 293, 291, 483, 257, 6889, 295, 264, 958, 1785, 13, 400, 550, 291, 393, 3373, 309, 484, 13, 509, 584, 11, 1392, 11, 51196], "temperature": 0.0, "avg_logprob": -0.14622757651589133, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.020327743142843246}, {"id": 231, "seek": 127520, "start": 1291.8400000000001, "end": 1295.68, "text": " there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there?", "tokens": [51196, 456, 311, 257, 958, 1785, 13, 286, 727, 584, 11, 1392, 11, 586, 7297, 286, 390, 456, 11, 437, 286, 727, 11, 437, 727, 286, 360, 456, 30, 51388], "temperature": 0.0, "avg_logprob": -0.14622757651589133, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.020327743142843246}, {"id": 232, "seek": 127520, "start": 1295.68, "end": 1301.8400000000001, "text": " And you can go on. But you really have many of the same problems because you have to learn", "tokens": [51388, 400, 291, 393, 352, 322, 13, 583, 291, 534, 362, 867, 295, 264, 912, 2740, 570, 291, 362, 281, 1466, 51696], "temperature": 0.0, "avg_logprob": -0.14622757651589133, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.020327743142843246}, {"id": 233, "seek": 130184, "start": 1301.84, "end": 1305.12, "text": " the distribution because you have to, you have to generate a sample from that distribution and", "tokens": [50364, 264, 7316, 570, 291, 362, 281, 11, 291, 362, 281, 8460, 257, 6889, 490, 300, 7316, 293, 50528], "temperature": 0.0, "avg_logprob": -0.1047744353612264, "compression_ratio": 1.8984375, "no_speech_prob": 0.00307474541477859}, {"id": 234, "seek": 130184, "start": 1305.12, "end": 1311.52, "text": " you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility.", "tokens": [50528, 291, 362, 281, 2906, 309, 13, 400, 370, 11, 370, 4033, 11, 341, 307, 11, 341, 307, 11, 341, 307, 11, 341, 307, 257, 957, 7959, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1047744353612264, "compression_ratio": 1.8984375, "no_speech_prob": 0.00307474541477859}, {"id": 235, "seek": 130184, "start": 1311.52, "end": 1317.04, "text": " But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from", "tokens": [50848, 583, 309, 311, 11, 457, 309, 311, 11, 309, 311, 11, 309, 311, 7263, 2252, 281, 652, 300, 11, 281, 1466, 264, 7316, 490, 51124], "temperature": 0.0, "avg_logprob": -0.1047744353612264, "compression_ratio": 1.8984375, "no_speech_prob": 0.00307474541477859}, {"id": 236, "seek": 130184, "start": 1317.04, "end": 1323.6799999999998, "text": " which you sample. And you notice that now planning has become stochastic, because there, you would", "tokens": [51124, 597, 291, 6889, 13, 400, 291, 3449, 300, 586, 5038, 575, 1813, 342, 8997, 2750, 11, 570, 456, 11, 291, 576, 51456], "temperature": 0.0, "avg_logprob": -0.1047744353612264, "compression_ratio": 1.8984375, "no_speech_prob": 0.00307474541477859}, {"id": 237, "seek": 130184, "start": 1323.6799999999998, "end": 1329.36, "text": " have to do many samples like in Monte Carlo tree search of that next state in order to average", "tokens": [51456, 362, 281, 360, 867, 10938, 411, 294, 38105, 45112, 4230, 3164, 295, 300, 958, 1785, 294, 1668, 281, 4274, 51740], "temperature": 0.0, "avg_logprob": -0.1047744353612264, "compression_ratio": 1.8984375, "no_speech_prob": 0.00307474541477859}, {"id": 238, "seek": 132936, "start": 1329.36, "end": 1333.6, "text": " over them and get an expected expectation. Whereas up here, it was deterministic. I get you", "tokens": [50364, 670, 552, 293, 483, 364, 5176, 14334, 13, 13813, 493, 510, 11, 309, 390, 15957, 3142, 13, 286, 483, 291, 50576], "temperature": 0.0, "avg_logprob": -0.13789105158980175, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.0013668215833604336}, {"id": 239, "seek": 132936, "start": 1333.6, "end": 1338.6399999999999, "text": " the whole deterministic distribution. Okay. And then there's the third case, which I like the best,", "tokens": [50576, 264, 1379, 15957, 3142, 7316, 13, 1033, 13, 400, 550, 456, 311, 264, 2636, 1389, 11, 597, 286, 411, 264, 1151, 11, 50828], "temperature": 0.0, "avg_logprob": -0.13789105158980175, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.0013668215833604336}, {"id": 240, "seek": 132936, "start": 1338.6399999999999, "end": 1344.24, "text": " which is that you learn the output of a projection is an expectation, an expected feature vector.", "tokens": [50828, 597, 307, 300, 291, 1466, 264, 5598, 295, 257, 22743, 307, 364, 14334, 11, 364, 5176, 4111, 8062, 13, 51108], "temperature": 0.0, "avg_logprob": -0.13789105158980175, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.0013668215833604336}, {"id": 241, "seek": 132936, "start": 1344.24, "end": 1352.3999999999999, "text": " Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out", "tokens": [51108, 7807, 505, 364, 14334, 2316, 13, 400, 370, 341, 307, 611, 15957, 3142, 11, 457, 1310, 309, 393, 380, 312, 14306, 484, 51516], "temperature": 0.0, "avg_logprob": -0.13789105158980175, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.0013668215833604336}, {"id": 242, "seek": 135240, "start": 1352.96, "end": 1360.72, "text": " because you get this exp, you know, this average of feature vectors for the next state. And it's", "tokens": [50392, 570, 291, 483, 341, 1278, 11, 291, 458, 11, 341, 4274, 295, 4111, 18875, 337, 264, 958, 1785, 13, 400, 309, 311, 50780], "temperature": 0.0, "avg_logprob": -0.2586078643798828, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.10663286596536636}, {"id": 243, "seek": 135240, "start": 1360.72, "end": 1365.1200000000001, "text": " straightforward to learn this expectation models, because that's what all of our Algorb zoo, they", "tokens": [50780, 15325, 281, 1466, 341, 14334, 5245, 11, 570, 300, 311, 437, 439, 295, 527, 967, 26465, 65, 25347, 11, 436, 51000], "temperature": 0.0, "avg_logprob": -0.2586078643798828, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.10663286596536636}, {"id": 244, "seek": 135240, "start": 1365.1200000000001, "end": 1370.24, "text": " learn expectations. And, but in general, we've lost information. If you only have the expected", "tokens": [51000, 1466, 9843, 13, 400, 11, 457, 294, 2674, 11, 321, 600, 2731, 1589, 13, 759, 291, 787, 362, 264, 5176, 51256], "temperature": 0.0, "avg_logprob": -0.2586078643798828, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.10663286596536636}, {"id": 245, "seek": 135240, "start": 1370.24, "end": 1374.64, "text": " next feature vector, instead of the whole distribution, you lose special things. But,", "tokens": [51256, 958, 4111, 8062, 11, 2602, 295, 264, 1379, 7316, 11, 291, 3624, 2121, 721, 13, 583, 11, 51476], "temperature": 0.0, "avg_logprob": -0.2586078643798828, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.10663286596536636}, {"id": 246, "seek": 135240, "start": 1376.64, "end": 1380.3200000000002, "text": " but there's this important fact, mathematical fact that in the special case of linear value", "tokens": [51576, 457, 456, 311, 341, 1021, 1186, 11, 18894, 1186, 300, 294, 264, 2121, 1389, 295, 8213, 2158, 51760], "temperature": 0.0, "avg_logprob": -0.2586078643798828, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.10663286596536636}, {"id": 247, "seek": 138032, "start": 1381.28, "end": 1390.96, "text": " functions, you actually don't lose anything. So I, I guess I don't have time to do this equation.", "tokens": [50412, 6828, 11, 291, 767, 500, 380, 3624, 1340, 13, 407, 286, 11, 286, 2041, 286, 500, 380, 362, 565, 281, 360, 341, 5367, 13, 50896], "temperature": 0.0, "avg_logprob": -0.14805574700383856, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0059094359166920185}, {"id": 248, "seek": 138032, "start": 1390.96, "end": 1395.36, "text": " So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically,", "tokens": [50896, 407, 286, 603, 445, 584, 300, 264, 935, 11, 309, 311, 445, 257, 5221, 5367, 11, 1177, 380, 1871, 4033, 13, 583, 1936, 11, 51116], "temperature": 0.0, "avg_logprob": -0.14805574700383856, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0059094359166920185}, {"id": 249, "seek": 138032, "start": 1395.36, "end": 1400.72, "text": " we can show that if you do, you're doing the update with the distribution model, and you can", "tokens": [51116, 321, 393, 855, 300, 498, 291, 360, 11, 291, 434, 884, 264, 5623, 365, 264, 7316, 2316, 11, 293, 291, 393, 51384], "temperature": 0.0, "avg_logprob": -0.14805574700383856, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0059094359166920185}, {"id": 250, "seek": 138032, "start": 1400.72, "end": 1405.76, "text": " write up mathematically, this is what it is. And then just through a few steps, you can prove that", "tokens": [51384, 2464, 493, 44003, 11, 341, 307, 437, 309, 307, 13, 400, 550, 445, 807, 257, 1326, 4439, 11, 291, 393, 7081, 300, 51636], "temperature": 0.0, "avg_logprob": -0.14805574700383856, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0059094359166920185}, {"id": 251, "seek": 140576, "start": 1405.76, "end": 1412.72, "text": " you get exactly the same thing if you, if you use an expectation model. So here, this is, this is", "tokens": [50364, 291, 483, 2293, 264, 912, 551, 498, 291, 11, 498, 291, 764, 364, 14334, 2316, 13, 407, 510, 11, 341, 307, 11, 341, 307, 50712], "temperature": 0.0, "avg_logprob": -0.11261668958162006, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.0027571760583668947}, {"id": 252, "seek": 140576, "start": 1412.72, "end": 1417.6, "text": " the probability distribution of the next states. Here we have the expected next feature vector for", "tokens": [50712, 264, 8482, 7316, 295, 264, 958, 4368, 13, 1692, 321, 362, 264, 5176, 958, 4111, 8062, 337, 50956], "temperature": 0.0, "avg_logprob": -0.11261668958162006, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.0027571760583668947}, {"id": 253, "seek": 140576, "start": 1417.6, "end": 1423.84, "text": " the state, and the, the action or option O. And you can show that these are equal in the special", "tokens": [50956, 264, 1785, 11, 293, 264, 11, 264, 3069, 420, 3614, 422, 13, 400, 291, 393, 855, 300, 613, 366, 2681, 294, 264, 2121, 51268], "temperature": 0.0, "avg_logprob": -0.11261668958162006, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.0027571760583668947}, {"id": 254, "seek": 140576, "start": 1423.84, "end": 1432.64, "text": " case, where the value functions are linear. Okay. So this is open questions, open questions. So", "tokens": [51268, 1389, 11, 689, 264, 2158, 6828, 366, 8213, 13, 1033, 13, 407, 341, 307, 1269, 1651, 11, 1269, 1651, 13, 407, 51708], "temperature": 0.0, "avg_logprob": -0.11261668958162006, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.0027571760583668947}, {"id": 255, "seek": 143264, "start": 1433.3600000000001, "end": 1439.2, "text": " this is just a proposed strategy, is that with linear value functions and expectation models.", "tokens": [50400, 341, 307, 445, 257, 10348, 5206, 11, 307, 300, 365, 8213, 2158, 6828, 293, 14334, 5245, 13, 50692], "temperature": 0.0, "avg_logprob": -0.16013168252032736, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0002034057688433677}, {"id": 256, "seek": 143264, "start": 1440.0, "end": 1445.2800000000002, "text": " And so, you know, I just want to talk a little bit about this question, should the value function", "tokens": [50732, 400, 370, 11, 291, 458, 11, 286, 445, 528, 281, 751, 257, 707, 857, 466, 341, 1168, 11, 820, 264, 2158, 2445, 50996], "temperature": 0.0, "avg_logprob": -0.16013168252032736, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0002034057688433677}, {"id": 257, "seek": 143264, "start": 1445.2800000000002, "end": 1451.0400000000002, "text": " be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question", "tokens": [50996, 312, 8213, 30, 467, 4045, 505, 281, 360, 341, 11, 293, 1177, 380, 534, 3624, 1340, 13, 583, 534, 11, 309, 311, 257, 1168, 51284], "temperature": 0.0, "avg_logprob": -0.16013168252032736, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0002034057688433677}, {"id": 258, "seek": 143264, "start": 1451.0400000000002, "end": 1455.92, "text": " of moving the work around, whatever you do, you have to learn the nonlinear relationship.", "tokens": [51284, 295, 2684, 264, 589, 926, 11, 2035, 291, 360, 11, 291, 362, 281, 1466, 264, 2107, 28263, 2480, 13, 51528], "temperature": 0.0, "avg_logprob": -0.16013168252032736, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0002034057688433677}, {"id": 259, "seek": 145592, "start": 1456.5600000000002, "end": 1463.04, "text": " And the strategy of, of an expectation model is that the nonlinear work is done in the state", "tokens": [50396, 400, 264, 5206, 295, 11, 295, 364, 14334, 2316, 307, 300, 264, 2107, 28263, 589, 307, 1096, 294, 264, 1785, 50720], "temperature": 0.0, "avg_logprob": -0.11946250915527344, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.02404613606631756}, {"id": 260, "seek": 145592, "start": 1463.04, "end": 1469.92, "text": " update function. So it puts the burden on the state update function. And so here I want to talk", "tokens": [50720, 5623, 2445, 13, 407, 309, 8137, 264, 12578, 322, 264, 1785, 5623, 2445, 13, 400, 370, 510, 286, 528, 281, 751, 51064], "temperature": 0.0, "avg_logprob": -0.11946250915527344, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.02404613606631756}, {"id": 261, "seek": 145592, "start": 1469.92, "end": 1480.24, "text": " about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this", "tokens": [51064, 466, 21028, 311, 1433, 13, 1119, 21028, 510, 30, 876, 11, 665, 13, 286, 393, 3932, 309, 390, 3892, 13, 400, 321, 362, 341, 11, 341, 11, 341, 51580], "temperature": 0.0, "avg_logprob": -0.11946250915527344, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.02404613606631756}, {"id": 262, "seek": 145592, "start": 1480.24, "end": 1484.8000000000002, "text": " picture from the book of the shape of all the backups. Now, these are the shapes of the backup", "tokens": [51580, 3036, 490, 264, 1446, 295, 264, 3909, 295, 439, 264, 50160, 13, 823, 11, 613, 366, 264, 10854, 295, 264, 14807, 51808], "temperature": 0.0, "avg_logprob": -0.11946250915527344, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.02404613606631756}, {"id": 263, "seek": 148480, "start": 1484.8, "end": 1490.32, "text": " really. This side is planning, that side was seen as not planning. You know, just TD and Monte", "tokens": [50364, 534, 13, 639, 1252, 307, 5038, 11, 300, 1252, 390, 1612, 382, 406, 5038, 13, 509, 458, 11, 445, 42606, 293, 38105, 50640], "temperature": 0.0, "avg_logprob": -0.14238093761687584, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251136116683483}, {"id": 264, "seek": 148480, "start": 1490.32, "end": 1495.76, "text": " Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning", "tokens": [50640, 45112, 2539, 13, 583, 586, 286, 528, 291, 281, 519, 300, 534, 11, 321, 393, 360, 1293, 4881, 382, 5038, 13, 29308, 50912], "temperature": 0.0, "avg_logprob": -0.14238093761687584, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251136116683483}, {"id": 265, "seek": 148480, "start": 1495.76, "end": 1502.0, "text": " could could could involve a short, not just the wide backups of dynamic programming and and tree", "tokens": [50912, 727, 727, 727, 9494, 257, 2099, 11, 406, 445, 264, 4874, 50160, 295, 8546, 9410, 293, 293, 4230, 51224], "temperature": 0.0, "avg_logprob": -0.14238093761687584, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251136116683483}, {"id": 266, "seek": 148480, "start": 1502.0, "end": 1510.96, "text": " search or exhaustive search. But you can do the skinny backups. And so my my long short start is", "tokens": [51224, 3164, 420, 14687, 488, 3164, 13, 583, 291, 393, 360, 264, 25193, 50160, 13, 400, 370, 452, 452, 938, 2099, 722, 307, 51672], "temperature": 0.0, "avg_logprob": -0.14238093761687584, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.002251136116683483}, {"id": 267, "seek": 151096, "start": 1510.96, "end": 1516.16, "text": " that I'm going to those are the those are the projections is that I'm going to argue that", "tokens": [50364, 300, 286, 478, 516, 281, 729, 366, 264, 729, 366, 264, 32371, 307, 300, 286, 478, 516, 281, 9695, 300, 50624], "temperature": 0.0, "avg_logprob": -0.12493377449238195, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.0028004890773445368}, {"id": 268, "seek": 151096, "start": 1516.16, "end": 1523.3600000000001, "text": " really everything can be done with the smallest, the smallest backup, just looking ahead from", "tokens": [50624, 534, 1203, 393, 312, 1096, 365, 264, 16998, 11, 264, 16998, 14807, 11, 445, 1237, 2286, 490, 50984], "temperature": 0.0, "avg_logprob": -0.12493377449238195, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.0028004890773445368}, {"id": 269, "seek": 151096, "start": 1523.3600000000001, "end": 1528.96, "text": " sample one action and sample one expectation outcome. And that's that's that's I think is a cool", "tokens": [50984, 6889, 472, 3069, 293, 6889, 472, 14334, 9700, 13, 400, 300, 311, 300, 311, 300, 311, 286, 519, 307, 257, 1627, 51264], "temperature": 0.0, "avg_logprob": -0.12493377449238195, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.0028004890773445368}, {"id": 270, "seek": 151096, "start": 1528.96, "end": 1533.76, "text": " way to do planning. And you can do that without losing anything. Because if you want to assemble a", "tokens": [51264, 636, 281, 360, 5038, 13, 400, 291, 393, 360, 300, 1553, 7027, 1340, 13, 1436, 498, 291, 528, 281, 22364, 257, 51504], "temperature": 0.0, "avg_logprob": -0.12493377449238195, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.0028004890773445368}, {"id": 271, "seek": 151096, "start": 1533.76, "end": 1539.6000000000001, "text": " bunch of these tiny backups in the right order, or in just over and over again, you can learn a", "tokens": [51504, 3840, 295, 613, 5870, 50160, 294, 264, 558, 1668, 11, 420, 294, 445, 670, 293, 670, 797, 11, 291, 393, 1466, 257, 51796], "temperature": 0.0, "avg_logprob": -0.12493377449238195, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.0028004890773445368}, {"id": 272, "seek": 153960, "start": 1539.6, "end": 1549.36, "text": " long plan. Okay. So I have one more slide, just going to briefly talk about the average award", "tokens": [50364, 938, 1393, 13, 1033, 13, 407, 286, 362, 472, 544, 4137, 11, 445, 516, 281, 10515, 751, 466, 264, 4274, 7130, 50852], "temperature": 0.0, "avg_logprob": -0.13293075561523438, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0015244600363075733}, {"id": 273, "seek": 153960, "start": 1549.36, "end": 1555.52, "text": " setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we", "tokens": [50852, 3287, 13, 286, 478, 445, 512, 295, 291, 458, 437, 309, 1355, 13, 2188, 295, 291, 500, 380, 13, 583, 498, 291, 360, 11, 534, 11, 562, 321, 51160], "temperature": 0.0, "avg_logprob": -0.13293075561523438, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0015244600363075733}, {"id": 274, "seek": 153960, "start": 1555.52, "end": 1559.6, "text": " use function approximation, we have to go to the average word setting, we have to give up discounting.", "tokens": [51160, 764, 2445, 28023, 11, 321, 362, 281, 352, 281, 264, 4274, 1349, 3287, 11, 321, 362, 281, 976, 493, 11635, 278, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13293075561523438, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0015244600363075733}, {"id": 275, "seek": 153960, "start": 1560.1599999999999, "end": 1565.04, "text": " And I just want to make the observation in front of you all that that this planning with", "tokens": [51392, 400, 286, 445, 528, 281, 652, 264, 14816, 294, 1868, 295, 291, 439, 300, 300, 341, 5038, 365, 51636], "temperature": 0.0, "avg_logprob": -0.13293075561523438, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0015244600363075733}, {"id": 276, "seek": 156504, "start": 1565.04, "end": 1571.2, "text": " average award, it's still a totally open question. I thought it was easy. But I was thinking about", "tokens": [50364, 4274, 7130, 11, 309, 311, 920, 257, 3879, 1269, 1168, 13, 286, 1194, 309, 390, 1858, 13, 583, 286, 390, 1953, 466, 50672], "temperature": 0.0, "avg_logprob": -0.1173649945305389, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.04601404070854187}, {"id": 277, "seek": 156504, "start": 1571.2, "end": 1577.44, "text": " the other day with Zach. And it's really an open open question. It's even open for the tabular", "tokens": [50672, 264, 661, 786, 365, 21028, 13, 400, 309, 311, 534, 364, 1269, 1269, 1168, 13, 467, 311, 754, 1269, 337, 264, 4421, 1040, 50984], "temperature": 0.0, "avg_logprob": -0.1173649945305389, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.04601404070854187}, {"id": 278, "seek": 156504, "start": 1577.44, "end": 1584.08, "text": " case. You just take one step dine and try to make an average reward version. That would be a paper", "tokens": [50984, 1389, 13, 509, 445, 747, 472, 1823, 274, 533, 293, 853, 281, 652, 364, 4274, 7782, 3037, 13, 663, 576, 312, 257, 3035, 51316], "temperature": 0.0, "avg_logprob": -0.1173649945305389, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.04601404070854187}, {"id": 279, "seek": 156504, "start": 1584.08, "end": 1590.32, "text": " because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for", "tokens": [51316, 570, 309, 311, 309, 311, 406, 412, 439, 1850, 577, 281, 360, 309, 13, 407, 456, 498, 291, 434, 1237, 337, 257, 22288, 4829, 337, 51628], "temperature": 0.0, "avg_logprob": -0.1173649945305389, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.04601404070854187}, {"id": 280, "seek": 159032, "start": 1590.32, "end": 1598.1599999999999, "text": " your to do this summer to get your master's done just in time for September, that would be a good", "tokens": [50364, 428, 281, 360, 341, 4266, 281, 483, 428, 4505, 311, 1096, 445, 294, 565, 337, 7216, 11, 300, 576, 312, 257, 665, 50756], "temperature": 0.0, "avg_logprob": -0.14726962465228458, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.004330103751271963}, {"id": 281, "seek": 159032, "start": 1598.1599999999999, "end": 1603.4399999999998, "text": " one. If you don't have one already. Okay. And there's also questions whether the model should", "tokens": [50756, 472, 13, 759, 291, 500, 380, 362, 472, 1217, 13, 1033, 13, 400, 456, 311, 611, 1651, 1968, 264, 2316, 820, 51020], "temperature": 0.0, "avg_logprob": -0.14726962465228458, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.004330103751271963}, {"id": 282, "seek": 159032, "start": 1603.4399999999998, "end": 1608.48, "text": " should give us the the the expected reward or the expected difference between the reward and the", "tokens": [51020, 820, 976, 505, 264, 264, 264, 5176, 7782, 420, 264, 5176, 2649, 1296, 264, 7782, 293, 264, 51272], "temperature": 0.0, "avg_logprob": -0.14726962465228458, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.004330103751271963}, {"id": 283, "seek": 159032, "start": 1608.48, "end": 1617.12, "text": " average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not", "tokens": [51272, 4274, 7782, 13, 1033, 11, 1732, 2077, 11, 286, 658, 1570, 813, 300, 11, 500, 380, 286, 30, 1033, 11, 572, 11, 286, 478, 406, 799, 286, 478, 406, 51704], "temperature": 0.0, "avg_logprob": -0.14726962465228458, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.004330103751271963}, {"id": 284, "seek": 161712, "start": 1617.12, "end": 1625.9199999999998, "text": " gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these", "tokens": [50364, 799, 312, 300, 1578, 13, 583, 1309, 291, 337, 885, 370, 14537, 13, 1033, 11, 370, 286, 519, 286, 478, 1096, 13, 400, 613, 50804], "temperature": 0.0, "avg_logprob": -0.11354967134188762, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.004754200112074614}, {"id": 285, "seek": 161712, "start": 1625.9199999999998, "end": 1630.2399999999998, "text": " are these are the questions that we started with the open questions. Should the model generate", "tokens": [50804, 366, 613, 366, 264, 1651, 300, 321, 1409, 365, 264, 1269, 1651, 13, 6454, 264, 2316, 8460, 51020], "temperature": 0.0, "avg_logprob": -0.11354967134188762, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.004754200112074614}, {"id": 286, "seek": 161712, "start": 1630.2399999999998, "end": 1634.8, "text": " sample states or expectations? And if it's going to give us expected states, should the value function", "tokens": [51020, 6889, 4368, 420, 9843, 30, 400, 498, 309, 311, 516, 281, 976, 505, 5176, 4368, 11, 820, 264, 2158, 2445, 51248], "temperature": 0.0, "avg_logprob": -0.11354967134188762, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.004754200112074614}, {"id": 287, "seek": 161712, "start": 1634.8, "end": 1641.28, "text": " be linear? We've seen how those fit together nice. And the question is a further question is can state", "tokens": [51248, 312, 8213, 30, 492, 600, 1612, 577, 729, 3318, 1214, 1481, 13, 400, 264, 1168, 307, 257, 3052, 1168, 307, 393, 1785, 51572], "temperature": 0.0, "avg_logprob": -0.11354967134188762, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.004754200112074614}, {"id": 288, "seek": 161712, "start": 1641.28, "end": 1645.6, "text": " update support that can it learn a good enough state features so that the value function can be", "tokens": [51572, 5623, 1406, 300, 393, 309, 1466, 257, 665, 1547, 1785, 4122, 370, 300, 264, 2158, 2445, 393, 312, 51788], "temperature": 0.0, "avg_logprob": -0.11354967134188762, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.004754200112074614}, {"id": 289, "seek": 164560, "start": 1645.6, "end": 1649.12, "text": " linear without losing something important? And then there are other questions about whether the", "tokens": [50364, 8213, 1553, 7027, 746, 1021, 30, 400, 550, 456, 366, 661, 1651, 466, 1968, 264, 50540], "temperature": 0.0, "avg_logprob": -0.12630814197016696, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.006486006546765566}, {"id": 290, "seek": 164560, "start": 1649.12, "end": 1653.76, "text": " model whether this suggests the model should be linear as well, or whether it can be semi linear,", "tokens": [50540, 2316, 1968, 341, 13409, 264, 2316, 820, 312, 8213, 382, 731, 11, 420, 1968, 309, 393, 312, 12909, 8213, 11, 50772], "temperature": 0.0, "avg_logprob": -0.12630814197016696, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.006486006546765566}, {"id": 291, "seek": 164560, "start": 1653.76, "end": 1659.52, "text": " which means like a squashing function applied to a linear function. We talked about how planning", "tokens": [50772, 597, 1355, 411, 257, 2339, 11077, 2445, 6456, 281, 257, 8213, 2445, 13, 492, 2825, 466, 577, 5038, 51060], "temperature": 0.0, "avg_logprob": -0.12630814197016696, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.006486006546765566}, {"id": 292, "seek": 164560, "start": 1660.48, "end": 1665.1999999999998, "text": " should you know, once we combine average reward with planning, unsolved problem, we should work", "tokens": [51108, 820, 291, 458, 11, 1564, 321, 10432, 4274, 7782, 365, 5038, 11, 2693, 29110, 1154, 11, 321, 820, 589, 51344], "temperature": 0.0, "avg_logprob": -0.12630814197016696, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.006486006546765566}, {"id": 293, "seek": 164560, "start": 1665.1999999999998, "end": 1672.08, "text": " on that. We should also worry about how should planning affect the actual actions. And what", "tokens": [51344, 322, 300, 13, 492, 820, 611, 3292, 466, 577, 820, 5038, 3345, 264, 3539, 5909, 13, 400, 437, 51688], "temperature": 0.0, "avg_logprob": -0.12630814197016696, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.006486006546765566}, {"id": 294, "seek": 167208, "start": 1672.6399999999999, "end": 1679.52, "text": " sub problems should direct the construction of the option models. And I can't I shouldn't try to", "tokens": [50392, 1422, 2740, 820, 2047, 264, 6435, 295, 264, 3614, 5245, 13, 400, 286, 393, 380, 286, 4659, 380, 853, 281, 50736], "temperature": 0.0, "avg_logprob": -0.17340179443359374, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.008313177153468132}, {"id": 295, "seek": 167208, "start": 1679.52, "end": 1684.0, "text": " explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is", "tokens": [50736, 2903, 264, 1036, 472, 13, 509, 393, 1029, 385, 466, 309, 498, 291, 1116, 411, 13, 876, 11, 293, 286, 1333, 295, 848, 11, 452, 6338, 307, 50960], "temperature": 0.0, "avg_logprob": -0.17340179443359374, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.008313177153468132}, {"id": 296, "seek": 167208, "start": 1684.0, "end": 1687.9199999999998, "text": " that we want maybe we want to expect the states, maybe one of the value functions be linear,", "tokens": [50960, 300, 321, 528, 1310, 321, 528, 281, 2066, 264, 4368, 11, 1310, 472, 295, 264, 2158, 6828, 312, 8213, 11, 51156], "temperature": 0.0, "avg_logprob": -0.17340179443359374, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.008313177153468132}, {"id": 297, "seek": 167208, "start": 1687.9199999999998, "end": 1693.9199999999998, "text": " maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition,", "tokens": [51156, 1310, 321, 393, 1406, 341, 11, 286, 500, 380, 458, 11, 286, 500, 380, 458, 13, 400, 341, 307, 264, 1168, 295, 4111, 21668, 11, 51456], "temperature": 0.0, "avg_logprob": -0.17340179443359374, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.008313177153468132}, {"id": 298, "seek": 167208, "start": 1693.9199999999998, "end": 1699.36, "text": " that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so", "tokens": [51456, 300, 820, 312, 264, 1422, 2740, 11, 1310, 11, 293, 1310, 321, 393, 6786, 552, 538, 264, 4122, 13, 1033, 11, 370, 51728], "temperature": 0.0, "avg_logprob": -0.17340179443359374, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.008313177153468132}, {"id": 299, "seek": 170208, "start": 1702.08, "end": 1704.6399999999999, "text": " I'm done. Thank you for your attention.", "tokens": [50364, 286, 478, 1096, 13, 1044, 291, 337, 428, 3202, 13, 50492], "temperature": 0.0, "avg_logprob": -0.18932793238391615, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0007670824998058379}, {"id": 300, "seek": 170208, "start": 1709.4399999999998, "end": 1714.3999999999999, "text": " Okay, we do have a little bit of time for questions. I ended abruptly there, but", "tokens": [50732, 1033, 11, 321, 360, 362, 257, 707, 857, 295, 565, 337, 1651, 13, 286, 4590, 49642, 456, 11, 457, 50980], "temperature": 0.0, "avg_logprob": -0.18932793238391615, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0007670824998058379}, {"id": 301, "seek": 170208, "start": 1715.4399999999998, "end": 1721.1999999999998, "text": " that's the story, open questions and model based reinforcement. Please.", "tokens": [51032, 300, 311, 264, 1657, 11, 1269, 1651, 293, 2316, 2361, 29280, 13, 2555, 13, 51320], "temperature": 0.0, "avg_logprob": -0.18932793238391615, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0007670824998058379}, {"id": 302, "seek": 170208, "start": 1723.6799999999998, "end": 1729.28, "text": " Okay, so that's probably should have been one of my my closed questions, because we definitely need", "tokens": [51444, 1033, 11, 370, 300, 311, 1391, 820, 362, 668, 472, 295, 452, 452, 5395, 1651, 11, 570, 321, 2138, 643, 51724], "temperature": 0.0, "avg_logprob": -0.18932793238391615, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0007670824998058379}, {"id": 303, "seek": 172928, "start": 1729.36, "end": 1735.2, "text": " off policy learning in order to learn the models, in order to be do it efficiently. And so the part", "tokens": [50368, 766, 3897, 2539, 294, 1668, 281, 1466, 264, 5245, 11, 294, 1668, 281, 312, 360, 309, 19621, 13, 400, 370, 264, 644, 50660], "temperature": 0.0, "avg_logprob": -0.1156561289514814, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0037638037465512753}, {"id": 304, "seek": 172928, "start": 1735.2, "end": 1742.24, "text": " of the premise is that we're doing off policy learning. And we have, we have a suite of a few", "tokens": [50660, 295, 264, 22045, 307, 300, 321, 434, 884, 766, 3897, 2539, 13, 400, 321, 362, 11, 321, 362, 257, 14205, 295, 257, 1326, 51012], "temperature": 0.0, "avg_logprob": -0.1156561289514814, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0037638037465512753}, {"id": 305, "seek": 172928, "start": 1742.24, "end": 1746.0, "text": " methods that will work on that nowadays. Yeah, off policy learning is essential.", "tokens": [51012, 7150, 300, 486, 589, 322, 300, 13434, 13, 865, 11, 766, 3897, 2539, 307, 7115, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1156561289514814, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0037638037465512753}, {"id": 306, "seek": 172928, "start": 1747.92, "end": 1751.44, "text": " So I would assume that you would want to learn lots of value functions and not just one.", "tokens": [51296, 407, 286, 576, 6552, 300, 291, 576, 528, 281, 1466, 3195, 295, 2158, 6828, 293, 406, 445, 472, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1156561289514814, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0037638037465512753}, {"id": 307, "seek": 172928, "start": 1752.16, "end": 1756.24, "text": " If you want all of them to be linear in your representation, then that's a lot of burden", "tokens": [51508, 759, 291, 528, 439, 295, 552, 281, 312, 8213, 294, 428, 10290, 11, 550, 300, 311, 257, 688, 295, 12578, 51712], "temperature": 0.0, "avg_logprob": -0.1156561289514814, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0037638037465512753}, {"id": 308, "seek": 175624, "start": 1756.24, "end": 1762.08, "text": " on your representation. Yes. So if all the complexity is in the state representation,", "tokens": [50364, 322, 428, 10290, 13, 1079, 13, 407, 498, 439, 264, 14024, 307, 294, 264, 1785, 10290, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09076252804007581, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0013451814884319901}, {"id": 309, "seek": 175624, "start": 1762.08, "end": 1768.48, "text": " then what is the model really giving you? Well, it's giving you the dynamics, which is it's not", "tokens": [50656, 550, 437, 307, 264, 2316, 534, 2902, 291, 30, 1042, 11, 309, 311, 2902, 291, 264, 15679, 11, 597, 307, 309, 311, 406, 50976], "temperature": 0.0, "avg_logprob": -0.09076252804007581, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0013451814884319901}, {"id": 310, "seek": 175624, "start": 1768.48, "end": 1773.44, "text": " in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state", "tokens": [50976, 294, 264, 1785, 11, 264, 1785, 1177, 380, 976, 291, 264, 15679, 13, 467, 307, 257, 688, 295, 589, 322, 264, 322, 264, 1785, 51224], "temperature": 0.0, "avg_logprob": -0.09076252804007581, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0013451814884319901}, {"id": 311, "seek": 175624, "start": 1773.44, "end": 1779.28, "text": " update function. And but more importantly, I'm just realizing that I forgot to thank my new", "tokens": [51224, 5623, 2445, 13, 400, 457, 544, 8906, 11, 286, 478, 445, 16734, 300, 286, 5298, 281, 1309, 452, 777, 51516], "temperature": 0.0, "avg_logprob": -0.09076252804007581, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0013451814884319901}, {"id": 312, "seek": 177928, "start": 1779.28, "end": 1787.28, "text": " collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this,", "tokens": [50364, 39789, 11, 597, 366, 41910, 420, 1176, 545, 347, 11, 293, 16747, 28932, 11, 567, 321, 366, 668, 1364, 322, 341, 11, 50764], "temperature": 0.0, "avg_logprob": -0.2507504508608863, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.03620040789246559}, {"id": 313, "seek": 177928, "start": 1787.28, "end": 1793.04, "text": " and they should be up here. And, and we have we submitted a paper on expectation models to", "tokens": [50764, 293, 436, 820, 312, 493, 510, 13, 400, 11, 293, 321, 362, 321, 14405, 257, 3035, 322, 14334, 5245, 281, 51052], "temperature": 0.0, "avg_logprob": -0.2507504508608863, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.03620040789246559}, {"id": 314, "seek": 177928, "start": 1794.6399999999999, "end": 1801.52, "text": " to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out.", "tokens": [51132, 281, 286, 73, 74, 1301, 11, 286, 73, 74, 1301, 293, 321, 9035, 281, 286, 73, 74, 1301, 13, 407, 300, 644, 311, 668, 3720, 493, 13, 400, 321, 434, 1364, 300, 484, 13, 51476], "temperature": 0.0, "avg_logprob": -0.2507504508608863, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.03620040789246559}, {"id": 315, "seek": 180152, "start": 1801.52, "end": 1808.08, "text": " Yeah. So so a lot of work is going into state update.", "tokens": [50364, 865, 13, 407, 370, 257, 688, 295, 589, 307, 516, 666, 1785, 5623, 13, 50692], "temperature": 0.0, "avg_logprob": -0.26454760664600435, "compression_ratio": 1.3862068965517242, "no_speech_prob": 0.005217618774622679}, {"id": 316, "seek": 180152, "start": 1812.32, "end": 1818.56, "text": " That's that's the strategy. I think I think it's might be appropriate. Good.", "tokens": [50904, 663, 311, 300, 311, 264, 5206, 13, 286, 519, 286, 519, 309, 311, 1062, 312, 6854, 13, 2205, 13, 51216], "temperature": 0.0, "avg_logprob": -0.26454760664600435, "compression_ratio": 1.3862068965517242, "no_speech_prob": 0.005217618774622679}, {"id": 317, "seek": 180152, "start": 1822.56, "end": 1825.68, "text": " I have a couple of observations. I don't know, even whether I'm on the", "tokens": [51416, 286, 362, 257, 1916, 295, 18163, 13, 286, 500, 380, 458, 11, 754, 1968, 286, 478, 322, 264, 51572], "temperature": 0.0, "avg_logprob": -0.26454760664600435, "compression_ratio": 1.3862068965517242, "no_speech_prob": 0.005217618774622679}, {"id": 318, "seek": 182568, "start": 1826.16, "end": 1833.28, "text": " page to you, but I am interested in applied intelligence. One observation is Wall Street,", "tokens": [50388, 3028, 281, 291, 11, 457, 286, 669, 3102, 294, 6456, 7599, 13, 1485, 14816, 307, 9551, 7638, 11, 50744], "temperature": 0.0, "avg_logprob": -0.15938782691955566, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.01064283587038517}, {"id": 319, "seek": 182568, "start": 1833.28, "end": 1840.4, "text": " they seem to have a numerical model of the world. You know, so I mean, that's one world or one model", "tokens": [50744, 436, 1643, 281, 362, 257, 29054, 2316, 295, 264, 1002, 13, 509, 458, 11, 370, 286, 914, 11, 300, 311, 472, 1002, 420, 472, 2316, 51100], "temperature": 0.0, "avg_logprob": -0.15938782691955566, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.01064283587038517}, {"id": 320, "seek": 182568, "start": 1840.4, "end": 1847.28, "text": " that you can look at. It seems to me they're far more numerical than other types of domains.", "tokens": [51100, 300, 291, 393, 574, 412, 13, 467, 2544, 281, 385, 436, 434, 1400, 544, 29054, 813, 661, 3467, 295, 25514, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15938782691955566, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.01064283587038517}, {"id": 321, "seek": 182568, "start": 1847.28, "end": 1853.28, "text": " The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's", "tokens": [51444, 440, 661, 472, 307, 257, 2590, 295, 257, 12482, 12599, 11, 2597, 11, 264, 30745, 12599, 34468, 13, 400, 456, 309, 311, 51744], "temperature": 0.0, "avg_logprob": -0.15938782691955566, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.01064283587038517}, {"id": 322, "seek": 185328, "start": 1853.28, "end": 1860.24, "text": " not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know,", "tokens": [50364, 406, 8213, 11, 411, 264, 12482, 311, 7959, 295, 264, 12482, 3963, 307, 8213, 11, 457, 309, 16335, 1024, 11, 291, 458, 11, 50712], "temperature": 0.0, "avg_logprob": -0.1981652358482624, "compression_ratio": 1.6292682926829267, "no_speech_prob": 0.003171211574226618}, {"id": 323, "seek": 185328, "start": 1860.24, "end": 1866.72, "text": " so it's each duck in the clock can radiate any number of different directions. So I'm not sure", "tokens": [50712, 370, 309, 311, 1184, 12482, 294, 264, 7830, 393, 2843, 13024, 604, 1230, 295, 819, 11095, 13, 407, 286, 478, 406, 988, 51036], "temperature": 0.0, "avg_logprob": -0.1981652358482624, "compression_ratio": 1.6292682926829267, "no_speech_prob": 0.003171211574226618}, {"id": 324, "seek": 185328, "start": 1866.72, "end": 1872.08, "text": " whether your model could cover the eagle catching the ducks or you want to give that to us.", "tokens": [51036, 1968, 428, 2316, 727, 2060, 264, 30745, 16124, 264, 34468, 420, 291, 528, 281, 976, 300, 281, 505, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1981652358482624, "compression_ratio": 1.6292682926829267, "no_speech_prob": 0.003171211574226618}, {"id": 325, "seek": 185328, "start": 1874.72, "end": 1877.04, "text": " Thank you. That's good. So this linearity thing,", "tokens": [51436, 1044, 291, 13, 663, 311, 665, 13, 407, 341, 8213, 507, 551, 11, 51552], "temperature": 0.0, "avg_logprob": -0.1981652358482624, "compression_ratio": 1.6292682926829267, "no_speech_prob": 0.003171211574226618}, {"id": 326, "seek": 187704, "start": 1877.68, "end": 1885.28, "text": " it's a very important that it's linear in the in the features. Okay, and and this is the this is", "tokens": [50396, 309, 311, 257, 588, 1021, 300, 309, 311, 8213, 294, 264, 294, 264, 4122, 13, 1033, 11, 293, 293, 341, 307, 264, 341, 307, 50776], "temperature": 0.0, "avg_logprob": -0.18964536984761557, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.031128032132983208}, {"id": 327, "seek": 187704, "start": 1885.28, "end": 1891.52, "text": " the trick. It's sort of already known, it's well known that anything can be linear if you arrange", "tokens": [50776, 264, 4282, 13, 467, 311, 1333, 295, 1217, 2570, 11, 309, 311, 731, 2570, 300, 1340, 393, 312, 8213, 498, 291, 9424, 51088], "temperature": 0.0, "avg_logprob": -0.18964536984761557, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.031128032132983208}, {"id": 328, "seek": 187704, "start": 1891.52, "end": 1898.8, "text": " the right features. So you could do the duck, you could you could presumably capture the higher", "tokens": [51088, 264, 558, 4122, 13, 407, 291, 727, 360, 264, 12482, 11, 291, 727, 291, 727, 26742, 7983, 264, 2946, 51452], "temperature": 0.0, "avg_logprob": -0.18964536984761557, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.031128032132983208}, {"id": 329, "seek": 187704, "start": 1898.8, "end": 1903.68, "text": " order, the interactions between the, so what do you lose when you get linearity? You know,", "tokens": [51452, 1668, 11, 264, 13280, 1296, 264, 11, 370, 437, 360, 291, 3624, 562, 291, 483, 8213, 507, 30, 509, 458, 11, 51696], "temperature": 0.0, "avg_logprob": -0.18964536984761557, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.031128032132983208}, {"id": 330, "seek": 190368, "start": 1903.76, "end": 1908.3200000000002, "text": " let's say you have two features a and b. Well, if the right choice or the right value", "tokens": [50368, 718, 311, 584, 291, 362, 732, 4122, 257, 293, 272, 13, 1042, 11, 498, 264, 558, 3922, 420, 264, 558, 2158, 50596], "temperature": 0.0, "avg_logprob": -0.16670015823742576, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0012445916654542089}, {"id": 331, "seek": 190368, "start": 1908.3200000000002, "end": 1914.64, "text": " depends upon both of them being present at the same time. And then then then then you can't do", "tokens": [50596, 5946, 3564, 1293, 295, 552, 885, 1974, 412, 264, 912, 565, 13, 400, 550, 550, 550, 550, 291, 393, 380, 360, 50912], "temperature": 0.0, "avg_logprob": -0.16670015823742576, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0012445916654542089}, {"id": 332, "seek": 190368, "start": 1914.64, "end": 1919.76, "text": " that linearly. The linear function, you can only say, oh, this has a certain effect, this has a", "tokens": [50912, 300, 43586, 13, 440, 8213, 2445, 11, 291, 393, 787, 584, 11, 1954, 11, 341, 575, 257, 1629, 1802, 11, 341, 575, 257, 51168], "temperature": 0.0, "avg_logprob": -0.16670015823742576, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0012445916654542089}, {"id": 333, "seek": 190368, "start": 1919.76, "end": 1925.44, "text": " certain effect. And there can't be a special thing that if they're on together. Okay. And so what", "tokens": [51168, 1629, 1802, 13, 400, 456, 393, 380, 312, 257, 2121, 551, 300, 498, 436, 434, 322, 1214, 13, 1033, 13, 400, 370, 437, 51452], "temperature": 0.0, "avg_logprob": -0.16670015823742576, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0012445916654542089}, {"id": 334, "seek": 190368, "start": 1925.44, "end": 1930.48, "text": " you do with that none, and when there's an interaction between variables, you know, maybe it's", "tokens": [51452, 291, 360, 365, 300, 6022, 11, 293, 562, 456, 311, 364, 9285, 1296, 9102, 11, 291, 458, 11, 1310, 309, 311, 51704], "temperature": 0.0, "avg_logprob": -0.16670015823742576, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0012445916654542089}, {"id": 335, "seek": 193048, "start": 1930.48, "end": 1936.8, "text": " it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or", "tokens": [50364, 309, 311, 257, 436, 434, 1293, 1578, 13, 583, 498, 436, 434, 322, 1214, 11, 550, 309, 311, 665, 13, 1033, 11, 370, 309, 311, 13005, 420, 50680], "temperature": 0.0, "avg_logprob": -0.11465196922177175, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.00032501324312761426}, {"id": 336, "seek": 193048, "start": 1937.44, "end": 1941.68, "text": " that's the kind of thing that you can't do. But but we've known since the beginning of neural", "tokens": [50712, 300, 311, 264, 733, 295, 551, 300, 291, 393, 380, 360, 13, 583, 457, 321, 600, 2570, 1670, 264, 2863, 295, 18161, 50924], "temperature": 0.0, "avg_logprob": -0.11465196922177175, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.00032501324312761426}, {"id": 337, "seek": 193048, "start": 1941.68, "end": 1947.84, "text": " networks, that that if you then add a third variable for the conjunction of the two original", "tokens": [50924, 9590, 11, 300, 300, 498, 291, 550, 909, 257, 2636, 7006, 337, 264, 27482, 295, 264, 732, 3380, 51232], "temperature": 0.0, "avg_logprob": -0.11465196922177175, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.00032501324312761426}, {"id": 338, "seek": 193048, "start": 1947.84, "end": 1953.28, "text": " features, then you can learn the nonlinear function in the original in the first two. Okay, and so", "tokens": [51232, 4122, 11, 550, 291, 393, 1466, 264, 2107, 28263, 2445, 294, 264, 3380, 294, 264, 700, 732, 13, 1033, 11, 293, 370, 51504], "temperature": 0.0, "avg_logprob": -0.11465196922177175, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.00032501324312761426}, {"id": 339, "seek": 193048, "start": 1953.28, "end": 1960.0, "text": " the same same is true. So it's not a principle limitation in any way. So think again about", "tokens": [51504, 264, 912, 912, 307, 2074, 13, 407, 309, 311, 406, 257, 8665, 27432, 294, 604, 636, 13, 407, 519, 797, 466, 51840], "temperature": 0.0, "avg_logprob": -0.11465196922177175, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.00032501324312761426}, {"id": 340, "seek": 196000, "start": 1960.08, "end": 1966.96, "text": " a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function,", "tokens": [50368, 257, 2107, 28263, 3209, 11, 411, 11, 411, 11, 291, 458, 11, 294, 20588, 12104, 11, 309, 27152, 341, 6179, 2445, 11, 50712], "temperature": 0.0, "avg_logprob": -0.11428120059351768, "compression_ratio": 1.868, "no_speech_prob": 0.0012840309645980597}, {"id": 341, "seek": 196000, "start": 1966.96, "end": 1972.96, "text": " many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way", "tokens": [50712, 867, 11, 867, 7914, 11, 309, 311, 2107, 28263, 13, 583, 309, 311, 8213, 294, 264, 1036, 4583, 13, 1033, 11, 370, 498, 291, 632, 512, 636, 51012], "temperature": 0.0, "avg_logprob": -0.11428120059351768, "compression_ratio": 1.868, "no_speech_prob": 0.0012840309645980597}, {"id": 342, "seek": 196000, "start": 1972.96, "end": 1978.24, "text": " of finding the features in the last layer, then you could be linear. So in some sense, what we're", "tokens": [51012, 295, 5006, 264, 4122, 294, 264, 1036, 4583, 11, 550, 291, 727, 312, 8213, 13, 407, 294, 512, 2020, 11, 437, 321, 434, 51276], "temperature": 0.0, "avg_logprob": -0.11428120059351768, "compression_ratio": 1.868, "no_speech_prob": 0.0012840309645980597}, {"id": 343, "seek": 196000, "start": 1978.24, "end": 1982.16, "text": " just saying, take the that thing that you worked on in the last layer and make that your state.", "tokens": [51276, 445, 1566, 11, 747, 264, 300, 551, 300, 291, 2732, 322, 294, 264, 1036, 4583, 293, 652, 300, 428, 1785, 13, 51472], "temperature": 0.0, "avg_logprob": -0.11428120059351768, "compression_ratio": 1.868, "no_speech_prob": 0.0012840309645980597}, {"id": 344, "seek": 196000, "start": 1982.16, "end": 1985.12, "text": " And so as your state, then you would have to your models would have to produce it.", "tokens": [51472, 400, 370, 382, 428, 1785, 11, 550, 291, 576, 362, 281, 428, 5245, 576, 362, 281, 5258, 309, 13, 51620], "temperature": 0.0, "avg_logprob": -0.11428120059351768, "compression_ratio": 1.868, "no_speech_prob": 0.0012840309645980597}, {"id": 345, "seek": 198512, "start": 1986.08, "end": 1995.1999999999998, "text": " You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating?", "tokens": [50412, 509, 584, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 257, 5206, 13, 467, 311, 11, 309, 311, 11, 309, 311, 11, 286, 519, 11, 370, 983, 669, 286, 32050, 30, 50868], "temperature": 0.0, "avg_logprob": -0.20332591213397125, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.010011916048824787}, {"id": 346, "seek": 198512, "start": 1995.1999999999998, "end": 1999.6, "text": " I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear", "tokens": [50868, 286, 478, 32050, 570, 754, 498, 291, 500, 380, 11, 498, 291, 360, 11, 498, 291, 645, 516, 281, 853, 281, 1466, 257, 2107, 28263, 51088], "temperature": 0.0, "avg_logprob": -0.20332591213397125, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.010011916048824787}, {"id": 347, "seek": 198512, "start": 1999.6, "end": 2004.7199999999998, "text": " map, then that nonlinear map would have to figure out that that a that these two variables are,", "tokens": [51088, 4471, 11, 550, 300, 2107, 28263, 4471, 576, 362, 281, 2573, 484, 300, 300, 257, 300, 613, 732, 9102, 366, 11, 51344], "temperature": 0.0, "avg_logprob": -0.20332591213397125, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.010011916048824787}, {"id": 348, "seek": 198512, "start": 2004.7199999999998, "end": 2008.8799999999999, "text": " are, need to be treated especially, and they would have to create the conjunction term inside", "tokens": [51344, 366, 11, 643, 281, 312, 8668, 2318, 11, 293, 436, 576, 362, 281, 1884, 264, 27482, 1433, 1854, 51552], "temperature": 0.0, "avg_logprob": -0.20332591213397125, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.010011916048824787}, {"id": 349, "seek": 198512, "start": 2008.8799999999999, "end": 2014.3999999999999, "text": " that that nonlinear mapping. So it's like we're doing the same work, we're pushing it", "tokens": [51552, 300, 300, 2107, 28263, 18350, 13, 407, 309, 311, 411, 321, 434, 884, 264, 912, 589, 11, 321, 434, 7380, 309, 51828], "temperature": 0.0, "avg_logprob": -0.20332591213397125, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.010011916048824787}, {"id": 350, "seek": 201440, "start": 2014.48, "end": 2020.16, "text": " into a different place. We're pushing it into the state update. Yeah, other questions?", "tokens": [50368, 666, 257, 819, 1081, 13, 492, 434, 7380, 309, 666, 264, 1785, 5623, 13, 865, 11, 661, 1651, 30, 50652], "temperature": 0.0, "avg_logprob": -0.26950645446777344, "compression_ratio": 1.0970873786407767, "no_speech_prob": 0.00035669549833983183}, {"id": 351, "seek": 201440, "start": 2030.3200000000002, "end": 2036.5600000000002, "text": " Okay, thank you very much.", "tokens": [51160, 1033, 11, 1309, 291, 588, 709, 13, 51472], "temperature": 0.0, "avg_logprob": -0.26950645446777344, "compression_ratio": 1.0970873786407767, "no_speech_prob": 0.00035669549833983183}], "language": "en"}