1
00:00:00,000 --> 00:00:12,400
Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just

2
00:00:12,400 --> 00:00:15,240
have a welcome you to tea.

3
00:00:15,240 --> 00:00:22,520
We do this each year and it's very informal and I welcome you all to sign up and participate

4
00:00:22,520 --> 00:00:28,440
and to ask questions and you know we're here just to talk about science and ideas and exchange

5
00:00:28,440 --> 00:00:29,760
points of view.

6
00:00:29,760 --> 00:00:37,520
So think of it as a community, enjoy the tea, have a cookie, it's all good.

7
00:00:37,520 --> 00:00:46,440
The other thing is Thursday, Thursday I think we still have an open slot and we could have

8
00:00:46,440 --> 00:00:56,600
Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that

9
00:00:57,560 --> 00:01:00,400
will work out, I'm pretty sure.

10
00:01:00,400 --> 00:01:08,040
Okay, so the topics are really intelligence and how do we understand how the mind might

11
00:01:08,040 --> 00:01:14,400
work and could work and today my topic, really the topic is model-based reinforcement learning.

12
00:01:14,400 --> 00:01:18,360
Model-based reinforcement learning is really it's sort of the whole enchilada.

13
00:01:18,360 --> 00:01:24,560
We try to get all the pieces present, model-based, model-free, make understanding of the world,

14
00:01:24,560 --> 00:01:31,440
be able to reason or plan with it and how many people here have just raised your hand

15
00:01:31,440 --> 00:01:36,560
if you've heard of the phrase model-based reinforcement learning.

16
00:01:36,560 --> 00:01:43,200
How many haven't, how many actually, let me say it a little differently.

17
00:01:43,200 --> 00:01:47,920
How many of you just think it's the model-based reinforcement like the most important thing

18
00:01:47,920 --> 00:01:57,600
in AI, like I do, or should I get a little bit of an argument from authority for it?

19
00:01:57,600 --> 00:02:03,880
Okay, I spent that long talking about it, I might as well give my, I just threw in a

20
00:02:03,880 --> 00:02:09,520
slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon

21
00:02:09,520 --> 00:02:15,440
talking about predictive learning, understanding the world, Yashio Benjo talks about the most

22
00:02:15,440 --> 00:02:19,520
important step in AI is to make predictive causal model of the world.

23
00:02:19,520 --> 00:02:25,680
This is a thing that's coming around at last and this is our topic, our topic is model-based

24
00:02:25,680 --> 00:02:29,080
reinforcement learning and it means making a model of the world, I can use this word

25
00:02:29,080 --> 00:02:33,520
model to mean model of the world, how the world works, I'll only use the word model

26
00:02:33,520 --> 00:02:39,040
as a world model, model of the MDP, the Markov decision process, the transition dynamics of

27
00:02:39,040 --> 00:02:44,800
the world and model-based reinforcement learning, we learn the model, we learn the model and

28
00:02:44,800 --> 00:02:50,280
we also then use the model to plan or reason about what to do.

29
00:02:50,280 --> 00:02:56,640
So this is, we call this planning and planning proceeds by, you imagine states, you look

30
00:02:56,640 --> 00:03:00,320
ahead from the individual states to see what might happen and you back up to improve your

31
00:03:00,320 --> 00:03:05,120
policy or your value with those states that you're imagining might happen and then this

32
00:03:05,120 --> 00:03:09,640
is how you figure out, decide what to do.

33
00:03:09,640 --> 00:03:16,480
Okay so it's decided this idea has been around for a while, I'm going to talk briefly about

34
00:03:16,480 --> 00:03:22,480
the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first

35
00:03:22,480 --> 00:03:27,200
proposed or first explicitly proposed that planning and learning would be radically similar.

36
00:03:27,200 --> 00:03:31,880
So there's some reinforcement learning algorithm that's interacting with the world and then

37
00:03:31,880 --> 00:03:36,360
it's also learning the world model which is another box that you can plug in place of

38
00:03:36,360 --> 00:03:41,460
the world and instead interact with the model for a while, in that sense, and so you're

39
00:03:41,460 --> 00:03:45,520
interacting with the model and you're saying what if I do this, the model says that would

40
00:03:45,520 --> 00:03:49,200
happen, you get this reward and you learn from that just as if it had really happened

41
00:03:49,200 --> 00:03:55,960
and so then this way the planning or the logic and the reasoning, certainly the planning

42
00:03:55,960 --> 00:04:02,280
is radically similar to just acting in the world, just acting in imagination.

43
00:04:02,280 --> 00:04:06,920
So that's the first half of the idea, the second half of the idea is that these things

44
00:04:06,920 --> 00:04:12,120
are all done simultaneously, so planning and learning and executing, they're all been going

45
00:04:12,120 --> 00:04:17,040
on all the time, so you're always interacting with the world, getting new experience, you

46
00:04:17,040 --> 00:04:20,800
can do a model for your reinforcement learning with that bit, but you also use that experience

47
00:04:20,800 --> 00:04:25,440
to make the model and the model through planning and that also affects your value function

48
00:04:25,440 --> 00:04:27,400
and your policy and makes you do better.

49
00:04:27,520 --> 00:04:33,840
Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm

50
00:04:33,840 --> 00:04:38,920
that's kind of useful to complement those diagrams, those visual things with a little

51
00:04:38,920 --> 00:04:43,520
bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from

52
00:04:43,520 --> 00:04:49,520
the book, so basically your Q is your action value function and model SA is your prediction

53
00:04:49,520 --> 00:04:53,440
about what would happen if you were in state S and did action A and you initialize that

54
00:04:53,560 --> 00:04:57,280
and then as you go through life, you're just doing this loop over and over again, you're

55
00:04:57,280 --> 00:05:02,480
looking at the state you're in, you're picking an action in it and set B somehow, then you

56
00:05:02,480 --> 00:05:08,400
take that action, send it to the world, the world gives you back a result, R, the reward

57
00:05:08,400 --> 00:05:14,200
and the next state S prime and so then you do model free learning with that little four

58
00:05:14,200 --> 00:05:20,000
tuple of experience, state action, reward, next state, so here we're doing Q learning

59
00:05:20,080 --> 00:05:25,520
in step D to update the action values and then we update the model because we've seen

60
00:05:25,520 --> 00:05:29,760
what will happen for that state and that action, that this will happen and this is where we're

61
00:05:29,760 --> 00:05:34,640
assuming a deterministic world because really many things might happen if you did that action

62
00:05:34,640 --> 00:05:38,720
in this state and we only saw one thing and now we're going to just overwrite the model

63
00:05:38,720 --> 00:05:42,360
so that I'll predict always that that would happen.

64
00:05:42,360 --> 00:05:46,200
So the thing about DynA, particularly the first instance of it in 1990, it's a very

65
00:05:46,200 --> 00:05:53,840
simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully

66
00:05:53,840 --> 00:05:54,840
express all the possibilities.

67
00:05:54,840 --> 00:05:58,840
We're going to talk about today's, how you can go and consider more possibilities.

68
00:05:58,840 --> 00:06:04,000
Anyway, after you do, now we've got as far as learning the model and then step F is where

69
00:06:04,000 --> 00:06:05,800
you do the plan.

70
00:06:05,800 --> 00:06:09,920
So by the plan, you just imagine some state, maybe randomly from something you've seen

71
00:06:09,920 --> 00:06:15,360
before, imagine an action, you ask the model what would happen if you were there and then

72
00:06:15,480 --> 00:06:19,520
you do that same step, same, it's the same as D, it's Q-learning, but here now we're

73
00:06:19,520 --> 00:06:23,560
doing Q-learning on these imagined things and there we're doing Q-learning on the real thing.

74
00:06:23,560 --> 00:06:28,000
So that's the basic idea of DynA, I'm sure many of you have seen that before.

75
00:06:28,000 --> 00:06:32,320
Now, maybe you've seen the demos of it before, so here's the standard demo where we have

76
00:06:32,320 --> 00:06:37,840
a grid world, we have a start state down in that corner and we have a goal state where

77
00:06:37,840 --> 00:06:43,480
you get a plus one reward and the actions are to move up, down, right and left, so four

78
00:06:43,520 --> 00:06:48,040
actions from each state, the states are all represented tabularly, so like this is state

79
00:06:48,040 --> 00:06:55,040
34 and this is state 92 and the model says, oh, when I go action four in state 34, I end

80
00:06:56,200 --> 00:07:01,720
up in state 92, that's what the model is learning and because it has the model, it is very quickly

81
00:07:01,720 --> 00:07:08,720
learning how to get from start to goal efficiently and it's actually able to learn about states

82
00:07:08,880 --> 00:07:13,040
that it's not in and this becomes most apparent if we pick up the goal and move it to a different

83
00:07:13,040 --> 00:07:18,240
place, so of course once we've moved it, the agent will go back to, it'll go here again

84
00:07:18,240 --> 00:07:23,520
where it used to be and be disappointed so to speak and then it has no idea where it

85
00:07:23,520 --> 00:07:27,040
is, it's built a model but the model says that there's no goal up there, as the last

86
00:07:27,040 --> 00:07:30,880
time it was there, there was no goal, okay, but eventually it will stumble upon the goal

87
00:07:30,880 --> 00:07:36,360
again and then it will be very quickly able to plot a path, so watch when it first finds

88
00:07:36,360 --> 00:07:42,320
it now, here it will eventually see it's learning the right way to go and it's learning a good

89
00:07:42,320 --> 00:07:48,840
path very quickly because it has a model, okay, so that's like tabular dyna, okay, and

90
00:07:48,840 --> 00:07:53,760
today we want to talk about the extensions of this, open questions in it, yeah I guess

91
00:07:53,760 --> 00:07:58,680
I didn't even say it, it's open questions and planning, I'm not going to tell you the answers,

92
00:07:58,680 --> 00:08:04,960
I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic

93
00:08:04,960 --> 00:08:09,840
dynamics, what you saw was just deterministic dynamics, we assume that the world always

94
00:08:09,840 --> 00:08:14,840
went the same way but you could instead of overwriting what the model says in the state

95
00:08:14,840 --> 00:08:18,640
in action pair, you could start to collect a list of all the things that might happen

96
00:08:18,640 --> 00:08:23,960
in their probabilities and then you could sample that and you could do exactly the same thing,

97
00:08:23,960 --> 00:08:28,840
you could add function approximation, now function approximation, I'm going to talk about it,

98
00:08:28,840 --> 00:08:34,840
but it's really a spectrum, a range of degrees of function approximation, so what we saw

99
00:08:34,840 --> 00:08:39,720
was tabular, I call it tabular and that means every state action pair is treated totally

100
00:08:39,720 --> 00:08:43,320
different from every other state action pair, there's no similarity between them, there's

101
00:08:43,320 --> 00:08:47,880
no generalization and so there's just a big table and I store things in that state action

102
00:08:47,880 --> 00:08:53,920
pair and really in real life certainly in computer go and in Atari games and in any

103
00:08:53,920 --> 00:08:59,080
robotics application you have to generalize from one state to another and that's, you

104
00:08:59,080 --> 00:09:03,840
know, you never see the same state twice, okay, but we start with the tabular and you

105
00:09:03,840 --> 00:09:08,240
think you're used to your deep learning, that'd be a nonlinear system, you could also have

106
00:09:08,240 --> 00:09:13,720
linear things that turn out to be quite important and even the aggregate case, state aggregate

107
00:09:13,720 --> 00:09:20,200
means you still have a table but there could be many states fall into the same table entry,

108
00:09:20,200 --> 00:09:29,040
okay, so you're aggregating states and treating them all the same, this is a nice case actually,

109
00:09:29,040 --> 00:09:36,280
we can get theoretical results for it that we can't get for the other cases, okay, so

110
00:09:36,320 --> 00:09:40,920
there's function approximation, we want to do that in some sense that's our bread and

111
00:09:40,920 --> 00:09:47,440
butter, we just generalize the table to a function approximator like supervised learning

112
00:09:47,440 --> 00:09:55,680
system, but let's go on, I want to extend it quite far, so let's list the things and

113
00:09:55,680 --> 00:10:00,640
the next big thing is partial observability because really the world doesn't even give

114
00:10:00,640 --> 00:10:06,360
us states, it gives us observations, it gives us things that happened, things that are senses,

115
00:10:06,360 --> 00:10:12,040
it doesn't tell us, we don't know the full state of the world, we just get an observation and now,

116
00:10:12,040 --> 00:10:17,440
we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest,

117
00:10:17,440 --> 00:10:20,720
the rest of it is basically the kind of thing we've talked about so far, we have the world,

118
00:10:20,720 --> 00:10:25,560
we have our policy and our value function and we're interacting with the world,

119
00:10:25,560 --> 00:10:30,440
we're getting rewards and we're getting some observations and then that red box is turning

120
00:10:30,440 --> 00:10:37,600
into a state and so once we get past the red box, it's just like before, we had a state and we can

121
00:10:37,600 --> 00:10:42,560
make the, send that state up to the model to be learned and we can send that state up to the

122
00:10:42,560 --> 00:10:51,160
planner to propose things and the planner will do some adjustments to the policy and value

123
00:10:51,160 --> 00:10:55,520
function just like the reward does, but it will come from the planner and this will be the common

124
00:10:56,120 --> 00:11:02,200
path between model free learning and model based learning. Okay, so the thing in the red box,

125
00:11:02,200 --> 00:11:08,560
this is the state update function which just says that the agent has to take responsibility for

126
00:11:08,560 --> 00:11:13,120
learning some mapping from the observation, the last state and its action to what it's going to use

127
00:11:13,120 --> 00:11:17,840
as its state, it stays as a summary of the past, it's good for making decisions and predicting

128
00:11:17,840 --> 00:11:26,520
the future and so the state update function is called U, it's exactly this thing and it's

129
00:11:26,520 --> 00:11:32,640
got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U

130
00:11:32,640 --> 00:11:38,800
box is given and I'm going to mostly assume. Anyway, when you talk about changing the state

131
00:11:38,800 --> 00:11:45,560
feature vector or the state representation, that will be the state update function. Okay,

132
00:11:45,600 --> 00:11:53,480
that's a major extension and at the same time it's almost done because I've got some kind of a

133
00:11:53,480 --> 00:11:58,400
box and so I've got some kind of a box, produces some kind of a state representation and my methods

134
00:11:58,400 --> 00:12:03,800
always, at least once I did the second step, a function approximation, they always were able

135
00:12:03,800 --> 00:12:09,680
to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect

136
00:12:09,680 --> 00:12:13,920
it is, I will be able to do a certain well with it just as I would be able to do certain well

137
00:12:13,960 --> 00:12:22,720
with a certain feature vector representing the state. Okay, another big step is that if we do it

138
00:12:22,720 --> 00:12:28,640
right, it doesn't we can separate it from all the other issues, just like we have here, which is to

139
00:12:28,640 --> 00:12:32,280
do temple abstraction. Really if you take your model of the world, your model of the world is not

140
00:12:32,280 --> 00:12:36,840
if I'm in this state, I do this action one step later, I'll be in this other state, it's really

141
00:12:36,840 --> 00:12:43,840
more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a

142
00:12:43,840 --> 00:12:50,560
sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big

143
00:12:50,560 --> 00:12:56,080
multi-step events and we're actually the kind of learning and kind of reasoning and planning we

144
00:12:56,080 --> 00:13:01,040
want to include, should include all those sorts of things. So there is a theory of options which

145
00:13:01,080 --> 00:13:08,680
enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases.

146
00:13:08,680 --> 00:13:14,200
Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in

147
00:13:14,200 --> 00:13:23,240
a little bit. So rushing along, I'm talking about open questions in model based reinforcement,

148
00:13:23,240 --> 00:13:29,480
so I have to say a little bit what's closed, what I'm not going to consider open. So these are my

149
00:13:30,320 --> 00:13:35,840
settings, these are my presumptions, and I say closed-ish because like lots of people will

150
00:13:35,840 --> 00:13:44,080
disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning

151
00:13:44,080 --> 00:13:49,320
should be online, incremental, like asynchronous dynamic programming and like the dynasy system

152
00:13:49,320 --> 00:13:55,160
you've just seen. I think that models and planning, they should be state to state. So

153
00:13:55,200 --> 00:14:01,360
many people in the literature make models and do planning where they include the observations in

154
00:14:01,360 --> 00:14:06,720
the plan. You're like, if I did this then I would see that and then I would, no, no, it should just

155
00:14:06,720 --> 00:14:16,640
be state to state. And if you think about it just a little bit longer, really it's obvious you've

156
00:14:16,640 --> 00:14:21,920
got to be state to state. You don't want to have your observations which are tied to the single

157
00:14:21,920 --> 00:14:27,680
time step and tied to state update. You want all those to be separated. Okay, now of course it's

158
00:14:27,680 --> 00:14:31,680
not really state to state, the state feature to state feature, state feature vector to state

159
00:14:31,680 --> 00:14:37,280
feature vector, and that will be where the feature vectors are coming from the learn state

160
00:14:37,280 --> 00:14:44,640
update function that we mentioned earlier. Okay, closed models, planning, they should be

161
00:14:44,640 --> 00:14:50,800
temporarily abstract, there should not be one step, they should be used based on options. Also,

162
00:14:51,440 --> 00:14:56,320
search control. Search control is how you decide which states to think about in imagination, and

163
00:14:56,320 --> 00:15:01,520
that's essential for your plan to be efficient. If you think about stupid states, you'll just learn

164
00:15:01,520 --> 00:15:07,120
stupid things, but if you can just select the key states to think about to form your plan, then you

165
00:15:07,120 --> 00:15:13,440
can be efficient and effective in your planning. And the last thing is that, so these are sort of

166
00:15:13,440 --> 00:15:17,680
like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it

167
00:15:17,680 --> 00:15:22,880
directly. And similarly, we need some problems to in order to structure the learning of the

168
00:15:22,880 --> 00:15:27,760
options and the option models. Okay, let's go on to the open questions. The open questions. Number

169
00:15:27,760 --> 00:15:34,000
one, should the model, what is this model, should it generate sample states, which I suggested,

170
00:15:34,000 --> 00:15:40,320
or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go

171
00:15:40,320 --> 00:15:44,080
through it in detail. But how should planning be done with average war? This is the other big thing

172
00:15:44,080 --> 00:15:50,080
that I hope to cover today, average war. And then all the other things I won't, I won't probably won't

173
00:15:50,080 --> 00:15:56,160
get to. But let's look at, so let's let's go to how we put function approximation in here. And

174
00:15:56,160 --> 00:16:02,480
what is the content of the model? So just a little bit of terminology. Of course, planning proceeds

175
00:16:02,480 --> 00:16:07,040
by using the model to look ahead, imagining something that might happen. Each one of these

176
00:16:07,040 --> 00:16:12,480
imaginings of the future from a state action pair is called a projection. I'm going to use this word

177
00:16:12,480 --> 00:16:18,320
projection. This is where we imagine a future. Okay, and then after one or more projections,

178
00:16:18,320 --> 00:16:23,920
we compute something. And then we back it up. That's called a backup. And this goes on forever.

179
00:16:23,920 --> 00:16:29,680
Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm

180
00:16:29,680 --> 00:16:33,200
looking at these state action pairs and imagining might happen. So what would be the projections

181
00:16:33,200 --> 00:16:40,160
in this picture? Uh, Schmach, where's the projections in this picture?

182
00:16:46,640 --> 00:16:51,600
At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then

183
00:16:51,600 --> 00:16:57,360
everyone can can just do it. Where are the projections? The projections are where you're

184
00:16:57,360 --> 00:17:02,720
imagining the future from a state action pair. This is my test to see if you're actually following

185
00:17:02,720 --> 00:17:09,680
my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state.

186
00:17:10,560 --> 00:17:14,240
This is a state action pair. Because you can tell because it has an action on it and it comes from

187
00:17:14,240 --> 00:17:18,720
a state. So it's a state action pair. And then you imagine the future, the projections are here.

188
00:17:18,720 --> 00:17:24,240
There are three projections. We're looking ahead, all the actions I might make, and I project what

189
00:17:24,240 --> 00:17:28,800
would happen. And I figure out how good they would be. I take the max and I back it up. Okay,

190
00:17:28,880 --> 00:17:35,520
so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick.

191
00:17:41,440 --> 00:17:45,840
Well, it's, it's from, from the state action pair to where it goes. This, this part is the

192
00:17:45,840 --> 00:17:53,280
projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here?

193
00:17:59,280 --> 00:17:59,920
Say that again.

194
00:18:05,440 --> 00:18:12,080
You should have, you should be sure by now. So there are the projections. So this is,

195
00:18:12,080 --> 00:18:18,720
this is a long skinny sequence. This is a skinny backup. So we're probably sampling,

196
00:18:18,720 --> 00:18:22,320
instead of doing all possible actions, we're sampling an action, we're sampling a next state,

197
00:18:22,320 --> 00:18:25,600
we're sampling an action after that, and we're sampling an x state after that. But these two

198
00:18:25,600 --> 00:18:29,680
are the, are the projection parts. The other parts are parts that the agent is doing. The agent

199
00:18:29,680 --> 00:18:35,120
says, suppose I do this action, and then ask the model, what would happen, the projection? Okay.

200
00:18:35,120 --> 00:18:42,000
And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves,

201
00:18:42,000 --> 00:18:43,440
always goes from the leaves to the top.

202
00:18:48,720 --> 00:18:55,200
No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part,

203
00:18:55,200 --> 00:18:59,280
anyway, is definitely your choice. It's not an imagination about what the world might do.

204
00:19:00,000 --> 00:19:05,040
Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one,

205
00:19:05,760 --> 00:19:12,640
the projections are here. Now these two states, they might be the same state. Maybe I imagined

206
00:19:12,640 --> 00:19:18,480
this one, and I said, huh, now what, what if I was there and I did this one? So that,

207
00:19:20,480 --> 00:19:24,880
so that if they were the same state, you might imagine doing the same thing. But in fact,

208
00:19:24,880 --> 00:19:29,200
by definition, as a backup, they are separate backups, and you'd get these two, two together.

209
00:19:29,200 --> 00:19:33,440
And if you did this one first, and then you did that one, then it might be a similar effect,

210
00:19:33,440 --> 00:19:37,120
because you would change the value, estimated value of this state, and then you change, use

211
00:19:37,120 --> 00:19:40,720
that to change the estimate of that one, that one. Okay. Okay. I have to keep going.

212
00:19:44,640 --> 00:19:45,520
Good. So,

213
00:19:50,480 --> 00:19:53,360
this is the biggest question. What is the output of a projection?

214
00:19:54,240 --> 00:19:59,840
Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide,

215
00:19:59,840 --> 00:20:05,520
what is it really? Because we're using a function approximation, and our states are probably real

216
00:20:05,520 --> 00:20:13,520
valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in

217
00:20:13,520 --> 00:20:18,240
this, I'm imagining being in the state, doing an action, but the world is stochastic, many things

218
00:20:18,240 --> 00:20:22,160
could happen. So one thing I could do is I could represent the whole distribution of all the things

219
00:20:22,160 --> 00:20:27,440
that could happen. Okay. This isn't totally crazy. People are doing this. This system called

220
00:20:27,440 --> 00:20:35,760
Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are,

221
00:20:35,760 --> 00:20:41,760
are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated,

222
00:20:41,760 --> 00:20:46,240
they're, they're going to, we want methods that are general and scalable and proximal. And so that

223
00:20:46,240 --> 00:20:51,520
we, can we do this without committing to a very specific form for the function approximator?

224
00:20:51,520 --> 00:20:56,400
I am skeptical that we can do this. Okay. This is the first question, the first open question.

225
00:20:56,400 --> 00:21:01,120
I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution.

226
00:21:02,320 --> 00:21:06,560
But if, if you did this, then how could you roll it out? How could you iterate it? How could you

227
00:21:06,560 --> 00:21:10,240
go to another step? Because you'd go from a state action period to a distribution. Here's a messy

228
00:21:10,800 --> 00:21:15,200
distribution thing. And then how could you go from there? How could you roll on to the next

229
00:21:15,280 --> 00:21:23,520
projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course,

230
00:21:23,520 --> 00:21:28,240
you can always sample that distribution. And then you have a sample model. So you get the state action

231
00:21:28,240 --> 00:21:31,840
pair and you get a sample of the next state. And then you can roll it out. You say, okay,

232
00:21:31,840 --> 00:21:35,680
there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there?

233
00:21:35,680 --> 00:21:41,840
And you can go on. But you really have many of the same problems because you have to learn

234
00:21:41,840 --> 00:21:45,120
the distribution because you have to, you have to generate a sample from that distribution and

235
00:21:45,120 --> 00:21:51,520
you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility.

236
00:21:51,520 --> 00:21:57,040
But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from

237
00:21:57,040 --> 00:22:03,680
which you sample. And you notice that now planning has become stochastic, because there, you would

238
00:22:03,680 --> 00:22:09,360
have to do many samples like in Monte Carlo tree search of that next state in order to average

239
00:22:09,360 --> 00:22:13,600
over them and get an expected expectation. Whereas up here, it was deterministic. I get you

240
00:22:13,600 --> 00:22:18,640
the whole deterministic distribution. Okay. And then there's the third case, which I like the best,

241
00:22:18,640 --> 00:22:24,240
which is that you learn the output of a projection is an expectation, an expected feature vector.

242
00:22:24,240 --> 00:22:32,400
Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out

243
00:22:32,960 --> 00:22:40,720
because you get this exp, you know, this average of feature vectors for the next state. And it's

244
00:22:40,720 --> 00:22:45,120
straightforward to learn this expectation models, because that's what all of our Algorb zoo, they

245
00:22:45,120 --> 00:22:50,240
learn expectations. And, but in general, we've lost information. If you only have the expected

246
00:22:50,240 --> 00:22:54,640
next feature vector, instead of the whole distribution, you lose special things. But,

247
00:22:56,640 --> 00:23:00,320
but there's this important fact, mathematical fact that in the special case of linear value

248
00:23:01,280 --> 00:23:10,960
functions, you actually don't lose anything. So I, I guess I don't have time to do this equation.

249
00:23:10,960 --> 00:23:15,360
So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically,

250
00:23:15,360 --> 00:23:20,720
we can show that if you do, you're doing the update with the distribution model, and you can

251
00:23:20,720 --> 00:23:25,760
write up mathematically, this is what it is. And then just through a few steps, you can prove that

252
00:23:25,760 --> 00:23:32,720
you get exactly the same thing if you, if you use an expectation model. So here, this is, this is

253
00:23:32,720 --> 00:23:37,600
the probability distribution of the next states. Here we have the expected next feature vector for

254
00:23:37,600 --> 00:23:43,840
the state, and the, the action or option O. And you can show that these are equal in the special

255
00:23:43,840 --> 00:23:52,640
case, where the value functions are linear. Okay. So this is open questions, open questions. So

256
00:23:53,360 --> 00:23:59,200
this is just a proposed strategy, is that with linear value functions and expectation models.

257
00:24:00,000 --> 00:24:05,280
And so, you know, I just want to talk a little bit about this question, should the value function

258
00:24:05,280 --> 00:24:11,040
be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question

259
00:24:11,040 --> 00:24:15,920
of moving the work around, whatever you do, you have to learn the nonlinear relationship.

260
00:24:16,560 --> 00:24:23,040
And the strategy of, of an expectation model is that the nonlinear work is done in the state

261
00:24:23,040 --> 00:24:29,920
update function. So it puts the burden on the state update function. And so here I want to talk

262
00:24:29,920 --> 00:24:40,240
about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this

263
00:24:40,240 --> 00:24:44,800
picture from the book of the shape of all the backups. Now, these are the shapes of the backup

264
00:24:44,800 --> 00:24:50,320
really. This side is planning, that side was seen as not planning. You know, just TD and Monte

265
00:24:50,320 --> 00:24:55,760
Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning

266
00:24:55,760 --> 00:25:02,000
could could could involve a short, not just the wide backups of dynamic programming and and tree

267
00:25:02,000 --> 00:25:10,960
search or exhaustive search. But you can do the skinny backups. And so my my long short start is

268
00:25:10,960 --> 00:25:16,160
that I'm going to those are the those are the projections is that I'm going to argue that

269
00:25:16,160 --> 00:25:23,360
really everything can be done with the smallest, the smallest backup, just looking ahead from

270
00:25:23,360 --> 00:25:28,960
sample one action and sample one expectation outcome. And that's that's that's I think is a cool

271
00:25:28,960 --> 00:25:33,760
way to do planning. And you can do that without losing anything. Because if you want to assemble a

272
00:25:33,760 --> 00:25:39,600
bunch of these tiny backups in the right order, or in just over and over again, you can learn a

273
00:25:39,600 --> 00:25:49,360
long plan. Okay. So I have one more slide, just going to briefly talk about the average award

274
00:25:49,360 --> 00:25:55,520
setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we

275
00:25:55,520 --> 00:25:59,600
use function approximation, we have to go to the average word setting, we have to give up discounting.

276
00:26:00,160 --> 00:26:05,040
And I just want to make the observation in front of you all that that this planning with

277
00:26:05,040 --> 00:26:11,200
average award, it's still a totally open question. I thought it was easy. But I was thinking about

278
00:26:11,200 --> 00:26:17,440
the other day with Zach. And it's really an open open question. It's even open for the tabular

279
00:26:17,440 --> 00:26:24,080
case. You just take one step dine and try to make an average reward version. That would be a paper

280
00:26:24,080 --> 00:26:30,320
because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for

281
00:26:30,320 --> 00:26:38,160
your to do this summer to get your master's done just in time for September, that would be a good

282
00:26:38,160 --> 00:26:43,440
one. If you don't have one already. Okay. And there's also questions whether the model should

283
00:26:43,440 --> 00:26:48,480
should give us the the the expected reward or the expected difference between the reward and the

284
00:26:48,480 --> 00:26:57,120
average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not

285
00:26:57,120 --> 00:27:05,920
gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these

286
00:27:05,920 --> 00:27:10,240
are these are the questions that we started with the open questions. Should the model generate

287
00:27:10,240 --> 00:27:14,800
sample states or expectations? And if it's going to give us expected states, should the value function

288
00:27:14,800 --> 00:27:21,280
be linear? We've seen how those fit together nice. And the question is a further question is can state

289
00:27:21,280 --> 00:27:25,600
update support that can it learn a good enough state features so that the value function can be

290
00:27:25,600 --> 00:27:29,120
linear without losing something important? And then there are other questions about whether the

291
00:27:29,120 --> 00:27:33,760
model whether this suggests the model should be linear as well, or whether it can be semi linear,

292
00:27:33,760 --> 00:27:39,520
which means like a squashing function applied to a linear function. We talked about how planning

293
00:27:40,480 --> 00:27:45,200
should you know, once we combine average reward with planning, unsolved problem, we should work

294
00:27:45,200 --> 00:27:52,080
on that. We should also worry about how should planning affect the actual actions. And what

295
00:27:52,640 --> 00:27:59,520
sub problems should direct the construction of the option models. And I can't I shouldn't try to

296
00:27:59,520 --> 00:28:04,000
explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is

297
00:28:04,000 --> 00:28:07,920
that we want maybe we want to expect the states, maybe one of the value functions be linear,

298
00:28:07,920 --> 00:28:13,920
maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition,

299
00:28:13,920 --> 00:28:19,360
that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so

300
00:28:22,080 --> 00:28:24,640
I'm done. Thank you for your attention.

301
00:28:29,440 --> 00:28:34,400
Okay, we do have a little bit of time for questions. I ended abruptly there, but

302
00:28:35,440 --> 00:28:41,200
that's the story, open questions and model based reinforcement. Please.

303
00:28:43,680 --> 00:28:49,280
Okay, so that's probably should have been one of my my closed questions, because we definitely need

304
00:28:49,360 --> 00:28:55,200
off policy learning in order to learn the models, in order to be do it efficiently. And so the part

305
00:28:55,200 --> 00:29:02,240
of the premise is that we're doing off policy learning. And we have, we have a suite of a few

306
00:29:02,240 --> 00:29:06,000
methods that will work on that nowadays. Yeah, off policy learning is essential.

307
00:29:07,920 --> 00:29:11,440
So I would assume that you would want to learn lots of value functions and not just one.

308
00:29:12,160 --> 00:29:16,240
If you want all of them to be linear in your representation, then that's a lot of burden

309
00:29:16,240 --> 00:29:22,080
on your representation. Yes. So if all the complexity is in the state representation,

310
00:29:22,080 --> 00:29:28,480
then what is the model really giving you? Well, it's giving you the dynamics, which is it's not

311
00:29:28,480 --> 00:29:33,440
in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state

312
00:29:33,440 --> 00:29:39,280
update function. And but more importantly, I'm just realizing that I forgot to thank my new

313
00:29:39,280 --> 00:29:47,280
collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this,

314
00:29:47,280 --> 00:29:53,040
and they should be up here. And, and we have we submitted a paper on expectation models to

315
00:29:54,640 --> 00:30:01,520
to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out.

316
00:30:01,520 --> 00:30:08,080
Yeah. So so a lot of work is going into state update.

317
00:30:12,320 --> 00:30:18,560
That's that's the strategy. I think I think it's might be appropriate. Good.

318
00:30:22,560 --> 00:30:25,680
I have a couple of observations. I don't know, even whether I'm on the

319
00:30:26,160 --> 00:30:33,280
page to you, but I am interested in applied intelligence. One observation is Wall Street,

320
00:30:33,280 --> 00:30:40,400
they seem to have a numerical model of the world. You know, so I mean, that's one world or one model

321
00:30:40,400 --> 00:30:47,280
that you can look at. It seems to me they're far more numerical than other types of domains.

322
00:30:47,280 --> 00:30:53,280
The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's

323
00:30:53,280 --> 00:31:00,240
not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know,

324
00:31:00,240 --> 00:31:06,720
so it's each duck in the clock can radiate any number of different directions. So I'm not sure

325
00:31:06,720 --> 00:31:12,080
whether your model could cover the eagle catching the ducks or you want to give that to us.

326
00:31:14,720 --> 00:31:17,040
Thank you. That's good. So this linearity thing,

327
00:31:17,680 --> 00:31:25,280
it's a very important that it's linear in the in the features. Okay, and and this is the this is

328
00:31:25,280 --> 00:31:31,520
the trick. It's sort of already known, it's well known that anything can be linear if you arrange

329
00:31:31,520 --> 00:31:38,800
the right features. So you could do the duck, you could you could presumably capture the higher

330
00:31:38,800 --> 00:31:43,680
order, the interactions between the, so what do you lose when you get linearity? You know,

331
00:31:43,760 --> 00:31:48,320
let's say you have two features a and b. Well, if the right choice or the right value

332
00:31:48,320 --> 00:31:54,640
depends upon both of them being present at the same time. And then then then then you can't do

333
00:31:54,640 --> 00:31:59,760
that linearly. The linear function, you can only say, oh, this has a certain effect, this has a

334
00:31:59,760 --> 00:32:05,440
certain effect. And there can't be a special thing that if they're on together. Okay. And so what

335
00:32:05,440 --> 00:32:10,480
you do with that none, and when there's an interaction between variables, you know, maybe it's

336
00:32:10,480 --> 00:32:16,800
it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or

337
00:32:17,440 --> 00:32:21,680
that's the kind of thing that you can't do. But but we've known since the beginning of neural

338
00:32:21,680 --> 00:32:27,840
networks, that that if you then add a third variable for the conjunction of the two original

339
00:32:27,840 --> 00:32:33,280
features, then you can learn the nonlinear function in the original in the first two. Okay, and so

340
00:32:33,280 --> 00:32:40,000
the same same is true. So it's not a principle limitation in any way. So think again about

341
00:32:40,080 --> 00:32:46,960
a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function,

342
00:32:46,960 --> 00:32:52,960
many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way

343
00:32:52,960 --> 00:32:58,240
of finding the features in the last layer, then you could be linear. So in some sense, what we're

344
00:32:58,240 --> 00:33:02,160
just saying, take the that thing that you worked on in the last layer and make that your state.

345
00:33:02,160 --> 00:33:05,120
And so as your state, then you would have to your models would have to produce it.

346
00:33:06,080 --> 00:33:15,200
You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating?

347
00:33:15,200 --> 00:33:19,600
I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear

348
00:33:19,600 --> 00:33:24,720
map, then that nonlinear map would have to figure out that that a that these two variables are,

349
00:33:24,720 --> 00:33:28,880
are, need to be treated especially, and they would have to create the conjunction term inside

350
00:33:28,880 --> 00:33:34,400
that that nonlinear mapping. So it's like we're doing the same work, we're pushing it

351
00:33:34,480 --> 00:33:40,160
into a different place. We're pushing it into the state update. Yeah, other questions?

352
00:33:50,320 --> 00:33:56,560
Okay, thank you very much.

