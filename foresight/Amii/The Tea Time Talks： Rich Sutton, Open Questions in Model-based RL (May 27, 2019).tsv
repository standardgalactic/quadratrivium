start	end	text
0	12400	Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just
12400	15240	have a welcome you to tea.
15240	22520	We do this each year and it's very informal and I welcome you all to sign up and participate
22520	28440	and to ask questions and you know we're here just to talk about science and ideas and exchange
28440	29760	points of view.
29760	37520	So think of it as a community, enjoy the tea, have a cookie, it's all good.
37520	46440	The other thing is Thursday, Thursday I think we still have an open slot and we could have
46440	56600	Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that
57560	60400	will work out, I'm pretty sure.
60400	68040	Okay, so the topics are really intelligence and how do we understand how the mind might
68040	74400	work and could work and today my topic, really the topic is model-based reinforcement learning.
74400	78360	Model-based reinforcement learning is really it's sort of the whole enchilada.
78360	84560	We try to get all the pieces present, model-based, model-free, make understanding of the world,
84560	91440	be able to reason or plan with it and how many people here have just raised your hand
91440	96560	if you've heard of the phrase model-based reinforcement learning.
96560	103200	How many haven't, how many actually, let me say it a little differently.
103200	107920	How many of you just think it's the model-based reinforcement like the most important thing
107920	117600	in AI, like I do, or should I get a little bit of an argument from authority for it?
117600	123880	Okay, I spent that long talking about it, I might as well give my, I just threw in a
123880	129520	slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon
129520	135440	talking about predictive learning, understanding the world, Yashio Benjo talks about the most
135440	139520	important step in AI is to make predictive causal model of the world.
139520	145680	This is a thing that's coming around at last and this is our topic, our topic is model-based
145680	149080	reinforcement learning and it means making a model of the world, I can use this word
149080	153520	model to mean model of the world, how the world works, I'll only use the word model
153520	159040	as a world model, model of the MDP, the Markov decision process, the transition dynamics of
159040	164800	the world and model-based reinforcement learning, we learn the model, we learn the model and
164800	170280	we also then use the model to plan or reason about what to do.
170280	176640	So this is, we call this planning and planning proceeds by, you imagine states, you look
176640	180320	ahead from the individual states to see what might happen and you back up to improve your
180320	185120	policy or your value with those states that you're imagining might happen and then this
185120	189640	is how you figure out, decide what to do.
189640	196480	Okay so it's decided this idea has been around for a while, I'm going to talk briefly about
196480	202480	the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first
202480	207200	proposed or first explicitly proposed that planning and learning would be radically similar.
207200	211880	So there's some reinforcement learning algorithm that's interacting with the world and then
211880	216360	it's also learning the world model which is another box that you can plug in place of
216360	221460	the world and instead interact with the model for a while, in that sense, and so you're
221460	225520	interacting with the model and you're saying what if I do this, the model says that would
225520	229200	happen, you get this reward and you learn from that just as if it had really happened
229200	235960	and so then this way the planning or the logic and the reasoning, certainly the planning
235960	242280	is radically similar to just acting in the world, just acting in imagination.
242280	246920	So that's the first half of the idea, the second half of the idea is that these things
246920	252120	are all done simultaneously, so planning and learning and executing, they're all been going
252120	257040	on all the time, so you're always interacting with the world, getting new experience, you
257040	260800	can do a model for your reinforcement learning with that bit, but you also use that experience
260800	265440	to make the model and the model through planning and that also affects your value function
265440	267400	and your policy and makes you do better.
267520	273840	Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm
273840	278920	that's kind of useful to complement those diagrams, those visual things with a little
278920	283520	bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from
283520	289520	the book, so basically your Q is your action value function and model SA is your prediction
289520	293440	about what would happen if you were in state S and did action A and you initialize that
293560	297280	and then as you go through life, you're just doing this loop over and over again, you're
297280	302480	looking at the state you're in, you're picking an action in it and set B somehow, then you
302480	308400	take that action, send it to the world, the world gives you back a result, R, the reward
308400	314200	and the next state S prime and so then you do model free learning with that little four
314200	320000	tuple of experience, state action, reward, next state, so here we're doing Q learning
320080	325520	in step D to update the action values and then we update the model because we've seen
325520	329760	what will happen for that state and that action, that this will happen and this is where we're
329760	334640	assuming a deterministic world because really many things might happen if you did that action
334640	338720	in this state and we only saw one thing and now we're going to just overwrite the model
338720	342360	so that I'll predict always that that would happen.
342360	346200	So the thing about DynA, particularly the first instance of it in 1990, it's a very
346200	353840	simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully
353840	354840	express all the possibilities.
354840	358840	We're going to talk about today's, how you can go and consider more possibilities.
358840	364000	Anyway, after you do, now we've got as far as learning the model and then step F is where
364000	365800	you do the plan.
365800	369920	So by the plan, you just imagine some state, maybe randomly from something you've seen
369920	375360	before, imagine an action, you ask the model what would happen if you were there and then
375480	379520	you do that same step, same, it's the same as D, it's Q-learning, but here now we're
379520	383560	doing Q-learning on these imagined things and there we're doing Q-learning on the real thing.
383560	388000	So that's the basic idea of DynA, I'm sure many of you have seen that before.
388000	392320	Now, maybe you've seen the demos of it before, so here's the standard demo where we have
392320	397840	a grid world, we have a start state down in that corner and we have a goal state where
397840	403480	you get a plus one reward and the actions are to move up, down, right and left, so four
403520	408040	actions from each state, the states are all represented tabularly, so like this is state
408040	415040	34 and this is state 92 and the model says, oh, when I go action four in state 34, I end
416200	421720	up in state 92, that's what the model is learning and because it has the model, it is very quickly
421720	428720	learning how to get from start to goal efficiently and it's actually able to learn about states
428880	433040	that it's not in and this becomes most apparent if we pick up the goal and move it to a different
433040	438240	place, so of course once we've moved it, the agent will go back to, it'll go here again
438240	443520	where it used to be and be disappointed so to speak and then it has no idea where it
443520	447040	is, it's built a model but the model says that there's no goal up there, as the last
447040	450880	time it was there, there was no goal, okay, but eventually it will stumble upon the goal
450880	456360	again and then it will be very quickly able to plot a path, so watch when it first finds
456360	462320	it now, here it will eventually see it's learning the right way to go and it's learning a good
462320	468840	path very quickly because it has a model, okay, so that's like tabular dyna, okay, and
468840	473760	today we want to talk about the extensions of this, open questions in it, yeah I guess
473760	478680	I didn't even say it, it's open questions and planning, I'm not going to tell you the answers,
478680	484960	I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic
484960	489840	dynamics, what you saw was just deterministic dynamics, we assume that the world always
489840	494840	went the same way but you could instead of overwriting what the model says in the state
494840	498640	in action pair, you could start to collect a list of all the things that might happen
498640	503960	in their probabilities and then you could sample that and you could do exactly the same thing,
503960	508840	you could add function approximation, now function approximation, I'm going to talk about it,
508840	514840	but it's really a spectrum, a range of degrees of function approximation, so what we saw
514840	519720	was tabular, I call it tabular and that means every state action pair is treated totally
519720	523320	different from every other state action pair, there's no similarity between them, there's
523320	527880	no generalization and so there's just a big table and I store things in that state action
527880	533920	pair and really in real life certainly in computer go and in Atari games and in any
533920	539080	robotics application you have to generalize from one state to another and that's, you
539080	543840	know, you never see the same state twice, okay, but we start with the tabular and you
543840	548240	think you're used to your deep learning, that'd be a nonlinear system, you could also have
548240	553720	linear things that turn out to be quite important and even the aggregate case, state aggregate
553720	560200	means you still have a table but there could be many states fall into the same table entry,
560200	569040	okay, so you're aggregating states and treating them all the same, this is a nice case actually,
569040	576280	we can get theoretical results for it that we can't get for the other cases, okay, so
576320	580920	there's function approximation, we want to do that in some sense that's our bread and
580920	587440	butter, we just generalize the table to a function approximator like supervised learning
587440	595680	system, but let's go on, I want to extend it quite far, so let's list the things and
595680	600640	the next big thing is partial observability because really the world doesn't even give
600640	606360	us states, it gives us observations, it gives us things that happened, things that are senses,
606360	612040	it doesn't tell us, we don't know the full state of the world, we just get an observation and now,
612040	617440	we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest,
617440	620720	the rest of it is basically the kind of thing we've talked about so far, we have the world,
620720	625560	we have our policy and our value function and we're interacting with the world,
625560	630440	we're getting rewards and we're getting some observations and then that red box is turning
630440	637600	into a state and so once we get past the red box, it's just like before, we had a state and we can
637600	642560	make the, send that state up to the model to be learned and we can send that state up to the
642560	651160	planner to propose things and the planner will do some adjustments to the policy and value
651160	655520	function just like the reward does, but it will come from the planner and this will be the common
656120	662200	path between model free learning and model based learning. Okay, so the thing in the red box,
662200	668560	this is the state update function which just says that the agent has to take responsibility for
668560	673120	learning some mapping from the observation, the last state and its action to what it's going to use
673120	677840	as its state, it stays as a summary of the past, it's good for making decisions and predicting
677840	686520	the future and so the state update function is called U, it's exactly this thing and it's
686520	692640	got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U
692640	698800	box is given and I'm going to mostly assume. Anyway, when you talk about changing the state
698800	705560	feature vector or the state representation, that will be the state update function. Okay,
705600	713480	that's a major extension and at the same time it's almost done because I've got some kind of a
713480	718400	box and so I've got some kind of a box, produces some kind of a state representation and my methods
718400	723800	always, at least once I did the second step, a function approximation, they always were able
723800	729680	to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect
729680	733920	it is, I will be able to do a certain well with it just as I would be able to do certain well
733960	742720	with a certain feature vector representing the state. Okay, another big step is that if we do it
742720	748640	right, it doesn't we can separate it from all the other issues, just like we have here, which is to
748640	752280	do temple abstraction. Really if you take your model of the world, your model of the world is not
752280	756840	if I'm in this state, I do this action one step later, I'll be in this other state, it's really
756840	763840	more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a
763840	770560	sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big
770560	776080	multi-step events and we're actually the kind of learning and kind of reasoning and planning we
776080	781040	want to include, should include all those sorts of things. So there is a theory of options which
781080	788680	enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases.
788680	794200	Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in
794200	803240	a little bit. So rushing along, I'm talking about open questions in model based reinforcement,
803240	809480	so I have to say a little bit what's closed, what I'm not going to consider open. So these are my
810320	815840	settings, these are my presumptions, and I say closed-ish because like lots of people will
815840	824080	disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning
824080	829320	should be online, incremental, like asynchronous dynamic programming and like the dynasy system
829320	835160	you've just seen. I think that models and planning, they should be state to state. So
835200	841360	many people in the literature make models and do planning where they include the observations in
841360	846720	the plan. You're like, if I did this then I would see that and then I would, no, no, it should just
846720	856640	be state to state. And if you think about it just a little bit longer, really it's obvious you've
856640	861920	got to be state to state. You don't want to have your observations which are tied to the single
861920	867680	time step and tied to state update. You want all those to be separated. Okay, now of course it's
867680	871680	not really state to state, the state feature to state feature, state feature vector to state
871680	877280	feature vector, and that will be where the feature vectors are coming from the learn state
877280	884640	update function that we mentioned earlier. Okay, closed models, planning, they should be
884640	890800	temporarily abstract, there should not be one step, they should be used based on options. Also,
891440	896320	search control. Search control is how you decide which states to think about in imagination, and
896320	901520	that's essential for your plan to be efficient. If you think about stupid states, you'll just learn
901520	907120	stupid things, but if you can just select the key states to think about to form your plan, then you
907120	913440	can be efficient and effective in your planning. And the last thing is that, so these are sort of
913440	917680	like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it
917680	922880	directly. And similarly, we need some problems to in order to structure the learning of the
922880	927760	options and the option models. Okay, let's go on to the open questions. The open questions. Number
927760	934000	one, should the model, what is this model, should it generate sample states, which I suggested,
934000	940320	or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go
940320	944080	through it in detail. But how should planning be done with average war? This is the other big thing
944080	950080	that I hope to cover today, average war. And then all the other things I won't, I won't probably won't
950080	956160	get to. But let's look at, so let's let's go to how we put function approximation in here. And
956160	962480	what is the content of the model? So just a little bit of terminology. Of course, planning proceeds
962480	967040	by using the model to look ahead, imagining something that might happen. Each one of these
967040	972480	imaginings of the future from a state action pair is called a projection. I'm going to use this word
972480	978320	projection. This is where we imagine a future. Okay, and then after one or more projections,
978320	983920	we compute something. And then we back it up. That's called a backup. And this goes on forever.
983920	989680	Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm
989680	993200	looking at these state action pairs and imagining might happen. So what would be the projections
993200	1000160	in this picture? Uh, Schmach, where's the projections in this picture?
1006640	1011600	At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then
1011600	1017360	everyone can can just do it. Where are the projections? The projections are where you're
1017360	1022720	imagining the future from a state action pair. This is my test to see if you're actually following
1022720	1029680	my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state.
1030560	1034240	This is a state action pair. Because you can tell because it has an action on it and it comes from
1034240	1038720	a state. So it's a state action pair. And then you imagine the future, the projections are here.
1038720	1044240	There are three projections. We're looking ahead, all the actions I might make, and I project what
1044240	1048800	would happen. And I figure out how good they would be. I take the max and I back it up. Okay,
1048880	1055520	so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick.
1061440	1065840	Well, it's, it's from, from the state action pair to where it goes. This, this part is the
1065840	1073280	projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here?
1079280	1079920	Say that again.
1085440	1092080	You should have, you should be sure by now. So there are the projections. So this is,
1092080	1098720	this is a long skinny sequence. This is a skinny backup. So we're probably sampling,
1098720	1102320	instead of doing all possible actions, we're sampling an action, we're sampling a next state,
1102320	1105600	we're sampling an action after that, and we're sampling an x state after that. But these two
1105600	1109680	are the, are the projection parts. The other parts are parts that the agent is doing. The agent
1109680	1115120	says, suppose I do this action, and then ask the model, what would happen, the projection? Okay.
1115120	1122000	And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves,
1122000	1123440	always goes from the leaves to the top.
1128720	1135200	No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part,
1135200	1139280	anyway, is definitely your choice. It's not an imagination about what the world might do.
1140000	1145040	Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one,
1145760	1152640	the projections are here. Now these two states, they might be the same state. Maybe I imagined
1152640	1158480	this one, and I said, huh, now what, what if I was there and I did this one? So that,
1160480	1164880	so that if they were the same state, you might imagine doing the same thing. But in fact,
1164880	1169200	by definition, as a backup, they are separate backups, and you'd get these two, two together.
1169200	1173440	And if you did this one first, and then you did that one, then it might be a similar effect,
1173440	1177120	because you would change the value, estimated value of this state, and then you change, use
1177120	1180720	that to change the estimate of that one, that one. Okay. Okay. I have to keep going.
1184640	1185520	Good. So,
1190480	1193360	this is the biggest question. What is the output of a projection?
1194240	1199840	Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide,
1199840	1205520	what is it really? Because we're using a function approximation, and our states are probably real
1205520	1213520	valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in
1213520	1218240	this, I'm imagining being in the state, doing an action, but the world is stochastic, many things
1218240	1222160	could happen. So one thing I could do is I could represent the whole distribution of all the things
1222160	1227440	that could happen. Okay. This isn't totally crazy. People are doing this. This system called
1227440	1235760	Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are,
1235760	1241760	are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated,
1241760	1246240	they're, they're going to, we want methods that are general and scalable and proximal. And so that
1246240	1251520	we, can we do this without committing to a very specific form for the function approximator?
1251520	1256400	I am skeptical that we can do this. Okay. This is the first question, the first open question.
1256400	1261120	I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution.
1262320	1266560	But if, if you did this, then how could you roll it out? How could you iterate it? How could you
1266560	1270240	go to another step? Because you'd go from a state action period to a distribution. Here's a messy
1270800	1275200	distribution thing. And then how could you go from there? How could you roll on to the next
1275280	1283520	projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course,
1283520	1288240	you can always sample that distribution. And then you have a sample model. So you get the state action
1288240	1291840	pair and you get a sample of the next state. And then you can roll it out. You say, okay,
1291840	1295680	there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there?
1295680	1301840	And you can go on. But you really have many of the same problems because you have to learn
1301840	1305120	the distribution because you have to, you have to generate a sample from that distribution and
1305120	1311520	you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility.
1311520	1317040	But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from
1317040	1323680	which you sample. And you notice that now planning has become stochastic, because there, you would
1323680	1329360	have to do many samples like in Monte Carlo tree search of that next state in order to average
1329360	1333600	over them and get an expected expectation. Whereas up here, it was deterministic. I get you
1333600	1338640	the whole deterministic distribution. Okay. And then there's the third case, which I like the best,
1338640	1344240	which is that you learn the output of a projection is an expectation, an expected feature vector.
1344240	1352400	Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out
1352960	1360720	because you get this exp, you know, this average of feature vectors for the next state. And it's
1360720	1365120	straightforward to learn this expectation models, because that's what all of our Algorb zoo, they
1365120	1370240	learn expectations. And, but in general, we've lost information. If you only have the expected
1370240	1374640	next feature vector, instead of the whole distribution, you lose special things. But,
1376640	1380320	but there's this important fact, mathematical fact that in the special case of linear value
1381280	1390960	functions, you actually don't lose anything. So I, I guess I don't have time to do this equation.
1390960	1395360	So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically,
1395360	1400720	we can show that if you do, you're doing the update with the distribution model, and you can
1400720	1405760	write up mathematically, this is what it is. And then just through a few steps, you can prove that
1405760	1412720	you get exactly the same thing if you, if you use an expectation model. So here, this is, this is
1412720	1417600	the probability distribution of the next states. Here we have the expected next feature vector for
1417600	1423840	the state, and the, the action or option O. And you can show that these are equal in the special
1423840	1432640	case, where the value functions are linear. Okay. So this is open questions, open questions. So
1433360	1439200	this is just a proposed strategy, is that with linear value functions and expectation models.
1440000	1445280	And so, you know, I just want to talk a little bit about this question, should the value function
1445280	1451040	be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question
1451040	1455920	of moving the work around, whatever you do, you have to learn the nonlinear relationship.
1456560	1463040	And the strategy of, of an expectation model is that the nonlinear work is done in the state
1463040	1469920	update function. So it puts the burden on the state update function. And so here I want to talk
1469920	1480240	about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this
1480240	1484800	picture from the book of the shape of all the backups. Now, these are the shapes of the backup
1484800	1490320	really. This side is planning, that side was seen as not planning. You know, just TD and Monte
1490320	1495760	Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning
1495760	1502000	could could could involve a short, not just the wide backups of dynamic programming and and tree
1502000	1510960	search or exhaustive search. But you can do the skinny backups. And so my my long short start is
1510960	1516160	that I'm going to those are the those are the projections is that I'm going to argue that
1516160	1523360	really everything can be done with the smallest, the smallest backup, just looking ahead from
1523360	1528960	sample one action and sample one expectation outcome. And that's that's that's I think is a cool
1528960	1533760	way to do planning. And you can do that without losing anything. Because if you want to assemble a
1533760	1539600	bunch of these tiny backups in the right order, or in just over and over again, you can learn a
1539600	1549360	long plan. Okay. So I have one more slide, just going to briefly talk about the average award
1549360	1555520	setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we
1555520	1559600	use function approximation, we have to go to the average word setting, we have to give up discounting.
1560160	1565040	And I just want to make the observation in front of you all that that this planning with
1565040	1571200	average award, it's still a totally open question. I thought it was easy. But I was thinking about
1571200	1577440	the other day with Zach. And it's really an open open question. It's even open for the tabular
1577440	1584080	case. You just take one step dine and try to make an average reward version. That would be a paper
1584080	1590320	because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for
1590320	1598160	your to do this summer to get your master's done just in time for September, that would be a good
1598160	1603440	one. If you don't have one already. Okay. And there's also questions whether the model should
1603440	1608480	should give us the the the expected reward or the expected difference between the reward and the
1608480	1617120	average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not
1617120	1625920	gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these
1625920	1630240	are these are the questions that we started with the open questions. Should the model generate
1630240	1634800	sample states or expectations? And if it's going to give us expected states, should the value function
1634800	1641280	be linear? We've seen how those fit together nice. And the question is a further question is can state
1641280	1645600	update support that can it learn a good enough state features so that the value function can be
1645600	1649120	linear without losing something important? And then there are other questions about whether the
1649120	1653760	model whether this suggests the model should be linear as well, or whether it can be semi linear,
1653760	1659520	which means like a squashing function applied to a linear function. We talked about how planning
1660480	1665200	should you know, once we combine average reward with planning, unsolved problem, we should work
1665200	1672080	on that. We should also worry about how should planning affect the actual actions. And what
1672640	1679520	sub problems should direct the construction of the option models. And I can't I shouldn't try to
1679520	1684000	explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is
1684000	1687920	that we want maybe we want to expect the states, maybe one of the value functions be linear,
1687920	1693920	maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition,
1693920	1699360	that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so
1702080	1704640	I'm done. Thank you for your attention.
1709440	1714400	Okay, we do have a little bit of time for questions. I ended abruptly there, but
1715440	1721200	that's the story, open questions and model based reinforcement. Please.
1723680	1729280	Okay, so that's probably should have been one of my my closed questions, because we definitely need
1729360	1735200	off policy learning in order to learn the models, in order to be do it efficiently. And so the part
1735200	1742240	of the premise is that we're doing off policy learning. And we have, we have a suite of a few
1742240	1746000	methods that will work on that nowadays. Yeah, off policy learning is essential.
1747920	1751440	So I would assume that you would want to learn lots of value functions and not just one.
1752160	1756240	If you want all of them to be linear in your representation, then that's a lot of burden
1756240	1762080	on your representation. Yes. So if all the complexity is in the state representation,
1762080	1768480	then what is the model really giving you? Well, it's giving you the dynamics, which is it's not
1768480	1773440	in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state
1773440	1779280	update function. And but more importantly, I'm just realizing that I forgot to thank my new
1779280	1787280	collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this,
1787280	1793040	and they should be up here. And, and we have we submitted a paper on expectation models to
1794640	1801520	to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out.
1801520	1808080	Yeah. So so a lot of work is going into state update.
1812320	1818560	That's that's the strategy. I think I think it's might be appropriate. Good.
1822560	1825680	I have a couple of observations. I don't know, even whether I'm on the
1826160	1833280	page to you, but I am interested in applied intelligence. One observation is Wall Street,
1833280	1840400	they seem to have a numerical model of the world. You know, so I mean, that's one world or one model
1840400	1847280	that you can look at. It seems to me they're far more numerical than other types of domains.
1847280	1853280	The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's
1853280	1860240	not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know,
1860240	1866720	so it's each duck in the clock can radiate any number of different directions. So I'm not sure
1866720	1872080	whether your model could cover the eagle catching the ducks or you want to give that to us.
1874720	1877040	Thank you. That's good. So this linearity thing,
1877680	1885280	it's a very important that it's linear in the in the features. Okay, and and this is the this is
1885280	1891520	the trick. It's sort of already known, it's well known that anything can be linear if you arrange
1891520	1898800	the right features. So you could do the duck, you could you could presumably capture the higher
1898800	1903680	order, the interactions between the, so what do you lose when you get linearity? You know,
1903760	1908320	let's say you have two features a and b. Well, if the right choice or the right value
1908320	1914640	depends upon both of them being present at the same time. And then then then then you can't do
1914640	1919760	that linearly. The linear function, you can only say, oh, this has a certain effect, this has a
1919760	1925440	certain effect. And there can't be a special thing that if they're on together. Okay. And so what
1925440	1930480	you do with that none, and when there's an interaction between variables, you know, maybe it's
1930480	1936800	it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or
1937440	1941680	that's the kind of thing that you can't do. But but we've known since the beginning of neural
1941680	1947840	networks, that that if you then add a third variable for the conjunction of the two original
1947840	1953280	features, then you can learn the nonlinear function in the original in the first two. Okay, and so
1953280	1960000	the same same is true. So it's not a principle limitation in any way. So think again about
1960080	1966960	a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function,
1966960	1972960	many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way
1972960	1978240	of finding the features in the last layer, then you could be linear. So in some sense, what we're
1978240	1982160	just saying, take the that thing that you worked on in the last layer and make that your state.
1982160	1985120	And so as your state, then you would have to your models would have to produce it.
1986080	1995200	You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating?
1995200	1999600	I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear
1999600	2004720	map, then that nonlinear map would have to figure out that that a that these two variables are,
2004720	2008880	are, need to be treated especially, and they would have to create the conjunction term inside
2008880	2014400	that that nonlinear mapping. So it's like we're doing the same work, we're pushing it
2014480	2020160	into a different place. We're pushing it into the state update. Yeah, other questions?
2030320	2036560	Okay, thank you very much.
