WEBVTT

00:00.000 --> 00:12.400
Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just

00:12.400 --> 00:15.240
have a welcome you to tea.

00:15.240 --> 00:22.520
We do this each year and it's very informal and I welcome you all to sign up and participate

00:22.520 --> 00:28.440
and to ask questions and you know we're here just to talk about science and ideas and exchange

00:28.440 --> 00:29.760
points of view.

00:29.760 --> 00:37.520
So think of it as a community, enjoy the tea, have a cookie, it's all good.

00:37.520 --> 00:46.440
The other thing is Thursday, Thursday I think we still have an open slot and we could have

00:46.440 --> 00:56.600
Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that

00:57.560 --> 01:00.400
will work out, I'm pretty sure.

01:00.400 --> 01:08.040
Okay, so the topics are really intelligence and how do we understand how the mind might

01:08.040 --> 01:14.400
work and could work and today my topic, really the topic is model-based reinforcement learning.

01:14.400 --> 01:18.360
Model-based reinforcement learning is really it's sort of the whole enchilada.

01:18.360 --> 01:24.560
We try to get all the pieces present, model-based, model-free, make understanding of the world,

01:24.560 --> 01:31.440
be able to reason or plan with it and how many people here have just raised your hand

01:31.440 --> 01:36.560
if you've heard of the phrase model-based reinforcement learning.

01:36.560 --> 01:43.200
How many haven't, how many actually, let me say it a little differently.

01:43.200 --> 01:47.920
How many of you just think it's the model-based reinforcement like the most important thing

01:47.920 --> 01:57.600
in AI, like I do, or should I get a little bit of an argument from authority for it?

01:57.600 --> 02:03.880
Okay, I spent that long talking about it, I might as well give my, I just threw in a

02:03.880 --> 02:09.520
slide where I say lots of people are talking about this, it's sort of the thing, Jan Lacoon

02:09.520 --> 02:15.440
talking about predictive learning, understanding the world, Yashio Benjo talks about the most

02:15.440 --> 02:19.520
important step in AI is to make predictive causal model of the world.

02:19.520 --> 02:25.680
This is a thing that's coming around at last and this is our topic, our topic is model-based

02:25.680 --> 02:29.080
reinforcement learning and it means making a model of the world, I can use this word

02:29.080 --> 02:33.520
model to mean model of the world, how the world works, I'll only use the word model

02:33.520 --> 02:39.040
as a world model, model of the MDP, the Markov decision process, the transition dynamics of

02:39.040 --> 02:44.800
the world and model-based reinforcement learning, we learn the model, we learn the model and

02:44.800 --> 02:50.280
we also then use the model to plan or reason about what to do.

02:50.280 --> 02:56.640
So this is, we call this planning and planning proceeds by, you imagine states, you look

02:56.640 --> 03:00.320
ahead from the individual states to see what might happen and you back up to improve your

03:00.320 --> 03:05.120
policy or your value with those states that you're imagining might happen and then this

03:05.120 --> 03:09.640
is how you figure out, decide what to do.

03:09.640 --> 03:16.480
Okay so it's decided this idea has been around for a while, I'm going to talk briefly about

03:16.480 --> 03:22.480
the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first

03:22.480 --> 03:27.200
proposed or first explicitly proposed that planning and learning would be radically similar.

03:27.200 --> 03:31.880
So there's some reinforcement learning algorithm that's interacting with the world and then

03:31.880 --> 03:36.360
it's also learning the world model which is another box that you can plug in place of

03:36.360 --> 03:41.460
the world and instead interact with the model for a while, in that sense, and so you're

03:41.460 --> 03:45.520
interacting with the model and you're saying what if I do this, the model says that would

03:45.520 --> 03:49.200
happen, you get this reward and you learn from that just as if it had really happened

03:49.200 --> 03:55.960
and so then this way the planning or the logic and the reasoning, certainly the planning

03:55.960 --> 04:02.280
is radically similar to just acting in the world, just acting in imagination.

04:02.280 --> 04:06.920
So that's the first half of the idea, the second half of the idea is that these things

04:06.920 --> 04:12.120
are all done simultaneously, so planning and learning and executing, they're all been going

04:12.120 --> 04:17.040
on all the time, so you're always interacting with the world, getting new experience, you

04:17.040 --> 04:20.800
can do a model for your reinforcement learning with that bit, but you also use that experience

04:20.800 --> 04:25.440
to make the model and the model through planning and that also affects your value function

04:25.440 --> 04:27.400
and your policy and makes you do better.

04:27.520 --> 04:33.840
Those are the two ideas of DynA, it's kind of very simple and we make it into an algorithm

04:33.840 --> 04:38.920
that's kind of useful to complement those diagrams, those visual things with a little

04:38.920 --> 04:43.520
bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from

04:43.520 --> 04:49.520
the book, so basically your Q is your action value function and model SA is your prediction

04:49.520 --> 04:53.440
about what would happen if you were in state S and did action A and you initialize that

04:53.560 --> 04:57.280
and then as you go through life, you're just doing this loop over and over again, you're

04:57.280 --> 05:02.480
looking at the state you're in, you're picking an action in it and set B somehow, then you

05:02.480 --> 05:08.400
take that action, send it to the world, the world gives you back a result, R, the reward

05:08.400 --> 05:14.200
and the next state S prime and so then you do model free learning with that little four

05:14.200 --> 05:20.000
tuple of experience, state action, reward, next state, so here we're doing Q learning

05:20.080 --> 05:25.520
in step D to update the action values and then we update the model because we've seen

05:25.520 --> 05:29.760
what will happen for that state and that action, that this will happen and this is where we're

05:29.760 --> 05:34.640
assuming a deterministic world because really many things might happen if you did that action

05:34.640 --> 05:38.720
in this state and we only saw one thing and now we're going to just overwrite the model

05:38.720 --> 05:42.360
so that I'll predict always that that would happen.

05:42.360 --> 05:46.200
So the thing about DynA, particularly the first instance of it in 1990, it's a very

05:46.200 --> 05:53.840
simple and almost a toy case, it's very clear but it's only expressed, it doesn't fully

05:53.840 --> 05:54.840
express all the possibilities.

05:54.840 --> 05:58.840
We're going to talk about today's, how you can go and consider more possibilities.

05:58.840 --> 06:04.000
Anyway, after you do, now we've got as far as learning the model and then step F is where

06:04.000 --> 06:05.800
you do the plan.

06:05.800 --> 06:09.920
So by the plan, you just imagine some state, maybe randomly from something you've seen

06:09.920 --> 06:15.360
before, imagine an action, you ask the model what would happen if you were there and then

06:15.480 --> 06:19.520
you do that same step, same, it's the same as D, it's Q-learning, but here now we're

06:19.520 --> 06:23.560
doing Q-learning on these imagined things and there we're doing Q-learning on the real thing.

06:23.560 --> 06:28.000
So that's the basic idea of DynA, I'm sure many of you have seen that before.

06:28.000 --> 06:32.320
Now, maybe you've seen the demos of it before, so here's the standard demo where we have

06:32.320 --> 06:37.840
a grid world, we have a start state down in that corner and we have a goal state where

06:37.840 --> 06:43.480
you get a plus one reward and the actions are to move up, down, right and left, so four

06:43.520 --> 06:48.040
actions from each state, the states are all represented tabularly, so like this is state

06:48.040 --> 06:55.040
34 and this is state 92 and the model says, oh, when I go action four in state 34, I end

06:56.200 --> 07:01.720
up in state 92, that's what the model is learning and because it has the model, it is very quickly

07:01.720 --> 07:08.720
learning how to get from start to goal efficiently and it's actually able to learn about states

07:08.880 --> 07:13.040
that it's not in and this becomes most apparent if we pick up the goal and move it to a different

07:13.040 --> 07:18.240
place, so of course once we've moved it, the agent will go back to, it'll go here again

07:18.240 --> 07:23.520
where it used to be and be disappointed so to speak and then it has no idea where it

07:23.520 --> 07:27.040
is, it's built a model but the model says that there's no goal up there, as the last

07:27.040 --> 07:30.880
time it was there, there was no goal, okay, but eventually it will stumble upon the goal

07:30.880 --> 07:36.360
again and then it will be very quickly able to plot a path, so watch when it first finds

07:36.360 --> 07:42.320
it now, here it will eventually see it's learning the right way to go and it's learning a good

07:42.320 --> 07:48.840
path very quickly because it has a model, okay, so that's like tabular dyna, okay, and

07:48.840 --> 07:53.760
today we want to talk about the extensions of this, open questions in it, yeah I guess

07:53.760 --> 07:58.680
I didn't even say it, it's open questions and planning, I'm not going to tell you the answers,

07:58.680 --> 08:04.960
I'm going to try to set the questions, so Dyna architecture extends naturally to stochastic

08:04.960 --> 08:09.840
dynamics, what you saw was just deterministic dynamics, we assume that the world always

08:09.840 --> 08:14.840
went the same way but you could instead of overwriting what the model says in the state

08:14.840 --> 08:18.640
in action pair, you could start to collect a list of all the things that might happen

08:18.640 --> 08:23.960
in their probabilities and then you could sample that and you could do exactly the same thing,

08:23.960 --> 08:28.840
you could add function approximation, now function approximation, I'm going to talk about it,

08:28.840 --> 08:34.840
but it's really a spectrum, a range of degrees of function approximation, so what we saw

08:34.840 --> 08:39.720
was tabular, I call it tabular and that means every state action pair is treated totally

08:39.720 --> 08:43.320
different from every other state action pair, there's no similarity between them, there's

08:43.320 --> 08:47.880
no generalization and so there's just a big table and I store things in that state action

08:47.880 --> 08:53.920
pair and really in real life certainly in computer go and in Atari games and in any

08:53.920 --> 08:59.080
robotics application you have to generalize from one state to another and that's, you

08:59.080 --> 09:03.840
know, you never see the same state twice, okay, but we start with the tabular and you

09:03.840 --> 09:08.240
think you're used to your deep learning, that'd be a nonlinear system, you could also have

09:08.240 --> 09:13.720
linear things that turn out to be quite important and even the aggregate case, state aggregate

09:13.720 --> 09:20.200
means you still have a table but there could be many states fall into the same table entry,

09:20.200 --> 09:29.040
okay, so you're aggregating states and treating them all the same, this is a nice case actually,

09:29.040 --> 09:36.280
we can get theoretical results for it that we can't get for the other cases, okay, so

09:36.320 --> 09:40.920
there's function approximation, we want to do that in some sense that's our bread and

09:40.920 --> 09:47.440
butter, we just generalize the table to a function approximator like supervised learning

09:47.440 --> 09:55.680
system, but let's go on, I want to extend it quite far, so let's list the things and

09:55.680 --> 10:00.640
the next big thing is partial observability because really the world doesn't even give

10:00.640 --> 10:06.360
us states, it gives us observations, it gives us things that happened, things that are senses,

10:06.360 --> 10:12.040
it doesn't tell us, we don't know the full state of the world, we just get an observation and now,

10:12.040 --> 10:17.440
we have a little trick, okay, now ignore the trick as the red box, but if you look at the rest,

10:17.440 --> 10:20.720
the rest of it is basically the kind of thing we've talked about so far, we have the world,

10:20.720 --> 10:25.560
we have our policy and our value function and we're interacting with the world,

10:25.560 --> 10:30.440
we're getting rewards and we're getting some observations and then that red box is turning

10:30.440 --> 10:37.600
into a state and so once we get past the red box, it's just like before, we had a state and we can

10:37.600 --> 10:42.560
make the, send that state up to the model to be learned and we can send that state up to the

10:42.560 --> 10:51.160
planner to propose things and the planner will do some adjustments to the policy and value

10:51.160 --> 10:55.520
function just like the reward does, but it will come from the planner and this will be the common

10:56.120 --> 11:02.200
path between model free learning and model based learning. Okay, so the thing in the red box,

11:02.200 --> 11:08.560
this is the state update function which just says that the agent has to take responsibility for

11:08.560 --> 11:13.120
learning some mapping from the observation, the last state and its action to what it's going to use

11:13.120 --> 11:17.840
as its state, it stays as a summary of the past, it's good for making decisions and predicting

11:17.840 --> 11:26.520
the future and so the state update function is called U, it's exactly this thing and it's

11:26.520 --> 11:32.640
got to be learned. Okay, but in this talk I'm going to assume that the state is given and the U

11:32.640 --> 11:38.800
box is given and I'm going to mostly assume. Anyway, when you talk about changing the state

11:38.800 --> 11:45.560
feature vector or the state representation, that will be the state update function. Okay,

11:45.600 --> 11:53.480
that's a major extension and at the same time it's almost done because I've got some kind of a

11:53.480 --> 11:58.400
box and so I've got some kind of a box, produces some kind of a state representation and my methods

11:58.400 --> 12:03.800
always, at least once I did the second step, a function approximation, they always were able

12:03.800 --> 12:09.680
to accept a representation that wasn't necessarily perfect and so whatever U gives me, however imperfect

12:09.680 --> 12:13.920
it is, I will be able to do a certain well with it just as I would be able to do certain well

12:13.960 --> 12:22.720
with a certain feature vector representing the state. Okay, another big step is that if we do it

12:22.720 --> 12:28.640
right, it doesn't we can separate it from all the other issues, just like we have here, which is to

12:28.640 --> 12:32.280
do temple abstraction. Really if you take your model of the world, your model of the world is not

12:32.280 --> 12:36.840
if I'm in this state, I do this action one step later, I'll be in this other state, it's really

12:36.840 --> 12:43.840
more like, oh, if I go to the talk, I'll learn something, or if I run home, I could eat a

12:43.840 --> 12:50.560
sandwich, or I can take a plane and travel to Surrando. Okay, so those are obviously all big

12:50.560 --> 12:56.080
multi-step events and we're actually the kind of learning and kind of reasoning and planning we

12:56.080 --> 13:01.040
want to include, should include all those sorts of things. So there is a theory of options which

13:01.080 --> 13:08.680
enables us to treat those surprisingly so, but we can treat all those as exactly in the same cases.

13:08.680 --> 13:14.200
Okay, and last what? The average reward setting, the average reward setting, I'll talk about that in

13:14.200 --> 13:23.240
a little bit. So rushing along, I'm talking about open questions in model based reinforcement,

13:23.240 --> 13:29.480
so I have to say a little bit what's closed, what I'm not going to consider open. So these are my

13:30.320 --> 13:35.840
settings, these are my presumptions, and I say closed-ish because like lots of people will

13:35.840 --> 13:44.080
disagree with me, or they would disagree with me if I gave them a chance, okay? I think planning

13:44.080 --> 13:49.320
should be online, incremental, like asynchronous dynamic programming and like the dynasy system

13:49.320 --> 13:55.160
you've just seen. I think that models and planning, they should be state to state. So

13:55.200 --> 14:01.360
many people in the literature make models and do planning where they include the observations in

14:01.360 --> 14:06.720
the plan. You're like, if I did this then I would see that and then I would, no, no, it should just

14:06.720 --> 14:16.640
be state to state. And if you think about it just a little bit longer, really it's obvious you've

14:16.640 --> 14:21.920
got to be state to state. You don't want to have your observations which are tied to the single

14:21.920 --> 14:27.680
time step and tied to state update. You want all those to be separated. Okay, now of course it's

14:27.680 --> 14:31.680
not really state to state, the state feature to state feature, state feature vector to state

14:31.680 --> 14:37.280
feature vector, and that will be where the feature vectors are coming from the learn state

14:37.280 --> 14:44.640
update function that we mentioned earlier. Okay, closed models, planning, they should be

14:44.640 --> 14:50.800
temporarily abstract, there should not be one step, they should be used based on options. Also,

14:51.440 --> 14:56.320
search control. Search control is how you decide which states to think about in imagination, and

14:56.320 --> 15:01.520
that's essential for your plan to be efficient. If you think about stupid states, you'll just learn

15:01.520 --> 15:07.120
stupid things, but if you can just select the key states to think about to form your plan, then you

15:07.120 --> 15:13.440
can be efficient and effective in your planning. And the last thing is that, so these are sort of

15:13.440 --> 15:17.680
like saying, I need this, I acknowledge I need this, even though I'm not going to deal with it

15:17.680 --> 15:22.880
directly. And similarly, we need some problems to in order to structure the learning of the

15:22.880 --> 15:27.760
options and the option models. Okay, let's go on to the open questions. The open questions. Number

15:27.760 --> 15:34.000
one, should the model, what is this model, should it generate sample states, which I suggested,

15:34.000 --> 15:40.320
or should generate expected states? Okay, there's a bunch of things under that. And I'm going to go

15:40.320 --> 15:44.080
through it in detail. But how should planning be done with average war? This is the other big thing

15:44.080 --> 15:50.080
that I hope to cover today, average war. And then all the other things I won't, I won't probably won't

15:50.080 --> 15:56.160
get to. But let's look at, so let's let's go to how we put function approximation in here. And

15:56.160 --> 16:02.480
what is the content of the model? So just a little bit of terminology. Of course, planning proceeds

16:02.480 --> 16:07.040
by using the model to look ahead, imagining something that might happen. Each one of these

16:07.040 --> 16:12.480
imaginings of the future from a state action pair is called a projection. I'm going to use this word

16:12.480 --> 16:18.320
projection. This is where we imagine a future. Okay, and then after one or more projections,

16:18.320 --> 16:23.920
we compute something. And then we back it up. That's called a backup. And this goes on forever.

16:23.920 --> 16:29.680
Okay, so now from this diagram is a typical backup. I'm thinking about this state and I'm

16:29.680 --> 16:33.200
looking at these state action pairs and imagining might happen. So what would be the projections

16:33.200 --> 16:40.160
in this picture? Uh, Schmach, where's the projections in this picture?

16:46.640 --> 16:51.600
At the top. Good. That's totally wrong. And since he's, since he got it totally wrong, then

16:51.600 --> 16:57.360
everyone can can just do it. Where are the projections? The projections are where you're

16:57.360 --> 17:02.720
imagining the future from a state action pair. This is my test to see if you're actually following

17:02.720 --> 17:09.680
my definitions. Starting from state action pair, you imagine the future. Okay, these, this is a state.

17:10.560 --> 17:14.240
This is a state action pair. Because you can tell because it has an action on it and it comes from

17:14.240 --> 17:18.720
a state. So it's a state action pair. And then you imagine the future, the projections are here.

17:18.720 --> 17:24.240
There are three projections. We're looking ahead, all the actions I might make, and I project what

17:24.240 --> 17:28.800
would happen. And I figure out how good they would be. I take the max and I back it up. Okay,

17:28.880 --> 17:35.520
so the backup then goes from the leaves to the top of the, of the process. Okay. Okay, Dylan, quick.

17:41.440 --> 17:45.840
Well, it's, it's from, from the state action pair to where it goes. This, this part is the

17:45.840 --> 17:53.280
projection. Right. Okay, good. And what about this picture, Dylan? Where are the projections here?

17:59.280 --> 17:59.920
Say that again.

18:05.440 --> 18:12.080
You should have, you should be sure by now. So there are the projections. So this is,

18:12.080 --> 18:18.720
this is a long skinny sequence. This is a skinny backup. So we're probably sampling,

18:18.720 --> 18:22.320
instead of doing all possible actions, we're sampling an action, we're sampling a next state,

18:22.320 --> 18:25.600
we're sampling an action after that, and we're sampling an x state after that. But these two

18:25.600 --> 18:29.680
are the, are the projection parts. The other parts are parts that the agent is doing. The agent

18:29.680 --> 18:35.120
says, suppose I do this action, and then ask the model, what would happen, the projection? Okay.

18:35.120 --> 18:42.000
And so what about the backup here? Okay. So the backup here goes from, from the, from the leaves,

18:42.000 --> 18:43.440
always goes from the leaves to the top.

18:48.720 --> 18:55.200
No, no, no, no. Not the way I'm going to use the word. Okay. And this, this is, this part,

18:55.200 --> 18:59.280
anyway, is definitely your choice. It's not an imagination about what the world might do.

19:00.000 --> 19:05.040
Okay. I'm going to use the world. I'm just going to, okay. So, and then the last one,

19:05.760 --> 19:12.640
the projections are here. Now these two states, they might be the same state. Maybe I imagined

19:12.640 --> 19:18.480
this one, and I said, huh, now what, what if I was there and I did this one? So that,

19:20.480 --> 19:24.880
so that if they were the same state, you might imagine doing the same thing. But in fact,

19:24.880 --> 19:29.200
by definition, as a backup, they are separate backups, and you'd get these two, two together.

19:29.200 --> 19:33.440
And if you did this one first, and then you did that one, then it might be a similar effect,

19:33.440 --> 19:37.120
because you would change the value, estimated value of this state, and then you change, use

19:37.120 --> 19:40.720
that to change the estimate of that one, that one. Okay. Okay. I have to keep going.

19:44.640 --> 19:45.520
Good. So,

19:50.480 --> 19:53.360
this is the biggest question. What is the output of a projection?

19:54.240 --> 19:59.840
Okay. Intuitively, it's, it's clear enough. But once we get serious, we have to decide,

19:59.840 --> 20:05.520
what is it really? Because we're using a function approximation, and our states are probably real

20:05.520 --> 20:13.520
valued feature vectors. And so, what do I need? What, what is the output of a model? Like, I'm in

20:13.520 --> 20:18.240
this, I'm imagining being in the state, doing an action, but the world is stochastic, many things

20:18.240 --> 20:22.160
could happen. So one thing I could do is I could represent the whole distribution of all the things

20:22.160 --> 20:27.440
that could happen. Okay. This isn't totally crazy. People are doing this. This system called

20:27.440 --> 20:35.760
Pilko by Mark Dissenroth, and he's doing that. But it's problematic because distributions are,

20:35.760 --> 20:41.760
are large of real value feature vectors. It's a, it's, it's, they're large, they're complicated,

20:41.760 --> 20:46.240
they're, they're going to, we want methods that are general and scalable and proximal. And so that

20:46.240 --> 20:51.520
we, can we do this without committing to a very specific form for the function approximator?

20:51.520 --> 20:56.400
I am skeptical that we can do this. Okay. This is the first question, the first open question.

20:56.400 --> 21:01.120
I'm going to say I'm skeptical, but I'm really saying it's open. Maybe you can do it as a distribution.

21:02.320 --> 21:06.560
But if, if you did this, then how could you roll it out? How could you iterate it? How could you

21:06.560 --> 21:10.240
go to another step? Because you'd go from a state action period to a distribution. Here's a messy

21:10.800 --> 21:15.200
distribution thing. And then how could you go from there? How could you roll on to the next

21:15.280 --> 21:23.520
projection? You would be, it's, it's, it's, it's a little bit problematic. Now, of course,

21:23.520 --> 21:28.240
you can always sample that distribution. And then you have a sample model. So you get the state action

21:28.240 --> 21:31.840
pair and you get a sample of the next state. And then you can roll it out. You say, okay,

21:31.840 --> 21:35.680
there's a next state. I could say, okay, now suppose I was there, what I could, what could I do there?

21:35.680 --> 21:41.840
And you can go on. But you really have many of the same problems because you have to learn

21:41.840 --> 21:45.120
the distribution because you have to, you have to generate a sample from that distribution and

21:45.120 --> 21:51.520
you have to represent it. And so, so anyway, this is, this is, this is, this is a real possibility.

21:51.520 --> 21:57.040
But it's, but it's, it's, it's potentially difficult to make that, to learn the distribution from

21:57.040 --> 22:03.680
which you sample. And you notice that now planning has become stochastic, because there, you would

22:03.680 --> 22:09.360
have to do many samples like in Monte Carlo tree search of that next state in order to average

22:09.360 --> 22:13.600
over them and get an expected expectation. Whereas up here, it was deterministic. I get you

22:13.600 --> 22:18.640
the whole deterministic distribution. Okay. And then there's the third case, which I like the best,

22:18.640 --> 22:24.240
which is that you learn the output of a projection is an expectation, an expected feature vector.

22:24.240 --> 22:32.400
Call us an expectation model. And so this is also deterministic, but maybe it can't be rolled out

22:32.960 --> 22:40.720
because you get this exp, you know, this average of feature vectors for the next state. And it's

22:40.720 --> 22:45.120
straightforward to learn this expectation models, because that's what all of our Algorb zoo, they

22:45.120 --> 22:50.240
learn expectations. And, but in general, we've lost information. If you only have the expected

22:50.240 --> 22:54.640
next feature vector, instead of the whole distribution, you lose special things. But,

22:56.640 --> 23:00.320
but there's this important fact, mathematical fact that in the special case of linear value

23:01.280 --> 23:10.960
functions, you actually don't lose anything. So I, I guess I don't have time to do this equation.

23:10.960 --> 23:15.360
So I'll just say that the point, it's just a math equation, doesn't matter anyway. But basically,

23:15.360 --> 23:20.720
we can show that if you do, you're doing the update with the distribution model, and you can

23:20.720 --> 23:25.760
write up mathematically, this is what it is. And then just through a few steps, you can prove that

23:25.760 --> 23:32.720
you get exactly the same thing if you, if you use an expectation model. So here, this is, this is

23:32.720 --> 23:37.600
the probability distribution of the next states. Here we have the expected next feature vector for

23:37.600 --> 23:43.840
the state, and the, the action or option O. And you can show that these are equal in the special

23:43.840 --> 23:52.640
case, where the value functions are linear. Okay. So this is open questions, open questions. So

23:53.360 --> 23:59.200
this is just a proposed strategy, is that with linear value functions and expectation models.

24:00.000 --> 24:05.280
And so, you know, I just want to talk a little bit about this question, should the value function

24:05.280 --> 24:11.040
be linear? It allows us to do this, and doesn't really lose anything. But really, it's a question

24:11.040 --> 24:15.920
of moving the work around, whatever you do, you have to learn the nonlinear relationship.

24:16.560 --> 24:23.040
And the strategy of, of an expectation model is that the nonlinear work is done in the state

24:23.040 --> 24:29.920
update function. So it puts the burden on the state update function. And so here I want to talk

24:29.920 --> 24:40.240
about Zach's term. Is Zach here? Oh, good. I can claim it was mine. And we have this, this, this

24:40.240 --> 24:44.800
picture from the book of the shape of all the backups. Now, these are the shapes of the backup

24:44.800 --> 24:50.320
really. This side is planning, that side was seen as not planning. You know, just TD and Monte

24:50.320 --> 24:55.760
Carlo learning. But now I want you to think that really, we can do both sides as planning. Planning

24:55.760 --> 25:02.000
could could could involve a short, not just the wide backups of dynamic programming and and tree

25:02.000 --> 25:10.960
search or exhaustive search. But you can do the skinny backups. And so my my long short start is

25:10.960 --> 25:16.160
that I'm going to those are the those are the projections is that I'm going to argue that

25:16.160 --> 25:23.360
really everything can be done with the smallest, the smallest backup, just looking ahead from

25:23.360 --> 25:28.960
sample one action and sample one expectation outcome. And that's that's that's I think is a cool

25:28.960 --> 25:33.760
way to do planning. And you can do that without losing anything. Because if you want to assemble a

25:33.760 --> 25:39.600
bunch of these tiny backups in the right order, or in just over and over again, you can learn a

25:39.600 --> 25:49.360
long plan. Okay. So I have one more slide, just going to briefly talk about the average award

25:49.360 --> 25:55.520
setting. I'm just some of you know what it means. Some of you don't. But if you do, really, when we

25:55.520 --> 25:59.600
use function approximation, we have to go to the average word setting, we have to give up discounting.

26:00.160 --> 26:05.040
And I just want to make the observation in front of you all that that this planning with

26:05.040 --> 26:11.200
average award, it's still a totally open question. I thought it was easy. But I was thinking about

26:11.200 --> 26:17.440
the other day with Zach. And it's really an open open question. It's even open for the tabular

26:17.440 --> 26:24.080
case. You just take one step dine and try to make an average reward version. That would be a paper

26:24.080 --> 26:30.320
because it's it's not at all clear how to do it. So there if you're looking for a thesis topic for

26:30.320 --> 26:38.160
your to do this summer to get your master's done just in time for September, that would be a good

26:38.160 --> 26:43.440
one. If you don't have one already. Okay. And there's also questions whether the model should

26:43.440 --> 26:48.480
should give us the the the expected reward or the expected difference between the reward and the

26:48.480 --> 26:57.120
average reward. Okay, five minutes, I got less than that, don't I? Okay, no, I'm not gonna I'm not

26:57.120 --> 27:05.920
gonna be that bad. But thank you for being so generous. Okay, so I think I'm done. And these

27:05.920 --> 27:10.240
are these are the questions that we started with the open questions. Should the model generate

27:10.240 --> 27:14.800
sample states or expectations? And if it's going to give us expected states, should the value function

27:14.800 --> 27:21.280
be linear? We've seen how those fit together nice. And the question is a further question is can state

27:21.280 --> 27:25.600
update support that can it learn a good enough state features so that the value function can be

27:25.600 --> 27:29.120
linear without losing something important? And then there are other questions about whether the

27:29.120 --> 27:33.760
model whether this suggests the model should be linear as well, or whether it can be semi linear,

27:33.760 --> 27:39.520
which means like a squashing function applied to a linear function. We talked about how planning

27:40.480 --> 27:45.200
should you know, once we combine average reward with planning, unsolved problem, we should work

27:45.200 --> 27:52.080
on that. We should also worry about how should planning affect the actual actions. And what

27:52.640 --> 27:59.520
sub problems should direct the construction of the option models. And I can't I shouldn't try to

27:59.520 --> 28:04.000
explain the last one. You can ask me about it if you'd like. Oh, and I sort of said, my answers is

28:04.000 --> 28:07.920
that we want maybe we want to expect the states, maybe one of the value functions be linear,

28:07.920 --> 28:13.920
maybe we can support this, I don't know, I don't know. And this is the question of feature acquisition,

28:13.920 --> 28:19.360
that should be the sub problems, maybe, and maybe we can describe them by the features. Okay, so

28:22.080 --> 28:24.640
I'm done. Thank you for your attention.

28:29.440 --> 28:34.400
Okay, we do have a little bit of time for questions. I ended abruptly there, but

28:35.440 --> 28:41.200
that's the story, open questions and model based reinforcement. Please.

28:43.680 --> 28:49.280
Okay, so that's probably should have been one of my my closed questions, because we definitely need

28:49.360 --> 28:55.200
off policy learning in order to learn the models, in order to be do it efficiently. And so the part

28:55.200 --> 29:02.240
of the premise is that we're doing off policy learning. And we have, we have a suite of a few

29:02.240 --> 29:06.000
methods that will work on that nowadays. Yeah, off policy learning is essential.

29:07.920 --> 29:11.440
So I would assume that you would want to learn lots of value functions and not just one.

29:12.160 --> 29:16.240
If you want all of them to be linear in your representation, then that's a lot of burden

29:16.240 --> 29:22.080
on your representation. Yes. So if all the complexity is in the state representation,

29:22.080 --> 29:28.480
then what is the model really giving you? Well, it's giving you the dynamics, which is it's not

29:28.480 --> 29:33.440
in the state, the state doesn't give you the dynamics. It is a lot of work on the on the state

29:33.440 --> 29:39.280
update function. And but more importantly, I'm just realizing that I forgot to thank my new

29:39.280 --> 29:47.280
collaborators, which are Mohammed or Zahir, and Yi Wan, who we are been working on this,

29:47.280 --> 29:53.040
and they should be up here. And, and we have we submitted a paper on expectation models to

29:54.640 --> 30:01.520
to Ijkai, Ijkai and we accepted to Ijkai. So that part's been written up. And we're working that out.

30:01.520 --> 30:08.080
Yeah. So so a lot of work is going into state update.

30:12.320 --> 30:18.560
That's that's the strategy. I think I think it's might be appropriate. Good.

30:22.560 --> 30:25.680
I have a couple of observations. I don't know, even whether I'm on the

30:26.160 --> 30:33.280
page to you, but I am interested in applied intelligence. One observation is Wall Street,

30:33.280 --> 30:40.400
they seem to have a numerical model of the world. You know, so I mean, that's one world or one model

30:40.400 --> 30:47.280
that you can look at. It seems to me they're far more numerical than other types of domains.

30:47.280 --> 30:53.280
The other one is a situation of a duck hunting, sorry, the eagle hunting ducks. And there it's

30:53.280 --> 31:00.240
not linear, like the duck's possibility of the duck movement is linear, but it radiates, you know,

31:00.240 --> 31:06.720
so it's each duck in the clock can radiate any number of different directions. So I'm not sure

31:06.720 --> 31:12.080
whether your model could cover the eagle catching the ducks or you want to give that to us.

31:14.720 --> 31:17.040
Thank you. That's good. So this linearity thing,

31:17.680 --> 31:25.280
it's a very important that it's linear in the in the features. Okay, and and this is the this is

31:25.280 --> 31:31.520
the trick. It's sort of already known, it's well known that anything can be linear if you arrange

31:31.520 --> 31:38.800
the right features. So you could do the duck, you could you could presumably capture the higher

31:38.800 --> 31:43.680
order, the interactions between the, so what do you lose when you get linearity? You know,

31:43.760 --> 31:48.320
let's say you have two features a and b. Well, if the right choice or the right value

31:48.320 --> 31:54.640
depends upon both of them being present at the same time. And then then then then you can't do

31:54.640 --> 31:59.760
that linearly. The linear function, you can only say, oh, this has a certain effect, this has a

31:59.760 --> 32:05.440
certain effect. And there can't be a special thing that if they're on together. Okay. And so what

32:05.440 --> 32:10.480
you do with that none, and when there's an interaction between variables, you know, maybe it's

32:10.480 --> 32:16.800
it's a they're both bad. But if they're on together, then it's good. Okay, so it's exclusive or

32:17.440 --> 32:21.680
that's the kind of thing that you can't do. But but we've known since the beginning of neural

32:21.680 --> 32:27.840
networks, that that if you then add a third variable for the conjunction of the two original

32:27.840 --> 32:33.280
features, then you can learn the nonlinear function in the original in the first two. Okay, and so

32:33.280 --> 32:40.000
the same same is true. So it's not a principle limitation in any way. So think again about

32:40.080 --> 32:46.960
a nonlinear network, like, like, you know, in AlphaGo, it learns this complicated function,

32:46.960 --> 32:52.960
many, many layers, it's nonlinear. But it's linear in the last layer. Okay, so if you had some way

32:52.960 --> 32:58.240
of finding the features in the last layer, then you could be linear. So in some sense, what we're

32:58.240 --> 33:02.160
just saying, take the that thing that you worked on in the last layer and make that your state.

33:02.160 --> 33:05.120
And so as your state, then you would have to your models would have to produce it.

33:06.080 --> 33:15.200
You say it's, it's, it's, it's, it's a strategy. It's, it's, it's, I think, so why am I advocating?

33:15.200 --> 33:19.600
I'm advocating because even if you don't, if you do, if you were going to try to learn a nonlinear

33:19.600 --> 33:24.720
map, then that nonlinear map would have to figure out that that a that these two variables are,

33:24.720 --> 33:28.880
are, need to be treated especially, and they would have to create the conjunction term inside

33:28.880 --> 33:34.400
that that nonlinear mapping. So it's like we're doing the same work, we're pushing it

33:34.480 --> 33:40.160
into a different place. We're pushing it into the state update. Yeah, other questions?

33:50.320 --> 33:56.560
Okay, thank you very much.

