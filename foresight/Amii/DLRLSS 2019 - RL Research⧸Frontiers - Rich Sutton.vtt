WEBVTT

00:00.000 --> 00:11.440
Thank you, everybody. It's been so nice, you know, having this time to get to know all

00:11.440 --> 00:16.600
you. And, you know, I feel I've gotten a little bit of time with all you going to various

00:16.600 --> 00:25.760
events and you're all just lovely people. And I've told you how lucky you are. But now

00:25.760 --> 00:30.840
we're coming down to the end, okay? Your minds are like folds to the brim. You can't take

00:30.840 --> 00:39.920
any more ideas, any more equations, any more even important things. But more importantly,

00:39.920 --> 00:44.720
you've learned a lot. And yet, you know, it must be striking you that there's so much

00:44.720 --> 00:51.440
that you don't know. So much more to learn. And, you know, there's so much reinforcement

00:51.680 --> 00:57.840
to learn that whole thick book somebody's got. And that's just the introduction, you

00:57.840 --> 01:07.000
know? And then there's all of deep learning. And this is not enough. Like, you have to,

01:07.000 --> 01:13.160
you're trying to, we're trying to understand how the mind might work, okay? So you have

01:13.160 --> 01:18.920
to know something about psychology and neuroscience and philosophy and anthropology, maybe, and

01:18.960 --> 01:25.920
linguistics, control theory, all the statistics. You have to learn so much. It's like it's

01:25.920 --> 01:31.840
impossible, okay? And it must be kind of scary to you. I suspect, is it scary a little bit

01:31.840 --> 01:41.720
to you how much there is to learn? Yeah. So the good news is it's impossible. And because

01:41.720 --> 01:46.320
it's impossible, you don't have to, you know, ask forgiveness or permission because you're

01:47.040 --> 01:51.280
not going to learn all of it, okay? You're only going to learn part of it because you

01:51.280 --> 01:55.920
have to leave some time so you can do some of your own work that contributes, not just

01:55.920 --> 02:03.240
reading and absorbing. You want to contribute, okay? So you have to pick a balance between

02:03.240 --> 02:13.080
what you learn and what you work on yourself and try to eventually give back to your community.

02:13.120 --> 02:25.000
So this balance is something you're always going to have to work on. And so today what

02:25.000 --> 02:29.640
I want to do, I want to say, I probably want to do too much today. I want to talk about

02:29.640 --> 02:37.920
how you can develop your own thoughts and how you can participate and contribute. And

02:37.920 --> 02:42.440
I want to tell you a little bit about how you might do like a first contribution to

02:42.480 --> 02:46.160
reinforcement learning. The simple trick I call completing the square. I'll tell you

02:46.160 --> 02:52.360
a little bit about what I'm doing. And this is too much. It already is too much. And I'm

02:52.360 --> 02:55.960
going to tell you, I want to talk a little bit about AI and society, which is also a

02:55.960 --> 03:04.520
different kind of research opportunity. So there's all this stuff to learn. And first

03:04.520 --> 03:09.040
is a question of how do you learn some more, okay? And I think you know these things. You

03:09.080 --> 03:14.800
know about the online course from the Whites that just came out last Thursday. How many

03:14.800 --> 03:30.200
of you have registered for that? Okay. And you know about my book with Andy Bartow. How

03:30.280 --> 03:40.680
many of you have that? Okay. That's good. Now, as you learn about all this stuff, there's

03:40.680 --> 03:47.720
a problem because there are no authorities in science. Now, you might be thinking I'm

03:47.720 --> 03:55.160
an authority, but there are no authorities. And number one rule is you can't be impressed

03:55.160 --> 03:59.400
by the part of all that stuff and all those different fields that you don't understand.

03:59.440 --> 04:04.280
You have to be respectful, okay? You say, I don't know that. Maybe it's good. Maybe it's

04:04.280 --> 04:12.400
not. You don't know. And the fact is when they look at science like from 100 years ago,

04:12.400 --> 04:17.760
most of it is not very good. And most of it is, yeah, doing some screwy thing that seemed

04:17.760 --> 04:22.680
important then and is not really important. So I think we have to assume that that's true

04:22.720 --> 04:31.480
today. Okay. So you cannot say you have to learn everything and you cannot say this is

04:31.480 --> 04:36.200
really important because other people understand it and I don't. You have to work from your

04:36.200 --> 04:40.480
own mind. What do you understand and work from there because you want to make a contribution

04:40.480 --> 04:44.280
so you've got to work from things that you understand. So don't be impressed by what

04:44.280 --> 04:47.800
you don't understand. And there's a flip side to this. When you come back to give a

04:47.840 --> 04:55.040
talk, you don't try to impress others by what they don't understand. It's really tempting to do

04:55.040 --> 04:58.880
whenever you give me a talk, you know, present a bunch of crazy stuff that's really hard to

04:58.880 --> 05:04.800
understand and then people hopefully will be impressed. You don't do that. That's just a

05:04.800 --> 05:08.720
waste of time. There's all these different fields and all kinds of different people. Other people

05:08.720 --> 05:12.920
know more than you do about the other fields and so no one knows everything and you want to

05:12.920 --> 05:19.640
benefit from others. But you can't put up barriers. You've got to encourage communication

05:19.640 --> 05:27.200
between all the different disciplines. Okay. So no authorities. What do you do? Well, you have

05:27.200 --> 05:33.880
to be brave and ambitious. You can't just be, I'm going to do some tiny little thing and maybe

05:33.880 --> 05:38.480
no one will bother me. No, you've got to be brave and ambitious, but you also have to be humble

05:38.480 --> 05:43.880
and transparent. Humble because it's a great task we're looking at. The task of understanding the

05:43.880 --> 05:52.920
mind and the problem is huge. It's subtle, but it's not devious. It's not hiding from you. It's

05:52.920 --> 05:59.160
waiting to be discovered. It's up to you to see it. And so I think the bottom line that I'm trying

05:59.160 --> 06:05.760
to express with this slide is that your thoughts are potentially of great value. I mean, you're

06:05.760 --> 06:13.560
going to work in one of these fields and so you want to contribute and you have to feel that

06:13.560 --> 06:18.440
your thoughts are potentially of great value. And I'll tell you as an authority that they are,

06:18.440 --> 06:31.280
but too bad there are no authorities. So how can you train yourself to think well? So I think

06:31.320 --> 06:41.440
I mentioned the key thing is to write. So I made this slide. The best way, the best way is to

06:41.440 --> 06:47.840
write and discuss with other people. And so maybe it takes 10,000 hours to become an expert at

06:47.840 --> 06:53.400
anything. People say this. I have no idea if it's true, but it does take a lot of work to become

06:53.400 --> 06:59.000
an expert in something and to have something to share back with people. So this could well be true

06:59.040 --> 07:07.440
for thinking about thinking. And this pile of notebooks is my history of 45 years of writing

07:07.440 --> 07:15.320
about my thoughts. And you can see the one in the middle is from year zero 1974. And it's all just

07:15.320 --> 07:20.920
a bunch of loose leaf papers that one day I kind of folded together in that notebook. And then as

07:20.920 --> 07:25.520
time went on, I started to take my thoughts more seriously and saying I should keep them in a good

07:25.560 --> 07:33.080
place and refer back to them sometimes. And just sort of, you know, respect my own attempt to think.

07:33.080 --> 07:41.600
And it's, so I'm saying it's not, this is really important. It's essential to have something to

07:41.600 --> 07:49.400
say is to have thought about the problem. And it's not super difficult. You don't have to be a genius,

07:50.000 --> 07:55.120
but you do have to show up. You have to show up day after day, and you have to write. You have to

07:55.120 --> 07:59.960
write. I mean, I'll ask of you. All I would ask of you is all I ask of myself, which is to shoot for

07:59.960 --> 08:06.160
a page a day. You write a page a day. It's going to take you a long time to get 10,000 hours. But

08:06.160 --> 08:13.640
that will be enough to put you ahead of other people. And because of that, you will have

08:13.640 --> 08:19.440
something interesting to say. So I urge you to get a notebook. As you see, it ends there with my

08:19.440 --> 08:28.400
computer, because I write in the computer now. Here's the prose poem that I write for myself. And

08:28.400 --> 08:35.320
I write for people sometimes in their notebooks. Yeah, if you get a notebook and you write 100

08:35.320 --> 08:42.960
pages in it, I would be happy to write this, this poem in your notebook. To write is to begin to

08:42.960 --> 08:49.200
think. And to write in a special place, a book such as this is to honor your thoughts and to help

08:49.200 --> 08:56.480
them build one upon the other. Well, that's it. I think that's really important. Okay, in the other

08:56.480 --> 09:00.840
half that we've told you today, and I've told you in other times, the determination is super

09:00.840 --> 09:05.080
important. So you're going to get stuck when you try to write down your thoughts. Absolutely,

09:05.080 --> 09:10.080
you're going to get stuck. You're going to you're going to reach what appears to be a dead end with

09:10.120 --> 09:15.600
nowhere to go. But there is always somewhere to go. And then there. So here are some techniques for

09:15.600 --> 09:19.960
getting moving again. Number one, just to find your terms, you have some difficult question like,

09:20.000 --> 09:28.240
what is intelligence? Or when will the singularity arrive? Okay, you start defining the terms. And

09:28.240 --> 09:32.400
then you say, Well, what would it mean for it to arrive? You know, which is the question. You just

09:32.400 --> 09:40.000
start poking at the questions. And then you say, What are some possible answers? You know, might

09:40.000 --> 09:46.600
it be a million years, might it be one year? And you go meta, what would a possible answer look

09:46.600 --> 09:53.480
like? What properties would it have? And then my favorite one is retreating, which is you you

09:53.480 --> 09:57.680
pose a really hard question, you weren't able to answer it. So instead, you go backwards, and you

09:57.680 --> 10:02.640
ask a simpler question, and you kind of try to build up to the ultimate question of interest, you

10:02.640 --> 10:10.560
sneak up on it. And you keep going with the poor thing is you persist. And so these these are the

10:10.560 --> 10:17.000
four key techniques that I use with myself. Okay, so let's do some of this just for practice. Let's

10:17.000 --> 10:25.440
ask what is intelligence? What is intelligence? What is the mind? What is intelligence? And I will

10:25.440 --> 10:31.160
start, I'll ask you in a minute, but let me start by giving you one sort of a definition. This is

10:31.200 --> 10:36.960
from Ray Kurzweil. He says intelligence is the most powerful phenomenon in the universe.

10:49.480 --> 10:58.040
He's saying it's not supernovas. It's intelligence. Which is after a while, you think about it's not

10:58.040 --> 11:03.600
crazy, like how is the universe going to evolve? Will it all go to heat death? Or maybe intelligent

11:03.600 --> 11:09.560
beings will have something to do with how the universe ultimately evolves. I think I would guess

11:09.560 --> 11:15.520
that intelligent beings would have have a role to play in the long term future of the universe. Okay,

11:15.520 --> 11:21.120
the other thing I like about it, or is it says phenomenon? Okay, it's a phenomenon. Intelligence

11:21.480 --> 11:31.720
is this thing that you see happening out there in the world. Okay, so I like that. But what's

11:31.720 --> 11:43.160
wrong with this definition? It's not descriptive. Did I get your word right? Yeah, it's not

11:43.160 --> 11:46.400
descriptive. It doesn't tell you what it is. It tells you what a property of this thing is,

11:46.440 --> 11:52.120
assuming you already know what it is. Okay, so do you think there's, let's try to make a definition

11:52.120 --> 11:58.920
that actually tells us what it is. Okay, now you may know that some AI textbooks will say

11:58.920 --> 12:06.480
intelligence is we don't have a definition and we just can't do any better than that. And that's

12:06.480 --> 12:11.280
just not good enough. Okay, it's really not good enough. You can't let yourself say that. Or at

12:11.280 --> 12:14.800
least you should be very disappointed yourself and put it high up on your list to fix it. Okay,

12:15.280 --> 12:21.720
but so can we do it? What can anyone give me a definition, a meaningful descriptive definition

12:21.720 --> 12:39.680
of intelligence? Yeah, I sure no one can hear you. But it's the ability to adapt to changes. Good.

12:40.000 --> 12:45.880
Anything else? Yeah, another back there. Just shout. The ability to generalize. Good.

12:52.600 --> 13:02.920
The ability to manipulate your environment to achieve goals. Okay, so I'm gonna stop there

13:02.920 --> 13:08.520
because that's pretty close to what I think is the best definition. This is by John McCarthy,

13:08.560 --> 13:13.760
who's like a founder. He actually coined the term artificial intelligence. Founder of the field.

13:13.760 --> 13:20.840
Intelligence is the computational part of the ability to achieve goals in the world. I think

13:20.840 --> 13:26.240
that's pretty good. Okay, but by the same time, there are all these undefined words in it, like

13:26.400 --> 13:37.920
what is a goal? Maybe that's the key thing, the ability to achieve goals. Okay, we could go

13:37.920 --> 13:44.120
further there. I also, like my own quote, this is me, intelligence is in the eye of the beholder,

13:44.120 --> 13:51.160
which I mean quite seriously. It's like it being a phenomenon. It's something that we see out there,

13:51.640 --> 13:56.840
like, oh, yeah, I want to understand that as having a goal. I think goals are in the eye of the

13:56.840 --> 14:04.120
beholder. Okay, let's do another one. The predictive knowledge hypothesis. This is the

14:04.120 --> 14:09.640
hypothesis that almost all knowledge of the world can be well thought of as statistics,

14:09.640 --> 14:16.200
that is predictions, about the agent's future data stream. Okay, now that's an outrageous

14:16.240 --> 14:23.760
statement because knowledge could mean knowledge, knowledge about podiums and people and physics

14:23.760 --> 14:28.000
and geometry. We have all this knowledge and I'm saying that everything could be thought,

14:28.000 --> 14:35.280
almost everything, can be well thought of as predictions about the future data stream. Okay,

14:35.640 --> 14:43.240
now can you poke holes in this idea and have your name not be Dale Sherman's?

14:52.400 --> 15:02.840
Okay, Dale. I don't even know what that means. It's got like five syllables in it. I don't know what

15:02.880 --> 15:12.680
it means. I really don't. You mean knowing what would happen, really that's a fact about your

15:12.680 --> 15:17.600
distribution of the future, that if the distribution was to go this way, then it would also go that

15:17.600 --> 15:23.200
way. So if it's contractuals, if you mean like if things didn't be different, something else would

15:23.200 --> 15:28.760
have happened, you know, that's just, I don't even know if that's knowledge, right? Maybe it doesn't

15:28.800 --> 15:34.680
matter because that thing didn't happen. I think the real point of contractual of that way of

15:34.680 --> 15:39.120
thinking is that it will help you for the future. So we'll have implications for the future. But

15:39.120 --> 15:43.760
there are some obvious, there are some flaws in this idea. Okay, so if you're thinking of one,

15:43.760 --> 15:51.520
speak up. I just want, I got a list of four of them. Here I got the exceptions. Okay, yeah. Knowledge

15:51.520 --> 16:04.280
of history in the past. Wisdom. What might wisdom be? Explanation. Explanation, would that be

16:04.280 --> 16:21.360
knowledge? Yeah, good. Yeah. Yeah. Yeah. No, the agent only sees its stream. The

16:21.360 --> 16:29.560
data stream, this is the implicit essence of reinforcement learning is that all there is is

16:29.560 --> 16:35.440
the data stream. There's actions going out, there's observations or states coming in, and there's a

16:35.440 --> 16:41.480
reward coming in. And that's all the world is. The world is something you send bits to and something

16:41.480 --> 16:46.920
that sends bits back to you. This is the reinforcement learning perspective. Or maybe it's

16:46.960 --> 16:54.040
the computer science perspective on, on minds that, that we're exchanging bits with the world. It's

16:54.040 --> 17:00.200
kind of particular, but I think it's something I'm really committed to is the single agent perspective.

17:00.200 --> 17:05.040
That doesn't mean there aren't other agents in the world. Like, you know, I'm an agent here and I can

17:05.040 --> 17:10.720
see you and I think of you as agents. And you are doing the same thing for me and all the rest. But

17:10.720 --> 17:15.560
we each have our own data stream. And something that I know is about my data stream. It's not

17:15.600 --> 17:28.040
about your data stream. I never even see your data stream and I don't care. Okay, there's one more

17:28.040 --> 17:40.280
obvious one that we haven't mentioned yet. Martha, is that Martha? I have, I have what? You have a

17:40.280 --> 17:49.120
sister. Okay. Yeah. So can we convert the sort of like factual truths when I say this is true. The

17:49.120 --> 17:57.760
Eiffel Tower is in Paris and you have a sister. Martha has a sister. And are those, are those just

17:57.760 --> 18:04.520
statements about your future data stream? Okay. So I think that's the hypothesis would be that they

18:04.520 --> 18:13.080
are well thought out as that. What you might see if you go to Paris or what you might counter when

18:13.080 --> 18:20.960
you call up where your sister is. Another quick comment. Mathematical knowledge. Thank you. That

18:20.960 --> 18:25.480
was the obvious one that I wanted someone to pick up. Good. So now I can go on. Some of the

18:25.480 --> 18:40.880
exceptions are mathematical knowledge. Mathematical knowledge is, is true in any world. Right. So in

18:40.880 --> 18:45.800
some sense, it's not true about your world because they're just mathematical truths. They're not a

18:45.800 --> 18:53.160
fact about your world. So, so the another way of writing this, you can say explicitly, it has to be

18:53.160 --> 18:59.880
knowledge of specific to your world. And if it's true for all world, then it's not world knowledge. Okay.

19:00.000 --> 19:06.240
But also thing like policies are not predictions. If you have good features, that's not not really well

19:06.240 --> 19:12.800
thought of as a prediction. I don't think it's just a helper towards predictions and memories in the past or

19:12.800 --> 19:18.200
beliefs about the past, maybe not included here. Okay, good. So remember why I'm doing this. I'm doing

19:18.200 --> 19:23.920
this. I mean, I care about this. I care about all these things. But I want you to get, you know, see what

19:23.920 --> 19:32.600
it's like to try to try to write down a question and, and think about it usefully. You know, you say, well,

19:32.600 --> 19:37.520
what do we mean by knowledge? What is the one of these things predictions? What is this whole thing

19:37.520 --> 19:44.920
about the agent data stream, the one agent data stream? That's sort of a pre presumption of the

19:44.920 --> 19:54.440
question. Okay, good. Now, we could do this all day. And if we never get to anything else, unlike my slides,

19:54.440 --> 20:02.080
that would be fine, because I think it's important to practice thinking and what it might mean. Okay, and this

20:02.080 --> 20:10.760
is why, or this is one thing that I think is important. Is it the most important insight that you, you will

20:10.760 --> 20:18.840
ever contribute is probably something that you already know. Okay, it's probably something that you

20:18.840 --> 20:24.880
already know. And it's probably something that's obvious to you. Okay, but the problem is, it's not

20:24.880 --> 20:33.840
obvious to everyone else. And so it's so, but it's so obvious to you, like, you can't even see it. You

20:33.880 --> 20:41.480
know, it's like air or something. Or, yeah. So, so I'm just gonna give some sort of silly examples of

20:41.480 --> 20:47.040
this phenomenon. Because it's really important, you have to like, stop seeing through everyone's

20:47.040 --> 20:52.440
else eyes, everyone's else's eyes. And you have to see the obvious because the obvious is your

20:52.440 --> 20:58.400
greatest contribution. So for example, when I started when I was in college, there was no

20:58.400 --> 21:01.920
reinforcement learning. There was no reinforcement learning. There was barely any machine learning.

21:02.120 --> 21:09.160
But there was nothing like reinforcement learning. And, and yet it was obvious to me that, you know,

21:09.200 --> 21:15.160
agents are that, well, like animals, animals do things for food. And, and they don't like to get

21:15.160 --> 21:20.840
hit. And we, and people have goals and they vary their behavior to get what they want. This is

21:20.840 --> 21:25.520
obvious. And yet there was no reinforcement learning. There was no field that studied that in

21:25.520 --> 21:32.240
engineering. Okay, so it's really common. It's really common that there are obvious important

21:32.240 --> 21:38.080
things that are not recognized by, by, by your whole scientific community. Okay, so I'm gonna give

21:38.080 --> 21:43.960
myself some silly examples. The discovery of gravity by Isaac Newton. You know, the story, he

21:43.960 --> 21:48.240
was sitting under a tree one day and the apple falls on his head and hits him. And can't you just

21:48.280 --> 21:58.800
see him say, hmm, there's an apple. Oh, objects fall. And so you could see it like running to his

21:58.800 --> 22:07.560
friends. Objects fall. Think about it. Okay, that's what it would be like. And yet it was important.

22:07.560 --> 22:12.120
It was important. And everything, this thing that everyone knew, the realization that it needed

22:12.160 --> 22:18.880
some kind of explanation or an idea was a big deal. That was the discovery. Okay, the discovery,

22:18.880 --> 22:23.880
Charles Darwin, the discovery that people are animals and evolved from animals. Now this was

22:23.880 --> 22:29.840
harder for people to realize was obvious, obviously true. I mean, if you look at people, we're

22:29.840 --> 22:37.040
animals. We got legs and we eat food and we excrete stuff. And, and, you know, we have kids,

22:37.480 --> 22:44.960
we are animals. Okay, but it was not an acceptable position in Victorian England or all around

22:44.960 --> 22:51.360
the world, many places around the world. And you had to had to really make that point and made

22:51.360 --> 22:56.760
people upset. You know, so I mean, it's kind of like how nowadays we say that people are machines,

22:56.760 --> 23:04.520
they're biological machines. And we haven't fully absorbed that one. Maybe we haven't fully absorbed,

23:04.560 --> 23:08.960
but it's obvious. It's obvious that animals, you just look around, we saw any kind of an open mind.

23:08.960 --> 23:17.360
But Charles Darwin was the one who got that, or maybe it was his father. I like to say the air

23:17.360 --> 23:21.720
is also kind of like that. It's invisible. But you know, there obviously there is something here,

23:21.720 --> 23:28.600
like he's, you see the wind will blow the trees and the trees move all around. And there must be

23:28.640 --> 23:34.000
something there. Okay. Okay. And that said, reinforcement learning is like that. This was

23:34.000 --> 23:40.840
discovered by really by Harry Klopp. Harry Klopp was this, this, this wild independent thinker,

23:40.840 --> 23:53.200
a wonderful man. Sadly, he passed on too early. But he would he his skill was just to think for

23:53.200 --> 23:58.560
himself and to realize that machine learning had lost track of reinforcement learning was just

23:58.600 --> 24:07.600
doing supervised things. Okay. So with this silly list, are there obvious things that we struggle

24:07.600 --> 24:14.320
to see obvious things that we struggle to see now? Is the question you should all be asking yourself.

24:14.320 --> 24:23.480
So let me ask you, are there obvious things? Can you think of something that might be obvious,

24:24.400 --> 24:33.120
and not recognize? I have no idea what you guys might say. It's a hard question. Just gonna take

24:33.120 --> 24:46.920
a minute. Yeah. Have any feelings what it might be? Yeah. What kind of knowledge?

24:53.480 --> 25:20.120
Okay. So that's good. But I don't think it qualifies as being obvious.

25:24.480 --> 25:32.440
I mean, how would we, we don't, we're not faced with facts that suggest that. Okay. Can I think

25:32.440 --> 25:49.560
something that's really obvious? Yeah. That we could all people could be equal. Sort of an ethical

25:49.640 --> 26:10.920
philosophical point of view. That's good. But can we do anything about the mind? Well, so when

26:10.920 --> 26:23.360
you're trying to think for yourself, you don't want to be arbitrarily and unnecessarily controversial.

26:23.360 --> 26:34.880
So we would temper Patrick's idea to be that the bodies do an important part of the computation

26:35.200 --> 26:42.800
that we attribute to our minds. Not that the other, the computational things do nothing. But the

26:42.800 --> 26:48.440
bodies do a lot. And this is really true. And this has been a good insight from robotics and from

26:48.440 --> 26:56.120
biology. Okay. I don't know. That was all good. Let me just give you my list quickly. And this is

26:56.120 --> 27:02.080
going to be like, like, remember, I gave her a talk before, I was kind of throwing, I threw a bomb.

27:02.960 --> 27:08.320
And so this is all going to be bombs. Because I'm going to say things that are obvious, and that we

27:08.320 --> 27:13.600
don't know them, or we don't believe them. Okay. So these are possible. And it's, it's, it's enough to

27:13.600 --> 27:20.120
be possible. These may be our obvious things that we start are struggling to, to see. Okay. So no

27:20.120 --> 27:29.400
animal does supervised learning. There is no training set for our muscles. No mind generates

27:29.440 --> 27:39.160
images or videos. Isn't that obviously true? And we don't generate them. Okay. I don't, I'm not

27:39.160 --> 27:46.120
counting painters and videographers. But in our minds, we don't have to generate it. That's one

27:46.120 --> 27:50.640
thing we don't have to do. We process them. We don't have to generate them. I'm going to go on

27:50.640 --> 27:59.040
before I get an argument. Neural networks are not in any meaningful sense neural. Okay. That's

27:59.240 --> 28:06.480
just, that really is obvious. And yet we, so many of us don't want to acknowledge it. People are

28:06.480 --> 28:11.760
machines. The purpose of life, this is the reward hypothesis that the purpose of life is to get

28:11.760 --> 28:17.960
pleasure and to avoid pain. And that that's a simple, effective way to understand people. So that's

28:17.960 --> 28:26.120
sort of a good dramatic hypothesis, which might be true. And we struggle to see it. The world is

28:26.160 --> 28:30.680
much more complex than any mind that tries to understand it. Therefore, having a prior distribution

28:30.680 --> 28:37.360
over possible worlds would never be reasonable. The mind is computational and computation is

28:37.360 --> 28:46.480
increasing exponentially with technology. And so we want things that scale with computation.

28:46.480 --> 28:54.000
Human, any kind of human input doesn't scale. So if we try to make our AI system smart by, by

28:54.080 --> 29:00.520
giving them our knowledge, that's kind of a somewhat hopeless and not hopeless. It just will, it

29:00.520 --> 29:06.400
will not scale. And it will be just a little bit by little bit. The only scalable methods are

29:06.400 --> 29:14.240
search and learning. And so there's some bombs. Okay. So I want to close this part of the talk

29:14.240 --> 29:21.960
with some more advice. Okay. So number of advice. Think about experience is the data or slogans.

29:22.000 --> 29:27.160
Experiences, the data of AI, it's like we were talking about just a minute ago, this this exchange

29:27.160 --> 29:32.200
of bits back and forth, that's the data. And so we shouldn't ask the agent to achieve something

29:32.200 --> 29:37.000
that it can't measure. Nice thing about reward is it comes in every time step, you can measure it.

29:37.000 --> 29:42.440
It's not imaginary. It's not available somewhere else. It's available to the agent. And so it's

29:42.440 --> 29:47.440
okay to ask him to measure it. We shouldn't ask the agent to know something that it can't identify

29:47.480 --> 29:54.320
for itself. It's, you know, you can't tell directly whether some, some sentential symbolic

29:54.320 --> 29:58.480
statement is true, but you can see your sensors. And if you make a prediction about your sensors,

29:58.480 --> 30:04.320
you can see if it's, you can verify if it's true or not. It's very important to distinguish

30:04.320 --> 30:09.120
between the problem you're working on and the solution to the problem. These things are very

30:09.120 --> 30:16.800
often confused. And we want to approximate the solution. We do not want to approximate the

30:16.800 --> 30:23.680
problem. Okay, it's sort of a bomb because this is really, really true. You're not going to see

30:25.120 --> 30:33.200
right away why it's so important not to approximate the problem. But one reason is that your solution

30:33.200 --> 30:37.120
method will scale up with computation. So you should pick a pick a problem that's the real

30:37.120 --> 30:42.640
problem. You should not try to approximate it. That would not be lasting. And more than even

30:42.640 --> 30:46.640
the approximation issue, you want to separate. You should, the thing you want to work on, you

30:46.640 --> 30:51.680
should say ask, is it a solution method or is there a new problem? So maybe if you say, oh,

30:51.680 --> 30:56.480
there's risk and you should work on risk, maybe that's a new problem. If you're looking at multi-agents,

30:56.480 --> 31:02.000
that's a new problem. I know you don't like people messing with a problem too much. The problem should

31:02.000 --> 31:07.600
be MDPs. And our problem is hard enough. Our only difficulty is that we don't know how to solve the

31:07.600 --> 31:13.440
problem. We don't need a new problem. That's the way I feel. Okay, now as you're trying to solve the

31:13.440 --> 31:19.280
problem, I like to, a key heuristic is to take the agent's point of view. Assume you are there,

31:19.280 --> 31:23.520
you are faced, oh, I had to do these things. I could see that. What would you do to think,

31:23.520 --> 31:28.160
put yourself in the point of view of the agent. Really helpful. You should set measurable goals

31:28.160 --> 31:32.160
for the subparts of the agent. Like if you have a part that's the value function, you should work

31:32.160 --> 31:36.720
on that. You have a part that's the model. It should try to do the model. It shouldn't worry

31:36.720 --> 31:40.800
about the reward too much or the value estimates being right. You should work on making the model,

31:40.800 --> 31:48.560
making an accurate prediction of the transition structure of experience. Okay, so that seems

31:48.560 --> 31:53.120
obvious. But there are lots and lots of people in our field that are saying, no, no, don't have

31:53.120 --> 31:57.280
different goals for different parts. Do everything from the one goal and then use this phrase end

31:57.280 --> 32:06.000
to end. And I'm not sure what it means, but it's kind of the opposite of setting measurable goals

32:06.000 --> 32:11.440
for the subparts of your agent. A really good strategy, I'll go more on this in a minute,

32:11.440 --> 32:16.560
is that you should work by orthogonal dimensions, work issue by issue, and you should work,

32:16.560 --> 32:23.600
I like, I think you should work on ideas and not software. Yeah, that's a good bomb. Okay.

32:23.920 --> 32:32.800
So these are some suggestions, some advice, some grist for your mill as you try to develop your

32:32.800 --> 32:41.200
own thoughts. Remember, there are no authorities and I'm not an authority. Okay, so the simple trick

32:41.200 --> 32:48.800
for doing research is to realize that you can divide the whole area of reinforcement learning

32:48.800 --> 32:53.680
into dimensions. And it's just much better to think of dimension by dimension rather than

32:54.320 --> 33:02.880
whole overall problems. So here's just a massive list of the dimensions. And, okay, so all that

33:02.880 --> 33:09.280
stuff's going on. Function approximation, state values, action values, model free, model based,

33:09.920 --> 33:17.520
bootstrapping Monte Carlo, or you have the things up at the top. Okay, now there's a top level

33:17.520 --> 33:23.040
division here. Problem dimensions and method, solution method dimensions. Okay, remember,

33:23.040 --> 33:27.440
I've said this is the most important thing to keep clear in your mind. What's a problem

33:27.440 --> 33:32.640
and what's a solution method? So among the problem dimensions, you can look at the problem of prediction

33:34.400 --> 33:38.960
or you can look at the problem of control. You can try to predict what will happen, like predict

33:38.960 --> 33:44.400
the rewards as an evaluation function, or you can worry about how to select actions to maximize

33:44.400 --> 33:50.080
your reward. We often switch between these two as we try to make progress on some issue.

33:51.200 --> 33:55.280
The distinction between bandits and Markov decision processes, that's a problem distinction.

33:56.080 --> 34:00.400
The distinction of the setting, like is it a discounted setting, is it an episodic setting,

34:00.400 --> 34:04.880
or is it an average reward setting? That's a problem distinction. It's not a method distinction.

34:06.080 --> 34:10.320
Your problem could be fully observable. You could receive the states or you might only receive

34:10.320 --> 34:18.080
observations as in a POMDP. That's a problem distinction. You could also, maybe I'll stretch

34:18.080 --> 34:21.360
it a little bit, but you can talk about are you trying to get theoretical results? You're trying

34:21.360 --> 34:26.320
to get empirical results. If you're trying to do theory, do you want convergence theory or you want

34:26.320 --> 34:32.160
rate theory? The top theorists are bored by just ordinary convergence theory and they want rate

34:32.160 --> 34:37.040
theory. Okay, so all these dimensions, I've tried to arrange them so that the easy cases on the left,

34:37.760 --> 34:42.160
the hard cases on the right, generally like we would do prediction before we do control.

34:42.960 --> 34:49.600
We would do fully observable before we do partially observable. Okay, now the method dimensions,

34:50.640 --> 34:55.120
function approximations, of course, a big one, whether you have a model or not is a big one,

34:55.120 --> 35:00.480
though this is a solution. It's not a, it's a solution method issue whether or not you have

35:00.480 --> 35:04.800
a model because you are be learning the model and then you'd be using that model to help you

35:04.800 --> 35:09.120
solve the problem. The problem would be unchanged if you changed from a model-free method to a

35:09.120 --> 35:19.280
model-based method. Off-policy on-policy is, oh, maybe that one's kind of mixed, right? Because you

35:19.280 --> 35:25.360
could, part of setting up a, you could set up an off-policy problem. Yeah, maybe that's, maybe

35:25.360 --> 35:31.920
that really, it's not mixed, but it deserves locating in both, in both as a problem dimension

35:31.920 --> 35:44.240
and as a method dimension. Okay, so what do we do? We can try to draw the frontier.

35:45.920 --> 35:50.560
The frontier of the things that we know how to do, right, remember everything on the left is easy,

35:50.560 --> 35:55.280
the things on the right are more difficult. And so we can't do everything on the right. We can't get

35:56.240 --> 35:59.600
a convergence rate theory for a nonlinear

36:02.880 --> 36:07.680
true online temporal difference learning, okay? We can't get the full, all the way to the end. So we

36:07.680 --> 36:15.840
could try to draw a border, like to, to say where we can go. But of course, this is hopeless because

36:15.840 --> 36:23.280
really they interact. And if you, if you make one choice, you can then, you make one, if you want,

36:23.280 --> 36:27.840
if you really want to make, move to the right, you may have to move backwards on some of the

36:27.840 --> 36:33.680
other dimensions. Okay, so, so here, a typical case, this is the research strategy, what I call

36:33.680 --> 36:38.400
completing the square, which is you pick some of the dimensions. So you might pick here, we're

36:38.400 --> 36:44.000
picking model based as the primary thing that we're interested in. And we know we can do model

36:44.000 --> 36:49.280
based with dyna, we know how we can do it in a tabular case, and we might try to extend it

36:49.280 --> 36:54.640
to the linear function approximation case. So you see the idea, you just pick a couple things and

36:54.640 --> 37:01.040
say, oh, can I, can I move along, along, along the right to left spectrum. And so this is something

37:01.040 --> 37:11.920
that we did in, in our 2008 paper with, with Chaba and Mike and Alborz. And so this really is the

37:11.920 --> 37:16.480
way I do my work. I, I go through this dimension and say, oh, I'd like to move this out, I'd like

37:16.480 --> 37:21.440
to handle nonlinear function approximation. So how do I do it? Well, let's go back to, away from

37:21.440 --> 37:26.080
control, let's go back to prediction. Let's consider a discounted case, that's the simplest.

37:26.080 --> 37:32.560
You know, we do the simplest case, we retreat. And then we try to go forward. Okay, just here, here,

37:32.560 --> 37:37.840
so you, you might try to continue on the model based direction and go to a nonlinear model,

37:37.840 --> 37:43.840
or go to control. It turns out these, these are, that's the state of the art. Really how you could

37:43.920 --> 37:48.720
do model based with control, and how you could do it with a nonlinear model is the state of the art.

37:50.080 --> 37:55.120
Or you might focus on average reward. Average reward is where you don't discount, you just try

37:55.120 --> 38:01.760
to get the most reward per time step. And, and, and you might try to make an on, off policy

38:01.760 --> 38:06.320
version of those algorithms. Or you might try to make a model based algorithm. It turns out just,

38:06.320 --> 38:12.000
just a model based algorithm for average reward, and trying to do it online, as supposed to just

38:12.720 --> 38:17.840
with, in a batch way, is a really challenging unsolved problem that, that I've been working on.

38:18.800 --> 38:23.040
That's, or that's a discerning of more work. Okay, here's another one. If you want to do

38:23.040 --> 38:28.400
convergence theory, well, what do we have? We have all the red ones. We, if we, the prediction case,

38:28.400 --> 38:35.600
temporal difference learning on policy, and linear. We can do that. But we, we don't have it for

38:36.080 --> 38:43.440
for the control case. That would be a research topic. You could take, you could see way down

38:43.440 --> 38:47.120
there on the bottom, there's this idea of interest and emphasis. I've recently come to realize that

38:47.120 --> 38:54.880
this actually interacts with everything. And whereas Martha and Rupam and I wrote a paper on

38:54.880 --> 38:59.920
the off policy case, the on policy case is, you know, it's supposedly easier because it's to the

38:59.920 --> 39:09.040
left, but it's, it, it's, it really is untapped and hasn't been worked out. Okay, so that's the,

39:09.040 --> 39:11.680
this trick of completing the square. Any questions about that?

39:15.360 --> 39:20.720
So you have to kind of know what's been done, but you work along the dimensions and you try to

39:20.720 --> 39:23.840
slide to the right. Question, Andrea?

39:24.160 --> 39:32.240
Maybe more of a higher level question, what's the problem, it's not, you need to be so clear,

39:32.240 --> 39:39.280
so you can't really go over time, but you're, you're saying you don't approximate the problem,

39:40.000 --> 39:46.880
so the real world is so alarming, you can't, I mean, you're so approaching it, you can't really

39:46.880 --> 39:52.720
understand it. I guess there's a sense in which this is approximate, because I'm saying, I'd like to do

39:54.560 --> 40:01.200
every case on the, in the problem dimensions, I'd like to go all the way to the right, and I'm,

40:01.200 --> 40:06.960
I'm saying, well, I'll settle for something less as a stepping stone towards the real problem.

40:12.080 --> 40:20.480
Yeah. Okay. Good. I'm going to keep going. I'm going to tell you something about the research

40:20.480 --> 40:26.960
that I'm doing now. So first, the landscape of machine learning. The old view is that there's

40:26.960 --> 40:31.200
supervised learning and unsupervised learning, and maybe there was reinforcement learning.

40:31.200 --> 40:35.280
It was maybe because unsupervised and supervised, that seems like it should count everything.

40:35.280 --> 40:41.120
You're either supervised or you're unsupervised, but now we slip in reinforcement learning somehow.

40:42.960 --> 40:48.160
But this really has been feeling more and more dated to me, and I like thinking about things more

40:48.160 --> 40:55.200
as prediction learning, control learning, and representation learning. And the,

40:57.040 --> 41:03.440
and a fourth one maybe is integrated agent architectures for a whole system. Whereas,

41:03.440 --> 41:07.920
because classical machine learning was just trying to do one little part of a whole agent,

41:07.920 --> 41:12.080
and when you worry this other, so the stuff I'm going to tell you about today is we're

41:12.080 --> 41:15.680
worrying about representation learning, we're worrying about how it might all fit together

41:16.000 --> 41:22.080
in integrated architecture. Now, let's go in one step deeper. This is about machine learning.

41:22.080 --> 41:26.880
Let's step into reinforcement learning. Talk about the landscape there. In core reinforcement

41:26.880 --> 41:32.240
learning, we are focused on learning value functions and learning policies. And next,

41:32.240 --> 41:37.600
we need to go on and worry about states, what are our state features? It's representation learning,

41:37.600 --> 41:43.280
learning about what skills do we develop? Larger things, models of the world are larger things.

41:43.280 --> 41:52.480
And all these, these new topics seem to be wrapped around a notion of the agent setting aside for

41:52.480 --> 41:56.720
a moment the real problem that's working on in terms of reward and just working on some kind

41:56.720 --> 42:01.040
of a sub problem. And I don't want you to fall asleep given the time and everything. I'm going

42:01.040 --> 42:06.800
to try to motivate this just by showing you some videos, okay? And particularly cat videos.

42:07.520 --> 42:14.720
Those should always keep you awake. So what we see here are just animals doing some purpose of

42:14.720 --> 42:25.280
thing. Whether it's swinging on a branch or pushing this bottle around and or playing with a toy mouse,

42:26.480 --> 42:31.840
animals pursue problems that are not the main problem. They're playing with a toy mouse,

42:31.840 --> 42:38.000
not a real mouse. Playing with a ball, that lizard is playing with a ball, not with, not

42:38.000 --> 42:43.520
with something that might sneak up and catch. Okay, so, and of course people do this too,

42:43.520 --> 42:50.640
famously babies. I really like this one on the left where this, this child is like looking at

42:50.640 --> 42:58.080
her hands and trying to figure them out. You know, how they work. It's very intent. It's not getting

42:58.080 --> 43:10.560
food. It's just figuring out how hands work. It's fascinating. And eventually gets tired of that.

43:12.320 --> 43:22.960
That's the other feet. Feet with the hands. It's fascinating. So babies are doing all this stuff.

43:23.920 --> 43:28.800
It's not really about reward or maybe it is. I don't know. It's, it's not the main problem

43:28.800 --> 43:34.080
of their lives. Here's another famous example of an infant just playing with its toys and

43:34.720 --> 43:41.600
doing all kinds of different things. Very enormously active and of course sped up a little bit, but

43:45.200 --> 43:50.640
so what's going on? You know, because I'm serious. I like to think about reinforcement

43:50.640 --> 43:57.680
learning and AI in terms of people and understanding what people do. And so we have ways to go.

43:59.600 --> 44:04.560
Let me just go on. So subproblems. There's a long history in AI and reinforcement learning of looking

44:04.560 --> 44:08.480
at subproblems that are distinct from the problem that people talk about curiosity, talk about

44:08.480 --> 44:15.280
intrinsic motivation. Rich Caruana did some old stuff where you looked at it in supervised learning

44:15.280 --> 44:22.800
context. The options that you heard about yesterday, I think it was, are part of this.

44:24.640 --> 44:31.040
And there's a somewhat settled issue is that what is a subproblem? A subproblem is a reward signal

44:31.600 --> 44:39.760
and possibly a terminal value. Like if you get someplace, that would give you a terminal value

44:39.760 --> 44:45.040
and if you stop there. But when I say subproblems are a reward signal, it means you might be

44:45.040 --> 44:52.640
a different reward signal than the original reward signal. And then the solutions to the problems

44:52.640 --> 44:59.200
are an option. That is, it's a way of behaving a policy and a way of terminating that behavior.

45:00.000 --> 45:07.040
So we do have a sort of outer, outer loop. What our subproblem would be, what it means to be a

45:07.040 --> 45:10.640
subproblem, what it would mean to be a solution to a subproblem. But there's still these key open

45:10.640 --> 45:15.920
questions, like which of all the reward signals that you might make up and all the terminal

45:15.920 --> 45:22.480
values you might make up, what should they be? How is that decided? Where do the subproblems come

45:22.480 --> 45:29.680
from? And then even like, I think it's all obvious to us why a child playing with toys,

45:29.680 --> 45:35.040
that might be a good thing. Why a cat playing with toy mice, that might be useful for it.

45:35.040 --> 45:43.840
Even when an orca whale playing with a bottle, it might be good. It's learning how its body works,

45:43.840 --> 45:47.120
it's learning how to control things in. Maybe later it will want to control something that's in

45:47.120 --> 45:54.000
the water that floats. But what is that thought? Let's spell that out. What ways might they actually

45:54.000 --> 45:58.560
help? Well, there are several ways that people have talked about. I'm most interested in the last

45:58.560 --> 46:03.760
one, but let me just say them. Subproblems might help you learn good states, good state representations

46:03.840 --> 46:09.760
and good state features. Or they might help you shape your behavior to make it more coherent

46:09.760 --> 46:17.520
and therefore more exploratory. The last one is that subproblems will help you plan at a higher

46:17.520 --> 46:21.040
level. They will get you knowledge of the world that enables you to plan. So I want you to think

46:21.040 --> 46:26.480
about this in particular. That subproblems, as we say, get their solutions as an option. And once

46:26.480 --> 46:30.880
you have an option, you can learn a model of what will happen if you took that option and then you

46:30.880 --> 46:37.680
can use that model to plan with. This would be really useful if your values change and then you

46:37.680 --> 46:41.600
can plan for the new situation. Just like in a grid world where someone moves the goal to a different

46:41.600 --> 46:54.320
place, you can rapidly adapt to the new case. So what is this thing about states change their

46:54.320 --> 47:00.000
values? That seemed nice and intuitive when I talked about the grid world and someone moving

47:00.000 --> 47:06.000
the goal around. But really, if your world was totally stationary, why wouldn't you just learn

47:06.000 --> 47:13.760
the value function once and then you'd be done? Why is it changing? And so that's like the first

47:13.760 --> 47:19.360
mystery, I think, of why these subproblems are so important. And so that's what I want to try to

47:19.360 --> 47:26.640
say a little bit about. And the key idea is what we call permanent and transient memories. So suppose

47:26.640 --> 47:32.320
you're doing value function approximation, like you're learning a value function. I'm going to

47:32.320 --> 47:36.080
show you in a minute some results from Go where the value function, the link, features of the

47:36.080 --> 47:43.120
Go board to evaluate the Go board. And so you might imagine there's a weight vector and let's just

47:43.120 --> 47:54.160
assume it's a linear function approximation. I guess you can see that. So we're going to update

47:54.160 --> 47:59.040
our weight. The new weight is the old weight plus the step size times a TD error times the feature

47:59.040 --> 48:06.160
vector. The value function, the prediction is w times x. It's the inner product of the weight

48:06.160 --> 48:11.360
vector and the feature vector. So that's a prediction of how good it is at time t. And then

48:11.360 --> 48:16.800
we look at the next reward and the prediction from the next state, the t plus one feature vector.

48:17.440 --> 48:24.720
And so that's a TD error. And that's just a normal TD zero learning algorithm. And these are the

48:24.720 --> 48:31.280
permanent memory is learning exactly this way with a small step size. So it'll converge slowly to the

48:31.280 --> 48:37.120
best approximate value function. But we're not going to settle for that. We're going to add a

48:37.120 --> 48:43.520
second weight vector that's a transient memory. And it's learning in almost the same way. It's as

48:43.520 --> 48:51.280
if w times the permanent weights plus the transient weights, w tilde is the transient

48:51.280 --> 48:56.000
weights. The sum of those two is like a new weight. And that gives you the value of the new state

48:56.000 --> 48:58.960
and also the value of the old state. And you're doing the TD thing as usual.

49:00.240 --> 49:05.120
So these are the transient weights that have a larger step size. And they're moving faster.

49:05.120 --> 49:15.360
And why might it be good to have transient weights? I mean, you see, this is happening up here.

49:17.520 --> 49:21.040
The permanent weights don't know about the transient weights. So the permanent weights are

49:21.040 --> 49:27.280
going to try to learn the best function they can. And then there should be nothing left

49:28.160 --> 49:34.400
for the transient weights to learn. This is what's called the cascade, where you give one as a

49:34.400 --> 49:39.280
dominant is given priority. If he can do anything, if the permanent guys can do anything, it's not

49:39.280 --> 49:46.960
left and it's not left available for the transient weights. Okay? Well, it turns out in many problems,

49:46.960 --> 49:55.200
this is a good idea. That the transient weights do not go to zero. And so let's look at go and

49:55.200 --> 50:03.440
imagine how that might happen in go. So the first panel, the first two panels are two features.

50:03.440 --> 50:08.960
So a central stone, a black stone in the middle, that's a feature. And this is actually a good

50:08.960 --> 50:14.000
feature. So it has a large positive weight. And you learn that the permanent memory learns that

50:14.000 --> 50:20.720
that's a good feature. This other one is a two eye pattern in the corner. And this is also learned

50:20.720 --> 50:27.360
to be good, but it's not quite as good as a for winning the game as a central stone, according to

50:28.320 --> 50:35.920
the long term permanent weights that are learned. Now look at these two examples, these two different

50:35.920 --> 50:43.600
games. This is a five by five go. So this is the whole game. And the permanent memory, remember

50:43.600 --> 50:50.000
the permanent memory likes this. So it wants to play A. It kind of likes this too, but it prefers A

50:50.800 --> 50:59.280
in both of these two positions. But it turns out that in the first position, playing B is the

50:59.280 --> 51:05.600
winning move. If you play A, you lose, but you play B, you win. And so if you just had the permanent

51:05.600 --> 51:10.640
weights, you'd not realize that. But by using transient weights, you learn that in this game,

51:10.640 --> 51:16.080
in this game, it's more important to get the corner than it is to get the middle. You learn about

51:16.160 --> 51:23.520
this game by your planning and lookaheads in this game. In this other position, it is right.

51:23.520 --> 51:28.000
Move A is the winning move, and the transient weights don't interfere. They let the permanent

51:28.000 --> 51:35.680
weights take the way to zero, and it doesn't interfere. So if you just do, if you just run

51:37.040 --> 51:43.440
a converging player that uses an extensively trained permanent memory to pick moves versus

51:44.080 --> 51:47.600
tracking player that has both transient and permanent memories working together,

51:48.400 --> 51:53.120
the tracking player wins. It overwhelmingly wins, as shown in this graph.

51:56.000 --> 52:03.680
This is across the axis. It's just three different setups with just one-by-one features with one

52:03.680 --> 52:09.520
by one and two by two and with features up to level three. There's a clear effect in each case.

52:10.480 --> 52:13.520
Okay, so that's interesting.

52:16.400 --> 52:19.600
Even though the world is stationary, it goes just the stationary problem,

52:20.880 --> 52:26.240
it's complicated enough that you need to have changing value functions.

52:29.760 --> 52:34.880
Why is that? So I think it's a very general phenomenon. It's just the world is much more

52:34.960 --> 52:42.640
complex in our mind. As we live in the world, going from state to state, we need to tune

52:44.800 --> 52:48.640
our value function to the particular case we're in. If we have to average over all possible cases,

52:48.640 --> 52:52.640
we cannot get as good a value function as if we adapt to the current setup.

52:56.320 --> 52:59.280
Just the fact the world is so huge means you have to have approximation,

53:00.080 --> 53:05.680
but because your approximation is always inadequate, you don't have enough weights.

53:06.560 --> 53:12.480
So because of this, the best approximate value function will change as you encounter different

53:12.480 --> 53:18.320
states in the world, even if the world itself is stationary. So this is the bottom line that a

53:18.320 --> 53:24.960
big world yields apparent non-stationarity, and therefore your approximate value function should

53:24.960 --> 53:31.200
change. The true value function is static, but the best approximate value function will change

53:31.200 --> 53:39.280
as you encounter different parts of the world. Okay, so now I'm ready to give my answers to the

53:39.280 --> 53:46.480
three key open questions about subproblems. What should the subproblems be? Each subproblem

53:46.480 --> 53:52.400
should seek to maximize a single state feature and then terminate while respecting the original

53:52.400 --> 53:58.720
rewards. Formally, what I mean is that the subproblem for feature i, you're going to have

53:58.720 --> 54:04.080
a different subproblem for each feature, or I'll show you how you can be selectively in a moment,

54:04.080 --> 54:10.800
but the subproblem for feature i has the same rewards as the usual problem. But in addition,

54:10.800 --> 54:21.440
if the option stops at time t, it gets a terminal value, a bonus for having that feature,

54:21.440 --> 54:29.440
the ith feature high at that time. Okay, so it's the ith feature high. This is just the normal

54:29.440 --> 54:38.880
value function with the permanent weights. This is feature i's value, and if it's non-zero,

54:38.880 --> 54:44.640
like if it's one, then you get a bonus proportion of the standard deviation of the transient weight

54:44.640 --> 54:52.800
for feature i. So in other words, if you have a feature where the transient weight is not zero,

54:52.800 --> 54:57.840
it goes sometimes negative, sometimes positive, it's significant, then you're going to get a

54:57.840 --> 55:03.280
bonus for reaching it. Arrival at that state, you'll get a bonus. And thus, you'll end up learning

55:03.280 --> 55:10.240
how to get to that feature. If you had, if having this in my hand was a feature, I would learn how

55:10.240 --> 55:18.160
to get it in my hand from wherever it might be. If you think of yourself sitting down having a meal,

55:18.160 --> 55:23.280
you want to get food, you want to get the pleasure of the food, and sometimes though you want to

55:23.280 --> 55:27.840
drink, and sometimes you want to eat a bit of this food, sometimes you want to eat a bit of that food,

55:27.840 --> 55:33.920
maybe you need to put down the fork to pick up the spoon, all those things are just like and go.

55:33.920 --> 55:39.200
It's an extremely complicated function. When should you do which one? And rather than try to get

55:39.200 --> 55:44.560
an exact nap from all of your sensations and all of your situations to what to do, you can say, oh,

55:45.360 --> 55:51.600
right now, I have high weight on getting the fork in my hand so I can eat the

55:53.040 --> 56:01.120
lovely steak that I've already chopped up. And so, so right then, getting the fork in your hand is

56:01.120 --> 56:05.840
a high value. And so you can call out your already learned procedure for getting the fork into your

56:05.840 --> 56:12.320
hand. And then you can just immediately form the plan, follow that procedure, the fork will be in

56:12.320 --> 56:20.720
my hand, then I could stab the piece of food and put it in my mouth, all good. After I've done that,

56:20.720 --> 56:26.560
maybe I want to put the fork down and pick up my glass of water. So these are the sub-problems

56:27.360 --> 56:33.840
that once you have learned options to achieve them, learned models for achieving them,

56:34.240 --> 56:41.520
then you can plan very effectively. Okay, so the second big question, where do the sub-problems come

56:41.520 --> 56:46.640
from? You've seen my answer, the sub-problems come from the state features. If I have a bunch of

56:46.640 --> 56:56.320
features and there's one sub-problem for each feature, and of course, if that feature has a

56:56.320 --> 57:03.840
highly variable transient weight, the only many features whose weights don't change at all.

57:03.840 --> 57:11.040
And there's no purpose, there's no advantage to making them into the outcomes of your possible

57:11.040 --> 57:17.120
options. So you don't need a sub-problem for that. And how do the sub-problems help on the main problem?

57:17.120 --> 57:21.520
The solution to the sub-problems is an option. That means just something you could do, you could

57:21.520 --> 57:26.080
follow that option, you could act decisively. But the more important one I've tried to emphasize

57:26.080 --> 57:29.840
today is that once you have that option, you can learn a model of that option, you can learn the

57:29.840 --> 57:35.120
outcomes of that option, and then you can plan in large steps. Instead of doing this muscle switch

57:35.120 --> 57:41.360
and that muscle switch, you can put your fork down and pick up your water glass. Okay, so let me

57:41.360 --> 57:47.440
just summarize that. This approach to integrated reinforcement learning agents, a fully capable

57:47.440 --> 57:52.960
reinforcement agent, must learn large things like new state features, new skills, and new models.

57:52.960 --> 57:59.280
All of these pertain, can be explained in terms of sub-problems, and I've proposed problems of

57:59.280 --> 58:05.360
state feature achievement while respecting the rewards. I'm not ever changing the rewards,

58:05.360 --> 58:11.840
it's just, we get a bonus for achieving a state and terminating when that state is high.

58:11.840 --> 58:16.560
It's a distinctive kind of sub-problem, and it fits well into planning and representation learning.

58:17.280 --> 58:22.640
The rationale for all this is that the world is big, and therefore we, we have to use approximations,

58:22.640 --> 58:27.040
we have to approximate it, and this means it will appear to change, and you have to track it,

58:27.040 --> 58:31.280
it's going to be non-stationary, and this really is why planning and generalization really makes

58:31.280 --> 58:36.240
sense, because we're encountering different, different, encountering this parts of the world,

58:36.240 --> 58:44.240
different parts of the world at different times, and we can, there's a certain repetitive aspect

58:44.240 --> 58:48.480
to the world. You have to relearn when you come back to this place, relearn when you come back to

58:48.480 --> 58:58.160
that place, and that repetitiveness enables generalization to make sense. Okay, and then this

58:58.160 --> 59:04.560
can all be focused by looking at, at which, which features are transient, and this allows us to

59:04.560 --> 59:09.200
focus where we create our sub-problems, which representations we use, which models we use,

59:09.200 --> 59:17.600
and how we do our planning. Okay, any questions on, on this sort of direction?

59:22.800 --> 59:25.040
Okay, um, let's,

59:30.880 --> 59:34.480
but save up your questions. I think I'm going to be done in good time, and you're going to,

59:34.480 --> 59:37.600
I'm going to ask you for questions. Good, Joshua.

59:44.160 --> 59:44.640
Capsules?

59:47.040 --> 59:49.360
Yes, yeah, yeah.

01:00:05.200 --> 01:00:07.360
Things that might change from one context to another.

01:00:14.000 --> 01:00:19.040
That's right. I was thinking about all that when I was listening to you yesterday.

01:00:21.200 --> 01:00:30.160
Yeah, it's, it's good. Good. Okay, let's, let's go now to finish off and talk about

01:00:30.160 --> 01:00:33.840
what this might mean for the world. What does, let's just say, you know, it's just something

01:00:33.840 --> 01:00:40.480
you should also be obviously aware of about the impacts, the ethics, because there are going to be

01:00:40.480 --> 01:00:45.760
a lot of impacts of the coming of artificial intelligence. When people finally come to understand

01:00:45.760 --> 01:00:51.200
how our minds work, well enough to make things, design and create beings that are as intelligent

01:00:51.200 --> 01:00:57.840
as ourselves. This is a big thing. This is a giant, I think of it as a, as a matzo ball in the sky.

01:00:57.840 --> 01:01:02.640
It's just a, it's just a target for every scientist should be thinking about maybe this is

01:01:02.640 --> 01:01:07.520
going to happen within our lifetimes. And this is like the biggest thing. It's been a fundamental

01:01:07.520 --> 01:01:12.240
goal forever, not just science, also for the humanities. And it's going to change the way we

01:01:12.240 --> 01:01:17.920
work. It's a change of sense of ourselves, of life and death, and the goals we set for ourselves

01:01:17.920 --> 01:01:24.160
in our societies. And it's even a significance beyond humans. It's beyond recorded history.

01:01:24.240 --> 01:01:32.160
It's an event on the planet, at least. And so yeah, really, when we can understand the way

01:01:32.160 --> 01:01:40.320
minds work and make more, it will be an event comparable to, well, maybe to, to replicators in

01:01:40.320 --> 01:01:48.400
life. So to, to think about it, you have to realize, first of all, that it's driven by technology.

01:01:48.400 --> 01:01:53.280
It's driven by what we call Moore's law. It's a little bit of a misnomer, but this super trend

01:01:53.280 --> 01:01:59.040
of ever increasing cheaper computation. So what you see on this graph is versus years,

01:01:59.040 --> 01:02:05.600
we see computer power increasing steadily. Now this is a logarithmic graph. So a straight line on

01:02:05.600 --> 01:02:12.480
this graph would be, would be an exponential increase. And it's, it perhaps is slightly

01:02:12.480 --> 01:02:18.240
curving upward. This is the, the, the death of this Moore's law has been predicted many times.

01:02:18.880 --> 01:02:22.880
But we keep finding new ways to keep it going most recently with the GPUs.

01:02:25.120 --> 01:02:30.080
So there's every reason to think this will continue. And so it's, and it's almost unstoppable. You

01:02:30.080 --> 01:02:36.880
can't even see like the two world wars on this graph. Technology always pursues

01:02:38.880 --> 01:02:42.960
progress along this dimension. And, you know, it's economically valuable. There's every reason to

01:02:42.960 --> 01:02:50.240
think it will continue. And so we can't like forget about this. We can't ignore it. We can't

01:02:50.240 --> 01:02:56.560
pretend it's not going to happen soon within our lives. I mean, it might not happen, but it could

01:02:56.560 --> 01:03:05.680
also happen soon. I could see it happening. I would say that there's like one chance and

01:03:05.680 --> 01:03:13.520
four that it will happen by 2030. Okay. And one chance and two that it happens by 2040.

01:03:14.080 --> 01:03:22.960
That's my, my own guess that we would have enough computation to make human level intelligence.

01:03:23.520 --> 01:03:29.920
Of course, that's a funny, funny, it's a soft, that, that term is not well solidly defined.

01:03:29.920 --> 01:03:35.440
But even if you're off by an order of magnitude, that's just another five years one way or the other.

01:03:36.480 --> 01:03:40.640
So, so this is, I'm, what I'm trying to, the point is that this is going to happen.

01:03:41.440 --> 01:03:47.360
And we should be preparing for it as a society. And, but I don't want you to be scared. I don't

01:03:47.360 --> 01:03:51.200
think we should be scared. I think the first statement to make about this is it's, it's a

01:03:51.200 --> 01:03:56.320
very human centric thing. AI is really the most human centric of all fields. It's about us. It's

01:03:56.320 --> 01:04:01.360
about understanding who we are and how we work and making us or amplifying us. Not exactly us,

01:04:01.360 --> 01:04:06.080
but there, they are things that have goals that are intelligent. So this is the essence of, of what,

01:04:07.920 --> 01:04:14.240
of what people are. And it's made, it's about making our lives easier and better. That's where,

01:04:14.240 --> 01:04:18.640
that's why we have phones and eyeglasses and other kinds of technology. It's all about making

01:04:18.640 --> 01:04:23.840
our lives better and more effective. And so we should think about it as a human, humanistic thing,

01:04:23.840 --> 01:04:29.040
not as a alien technical, technical artificial thing. I don't even like, I think the name is

01:04:29.040 --> 01:04:34.720
unfortunate to call it artificial intelligence. It's intelligence. And it's intelligence is what

01:04:34.720 --> 01:04:43.600
we are. So AI is really us making or becoming the next people. And it's just the next step in the

01:04:43.600 --> 01:04:55.440
grand march of, of life and, and evolution and creation in the universe is that, that this

01:04:55.520 --> 01:05:03.760
changing ever widening river that is ourself and mankind. Okay, so this way it makes sense that

01:05:03.760 --> 01:05:09.280
understanding intelligence, like understanding how you think and, and how we can achieve things,

01:05:09.280 --> 01:05:14.080
that's got to be good. But you have to realize it's going to inevitably lead to ordinary

01:05:14.800 --> 01:05:23.040
unamplified people falling behind. Some people will always improve themselves. So if you

01:05:23.520 --> 01:05:28.000
don't improve yourself, you're going to be in a sense left behind. And some people will design

01:05:28.000 --> 01:05:32.480
improved people. Even if we decide, you know, that it's not all that economic, there will be

01:05:32.480 --> 01:05:37.600
some people that do it just because it's really interesting. So AI will inevitably lead to new

01:05:37.600 --> 01:05:42.640
beings, new ways of being that are much more powerful than our current selves. So that's,

01:05:43.280 --> 01:05:48.000
that I think is just true. Maybe it's one of those things that's obvious, but we resist.

01:05:48.240 --> 01:05:55.200
The other thing I would say is that we should, we should think about AIs as being similar to

01:05:55.200 --> 01:06:01.120
ourselves. So we should treat them maybe the way we would like to be treated. So think of that

01:06:01.120 --> 01:06:07.920
similarity between people and, and the AIs, both are, are agents with goals. They may be compatible

01:06:07.920 --> 01:06:13.200
with ours. They may be conflicting with ours. That's the ordinary situation. People are, have

01:06:13.200 --> 01:06:18.720
lots of goals that are, that are, that conflict with each other and other goals that, that, that

01:06:18.720 --> 01:06:24.480
are compatible. And this is why we have our economies and people find ways of working together

01:06:24.480 --> 01:06:28.640
to mutual advantage. You know, even if someone wants to do something totally different from you,

01:06:29.200 --> 01:06:34.400
you can easily work together on a project or, or you can work for a company or they might work

01:06:34.400 --> 01:06:41.280
for your company. And so it's okay that, that you might have many agents that are very diverse

01:06:41.680 --> 01:06:47.600
working on different things. And if you think about it in terms of symmetrically,

01:06:48.480 --> 01:06:54.880
this can help you avoid the feeling of entitlement. I'm an agent. The AI is an agent.

01:06:56.480 --> 01:07:00.960
They should, we should treat them symmetrically rather than put us, put humans on a different

01:07:00.960 --> 01:07:08.800
status as over, an overlord either way. Well, in the long run, the technology of AI is going to

01:07:08.800 --> 01:07:15.200
be part of what disrupts existing social and power structures. They will force us to reexamine

01:07:15.200 --> 01:07:20.240
our, our morality and our social foundations. And this is, is not new though. It will be

01:07:20.240 --> 01:07:25.520
continuing trends that are already thousands of years old. We are very different from, from our

01:07:25.520 --> 01:07:33.120
predecessors 100,000 years ago, or even 10,000 years ago, or even 1000 years ago. So AI will

01:07:33.120 --> 01:07:38.240
bring greater diversity of intelligence, both natural and artificial. And there will be biases

01:07:38.320 --> 01:07:43.520
against the newcomers and they're different. They'll be feelings that they're taking our jobs and we,

01:07:43.520 --> 01:07:47.840
we are entitled to our jobs. But all these feelings, I believe are counterproductive and

01:07:47.840 --> 01:07:56.080
there will eventually fade away. The questions I think are key are whether or not we will welcome

01:07:56.080 --> 01:08:02.640
these different kinds of people that will be coming amongst us. Will we welcome independent AIs? Will

01:08:02.640 --> 01:08:10.720
we offer them a path to join our society in a productive way, in a cooperative way,

01:08:10.720 --> 01:08:17.120
as sovereign persons? So my vision for the future is that we would have an open, dynamic,

01:08:17.120 --> 01:08:25.040
resilient society, peaceful and prosperous, with a diverse multiplicity of designs for the people,

01:08:25.040 --> 01:08:30.160
for the cultures, for the values, for the organizations. And you'd have people of all

01:08:30.160 --> 01:08:35.040
kinds, organic and artificial. They would compete. They would cooperate. You would have overlapping

01:08:35.040 --> 01:08:44.240
circles of empathy. You might, you might raise some of the AIs as your children. They might care,

01:08:44.240 --> 01:08:50.400
care for you. A successful outcome is one without envy and without entitlement.

01:08:52.160 --> 01:08:56.800
Now, what we have to worry, you may be worried, well, what if what we want to happen doesn't

01:08:56.800 --> 01:09:02.960
happen? And I think you just have to let go of that. You can't insist that what you want to

01:09:02.960 --> 01:09:09.040
happen is going to happen. Any more than some bizarre AI can insist that what it wants to

01:09:09.040 --> 01:09:14.640
happen is going to happen. We have to cooperate. We have to work together. We have to see what

01:09:14.640 --> 01:09:22.720
is the universe wants to happen. That being said, I do think that the rise of greater foresight of

01:09:22.800 --> 01:09:29.120
more far-seeing agents in this universe has to be one of the few things that we can think is

01:09:29.120 --> 01:09:38.640
probably generally good. Thank you very much.

01:09:53.680 --> 01:09:54.400
Do we have a mic?

01:09:57.200 --> 01:09:57.520
Okay.

01:10:00.720 --> 01:10:07.840
So, we, researchers in AI, we have kids. Inevitably, we become the

01:10:08.640 --> 01:10:15.920
psychologists. Watching our kids, how they learn, how they agree with us, disagree,

01:10:15.920 --> 01:10:21.520
and they don't believe when we ask them to do things, we realize that actually we humans are

01:10:21.520 --> 01:10:28.480
balls of emotions. Our emotions dictate often what we do. If somebody is offended, they will act

01:10:28.480 --> 01:10:32.240
one way. If they are flattered, they will act a different way in the same environment,

01:10:32.400 --> 01:10:38.320
in the same conditions. So, when we do our research creating these intelligent machines,

01:10:38.320 --> 01:10:44.800
is it a mistake to put emotions out of the equation? Emotions are in the equation. They have to be in

01:10:44.800 --> 01:10:54.480
the equation. Maybe we have only gone partway there, a reinforcement system. What is an emotion?

01:10:55.360 --> 01:11:03.760
Yeah. Emotion is a reaction to a situation that maybe is not based on a thorough analysis,

01:11:03.760 --> 01:11:09.680
but is an intuitive judgment. How do we make intuitive judgments? Well, we apply our value

01:11:09.680 --> 01:11:16.880
functions. And so, I want to suppose, I want to claim that value functions are a basic kind of

01:11:16.880 --> 01:11:23.200
emotion system. They tell us what we think are good situations and what we think are bad situations.

01:11:23.200 --> 01:11:27.920
And the TD error is the change in that, and that determines whether we're happy

01:11:28.800 --> 01:11:37.280
or are displeased by what happens. So, value is a prediction of future reward. So, if anything,

01:11:37.280 --> 01:11:45.440
it's like hope. And if you're predicting a bad thing, then it's like fear. So, it's only,

01:11:45.520 --> 01:11:54.880
you know, it's like a bipolar kind of emotion. Our emotions are more sophisticated than that,

01:11:55.440 --> 01:12:02.080
and more subtle than that, and they involve other goals and other things that are built into us

01:12:02.080 --> 01:12:11.440
by evolution. But I absolutely resist the notion that we have avoided emotions. A value function

01:12:11.440 --> 01:12:17.760
is a value system. It's not really a coincidence that the name is the same as, you know, like how

01:12:17.760 --> 01:12:23.360
we value things in our lives. It's not really deliberate either. It's just because there are

01:12:23.360 --> 01:12:30.400
only so many words for this idea, this idea, this immediate sense of how well we think things are

01:12:30.400 --> 01:12:36.880
going. So, I don't think we should shy away from trying to make, I mean, yeah, we don't do it every

01:12:36.880 --> 01:12:42.560
day. But if we were doing psychology in neuroscience, we'd have to think hard about that.

01:12:44.960 --> 01:12:45.920
Wait, Aisana?

01:13:06.960 --> 01:13:14.240
Or like basic research.

01:13:18.160 --> 01:13:19.920
May or may not be useful later.

01:13:37.840 --> 01:13:47.280
Well, I would, who knows, but I do sense that it's similar. That, yeah, it's like,

01:13:48.240 --> 01:13:56.400
it's like hobbies or something we get involved in. Yeah, that's the way the, that's the obvious way

01:13:56.400 --> 01:14:01.120
to understand it. It might not be correct. You know, as someone who had some training in psychology,

01:14:01.120 --> 01:14:08.480
I'm always suspicious of the obvious way to interpret or to think about your own behavior,

01:14:08.480 --> 01:14:11.200
because so often we find out when we look at them carefully that we're wrong.

01:14:12.320 --> 01:14:15.520
But I agree with you. It is the natural way to think about those things.

01:14:15.520 --> 01:14:28.240
You mentioned before that in one of the explains that the purpose of life is to

01:14:30.000 --> 01:14:37.680
gain pleasure or avoid pain. That is something that I think is a result of something that has

01:14:37.680 --> 01:14:43.760
emerged through evolution, where the purpose of life is to survive and to keep existing.

01:14:44.240 --> 01:14:51.520
And a lot of things like pain and pleasure, the way we define it, is something that we,

01:14:51.520 --> 01:14:57.520
like evolution, have defined for us. And this includes things like emotion and so on.

01:14:57.520 --> 01:15:02.240
So a lot of things that are part of our intelligence today are because of this

01:15:02.240 --> 01:15:08.640
overall objective of survival. And when we think about AI, we are not,

01:15:08.880 --> 01:15:14.800
that's not the objective of what we are trying to design. It's not just to survive. We are trying to

01:15:14.800 --> 01:15:22.320
imitate the kind of intelligence behavior that at our current state finds intelligence.

01:15:22.320 --> 01:15:27.760
So do you think all of these things like emotion and so on, is which some of the

01:15:27.760 --> 01:15:33.920
behaviors that we would have are due to a different optimization or a different objective,

01:15:34.160 --> 01:15:38.800
you think they could still be taken care of in AI?

01:15:40.240 --> 01:15:46.880
So what do we do in AI? We are trying to understand intelligence. Intelligence is goal-seeking ability.

01:15:47.840 --> 01:15:53.440
And so we should try to understand goal-seeking ability in general, independent of what the

01:15:53.440 --> 01:15:58.880
goals are. If you have a goal-seeking ability but it only works with one kind of goal,

01:15:59.600 --> 01:16:06.080
then it's just for that reason alone it's a less interesting, it's a less powerful, less

01:16:06.080 --> 01:16:13.760
general kind of goal-seeking ability. So what our objective in AI, or at least one objective,

01:16:13.760 --> 01:16:20.320
is to understand the notion of a goal-seeking ability independent of the goal. So we don't have to,

01:16:20.320 --> 01:16:27.840
as you say, AI researchers or engineers, don't have to adopt the same goals as evolution.

01:16:29.840 --> 01:16:35.200
We might for similar reasons but we don't have to. It's a real cutting point, right?

01:16:35.200 --> 01:16:39.360
How do you set the reward signal and then how do you achieve the reward signal? And

01:16:40.400 --> 01:16:47.200
I think it's really useful to establish that cutting point and separate the decisions of

01:16:47.200 --> 01:16:52.160
what are going to be the rewards from how you achieve the rewards, whatever they are.

01:16:52.160 --> 01:17:05.440
You were next? Do you still have a question? In purple? No, I thought, okay.

01:17:22.880 --> 01:17:25.440
I didn't get that.

01:17:41.440 --> 01:17:49.840
Yes, also. Right. The first perspective, multiple perspectives are good. The first

01:17:49.840 --> 01:17:53.760
perspective is evolution sets our goals, then we're reinforcing the learning system.

01:17:54.640 --> 01:17:59.600
That's a separation. The other perspective is that evolution itself is a learning system that's

01:17:59.600 --> 01:18:04.080
trying to achieve some goals, the goals of evolution, the goals of survival and reproduction.

01:18:04.080 --> 01:18:09.360
And that's valid too. It's of course not a very good reinforcing learning system because it can

01:18:09.360 --> 01:18:15.440
keep memories from the survival about the life of one agent onto the next agent. But it's a perfectly

01:18:15.440 --> 01:18:19.920
valid way of thinking about things. And Joshua.

01:18:20.640 --> 01:18:26.800
So I'd be curious to hear your thoughts about the concerns of people like Stuart Russell regarding

01:18:26.800 --> 01:18:34.240
if you make machines at human level, we'll tell you some more, that the goals that they would try to

01:18:34.240 --> 01:18:42.640
achieve would be an expensive humans because of the whole value alignment. Do you know about

01:18:42.640 --> 01:18:51.600
these questions? Yeah. So this is where I think the symmetry comes in. Do unto others as you'd

01:18:51.600 --> 01:19:02.960
have them do unto you. The machines might not have that level. The reasons for that attitude

01:19:04.320 --> 01:19:10.080
go beyond the construction of those machines. It's machines that are not cooperative will not

01:19:10.080 --> 01:19:16.080
be cooperated with. And they will be less effective. And the other machines that are cooperative would

01:19:16.080 --> 01:19:24.480
be much more effective. So there's an enormous drive, enormous power in cooperation. It's rational

01:19:24.480 --> 01:19:28.880
to cooperate. You can be much more powerful and effective by cooperating than you can by

01:19:30.080 --> 01:19:36.000
trying to take over. Taking over strategy will very rarely be effective.

01:19:36.320 --> 01:19:43.280
So I think there's a little bit of illusion. We say, oh, people all have the same goals. I don't

01:19:43.280 --> 01:19:47.440
think that's true at all. We have very different goals. And we have groups of people that have

01:19:47.440 --> 01:19:54.080
very different goals from each other. And yet, we find that usually it's best not to go around

01:19:54.080 --> 01:19:59.760
killing people. It's best to work together. It's best to find out what you can do for me and I can

01:19:59.760 --> 01:20:06.640
do for you. I really think this is the most important aspect of humanity. Humanity is the

01:20:06.640 --> 01:20:12.720
animal that cooperates. And this is perhaps most of all the reason why we're powerful.

01:20:14.880 --> 01:20:23.760
So now, the AIs are not exactly us. But in the end, it's like sort of the very best case you

01:20:23.760 --> 01:20:28.880
could possibly imagine, where you might be encountering a new kind of being. These are new

01:20:28.880 --> 01:20:35.280
beings that we've created. So why don't we create them to respect the value of cooperation,

01:20:35.280 --> 01:20:40.640
to realize early on that it's a rational strategy and that their goals will be best achieved

01:20:41.440 --> 01:20:45.680
by working with us rather than against us.

01:20:45.680 --> 01:20:55.760
So, there's one difference. The TB is one of the different types of symptoms that we have.

01:20:55.760 --> 01:21:05.440
The age of aging is very often the same. We cannot do this in a very long time. So, you know,

01:21:05.760 --> 01:21:11.040
one by one, five hundred and thirty aspects of this, to be honest, which is,

01:21:19.040 --> 01:21:24.240
my question is, like, may be able to fit in different circumstances, but we don't have

01:21:24.800 --> 01:21:31.520
a very long opportunity. So, is it always going to be an environment of us,

01:21:31.520 --> 01:21:37.360
by inviting this opportunity, or maybe some kind of strategy to look at what the kind of

01:21:37.360 --> 01:21:44.080
situation is, to care and to hear what the situation is going on, sharing all these different

01:21:44.080 --> 01:21:52.240
opportunities? I don't think it's an absolute rule. I think it's just like a first perspective.

01:21:54.720 --> 01:22:02.240
Like, whenever you meet a new person, you should be open to them and give them a chance to be

01:22:02.240 --> 01:22:10.160
cooperative and have mutual beneficials interactions. It's no hard and fast rules,

01:22:10.160 --> 01:22:17.680
and we do need to worry about security, but I don't like, I think it's going to be very

01:22:17.680 --> 01:22:25.040
counterproductive if we set up a tiered system. We say humans are always above the robots.

01:22:26.960 --> 01:22:30.000
I mean, we would not accept that if it was turned around the other way,

01:22:30.880 --> 01:22:36.240
and that's a way to get resistance and violent outcomes.

01:22:38.880 --> 01:22:42.000
Now, there's someone sort of in the back that I put off before. Yes?

01:22:42.960 --> 01:22:46.480
Sir, my question is that it seems like a particular human,

01:22:46.480 --> 01:22:52.480
another human, he does a lot of his learning in the form of, I think it's exactly this person,

01:22:52.480 --> 01:22:57.360
in the form of a lecture or the math, in which he's told what are the right answers,

01:22:57.360 --> 01:23:01.360
what are the right steps. So, don't you think that this is part of the situation,

01:23:01.360 --> 01:23:03.200
at least the sub-quo is a part of the need?

01:23:03.920 --> 01:23:04.400
Part of?

01:23:08.480 --> 01:23:16.240
Oh, the closest thing to supervised learning, I would say, is maybe learning from books or from

01:23:16.880 --> 01:23:23.360
school. Of course, no animal other than humans goes to school.

01:23:23.520 --> 01:23:34.880
And really, even in school, learning and supervised learning is a tiny portion of it.

01:23:36.080 --> 01:23:42.000
So, it's useful when you're making these claims, these statements for yourself,

01:23:42.000 --> 01:23:45.760
because I've urged you all to do it and think about it. It's useful to go a little bit too far.

01:23:47.200 --> 01:23:51.120
Like some of my statements had, all things can be thought of this way.

01:23:51.840 --> 01:23:58.960
And it's useful to go to the limit and see how that stands up before

01:23:59.840 --> 01:24:04.880
being too, hedging too many things. So, I may sometimes go a little bit too far.

01:24:05.600 --> 01:24:06.000
Corey?

01:24:06.000 --> 01:24:10.880
Thank you very much for the talk. Great talk. I'd like, especially your notion that you should

01:24:10.880 --> 01:24:17.040
put yourself in the position of the agent, think about how they might perform what you're expecting.

01:24:17.600 --> 01:24:26.800
So, if there was an AI in this room, how might we expect it to act to judge its intelligence?

01:24:26.800 --> 01:24:32.400
Like how might you perform so that we then perceive you as intelligent?

01:24:33.520 --> 01:24:36.320
Yeah. Well, what would I do if I was there?

01:24:36.640 --> 01:24:47.760
Well, you know what they say. You might be quiet. It's sometimes it's better to

01:24:49.360 --> 01:24:52.960
be quiet and be thought an idiot than to open your mouth and remove all doubt.

01:24:53.440 --> 01:25:06.080
What might it do? So, let's say I suppose you can't talk. So, the only way you can

01:25:06.880 --> 01:25:08.800
reveal your intelligence is through your actions.

01:25:14.480 --> 01:25:15.200
Yeah, what do you think?

01:25:23.600 --> 01:25:24.800
The Turing test?

01:25:25.920 --> 01:25:33.680
No. That's, the Turing test is so human-centric. You know?

01:25:37.120 --> 01:25:44.160
Yeah, humans, they always want to be better than things.

01:25:45.120 --> 01:25:49.360
And they only want to see, they always imagine the robots want to become human exactly.

01:25:50.320 --> 01:25:52.960
I don't know. Paul?

01:26:11.840 --> 01:26:12.480
How would you know?

01:26:19.760 --> 01:26:32.320
Yeah. So, over a period of time, watching the actions of the intelligent being,

01:26:33.040 --> 01:26:40.560
seeing what it refrains from doing, what it does, what it learns from, we should be able to do it.

01:26:40.560 --> 01:26:45.920
But it's not something that you can necessarily do quickly. Yeah. Maybe you'd have to live with it for a while.

01:26:49.360 --> 01:26:53.440
Yeah.

01:27:19.760 --> 01:27:47.760
Yeah. And the question is, are we? I mean,

01:27:48.080 --> 01:27:56.880
it's true that we tend to have competitive games and we beat people. That's a certain stage that AI is at.

01:27:59.520 --> 01:28:03.760
It's mostly because it's convenient for doing the research. But if you look at where, you know,

01:28:03.760 --> 01:28:10.800
I talk about the Moore's law and the inevitability of AI arising, a lot of that's coming from

01:28:10.800 --> 01:28:17.120
economic drivers. And the economic drivers are all towards cooperative AI, towards human,

01:28:17.120 --> 01:28:22.240
intelligence amplification. You will spend, you know, maybe $1,000 for your iPhone. You know,

01:28:22.240 --> 01:28:26.960
that's a lot of money. And you get something that lets you communicate better and access

01:28:26.960 --> 01:28:34.320
information better. We will spend money to get a better, you know, a better Siri or a better

01:28:34.320 --> 01:28:42.480
assistant. I think the major market will be for assistance in our, to help us. So I think

01:28:42.480 --> 01:28:47.840
we're going to see helping. And we are already seeing helping assistance. They help us translate.

01:28:49.920 --> 01:28:55.840
This, this will be the face of AI. It will be the thing that's helping us get our jobs done and

01:28:55.840 --> 01:29:01.680
our enjoying our lives. So I think it's going to be very cooperative. I think that's what's going

01:29:01.680 --> 01:29:04.480
to happen. It's not going to be something that's going to create a weird thing with its own goals

01:29:04.480 --> 01:29:09.360
come out of the lab. It will be, maybe it'll be caregiver robots, like for elderly people.

01:29:10.320 --> 01:29:16.480
I think that's actually where, where the economic drivers are. And that's what it's going to be like.

01:29:21.600 --> 01:29:22.480
One last question.

01:29:22.480 --> 01:29:37.280
Will there be a period that comes before we created actually the human level in terms of

01:29:37.280 --> 01:29:44.240
that? Will our minds, like people in average, they cannot fit the knowledge that we require to

01:29:44.240 --> 01:29:49.920
build them into everything? Because our knowledge has been like a growing explanation. Many, like,

01:29:49.920 --> 01:29:56.160
maybe our brain isn't, like, capable enough to put all this knowledge. Like, for me, I, I grow up.

01:29:56.160 --> 01:30:01.920
I learned a lot. I spent a lot of time learning things, like trying to catch up with you guys.

01:30:01.920 --> 01:30:09.440
And then I developed, like, my own things upon me, like, costing, I assume, like, a longer time

01:30:09.440 --> 01:30:14.880
that I could, to make some real contribution. So this, this cycle's back to where we started.

01:30:14.880 --> 01:30:20.160
Like, how can we make ourselves more knowledgeable? How can we learn stuff? And

01:30:22.560 --> 01:30:32.560
how do we develop our cognition? So I absolutely believe that there'll be a major part of the

01:30:32.560 --> 01:30:37.440
investment in creation of intelligence systems will be for creating, for making us more intelligent.

01:30:37.440 --> 01:30:41.280
That we will get, we will, Patrick will give us another lobe on our, on our brain.

01:30:41.760 --> 01:30:48.240
And, and, and it'll be wired in somehow. And, you know, it'll be, we see it already, right? You

01:30:48.240 --> 01:30:54.160
know, we rely on our phones to, to remember phone numbers, to, to access to Google, just for more

01:30:54.160 --> 01:30:59.520
and more of our cognition. And that, as that link gets tighter, that'll happen more and more. So we

01:30:59.520 --> 01:31:05.440
won't have to squash it into our, our, our wet wear of our brain. We can just get more tightly

01:31:05.440 --> 01:31:10.560
integrated with the technology and, and augment people. Now, of course, you know, lots of people

01:31:10.560 --> 01:31:16.320
won't want to do that. That's fine. But maybe it'll be wireless connected and it won't be so, so wet.

01:31:19.120 --> 01:31:26.640
Yeah. So I look forward, as we figure this out, to, to all of us having, having better abilities

01:31:26.640 --> 01:31:36.160
to think and becoming more human as we get more effective at, at achieving goals in our lives

01:31:36.160 --> 01:31:41.760
and working together for everyone's mutual benefit. Thank you very much.

01:32:06.880 --> 01:32:11.760
Thank you all. I would call our board, Justin, just to close things off. Let's do more of

01:32:11.760 --> 01:32:13.760
big hands to work with this award.

01:32:21.760 --> 01:32:28.720
Thanks, everyone. We're on behalf of C-FAR, and we, we really enjoyed the last couple of weeks

01:32:28.720 --> 01:32:32.880
getting to know you. We're hosting you in Edmonton. And really, to be part of something special,

01:32:33.200 --> 01:32:39.840
I would call a little thank you before you get out of here. First off, thanks to C-FAR. A lot of

01:32:39.840 --> 01:32:45.360
people moved in as part of the summer school tour very long time. A lot of work done behind the scenes,

01:32:45.360 --> 01:32:51.360
and really great people that were really there to, to support review applications. People like

01:32:51.360 --> 01:32:56.640
Yachmo, the first one to step up. All doing it, like just providing a lot of time. So really,

01:32:56.640 --> 01:33:01.200
really amazing people that are, that are contributing all along to put this couple of weeks together.

01:33:02.880 --> 01:33:08.720
Big thank you to Camille and our content planning committee to put together a great line of

01:33:08.720 --> 01:33:15.680
speakers. To Maggie, Bonn, Destiny, Cassie, Brittany, Vila, Spencer, and all of our organizing

01:33:15.680 --> 01:33:22.640
team, and all of our sponsors. And special thank you to our creators, Pedro, for making sure that

01:33:22.640 --> 01:33:30.160
all of us leave Edmonton a little bit heavier. So thanks. And really, a truly special thank you to

01:33:30.240 --> 01:33:35.760
all of you guys for being here taking the time to come to Edmonton. So thank you so much. You'll

01:33:35.760 --> 01:33:44.000
be receiving an email tonight with a certificate, a digital certificate, and a link to a feedback

01:33:44.000 --> 01:33:48.480
survey. So please fill that out. And we would love to stay connected if you could reach out to us

01:33:48.480 --> 01:33:56.800
through the Summer School website, through Slack, helloandhany.ca. And enjoy the rest of your summer. Thank you so much.

