1
00:00:00,000 --> 00:00:08,120
All right. Hello everyone. Welcome to AME HQ. For those who I haven't met, my name is

2
00:00:08,120 --> 00:00:12,480
Cam Linky. I'm the CEO here at AME and we're pleased to welcome everybody today,

3
00:00:12,480 --> 00:00:17,680
both to our office here and to our friends who've tuned in online on the

4
00:00:17,680 --> 00:00:22,680
live stream. Many of you probably know about AME, but for those who don't, AME is one

5
00:00:22,680 --> 00:00:26,800
of Canada's three national standards of artificial intelligence. We're tasked

6
00:00:27,120 --> 00:00:31,600
with advancing Canada's AI potential and we're proud to collaborate with the

7
00:00:31,600 --> 00:00:38,160
University of Alberta on driving AME's AI research excellence forward. We're

8
00:00:38,160 --> 00:00:41,840
really, really excited to have everyone here today for this special

9
00:00:41,840 --> 00:00:46,200
announcement from our Chief Scientific Advisor, Rich Sutton. Before we get

10
00:00:46,200 --> 00:00:50,480
started, we'd like to respectfully acknowledge that we're on Treaty Six

11
00:00:50,480 --> 00:00:54,640
Territory, a traditional gathering place for the diverse Indigenous peoples,

12
00:00:55,040 --> 00:01:02,320
including the Cree, Blackfoot, MÃ©tis, Nakoda Sioux, Haudenosaunee, Dene, Ojibwe,

13
00:01:02,320 --> 00:01:08,560
Sotu, Anishinaabe, Inuit, and many other peoples whose histories, languages, and

14
00:01:08,560 --> 00:01:14,480
cultures continue to influence our vibrant community. And it's in the spirit of

15
00:01:14,480 --> 00:01:18,880
that exchange of knowledge and of that gathering that we're excited to welcome

16
00:01:18,880 --> 00:01:23,840
everyone here today. AME, we're really excited to support our friend and mentor,

17
00:01:23,840 --> 00:01:27,680
Rich Sutton. There's so much that could be said about Rich and the pioneering

18
00:01:27,680 --> 00:01:32,000
work that he's done in reinforcement learning and AI. The span of Rich's

19
00:01:32,000 --> 00:01:36,560
impact is almost hard to enumerate from his dedication to fundamental research

20
00:01:36,560 --> 00:01:41,680
and the advancement of the science of AI to his efforts to train the next

21
00:01:41,680 --> 00:01:46,720
generation of AI researchers. His book, Reinforcement Learning and Introduction,

22
00:01:46,720 --> 00:01:52,000
has both educated and inspired scores of graduate students and beyond, and is one

23
00:01:52,000 --> 00:01:58,160
of the most approachable people in the field, always having time for researchers,

24
00:01:58,160 --> 00:02:04,240
for curious observers, and for everyone in between to sit down, have a discussion,

25
00:02:04,240 --> 00:02:08,320
discuss reinforcement learning, discuss artificial intelligence. He's one of the

26
00:02:08,320 --> 00:02:13,600
people that, while many people have already left either the party or the conference,

27
00:02:13,600 --> 00:02:16,800
Rich will be in the corner somewhere having a conversation with someone about

28
00:02:17,520 --> 00:02:23,280
reinforcement learning, about some nuance of the field, and one of the things we talk

29
00:02:23,280 --> 00:02:29,040
about here a lot at AME is approachability, and that approachability is something that we've

30
00:02:29,040 --> 00:02:35,280
really been inspired by Rich from. Speaking of pioneers, the guest I'd like to invite up with

31
00:02:35,280 --> 00:02:40,080
Rich has trailblazed multiple fields. Among his list of pioneering accomplishments are

32
00:02:40,080 --> 00:02:46,480
co-founding id software, and his leading work there in computer graphics and computer gaming created

33
00:02:46,480 --> 00:02:51,600
an entire genre of gaming. Since then, John's also known for his work in rocketry,

34
00:02:51,600 --> 00:02:57,600
and ushering in a modern era of virtual reality with Oculus Rift. So we're so excited to have

35
00:02:57,600 --> 00:03:03,600
them both here at AME HQ, and everyone please join me in welcoming John Carmack and Rich Sutton to the

36
00:03:03,600 --> 00:03:25,760
stage. Thank you so much, Cam. It's a delight to speak to you all this afternoon. Today I am

37
00:03:25,760 --> 00:03:31,840
pleased to announce the formation of a partnership between John Carmack and myself to work directly

38
00:03:31,840 --> 00:03:36,000
towards the challenge of understanding and creating an artificial general intelligence

39
00:03:37,040 --> 00:03:41,760
based on reinforcement learning and neural networks. The partnership will be embodied

40
00:03:41,760 --> 00:03:48,320
within Keen Technologies, which is a startup that John created about a year ago. And as of today,

41
00:03:48,320 --> 00:03:53,680
I am an employee of that company. Of course, I will continue as Professor at the University of Alberta,

42
00:03:53,680 --> 00:04:00,080
and as the Chief Scientific Advisor at AME, the Alberta Machine Intelligence Institute that you're

43
00:04:01,040 --> 00:04:08,160
all here at. So I'm very excited to be partnering with John. John is the world's preeminent developer

44
00:04:08,160 --> 00:04:13,680
of complex high-performance real-time systems. His skill and brilliance was first demonstrated

45
00:04:14,320 --> 00:04:20,880
in technical innovations in 2D and 3D graphics in computer games such as Doom and Quake. His

46
00:04:20,880 --> 00:04:27,920
engineering skills were honed on rockets at Armadillo Aerospace and by his work on immersive

47
00:04:27,920 --> 00:04:36,240
virtual reality at Oculus. Roughly five years ago, John became, you know, he turned to the

48
00:04:36,240 --> 00:04:43,120
challenge of artificial general intelligence and that eventually led to Keen. Now, when I first

49
00:04:43,120 --> 00:04:49,840
learned about John and Keen earlier this year, I was struck by how aligned we were, I was with him,

50
00:04:50,480 --> 00:04:56,960
despite, you know, my having done this all my life and John just turning to it a few years ago,

51
00:04:57,920 --> 00:05:02,960
there are many aspects to the way we are so strongly aligned, but let me identify just four of them.

52
00:05:03,600 --> 00:05:08,960
We both strongly felt that the field of artificial general intelligence was dominated by

53
00:05:08,960 --> 00:05:14,880
too narrow a set of ideas. This was groupthink and this groupthink was to be avoided. We both

54
00:05:15,520 --> 00:05:22,400
strongly felt that trying to make money too soon was like an off-ramp on the road to AGI,

55
00:05:22,400 --> 00:05:27,760
and we need to keep our eyes on the long-term prize of full AGI, eyes on the prize.

56
00:05:29,360 --> 00:05:35,440
And number three, artificial general intelligence is not too complex for one person to understand

57
00:05:35,440 --> 00:05:43,120
the principles of it or even to write the code for it. That's that as an important part of our

58
00:05:43,120 --> 00:05:52,320
alignment. And finally, 2030 is a good target to have a success, to have a prototype AGI,

59
00:05:52,960 --> 00:06:00,480
to show signs of life. We're a toddler, as John likes to say. So John and I are really well aligned,

60
00:06:01,360 --> 00:06:10,960
and, but there's a sort of an elephant in the room and it's become clear. Keen is a small team,

61
00:06:10,960 --> 00:06:21,360
the entire technical team of Keen Technologies is here today, and it's John and me and Gloria and

62
00:06:21,360 --> 00:06:27,920
Lucas, if you could stand up, and also Joseph, who's we expect to join us by the end of the year,

63
00:06:28,880 --> 00:06:37,840
and now they're all these Keen people are now part of our community. And could you all give

64
00:06:37,840 --> 00:06:56,400
them a good Alberta applause? Thank you for that. So the elephant in the room is it this is a small

65
00:06:56,400 --> 00:07:05,680
team and other companies have thousands of technical staff and are spending billions of dollars on

66
00:07:05,680 --> 00:07:11,040
AGI. It's truly audacious of us to think that we can make a contribution.

67
00:07:13,920 --> 00:07:21,440
But there are actually it's audacious of us to think we can compete with them. And we think we

68
00:07:21,440 --> 00:07:27,120
can or at least we can make a contribution. There's much to say about why that might be

69
00:07:27,120 --> 00:07:32,160
reasonable to think and I'm going to refrain from trying to explain it. But I will only remind you

70
00:07:32,160 --> 00:07:38,960
of what Margaret Mead said. She said, never doubt that a small group of thoughtful committed people

71
00:07:38,960 --> 00:07:43,200
can change the world. Indeed, it is the only thing that ever has.

72
00:07:45,440 --> 00:07:49,760
Now I'd like to turn the floor over to John to say a few words. The legendary John Karnack.

73
00:07:49,760 --> 00:08:04,320
So a couple months ago, I got an email from Richard Sutton. I'm like, well, this is cool.

74
00:08:04,320 --> 00:08:10,480
So I am relatively new to the artificial intelligence field. And when I started kind of my larval

75
00:08:10,480 --> 00:08:15,440
phases, I like to call it where I just inhale all of the relevant information in kind of the first

76
00:08:15,440 --> 00:08:21,200
year. One of the very important references for me was Richard's book on reinforcement learning.

77
00:08:21,200 --> 00:08:26,160
And in fact, I later went through the first half of it again with my son doing exercises

78
00:08:26,160 --> 00:08:31,840
just a couple years ago. And of course, I reference the bitter lesson all the time as one of the

79
00:08:31,840 --> 00:08:36,240
deep fundamental insights to the kind of broader effort of everything that goes on here.

80
00:08:36,800 --> 00:08:42,800
So I'm Richard was trying to figure out I'm kind of like how he should be pursuing his efforts

81
00:08:42,800 --> 00:08:47,360
towards research across different options in commercial, academic and nonprofit.

82
00:08:47,360 --> 00:08:52,880
And I wanted to be super helpful. I hooked him up with a few other people and I tried to kind of

83
00:08:52,880 --> 00:08:58,480
give him whatever help that I could. But I did very tentatively kind of broach the subject that,

84
00:08:58,480 --> 00:09:02,800
all right, you're the godfather of reinforcement learning, you can write your own ticket anywhere

85
00:09:02,800 --> 00:09:08,480
you want to go work. But there are some downsides to especially larger established organizations.

86
00:09:09,040 --> 00:09:14,960
And you may get access to lots of resources and still have the freedom to do whatever you want.

87
00:09:14,960 --> 00:09:22,000
But there are problems with culture and direction, kind of strategic direction in other areas.

88
00:09:22,000 --> 00:09:28,240
It's why as fired as I was when OpenAI tried to recruit me, I respect all that they do,

89
00:09:28,240 --> 00:09:32,640
but they've got their plan, they've got their directions, it's not really what I'm doing.

90
00:09:33,360 --> 00:09:39,200
And there began a little process of kind of feeling out, well, exactly how do you feel about

91
00:09:39,200 --> 00:09:43,760
certain of these directions? Like, what are the odds of technologies going this way? How

92
00:09:43,760 --> 00:09:50,240
important is this? How important is that? And it turned out that we really did have a remarkable

93
00:09:50,240 --> 00:09:55,280
amount of overlap in what we think is possible, what we think the remaining challenges are.

94
00:09:56,000 --> 00:10:03,280
And to be clear, nobody has line of sight on the solution to this today, but we feel,

95
00:10:03,280 --> 00:10:10,000
and I am a number of other people, that it's not that much left. There are things we don't know,

96
00:10:10,000 --> 00:10:14,800
we don't know how to get there, but they all feel like the same scope of things that mattered in

97
00:10:14,800 --> 00:10:19,920
this last decade. They're relatively simple things in the way that you set up your architectures,

98
00:10:19,920 --> 00:10:25,200
the way you do your training, the way you query it in different ways. And I certainly think that

99
00:10:25,280 --> 00:10:30,080
in 30 years, when the textbook of artificial general intelligence is written, it's going

100
00:10:30,080 --> 00:10:35,040
to get digested down to a chapter that people are going to be able to understand. We just need

101
00:10:35,040 --> 00:10:42,480
to figure out what those sort of core ideas are. So it was really still to my great surprise,

102
00:10:42,480 --> 00:10:47,840
but immense pleasure, that we have decided to go ahead and work on this together. So

103
00:10:47,840 --> 00:10:53,920
Richard is working at Keen Technologies now, he's in our workplace, and we spent all day today

104
00:10:53,920 --> 00:10:58,640
basically talking about the plan of research, how we want to start figuring out what we're going

105
00:10:58,640 --> 00:11:04,640
to be doing. So this is incredibly exciting for me. And would you like to say anything else Richard?

106
00:11:15,600 --> 00:11:19,600
Thank you, John. I think now we'd like to take questions from the media.

107
00:11:24,400 --> 00:11:31,120
So we're going to open up the floor for any media questions. For the media in the room,

108
00:11:31,120 --> 00:11:37,360
if you have a question, please come over here. And for media on the line. So operator, please open

109
00:11:37,360 --> 00:11:53,040
up the line for the first question. Oh, there's no media on the phone.

110
00:11:53,680 --> 00:12:03,040
Any media questions from the room? Oh, hi, I'm Renali Unchin with CBC Edmonton.

111
00:12:03,040 --> 00:12:07,680
So following this partnership, I'm wondering if you could talk about some of the tangible steps

112
00:12:07,680 --> 00:12:15,120
that are going to be taken to kind of develop AGI more. So one of the big points that we do have

113
00:12:15,120 --> 00:12:20,480
alignment on is that there's not a near term offer answer where there's not going to be a chat

114
00:12:20,480 --> 00:12:26,320
gbt like deliverable that goes out and allows the world where there are fundamental research

115
00:12:26,320 --> 00:12:32,240
questions that need to be answered. And this is me learning how to be a researcher and hopefully

116
00:12:32,240 --> 00:12:39,120
learning from Richard a lot about that process. But we have internal projects and angles of attack

117
00:12:39,120 --> 00:12:45,040
on things, but they probably will not have a lot of publicly visible effects for likely years.

118
00:12:45,360 --> 00:12:51,520
We are fairly aligned on this sort of we are seven ish six seven eight years out from something

119
00:12:51,520 --> 00:12:59,360
really big and important being publicly visible. Anything? That's perfect. Next question.

120
00:13:00,880 --> 00:13:04,560
Yeah, I have a follow up. So for the average person, obviously, there was a lot of terminology

121
00:13:04,560 --> 00:13:09,760
thrown around. Why should the average person care about this if you can describe in the most simple

122
00:13:09,760 --> 00:13:14,560
terms? Well, I would say the average person probably shouldn't care a whole lot about this,

123
00:13:14,640 --> 00:13:19,200
you know, I mean, this is something this is inside baseball work for people that are

124
00:13:19,200 --> 00:13:24,000
in the field. I am, you know, a lot of people are going to be geeking out about this where,

125
00:13:24,000 --> 00:13:29,520
you know, rich is a big, big name, big percentage, and I'm a big name in a very different field.

126
00:13:29,520 --> 00:13:33,840
And we're kind of coming together. And I do think there's synergistic benefits for that.

127
00:13:33,840 --> 00:13:38,800
And it's kind of exciting just as a, you know, two great taste, taste great together sort of

128
00:13:38,800 --> 00:13:44,800
mix things up. I am between the, you know, different backgrounds, but I know, I mean,

129
00:13:44,800 --> 00:13:49,040
I don't want us to try to make this out to be something that the man in the street should

130
00:13:49,040 --> 00:13:54,880
actually care about. It may yet lead to one of the most important things in the world or in history,

131
00:13:54,880 --> 00:14:00,480
but I there's very little guarantees about any of that. And that's the benefit of the way we're

132
00:14:00,480 --> 00:14:05,200
financed in the way we're structured right now. We don't have to have a rush to a product. We don't

133
00:14:05,200 --> 00:14:10,160
have to have a rush to make sure that there's an investor return in a very short amount of time.

134
00:14:10,160 --> 00:14:14,000
We can concentrate on just trying to answer these critical important questions.

135
00:14:15,120 --> 00:14:19,280
Okay. Cool. Great. Any other questions?

136
00:14:22,480 --> 00:14:30,080
Fantastic. That concludes the Q&A portion of the program. Oh, yes, Rich. If we're done,

137
00:14:30,080 --> 00:14:38,400
I have a few things I want to say. Please. Before we close, I want to thank Amy and the

138
00:14:38,400 --> 00:14:42,800
University of Alberta for their help today, particularly Stephanie Enders and Laura Carter

139
00:14:42,800 --> 00:14:50,560
and Cam and Linda Vang, express my appreciation for the entire Edmonton and Alberta community

140
00:14:50,560 --> 00:14:56,000
for all they do in creating the rich intellectual environment in which I and this part of Keen

141
00:14:56,000 --> 00:15:02,160
can thrive. The community has supported fundamental research in AI for 30 years

142
00:15:03,360 --> 00:15:09,760
before I came here. And this is bearing fruit now in Amy and in industry and startup companies and

143
00:15:09,760 --> 00:15:15,200
more fundamental research results. Please join me in a huge round of applause for Amy and for

144
00:15:15,200 --> 00:15:30,800
yourselves, the entire AI community. And with that, I want to turn the floor over to someone else.

145
00:15:32,960 --> 00:15:38,640
Cam, there you are. All right. Well, thanks everyone. How about one big round of applause

146
00:15:38,720 --> 00:15:51,040
for Rich and John again. So thank you everyone for joining us today. That concludes the program

147
00:15:51,040 --> 00:15:56,160
for the announcement. We're going to reset the room for the fireside chat. So if everyone can do

148
00:15:56,160 --> 00:16:01,440
us a favor, I know this is a pain, but we need you to all leave the room, continue the conversation.

149
00:16:01,440 --> 00:16:07,440
Everybody can have their favorite conversation about what took place here today. Head over to

150
00:16:07,440 --> 00:16:11,920
the co-working space in the cafe. We have food and drinks and caffeine to really amp you up for

151
00:16:11,920 --> 00:16:17,600
this next portion. If you've RSVP'd for the fireside chat, we'll we'll readmit you shortly.

152
00:16:17,600 --> 00:16:22,320
So please do us a favor, move over there to our beautiful space and we'll see you back here shortly.

153
00:16:22,320 --> 00:16:35,600
Thanks. All right. Okay, we're going. Welcome to the second half of the show. If you can do us a

154
00:16:35,600 --> 00:16:41,120
favor, if you're on the edge of the roll, please squeeze into the middle. That will help as stragglers

155
00:16:41,120 --> 00:16:46,080
come in, not doing this, excuse me, pardon me, excuse me, pardon me, and distracting the speakers,

156
00:16:46,080 --> 00:16:51,280
so or just me. So do me a favor, squeeze into the middle, help us out. Thank you very much.

157
00:16:52,000 --> 00:16:58,720
Get close, get to know your neighbor. Welcome back for those who missed the earlier part of this.

158
00:16:58,720 --> 00:17:04,880
I'm Cam Linky, I'm the CEO here at Amy. We're excited to welcome back to the second part of our

159
00:17:04,880 --> 00:17:12,000
doubleheader with a special fireside chat. So our host for the night is well known for his hate of

160
00:17:12,000 --> 00:17:19,200
love of games, love of games. Yes, loves games. An Amy fellow, Canada CIFAR AI chair and full

161
00:17:19,200 --> 00:17:24,880
professor at the University of Alberta. Mike is best known for his work in poker, most notably

162
00:17:24,880 --> 00:17:32,080
solving the game of heads up, no limit, Texas hold them in 2015. And for deep stack in 2016,

163
00:17:32,080 --> 00:17:37,280
the first AI to be human professionals at heads up, no limit, Texas hold them. I think I made an

164
00:17:37,280 --> 00:17:44,320
error in there that Mike's going to correct. He's glaring at me. We're so excited for Mike to be our

165
00:17:44,320 --> 00:17:49,280
host and to host our special guests tonight. And so I don't make any more errors. Please welcome

166
00:17:49,280 --> 00:17:57,200
Mike Bowling to the stage. Thank you very much, Cam. I'm sorry, I do have to correct you. So we

167
00:17:57,280 --> 00:18:02,960
did essentially solve heads up limit. And we beat the first to be professional players,

168
00:18:02,960 --> 00:18:08,480
heads up no limit. Okay, but you're not here for that. I get to have the great honor of introducing

169
00:18:08,480 --> 00:18:13,200
John Carmack. Many people have already said wonderful words about him, but maybe you weren't

170
00:18:13,200 --> 00:18:20,480
here for that. I'll just say that he's a pretty big deal, maybe a BFD, if you will. But I'll just say

171
00:18:20,480 --> 00:18:28,560
my first connection to John was probably my first memory of seeing something really innovative

172
00:18:28,560 --> 00:18:32,560
was thanks to John. Because if you're really young, you can't see innovation. Everything is new.

173
00:18:32,560 --> 00:18:37,680
So you didn't know what came before. So you don't know this change things. And when I was 17, I went

174
00:18:37,680 --> 00:18:41,920
to a gaming convention and they're sitting was a rows of computers. This wasn't even a video game

175
00:18:41,920 --> 00:18:46,320
convention. It was board games and real playing games. And they're sitting was a rows of computers

176
00:18:46,320 --> 00:18:52,320
with Wolfenstein 3D running on them in the summer of 1992. And my mind was blown. So thank you,

177
00:18:52,320 --> 00:19:03,840
John Carmack, and welcome to stage. Welcome, John. Thank you.

178
00:19:03,840 --> 00:19:13,360
Oh, I'm supposed to push the button.

179
00:19:15,280 --> 00:19:21,760
Okay. Yeah, when I was asked to do this, I was trying to think through. I felt like I had a lot

180
00:19:21,760 --> 00:19:26,880
of pressure on me to ask you really insightful questions for the audience. But I decided I'm

181
00:19:26,880 --> 00:19:30,640
going to kind of almost ignore the audience. I'm just going to ask you questions. I feel like I

182
00:19:30,640 --> 00:19:35,360
want to ask you totally selfishly. And I'm going to hope because I have a love of games,

183
00:19:36,480 --> 00:19:40,960
you know, I have a love of innovation, and I have a love for AI that maybe these are the

184
00:19:40,960 --> 00:19:46,240
questions that the audience wants to hear too. But so I want to walk you back to the beginnings

185
00:19:46,240 --> 00:19:53,520
of id and Wolfenstein 3D. And maybe you can say some words about how did you even end up

186
00:19:54,080 --> 00:19:57,280
at that place? And then I want to turn around and talk a bit more about innovation from there.

187
00:19:58,240 --> 00:20:03,840
Yeah, so it did. Games were super important for me in my childhood. You know, I love the

188
00:20:03,840 --> 00:20:09,840
arcade games. I love the early 8-bit video games and the tabletop games, all of that. And so it

189
00:20:09,840 --> 00:20:15,280
did seem obvious to me that this love of computers that I had that the best way to express it was

190
00:20:15,280 --> 00:20:20,320
through games. Now, it is important to kind of differentiate a little bit there where there

191
00:20:20,320 --> 00:20:25,200
are a lot of people that go into gaming and they learn about computers so they can make games.

192
00:20:25,280 --> 00:20:31,120
And it was a little more fortuitous for me because I had this intrinsic love of computers as well.

193
00:20:31,120 --> 00:20:36,560
And I probably could have been happy and found interest in doing more mundane things. But the

194
00:20:36,560 --> 00:20:41,520
games were there. They were the obvious place and it turned out to work, you know, really well for

195
00:20:41,520 --> 00:20:45,840
me. And so I had been, you know, for the first time I could do anything on a computer. I was

196
00:20:45,840 --> 00:20:50,080
trying to write games. I was trying to, you know, at first mimic some of the other games that I

197
00:20:50,080 --> 00:20:55,440
would see. The first, the early text adventure games, writing my dungeon crawlers, you know,

198
00:20:55,440 --> 00:21:01,040
kind of inspired by wizardry and Ultima and the early things like that. But as I got to be a teenager

199
00:21:01,040 --> 00:21:06,000
and I built the assembly language skills and started figuring out how to do the sort of the

200
00:21:06,000 --> 00:21:12,400
real world things and I had a talent for it. I got good at it and I had decided that I wanted to

201
00:21:12,400 --> 00:21:17,280
actually try to make a living making games. And there was, you know, there was a lean year there

202
00:21:17,280 --> 00:21:22,880
of barely scraping by doing contract programming work before I accepted the position to come down

203
00:21:22,880 --> 00:21:29,120
and work at soft disk publishing where I met the other founders of id software. And we started out

204
00:21:29,120 --> 00:21:34,320
making our initial side scroller games, which was Commander Keen, which I have kind of harkened

205
00:21:34,320 --> 00:21:40,720
back to with the name of my latest company with Keen Technologies. But then we, you know, we really

206
00:21:40,720 --> 00:21:47,200
broke out and kind of made our name with the 3D games. And I always did kind of look at that as

207
00:21:47,520 --> 00:21:52,800
it wasn't the games were the same things that you could do in 2D, but the change in perspective

208
00:21:52,800 --> 00:22:00,000
of going to 3D, it made a qualitative difference, even if sort of symbolically it was the same game,

209
00:22:00,000 --> 00:22:04,880
but it made a great difference in the impact it had on people. Tell me more about that that jump

210
00:22:04,880 --> 00:22:08,720
to 3D. Was that was that intentional where you like I could think of so many ways this could

211
00:22:08,720 --> 00:22:12,080
have gone where you could have just been like that seems like a hard challenging computer's

212
00:22:12,080 --> 00:22:16,880
problem. And so I'm going to solve that versus feeling like this is the future of games. So we

213
00:22:16,880 --> 00:22:20,800
need to innovate in that front versus not even realizing maybe that you were being innovative.

214
00:22:20,800 --> 00:22:26,240
Like how did that work? No, I knew I was heading for that from the early 80s. I am, you know, as a

215
00:22:26,240 --> 00:22:31,360
you know, even as a young teenager, you would see the representations of 3D graphics or in the movie

216
00:22:31,360 --> 00:22:37,520
and you think about you see Tron or you see 3D animated logos. I can remember making a little

217
00:22:37,520 --> 00:22:43,360
wireframe MTV logo spin around on my Apple to kind of figuring out these basics and looking back,

218
00:22:43,360 --> 00:22:46,800
like I didn't know how to do clipping at the time so nothing can get close to the edge of the

219
00:22:46,880 --> 00:22:52,560
screen. It just has to stay in the center and rotate around. But that idea of wanting to be

220
00:22:52,560 --> 00:22:57,760
inside the video game, I mean, everybody that's a gamer kind of got that sense of you want,

221
00:22:57,760 --> 00:23:02,000
you look at the game, you play the game, you appreciate it, but how amazing would it be to

222
00:23:02,000 --> 00:23:06,960
put yourself inside it and then, you know, see the enemies coming at you rather than looking

223
00:23:06,960 --> 00:23:13,600
down at it as that 16 by 16 block of pixels. And that was magical. And you could take exactly the

224
00:23:13,600 --> 00:23:19,760
same game and put the user inside it and that really followed on even more with virtual reality

225
00:23:19,760 --> 00:23:24,320
later on where getting the sense of presence where you kind of believe that you're in this

226
00:23:24,320 --> 00:23:30,400
other environment. So that was always a big issue for me, but I do, you know, it is important to

227
00:23:30,400 --> 00:23:35,520
say that that's kind of the obvious visible thing that stands out that there was a time period where

228
00:23:35,520 --> 00:23:41,280
there was nothing else like that, but there are a thousand good decisions that go into making a game

229
00:23:41,280 --> 00:23:47,920
and it was never about just the technology. It was about doing those thousand things right.

230
00:23:47,920 --> 00:23:53,600
All the subtleties about how you feel, how you move, how the guns react, what the sound effect is,

231
00:23:53,600 --> 00:23:58,800
there's 10 things you layer on top of every action that happens in a game to wind up giving

232
00:23:58,800 --> 00:24:03,440
it this really good feel. And most of them will be things that people don't even notice. I mean,

233
00:24:03,440 --> 00:24:09,040
everybody points to Oh my God, this 3d black magic that was going on that that left people,

234
00:24:09,600 --> 00:24:13,040
you know, just wondering how they could compete with something like that until

235
00:24:13,040 --> 00:24:18,480
you know, engine technologies got out there. But there were plenty of other now forgotten games

236
00:24:18,480 --> 00:24:23,600
that wound up having flashy 3d graphics, but people don't remember them because they didn't

237
00:24:23,600 --> 00:24:28,160
do all the other things, right? I mean, I definitely remember the just smoothness of the

238
00:24:28,160 --> 00:24:32,000
original it games were just, I think just blew people away. Like it felt like you could play them

239
00:24:32,000 --> 00:24:36,640
as opposed to being pulled out of the immersion from the clunkiness. But that's also innovative

240
00:24:36,640 --> 00:24:41,520
technology too, right? That's some of the things that were actually subtle that were really important

241
00:24:41,520 --> 00:24:46,960
to me. And nobody actually called these out. But in terms of the graphics, the way they were

242
00:24:46,960 --> 00:24:53,360
rendered, there were a lot of 3d games that had this kind of non solid feel to it, where there

243
00:24:53,360 --> 00:24:58,720
are these subtle things that happen to do with pixel centers and avoiding cracks between polygons.

244
00:24:58,720 --> 00:25:03,360
And you can still make great games for that. Like the entire PlayStation one, I am, you know,

245
00:25:03,360 --> 00:25:09,440
set of titles was all done with this integer snapped affine interpolated rendering technology.

246
00:25:09,440 --> 00:25:14,640
You can do good things with that. But I always took pride in the solidity, the kind of sense that

247
00:25:14,640 --> 00:25:19,760
everything was really rock solid there, where some games felt fragile, like, you know, you bump the

248
00:25:19,760 --> 00:25:23,840
wrong way and you're going to slide through the wall, get caught and see a flickering mess of

249
00:25:23,840 --> 00:25:29,520
stuff. And for the most part, I, we took great pains to make that not happen in ours.

250
00:25:29,520 --> 00:25:32,960
That's awesome. Is there anything else that like you think like this was the innovative part,

251
00:25:33,040 --> 00:25:37,280
but in the, you know, decades later, no one remembers that being the innovation?

252
00:25:37,280 --> 00:25:43,120
Well, what was great is we had this period of like this five year period during the development

253
00:25:43,120 --> 00:25:49,440
going from Wolfenstein 3d to doom to quake. And there were so many things that wound up setting

254
00:25:49,440 --> 00:25:56,080
the tone for gaming for the following 30 something years. And things like the multiplayer gaming,

255
00:25:56,080 --> 00:26:01,600
the modding, these were not the flashy 3d graphics technology. But they, you know, they were things

256
00:26:01,600 --> 00:26:06,800
that were super valuable. And it was happy to see people, you know, follow up on it, even little

257
00:26:06,800 --> 00:26:11,600
things like having a console, you know, having the, you know, the override ability for the different

258
00:26:11,600 --> 00:26:16,720
data sets. There were a lot of decisions like that, that, you know, that worked out pretty well.

259
00:26:16,720 --> 00:26:22,000
And I could imagine a world where 3d graphics was inevitable. It was being done on higher end

260
00:26:22,000 --> 00:26:28,880
systems, offline rendering. But it is possible to imagine a contingent set of history where

261
00:26:28,880 --> 00:26:34,400
you didn't wind up with this action oriented things because the dominant vision was you do sims,

262
00:26:34,400 --> 00:26:40,880
you do flight sims, driving sims, tank sims, you know, destroyer sims. And this idea of this run

263
00:26:40,880 --> 00:26:46,240
and gun really fast paced action, twitch reaction that, you know, whipping around the mouse directly

264
00:26:46,240 --> 00:26:51,920
at inhuman speeds and all of these things, that might not have happened without id software in

265
00:26:51,920 --> 00:26:57,680
the early days. I wonder just like how much that affected the development even get to where we are

266
00:26:57,680 --> 00:27:03,440
today in terms of would, you know, we've seen gaming drive technology a lot, right? So would we

267
00:27:03,440 --> 00:27:08,160
have GPUs today? Would we be sitting on top of AI on top of Dean learning sitting on top of GPUs

268
00:27:08,160 --> 00:27:12,400
today? If we didn't have those initial games saying we could have action oriented, broader,

269
00:27:12,400 --> 00:27:17,120
open games. Do you think that's a possibility? So the GPU side of laddering you, but yeah,

270
00:27:17,120 --> 00:27:23,120
I mean, I do, you know, it's one of those things where I smile and I'll talk about it a little bit.

271
00:27:24,080 --> 00:27:30,800
I do take some satisfaction in the fact that the GPUs were largely built to play the Quake series

272
00:27:30,800 --> 00:27:37,040
of games at the beginning. And then you had this great insight from like Jensen at Nvidia saying

273
00:27:37,040 --> 00:27:41,840
it's like, well, all these pixel processing things that we're doing, we can do other things with them.

274
00:27:41,840 --> 00:27:47,600
And they had a lot of foresight to do the long game investment in CUDA and give us the kind of

275
00:27:47,680 --> 00:27:53,680
generalized processing. Because people were doing general processing before that in this horrible

276
00:27:53,680 --> 00:27:58,800
way. You'd paint, you'd encode your values into pixels, you draw a giant triangle that covers

277
00:27:58,800 --> 00:28:04,400
the screen to do an array processing action on there. And it was, you know, it was effective a

278
00:28:04,400 --> 00:28:10,960
little bit, but it was awful. But the evolution of GPUs to where they are today as this quite

279
00:28:10,960 --> 00:28:16,080
general purpose device that does underpin all of the modern era of artificial intelligence.

280
00:28:16,720 --> 00:28:22,000
It's nice to, even if I wound up not working in artificial intelligence, it would be something

281
00:28:22,000 --> 00:28:26,880
that I'm proud of that I at least contributed at some point to that evolution. I have one more

282
00:28:26,880 --> 00:28:31,360
question of curiosity from this time that relates to Wolfenstein itself. So I played

283
00:28:31,360 --> 00:28:37,520
the original Wolfenstein games and Beyond Wolfenstein and love them. How did it, how did this,

284
00:28:37,520 --> 00:28:42,080
like, did you have that IP in mind and then we're designing a game for it? Did you, did you,

285
00:28:42,720 --> 00:28:47,120
you know, have the game first? Like, what was the order of events and, and how did that come

286
00:28:47,120 --> 00:28:52,320
back? So all of us at Id, I am, you know, John, Tom, Jay and I, I, we were all Apple II background

287
00:28:52,320 --> 00:28:58,240
people. So we all had this background with the original Escape from Castle Wolfenstein. And it

288
00:28:58,240 --> 00:29:02,480
was a game that had these, it was more what you today would think of as a stealth game. You would

289
00:29:02,480 --> 00:29:08,000
sneak around, you'd wear guards uniforms, you'd drag the bodies out of the way. But it still did

290
00:29:08,000 --> 00:29:14,640
have that sense of shooting Nazis. And I, we had originally thought we were going to do some alien

291
00:29:14,640 --> 00:29:19,520
based game, you're calling it it's green and pissed, just kind of a generic alien shooter. I, this

292
00:29:19,520 --> 00:29:25,520
was following up off of our kind of fantasy themed Catacombs 3D. But when the idea came up that's

293
00:29:25,520 --> 00:29:30,880
like, well, what if we did Wolfenstein 3D? I am, you know, all the good memories for us. It was

294
00:29:30,880 --> 00:29:36,640
playing the nostalgia card even for us at that time, some 15 years after the that was initially

295
00:29:36,640 --> 00:29:44,560
released. And back at that time, I look back and kind of cringe at, like our business practices at

296
00:29:44,560 --> 00:29:50,160
the time, we did not sort this out very well. We kind of just charged ahead and looked around a

297
00:29:50,160 --> 00:29:55,920
little bit thinking, well, maybe the rights are clear, these companies seem bankrupt. We eventually

298
00:29:55,920 --> 00:30:02,080
ran into Silas Warner, the original author at an Apple II convention. And he was, you know, he

299
00:30:02,080 --> 00:30:06,160
was delighted to see Wolfenstein something. Of course, he didn't own the rights. His blessing

300
00:30:06,160 --> 00:30:11,760
didn't actually mean anything from a legal term. But to us, it felt good to have the original creator

301
00:30:11,760 --> 00:30:17,120
kind of bless our effort and what we were doing there. And in the end, we got out of it unscathed.

302
00:30:17,120 --> 00:30:24,480
But that was luck involved, I think. Awesome. Also, in the early times, I feel like it formed

303
00:30:24,480 --> 00:30:28,400
another aspect of your career, which is your proponent for open source software,

304
00:30:29,200 --> 00:30:32,800
both in distribution models of how software is distributed. We're pretty original in those

305
00:30:32,800 --> 00:30:37,760
early games. I'm wondering what, well, yeah, what's your trajectory through that? What's your

306
00:30:37,760 --> 00:30:43,840
thoughts about open source, say, then and now, and what connections are between them?

307
00:30:43,840 --> 00:30:48,640
So probably the most formative book of my teenage years was Stephen Levy's book,

308
00:30:48,640 --> 00:30:54,880
Hackers, Heroes of the Computer Revolution. And, you know, I read it dog-eared and it had

309
00:30:54,880 --> 00:31:00,160
these major themes about the hacker ethic and the kind of sharing of information and

310
00:31:00,160 --> 00:31:05,680
communal use of code in different ways. And this was before, you know, open source became what it

311
00:31:05,680 --> 00:31:10,960
is today or even before the Free Software Foundation kind of had their mission. And I, you know,

312
00:31:10,960 --> 00:31:17,760
coined the term. So that was, you know, that was pretty deeply in me early on. And it was fortunate

313
00:31:17,760 --> 00:31:24,400
that John Romero, my kind of co-programmer, had similar feelings about it, that this idea that

314
00:31:24,400 --> 00:31:29,520
it's just amazingly cool to be able to share the program to make it possible, where we had these

315
00:31:29,520 --> 00:31:33,600
memories of hacking the games, like you'd get Ultima, you'd get your Sector Editor out, you'd

316
00:31:33,600 --> 00:31:41,520
find out, oh, that's my gold, I want 9999 in there. And I always, I mean, I remember fervently wishing

317
00:31:41,520 --> 00:31:45,840
that I could look at the source code for these things to be able to go, these games that I

318
00:31:45,840 --> 00:31:52,080
adored that I spent a lot of time on, I wanted to see exactly how they were made. And to get into

319
00:31:52,080 --> 00:31:58,320
a position as a small private company, you know, where we owned all of our own IP, the ability to

320
00:31:58,320 --> 00:32:03,680
kind of make that earlier childhood wish that I had had come true for a whole new generation of

321
00:32:03,680 --> 00:32:10,160
programmers was, you know, it was very motivating for me. And it was a drawn out process inside the

322
00:32:10,160 --> 00:32:14,720
company, because there's a huge divide generally between the technical and the creative people

323
00:32:14,720 --> 00:32:20,720
here, where, I mean, it's not a judgmental side of things, but just the technical people tend to

324
00:32:20,720 --> 00:32:27,120
get this sharing more than most of the artists and designers do, where there's a lot more worry

325
00:32:27,120 --> 00:32:33,920
about chain of credit and, you know, where the work builds upon other people's work. So it was

326
00:32:33,920 --> 00:32:40,480
not fully understood by everyone in the company, but, you know, a little bit of it was, I was able

327
00:32:40,480 --> 00:32:44,560
to throw a little bit of my weight around in my position. And I know there was a little bit of

328
00:32:44,560 --> 00:32:48,960
hard feelings for a while that I did something that there was thinking was bad for the business.

329
00:32:48,960 --> 00:32:52,720
Is this related to the leak of the Quake source code? Or do you think you have a different,

330
00:32:52,880 --> 00:32:58,160
no, just the open source in general, where there was this point early on where we had released

331
00:32:58,160 --> 00:33:04,320
the tools for Doom and the ability to do all of this. And a product came out called Dezone,

332
00:33:04,320 --> 00:33:10,080
which was a CD-ROM just full of hundreds and hundreds of maps. And the people that did that

333
00:33:10,080 --> 00:33:16,880
actually made more money than we made on Doom 2, because we did not have a great royalty deal at

334
00:33:16,880 --> 00:33:22,400
the time. And, you know, they shoveled that out. And there was some genuine bitterness that that

335
00:33:22,400 --> 00:33:28,080
was possible and that to some point, degree, I had enabled that by sharing the tools. But I did

336
00:33:28,080 --> 00:33:33,760
feel very good that a decade or more goes on, a couple decades now. And the people,

337
00:33:34,800 --> 00:33:40,400
like Kevin Cloud, who is one of the artists on there, had later told me that, no, that really

338
00:33:40,400 --> 00:33:45,920
was all for the best, looking back on it. And we are in this almost unique position where Doom

339
00:33:45,920 --> 00:33:49,360
will never die. As long as there are processors, Doom will run on them.

340
00:33:50,320 --> 00:33:55,040
Right. Do you think there's lessons to take now? And then, like, I feel like the open source

341
00:33:55,040 --> 00:34:00,160
community has played a role in the last 10-year development of AI. What do you see the lessons

342
00:34:00,160 --> 00:34:04,080
that we should learn in thinking about how that connects to the current innovation?

343
00:34:04,080 --> 00:34:09,280
Yeah, I'm still surprised that it doesn't play more of a role in the game industry, where I

344
00:34:11,680 --> 00:34:18,080
I'm genuinely surprised that there is not more like full open source development. You see bits

345
00:34:18,080 --> 00:34:23,600
of it in the Minecraft mod scene where you have projects up on GitHub. But people that are generally

346
00:34:23,600 --> 00:34:29,680
making games, they still feel very protective of their source. And then you had the commercial

347
00:34:29,680 --> 00:34:37,440
companies with Unity coming in and making a very powerful product that gets the job done that was

348
00:34:37,440 --> 00:34:43,200
better than open source alternatives in many cases. I think Epic does a really grand thing by

349
00:34:43,200 --> 00:34:47,040
they have strict licensing terms, but the fact that the source code is all available,

350
00:34:47,040 --> 00:34:52,960
that's half the battle. I am pragmatic. I'm not a purist. I'm not going to license snipe somebody

351
00:34:52,960 --> 00:34:59,200
about what they're using. Just having the source code available for view is a large chunk of the

352
00:34:59,200 --> 00:35:05,360
value. So there's good stuff there. But in the AI space, it's much more open. And I think in the

353
00:35:05,360 --> 00:35:12,320
broader sense of where academia has gone, where comparing, I go back and I do read a lot of papers

354
00:35:12,320 --> 00:35:18,880
from the 90s and stuff. And it's papers without code, without data, unreproducible. There's a whole

355
00:35:18,880 --> 00:35:25,520
lot of things that are just probably are not right. And the path to having the norm, it's still not

356
00:35:25,520 --> 00:35:31,520
fully established and there's still pushback about it. But the world is a far, far better place now

357
00:35:31,520 --> 00:35:37,120
for the openness that we do have on the pace of artificial intelligence and most other science

358
00:35:37,200 --> 00:35:42,880
that I don't even have windows into the fact that code available on GitHub. I ideally data there.

359
00:35:42,880 --> 00:35:47,520
We still have too many cases of data available upon request and there's still norms that need

360
00:35:47,520 --> 00:35:52,080
to be pushed there because you do still have sensitive people. It's like, oh, if I release this,

361
00:35:52,080 --> 00:35:56,400
people will find mistakes. They'll critique my code. They'll, you know, they'll have all of this.

362
00:35:56,400 --> 00:36:01,440
And that's still a problem being worked through. But I have no complaints about the state that we're

363
00:36:01,440 --> 00:36:05,120
at now and the trajectory that we're on. I think it really is one of the great things for the world

364
00:36:05,120 --> 00:36:10,640
today. Cool. I want to have an AI, but before that, let me take your detour through VR for a minute.

365
00:36:12,160 --> 00:36:15,600
Tell me what would, did that just feel like a natural extension for you for games or were

366
00:36:15,600 --> 00:36:20,240
you thinking something bigger? What was, what was motivating you to move? Yeah, so I had actually

367
00:36:20,240 --> 00:36:26,240
tried some VR stuff back in the nineties. We had one of the really old headsets that cost $10,000

368
00:36:26,240 --> 00:36:32,480
and was like 320 by 240 resolution screens and pixels the size of small footballs in your peripheral

369
00:36:32,480 --> 00:36:39,120
vision. And it was clearly not the right time for something there. And it was interesting where

370
00:36:39,120 --> 00:36:43,440
I went and I said, I'm just going to go look at the state of VR because it's been two decades.

371
00:36:43,440 --> 00:36:50,880
Surely it's all fixed by now. And I was really surprised to find that even though technically

372
00:36:50,880 --> 00:36:57,520
we had good screens, good accelerometers, we had all the things that were seem to be necessary to

373
00:36:57,520 --> 00:37:02,640
make this work. But there were still just this handful of government contractors making headsets.

374
00:37:02,640 --> 00:37:08,080
They, there were $50,000 headsets in some cases and they still weren't even all that great.

375
00:37:08,720 --> 00:37:14,320
And that was one of those points where, you know, you can say that opportunity is the difference

376
00:37:14,320 --> 00:37:18,720
between what's possible and what people are actually doing. And it did seem there. It's like

377
00:37:18,720 --> 00:37:24,400
from a technical level, these things were now possible. And the early experiments that I did

378
00:37:24,400 --> 00:37:30,560
there showed that it can also be really quite compelling. Now there's, there's the obvious

379
00:37:30,560 --> 00:37:34,560
play there about, well, you, you make games, it's more immersive. There's the step from

380
00:37:34,560 --> 00:37:39,760
looking at a 2D game to looking at a 3D game to being inside a 3D game. And that is, you know,

381
00:37:39,760 --> 00:37:46,400
absolutely true. But I do think there's the even more powerful case about once you have a virtual

382
00:37:46,400 --> 00:37:53,360
interface, you know, everything that you do on screens can at least in theory be done better

383
00:37:53,360 --> 00:38:00,080
in a more flexible way with a virtual in virtual reality headset. So I do think there is that

384
00:38:00,640 --> 00:38:06,400
many billion dollar value there. You know, I spent eight years involved with it. And,

385
00:38:07,040 --> 00:38:11,120
you know, there's things that I'm very proud of the quest to the quest three being announced

386
00:38:11,120 --> 00:38:17,520
officially and coming out real soon now are great pieces of hardware. And there's an interview that

387
00:38:17,520 --> 00:38:25,040
I did 10 years ago in 2012 or 2013 where I'm saying this is the way I want things to go. I want

388
00:38:25,040 --> 00:38:30,800
this self contained device that has inside out position tracking not cabled anything no external

389
00:38:30,800 --> 00:38:37,760
tracking aids that can run lightweight applications internally all by itself and can connect wirelessly

390
00:38:37,760 --> 00:38:43,840
to PCs to go ahead and have higher performance things. And that turned out just the way I wanted.

391
00:38:43,840 --> 00:38:49,280
But there was a lot of things that I had friction at meta with trying to get the rest of the way.

392
00:38:49,280 --> 00:38:54,240
Right. I was going to I was going to ask you. I started thinking about the transition into AI.

393
00:38:54,240 --> 00:38:58,160
I don't know how much of that was somewhat being disillusioned by the impact that you want to see

394
00:38:58,160 --> 00:39:03,840
in VR. No, it really wasn't. I was still fighting the good fight as much as I could from my position

395
00:39:03,840 --> 00:39:11,680
in VR. But it is funny how my origin story for the AI really does go back to Sam Altman and open AI

396
00:39:11,680 --> 00:39:16,720
trying to recruit me when I knew nothing about the state of AI. And I was very flattered by that,

397
00:39:16,720 --> 00:39:22,240
that they just thought that I you know that my skill set in background could play a useful part

398
00:39:22,240 --> 00:39:26,560
in the company that they were building and putting together. Now, we were talking about that just

399
00:39:26,560 --> 00:39:32,000
before we got on before we got on stage. But had you already started to look at that point that

400
00:39:32,000 --> 00:39:37,680
raised your profile? No, no, just like they went after you as purely you are a brilliant engineer.

401
00:39:37,680 --> 00:39:43,200
You will help get us the rest of the way there. Yeah, the only I had some vague relationship to

402
00:39:43,200 --> 00:39:48,080
it for like there were hand tracking models and eye tracking and face tracking neural nets were used

403
00:39:48,080 --> 00:39:55,600
a little bit in the VR side of things. But I hadn't I I had barely touched it. I had done sort of my

404
00:39:55,600 --> 00:40:00,640
neural nets in C. I just spent a week just kind of writing that by myself. But that was about it.

405
00:40:01,280 --> 00:40:06,080
Yeah. So then so that was so they reached out to you. And then that made you think I should

406
00:40:06,080 --> 00:40:12,160
look into this AI stuff. Yeah, exactly. And then I looked into it. And, you know, and I was very

407
00:40:12,160 --> 00:40:16,480
happy they they helped me kind of get my feet under me. And like, here's roughly what you need to

408
00:40:16,480 --> 00:40:22,560
learn. And when I started looking into it, it, I carefully thought about it. And I reached this

409
00:40:22,560 --> 00:40:27,760
conclusion that this is probably the highest leverage time for someone like me, you know,

410
00:40:27,760 --> 00:40:33,440
an individual engineer in the history of ever, you know, that there are so many important things

411
00:40:33,440 --> 00:40:39,280
that require huge teams of people that require lots of management. But the gap between where we

412
00:40:39,280 --> 00:40:45,040
are now standing on the shoulders of all those giants, and this really transformative thing with

413
00:40:45,040 --> 00:40:50,160
general intelligences, applying to almost everything that we do intellectually in the world,

414
00:40:50,960 --> 00:40:55,760
that feels like this small modest number of things. And when I look back at what

415
00:40:55,760 --> 00:41:00,320
mattered in the last decade, they all feel like things that I would come up with. So

416
00:41:00,960 --> 00:41:07,040
it was exciting. Yeah. So I think it's kind of funny that if you look at what Edmonton's

417
00:41:07,040 --> 00:41:11,760
innovative scenes are, there's one on the side of games, and there's one on the side of AI.

418
00:41:11,760 --> 00:41:18,080
Do you think that that's you've you're traveling a similar set of spaces? Do you think that that's

419
00:41:18,080 --> 00:41:21,120
coincidence? Or do you think that they're related? It's interesting, because of course,

420
00:41:21,120 --> 00:41:26,240
famously, like Demis Hussabis at Deep Mind was also a gaming background person. I'm

421
00:41:27,200 --> 00:41:33,600
Yeah, it's hard to say there are certainly some aspects of being all about virtual worlds and

422
00:41:33,600 --> 00:41:39,200
simulations that plays a little bit into it, but it's a little bit of a stretch. I think it might

423
00:41:39,200 --> 00:41:45,680
be more fundamental that the people that are overjoyed at the technology of games, it's this

424
00:41:45,680 --> 00:41:51,040
general sort of optimism about what you can do with technology. And you've had you've gone through

425
00:41:51,040 --> 00:41:55,920
the feedback cycle of getting this huge reward and joy from solving a wonderful technical

426
00:41:55,920 --> 00:42:01,280
problem, and it causes you to delve deeper into it and push harder on it. And it does, you know,

427
00:42:01,280 --> 00:42:05,200
in some degree give you confidence or maybe even hubris to think that you can make a difference

428
00:42:05,200 --> 00:42:14,240
in some of the other areas. Cool. So this this partnership with Rich, I'm kind of curious.

429
00:42:14,240 --> 00:42:19,040
All disclosures. I was part of, I was also a co-author on the paper of the Alberta Plan with

430
00:42:19,040 --> 00:42:24,000
Rich and Patrick. But I want to ask you, have you read the Alberta Plan? So I hadn't before I

431
00:42:24,000 --> 00:42:28,880
started talking with Rich. So I was familiar with, you know, I had gone through his textbook,

432
00:42:28,880 --> 00:42:35,600
I'm, you know, twice actually, and I loved his bitter lesson paper and I had caught like a couple

433
00:42:35,600 --> 00:42:41,840
of his presentations. But I was several years behind on sort of the research work that he was doing.

434
00:42:41,840 --> 00:42:48,160
And I had it, you know, I had my, my mental model of Richard Sutton had a few things that I had

435
00:42:48,160 --> 00:42:53,040
some concerns about from my view of reinforcement learning from several, you know, several years

436
00:42:53,040 --> 00:42:57,920
back. And when I started talking with him and started going on, like all of his more recent

437
00:42:57,920 --> 00:43:03,520
work, I got most of my concerns there were pretty much elate. And it was kind of surprising how

438
00:43:03,520 --> 00:43:09,040
aligned we were on a lot of things. Okay, if you could pick one thing that you think the Alberta

439
00:43:09,040 --> 00:43:16,160
Plan gets right and one thing that you think the Alberta Plan gets totally wrong. So yeah,

440
00:43:17,120 --> 00:43:24,720
I'm setting you guys up for success. So I have concerns that the monolithic nature of the models

441
00:43:24,720 --> 00:43:30,720
that are talked about there may not capture some important aspects of the way human brains work

442
00:43:30,720 --> 00:43:37,680
as a much more distributed semi-consensus finding system. I mean, we draw these neat boxes about

443
00:43:37,680 --> 00:43:43,200
your state, your policy and implemented with monolithic models. You know, these are,

444
00:43:43,200 --> 00:43:47,840
we have Turing equivalents on a lot of these things. I won't say anything can't get across

445
00:43:47,840 --> 00:43:53,680
the finish line, but I do like to at least gesture in the direction of our one existence

446
00:43:53,680 --> 00:43:59,520
proof of in general intelligence with biology and say that there, there is much less of a central

447
00:43:59,520 --> 00:44:05,440
driving force behind these things that I believe they operate more at these somewhat lower levels.

448
00:44:05,440 --> 00:44:10,800
And I, you know, I don't have this resolved, but that's always a concern for me when something,

449
00:44:10,800 --> 00:44:15,360
I mean, we've, we've all stepped back away from anything smelling of symbolism, but

450
00:44:15,360 --> 00:44:21,440
there's still monolithic tendencies that I think may yet prove to be a little bit of a retarded

451
00:44:21,440 --> 00:44:27,840
for it. But the, you know, the important things about what an AI does is digest its experience

452
00:44:27,840 --> 00:44:32,560
into a state representing, you know, its view of the world and how it predicts things going forward

453
00:44:32,560 --> 00:44:37,440
and how it needs to have motivations both internally and externally imposed. These are

454
00:44:37,440 --> 00:44:42,880
core that, that are completely unaddressed by the, you know, the current, the current sensation

455
00:44:42,880 --> 00:44:47,600
with the large language models, you know, these take a very, very wide context and they throw

456
00:44:47,600 --> 00:44:51,760
something at it and you get an answer out the end. And there's enormous value. I don't want to

457
00:44:51,760 --> 00:44:56,560
take anything away from everything that's being done with that, but that's not how our brains are

458
00:44:56,560 --> 00:45:01,920
working. And I think that there are important things that, that it doesn't, that it doesn't

459
00:45:02,000 --> 00:45:07,040
encompass. And yet every lab in the world is throwing so much of the dominant share of

460
00:45:07,040 --> 00:45:12,640
their resources at that. So being a little bit contrarian, both of us, I think is, is a positive

461
00:45:12,640 --> 00:45:17,760
thing. Yeah, I'm glad you used that word. That seems like a word when I first thought of, of, of

462
00:45:17,760 --> 00:45:21,360
you and Rich, I was like, well, those two things I know you're aligned on being contrarian, but I

463
00:45:21,360 --> 00:45:27,360
don't know if you can be aligned on being contrarian. I'm going to ask you one more question, but I'll,

464
00:45:27,360 --> 00:45:31,200
I'll say that I'm going to open this up to the audience for questions after this last question

465
00:45:31,200 --> 00:45:38,800
and the microphone I believe is over here. So if you want to rush to get in line to ask a question,

466
00:45:38,800 --> 00:45:42,720
I'll let you do that while I ask my last question to John, which is, you know, what's,

467
00:45:42,720 --> 00:45:48,800
what's your future? You talked a little bit about 2030 as being a goal of, we might see some AGI,

468
00:45:48,800 --> 00:45:53,840
but maybe you could describe what, when you say there's a 60, 50 to 60% chance that it happens

469
00:45:53,840 --> 00:46:00,240
by 2030, paint me a picture of what happens, looks like. So it does appear that I, you know,

470
00:46:00,240 --> 00:46:06,240
I'm a person of decade-long efforts where I, there's a spectrum of useful there. There's a lot

471
00:46:06,240 --> 00:46:10,960
of people that have 18-month kind of passions and cycle through and you get more breadth with that,

472
00:46:10,960 --> 00:46:15,680
but I've been a fairly in-depth person, you know, overlapping with, you know, gaming and

473
00:46:15,680 --> 00:46:21,680
rocketry and virtual reality and now AI, and I fully expect this to consume a decade of my life

474
00:46:21,680 --> 00:46:27,920
or more. So I certainly wouldn't give up in less than a decade, and if things are going well,

475
00:46:27,920 --> 00:46:35,040
it'll carry on longer than that. I do intend us to stay intentionally away from anything

476
00:46:35,040 --> 00:46:41,200
smacking a commercial product. I think that it is important. So this is one of the things I do

477
00:46:41,200 --> 00:46:45,200
struggle with a lot though. I mean, there's, there are all of these things. I'm far from having this,

478
00:46:45,200 --> 00:46:51,520
I, you know, surety of direction or anything. One of the problems that I had a lot at META

479
00:46:51,520 --> 00:46:56,480
was a lot of the research I thought was actually not particularly valuable, that it was building

480
00:46:56,480 --> 00:47:00,960
for things, and I wanted everyone to concentrate just on product. You know, with, you have product,

481
00:47:00,960 --> 00:47:05,280
there's a million things you can do, just do those million things, don't look too far ahead,

482
00:47:05,840 --> 00:47:11,440
and yet here I am doing exactly the opposite, and I, you know, I have the arguments with myself,

483
00:47:11,440 --> 00:47:15,760
do I have a legitimate reason why this is different, or should I just be doing some data

484
00:47:15,760 --> 00:47:19,840
generation startup, you know, the, the 10 companies that want me to help them do game

485
00:47:19,840 --> 00:47:25,120
generation with generative AI, there's, you know, there's tens of millions of dollars just to be

486
00:47:25,120 --> 00:47:30,960
had at the drop of a hat to go do something like that, and I can't say with surety that

487
00:47:30,960 --> 00:47:34,960
that's not true, that you don't learn as much along the way building those skills there,

488
00:47:35,760 --> 00:47:41,680
but I, my hunch, my bet that I'm making with this company and putting my effort into it is

489
00:47:41,680 --> 00:47:46,560
that there are a number of things that are very important, that are not immediately commercially

490
00:47:46,560 --> 00:47:53,600
valuable, that contribute to the big brass ring of the big deal there, so we have,

491
00:47:54,640 --> 00:47:59,360
just today we spent most of the day at white rooms, at conference rooms, kind of talking about

492
00:48:00,400 --> 00:48:04,480
finding out what areas we're aligned on, what areas we think need to be explored,

493
00:48:04,480 --> 00:48:09,360
I, hashing out a little bit of differences in understanding on some of the air, the questions,

494
00:48:09,360 --> 00:48:14,640
but I have a, you know, a series of things that I'm building up that I hope will demonstrate to me

495
00:48:14,640 --> 00:48:19,600
that we're at least on the right track, but in the end, you want to get something that winds up

496
00:48:19,600 --> 00:48:26,160
being sort of a virtual being that you can delegate tasks to, that's not just a chat bot,

497
00:48:26,160 --> 00:48:32,240
it's not just a tool that you use, but it starts winding up being options in remote

498
00:48:32,240 --> 00:48:37,840
employees for, for companies where you have the option of like, oh, I liked working with AI Tom

499
00:48:37,840 --> 00:48:42,640
before I'll take five more, you know, of him, put them on the different tasks here, and that's,

500
00:48:42,720 --> 00:48:48,560
I, you know, at the end result, so many of the things that humans do now are mediated by

501
00:48:48,560 --> 00:48:53,040
computer interactions, and the pandemic really did put quite a point on that where

502
00:48:53,040 --> 00:48:57,360
more things than people would have thought possible five years ago really are capable of

503
00:48:57,360 --> 00:49:02,720
being done just through the computer intermediation. And I think that, you know, that is the opportunity

504
00:49:02,720 --> 00:49:07,440
to have this golden age of creative power and ability with magnifying all of that.

505
00:49:08,400 --> 00:49:14,320
Awesome, thanks. I didn't see anyone walk over to the microphone, which I told you,

506
00:49:14,320 --> 00:49:19,680
there's no hand raising just to be clear. There's no hand raising, you have to walk to the microphone.

507
00:49:25,120 --> 00:49:31,600
Take it away. Hello. Thank you so much. So I have a question about kind of this path

508
00:49:31,600 --> 00:49:37,120
towards artificial general intelligence, and I'll kind of frame it in terms of quake. So

509
00:49:37,200 --> 00:49:43,040
in quake, you have fast inverse square root, which was a method that no one would have thought of

510
00:49:43,040 --> 00:49:47,120
to do this process much more efficiently. So I'm wondering when you think towards the path of

511
00:49:47,120 --> 00:49:52,320
artificial general intelligence, how much of it do you think is these ideas that no one was thinking

512
00:49:52,320 --> 00:49:56,480
of that are just a little bit more efficient than before? And how much of it is kind of a combination

513
00:49:56,480 --> 00:50:00,320
of ideas that we already have and that are floating around our circles? Yeah, so there's

514
00:50:00,320 --> 00:50:05,200
some interesting aspects to the question of efficiency, where I have to guard my time to

515
00:50:05,200 --> 00:50:10,720
not spend too much time on optimization, because I love that work. It's like a vacation for me to

516
00:50:10,720 --> 00:50:18,320
have something so clear, just make this number go down. That's fun for me, but it's not the most

517
00:50:18,320 --> 00:50:24,080
important thing, because in so many ways, making something two times faster, four times faster,

518
00:50:24,080 --> 00:50:29,920
even 10 times faster, if it's not on the critical path, that's probably not the right thing to do

519
00:50:29,920 --> 00:50:35,520
now. Now, where it does matter, you get into factors of 100 and 1000, and there are architectural

520
00:50:35,520 --> 00:50:41,520
choices that you make that have these three plus orders of magnitude importance. And those are

521
00:50:41,520 --> 00:50:46,160
important that you not mess those up, that you not do something. And there are architectures that

522
00:50:46,160 --> 00:50:50,720
you could make that are just going to be like involve pointer chasing or something that are just

523
00:50:50,720 --> 00:50:56,320
not going to be good enough, because three orders of magnitude matters. You can't run your experiments

524
00:50:56,320 --> 00:51:01,920
in time, you can't deploy it and all that. But on the other hand, there are some architectures that

525
00:51:02,960 --> 00:51:08,720
may be important that just because they're not easily expressed as tensors in PyTorch or Jax

526
00:51:08,720 --> 00:51:14,560
or whatever, that people shy away from, because you only build the things that the tools you're

527
00:51:14,560 --> 00:51:20,720
familiar with are capable of building. And I do think there is potentially some value there for

528
00:51:20,720 --> 00:51:26,800
architectures that as a low-level programmer that's comfortable writing just raw CUDA and managing

529
00:51:26,800 --> 00:51:35,040
my own network communications for things, there are things that I may do that others wouldn't

530
00:51:35,040 --> 00:51:40,720
consider. And one of the aspects is the boundaries that we put on the things that we're going to do,

531
00:51:40,720 --> 00:51:46,000
one of my boundaries is it has to be able to run in real time. Maybe not from my very first experiment,

532
00:51:46,080 --> 00:51:53,200
but if I can't manage a 30 hertz, 33 millisecond sort of training update for a continuous online

533
00:51:53,200 --> 00:52:00,400
learned algorithm, then I probably won't consider it, because I do consider that necessary for

534
00:52:00,960 --> 00:52:05,280
even while you might grow in a simulated world, at some point people aren't going to really buy it

535
00:52:05,280 --> 00:52:11,440
until they're having a Zoom call with the AI and poking it in various ways and having conversations.

536
00:52:11,520 --> 00:52:17,760
That ability to run in real time goes against the current grain, because things are moving up to

537
00:52:17,760 --> 00:52:22,240
you use a warehouse full of computers, but your step time might be a second and a half or something.

538
00:52:22,960 --> 00:52:27,680
There are alternate ways of structuring your systems such that you use a warehouse full of

539
00:52:27,680 --> 00:52:33,040
computers and you get a 30 millisecond time, but you can't do that with the tools that most people

540
00:52:33,040 --> 00:52:38,080
are using today. So I hope that it does have some benefit, but that's one of those speculative things.

541
00:52:38,080 --> 00:52:42,320
My background may let me do something that might be important, but I can't say with any

542
00:52:42,320 --> 00:52:46,320
confidence that it actually is critical. That's awesome. Thank you very much.

543
00:52:49,360 --> 00:52:54,880
All right, next question. Hey there, John. So I'm a huge fan boy. I just wanted to say thank

544
00:52:54,880 --> 00:53:00,960
you for everything and you've done for this industry and specifically thank you for building Quake.

545
00:53:01,280 --> 00:53:03,280
Excellent.

546
00:53:06,480 --> 00:53:11,200
Quake was a very important game for me for both my social and professional life. Thank you.

547
00:53:12,320 --> 00:53:18,160
But my question is about who in this world has the ability to build the next great thing?

548
00:53:18,960 --> 00:53:25,600
In my mind, throughout history of computers, there's always been like a sole person or a small team

549
00:53:26,080 --> 00:53:32,720
and not some giant corporation that's really pushed things forward. Every programming language

550
00:53:32,720 --> 00:53:39,600
was created by like one or two people. There's Linux. Google was famously started by two dudes in

551
00:53:39,600 --> 00:53:46,640
a garage. Your former employer, Facebook, was created by one guy or one set of twins, depending

552
00:53:46,640 --> 00:53:52,000
on who you ask. And I basically believe you created a lot of these technologies on your own.

553
00:53:52,960 --> 00:53:59,280
But when we look towards AI and VR, there seems to be like these huge requirements for enormous

554
00:53:59,280 --> 00:54:06,880
amounts of data, big training sets, multi-million dollar compute time. So I ask like can a small

555
00:54:06,880 --> 00:54:14,160
team still build the next great thing? So there are, I would say most projects in the world of

556
00:54:14,160 --> 00:54:21,600
importance actually do need larger teams and resources. Building commercial infrastructure,

557
00:54:21,600 --> 00:54:25,680
building highways, these take large teams. And even in the software side of things, building

558
00:54:25,680 --> 00:54:30,960
Chrome takes a large team, building Android takes a large team. And these are super important things

559
00:54:30,960 --> 00:54:37,120
that the world runs on to some degree. So the default state is it probably needs a lot of people

560
00:54:37,120 --> 00:54:44,560
for these big things. But my point about AI or AGI right now being this potentially unique

561
00:54:44,560 --> 00:54:52,400
point of high leverage comes from my belief that I have this whole set of spiel about how

562
00:54:52,400 --> 00:54:56,960
the data is not that large, the compute is probably not that large in the larger scheme of

563
00:54:56,960 --> 00:55:02,720
things. Like a point I make, a year of life is a billion frames at 30 frames per second.

564
00:55:02,720 --> 00:55:08,080
And that fits on a thumb drive. And you can tell that a one-year-old is a conscious intelligent

565
00:55:08,080 --> 00:55:13,520
being. So it does not require all the data on the internet to demonstrate artificial general

566
00:55:13,520 --> 00:55:19,440
intelligence if your algorithm is correct. The arguments about how much compute you may need

567
00:55:19,440 --> 00:55:25,360
are, I don't have as ironclad positions about that, but I have plenty of reason to believe

568
00:55:25,360 --> 00:55:30,000
based on the capabilities of what the networks are doing today and the size of the brain and the

569
00:55:30,000 --> 00:55:35,760
parts that we think we understand what's going on, that it is not a warehouse full of computers.

570
00:55:35,760 --> 00:55:42,240
I mean, I don't think it's one node or even one rack, but in that scale over the course of the

571
00:55:42,240 --> 00:55:48,640
coming decade, we will factor that by, it will be another order of magnitude less. And I do think

572
00:55:48,640 --> 00:55:56,320
it's inevitable, but there is this period where well-healed individuals right now can potentially

573
00:55:56,320 --> 00:56:00,400
take a crack at this. If none of us make it, then eventually it's going to get to the point where

574
00:56:00,400 --> 00:56:06,320
every grad student has the resources to take a stab at these problems and the problem will fall

575
00:56:06,320 --> 00:56:10,960
shortly after that point if it hasn't fallen before that. But I think we're in this magical time

576
00:56:10,960 --> 00:56:16,640
right now where it is a golden opportunity. And I'm honestly surprised that there aren't

577
00:56:16,640 --> 00:56:21,840
more people. I mean, there are hundreds of people like me that were technical people that

578
00:56:23,040 --> 00:56:28,400
succeeded, sold companies, have the resources to go and apply this. I'm kind of surprised that

579
00:56:28,400 --> 00:56:33,280
there aren't more of them taking small stabs at it because even if you think you've got one tenth

580
00:56:33,280 --> 00:56:39,200
of one percent chance of kind of making it and getting all the way, it's kind of a Pascal's

581
00:56:39,200 --> 00:56:45,440
wager sort of thing where the expected value from that, if you have no fear of ruin, is still quite

582
00:56:45,440 --> 00:56:54,960
significant. Great answer. Thank you so much. Hello, John. So I understand that the AGI Gold,

583
00:56:54,960 --> 00:57:00,960
my team, a little bit abstract. So my question goes towards, if you're taking steps towards this

584
00:57:00,960 --> 00:57:06,640
goal, how are you making sure your, how are you measuring your progress? Are you decomposing it

585
00:57:06,640 --> 00:57:12,800
into some problems? Like what are the proofs for the steps? Or maybe abstractly, are you sure

586
00:57:12,800 --> 00:57:18,800
you're going to be making progress towards this 2030 goal? Yeah, so I'm not at all sure about it.

587
00:57:18,800 --> 00:57:25,920
And it is almost one of the key questions. How do you gauge whether your child is growing properly?

588
00:57:25,920 --> 00:57:30,640
Do you have things that are crude like the basic responsiveness tests about things? You track

589
00:57:30,720 --> 00:57:36,960
pupil movement, but how do you tell at the earliest level before a billion steps have gone by what

590
00:57:36,960 --> 00:57:42,960
level of cognitive processing is going on there? And I have like my angles on this involve, you

591
00:57:42,960 --> 00:57:47,920
know, like moving foveas and centers of attention. And there's low level things that I can look at

592
00:57:47,920 --> 00:57:52,960
and say, I believe in intelligence should be following these patterns. And I have these indirect

593
00:57:52,960 --> 00:57:58,640
measures that I can make of it. But I dearly wish that I had better objective measures because

594
00:57:59,600 --> 00:58:05,680
it's undervalued how critical benchmarks were to the pace of this last decade of terrific AI

595
00:58:05,680 --> 00:58:12,080
progress where turning things from a discussion section into numeric values. And yes, there's

596
00:58:12,080 --> 00:58:16,720
downsides and people talk about grad student descent of the problems with leader boards and

597
00:58:16,720 --> 00:58:22,400
different things like that. But overall, it's been enormously valuable. And I, you know, I do worry

598
00:58:22,400 --> 00:58:27,520
a little bit even with the LLMs that I don't really buy a lot of the measures of how they're

599
00:58:27,520 --> 00:58:32,960
benchmarked against each other. And it's an even harder problem for AGI. So I wish I had

600
00:58:32,960 --> 00:58:37,600
a better answer to that because I think it's important. But I think that there's at least

601
00:58:37,600 --> 00:58:43,040
directions that we could be following that we feel pretty good about. But it's entirely possible.

602
00:58:43,040 --> 00:58:47,600
Years could go by and it turns out it was the wrong direction, but it's still worth the try.

603
00:58:48,400 --> 00:58:49,040
Okay, thank you.

604
00:58:52,240 --> 00:58:57,120
Yeah, I had a question about the kind of AGI you visualize building at Keen. So when I think

605
00:58:57,120 --> 00:59:01,200
about AGI, I think there's an embodiment, there's an actual robot functioning in the real world.

606
00:59:01,200 --> 00:59:04,880
So are you thinking about something which is animal like or human like, or is it come

607
00:59:04,880 --> 00:59:10,560
completely virtual? Alright, so I'm not a fan of robots, which puts me at odds with Joseph here.

608
00:59:11,920 --> 00:59:17,280
And, you know, it comes from, of course, I'm the virtual reality guy. So I believe in simulation

609
00:59:17,280 --> 00:59:23,680
and the general ability to, you know, to do valuable things in simulation. While robotics puts

610
00:59:24,080 --> 00:59:29,360
much of the value of working with robots is this discipline of saying you're going to be real time,

611
00:59:29,360 --> 00:59:34,160
you're working with reality, you can't just get slower and slower and make your model better by,

612
00:59:34,160 --> 00:59:39,440
you know, by scaling it in a way contrary to time. So I think that that discipline can still

613
00:59:39,440 --> 00:59:44,160
be maintained in a virtual environment. I am. And I think that's the primary benefit. And there's a

614
00:59:44,160 --> 00:59:50,880
lot of downsides to robots where I did spend over a decade building rocket ships and that drives home

615
00:59:50,880 --> 00:59:56,320
sort of that, you know, the cussedness of physical things. And the less you can be forced to work

616
00:59:56,320 --> 01:00:06,000
with physical objects, the better in most cases. So I think in general, I don't expect us to be

617
01:00:06,000 --> 01:00:11,200
doing anything robot based. Now the question of exactly what simulated environment you have,

618
01:00:11,200 --> 01:00:16,800
there's a broad range there where like right now I'm working with sort of this 2d infinite movie

619
01:00:16,800 --> 01:00:20,960
wall of like moving around looking at different things. And there's a degree of agency there.

620
01:00:20,960 --> 01:00:26,000
And you could learn about 3d environments and you can put 3d games inside there. But to me,

621
01:00:26,000 --> 01:00:30,400
like it's an open question, should it be a virtual reality environment of instead of a

622
01:00:30,400 --> 01:00:35,200
virtual 2d wall? Should it be a physical space where they have to like walk around essentially

623
01:00:35,200 --> 01:00:39,600
to look at different things? And I don't know, I'm leaning towards the simpler salute, the simpler

624
01:00:39,600 --> 01:00:44,720
possibilities right now. But if it turns out that I wind up making a full fledged, I am, you know,

625
01:00:44,720 --> 01:00:49,280
game engine rendered virtual world, I wouldn't be at all surprised if that's where we wind

626
01:00:49,280 --> 01:00:59,920
up in a year or two. Thank you. Thanks John. So it's very special to have you joining our community

627
01:00:59,920 --> 01:01:07,360
here in Edmonton. And when Rich emailed you, you must have thought to yourself whether you wanted

628
01:01:07,360 --> 01:01:15,440
to start this partnership. And obviously you did. So I was wondering about what were the behaviors

629
01:01:15,440 --> 01:01:21,920
that you saw in this community that wanted to make you join our community? And more broadly,

630
01:01:21,920 --> 01:01:26,640
like what should we hold on to? Because we might have taken it for granted because we've been

631
01:01:26,640 --> 01:01:32,160
fostering for so long. Yeah. So the, you know, initially I did not get my hopes up. I was just

632
01:01:32,160 --> 01:01:37,120
like, Hey, it's cool to be having a communication with Richard Sutton. And I was going to try to

633
01:01:37,520 --> 01:01:43,680
my helpful self as much as I could be. And the fact that we did wind up hitting it off well enough

634
01:01:44,320 --> 01:01:50,880
that it's almost remarkable how many areas of overlap that we've got in terms of the way we

635
01:01:50,880 --> 01:02:00,320
look at this. And, you know, in some ways, both me and Dallas and Rich up here in Alberta,

636
01:02:00,320 --> 01:02:05,520
it is, this is not the center of gravity of artificial intelligence research. I, you know,

637
01:02:05,520 --> 01:02:11,200
we are in our own way in the wilderness. And I think there's some commonality that came from that

638
01:02:11,840 --> 01:02:19,600
where there is, you know, there is this kind of fashion in research and especially in the

639
01:02:19,600 --> 01:02:26,800
commercial side of it. And the current looking at things like the reinforcement learning and the

640
01:02:26,800 --> 01:02:32,160
real time online continuous learning, these are not the current fashion for things. I, you know,

641
01:02:32,160 --> 01:02:38,240
great strides are being made with large language models, large batch training, slow steps, inference

642
01:02:38,240 --> 01:02:44,240
only. And, you know, and that's all great. But everybody should be aware that this is not the

643
01:02:44,240 --> 01:02:49,680
be all end all, it doesn't solve all of the problems. So somebody's still got to be looking for

644
01:02:49,680 --> 01:02:55,360
the remaining answers, you know, we don't have them all. So being willing to go against the grain

645
01:02:55,360 --> 01:03:01,280
and to, to step a little bit outside and have a lot of people say, Why are you working on that?

646
01:03:01,280 --> 01:03:06,960
You know, that's not the, you know, the mainstream. That's not where you can go impress the VCs or

647
01:03:06,960 --> 01:03:13,200
whatever. But at some point, somebody has to solve some of these problems. And, you know, the willingness

648
01:03:13,200 --> 01:03:18,960
to go ahead and do that, because you think that it's the right long term solution. It's I, you

649
01:03:18,960 --> 01:03:24,080
know, I do tip my hat to the kind of the academic virtues of you are trying to find truth, you're

650
01:03:24,080 --> 01:03:28,160
trying to find, you know, the knowledge and the way to solve these problems in the hopes that

651
01:03:28,160 --> 01:03:32,560
then they will be applied into the world to produce great value across the different areas.

652
01:03:34,560 --> 01:03:38,800
But yeah, the biggest thing right now is we're both outside the mainstream, but we both have

653
01:03:39,360 --> 01:03:45,520
conviction and we have, you know, reason to believe that it's a profitable direction to be

654
01:03:45,520 --> 01:03:50,880
pursuing. And this is far from the only direction. Again, I'm surprised that there aren't more efforts

655
01:03:50,880 --> 01:03:55,760
like this going on, rather than having the 10th company training their own large language model

656
01:03:55,760 --> 01:04:00,560
to compete with something. There are other interesting problems that may wind up being

657
01:04:00,560 --> 01:04:07,360
even more important. And being excited about the general technology while kind of picking a path

658
01:04:07,360 --> 01:04:14,080
that's not just following somebody else's trail in the specifics, I think is important. You know,

659
01:04:14,080 --> 01:04:22,560
the world needs more people like that. Thanks. Hello. I'm very excited to hear about this

660
01:04:22,560 --> 01:04:28,720
partnership that you have here. And so one of the things that I like about it is that there's a

661
01:04:28,720 --> 01:04:34,400
lot of blue sky research that you sort of have in mind, right, that for the next seven, 10 years,

662
01:04:34,400 --> 01:04:41,760
whatever, there's going to be a lot of focus on these fundamental things, some maybe online,

663
01:04:42,320 --> 01:04:50,000
real time algorithms to do certain things. But you also emphasize that there is no need for a

664
01:04:50,000 --> 01:04:55,280
tangible product in, let's say, in this timeframe, right? So all this sounds very much like an

665
01:04:55,280 --> 01:05:02,240
academic lab. And while this is certainly not one, right? And perhaps, so my question is out of

666
01:05:02,240 --> 01:05:07,040
curiosity that, is this something you are specifically guarding against? Because like deep

667
01:05:07,040 --> 01:05:12,640
mind and companies like this also started with a similar perhaps intention, right, that lots of

668
01:05:12,640 --> 01:05:18,640
blue sky research, solve intelligence, use that to solve everything else. But now, maybe it's not

669
01:05:18,640 --> 01:05:23,120
exactly the same anymore, right? So are you specifically guarding against that? Or is that

670
01:05:23,120 --> 01:05:28,960
something you don't really think about? So I am unashamedly a capitalist. I, you know, I do,

671
01:05:28,960 --> 01:05:33,840
I think that this could make me a trillionaire if everything goes well, it's worth, I, you know,

672
01:05:33,840 --> 01:05:38,320
it's worth kind of aiming for some things that I, you know, producing value, I think it's good for

673
01:05:38,320 --> 01:05:43,760
the world. I mean, I deeply believe that building commercial enterprises is most of what has made

674
01:05:43,760 --> 01:05:49,760
the world what it is today in a very positive sense. So, you know, and I don't have the academic

675
01:05:49,760 --> 01:05:55,360
background, I am, you know, I respect, I recognize I'm standing on the shoulders of all of the academic

676
01:05:55,360 --> 01:05:59,760
work that's been done before. And it was funny because I talked to, I'm, you know, ahead of a

677
01:05:59,760 --> 01:06:03,520
university one time, and I was kind of mentioning how I feel a little guilty being a commercial

678
01:06:03,520 --> 01:06:08,240
company that is built on lots of this research. And he said, you know, your tax dollars have paid

679
01:06:08,240 --> 01:06:12,720
for a whole lot of this research, don't feel bad at all. You know, this is the point of all of this

680
01:06:12,720 --> 01:06:19,600
is to let people try to build on it. So, yeah, it's, it is a commercial effort. And part of that

681
01:06:19,600 --> 01:06:25,760
was a focusing tool for myself, where I spent kind of the prior few years in what I call Victorian

682
01:06:25,760 --> 01:06:30,720
gentleman scientist mode. I was kind of styled like Darwin or Babbage, where I'm a rich guy that

683
01:06:30,720 --> 01:06:35,840
can buy all the scientific tools that he needs and can kind of have my backyard laboratory for

684
01:06:35,840 --> 01:06:41,520
things. And, you know, and I learned a whole lot. I spent the time kind of going through this

685
01:06:41,520 --> 01:06:45,920
extended larval stage about finding out what everybody is up to and getting myself

686
01:06:45,920 --> 01:06:51,120
up to the modern standards there. But it always gave me an out, you know, just at that level,

687
01:06:51,120 --> 01:06:55,520
I could think of it as like, well, it's almost a hobby. I could just quit at any time. I could

688
01:06:55,520 --> 01:07:01,280
divert my time any way that I want. And I had had several, several different organizations

689
01:07:01,280 --> 01:07:06,640
pestering me about form a company, take our investment money. And I didn't need it for the

690
01:07:06,720 --> 01:07:12,640
money. But I looked at it as a focusing tool, where I have in many ways an overactive sense

691
01:07:12,640 --> 01:07:19,280
of responsibility to, you know, to investors, you know, it, it killed me at, at meta, just seeing

692
01:07:19,280 --> 01:07:24,240
all that money going out, I went and like, no, turn it into profitable businesses earlier.

693
01:07:24,240 --> 01:07:29,600
So that is an aspect of it. And then hiring employees means it's like, okay, now I've got

694
01:07:29,600 --> 01:07:35,520
people's paychecks that I am responsible for. So it makes me focus better on the, you know,

695
01:07:35,520 --> 01:07:41,440
on the tasks at hand as well as actually bringing the resources to bear. So yeah, in some ways,

696
01:07:41,440 --> 01:07:48,000
it's a pure research play. And, but at some point it turns into if it works products that

697
01:07:48,000 --> 01:07:57,440
really literally reshape the world. Thank you. Yeah, so I think you touched on this a bit earlier,

698
01:07:57,440 --> 01:08:05,200
but I'm sure many people have tried to hire Dr. Sutton before and failed. And so, yeah,

699
01:08:05,200 --> 01:08:10,080
like, could you talk a bit more about like, why you succeeded where no one else could and was it

700
01:08:10,080 --> 01:08:14,800
like fully attributed to being aligned or? Yeah, so I still, you know, I still wind up

701
01:08:14,800 --> 01:08:19,840
being happily surprised that I'm here today with this because I, you know, a couple of times I

702
01:08:19,840 --> 01:08:24,000
just said, I know you can write your own ticket anywhere, you can go to any of these companies

703
01:08:24,000 --> 01:08:27,680
and everybody would be happy to have the father of modern reinforcement learning just

704
01:08:28,560 --> 01:08:32,400
in their collection. And for some big companies, it is almost like a Pokemon collection,

705
01:08:32,400 --> 01:08:36,880
you know, collect your favorite researchers and, you know, make sure they're on your team and your

706
01:08:36,880 --> 01:08:43,120
deck. But I, you know, I think, you know, part of it is that we want to actually do these things.

707
01:08:43,120 --> 01:08:48,560
It's not about prestige and being out there and kind of staking a claim. We want it to exist.

708
01:08:48,560 --> 01:08:54,880
We want to build it. I am, and I think that there is a sense that a smaller team, there's, I mean,

709
01:08:54,880 --> 01:09:00,160
there's all sorts of drags and friction that you get in big teams. It is wonderful to have a 10,000

710
01:09:00,160 --> 01:09:05,360
GPU cluster that you could go ahead and just kind of get time on whenever you feel like it.

711
01:09:05,360 --> 01:09:10,560
But I am, again, I've got enough options for, you know, for doing things like that,

712
01:09:11,440 --> 01:09:16,880
that I don't think that's going to hold us back. And the ability to have a purity of focus, I think

713
01:09:16,880 --> 01:09:23,680
is important. But I'm, yeah, I'm, I'm still, you know, really very happy that it all worked out this

714
01:09:23,680 --> 01:09:33,200
way. Thank you. Hello. Thank you so much for the talk, John. It was fantastic. So my question

715
01:09:33,200 --> 01:09:41,520
goes back to the discussion about game development and open source. And I would like just to hear

716
01:09:41,520 --> 01:09:48,400
your opinion about what is, what do you see as the future of open source software in the video game

717
01:09:48,400 --> 01:09:56,480
industry, especially since it's a world full of full of IPs and proprietary platforms?

718
01:09:57,440 --> 01:10:03,040
So there is, you know, right now is an interesting inflection point because Unity had their whole

719
01:10:03,040 --> 01:10:08,240
change their terms of services. And this is making a lot of people reevaluate what they're doing.

720
01:10:08,800 --> 01:10:14,000
And it's been a long time since I was really close to the work being done on the open source engines.

721
01:10:14,560 --> 01:10:20,560
But, you know, I believe it's fair to say that Unity with all their employees, they do a lot to

722
01:10:20,560 --> 01:10:26,000
make it a comfortable, cozy development experience. They do provide a lot of value. And you do get a

723
01:10:26,000 --> 01:10:31,120
lot of open source crusaders that just don't acknowledge the value that the commercial companies

724
01:10:31,120 --> 01:10:35,600
bring to the table when they build their software. Because there are a lot of blind spots that the

725
01:10:35,600 --> 01:10:40,240
open source teams, it's, it's weird how the DNA of the people that are willing to work on open

726
01:10:40,240 --> 01:10:44,880
source wind up with certain important blind spots in the kind of applicability of their

727
01:10:44,880 --> 01:10:50,000
software and usability. You make super powerful tools, but, you know, they're not very comfortable

728
01:10:50,000 --> 01:10:56,080
and it winds up making them not the right call for a lot of people. But like I said, I very much

729
01:10:56,080 --> 01:11:00,720
respect EPICS in between position where even if you don't pay them a cent, you can read every line

730
01:11:00,720 --> 01:11:06,560
of code and learn from it and understand how things work. You can pay reasonable license fees.

731
01:11:06,560 --> 01:11:11,280
Having competition there is great, you know, letting people choose. And I always thought that

732
01:11:11,280 --> 01:11:16,720
the trade-offs between Unity and Unreal was a nice market balance to have. And like in virtual

733
01:11:16,720 --> 01:11:22,400
reality, 95% of the developers wound up with Unity because of just the crowd that was attracted.

734
01:11:22,400 --> 01:11:27,520
But it was always expected that if you need serious control, you'd then go with Unreal and

735
01:11:28,080 --> 01:11:33,360
you could write things at a lower level. So there's an opportunity now, I think there will

736
01:11:33,360 --> 01:11:38,800
inevitably be some larger move after Unity stumble here to open source projects.

737
01:11:39,840 --> 01:11:46,240
I don't expect it to be a tsunami that like accelerates and takes over everything because

738
01:11:46,240 --> 01:11:51,120
there are just so many of these grubby business things that are just not fun to do on the open

739
01:11:51,120 --> 01:11:58,240
source projects that I, you know, I could imagine a scenario where somebody like, you know, Meta

740
01:11:58,240 --> 01:12:03,680
gets behind open source things and winds up helping do some of the unpopular, unfun things

741
01:12:04,880 --> 01:12:11,840
for their own kind of self-interested reasons that could be positive. But I think I'm resigned to

742
01:12:11,840 --> 01:12:19,360
a slow pace of migration towards more open source tools. Again, at this point, 30 years back,

743
01:12:19,360 --> 01:12:24,400
I'm disappointed that we aren't further along there relative to like compiler tool chains and

744
01:12:24,400 --> 01:12:29,440
things where the world is a better place for having those be almost universally open source.

745
01:12:29,440 --> 01:12:34,000
And we probably could have been there on some game engine type things, but I don't think the

746
01:12:34,720 --> 01:12:39,520
fundamental reasons why we're not have changed. So I think it's going to be modest changes going

747
01:12:39,520 --> 01:12:45,280
forward still. Thank you so much. I'm being told we're almost out of time and we have a line of

748
01:12:45,280 --> 01:12:49,360
six people. So I'm going to go in a lightning round mode and you get 10 seconds to ask your

749
01:12:49,360 --> 01:12:54,320
question. I can actually hang around a little bit after the actual official end.

750
01:12:55,840 --> 01:12:59,840
Okay. Well, so I worked with Mike and Rich at DeepMind and actually some friends and I left

751
01:12:59,840 --> 01:13:04,720
DeepMind to create a startup here in town using AI to make AI in video games. So first of all,

752
01:13:04,720 --> 01:13:08,640
thank you for not shooting to make a commercial product in the next couple years in the AI and

753
01:13:08,640 --> 01:13:13,920
game space. But my question is more like, I know Rich has a really strong commitment to open research

754
01:13:13,920 --> 01:13:16,960
and it sounds like you embraced that as well with the hacker mentality. So

755
01:13:17,760 --> 01:13:21,440
what are the plans for Keen to like share your insights, your breakthroughs, maybe even your

756
01:13:21,440 --> 01:13:28,800
code? So this was one of the significant conversations that we had. And Rich does

757
01:13:28,800 --> 01:13:33,600
understand the importance of commercial businesses and that there's reasons why these are

758
01:13:33,600 --> 01:13:40,320
fundamentally important and the incentives matter. And there's a talk about like, yes,

759
01:13:40,320 --> 01:13:45,920
I did champion a lot of releases of different things and I would, there's certain technologies

760
01:13:45,920 --> 01:13:50,960
that I'm happy to have us publish. There's other things more like around experiment design and

761
01:13:50,960 --> 01:13:56,960
architectures that I probably don't want us talking about. And it's going to be in a situation like

762
01:13:56,960 --> 01:14:02,320
if, if somebody wants to write a paper about something, we'll have a conversation about it.

763
01:14:02,320 --> 01:14:06,640
And if it's, you know, if it's something at the very tactical level, like, hey, this is an

764
01:14:06,640 --> 01:14:11,440
interesting way to do an optimizer. This is, you know, another way to calculate your, your,

765
01:14:11,440 --> 01:14:17,200
your TD assignments, whatever, that's, that's probably fine. But here is the architecture

766
01:14:17,200 --> 01:14:23,520
of what our proto AI, AGI looks like that's probably not going to be talked about. Because

767
01:14:24,320 --> 01:14:28,560
all of these are going to be things that we see it today where all of the major laboratories,

768
01:14:28,560 --> 01:14:34,480
they can reproduce anybody else's work with a couple sentences of direction and a couple weeks of

769
01:14:34,480 --> 01:14:39,600
time. And I think that's going to be the case for AGI as well. When, you know, when it does all

770
01:14:39,600 --> 01:14:44,320
get solved, the textbook from the future is going to have a relatively thin chapter about this is

771
01:14:44,320 --> 01:14:51,600
actually all that's really required built on the baseline to make it happen. So yeah, there's some

772
01:14:51,600 --> 01:14:55,200
sense of secrecy. And then there's real concerns about any real company is going to have employee

773
01:14:55,200 --> 01:14:59,120
turnover, they're going to walk out the door, you know, NDA or not, they're going to know what the

774
01:14:59,120 --> 01:15:05,760
things are. So there is a large part of this gamble on even if all of this value gets developed,

775
01:15:05,760 --> 01:15:10,720
much of it may dissipate into the broader world, which is great for the world, not so great for

776
01:15:10,720 --> 01:15:16,800
my investors, but we're all still collectively willing to take that risk. Thank you. Okay,

777
01:15:16,800 --> 01:15:22,080
it looked like lightning round didn't work. So I'm going to call it here because we're out of time,

778
01:15:22,080 --> 01:15:27,040
but John has very generously offered to answer some questions. If you have some, we'll make

779
01:15:27,040 --> 01:15:32,320
sure we start with this line first. Yeah. But let's first thank John for telling us so much

780
01:15:32,320 --> 01:15:47,680
insight about his past, present and future. All right, thanks. Do I have to vacate the stage?

781
01:15:47,680 --> 01:15:59,040
Yeah, I don't know what we do next. Thank you. Bye.

